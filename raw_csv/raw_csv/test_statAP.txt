2017-03-16T23:25:38Z|2017-03-15T17:03:56Z|http://arxiv.org/abs/1703.05264v1|http://arxiv.org/pdf/1703.05264v1|Smooth Image-on-Scalar Regression for Brain Mapping|Brain mapping is an increasingly important tool in neurology and psychiatry researches for the realization of data-driven personalized medicine in the big data era, which learns the statistical links between brain images and subject level features. Taking images as responses, the task raises a lot of challenges due to the high dimensionality of the image with relatively small number of samples, as well as the noisiness of measurements in medical images.   In this paper we propose a novel method {\it Smooth Image-on-scalar Regression} (SIR) for recovering the true association between an image outcome and scalar predictors. The estimator is achieved by minimizing a mean squared error with a total variation (TV) regularization term on the predicted mean image across all subjects. It denoises the images from all subjects and at the same time returns the coefficient maps estimation. We propose an algorithm to solve this optimization problem, which is efficient when combined with recent advances in graph fused lasso solvers. The statistical consistency of the estimator is shown via an oracle inequality.   Simulation results demonstrate that the proposed method outperforms existing methods with separate denoising and regression steps. Especially, SIR shows an evident advantage in recovering signals in small regions. We apply SIR on Alzheimer's Disease Neuroimaging Initiative data and produce interpretable brain maps of the PET image to patient-level features include age, gender, genotype and disease groups.|['Ying Liu', 'Bowei Yan']|['stat.ME', 'stat.AP']
2017-03-16T23:25:38Z|2017-03-15T14:22:09Z|http://arxiv.org/abs/1703.05172v1|http://arxiv.org/pdf/1703.05172v1|Bayesian adaptive bandit-based designs using the Gittins index for   multi-armed trials with normally distributed endpoints|Adaptive designs for multi-armed clinical trials have become increasingly popular recently in many areas of medical research because of their potential to shorten development times and to increase patient response. However, developing response-adaptive trial designs that offer patient benefit while ensuring the resulting trial avoids bias and provides a statistically rigorous comparison of the different treatments included is highly challenging. In this paper, the theory of Multi-Armed Bandit Problems is used to define a family of near optimal adaptive designs in the context of a clinical trial with a normally distributed endpoint with known variance. Through simulation studies based on an ongoing trial as a motivation we report the operating characteristics (type I error, power, bias) and patient benefit of these approaches and compare them to traditional and existing alternative designs. These results are then compared to those recently published in the context of Bernoulli endpoints. Many limitations and advantages are similar in both cases but there are also important differences, specially with respect to type I error control. This paper proposes a simulation-based testing procedure to correct for the observed type I error inflation that bandit-based and adaptive rules can induce. Results presented extend recent work by considering a normally distributed endpoint, a very common case in clinical practice yet mostly ignored in the response-adaptive theoretical literature, and illustrate the potential advantages of using these methods in a rare disease context. We also recommend a suitable modified implementation of the bandit-based adaptive designs for the case of common diseases.|['Adam Smith', 'Sofia S. Villar']|['stat.AP']
2017-03-16T23:25:38Z|2017-03-15T12:07:47Z|http://arxiv.org/abs/1703.05103v1|http://arxiv.org/pdf/1703.05103v1|Do pay-for-performance incentives lead to a better health outcome?|Pay-for-performance approaches have been widely adopted in order to drive improvements in the quality of healthcare provision. Previous studies evaluating the impact of these programs are either limited by the number of health outcomes or of medical conditions considered. In this paper, we evaluate the effectiveness of a pay-for-performance program on the basis of five health outcomes and across a wide range of medical conditions. The context of the study is the Lombardy region in Italy, where a rewarding program was introduced in 2012. The policy evaluation is based on a difference-in-differences approach. The model includes multiple dependent outcomes, that allow quantifying the joint effect of the program, and random effects, that account for the heterogeneity of the data at the ward and hospital level. Our results show that the policy had a positive effect on the hospitals' performance in terms of those outcomes that can be more influenced by a managerial activity, namely the number of readmissions, transfers and returns to the surgery room. No significant changes which can be related to the pay-for-performance introduction are observed for the number of voluntary discharges and for mortality. Finally, our study shows evidence that the medical wards have reacted more strongly to the pay-for-performance program than the surgical ones, whereas only limited evidence is found in support of a different policy reaction across different types of hospital ownership.|['Alina Peluso', 'Paolo Berta', 'Veronica Vinciotti']|['stat.AP']
2017-03-16T23:25:38Z|2017-03-15T06:42:41Z|http://arxiv.org/abs/1703.04961v1|http://arxiv.org/pdf/1703.04961v1|Predicting with limited data - Increasing the accuracy in VIS-NIR   diffuse reflectance spectroscopy by SMOTE|Diffuse reflectance spectroscopy is a powerful technique to predict soil properties. It can be used in situ to provide data inexpensively and rapidly compared to the standard laboratory measurements. Because most spectral data bases contain air-dried samples scanned in the laboratory, field spectra acquired in situ are either absent or rare in calibration data sets. However, when models are calibrated on air-dried spectra, prediction using field spectra are often inaccurate. We propose a framework to calibrate partial least squares models when field spectra are rare using synthetic minority oversampling technique (SMOTE). We calibrated a model to predict soil organic carbon content using air-dried spectra spiked with synthetic field spectra. The root mean-squared error of prediction decreased from 6.18 to 2.12 mg g$^{-1}$ and $R^2$ increased from $-$0.53 to 0.82 compared to the model calibrated on air-dried spectra only.|['Christina Bogner', 'Anna KÃ¼hnel', 'Bernd Huwe']|['stat.AP']
2017-03-16T23:25:38Z|2017-03-15T06:36:58Z|http://arxiv.org/abs/1703.04957v1|http://arxiv.org/pdf/1703.04957v1|An algorithm for removing sensitive information: application to   race-independent recidivism prediction|"Predictive modeling is increasingly being employed to assist human decision-makers. One purported advantage of replacing or augmenting human judgment with computer models in high stakes settings-- such as sentencing, hiring, policing, college admissions, and parole decisions-- is the perceived ""neutrality"" of computers. It is argued that because computer models do not hold personal prejudice, the predictions they produce will be equally free from prejudice. There is growing recognition that employing algorithms does not remove the potential for bias, and can even amplify it if the training data were generated by a process that is itself biased. In this paper, we provide a probabilistic notion of algorithmic bias. We propose a method to eliminate bias from predictive models by removing all information regarding protected variables from the data to which the models will ultimately be trained. Unlike previous work in this area, our framework is general enough to accommodate data on any measurement scale. Motivated by models currently in use in the criminal justice system that inform decisions on pre-trial release and parole, we apply our proposed method to a dataset on the criminal histories of individuals at the time of sentencing to produce ""race-neutral"" predictions of re-arrest. In the process, we demonstrate that a common approach to creating ""race-neutral"" models-- omitting race as a covariate-- still results in racially disparate predictions. We then demonstrate that the application of our proposed method to these data removes racial disparities from predictions with minimal impact on predictive accuracy."|['James E. Johndrow', 'Kristian Lum']|['stat.AP']
2017-03-16T23:25:38Z|2017-03-14T23:04:14Z|http://arxiv.org/abs/1703.04812v1|http://arxiv.org/pdf/1703.04812v1|An alternative representation of the negative binomial-Lindley   distribution. New results and applications|In this paper we present an alternative representation of the Negative Binomial--Lindley distribution recently proposed by Zamani and Ismail (2010) which shows some advantages over the latter model. This new formulation provides a tractable model with attractive properties which makes it suitable for application not only in insurance settings but also in other fields where overdispersion is observed. Basic properties of the new distribution are studied. A recurrence for the probabilities of the new distribution and an integral equation for the probability density function of the compound version, when the claim severities are absolutely continuous, are derived. Estimation methods are discussed and a numerical application is given.|['Emilio Gomez-Deniz', 'Enrique Calderin-Ojeda']|['stat.AP']
2017-03-16T23:25:38Z|2017-03-14T18:16:06Z|http://arxiv.org/abs/1703.04642v1|http://arxiv.org/pdf/1703.04642v1|Robust Morphometric Analysis based on Landmarks. Applications|Procrustes Analysis is a Morphometric method based on Configurations of Landmarks that estimates the superimposition parameters by least-squares; for this reason, the procedure is very sensitive to outliers. In the first part of the paper we robustify this technique to classify individuals from a descriptive point of view. In the literature there are also classical results, based on the normality of the observations, to test whether there are significant differences between individuals. In the second part of the paper we determine a Von Mises plus Saddlepoint approximation for the tail probability of the Procrustes Statistic when the observations come from a model close to the normal. We conclude the paper with some applications using the Geographical Information System QGIS.|['A. Garcia-Perez', 'M. A. Cabrero-Ortega']|['stat.AP']
2017-03-16T23:25:38Z|2017-03-13T11:31:42Z|http://arxiv.org/abs/1703.04341v1|http://arxiv.org/pdf/1703.04341v1|Response adaptive designs for binary responses: how to offer patient   benefit while being robust to time trends?|"Response-adaptive randomisation (RAR) can considerably improve the chances of a successful treatment outcome for patients in a clinical trial by skewing the allocation probability towards better performing treatments as data accumulates. There is considerable interest in using RAR designs in drug development for rare diseases, where traditional designs are not feasible or ethically objectionable. In this paper we discuss and address a major criticism of RAR: the undesirable type I error inflation due to unknown time trends in the trial. Time trends can appear because of changes in the characteristics of recruited patients - so-called ""patient drift"". Patient drift is a realistic concern for clinical trials in rare diseases because these typically recruit patients over a very long period of time. We compute by simulations how large the type I error inflation is as a function of the time trend magnitude in order to determine in which contexts a potentially costly correction is actually necessary. We then assess the ability of different correction methods to preserve type I error in this context and their performance in terms of other operating characteristics, including patient benefit and power. We make recommendations of which correction methods are most suitable in the rare disease context for several RAR rules, differentiating between the two-armed and the multi-armed case. We further propose a RAR design for multi-armed clinical trials, which is computationally cheap and robust to several time trends considered."|['Sofia S. Villar', 'Jack Bowden', 'James Wason']|['stat.AP']
2017-03-16T23:25:38Z|2017-03-13T10:14:20Z|http://arxiv.org/abs/1703.04312v1|http://arxiv.org/pdf/1703.04312v1|Assessing Potential Wind Energy Resources in Saudi Arabia with a Skew-t   Distribution|Facing increasing domestic energy consumption from population growth and industrialization, Saudi Arabia is aiming to reduce its reliance on fossil fuels and to broaden its energy mix by expanding investment in renewable energy sources, including wind energy. A preliminary task in the development of wind energy infrastructure is the assessment of wind energy potential, a key aspect of which is the characterization of its spatio-temporal behavior. In this study we examine the impact of internal climate variability on seasonal wind power density fluctuations using 30 simulations from the Large Ensemble Project (LENS) developed at the National Center for Atmospheric Research. Furthermore, a spatio-temporal model for daily wind speed is proposed with neighbor-based cross-temporal dependence, and a multivariate skew-t distribution to capture the spatial patterns of higher order moments. The model can be used to generate synthetic time series over the entire spatial domain that adequately reproduces the internal variability of the LENS dataset.|['Felipe Tagle', 'Stefano Castruccio', 'Paola Crippa', 'Marc G. Genton']|['stat.AP']
2017-03-16T23:25:38Z|2017-03-12T08:11:29Z|http://arxiv.org/abs/1703.04081v1|http://arxiv.org/pdf/1703.04081v1|Feature overwriting as a finite mixture process: Evidence from   comprehension data|"The ungrammatical sentence ""The key to the cabinets are on the table"" is known to lead to an illusion of grammaticality. As discussed in the meta-analysis by Jaeger et al., 2017, faster reading times are observed at the verb are in the agreement-attraction sentence above compared to the equally ungrammatical sentence ""The key to the cabinet are on the table"". One explanation for this facilitation effect is the feature percolation account: the plural feature on cabinets percolates up to the head noun key, leading to the illusion. An alternative account is in terms of cue-based retrieval (Lewis & Vasishth, 2005), which assumes that the non-subject noun cabinets is misretrieved due to a partial feature-match when a dependency completion process at the auxiliary initiates a memory access for a subject with plural marking. We present evidence for yet another explanation for the observed facilitation. Because the second sentence has two nouns with identical number, it is possible that these are, in some proportion of trials, more difficult to keep distinct, leading to slower reading times at the verb in the first sentence above; this is the feature overwriting account of Nairne, 1990. We show that the feature overwriting proposal can be implemented as a finite mixture process. We reanalysed ten published data-sets, fitting hierarchical Bayesian mixture models to these data assuming a two-mixture distribution. We show that in nine out of the ten studies, a mixture distribution corresponding to feature overwriting furnishes a superior fit over both the feature percolation and the cue-based retrieval accounts."|['Shravan Vasishth', 'Lena A. Jaeger', 'Bruno Nicenboim']|['stat.ML', 'cs.CL', 'stat.AP']
2017-03-16T23:25:42Z|2017-03-12T02:07:37Z|http://arxiv.org/abs/1703.04056v1|http://arxiv.org/pdf/1703.04056v1|Quantifying the strength of structural connectivity underlying   functional brain networks|In recent years, there has been strong interest in neuroscience studies to investigate brain organization through networks of brain regions that demonstrate strong functional connectivity (FC). These networks are extracted from observed fMRI using data-driven analytic methods such as independent component analysis (ICA). A notable limitation of these FC methods is that they do not provide any information on the underlying structural connectivity (SC), which is believed to serve as the basis for interregional interactions in brain activity. We propose a new statistical measure of the strength of SC (sSC) underlying FC networks obtained from data-driven methods. The sSC measure is developed using information from diffusion tensor imaging (DTI) data, and can be applied to compare the strength of SC across different FC networks. Furthermore, we propose a reliability index for data-driven FC networks to measure the reproducibility of the networks through re-sampling the observed data. To perform statistical inference such as hypothesis testing on the sSC, we develop a formal variance estimator of sSC based a spatial semivariogram model with a novel distance metric. We demonstrate the performance of the sSC measure and its estimation and inference methods with simulation studies. For real data analysis, we apply our methods to a multimodal imaging study with resting-state fMRI and DTI data from 20 healthy controls and 20 subjects with major depressive disorder. Results show that well-known resting state networks all demonstrate higher SC within the network as compared to the average structural connections across the brain. We also found that sSC is positively associated with the reliability index, indicating that the FC networks that have stronger underlying SC are more reproducible across samples.|['Phebe Brenne Kemmer', 'F. DuBois Bowman', 'Helen Mayberg', 'Ying Guo']|['stat.AP', 'q-bio.NC']
2017-03-16T23:25:42Z|2017-03-10T22:46:09Z|http://arxiv.org/abs/1703.03862v1|http://arxiv.org/pdf/1703.03862v1|Joint Embedding of Graphs|Feature extraction and dimension reduction for networks is critical in a wide variety of domains. Efficiently and accurately learning features for multiple graphs has important applications in statistical inference on graphs. We propose a method to jointly embed multiple undirected graphs. Given a set of graphs, the joint embedding method identifies a linear subspace spanned by rank one symmetric matrices and projects adjacency matrices of graphs into this subspace. The projection coefficients can be treated as features of the graphs. We also propose a random graph model which generalizes classical random graph model and can be used to model multiple graphs. We show through theory and numerical experiments that under the model, the joint embedding method produces estimates of parameters with small errors. Via simulation experiments, we demonstrate that the joint embedding method produces features which lead to state of the art performance in classifying graphs. Applying the joint embedding method to human brain graphs, we find it extract interpretable features that can be used to predict individual composite creativity index.|['Shangsi Wang', 'Joshua T. Vogelstein', 'Carey E. Priebe']|['stat.AP', 'cs.LG', 'stat.ML']
2017-03-16T23:25:42Z|2017-03-10T22:03:17Z|http://arxiv.org/abs/1703.03853v1|http://arxiv.org/pdf/1703.03853v1|PairCloneTree: Reconstruction of Tumor Subclone Phylogeny Based on   Mutation Pairs using Next Generation Sequencing Data|We present a latent feature allocation model to reconstruct tumor subclones subject to phylogenetic evolution that mimics tumor evolution. Similar to most current methods, we consider data from next-generation sequencing. Unlike most methods that use information in short reads mapped to single nucleotide variants (SNVs), we consider subclone reconstruction using pairs of two proximal SNVs that can be mapped by the same short reads. As part of the Bayesian inference model, we construct a phylogenetic tree prior. The use of the tree structure in the prior greatly strengthens inference. Only subclones that can be approximated by a phylogenetic tree are assigned non-negligible probability. The proposed Bayesian framework implies posterior distributions on the number of subclones, their genotypes, cellular proportions, and the phylogenetic tree spanned by the inferred subclones. The proposed method is validated against different sets of simulated and real-world data using single and multiple tumor samples. An open source software package is available at http://www.compgenome.org/pairclonetree|['Tianjian Zhou', 'Subhajit Sengupta', 'Peter Mueller', 'Yuan Ji']|['stat.AP']
2017-03-16T23:25:42Z|2017-03-10T18:41:00Z|http://arxiv.org/abs/1703.03790v1|http://arxiv.org/pdf/1703.03790v1|Summertime, and the livin is easy: Winter and summer pseudoseasonal life   expectancy in the United States|"In temperate climates, mortality is seasonal with a winter-dominant pattern, due in part to pneumonia and influenza. Cardiac causes, which are the leading cause of death in the United States, are also winter-seasonal although it is not clear why. Interactions between circulating respiratory viruses (f.e., influenza) and cardiac conditions have been suggested as a cause of winter-dominant mortality patterns. We propose and implement a way to estimate an upper bound on mortality attributable to winter-dominant viruses like influenza. We calculate 'pseudo-seasonal' life expectancy, dividing the year into two six-month spans, one encompassing winter the other summer. During the summer when the circulation of respiratory viruses is drastically reduced, life expectancy is about one year longer. We also quantify the seasonal mortality difference in terms of seasonal ""equivalent ages"" (defined herein) and proportional hazards. We suggest that even if viruses cause excess winter cardiac mortality, the population-level mortality reduction of a perfect influenza vaccine would be much more modest than is often recognized."|['Tina Ho', 'Andrew Noymer']|['stat.AP']
2017-03-16T23:25:42Z|2017-03-10T16:35:40Z|http://arxiv.org/abs/1703.03753v1|http://arxiv.org/pdf/1703.03753v1|Latent Gaussian Mixture Models for Nationwide Kidney Transplant Center   Evaluation|Five year post-transplant survival rate is an important indicator on quality of care delivered by kidney transplant centers in the United States. To provide a fair assessment of each transplant center, an effect that represents the center-specific care quality, along with patient level risk factors, is often included in the risk adjustment model. In the past, the center effects have been modeled as either fixed effects or Gaussian random effects, with various pros and cons. Our numerical analyses reveal that the distributional assumptions do impact the prediction of center effects especially when the effect is extreme. To bridge the gap between these two approaches, we propose to model the transplant center effect as a latent random variable with a finite Gaussian mixture distribution. Such latent Gaussian mixture models provide a convenient framework to study the heterogeneity among the transplant centers. To overcome the weak identifiability issues, we propose to estimate the latent Gaussian mixture model using a penalized likelihood approach, and develop sequential locally restricted likelihood ratio tests to determine the number of components in the Gaussian mixture distribution. The fitted mixture model provides a convenient means of controlling the false discovery rate when screening for underperforming or outperforming transplant centers. The performance of the methods is verified by simulations and by the analysis of the motivating data example.|['Lanfeng Pan', 'Yehua Li', 'Kevin He', 'Yanming Li', 'Yi Li']|['stat.AP']
2017-03-16T23:25:42Z|2017-03-09T16:56:27Z|http://arxiv.org/abs/1703.03340v1|http://arxiv.org/pdf/1703.03340v1|Adaptive Non-uniform Compressive Sampling for Time-varying Signals|In this paper, adaptive non-uniform compressive sampling (ANCS) of time-varying signals, which are sparse in a proper basis, is introduced. ANCS employs the measurements of previous time steps to distribute the sensing energy among coefficients more intelligently. To this aim, a Bayesian inference method is proposed that does not require any prior knowledge of importance levels of coefficients or sparsity of the signal. Our numerical simulations show that ANCS is able to achieve the desired non-uniform recovery of the signal. Moreover, if the signal is sparse in canonical basis, ANCS can reduce the number of required measurements significantly.|['Alireza Zaeemzadeh', 'Mohsen Joneidi', 'Nazanin Rahnavard']|['stat.AP', 'cs.IT', 'math.IT']
2017-03-16T23:25:42Z|2017-03-09T10:19:53Z|http://arxiv.org/abs/1703.03213v1|http://arxiv.org/pdf/1703.03213v1|Kernel intensity estimation, bootstrapping and bandwidth selection for   inhomogeneous point processes depending on spatial covariates|In the point process context, kernel intensity estimation has been mainly restricted to exploratory analysis due to its lack of consistency. However the use of covariates has allow to design consistent alternatives under some restrictive assumptions. In this paper we focus our attention on de\-fi\-ning an appropriate framework to derive a consistent kernel intensity estimator using covariates, as well as a consistent smooth bootstrap procedure. For spatial point processes with covariates there is no specific bandwidth selector, hence, we define two new data-driven procedures specifically designed for this scenario: a rule-of-thumb and a plug-in bandwidth based on the bootstrap method previously introduced. A simulation study is accomplished to understand the behaviour of these procedures in finite samples. Finally, we apply the techniques to a real set of data made up of wildfires in Canada during June 2015, using meteorological information as covariates.|['M. I. Borrajo', 'W. GonzÃ¡lez-Manteiga', 'M. D. MartÃ­nez-Miranda']|['stat.ME', 'stat.AP', '62G05, 62G09, 62H11, 60G55, 60-08']
2017-03-16T23:25:42Z|2017-03-08T15:23:00Z|http://arxiv.org/abs/1703.02870v1|http://arxiv.org/abs/1703.02870v1|Statistical Inference in Political Networks Research|Researchers interested in statistically modeling network data have a well-established and quickly growing set of approaches from which to choose. Several of these methods have been regularly applied in research on political networks, while others have yet to permeate the field. Here, we review the most prominent methods of inferential network analysis---both for cross-sectionally and longitudinally observed networks including (temporal) exponential random graph models, latent space models, the quadratic assignment procedure, and stochastic actor oriented models. For each method, we summarize its analytic form, identify prominent published applications in political science and discuss computational considerations. We conclude with a set of guidelines for selecting a method for a given application.|['Bruce A. Desmarais', 'Skyler J. Cranmer']|['stat.AP', 'cs.SI', 'physics.soc-ph']
2017-03-16T23:25:42Z|2017-03-08T00:47:45Z|http://arxiv.org/abs/1703.02650v1|http://arxiv.org/pdf/1703.02650v1|Joint Multichannel Deconvolution and Blind Source Separation|Blind Source Separation (BSS) is a challenging matrix factorization problem that plays a central role in multichannel imaging science. In a large number of applications, such as astrophysics, current unmixing methods are limited since real-world mixtures are generally affected by extra instrumental effects like blurring. Therefore, BSS has to be solved jointly with a deconvolution problem, which requires tackling a new inverse problem: deconvolution BSS (DBSS). In this article, we introduce an innovative DBSS approach, called DecGMCA, based on sparse signal modeling and an efficient alternative projected least square algorithm. Numerical results demonstrate that the DecGMCA algorithm performs very well on simulations. It further highlights the importance of jointly solving BSS and deconvolution instead of considering these two problems independently. Furthermore, the performance of the proposed DecGMCA algorithm is demonstrated on simulated radio-interferometric data.|['Ming Jiang', 'JÃ©rÃ´me Bobin', 'Jean-Luc Starck']|['stat.AP', 'cs.IT', 'math.IT']
2017-03-16T23:25:42Z|2017-03-07T18:14:54Z|http://arxiv.org/abs/1703.02502v1|http://arxiv.org/pdf/1703.02502v1|Clustering Methods for Electricity Consumers: An Empirical Study in   Hvaler-Norway|The development of Smart Grid in Norway in specific and Europe/US in general will shortly lead to the availability of massive amount of fine-grained spatio-temporal consumption data from domestic households. This enables the application of data mining techniques for traditional problems in power system. Clustering customers into appropriate groups is extremely useful for operators or retailers to address each group differently through dedicated tariffs or customer-tailored services. Currently, the task is done based on demographic data collected through questionnaire, which is error-prone. In this paper, we used three different clustering techniques (together with their variants) to automatically segment electricity consumers based on their consumption patterns. We also proposed a good way to extract consumption patterns for each consumer. The grouping results were assessed using four common internal validity indexes. We found that the combination of Self Organizing Map (SOM) and k-means algorithms produce the most insightful and useful grouping. We also discovered that grouping quality cannot be measured effectively by automatic indicators, which goes against common suggestions in literature.|['The-Hien Dang-Ha', 'Roland Olsson', 'Hao Wang']|['stat.AP']
2017-03-16T23:25:46Z|2017-03-07T15:43:22Z|http://arxiv.org/abs/1703.02441v1|http://arxiv.org/pdf/1703.02441v1|Statistical Analysis of the Ricker Model|The Ricker model was introduced in the context of managing fishing stocks. It is a discrete non-linear iterative model given by $N(t+1)=rN(t)\exp(-N(t))$ where $N(t)$ is the population at time $t$. The model treated in this paper includes a random component $N(t+1)=rN(t)\exp(-N(t)+\varepsilon(t+1))$ and what is observed at time $t$ is a Poisson random variable with parameter $\varphi N(t)$. Such a model has been analysed using `synthetic likelihood' and ABC (Approximate Bayesian Computation). In contrast this paper takes a non-likelihood approach and treats the model in a consistent manner as an approximation. The goal is to specify those parameter values if any which are consistent with the data.|['Laurie Davies']|['stat.AP', '62M99']
2017-03-16T23:25:46Z|2017-03-07T11:15:14Z|http://arxiv.org/abs/1703.02329v1|http://arxiv.org/pdf/1703.02329v1|Time and media-use of Italian Generation Y: dimensions of leisure   preferences|"Time spent in leisure is not a minor research question as it is acknowledged as a key aspect of one's quality of life. The primary aim of this article is to qualify time and Internet use of Italian Generation Y beyond media hype and assumptions. To this aim, we apply a multidimensional extension of Item Response Theory models to the Italian ""Multipurpose survey on households: aspects of daily life"" to ascertain the relevant dimensions of Generation Y time-use. We show that the use of technology is neither the first nor the foremost time-use activity of Italian Generation Y, who still prefers to use its time to socialise and have fun with friends in a non media-medalled manner."|['Michela Gnaldi', 'Simone Del Sarto']|['stat.AP', 'cs.CY']
2017-03-16T23:25:46Z|2017-03-14T18:52:48Z|http://arxiv.org/abs/1703.02236v2|http://arxiv.org/pdf/1703.02236v2|Propensity score prediction for electronic healthcare databases using   Super Learner and High-dimensional Propensity Score Methods|"The optimal learner for prediction modeling varies depending on the underlying data-generating distribution. Super Learner (SL) is a generic ensemble learning algorithm that uses cross-validation to select among a ""library"" of candidate prediction models. The SL is not restricted to a single prediction model, but uses the strengths of a variety of learning algorithms to adapt to different databases. While the SL has been shown to perform well in a number of settings, it has not been thoroughly evaluated in large electronic healthcare databases that are common in pharmacoepidemiology and comparative effectiveness research. In this study, we applied and evaluated the performance of the SL in its ability to predict treatment assignment using three electronic healthcare databases. We considered a library of algorithms that consisted of both nonparametric and parametric models. We also considered a novel strategy for prediction modeling that combines the SL with the high-dimensional propensity score (hdPS) variable selection algorithm. Predictive performance was assessed using three metrics: the negative log-likelihood, area under the curve (AUC), and time complexity. Results showed that the best individual algorithm, in terms of predictive performance, varied across datasets. The SL was able to adapt to the given dataset and optimize predictive performance relative to any individual learner. Combining the SL with the hdPS was the most consistent prediction method and may be promising for PS estimation and prediction modeling in electronic healthcare databases."|['Cheng Ju', 'Mary Combs', 'Samuel D Lendle', 'Jessica M Franklin', 'Richard Wyss', 'Sebastian Schneeweiss', 'Mark J. van der Laan']|['stat.AP', 'stat.ML']
2017-03-16T23:25:46Z|2017-03-06T21:17:42Z|http://arxiv.org/abs/1703.02112v1|http://arxiv.org/pdf/1703.02112v1|Process convolution approaches for modeling interacting trajectories|"Gaussian processes are a fundamental statistical tool used in a wide range of applications. In the spatio-temporal setting, several families of covariance functions exist to accommodate a wide variety of dependence structures arising in different applications. These parametric families can be restrictive and are insufficient in some situations. In contrast, process convolutions represent a flexible, interpretable approach to defining the covariance of a Gaussian process and have modest requirements to ensure validity. We introduce a generalization of the process convolution approach that employs multiple convolutions sequentially to form a ""process convolution chain."" In our proposed multi-stage framework, complex dependencies that arise from a combination of different interacting mechanisms are decomposed into a series of interpretable kernel smoothers. We demonstrate an application of process convolution chains to model killer whale movement, in which the paths taken by multiple individuals are not independent, but reflect dynamic social interactions within the population. Our proposed model for dependent movement provides inference for the latent dynamic social structure in the study population. Additionally, by leveraging the positive dependence among individual paths, we achieve a reduction in uncertainty for the estimated locations of the whales, compared to a model that treats paths as independent."|['Henry R. Scharf', 'Mevin B. Hooten', 'Devin S. Johnson', 'John W. Durban']|['stat.ME', 'stat.AP']
2017-03-16T23:25:46Z|2017-03-06T19:33:24Z|http://arxiv.org/abs/1703.02078v1|http://arxiv.org/pdf/1703.02078v1|Cross-screening in observational studies that test many hypotheses|"We discuss observational studies that test many causal hypotheses, either hypotheses about many outcomes or many treatments. To be credible an observational study that tests many causal hypotheses must demonstrate that its conclusions are neither artifacts of multiple testing nor of small biases from nonrandom treatment assignment. In a sense that needs to be defined carefully, hidden within a sensitivity analysis for nonrandom assignment is an enormous correction for multiple testing: in the absence of bias, it is extremely improbable that multiple testing alone would create an association insensitive to moderate biases. We propose a new strategy called ""cross-screening"", different from but motivated by recent work of Bogomolov and Heller on replicability. Cross-screening splits the data in half at random, uses the first half to plan a study carried out on the second half, then uses the second half to plan a study carried out on the first half, and reports the more favorable conclusions of the two studies correcting using the Bonferroni inequality for having done two studies. If the two studies happen to concur, then they achieve Bogomolov-Heller replicability; however, importantly, replicability is not required for strong control of the family-wise error rate, and either study alone suffices for firm conclusions. In randomized studies with a few hypotheses, cross-split screening is not an attractive method when compared with conventional methods of multiplicity control, but it can become attractive when hundreds or thousands of hypotheses are subjected to sensitivity analyses in an observational study. We illustrate the technique by comparing 46 biomarkers in individuals who consume large quantities of fish versus little or no fish."|['Qingyuan Zhao', 'Dylan S. Small', 'Paul R. Rosenbaum']|['stat.ME', 'stat.AP']
2017-03-16T23:25:46Z|2017-03-06T15:58:26Z|http://arxiv.org/abs/1703.01937v1|http://arxiv.org/pdf/1703.01937v1|Reputation Dynamics in a Market for Illicit Drugs|We analyze reputation dynamics in an online market for illicit drugs using a novel dataset of prices and ratings. The market is a black market, and so contracts cannot be enforced. We study the role that reputation plays in alleviating adverse selection in this market. We document the following stylized facts: (i) There is a positive relationship between the price and the rating of a seller. This effect is increasing in the number of reviews left for a seller. A mature highly-rated seller charges a 20% higher price than a mature low-rated seller. (ii) Sellers with more reviews charge higher prices regardless of rating. (iii) Low-rated sellers are more likely to exit the market and make fewer sales. We show that these stylized facts are explained by a dynamic model of adverse selection, ratings, and exit, in which buyers form rational inferences about the quality of a seller jointly from his rating and number of sales. Sellers who receive low ratings initially charge the same price as highly-rated sellers since early reviews are less informative about quality. Bad sellers exit rather than face lower prices in the future. We provide conditions under which our model admits a unique equilibrium. We estimate the model, and use the result to compute the returns to reputation in the market. We find that the market would have collapsed due to adverse selection in the absence of a rating system.|['Nick Janetos', 'Jan Tilly']|['stat.AP']
2017-03-16T23:25:46Z|2017-03-06T09:24:07Z|http://arxiv.org/abs/1703.01776v1|http://arxiv.org/pdf/1703.01776v1|Online Sequential Monte Carlo smoother for partially observed stochastic   differential equations|This paper introduces a new algorithm to approximate smoothed additive functionals for partially observed stochastic differential equations. This method relies on a recent procedure which allows to compute such approximations online, i.e. as the observations are received, and with a computational complexity growing linearly with the number of Monte Carlo samples. This online smoother cannot be used directly in the case of partially observed stochastic differential equations since the transition density of the latent data is usually unknown. We prove that a similar algorithm may still be defined for partially observed continuous processes by replacing this unknown quantity by an unbiased estimator obtained for instance using general Poisson estimators. We prove that this estimator is consistent and its performance are illustrated using data from two models.|['Pierre Gloaguen', 'Marie-Pierre Etienne', 'Sylvain Le Corff']|['stat.ME', 'stat.AP']
2017-03-16T23:25:46Z|2017-03-04T21:50:25Z|http://arxiv.org/abs/1703.01526v1|http://arxiv.org/abs/1703.01526v1|High Accuracy Classification of Parkinson's Disease through Shape   Analysis and Surface Fitting in $^{123}$I-Ioflupane SPECT Imaging|Early and accurate identification of parkinsonian syndromes (PS) involving presynaptic degeneration from non-degenerative variants such as Scans Without Evidence of Dopaminergic Deficit (SWEDD) and tremor disorders, is important for effective patient management as the course, therapy and prognosis differ substantially between the two groups. In this study, we use Single Photon Emission Computed Tomography (SPECT) images from healthy normal, early PD and SWEDD subjects, as obtained from the Parkinson's Progression Markers Initiative (PPMI) database, and process them to compute shape- and surface fitting-based features for the three groups. We use these features to develop and compare various classification models that can discriminate between scans showing dopaminergic deficit, as in PD, from scans without the deficit, as in healthy normal or SWEDD. Along with it, we also compare these features with Striatal Binding Ratio (SBR)-based features, which are well-established and clinically used, by computing a feature importance score using Random forests technique. We observe that the Support Vector Machine (SVM) classifier gave the best performance with an accuracy of 97.29%. These features also showed higher importance than the SBR-based features. We infer from the study that shape analysis and surface fitting are useful and promising methods for extracting discriminatory features that can be used to develop diagnostic models that might have the potential to help clinicians in the diagnostic process.|['R. Prashanth', 'Sumantra Dutta Roy', 'Pravat K. Mandal', 'Shantanu Ghosh']|['stat.AP', 'cs.CV', 'physics.data-an', 'stat.CO', 'stat.ML']
2017-03-16T23:25:46Z|2017-03-04T19:07:42Z|http://arxiv.org/abs/1703.01506v1|http://arxiv.org/pdf/1703.01506v1|Accelerating Permutation Testing in Voxel-wise Analysis through Subspace   Tracking: A new plugin for SnPM|Permutation testing is a non-parametric method for obtaining the max null distribution used to compute corrected $p$-values to provide strong control of false positives. In neuroimaging, however, the computational burden of running such algorithm can be significant. We find that by viewing the permutation testing procedure as the construction of a very large permutation testing matrix $T$, one can exploit structural properties derived from the data and the test statistics to reduce the runtime under certain conditions. In particular, we see that $T$ has a low-rank plus a low-variance residual. This makes $T$ a good candidate for low-rank matrix completion methods, where only a very small number of entries of $T$ ($~0.35\%$ of all entries in our experiments) have to be computed to obtain good estimate of it. Based on this observation, we developed an algorithm, RapidPT, that is able to efficiently recover the max null distribution commonly obtained through regular permutation testing in neuroimage analysis. We present an extensive experimental validation on four varying sized datasets against two baselines: Statistical NonParametric Mapping (SnPM13) and a standard permutation testing implementation (referred to as NaivePT). We find that RapidPT achieves its best runtime performance on medium sized datasets ($50 \leq n \leq 200$), with speedup gains of 1.5x - 38x (vs. SnPM13) and 20x-1000x (vs. NaivePT). For larger datasets ($n \geq 200$) RapidPT outperforms NaivePT (6x - 200x), and provides substantial speedups over SnPM13 when performing more than 10000 permutations (2x - 15x). The Matlab implementation is available as a standalone toolbox called RapidPT. Our code is also integrated within SnPM13, and is able to leverage multi-core architectures when available.|['Felipe Gutierrez-Barragan', 'Vamsi K. Ithapu', 'Chris Hinrichs', 'Camille Maumet', 'Sterling C. Johnson', 'Thomas E. Nichols', 'Vikas Singh', 'the ADNI']|['stat.AP', 'cs.CV', 'stat.ML']
2017-03-16T23:25:46Z|2017-03-03T16:29:21Z|http://arxiv.org/abs/1703.01234v1|http://arxiv.org/pdf/1703.01234v1|A Bayesian computer model analysis of Robust Bayesian analyses|We harness the power of Bayesian emulation techniques, designed to aid the analysis of complex computer models, to examine the structure of complex Bayesian analyses themselves. These techniques facilitate robust Bayesian analyses and/or sensitivity analyses of complex problems, and hence allow global exploration of the impacts of choices made in both the likelihood and prior specification. We show how previously intractable problems in robustness studies can be overcome using emulation techniques, and how these methods allow other scientists to quickly extract approximations to posterior results corresponding to their own particular subjective specification. The utility and flexibility of our method is demonstrated on a reanalysis of a real application where Bayesian methods were employed to capture beliefs about river flow. We discuss the obvious extensions and directions of future research that such an approach opens up.|['Ian Vernon', 'John Paul Gosling']|['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']
2017-03-16T23:25:50Z|2017-03-03T06:28:36Z|http://arxiv.org/abs/1703.01051v1|http://arxiv.org/pdf/1703.01051v1|Interval Estimation of the Unknown Exponential Parameter Based on Time   Truncated Data|In this paper we consider the statistical inference of the unknown parameter of an exponential distribution based on the time truncated data. The time truncated data occurs quite often in the reliability analysis for type-I or hybrid censoring cases. All the results available today are based on the conditional argument that at least one failure occurs during the experiment. In this paper we provide some inferential results based on the unconditional argument. We extend the results for some two-parameter distributions also.|['Arnab Koley', 'Debasis Kundu']|['stat.AP', '62F10, 62F03, 62H12']
2017-03-16T23:25:50Z|2017-03-03T05:54:09Z|http://arxiv.org/abs/1703.01044v1|http://arxiv.org/pdf/1703.01044v1|On Generalized Progressive Hybrid Censoring in presence of competing   risks|The progressive Type-II hybrid censoring scheme introduced by Kundu and Joarder (\textit{Computational Statistics and Data Analysis}, 2509-2528, 2006), has received some attention in the last few years. One major drawback of this censoring scheme is that very few observations (even no observation at all) may be observed at the end of the experiment. To overcome this problem, Cho, Sun and Lee (\textit{Statistical Methodology}, 23, 18-34, 2015) recently introduced generalized progressive censoring which ensures to get a pre specified number of failures. In this paper we analyze generalized progressive censored data in presence of competing risks. For brevity we have considered only two competing causes of failures, and it is assumed that the lifetime of the competing causes follow one parameter exponential distributions with different scale parameters. We obtain the maximum likelihood estimators of the unknown parameters and also provide their exact distributions. Based on the exact distributions of the maximum likelihood estimators exact confidence intervals can be obtained. Asymptotic and bootstrap confidence intervals are also provided for comparison purposes. We further consider the Bayesian analysis of the unknown parameters under a very flexible Beta-Gamma prior. We provide the Bayes estimates and the associated credible intervals of the unknown parameters based on the above priors. We present extensive simulation results to see the effectiveness of the proposed method and finally one real data set is analyzed for illustrative purpose.|['Arnab Koley', 'Debasis Kundu']|['stat.AP', '62F10, 62F03, 62H12']
2017-03-16T23:25:50Z|2017-03-03T00:32:31Z|http://arxiv.org/abs/1703.01002v1|http://arxiv.org/pdf/1703.01002v1|Statistical Properties of the Risk-Transfer Formula in the Affordable   Care Act|The Affordable Care Act, signed into United States law in 2010, led to the formation of competitive insurance plans which provide universal health-insurance coverage without regard to pre-existing medical conditions. To assist insurers during a transitional period, the Act introduced a risk-transfer formula which requires insurance plans with healthier enrollees to transfer funds to plans with sicker enrollees, thereby dissuading plans from favoring healthier enrollees.   In this paper, we treat the risk-transfer amounts as random variables and derive some of their statistical properties by analyzing their means and variances. The results in this paper lead to an explanation for the empirical phenomena, observed by the American Academy of Actuaries, that risk-transfer amounts were more variable and can be extremely large for insurers with smaller market shares. Our results provide conditions, as functions of the market shares of insurance plans, under which those phenomena hold.|['Michelle Li', 'Donald Richards']|['stat.AP', 'Primary: 62P05, Secondary: 60E05']
2017-03-16T23:25:50Z|2017-03-03T00:21:34Z|http://arxiv.org/abs/1703.01000v1|http://arxiv.org/pdf/1703.01000v1|The M33 Synoptic Stellar Survey. II. Mira Variables|We present the discovery of 1847 Mira candidates in the Local Group galaxy M33 using a novel semi-parametric periodogram technique coupled with a Random Forest classifier. The algorithms were applied to ~2.4x10^5 I-band light curves previously obtained by the M33 Synoptic Stellar Survey. We derive preliminary Period-Luminosity relations at optical, near- & mid-infrared wavelengths and compare them to the corresponding relations in the Large Magellanic Cloud.|['Wenlong Yuan', 'Shiyuan He', 'Lucas M. Macri', 'James Long', 'Jianhua Z. Huang']|['astro-ph.SR', 'stat.AP']
2017-03-16T23:25:50Z|2017-03-02T23:03:56Z|http://arxiv.org/abs/1703.00981v1|http://arxiv.org/pdf/1703.00981v1|A Restaurant Process Mixture Model for Connectivity Based Parcellation   of the Cortex|One of the primary objectives of human brain mapping is the division of the cortical surface into functionally distinct regions, i.e. parcellation. While it is generally agreed that at macro-scale different regions of the cortex have different functions, the exact number and configuration of these regions is not known. Methods for the discovery of these regions are thus important, particularly as the volume of available information grows. Towards this end, we present a parcellation method based on a Bayesian non-parametric mixture model of cortical connectivity.|['Daniel Moyer', 'Boris A Gutman', 'Neda Jahanshad', 'Paul M. Thompson']|['q-bio.NC', 'cs.CE', 'cs.CV', 'q-bio.QM', 'stat.AP']
2017-03-16T23:25:50Z|2017-03-02T07:57:20Z|http://arxiv.org/abs/1703.00654v1|http://arxiv.org/pdf/1703.00654v1|Nonparametric estimation of galaxy cluster's emissivity and point source   detection in astrophysics with two lasso penalties|Astrophysicists are interested in recovering the 3D gas emissivity of a galaxy cluster from a 2D image taken by a telescope. A blurring phenomenon and presence of point sources make this inverse problem even harder to solve. The current state-of-the-art technique is two step: first identify the location of potential point sources, then mask these locations and deproject the data.   We instead model the data as a Poisson generalized linear model (involving blurring, Abel and wavelets operators) regularized by two lasso penalties to induce sparse wavelet representation and sparse point sources. The amount of sparsity is controlled by two quantile universal thresholds. As a result, our method outperforms the existing one.|['Jairo Diaz Rodriguez', 'Dominique Eckert', 'Hatef Monajemi', 'StÃ©phane Paltani', 'Sylvain Sardy']|['stat.AP', 'stat.ME']
2017-03-16T23:25:50Z|2017-03-01T17:42:47Z|http://arxiv.org/abs/1703.00409v1|http://arxiv.org/pdf/1703.00409v1|Sequence of purchases in credit card data reveal life styles in urban   populations|From our most basic consumption to secondary needs, our spending habits reflect our life styles. Yet, in computational social sciences there is an open question about the existence of ubiquitous trends in spending habits by various groups at urban scale. Limited information collected by expenditure surveys have not proven conclusive in this regard. This is because, the frequency of purchases by type is highly uneven and follows a Zipf-like distribution. In this work, we apply text compression techniques to the purchase codes of credit card data to detect the significant sequences of transactions of each user. Five groups of consumers emerge when grouped by their similarity based on these sequences. Remarkably, individuals in each consumer group are also similar in age, total expenditure, gender, and the diversity of their social and mobility networks extracted by their mobile phone records. By properly deconstructing transaction data with Zipf-like distributions, we find that it can give us insights on collective behavior.|['Riccardo Di Clemente', 'Miguel Luengo-Oroz', 'Matias Travizano', 'Bapu Vaitla', 'Marta C. Gonzalez']|['physics.soc-ph', 'cs.IT', 'cs.SI', 'math.IT', 'stat.AP']
2017-03-16T23:25:50Z|2017-03-01T07:00:01Z|http://arxiv.org/abs/1703.00154v1|http://arxiv.org/pdf/1703.00154v1|Inertial Odometry on Handheld Smartphones|Building a complete inertial navigation system using the limited quality data provided by current smartphones has been regarded challenging, if not impossible. We present a probabilistic approach for orientation and use-case free inertial odometry, which is based on double-integrating rotated accelerations. Our approach uses a probabilistic approach in fusing the noisy sensor data and learning the model parameters online. It is able to track the phone position, velocity, and pose in real-time and in a computationally lightweight fashion. The information fusion is completed with altitude correction from barometric pressure readings (if available), zero-velocity updates (if the phone remains stationary), and pseudo-updates limiting the momentary speed. We demonstrate our approach using a standard iPad and iPhone in several indoor dead-reckoning applications and in a measurement tool setup.|['Arno Solin', 'Santiago Cortes', 'Esa Rahtu', 'Juho Kannala']|['cs.CV', 'stat.AP']
2017-03-16T23:25:50Z|2017-02-28T21:12:37Z|http://arxiv.org/abs/1703.00056v1|http://arxiv.org/pdf/1703.00056v1|Fair prediction with disparate impact: A study of bias in recidivism   prediction instruments|Recidivism prediction instruments (RPI's) provide decision makers with an assessment of the likelihood that a criminal defendant will reoffend at a future point in time. While such instruments are gaining increasing popularity across the country, their use is attracting tremendous controversy. Much of the controversy concerns potential discriminatory bias in the risk assessments that are produced. This paper discusses several fairness criteria that have recently been applied to assess the fairness of recidivism prediction instruments. We demonstrate that the criteria cannot all be simultaneously satisfied when recidivism prevalence differs across groups. We then show how disparate impact can arise when a recidivism prediction instrument fails to satisfy the criterion of error rate balance.|['Alexandra Chouldechova']|['stat.AP', 'cs.CY', 'stat.ML']
2017-03-16T23:25:50Z|2017-02-28T04:25:44Z|http://arxiv.org/abs/1702.08638v1|http://arxiv.org/pdf/1702.08638v1|Single-lead f-wave extraction using diffusion geometry|A novel single-lead f-wave extraction algorithm based on the modern diffusion geometry data analysis framework is proposed. The algorithm is essentially an averaged beat subtraction algorithm, where the ventricular activity template is estimated by combining a newly designed metric, the diffusion distance, and the non-local Euclidean median based on the non-linear manifold setup. To validate the algorithm, two simulation schemes are proposed and tested, and state-of-the-art results are reported. The clinical potential is shown in the real Holter signal, and we introduce a new score to evaluate the performance of the algorithm.|['John Malik', 'Neil Reed', 'Chun-Li Wang', 'Hautieng Wu']|['physics.data-an', 'q-bio.QM', 'stat.AP']
2017-03-16T23:25:54Z|2017-02-27T22:22:03Z|http://arxiv.org/abs/1702.08560v1|http://arxiv.org/pdf/1702.08560v1|Estimating the reproductive number, total outbreak size, and reporting   rates for Zika epidemics in South and Central America|As South and Central American countries prepare for increased birth defects from Zika virus outbreaks and plan for mitigation strategies to minimize ongoing and future outbreaks, understanding important characteristics of Zika outbreaks and how they vary across regions is a challenging and important problem. We developed a mathematical model for the 2015 Zika virus outbreak dynamics in Colombia, El Salvador, and Suriname. We fit the model to publicly available data provided by the Pan American Health Organization, using Approximate Bayesian Computation to estimate parameter distributions and provide uncertainty quantification. An important model input is the at-risk susceptible population, which can vary with a number of factors including climate, elevation, population density, and socio-economic status. We informed this initial condition using the highest historically reported dengue incidence modified by the probable dengue reporting rates in the chosen countries. The model indicated that a country-level analysis was not appropriate for Colombia. We then estimated the basic reproduction number, or the expected number of new human infections arising from a single infected human, to range between 4 and 6 for El Salvador and Suriname with a median of 4.3 and 5.3, respectively. We estimated the reporting rate to be around 16% in El Salvador and 18% in Suriname with estimated total outbreak sizes of 73,395 and 21,647 people, respectively. The uncertainty in parameter estimates highlights a need for research and data collection that will better constrain parameter ranges.|['Deborah P. Shutt', 'Carrie A. Manore', 'Stephen Pankavich', 'Aaron T. Porter', 'Sara Y. Del Valle']|['q-bio.PE', 'q-bio.QM', 'stat.AP', '92D30']
2017-03-16T23:25:54Z|2017-02-27T18:38:28Z|http://arxiv.org/abs/1703.01237v1|http://arxiv.org/pdf/1703.01237v1|How real is the random censorship model in medical studies?|In survival analysis the random censorship model refers to censoring and survival times being independent of each other. It is one of the fundamental assumptions in the theory of survival analysis. We explain the reason for it being so ubiquitous, and we investigate its presence in medical studies. We differentiate two types of censoring in medical studies (dropout and administrative), and we explain their importance in examining the existence of the random censorship model. We show that in order to presume the random censorship model it is not enough to have a design study which conforms to it, but that one needs to provide evidence for its presence in the results. Blindly presuming the random censorship model might lead to the Kaplan-Meier estimator producing biased results, which might have serious consequences when estimating survival in medical studies.|['Damjan Krstajic']|['stat.AP', 'math.ST', 'stat.TH']
2017-03-16T23:25:54Z|2017-02-27T13:03:33Z|http://arxiv.org/abs/1702.08262v1|http://arxiv.org/pdf/1702.08262v1|Sequential Discrete Kalman Filter for Real-Time State Estimation in   Power Distribution Systems: Theory and Implementation|This paper demonstrates the feasibility of implementing Real-Time State Estimators (RTSEs) for Active Distribution Networks (ADNs) in Field-Programmable Gate Arrays (FPGAs) by presenting an operational prototype. The prototype is based on a Linear State Estimator (LSE) that uses synchrophasor measurements from Phasor Measurement Units (PMUs). The underlying algorithm is the Sequential Discrete Kalman Filter (SDKF), an equivalent formulation of the Discrete Kalman Filter (DKF) for the case of uncorrelated measurement noise. In this regard, this work formally proves the equivalence the SDKF and the DKF, and highlights the suitability of the SDKF for an FPGA implementation by means of a computational complexity analysis. The developed prototype is validated using a case study adapted from the IEEE 34-node distribution test feeder.|['Andreas Martin Kettner', 'Mario Paolone']|['stat.AP']
2017-03-16T23:25:54Z|2017-02-27T08:33:26Z|http://arxiv.org/abs/1702.08185v1|http://arxiv.org/pdf/1702.08185v1|An update on statistical boosting in biomedicine|Statistical boosting algorithms have triggered a lot of research during the last decade. They combine a powerful machine-learning approach with classical statistical modelling, offering various practical advantages like automated variable selection and implicit regularization of effect estimates. They are extremely flexible, as the underlying base-learners (regression functions defining the type of effect for the explanatory variables) can be combined with any kind of loss function (target function to be optimized, defining the type of regression setting). In this review article, we highlight the most recent methodological developments on statistical boosting regarding variable selection, functional regression and advanced time-to-event modelling. Additionally, we provide a short overview on relevant applications of statistical boosting in biomedicine.|['Andreas Mayr', 'Benjamin Hofner', 'Elisabeth Waldmann', 'Tobias Hepp', 'Olaf Gefeller', 'Matthias Schmid']|['stat.AP', 'stat.CO', 'stat.ML']
2017-03-16T23:25:54Z|2017-02-27T04:17:36Z|http://arxiv.org/abs/1702.08140v1|http://arxiv.org/pdf/1702.08140v1|A mixture model approach to infer land-use influence on point referenced   water quality|The assessment of water quality across space and time is of considerable interest for both agricultural and public health reasons. The standard method to assess the water quality of a catchment, or a group of catchments, usually involves collecting point measurements of water quality and other additional information such as the date and time of measurements, rainfall amounts, the land-use and soil-type of the catchment and the elevation. Some of this auxiliary information will be point data, measured at the exact location, whereas other such as land-use will be areal data often in a compositional format. Two problems arise if analysts try to incorporate this information into a statistical model in order to predict (for example) the influence of land-use on water quality. First is the spatial change of support problem that arises when using areal data to predict outcomes at point locations. Secondly, the physical process driving water quality is not compositional, rather it is the observation process that provides compositional data. In this paper we present an approach that accounts for these two issues by using a latent variable to identify the land-use that most likely influences water quality. This latent variable is used in a spatial mixture model to help estimate the influence of land-use on water quality. We demonstrate the potential of this approach with data from a water quality research study in the Mount Lofty range, in South Australia.|['Adrien Ickowicz', 'Jessica H. Ford', 'Keith R. Hayes']|['stat.AP', 'stat.CO']
2017-03-16T23:25:54Z|2017-02-26T21:23:33Z|http://arxiv.org/abs/1702.08088v1|http://arxiv.org/pdf/1702.08088v1|Selection of training populations (and other subset selection problems)   with an accelerated genetic algorithm (STPGA: An R-package for selection of   training populations with a genetic algorithm)|Optimal subset selection is an important task that has numerous algorithms designed for it and has many application areas. STPGA contains a special genetic algorithm supplemented with a tabu memory property (that keeps track of previously tried solutions and their fitness for a number of iterations), and with a regression of the fitness of the solutions on their coding that is used to form the ideal estimated solution (look ahead property) to search for solutions of generic optimal subset selection problems. I have initially developed the programs for the specific problem of selecting training populations for genomic prediction or association problems, therefore I give discussion of the theory behind optimal design of experiments to explain the default optimization criteria in STPGA, and illustrate the use of the programs in this endeavor. Nevertheless, I have picked a few other areas of application: supervised and unsupervised variable selection based on kernel alignment, supervised variable selection with design criteria, influential observation identification for regression, solving mixed integer quadratic optimization problems, balancing gains and inbreeding in a breeding population. Some of these illustrations pertain new statistical approaches.|['Deniz Akdemir']|['stat.ME', 'cs.LG', 'q-bio.GN', 'q-bio.QM', 'stat.AP']
2017-03-16T23:25:54Z|2017-02-26T10:41:26Z|http://arxiv.org/abs/1703.01977v1|http://arxiv.org/pdf/1703.01977v1|Linear, Machine Learning and Probabilistic Approaches for Time Series   Analysis|In this paper we study different approaches for time series modeling. The forecasting approaches using linear models, ARIMA alpgorithm, XGBoost machine learning algorithm are described. Results of different model combinations are shown. For probabilistic modeling the approaches using copulas and Bayesian inference are considered.|['B. M. Pavlyshenko']|['stat.AP', 'cs.LG', 'stat.ME']
2017-03-16T23:25:54Z|2017-02-26T03:10:41Z|http://arxiv.org/abs/1702.07981v1|http://arxiv.org/pdf/1702.07981v1|BayCount: A Bayesian Decomposition Method for Inferring Tumor   Heterogeneity using RNA-Seq Counts|Tumor is heterogeneous - a tumor sample usually consists of a set of subclones with distinct transcriptional profiles and potentially different degrees of aggressiveness and responses to drugs. Understanding tumor heterogeneity is therefore critical to precise cancer prognosis and treatment. In this paper, we introduce BayCount, a Bayesian decomposition method to infer tumor heterogeneity with highly over-dispersed RNA sequencing count data. Using negative binomial factor analysis, BayCount takes into account both the between-sample and gene-specific random effects on raw counts of sequencing reads mapped to each gene. For posterior inference, we develop an efficient compound Poisson based blocked Gibbs sampler. Through extensive simulation studies and analysis of The Cancer Genome Atlas lung cancer and kidney cancer RNA sequencing count data, we show that BayCount is able to accurately estimate the number of subclones, the proportions of these subclones in each tumor sample, and the gene expression profiles in each subclone. Our method represents the first effort in characterizing tumor heterogeneity using RNA sequencing count data that simultaneously removes the need of normalizing the counts, achieves statistical robustness, and obtains biologically and clinically meaningful insights.|['Fangzheng Xie', 'Mingyuan Zhou', 'Yanxun Xu']|['stat.AP']
2017-03-16T23:25:54Z|2017-03-04T00:20:43Z|http://arxiv.org/abs/1702.07909v2|http://arxiv.org/pdf/1702.07909v2|Analysis of Urban Vibrancy and Safety in Philadelphia|"Statistical analyses of urban environments have been recently improved through publicly available high resolution data and mapping technologies that have adopted across industries. These technologies allow us to create metrics to empirically investigate urban design principles of the past half-century. Philadelphia is an interesting case study for this work, with its rapid urban development and population increase in the last decade. We focus on features of what urban planners call ""vibrancy"": measures of positive, healthy activity or energy in an area. Historically, vibrancy has been very challenging to measure empirically. We explore the association between safety (violent and non-violent crime) and features of local neighborhood vibrancy such as population, economic measures and land use zoning. Despite rhetoric about the negative effects of population density in the 1960s and 70s, we find very little association between crime and population density. Measures based on land use zoning are not an adequate description of local vibrancy and so we construct a database and set of measures of business activity in each neighborhood. We employ several matching analyses within census block groups to explore the relationship between neighborhood vibrancy and safety at a higher resolution. We find that neighborhoods with more vacancy have higher crime but within neighborhoods, crimes tend not to be located near vacant properties. We also find that more crimes occur near business locations but businesses that are active (open) for longer periods are associated with fewer crimes."|['Colman Humphrey', 'Shane T. Jensen', 'Dylan Small', 'Rachel Thurston']|['stat.AP']
2017-03-16T23:25:54Z|2017-02-25T10:26:59Z|http://arxiv.org/abs/1702.07869v1|http://arxiv.org/pdf/1702.07869v1|Signal Denoising Using the Minimum-Probability-of-Error Criterion|We address the problem of signal denoising via transform-domain shrinkage based on a novel $\textit{risk}$ criterion called the minimum probability of error (MPE), which measures the probability that the estimated parameter lies outside an $\epsilon$-neighborhood of the actual value. However, the MPE, similar to the mean-squared error (MSE), depends on the ground-truth parameter, and has to be estimated from the noisy observations. We consider linear shrinkage-based denoising functions, wherein the optimum shrinkage parameter is obtained by minimizing an estimate of the MPE. When the probability of error is integrated over $\epsilon$, it leads to the expected $\ell_1$ distortion. The proposed MPE and $\ell_1$ distortion formulations are applicable to various noise distributions by invoking a Gaussian mixture model approximation. Within the realm of MPE, we also develop an extension of the transform-domain shrinkage by grouping transform coefficients, resulting in $\textit{subband shrinkage}$. The denoising performance obtained within the proposed framework is shown to be better than that obtained using the minimum MSE-based approaches formulated within $\textbf{$\textit {Stein's unbiased risk estimation}$}$ (SURE) framework, especially in the low measurement signal-to-noise ratio (SNR) regime. Performance comparison with three state-of-the-art denoising algorithms, carried out on electrocardiogram signals and two test signals taken from the $\textit{Wavelab}$ toolbox, exhibits that the MPE framework results in consistent SNR gains for input SNRs below $5$ dB.|['Jishnu Sadasivan', 'Subhadip Mukherjee', 'Chandra Sekhar Seelamantula']|['stat.AP']
2017-03-16T23:25:58Z|2017-02-24T05:16:23Z|http://arxiv.org/abs/1702.07465v1|http://arxiv.org/pdf/1702.07465v1|PairClone: A Bayesian Subclone Caller Based on Mutation Pairs|Tumor cell populations can be thought of as being composed of homogeneous cell subpopulations, with each subpopulation being characterized by overlapping sets of single nucleotide variants (SNVs). Such subpopulations are known as subclones and are an important target for precision medicine. Reconstructing such subclones from next-generation sequencing (NGS) data is one of the major challenges in precision medicine. We present PairClone as a new tool to implement this reconstruction. The main idea of PairClone is to model short reads mapped to pairs of proximal SNVs. In contrast, most existing methods use only marginal reads for unpaired SNVs. Using Bayesian nonparametric models, we estimate posterior probabilities of the number, genotypes and population frequencies of subclones in one or more tumor sample. We use the categorical Indian buffet process (cIBP) as a prior probability model for subclones that are represented as vectors of categorical matrices that record the corresponding sets of mutation pairs. Performance of PairClone is assessed using simulated and real datasets. An open source software package can be obtained at http://www.compgenome.org/pairclone.|['Tianjian Zhou', 'Peter Mueller', 'Subhajit Sengupta', 'Yuan Ji']|['stat.AP']
2017-03-16T23:25:58Z|2017-02-23T23:25:32Z|http://arxiv.org/abs/1702.07422v1|http://arxiv.org/pdf/1702.07422v1|sourceR: Classification and Source Attribution of Infectious Agents   among Heterogeneous Populations|Zoonotic diseases are a major cause of morbidity, and productivity losses in both humans and animal populations. Identifying the source of food-borne zoonoses (e.g. an animal reservoir or food product) is crucial for the identification and prioritisation of food safety interventions. For many zoonotic diseases it is difficult to attribute human cases to sources of infection because there is little epidemiological information on the cases. However, microbial strain typing allows zoonotic pathogens to be categorised, and the relative frequencies of the strain types among the sources and in human cases allows inference on the likely source of each infection. We introduce sourceR, an R package for quantitative source attribution, aimed at food-borne diseases. It implements a fully joint Bayesian model using strain-typed surveillance data from both human cases and source samples, capable of identifying important sources of infection. The model measures the force of infection from each source, allowing for varying survivability, pathogenicity and virulence of pathogen strains, and varying abilities of the sources to act as vehicles of infection. A Bayesian non-parametric (Dirichlet process) approach is used to cluster pathogen strain types by epidemiological behaviour, avoiding model overfitting and allowing detection of strain types associated with potentially high 'virulence'.   sourceR is demonstrated using Campylobacter jejuni isolate data collected in New Zealand between 2005 and 2008. It enables straightforward attribution of cases of zoonotic infection to putative sources of infection by epidemiologists and public health decision makers. As sourceR develops, we intend it to become an important and flexible resource for food-borne disease attribution studies.|['Poppy Miller', 'Jonathan Marshall', 'Nigel French', 'Chris Jewell']|['stat.AP']
2017-03-16T23:25:58Z|2017-02-23T18:24:58Z|http://arxiv.org/abs/1702.07326v1|http://arxiv.org/pdf/1702.07326v1|Time-Series Adaptive Estimation of Vaccination Uptake Using Web Search   Queries|Estimating vaccination uptake is an integral part of ensuring public health. It was recently shown that vaccination uptake can be estimated automatically from web data, instead of slowly collected clinical records or population surveys. All prior work in this area assumes that features of vaccination uptake collected from the web are temporally regular. We present the first ever method to remove this assumption from vaccination uptake estimation: our method dynamically adapts to temporal fluctuations in time series web data used to estimate vaccination uptake. We show our method to outperform the state of the art compared to competitive baselines that use not only web data but also curated clinical data. This performance improvement is more pronounced for vaccines whose uptake has been irregular due to negative media attention (HPV-1 and HPV-2), problems in vaccine supply (DiTeKiPol), and targeted at children of 12 years old (whose vaccination is more irregular compared to younger children).|['Niels Dalum Hansen', 'KÃ¥re MÃ¸lbak', 'Ingemar J. Cox', 'Christina Lioma']|['cs.IR', 'q-bio.QM', 'stat.AP']
2017-03-16T23:25:58Z|2017-02-22T21:21:54Z|http://arxiv.org/abs/1702.07009v1|http://arxiv.org/pdf/1702.07009v1|The Impact of Confounder Selection in Propensity Scores for Rare Events   Data - with Applications to Birth Defects|Our work was motivated by a recent study on birth defects of infants born to pregnant women exposed to a certain medication for treating chronic diseases. Outcomes such as birth defects are rare events in the general population, which often translate to very small numbers of events in the unexposed group. As drug safety studies in pregnancy are typically observational in nature, we control for confounding in this rare events setting using propensity scores (PS). Using our empirical data, we noticed that the estimated odds ratio for birth defects due to exposure varied drastically depending on the specific approach used. The commonly used approaches with PS are matching, stratification, inverse probability weighting (IPW) and regression adjustment. The extremely rare events setting renders the matching or stratification infeasible. In addition, the PS itself may be formed via different approaches to select confounders from a relatively long list of potential confounders. We carried out simulation experiments to compare different combinations of approaches: IPW or regression adjustment, with 1) including all potential confounders without selection, 2) selection based on univariate association between the candidate variable and the outcome, 3) selection based on change in effects (CIE). The simulation showed that IPW without selection leads to extremely large variances in the estimated odds ratio, which help to explain the empirical data analysis results that we had observed. The simulation also showed that IPW with selection based on univariate association with the outcome is preferred over IPW with CIE. Regression adjustment has small variances of the estimated odds ratio regardless of the selection methods used.|['Ronghui Xu', 'Jue Hou', 'Christina D. Chambers']|['stat.AP']
2017-03-16T23:25:58Z|2017-02-22T21:09:19Z|http://arxiv.org/abs/1702.07007v1|http://arxiv.org/pdf/1702.07007v1|Detecting causal associations in large nonlinear time series datasets|Detecting causal associations in time series datasets is a key challenge for novel insights into complex dynamical systems such as the Earth system or the human brain. Interactions in high-dimensional dynamical systems often involve time-delays, nonlinearity, and strong autocorrelations. These present major challenges for causal discovery techniques such as Granger causality leading to low detection power, biases, and unreliable hypothesis tests. Here we introduce a reliable and fast method that outperforms current approaches in detection power and scales up to high-dimensional datasets. It overcomes detection biases, especially when strong autocorrelations are present, and allows ranking associations in large-scale analyses by their causal strength. We provide mathematical proofs, evaluate our method in extensive numerical experiments, and illustrate its capabilities in a large-scale analysis of the global surface-pressure system where we unravel spurious associations and find several potentially causal links that are difficult to detect with standard methods. The broadly applicable method promises to discover novel causal insights also in many other fields of science.|['Jakob Runge', 'Dino Sejdinovic', 'Seth Flaxman']|['stat.ME', 'physics.ao-ph', 'stat.AP']
2017-03-16T23:25:58Z|2017-02-22T20:18:36Z|http://arxiv.org/abs/1702.06993v1|http://arxiv.org/pdf/1702.06993v1|Generalized Pareto Processes and Liquidity|Motivated by the modeling of liquidity risk in fund management in a dynamic setting, we propose and investigate a class of time series models with generalized Pareto marginals: the autoregressive generalized Pareto process (ARGP), a modified ARGP (MARGP) and a thresholded ARGP (TARGP). These models are able to capture key data features apparent in fund liquidity data and reflect the underlying phenomena via easily interpreted, low-dimensional model parameters. We establish stationarity and ergodicity, provide a link to the class of shot-noise processes, and determine the associated interarrival distributions for exceedances. Moreover, we provide estimators for all relevant model parameters and establish consistency and asymptotic normality for all estimators (except the threshold parameter, which as usual must be dealt with separately). Finally, we illustrate our approach using real-world fund redemption data, and we discuss the goodness-of-fit of the estimated models.|['Sascha Desmettre', 'Johan de Kock', 'Peter Ruckdeschel', 'Frank Thomas Seifried']|['stat.AP', '60G70, 62P05']
2017-03-16T23:25:58Z|2017-02-22T03:10:41Z|http://arxiv.org/abs/1702.06661v1|http://arxiv.org/pdf/1702.06661v1|Social Learning and Diffusion of Pervasive Goods: An Empirical Study of   an African App Store|In this study, the authors develop a structural model that combines a macro diffusion model with a micro choice model to control for the effect of social influence on the mobile app choices of customers over app stores. Social influence refers to the density of adopters within the proximity of other customers. Using a large data set from an African app store and Bayesian estimation methods, the authors quantify the effect of social influence and investigate the impact of ignoring this process in estimating customer choices. The findings show that customer choices in the app store are explained better by offline than online density of adopters and that ignoring social influence in estimations results in biased estimates. Furthermore, the findings show that the mobile app adoption process is similar to adoption of music CDs, among all other classic economy goods. A counterfactual analysis shows that the app store can increase its revenue by 13.6% through a viral marketing policy (e.g., a sharing with friends and family button).|['Meisam Hejazi Nia', 'Brian T. Ratchford', 'Norris Bruce']|['stat.ML', 'cs.SI', 'stat.AP']
2017-03-16T23:25:58Z|2017-02-22T02:21:02Z|http://arxiv.org/abs/1702.06650v1|http://arxiv.org/pdf/1702.06650v1|Reducing the uncertainty in the forest volume-to-biomass relationship   built from limited field plots|The method of biomass estimation based on a volume-to-biomass relationship has been applied in estimating forest biomass conventionally through the mean volume (m3 ha-1). However, few studies have been reported concerning the verification of the volume-biomass equations regressed using field data. The possible bias may result from the volume measurements and extrapolations from sample plots to stands or a unit area. This paper addresses (i) how to verify the volume-biomass equations, and (ii) how to reduce the bias while building these equations. This paper presents an applicable method for verifying the field data using reasonable wood densities, restricting the error in field data processing based on limited field plots, and achieving a better understanding of the uncertainty in building those equations. The verified and improved volume-biomass equations are more reliable and will help to estimate forest carbon sequestration and carbon balance at any large scale.|['Caixia Liu', 'Xiaolu Zhou', 'Xiangdong Lei', 'Huabing Huang', 'Changhui Peng', 'Xiaoyi Wang', 'Jianfeng Sun', 'Carl Zhou']|['stat.AP', 'q-bio.QM']
2017-03-16T23:25:58Z|2017-02-21T18:32:22Z|http://arxiv.org/abs/1702.06512v1|http://arxiv.org/pdf/1702.06512v1|Semiparametric panel data models using neural networks|This paper presents an estimator for semiparametric models that uses a feed-forward neural network to fit the nonparametric component. Unlike many methodologies from the machine learning literature, this approach is suitable for longitudinal/panel data. It provides unbiased estimation of the parametric component of the model, with associated confidence intervals that have near-nominal coverage rates. It is further shown that this model and estimator nests a nonparametric heterogeneous treatment effects model and estimator, which can consistently estimate individualized treatment effects conditional on covariates. Simulations demonstrate (1) efficiency, (2) that parametric estimates are unbiased, and (3) coverage properties of estimated intervals. An application section demonstrates the method by predicting county-level corn yield using daily weather data from the period 1981-2015, along with parametric time trends representing technological change. The method is shown to out-perform linear methods such as OLS and ridge/lasso, as well as random forest. The procedures described in this paper are implemented in the R package panelNNET.|['Andrew Crane-Droesch']|['stat.AP']
2017-03-16T23:25:58Z|2017-02-21T01:38:28Z|http://arxiv.org/abs/1703.00398v1|http://arxiv.org/pdf/1703.00398v1|Analysis on Cohort Effects in view of Differential Geometry and its   Applications|This paper analyzes birth cohort effects and develops an approach which is based on differential geometry to identify and measure cohort effects in mortality data sets. The measurement is quantitative and provides a potential method to compare cohort effects among different countries or groups. Data sets of four countries (e.g. U.k., U.S., Canada and Japan) are taken as examples to explain our approach and applications of the measurement to a modified Lee-Carter model are analyzed. In fact, this paper is an upgrade version of our paper arXiv:1504.00327. There is a new section which gives applications of our approach based on the Lee-Carter and APC models.|['Ning Zhang', 'Liang Zhao']|['stat.AP', '91G80, 91B30']
2017-03-16T23:26:02Z|2017-02-19T10:08:16Z|http://arxiv.org/abs/1702.05732v1|http://arxiv.org/pdf/1702.05732v1|Low-dose cryo electron ptychography via non-convex Bayesian optimization|Electron ptychography has seen a recent surge of interest for phase sensitive imaging at atomic or near-atomic resolution. However, applications are so far mainly limited to radiation-hard samples because the required doses are too high for imaging biological samples at high resolution. We propose the use of non-convex, Bayesian optimization to overcome this problem and reduce the dose required for successful reconstruction by two orders of magnitude compared to previous experiments. We suggest to use this method for imaging single biological macromolecules at cryogenic temperatures and demonstrate 2D single-particle reconstructions from simulated data with a resolution of 7.9 \AA$\,$ at a dose of 20 $e^- / \AA^2$. When averaging over only 15 low-dose datasets, a resolution of 4 \AA$\,$ is possible for large macromolecular complexes. With its independence from microscope transfer function, direct recovery of phase contrast and better scaling of signal-to-noise ratio, cryo-electron ptychography may become a promising alternative to Zernike phase-contrast microscopy.|['Philipp Michael Pelz', 'Wen Xuan Qiu', 'Robert BÃ¼cker', 'GÃ¼nther Kassier', 'R. J. Dwayne Miller']|['physics.comp-ph', 'math.OC', 'physics.data-an', 'stat.AP']
2017-03-16T23:26:02Z|2017-02-19T04:08:18Z|http://arxiv.org/abs/1702.05698v1|http://arxiv.org/pdf/1702.05698v1|Online Robust Principal Component Analysis with Change Point Detection|Robust PCA methods are typically batch algorithms which requires loading all observations into memory before processing. This makes them inefficient to process big data. In this paper, we develop an efficient online robust principal component methods, namely online moving window robust principal component analysis (OMWRPCA). Unlike existing algorithms, OMWRPCA can successfully track not only slowly changing subspace but also abruptly changed subspace. By embedding hypothesis testing into the algorithm, OMWRPCA can detect change points of the underlying subspaces. Extensive simulation studies demonstrate the superior performance of OMWRPCA comparing with other state-of-art approach. We also apply the algorithm for real-time background subtraction of surveillance video.|['Wei Xiao', 'Xiaolin Huang', 'Jorge Silva', 'Saba Emrani', 'Arin Chaudhuri']|['cs.LG', 'cs.CV', 'stat.AP', 'stat.CO', 'stat.ML']
2017-03-16T23:26:02Z|2017-02-18T21:57:40Z|http://arxiv.org/abs/1702.05662v1|http://arxiv.org/pdf/1702.05662v1|Spatial modeling of shot conversion in soccer to single out goalscoring   ability|Goals are results of pin-point shots and it is a pivotal decision in soccer when, how and where to shoot. The main contribution of this study is two-fold. At first, after showing that there exists high spatial correlation in the data of shots across games, we introduce a spatial process in the error structure to model the probability of conversion from a shot depending on positional and situational covariates. The model is developed using a full Bayesian framework. Secondly, based on the proposed model, we define two new measures that can appropriately quantify the impact of an individual in soccer, by evaluating the positioning senses and shooting abilities of the players. As a practical application, the method is implemented on Major League Soccer data from 2016/17 season.|['Soudeep Deb', 'Debangan Dey']|['stat.AP']
2017-03-16T23:26:02Z|2017-02-17T18:06:27Z|http://arxiv.org/abs/1702.05462v1|http://arxiv.org/pdf/1702.05462v1|Objective Bayesian Analysis for Change Point Problems|In this paper we present an objective approach to change point analysis. In particular, we look at the problem from two perspectives. The first focuses on the definition of an objective prior when the number of change points is known a priori. The second contribution aims to estimate the number of change points by using an objective approach, recently introduced in the literature, based on losses. The latter considers change point estimation as a model selection exercise. We show the performance of the proposed approach on simulated data and on real data sets.|['Laurentiu Hinoveanu', 'Fabrizio Leisen', 'Cristiano Villa']|['stat.ME', 'math.ST', 'stat.AP', 'stat.CO', 'stat.ML', 'stat.TH']
2017-03-16T23:26:02Z|2017-02-17T12:13:24Z|http://arxiv.org/abs/1702.06913v1|http://arxiv.org/pdf/1702.06913v1|Structural Change in (Economic) Time Series|Methods for detecting structural changes, or change points, in time series data are widely used in many fields of science and engineering. This chapter sketches some basic methods for the analysis of structural changes in time series data. The exposition is confined to retrospective methods for univariate time series. Several recent methods for dating structural changes are compared using a time series of oil prices spanning more than 60 years. The methods broadly agree for the first part of the series up to the mid-1980s, for which changes are associated with major historical events, but provide somewhat different solutions thereafter, reflecting a gradual increase in oil prices that is not well described by a step function. As a further illustration, 1990s data on the volatility of the Hang Seng stock market index are reanalyzed.|['Christian Kleiber']|['q-fin.ST', 'physics.data-an', 'stat.AP', '62P20']
2017-03-16T23:26:02Z|2017-02-17T12:12:57Z|http://arxiv.org/abs/1702.05982v1|http://arxiv.org/pdf/1702.05982v1|Wages of wins: could an amateur make money from match outcome   predictions?|Evaluating the accuracies of models for match outcome predictions is nice and well but in the end the real proof is in the money to be made by betting. To evaluate the question whether the models developed by us could be used easily to make money via sports betting, we evaluate three cases: NCAAB post-season, NBA season, and NFL season, and find that it is possible yet not without its pitfalls. In particular, we illustrate that high accuracy does not automatically equal high pay-out, by looking at the type of match-ups that are predicted correctly by different models.|['Albrecht Zimmermann']|['stat.AP', 'cs.LG']
2017-03-16T23:26:02Z|2017-02-17T08:22:52Z|http://arxiv.org/abs/1702.05254v1|http://arxiv.org/pdf/1702.05254v1|A Biased Look at Phase Locking: Brief Critical Review and Proposed   Remedy|A number of popular measures of dependence between pairs of band-limited signals rely on analytic phase. A common misconception is that the dependence revealed by these measures must be specific to the spectral range of the filtered input signals. Implicitly or explicitly, obtaining analytic phase involves normalizing the signal by its own envelope, which is a nonlinear operation that introduces broad spectral leakage. We review how this generates bias and complicates the interpretation of commonly used measures of phase locking. A specific example of this effect may create spurious phase locking as a consequence of nonzero circular mean in the phase of input signals, which can be viewed as spectral leakage to 0 Hz. Corrections for this problem which recenter or uniformize the distribution of phase may fail when the amplitudes of the compared signals are correlated. To address the more general problem of spectral bias, a novel measure of phase locking is proposed, the amplitude-weighted phase locking value (awPLV). This measure is closely related to coherence, but it removes ambiguities of interpretation that detract from the latter.|['Christopher K. Kovach']|['q-bio.NC', 'stat.AP', '94A12']
2017-03-16T23:26:02Z|2017-02-16T04:08:56Z|http://arxiv.org/abs/1702.04854v1|http://arxiv.org/pdf/1702.04854v1|Two-stage Plant Species Recognition by Combining Local K-NN and Weighted   Sparse Representation|In classical sparse representation based classification and weighted SRC algorithms, the test samples are sparely represented by all training samples. They emphasize the sparsity of the coding coefficients but without considering the local structure of the input data. To overcome the shortcoming, aiming at the difficult problem of plant leaf recognition on the large-scale database, a two-stage local similarity based classification learning method is proposed by combining local mean-based classification method and local WSRC. In the first stage, LMC is applied to coarsely classifying the test sample. nearest neighbors of the test sample, as a neighbor subset, is selected from each training class, then the local geometric center of each class is calculated. S candidate neighbor subsets of the test sample are determined with the first smallest distances between the test sample and each local geometric center. In the second stage, LWSRC is proposed to approximately represent the test sample through a linear weighted sum of all samples of the candidate neighbor subsets. The rationale of the proposed method is as follows: the first stage aims to eliminate the training samples that are far from the test sample and assume that these samples have no effects on the ultimate classification decision, then select the candidate neighbor subsets of the test sample. Thus the classification problem becomes simple with fewer subsets; the second stage pays more attention to those training samples of the candidate neighbor subsets in weighted representing the test sample. This is helpful to accurately represent the test sample. Experimental results on the leaf image database demonstrate that the proposed method not only has a high accuracy and low time cost, but also can be clearly interpreted.|['Shanwen Zhang', 'Harry Wang', 'Wenzhun Huang']|['stat.AP']
2017-03-16T23:26:02Z|2017-02-16T03:46:19Z|http://arxiv.org/abs/1702.04851v1|http://arxiv.org/pdf/1702.04851v1|A Comparison of Parametric and Permutation Tests for Regression Analysis   of Randomized Experiments|Hypothesis tests based on linear models are widely accepted by organizations that regulate clinical trials. These tests are derived using strong assumptions about the data-generating process so that the resulting inference can be based on parametric distributions. Because these methods are well understood and robust, they are sometimes applied to data that depart from assumptions, such as ordinal integer scores. Permutation tests are a nonparametric alternative that require minimal assumptions which are often guaranteed by the randomization that was conducted. We compare analysis of covariance (ANCOVA), a special case of linear regression that incorporates stratification, to several permutation tests based on linear models that control for pretreatment covariates. In simulations using a variety of data-generating processes, some of which violate the parametric assumptions, the permutation tests maintain power comparable to ANCOVA. We illustrate the use of these permutation tests alongside ANCOVA with data from a clinical trial comparing the effectiveness of two treatments for gastroesophageal reflux disease. Given the considerable costs and scientific importance of clinical trials, one may want to include an additional nonparametric method, such as a linear model permutation test, as a robustness check on the statistical inference for the main study endpoints.|['Kellie Ottoboni', 'Fraser Lewis', 'Luigi Salmaso']|['stat.AP']
2017-03-16T23:26:02Z|2017-02-16T03:16:14Z|http://arxiv.org/abs/1702.04846v1|http://arxiv.org/pdf/1702.04846v1|FMRI Clustering and False Positive Rates|"Recently, Eklund et al. (2016) analyzed clustering methods in standard FMRI packages: AFNI (which we maintain), FSL, and SPM [1]. They claimed: 1) false positive rates (FPRs) in traditional approaches are greatly inflated, questioning the validity of ""countless published fMRI studies""; 2) nonparametric methods produce valid, but slightly conservative, FPRs; 3) a common flawed assumption is that the spatial autocorrelation function (ACF) of FMRI noise is Gaussian-shaped; and 4) a 15-year-old bug in AFNI's 3dClustSim significantly contributed to producing ""particularly high"" FPRs compared to other software. We repeated simulations from [1] (Beijing-Zang data [2], see [3]), and comment on each point briefly."|['Robert W. Cox', 'Gang Chen', 'Daniel R. Glen', 'Richard C. Reynolds', 'Paul A. Taylor']|['q-bio.QM', 'stat.AP']
2017-03-16T23:26:06Z|2017-02-16T03:12:50Z|http://arxiv.org/abs/1702.04845v1|http://arxiv.org/pdf/1702.04845v1|FMRI Clustering in AFNI: False Positive Rates Redux|"Recent reports of inflated false positive rates (FPRs) in FMRI group analysis tools by Eklund et al. (2016) have become a large topic within (and outside) neuroimaging. They concluded that: existing parametric methods for determining statistically significant clusters had greatly inflated FPRs (""up to 70%,"" mainly due to the faulty assumption that the noise spatial autocorrelation function is Gaussian- shaped and stationary), calling into question potentially ""countless"" previous results; in contrast, nonparametric methods, such as their approach, accurately reflected nominal 5% FPRs. They also stated that AFNI showed ""particularly high"" FPRs compared to other software, largely due to a bug in 3dClustSim. We comment on these points using their own results and figures and by repeating some of their simulations. Briefly, while parametric methods show some FPR inflation in those tests (and assumptions of Gaussian-shaped spatial smoothness also appear to be generally incorrect), their emphasis on reporting the single worst result from thousands of simulation cases greatly exaggerated the scale of the problem. Importantly, FPR statistics depend on ""task"" paradigm and voxelwise p-value threshold; as such, we show how results of their study provide useful suggestions for FMRI study design and analysis, rather than simply a catastrophic downgrading of the field's earlier results. Regarding AFNI (which we maintain), 3dClustSim's bug-effect was greatly overstated - their own results show that AFNI results were not ""particularly"" worse than others. We describe further updates in AFNI for characterizing spatial smoothness more appropriately (greatly reducing FPRs, though some remain >5%); additionally, we outline two newly implemented permutation/randomization-based approaches producing FPRs clustered much more tightly about 5% for voxelwise p<=0.01."|['Robert W. Cox', 'Gang Chen', 'Daniel R. Glen', 'Richard C. Reynolds', 'Paul A. Taylor']|['q-bio.QM', 'stat.AP']
2017-03-16T23:26:06Z|2017-02-15T22:50:27Z|http://arxiv.org/abs/1702.04808v1|http://arxiv.org/pdf/1702.04808v1|A Model for Paired-Multinomial Data and Its Application to Analysis of   Data on a Taxonomic Tree|In human microbiome studies, sequencing reads data are often summarized as counts of bacterial taxa at various taxonomic levels specified by a taxonomic tree. This paper considers the problem of analyzing two repeated measurements of microbiome data from the same subjects. Such data are often collected to assess the change of microbial composition after certain treatment, or the difference in microbial compositions across body sites. Existing models for such count data are limited in modeling the covariance structure of the counts and in handling paired multinomial count data. A new probability distribution is proposed for paired-multinomial count data, which allows flexible covariance structure and can be used to model repeatedly measured multivariate count data. Based on this distribution, a test statistic is developed for testing the difference in compositions based on paired multinomial count data. The proposed test can be applied to the count data observed on a taxonomic tree in order to test difference in microbiome compositions and to identify the subtrees with different subcompositions. Simulation results indicate that proposed test has correct type 1 errors and increased power compared to some commonly used methods. An analysis of an upper respiratory tract microbiome data set is used to illustrate the proposed methods.|['Pixu Shi', 'Hongzhe Li']|['stat.AP']
2017-03-16T23:26:06Z|2017-02-16T21:06:31Z|http://arxiv.org/abs/1702.04690v2|http://arxiv.org/pdf/1702.04690v2|Simple rules for complex decisions|From doctors diagnosing patients to judges setting bail, experts often base their decisions on experience and intuition rather than on statistical models. While understandable, relying on intuition over models has often been found to result in inferior outcomes. Here we present a new method, select-regress-and-round, for constructing simple rules that perform well for complex decisions. These rules take the form of a weighted checklist, can be applied mentally, and nonetheless rival the performance of modern machine learning algorithms. Our method for creating these rules is itself simple, and can be carried out by practitioners with basic statistics knowledge. We demonstrate this technique with a detailed case study of judicial decisions to release or detain defendants while they await trial. In this application, as in many policy settings, the effects of proposed decision rules cannot be directly observed from historical data: if a rule recommends releasing a defendant that the judge in reality detained, we do not observe what would have happened under the proposed action. We address this key counterfactual estimation problem by drawing on tools from causal inference. We find that simple rules significantly outperform judges and are on par with decisions derived from random forests trained on all available features. Generalizing to 22 varied decision-making domains, we find this basic result replicates. We conclude with an analytical framework that helps explain why these simple decision rules perform as well as they do.|['Jongbin Jung', 'Connor Concannon', 'Ravi Shroff', 'Sharad Goel', 'Daniel G. Goldstein']|['stat.AP', 'stat.ML']
2017-03-16T23:26:06Z|2017-02-15T11:26:02Z|http://arxiv.org/abs/1702.04552v1|http://arxiv.org/pdf/1702.04552v1|A new class of robust two-sample Wald-type tests|Parametric hypothesis testing associated with two independent samples arises frequently in several applications in biology, medical sciences, epidemiology, reliability and many more. In this paper, we propose robust Wald-type tests for testing such two sample problems using the minimum density power divergence estimators of the underlying parameters. In particular, we consider the simple two-sample hypothesis concerning the full parametric homogeneity of the samples as well as the general two-sample (composite) hypotheses involving nuisance parameters also. The asymptotic and theoretical robustness properties of the proposed Wald-type tests have been developed for both the simple and general composite hypotheses. Some particular cases of testing against one-sided alternatives are discussed with specific attention to testing the effectiveness of a treatment in clinical trials. Performances of the proposed tests have also been illustrated numerically through appropriate real data examples.|['Abhik Ghosh', 'Nirian Martin', 'Ayanendranath Basu', 'Leandro Pardo']|['stat.ME', 'stat.AP']
2017-03-16T23:26:06Z|2017-02-15T03:02:20Z|http://arxiv.org/abs/1702.04450v1|http://arxiv.org/pdf/1702.04450v1|Applying Spatial Bootstrap and Bayesian Update in uncertainty assessment   at oil reservoir appraisal stages|Geostatistical modeling of the reservoir intrinsic properties starts only with sparse data available. These estimates will depend largely on the number of wells and their location. The drilling costs are so high that they do not allow new wells to be placed for uncertainty assessment. Besides that difficulty, usual geostatistical models do not account for the uncertainty of conceptual models, which should be considered.   Spatial bootstrap is applied to assess the estimate reliability when resampling from original field is not an option. Considering different realities (conceptual models) and different scenarios (estimates), spatial bootstrapping applied with Bayesian update allows uncertainty assessment of the initial estimate and of the conceptual model.   In this work an approach is suggested to integrate both these techniques, resulting in a method to assess which models are more appropriate for a given scenario.|['JÃºlio Caineta']|['stat.AP']
2017-03-16T23:26:06Z|2017-02-14T16:35:31Z|http://arxiv.org/abs/1702.04281v1|http://arxiv.org/pdf/1702.04281v1|Fitting Markovian binary trees using global and individual demographic   data|We consider a class of branching processes called Markovian binary trees, in which the individuals lifetime and reproduction epochs are modeled using a transient Markovian arrival process (TMAP). We estimate the parameters of the TMAP based on population data containing information on age-specific fertility and mortality rates. Depending on the degree of detail of the available data, a weighted non-linear regression method or a maximum likelihood method is applied. We discuss the optimal choice of the number of phases in the TMAP, and we provide confidence intervals for the model outputs. The results are illustrated using real data on human and bird populations.|['Sophie Hautphenne', 'Melanie Massaro', 'Katharine Turner']|['stat.AP', 'q-bio.PE']
2017-03-16T23:26:06Z|2017-02-14T13:27:12Z|http://arxiv.org/abs/1702.04197v1|http://arxiv.org/pdf/1702.04197v1|Dissimilar Symmetric Word Pairs in the Human Genome|In this work we explore the dissimilarity between symmetric word pairs, by comparing the inter-word distance distribution of a word to that of its reversed complement. We propose a new measure of dissimilarity between such distributions. Since symmetric pairs with different patterns could point to evolutionary features, we search for the pairs with the most dissimilar behaviour. We focus our study on the complete human genome and its repeat-masked version.|['Ana Helena Tavares', 'Jakob Raymaekers', 'Peter J. Rousseeuw', 'Raquel M. Silva', 'Carlos A. C. Bastos', 'Armando Pinho', 'Paula Brito', 'Vera Afreixo']|['stat.AP', 'q-bio.GN']
2017-03-16T23:26:06Z|2017-02-14T08:26:33Z|http://arxiv.org/abs/1702.04108v1|http://arxiv.org/pdf/1702.04108v1|Structure-Based Subspace Method for Multi-Channel Blind System   Identification|In this work, a novel subspace-based method for blind identification of multichannel finite impulse response (FIR) systems is presented. Here, we exploit directly the impeded Toeplitz channel structure in the signal linear model to build a quadratic form whose minimization leads to the desired channel estimation up to a scalar factor. This method can be extended to estimate any predefined linear structure, e.g. Hankel, that is usually encountered in linear systems. Simulation findings are provided to highlight the appealing advantages of the new structure-based subspace (SSS) method over the standard subspace (SS) method in certain adverse identification scenarii.|['Qadri Mayyala', 'Karim Abed-Meraim', 'Azzedine Zerguine']|['stat.AP', 'cs.IT', 'math.IT']
2017-03-16T23:26:06Z|2017-02-14T02:56:58Z|http://arxiv.org/abs/1702.04052v1|http://arxiv.org/pdf/1702.04052v1|When does poor governance presage biosecurity risk?|Border inspection, and the challenge of deciding which of the tens of millions of consignments that arrive should be inspected, is a perennial problem for regulatory authorities. The objective of these inspections is to minimise the risk of contraband entering the country. As an example, for regulatory authorities in charge of biosecurity material, consignments of goods are classified before arrival according to their economic tariff number (Department of Immigration and Border Protection, 2016). This classification, perhaps along with other information, is used as a screening step to determine whether further biosecurity intervention, such as inspection, is necessary. Other information associated with consignments includes details such as the country of origin, supplier, and importer, for example.   The choice of which consignments to inspect has typically been informed by historical records of intercepted material. Fortunately for regulators, interception is a rare event, however this sparsity undermines the utility of historical records for deciding which containers to inspect.   In this paper we report on an analysis that uses more detailed information to inform inspection. Using quarantine biosecurity as a case study, we create statistical profiles using generalised linear mixed models and compare different model specifications with historical information alone, demonstrating the utility of a statistical modelling approach. We also demonstrate some graphical model summaries that provide managers with insight into pathway governance.|['Stephen E Lane', 'Tony Arthur', 'Christina Aston', 'Sam Zhao', 'Andrew P Robinson']|['stat.AP']
2017-03-16T23:26:06Z|2017-02-14T01:41:21Z|http://arxiv.org/abs/1702.04044v1|http://arxiv.org/pdf/1702.04044v1|Statistical profiling to predict the biosecurity risk presented by   non-compliant international passengers|Biosecurity risk material (BRM) presents a clear and significant threat to national and international environmental and economic assets. Intercepting BRM carried by non-compliant international passengers is a key priority of border biosecurity services. Global travel rates are constantly increasing, which complicates this important responsibility, and necessitates judicious intervention. Selection of passengers for intervention is generally performed manually, and the quality of the selection depends on the experience and judgement of the officer making the selection. In this article we report on a case study to assess the predictive ability of statistical profiling methods that predict non-compliance with biosecurity regulations using data obtained from regulatory documents as inputs. We then evaluate the performance arising from using risk predictions to select higher risk passengers for screening. We find that both prediction performance and screening higher risk passengers from regulatory documents are superior to manual and random screening, and recommend that authorities further investigate statistical profiling for efficient intervention of biosecurity risk material on incoming passengers.|['Stephen E Lane', 'Richard Gao', 'Matthew Chisholm', 'Andrew P Robinson']|['stat.AP']
2017-03-16T23:26:10Z|2017-02-13T13:19:07Z|http://arxiv.org/abs/1702.03762v1|http://arxiv.org/pdf/1702.03762v1|An integrate-and-fire model to generate spike trains with long memory|We consider a new model of individual neuron of Integrate-and-Fire (IF) type with fractional noise. The correlations of its spike trains are studied and proved to have long memory, unlike classical IF models. To measure correctly long-range dependence, it is often necessary to know if the data are stationary. Thus, a methodology to evaluate stationarity of the interspike intervals (ISIs) is presented and applied to various IF models. It appears that the spike trains of our fractional model have the long-range dependence property, while those from classical Markovian models do not. However they may seem to have it because of non-stationarities.|['Alexandre Richard', 'Patricio Orio', 'Etienne TanrÃ©']|['q-bio.NC', 'stat.AP']
2017-03-16T23:26:10Z|2017-02-13T02:48:16Z|http://arxiv.org/abs/1702.03613v1|http://arxiv.org/pdf/1702.03613v1|A Multi-model Combination Approach for Probabilistic Wind Power   Forecasting|Short-term probabilistic wind power forecasting can provide critical quantified uncertainty information of wind generation for power system operation and control. As the complicated characteristics of wind power prediction error, it would be difficult to develop a universal forecasting model dominating over other alternative models. Therefore, a novel multi-model combination (MMC) approach for short-term probabilistic wind generation forecasting is proposed in this paper to exploit the advantages of different forecasting models. The proposed approach can combine different forecasting models those provide different kinds of probability density functions to improve the probabilistic forecast accuracy. Three probabilistic forecasting models based on the sparse Bayesian learning, kernel density estimation and beta distribution fitting are used to form the combined model. The parameters of the MMC model are solved based on Bayesian framework. Numerical tests illustrate the effectiveness of the proposed MMC approach.|['You Lin', 'Ming Yang', 'Can Wan', 'Jianhui Wang', 'Yonghua Song']|['cs.LG', 'stat.AP']
2017-03-16T23:26:10Z|2017-02-11T10:52:36Z|http://arxiv.org/abs/1702.03409v1|http://arxiv.org/pdf/1702.03409v1|Disruptive Behavior Disorder (DBD) Rating Scale for Georgian Population|In the presented study Parent/Teacher Disruptive Behavior Disorder (DBD) rating scale based on the Diagnostic and Statistical Manual of Mental Disorders (DSM-IV-TR [APA, 2000]) which was developed by Pelham and his colleagues (Pelham et al., 1992) was translated and adopted for assessment of childhood behavioral abnormalities, especially ADHD, ODD and CD in Georgian children and adolescents. The DBD rating scale was translated into Georgian language using back translation technique by English language philologists and checked and corrected by qualified psychologists and psychiatrist of Georgia. Children and adolescents in the age range of 6 to 16 years (N 290; Mean Age 10.50, SD=2.88) including 153 males (Mean Age 10.42, SD= 2.62) and 141 females (Mean Age 10.60, SD=3.14) were recruited from different public schools of Tbilisi and the Neurology Department of the Pediatric Clinic of the Tbilisi State Medical University. Participants objectively were assessed via interviewing parents/teachers and qualified psychologists in three different settings including school, home and clinic. In terms of DBD total scores revealed statistically significant differences between healthy controls (M=27.71, SD=17.26) and children and adolescents with ADHD (M=61.51, SD= 22.79). Statistically significant differences were found for inattentive subtype between control (M=8.68, SD=5.68) and ADHD (M=18.15, SD=6.57) groups. In general it was shown that children and adolescents with ADHD had high score on DBD in comparison to typically developed persons. In the study also was determined gender wise prevalence in children and adolescents with ADHD, ODD and CD. The research revealed prevalence of males in comparison with females in all investigated categories.|['Vera Bzhalava', 'Ketevan Inasaridze']|['q-bio.NC', 'stat.AP']
2017-03-16T23:26:10Z|2017-02-10T17:08:43Z|http://arxiv.org/abs/1702.03252v1|http://arxiv.org/pdf/1702.03252v1|Markov Models for Health Economic Evaluation: The R Package heemod|Health economic evaluation studies are widely used in public health to assess health strategies in terms of their cost-effectiveness and inform public policies. We developed an R package for Markov models implementing most of the modelling and reporting features described in reference textbooks and guidelines: deterministic and probabilistic sensitivity analysis, heterogeneity analysis, time dependency on state-time and model-time (semi-Markov and non-homogeneous Markov models), etc. In this paper we illustrate the features of heemod by building and analysing an example Markov model. We then explain the design and the underlying implementation of the package.|['Antoine FilipoviÄ-Pierucci', 'Kevin Zarca', 'Isabelle Durand-Zaleski']|['stat.AP']
2017-03-16T23:26:10Z|2017-02-09T20:10:32Z|http://arxiv.org/abs/1702.02966v1|http://arxiv.org/pdf/1702.02966v1|A monitoring and diagnostic approach for stochastic textured surfaces|We develop a supervised-learning-based approach for monitoring and diagnosing texture-related defects in manufactured products characterized by stochastic textured surfaces that satisfy the locality and stationarity properties of Markov random fields. Examples of stochastic textured surface data include images of woven textiles; image or surface metrology data for machined, cast, or formed metal parts; microscopy images of material microstructure samples; etc. To characterize the complex spatial statistical dependencies of in-control samples of the stochastic textured surface, we use rather generic supervised learning methods, which provide an implicit characterization of the joint distribution of the surface texture. We propose two spatial moving statistics, which are computed from residual errors of the fitted supervised learning model, for monitoring and diagnosing local aberrations in the general spatial statistical behavior of newly manufactured stochastic textured surface samples in a statistical process control context. We illustrate the approach using images of textile fabric samples and simulated 2-D stochastic processes, for which the algorithm successfully detects local defects of various natures. Additional discussions and results are available in the supplementary materials.|['Anh Tuan Bui', 'Daniel W. Apley']|['stat.AP']
2017-03-16T23:26:10Z|2017-02-09T17:57:14Z|http://arxiv.org/abs/1702.03862v1|http://arxiv.org/pdf/1702.03862v1|Bayesian Networks Analysis of Malocclusion Data|In this paper we use Bayesian networks to determine and visualise the interactions among various Class III malocclusion maxillofacial features during growth and treatment. We start from a sample of 143 patients characterised through a series of a maximum of 21 different craniofacial features. We estimate a network model from these data and we test its consistency by verifying some commonly accepted hypotheses on the evolution of these disharmonies by means of Bayesian statistics. We show that untreated subjects develop different Class III craniofacial growth patterns as compared to patients submitted to orthodontic treatment with rapid maxillary expantion and facemask therapy. Among treated patients the CoA segment (the maxillary length) and the ANB angle (the antero-posterior relation of the maxilla to the mandible) seem to be the skeletal subspaces that receive the main effect of the treatment.|['Marco Scutari', 'Pietro Auconi', 'Guido Caldarelli', 'Lorenzo Franchi']|['stat.AP']
2017-03-16T23:26:10Z|2017-02-08T03:56:00Z|http://arxiv.org/abs/1702.02268v1|http://arxiv.org/pdf/1702.02268v1|Efficient Modelling & Forecasting with range based volatility models and   application|This paper considers an alternative method for fitting CARR models using combined estimating functions (CEF) by showing its usefulness in applications in economics and quantitative finance. The associated information matrix for corresponding new estimates is derived to calculate the standard errors. A simulation study is carried out to demonstrate its superiority relative to other two competitors: linear estimating functions (LEF) and the maximum likelihood (ML). Results show that CEF estimates are more efficient than LEF and ML estimates when the error distribution is mis-specified. Taking a real data set from financial economics, we illustrate the usefulness and applicability of the CEF method in practice and report reliable forecast values to minimize the risk in the decision making process.|['Kok-Haur Ng', 'Shelton Peiris', 'Jennifer So-kuen-Chan', 'David Allen', 'Kooi-Huat Ng']|['stat.AP', 'math.ST', 'stat.TH']
2017-03-16T23:26:10Z|2017-02-08T03:32:19Z|http://arxiv.org/abs/1702.02264v1|http://arxiv.org/pdf/1702.02264v1|Precision Therapeutic Biomarker Identification with Application to the   Cancer Genome Project|Cancer cell lines have frequently been used to link drug sensitivity and resistance with genomic profiles. To capture genomic complexity in cancer, the Cancer Genome Project (CGP) (Garnett et al., 2012) screened 639 human tumor cell lines with 130 drugs ranging from known chemotherapeutic agents to experimental compounds. Questions of interest include: i) can cancer-specific therapeutic biomarkers be detected, ii) can drug resistance patterns be identified along with predictive strategies to circumvent resistance using alternate drugs, iii) can biomarkers of drug synergies be predicted ? To tackle these questions, following statistical challenges still exist: i)biomarkers cluster among the cell lines; ii) clusters can overlap (e.g. a cell line may belong to multiple clusters); iii) drugs should be modeled jointly. We introduce a multivariate regression model with a latent overlapping cluster indicator variable to address above issues. A generalized finite mixture of multivariate regression (FMMR) model in connection with the new model and a new EM algorithm for fitting are proposed. Re-analysis of the dataset sheds new light on the therapeutic inter-relationships between cancers as well existing and novel drug behaviors for the treatment and management of cancer.|['Hongmei Liu', 'J. Sunil Rao']|['stat.AP', '62-07, 62J07']
2017-03-16T23:26:10Z|2017-02-07T19:00:00Z|http://arxiv.org/abs/1702.02149v1|http://arxiv.org/abs/1702.02149v1|An intermediate-mass black hole in the centre of the globular cluster 47   Tucanae|Intermediate mass black holes play a critical role in understanding the evolutionary connection between stellar mass and super-massive black holes. However, to date the existence of these species of black holes remains ambiguous and their formation process is therefore unknown. It has been long suspected that black holes with masses $10^{2}-10^{4}M_{\odot}$ should form and reside in dense stellar systems. Therefore, dedicated observational campaigns have targeted globular cluster for many decades searching for signatures of these elusive objects. All candidates found in these targeted searches appear radio dim and do not have the X-ray to radio flux ratio predicted by the fundamental plane for accreting black holes. Based on the lack of an electromagnetic counterpart upper limits of $2060 M_{\odot}$ and $470 M_{\odot}$ have been placed on the mass of a putative black hole in 47 Tucanae (NGC 104) from radio and X-ray observations respectively. Here we show there is evidence for a central black hole in 47 Tuc with a mass of M$_{\bullet}\sim2200 M_{\odot}$$_{-800}^{+1500}$ when the dynamical state of the globular cluster is probed with pulsars. The existence of an intermediate mass black hole in the centre of one of the densest clusters with no detectable electromagnetic counterpart suggests that the black hole is not accreting at a sufficient rate and therefore contrary to expectations is gas starved. This intermediate mass black hole might be a member of electromagnetically invisible population of black holes that are the elusive seeds leading to the formation of supermassive black holes in galaxies.|['BÃ¼lent KÄ±zÄ±ltan', 'Holger Baumgardt', 'Abraham Loeb']|['astro-ph.GA', 'astro-ph.IM', 'astro-ph.SR', 'stat.AP']
2017-03-16T23:26:10Z|2017-02-07T14:06:58Z|http://arxiv.org/abs/1702.02025v1|http://arxiv.org/pdf/1702.02025v1|Efficient fetal-maternal ECG signal separation from two channel maternal   abdominal ECG via diffusion-based channel selection|There is a need for affordable, widely deployable maternal-fetal ECG monitors to improve maternal and fetal health during pregnancy and delivery. Based on the diffusion-based channel selection, here we present the mathematical formalism and clinical validation of an algorithm capable of accurate separation of maternal and fetal ECG from a two channel signal acquired over maternal abdomen.|['Ruilin Li', 'Martin G. Frasch', 'Hau-tieng Wu']|['physics.med-ph', 'physics.data-an', 'stat.AP', 'stat.ML']
2017-03-16T23:26:15Z|2017-02-07T13:12:51Z|http://arxiv.org/abs/1702.01995v1|http://arxiv.org/pdf/1702.01995v1|Statistics-Based Compression of Global Wind Fields|Wind has the potential to make a significant contribution to future energy resources; however, the task of locating the sources of this renewable energy on a global scale with climate models, along with the associated uncertainty, is hampered by the storage challenges associated with the extremely large amounts of computer output. Various data compression techniques can be used to mitigate this problem, but traditional algorithms deliver relatively small compression rates by focusing on individual simulations. We propose a statistical model that aims at reproducing the data-generating mechanism of an ensemble of runs by providing a stochastic approximation of global annual wind data and compressing all the scientific information in the estimated statistical parameters. We introduce an evolutionary spectrum approach with spatially varying parameters based on large-scale geographical descriptors such as altitude to better account for different regimes across the Earth's orography. We consider a multi-step conditional likelihood approach to estimate the parameters that explicitly accounts for nonstationary features while also balancing memory storage and distributed computation, and we apply the proposed model to more than 18 million points on yearly global wind speed. The proposed model achieves compression rates that are orders of magnitude higher than those achieved by traditional algorithms on yearly-averaged variables, and once the statistical model is fitted, decompressed runs can be almost instantaneously generated to better assess wind speed uncertainty due to internal variability.|['Jaehong Jeong', 'Stefano Castruccio', 'Paola Crippa', 'Marc G. Genton']|['stat.AP']
2017-03-16T23:26:15Z|2017-02-07T12:45:25Z|http://arxiv.org/abs/1702.01987v1|http://arxiv.org/pdf/1702.01987v1|Importance sampling with transformed weights|The importance sampling (IS) method lies at the core of many Monte Carlo-based techniques. IS allows the approximation of a target probability distribution by drawing samples from a proposal (or importance) distribution, different from the target, and computing importance weights (IWs) that account for the discrepancy between these two distributions. The main drawback of IS schemes is the degeneracy of the IWs, which significantly reduces the efficiency of the method. It has been recently proposed to use transformed IWs (TIWs) to alleviate the degeneracy problem in the context of Population Monte Carlo, which is an iterative version of IS. However, the effectiveness of this technique for standard IS is yet to be investigated. In this letter we numerically assess the performance of IS when using TIWs, and show that the method can attain robustness to weight degeneracy thanks to a bias/variance trade-off.|['Manuel A. VÃ¡zquez', 'JoaquÃ­n MÃ­guez']|['stat.AP']
2017-03-16T23:26:15Z|2017-02-07T06:14:55Z|http://arxiv.org/abs/1702.01890v1|http://arxiv.org/pdf/1702.01890v1|Graphical Models and Belief Propagation-hierarchy for Optimal   Physics-Constrained Network Flows|In this manuscript we review new ideas and first results on application of the Graphical Models approach, originated from Statistical Physics, Information Theory, Computer Science and Machine Learning, to optimization problems of network flow type with additional constraints related to the physics of the flow. We illustrate the general concepts on a number of enabling examples from power system and natural gas transmission (continental scale) and distribution (district scale) systems.|['Michael Chertkov', 'Sidhant Misra', 'Marc Vuffray', 'Dvijotham Krishnamurty', 'Pascal Van Hentenryck']|['cs.SY', 'stat.AP']
2017-03-16T23:26:15Z|2017-02-07T02:52:52Z|http://arxiv.org/abs/1702.01858v1|http://arxiv.org/pdf/1702.01858v1|2D Sinusoidal Parameter Estimation with Offset Term|We consider the parameter estimation of a 2D sinusoid. Although sinusoidal parameter estimation has been extensively studied, our model differs from those examined in the available literature by the inclusion of an offset term. We derive both the maximum likelihood estimation (MLE) solution and the Cramer-Rao lower bound (CRLB) on the variance of the model's estimators.|['A. Pasha Hosseinbor', 'Renat Zhdanov']|['stat.AP']
2017-03-16T23:26:15Z|2017-02-07T01:16:03Z|http://arxiv.org/abs/1702.01838v1|http://arxiv.org/pdf/1702.01838v1|Meta Analytic Data Integration for Phenotype Prediction: Application to   Chronic Fatigue Syndrome|Predictive modeling plays key role in providing accurate prognosis and enables us to take a step closer to personalized treatment. We identified two potential sources of human induced biases that can lead to disparate conclusions. We illustrate through a complex phenotype that robust results can still be drawn after accounting for such biases.   Often predictive models build based in high dimensional data suffers from the drawback of lack of interpretability. To achieve interpretability in the form of description of the organism level phenomena in term of molecular or cellular level activities, functional and pathway information is often augmented. Functional information can greatly facilitate the interpretation of the results of the predictive model.   However an important aspect of (vertical) data augmentation is routinely ignored, that is there could be several stages of analysis where such information could be meaningfully integrated. There is no know criteria to enable us to assess the effect of such augmentation. A novel aspect of the proposed work is in exploring possibilities of stages of analysis where functional information may be incorporated and in assessing the extent to which the ultimate conclusions would differ depending on level of amalgamation.   To boost our confidence on the key findings a first level of meta-analysis is done by exploring different levels of data augmentation. This is followed by comparison of predictive models across different definitions of the same phenotype developed by different groups, which is also an extended form of meta-analysis.   We have used real life data on a complex phenotype to illustrate the above. The data pertains to Chronic Fatigue Syndrome (CFS) and another novel aspect of the current work is in modeling the underlying continuous symptom measurements for CFS, which is the first for this disease to our knowledge.|['Madhuchhanda Bhattacharjee']|['stat.AP']
2017-03-16T23:26:15Z|2017-02-07T00:32:25Z|http://arxiv.org/abs/1702.01830v1|http://arxiv.org/pdf/1702.01830v1|Incoherence of Partial-Component Sampling in multidimensional NMR|In NMR spectroscopy, undersampling in the indirect dimensions causes reconstruction artifacts whose size can be bounded using the so-called {\it coherence}. In experiments with multiple indirect dimensions, new undersampling approaches were recently proposed: random phase detection (RPD) \cite{Maciejewski11} and its generalization, partial component sampling (PCS) \cite{Schuyler13}. The new approaches are fully aware of the fact that high-dimensional experiments generate hypercomplex-valued free induction decays; they randomly acquire only certain low-dimensional components of each high-dimensional hypercomplex entry. We provide a classification of various hypercomplex-aware undersampling schemes, and define a hypercomplex-aware coherence appropriate for such undersampling schemes; we then use it to quantify undersampling artifacts of RPD and various PCS schemes.|['Hatef Monajemi', 'David L. Donoho', 'Jeffrey C. Hoch', 'Adam D. Schuyler']|['stat.AP']
2017-03-16T23:26:15Z|2017-02-06T21:10:43Z|http://arxiv.org/abs/1702.01793v1|http://arxiv.org/pdf/1702.01793v1|Multiuser Communication Based on the DFT Eigenstructure|The eigenstructure of the discrete Fourier transform (DFT) is examined and new systematic procedures to generate eigenvectors of the unitary DFT are proposed. DFT eigenvectors are suggested as user signatures for data communication over the real adder channel (RAC). The proposed multiuser communication system over the 2-user RAC is detailed.|['R. M. Campello de Souza', 'H. M. de Oliveira', 'R. J. Cintra']|['cs.IT', 'math.IT', 'stat.AP', '94A05, 94A11, 94A40']
2017-03-16T23:26:15Z|2017-02-06T11:45:39Z|http://arxiv.org/abs/1702.01576v1|http://arxiv.org/pdf/1702.01576v1|Quickest Localization of Anomalies in Power Grids: A Stochastic   Graphical Framework|Agile localization of anomalous events plays a pivotal role in enhancing the overall reliability of the grid and avoiding cascading failures. This is especially of paramount significance in the large-scale grids due to their geographical expansions and the large volume of data generated. This paper proposes a stochastic graphical framework, by leveraging which it aims to localize the anomalies with the minimum amount of data. This framework capitalizes on the strong correlation structures observed among the measurements collected from different buses. The proposed approach, at its core, collects the measurements sequentially and progressively updates its decision about the location of the anomaly. The process resumes until the location of the anomaly can be identified with desired reliability. We provide a general theory for the quickest anomaly localization and also investigate its application for quickest line outage localization. Simulations in the IEEE 118-bus model are provided to establish the gains of the proposed approach.|['Javad Heydari', 'Ali Tajer']|['stat.AP', 'cs.IT', 'math.IT']
2017-03-16T23:26:15Z|2017-02-03T23:55:52Z|http://arxiv.org/abs/1702.01206v1|http://arxiv.org/pdf/1702.01206v1|Adaptation of the visibility graph algorithm to find the time lag   between hydrogeological time series|Estimating the time lag between two hydrogeologic time series (e.g. precipitation and water levels in an aquifer) is of significance for a hydrogeologist-modeler. In this paper, we present a method to quantify such lags by adapting the visibility graph algorithm, which converts time series into a mathematical graph. We present simulation results to assess the performance of the method. We also illustrate the utility of our approach using a real world hydrogeologic dataset.|['Rahul John', 'Majnu John']|['stat.AP']
2017-03-16T23:26:15Z|2017-02-10T21:47:39Z|http://arxiv.org/abs/1702.01201v2|http://arxiv.org/pdf/1702.01201v2|Statistical details of the default priors in the Bambi library|This is a companion paper to Yarkoni and Westfall (2017), which describes the Python package Bambi for estimating Bayesian generalized linear mixed models using a simple interface. Here I give the statistical details underlying the default, weakly informative priors used in all models when the user does not specify the priors. Our approach is to first deduce what the variances of the slopes would be if we were instead to have defined the priors on the partial correlation scale, and then to set independent Normal priors on the slopes with variances equal to these implied variances. Our approach is similar in spirit to that of Zellner's g-prior (Zellner 1986), in that it involves a multivariate normal prior on the regression slopes, with a tuning parameter to control the width or informativeness of the priors irrespective of the scales of the data and predictors. The primary differences are that here the tuning parameter is directly interpretable as the standard deviation of the distribution of plausible partial correlations, and that this tuning parameter can have different values for different coefficients. The default priors for the intercepts and random effects are ultimately based on the prior slope variances.|['Jacob Westfall']|['stat.AP']
