2017-03-16T23:27:02Z|2017-03-15T17:03:56Z|http://arxiv.org/abs/1703.05264v1|http://arxiv.org/pdf/1703.05264v1|Smooth Image-on-Scalar Regression for Brain Mapping|Brain mapping is an increasingly important tool in neurology and psychiatry researches for the realization of data-driven personalized medicine in the big data era, which learns the statistical links between brain images and subject level features. Taking images as responses, the task raises a lot of challenges due to the high dimensionality of the image with relatively small number of samples, as well as the noisiness of measurements in medical images.   In this paper we propose a novel method {\it Smooth Image-on-scalar Regression} (SIR) for recovering the true association between an image outcome and scalar predictors. The estimator is achieved by minimizing a mean squared error with a total variation (TV) regularization term on the predicted mean image across all subjects. It denoises the images from all subjects and at the same time returns the coefficient maps estimation. We propose an algorithm to solve this optimization problem, which is efficient when combined with recent advances in graph fused lasso solvers. The statistical consistency of the estimator is shown via an oracle inequality.   Simulation results demonstrate that the proposed method outperforms existing methods with separate denoising and regression steps. Especially, SIR shows an evident advantage in recovering signals in small regions. We apply SIR on Alzheimer's Disease Neuroimaging Initiative data and produce interpretable brain maps of the PET image to patient-level features include age, gender, genotype and disease groups.|['Ying Liu', 'Bowei Yan']|['stat.ME', 'stat.AP']
2017-03-16T23:27:02Z|2017-03-15T15:27:46Z|http://arxiv.org/abs/1703.05208v1|http://arxiv.org/pdf/1703.05208v1|Understanding the Probabilistic Latent Component Analysis Framework|Probabilistic Component Latent Analysis (PLCA) is a statistical modeling method for feature extraction from non-negative data. It has been fruitfully applied to various research fields of information retrieval. However, the EM-solved optimization problem coming with the parameter estimation of PLCA-based models has never been properly posed and justified. We then propose in this short paper to re-define the theoretical framework of this problem, with the motivation of making it clearer to understand, and more admissible for further developments of PLCA-based computational systems.|['D. Cazau', 'G. Nuel']|['stat.ME']
2017-03-16T23:27:02Z|2017-03-15T15:19:14Z|http://arxiv.org/abs/1703.05203v1|http://arxiv.org/pdf/1703.05203v1|Growing simplified vine copula trees: improving Dißmann's algorithm|Vine copulas are pair-copula constructions enabling multivariate dependence modeling in terms of bivariate building blocks. One of the main tasks of fitting a vine copula is the selection of a suitable tree structure. For this the prevalent method is a heuristic called Di{\ss}mann's algorithm. It sequentially constructs the vine's trees by maximizing dependence at each tree level, where dependence is measured in terms of absolute Kendall's $\tau$. However, the algorithm disregards any implications of the tree structure on the simplifying assumption that is usually made for vine copulas to keep inference tractable. We develop two new algorithms that select tree structures focused on producing simplified vine copulas for which the simplifying assumption is violated as little as possible. For this we make use of a recently developed statistical test of the simplifying assumption. In a simulation study we show that our proposed methods outperform the benchmark given by Di{\ss}mann's algorithm by a great margin. Several real data applications emphasize their practical relevance.|['Daniel Kraus', 'Claudia Czado']|['stat.ME']
2017-03-16T23:27:02Z|2017-03-15T14:47:02Z|http://arxiv.org/abs/1703.05189v1|http://arxiv.org/pdf/1703.05189v1|Student-t Process Quadratures for Filtering of Non-Linear Systems with   Heavy-Tailed Noise|The aim of this article is to design a moment transformation for Student- t distributed random variables, which is able to account for the error in the numerically computed mean. We employ Student-t process quadrature, an instance of Bayesian quadrature, which allows us to treat the integral itself as a random variable whose variance provides information about the incurred integration error. Advantage of the Student- t process quadrature over the traditional Gaussian process quadrature, is that the integral variance depends also on the function values, allowing for a more robust modelling of the integration error. The moment transform is applied in nonlinear sigma-point filtering and evaluated on two numerical examples, where it is shown to outperform the state-of-the-art moment transforms.|['Jakub Prüher', 'Filip Tronarp', 'Toni Karvonen', 'Simo Särkkä', 'Ondřej Straka']|['stat.ME', 'stat.ML']
2017-03-16T23:27:02Z|2017-03-15T13:51:52Z|http://arxiv.org/abs/1703.05157v1|http://arxiv.org/pdf/1703.05157v1|One-Sided Cross-Validation for Nonsmooth Density Functions|One-sided cross-validation (OSCV) is a bandwidth selection method initially introduced by Hart and Yi (1998) in the context of smooth regression functions. Mart\'{\i}nez-Miranda et al. (2009) developed a version of OSCV for smooth density functions. This article extends the method for nonsmooth densities. It also introduces the fully robust OSCV modification that produces consistent OSCV bandwidths for both smooth and nonsmooth cases. Practical implementations of the OSCV method for smooth and nonsmooth densities are discussed. One of the considered cross-validation kernels has potential for improving the OSCV method's implementation in the regression context.|['Olga Y. Savchuk']|['stat.ME', '62G07']
2017-03-16T23:27:02Z|2017-03-15T12:18:21Z|http://arxiv.org/abs/1703.05109v1|http://arxiv.org/pdf/1703.05109v1|Quantile Treatment Effects in the Regression Kink Design|This paper studies identification, estimation, and inference of quantile treatment effects in the fuzzy regression kink design with a binary treatment variable. We first show the identification of conditional quantile treatment effects given the event of local compliance. We then propose a bootstrap method of uniform inference for the local quantile process. This bootstrap method is fast and is robust against common optimal choices of bandwidth parameters. We provide practical guidelines as well as a formal theory. Simulation studies show accurate coverage probabilities for tests of uniform treatment significance and treatment heterogeneity.|['Harold D. Chiang', 'Yuya Sasaki']|['stat.ME']
2017-03-16T23:27:02Z|2017-03-15T06:34:38Z|http://arxiv.org/abs/1703.04956v1|http://arxiv.org/pdf/1703.04956v1|A Short Note on Almost Sure Convergence of Bayes Factors in the General   Set-Up|In this article we derive the almost sure convergence theory of Bayes factor in the general set-up that includes even dependent data and misspecified models, as a simple application of a result of Shalizi (2009) to a well-known identity satisfied by the Bayes factor.|['Debashis Chatterjee', 'Trisha Maitra', 'Sourabh Bhattacharya']|['math.ST', 'stat.ME', 'stat.TH']
2017-03-16T23:27:02Z|2017-03-15T06:17:40Z|http://arxiv.org/abs/1703.04951v1|http://arxiv.org/pdf/1703.04951v1|Robust and sparse estimation methods for high dimensional linear and   logistic regression|Fully robust versions of the elastic net estimator are introduced for linear and logistic regression. The algorithms to compute the estimators are based on the idea of repeatedly applying the non-robust classical estimators to data subsets only. It is shown how outlier-free subsets can be identified efficiently, and how appropriate tuning parameters for the elastic net penalties can be selected. A final reweighting step improves the efficiency of the estimators. Simulation studies compare with non-robust and other competing robust estimators and reveal the superiority of the newly proposed methods. This is also supported by a reasonable computation time and by good performance in real data examples.|['Fatma Sevinc Kurnaz', 'Irene Hoffmann', 'Peter Filzmoser']|['stat.ME']
2017-03-16T23:27:02Z|2017-03-15T02:08:07Z|http://arxiv.org/abs/1703.04882v1|http://arxiv.org/pdf/1703.04882v1|Element analysis: a wavelet-based method for analyzing time-localized   events in noisy time series|"A method is derived for the quantitative analysis of signals that are composed of superpositions of isolated, time-localized ""events"". Here these events are taken to be well represented as rescaled and phase-rotated versions of generalized Morse wavelets, a broad family of continuous analytic functions. Analyzing a signal composed of replicates of such a function using another Morse wavelet allows one to directly estimate the properties of events from the values of the wavelet transform at its own maxima. The distribution of events in general power-law noise is determined in order to establish significance based on an expected false detection rate. Finally, an expression for an event's ""region of influence"" within the wavelet transform permits the formation of a criterion for rejecting spurious maxima due to numerical artifacts or other unsuitable events. Signals can then be reconstructed based on a small number of isolated points on the time/scale plane. This method, termed element analysis, is applied to the identification of long-lived eddy structures in ocean currents as observed by along-track measurements of sea surface elevation from satellite altimetry"|['J. M. Lilly']|['stat.ME']
2017-03-16T23:27:02Z|2017-03-13T11:19:28Z|http://arxiv.org/abs/1703.04334v1|http://arxiv.org/pdf/1703.04334v1|Probabilistic Matching: Causal Inference under Measurement Errors|The abundance of data produced daily from large variety of sources has boosted the need of novel approaches on causal inference analysis from observational data. Observational data often contain noisy or missing entries. Moreover, causal inference studies may require unobserved high-level information which needs to be inferred from other observed attributes. In such cases, inaccuracies of the applied inference methods will result in noisy outputs. In this study, we propose a novel approach for causal inference when one or more key variables are noisy. Our method utilizes the knowledge about the uncertainty of the real values of key variables in order to reduce the bias induced by noisy measurements. We evaluate our approach in comparison with existing methods both on simulated and real scenarios and we demonstrate that our method reduces the bias and avoids false causal inference conclusions in most cases.|['Fani Tsapeli', 'Peter Tino', 'Mirco Musolesi']|['stat.ME', 'stat.CO', 'stat.ML']
2017-03-16T23:27:06Z|2017-03-13T06:08:51Z|http://arxiv.org/abs/1703.04264v1|http://arxiv.org/pdf/1703.04264v1|Poisson multi-Bernoulli mixture filter: direct derivation and   implementation|We provide a derivation of the Poisson multi-Bernoulli mixture (PMBM) filter for multi-target tracking with the standard point target measurements without using probability generating functionals or functional derivatives. We also establish the connection with the \delta-generalised labelled multi-Bernoulli (\delta-GLMB) filter, showing that a \delta-GLMB density represents a multi-Bernoulli mixture with labelled targets so it can be seen as a special case of PMBM. In addition, we propose an implementation for linear/Gaussian dynamic and measurement models and how to efficiently obtain typical estimators in the literature from the PMBM. The PMBM filter is shown to outperform other filters in the literature in a challenging scenario|['Ángel F. García-Fernández', 'Jason L. Williams', 'Karl Granström', 'Lennart Svensson']|['cs.CV', 'stat.ME']
2017-03-16T23:27:06Z|2017-03-12T21:19:29Z|http://arxiv.org/abs/1703.04180v1|http://arxiv.org/pdf/1703.04180v1|MEDL and MEDLA: Methods for Assessment of Scaling by Medians of   Log-Squared Nondecimated Wavelet Coefficients|"High-frequency measurements and images acquired from various sources in the real world often possess a degree of self-similarity and inherent regular scaling. When data look like a noise, the scaling exponent may be the only informative feature that summarizes such data. Methods for the assessment of self-similarity by estimating Hurst exponent often involve analysis of rate of decay in a spectrum defined in various multiresolution domains. When this spectrum is calculated using discrete non-decimated wavelet transforms, due to increased autocorrelation in wavelet coefficients, the estimators of $H$ show increased bias compared to the estimators that use traditional orthogonal transforms. At the same time, non-decimated transforms have a number of advantages when employed for calculation of wavelet spectra and estimation of Hurst exponents: the variance of the estimator is smaller, input signals and images could be of arbitrary size, and due to the shift-invariance, the local scaling can be assessed as well. We propose two methods based on robust estimation and resampling that alleviate the effect of increased autocorrelation while maintaining all advantages of non-decimated wavelet transforms. The proposed methods extend the approaches in existing literature where the logarithmic transformation and pairing of wavelet coefficients are used for lowering the bias. In a simulation study we use fractional Brownian motions with a range of theoretical Hurst exponents. For such signals for which ""true"" $H$ is known, we demonstrate bias reduction and overall reduction of the mean-squared error by the two proposed estimators. For fractional Brownian motions, both proposed methods yield estimators of $H$ that are asymptotically normal and unbiased."|['Minkyoung Kang', 'Brani Vidakovic']|['stat.ME']
2017-03-16T23:27:06Z|2017-03-12T18:29:03Z|http://arxiv.org/abs/1703.04157v1|http://arxiv.org/pdf/1703.04157v1|Using Aggregated Relational Data to feasibly identify network structure   without network data|"Social and economic network data can be useful for both researchers and policymakers, but can often be impractical to collect. We propose collecting Aggregated Relational Data (ARD) using questions that are simple and easy to add to any survey. These question are of the form ""how many of your friends in the village have trait k?""   We show that by collecting ARD on even a small share of the population, researchers can recover the likely distribution of statistics from the underlying network. We provide three empirical examples. We first apply the technique to the 75 village networks in Karnataka, India, where Banerjee et al. (2016b) collected near-complete network data. We show that with ARD alone on even a 29% sample, we can accurately estimate both node-level features (such as eigenvector centrality, clustering) and network-level features (such as the maximum eigenvalue, average path length). To further demonstrate the power of the approach, we apply our technique to two settings analyzed previously by the authors. We show ARD could have been used to predict how to assign monitors to savers to increase savings in rural villages (Breza and Chandrasekhar, 2016). ARD would have led to the same conclusions the authors arrived at when they used expensive near-complete network data. We then provide an example where survey ARD was collected, along with some partial network data, and demonstrate that the same conclusions would have been drawn using only the ARD data, and that with the ARD, the researchers could more generally measure the impact of microfinance exposure on social capital in urban slums (Banerjee et al., 2016a)."|['Emily Breza', 'Arun G. Chandrasekhar', 'Tyler H. McCormick', 'Mengjie Pan']|['stat.ME']
2017-03-16T23:27:06Z|2017-03-11T20:07:06Z|http://arxiv.org/abs/1703.04025v1|http://arxiv.org/pdf/1703.04025v1|Learning Large-Scale Bayesian Networks with the sparsebn Package|Learning graphical models from data is an important problem with wide applications, ranging from genomics to the social sciences. Nowadays datasets typically have upwards of thousands---sometimes tens or hundreds of thousands---of variables and far fewer samples. To meet this challenge, we develop a new R package called sparsebn for learning the structure of large, sparse graphical models with a focus on Bayesian networks. While there are many existing packages for this task within the R ecosystem, this package focuses on the unique setting of learning large networks from high-dimensional data, possibly with interventions. As such, the methods provided place a premium on scalability and consistency in a high-dimensional setting. Furthermore, in the presence of interventions, the methods implemented here achieve the goal of learning a causal network from data. The sparsebn package is open-source and available on CRAN.|['Bryon Aragam', 'Jiaying Gu', 'Qing Zhou']|['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']
2017-03-16T23:27:06Z|2017-03-11T00:54:13Z|http://arxiv.org/abs/1703.03882v1|http://arxiv.org/pdf/1703.03882v1|Generalized Full Matching|Matching methods are used to make units comparable on observed characteristics. Full matching can be used to derive optimal matches. However, the method has only been defined in the case of two treatment categories, it places unnecessary restrictions on the matched groups, and existing implementations are computationally intractable in large samples. As a result, the method has not been feasible in studies with large samples or complex designs. We introduce a generalization of full matching that inherits its optimality properties but allows the investigator to specify any desired structure of the matched groups over any number of treatment conditions. We also describe a new approximation algorithm to derive generalized full matchings. In the worst case, the maximum within-group dissimilarity produced by the algorithm is no worse than four times the optimal solution, but it typically performs close to on par with existing optimal algorithms when they exist. Despite its performance, the algorithm is fast and uses little memory: it terminates, on average, in linearithmic time using linear space. This enables investigators to derive well-performing matchings within minutes even in complex studies with samples of several million units.|['Fredrik Sävje', 'Michael J. Higgins', 'Jasjeet S. Sekhon']|['stat.ME']
2017-03-16T23:27:06Z|2017-03-09T15:56:58Z|http://arxiv.org/abs/1703.03312v1|http://arxiv.org/pdf/1703.03312v1|Split Sample Empirical Likelihood|We propose a new approach that combines multiple non-parametric likelihood-type components to build a data-driven approximation of the true likelihood function. Our approach is built on empirical likelihood, a non-parametric approximation of the likelihood function. We show the asymptotic behaviors of our approach are identical to those seen in empirical likelihood. We demonstrate that our method performs comparably to empirical likelihood while significantly decreasing computational time.|['Adam Jaeger', 'Nicole Lazar']|['stat.ME']
2017-03-16T23:27:06Z|2017-03-09T10:19:53Z|http://arxiv.org/abs/1703.03213v1|http://arxiv.org/pdf/1703.03213v1|Kernel intensity estimation, bootstrapping and bandwidth selection for   inhomogeneous point processes depending on spatial covariates|In the point process context, kernel intensity estimation has been mainly restricted to exploratory analysis due to its lack of consistency. However the use of covariates has allow to design consistent alternatives under some restrictive assumptions. In this paper we focus our attention on de\-fi\-ning an appropriate framework to derive a consistent kernel intensity estimator using covariates, as well as a consistent smooth bootstrap procedure. For spatial point processes with covariates there is no specific bandwidth selector, hence, we define two new data-driven procedures specifically designed for this scenario: a rule-of-thumb and a plug-in bandwidth based on the bootstrap method previously introduced. A simulation study is accomplished to understand the behaviour of these procedures in finite samples. Finally, we apply the techniques to a real set of data made up of wildfires in Canada during June 2015, using meteorological information as covariates.|['M. I. Borrajo', 'W. González-Manteiga', 'M. D. Martínez-Miranda']|['stat.ME', 'stat.AP', '62G05, 62G09, 62H11, 60G55, 60-08']
2017-03-16T23:27:06Z|2017-03-09T07:27:33Z|http://arxiv.org/abs/1703.03165v1|http://arxiv.org/pdf/1703.03165v1|Perturbation Bootstrap in Adaptive Lasso|The Adaptive LASSO (ALASSO) was proposed by Zou [J. Amer. Statist. Assoc. 101 (2006) 1418-1429] as a modification of the LASSO for the purpose of simultaneous variable selection and estimation of the parameters in a linear regression model. Zou (2006) established that the ALASSO estimator is variable-selection consistent as well as asymptotically Normal in the indices corresponding to the nonzero regression coefficients in certain fixed-dimensional settings. In an influential paper, Minnier, Tian and Cai [J. Amer. Statist. Assoc. 106 (2011) 1371-1382] proposed a perturbation bootstrap method and established its distributional consistency for the ALASSO estimator in the fixed-dimensional setting. In this paper, however, we show that this (naive) perturbation bootstrap fails to achieve second order correctness in approximating the distribution of the ALASSO estimator. We propose a modification to the perturbation bootstrap objective function and show that a suitably studentized version of our modified perturbation bootstrap ALASSO estimator achieves second-order correctness even when the dimension of the model is allowed to grow to infinity with the sample size. As a consequence, inferences based on the modified perturbation bootstrap will be more accurate than the inferences based on the oracle Normal approximation. We give simulation studies demonstrating good finite-sample properties of our modified perturbation bootstrap method as well as an illustration of our method on a real data set.|['Debraj Das', 'Karl Gregory', 'S. N. Lahiri']|['stat.ME', 'math.ST', 'stat.TH']
2017-03-16T23:27:06Z|2017-03-09T03:49:33Z|http://arxiv.org/abs/1703.03123v1|http://arxiv.org/pdf/1703.03123v1|Calibrated Data Augmentation for Scalable Markov Chain Monte Carlo|Data augmentation is a common technique for building tuning-free Markov chain Monte Carlo algorithms. Although these algorithms are very popular, autocorrelations are often high in large samples, leading to poor computational efficiency. This phenomenon has been attributed to a discrepancy between Gibbs step sizes and the rate of posterior concentration. In this article, we propose a family of calibrated data augmentation algorithms, which adjust for this discrepancy by inflating Gibbs step sizes while adjusting for bias. A Metropolis-Hastings step is included to account for the slight discrepancy between the stationary distribution of the resulting sampler and the exact posterior distribution. The approach is applicable to a broad variety of existing data augmentation algorithms, and we focus on three popular models: probit, logistic and Poisson log-linear. Dramatic gains in computational efficiency are shown in applications.|['Leo L. Duan', 'James E. Johndrow', 'David B. Dunson']|['stat.ME']
2017-03-16T23:27:06Z|2017-03-09T01:19:39Z|http://arxiv.org/abs/1703.03095v1|http://arxiv.org/pdf/1703.03095v1|Fitting the Linear Preferential Attachment Model|Preferential attachment is an appealing mechanism for modeling power-law behavior of the degree distributions in directed social networks. In this paper, we consider methods for fitting a 5-parameter linear preferential model to network data under two data scenarios. In the case where full history of the network formation is given, we derive the maximum likelihood estimator of the parameters and show that it is strongly consistent and asymptotically normal. In the case where only a single-time snapshot of the network is available, we propose an estimation method which combines method of moments with an approximation to the likelihood. The resulting estimator is also strongly consistent and performs quite well compared to the MLE estimator. We illustrate both estimation procedures through simulated data, and explore the usage of this model in a real data example. At the end of the paper, we also present a semi-parametric method to model heavy-tailed features of the degree distributions of the network using ideas from extreme value theory.|['Phyllis Wan', 'Tiandong Wang', 'Richard A. Davis', 'Sidney I. Resnick']|['stat.ME', '05C80, 90B15, 62F12']
2017-03-16T23:27:10Z|2017-03-08T21:40:57Z|http://arxiv.org/abs/1703.03043v1|http://arxiv.org/pdf/1703.03043v1|Bootstrap with Clustering in Two or More Dimensions|We propose a bootstrap procedure for data that may exhibit clustering in two or more dimensions. We use insights from the theory of generalized U-statistics to analyze the large-sample properties of statistics that are sample averages from the observations pooled across clusters. The asymptotic distribution of these statistics may be non-standard if there is no clustering in means. We show that the proposed bootstrap procedure is (a) point-wise consistent for any fixed data-generating process (DGP), (b) uniformly consistent if we exclude the case of clustering without clustering in means, and (c) provides refinements for any DGP such that the limiting distribution is Gaussian.|['Konrad Menzel']|['stat.ME', 'stat.OT']
2017-03-16T23:27:10Z|2017-03-08T20:32:20Z|http://arxiv.org/abs/1703.03023v1|http://arxiv.org/pdf/1703.03023v1|Elicitation, measuring bias, checking for prior-data conflict and   inference with a Dirichlet prior|Methods are developed for eliciting a Dirichlet prior based upon bounds on the individual probabilities that hold with virtual certainty. This approach to selecting a prior is applied to a contingency table problem where it is demonstrated how to assess the bias in the prior as well as how to check for prior-data conflict. It is shown that the assessment of a hypothesis via relative belief can easily take into account what it means for the falsity of the hypothesis to correspond to a difference of practical importance and provide evidence in favor of a hypothesis.|['Michael Evans', 'Irwin Guttman', 'Peiying Li']|['stat.ME', '62F15']
2017-03-16T23:27:10Z|2017-03-08T20:14:54Z|http://arxiv.org/abs/1703.03022v1|http://arxiv.org/pdf/1703.03022v1|A New Capture-Recapture Model in Dual-record System|Population size estimation based on two sample capture-recapture type experiment is an interesting problem in various fields including epidemiology, ecology, population studies, etc. Lincoln-Petersen estimate is popularly used under the assumption that capture and recapture status of each individual is independent. However, in many real life scenarios, there is some inherent dependency between capture and recapture attempts which is not well-studied in the literature for two sample capture-recapture method. In this article, we propose a novel model that successfully incorporates the possible causal dependency and provide corresponding estimation methodologies for the associated model parameters. Simulation results show superiority of the performance of the proposed method over existing competitors. The method is illustrated through the analysis of real data sets.|['Kiranmoy Chatterjee', 'Prajamitra Bhuyan']|['stat.ME', '62F10']
2017-03-16T23:27:10Z|2017-03-08T13:47:17Z|http://arxiv.org/abs/1703.02834v1|http://arxiv.org/pdf/1703.02834v1|Exact Dimensionality Selection for Bayesian PCA|We present a Bayesian model selection approach to estimate the intrinsic dimensionality of a high-dimensional dataset. To this end, we introduce a novel formulation of the probabilisitic principal component analysis model based on a normal-gamma prior distribution. In this context, we exhibit a closed-form expression of the marginal likelihood which allows to infer an optimal number of components. We also propose a heuristic based on the expected shape of the marginal likelihood curve in order to choose the hyperparameters. In non-asymptotic frameworks, we show on simulated data that this exact dimensionality selection approach is competitive with both Bayesian and frequentist state-of-the-art methods.|['Charles Bouveyron', 'Pierre Latouche', 'Pierre-Alexandre Mattei']|['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']
2017-03-16T23:27:10Z|2017-03-08T07:41:13Z|http://arxiv.org/abs/1703.02736v1|http://arxiv.org/pdf/1703.02736v1|Profile Estimation for Partial Functional Partially Linear Single-Index   Model|This paper studies a \textit{partial functional partially linear single-index model} that consists of a functional linear component as well as a linear single-index component. This model generalizes many well-known existing models and is suitable for more complicated data structures. However, its estimation inherits the difficulties and complexities from both components and makes it a challenging problem, which calls for new methodology. We propose a novel profile B-spline method to estimate the parameters by approximating the unknown nonparametric link function in the single-index component part with B-spline, while the linear slope function in the functional component part is estimated by the functional principal component basis. The consistency and asymptotic normality of the parametric estimators are derived, and the global convergence of the proposed estimator of the linear slope function is also established. More excitingly, the latter convergence is optimal in the minimax sense. A two-stage procedure is implemented to estimate the nonparametric link function, and the resulting estimator possesses the optimal global rate of convergence. Furthermore, the convergence rate of the mean squared prediction error for a predictor is also obtained. Empirical properties of the proposed procedures are studied through Monte Carlo simulations. A real data example is also analyzed to illustrate the power and flexibility of the proposed methodology.|['Qingguo Tang', 'Linglong Kong', 'David Ruppert', 'Rohana J. Karunamuni']|['math.ST', 'stat.ME', 'stat.TH']
2017-03-16T23:27:10Z|2017-03-08T06:22:56Z|http://arxiv.org/abs/1703.02724v1|http://arxiv.org/pdf/1703.02724v1|Guaranteed Tensor PCA with Optimality in Statistics and Computation|Tensors, or high-order arrays, attract much attention in recent research. In this paper, we propose a general framework for tensor principal component analysis (tensor PCA), which focuses on the methodology and theory for extracting the hidden low-rank structure from the high-dimensional tensor data. A unified solution is provided for tensor PCA with considerations in both statistical limits and computational costs. The problem exhibits three different phases according to the signal-noise-ratio (SNR). In particular, with strong SNR, we propose a fast spectral power iteration method that achieves the minimax optimal rate of convergence in estimation; with weak SNR, the information-theoretical lower bound shows that it is impossible to have consistent estimation in general; with moderate SNR, we show that the non-convex maximum likelihood estimation provides optimal solution, but with NP-hard computational cost; moreover, under the hardness hypothesis of hypergraphic planted clique detection, there are no polynomial-time algorithms performing consistently in general. Simulation studies show that the proposed spectral power iteration method have good performance under a variety of settings.|['Anru Zhang', 'Dong Xia']|['math.ST', 'cs.LG', 'stat.ME', 'stat.ML', 'stat.TH']
2017-03-16T23:27:10Z|2017-03-08T03:07:37Z|http://arxiv.org/abs/1703.02679v1|http://arxiv.org/pdf/1703.02679v1|Performance Bounds for Graphical Record Linkage|Record linkage involves merging records in large, noisy databases to remove duplicate entities. It has become an important area because of its widespread occurrence in bibliometrics, public health, official statistics production, political science, and beyond. Traditional linkage methods directly linking records to one another are computationally infeasible as the number of records grows. As a result, it is increasingly common for researchers to treat record linkage as a clustering task, in which each latent entity is associated with one or more noisy database records. We critically assess performance bounds using the Kullback-Leibler (KL) divergence under a Bayesian record linkage framework, making connections to Kolchin partition models. We provide an upper bound using the KL divergence and a lower bound on the minimum probability of misclassifying a latent entity. We give insights for when our bounds hold using simulated data and provide practical user guidance.|['Rebecca C. Steorts', 'Matt Barnes', 'Willie Neiswanger']|['math.ST', 'cs.IT', 'math.IT', 'stat.ME', 'stat.ML', 'stat.TH']
2017-03-16T23:27:10Z|2017-03-07T16:50:38Z|http://arxiv.org/abs/1703.02468v1|http://arxiv.org/pdf/1703.02468v1|Data-Driven Estimation Of Mutual Information Between Dependent Data|We consider the problem of estimating mutual information between dependent data, an important problem in many science and engineering applications. We propose a data-driven, non-parametric estimator of mutual information in this paper. The main novelty of our solution lies in transforming the data to frequency domain to make the problem tractable. We define a novel metric--mutual information in frequency--to detect and quantify the dependence between two random processes across frequency using Cram\'{e}r's spectral representation. Our solution calculates mutual information as a function of frequency to estimate the mutual information between the dependent data over time. We validate its performance on linear and nonlinear models. In addition, mutual information in frequency estimated as a part of our solution can also be used to infer cross-frequency coupling in the data.|['Rakesh Malladi', 'Don H Johnson', 'Behnaam Aazhang']|['cs.IT', 'math.IT', 'stat.ME']
2017-03-16T23:27:10Z|2017-03-07T16:38:11Z|http://arxiv.org/abs/1703.02462v1|http://arxiv.org/pdf/1703.02462v1|Convex and non-convex regularization methods for spatial point processes   intensity estimation|This paper deals with feature selection procedures for spatial point processes intensity estimation. We consider regularized versions of estimating equations based on Campbell theorem derived from two classical functions: Poisson likelihood and logistic regression likelihood. We provide general conditions on the spatial point processes and on penalty functions which ensure consistency, sparsity and asymptotic normality. We discuss the numerical implementation and assess finite sample properties in a simulation study. Finally, an application to tropical forestry datasets illustrates the use of the proposed methods.|['Achmad Choiruddin', 'Jean-François Coeurjolly', 'Frédérique Letué']|['stat.ME', 'math.ST', 'stat.TH']
2017-03-16T23:27:10Z|2017-03-07T15:13:08Z|http://arxiv.org/abs/1703.02428v1|http://arxiv.org/pdf/1703.02428v1|Robust Bayesian Filtering and Smoothing Using Student's t Distribution|State estimation in heavy-tailed process and measurement noise is an important challenge that must be addressed in, e.g., tracking scenarios with agile targets and outlier-corrupted measurements. The performance of the Kalman filter (KF) can deteriorate in such applications because of the close relation to the Gaussian distribution. Therefore, this paper describes the use of Student's t distribution to develop robust, scalable, and simple filtering and smoothing algorithms.   After a discussion of Student's t distribution, exact filtering in linear state-space models with t noise is analyzed. Intermediate approximation steps are used to arrive at filtering and smoothing algorithms that closely resemble the KF and the Rauch-Tung-Striebel (RTS) smoother except for a nonlinear measurement-dependent matrix update. The required approximations are discussed and an undesirable behavior of moment matching for t densities is revealed. A favorable approximation based on minimization of the Kullback-Leibler divergence is presented. Because of its relation to the KF, some properties and algorithmic extensions are inherited by the t filter. Instructive simulation examples demonstrate the performance and robustness of the novel algorithms.|['Michael Roth', 'Tohid Ardeshiri', 'Emre Özkan', 'Fredrik Gustafsson']|['stat.ME', 'cs.SY', 'stat.CO']
2017-03-16T23:27:14Z|2017-03-07T09:39:00Z|http://arxiv.org/abs/1703.02296v1|http://arxiv.org/pdf/1703.02296v1|Low-rank Interaction Contingency Tables|Log-linear models are popular tools to analyze contingency tables, particularly to model row and column effects as well as row-column interactions in two-way tables. In this paper, we introduce a regularized log-linear model designed for denoising and visualizing count data, which can incorporate side information such as row and column features. The estimation is performed through a convex optimization problem where we minimize a negative Poisson log-likelihood penalized by the nuclear norm of the interaction matrix. We derive an upper bound on the Frobenius estimation error, which improves previous rates for Poisson matrix recovery, and an algorithm based on the alternating direction method of multipliers to compute our estimator. To propose a complete methodology to users, we also address automatic selection of the regularization parameter. A Monte Carlo simulation reveals that our estimator is particularly well suited to estimate the rank of the interaction in low signal to noise ratio regimes. We illustrate with two data analyses that the results can be easily interpreted through biplot vizualization. The method is available as an R code.|['Geneviève Robin', 'Julie Josse', 'Eric Moulines', 'Sylvain Sardy']|['stat.ME']
2017-03-16T23:27:14Z|2017-03-07T06:40:44Z|http://arxiv.org/abs/1703.02237v1|http://arxiv.org/pdf/1703.02237v1|Scalable Collaborative Targeted Learning for High-Dimensional Data|Robust inference of a low-dimensional parameter in a large semi-parametric model relies on external estimators of infinite-dimensional features of the distribution of the data. Typically, only one of the latter is optimized for the sake of constructing a well behaved estimator of the low-dimensional parameter of interest. Optimizing more than one of them for the sake of achieving a better bias-variance trade-off in the estimation of the parameter of interest is the core idea driving the general template of the collaborative targeted minimum loss-based estimation (C-TMLE) procedure. The original implementation/instantiation of the C-TMLE template can be presented as a greedy forward stepwise C-TMLE algorithm. It does not scale well when the number $p$ of covariates increases drastically. This motivates the introduction of a novel instantiation of the C-TMLE template where the covariates are pre-ordered. Its time complexity is $\mathcal{O}(p)$ as opposed to the original $\mathcal{O}(p^2)$, a remarkable gain. We propose two pre-ordering strategies and suggest a rule of thumb to develop other meaningful strategies. Because it is usually unclear a priori which pre-ordering strategy to choose, we also introduce another implementation/instantiation called SL-C-TMLE algorithm that enables the data-driven choice of the better pre-ordering strategy given the problem at hand. Its time complexity is $\mathcal{O}(p)$ as well. The computational burden and relative performance of these algorithms were compared in simulation studies involving fully synthetic data or partially synthetic data based on a real world large electronic health database; and in analyses of three real, large electronic health databases. In all analyses involving electronic health databases, the greedy C-TMLE algorithm is unacceptably slow. Simulation studies indicate our scalable C-TMLE and SL-C-TMLE algorithms work well.|['Cheng Ju', 'Susan Gruber', 'Samuel D. Lendle', 'Antoine Chambaz', 'Jessica M. Franklin', 'Richard Wyss', 'Sebastian Schneeweiss', 'Mark J. van der Laan']|['stat.CO', 'stat.ME']
2017-03-16T23:27:14Z|2017-03-07T02:14:38Z|http://arxiv.org/abs/1703.02177v1|http://arxiv.org/pdf/1703.02177v1|Mixtures of Generalized Hyperbolic Distributions and Mixtures of Skew-t   Distributions for Model-Based Clustering with Incomplete Data|Robust clustering from incomplete data is an important topic because, in many practical situations, real data sets are heavy-tailed, asymmetric, and/or have arbitrary patterns of missing observations. Flexible methods and algorithms for model-based clustering are presented via mixture of the generalized hyperbolic distributions and its limiting case, the mixture of multivariate skew-t distributions. An analytically feasible EM algorithm is formulated for parameter estimation and imputation of missing values for mixture models employing missing at random mechanisms. The proposed methodologies are investigated through a simulation study with varying proportions of synthetic missing values and illustrated using a real dataset. Comparisons are made with those obtained from the traditional mixture of generalized hyperbolic distribution counterparts by filling in the missing data using the mean imputation method.|['Yuhong Wei', 'Paul D. McNicholas']|['stat.ME', 'stat.CO']
2017-03-16T23:27:14Z|2017-03-06T21:19:20Z|http://arxiv.org/abs/1703.02113v1|http://arxiv.org/pdf/1703.02113v1|A fresh look at effect aliasing and interactions: some new wine in old   bottles|"Interactions and effect aliasing are among the fundamental concepts in experimental design. In this paper, some new insights and approaches are provided on these subjects. In the literature, the ""de-aliasing"" of aliased effects is deemed to be impossible. We argue that this ""impossibility"" can indeed be resolved by employing a new approach which consists of reparametrization of effects and exploitation of effect non-orthogonality. This approach is successfully applied to three classes of designs: regular and nonregular two-level fractional factorial designs, and three-level fractional factorial designs. For reparametrization, the notion of conditional main effects (cme's) is employed for two-level regular designs, while the linear-quadratic system is used for three-level designs. For nonregular two-level designs, reparametrization is not needed because the partial aliasing of their effects already induces non-orthogonality. The approach can be extended to general observational data by using a new bi-level variable selection technique based on the cme's. A historical recollection is given on how these ideas were discovered."|['C. F. Jeff Wu']|['stat.ME']
2017-03-16T23:27:14Z|2017-03-06T21:17:42Z|http://arxiv.org/abs/1703.02112v1|http://arxiv.org/pdf/1703.02112v1|Process convolution approaches for modeling interacting trajectories|"Gaussian processes are a fundamental statistical tool used in a wide range of applications. In the spatio-temporal setting, several families of covariance functions exist to accommodate a wide variety of dependence structures arising in different applications. These parametric families can be restrictive and are insufficient in some situations. In contrast, process convolutions represent a flexible, interpretable approach to defining the covariance of a Gaussian process and have modest requirements to ensure validity. We introduce a generalization of the process convolution approach that employs multiple convolutions sequentially to form a ""process convolution chain."" In our proposed multi-stage framework, complex dependencies that arise from a combination of different interacting mechanisms are decomposed into a series of interpretable kernel smoothers. We demonstrate an application of process convolution chains to model killer whale movement, in which the paths taken by multiple individuals are not independent, but reflect dynamic social interactions within the population. Our proposed model for dependent movement provides inference for the latent dynamic social structure in the study population. Additionally, by leveraging the positive dependence among individual paths, we achieve a reduction in uncertainty for the estimated locations of the whales, compared to a model that treats paths as independent."|['Henry R. Scharf', 'Mevin B. Hooten', 'Devin S. Johnson', 'John W. Durban']|['stat.ME', 'stat.AP']
2017-03-16T23:27:14Z|2017-03-06T19:33:24Z|http://arxiv.org/abs/1703.02078v1|http://arxiv.org/pdf/1703.02078v1|Cross-screening in observational studies that test many hypotheses|"We discuss observational studies that test many causal hypotheses, either hypotheses about many outcomes or many treatments. To be credible an observational study that tests many causal hypotheses must demonstrate that its conclusions are neither artifacts of multiple testing nor of small biases from nonrandom treatment assignment. In a sense that needs to be defined carefully, hidden within a sensitivity analysis for nonrandom assignment is an enormous correction for multiple testing: in the absence of bias, it is extremely improbable that multiple testing alone would create an association insensitive to moderate biases. We propose a new strategy called ""cross-screening"", different from but motivated by recent work of Bogomolov and Heller on replicability. Cross-screening splits the data in half at random, uses the first half to plan a study carried out on the second half, then uses the second half to plan a study carried out on the first half, and reports the more favorable conclusions of the two studies correcting using the Bonferroni inequality for having done two studies. If the two studies happen to concur, then they achieve Bogomolov-Heller replicability; however, importantly, replicability is not required for strong control of the family-wise error rate, and either study alone suffices for firm conclusions. In randomized studies with a few hypotheses, cross-split screening is not an attractive method when compared with conventional methods of multiplicity control, but it can become attractive when hundreds or thousands of hypotheses are subjected to sensitivity analyses in an observational study. We illustrate the technique by comparing 46 biomarkers in individuals who consume large quantities of fish versus little or no fish."|['Qingyuan Zhao', 'Dylan S. Small', 'Paul R. Rosenbaum']|['stat.ME', 'stat.AP']
2017-03-16T23:27:14Z|2017-03-06T13:41:33Z|http://arxiv.org/abs/1703.01866v1|http://arxiv.org/pdf/1703.01866v1|Weighted empirical likelihood for quantile regression with nonignorable   missing covariates|In this paper, we propose an empirical likelihood-based weighted (ELW) estimator of regression parameter in quantile regression model with nonignorable missing covariates. The proposed ELW estimator is computationally simple and more efficient than the CCA estimator. Simulation results show that the ELW method works remarkably well in finite samples. A real data example is used to illustrate the proposed ELW method.|['Xiaohui Yuan', 'He Si']|['stat.ME']
2017-03-16T23:27:14Z|2017-03-06T10:32:54Z|http://arxiv.org/abs/1703.01805v1|http://arxiv.org/pdf/1703.01805v1|Bayesian Estimation of Kendall's tau Using a Latent Normal Approach|The rank-based association between two variables can be modeled by introducing a latent normal level to ordinal data. We demonstrate how this approach yields Bayesian inference for Kendall's rank correlation coefficient, improving on a recent Bayesian solution from asymptotic properties of the test statistic.|['Johnny van Doorn', 'Alexander Ly', 'Maarten Marsman', 'Eric-Jan Wagenmakers']|['stat.ME']
2017-03-16T23:27:14Z|2017-03-06T09:24:07Z|http://arxiv.org/abs/1703.01776v1|http://arxiv.org/pdf/1703.01776v1|Online Sequential Monte Carlo smoother for partially observed stochastic   differential equations|This paper introduces a new algorithm to approximate smoothed additive functionals for partially observed stochastic differential equations. This method relies on a recent procedure which allows to compute such approximations online, i.e. as the observations are received, and with a computational complexity growing linearly with the number of Monte Carlo samples. This online smoother cannot be used directly in the case of partially observed stochastic differential equations since the transition density of the latent data is usually unknown. We prove that a similar algorithm may still be defined for partially observed continuous processes by replacing this unknown quantity by an unbiased estimator obtained for instance using general Poisson estimators. We prove that this estimator is consistent and its performance are illustrated using data from two models.|['Pierre Gloaguen', 'Marie-Pierre Etienne', 'Sylvain Le Corff']|['stat.ME', 'stat.AP']
2017-03-16T23:27:14Z|2017-03-06T00:14:36Z|http://arxiv.org/abs/1703.01692v1|http://arxiv.org/pdf/1703.01692v1|Detecting Spatial Patterns of Disease in Large Collections of Electronic   Medical Records Using Neighbor-Based Bootstrapping (NB2)|We introduce a method called neighbor-based bootstrapping (NB2) that can be used to quantify the geospatial variation of a variable. We applied this method to an analysis of the incidence rates of disease from electronic medical record data (ICD-9 codes) for approximately 100 million individuals in the US over a period of 8 years. We considered the incidence rate of disease in each county and its geospatially contiguous neighbors and rank ordered diseases in terms of their degree of geospatial variation as quantified by the NB2 method.   We show that this method yields results in good agreement with established methods for detecting spatial autocorrelation (Moran's I method and kriging). Moreover, the NB2 method can be tuned to identify both large area and small area geospatial variations. This method also applies more generally in any parameter space that can be partitioned to consist of regions and their neighbors.|['Maria T Patterson', 'Robert L Grossman']|['stat.ME']
2017-03-16T23:27:21Z|2017-03-05T21:14:28Z|http://arxiv.org/abs/1703.01665v1|http://arxiv.org/pdf/1703.01665v1|Anisotropic functional Laplace deconvolution|In the present paper we consider the problem of estimating a three-dimensional function $f$ based on observations from its noisy Laplace convolution. Our study is motivated by the analysis of Dynamic Contrast Enhanced (DCE) imaging data. We construct an adaptive wavelet-Laguerre estimator of $f$, derive minimax lower bounds for the $L^2$-risk when $f$ belongs to a three-dimensional Laguerre-Sobolev ball and demonstrate that the wavelet-Laguerre estimator is adaptive and asymptotically near-optimal in a wide range of Laguerre-Sobolev spaces. We carry out a limited simulations study and show that the estimator performs well in a finite sample setting. Finally, we use the technique for the solution of the Laplace deconvolution problem on the basis of DCE Computerized Tomography data.|['Rida Benhaddou', 'Marianna Pensky', 'Rasika Rajapakshage']|['stat.ME', 'Primary 62G05, , secondary 62G08, 62P35']
2017-03-16T23:27:21Z|2017-03-04T20:27:02Z|http://arxiv.org/abs/1703.01518v1|http://arxiv.org/pdf/1703.01518v1|Model-Independent Analytic Nonlinear Blind Source Separation|Consider a time series of measurements of the state of an evolving system, x(t), where x has two or more components. This paper shows how to perform nonlinear blind source separation; i.e., how to determine if these signals are equal to linear or nonlinear mixtures of the state variables of two or more statistically independent subsystems. First, the local distributions of measurement velocities are processed in order to derive vectors at each point in x-space. If the data are separable, each of these vectors must be directed along a subspace of x-space that is traversed by varying the state variable of one subsystem, while all other subsystems are kept constant. Because of this property, these vectors can be used to construct a small set of mappings, which must contain the unmixing function, if it exists. Therefore, nonlinear blind source separation can be performed by examining the separability of the data after it has been transformed by each of these mappings. The method is analytic, constructive, and model-independent. It is illustrated by blindly recovering the separate utterances of two speakers from nonlinear combinations of their audio waveforms.|['David N. Levin']|['stat.ME']
2017-03-16T23:27:21Z|2017-03-04T09:12:42Z|http://arxiv.org/abs/1703.01421v1|http://arxiv.org/pdf/1703.01421v1|$l_0$-estimation of piecewise-constant signals on graphs|We study recovery of piecewise-constant signals over arbitrary graphs by the estimator minimizing an $l_0$-edge-penalized objective. Although exact minimization of this objective may be computationally intractable, we show that the same statistical risk guarantees are achieved by the alpha-expansion algorithm which approximately minimizes this objective in polynomial time. We establish that for graphs with small average vertex degree, these guarantees are rate-optimal in a minimax sense over classes of edge-sparse signals. For application to spatially inhomogeneous graphs, we propose minimization of an edge-weighted variant of this objective where each edge is weighted by its effective resistance or another measure of its contribution to the graph's connectivity. We establish minimax optimality of the resulting estimators over corresponding edge-weighted sparsity classes. We show theoretically that these risk guarantees are not always achieved by the estimator minimizing the $l_1$/total-variation relaxation, and empirically that the $l_0$-based estimates are more accurate in high signal-to-noise settings.|['Zhou Fan', 'Leying Guan']|['stat.ME', 'math.ST', 'stat.CO', 'stat.TH']
2017-03-16T23:27:21Z|2017-03-04T00:16:54Z|http://arxiv.org/abs/1703.01364v1|http://arxiv.org/pdf/1703.01364v1|A Matrix Variate Skew-t Distribution|Although there is ample work in the literature dealing with skewness in the multivariate setting, there is a relative paucity of work in the matrix variate paradigm. Such work is, for example, useful for modelling three-way data. A matrix variate skew-t distribution is derived based on a mean-variance matrix normal mixture. An expectation-conditional maximization algorithm is developed for parameter estimation. Simulated data are used for illustration.|['Michael P. B. Gallaugher', 'Paul D. McNicholas']|['stat.ME', 'math.ST', 'stat.TH']
2017-03-16T23:27:21Z|2017-03-03T16:29:21Z|http://arxiv.org/abs/1703.01234v1|http://arxiv.org/pdf/1703.01234v1|A Bayesian computer model analysis of Robust Bayesian analyses|We harness the power of Bayesian emulation techniques, designed to aid the analysis of complex computer models, to examine the structure of complex Bayesian analyses themselves. These techniques facilitate robust Bayesian analyses and/or sensitivity analyses of complex problems, and hence allow global exploration of the impacts of choices made in both the likelihood and prior specification. We show how previously intractable problems in robustness studies can be overcome using emulation techniques, and how these methods allow other scientists to quickly extract approximations to posterior results corresponding to their own particular subjective specification. The utility and flexibility of our method is demonstrated on a reanalysis of a real application where Bayesian methods were employed to capture beliefs about river flow. We discuss the obvious extensions and directions of future research that such an approach opens up.|['Ian Vernon', 'John Paul Gosling']|['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']
2017-03-16T23:27:21Z|2017-03-03T10:30:11Z|http://arxiv.org/abs/1703.01102v1|http://arxiv.org/pdf/1703.01102v1|A New Test of Multivariate Nonlinear Causality|The multivariate nonlinear Granger causality developed by Bai et al. (2010) plays an important role in detecting the dynamic interrelationships between two groups of variables. Following the idea of Hiemstra-Jones (HJ) test proposed by Hiemstra and Jones (1994), they attempt to establish a central limit theorem (CLT) of their test statistic by applying the asymptotical property of multivariate $U$-statistic. However, Bai et al. (2016) revisit the HJ test and find that the test statistic given by HJ is NOT a function of $U$-statistics which implies that the CLT neither proposed by Hiemstra and Jones (1994) nor the one extended by Bai et al. (2010) is valid for statistical inference. In this paper, we re-estimate the probabilities and reestablish the CLT of the new test statistic. Numerical simulation shows that our new estimates are consistent and our new test performs decent size and power.|['Zhidong Bai', 'Yongchang Hui', 'Zhihui Lv', 'Wing-Keung Wong', 'Shurong Zheng', 'Zhenzhen Zhu']|['stat.ME']
2017-03-16T23:27:21Z|2017-03-02T22:17:52Z|http://arxiv.org/abs/1703.00968v1|http://arxiv.org/pdf/1703.00968v1|Bayesian inference for generalized extreme value distribution with   Gaussian copula dependence|Dependent generalized extreme value (dGEV) models have attracted much attention due to the dependency structure that often appears in real datasets. To construct a dGEV model, a natural approach is to assume that some parameters in the model are time-varying. A previous study has shown that a dependent Gumbel process can be naturally incorporated into a GEV model. The model is a nonlinear state space model with a hidden state that follows a Markov process, with its innovation following a Gumbel distribution. Inference may be made for the model using Bayesian methods, sampling the hidden process from a mixture normal distribution, used to approximate the Gumbel distribution. Thus the response follows an approximate GEV model. We propose a new model in which each marginal distribution is an exact GEV distribution. We use a variable transformation to combine the marginal CDF of a Gumbel distribution with the standard normal copula. Then our model is a nonlinear state space model in which the hidden state equation is Gaussian. We analyze this model using Bayesian methods, and sample the elements of the state vector using particle Gibbs with ancestor sampling (PGAS). The PGAS algorithm turns out to be very efficient in solving nonlinear state space models. We also show our model is flexible enough to incorporate seasonality.|['Bo Ning', 'Peter Bloomfield']|['stat.ME']
2017-03-16T23:27:21Z|2017-03-02T18:20:18Z|http://arxiv.org/abs/1703.00884v1|http://arxiv.org/pdf/1703.00884v1|A Dichotomy for Sampling Barrier-Crossing Events of Random Walks with   Regularly Varying Tails|We study how to sample paths of a random walk up to the first time it crosses a fixed barrier, in the setting where the step sizes are iid with negative mean and have a regularly varying right tail. We introduce a desirable property for a change of measure to be suitable for exact simulation. We study whether the change of measure of Blanchet and Glynn (2008) satisfies this property and show that it does so if and only if the tail index $\alpha$ of the right tail lies in the interval $(1, \, 3/2)$.|['Ton Dieker', 'Guido Lagos']|['math.PR', 'stat.ME', '68U20, 60G50, 68W40']
2017-03-16T23:27:21Z|2017-03-02T11:48:24Z|http://arxiv.org/abs/1703.00734v1|http://arxiv.org/pdf/1703.00734v1|Distributed Bayesian Matrix Factorization with Minimal Communication|Bayesian matrix factorization (BMF) is a powerful tool for producing low-rank representations of matrices, and giving principled predictions of missing values. However, scaling up MCMC samplers to large matrices has proven to be difficult with parallel algorithms that require communication between MCMC iterations. On the other hand, designing communication-free algorithms is challenging due to the inherent unidentifiability of BMF solutions. We propose posterior propagation, an embarrassingly parallel inference procedure, which hierarchically introduces dependencies between data subsets and thus alleviates the unidentifiability problem.|['Xiangju Qin', 'Paul Blomstedt', 'Eemeli Leppäaho', 'Pekka Parviainen', 'Samuel Kaski']|['stat.ML', 'cs.DC', 'cs.LG', 'cs.NA', 'stat.ME']
2017-03-16T23:27:21Z|2017-03-02T11:02:22Z|http://arxiv.org/abs/1703.02089v1|http://arxiv.org/pdf/1703.02089v1|The variational Laplace approach to approximate Bayesian inference|Variational approaches to approximate Bayesian inference provide very efficient means of performing parameter estimation and model selection. Among these, so-called variational-Laplace or VL schemes rely on Gaussian approximations to posterior densities on model parameters. In this note, we review the main variants of VL approaches, that follow from considering nonlinear models of continuous and/or categorical data. En passant, we also derive a few novel theoretical results that complete the portfolio of existing analyses of variational Bayesian approaches, including investigations of their asymptotic convergence. We also suggest practical ways of extending existing VL approaches to hierarchical generative models that include (e.g., precision) hyperparameters.|['Jean Daunizeau']|['stat.ME', 'q-bio.NC', 'stat.ML']
2017-03-16T23:27:25Z|2017-03-02T07:57:20Z|http://arxiv.org/abs/1703.00654v1|http://arxiv.org/pdf/1703.00654v1|Nonparametric estimation of galaxy cluster's emissivity and point source   detection in astrophysics with two lasso penalties|Astrophysicists are interested in recovering the 3D gas emissivity of a galaxy cluster from a 2D image taken by a telescope. A blurring phenomenon and presence of point sources make this inverse problem even harder to solve. The current state-of-the-art technique is two step: first identify the location of potential point sources, then mask these locations and deproject the data.   We instead model the data as a Poisson generalized linear model (involving blurring, Abel and wavelets operators) regularized by two lasso penalties to induce sparse wavelet representation and sparse point sources. The amount of sparsity is controlled by two quantile universal thresholds. As a result, our method outperforms the existing one.|['Jairo Diaz Rodriguez', 'Dominique Eckert', 'Hatef Monajemi', 'Stéphane Paltani', 'Sylvain Sardy']|['stat.AP', 'stat.ME']
2017-03-16T23:27:25Z|2017-03-02T03:45:27Z|http://arxiv.org/abs/1703.00604v1|http://arxiv.org/pdf/1703.00604v1|Marcinkiewicz's strong law of large numbers for non-additive expectation|The sub-linear expectation space is a nonlinear expectation space having advantages of modelling the uncertainty of probability and distribution. In the sub-linear expectation space, we use capacity and sub-linear expectation to replace probability and expectation of classical probability theory. In this paper, the method of selecting subsequence is used to prove Marcinkiewicz type strong law of large numbers under sub-linear expectation space. This result is a natural extension of the classical Marcinkiewicz's strong law of large numbers to the case where the expectation is nonadditive. In addition, this paper also gives a theorem about convergence of a random series.|['Lixin Zhang', 'Jinghang Lin']|['stat.ME']
2017-03-16T23:27:25Z|2017-03-01T12:03:54Z|http://arxiv.org/abs/1703.00253v1|http://arxiv.org/pdf/1703.00253v1|Improving phase II oncology trials using best observed RECIST response   as an endpoint by modelling continuous tumour measurements|In many phase II trials in solid tumours, patients are assessed using endpoints based on the Response Evaluation Criteria in Solid Tumours (RECIST) scale. Often, analyses are based on the response rate. This is the proportion of patients who have an observed tumour shrinkage above a pre-defined level and no new tumour lesions. The augmented binary method has been proposed to improve the precision of the estimator of the response rate. The method involves modelling the tumour shrinkage to avoid dichotomising it. However, in many trials the best observed response is used as the primary outcome. In such trials, patients are followed until progression, and their best observed RECIST outcome is used as the primary endpoint. In this paper, we propose a method that extends the augmented binary method so that it can be used when the outcome is best observed response. We show through simulated data and data from a real phase II cancer trial that this method improves power in both single-arm and randomised trials. The average gain in power compared to the traditional analysis is equivalent to approximately a 35% increase in sample size. A modified version of the method is proposed to reduce the computational effort required. We show this modified method maintains much of the efficiency advantages.|['Chien-Ju Lin', 'James Wason']|['stat.ME']
2017-03-16T23:27:25Z|2017-03-06T15:19:31Z|http://arxiv.org/abs/1702.08900v2|http://arxiv.org/pdf/1702.08900v2|Asymptotic Exponentiality of the First Exit Time of the Shiryaev-Roberts   Diffusion with Constant Positive Drift|We consider the first exit time of a Shiryaev-Roberts diffusion with constant positive drift from the interval $[0,A]$ where $A>0$. We show that the moment generating function (Laplace transform) of a suitably standardized version of the first exit time converges to that of the unit-mean exponential distribution as $A\to+\infty$. The proof is explicit in that the moment generating function of the first exit time is first expressed analytically and in a closed form, and then the desired limit as $A\to+\infty$ is evaluated directly. The result is of importance in the area of quickest change-point detection, and its discrete-time counterpart has been previously established - although in a different manner - by Pollak and Tartakovsky (2009).|['Aleksey S. Polunchenko']|['stat.ME', 'math.ST', 'stat.TH', '62L10, 60G40, 60J60']
2017-03-16T23:27:25Z|2017-02-28T18:34:39Z|http://arxiv.org/abs/1702.08897v1|http://arxiv.org/pdf/1702.08897v1|Semiparametric Estimation of Symmetric Mixture Models with Monotone and   Log-Concave Densities|In this article, we revisit the problem of fitting a mixture model under the assumption that the mixture components are symmetric and log-concave. To this end, we first study the nonparametric maximum likelihood estimation (NPMLE) of a monotone log-concave probability density. By following the arguments of Rufibach (2006), we show that the NPMLE is uniformly consistent with respect to the supremum norm on compact subsets of the interior of the support. To fit the mixture model, we propose a semiparametric EM (SEM) algorithm, which can be adapted to other semiparametric mixture models. In our numerical experiments, we compare our algorithm to that of Balabdaoui and Doss (2014) and other mixture models both on simulated and real-world datasets.|['Xiao Pu', 'Ery Arias-Castro']|['stat.ME']
2017-03-16T23:27:25Z|2017-03-13T17:28:04Z|http://arxiv.org/abs/1702.08896v2|http://arxiv.org/pdf/1702.08896v2|Deep and Hierarchical Implicit Models|Implicit probabilistic models are a flexible class for modeling data. They define a process to simulate observations, and unlike traditional models, they do not require a tractable likelihood function. In this paper, we develop two families of models: hierarchical implicit models and deep implicit models. They combine the idea of implicit densities with hierarchical Bayesian modeling and deep neural networks. The use of implicit models with Bayesian analysis has been limited by our ability to perform accurate and scalable inference. We develop likelihood-free variational inference (LFVI). Key to LFVI is specifying a variational family that is also implicit. This matches the model's flexibility and allows for accurate approximation of the posterior. Our work scales up implicit models to sizes previously not possible and advances their modeling design. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for text generation.|['Dustin Tran', 'Rajesh Ranganath', 'David M. Blei']|['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']
2017-03-16T23:27:25Z|2017-02-28T16:28:17Z|http://arxiv.org/abs/1702.08846v1|http://arxiv.org/pdf/1702.08846v1|Reduced Modeling of Unknown System Trajectories|"This paper deals with model order reduction of parametrical dynamical systems. We consider the specific setup where the distribution of the system's trajectories is unknown but the following two sources of information are available: \textit{(i)} some ""rough"" prior knowledge on the system's realisations; \textit{(ii)} a set of ""incomplete"" observations of the system's trajectories. We propose a Bayesian methodological framework to build reduced-order models (ROMs) by exploiting these two sources of information. We emphasise that complementing the prior knowledge with the collected data provably enhances the knowledge of the distribution of the system's trajectories. We then propose an implementation of the proposed methodology based on Monte-Carlo methods. In this context, we show that standard ROM learning techniques, such e.g. Proper Orthogonal Decomposition or Dynamic Mode Decomposition, can be revisited and recast within the probabilistic framework considered in this paper.~We illustrate the performance of the proposed approach by numerical results obtained for a standard geophysical model."|['Patrick Héas', 'Cédric Herzet']|['stat.ME']
2017-03-16T23:27:25Z|2017-02-28T14:22:03Z|http://arxiv.org/abs/1702.08797v1|http://arxiv.org/pdf/1702.08797v1|Fused Gaussian Process for Very Large Spatial Data|With the development of new remote sensing technology, large or even massive spatial datasets covering the globe becomes available. Statistical analysis of such data is challenging. This article proposes a semiparametric approach to model large or massive spatial datasets. In particular, a Gaussian process with additive components is proposed, with its covariance structure consisting of two components: one component is flexible without assuming a specific parametric covariance function but is able to achieve dimension reduction; the other is parametric and simultaneously induces sparsity. The inference algorithm for parameter estimation and spatial prediction is devised. The resulting spatial prediction method that we call fused Gaussian process (FGP), is applied to simulated data and a massive satellite dataset. The results demonstrate the computational and inferential benefits of the FGP over competing methods and show that the FGP is more flexible and robust against model misspecification.|['Pulong Ma', 'Emily L. Kang']|['stat.ME']
2017-03-16T23:27:25Z|2017-02-28T08:46:37Z|http://arxiv.org/abs/1702.08694v1|http://arxiv.org/pdf/1702.08694v1|Significant Pattern Mining on Continuous Variables|Significant pattern mining, the search for sets of binary features that are statistically significantly enriched in a class of objects, is of fundamental importance in a wide range of applications from economics to statistical genetics. Still, all existing approaches make the restrictive assumption that the features are binary and require a binarization of continuous data during preprocessing, which often leads to a loss of information. Here, we solve the open problem of significant pattern mining on continuous variables. Our approach detects all patterns that are statistically significantly associated with a class of interest, while rigorously correcting for multiple testing. Key to this approach is the use of Spearman's rank correlation coefficient to represent the frequency of a pattern. Our experiments demonstrate that our novel approach detects true patterns with higher precision and recall than competing methods that require a prior binarization of the data.|['Mahito Sugiyama', 'Karsten M. Borgwardt']|['stat.ML', 'cs.LG', 'stat.ME']
2017-03-16T23:27:25Z|2017-02-27T22:52:29Z|http://arxiv.org/abs/1702.08572v1|http://arxiv.org/pdf/1702.08572v1|Comparison of Confidence Interval Estimators: an Index Approach|We develop a confidence interval index for comparing confidence interval estimators based on the confidence interval length and coverage probability. We show that the confidence interval index has range of values within the neighborhood of the range of the coverage probability, [0,1]. In addition, a good confidence interval estimator is shown to have an index value approaching 1; and a bad confidence interval has an index value approaching 0. A simulation study is conducted to assess the finite sample performance of the index. Finally, the proposed index is illustrated with a practical example from the literature.|['Richard Minkah', 'Tertius de Wet']|['stat.ME', 'stat.CO', '62F99, 62G99']
2017-03-16T23:27:29Z|2017-02-27T19:59:41Z|http://arxiv.org/abs/1702.08496v1|http://arxiv.org/pdf/1702.08496v1|Bayesian nonparametric generative models for causal inference with   missing at random covariates|We propose a general Bayesian nonparametric (BNP) approach to causal inference in the point treatment setting. The joint distribution of the observed data (outcome, treatment, and confounders) is modeled using an enriched Dirichlet process. The combination of the observed data model and causal assumptions allows us to identify any type of causal effect - differences, ratios, or quantile effects, either marginally or for subpopulations of interest. The proposed BNP model is well-suited for causal inference problems, as it does not require parametric assumptions about the distribution of confounders and naturally leads to a computationally efficient Gibbs sampling algorithm. By flexibly modeling the joint distribution, we are also able to impute (via data augmentation) values for missing covariates within the algorithm under an assumption of ignorable missingness, obviating the need to create separate imputed data sets. This approach for imputing the missing covariates has the additional advantage of guaranteeing congeniality between the imputation model and the analysis model, and because we use a BNP approach, parametric models are avoided for imputation. The performance of the method is assessed using simulation studies. The method is applied to data from a cohort study of human immunodeficiency virus/hepatitis C virus co-infected patients.|['Jason Roy', 'Kirsten J Lum', 'Michael J. Daniels', 'Bret Zeldow', 'Jordan Dworkin', 'Vincent Lo Re III']|['stat.ME']
2017-03-16T23:27:29Z|2017-02-27T05:02:58Z|http://arxiv.org/abs/1702.08148v1|http://arxiv.org/pdf/1702.08148v1|A Copula-based Imputation Model for Missing Data of Mixed Type in   Multilevel Data Sets|We propose a copula based method to handle missing values in multivariate data of mixed types in multilevel data sets. Building upon the extended rank likelihood of \cite{hoff2007extending} and the multinomial probit model, our model is a latent variable model which is able to capture the relationship among variables of different types as well as accounting for the clustering structure. We fit the model by approximating the posterior distribution of the parameters and the missing values through a Gibbs sampling scheme. We use the multiple imputation procedure to incorporate the uncertainty due to missing values in the analysis of the data. Our proposed method is evaluated through simulations to compare it with several conventional methods of handling missing data. We also apply our method to a data set from a cluster randomized controlled trial of a multidisciplinary intervention in acute stroke units. We conclude that our proposed copula based imputation model for mixed type variables achieves reasonably good imputation accuracy and recovery of parameters in some models of interest, and that adding random effects enhances performance when the clustering effect is strong.|['Jiali Wang', 'Bronwyn Loong', 'Anton H. Westveld', 'Alan H. Welsh']|['stat.ME']
2017-03-16T23:27:29Z|2017-02-27T04:30:06Z|http://arxiv.org/abs/1702.08142v1|http://arxiv.org/pdf/1702.08142v1|Tensor Balancing on Statistical Manifold|"We solve tensor balancing, rescaling an Nth order nonnegative tensor by multiplying (N - 1)th order N tensors so that every fiber sums to one. This generalizes a fundamental process of matrix balancing used to compare matrices in a wide range of applications from biology to economics. We present an efficient balancing algorithm with quadratic convergence using Newton's method and show in numerical experiments that the proposed algorithm is several orders of magnitude faster than existing ones. To theoretically prove the correctness of the algorithm, we model tensors as probability distributions in a statistical manifold and realize tensor balancing as projection onto a submanifold. The key to our algorithm is that the gradient of the manifold, used as a Jacobian matrix in Newton's method, can be analytically obtained using the M\""obius inversion formula, the essential of combinatorial mathematics. Our model is not limited to tensor balancing but has a wide applicability as it includes various statistical and machine learning models such as weighted DAGs and Boltzmann machines."|['Mahito Sugiyama', 'Hiroyuki Nakahara', 'Koji Tsuda']|['stat.ME', 'cs.IT', 'cs.NA', 'math.IT', 'stat.ML']
2017-03-16T23:27:29Z|2017-02-26T21:23:33Z|http://arxiv.org/abs/1702.08088v1|http://arxiv.org/pdf/1702.08088v1|Selection of training populations (and other subset selection problems)   with an accelerated genetic algorithm (STPGA: An R-package for selection of   training populations with a genetic algorithm)|Optimal subset selection is an important task that has numerous algorithms designed for it and has many application areas. STPGA contains a special genetic algorithm supplemented with a tabu memory property (that keeps track of previously tried solutions and their fitness for a number of iterations), and with a regression of the fitness of the solutions on their coding that is used to form the ideal estimated solution (look ahead property) to search for solutions of generic optimal subset selection problems. I have initially developed the programs for the specific problem of selecting training populations for genomic prediction or association problems, therefore I give discussion of the theory behind optimal design of experiments to explain the default optimization criteria in STPGA, and illustrate the use of the programs in this endeavor. Nevertheless, I have picked a few other areas of application: supervised and unsupervised variable selection based on kernel alignment, supervised variable selection with design criteria, influential observation identification for regression, solving mixed integer quadratic optimization problems, balancing gains and inbreeding in a breeding population. Some of these illustrations pertain new statistical approaches.|['Deniz Akdemir']|['stat.ME', 'cs.LG', 'q-bio.GN', 'q-bio.QM', 'stat.AP']
2017-03-16T23:27:29Z|2017-02-28T09:59:57Z|http://arxiv.org/abs/1702.08061v2|http://arxiv.org/pdf/1702.08061v2|The Ensemble Kalman Filter: A Signal Processing Perspective|The ensemble Kalman filter (EnKF) is a Monte Carlo based implementation of the Kalman filter (KF) for extremely high-dimensional, possibly nonlinear and non-Gaussian state estimation problems. Its ability to handle state dimensions in the order of millions has made the EnKF a popular algorithm in different geoscientific disciplines. Despite a similarly vital need for scalable algorithms in signal processing, e.g., to make sense of the ever increasing amount of sensor data, the EnKF is hardly discussed in our field.   This self-contained review paper is aimed at signal processing researchers and provides all the knowledge to get started with the EnKF. The algorithm is derived in a KF framework, without the often encountered geoscientific terminology. Algorithmic challenges and required extensions of the EnKF are provided, as well as relations to sigma-point KF and particle filters. The relevant EnKF literature is summarized in an extensive survey and unique simulation examples, including popular benchmark problems, complement the theory with practical insights. The signal processing perspective highlights new directions of research and facilitates the exchange of potentially beneficial ideas, both for the EnKF and high-dimensional nonlinear and non-Gaussian filtering in general.|['Michael Roth', 'Gustaf Hendeby', 'Carsten Fritsche', 'Fredrik Gustafsson']|['stat.ME', 'cs.SY', 'stat.CO']
2017-03-16T23:27:29Z|2017-02-26T10:41:26Z|http://arxiv.org/abs/1703.01977v1|http://arxiv.org/pdf/1703.01977v1|Linear, Machine Learning and Probabilistic Approaches for Time Series   Analysis|In this paper we study different approaches for time series modeling. The forecasting approaches using linear models, ARIMA alpgorithm, XGBoost machine learning algorithm are described. Results of different model combinations are shown. For probabilistic modeling the approaches using copulas and Bayesian inference are considered.|['B. M. Pavlyshenko']|['stat.AP', 'cs.LG', 'stat.ME']
2017-03-16T23:27:29Z|2017-02-24T23:43:44Z|http://arxiv.org/abs/1702.07804v1|http://arxiv.org/pdf/1702.07804v1|A Constrained Conditional Likelihood Approach for Estimating the Means   of Selected Populations|Given p independent normal populations, we consider the problem of estimating the mean of those populations, that based on the observed data, give the strongest signals. We explicitly condition on the ranking of the sample means, and consider a constrained conditional maximum likelihood (CCMLE) approach, avoiding the use of any priors and of any sparsity requirement between the population means. Our results show that if the observed means are too close together, we should in fact use the grand mean to estimate the mean of the population with the larger sample mean. If they are separated by more than a certain threshold, we should shrink the observed means towards each other. As intuition suggests, it is only if the observed means are far apart that we should conclude that the magnitude of separation and consequent ranking are not due to chance. Unlike other methods, our approach does not need to pre-specify the number of selected populations and the proposed CCMLE is able to perform simultaneous inference. Our method, which is conceptually straightforward, can be easily adapted to incorporate other selection criteria.   Selected populations, Maximum likelihood, Constrained MLE, Post-selection inference|['Claudio Fuentes', 'Vik Gopal']|['stat.ME']
2017-03-16T23:27:29Z|2017-02-24T21:44:47Z|http://arxiv.org/abs/1702.07778v1|http://arxiv.org/pdf/1702.07778v1|A Note on Nonlocal Prior Method|We propose a new class of nonlocal prior to improve the performance of variable selection in high dimensional setting. We prove our new prior possesses the robustness to hyper parameter settings and is able to detect smaller decreasing signals.|['Yuanyuan Bian', 'Ho-Hsiang Wu']|['stat.ME']
2017-03-16T23:27:29Z|2017-02-24T21:03:27Z|http://arxiv.org/abs/1702.07763v1|http://arxiv.org/pdf/1702.07763v1|Survival Trees for Interval-Censored Survival data|Interval-censored data, in which the event time is only known to lie in some time interval, arise commonly in practice; for example, in a medical study in which patients visit clinics or hospitals at pre-scheduled times, and the events of interest occur between visits. Such data are appropriately analyzed using methods that account for this uncertainty in event time measurement. In this paper we propose a survival tree method for interval-censored data based on the conditional inference framework. Using Monte Carlo simulations we find that the tree is effective in uncovering underlying tree structure, performs similarly to an interval-censored Cox proportional hazards model fit when the true relationship is linear, and performs at least as well as (and in the presence of right-censoring outperforms) the Cox model when the true relationship is not linear. Further, the interval-censored tree outperforms survival trees based on imputing the event time as an endpoint or the midpoint of the censoring interval. We illustrate the application of the method on tooth emergence data.|['Wei Fu', 'Jeffrey S. Simonoff']|['stat.ME']
2017-03-16T23:27:29Z|2017-02-24T17:01:59Z|http://arxiv.org/abs/1702.07662v1|http://arxiv.org/pdf/1702.07662v1|A Network Epidemic Model for Online Community Commissioning Data|Statistical models for network epidemics usually assume a Bernoulli random graph, in which any two nodes have the same probability of being connected. This assumption provides computational simplicity but does not describe real-life networks well. We propose an epidemic model based on the preferential attachment model, which adds nodes sequentially by simple rules to generate a network. A simulation study based on the subsequent Markov Chain Monte Carlo algorithm reveals an identifiability issue with the model parameters, so an alternative parameterisation is suggested. Finally, the model is applied to a set of online commissioning data.|['Clement Lee', 'Andrew Garbett', 'Darren J. Wilkinson']|['stat.CO', 'cs.SI', 'stat.ME']
2017-03-16T23:27:33Z|2017-02-24T15:43:10Z|http://arxiv.org/abs/1702.07630v1|http://arxiv.org/pdf/1702.07630v1|Inertia-Constrained Pixel-by-Pixel Nonnegative Matrix Factorisation: a   Hyperspectral Unmixing Method Dealing with Intra-class Variability|Blind source separation is a common processing tool to analyse the constitution of pixels of hyperspectral images. Such methods usually suppose that pure pixel spectra (endmembers) are the same in all the image for each class of materials. In the framework of remote sensing, such an assumption is no more valid in the presence of intra-class variabilities due to illumination conditions, weathering, slight variations of the pure materials, etc... In this paper, we first describe the results of investigations highlighting intra-class variability measured in real images. Considering these results, a new formulation of the linear mixing model is presented leading to two new methods. Unconstrained Pixel-by-pixel NMF (UP-NMF) is a new blind source separation method based on the assumption of a linear mixing model, which can deal with intra-class variability. To overcome UP-NMF limitations an extended method is proposed, named Inertia-constrained Pixel-by-pixel NMF (IP-NMF). For each sensed spectrum, these extended versions of NMF extract a corresponding set of source spectra. A constraint is set to limit the spreading of each source's estimates in IP-NMF. The methods are tested on a semi-synthetic data set built with spectra extracted from a real hyperspectral image and then numerically mixed. We thus demonstrate the interest of our methods for realistic source variabilities. Finally, IP-NMF is tested on a real data set and it is shown to yield better performance than state of the art methods.|['Charlotte Revel', 'Yannick Deville', 'Véronique Achard', 'Xavier Briottet']|['stat.ME', 'cs.CV', 'physics.data-an', 'stat.ML']
2017-03-16T23:27:33Z|2017-02-24T02:28:26Z|http://arxiv.org/abs/1702.07449v1|http://arxiv.org/pdf/1702.07449v1|Characterizing Spatiotemporal Transcriptome of Human Brain via Low Rank   Tensor Decomposition|Spatiotemporal gene expression data of the human brain offer insights on the spa- tial and temporal patterns of gene regulation during brain development. Most existing methods for analyzing these data consider spatial and temporal profiles separately with the implicit assumption that different brain regions develop in similar trajectories, and that the spatial patterns of gene expression remain similar at different time points. Al- though these analyses may help delineate gene regulation either spatially or temporally, they are not able to characterize heterogeneity in temporal dynamics across different brain regions, or the evolution of spatial patterns of gene regulation over time. In this article, we develop a statistical method based on low rank tensor decomposition to more effectively analyze spatiotemporal gene expression data. We generalize the clas- sical principal component analysis (PCA) which is applicable only to data matrices, to tensor PCA that can simultaneously capture spatial and temporal effects. We also propose an efficient algorithm that combines tensor unfolding and power iteration to estimate the tensor principal components, and provide guarantees on their statistical performances. Numerical experiments are presented to further demonstrate the mer- its of the proposed method. An application of our method to a spatiotemporal brain expression data provides insights on gene regulation patterns in the brain.|['Tianqi Liu', 'Ming Yuan', 'Hongyu Zhao']|['stat.ME']
2017-03-16T23:27:33Z|2017-02-23T17:44:10Z|http://arxiv.org/abs/1702.07304v1|http://arxiv.org/pdf/1702.07304v1|Conflict diagnostics for evidence synthesis in a multiple testing   framework|Evidence synthesis models that combine multiple datasets of varying design, to estimate quantities that cannot be directly observed, require the formulation of complex probabilistic models that can be expressed as graphical models. An assessment of whether the different datasets synthesised contribute information that is consistent with each other (and in a Bayesian context, with the prior distribution) is a crucial component of the model criticism process. However, a systematic assessment of conflict in evidence syntheses suffers from the multiple testing problem, through testing for conflict at multiple locations in a model. We demonstrate how conflict diagnostics can be employed throughout a graphical model while accounting for the multiple hypothesis tests of no conflict at each location in the graph. The method is illustrated by a network meta-analysis to estimate treatment effects in smoking cessation programs and an evidence synthesis to estimate HIV prevalence in Poland.|['Anne M. Presanis', 'David Ohlssen', 'Kai Cui', 'Magdalena Rosinska', 'Daniela De Angelis']|['stat.ME']
2017-03-16T23:27:33Z|2017-02-23T16:43:02Z|http://arxiv.org/abs/1702.07283v1|http://arxiv.org/pdf/1702.07283v1|Non-penalized variable selection in high-dimensional linear model   settings via generalized fiducial inference|Standard penalized methods of variable selection and parameter estimation rely on the magnitude of coefficient estimates to decide which variables to include in the final model. However, coefficient estimates are unreliable when the design matrix is collinear. To overcome this challenge an entirely new method of variable selection is presented within a generalized fiducial inference framework. This new procedure is able to effectively account for linear dependencies among subsets of covariates in a high-dimensional setting where $p$ can grow almost exponentially in $n$, as well as in the classical setting where $p \le n$.   It is shown that the procedure very naturally assigns small probabilities to subsets of covariates which include redundancies by way of explicit $L_{0}$ minimization. Furthermore, with a typical sparsity assumption, it is shown that the proposed method is consistent in the sense that the probability of the true sparse subset of covariates converges in probability to 1 as $n \to \infty$, or as $n \to \infty$ and $p \to \infty$. Very reasonable conditions are needed, and little restriction is placed on the class of $2^{p}$ possible subsets of covariates to achieve this consistency result.|['Jonathan P Williams', 'Jan Hannig']|['stat.ME']
2017-03-16T23:27:33Z|2017-02-23T15:58:00Z|http://arxiv.org/abs/1702.07269v1|http://arxiv.org/abs/1702.07269v1|Particle Filters for Partially-Observed Boolean Dynamical Systems|Partially-observed Boolean dynamical systems (POBDS) are a general class of nonlinear models with application in estimation and control of Boolean processes based on noisy and incomplete measurements. The optimal minimum mean square error (MMSE) algorithms for POBDS state estimation, namely, the Boolean Kalman filter (BKF) and Boolean Kalman smoother (BKS), are intractable in the case of large systems, due to computational and memory requirements. To address this, we propose approximate MMSE filtering and smoothing algorithms based on the auxiliary particle filter (APF) method from sequential Monte-Carlo theory. These algorithms are used jointly with maximum-likelihood (ML) methods for simultaneous state and parameter estimation in POBDS models. In the presence of continuous parameters, ML estimation is performed using the expectation-maximization (EM) algorithm; we develop for this purpose a special smoother which reduces the computational complexity of the EM algorithm. The resulting particle-based adaptive filter is applied to a POBDS model of Boolean gene regulatory networks observed through noisy RNA-Seq time series data, and performance is assessed through a series of numerical experiments using the well-known cell cycle gene regulatory model.|['Mahdi Imani', 'Ulisses Braga-Neto']|['stat.ME', 'math.DS', 'q-bio.MN']
2017-03-16T23:27:33Z|2017-02-23T04:13:42Z|http://arxiv.org/abs/1702.07089v1|http://arxiv.org/pdf/1702.07089v1|A Nonparametric Bayesian Approach to Copula Estimation|We propose a novel Dirichlet-based P\'olya tree (D-P tree) prior on the copula and based on the D-P tree prior, a nonparametric Bayesian inference procedure. Through theoretical analysis and simulations, we are able to show that the flexibility of the D-P tree prior ensures its consistency in copula estimation, thus able to detect more subtle and complex copula structures than earlier nonparametric Bayesian models, such as a Gaussian copula mixture. Further, the continuity of the imposed D-P tree prior leads to a more favorable smoothing effect in copula estimation over classic frequentist methods, especially with small sets of observations. We also apply our method to the copula prediction between the S\&P 500 index and the IBM stock prices during the 2007-08 financial crisis, finding that D-P tree-based methods enjoy strong robustness and flexibility over classic methods under such irregular market behaviors.|['Shaoyang Ning', 'Neil Shephard']|['stat.ME']
2017-03-16T23:27:33Z|2017-02-26T20:06:00Z|http://arxiv.org/abs/1702.07027v2|http://arxiv.org/pdf/1702.07027v2|Nonparametric Inference via Bootstrapping the Debiased Estimator|In this paper, we propose to construct confidence bands by bootstrapping the debiased kernel density estimator (for density estimation) and the debiased local polynomial regression estimator (for regression analysis). The idea of using a debiased estimator was first introduced in Calonico et al. (2015), where they construct a confidence interval of the density function (and regression function) at a given point by explicitly estimating stochastic variations. We extend their ideas and propose a bootstrap approach for constructing confidence bands that is uniform for every point in the support. We prove that the resulting bootstrap confidence band is asymptotically valid and is compatible with most tuning parameter selection approaches, such as the rule of thumb and cross-validation. We further generalize our method to confidence sets of density level sets and inverse regression problems. Simulation studies confirm the validity of the proposed confidence bands/sets.|['Yen-Chi Chen']|['stat.ME', 'math.ST', 'stat.TH', 'Primary 62G15, secondary 62G09, 62G07, 62G08']
2017-03-16T23:27:33Z|2017-02-22T21:09:19Z|http://arxiv.org/abs/1702.07007v1|http://arxiv.org/pdf/1702.07007v1|Detecting causal associations in large nonlinear time series datasets|Detecting causal associations in time series datasets is a key challenge for novel insights into complex dynamical systems such as the Earth system or the human brain. Interactions in high-dimensional dynamical systems often involve time-delays, nonlinearity, and strong autocorrelations. These present major challenges for causal discovery techniques such as Granger causality leading to low detection power, biases, and unreliable hypothesis tests. Here we introduce a reliable and fast method that outperforms current approaches in detection power and scales up to high-dimensional datasets. It overcomes detection biases, especially when strong autocorrelations are present, and allows ranking associations in large-scale analyses by their causal strength. We provide mathematical proofs, evaluate our method in extensive numerical experiments, and illustrate its capabilities in a large-scale analysis of the global surface-pressure system where we unravel spurious associations and find several potentially causal links that are difficult to detect with standard methods. The broadly applicable method promises to discover novel causal insights also in many other fields of science.|['Jakob Runge', 'Dino Sejdinovic', 'Seth Flaxman']|['stat.ME', 'physics.ao-ph', 'stat.AP']
2017-03-16T23:27:33Z|2017-02-22T19:58:06Z|http://arxiv.org/abs/1702.06986v1|http://arxiv.org/pdf/1702.06986v1|Rank conditional coverage and confidence intervals in high dimensional   problems|Confidence interval procedures used in low dimensional settings are often inappropriate for high dimensional applications. When a large number of parameters are estimated, marginal confidence intervals associated with the most significant estimates have very low coverage rates: They are too small and centered at biased estimates. The problem of forming confidence intervals in high dimensional settings has previously been studied through the lens of selection adjustment. In this framework, the goal is to control the proportion of non-covering intervals formed for selected parameters.   In this paper we approach the problem by considering the relationship between rank and coverage probability. Marginal confidence intervals have very low coverage rates for significant parameters and high rates for parameters with more boring estimates. Many selection adjusted intervals display the same pattern. This connection motivates us to propose a new coverage criterion for confidence intervals in multiple testing/covering problems --- the rank conditional coverage (RCC). This is the expected coverage rate of an interval given the significance ranking for the associated estimator. We propose interval construction via bootstrapping which produces small intervals and have a rank conditional coverage close to the nominal level. These methods are implemented in the R package rcc.|['Jean Morrison', 'Noah Simon']|['stat.ME']
2017-03-16T23:27:33Z|2017-02-22T13:27:02Z|http://arxiv.org/abs/1702.06790v1|http://arxiv.org/pdf/1702.06790v1|Guided projections for analysing the structure of high-dimensional data|A powerful data transformation method named guided projections is proposed creating new possibilities to reveal the group structure of high-dimensional data in the presence of noise variables. Utilising projections onto a space spanned by a selection of a small number of observations allows measuring the similarity of other observations to the selection based on orthogonal and score distances. Observations are iteratively exchanged from the selection creating a non-random sequence of projections which we call guided projections. In contrast to conventional projection pursuit methods, which typically identify a low-dimensional projection revealing some interesting features contained in the data, guided projections generate a series of projections that serve as a basis not just for diagnostic plots but to directly investigate the group structure in data. Based on simulated data we identify the strengths and limitations of guided projections in comparison to commonly employed data transformation methods. We further show the relevance of the transformation by applying it to real-world data sets.|['Thomas Ortner', 'Peter Filzmoser', 'Maia Zaharieva', 'Christian Breiteneder', 'Sarka Brodinova']|['stat.ME']
2017-03-16T23:27:37Z|2017-02-22T01:14:43Z|http://arxiv.org/abs/1702.06635v1|http://arxiv.org/pdf/1702.06635v1|Robust Empirical Bayes Small Area Estimation with Density Power   Divergence|Empirical Bayes estimators are widely used to provide indirect and model-based estimates of means in small areas. The most common model is two-stage normal hierarchical model called Fay-Herriot model. However, due to the normality assumption, it can be highly influenced by the presence of outliers. In this article, we propose a simple modification of the conventional method by using density power divergence and derive a new robust empirical Bayes small area estimator. Based on some asymptotic properties of the robust estimator of the model parameters, we obtain an expression of second order approximation of the mean squared error of the proposed empirical Bayes estimator. We investigate some numerical performances of the proposed method through simulations and a real data application.|['Shonosuke Sugasawa']|['stat.ME']
2017-03-16T23:27:37Z|2017-02-21T20:23:35Z|http://arxiv.org/abs/1702.06570v1|http://arxiv.org/pdf/1702.06570v1|Inference for Stochastically Contaminated Variable Length Markov Chains|In this paper, we present a methodology to estimate the parameters of stochastically contaminated models under two contamination regimes. In both regimes, we assume that the original process is a variable length Markov chain that is contaminated by a random noise. In the first regime we consider that the random noise is added to the original source and in the second regime, the random noise is multiplied by the original source. Given a contaminated sample of these models, the original process is hidden. Then we propose a two steps estimator for the parameters of these models, that is, the probability transitions and the noise parameter, and prove its consistency. The first step is an adaptation of the Baum-Welch algorithm for Hidden Markov Models. This step provides an estimate of a complete order $k$ Markov chain, where $k$ is bigger than the order of the variable length Markov chain if it has finite order and is a constant depending on the sample size if the hidden process has infinite order. In the second estimation step, we propose a bootstrap Bayesian Information Criterion, given a sample of the Markov chain estimated in the first step, to obtain the variable length time dependence structure associated with the hidden process. We present a simulation study showing that our methodology is able to accurately recover the parameters of the models for a reasonable interval of random noises.|['Denise Duarte', 'Sokol Ndreca', 'Wecsley O. Prates']|['stat.ME', '60J10, 62M05']
2017-03-16T23:27:37Z|2017-02-21T02:23:41Z|http://arxiv.org/abs/1702.06240v1|http://arxiv.org/pdf/1702.06240v1|Best Linear Predictor with Missing Response: Locally Robust Approach|This paper provides asymptotic theory for Inverse Probability Weighing (IPW) and Locally Robust Estimator (LRE) of Best Linear Predictor where the response missing at random (MAR), but not completely at random (MCAR). We relax previous assumptions in the literature about the first-step nonparametric components, requiring only their mean square convergence. This relaxation allows to use a wider class of machine leaning methods for the first-step, such as lasso. For a generic first-step, IPW incurs a first-order bias unless the model it approximates is truly linear in the predictors. In contrast, LRE remains first-order unbiased provided one can estimate the conditional expectation of the response with sufficient accuracy. An additional novelty is allowing the dimension of Best Linear Predictor to grow with sample size. These relaxations are important for estimation of best linear predictor of teacher-specific and hospital-specific effects with large number of individuals.|['Victor Chernozhukov', 'Vira Semenova']|['stat.ME', 'stat.ML']
2017-03-16T23:27:37Z|2017-02-21T00:28:39Z|http://arxiv.org/abs/1702.06220v1|http://arxiv.org/pdf/1702.06220v1|Eigenvector spatial filtering for large data sets: fixed and random   effects approaches|"Eigenvector spatial filtering (ESF) is a spatial modeling approach, which has been applied in urban and regional studies, ecological studies, and so on. However, it is computationally demanding, and may not be suitable for large data modeling. The objective of this study is developing fast ESF and random effects ESF (RE-ESF), which are capable of handling very large samples. To achieve it, we accelerate eigen-decomposition and parameter estimation, which make ESF and RE-ESF slow. The former is accelerated by utilizing the Nystr\""om extension, whereas the latter is by small matrix tricks. The resulting fast ESF and fast RE-ESF are compared with non-approximated ESF and RE-ESF in Mote Carlo simulation experiments. The result shows that, while ESF and RE-ESF are slow for several thousand sample size, fast ESF and RE-ESF require only several minutes even for 500,000 sample size. It is also verified that their approximation errors are very small. We subsequently apply fast ESF and RE-ESF approaches to a land price analysis."|['Daisuke Murakami', 'Daniel A. Griffith']|['stat.ME']
2017-03-16T23:27:37Z|2017-02-21T00:28:39Z|http://arxiv.org/abs/1702.06221v1|http://arxiv.org/pdf/1702.06221v1|Determination of hysteresis in finite-state random walks using Bayesian   cross validation|Consider the problem of modeling hysteresis for finite-state random walks using higher-order Markov chains. This Letter introduces a Bayesian framework to determine, from data, the number of prior states of recent history upon which a trajectory is statistically dependent. The general recommendation is to use leave-one-out cross validation, using an easily-computable formula that is provided in closed form. Importantly, Bayes factors using flat model priors are biased in favor of too-complex a model (more hysteresis) when a large amount of data is present and the Akaike information criterion (AIC) is biased in favor of too-sparse a model (less hysteresis) when few data are present.|['Joshua C. Chang']|['stat.ME', 'cs.LG', 'physics.data-an', 'q-bio.QM']
2017-03-16T23:27:37Z|2017-02-25T14:17:44Z|http://arxiv.org/abs/1702.06166v2|http://arxiv.org/pdf/1702.06166v2|Bayesian Boolean Matrix Factorisation|Boolean matrix factorisation aims to decompose a binary data matrix into an approximate Boolean product of two low rank, binary matrices: one containing meaningful patterns, the other quantifying how the observations can be expressed as a combination of these patterns. We introduce the OrMachine, a probabilistic generative model for Boolean matrix factorisation and derive a Metropolised Gibbs sampler that facilitates efficient parallel posterior inference. On real world and simulated data, our method outperforms all currently existing approaches for Boolean matrix factorisation and completion. This is the first method to provide full posterior inference for Boolean Matrix factorisation which is relevant in applications, e.g. for controlling false positive rates in collaborative filtering and, crucially, improves the interpretability of the inferred patterns. The proposed algorithm scales to large datasets as we demonstrate by analysing single cell gene expression data in 1.3 million mouse brain cells across 11 thousand genes on commodity hardware.|['Tammo Rukat', 'Chris C. Holmes', 'Michalis K. Titsias', 'Christopher Yau']|['stat.ML', 'cs.LG', 'cs.NA', 'q-bio.GN', 'q-bio.QM', 'stat.ME']
2017-03-16T23:27:37Z|2017-02-20T13:56:48Z|http://arxiv.org/abs/1702.05972v1|http://arxiv.org/pdf/1702.05972v1|Generalising rate heterogeneity across sites in statistical   phylogenetics|In phylogenetics, alignments of molecular sequence data for a collection of species are used to learn about their phylogeny - an evolutionary tree which places these species as leaves and ancestors as internal nodes. Sequence evolution on each branch of the tree is generally modelled using a continuous time Markov process, characterised by an instantaneous rate matrix. Early models assumed the same rate matrix governed substitutions at all sites of the alignment, ignoring the variation in evolutionary constraints. Substantial improvements in phylogenetic inference and model fit were achieved by augmenting these models with a set of multiplicative random effects that allowed different sites to evolve at different rates which scaled the baseline rate matrix. Motivated by this pioneering work, we consider an extension which allows quadratic, rather than linear, site-specific transformations of the baseline rate matrix.   We derive properties of the resulting process and show that when combined with a particular class of non-stationary models, we obtain one that allows sequence composition to vary across both sites of the alignment and taxa. Formulating the model in a Bayesian framework, a Markov chain Monte Carlo algorithm for posterior inference is described. We consider two applications to alignments concerning the tree of life, fitting stationary and non-stationary models. In each case we compare inferences obtained under our site-specific quadratic transformation, with those under linear and site-homogeneous models.|['Sarah E. Heaps', 'Tom M. W. Nye', 'Richard J. Boys', 'Tom A. Williams', 'Svetlana Cherlin', 'T. Martin Embley']|['stat.ME']
2017-03-16T23:27:37Z|2017-03-08T18:22:26Z|http://arxiv.org/abs/1702.05960v2|http://arxiv.org/pdf/1702.05960v2|A Statistical Learning Approach to Modal Regression|This paper studies the nonparametric modal regression problem systematically from a statistical learning view. Originally motivated by pursuing a theoretical understanding of the maximum correntropy criterion based regression (MCCR), our study reveals that MCCR with a tending-to-zero scale parameter is essentially modal regression. We show that nonparametric modal regression problem can be approached via the classical empirical risk minimization. Some efforts are then made to develop a framework for analyzing and implementing modal regression. For instance, the modal regression function is described, the modal regression risk is defined explicitly and its \textit{Bayes} rule is characterized; for the sake of computational tractability, the surrogate modal regression risk, which is termed as the generalization risk in our study, is introduced. On the theoretical side, the excess modal regression risk, the excess generalization risk, the function estimation error, and the relations among the above three quantities are studied rigorously. It turns out that under mild conditions, function estimation consistency and convergence may be pursued in modal regression as in vanilla regression protocols, such as mean regression, median regression, and quantile regression. However, it outperforms these regression models in terms of robustness as shown in our study from a re-descending M-estimation view. This coincides with and in return explains the merits of MCCR on robustness. On the practical side, the implementation issues of modal regression including the computational algorithm and the tuning parameters selection are discussed. Numerical assessments on modal regression are also conducted to verify our findings empirically.|['Yunlong Feng', 'Jun Fan', 'Johan A. K. Suykens']|['stat.ML', 'math.ST', 'stat.ME', 'stat.TH']
2017-03-16T23:27:37Z|2017-02-20T07:07:05Z|http://arxiv.org/abs/1702.05879v1|http://arxiv.org/pdf/1702.05879v1|Complexity of Possibly-gapped Histogram and Analysis of Histogram   (ANOHT)|Without unrealistic continuity and smoothness assumptions on a distributional density of one dimensional dataset, constructing an authentic possibly-gapped histogram becomes rather complex. The candidate ensemble is described via a two-layer Ising model, and its size is shown to grow exponentially. This exponential complexity makes any exhaustive search in-feasible and all boundary parameters local. For data compression via Uniformity, the decoding error criterion is nearly independent of sample size. These characteristics nullify statistical model selection techniques, such as Minimum Description Length (MDL). Nonetheless practical and nearly optimal solutions are algorithmically computable. A data-driven algorithm is devised to construct such histograms along the branching hierarchy of a Hierarchical Clustering tree. Such resultant histograms naturally manifest data's physical information contents: deterministic structures of bin-boundaries coupled with stochastic structures of Uniformity within each bin. Without enforcing unrealistic Normality and constant variance assumptions, an application of possibly-gapped histogram is devised, called analysis of Histogram (ANOHT), to replace Analysis of Variance (ANOVA). Its potential applications are foreseen in digital re-normalization schemes and associative pattern extraction among features of heterogeneous data types. Thus constructing possibly-gapped histograms becomes a prerequisite for knowledge discovery, via exploratory data analysis and unsupervised Machine Learning.|['Fushing Hsieh', 'Tania Roy']|['stat.ME']
2017-03-16T23:27:37Z|2017-02-20T01:50:34Z|http://arxiv.org/abs/1702.05832v1|http://arxiv.org/pdf/1702.05832v1|Robust Hierarchical Bayes Small Area Estimation for Nested Error   Regression Model|"National statistical institutes in many countries are now mandated to produce reliable statistics for important variables such as population, income, unemployment, health outcomes, etc. for small areas, defined by geography and/or demography. Due to small samples from these areas, direct sample-based estimates are often unreliable. Model-based small area estimation is now extensively used to generate reliable statistics by ""borrowing strength"" from other areas and related variables through suitable models. Outliers adversely influence standard model-based small area estimates. To deal with outliers, Sinha and Rao (2009) proposed a robust frequentist approach. In this article, we present a robust Bayesian alternative to the nested error regression model for unit-level data to mitigate outliers. We consider a two-component scale mixture of normal distributions for the unit-level error to model outliers and present a computational approach to produce Bayesian predictors of small area means under a noninformative prior for model parameters. A real example and extensive simulations convincingly show robustness of our Bayesian predictors to outliers. Simulations comparison of these two procedures with Bayesian predictors by Datta and Ghosh (1991) and M-quantile estimators by Chambers et al. (2014) shows that our proposed procedure is better than the others in terms of bias, variability, and coverage probability of prediction intervals, when there are outliers. The superior frequentist performance of our procedure shows its dual (Bayes and frequentist) dominance, and makes it attractive to all practitioners, both Bayesian and frequentist, of small area estimation."|['Adrijo Chakraborty', 'Gauri Sankar Datta', 'Abhyuday Mandal']|['stat.ME']
2017-03-16T23:27:41Z|2017-02-26T00:37:27Z|http://arxiv.org/abs/1702.05829v2|http://arxiv.org/pdf/1702.05829v2|Copula-based piecewise regression|Most common parametric families of copulas are totally ordered, and in many cases they are also positively or negatively regression dependent and therefore they lead to monotone regression functions, which makes them not suitable for dependence relationships that imply or suggest a non-monotone regression function. A gluing copula approach is proposed to decompose the underlying copula into totally ordered copulas that combined may lead to a non-monotone regression function.|['Arturo Erdely']|['stat.ME', '62J02, 62H20']
2017-03-16T23:27:41Z|2017-02-17T18:06:27Z|http://arxiv.org/abs/1702.05462v1|http://arxiv.org/pdf/1702.05462v1|Objective Bayesian Analysis for Change Point Problems|In this paper we present an objective approach to change point analysis. In particular, we look at the problem from two perspectives. The first focuses on the definition of an objective prior when the number of change points is known a priori. The second contribution aims to estimate the number of change points by using an objective approach, recently introduced in the literature, based on losses. The latter considers change point estimation as a model selection exercise. We show the performance of the proposed approach on simulated data and on real data sets.|['Laurentiu Hinoveanu', 'Fabrizio Leisen', 'Cristiano Villa']|['stat.ME', 'math.ST', 'stat.AP', 'stat.CO', 'stat.ML', 'stat.TH']
2017-03-16T23:27:41Z|2017-02-17T13:53:15Z|http://arxiv.org/abs/1702.05340v1|http://arxiv.org/pdf/1702.05340v1|Combinatorics of Distance Covariance: Inclusion-Minimal Maximizers of   Quasi-Concave Set Functions for Diverse Variable Selection|In this paper we show that the negative sample distance covariance function is a quasi-concave set function of samples of random variables that are not statistically independent. We use these properties to propose greedy algorithms to combinatorially optimize some diversity (low statistical dependence) promoting functions of distance covariance. Our greedy algorithm obtains all the inclusion-minimal maximizers of this diversity promoting objective. Inclusion-minimal maximizers are multiple solution sets of globally optimal maximizers that are not a proper subset of any other maximizing set in the solution set. We present results upon applying this approach to obtain diverse features (covariates/variables/predictors) in a feature selection setting for regression (or classification) problems. We also combine our diverse feature selection algorithm with a distance covariance based relevant feature selection algorithm of [7] to produce subsets of covariates that are both relevant yet ordered in non-increasing levels of diversity of these subsets.|['Praneeth Vepakomma', 'Yulia Kempner']|['stat.ME', 'stat.OT']
2017-03-16T23:27:41Z|2017-02-17T00:38:13Z|http://arxiv.org/abs/1702.05195v1|http://arxiv.org/pdf/1702.05195v1|Empirical Bayes, SURE and Sparse Normal Mean Models|This paper studies the sparse normal mean models under the empirical Bayes framework. We focus on the mixture priors with an atom at zero and a density component centered at a data driven location determined by maximizing the marginal likelihood or minimizing the Stein Unbiased Risk Estimate. We study the properties of the corresponding posterior median and posterior mean. In particular, the posterior median is a thresholding rule and enjoys the multi-direction shrinkage property that shrinks the observation toward either the origin or the data-driven location. The idea is extended by considering a finite mixture prior, which is flexible to model the cluster structure of the unknown means. We further generalize the results to heteroscedastic normal mean models. Specifically, we propose a semiparametric estimator which can be calculated efficiently by combining the familiar EM algorithm with the Pool-Adjacent-Violators algorithm for isotonic regression. The effectiveness of our methods is demonstrated via extensive numerical studies.|['Xianyang Zhang', 'Anirban Bhattacharya']|['stat.ME']
2017-03-16T23:27:41Z|2017-02-16T23:56:19Z|http://arxiv.org/abs/1702.05189v1|http://arxiv.org/pdf/1702.05189v1|Upper bounds on the minimum coverage probability of model averaged tail   area confidence intervals in regression|"Frequentist model averaging has been proposed as a method for incorporating ""model uncertainty"" into confidence interval construction. Such proposals have been of particular interest in the environmental and ecological statistics communities. A promising method of this type is the model averaged tail area (MATA) confidence interval put forward by Turek and Fletcher, 2012. The performance of this interval depends greatly on the data-based model weights on which it is based. A computationally convenient formula for the coverage probability of this interval is provided by Kabaila, Welsh and Abeysekera, 2016, in the simple scenario of two nested linear regression models. We consider the more complicated scenario that there are many (32,768 in the example considered) linear regression models obtained as follows. For each of a specified set of components of the regression parameter vector, we either set the component to zero or let it vary freely. We provide an easily-computed upper bound on the minimum coverage probability of the MATA confidence interval. This upper bound provides evidence against the use of a model weight based on the Bayesian Information Criterion (BIC)."|['Paul Kabaila']|['stat.ME']
2017-03-16T23:27:41Z|2017-02-16T17:11:01Z|http://arxiv.org/abs/1702.05056v1|http://arxiv.org/pdf/1702.05056v1|An Empirical Bayes Approach for High Dimensional Classification|We propose an empirical Bayes estimator based on Dirichlet process mixture model for estimating the sparse normalized mean difference, which could be directly applied to the high dimensional linear classification. In theory, we build a bridge to connect the estimation error of the mean difference and the misclassification error, also provide sufficient conditions of sub-optimal classifiers and optimal classifiers. In implementation, a variational Bayes algorithm is developed to compute the posterior efficiently and could be parallelized to deal with the ultra-high dimensional case.|['Yunbo Ouyang', 'Feng Liang']|['stat.ML', 'stat.ME']
2017-03-16T23:27:41Z|2017-02-16T15:16:27Z|http://arxiv.org/abs/1702.05008v1|http://arxiv.org/pdf/1702.05008v1|Tree Ensembles with Rule Structured Horseshoe Regularization|We propose a new Bayesian model for flexible nonlinear regression and classification using tree ensembles. The model is based on the RuleFit approach in Friedman and Popescu (2008) where rules from decision trees and linear terms are used in a L1-regularized regression. We modify RuleFit by replacing the L1-regularization by a horseshoe prior, which is well known to give aggressive shrinkage of noise predictor while leaving the important signal essentially untouched. This is especially important when a large number of rules are used as predictors as many of them only contribute noise. Our horseshoe prior has an additional hierarchical layer that applies more shrinkage a priori to rules with a large number of splits, and to rules that are only satisfied by a few observations. The aggressive noise shrinkage of our prior also makes it possible to complement the rules from boosting in Friedman and Popescu (2008) with an additional set of trees from random forest, which brings a desirable diversity to the ensemble. We sample from the posterior distribution using a very efficient and easily implemented Gibbs sampler. The new model is shown to outperform state-of-the-art methods like RuleFit, BART and random forest on 16 datasets. The model and its interpretation is demonstrated on the well known Boston housing data, and on gene expression data for cancer classification. The posterior sampling, prediction and graphical tools for interpreting the model results are implemented in a publicly available R package.|['Malte Nalenz', 'Mattias Villani']|['stat.ME', 'stat.ML']
2017-03-16T23:27:41Z|2017-02-15T19:54:38Z|http://arxiv.org/abs/1702.04755v1|http://arxiv.org/pdf/1702.04755v1|Estimating Individualized Treatment Rules for Ordinal Treatments|Precision medicine is an emerging scientific topic for disease treatment and prevention that takes into account individual patient characteristics. It is an important direction for clinical research, and many statistical methods have been recently proposed. One of the primary goals of precision medicine is to obtain an optimal individual treatment rule (ITR), which can help make decisions on treatment selection according to each patient's specific characteristics. Recently, outcome weighted learning (OWL) has been proposed to estimate such an optimal ITR in a binary treatment setting by maximizing the expected clinical outcome. However, for ordinal treatment settings, such as individualized dose finding, it is unclear how to use OWL. In this paper, we propose a new technique for estimating ITR with ordinal treatments. In particular, we propose a data duplication technique with a piecewise convex loss function. We establish Fisher consistency for the resulting estimated ITR under certain conditions, and obtain the convergence and risk bound properties. Simulated examples and two applications to datasets from an irritable bowel problem and a type 2 diabetes mellitus observational study demonstrate the highly competitive performance of the proposed method compared to existing alternatives.|['Jingxiang Chen', 'Haoda Fu', 'Xuanyao He', 'Michael R. Kosorok', 'Yufeng Liu']|['stat.ME']
2017-03-16T23:27:41Z|2017-02-15T16:55:56Z|http://arxiv.org/abs/1702.04682v1|http://arxiv.org/pdf/1702.04682v1|Targeted Learning Ensembles for Optimal Individualized Treatment Rules   with Time-to-Event Outcomes|We consider estimation of an optimal individualized treatment rule (ITR) from observational and randomized studies when data for a high-dimensional baseline variable is available. Our optimality criterion is with respect to delaying time to occurrence of an event of interest (e.g., death or relapse of cancer). We leverage semiparametric efficiency theory to construct estimators with desirable properties such as double robustness. We propose two estimators of the optimal ITR, which arise from considering two loss functions aimed at (i) directly estimating the conditional treatment effect (also know as the blip function), and (ii) recasting the problem as a weighted classification problem that uses the 0-1 loss function. Our estimators are \textit{super learning} ensembles that minimize the cross-validated risk of a linear combination of estimators in a user-supplied library of candidate estimators. We prove oracle inequalities bounding the finite sample excess risk of the estimator. The bounds depend on the excess risk of the oracle selector and the bias in estimation of the nuisance parameters. These oracle inequalities imply asymptotic optimality of the estimated optimal ITR in the sense that one of the two following claims holds: the estimated optimal ITR is consistent, or it is equivalent with the oracle selector. In a randomized trial with uninformative censoring, we show that the value of the super learner based on (ii) achieves rate at least as fast as $\log n/n$,whereas the value of the super learner based on (i) achieves the slower rate $(\log n/n)^{1/2}$. We illustrate our methods in the analysis of a phase III randomized study testing the efficacy of a new therapy for the treatment of breast cancer.|['Iván Díaz', 'Oleksandr Savenkov', 'Karla Ballman']|['stat.ME']
2017-03-16T23:27:41Z|2017-02-15T14:36:44Z|http://arxiv.org/abs/1702.04625v1|http://arxiv.org/pdf/1702.04625v1|Non-separable Models with High-dimensional Data|This paper studies non-separable models with a continuous treatment when control variables are high-dimensional. We propose an estimation and inference procedure for average, quantile, and marginal treatment effects. In the procedure, control variables are selected via a localized method of $L_1$-penalization at each value of the continuous treatment. Finite sample properties of the new procedure are illustrated through both simulation and empirical applications.|['Liangjun Su', 'Takuya Ura', 'Yichong Zhang']|['stat.ME']
