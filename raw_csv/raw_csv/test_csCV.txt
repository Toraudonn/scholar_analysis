2017-03-16T23:31:59Z|2017-03-15T17:44:37Z|http://arxiv.org/abs/1703.05289v1|http://arxiv.org/pdf/1703.05289v1|A clever elimination strategy for efficient minimal solvers|We present a new insight into the systematic generation of minimal solvers in computer vision, which leads to smaller and faster solvers. Many minimal problem formulations are coupled sets of linear and polynomial equations where image measurements enter the linear equations only. We show that it is useful to solve such systems by first eliminating all the unknowns that do not appear in the linear equations and then extending solutions to the rest of unknowns. This can be generalized to fully non-linear systems by linearization via lifting. We demonstrate that this approach leads to more efficient solvers in three problems of partially calibrated relative camera pose computation with unknown focal length and/or radial distortion. Our approach also generates new interesting constraints on the fundamental matrices of partially calibrated cameras, which were not known before.|['Zuzana Kukelova', 'Joe Kileel', 'Bernd Sturmfels', 'Tomas Pajdla']|['cs.CV', 'cs.SC']
2017-03-16T23:31:59Z|2017-03-15T16:35:31Z|http://arxiv.org/abs/1703.05243v1|http://arxiv.org/pdf/1703.05243v1|Hybrid Supervised-unsupervised Image Topic Visualization with   Convolutional Neural Network and LDA|"The system generates three errors of ""Bad character(s) in field Abstract"" for no reason. Please refer to manuscript for the full abstract."|['Kai Zhen', 'Mridul Birla', 'David Crandall', 'Bingjing Zhang', 'Judy Qiu']|['cs.CV']
2017-03-16T23:31:59Z|2017-03-15T16:31:16Z|http://arxiv.org/abs/1703.05235v1|http://arxiv.org/pdf/1703.05235v1|Transfer Learning for Melanoma Detection: Participation in ISIC 2017   Skin Lesion Classification Challenge|This manuscript describes our participation in the International Skin Imaging Collaboration's 2017 Skin Lesion Analysis Towards Melanoma Detection competition. We participated in Part 3: Lesion Classification. The two stated goals of this binary image classification challenge were to distinguish between (a) melanoma and (b) nevus and seborrheic keratosis, followed by distinguishing between (a) seborrheic keratosis and (b) nevus and melanoma. We chose a deep neural network approach with a transfer learning strategy, using a pre-trained Inception V3 network as both a feature extractor to provide input for a multi-layer perceptron as well as fine-tuning an augmented Inception network. This approach yielded validation set AUC's of 0.84 on the second task and 0.76 on the first task, for an average AUC of 0.80. We joined the competition unfortunately late, and we look forward to improving on these results.|['Dennis H. Murphree', 'Che Ngufor']|['cs.CV']
2017-03-16T23:31:59Z|2017-03-15T16:14:52Z|http://arxiv.org/abs/1703.05230v1|http://arxiv.org/pdf/1703.05230v1|Texture segmentation with Fully Convolutional Networks|In the last decade, deep learning has contributed to advances in a wide range computer vision tasks including texture analysis. This paper explores a new approach for texture segmentation using deep convolutional neural networks, sharing important ideas with classic filter bank based texture segmentation methods. Several methods are developed to train Fully Convolutional Networks to segment textures in various applications. We show in particular that these networks can learn to recognize and segment a type of texture, e.g. wood and grass from texture recognition datasets (no training segmentation). We demonstrate that Fully Convolutional Networks can learn from repetitive patterns to segment a particular texture from a single image or even a part of an image. We take advantage of these findings to develop a method that is evaluated on a series of supervised and unsupervised experiments and improve the state of the art on the Prague texture segmentation datasets.|['Vincent Andrearczyk', 'Paul F. Whelan']|['cs.CV']
2017-03-16T23:31:59Z|2017-03-15T14:53:15Z|http://arxiv.org/abs/1703.05192v1|http://arxiv.org/pdf/1703.05192v1|Learning to Discover Cross-Domain Relations with Generative Adversarial   Networks|While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity.|['Taeksoo Kim', 'Moonsu Cha', 'Hyunsoo Kim', 'Jungkwon Lee', 'Jiwon Kim']|['cs.CV']
2017-03-16T23:31:59Z|2017-03-15T14:18:23Z|http://arxiv.org/abs/1703.05165v1|http://arxiv.org/pdf/1703.05165v1|Automatic skin lesion segmentation with fully   convolutional-deconvolutional networks|This paper summarizes our method and validation results for the ISBI Challenge 2017 - Skin Lesion Analysis Towards Melanoma Detection - Part I: Lesion Segmentation|['Yading Yuan', 'Ming Chao', 'Yeh-Chi Lo']|['cs.CV']
2017-03-16T23:31:59Z|2017-03-15T14:03:47Z|http://arxiv.org/abs/1703.05161v1|http://arxiv.org/pdf/1703.05161v1|Real-Time Panoramic Tracking for Event Cameras|Event cameras are a paradigm shift in camera technology. Instead of full frames, the sensor captures a sparse set of {\em events} caused by intensity changes. Since only the changes are transferred, those cameras are able to capture quick movements of objects in the scene or of the camera itself. In this work we propose a novel method to perform camera tracking of event cameras in a panoramic setting with three degrees of freedom. We propose a direct camera tracking formulation, similar to state-of-the-art in visual odometry. We show that the minimal information needed for simultaneous tracking and mapping is the spatial position of events, without using the appearance of the imaged scene point. We verify the robustness to fast camera movements and dynamic objects in the scene on a recently proposed dataset \cite{Mueggler2016} and self-recorded sequences.|['Christian Reinbacher', 'Gottfried Munda', 'Thomas Pock']|['cs.CV']
2017-03-16T23:31:59Z|2017-03-15T13:33:55Z|http://arxiv.org/abs/1703.05148v1|http://arxiv.org/pdf/1703.05148v1|Random Forests and VGG-NET: An Algorithm for the ISIC 2017 Skin Lesion   Classification Challenge|This manuscript briefly describes an algorithm developed for the ISIC 2017 Skin Lesion Classification Competition. In this task, participants are asked to complete two independent binary image classification tasks that involve three unique diagnoses of skin lesions (melanoma, nevus, and seborrheic keratosis). In the first binary classification task, participants are asked to distinguish between (a) melanoma and (b) nevus and seborrheic keratosis. In the second binary classification task, participants are asked to distinguish between (a) seborrheic keratosis and (b) nevus and melanoma. The other phases of the competition are not considered. Our proposed algorithm consists of three steps: preprocessing, classification using VGG-NET and Random Forests, and calculation of a final score.|['Songtao Guo', 'Yixin Luo', 'Yanzhi Song']|['cs.CV']
2017-03-16T23:31:59Z|2017-03-15T12:56:25Z|http://arxiv.org/abs/1703.05130v1|http://arxiv.org/pdf/1703.05130v1|Block Compressive Sensing of Image and Video with Nonlocal Lagrangian   Multiplier and Patch-based Sparse Representation|Although block compressive sensing (BCS) makes it tractable to sense large-sized images and video, its recovery performance has yet to be significantly improved because its recovered images or video usually suffer from blurred edges, loss of details, and high-frequency oscillatory artifacts, especially at a low subrate. This paper addresses these problems by designing a modified total variation technique that employs multi-block gradient processing, a denoised Lagrangian multiplier, and patch-based sparse representation. In the case of video, the proposed recovery method is able to exploit both spatial and temporal similarities. Simulation results confirm the improved performance of the proposed method for compressive sensing of images and video in terms of both objective and subjective qualities.|['Trinh Van Chien', 'Khanh Quoc Dinh', 'Byeungwoo Jeon', 'Martin Burger']|['cs.CV']
2017-03-16T23:31:59Z|2017-03-15T12:10:36Z|http://arxiv.org/abs/1703.05105v1|http://arxiv.org/pdf/1703.05105v1|A Data Driven Approach for Compound Figure Separation Using   Convolutional Neural Networks|A key problem in automatic analysis and understanding of scientific papers is to extract semantic information from non-textual paper components like figures, diagrams, tables, etc. This research always requires a very first preprocessing step: decomposing compound multi-part figures into individual subfigures. Previous work in compound figure separation has been based on manually designed features and separation rules, which often fail for less common figure types and layouts. Moreover, no implementation for compound figure decomposition is publicly available.   This paper proposes a data driven approach to separate compound figures using modern deep Convolutional Neural Networks (CNNs) to train the separator in an end-to-end manner. CNNs eliminate the need for manually designing features and separation rules, but require large amount of annotated training data. We overcome this challenge using transfer learning as well as automatically synthesizing training exemplars. We evaluate our technique on the ImageCLEF Medical dataset, achieving 85.9% accuracy and outperforming manually engineered previous techniques. We made the resulting approach available as an easy-to-use Python library, aiming to promote further research in scientific figure mining.|['Satoshi Tsutsui', 'David Crandall']|['cs.CV']
2017-03-16T23:32:03Z|2017-03-15T10:30:21Z|http://arxiv.org/abs/1703.05065v1|http://arxiv.org/pdf/1703.05065v1|Joint Epipolar Tracking (JET): Simultaneous optimization of epipolar   geometry and feature correspondences|Traditionally, pose estimation is considered as a two step problem. First, feature correspondences are determined by direct comparison of image patches, or by associating feature descriptors. In a second step, the relative pose and the coordinates of corresponding points are estimated, most often by minimizing the reprojection error (RPE). RPE optimization is based on a loss function that is merely aware of the feature pixel positions but not of the underlying image intensities. In this paper, we propose a sparse direct method which introduces a loss function that allows to simultaneously optimize the unscaled relative pose, as well as the set of feature correspondences directly considering the image intensity values. Furthermore, we show how to integrate statistical prior information on the motion into the optimization process. This constructive inclusion of a Bayesian bias term is particularly efficient in application cases with a strongly predictable (short term) dynamic, e.g. in a driving scenario. In our experiments, we demonstrate that the JET algorithm we propose outperforms the classical reprojection error optimization on two synthetic datasets and on the KITTI dataset. The JET algorithm runs in real-time on a single CPU thread.|['Henry Bradler', 'Matthias Ochs', 'Rudolf Mester']|['cs.CV']
2017-03-16T23:32:03Z|2017-03-15T10:22:21Z|http://arxiv.org/abs/1703.05061v1|http://arxiv.org/pdf/1703.05061v1|Learning Rank Reduced Interpolation with Principal Component Analysis|In computer vision most iterative optimization algorithms, both sparse and dense, rely on a coarse and reliable dense initialization to bootstrap their optimization procedure. For example, dense optical flow algorithms profit massively in speed and robustness if they are initialized well in the basin of convergence of the used loss function. The same holds true for methods as sparse feature tracking when initial flow or depth information for new features at arbitrary positions is needed. This makes it extremely important to have techniques at hand that allow to obtain from only very few available measurements a dense but still approximative sketch of a desired 2D structure (e.g. depth maps, optical flow, disparity maps, etc.). The 2D map is regarded as sample from a 2D random process. The method presented here exploits the complete information given by the principal component analysis (PCA) of that process, the principal basis and its prior distribution. The method is able to determine a dense reconstruction from sparse measurement. When facing situations with only very sparse measurements, typically the number of principal components is further reduced which results in a loss of expressiveness of the basis. We overcome this problem and inject prior knowledge in a maximum a posterior (MAP) approach. We test our approach on the KITTI and the virtual KITTI datasets and focus on the interpolation of depth maps for driving scenes. The evaluation of the results show good agreement to the ground truth and are clearly better than results of interpolation by the nearest neighbor method which disregards statistical information.|['Matthias Ochs', 'Henry Bradler', 'Rudolf Mester']|['cs.CV']
2017-03-16T23:32:03Z|2017-03-15T09:02:11Z|http://arxiv.org/abs/1703.05020v1|http://arxiv.org/pdf/1703.05020v1|Large Margin Object Tracking with Circulant Feature Maps|Structured output support vector machine (SVM) based tracking algorithms have shown favorable performance recently. Nonetheless, the time-consuming candidate sampling and complex optimization limit their real-time applications. In this paper, we propose a novel large margin object tracking method which absorbs the strong discriminative ability from structured output SVM and speeds up by the correlation filter algorithm significantly. Secondly, a multimodal target detection technique is proposed to improve the target localization precision and prevent model drift introduced by similar objects or background noise. Thirdly, we exploit the feedback from high-confidence tracking results to avoid the model corruption problem. We implement two versions of the proposed tracker with the representations from both conventional hand-crafted and deep convolution neural networks (CNNs) based features to validate the strong compatibility of the algorithm. The experimental results demonstrate that the proposed tracker performs superiorly against several state-of-the-art algorithms on the challenging benchmark sequences while runs at speed in excess of 80 frames per second. The source code and experimental results will be made publicly available.|['Mengmeng Wang', 'Yong Liu', 'Zeyi Huang']|['cs.CV']
2017-03-16T23:32:03Z|2017-03-15T08:28:58Z|http://arxiv.org/abs/1703.05002v1|http://arxiv.org/pdf/1703.05002v1|Zero-Shot Recognition using Dual Visual-Semantic Mapping Paths|Zero-shot recognition aims to accurately recognize objects of unseen classes by using a shared visual-semantic mapping between the image feature space and the semantic embedding space. This mapping is learned on training data of seen classes and is expected to have transfer ability to unseen classes. In this paper, we tackle this problem by exploiting the intrinsic relationship between the semantic space manifold and the transfer ability of visual-semantic mapping. We formalize their connection and cast zero-shot recognition as a joint optimization problem. Motivated by this, we propose a novel framework for zero-shot recognition, which contains dual visual-semantic mapping paths. Our analysis shows this framework can not only apply prior semantic knowledge to infer underlying semantic manifold in the image feature space, but also generate optimized semantic embedding space, which can enhance the transfer ability of the visual-semantic mapping to unseen classes. The proposed method is evaluated for zero-shot recognition on four benchmark datasets, achieving outstanding results.|['Yanan Li', 'Donghui Wang', 'Huanhang Hu', 'Yuetan Lin', 'Yueting Zhuang']|['cs.CV']
2017-03-16T23:32:03Z|2017-03-15T07:46:18Z|http://arxiv.org/abs/1703.04986v1|http://arxiv.org/abs/1703.04986v1|Label Stability in Multiple Instance Learning|We address the problem of \emph{instance label stability} in multiple instance learning (MIL) classifiers. These classifiers are trained only on globally annotated images (bags), but often can provide fine-grained annotations for image pixels or patches (instances). This is interesting for computer aided diagnosis (CAD) and other medical image analysis tasks for which only a coarse labeling is provided. Unfortunately, the instance labels may be unstable. This means that a slight change in training data could potentially lead to abnormalities being detected in different parts of the image, which is undesirable from a CAD point of view. Despite MIL gaining popularity in the CAD literature, this issue has not yet been addressed. We investigate the stability of instance labels provided by several MIL classifiers on 5 different datasets, of which 3 are medical image datasets (breast histopathology, diabetic retinopathy and computed tomography lung images). We propose an unsupervised measure to evaluate instance stability, and demonstrate that a performance-stability trade-off can be made when comparing MIL classifiers.|['Veronika Cheplygina', 'Lauge Sørensen', 'David M. J. Tax', 'Marleen de Bruijne', 'Marco Loog']|['cs.CV', 'stat.ML']
2017-03-16T23:32:03Z|2017-03-15T07:43:10Z|http://arxiv.org/abs/1703.04981v1|http://arxiv.org/pdf/1703.04981v1|Transfer Learning by Asymmetric Image Weighting for Segmentation across   Scanners|Supervised learning has been very successful for automatic segmentation of images from a single scanner. However, several papers report deteriorated performances when using classifiers trained on images from one scanner to segment images from other scanners. We propose a transfer learning classifier that adapts to differences between training and test images. This method uses a weighted ensemble of classifiers trained on individual images. The weight of each classifier is determined by the similarity between its training image and the test image.   We examine three unsupervised similarity measures, which can be used in scenarios where no labeled data from a newly introduced scanner or scanning protocol is available. The measures are based on a divergence, a bag distance, and on estimating the labels with a clustering procedure. These measures are asymmetric. We study whether the asymmetry can improve classification. Out of the three similarity measures, the bag similarity measure is the most robust across different studies and achieves excellent results on four brain tissue segmentation datasets and three white matter lesion segmentation datasets, acquired at different centers and with different scanners and scanning protocols. We show that the asymmetry can indeed be informative, and that computing the similarity from the test image to the training images is more appropriate than the opposite direction.|['Veronika Cheplygina', 'Annegreet van Opbroek', 'M. Arfan Ikram', 'Meike W. Vernooij', 'Marleen de Bruijne']|['cs.CV', 'stat.ML']
2017-03-16T23:32:03Z|2017-03-15T07:41:49Z|http://arxiv.org/abs/1703.04980v1|http://arxiv.org/abs/1703.04980v1|Classification of COPD with Multiple Instance Learning|Chronic obstructive pulmonary disease (COPD) is a lung disease where early detection benefits the survival rate. COPD can be quantified by classifying patches of computed tomography images, and combining patch labels into an overall diagnosis for the image. As labeled patches are often not available, image labels are propagated to the patches, incorrectly labeling healthy patches in COPD patients as being affected by the disease. We approach quantification of COPD from lung images as a multiple instance learning (MIL) problem, which is more suitable for such weakly labeled data. We investigate various MIL assumptions in the context of COPD and show that although a concept region with COPD-related disease patterns is present, considering the whole distribution of lung tissue patches improves the performance. The best method is based on averaging instances and obtains an AUC of 0.742, which is higher than the previously reported best of 0.713 on the same dataset. Using the full training set further increases performance to 0.776, which is significantly higher (DeLong test) than previous results.|['Veronika Cheplygina', 'Lauge Sørensen', 'David M. J. Tax', 'Jesper Holst Pedersen', 'Marco Loog', 'Marleen de Bruijne']|['cs.CV', 'stat.ML']
2017-03-16T23:32:03Z|2017-03-15T07:27:12Z|http://arxiv.org/abs/1703.04977v1|http://arxiv.org/pdf/1703.04977v1|What Uncertainties Do We Need in Bayesian Deep Learning for Computer   Vision?|There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model -- uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.|['Alex Kendall', 'Yarin Gal']|['cs.CV']
2017-03-16T23:32:03Z|2017-03-15T06:49:01Z|http://arxiv.org/abs/1703.04967v1|http://arxiv.org/pdf/1703.04967v1|Comparison of the Deep-Learning-Based Automated Segmentation Methods for   the Head Sectioned Images of the Virtual Korean Human Project|This paper presents an end-to-end pixelwise fully automated segmentation of the head sectioned images of the Visible Korean Human (VKH) project based on Deep Convolutional Neural Networks (DCNNs). By converting classification networks into Fully Convolutional Networks (FCNs), a coarse prediction map, with smaller size than the original input image, can be created for segmentation purposes. To refine this map and to obtain a dense pixel-wise output, standard FCNs use deconvolution layers to upsample the coarse map. However, upsampling based on deconvolution increases the number of network parameters and causes loss of detail because of interpolation. On the other hand, dilated convolution is a new technique introduced recently that attempts to capture multi-scale contextual information without increasing the network parameters while keeping the resolution of the prediction maps high. We used both a standard FCN and a dilated convolution based FCN for semantic segmentation of the head sectioned images of the VKH dataset. Quantitative results showed approximately 20% improvement in the segmentation accuracy when using FCNs with dilated convolutions.|['Mohammad Eshghi', 'Holger R. Roth', 'Masahiro Oda', 'Min Suk Chung', 'Kensaku Mori']|['cs.CV']
2017-03-16T23:32:03Z|2017-03-15T06:41:31Z|http://arxiv.org/abs/1703.04960v1|http://arxiv.org/pdf/1703.04960v1|End-to-end Binary Representation Learning via Direct Binary Embedding|Learning binary representation is essential to large-scale computer vision tasks. Most existing algorithms require a separate quantization constraint to learn effective hashing functions. In this work, we present Direct Binary Embedding (DBE), a simple yet very effective algorithm to learn binary representation in an end-to-end fashion. By appending an ingeniously designed DBE layer to the deep convolutional neural network (DCNN), DBE learns binary code directly from the continuous DBE layer activation without quantization error. By employing the deep residual network (ResNet) as DCNN component, DBE captures rich semantics from images. Furthermore, in the effort of handling multilabel images, we design a joint cross entropy loss that includes both softmax cross entropy and weighted binary cross entropy in consideration of the correlation and independence of labels, respectively. Extensive experiments demonstrate the significant superiority of DBE over state-of-the-art methods on tasks of natural object recognition, image retrieval and image annotation.|['Liu Liu', 'Alireza Rahimpour', 'Ali Taalimi', 'Hairong Qi']|['cs.CV', 'cs.IR']
2017-03-16T23:32:07Z|2017-03-15T01:48:30Z|http://arxiv.org/abs/1703.04877v1|http://arxiv.org/pdf/1703.04877v1|Real-time 3D Human Tracking for Mobile Robots with Multisensors|Acquiring the accurate 3-D position of a target person around a robot provides fundamental and valuable information that is applicable to a wide range of robotic tasks, including home service, navigation and entertainment. This paper presents a real-time robotic 3-D human tracking system which combines a monocular camera with an ultrasonic sensor by the extended Kalman filter (EKF). The proposed system consists of three sub-modules: monocular camera sensor tracking model, ultrasonic sensor tracking model and multi-sensor fusion. An improved visual tracking algorithm is presented to provide partial location estimation (2-D). The algorithm is designed to overcome severe occlusions, scale variation, target missing and achieve robust re-detection. The scale accuracy is further enhanced by the estimated 3-D information. An ultrasonic sensor array is employed to provide the range information from the target person to the robot and Gaussian Process Regression is used for partial location estimation (2-D). EKF is adopted to sequentially process multiple, heterogeneous measurements arriving in an asynchronous order from the vision sensor and the ultrasonic sensor separately. In the experiments, the proposed tracking system is tested in both simulation platform and actual mobile robot for various indoor and outdoor scenes. The experimental results show the superior performance of the 3-D tracking system in terms of both the accuracy and robustness.|['Mengmeng Wang', 'Daobilige Su', 'Lei Shi', 'Yong Liu', 'Jaime Valls Miro']|['cs.RO', 'cs.CV']
2017-03-16T23:32:07Z|2017-03-15T01:00:44Z|http://arxiv.org/abs/1703.04861v1|http://arxiv.org/pdf/1703.04861v1|Robust Non-Rigid Registration With Reweighted Dual Sparsities|Non-rigid registration is challenging because it is ill-posed with high degrees of freedom and is thus sensitive to noise and outliers. We propose a robust non-rigid registration method using reweighted sparsities on position and transformation to estimate the deformations between 3-D shapes. We formulate the energy function with dual sparsities on both the data term and the smoothness term, and define the smoothness constraint using local rigidity. The dual-sparsity based non-rigid registration model is enhanced with a reweighting scheme, and solved by transferring the model into some alternating optimized subproblems which have exact solutions and guaranteed convergence. Experimental results on both public datasets and real scanned datasets show that our method outperforms the state-of-the-art methods and is more robust to noise and outliers than conventional non-rigid registration methods.|['Jingyu Yang', 'Kun Li', 'Yu-Kun Lai', 'Daoliang Guo']|['cs.CV', 'cs.CG', 'cs.GR']
2017-03-16T23:32:07Z|2017-03-15T00:52:50Z|http://arxiv.org/abs/1703.04856v1|http://arxiv.org/pdf/1703.04856v1|Source Camera Identification Based On Content-Adaptive Fusion Network|Source camera identification is still a hard task in forensics community, especially for the case of the small query image size. In this paper, we propose a solution to identify the source camera of the small-size images: content-adaptive fusion network. In order to learn better feature representation from the input data, content-adaptive convolutional neural networks(CA-CNN) are constructed. We add a convolutional layer in preprocessing stage. Moreover, with the purpose of capturing more comprehensive information, we parallel three CA-CNNs: CA3-CNN, CA5-CNN, CA7-CNN to get the content-adaptive fusion network. The difference of three CA-CNNs lies in the convolutional kernel size of pre-processing layer. The experimental results show that the proposed method is practicable and satisfactory.|['Pengpeng Yang', 'Wei Zhao', 'Rongrong Ni', 'Yao Zhao']|['cs.CV']
2017-03-16T23:32:07Z|2017-03-15T00:38:01Z|http://arxiv.org/abs/1703.04853v1|http://arxiv.org/pdf/1703.04853v1|Face Recognition using Multi-Modal Low-Rank Dictionary Learning|Face recognition has been widely studied due to its importance in different applications; however, most of the proposed methods fail when face images are occluded or captured under illumination and pose variations. Recently several low-rank dictionary learning methods have been proposed and achieved promising results for noisy observations. While these methods are mostly developed for single-modality scenarios, recent studies demonstrated the advantages of feature fusion from multiple inputs. We propose a multi-modal structured low-rank dictionary learning method for robust face recognition, using raw pixels of face images and their illumination invariant representation. The proposed method learns robust and discriminative representations from contaminated face images, even if there are few training samples with large intra-class variations. Extensive experiments on different datasets validate the superior performance and robustness of our method to severe illumination variations and occlusion.|['Homa Foroughi', 'Moein Shakeri', 'Nilanjan Ray', 'Hong Zhang']|['cs.CV']
2017-03-16T23:32:07Z|2017-03-15T00:15:01Z|http://arxiv.org/abs/1703.04845v1|http://arxiv.org/pdf/1703.04845v1|Skin lesion segmentation based on preprocessing, thresholding and neural   networks|This abstract describes the segmentation system used to participate in the challenge ISIC 2017: Skin Lesion Analysis Towards Melanoma Detection. Several preprocessing techniques have been tested for three color representations (RGB, YCbCr and HSV) of 392 images. Results have been used to choose the better preprocessing for each channel. In each case a neural network is trained to predict the Jaccard Index based on object characteristics. The system includes black frames and reference circle detection algorithms but no special treatment is done for hair removal. Segmentation is performed in two steps first the best channel to be segmented is chosen by selecting the best neural network output. If this output does not predict a Jaccard Index over 0.5 a more aggressive preprocessing is performed using open and close morphological operations and the segmentation of the channel that obtains the best output from the neural networks is selected as the lesion.|['Juana M. Gutiérrez-Arriola', 'Marta Gómez-Álvarez', 'Victor Osma-Ruiz', 'Nicolás Sáenz-Lechón', 'Rubén Fraile']|['cs.CV']
2017-03-16T23:32:07Z|2017-03-14T23:41:45Z|http://arxiv.org/abs/1703.04835v1|http://arxiv.org/pdf/1703.04835v1|A Proximity-Aware Hierarchical Clustering of Faces|"In this paper, we propose an unsupervised face clustering algorithm called ""Proximity-Aware Hierarchical Clustering"" (PAHC) that exploits the local structure of deep representations. In the proposed method, a similarity measure between deep features is computed by evaluating linear SVM margins. SVMs are trained using nearest neighbors of sample data, and thus do not require any external training data. Clusters are then formed by thresholding the similarity scores. We evaluate the clustering performance using three challenging unconstrained face datasets, including Celebrity in Frontal-Profile (CFP), IARPA JANUS Benchmark A (IJB-A), and JANUS Challenge Set 3 (JANUS CS3) datasets. Experimental results demonstrate that the proposed approach can achieve significant improvements over state-of-the-art methods. Moreover, we also show that the proposed clustering algorithm can be applied to curate a set of large-scale and noisy training dataset while maintaining sufficient amount of images and their variations due to nuisance factors. The face verification performance on JANUS CS3 improves significantly by finetuning a DCNN model with the curated MS-Celeb-1M dataset which contains over three million face images."|['Wei-An Lin', 'Jun-Cheng Chen', 'Rama Chellappa']|['cs.CV']
2017-03-16T23:32:07Z|2017-03-14T23:21:26Z|http://arxiv.org/abs/1703.04824v1|http://arxiv.org/pdf/1703.04824v1|In Search of a Dataset for Handwritten Optical Music Recognition:   Introducing MUSCIMA++|Optical Music Recognition (OMR) has long been without an adequate dataset and ground truth for evaluating OMR systems, which has been a major problem for establishing a state of the art in the field. Furthermore, machine learning methods require training data. We analyze how the OMR processing pipeline can be expressed in terms of gradually more complex ground truth, and based on this analysis, we design the MUSCIMA++ dataset of handwritten music notation that addresses musical symbol recognition and notation reconstruction. The MUSCIMA++ dataset version 0.9 consists of 140 pages of handwritten music, with 91255 manually annotated notation symbols and 82261 explicitly marked relationships between symbol pairs. The dataset allows training and evaluating models for symbol classification, symbol localization, and notation graph assembly, both in isolation and jointly. Open-source tools are provided for manipulating the dataset, visualizing the data and further annotation, and the dataset itself is made available under an open license.|['Jan Hajič jr.', 'Pavel Pecina']|['cs.CV', 'I.7.5']
2017-03-16T23:32:07Z|2017-03-14T23:11:04Z|http://arxiv.org/abs/1703.04819v1|http://arxiv.org/pdf/1703.04819v1|RECOD Titans at ISIC Challenge 2017|"This extended abstract describes the participation of RECOD Titans in parts 1 and 3 of the ISIC Challenge 2017 ""Skin Lesion Analysis Towards Melanoma Detection"" (ISBI 2017). Although our team has a long experience with melanoma classification, the ISIC Challenge 2017 was the very first time we worked on skin-lesion segmentation. For part 1 (segmentation), our final submission used four of our models: two trained with all 2000 samples, without a validation split, for 250 and for 500 epochs respectively; and other two trained and validated with two different 1600/400 splits, for 220 epochs. Those four models, individually, achieved between 0.780 and 0.783 official validation scores. Our final submission averaged the output of those four models achieved a score of 0.793. For part 3 (classification), the submitted test run as well as our last official validation run were the result from a meta-model that assembled seven base deep-learning models: three based on Inception-V4 trained on our largest dataset; three based on Inception trained on our smallest dataset; and one based on ResNet-101 trained on our smaller dataset. The results of those component models were stacked in a meta-learning layer based on an SVM trained on the validation set of our largest dataset."|['Afonso Menegola', 'Julia Tavares', 'Michel Fornaciali', 'Lin Tzy Li', 'Sandra Avila', 'Eduardo Valle']|['cs.CV']
2017-03-16T23:32:07Z|2017-03-14T22:21:48Z|http://arxiv.org/abs/1703.04775v1|http://arxiv.org/pdf/1703.04775v1|Discriminate-and-Rectify Encoders: Learning from Image Transformation   Sets|The complexity of a learning task is increased by transformations in the input space that preserve class identity. Visual object recognition for example is affected by changes in viewpoint, scale, illumination or planar transformations. While drastically altering the visual appearance, these changes are orthogonal to recognition and should not be reflected in the representation or feature encoding used for learning. We introduce a framework for weakly supervised learning of image embeddings that are robust to transformations and selective to the class distribution, using sets of transforming examples (orbit sets), deep parametrizations and a novel orbit-based loss. The proposed loss combines a discriminative, contrastive part for orbits with a reconstruction error that learns to rectify orbit transformations. The learned embeddings are evaluated in distance metric-based tasks, such as one-shot classification under geometric transformations, as well as face verification and retrieval under more realistic visual variability. Our results suggest that orbit sets, suitably computed or observed, can be used for efficient, weakly-supervised learning of semantically relevant image embeddings.|['Andrea Tacchetti', 'Stephen Voinea', 'Georgios Evangelopoulos']|['cs.CV', 'cs.LG', 'stat.ML']
2017-03-16T23:32:07Z|2017-03-14T21:06:08Z|http://arxiv.org/abs/1703.04727v1|http://arxiv.org/pdf/1703.04727v1|Tracking Gaze and Visual Focus of Attention of People Involved in Social   Interaction|The visual focus of attention (VFOA) has been recognized as a prominent conversational cue. We are interested in the VFOA tracking of a group of people involved in social interaction. We note that in this case the participants look either at each other or at an object of interest; therefore they don't always face a camera and, consequently, their gazes (and their VFOAs) cannot be based on eye detection and tracking. We propose a method that exploits the correlation between gaze direction and head orientation. Both VFOA and gaze are modeled as latent variables in a Bayesian switching linear dynamic model. The proposed formulation leads to a tractable learning procedure and to an efficient gaze-and-VFOA tracking algorithm. The method is tested and benchmarked using a publicly available dataset that contains typical multi-party human-robot interaction scenarios, and that was recorded with both a motion capture system, and with a camera mounted onto a robot head.|['Benoît Massé', 'Silèye Ba', 'Radu Horaud']|['cs.CV']
2017-03-16T23:32:11Z|2017-03-14T20:23:48Z|http://arxiv.org/abs/1703.04699v1|http://arxiv.org/pdf/1703.04699v1|A fully end-to-end deep learning approach for real-time simultaneous 3D   reconstruction and material recognition|This paper addresses the problem of simultaneous 3D reconstruction and material recognition and segmentation. Enabling robots to recognise different materials (concrete, metal etc.) in a scene is important for many tasks, e.g. robotic interventions in nuclear decommissioning. Previous work on 3D semantic reconstruction has predominantly focused on recognition of everyday domestic objects (tables, chairs etc.), whereas previous work on material recognition has largely been confined to single 2D images without any 3D reconstruction. Meanwhile, most 3D semantic reconstruction methods rely on computationally expensive post-processing, using Fully-Connected Conditional Random Fields (CRFs), to achieve consistent segmentations. In contrast, we propose a deep learning method which performs 3D reconstruction while simultaneously recognising different types of materials and labelling them at the pixel level. Unlike previous methods, we propose a fully end-to-end approach, which does not require hand-crafted features or CRF post-processing. Instead, we use only learned features, and the CRF segmentation constraints are incorporated inside the fully end-to-end learned system. We present the results of experiments, in which we trained our system to perform real-time 3D semantic reconstruction for 23 different materials in a real-world application. The run-time performance of the system can be boosted to around 10Hz, using a conventional GPU, which is enough to achieve real-time semantic reconstruction using a 30fps RGB-D camera. To the best of our knowledge, this work is the first real-time end-to-end system for simultaneous 3D reconstruction and material recognition.|['Cheng Zhao', 'Li Sun', 'Rustam Stolkin']|['cs.CV']
2017-03-16T23:32:11Z|2017-03-14T18:58:46Z|http://arxiv.org/abs/1703.04670v1|http://arxiv.org/pdf/1703.04670v1|6-DoF Object Pose from Semantic Keypoints|This paper presents a novel approach to estimating the continuous six degree of freedom (6-DoF) pose (3D translation and rotation) of an object from a single RGB image. The approach combines semantic keypoints predicted by a convolutional network (convnet) with a deformable shape model. Unlike prior work, we are agnostic to whether the object is textured or textureless, as the convnet learns the optimal representation from the available training image data. Furthermore, the approach can be applied to instance- and class-based pose recovery. Empirically, we show that the proposed approach can accurately recover the 6-DoF object pose for both instance- and class-based scenarios with a cluttered background. For class-based object pose estimation, state-of-the-art accuracy is shown on the large-scale PASCAL3D+ dataset.|['Georgios Pavlakos', 'Xiaowei Zhou', 'Aaron Chan', 'Konstantinos G. Derpanis', 'Kostas Daniilidis']|['cs.CV', 'cs.RO']
2017-03-16T23:32:11Z|2017-03-14T18:51:18Z|http://arxiv.org/abs/1703.04665v1|http://arxiv.org/pdf/1703.04665v1|Geometry-Based Region Proposals for Real-Time Robot Detection of   Tabletop Objects|We present a novel object detection pipeline for localization and recognition in three dimensional environments. Our approach makes use of an RGB-D sensor and combines state-of-the-art techniques from the robotics and computer vision communities to create a robust, real-time detection system. We focus specifically on solving the object detection problem for tabletop scenes, a common environment for assistive manipulators. Our detection pipeline locates objects in a point cloud representation of the scene. These clusters are subsequently used to compute a bounding box around each object in the RGB space. Each defined patch is then fed into a Convolutional Neural Network (CNN) for object recognition. We also demonstrate that our region proposal method can be used to develop novel datasets that are both large and diverse enough to train deep learning models, and easy enough to collect that end-users can develop their own datasets. Lastly, we validate the resulting system through an extensive analysis of the accuracy and run-time of the full pipeline.|['Alexander Broad', 'Brenna Argall']|['cs.RO', 'cs.CV']
2017-03-16T23:32:11Z|2017-03-14T18:36:16Z|http://arxiv.org/abs/1703.04653v1|http://arxiv.org/pdf/1703.04653v1|A Framework for Dynamic Image Sampling Based on Supervised Learning   (SLADS)|Sparse sampling schemes have the potential to dramatically reduce image acquisition time while simultaneously reducing radiation damage to samples. However, for a sparse sampling scheme to be useful it is important that we are able to reconstruct the underlying object with sufficient clarity using the sparse measurements. In dynamic sampling, each new measurement location is selected based on information obtained from previous measurements. Therefore, dynamic sampling schemes have the potential to dramatically reduce the number of measurements needed for high fidelity reconstructions. However, most existing dynamic sampling methods for point-wise measurement acquisition tend to be computationally expensive and are therefore too slow for practical applications.   In this paper, we present a framework for dynamic sampling based on machine learning techniques, which we call a supervised learning approach for dynamic sampling (SLADS). In each step of SLADS, the objective is to find the pixel that maximizes the expected reduction in distortion (ERD) given previous measurements. SLADS is fast because we use a simple regression function to compute the ERD, and it is accurate because the regression function is trained using data sets that are representative of the specific application. In addition, we introduce a method to terminate dynamic sampling at a desired level of distortion, and we extended the SLADS methodology to sample groups of pixels at each step. Finally, we present results on computationally-generated synthetic data and experimentally-collected data to demonstrate a dramatic improvement over state-of-the-art static sampling methods.|['G. M. Dilshan P. Godaliyadda', 'Dong Hye Ye', 'Michael D. Uchic', 'Michael A. Groeber', 'Gregery T. Buzzard', 'Charles A. Bouman']|['cs.CV']
2017-03-16T23:32:11Z|2017-03-14T18:08:49Z|http://arxiv.org/abs/1703.04636v1|http://arxiv.org/pdf/1703.04636v1|A PatchMatch-based Dense-field Algorithm for Video Copy-Move Detection   and Localization|We propose a new algorithm for the reliable detection and localization of video copy-move forgeries. Discovering well crafted video copy-moves may be very difficult, especially when some uniform background is copied to occlude foreground objects. To reliably detect both additive and occlusive copy-moves we use a dense-field approach, with invariant features that guarantee robustness to several post-processing operations. To limit complexity, a suitable video-oriented version of PatchMatch is used, with a multiresolution search strategy, and a focus on volumes of interest. Performance assessment relies on a new dataset, designed ad hoc, with realistic copy-moves and a wide variety of challenging situations. Experimental results show the proposed method to detect and localize video copy-moves with good accuracy even in adverse conditions.|"[""Luca D'Amiano"", 'Davide Cozzolino', 'Giovanni Poggi', 'Luisa Verdoliva']"|['cs.CV']
2017-03-16T23:32:11Z|2017-03-14T17:42:13Z|http://arxiv.org/abs/1703.04615v1|http://arxiv.org/pdf/1703.04615v1|Recasting Residual-based Local Descriptors as Convolutional Neural   Networks: an Application to Image Forgery Detection|Local descriptors based on the image noise residual have proven extremely effective for a number of forensic applications, like forgery detection and localization. Nonetheless, motivated by promising results in computer vision, the focus of the research community is now shifting on deep learning. In this paper we show that a class of residual-based descriptors can be actually regarded as a simple constrained convolutional neural network (CNN). Then, by relaxing the constraints, and fine-tuning the net on a relatively small training set, we obtain a significant performance improvement with respect to the conventional detector.|['Davide Cozzolino', 'Giovanni Poggi', 'Luisa Verdoliva']|['cs.CV']
2017-03-16T23:32:11Z|2017-03-14T17:37:53Z|http://arxiv.org/abs/1703.04611v1|http://arxiv.org/pdf/1703.04611v1|Subspace Learning in The Presence of Sparse Structured Outliers and   Noise|Subspace learning is an important problem, which has many applications in image and video processing. It can be used to find a low-dimensional representation of signals and images. But in many applications, the desired signal is heavily distorted by outliers and noise, which negatively affect the learned subspace. In this work, we present a novel algorithm for learning a subspace for signal representation, in the presence of structured outliers and noise. The proposed algorithm tries to jointly detect the outliers and learn the subspace for images. We present an alternating optimization algorithm for solving this problem, which iterates between learning the subspace and finding the outliers. This algorithm has been trained on a large number of image patches, and the learned subspace is used for image segmentation, and is shown to achieve better segmentation results than prior methods, including least absolute deviation fitting, k-means clustering based segmentation in DjVu, and shape primitive extraction and coding algorithm.|['Shervin Minaee', 'Yao Wang']|['cs.CV']
2017-03-16T23:32:11Z|2017-03-14T17:16:23Z|http://arxiv.org/abs/1703.04590v1|http://arxiv.org/pdf/1703.04590v1|Learning Background-Aware Correlation Filters for Visual Tracking|"Correlation Filters (CFs) have recently demonstrated excellent performance in terms of rapidly tracking objects under challenging photometric and geometric variations. The strength of the approach comes from its ability to efficiently learn - ""on the fly"" - how the object is changing over time. A fundamental drawback to CFs, however, is that the background of the object is not be modelled over time which can result in suboptimal results. In this paper we propose a Background-Aware CF that can model how both the foreground and background of the object varies over time. Our approach, like conventional CFs, is extremely computationally efficient - and extensive experiments over multiple tracking benchmarks demonstrate the superior accuracy and real-time performance of our method compared to the state-of-the-art trackers including those based on a deep learning paradigm."|['Hamed Kiani Galoogahi', 'Ashton Fagg', 'Simon Lucey']|['cs.CV']
2017-03-16T23:32:11Z|2017-03-14T00:54:18Z|http://arxiv.org/abs/1703.04559v1|http://arxiv.org/pdf/1703.04559v1|Fully Convolutional Networks to Detect Clinical Dermoscopic Features|We use a pretrained fully convolutional neural network to detect clinical dermoscopic features from dermoscopy skin lesion images. We reformulate the superpixel classification task as an image segmentation problem, and extend a neural network architecture originally designed for image classification to detect dermoscopic features. Specifically, we interpolate the feature maps from several layers in the network to match the size of the input, concatenate the resized feature maps, and train the network to minimize a smoothed negative F1 score. Over the public validation leaderboard of the 2017 ISIC/ISBI Lesion Dermoscopic Feature Extraction Challenge, our approach achieves 89.3% AUROC, the highest averaged score when compared to the other two entries. Results over the private test leaderboard are still to be announced.|['Jeremy Kawahara', 'Ghassan Hamarneh']|['cs.CV']
2017-03-16T23:32:11Z|2017-03-13T15:41:36Z|http://arxiv.org/abs/1703.04454v1|http://arxiv.org/pdf/1703.04454v1|Detailed, accurate, human shape estimation from clothed 3D scan   sequences|We address the problem of estimating human body shape from 3D scans over time. Reliable estimation of 3D body shape is necessary for many applications including virtual try-on, health monitoring, and avatar creation for virtual reality. Scanning bodies in minimal clothing, however, presents a practical barrier to these applications. We address this problem by estimating body shape under clothing from a sequence of 3D scans. Previous methods that have exploited statistical models of body shape produce overly smooth shapes lacking personalized details. In this paper we contribute a new approach to recover not only an approximate shape of the person, but also their detailed shape. Our approach allows the estimated shape to deviate from a parametric model to fit the 3D scans. We demonstrate the method using high quality 4D data as well as sequences of visual hulls extracted from multi-view images. We also make available a new high quality 4D dataset that enables quantitative evaluation. Our method outperforms the previous state of the art, both qualitatively and quantitatively.|['Chao Zhang', 'Sergi Pujades', 'Michael Black', 'Gerard Pons-Moll']|['cs.CV']
2017-03-16T23:32:18Z|2017-03-13T15:29:32Z|http://arxiv.org/abs/1703.04446v1|http://arxiv.org/pdf/1703.04446v1|A Lagrangian Gauss-Newton-Krylov Solver for Mass- and   Intensity-Preserving Diffeomorphic Image Registration|We present an efficient solver for diffeomorphic image registration problems in the framework of Large Deformations Diffeomorphic Metric Mappings (LDDMM). We use an optimal control formulation, in which the velocity field of a hyperbolic PDE needs to be found such that the distance between the final state of the system (the transformed/transported template image) and the observation (the reference image) is minimized. Our solver supports both stationary and non-stationary (i.e., transient or time-dependent) velocity fields. As transformation models, we consider both the transport equation (assuming intensities are preserved during the deformation) and the continuity equation (assuming mass-preservation).   We consider the reduced form of the optimal control problem and solve the resulting unconstrained optimization problem using a discretize-then-optimize approach. A key contribution is the elimination of the PDE constraint using a Lagrangian hyperbolic PDE solver. Lagrangian methods rely on the concept of characteristic curves that we approximate here using a fourth-order Runge-Kutta method. We also present an efficient algorithm for computing the derivatives of final state of the system with respect to the velocity field. This allows us to use fast Gauss-Newton based methods. We present quickly converging iterative linear solvers using spectral preconditioners that render the overall optimization efficient and scalable. Our method is embedded into the image registration framework FAIR and, thus, supports the most commonly used similarity measures and regularization functionals. We demonstrate the potential of our new approach using several synthetic and real world test problems with up to 14.7 million degrees of freedom.|['Andreas Mang', 'Lars Ruthotto']|['cs.CV', 'cs.CE', '68U10, 49J20, 35Q93, 65M32, 76D55, 65K10']
2017-03-16T23:32:18Z|2017-03-13T14:40:08Z|http://arxiv.org/abs/1703.04421v1|http://arxiv.org/pdf/1703.04421v1|Guetzli: Perceptually Guided JPEG Encoder|Guetzli is a new JPEG encoder that aims to produce visually indistinguishable images at a lower bit-rate than other common JPEG encoders. It optimizes both the JPEG global quantization tables and the DCT coefficient values in each JPEG block using a closed-loop optimizer. Guetzli uses Butteraugli, our perceptual distance metric, as the source of feedback in its optimization process. We reach a 29-45% reduction in data size for a given perceptual distance, according to Butteraugli, in comparison to other compressors we tried. Guetzli's computation is currently extremely slow, which limits its applicability to compressing static content and serving as a proof- of-concept that we can achieve significant reductions in size by combining advanced psychovisual models with lossy compression techniques.|['Jyrki Alakuijala', 'Robert Obryk', 'Ostap Stoliarchuk', 'Zoltan Szabadka', 'Lode Vandevenne', 'Jan Wassenberg']|['cs.CV', 'cs.HC']
2017-03-16T23:32:18Z|2017-03-13T14:36:21Z|http://arxiv.org/abs/1703.04418v1|http://arxiv.org/pdf/1703.04418v1|Improving LBP and its variants using anisotropic diffusion|The main purpose of this paper is to propose a new preprocessing step in order to improve local feature descriptors and texture classification. Preprocessing is implemented by using transformations which help highlight salient features that play a significant role in texture recognition. We evaluate and compare four different competing methods: three different anisotropic diffusion methods including the classical anisotropic Perona-Malik diffusion and two subsequent regularizations of it and the application of a Gaussian kernel, which is the classical multiscale approach in texture analysis. The combination of the transformed images and the original ones are analyzed. The results show that the use of the preprocessing step does lead to improved texture recognition.|['Mariane B. Neiva', 'Patrick Guidotti', 'Odemir M. Bruno']|['cs.CV']
2017-03-16T23:32:18Z|2017-03-13T14:33:47Z|http://arxiv.org/abs/1703.04416v1|http://arxiv.org/pdf/1703.04416v1|Users prefer Guetzli JPEG over same-sized libjpeg|We report on pairwise comparisons by human raters of JPEG images from libjpeg and our new Guetzli encoder. Although both files are size-matched, 75% of ratings are in favor of Guetzli. This implies the Butteraugli psychovisual image similarity metric which guides Guetzli is reasonably close to human perception at high quality levels. We provide access to the raw ratings and source images for further analysis and study.|['Jyrki Alakuijala', 'Robert Obryk', 'Zoltan Szabadka', 'Jan Wassenberg']|['cs.CV', 'cs.HC']
2017-03-16T23:32:18Z|2017-03-13T13:53:19Z|http://arxiv.org/abs/1703.04394v1|http://arxiv.org/pdf/1703.04394v1|Zero-Shot Learning - The Good, the Bad and the Ugly|Due to the importance of zero-shot learning, the number of proposed approaches has increased steadily recently. We argue that it is time to take a step back and to analyze the status quo of the area. The purpose of this paper is three-fold. First, given the fact that there is no agreed upon zero-shot learning benchmark, we first define a new benchmark by unifying both the evaluation protocols and data splits. This is an important contribution as published results are often not comparable and sometimes even flawed due to, e.g. pre-training on zero-shot test classes. Second, we compare and analyze a significant number of the state-of-the-art methods in depth, both in the classic zero-shot setting but also in the more realistic generalized zero-shot setting. Finally, we discuss limitations of the current status of the area which can be taken as a basis for advancing it.|['Yongqin Xian', 'Bernt Schiele', 'Zeynep Akata']|['cs.CV']
2017-03-16T23:32:18Z|2017-03-13T12:56:00Z|http://arxiv.org/abs/1703.04364v1|http://arxiv.org/pdf/1703.04364v1|Deep Learning for Skin Lesion Classification|Melanoma, a malignant form of skin cancer is very threatening to life. Diagnosis of melanoma at an earlier stage is highly needed as it has a very high cure rate. Benign and malignant forms of skin cancer can be detected by analyzing the lesions present on the surface of the skin using dermoscopic images. In this work, an automated skin lesion detection system has been developed which learns the representation of the image using Google's pretrained CNN model known as Inception-v3 \cite{cnn}. After obtaining the representation vector for our input dermoscopic images we have trained two layer feed forward neural network to classify the images as malignant or benign. The system also classifies the images based on the cause of the cancer either due to melanocytic or non-melanocytic cells using a different neural network. These classification tasks are part of the challenge organized by International Skin Imaging Collaboration (ISIC) 2017. Our system learns to classify the images based on the model built using the training images given in the challenge and the experimental results were evaluated using validation and test sets. Our system has achieved an overall accuracy of 65.8\% for the validation set.|['P. Mirunalini', 'Aravindan Chandrabose', 'Vignesh Gokul', 'S. M. Jaisakthi']|['cs.CV']
2017-03-16T23:32:18Z|2017-03-13T12:49:20Z|http://arxiv.org/abs/1703.04363v1|http://arxiv.org/pdf/1703.04363v1|Deep Value Networks Learn to Evaluate and Iteratively Refine Structured   Outputs|We approach structured output prediction by learning a deep value network (DVN) that evaluates different output structures for a given input. For example, when applied to image segmentation, the value network takes an image and a segmentation mask as inputs and predicts a scalar score evaluating the mask quality and its correspondence with the image. Once the value network is optimized, at inference, it finds output structures that maximize the score of the value net via gradient descent on continuous relaxations of structured outputs. Thus DVN takes advantage of the joint modeling of the inputs and outputs. Our framework applies to a wide range of structured output prediction problems. We conduct experiments on multi-label classification based on text data and on image segmentation problems. DVN outperforms several strong baselines and the state-of-the-art results on these benchmarks. In addition, on image segmentation, the proposed deep value network learns complex shape priors and effectively combines image information with the prior to obtain competitive segmentation results.|['Michael Gygli', 'Mohammad Norouzi', 'Anelia Angelova']|['cs.LG', 'cs.AI', 'cs.CV']
2017-03-16T23:32:18Z|2017-03-13T11:55:16Z|http://arxiv.org/abs/1703.04347v1|http://arxiv.org/pdf/1703.04347v1|A Localisation-Segmentation Approach for Multi-label Annotation of   Lumbar Vertebrae using Deep Nets|Multi-class segmentation of vertebrae is a non-trivial task mainly due to the high correlation in the appearance of adjacent vertebrae. Hence, such a task calls for the consideration of both global and local context. Based on this motivation, we propose a two-staged approach that, given a computed tomography dataset of the spine, segments the five lumbar vertebrae and simultaneously labels them. The first stage employs a multi-layered perceptron performing non-linear regression for locating the lumbar region using the global context. The second stage, comprised of a fully-convolutional deep network, exploits the local context in the localised lumbar region to segment and label the lumbar vertebrae in one go. Aided with practical data augmentation for training, our approach is highly generalisable, capable of successfully segmenting both healthy and abnormal vertebrae (fractured and scoliotic spines). We consistently achieve an average Dice coefficient of over 90 percent on a publicly available dataset of the xVertSeg segmentation challenge of MICCAI 2016. This is particularly noteworthy because the xVertSeg dataset is beset with severe deformities in the form of vertebral fractures and scoliosis.|['Anjany Sekuboyina', 'Alexander Valentinitsch', 'Jan S. Kirschke', 'Bjoern H. Menze']|['cs.CV']
2017-03-16T23:32:18Z|2017-03-13T11:19:00Z|http://arxiv.org/abs/1703.04332v1|http://arxiv.org/pdf/1703.04332v1|What You Expect is NOT What You Get! Questioning   Reconstruction/Classification Correlation of Stacked Convolutional   Auto-Encoder Features|In this paper, we thoroughly investigate the quality of features produced by deep neural network architectures obtained by stacking and convolving Auto-Encoders. In particular, we are interested into the relation of their reconstruction score with their performance on document layout analysis. When using Auto-Encoders, intuitively one could assume that features which are good for reconstruction will also lead to high classification accuracies. However, we prove that this is not always the case. We examine the reconstruction score, training error and the results obtained if we were to use the same features for both input reconstruction and a classification task. We show that the reconstruction score is not a good metric because it is biased by the decoder quality. Furthermore, experimental results suggest that there is no correlation between the reconstruction score and the quality of features for a classification task and that given the network size and configuration it is not possible to make assumptions on its training error magnitude. Therefore we conclude that both, reconstruction score and training error should not be used jointly to evaluate the quality of the features produced by a Stacked Convolutional Auto-Encoders for a classification task. Consequently one should independently investigate the network classification abilities directly.|['Michele Alberti', 'Mathias Seuret', 'Rolf Ingold', 'Marcus Liwicki']|['cs.CV', 'I.2.6; I.5.2; I.7.5']
2017-03-16T23:32:18Z|2017-03-13T10:00:52Z|http://arxiv.org/abs/1703.04309v1|http://arxiv.org/pdf/1703.04309v1|End-to-End Learning of Geometry and Context for Deep Stereo Regression|We propose a novel deep learning architecture for regressing disparity from a rectified pair of stereo images. We leverage knowledge of the problem's geometry to form a cost volume using deep feature representations. We learn to incorporate contextual information using 3-D convolutions over this volume. Disparity values are regressed from the cost volume using a proposed differentiable soft argmin operation, which allows us to train our method end-to-end to sub-pixel accuracy without any additional post-processing or regularization. We evaluate our method on the Scene Flow and KITTI datasets and on KITTI we set a new state-of-the-art benchmark, while being significantly faster than competing approaches.|['Alex Kendall', 'Hayk Martirosyan', 'Saumitro Dasgupta', 'Peter Henry', 'Ryan Kennedy', 'Abraham Bachrach', 'Adam Bry']|['cs.CV', 'cs.NE']
2017-03-16T23:32:22Z|2017-03-13T09:30:34Z|http://arxiv.org/abs/1703.04301v1|http://arxiv.org/pdf/1703.04301v1|Automatic Skin Lesion Segmentation using Semi-supervised Learning   Technique|Skin cancer is the most common of all cancers and each year million cases of skin cancer are treated. Treating and curing skin cancer is easy, if it is diagnosed and treated at an early stage. In this work we propose an automatic technique for skin lesion segmentation in dermoscopic images which helps in classifying the skin cancer types. The proposed method comprises of two major phases (1) preprocessing and (2) segmentation using semi-supervised learning algorithm. In the preprocessing phase noise are removed using filtering technique and in the segmentation phase skin lesions are segmented based on clustering technique. K-means clustering algorithm is used to cluster the preprocessed images and skin lesions are filtered from these clusters based on the color feature. Color of the skin lesions are learned from the training images using histograms calculations in RGB color space. The training images were downloaded from the ISIC 2017 challenge website and the experimental results were evaluated using validation and test sets.|['S. M. Jaisakthi', 'Aravindan Chandrabose', 'P. Mirunalini']|['cs.CV']
2017-03-16T23:32:22Z|2017-03-13T06:08:51Z|http://arxiv.org/abs/1703.04264v1|http://arxiv.org/pdf/1703.04264v1|Poisson multi-Bernoulli mixture filter: direct derivation and   implementation|We provide a derivation of the Poisson multi-Bernoulli mixture (PMBM) filter for multi-target tracking with the standard point target measurements without using probability generating functionals or functional derivatives. We also establish the connection with the \delta-generalised labelled multi-Bernoulli (\delta-GLMB) filter, showing that a \delta-GLMB density represents a multi-Bernoulli mixture with labelled targets so it can be seen as a special case of PMBM. In addition, we propose an implementation for linear/Gaussian dynamic and measurement models and how to efficiently obtain typical estimators in the literature from the PMBM. The PMBM filter is shown to outperform other filters in the literature in a challenging scenario|['Ángel F. García-Fernández', 'Jason L. Williams', 'Karl Granström', 'Lennart Svensson']|['cs.CV', 'stat.ME']
2017-03-16T23:32:22Z|2017-03-13T04:34:12Z|http://arxiv.org/abs/1703.04244v1|http://arxiv.org/pdf/1703.04244v1|GUN: Gradual Upsampling Network for single image super-resolution|In this paper, we propose an efficient super-resolution (SR) method based on deep convolutional neural network (CNN), namely gradual upsampling network (GUN). Recent CNN based SR methods either preliminarily magnify the low resolution (LR) input to high resolution (HR) and then reconstruct the HR input, or directly reconstruct the LR input and then recover the HR result at the last layer. The proposed GUN utilizes a gradual process instead of these two kinds of frameworks. The GUN consists of an input layer, multistep upsampling and convolutional layers, and an output layer. By means of the gradual process, the proposed network can simplify the difficult direct SR problem to multistep easier upsampling tasks with very small magnification factor in each step. Furthermore, a gradual training strategy is presented for the GUN. In the proposed training process, an initial network can be easily trained with edge-like samples, and then the weights are gradually tuned with more complex samples. The GUN can recover fine and vivid results, and is easy to be trained. The experimental results on several image sets demonstrate the effectiveness of the proposed network.|['Yang Zhao', 'Ronggang Wang', 'Weisheng Dong', 'Wei Jia', 'Jianchao Yang', 'Xiaoping Liu', 'Wen Gao']|['cs.CV']
2017-03-16T23:32:22Z|2017-03-12T23:32:18Z|http://arxiv.org/abs/1703.04197v1|http://arxiv.org/pdf/1703.04197v1|Automatic Skin Lesion Analysis using Large-scale Dermoscopy Images and   Deep Residual Networks|Malignant melanoma has one of the most rapidly increasing incidences in the world and has a considerable mortality rate. Early diagnosis is particularly important since melanoma can be cured with prompt excision. Dermoscopy images play an important role in the non-invasive early detection of melanoma [1]. However, melanoma detection using human vision alone can be subjective, inaccurate and poorly reproducible even among experienced dermatologists. This is attributed to the challenges in interpreting images with diverse characteristics including lesions of varying sizes and shapes, lesions that may have fuzzy boundaries, different skin colors and the presence of hair [2]. Therefore, the automatic analysis of dermoscopy images is a valuable aid for clinical decision making and for image-based diagnosis to identify diseases such as melanoma [1-4]. Deep residual networks (ResNets) has achieved state-of-the-art results in image classification and detection related problems [5-8]. In this ISIC 2017 skin lesion analysis challenge [9], we propose to exploit the deep ResNets for robust visual features learning and representations.|['Lei Bi', 'Jinman Kim', 'Euijoon Ahn', 'Dagan Feng']|['cs.CV']
2017-03-16T23:32:22Z|2017-03-12T21:13:28Z|http://arxiv.org/abs/1703.04706v1|http://arxiv.org/pdf/1703.04706v1|Tree Memory Networks for Modelling Long-term Temporal Dependencies|In the domain of sequence modelling, Recurrent Neural Networks (RNN) have been capable of achieving impressive results in a variety of application areas including visual question answering, part-of-speech tagging and machine translation. However this success in modelling short term dependencies has not successfully transitioned to application areas such as trajectory prediction, which require capturing both short term and long term relationships. In this paper, we propose a Tree Memory Network (TMN) for modelling long term and short term relationships in sequence-to-sequence mapping problems. The proposed network architecture is composed of an input module, controller and a memory module. In contrast to related literature, which models the memory as a sequence of historical states, we model the memory as a recursive tree structure. This structure more effectively captures temporal dependencies across both short term and long term sequences using its hierarchical structure. We demonstrate the effectiveness and flexibility of the proposed TMN in two practical problems, aircraft trajectory modelling and pedestrian trajectory modelling in a surveillance setting, and in both cases we outperform the current state-of-the-art. Furthermore, we perform an in depth analysis on the evolution of the memory module content over time and provide visual evidence on how the proposed TMN is able to map both long term and short term relationships efficiently via a hierarchical structure.|['Tharindu Fernando', 'Simon Denman', 'Aaron McFadyen', 'Sridha Sridharan', 'Clinton Fookes']|['cs.LG', 'cs.CV', 'cs.NE']
2017-03-16T23:32:22Z|2017-03-12T15:27:23Z|http://arxiv.org/abs/1703.04135v1|http://arxiv.org/pdf/1703.04135v1|Hardware-Driven Nonlinear Activation for Stochastic Computing Based Deep   Convolutional Neural Networks|Recently, Deep Convolutional Neural Networks (DCNNs) have made unprecedented progress, achieving the accuracy close to, or even better than human-level perception in various tasks. There is a timely need to map the latest software DCNNs to application-specific hardware, in order to achieve orders of magnitude improvement in performance, energy efficiency and compactness. Stochastic Computing (SC), as a low-cost alternative to the conventional binary computing paradigm, has the potential to enable massively parallel and highly scalable hardware implementation of DCNNs. One major challenge in SC based DCNNs is designing accurate nonlinear activation functions, which have a significant impact on the network-level accuracy but cannot be implemented accurately by existing SC computing blocks. In this paper, we design and optimize SC based neurons, and we propose highly accurate activation designs for the three most frequently used activation functions in software DCNNs, i.e, hyperbolic tangent, logistic, and rectified linear units. Experimental results on LeNet-5 using MNIST dataset demonstrate that compared with a binary ASIC hardware DCNN, the DCNN with the proposed SC neurons can achieve up to 61X, 151X, and 2X improvement in terms of area, power, and energy, respectively, at the cost of small precision degradation.In addition, the SC approach achieves up to 21X and 41X of the area, 41X and 72X of the power, and 198200X and 96443X of the energy, compared with CPU and GPU approaches, respectively, while the error is increased by less than 3.07%. ReLU activation is suggested for future SC based DCNNs considering its superior performance under a small bit stream length.|['Ji Li', 'Zihao Yuan', 'Zhe Li', 'Caiwen Ding', 'Ao Ren', 'Qinru Qiu', 'Jeffrey Draper', 'Yanzhi Wang']|['cs.CV']
2017-03-16T23:32:22Z|2017-03-12T12:41:16Z|http://arxiv.org/abs/1703.04111v1|http://arxiv.org/pdf/1703.04111v1|Co-occurrence Filter|Co-occurrence Filter (CoF) is a boundary preserving filter. It is based on the Bilateral Filter (BF) but instead of using a Gaussian on the range values to preserve edges it relies on a co-occurrence matrix. Pixel values that co-occur frequently in the image (i.e., inside textured regions) will have a high weight in the co-occurrence matrix. This, in turn, means that such pixel pairs will be averaged and hence smoothed, regardless of their intensity differences. On the other hand, pixel values that rarely co-occur (i.e., across texture boundaries) will have a low weight in the co-occurrence matrix. As a result, they will not be averaged and the boundary between them will be preserved. The CoF therefore extends the BF to deal with boundaries, not just edges. It learns co-occurrences directly from the image. We can achieve various filtering results by directing it to learn the co-occurrence matrix from a part of the image, or a different image. We give the definition of the filter, discuss how to use it with color images and show several use cases.|['Roy J Jevnisek', 'Shai Avidan']|['cs.CV']
2017-03-16T23:32:22Z|2017-03-15T08:32:06Z|http://arxiv.org/abs/1703.04105v2|http://arxiv.org/pdf/1703.04105v2|Combining Residual Networks with LSTMs for Lipreading|We propose an end-to-end deep learning architecture for word-level visual speech recognition. The system is a combination of spatiotemporal convolutional, residual and bidirectional Long Short-Term Memory networks. We trained and evaluated it on the Lipreading In-The-Wild benchmark, a challenging database of 500-size vocabulary consisting of video excerpts from BBC TV broadcasts. The proposed network attains word accuracy equal to 83.0%, yielding 6.8% absolute improvement over the current state-of-the-art.|['Themos Stafylakis', 'Georgios Tzimiropoulos']|['cs.CV']
2017-03-16T23:32:22Z|2017-03-12T11:39:41Z|http://arxiv.org/abs/1703.04103v1|http://arxiv.org/pdf/1703.04103v1|Detection of Human Rights Violations in Images: Can Convolutional Neural   Networks help?|After setting the performance benchmarks for image, video, speech and audio processing, deep convolutional networks have been core to the greatest advances in image recognition tasks in recent times. This raises the question of whether there are any benefit in targeting these remarkable deep architectures with the unattempted task of recognising human rights violations through digital images. Under this perspective, we introduce a new, well-sampled human rights-centric dataset called Human Rights Understanding (HRUN). We conduct a rigorous evaluation on a common ground by combining this dataset with different state-of-the-art deep convolutional architectures in order to achieve recognition of human rights violations. Experimental results on the HRUN dataset have shown that the best performing CNN architectures can achieve up to 88.10\% mean average precision. Additionally, our experiments demonstrate that increasing the size of the training samples is crucial for achieving an improvement on mean average precision principally when utilising very deep networks.|['Grigorios Kalliatakis', 'Shoaib Ehsan', 'Maria Fasli', 'Ales Leonardis', 'Juergen Gall', 'Klaus D. McDonald-Maier']|['cs.CV']
2017-03-16T23:32:22Z|2017-03-12T11:29:00Z|http://arxiv.org/abs/1703.04101v1|http://arxiv.org/pdf/1703.04101v1|Evaluating Deep Convolutional Neural Networks for Material   Classification|Determining the material category of a surface from an image is a demanding task in perception that is drawing increasing attention. Following the recent remarkable results achieved for image classification and object detection utilising Convolutional Neural Networks (CNNs), we empirically study material classification of everyday objects employing these techniques. More specifically, we conduct a rigorous evaluation of how state-of-the art CNN architectures compare on a common ground over widely used material databases. Experimental results on three challenging material databases show that the best performing CNN architectures can achieve up to 94.99\% mean average precision when classifying materials.|['Grigorios Kalliatakis', 'Georgios Stamatiadis', 'Shoaib Ehsan', 'Ales Leonardis', 'Juergen Gall', 'Anca Sticlaru', 'Klaus D. McDonald-Maier']|['cs.CV']
2017-03-16T23:32:26Z|2017-03-12T10:38:10Z|http://arxiv.org/abs/1703.04096v1|http://arxiv.org/pdf/1703.04096v1|Improving Interpretability of Deep Neural Networks with Semantic   Information|Interpretability of deep neural networks (DNNs) is essential since it enables users to understand the overall strengths and weaknesses of the models, conveys an understanding of how the models will behave in the future, and how to diagnose and correct potential problems. However, it is challenging to reason about what a DNN actually does due to its opaque or black-box nature. To address this issue, we propose a novel technique to improve the interpretability of DNNs by leveraging the rich semantic information embedded in human descriptions. By concentrating on the video captioning task, we first extract a set of semantically meaningful topics from the human descriptions that cover a wide range of visual concepts, and integrate them into the model with an interpretive loss. We then propose a prediction difference maximization algorithm to interpret the learned features of each neuron. Experimental results demonstrate its effectiveness in video captioning using the interpretable features, which can also be transferred to video action recognition. By clearly understanding the learned features, users can easily revise false predictions via a human-in-the-loop procedure.|['Yinpeng Dong', 'Hang Su', 'Jun Zhu', 'Bo Zhang']|['cs.CV']
2017-03-16T23:32:26Z|2017-03-12T09:47:51Z|http://arxiv.org/abs/1703.04088v1|http://arxiv.org/pdf/1703.04088v1|Local Patch Classification Based Framework for Single Image   Super-Resolution|Recent learning-based super-resolution (SR) methods often focus on the dictionary learning or network training. In this paper, we detailedly discuss a new SR framework based on local classification instead of traditional dictionary learning. The proposed efficient and extendible SR framework is named as local patch classification (LPC) based framework. The LPC framework consists of a learning stage and a reconstructing stage. In the learning stage, image patches are classified into different classes by means of the proposed local patch encoding (LPE), and then a projection matrix is computed for each class by utilizing a simple constraint. In the reconstructing stage, an input LR patch can be simply reconstructed by computing its LPE code and then multiplying corresponding projection matrix. Furthermore, we establish the relationship between the proposed method and the anchored neighborhood regression based methods; and we also analyze the extendibility of the proposed framework. The experimental results on several image sets demonstrate the effectiveness of the proposed framework.|['Yang Zhao', 'Ronggang Wang', 'Wei Jia', 'Jianchao Yang', 'Wenmin Wang', 'Wen Gao']|['cs.CV']
2017-03-16T23:32:26Z|2017-03-12T07:21:50Z|http://arxiv.org/abs/1703.04079v1|http://arxiv.org/pdf/1703.04079v1|SurfNet: Generating 3D shape surfaces using deep residual networks|3D shape models are naturally parameterized using vertices and faces, \ie, composed of polygons forming a surface. However, current 3D learning paradigms for predictive and generative tasks using convolutional neural networks focus on a voxelized representation of the object. Lifting convolution operators from the traditional 2D to 3D results in high computational overhead with little additional benefit as most of the geometry information is contained on the surface boundary. Here we study the problem of directly generating the 3D shape surface of rigid and non-rigid shapes using deep convolutional neural networks. We develop a procedure to create consistent `geometry images' representing the shape surface of a category of 3D objects. We then use this consistent representation for category-specific shape surface generation from a parametric representation or an image by developing novel extensions of deep residual networks for the task of geometry image generation. Our experiments indicate that our network learns a meaningful representation of shape surfaces allowing it to interpolate between shape orientations and poses, invent new shape surfaces and reconstruct 3D shape surfaces from previously unseen images.|['Ayan Sinha', 'Asim Unmesh', 'Qixing Huang', 'Karthik Ramani']|['cs.CV', 'cs.CG']
2017-03-16T23:32:26Z|2017-03-12T07:19:55Z|http://arxiv.org/abs/1703.04078v1|http://arxiv.org/abs/1703.04078v1|Prostate Cancer Diagnosis using Deep Learning with 3D Multiparametric   MRI|A novel deep learning architecture (XmasNet) based on convolutional neural networks was developed for the classification of prostate cancer lesions, using the 3D multiparametric MRI data provided by the PROSTATEx challenge. End-to-end training was performed for XmasNet, with data augmentation done through 3D rotation and slicing, in order to incorporate the 3D information of the lesion. XmasNet outperformed traditional machine learning models based on engineered features, for both train and test data. For the test data, XmasNet outperformed 69 methods from 33 participating groups and achieved the second highest AUC (0.84) in the PROSTATEx challenge. This study shows the great potential of deep learning for cancer imaging.|['Saifeng Liu', 'Huaixiu Zheng', 'Yesu Feng', 'Wei Li']|['cs.CV', 'stat.ML']
2017-03-16T23:32:26Z|2017-03-12T05:07:00Z|http://arxiv.org/abs/1703.04071v1|http://arxiv.org/pdf/1703.04071v1|A Compact DNN: Approaching GoogLeNet-Level Accuracy of Classification   and Domain Adaptation|Recently, DNN model compression based on network architecture design, e.g., SqueezeNet, attracted a lot attention. No accuracy drop on image classification is observed on these extremely compact networks, compared to well-known models. An emerging question, however, is whether these model compression techniques hurt DNN's learning ability other than classifying images on a single dataset. Our preliminary experiment shows that these compression methods could degrade domain adaptation (DA) ability, though the classification performance is preserved. Therefore, we propose a new compact network architecture and unsupervised DA method in this paper. The DNN is built on a new basic module Conv-M which provides more diverse feature extractors without significantly increasing parameters. The unified framework of our DA method will simultaneously learn invariance across domains, reduce divergence of feature representations, and adapt label prediction. Our DNN has 4.1M parameters, which is only 6.7% of AlexNet or 59% of GoogLeNet. Experiments show that our DNN obtains GoogLeNet-level accuracy both on classification and DA, and our DA method slightly outperforms previous competitive ones. Put all together, our DA strategy based on our DNN achieves state-of-the-art on sixteen of total eighteen DA tasks on popular Office-31 and Office-Caltech datasets.|['Chunpeng Wu', 'Wei Wen', 'Tariq Afzal', 'Yongmei Zhang', 'Yiran Chen', 'Hai Li']|['cs.CV', 'cs.AI', 'cs.NE']
2017-03-16T23:32:26Z|2017-03-12T02:58:07Z|http://arxiv.org/abs/1703.04062v1|http://arxiv.org/pdf/1703.04062v1|Multi-Pose Face Recognition Using Hybrid Face Features Descriptor|This paper presents a multi-pose face recognition approach using hybrid face features descriptors (HFFD). The HFFD is a face descriptor containing of rich discriminant information that is created by fusing some frequency-based features extracted using both wavelet and DCT analysis of several different poses of 2D face images. The main aim of this method is to represent the multi-pose face images using a dominant frequency component with still having reasonable achievement compared to the recent multi-pose face recognition methods. The HFFD based face recognition tends to achieve better performance than that of the recent 2D-based face recognition method. In addition, the HFFD-based face recognition also is sufficiently to handle large face variability due to face pose variations .|['I Gede Pasek Suta Wijaya', 'Keiichi Uchimura', 'Gou Koutaki']|['cs.CV']
2017-03-16T23:32:26Z|2017-03-11T23:32:59Z|http://arxiv.org/abs/1703.04044v1|http://arxiv.org/pdf/1703.04044v1|Colorization as a Proxy Task for Visual Understanding|We investigate and improve self-supervision as a drop-in replacement for ImageNet pretraining, focusing on automatic colorization as the proxy task. Self-supervised training has been shown to be more promising for utilizing unlabeled data than other, traditional unsupervised learning methods. We show the ability of our self-supervised network in several contexts. On VOC segmentation and classification tasks, we present results that are state-of-the-art among methods not using ImageNet labels for pretraining representations.   Moreover, we present the first in-depth analysis of self-supervision via colorization, concluding that formulation of the loss, training details and network architecture play important roles in its effectiveness. This investigation is further expanded by revisiting the ImageNet pretraining paradigm, asking questions such as: How much training data is needed? How many labels are needed? How much do features change when fine-tuned? We relate these questions back to self-supervision by showing that self-supervised colorization provides a similarly powerful supervision as various flavors of ImageNet pretraining.|['Gustav Larsson', 'Michael Maire', 'Gregory Shakhnarovich']|['cs.CV']
2017-03-16T23:32:26Z|2017-03-11T19:16:38Z|http://arxiv.org/abs/1703.04019v1|http://arxiv.org/pdf/1703.04019v1|Negentropic Planar Symmetry Detector|In this paper we observe that information theoretical concepts are valuable tools for extracting information from images and, in particular, information on image symmetries. It is shown that the problem of detecting reflectional and rotational symmetries in a two-dimensional image can be reduced to the problem of detecting point-symmetry and periodicity in one-dimensional negentropy functions. Based on these findings a detector of reflectional and rotational global symmetries in greyscale images is constructed. We discuss the importance of high precision in symmetry detection in applications arising from quality control and illustrate how the proposed method satisfies this requirement. Finally, a superior performance of our method to other existing methods, demonstrated by the results of a rigorous experimental verification, is an indication that our approach rooted in information theory is a promising direction in a development of a robust and widely applicable symmetry detector.|['Agata Migalska', 'JP Lewis']|['cs.CV', 'cs.IT', 'math.IT']
2017-03-16T23:32:26Z|2017-03-11T11:29:01Z|http://arxiv.org/abs/1703.03957v1|http://arxiv.org/pdf/1703.03957v1|Neural method for Explicit Mapping of Quasi-curvature Locally Linear   Embedding in image retrieval|This paper proposed a new explicit nonlinear dimensionality reduction using neural networks for image retrieval tasks. We first proposed a Quasi-curvature Locally Linear Embedding (QLLE) for training set. QLLE guarantees the linear criterion in neighborhood of each sample. Then, a neural method (NM) is proposed for out-of-sample problem. Combining QLLE and NM, we provide a explicit nonlinear dimensionality reduction approach for efficient image retrieval. The experimental results in three benchmark datasets illustrate that our method can get better performance than other state-of-the-art out-of-sample methods.|['Shenglan Liu', 'Jun Wu', 'Lin Feng', 'Feilong Wang']|['cs.CV']
2017-03-16T23:32:26Z|2017-03-11T11:07:22Z|http://arxiv.org/abs/1703.03949v1|http://arxiv.org/abs/1703.03949v1|Web-based visualisation of head pose and facial expressions changes:   monitoring human activity using depth data|Despite significant recent advances in the field of head pose estimation and facial expression recognition, raising the cognitive level when analysing human activity presents serious challenges to current concepts. Motivated by the need of generating comprehensible visual representations from different sets of data, we introduce a system capable of monitoring human activity through head pose and facial expression changes, utilising an affordable 3D sensing technology (Microsoft Kinect sensor). An approach build on discriminative random regression forests was selected in order to rapidly and accurately estimate head pose changes in unconstrained environment. In order to complete the secondary process of recognising four universal dominant facial expressions (happiness, anger, sadness and surprise), emotion recognition via facial expressions (ERFE) was adopted. After that, a lightweight data exchange format (JavaScript Object Notation-JSON) is employed, in order to manipulate the data extracted from the two aforementioned settings. Such mechanism can yield a platform for objective and effortless assessment of human activity within the context of serious gaming and human-computer interaction.|['Grigorios Kalliatakis', 'Nikolaos Vidakis', 'Georgios Triantafyllidis']|['cs.CV']
2017-03-16T23:32:30Z|2017-03-11T10:09:02Z|http://arxiv.org/abs/1703.03940v1|http://arxiv.org/pdf/1703.03940v1|A 3D Object Detection and Pose Estimation Pipeline Using RGB-D Images|3D object detection and pose estimation has been studied extensively in recent decades for its potential applications in robotics. However, there still remains challenges when we aim at detecting multiple objects while retaining low false positive rate in cluttered environments. This paper proposes a robust 3D object detection and pose estimation pipeline based on RGB-D images, which can detect multiple objects simultaneously while reducing false positives. Detection begins with template matching and yields a set of template matches. A clustering algorithm then groups templates of similar spatial location and produces multiple-object hypotheses. A scoring function evaluates the hypotheses using their associated templates and non-maximum suppression is adopted to remove duplicate results based on the scores. Finally, a combination of point cloud processing algorithms are used to compute objects' 3D poses. Existing object hypotheses are verified by computing the overlap between model and scene points. Experiments demonstrate that our approach provides competitive results comparable to the state-of-the-arts and can be applied to robot random bin-picking.|['Ruotao He', 'Juan Rojas', 'Yisheng Guan']|['cs.RO', 'cs.CV']
2017-03-16T23:32:30Z|2017-03-15T07:36:58Z|http://arxiv.org/abs/1703.03937v2|http://arxiv.org/pdf/1703.03937v2|Viraliency: Pooling Local Virality|In our overly-connected world, the automatic recognition of virality - the quality of an image or video to be rapidly and widely spread in social networks - is of crucial importance, and has recently awaken the interest of the computer vision community. Concurrently, recent progress in deep learning architectures showed that global pooling strategies allow the extraction of activation maps, which highlight the parts of the image most likely to contain instances of a certain class. We extend this concept by introducing a pooling layer that learns the size of the support area to be averaged: the learned top-N average (LENA) pooling. We hypothesize that the latent concepts (feature maps) describing virality may require such a rich pooling strategy. We assess the effectiveness of the LENA layer by appending it on top of a convolutional siamese architecture and evaluate its performance on the task of predicting and localizing virality. We report experiments on two publicly available datasets annotated for virality and show that our method outperforms state-of-the-art approaches.|['Xavier Alameda-Pineda', 'Andrea Pilzer', 'Dan Xu', 'Nicu Sebe', 'Elisa Ricci']|['cs.CV']
2017-03-16T23:32:30Z|2017-03-11T07:32:01Z|http://arxiv.org/abs/1703.03921v1|http://arxiv.org/pdf/1703.03921v1|Gait Pattern Recognition Using Accelerometers|Motion ability is one of the most important human properties, including gait as a basis of human transitional movement. Gait, as a biometric for recognizing human identities, can be non-intrusively captured signals using wearable or portable smart devices. In this study gait patterns is collected using a wireless platform of two sensors located at chest and right ankle of the subjects. Then the raw data has undergone some preprocessing methods and segmented into 5 seconds windows. Some time and frequency domain features is extracted and the performance evaluated by 5 different classifiers. Decision Tree (with all features) and K-Nearest Neighbors (with 10 selected features) classifiers reached 99.4% and 100% respectively.|['Vahid Alizadeh']|['cs.CV']
2017-03-16T23:32:30Z|2017-03-11T01:18:14Z|http://arxiv.org/abs/1703.03888v1|http://arxiv.org/pdf/1703.03888v1|Segmentation of skin lesions based on fuzzy classification of pixels and   histogram thresholding|This paper proposes an innovative method for segmentation of skin lesions in dermoscopy images developed by the authors, based on fuzzy classification of pixels and histogram thresholding.|['Jose Luis Garcia-Arroyo', 'Begonya Garcia-Zapirain']|['cs.CV', 'cs.AI', 'stat.ML']
2017-03-16T23:32:30Z|2017-03-10T23:43:17Z|http://arxiv.org/abs/1703.03872v1|http://arxiv.org/pdf/1703.03872v1|Deep Image Matting|Image matting is a fundamental computer vision problem and has many applications. Previous algorithms have poor performance when an image has similar foreground and background colors or complicated textures. The main reasons are prior methods 1) only use low-level features and 2) lack high-level context. In this paper, we propose a novel deep learning based algorithm that can tackle both these problems. Our deep model has two parts. The first part is a deep convolutional encoder-decoder network that takes an image and the corresponding trimap as inputs and predict the alpha matte of the image. The second part is a small convolutional network that refines the alpha matte predictions of the first network to have more accurate alpha values and sharper edges. In addition, we also create a large-scale image matting dataset including 49300 training images and 1000 testing images. We evaluate our algorithm on the image matting benchmark, our testing set, and a wide variety of real images. Experimental results clearly demonstrate the superiority of our algorithm over previous methods.|['Ning Xu', 'Brian Price', 'Scott Cohen', 'Thomas Huang']|['cs.CV']
2017-03-16T23:32:30Z|2017-03-10T23:10:11Z|http://arxiv.org/abs/1703.03867v1|http://arxiv.org/pdf/1703.03867v1|Depth from Monocular Images using a Semi-Parallel Deep Neural Network   (SPDNN) Hybrid Architecture|Computing pixel depth values provide a basis for understanding the 3D geometrical structure of an image. As it has been presented in recent research, using stereo images provides an accurate depth due to the advantage of having local correspondences; however, the processing time of these methods are still an open issue. To solve this problem, it has been suggested to use single images to compute the depth values but extracting depth from monocular images requires extracting a large number of cues from the global and local information in the image. This challenge has been studied for a decade and it is still an open problem. Recently the idea of using neural networks to solve this problem has attracted attention. In this paper, we tackle this challenge by employing a Deep Neural Network (DNN) equipped with semantic pixel-wise segmentation utilizing our recently published disparity post-processing method. Four models are trained in this study and they have been evaluated at 2 stages on KITTI dataset. The ground truth images in the first part of the experiment come from the benchmark and for the second part, the ground truth images are considered to be the disparity results from applying a state-of-art stereo matching method. The results of this evaluation demonstrate that using post-processing techniques to refine the target of the network increases the accuracy of depth estimation on individual mono images. The second evaluation shows that using segmentation data as the input can improve the depth estimation results to a point where performance is comparable with stereo depth matching.|['S. Bazrafkan', 'H. Javidnia', 'J. Lemley', 'P. Corcoran']|['cs.CV']
2017-03-16T23:32:30Z|2017-03-10T22:09:20Z|http://arxiv.org/abs/1703.03854v1|http://arxiv.org/pdf/1703.03854v1|Convolutional Spike Timing Dependent Plasticity based Feature Learning   in Spiking Neural Networks|Brain-inspired learning models attempt to mimic the cortical architecture and computations performed in the neurons and synapses constituting the human brain to achieve its efficiency in cognitive tasks. In this work, we present convolutional spike timing dependent plasticity based feature learning with biologically plausible leaky-integrate-and-fire neurons in Spiking Neural Networks (SNNs). We use shared weight kernels that are trained to encode representative features underlying the input patterns thereby improving the sparsity as well as the robustness of the learning model. We demonstrate that the proposed unsupervised learning methodology learns several visual categories for object recognition with fewer number of examples and outperforms traditional fully-connected SNN architectures while yielding competitive accuracy. Additionally, we observe that the learning model performs out-of-set generalization further making the proposed biologically plausible framework a viable and efficient architecture for future neuromorphic applications.|['Priyadarshini Panda', 'Gopalakrishnan Srinivasan', 'Kaushik Roy']|['cs.NE', 'cs.AI', 'cs.CV']
2017-03-16T23:32:30Z|2017-03-10T21:39:49Z|http://arxiv.org/abs/1703.03848v1|http://arxiv.org/abs/1703.03848v1|Development of An Android Application for Object Detection Based on   Color, Shape, or Local Features|Object detection and recognition is an important task in many computer vision applications. In this paper an Android application was developed using Eclipse IDE and OpenCV3 Library. This application is able to detect objects in an image that is loaded from the mobile gallery, based on its color, shape, or local features. The image is processed in the HSV color domain for better color detection. Circular shapes are detected using Circular Hough Transform and other shapes are detected using Douglas-Peucker algorithm. BRISK (binary robust invariant scalable keypoints) local features were applied in the developed Android application for matching an object image in another scene image. The steps of the proposed detection algorithms are described, and the interfaces of the application are illustrated. The application is ported and tested on Galaxy S3, S6, and Note1 Smartphones. Based on the experimental results, the application is capable of detecting eleven different colors, detecting two dimensional geometrical shapes including circles, rectangles, triangles, and squares, and correctly match local features of object and scene images for different conditions. The application could be used as a standalone application, or as a part of another application such as Robot systems, traffic systems, e-learning applications, information retrieval and many others.|['Lamiaa A. Elrefaei', 'Mona Omar Al-musawa', 'Norah Abdullah Al-gohany']|['cs.CV']
2017-03-16T23:32:30Z|2017-03-10T18:01:20Z|http://arxiv.org/abs/1703.05298v1|http://arxiv.org/pdf/1703.05298v1|Neural Networks for Beginners. A fast implemention in Matlab, Torch,   TensorFlow|This report provides an introduction to some Machine Learning tools within the most common development environments. It mainly focuses on practical problems, skipping any theoretical introduction. It is oriented to both students trying to approach Machine Learning and experts looking for new frameworks.|['Francesco Giannini', 'Vincenzo Laveglia', 'Alessandro Rossi', 'Dario Zanca', 'Andrea Zugarini']|['cs.LG', 'cs.CV', 'cs.MS', 'stat.ML']
2017-03-16T23:32:30Z|2017-03-10T14:39:11Z|http://arxiv.org/abs/1703.03702v1|http://arxiv.org/pdf/1703.03702v1|Data-Driven Color Augmentation Techniques for Deep Skin Image Analysis|Dermoscopic skin images are often obtained with different imaging devices, under varying acquisition conditions. In this work, instead of attempting to perform intensity and color normalization, we propose to leverage computational color constancy techniques to build an artificial data augmentation technique suitable for this kind of images. Specifically, we apply the \emph{shades of gray} color constancy technique to color-normalize the entire training set of images, while retaining the estimated illuminants. We then draw one sample from the distribution of training set illuminants and apply it on the normalized image. We employ this technique for training two deep convolutional neural networks for the tasks of skin lesion segmentation and skin lesion classification, in the context of the ISIC 2017 challenge and without using any external dermatologic image set. Our results on the validation set are promising, and will be supplemented with extended results on the hidden test set when available.|['Adrian Galdran', 'Aitor Alvarez-Gila', 'Maria Ines Meyer', 'Cristina L. Saratxaga', 'Teresa Araújo', 'Estibaliz Garrote', 'Guilherme Aresta', 'Pedro Costa', 'A. M. Mendonça', 'Aurélio Campilho']|['cs.CV']
2017-03-16T23:32:35Z|2017-03-10T12:58:23Z|http://arxiv.org/abs/1703.03664v1|http://arxiv.org/pdf/1703.03664v1|Parallel Multiscale Autoregressive Density Estimation|PixelCNN achieves state-of-the-art results in density estimation for natural images. Although training is fast, inference is costly, requiring one network evaluation per pixel; O(N) for N pixels. This can be sped up by caching activations, but still involves generating each pixel sequentially. In this work, we propose a parallelized PixelCNN that allows more efficient inference by modeling certain pixel groups as conditionally independent. Our new PixelCNN model achieves competitive density estimation and orders of magnitude speedup - O(log N) sampling instead of O(N) - enabling the practical generation of 512x512 images. We evaluate the model on class-conditional image generation, text-to-image synthesis, and action-conditional video generation, showing that our model achieves the best results among non-pixel-autoregressive density models that allow efficient sampling.|['Scott Reed', 'Aäron van den Oord', 'Nal Kalchbrenner', 'Sergio Gómez Colmenarejo', 'Ziyu Wang', 'Dan Belov', 'Nando de Freitas']|['cs.CV', 'cs.NE']
2017-03-16T23:32:35Z|2017-03-10T11:08:50Z|http://arxiv.org/abs/1703.03624v1|http://arxiv.org/pdf/1703.03624v1|From Depth Data to Head Pose Estimation: a Siamese approach|The correct estimation of the head pose is a problem of the great importance for many applications. For instance, it is an enabling technology in automotive for driver attention monitoring. In this paper, we tackle the pose estimation problem through a deep learning network working in regression manner. Traditional methods usually rely on visual facial features, such as facial landmarks or nose tip position. In contrast, we exploit a Convolutional Neural Network (CNN) to perform head pose estimation directly from depth data. We exploit a Siamese architecture and we propose a novel loss function to improve the learning of the regression network layer. The system has been tested on two public datasets, Biwi Kinect Head Pose and ICT-3DHP database. The reported results demonstrate the improvement in accuracy with respect to current state-of-the-art approaches and the real time capabilities of the overall framework.|['Marco Venturelli', 'Guido Borghi', 'Roberto Vezzani', 'Rita Cucchiara']|['cs.CV']
2017-03-16T23:32:35Z|2017-03-10T10:26:24Z|http://arxiv.org/abs/1703.03613v1|http://arxiv.org/pdf/1703.03613v1|Fast LIDAR-based Road Detection Using Convolutional Neural Networks|In this work, a deep learning approach has been developed to carry out road detection using only LIDAR data. Starting from an unstructured point cloud, top-view images encoding several basic statistics such as mean height and density are generated. By considering a top-view representation, road detection is reduced to a single-scale problem that can be addressed with a simple and fast convolutional neural network (CNN). The CNN is specifically designed for the task of pixel-wise semantic segmentation by combining a large receptive field with high-resolution feature maps. The proposed system achieves state-of-the-art results on the KITTI road benchmark. It is currently the top-performing algorithm among the published methods in the overall category urban road and outperforms the second best LIDAR-only approach by 7.4 percentage points. Its fast inference makes it particularly suitable for real-time applications.|['Luca Caltagirone', 'Samuel Scheidegger', 'Lennart Svensson', 'Mattias Wahde']|['cs.CV']
2017-03-16T23:32:35Z|2017-03-10T10:17:10Z|http://arxiv.org/abs/1703.03608v1|http://arxiv.org/pdf/1703.03608v1|Multi-frequency image reconstruction for radio-interferometry with   self-tuned regularization parameters|As the world's largest radio telescope, the Square Kilometer Array (SKA) will provide radio interferometric data with unprecedented detail. Image reconstruction algorithms for radio interferometry are challenged to scale well with TeraByte image sizes never seen before. In this work, we investigate one such 3D image reconstruction algorithm known as MUFFIN (MUlti-Frequency image reconstruction For radio INterferometry). In particular, we focus on the challenging task of automatically finding the optimal regularization parameter values. In practice, finding the regularization parameters using classical grid search is computationally intensive and nontrivial due to the lack of ground- truth. We adopt a greedy strategy where, at each iteration, the optimal parameters are found by minimizing the predicted Stein unbiased risk estimate (PSURE). The proposed self-tuned version of MUFFIN involves parallel and computationally efficient steps, and scales well with large- scale data. Finally, numerical results on a 3D image are presented to showcase the performance of the proposed approach.|['Rita Ammanouil', 'André Ferrari', 'Rémi Flamary', 'Chiara Ferrari', 'David Mary']|['cs.CV']
2017-03-16T23:32:35Z|2017-03-10T07:56:01Z|http://arxiv.org/abs/1703.03567v1|http://arxiv.org/pdf/1703.03567v1|A New Evaluation Protocol and Benchmarking Results for Extendable   Cross-media Retrieval|This paper proposes a new evaluation protocol for cross-media retrieval which better fits the real-word applications. Both image-text and text-image retrieval modes are considered. Traditionally, class labels in the training and testing sets are identical. That is, it is usually assumed that the query falls into some pre-defined classes. However, in practice, the content of a query image/text may vary extensively, and the retrieval system does not necessarily know in advance the class label of a query. Considering the inconsistency between the real-world applications and laboratory assumptions, we think that the existing protocol that works under identical train/test classes can be modified and improved.   This work is dedicated to addressing this problem by considering the protocol under an extendable scenario, \ie, the training and testing classes do not overlap. We provide extensive benchmarking results obtained by the existing protocol and the proposed new protocol on several commonly used datasets. We demonstrate a noticeable performance drop when the testing classes are unseen during training. Additionally, a trivial solution, \ie, directly using the predicted class label for cross-media retrieval, is tested. We show that the trivial solution is very competitive in traditional non-extendable retrieval, but becomes less so under the new settings. The train/test split, evaluation code, and benchmarking results are publicly available on our website.|['Ruoyu Liu', 'Yao Zhao', 'Liang Zheng', 'Shikui Wei', 'Yi Yang']|['cs.CV']
2017-03-16T23:32:35Z|2017-03-09T23:47:27Z|http://arxiv.org/abs/1703.03492v1|http://arxiv.org/pdf/1703.03492v1|A New Representation of Skeleton Sequences for 3D Action Recognition|Skeleton sequences provide 3D trajectories of human skeleton joints. The spatial temporal information is very important for action recognition. Considering that deep convolutional neural network (CNN) is very powerful for feature learning in images, in this paper, we propose to transform a skeleton sequence into an image-based representation for spatial temporal information learning with CNN. Specifically, for each channel of the 3D coordinates, we represent the sequence into a clip with several gray images, which represent multiple spatial structural information of the joints. Those images are fed to a deep CNN to learn high-level features. The CNN features of all the three clips at the same time-step are concatenated in a feature vector. Each feature vector represents the temporal information of the entire skeleton sequence and one particular spatial relationship of the joints. We then propose a Multi-Task Learning Network (MTLN) to jointly process the feature vectors of all time-steps in parallel for action recognition. Experimental results clearly show the effectiveness of the proposed new representation and feature learning method for 3D action recognition.|['Qiuhong Ke', 'Mohammed Bennamoun', 'Senjian An', 'Ferdous Sohel', 'Farid Boussaid']|['cs.CV']
2017-03-16T23:32:35Z|2017-03-09T21:19:48Z|http://arxiv.org/abs/1703.03468v1|http://arxiv.org/pdf/1703.03468v1|Position Tracking for Virtual Reality Using Commodity WiFi|Today, experiencing virtual reality (VR) is a cumbersome experience which either requires dedicated infrastructure like infrared cameras to track the headset and hand-motion controllers (e.g., Oculus Rift, HTC Vive), or provides only 3-DoF (Degrees of Freedom) tracking which severely limits the user experience (e.g., Samsung Gear). To truly enable VR everywhere, we need position tracking to be available as a ubiquitous service. This paper presents WiCapture, a novel approach which leverages commodity WiFi infrastructure, which is ubiquitous today, for tracking purposes. We prototype WiCapture using off-the-shelf WiFi radios and show that it achieves an accuracy of 0.88 cm compared to sophisticated infrared based tracking systems like the Oculus, while providing much higher range, resistance to occlusion, ubiquity and ease of deployment.|['Manikanta Kotaru', 'Sachin Katti']|['cs.CV', 'cs.NI']
2017-03-16T23:32:35Z|2017-03-09T18:58:03Z|http://arxiv.org/abs/1703.03400v1|http://arxiv.org/pdf/1703.03400v1|Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks|We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on a few-shot image classification benchmark, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.|['Chelsea Finn', 'Pieter Abbeel', 'Sergey Levine']|['cs.LG', 'cs.AI', 'cs.CV', 'cs.NE']
2017-03-16T23:32:35Z|2017-03-15T01:37:18Z|http://arxiv.org/abs/1703.03372v3|http://arxiv.org/pdf/1703.03372v3|LesionSeg: Semantic segmentation of skin lesions using Deep   Convolutional Neural Network|We present a method for skin lesion segmentation for the ISIC 2017 Skin Lesion Segmentation Challenge. Our approach is based on a Fully Convolutional Network architecture which is trained end to end, from scratch, on a limited dataset. Our semantic segmentation architecture utilizes several recent innovations in particularly in the combined use of (i) use of atrous convolutions to increase the effective field of view of the network's receptive field without increasing the number of parameters, (ii) the use of network-in-network $1\times1$ convolution layers to add capacity to the network and (iii) state-of-art super-resolution upsampling of predictions using subpixel CNN layers. We reported a mean IOU score of 0.642 on the validation set provided by the organisers.|['Dhanesh Ramachandram', 'Terrance DeVries']|['cs.CV', 'cs.AI', 'cs.NE']
2017-03-16T23:32:35Z|2017-03-09T17:15:18Z|http://arxiv.org/abs/1703.03349v1|http://arxiv.org/pdf/1703.03349v1|Fast and Robust Detection of Fallen People from a Mobile Robot|This paper deals with the problem of detecting fallen people lying on the floor by means of a mobile robot equipped with a 3D depth sensor. In the proposed algorithm, inspired by semantic segmentation techniques, the 3D scene is over-segmented into small patches. Fallen people are then detected by means of two SVM classifiers: the first one labels each patch, while the second one captures the spatial relations between them. This novel approach showed to be robust and fast. Indeed, thanks to the use of small patches, fallen people in real cluttered scenes with objects side by side are correctly detected. Moreover, the algorithm can be executed on a mobile robot fitted with a standard laptop making it possible to exploit the 2D environmental map built by the robot and the multiple points of view obtained during the robot navigation. Additionally, this algorithm is robust to illumination changes since it does not rely on RGB data but on depth data. All the methods have been thoroughly validated on the IASLAB-RGBD Fallen Person Dataset, which is published online as a further contribution. It consists of several static and dynamic sequences with 15 different people and 2 different environments.|['Morris Antonello', 'Marco Carraro', 'Marco Pierobon', 'Emanuele Menegatti']|['cs.RO', 'cs.CV']
2017-03-16T23:32:39Z|2017-03-09T17:14:21Z|http://arxiv.org/abs/1703.03347v1|http://arxiv.org/pdf/1703.03347v1|A Self-supervised Learning System for Object Detection using Physics   Simulation and Multi-view Pose Estimation|Impressive progress has been achieved in object detection with the use of deep learning. Nevertheless, such tools typically require a large amount of training data and significant manual effort for labeling objects. This limits their applicability in robotics, where it is necessary to scale solutions to a large number of objects and a variety of conditions. This work proposes a fully autonomous process to train a Convolutional Neural Network (CNN) for object detection and pose estimation in robotic setups. The application involves detection of objects placed in a clutter and in tight environments, such as a shelf. In particular, given access to 3D object models, several aspects of the environment are simulated and the models are placed in physically realistic poses with respect to their environment to generate a labeled synthetic dataset. To further improve object detection, the network self-trains over real images that are labeled using a robust multi-view pose estimation process. The proposed training process is evaluated on several existing datasets and on a dataset that we collected with a Motoman robotic manipulator. Results show that the proposed process outperforms popular training processes relying on synthetic data generation and manual annotation.|['Chaitanya Mitash', 'Kostas E. Bekris', 'Abdeslam Boularias']|['cs.RO', 'cs.CV']
2017-03-16T23:32:39Z|2017-03-09T16:29:39Z|http://arxiv.org/abs/1703.03329v1|http://arxiv.org/pdf/1703.03329v1|UntrimmedNets for Weakly Supervised Action Recognition and Detection|Current action recognition methods heavily rely on trimmed videos for model training. However, it is very expensive and time-consuming to acquire a large-scale trimmed video dataset. This paper presents a new weakly supervised architecture, called UntrimmedNet, which is able to directly learn from untrimmed videos without the need of temporal annotations of action instances. Our UntrimmedNet couples two important components, the classification module and the selection module, to learn the action models and reason about the temporal duration of action instances, respectively. These two modules are implemented with feed-forward networks. UntrimmedNet is essentially an end-to-end trainable architecture, which allows for the joint optimization of model parameters of both components. We exploit the learned models for the problems of action recognition (WSR) and detection (WSD) on the untrimmed video datasets of THUMOS14 and ActivityNet. Although our UntrimmedNet only employs weak supervision, our method achieves performance superior or comparable to that of strongly supervised approaches on these two datasets.|['Limin Wang', 'Yuanjun Xiong', 'Dahua Lin', 'Luc Van Gool']|['cs.CV']
2017-03-16T23:32:39Z|2017-03-09T15:48:22Z|http://arxiv.org/abs/1703.03305v1|http://arxiv.org/pdf/1703.03305v1|End-to-end semantic face segmentation with conditional random fields as   convolutional, recurrent and adversarial networks|Recent years have seen a sharp increase in the number of related yet distinct advances in semantic segmentation. Here, we tackle this problem by leveraging the respective strengths of these advances. That is, we formulate a conditional random field over a four-connected graph as end-to-end trainable convolutional and recurrent networks, and estimate them via an adversarial process. Importantly, our model learns not only unary potentials but also pairwise potentials, while aggregating multi-scale contexts and controlling higher-order inconsistencies. We evaluate our model on two standard benchmark datasets for semantic face segmentation, achieving state-of-the-art results on both of them.|['Umut Güçlü', 'Yağmur Güçlütürk', 'Meysam Madadi', 'Sergio Escalera', 'Xavier Baró', 'Jordi González', 'Rob van Lier', 'Marcel A. J. van Gerven']|['cs.CV']
2017-03-16T23:32:39Z|2017-03-15T06:55:44Z|http://arxiv.org/abs/1703.03230v2|http://arxiv.org/pdf/1703.03230v2|WebCaricature: a benchmark for caricature face recognition|Caricatures are facial drawings by artists with exaggeration on certain facial parts. The exaggerations are often beyond realism and yet the caricatures are still recognizable by humans. With the advent of deep learning, recognition performances by computers on real-world faces has become comparable to human performance even under unconstrained situations. However, there is still a gap in caricature recognition performance between computer and human. This is mainly due to the lack of publicly available caricature datasets of large scale. To facilitate the research in caricature recognition, a new caricature dataset is built. All the caricature images and face images were collected from the web.Compared with two existing datasets, this dataset is of larger size and has various artistic styles. We also offer evaluation protocols and present baseline performances on the dataset. Specifically, four evaluation protocols are provided: restricted and unrestricted caricature verifications, caricature to photo and photo to caricature face identifications. Based on the evaluation protocols, three face alignment methods together with five kinds of features and nine subspace and metric learning algorithms have been applied to provide the baseline performances on this dataset. Main conclusion is that there is still a space for improvement in caricature face recognition.|['Jing Huo', 'Wenbin Li', 'Yinghuan Shi', 'Yang Gao', 'Hujun Yin']|['cs.CV']
2017-03-16T23:32:39Z|2017-03-09T09:41:28Z|http://arxiv.org/abs/1703.03196v1|http://arxiv.org/pdf/1703.03196v1|Prior-based Hierarchical Segmentation Highlighting Structures of   Interest|Image segmentation is the process of partitioning an image into a set of meaningful regions according to some criteria. Hierarchical segmentation has emerged as a major trend in this regard as it favors the emergence of important regions at different scales. On the other hand, many methods allow us to have prior information on the position of structures of interest in the images. In this paper, we present a versatile hierarchical segmentation method that takes into account any prior spatial information and outputs a hierarchical segmentation that emphasizes the contours or regions of interest while preserving the important structures in the image. Several applications are presented that illustrate the method versatility and efficiency.|['Amin Fehri', 'Santiago Velasco-Forero', 'Fernand Meyer']|['cs.CV']
2017-03-16T23:32:39Z|2017-03-09T09:14:40Z|http://arxiv.org/abs/1703.03186v1|http://arxiv.org/pdf/1703.03186v1|Segmenting Dermoscopic Images|We propose an automatic algorithm, named SDI, for the segmentation of skin lesions in dermoscopic images, articulated into three main steps: selection of the image ROI, selection of the segmentation band, and segmentation. We present extensive experimental results achieved by the SDI algorithm on the lesion segmentation dataset made available for the ISIC 2017 challenge on Skin Lesion Analysis Towards Melanoma Detection, highlighting its advantages and disadvantages.|['Mario Rosario Guarracino', 'Lucia Maddalena']|['cs.CV']
2017-03-16T23:32:39Z|2017-03-09T06:48:47Z|http://arxiv.org/abs/1703.03156v1|http://arxiv.org/pdf/1703.03156v1|Face-to-BMI: Using Computer Vision to Infer Body Mass Index on Social   Media|"A person's weight status can have profound implications on their life, ranging from mental health, to longevity, to financial income. At the societal level, ""fat shaming"" and other forms of ""sizeism"" are a growing concern, while increasing obesity rates are linked to ever raising healthcare costs. For these reasons, researchers from a variety of backgrounds are interested in studying obesity from all angles. To obtain data, traditionally, a person would have to accurately self-report their body-mass index (BMI) or would have to see a doctor to have it measured. In this paper, we show how computer vision can be used to infer a person's BMI from social media images. We hope that our tool, which we release, helps to advance the study of social aspects related to body weight."|['Enes Kocabey', 'Mustafa Camurcu', 'Ferda Ofli', 'Yusuf Aytar', 'Javier Marin', 'Antonio Torralba', 'Ingmar Weber']|['cs.HC', 'cs.CV', 'cs.CY']
2017-03-16T23:32:39Z|2017-03-09T04:19:17Z|http://arxiv.org/abs/1703.03126v1|http://arxiv.org/pdf/1703.03126v1|DeepSD: Generating High Resolution Climate Change Projections through   Single Image Super-Resolution|The impacts of climate change are felt by most critical systems, such as infrastructure, ecological systems, and power-plants. However, contemporary Earth System Models (ESM) are run at spatial resolutions too coarse for assessing effects this localized. Local scale projections can be obtained using statistical downscaling, a technique which uses historical climate observations to learn a low-resolution to high-resolution mapping. Depending on statistical modeling choices, downscaled projections have been shown to vary significantly terms of accuracy and reliability. The spatio-temporal nature of the climate system motivates the adaptation of super-resolution image processing techniques to statistical downscaling. In our work, we present DeepSD, a generalized stacked super resolution convolutional neural network (SRCNN) framework for statistical downscaling of climate variables. DeepSD augments SRCNN with multi-scale input channels to maximize predictability in statistical downscaling. We provide a comparison with Bias Correction Spatial Disaggregation as well as three Automated-Statistical Downscaling approaches in downscaling daily precipitation from 1 degree (~100km) to 1/8 degrees (~12.5km) over the Continental United States. Furthermore, a framework using the NASA Earth Exchange (NEX) platform is discussed for downscaling more than 20 ESM models with multiple emission scenarios.|['Thomas Vandal', 'Evan Kodra', 'Sangram Ganguly', 'Andrew Michaelis', 'Ramakrishna Nemani', 'Auroop R Ganguly']|['cs.CV']
2017-03-16T23:32:39Z|2017-03-09T02:35:59Z|http://arxiv.org/abs/1703.03108v1|http://arxiv.org/pdf/1703.03108v1|Image Classification of Melanoma, Nevus and Seborrheic Keratosis by Deep   Neural Network Ensemble|This short paper reports the method and the evaluation results of Casio and Shinshu University joint team for the ISBI Challenge 2017 - Skin Lesion Analysis Towards Melanoma Detection - Part 3: Lesion Classification hosted by ISIC. Our online validation score was 0.958 with melanoma classifier AUC 0.924 and seborrheic keratosis classifier AUC 0.993.|['Kazuhisa Matsunaga', 'Akira Hamada', 'Akane Minagawa', 'Hiroshi Koga']|['cs.CV']
2017-03-16T23:32:39Z|2017-03-09T01:29:23Z|http://arxiv.org/abs/1703.03098v1|http://arxiv.org/pdf/1703.03098v1|DA-RNN: Semantic Mapping with Data Associated Recurrent Neural Networks|3D scene understanding is important for robots to interact with the 3D world in a meaningful way. Most previous works on 3D scene understanding focus on recognizing geometrical or semantic properties of the scene independently. In this work, we introduce Data Associated Recurrent Neural Networks (DA-RNNs), a novel framework for joint 3D scene mapping and semantic labeling. DA-RNNs use a new recurrent neural network architecture for semantic labeling on RGB-D videos. The output of the network is integrated with mapping techniques such as KinectFusion in order to inject semantic information into the reconstructed 3D scene. Experiments conducted on a real world dataset and a synthetic dataset with RGB-D videos demonstrate the ability of our method in semantic 3D scene mapping.|['Yu Xiang', 'Dieter Fox']|['cs.CV', 'cs.RO']
