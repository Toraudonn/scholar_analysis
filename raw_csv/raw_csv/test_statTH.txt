2017-03-16T23:27:45Z|2017-03-15T12:04:34Z|http://arxiv.org/abs/1703.05101v1|http://arxiv.org/pdf/1703.05101v1|Optimal graphon estimation in cut distance|Consider the twin problems of estimating the connection probability matrix of an inhomogeneous random graph and the graphon of a W-random graph. We establish the minimax estimation rates with respect to the cut metric for classes of block constant matrices and step function graphons. Surprisingly, our results imply that, from the minimax point of view, the raw data, that is, the adjacency matrix of the observed graph, is already optimal and more involved procedures cannot improve the convergence rates for this metric. This phenomenon contrasts with optimal rates of convergence with respect to other classical distances for graphons such as the l 1 or l 2 metrics.|['Olga Klopp', 'Nicolas Verzelen']|['math.ST', 'stat.TH']
2017-03-16T23:27:45Z|2017-03-15T06:34:38Z|http://arxiv.org/abs/1703.04956v1|http://arxiv.org/pdf/1703.04956v1|A Short Note on Almost Sure Convergence of Bayes Factors in the General   Set-Up|In this article we derive the almost sure convergence theory of Bayes factor in the general set-up that includes even dependent data and misspecified models, as a simple application of a result of Shalizi (2009) to a well-known identity satisfied by the Bayes factor.|['Debashis Chatterjee', 'Trisha Maitra', 'Sourabh Bhattacharya']|['math.ST', 'stat.ME', 'stat.TH']
2017-03-16T23:27:45Z|2017-03-15T06:26:43Z|http://arxiv.org/abs/1703.04955v1|http://arxiv.org/pdf/1703.04955v1|Theoretical Limits of Record Linkage and Microclustering|"There has been substantial recent interest in record linkage, attempting to group the records pertaining to the same entities from a large database lacking unique identifiers. This can be viewed as a type of ""microclustering,"" with few observations per cluster and a very large number of clusters. A variety of methods have been proposed, but there is a lack of literature providing theoretical guarantees on performance. We show that the problem is fundamentally hard from a theoretical perspective, and even in idealized cases, accurate entity resolution is effectively impossible when the number of entities is small relative to the number of records and/or the separation among records from different entities is not extremely large. To characterize the fundamental difficulty, we focus on entity resolution based on multivariate Gaussian mixture models, but our conclusions apply broadly and are supported by simulation studies inspired by human rights applications. These results suggest conservatism in interpretation of the results of record linkage, support collection of additional data to more accurately disambiguate the entities, and motivate a focus on coarser inference. For example, results from a simulation study suggest that sometimes one may obtain accurate results for population size estimation even when fine scale entity resolution is inaccurate."|['James E. Johndrow', 'Kristian Lum', 'David B. Dunson']|['math.ST', 'stat.TH']
2017-03-16T23:27:45Z|2017-03-15T02:25:31Z|http://arxiv.org/abs/1703.04886v1|http://arxiv.org/pdf/1703.04886v1|Towards Optimal Sparse Inverse Covariance Selection through Non-Convex   Optimization|We study the problem of reconstructing the graph of a sparse Gaussian Graphical Model from independent observations, which is equivalent to finding non-zero elements of an inverse covariance matrix. For a model of size $p$ and maximum degree $d$, information theoretic lower bounds established in prior works require that the number of samples needed for recovering the graph perfectly is at least $d \log p/\kappa^2$, where $\kappa$ is the minimum normalized non-zero entry of the inverse covariance matrix. Existing algorithms require additional assumptions to guarantee perfect graph reconstruction, and consequently, their sample complexity is dependent on parameters that are not present in the information theoretic lower bound. We propose an estimator, called SLICE, that consists of a cardinality constrained least-squares regression followed by a thresholding procedure. Without any additional assumptions we show that SLICE attains a sample complexity of $\frac{64}{\kappa^4}d \log p$, which differs from the lower bound by only a factor proportional to $1/\kappa^2$ and depends only on parameters present in the lower bound.|['Sidhant Misra', 'Marc Vuffray', 'Andrey Y. Lokhov', 'Michael Chertkov']|['cs.LG', 'cs.IT', 'math.IT', 'math.ST', 'stat.TH']
2017-03-16T23:27:45Z|2017-03-14T22:49:50Z|http://arxiv.org/abs/1703.04799v1|http://arxiv.org/pdf/1703.04799v1|Multi-parameter One-Sided Monitoring Test|Multi-parameter one-sided hypothesis test problems arise naturally in many applications. We are particularly interested in effective tests for monitoring multiple quality indices in forestry products. Our search reveals that there are many effective statistical methods in the literature for normal data, and that they can easily be adapted for non-normal data. We find that the beautiful likelihood ratio test is unsatisfactory, because in order to control the size, it must cope with the least favorable distributions at the cost of power. In this paper, we find a novel way to slightly ease the size control, obtaining a much more powerful test. Simulation confirms that the new test retains good control of the type I error and is markedly more powerful than the likelihood ratio test as well as many competitors based on normal data. The new method performs well in the context of monitoring multiple quality indices.|['Guangyu Zhu', 'Jiahua Chen']|['math.ST', 'stat.TH']
2017-03-16T23:27:45Z|2017-03-14T22:36:09Z|http://arxiv.org/abs/1703.04790v1|http://arxiv.org/pdf/1703.04790v1|Robust Power System Dynamic State Estimator with Non-Gaussian   Measurement Noise: Part I--Theory|This paper develops the theoretical framework and the equations of a new robust Generalized Maximum-likelihood-type Unscented Kalman Filter (GM-UKF) that is able to suppress observation and innovation outliers while filtering out non-Gaussian measurement noise. Because the errors of the real and reactive power measurements calculated using Phasor Measurement Units (PMUs) follow long-tailed probability distributions, the conventional UKF provides strongly biased state estimates since it relies on the weighted least squares estimator. By contrast, the state estimates and residuals of our GM-UKF are proved to be roughly Gaussian, allowing the sigma points to reliably approximate the mean and the covariance matrices of the predicted and corrected state vectors. To develop our GM-UKF, we first derive a batch-mode regression form by processing the predictions and observations simultaneously, where the statistical linearization approach is used. We show that the set of equations so derived are equivalent to those of the unscented transformation. Then, a robust GM-estimator that minimizes a convex Huber cost function while using weights calculated via Projection Statistics (PS's) is proposed. The PS's are applied to a two-dimensional matrix that consists of serially correlated predicted state and innovation vectors to detect observation and innovation outliers. These outliers are suppressed by the GM-estimator using the iteratively reweighted least squares algorithm. Finally, the asymptotic error covariance matrix of the GM-UKF state estimates is derived from the total influence function. In the companion paper, extensive simulation results will be shown to verify the effectiveness and robustness of the proposed method.|['Junbo Zhao', 'Lamine Mili']|['math.ST', 'stat.TH']
2017-03-16T23:27:45Z|2017-03-14T20:19:08Z|http://arxiv.org/abs/1703.04697v1|http://arxiv.org/pdf/1703.04697v1|On the benefits of output sparsity for multi-label classification|The multi-label classification framework, where each observation can be associated with a set of labels, has generated a tremendous amount of attention over recent years. The modern multi-label problems are typically large-scale in terms of number of observations, features and labels, and the amount of labels can even be comparable with the amount of observations. In this context, different remedies have been proposed to overcome the curse of dimensionality. In this work, we aim at exploiting the output sparsity by introducing a new loss, called the sparse weighted Hamming loss. This proposed loss can be seen as a weighted version of classical ones, where active and inactive labels are weighted separately. Leveraging the influence of sparsity in the loss function, we provide improved generalization bounds for the empirical risk minimizer, a suitable property for large-scale problems. For this new loss, we derive rates of convergence linear in the underlying output-sparsity rather than linear in the number of labels. In practice, minimizing the associated risk can be performed efficiently by using convex surrogates and modern convex optimization algorithms. We provide experiments on various real-world datasets demonstrating the pertinence of our approach when compared to non-weighted techniques.|['Evgenii Chzhen', 'Christophe Denis', 'Mohamed Hebiri', 'Joseph Salmon']|['math.ST', 'cs.LG', 'stat.ML', 'stat.TH']
2017-03-16T23:27:45Z|2017-03-14T18:48:35Z|http://arxiv.org/abs/1703.04661v1|http://arxiv.org/pdf/1703.04661v1|A Noninformative Prior on a Space of Functions|In a given problem, the Bayesian statistical paradigm requires the specification of a prior distribution that quantifies relevant information, about the unknowns of main interest, external to the data. In cases where little such information is available, the problem under study may possess an invariance under a transformation group that encodes a lack of information, leading to a unique prior. Previous successful examples of this idea have included location-scale invariance under linear transformation, multiplicative invariance of the rate at which events in a counting process are observed, and the derivation of the Haldane prior for a Bernoulli success probability. In this paper we show that this method can be extended in two ways: (1) to yield families of approximately invariant priors, and (2) to the infinite-dimensional setting, yielding families of priors on spaces of distribution functions. Our results can be used to describe conditions under which a particular Dirichlet Process posterior arises from an optimal Bayesian analysis, in the sense that invariances in the prior and likelihood lead to one and only one posterior distribution.|['Alexander Terenin', 'David Draper']|['math.ST', 'stat.TH']
2017-03-16T23:27:45Z|2017-03-13T17:54:07Z|http://arxiv.org/abs/1703.04517v1|http://arxiv.org/pdf/1703.04517v1|Variable selection in discriminant analysis for mixed variables and   several groups|We propose a method for variable selection in discriminant analysis with mixed categorical and continuous variables. This method is based on a criterion that permits to reduce the variable selection problem to a problem of estimating suitable permutation and dimensionality. Then, estimators for these parameters are proposed and the resulting method for selecting variables is shown to be consistent. A simulation study that permits to study several poperties of the proposed approach and to compare it with an existing method is given.|['Alban Mbina Mbina', 'Guy Martial Nkiet', 'Fulgence Eyi Obiang']|['math.ST', 'math.PR', 'stat.TH']
2017-03-16T23:27:45Z|2017-03-13T14:36:48Z|http://arxiv.org/abs/1703.04419v1|http://arxiv.org/pdf/1703.04419v1|Iterated failure rate monotonicity and ordering relations within Gamma   and Weibull distributions|Stochastic ordering of distributions of random variables may be defined by the relative convexity of the tail functions. This has been extended to higher order stochastic orderings, by iteratively reassigning tail-weights. The actual verification of those stochastic orderings is not simple, as this depends on inverting distribution functions for which there may be no explicit expression. The iterative definition of distributions, of course, contributes to make that verification even harder. We have a look at the stochastic ordering, introducing a method that allows for explicit usage, applying it to the Gamma and Weibull distributions, giving a complete description of the order relations within each of those families.|['Idir Arab', 'Paulo Eduardo Oliveira']|['math.ST', 'stat.TH', '60E15, 26A51']
2017-03-16T23:27:49Z|2017-03-13T10:34:44Z|http://arxiv.org/abs/1703.04320v1|http://arxiv.org/pdf/1703.04320v1|Fourier analysis of serial dependence measures|Classical spectral analysis is based on the discrete Fourier transform of the auto-covariances. In this paper we investigate the asymptotic properties of new frequency domain methods where the auto-covariances in the spectral density are replaced by alternative dependence measures which can be estimated by U-statistics. An interesting example is given by Kendall{'}s $\tau$ , for which the limiting variance exhibits a surprising behavior.|['Ria van Hecke', 'Stanislav Volgushev', 'Holger Dette']|['math.ST', 'stat.TH']
2017-03-16T23:27:49Z|2017-03-12T02:16:11Z|http://arxiv.org/abs/1703.04058v1|http://arxiv.org/pdf/1703.04058v1|Think globally, fit locally under the Manifold Setup: Asymptotic   Analysis of Locally Linear Embedding|Since its introduction in 2000, the locally linear embedding (LLE) has been widely applied in data science. We provide an asymptotical analysis of the LLE under the manifold setup. We show that for the general manifold, asymptotically we may not obtain the Laplace-Beltrami operator, and the result may depend on the non-uniform sampling, unless a correct regularization is chosen. We also derive the corresponding kernel function, which indicates that the LLE is not a Markov process. A comparison with the other commonly applied nonlinear algorithms, particularly the diffusion map, is provided, and its relationship with the locally linear regression is also discussed.|['Hau-Tieng Wu', 'Nan Wu']|['math.ST', 'stat.TH', '62-07']
2017-03-16T23:27:49Z|2017-03-11T12:39:33Z|http://arxiv.org/abs/1703.03965v1|http://arxiv.org/pdf/1703.03965v1|Sparse Poisson Regression with Penalized Weighted Score Function|We proposed a new penalized method in this paper to solve sparse Poisson Regression problems. Being different from $\ell_1$ penalized log-likelihood estimation, our new method can be viewed as penalized weighted score function method. We show that under mild conditions, our estimator is $\ell_1$ consistent and the tuning parameter can be pre-specified, which shares the same good property of the square-root Lasso.|['Jinzhu Jia', 'Fang Xie', 'Lihu Xu']|['math.ST', 'stat.TH']
2017-03-16T23:27:49Z|2017-03-10T13:43:06Z|http://arxiv.org/abs/1703.03680v1|http://arxiv.org/pdf/1703.03680v1|Strong convergence rates of probabilistic integrators for ordinary   differential equations|Probabilistic integration of a continuous dynamical system is a way of systematically introducing model error, at scales no larger than errors inroduced by standard numerical discretisation, in order to enable thorough exploration of possible responses of the system to inputs. It is thus a potentially useful approach in a number of applications such as forward uncertainty quantification, inverse problems, and data assimilation. We extend the convergence analysis of probabilistic integrators for deterministic ordinary differential equations, as proposed by Conrad et al. (Stat. Comput., 2016), to establish mean-square convergence in the uniform norm on discrete- or continuous-time solutions under relaxed regularity assumptions on the driving vector fields and their induced flows. Specifically, we show that randomised high-order integrators for globally Lipschitz flows and randomised Euler integrators for dissipative vector fields with polynomially-bounded local Lipschitz constants all have the same mean-square convergence rate as their deterministic counterparts, provided that the variance of the integration noise is not of higher order than the corresponding deterministic integrator.|['H. C. Lie', 'A. M. Stuart', 'T. J. Sullivan']|['math.NA', 'math.PR', 'math.ST', 'stat.CO', 'stat.TH', '65L20, 65C99, 37H10, 68W20']
2017-03-16T23:27:49Z|2017-03-10T12:42:32Z|http://arxiv.org/abs/1703.03658v1|http://arxiv.org/pdf/1703.03658v1|Construction of Non-asymptotic Confidence Sets in 2-Wasserstein Space|In this paper, we consider a probabilistic setting where the probability measures are considered to be random objects. We propose a procedure of construction non-asymptotic confidence sets for empirical barycenters in 2-Wasserstein space and develop the idea further to construction of a non-parametric two-sample test that is then applied to the detection of structural breaks in data with complex geometry. Both procedures mainly rely on the idea of multiplier bootstrap (Spokoiny and Zhilova (2015), Chernozhukov et al. (2014)). The main focus lies on probability measures that have commuting covariance matrices and belong to the same scatter-location family: we proof the validity of a bootstrap procedure that allows to compute confidence sets and critical values for a Wasserstein-based two-sample test.|['Johannes Ebert', 'Vladimir Spokoiny', 'Alexandra Suvorikova']|['math.ST', 'stat.TH']
2017-03-16T23:27:49Z|2017-03-09T19:00:01Z|http://arxiv.org/abs/1703.03412v1|http://arxiv.org/pdf/1703.03412v1|Uniform estimation of a class of random graph functionals|We consider estimation of certain functionals of random graphs. The random graph is generated by a stochastic block model (SBM). The number of classes is fixed or grows with the number of vertices. Minimax lower and upper bounds of estimation along specific submodels are derived. The results are nonasymptotic and imply that uniform estimation of a single connectivity parameter is much slower than the expected asymptotic pointwise rate. Specifically, the uniform quadratic rate does not scale as the number of edges, but only as the number of vertices. The lower bounds are local around any possible SBM. An analogous result is derived for functionals of a class of smooth graphons.|['Ismaël Castillo', 'Peter Orbanz']|['math.ST', 'stat.TH']
2017-03-16T23:27:49Z|2017-03-09T17:18:21Z|http://arxiv.org/abs/1703.03353v1|http://arxiv.org/pdf/1703.03353v1|A Note on Bayesian Model Selection for Discrete Data Using Proper   Scoring Rules|We consider the problem of choosing between parametric models for a discrete observable, taking a Bayesian approach in which the within-model prior distributions are allowed to be improper. In order to avoid the ambiguity in the marginal likelihood function in such a case, we apply a homogeneous scoring rule. For the particular case of distinguishing between Poisson and Negative Binomial models, we conduct simulations that indicate that, applied prequentially, the method will consistently select the true model.|['A. Philip Dawid', 'Monica Musio', 'Silvia Columbu']|['math.ST', 'stat.TH', 'Primary 62C99, secondary 62F15, 62A99']
2017-03-16T23:27:49Z|2017-03-10T16:16:31Z|http://arxiv.org/abs/1703.03282v2|http://arxiv.org/pdf/1703.03282v2|Confidence intervals in high-dimensional regression based on regularized   pseudoinverses|In modern data sets, the number of available variables can greatly exceed the number of observations. In this paper we show how valid confidence intervals can be constructed by approximating the inverse covariance matrix by a scaled Moore-Penrose pseudoinverse, and using the lasso to perform a bias correction. In addition, we propose random least squares, a new regularization technique which yields narrower confidence intervals with the same theoretical validity. Random least squares estimates the inverse covariance matrix using multiple low-dimensional random projections of the data. This is shown to be equivalent to a generalized form of ridge regularization. The methods are illustrated in Monte Carlo experiments and an empirical example using quarterly data from the FRED-QD database, where gross domestic product is explained by a large number of macroeconomic and financial indicators.|['Tom Boot', 'Didier Nibbering']|['math.ST', 'stat.TH']
2017-03-16T23:27:49Z|2017-03-09T11:42:34Z|http://arxiv.org/abs/1703.03237v1|http://arxiv.org/pdf/1703.03237v1|Fractional compound Poisson processes with multiple internal states|For the particles undergoing the anomalous diffusion with different waiting time distributions for different internal states, we derive the Fokker-Planck and Feymann-Kac equations, respectively, describing positions of the particles and functional distributions of the trajectories of particles; in particular, the equations governing the functional distribution of internal states are also obtained. The dynamics of the stochastic processes are analyzed and the applications, calculating the distribution of the first passage time and the distribution of the fraction of the occupation time, of the equations are given.|['Pengbo Xu', 'Weihua Deng']|['math.ST', 'cond-mat.stat-mech', 'stat.TH']
2017-03-16T23:27:49Z|2017-03-09T07:40:53Z|http://arxiv.org/abs/1703.03167v1|http://arxiv.org/pdf/1703.03167v1|Cross-validation|This text is a survey on cross-validation. We define all classical cross-validation procedures, and we study their properties for two different goals: estimating the risk of a given estimator, and selecting the best estimator among a given family. For the risk estimation problem, we compute the bias (which can also be corrected) and the variance of cross-validation methods. For estimator selection, we first provide a first-order analysis (based on expectations). Then, we explain how to take into account second-order terms (from variance computations, and by taking into account the usefulness of overpenalization). This allows, in the end, to provide some guidelines for choosing the best cross-validation method for a given learning problem.|['Sylvain Arlot']|['math.ST', 'stat.ML', 'stat.TH']
2017-03-16T23:27:53Z|2017-03-09T07:27:33Z|http://arxiv.org/abs/1703.03165v1|http://arxiv.org/pdf/1703.03165v1|Perturbation Bootstrap in Adaptive Lasso|The Adaptive LASSO (ALASSO) was proposed by Zou [J. Amer. Statist. Assoc. 101 (2006) 1418-1429] as a modification of the LASSO for the purpose of simultaneous variable selection and estimation of the parameters in a linear regression model. Zou (2006) established that the ALASSO estimator is variable-selection consistent as well as asymptotically Normal in the indices corresponding to the nonzero regression coefficients in certain fixed-dimensional settings. In an influential paper, Minnier, Tian and Cai [J. Amer. Statist. Assoc. 106 (2011) 1371-1382] proposed a perturbation bootstrap method and established its distributional consistency for the ALASSO estimator in the fixed-dimensional setting. In this paper, however, we show that this (naive) perturbation bootstrap fails to achieve second order correctness in approximating the distribution of the ALASSO estimator. We propose a modification to the perturbation bootstrap objective function and show that a suitably studentized version of our modified perturbation bootstrap ALASSO estimator achieves second-order correctness even when the dimension of the model is allowed to grow to infinity with the sample size. As a consequence, inferences based on the modified perturbation bootstrap will be more accurate than the inferences based on the oracle Normal approximation. We give simulation studies demonstrating good finite-sample properties of our modified perturbation bootstrap method as well as an illustration of our method on a real data set.|['Debraj Das', 'Karl Gregory', 'S. N. Lahiri']|['stat.ME', 'math.ST', 'stat.TH']
2017-03-16T23:27:53Z|2017-03-08T21:03:22Z|http://arxiv.org/abs/1703.03031v1|http://arxiv.org/pdf/1703.03031v1|Statistical Inference on Panel Data Models: A Kernel Ridge Regression   Method|We propose statistical inferential procedures for panel data models with interactive fixed effects in a kernel ridge regression framework.Compared with traditional sieve methods, our method is automatic in the sense that it does not require the choice of basis functions and truncation parameters.Model complexity is controlled by a continuous regularization parameter which can be automatically selected by generalized cross validation. Based on empirical processes theory and functional analysis tools, we derive joint asymptotic distributions for the estimators in the heterogeneous setting. These joint asymptotic results are then used to construct confidence intervals for the regression means and prediction intervals for the future observations, both being the first provably valid intervals in literature. Marginal asymptotic normality of the functional estimators in homogeneous setting is also obtained. Simulation and real data analysis demonstrate the advantages of our method.|['Shunan Zhao', 'Ruiqi Liu', 'Zuofeng Shang']|['math.ST', 'stat.TH']
2017-03-16T23:27:53Z|2017-03-09T16:57:54Z|http://arxiv.org/abs/1703.02907v2|http://arxiv.org/pdf/1703.02907v2|Improved bounds for Square-Root Lasso and Square-Root Slope|We show that two estimators, the Square-Root Lasso and the Square-Root Slope can achieve the exact optimal minimax prediction rate, which is $(s/n) \log(p/s)$ in the setting of the sparse high-dimensional linear regression. Here, $n$ is the sample size, $p$ is the dimension and $s$ is the sparsity parameter. We also prove optimality for the estimation error in the $l_q$-norm, with $q \in [1,2]$ for the Square-Root Lasso, and in the $l_2$ and sorted $l_1$ norms for the Square-Root Slope. Both estimators are adaptive to the unknown variance of the noise. The Square-Root Slope is also adaptive to the sparsity $s$ of the true parameter. Moreover, in both cases, the chosen tuning parameters are independent of the confidence level under which the rate is valid, and we obtain improved concentration properties as in [Bellec, Lecu\'e and Tsybakov, 2016] where the case of known variance is treated. Our results are non-asymptotic.|['Alexis Derumigny']|['math.ST', 'stat.TH', '62G08 (Primary), 62C20, 62G05 (Secondary)']
2017-03-16T23:27:53Z|2017-03-08T13:47:17Z|http://arxiv.org/abs/1703.02834v1|http://arxiv.org/pdf/1703.02834v1|Exact Dimensionality Selection for Bayesian PCA|We present a Bayesian model selection approach to estimate the intrinsic dimensionality of a high-dimensional dataset. To this end, we introduce a novel formulation of the probabilisitic principal component analysis model based on a normal-gamma prior distribution. In this context, we exhibit a closed-form expression of the marginal likelihood which allows to infer an optimal number of components. We also propose a heuristic based on the expected shape of the marginal likelihood curve in order to choose the hyperparameters. In non-asymptotic frameworks, we show on simulated data that this exact dimensionality selection approach is competitive with both Bayesian and frequentist state-of-the-art methods.|['Charles Bouveyron', 'Pierre Latouche', 'Pierre-Alexandre Mattei']|['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']
2017-03-16T23:27:53Z|2017-03-08T07:41:13Z|http://arxiv.org/abs/1703.02736v1|http://arxiv.org/pdf/1703.02736v1|Profile Estimation for Partial Functional Partially Linear Single-Index   Model|This paper studies a \textit{partial functional partially linear single-index model} that consists of a functional linear component as well as a linear single-index component. This model generalizes many well-known existing models and is suitable for more complicated data structures. However, its estimation inherits the difficulties and complexities from both components and makes it a challenging problem, which calls for new methodology. We propose a novel profile B-spline method to estimate the parameters by approximating the unknown nonparametric link function in the single-index component part with B-spline, while the linear slope function in the functional component part is estimated by the functional principal component basis. The consistency and asymptotic normality of the parametric estimators are derived, and the global convergence of the proposed estimator of the linear slope function is also established. More excitingly, the latter convergence is optimal in the minimax sense. A two-stage procedure is implemented to estimate the nonparametric link function, and the resulting estimator possesses the optimal global rate of convergence. Furthermore, the convergence rate of the mean squared prediction error for a predictor is also obtained. Empirical properties of the proposed procedures are studied through Monte Carlo simulations. A real data example is also analyzed to illustrate the power and flexibility of the proposed methodology.|['Qingguo Tang', 'Linglong Kong', 'David Ruppert', 'Rohana J. Karunamuni']|['math.ST', 'stat.ME', 'stat.TH']
2017-03-16T23:27:53Z|2017-03-08T06:22:56Z|http://arxiv.org/abs/1703.02724v1|http://arxiv.org/pdf/1703.02724v1|Guaranteed Tensor PCA with Optimality in Statistics and Computation|Tensors, or high-order arrays, attract much attention in recent research. In this paper, we propose a general framework for tensor principal component analysis (tensor PCA), which focuses on the methodology and theory for extracting the hidden low-rank structure from the high-dimensional tensor data. A unified solution is provided for tensor PCA with considerations in both statistical limits and computational costs. The problem exhibits three different phases according to the signal-noise-ratio (SNR). In particular, with strong SNR, we propose a fast spectral power iteration method that achieves the minimax optimal rate of convergence in estimation; with weak SNR, the information-theoretical lower bound shows that it is impossible to have consistent estimation in general; with moderate SNR, we show that the non-convex maximum likelihood estimation provides optimal solution, but with NP-hard computational cost; moreover, under the hardness hypothesis of hypergraphic planted clique detection, there are no polynomial-time algorithms performing consistently in general. Simulation studies show that the proposed spectral power iteration method have good performance under a variety of settings.|['Anru Zhang', 'Dong Xia']|['math.ST', 'cs.LG', 'stat.ME', 'stat.ML', 'stat.TH']
2017-03-16T23:27:53Z|2017-03-08T06:16:28Z|http://arxiv.org/abs/1703.02720v1|http://arxiv.org/pdf/1703.02720v1|Model Selection for Explosive Models|This paper examines the limit properties of information criteria (such as AIC, BIC, HQIC) for distinguishing between the unit root model and the various kinds of explosive models. The explosive models include the local-to-unit-root model, the mildly explosive model and the regular explosive model. Initial conditions with different order of magnitude are considered. Both the OLS estimator and the indirect inference estimator are studied. It is found that BIC and HQIC, but not AIC, consistently select the unit root model when data come from the unit root model. When data come from the local-to-unit-root model, both BIC and HQIC select the wrong model with probability approaching 1 while AIC has a positive probability of selecting the right model in the limit. When data come from the regular explosive model or from the mildly explosive model in the form of $1+n^{\alpha }/n$ with $\alpha \in (0,1)$, all three information criteria consistently select the true model. Indirect inference estimation can increase or decrease the probability for information criteria to select the right model asymptotically relative to OLS, depending on the information criteria and the true model. Simulation results confirm our asymptotic results in finite sample.|['Yubo Tao', 'Jun Yu']|['math.ST', 'stat.TH', 'G.3']
2017-03-16T23:27:53Z|2017-03-08T03:07:37Z|http://arxiv.org/abs/1703.02679v1|http://arxiv.org/pdf/1703.02679v1|Performance Bounds for Graphical Record Linkage|Record linkage involves merging records in large, noisy databases to remove duplicate entities. It has become an important area because of its widespread occurrence in bibliometrics, public health, official statistics production, political science, and beyond. Traditional linkage methods directly linking records to one another are computationally infeasible as the number of records grows. As a result, it is increasingly common for researchers to treat record linkage as a clustering task, in which each latent entity is associated with one or more noisy database records. We critically assess performance bounds using the Kullback-Leibler (KL) divergence under a Bayesian record linkage framework, making connections to Kolchin partition models. We provide an upper bound using the KL divergence and a lower bound on the minimum probability of misclassifying a latent entity. We give insights for when our bounds hold using simulated data and provide practical user guidance.|['Rebecca C. Steorts', 'Matt Barnes', 'Willie Neiswanger']|['math.ST', 'cs.IT', 'math.IT', 'stat.ME', 'stat.ML', 'stat.TH']
2017-03-16T23:27:53Z|2017-03-07T22:18:35Z|http://arxiv.org/abs/1703.02625v1|http://arxiv.org/pdf/1703.02625v1|On Sampling from Massive Graph Streams|We propose Graph Priority Sampling (GPS), a new paradigm for order-based reservoir sampling from massive streams of graph edges. GPS provides a general way to weight edge sampling according to auxiliary and/or size variables so as to accomplish various estimation goals of graph properties. In the context of subgraph counting, we show how edge sampling weights can be chosen so as to minimize the estimation variance of counts of specified sets of subgraphs. In distinction with many prior graph sampling schemes, GPS separates the functions of edge sampling and subgraph estimation. We propose two estimation frameworks: (1) Post-Stream estimation, to allow GPS to construct a reference sample of edges to support retrospective graph queries, and (2) In-Stream estimation, to allow GPS to obtain lower variance estimates by incrementally updating the subgraph count estimates during stream processing. Unbiasedness of subgraph estimators is established through a new Martingale formulation of graph stream order sampling, which shows that subgraph estimators, written as a product of constituent edge estimators are unbiased, even when computed at different points in the stream. The separation of estimation and sampling enables significant resource savings relative to previous work. We illustrate our framework with applications to triangle and wedge counting. We perform a large-scale experimental study on real-world graphs from various domains and types. GPS achieves high accuracy with less than 1% error for triangle and wedge counting, while storing a small fraction of the graph with average update times of a few microseconds per edge. Notably, for a large Twitter graph with more than 260M edges, GPS accurately estimates triangle counts with less than 1% error, while storing only 40K edges.|['Nesreen K. Ahmed', 'Nick Duffield', 'Theodore Willke', 'Ryan A. Rossi']|['cs.SI', 'cs.DS', 'cs.IR', 'math.ST', 'stat.TH']
2017-03-16T23:27:53Z|2017-03-07T16:38:11Z|http://arxiv.org/abs/1703.02462v1|http://arxiv.org/pdf/1703.02462v1|Convex and non-convex regularization methods for spatial point processes   intensity estimation|This paper deals with feature selection procedures for spatial point processes intensity estimation. We consider regularized versions of estimating equations based on Campbell theorem derived from two classical functions: Poisson likelihood and logistic regression likelihood. We provide general conditions on the spatial point processes and on penalty functions which ensure consistency, sparsity and asymptotic normality. We discuss the numerical implementation and assess finite sample properties in a simulation study. Finally, an application to tropical forestry datasets illustrates the use of the proposed methods.|['Achmad Choiruddin', 'Jean-François Coeurjolly', 'Frédérique Letué']|['stat.ME', 'math.ST', 'stat.TH']
2017-03-16T23:27:58Z|2017-03-07T13:44:56Z|http://arxiv.org/abs/1703.02376v1|http://arxiv.org/pdf/1703.02376v1|On conditional least squares estimation for affine diffusions based on   continuous time observations|We study asymptotic properties of conditional least squares estimators for the drift parameters of two-factor affine diffusions based on continuous time observations. We distinguish three cases: subcritical, critical and supercritical. For all the drift parameters, in the subcritical and supercritical cases, asymptotic normality and asymptotic mixed normality is proved, while in the critical case, non-standard asymptotic behavior is described.|['Beáta Bolyog', 'Gyula Pap']|['math.PR', 'math.ST', 'stat.TH', '60J60, 62F12']
2017-03-16T23:27:58Z|2017-03-13T15:44:35Z|http://arxiv.org/abs/1703.02307v2|http://arxiv.org/pdf/1703.02307v2|Post hoc inference via joint family-wise error rate control|"We introduce a general methodology for post hoc inference in a large-scale multiple testing framework. The approach is called "" user-agnostic "" in the sense that the statistical guarantee on the number of correct rejections holds for any set of candidate items selected by the user (after having seen the data). This task is investigated by defining a suitable criterion, named the joint-family-wise-error rate (JER for short). We propose several procedures for controlling the JER, with a special focus on incorporating dependencies while adapting to the unknown quantity of signal (via a step-down approach). We show that our proposed setting incorporates as particular cases a version of the higher criticism as well as the closed testing based approach of Goeman and Solari (2011). Our theoretical statements are supported by numerical experiments."|['Gilles Blanchard', 'Pierre Neuvial', 'Etienne Roquain']|['math.ST', 'stat.TH']
2017-03-16T23:27:58Z|2017-03-07T07:44:52Z|http://arxiv.org/abs/1703.02251v1|http://arxiv.org/pdf/1703.02251v1|The Maximum Likelihood Degree of Toric Varieties|We study the maximum likelihood degree (ML degree) of toric varieties, known as discrete exponential models in statistics. By introducing scaling coefficients to the monomial parameterization of the toric variety, one can change the ML degree. We show that the ML degree is equal to the degree of the toric variety for generic scalings, while it drops if and only if the scaling vector is in the locus of the principal $A$-determinant. We also illustrate how to compute the ML estimate of a toric variety numerically via homotopy continuation from a scaled toric variety with low ML degree. Throughout, we include examples motivated by algebraic geometry and statistics. We compute the ML degree of rational normal scrolls and a large class of Veronese-type varieties. In addition, we investigate the ML degree of scaled Segre varieties, hierarchical loglinear models, and graphical models.|['Carlos Améndola', 'Nathan Bliss', 'Isaac Burke', 'Courtney R. Gibbons', 'Martin Helmer', 'Serkan Hoşten', 'Evan D. Nash', 'Jose Israel Rodriguez', 'Daniel Smolkin']|['math.AG', 'math.ST', 'stat.CO', 'stat.TH', '14Q15, 14M25, 13P15, 62F10']
2017-03-16T23:27:58Z|2017-03-06T15:03:55Z|http://arxiv.org/abs/1703.01913v1|http://arxiv.org/pdf/1703.01913v1|Near-Optimal Closeness Testing of Discrete Histogram Distributions|We investigate the problem of testing the equivalence between two discrete histograms. A {\em $k$-histogram} over $[n]$ is a probability distribution that is piecewise constant over some set of $k$ intervals over $[n]$. Histograms have been extensively studied in computer science and statistics. Given a set of samples from two $k$-histogram distributions $p, q$ over $[n]$, we want to distinguish (with high probability) between the cases that $p = q$ and $\ p-q\ _1 \geq \epsilon$. The main contribution of this paper is a new algorithm for this testing problem and a nearly matching information-theoretic lower bound. Specifically, the sample complexity of our algorithm matches our lower bound up to a logarithmic factor, improving on previous work by polynomial factors in the relevant parameters. Our algorithmic approach applies in a more general setting and yields improved sample upper bounds for testing closeness of other structured distributions as well.|['Ilias Diakonikolas', 'Daniel M. Kane', 'Vladimir Nikishkin']|['cs.DS', 'cs.IT', 'cs.LG', 'math.IT', 'math.ST', 'stat.TH']
2017-03-16T23:27:58Z|2017-03-06T09:25:09Z|http://arxiv.org/abs/1703.01777v1|http://arxiv.org/pdf/1703.01777v1|D-optimal design for multivariate polynomial regression via the   Christoffel function and semidefinite relaxations|We present a new approach to the design of D-optimal experiments with multivariate polynomial regressions on compact semi-algebraic design spaces. We apply the moment-sum-of-squares hierarchy of semidefinite programming problems to solve numerically and approximately the optimal design problem. The geometry of the design is recovered with semidefinite programming duality theory and the Christoffel polynomial.|['Yohann De Castro', 'F Gamboa', 'D Henrion', 'R Hess', 'J. -B Lasserre']|['math.ST', 'math.OC', 'stat.TH']
2017-03-16T23:27:58Z|2017-03-06T04:31:36Z|http://arxiv.org/abs/1703.01721v1|http://arxiv.org/pdf/1703.01721v1|The Bennett-Orlicz norm|Lederer and van de Geer (2013) introduced a new Orlicz norm, the Bernstein-Orlicz norm, which is connected to Bernstein type inequalities. Here we introduce another Orlicz norm, the Bennett-Orlicz norm, which is connected to Bennett type inequalities. The new Bennett-Orlicz norm yields inequalities for expectations of maxima which are potentially somewhat tighter than those resulting from the Bernstein-Orlicz norm when they are both applicable. We discuss cross connections between these norms, exponential inequalities of the Bernstein, Bennett, and Prokhorov types, and make comparisons with results of Talagrand (1989, 1994), and Boucheron, Lugosi, and Massart (2013).|['Jon A. Wellner']|['math.ST', 'stat.TH', '60E15, 62E17, 62H10, 46E30']
2017-03-16T23:27:58Z|2017-03-05T20:09:44Z|http://arxiv.org/abs/1703.01658v1|http://arxiv.org/pdf/1703.01658v1|The wrapping hull and a unified framework for estimating the volume of a   body|This paper develops a unified framework for estimating the volume of a set in $\mathbb{R}^d$ based on observations of points uniformly distributed over the set. The framework applies to all classes of sets satisfying one simple axiom: a class is assumed to be intersection stable. No further hypotheses on the boundary of the set are imposed; in particular, the convex sets and the so-called weakly-convex sets are covered by the framework. The approach rests upon a homogeneous Poisson point process model. We introduce the so-called wrapping hull, a generalization of the convex hull, and prove that it is a sufficient and complete statistic. The proposed estimator of the volume is simply the volume of the wrapping hull scaled with an appropriate factor. It is shown to be consistent for all classes of sets satisfying the axiom and mimics an unbiased estimator with uniformly minimal variance. The construction and proofs hinge upon an interplay between probabilistic and geometric arguments. The tractability of the framework is numerically confirmed in a variety of examples.|['Nicolai Baldin']|['math.ST', 'stat.TH', '60G55, 62G05, 62M30']
2017-03-16T23:27:58Z|2017-03-05T19:37:52Z|http://arxiv.org/abs/1703.01654v1|http://arxiv.org/pdf/1703.01654v1|"À propos des tentatives visant à construire un estimateur   ""universel"" à partir de données indépendantes"|This paper is based on our personal notes for the short course we gave on January 5, 2017 at Institut Henri Poincar\'e, after an invitation of the SFdS. Our purpose is to give an overview of the method of $\rho$-estimation and of the optimality and robustness properties of the estimators built according to this procedure. This method can be viewed as the sequel of a long series of researches which were devoted to the construction of estimators with good properties in various statistical frameworks. We shall emphasize the connection between the $\rho$-estimators and the previous ones, in particular the maximum likelihood estimator, and we shall show, via some typical examples, that the $\rho$-estimators perform better from various points of view.   ------   Cet article est fond\'e sur les notes du mini-cours que nous avons donn\'e le 5 janvier 2017 \`a l'Institut Henri Poincar\'e \`a l'occasion d'une journ\'ee organis\'ee par la SFdS et consacr\'ee \`a la Statistique Math\'ematique. Il vise \`a donner un aper\c{c}u de la m\'ethode de $\rho$-estimation ainsi que des propri\'et\'es d'optimalit\'e et de robustesse des estimateurs construits selon cette proc\'edure. Cette m\'ethode s'inscrit dans une longue lign\'ee de recherches dont l'objectif a \'et\'e de produire des estimateurs poss\'edant de bonnes propri\'et\'es pour un ensemble de cadres statistiques aussi vaste que possible. Nous mettrons en lumi\`ere les liens forts qui existent entre les $\rho$-estimateurs et ces pr\'ed\'ecesseurs, notamment les estimateurs du maximum de vraisemblance, mais montrerons \'egalement, au travers d'exemples choisis, que les $\rho$-estimateurs les surpassent sur bien des aspects.|['Yannick Baraud', 'Lucien Birgé']|['math.ST', 'stat.TH', '62G05']
2017-03-16T23:27:58Z|2017-03-04T22:15:27Z|http://arxiv.org/abs/1703.01527v1|http://arxiv.org/pdf/1703.01527v1|Power Allocation for Full-Duplex Relay Selection in Underlay Cognitive   Radio Networks: Coherent versus Non-Coherent Scenarios|This paper investigates power control and relay selection in Full Duplex Cognitive Relay Networks (FDCRNs), where the secondary-user (SU) relays can simultaneously receive data from the SU source and forward them to the SU destination. We study both non-coherent and coherent scenarios. In the non-coherent case, the SU relay forwards the signal from the SU source without regulating the phase; while in the coherent scenario, the SU relay regulates the phase when forwarding the signal to minimize the interference at the primary-user (PU) receiver. We consider the problem of maximizing the transmission rate from the SU source to the SU destination subject to the interference constraint at the PU receiver and power constraints at both the SU source and SU relay. We then develop a mathematical model to analyze the data rate performance of the FDCRN considering the self-interference effects at the FD relay. We develop low-complexity and high-performance joint power control and relay selection algorithms. Extensive numerical results are presented to illustrate the impacts of power level parameters and the self-interference cancellation quality on the rate performance. Moreover, we demonstrate the significant gain of phase regulation at the SU relay.|['Le Thanh Tan', 'Lei Ying', 'Daniel W. Bliss']|['cs.IT', 'cs.NI', 'math.IT', 'math.ST', 'stat.TH']
2017-03-16T23:27:58Z|2017-03-04T21:48:41Z|http://arxiv.org/abs/1703.01525v1|http://arxiv.org/pdf/1703.01525v1|Power Control and Relay Selection in Full-Duplex Cognitive Relay   Networks: Coherent versus Non-coherent Scenarios|This paper investigates power control and relay selection in Full Duplex Cognitive Relay Networks (FDCRNs), where the secondary-user (SU) relays can simultaneously receive and forward the signal from the SU source. We study both non-coherent and coherent scenarios. In the non-coherent case, the SU relay forwards the signal from the SU source without regulating the phase, while in the coherent scenario, the SU relay regulates the phase when forwarding the signal to minimize the interference at the primary-user (PU) receiver. We consider the problem of maximizing the transmission rate from the SU source to the SU destination subject to the interference constraint at the PU receiver and power constraints at both the SU source and SU relay. We develop low-complexity and high-performance joint power control and relay selection algorithms. The superior performance of the proposed algorithms are confirmed using extensive numerical evaluation. In particular, we demonstrate the significant gain of phase regulation at the SU relay (i.e., the gain of the coherent mechanism over the noncoherent mechanism).|['Le Thanh Tan', 'Lei Ying', 'Daniel W. Bliss']|['cs.IT', 'cs.NI', 'math.IT', 'math.ST', 'stat.TH']
2017-03-16T23:28:02Z|2017-03-04T15:13:41Z|http://arxiv.org/abs/1703.01474v1|http://arxiv.org/pdf/1703.01474v1|Sharp bounds for population recovery|The population recovery problem is a basic problem in noisy unsupervised learning that has attracted significant research attention in recent years [WY12,DRWY12, MS13, BIMP13, LZ15,DST16]. A number of different variants of this problem have been studied, often under assumptions on the unknown distribution (such as that it has restricted support size). In this work we study the sample complexity and algorithmic complexity of the most general version of the problem, under both bit-flip noise and erasure noise model. We give essentially matching upper and lower sample complexity bounds for both noise models, and efficient algorithms matching these sample complexity bounds up to polynomial factors.|"['Anindya De', ""Ryan O'Donnell"", 'Rocco Servedio']"|['cs.DS', 'cs.LG', 'math.ST', 'stat.TH']
2017-03-16T23:28:02Z|2017-03-04T09:12:42Z|http://arxiv.org/abs/1703.01421v1|http://arxiv.org/pdf/1703.01421v1|$l_0$-estimation of piecewise-constant signals on graphs|We study recovery of piecewise-constant signals over arbitrary graphs by the estimator minimizing an $l_0$-edge-penalized objective. Although exact minimization of this objective may be computationally intractable, we show that the same statistical risk guarantees are achieved by the alpha-expansion algorithm which approximately minimizes this objective in polynomial time. We establish that for graphs with small average vertex degree, these guarantees are rate-optimal in a minimax sense over classes of edge-sparse signals. For application to spatially inhomogeneous graphs, we propose minimization of an edge-weighted variant of this objective where each edge is weighted by its effective resistance or another measure of its contribution to the graph's connectivity. We establish minimax optimality of the resulting estimators over corresponding edge-weighted sparsity classes. We show theoretically that these risk guarantees are not always achieved by the estimator minimizing the $l_1$/total-variation relaxation, and empirically that the $l_0$-based estimates are more accurate in high signal-to-noise settings.|['Zhou Fan', 'Leying Guan']|['stat.ME', 'math.ST', 'stat.CO', 'stat.TH']
2017-03-16T23:28:02Z|2017-03-04T00:16:54Z|http://arxiv.org/abs/1703.01364v1|http://arxiv.org/pdf/1703.01364v1|A Matrix Variate Skew-t Distribution|Although there is ample work in the literature dealing with skewness in the multivariate setting, there is a relative paucity of work in the matrix variate paradigm. Such work is, for example, useful for modelling three-way data. A matrix variate skew-t distribution is derived based on a mean-variance matrix normal mixture. An expectation-conditional maximization algorithm is developed for parameter estimation. Simulated data are used for illustration.|['Michael P. B. Gallaugher', 'Paul D. McNicholas']|['stat.ME', 'math.ST', 'stat.TH']
2017-03-16T23:28:02Z|2017-03-14T22:44:25Z|http://arxiv.org/abs/1703.01332v2|http://arxiv.org/pdf/1703.01332v2|Optimistic lower bounds for convex regularized least-squares|Minimax lower bounds are pessimistic in nature: for any given estimator, minimax lower bounds yield the existence of a worst-case target vector $\beta^*_{worst}$ for which the prediction error of the given estimator is bounded from below. However, minimax lower bounds shed no light on the prediction error of the given estimator for target vectors different than $\beta^*_{worst}$. A characterization of the prediction error of any convex regularized least-squares is given. This characterization provide both a lower bound and an upper bound on the prediction error. This produces lower bounds that are applicable for any target vector and not only for a single, worst-case $\beta^*_{worst}$. Finally, these lower and upper bounds on the prediction error are applied to the Lasso is sparse linear regression. We obtain a lower bound involving the compatibility constant for any tuning parameter, matching upper and lower bounds for the universal choice of the tuning parameter, and a lower bound for the Lasso with small tuning parameter.|['Pierre C Bellec']|['math.ST', 'stat.TH']
2017-03-16T23:28:02Z|2017-03-03T20:14:59Z|http://arxiv.org/abs/1703.01326v1|http://arxiv.org/pdf/1703.01326v1|Prediction based on the Kennedy-O'Hagan calibration model: asymptotic   consistency and other properties|Kennedy and O'Hagan (2001) propose a model for calibrating some unknown parameters in a computer model and estimating the discrepancy between the computer output and physical response. This model is known to have certain identifiability issues. Tuo and Wu (2016) show that there are examples for which the Kennedy-O'Hagan method renders unreasonable results in calibration. In spite of its unstable performance in calibration, the Kennedy-O'Hagan approach has a more robust behavior in predicting the physical response. In this work, we present some theoretical analysis to show the consistency of predictor based on their calibration model in the context of radial basis functions.|['Rui Tuo', 'C. F. Jeff Wu']|['math.ST', 'stat.TH']
2017-03-16T23:28:02Z|2017-03-07T08:29:02Z|http://arxiv.org/abs/1703.01232v2|http://arxiv.org/pdf/1703.01232v2|Inconsistency of Template Estimation with the Fr{é}chet mean in   Quotient Space|We tackle the problem of template estimation when data have been randomly transformed under an isometric group action in the presence of noise. In order to estimate the template, one often minimizes the variance when the influence of the transformations have been removed (computation of the Fr{\'e}chet mean in quotient space). The consistency bias is defined as the distance (possibly zero) between the orbit of the template and the orbit of one element which minimizes the variance. In this article we establish an asymptotic behavior of the consistency bias with respect to the noise level. This behavior is linear with respect to the noise level. As a result the inconsistency is unavoidable as soon as the noise is large enough. In practice, the template estimation with a finite sample is often done with an algorithm called max-max. We show the convergence of this algorithm to an empirical Karcher mean. Finally, our numerical experiments show that the bias observed in practice cannot be attributed to the small sample size or to a convergence problem but is indeed due to the previously studied inconsistency.|['Loïc Devilliers', 'Xavier Pennec', 'Stéphanie Allassonnière']|['math.ST', 'stat.TH']
2017-03-16T23:28:02Z|2017-03-02T19:05:00Z|http://arxiv.org/abs/1703.00918v1|http://arxiv.org/pdf/1703.00918v1|A note on conditional covariance matrices for elliptical distributions|In this short note we provide an analytical formula for the conditional covariance matrices of the elliptically distributed random vectors, when the conditioning is based on the values of any linear combination of the marginal random variables. We show that one could introduce the univariate invariant depending solely on the conditioning set, which greatly simplifies the calculations. As an application, we show that one could define uniquely defined quantile-based sets on which conditional covariance matrices must be equal to each other if only the vector is multivariate normal. The similar results are obtained for conditional correlation matrices of the general elliptic case.|['Piotr Jaworski', 'Marcin Pitera']|['math.PR', 'math.ST', 'q-fin.RM', 'stat.TH', '62H05, 60E05']
2017-03-16T23:28:02Z|2017-03-02T17:48:18Z|http://arxiv.org/abs/1703.00871v1|http://arxiv.org/pdf/1703.00871v1|Bootstrap confidence sets for spectral projectors of sample covariance|Let $X_{1},\ldots,X_{n}$ be i.i.d. sample in $\mathbb{R}^{p}$ with zero mean and the covariance matrix $\mathbf{\Sigma}$. The problem of recovering the projector onto an eigenspace of $\mathbf{\Sigma}$ from these observations naturally arises in many applications. Recent technique from [Koltchinskii, Lounici, 2015] helps to study the asymptotic distribution of the distance in the Frobenius norm $\  \mathbf{P}_r - \widehat{\mathbf{P}}_r \ _{2}$ between the true projector $\mathbf{P}_r$ on the subspace of the $r$-th eigenvalue and its empirical counterpart $\widehat{\mathbf{P}}_r$ in terms of the effective rank of $\mathbf{\Sigma}$. This paper offers a bootstrap procedure for building sharp confidence sets for the true projector $\mathbf{P}_r$ from the given data. This procedure does not rely on the asymptotic distribution of $\  \mathbf{P}_r - \widehat{\mathbf{P}}_r \ _{2}$ and its moments. It could be applied for small or moderate sample size $n$ and large dimension $p$. The main result states the validity of the proposed procedure for finite samples with an explicit error bound for the error of bootstrap approximation. This bound involves some new sharp results on Gaussian comparison and Gaussian anti-concentration in high-dimensional spaces. Numeric results confirm a good performance of the method in realistic examples.|['Alexey Naumov', 'Vladimir Spokoiny', 'Vladimir Ulyanov']|['math.ST', 'math.PR', 'stat.TH']
2017-03-16T23:28:02Z|2017-03-02T07:31:13Z|http://arxiv.org/abs/1703.00647v1|http://arxiv.org/pdf/1703.00647v1|Inference for Multiple Change-points in Linear and Non-linear Time   Series Models|In this paper we develop a generalized likelihood ratio scan method (GLRSM) for multiple change-points inference in piecewise stationary time series, which estimates the number and positions of change-points and provides a confidence interval for each change-point. The computational complexity of using GLRSM for multiple change-points detection is as low as $O(n(\log n)^3)$ for a series of length $n$. Consistency of the estimated numbers and positions of the change-points is established. Extensive simulation studies are provided to demonstrate the effectiveness of the proposed methodology under different scenarios.|['Wai Leong Ng', 'Shenyi Pan', 'Chun Yip Yau']|['math.ST', 'stat.TH']
2017-03-16T23:28:02Z|2017-03-01T23:08:33Z|http://arxiv.org/abs/1703.00542v1|http://arxiv.org/pdf/1703.00542v1|A note on the approximate admissibility of regularized estimators in the   Gaussian sequence model|"We study the problem of estimating an unknown vector $\theta$ from an observation $X$ drawn according to the normal distribution with mean $\theta$ and identity covariance matrix under the knowledge that $\theta$ belongs to a known closed convex set $\Theta$. In this general setting, Chatterjee (2014) proved that the natural constrained least squares estimator is ""approximately admissible"" for every $\Theta$. We extend this result by proving that the same property holds for all convex penalized estimators as well. Moreover, we simplify and shorten the original proof considerably. We also provide explicit upper and lower bounds for the universal constant underlying the notion of approximate admissibility."|['Xi Chen', 'Adityanand Guntuboyina', 'Yuchen Zhang']|['math.ST', 'stat.TH']
2017-03-16T23:28:06Z|2017-03-01T22:53:13Z|http://arxiv.org/abs/1703.00539v1|http://arxiv.org/pdf/1703.00539v1|Learning Determinantal Point Processes with Moments and Cycles|Determinantal Point Processes (DPPs) are a family of probabilistic models that have a repulsive behavior, and lend themselves naturally to many tasks in machine learning where returning a diverse set of objects is important. While there are fast algorithms for sampling, marginalization and conditioning, much less is known about learning the parameters of a DPP. Our contribution is twofold: (i) we establish the optimal sample complexity achievable in this problem and show that it is governed by a natural parameter, which we call the \emph{cycle sparsity}; (ii) we propose a provably fast combinatorial algorithm that implements the method of moments efficiently and achieves optimal sample complexity. Finally, we give experimental results that confirm our theoretical findings.|['John Urschel', 'Victor-Emmanuel Brunel', 'Ankur Moitra', 'Philippe Rigollet']|['math.ST', 'stat.TH', '62M30, 60G55, 62C20, 05C38']
2017-03-16T23:28:06Z|2017-03-01T19:25:14Z|http://arxiv.org/abs/1703.00471v1|http://arxiv.org/pdf/1703.00471v1|Multidimensional Sampling of Isotropically Bandlimited Signals|A new lower bound on the average reconstruction error variance of multidimensional sampling and reconstruction is presented. It applies to sampling on arbitrary lattices in arbitrary dimensions, assuming a stochastic process with constant, isotropically bandlimited spectrum and reconstruction by the best linear interpolator. The lower bound is exact for any lattice at sufficiently high and low sampling rates. The two threshold rates where the error variance deviates from the lower bound gives two optimality criteria for sampling lattices. It is proved that at low rates, near the first threshold, the optimal lattice is the dual of the best sphere-covering lattice, which for the first time establishes a rigorous relation between optimal sampling and optimal sphere covering. A previously known result is confirmed at high rates, near the second threshold, namely, that the optimal lattice is the dual of the best sphere-packing lattice. Numerical results quantify the performance of various lattices for sampling and support the theoretical optimality criteria.|['Erik Agrell', 'Balázs Csébfalvi']|['cs.IT', 'math.IT', 'math.ST', 'stat.TH']
2017-03-16T23:28:06Z|2017-03-01T19:08:00Z|http://arxiv.org/abs/1703.00469v1|http://arxiv.org/pdf/1703.00469v1|Confidence Bands for Coefficients in High Dimensional Linear Models with   Error-in-variables|We study high-dimensional linear models with error-in-variables. Such models are motivated by various applications in econometrics, finance and genetics. These models are challenging because of the need to account for measurement errors to avoid non-vanishing biases in addition to handle the high dimensionality of the parameters. A recent growing literature has proposed various estimators that achieve good rates of convergence. Our main contribution complements this literature with the construction of simultaneous confidence regions for the parameters of interest in such high-dimensional linear models with error-in-variables.   These confidence regions are based on the construction of moment conditions that have an additional orthogonal property with respect to nuisance parameters. We provide a construction that requires us to estimate an additional high-dimensional linear model with error-in-variables for each component of interest. We use a multiplier bootstrap to compute critical values for simultaneous confidence intervals for a subset $S$ of the components. We show its validity despite of possible model selection mistakes, and allowing for the cardinality of $S$ to be larger than the sample size.   We apply and discuss the implications of our results to two examples and conduct Monte Carlo simulations to illustrate the performance of the proposed procedure.|['Alexandre Belloni', 'Victor Chernozhukov', 'Abhishek Kaul']|['math.ST', 'stat.TH']
2017-03-16T23:28:06Z|2017-03-01T15:49:03Z|http://arxiv.org/abs/1703.00353v1|http://arxiv.org/pdf/1703.00353v1|Matrix product moments in normal variables|Let ${\cal X }=XX^{\prime}$ be a random matrix associated with a centered $r$-column centered Gaussian vector $X$ with a covariance matrix $P$. In this article we compute expectations of matrix-products of the form $\prod_{1\leq i\leq n}({\cal X } P^{v_i})$ for any $n\geq 1$ and any multi-index parameters $v_i\in\mathbb{N}$. We derive closed form formulae and a simple sequential algorithm to compute these matrices w.r.t. the parameter $n$. The second part of the article is dedicated to a non commutative binomial formula for the central matrix-moments $\mathbb{E}\left(\left[{\cal X }-P\right]^n\right)$. The matrix product moments discussed in this study are expressed in terms of polynomial formulae w.r.t. the powers of the covariance matrix, with coefficients depending on the trace of these matrices. We also derive a series of estimates w.r.t. the Loewner order on quadratic forms. For instance we shall prove the rather crude estimate $\mathbb{E}\left(\left[{\cal X }-P\right]^n\right)\leq \mathbb{E}\left({\cal X }^n-P^n\right)$, for any $n\geq 1$|['Pierre Del Moral', 'Adrian N. Bishop']|['math.ST', 'stat.TH', '15B52, 60B20, 46L53, 05A10']
2017-03-16T23:28:06Z|2017-03-01T15:03:04Z|http://arxiv.org/abs/1703.00329v1|http://arxiv.org/pdf/1703.00329v1|Convergence rate of a simulated annealing algorithm with noisy   observations|In this paper we propose a modified version of the simulated annealing algorithm for solving a stochastic global optimization problem. More precisely, we address the problem of finding a global minimizer of a function with noisy evaluations. We provide a rate of convergence and its optimized parametrization to ensure a minimal number of evaluations for a given accuracy and a confidence level close to 1. This work is completed with a set of numerical experimentations and assesses the practical performance both on benchmark test cases and on real world examples.|['Clément Bouttier', 'Ioana Gavra']|['stat.ML', 'math.OC', 'math.ST', 'stat.TH']
2017-03-16T23:28:06Z|2017-03-01T07:57:26Z|http://arxiv.org/abs/1703.00167v1|http://arxiv.org/pdf/1703.00167v1|Adaptive estimation of the sparsity in the Gaussian vector model|Consider the Gaussian vector model with mean value {\theta}. We study the twin problems of estimating the number  {\theta} _0 of non-zero components of {\theta} and testing whether  {\theta} _0 is smaller than some value. For testing, we establish the minimax separation distances for this model and introduce a minimax adaptive test. Extensions to the case of unknown variance are also discussed. Rewriting the estimation of  {\theta} _0 as a multiple testing problem of all hypotheses { {\theta} _0 <= q}, we both derive a new way of assessing the optimality of a sparsity estimator and we exhibit such an optimal procedure. This general approach provides a roadmap for estimating the complexity of the signal in various statistical models.|['Alexandra Carpentier', 'Nicolas Verzelen']|['math.ST', 'stat.TH', '62C20, 62G10, 62B10']
2017-03-16T23:28:06Z|2017-03-06T15:19:31Z|http://arxiv.org/abs/1702.08900v2|http://arxiv.org/pdf/1702.08900v2|Asymptotic Exponentiality of the First Exit Time of the Shiryaev-Roberts   Diffusion with Constant Positive Drift|We consider the first exit time of a Shiryaev-Roberts diffusion with constant positive drift from the interval $[0,A]$ where $A>0$. We show that the moment generating function (Laplace transform) of a suitably standardized version of the first exit time converges to that of the unit-mean exponential distribution as $A\to+\infty$. The proof is explicit in that the moment generating function of the first exit time is first expressed analytically and in a closed form, and then the desired limit as $A\to+\infty$ is evaluated directly. The result is of importance in the area of quickest change-point detection, and its discrete-time counterpart has been previously established - although in a different manner - by Pollak and Tartakovsky (2009).|['Aleksey S. Polunchenko']|['stat.ME', 'math.ST', 'stat.TH', '62L10, 60G40, 60J60']
2017-03-16T23:28:06Z|2017-02-28T18:28:07Z|http://arxiv.org/abs/1702.08895v1|http://arxiv.org/pdf/1702.08895v1|Minimax density estimation for growing dimension|This paper presents minimax rates for density estimation when the data dimension $d$ is allowed to grow with the number of observations $n$ rather than remaining fixed as in previous analyses. We prove a non-asymptotic lower bound which gives the worst-case rate over standard classes of smooth densities, and we show that kernel density estimators achieve this rate. We also give oracle choices for the bandwidth and derive the fastest rate $d$ can grow with $n$ to maintain estimation consistency.|['Daniel J. McDonald']|['math.ST', 'stat.TH']
2017-03-16T23:28:06Z|2017-02-28T13:50:56Z|http://arxiv.org/abs/1702.08787v1|http://arxiv.org/pdf/1702.08787v1|Compound Poisson approximation to estimate the Lévy density|"We construct an estimator of the L\'evy density, with respect to the Lebesgue measure, of a pure jump L\'evy process from high frequency observations: we observe one trajectory of the L\'evy process over [0, T] at the sampling rate $\Delta$, where $\Delta$ $\rightarrow$ 0 as T $\rightarrow$ $\infty$. The main novelty of our result is that we directly estimate the L\'evy density in cases where the process may present infinite activity. Moreover, we study the risk of the estimator with respect to L\_p loss functions, 1 $\le$ p \textless{} $\infty$, whereas existing results only focus on p $\in$ {2, $\infty$}. The main idea behind the estimation procedure that we propose is to use that ""every infinitely divisible distribution is the limit of a sequence of compound Poisson distributions"" (see e.g. Corollary 8.8 in Sato (1999)) and to take advantage of the fact that it is well known how to estimate the L\'evy density of a compound Poisson process in the high frequency setting. We consider linear wavelet estimators and the performance of our procedure is studied in term of L\_p loss functions, p $\ge$ 1, over Besov balls. The results are illustrated on several examples."|['Céline Duval', 'Ester Mariucci']|['math.PR', 'math.ST', 'stat.TH']
2017-03-16T23:28:06Z|2017-02-28T02:54:56Z|http://arxiv.org/abs/1702.08615v1|http://arxiv.org/pdf/1702.08615v1|Bridging Finite and Super Population Causal Inference|There are two general views in causal analysis of experimental data: the super population view that the units are an independent sample from some hypothetical infinite populations, and the finite population view that the potential outcomes of the experimental units are fixed and the randomness comes solely from the physical randomization of the treatment assignment. These two views differs conceptually and mathematically, resulting in different sampling variances of the usual difference-in-means estimator of the average causal effect. Practically, however, these two views result in identical variance estimators. By recalling a variance decomposition and exploiting a completeness-type argument, we establish a connection between these two views in completely randomized experiments. This alternative formulation could serve as a template for bridging finite and super population causal inference in other scenarios.|['Peng Ding', 'Xinran Li', 'Luke W. Miratrix']|['math.ST', 'stat.TH']
2017-03-16T23:28:10Z|2017-02-27T21:52:17Z|http://arxiv.org/abs/1702.08546v1|http://arxiv.org/pdf/1702.08546v1|Optimal rates of estimation for multi-reference alignment|This paper describes optimal rates of adaptive estimation of a vector in the multi-reference alignment model, a problem with important applications in fields such as signal processing, image processing, and computer vision, among others. We describe how this model can be viewed as a multivariate Gaussian mixture model under the constraint that the centers belong to the orbit of a group. This enables us to derive matching upper and lower bounds that feature an interesting dependence on the signal-to-noise ratio of the model. Both upper and lower bounds are articulated around a tight local control of Kullback-Leibler divergences that showcases the central role of moment tensors in this problem.|['Afonso Bandeira', 'Philippe Rigollet', 'Jonathan Weed']|['math.ST', 'stat.TH']
2017-03-16T23:28:10Z|2017-02-27T18:38:28Z|http://arxiv.org/abs/1703.01237v1|http://arxiv.org/pdf/1703.01237v1|How real is the random censorship model in medical studies?|In survival analysis the random censorship model refers to censoring and survival times being independent of each other. It is one of the fundamental assumptions in the theory of survival analysis. We explain the reason for it being so ubiquitous, and we investigate its presence in medical studies. We differentiate two types of censoring in medical studies (dropout and administrative), and we explain their importance in examining the existence of the random censorship model. We show that in order to presume the random censorship model it is not enough to have a design study which conforms to it, but that one needs to provide evidence for its presence in the results. Blindly presuming the random censorship model might lead to the Kaplan-Meier estimator producing biased results, which might have serious consequences when estimating survival in medical studies.|['Damjan Krstajic']|['stat.AP', 'math.ST', 'stat.TH']
2017-03-16T23:28:10Z|2017-02-27T10:01:36Z|http://arxiv.org/abs/1702.08211v1|http://arxiv.org/pdf/1702.08211v1|Online Nonparametric Learning, Chaining, and the Role of Partial   Feedback|We investigate contextual online learning with nonparametric (Lipschitz) comparison classes under different assumptions on losses and feedback information. For full information feedback and Lipschitz losses, we characterize the minimax regret up to log factors by proving an upper bound matching a previously known lower bound. In a partial feedback model motivated by second-price auctions, we prove upper bounds for Lipschitz and semi-Lipschitz losses that improve on the known bounds for standard bandit feedback. Our analysis combines novel results for contextual second-price auctions with a novel algorithmic approach based on chaining. When the context space is Euclidean, our chaining approach is efficient and delivers an even better regret bound.|['Nicolò Cesa-Bianchi', 'Pierre Gaillard', 'Claudio Gentile', 'Sébastien Gerchinovitz']|['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']
2017-03-16T23:28:10Z|2017-02-26T22:59:02Z|http://arxiv.org/abs/1702.08109v1|http://arxiv.org/pdf/1702.08109v1|Constrained Maximum Likelihood Estimators for Densities|We put forward a framework for nonparametric density estimation in situations where the sample is supplemented by information and assumptions about shape, support, continuity, slope, location of modes, density values, etc. These supplements are incorporated as constraints that in conjunction with a maximum likelihood criterion lead to constrained infinite-dimensional optimization problems that we formulate over spaces of semicontinuous functions. These spaces, when equipped with an appropriate metric, offer a series of advantages including simple conditions for existence of estimators and their limits and, in particular, guarantee the convergence of modes of densities. Relying on the approximation theory---epi-convergence---for optimization problems, we provide general conditions under which estimators subject to essentially arbitrary constraints are consistent and illustrate the framework with a number of examples that span classical and novel shape constraints.|['Johannes O. Royset', 'Roger J-B Wets']|['math.ST', 'stat.TH']
2017-03-16T23:28:10Z|2017-02-25T14:53:53Z|http://arxiv.org/abs/1702.07899v1|http://arxiv.org/pdf/1702.07899v1|Are there needles in a moving haystack? Adaptive sensing for detection   of dynamically evolving signals|In this paper we investigate the problem of detecting dynamically evolving signals. We model the signal as an $n$ dimensional vector that is either zero or has $s$ non-zero components. At each time step $t\in \mathbb{N}$ the non-zero components change their location independently with probability $p$. The statistical problem is to decide whether the signal is a zero vector or in fact it has non-zero components. This decision is based on $m$ noisy observations of individual signal components collected at times $t=1,\ldots,m$. We consider two different sensing paradigms, namely adaptive and non-adaptive sensing. For non-adaptive sensing the choice of components to measure has to be decided before the data collection process started, while for adaptive sensing one can adjust the sensing process based on observations collected earlier. We characterize the difficulty of this detection problem in both sensing paradigms in terms of the aforementioned parameters, with special interest to the speed of change of the active components. In addition we provide an adaptive sensing algorithm for this problem and contrast its performance to that of non-adaptive detection algorithms.|['Rui M. Castro', 'Ervin Tánczos']|['math.ST', 'stat.TH']
2017-03-16T23:28:10Z|2017-02-24T23:43:06Z|http://arxiv.org/abs/1702.07803v1|http://arxiv.org/pdf/1702.07803v1|Nonparanormal Information Estimation|We study the problem of using i.i.d. samples from an unknown multivariate probability distribution $p$ to estimate the mutual information of $p$. This problem has recently received attention in two settings: (1) where $p$ is assumed to be Gaussian and (2) where $p$ is assumed only to lie in a large nonparametric smoothness class. Estimators proposed for the Gaussian case converge in high dimensions when the Gaussian assumption holds, but are brittle, failing dramatically when $p$ is not Gaussian. Estimators proposed for the nonparametric case fail to converge with realistic sample sizes except in very low dimensions. As a result, there is a lack of robust mutual information estimators for many realistic data. To address this, we propose estimators for mutual information when $p$ is assumed to be a nonparanormal (a.k.a., Gaussian copula) model, a semiparametric compromise between Gaussian and nonparametric extremes. Using theoretical bounds and experiments, we show these estimators strike a practical balance between robustness and scaling with dimensionality.|['Shashank Singh', 'Barnabás Pøczos']|['math.ST', 'cs.IT', 'math.IT', 'stat.ML', 'stat.TH']
2017-03-16T23:28:10Z|2017-02-24T23:31:04Z|http://arxiv.org/abs/1702.07801v1|http://arxiv.org/pdf/1702.07801v1|Consistent structure estimation of exponential-family random graph   models with additional structure|We consider the challenging problem of statistical inference for exponential-family random graph models given one observation of a random graph with complex dependence (e.g., transitivity). To facilitate statistical inference, we endow random graphs with additional structure. The basic idea is that random graphs are composed of subgraphs with complex dependence. We have shown elsewhere that when the composition of random graphs is known, $M$-estimators of canonical and curved exponential families with complex dependence are consistent. In practice, the composition is known in some applications, but is unknown in others. If the composition is unknown, the first and foremost question is whether it can be recovered. The main consistency results of the paper show that it is possible to do so as long as exponential families satisfy weak dependence and smoothness conditions. These results confirm that exponential-family random graph models with additional structure constitute a promising direction of statistical network analysis.|['Michael Schweinberger']|['math.ST', 'stat.TH']
2017-03-16T23:28:10Z|2017-03-01T00:17:04Z|http://arxiv.org/abs/1702.07795v2|http://arxiv.org/pdf/1702.07795v2|A Study of the Allan Variance for Constant-Mean Non-Stationary Processes|The Allan Variance (AV) is a widely used quantity in areas focusing on error measurement as well as in the general analysis of variance for autocorrelated processes in domains such as engineering and, more specifically, metrology. The form of this quantity is widely used to detect noise patterns and indications of stability within signals. However, the properties of this quantity are not known for commonly occurring processes whose covariance structure is non-stationary and, in these cases, an erroneous interpretation of the AV could lead to misleading conclusions. This paper generalizes the theoretical form of the AV to some non-stationary processes while at the same time being valid also for weakly stationary processes. Some simulation examples show how this new form can help to understand the processes for which the AV is able to distinguish these from the stationary cases and hence allow for a better interpretation of this quantity in applied cases.|['Haotian Xu', 'Stéphane Guerrier', 'Roberto Molinari', 'Yuming Zhang']|['math.ST', 'stat.TH']
2017-03-16T23:28:10Z|2017-02-24T02:09:04Z|http://arxiv.org/abs/1702.07448v1|http://arxiv.org/pdf/1702.07448v1|Optimal Bayesian Minimax Rates for Unconstrained Large Covariance   Matrices|We obtain the optimal Bayesian minimax rate for the unconstrained large covariance matrix of multivariate normal sample with mean zero, when both the sample size, n, and the dimension, p, of the covariance matrix tend to infinity. Traditionally the posterior convergence rate is used to compare the frequentist asymptotic performance of priors, but defining the optimality with it is elusive. We propose a new decision theoretic framework for prior selection and define Bayesian minimax rate. Under the proposed framework, we obtain the optimal Bayesian minimax rate for the spectral norm for all rates of p. We also considered Frobenius norm, Bregman divergence and squared log-determinant loss and obtain the optimal Bayesian minimax rate under certain rate conditions on p. A simulation study is conducted to support the theoretical results.|['Kyoungjae Lee', 'Jaeyong Lee']|['math.ST', 'stat.TH']
2017-03-16T23:28:10Z|2017-02-23T13:49:57Z|http://arxiv.org/abs/1702.07211v1|http://arxiv.org/pdf/1702.07211v1|A minimax and asymptotically optimal algorithm for stochastic bandits|We propose the kl-UCB ++ algorithm for regret minimization in stochastic bandit models with exponential families of distributions. We prove that it is simultaneously asymptotically optimal (in the sense of Lai and Robbins' lower bound) and minimax optimal. This is the first algorithm proved to enjoy these two properties at the same time. This work thus merges two different lines of research, with simple proofs involving no complexity overhead.|['Pierre Ménard', 'Aurélien Garivier']|['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']
2017-03-16T23:28:14Z|2017-02-23T07:22:32Z|http://arxiv.org/abs/1702.07118v1|http://arxiv.org/pdf/1702.07118v1|Warped metrics for location-scale models|This paper argues that a class of Riemannian metrics, called warped metrics, plays a fundamental role in statistical problems involving location-scale models. The paper reports three new results : i) the Rao-Fisher metric of any location-scale model is a warped metric, provided that this model satisfies a natural invariance condition, ii) the analytic expression of the sectional curvature of this metric, iii) the exact analytic solution of the geodesic equation of this metric. The paper applies these new results to several examples of interest, where it shows that warped metrics turn location-scale models into complete Riemannian manifolds of negative sectional curvature. This is a very suitable situation for developing algorithms which solve problems of classification and on-line estimation. Thus, by revealing the connection between warped metrics and location-scale models, the present paper paves the way to the introduction of new efficient statistical algorithms.|['Salem Said', 'Yannick Berthoumieu']|['math.ST', 'math.DG', 'stat.TH']
2017-03-16T23:28:14Z|2017-02-23T03:31:22Z|http://arxiv.org/abs/1702.07082v1|http://arxiv.org/pdf/1702.07082v1|Distributions and Statistical Power of Optimal Signal-Detection Methods   In Finite Cases|In big data analysis for detecting rare and weak signals among $n$ features, some grouping-test methods such as Higher Criticism test (HC), Berk-Jones test (B-J), and $\phi$-divergence test share the similar asymptotical optimality when $n \rightarrow \infty$. However, in practical data analysis $n$ is frequently small and moderately large at most. In order to properly apply these optimal tests and wisely choose them for practical studies, it is important to know how to get the p-values and statistical power of them. To address this problem in an even broader context, this paper provides analytical solutions for a general family of goodness-of-fit (GOF) tests, which covers these optimal tests. For any given i.i.d. and continuous distributions of the input test statistics of the $n$ features, both p-value and statistical power of such a GOF test can be calculated. By calculation we compared the finite-sample performances of asymptotically optimal tests under the normal mixture alternative. Results show that HC is the best choice when signals are rare, while B-J is more robust over various signal patterns. In the application to a real genome-wide association study, results illustrate that the p-value calculation works well, and the optimal tests have potentials for detecting novel disease genes with weak genetic effects. The calculations have been implemented in an R package SetTest and published on the CRAN.|['Hong Zhang', 'Jiashun Jin', 'Zheyang Wu']|['math.ST', 'stat.TH']
2017-03-16T23:28:14Z|2017-02-26T20:06:00Z|http://arxiv.org/abs/1702.07027v2|http://arxiv.org/pdf/1702.07027v2|Nonparametric Inference via Bootstrapping the Debiased Estimator|In this paper, we propose to construct confidence bands by bootstrapping the debiased kernel density estimator (for density estimation) and the debiased local polynomial regression estimator (for regression analysis). The idea of using a debiased estimator was first introduced in Calonico et al. (2015), where they construct a confidence interval of the density function (and regression function) at a given point by explicitly estimating stochastic variations. We extend their ideas and propose a bootstrap approach for constructing confidence bands that is uniform for every point in the support. We prove that the resulting bootstrap confidence band is asymptotically valid and is compatible with most tuning parameter selection approaches, such as the rule of thumb and cross-validation. We further generalize our method to confidence sets of density level sets and inverse regression problems. Simulation studies confirm the validity of the proposed confidence bands/sets.|['Yen-Chi Chen']|['stat.ME', 'math.ST', 'stat.TH', 'Primary 62G15, secondary 62G09, 62G07, 62G08']
2017-03-16T23:28:14Z|2017-02-22T19:25:29Z|http://arxiv.org/abs/1702.06975v1|http://arxiv.org/pdf/1702.06975v1|High dimensional deformed rectangle matrices with applications in matrix   denoising|We consider the recovery of a low rank $M \times N$ matrix $S$ from its noisy observation $\tilde{S}$ in two different regimes. Under the assumption that $M$ is comparable to $N$, we propose two optimal estimators for $S$. Our analysis rely on the local behavior of the large dimensional rectangle matrices with finite rank perturbation. We also derive the convergent limits and rates for the singular values and vectors of such matrices.|['Xiucai Ding']|['math.ST', 'stat.TH']
2017-03-16T23:28:14Z|2017-02-22T19:22:55Z|http://arxiv.org/abs/1702.06972v1|http://arxiv.org/pdf/1702.06972v1|Approximations of the Restless Bandit Problem|The multi-armed restless bandit problem is studied in the case where the pay-offs are not necessarily independent over time nor across the arms. Even though this version of the problem provides a more realistic model for most real-world applications, it cannot be optimally solved in practice since it is known to be PSPACE-hard. The objective of this paper is to characterize special sub-classes of the problem where good approximate solutions can be found using tractable approaches. Specifically, it is shown that in the case where the joint distribution over the arms is $\varphi$-mixing, and under some conditions on the $\varphi$-mixing coefficients, a modified version of UCB can prove optimal. On the other hand, it is shown that when the pay-off distributions are strongly dependent, simple switching strategies may be devised which leverage the strong inter-dependencies. To this end, an example is provided using Gaussian Processes. The techniques developed in this paper apply, more generally, to the problem of online sampling under dependence.|['Steffen Grunewalder', 'Azadeh Khaleghi']|['math.ST', 'cs.LG', 'math.PR', 'stat.ML', 'stat.TH']
2017-03-16T23:28:14Z|2017-02-23T19:01:53Z|http://arxiv.org/abs/1702.06488v2|http://arxiv.org/pdf/1702.06488v2|Distributed Estimation of Principal Eigenspaces|"Principal component analysis (PCA) is fundamental to statistical machine learning. It extracts latent principal factors that contribute to the most variation of the data. When data are stored across multiple machines, however, communication cost can prohibit the computation of PCA in a central location and distributed algorithms for PCA are thus needed. This paper proposes and studies a distributed PCA algorithm: each node machine computes the top $K$ eigenvectors and transmits them to the central server; the central server then aggregates the information from all the node machines and conducts a PCA based on the aggregated information. We investigate the bias and variance for the resulting distributed estimator of the top $K$ eigenvectors. In particular, we show that for distributions with symmetric innovation, the distributed PCA is ""unbiased"". We derive the rate of convergence for distributed PCA estimators, which depends explicitly on the effective rank of covariance, eigen-gap, and the number of machines. We show that when the number of machines is not unreasonably large, the distributed PCA performs as well as the whole sample PCA, even without full access of whole data. The theoretical results are verified by an extensive simulation study. We also extend our analysis to the heterogeneous case where the population covariance matrices are different across local machines but share similar top eigen-structures."|['Jianqing Fan', 'Dong Wang', 'Kaizheng Wang', 'Ziwei Zhu']|['stat.CO', 'math.ST', 'stat.TH']
2017-03-16T23:28:14Z|2017-02-20T16:40:45Z|http://arxiv.org/abs/1702.06055v1|http://arxiv.org/pdf/1702.06055v1|Performance of information criteria used for model selection of Hawkes   process models of financial data|We test three common information criteria (IC) for selecting the order of a Hawkes process with an intensity kernel that can be expressed as a mixture of exponential terms. These processes find application in high-frequency financial data modelling. The information criteria are Akaike's information criterion (AIC), the Bayesian information criterion (BIC) and the Hannan-Quinn criterion (HQ). Since we work with simulated data, we are able to measure the performance of model selection by the success rate of the IC in selecting the model that was used to generate the data. In particular, we are interested in the relation between correct model selection and underlying sample size. The analysis includes realistic sample sizes and parameter sets from recent literature where parameters were estimated using empirical financial intra-day data. We compare our results to theoretical predictions and similar empirical findings on the asymptotic distribution of model selection for consistent and inconsistent IC.|['J. M. Chen', 'A. G. Hawkes', 'E. Scalas', 'M. Trinh']|['q-fin.ST', 'math.ST', 'stat.TH', '60G55']
2017-03-16T23:28:14Z|2017-02-20T14:34:46Z|http://arxiv.org/abs/1702.05985v1|http://arxiv.org/pdf/1702.05985v1|Fano's inequality for random variables|We extend Fano's inequality, which controls the average probability of (disjoint) events in terms of the average of some Kullback-Leibler divergences, to work with arbitrary [0,1]-valued random variables. Our simple two-step methodology is general enough to cover the case of an arbitrary (possibly continuously infinite) family of distributions as well as [0,1]-valued random variables not necessarily summing up to 1. Several novel applications are provided, in which the consideration of random variables is particularly handy. The most important applications deal with the problem of Bayesian posterior concentration (minimax or distribution-dependent) rates and with a lower bound on the regret in non-stochastic sequential learning. We also improve in passing some earlier fundamental results: in particular, we provide a simple and enlightening proof of the refined Pinsker's inequality of Ordentlich and Weinberger and derive a sharper Bretagnolle-Huber inequality.|['Sebastien Gerchinovitz', 'Pierre Ménard', 'Gilles Stoltz']|['math.ST', 'cs.IT', 'math.IT', 'stat.TH']
2017-03-16T23:28:14Z|2017-03-08T18:22:26Z|http://arxiv.org/abs/1702.05960v2|http://arxiv.org/pdf/1702.05960v2|A Statistical Learning Approach to Modal Regression|This paper studies the nonparametric modal regression problem systematically from a statistical learning view. Originally motivated by pursuing a theoretical understanding of the maximum correntropy criterion based regression (MCCR), our study reveals that MCCR with a tending-to-zero scale parameter is essentially modal regression. We show that nonparametric modal regression problem can be approached via the classical empirical risk minimization. Some efforts are then made to develop a framework for analyzing and implementing modal regression. For instance, the modal regression function is described, the modal regression risk is defined explicitly and its \textit{Bayes} rule is characterized; for the sake of computational tractability, the surrogate modal regression risk, which is termed as the generalization risk in our study, is introduced. On the theoretical side, the excess modal regression risk, the excess generalization risk, the function estimation error, and the relations among the above three quantities are studied rigorously. It turns out that under mild conditions, function estimation consistency and convergence may be pursued in modal regression as in vanilla regression protocols, such as mean regression, median regression, and quantile regression. However, it outperforms these regression models in terms of robustness as shown in our study from a re-descending M-estimation view. This coincides with and in return explains the merits of MCCR on robustness. On the practical side, the implementation issues of modal regression including the computational algorithm and the tuning parameters selection are discussed. Numerical assessments on modal regression are also conducted to verify our findings empirically.|['Yunlong Feng', 'Jun Fan', 'Johan A. K. Suykens']|['stat.ML', 'math.ST', 'stat.ME', 'stat.TH']
2017-03-16T23:28:14Z|2017-02-20T11:40:47Z|http://arxiv.org/abs/1702.05933v1|http://arxiv.org/pdf/1702.05933v1|Qualitative robustness for bootstrap approximations|An important property of statistical estimators is qualitative robustness, that is small changes in the distribution of the data only result in small chances of the distribution of the estimator. Moreover, in practice, the distribution of the data is commonly unknown, therefore bootstrap approximations can be used to approximate the distribution of the estimator. Hence qualitative robustness of the statistical estimator under the bootstrap approximation is a desirable property. Currently most theoretical investigations on qualitative robustness assume independent and identically distributed pairs of random variables. However, in practice this assumption is not fulfilled. Therefore, we examine the qualitative robustness of bootstrap approximations for non-i.i.d. random variables, for example $\alpha$-mixing and weakly dependent processes. In the i.i.d. case qualitative robustness is ensured via the continuity of the statistical operator, representing the estimator, see Hampel (1971) and Cuevas and Romo (1993). We show, that qualitative robustness of the bootstrap approximation is still ensured under the assumption that the statistical operator is continuous and under an additional assumption on the stochastic process. In particular, we require a convergence condition of the empirical measure of the underlying process, the so called Varadarajan property.|['Katharina Strohriegl']|['math.ST', 'math.PR', 'stat.TH', '60G20, 62G08, 62G09, 62G35']
2017-03-16T23:28:18Z|2017-02-20T09:51:03Z|http://arxiv.org/abs/1702.05910v1|http://arxiv.org/pdf/1702.05910v1|Spacings Around An Order Statistic|We determine the joint limiting distribution of adjacent spacings around a central, intermediate, or an extreme order statistic $X_{k:n}$ of a random sample of size $n$ from a continuous distribution $F$. For central and intermediate cases, normalized spacings in the left and right neighborhoods are asymptotically i.i.d. exponential random variables. The associated independent Poisson arrival processes are independent of $X_{k:n}$. For an extreme $X_{k:n}$, the asymptotic independence property of spacings fails for $F$ in the domain of attraction of Fr\'{e}chet and Weibull ($\alpha \neq 1$) distributions. This work also provides additional insight into the limiting distribution for the number of observations around $X_{k:n}$ for all three cases.|['H. N. Nagaraja', 'Karthik Bharath', 'Fangyuan Zhang']|['math.ST', 'stat.TH']
2017-03-16T23:28:18Z|2017-02-18T18:23:38Z|http://arxiv.org/abs/1702.05641v1|http://arxiv.org/pdf/1702.05641v1|On discrimination between two close distribution tails|The goodness-of-fit test for discrimination of two tail distribution using higher order statistics is proposed. The consistency of proposed test is proved for two different alternatives. We do not assume belonging the corresponding distribution function to a maximum domain of attraction.|['Igor Vladimirovich Rodionov']|['math.ST', 'stat.TH', '62N03, 62G10']
2017-03-16T23:28:18Z|2017-02-18T10:58:00Z|http://arxiv.org/abs/1702.05599v1|http://arxiv.org/pdf/1702.05599v1|A representation theorem for stochastic processes with separable   covariance functions, and its implications for emulation|Many applications require stochastic processes specified on two- or higher-dimensional domains; spatial or spatial-temporal modelling, for example. In these applications it is attractive, for conceptual simplicity and computational tractability, to propose a covariance function that is separable; e.g., the product of a covariance function in space and one in time. This paper presents a representation theorem for such a proposal, and shows that all processes with continuous separable covariance functions are second-order identical to the product of second-order uncorrelated processes. It discusses the implications of separable or nearly separable prior covariances for the statistical emulation of complicated functions such as computer codes, and critically reexamines the conventional wisdom concerning emulator structure, and size of design.|['Jonathan Rougier']|['math.ST', 'stat.TH']
2017-03-16T23:28:18Z|2017-02-18T06:11:25Z|http://arxiv.org/abs/1702.05574v1|http://arxiv.org/pdf/1702.05574v1|Sample complexity of population recovery|The problem of population recovery refers to estimating a distribution based on incomplete or corrupted samples. Consider a random poll of sample size $n$ conducted on a population of individuals, where each pollee is asked to answer $d$ binary questions. We consider one of the two polling impediments: (a) in lossy population recovery, a pollee may skip each question with probability $\epsilon$; (b) in noisy population recovery, a pollee may lie on each question with probability $\epsilon$. Given $n$ lossy or noisy samples, the goal is to estimate the probabilities of all $2^d$ binary vectors simultaneously within accuracy $\delta$ with high probability.   This paper settles the sample complexity of population recovery. For lossy model, the optimal sample complexity is $\tilde\Theta(\delta^{ -2\max\{\frac{\epsilon}{1-\epsilon},1\}})$, improving the state of the art by Moitra and Saks (2013) in several ways: a lower bound is established, the upper bound is improved and the result is dimension-free. Surprisingly, the sample complexity undergoes a phase transition from parametric to nonparametric rate when $\epsilon$ exceeds $1/2$. For noisy population recovery, the sharp sample complexity turns out to be dimension-dependent and scales as $\exp(\Theta(d^{1/3} \log^{2/3}(1/\delta)))$ except for the trivial cases of $\epsilon=0,1/2$ or $1$.   For both models, our estimators simply compute the empirical mean of a certain function, which is found by pre-solving a linear program (LP). Curiously, the dual LP can be understood as Le Cam's method for lower-bounding the minimax risk, thus establishing the statistical optimality of the proposed estimators. The value of the LP is determined by complex-analytic methods.|['Yury Polyanskiy', 'Ananda Theertha Suresh', 'Yihong Wu']|['math.ST', 'cs.IT', 'math.IT', 'stat.ML', 'stat.TH']
2017-03-16T23:28:18Z|2017-02-18T00:04:15Z|http://arxiv.org/abs/1702.05545v1|http://arxiv.org/pdf/1702.05545v1|Some Theorems on Optimality of a Single Observation Confidence Interval   for the Mean of a Normal Distribution|We consider the problem of finding a proper confidence interval for the mean based on a single observation from a normal distribution with both mean and variance unknown. Portnoy (2017) characterizes the scale-sign invariant rules and shows that the Hunt-Stein construction provides a randomized invariant rule that improves on any given randomized rule in the sense that it has greater minimal coverage among all procedures with a fixed expected length. Mathematical results here provide a specific mixture of two non-randomized invariant rules that achieve the minimax optimality. A multivariate confidence set based on a single observation vector is also developed.|['Stephen Portnoy']|['math.ST', 'stat.TH']
2017-03-16T23:28:18Z|2017-02-17T18:06:27Z|http://arxiv.org/abs/1702.05462v1|http://arxiv.org/pdf/1702.05462v1|Objective Bayesian Analysis for Change Point Problems|In this paper we present an objective approach to change point analysis. In particular, we look at the problem from two perspectives. The first focuses on the definition of an objective prior when the number of change points is known a priori. The second contribution aims to estimate the number of change points by using an objective approach, recently introduced in the literature, based on losses. The latter considers change point estimation as a model selection exercise. We show the performance of the proposed approach on simulated data and on real data sets.|['Laurentiu Hinoveanu', 'Fabrizio Leisen', 'Cristiano Villa']|['stat.ME', 'math.ST', 'stat.AP', 'stat.CO', 'stat.ML', 'stat.TH']
2017-03-16T23:28:18Z|2017-02-17T17:15:23Z|http://arxiv.org/abs/1702.05443v1|http://arxiv.org/pdf/1702.05443v1|How close are the eigenvectors and eigenvalues of the sample and actual   covariance matrices?|How many samples are sufficient to guarantee that the eigenvectors and eigenvalues of the sample covariance matrix are close to those of the actual covariance matrix? For a wide family of distributions, including distributions with finite second moment and distributions supported in a centered Euclidean ball, we prove that the inner product between eigenvectors of the sample and actual covariance matrices decreases proportionally to the respective eigenvalue distance. Our findings imply non-asymptotic concentration bounds for eigenvectors, eigenspaces, and eigenvalues. They also provide conditions for distinguishing principal components based on a constant number of samples.|['Andreas Loukas']|['stat.ML', 'math.ST', 'stat.TH']
2017-03-16T23:28:18Z|2017-02-17T12:06:48Z|http://arxiv.org/abs/1702.05315v1|http://arxiv.org/pdf/1702.05315v1|Estimation for the Prediction of Point Processes with Many Covariates|Estimation of the intensity of a point process is considered within a nonparametric framework. The intensity measure is unknown and depends on covariates, possibly many more than the observed number of jumps. Only a single trajectory of the counting process is observed. Interest lies in estimating the intensity conditional on the covariates. The impact of the covariates is modelled by an additive model where each component can be written as a linear combination of possibly unknown functions. The focus is on prediction as opposed to variable screening. Conditions are imposed on the coefficients of this linear combination in order to control the estimation error. The rates of convergence are optimal when the number of active covariates is large. As an application, the intensity of the buy and sell trades of the New Zealand dollar futures is estimated and a test for forecast evaluation is presented. A simulation is included to provide some finite sample intuition on the model and asymptotic properties.|['Alessio Sancetta']|['math.ST', 'q-fin.TR', 'stat.TH']
2017-03-16T23:28:18Z|2017-02-17T00:12:24Z|http://arxiv.org/abs/1702.05193v1|http://arxiv.org/pdf/1702.05193v1|Stochastic detection of some topological and geometric feature|"This work is closely related to the theories of set estimation and manifold estimation. Our object of interest is a, possibly lower-dimensional, compact set $S \subset {\mathbb R}^d$.   The general aim is to identify (via stochastic procedures) some qualitative or quantitative features of $S$, of geometric or topological character. The available information is just a random sample of points drawn on $S$.   The term ""to identify"" means here to achieve a correct answer almost surely (a.s.) when the sample size tends to infinity. More specifically the paper aims at giving some partial answers to the following questions:   1. Is $S$ full dimensional?   2. If $S$ is full dimensional, is it ""close to a lower dimensional set"" $\mathcal{M}$?   3. If $S$ is ""close to a lower dimensional $\mathcal{M}$"", can we   \indent a) estimate $\mathcal{M}$?   \indent b) estimate some functionals defined on $\mathcal{M}$ (in particular, the Minkowski content of $\mathcal{M}$)?   The theoretical results are complemented with some simulations and graphical illustrations."|['Catherine Aaron', 'Alejandro Cholaquidis', 'Antonio Cuevas']|['math.ST', 'stat.TH']
2017-03-16T23:28:18Z|2017-02-16T19:14:54Z|http://arxiv.org/abs/1702.05113v1|http://arxiv.org/pdf/1702.05113v1|Spatial Adaptation in Trend Filtering|"We study trend filtering, a relatively recent method for univariate nonparametric regression. For a given integer $r \geq 1$, the trend filtering estimator of order $r$ is defined as the minimizer of the sum of squared errors when we constrain (or penalize) the sum of the absolute discrete derivatives of order $r$ over the input points. For $r = 1$, the estimator reduces to total variation regularization which has received much attention in the statistics and image processing literature. In this paper, we study the performance of the trend filtering estimator for every $r \geq 1$, both in the constrained and penalized forms. Our main results show that the estimator is optimally spatially adaptive. Specifically, we prove that, in spite of being a nonparametric estimator, when the underlying function is a (discrete) spline with few ""knots"", the risk (under the global squared error loss) of the trend filtering estimator (with an appropriate choice of the tuning parameter) achieves the parametric $n^{-1}$-rate, up to a logarithmic (multiplicative) factor. Our results also include high probability statements on the loss and are stated in sharp oracle form, i.e., our results apply to misspecification and have leading constant one for the misspecified term. Moreover, some of the metric entropy results used in this paper, derived using connections to fat shattering, are of independent interest."|['Adityanand Guntuboyina', 'Donovan Lieu', 'Sabyasachi Chatterjee', 'Bodhisattva Sen']|['math.ST', 'stat.TH']
2017-03-16T23:28:22Z|2017-02-16T17:53:07Z|http://arxiv.org/abs/1702.05066v1|http://arxiv.org/pdf/1702.05066v1|Maximum Number of Modes of Gaussian Mixtures|Gaussian mixture models are widely used in Statistics. A fundamental aspect of these distributions is the study of the local maxima of the density, or modes. In particular, the number of modes that can arise in a mixture of $k$ Gaussians in $d$ dimensions remains unknown in the general case. We give a brief account of this problem's history and expand on known results in this direction. In particular, we give improved lower bounds and the first upper bound on the maximum number of non-degenerate modes.|['Carlos Améndola', 'Alexander Engström', 'Christian Haase']|['math.ST', 'math.OC', 'math.PR', 'stat.TH', '62E10 (primary), 62H05 (secondary)']
2017-03-16T23:28:22Z|2017-02-16T17:35:06Z|http://arxiv.org/abs/1702.05063v1|http://arxiv.org/pdf/1702.05063v1|A new concentration inequality for the excess risk in least-squares   regression with random design and heteroscedastic noise|We prove a new concentration inequality for the excess risk of a M-estimator in least-squares regression with random design and heteroscedastic noise. This kind of result is a central tool in modern model selection theory, as well as in recent achievements concerning the behavior of regularized estimators such as LASSO, group LASSO and SLOPE.|['Adrien Saumard']|['math.ST', 'stat.ML', 'stat.TH']
2017-03-16T23:28:22Z|2017-02-15T16:31:15Z|http://arxiv.org/abs/1702.04672v1|http://arxiv.org/pdf/1702.04672v1|Factor Analysis for Spectral Estimation|Power spectrum estimation is an important tool in many applications, such as the whitening of noise. The popular multitaper method enjoys significant success, but fails for short signals with few samples. We propose a statistical model where a signal is given by a random linear combination of fixed, yet unknown, stochastic sources. Given multiple such signals, we estimate the subspace spanned by the power spectra of these fixed sources. Projecting individual power spectrum estimates onto this subspace increases estimation accuracy. We provide accuracy guarantees for this method and demonstrate it on simulated and experimental data from cryo-electron microscopy.|['Joakim Andén', 'Amit Singer']|['math.ST', 'stat.TH']
2017-03-16T23:28:22Z|2017-02-15T16:07:34Z|http://arxiv.org/abs/1702.04665v1|http://arxiv.org/pdf/1702.04665v1|Means Moments and Newton's Inequalities|It is shown that Newton's inequalities and the related Maclaurin's inequalities provide several refinements of the fundamental Arithmetic mean - Geometric mean - Harmonic mean inequality in terms of the means and variance of positive real numbers. We also obtain some inequalities involving third and fourth central moments of real numbers.|['R. Sharma', 'A. Sharma', 'R. Saini', 'G. Kapoor']|['math.ST', 'stat.TH']
2017-03-16T23:28:22Z|2017-02-15T15:48:30Z|http://arxiv.org/abs/1702.04656v1|http://arxiv.org/pdf/1702.04656v1|Robust Regression via Mutivariate Regression Depth|This paper studies robust regression in the settings of Huber's $\epsilon$-contamination models. We consider estimators that are maximizers of multivariate regression depth functions. These estimators are shown to achieve minimax rates in the settings of $\epsilon$-contamination models for various regression problems including nonparametric regression, sparse linear regression, reduced rank regression, etc. We also discuss a general notion of depth function for linear operators that has potential applications in robust functional linear regression.|['Chao Gao']|['math.ST', 'stat.ML', 'stat.TH']
2017-03-16T23:28:22Z|2017-02-15T06:53:03Z|http://arxiv.org/abs/1702.04477v1|http://arxiv.org/pdf/1702.04477v1|The Multiple Roots Phenomenon in Maximum Likelihood Estimation for   Factor Analysis|Multiple root estimation problems in statistical inference arise in many contexts in the literature. In the context of maximum likelihood estimation, the existence of multiple roots causes uncertainty in the computation of maximum likelihood estimators using hill-climbing algorithms, and consequent difficulties in the resulting statistical inference.   In this paper, we study the multiple roots phenomenon in maximum likelihood estimation for factor analysis. We prove that the corresponding likelihood equations have uncountably many feasible solutions even in the simplest cases. For the case in which the observed data are two-dimensional and the unobserved factor scores are one-dimensional, we prove that the solutions to the likelihood equations form a one-dimensional real curve.|['Elizabeth Gross', 'Sonja Petrović', 'Donald Richards', 'Despina Stasi']|['math.ST', 'stat.TH']
2017-03-16T23:28:22Z|2017-02-14T03:45:37Z|http://arxiv.org/abs/1702.04065v1|http://arxiv.org/pdf/1702.04065v1|Wishart exponential families on cones related to An graphs|Let G = An be the graph corresponding to the graphical model of nearest neighbour interaction in a Gaussian character. We study Natural Exponential Families( NEF) ofWishart distributions on convex cones QG and PG, where PG is the cone of positive definite real symmetric matrices with obligatory zeros prescribed by G, and QG is the dual cone of PG. The Wishart NEF that we construct include Wishart distributions considered earlier by Lauritzen (1996) and Letac and Massam (2007) for models based on decomposable graphs. Our approach is however different and allows us to study the basic objects ofWishart NEF on the cones QG and PG.We determine Riesz measures generating Wishart exponential families on QG and PG, and we give the quadratic construction of these Riesz measures and exponential families. The mean, inverse-mean, covariance and variance functions, as well as moments of higher order are studied and their explicit formulas are given.|['Piotr Graczyk', 'Hideyuki Ishi', 'Salha Mamane']|['math.ST', 'math.PR', 'stat.TH']
2017-03-16T23:28:22Z|2017-02-14T00:34:06Z|http://arxiv.org/abs/1702.04031v1|http://arxiv.org/pdf/1702.04031v1|Maximum likelihood estimation in Gaussian models under total positivity|We analyze the problem of maximum likelihood estimation for Gaussian distributions that are multivariate totally positive of order two (MTP2). By exploiting connections to phylogenetics and single-linkage clustering, we give a simple proof that the maximum likelihood estimator (MLE) for such distributions exists based on at least 2 observations, irrespective of the underlying dimension. Slawski and Hein, who first proved this result, also provided empirical evidence showing that the MTP2 constraint serves as an implicit regularizer and leads to sparsity in the estimated inverse covariance matrix, determining what we name the ML graph. We show that the maximum weight spanning forest (MWSF) of the empirical correlation matrix is a spanning forest of the ML graph. In addition, we show that we can find an upper bound for the ML graph by adding edges to the MSWF corresponding to correlations in excess of those explained by the forest. This also gives new theoretical results in the study of inverse M-matrices. We provide globally convergent coordinate descent algorithms for calculating the MLE under the MTP2 constraint which are structurally similar to iterative proportional scaling. We conclude the paper with a discussion of signed MTP2 distributions.|['Steffen Lauritzen', 'Caroline Uhler', 'Piotr Zwiernik']|['stat.ME', 'math.ST', 'stat.TH', '60E15, 62H99, 15B48']
2017-03-16T23:28:22Z|2017-02-13T17:14:58Z|http://arxiv.org/abs/1702.03884v1|http://arxiv.org/pdf/1702.03884v1|Determinantal Generalizations of Instrumental Variables|Linear structural equation models relate the components of a random vector using linear interdependencies and Gaussian noise. Each such model can be naturally associated with a mixed graph whose vertices correspond to the components of the random vector. The graph contains directed edges that represent the linear relationships between components, and bidirected edges that encode unobserved confounding. We study the problem of generic identifiability, that is, whether a generic choice of linear and confounding effects can be uniquely recovered from the joint covariance matrix of the observed random vector. An existing combinatorial criterion for establishing generic identifiability is the half-trek criterion (HTC), which uses the existence of trek systems in the mixed graph to iteratively discover generically invertible linear equation systems in polynomial time. By focusing on edges one at a time, we establish new sufficient and necessary conditions for generic identifiability of edge effects extending those of the HTC. In particular, we show how edge coefficients can be recovered as quotients of subdeterminants of the covariance matrix, which constitutes a determinantal generalization of formulas obtained when using instrumental variables for identification.|['Luca Weihs', 'Bill Robinson', 'Emilie Dufresne', 'Jennifer Kenkel', 'Kaie Kubjas', 'Reginald L. McGee II', 'Nhan Nguyen', 'Elina Robeva', 'Mathias Drton']|['math.ST', 'stat.TH']
2017-03-16T23:28:22Z|2017-02-13T13:15:35Z|http://arxiv.org/abs/1702.03760v1|http://arxiv.org/pdf/1702.03760v1|Minimax Euclidean Separation Rates for Testing Convex Hypotheses in   $\mathbb{R}^d$|We consider composite-composite testing problems for the expectation in the Gaussian sequence model where the null hypothesis corresponds to a convex subset $\mathcal{C}$ of $\mathbb{R}^d$. We adopt a minimax point of view and our primary objective is to describe the smallest Euclidean distance between the null and alternative hypotheses such that there is a test with small total error probability. In particular, we focus on the dependence of this distance on the dimension $d$ and the sample size/variance parameter $n$ giving rise to the minimax separation rate. In this paper we discuss lower and upper bounds on this rate for different smooth and non- smooth choices for $\mathcal{C}$.|['Gilles Blanchard', 'Alexandra Carpentier', 'Maurilio Gutzeit']|['math.ST', 'stat.TH', '62G10']
