2017-03-16T23:31:15Z|2017-03-14T22:13:20Z|http://arxiv.org/abs/1703.04756v1|http://arxiv.org/pdf/1703.04756v1|Weighted Voting Via No-Regret Learning|Voting systems typically treat all voters equally. We argue that perhaps they should not: Voters who have supported good choices in the past should be given higher weight than voters who have supported bad ones. To develop a formal framework for desirable weighting schemes, we draw on no-regret learning. Specifically, given a voting rule, we wish to design a weighting scheme such that applying the voting rule, with voters weighted by the scheme, leads to choices that are almost as good as those endorsed by the best voter in hindsight. We derive possibility and impossibility results for the existence of such weighting schemes, depending on whether the voting rule and the weighting scheme are deterministic or randomized, as well as on the social choice axioms satisfied by the voting rule.|['Nika Haghtalab', 'Ritesh Noothigattu', 'Ariel D. Procaccia']|['cs.GT', 'cs.AI', 'cs.LG', 'cs.MA']
2017-03-16T23:31:15Z|2017-03-13T02:05:58Z|http://arxiv.org/abs/1703.04225v1|http://arxiv.org/pdf/1703.04225v1|New algorithms for matching problems|The standard two-sided and one-sided matching problems, and the closely related school choice problem, have been widely studied from an axiomatic viewpoint. A small number of algorithms dominate the literature. For two-sided matching, the Gale-Shapley algorithm; for one-sided matching, (random) Serial Dictatorship and Probabilistic Serial rule; for school choice, Gale-Shapley and the Boston mechanisms.   The main reason for the dominance of these algorithms is their good (worst-case) axiomatic behaviour with respect to notions of efficiency and strategyproofness. However if we shift the focus to fairness, social welfare, tradeoffs between incompatible axioms, and average-case analysis, it is far from clear that these algorithms are optimal.   We investigate new algorithms several of which have not appeared (to our knowledge) in the literature before. We give a unified presentation in which algorithms for 2-sided matching yield 1-sided matching algorithms in a systematic way. In addition to axiomatic properties, we investigate agent welfare using both theoretical and computational approaches. We find that some of the new algorithms are worthy of consideration for certain applications. In particular, when considering welfare under truthful preferences, some of the new algorithms outperform the classic ones.|['Jacky Lo', 'Mark C. Wilson']|['cs.GT', '91B68', 'J.4']
2017-03-16T23:31:15Z|2017-03-12T17:11:49Z|http://arxiv.org/abs/1703.04143v1|http://arxiv.org/pdf/1703.04143v1|Bernoulli Factories and Black-Box Reductions in Mechanism Design|"We provide a polynomial time reduction from Bayesian incentive compatible mechanism design to Bayesian algorithm design for welfare maximization problems. Unlike prior results, our reduction achieves exact incentive compatibility for problems with multi-dimensional and continuous type spaces. The key technical barrier preventing exact incentive compatibility in prior black-box reductions is that repairing violations of incentive constraints requires understanding the distribution of the mechanism's output. Reductions that instead estimate the output distribution by sampling inevitably suffer from sampling error, which typically precludes exact incentive compatibility.   We overcome this barrier by employing and generalizing the computational model in the literature on Bernoulli Factories. In a Bernoulli factory problem, one is given a function mapping the bias of an ""input coin"" to that of an ""output coin"", and the challenge is to efficiently simulate the output coin given sample access to the input coin. We generalize this to the ""expectations from samples"" computational model, in which an instance is specified by a function mapping the expected values of a set of input distributions to a distribution over outcomes. The challenge is to give a polynomial time algorithm that exactly samples from the distribution over outcomes given only sample access to the input distributions. In this model, we give a polynomial time algorithm for the exponential weights: expected values of the input distributions correspond to the weights of alternatives and we wish to select an alternative with probability proportional to an exponential function of its weight. This algorithm is the key ingredient in designing an incentive compatible mechanism for bipartite matching, which can be used to make the approximately incentive compatible reduction of Hartline et al. (2015) exactly incentive compatible."|['Shaddin Dughmi', 'Jason Hartline', 'Robert Kleinberg', 'Rad Niazadeh']|['cs.GT', 'cs.CC', 'cs.DS', 'math.PR']
2017-03-16T23:31:15Z|2017-03-11T05:35:09Z|http://arxiv.org/abs/1703.03912v1|http://arxiv.org/pdf/1703.03912v1|The Curse of Correlation in Security Games and Principle of Max-Entropy|In this paper, we identify and study a fundamental, yet underexplored, phenomenon in security games, which we term the Curse of Correlation (CoC). Specifically, we observe that there is inevitable correlation among the protection status of different targets. Such correlation is a crucial concern, especially in spatio-temporal domains like conservation area patrolling, where attackers can monitor patrollers at certain areas and then infer their patrolling routes using such correlation. To mitigate this issue, we introduce the principle of max-entropy to security games, and focus on designing entropy-maximizing defending strategies for the spatio-temporal security game -- a major victim of CoC. We prove that the problem is #P-hard in general, but propose efficient algorithms in well-motivated special settings. Our experiments show significant advantages of the max-entropy algorithms against previous algorithms.|['Haifeng Xu', 'Milind Tambe', 'Shaddin Dughmi', 'Venil Loyd Noronha']|['cs.GT', 'cs.AI', 'cs.CR']
2017-03-16T23:31:15Z|2017-03-10T21:31:32Z|http://arxiv.org/abs/1703.03846v1|http://arxiv.org/pdf/1703.03846v1|Socially Optimal Mining Pools|"Mining for Bitcoins is a high-risk high-reward activity. Miners, seeking to reduce their variance and earn steadier rewards, collaborate in pooling strategies where they jointly mine for Bitcoins. Whenever some pool participant is successful, the earned rewards are appropriately split among all pool participants. Currently a dozen of different pooling strategies (i.e., methods for distributing the rewards) are in use for Bitcoin mining.   We here propose a formal model of utility and social welfare for Bitcoin mining (and analogous mining systems) based on the theory of discounted expected utility, and next study pooling strategies that maximize the social welfare of miners. Our main result shows that one of the pooling strategies actually employed in practice--the so-called geometric pay pool--achieves the optimal steady-state utility for miners when its parameters are set appropriately.   Our results apply not only to Bitcoin mining pools, but any other form of pooled mining or crowdsourcing computations where the participants engage in repeated random trials towards a common goal, and where ""partial"" solutions can be efficiently verified."|['Ben A. Fisch', 'Rafael Pass', 'Abhi Shelat']|['cs.GT']
2017-03-16T23:31:15Z|2017-03-10T16:10:09Z|http://arxiv.org/abs/1703.03741v1|http://arxiv.org/pdf/1703.03741v1|Opinion-Based Centrality in Multiplex Networks: A Convex Optimization   Approach|Most people simultaneously belong to several distinct social networks, in which their relations can be different. They have opinions about certain topics, which they share and spread on these networks, and are influenced by the opinions of other persons. In this paper, we build upon this observation to propose a new nodal centrality measure for multiplex networks. Our measure, called Opinion centrality, is based on a stochastic model representing opinion propagation dynamics in such a network. We formulate an optimization problem consisting in maximizing the opinion of the whole network when controlling an external influence able to affect each node individually. We find a mathematical closed form of this problem, and use its solution to derive our centrality measure. According to the opinion centrality, the more a node is worth investing external influence, and the more it is central. We perform an empirical study of the proposed centrality over a toy network, as well as a collection of real-world networks. Our measure is generally negatively correlated with existing multiplex centrality measures, and highlights different types of nodes, accordingly to its definition.|['Alexandre Reiffers-Masson', 'Vincent Labatut']|['cs.SI', 'cs.GT', 'physics.soc-ph']
2017-03-16T23:31:15Z|2017-03-10T13:54:29Z|http://arxiv.org/abs/1703.03687v1|http://arxiv.org/pdf/1703.03687v1|Best Laid Plans of Lions and Men|"We answer the following question dating back to J.E. Littlewood (1885 - 1977): Can two lions catch a man in a bounded area with rectifiable lakes? The lions and the man are all assumed to be points moving with at most unit speed. That the lakes are rectifiable means that their boundaries are finitely long. This requirement is to avoid pathological examples where the man survives forever because any path to the lions is infinitely long. We show that the answer to the question is not always ""yes"" by giving an example of a region $R$ in the plane where the man has a strategy to survive forever. $R$ is a polygonal region with holes and the exterior and interior boundaries are pairwise disjoint, simple polygons. Our construction is the first truly two-dimensional example where the man can survive.   Next, we consider the following game played on the entire plane instead of a bounded area: There is any finite number of unit speed lions and one fast man who can run with speed $1+\varepsilon$ for some value $\varepsilon>0$. Can the man always survive? We answer the question in the affirmative for any constant $\varepsilon>0$."|['Mikkel Abrahamsen', 'Jacob Holm', 'Eva Rotenberg', 'Christian Wulff-Nilsen']|['cs.CG', 'cs.GT']
2017-03-16T23:31:15Z|2017-03-10T01:47:23Z|http://arxiv.org/abs/1703.03511v1|http://arxiv.org/pdf/1703.03511v1|Towards Computing Victory Margins in STV Elections|The Single Transferable Vote (STV) is a system of preferential voting employed in multi-seat elections. Each vote cast by a voter is a (potentially partial) ranking over a set of candidates. No techniques currently exist for computing the margin of victory (MOV) in STV elections. The MOV is the smallest number of vote manipulations (changes, additions, and deletions) required to bring about a change in the set of elected candidates. Knowledge of the MOV of an election gives greater insight into both how much time and money should be spent on the auditing of the election, and whether uncovered mistakes (such as ballot box losses) throw the election result into doubt---requiring a costly repeat election---or can be safely ignored. In this paper, we present algorithms for computing lower and upper bounds on the MOV in STV elections. In small instances, these algorithms are able to compute exact margins.|['Michelle Blom', 'Peter J. Stuckey', 'Vanessa J. Teague']|['cs.GT']
2017-03-16T23:31:15Z|2017-03-09T22:47:10Z|http://arxiv.org/abs/1703.03484v1|http://arxiv.org/pdf/1703.03484v1|Combinatorial Auctions with Online XOS Bidders|In combinatorial auctions, a designer must decide how to allocate a set of indivisible items amongst a set of bidders. Each bidder has a valuation function which gives the utility they obtain from any subset of the items. Our focus is specifically on welfare maximization, where the objective is to maximize the sum of valuations that the bidders place on the items that they were allocated (the valuation functions are assumed to be reported truthfully). We analyze an online problem in which the algorithm is not given the set of bidders in advance. Instead, the bidders are revealed sequentially in a uniformly random order, similarly to secretary problems. The algorithm must make an irrevocable decision about which items to allocate to the current bidder before the next one is revealed. When the valuation functions lie in the class $XOS$ (which includes submodular functions), we provide a black box reduction from offline to online optimization. Specifically, given an $\alpha$-approximation algorithm for offline welfare maximization, we show how to create a $(0.199 \alpha)$-approximation algorithm for the online problem. Our algorithm draws on connections to secretary problems; in fact, we show that the online welfare maximization problem itself can be viewed as a particular kind of secretary problem with nonuniform arrival order.|['Shaddin Dughmi', 'Bryan Wilder']|['cs.GT', 'cs.DS']
2017-03-16T23:31:15Z|2017-03-09T13:45:45Z|http://arxiv.org/abs/1703.03262v1|http://arxiv.org/pdf/1703.03262v1|Does Nash Envy Immunity|The most popular stability notion in games should be Nash equilibrium under the rationality of players who maximize their own payoff individually. In contrast, in many scenarios, players can be (partly) irrational with some unpredictable factors. Hence a strategy profile can be more robust if it is resilient against certain irrational behaviors. In this paper, we propose a stability notion that is resilient against envy. A strategy profile is said to be envy-proof if each player cannot gain a competitive edge with respect to the change in utility over the other players by deviation. Together with Nash equilibrium and another stability notion called immunity, we show how these separate notions are related to each other, whether they exist in games, and whether and when a strategy profile satisfying these notions can be efficiently found. We answer these questions by starting with the general two player game and extend the discussion for the approximate stability and for the corresponding fault-tolerance notions in multi-player games.|['Ching-Hua Yu']|['cs.GT', 'cs.CC', 'cs.CR', 'I.2.1; J.4; K.6.0; C.4']
2017-03-16T23:31:20Z|2017-03-09T02:50:49Z|http://arxiv.org/abs/1703.03111v1|http://arxiv.org/pdf/1703.03111v1|Statistical Cost Sharing|We study the cost sharing problem for cooperative games in situations where the cost function $C$ is not available via oracle queries, but must instead be derived from data, represented as tuples $(S, C(S))$, for different subsets $S$ of players. We formalize this approach, which we call statistical cost sharing, and consider the computation of the core and the Shapley value, when the tuples are drawn from some distribution $\mathcal{D}$.   Previous work by Balcan et al. in this setting showed how to compute cost shares that satisfy the core property with high probability for limited classes of functions. We expand on their work and give an algorithm that computes such cost shares for any function with a non-empty core. We complement these results by proving an inapproximability lower bound for a weaker relaxation.   We then turn our attention to the Shapley value. We first show that when cost functions come from the family of submodular functions with bounded curvature, $\kappa$, the Shapley value can be approximated from samples up to a $\sqrt{1 - \kappa}$ factor, and that the bound is tight. We then define statistical analogues of the Shapley axioms, and derive a notion of statistical Shapley value. We show that these can always be approximated arbitrarily well for general functions over any distribution $\mathcal{D}$.|['Eric Balkanski', 'Umar Syed', 'Sergei Vassilvitskii']|['cs.GT', 'cs.LG']
2017-03-16T23:31:20Z|2017-03-08T14:41:30Z|http://arxiv.org/abs/1703.02851v1|http://arxiv.org/pdf/1703.02851v1|On the Importance of Correlations in Rational Choice: A Case for   Non-Nashian Game Theory|"The Nash equilibrium paradigm, and Rational Choice Theory in general, rely on agents acting independently from each other. This note shows how this assumption is crucial in the definition of Rational Choice Theory. It explains how a consistent Alternate Rational Choice Theory, as suggested by Jean-Pierre Dupuy, can be built on the exact opposite assumption, and how it provides a viable account for alternate, actually observed behavior of rational agents that is based on correlations between their decisions.   The end goal of this note is three-fold: (i) to motivate that the Perfect Prediction Equilibrium, implementing Dupuy's notion of projected time and previously called ""projected equilibrium"", is a reasonable approach in certain real situations and a meaningful complement to the Nash paradigm, (ii) to summarize common misconceptions about this equilibrium, and (iii) to give a concise motivation for future research on non-Nashian game theory."|['Ghislain Fourny']|['cs.GT', '91A35', 'J.4']
2017-03-16T23:31:20Z|2017-03-07T19:33:50Z|http://arxiv.org/abs/1703.02567v1|http://arxiv.org/pdf/1703.02567v1|Online Learning of Optimal Bidding Strategy in Repeated Multi-Commodity   Auctions|We study the online learning problem of a bidder who participates in repeated auctions. With the goal of maximizing his total T-period payoff, the bidder wants to determine the optimal allocation of his fixed budget among his bids for $K$ different goods at each period. As a bidding strategy, we propose a polynomial time algorithm, referred to as dynamic programming on discrete set (DPDS), which is inspired by the dynamic programming approach to Knapsack problems. We show that DPDS achieves the regret order of $O(\sqrt{T\log{T}})$. Also, by showing that the regret growth rate is lower bounded by $\Omega(\sqrt{T})$ for any bidding strategy, we conclude that DPDS algorithm is order optimal up to a $\sqrt{\log{T}}$ term. We also evaluate the performance of DPDS empirically in the context of virtual bidding in wholesale electricity markets by using historical data from the New York energy market.|['Sevi Baltaoglu', 'Lang Tong', 'Qing Zhao']|['cs.GT', 'cs.LG']
2017-03-16T23:31:20Z|2017-03-06T16:35:31Z|http://arxiv.org/abs/1703.01957v1|http://arxiv.org/pdf/1703.01957v1|Solving Two-Player Zero-Sum Repeated Bayesian Games|This paper studies two-player zero-sum repeated Bayesian games in which every player has a private type that is unknown to the other player, and the initial probability of the type of every player is publicly known. The types of players are independently chosen according to the initial probabilities, and are kept the same all through the game. At every stage, players simultaneously choose actions, and announce their actions publicly. For finite horizon cases, an explicit linear program is provided to compute players' security strategies. Moreover, based on the existing results in [1], this paper shows that a player's sufficient statistics, which is independent of the strategy of the other player, consists of the belief over the player's own type, the regret with respect to the other player's type, and the stage. Explicit linear programs are provided to compute the initial regrets, and the security strategies that only depends on the sufficient statistics. For discounted cases, following the same idea in the finite horizon, this paper shows that a player's sufficient statistics consists of the belief of the player's own type and the anti-discounted regret with respect to the other player's type. Besides, an approximated security strategy depending on the sufficient statistics is provided, and an explicit linear program to compute the approximated security strategy is given. This paper also obtains a bound on the performance difference between the approximated security strategy and the security strategy.|['Lichun Li', 'Cedric Langbort', 'Jeff Shamma']|['cs.GT']
2017-03-16T23:31:20Z|2017-03-06T16:23:50Z|http://arxiv.org/abs/1703.01952v1|http://arxiv.org/pdf/1703.01952v1|Efficient Strategy Computation in Zero-Sum Asymmetric Repeated Games|Zero-sum asymmetric games model decision making scenarios involving two competing players who have different information about the game being played. A particular case is that of nested information, where one (informed) player has superior information over the other (uninformed) player. This paper considers the case of nested information in repeated zero-sum games and studies the computation of strategies for both the informed and uninformed players for finite-horizon and discounted infinite-horizon nested information games. For finite-horizon settings, we exploit that for both players, the security strategy, i.e. Nash equilibrium, and also the opponent's corresponding best response depend only on the informed player's history of actions. Using this property, we refine the sequence form, and formulate an LP computation of player strategies that is linear in the size of the uninformed player's action set. For the infinite-horizon discounted game, we construct LP formulations to compute the approximated security strategies for both players, and provide a bound on the performance difference between the suboptimal strategies and the security strategies. Finally, we illustrate the results on a network interdiction game between an informed system administrator and uniformed intruder.|['Lichun Li', 'Jeff S. Shamma']|['cs.GT']
2017-03-16T23:31:20Z|2017-03-06T13:01:12Z|http://arxiv.org/abs/1703.01851v1|http://arxiv.org/pdf/1703.01851v1|Approximation Algorithms for Maximin Fair Division|We consider the problem of dividing indivisible goods fairly among $n$ agents who have additive and submodular valuations for the goods. Our fairness guarantees are in terms of the maximin share, that is defined to be the maximum value that an agent can ensure for herself, if she were to partition the goods into $n$ bundles, and then receive a minimum valued bundle. Since maximin fair allocations (i.e., allocations in which each agent gets at least her maximin share) do not always exist, prior work has focussed on approximation results that aim to find allocations in which the value of the bundle allocated to each agent is (multiplicatively) as close to her maximin share as possible. In particular, Procaccia and Wang (2014) along with Amanatidis et al. (2015) have shown that under a $2/3$-approximate maximin fair allocation always exists and can be found in polynomial time. We complement these results by developing a simple and efficient algorithm that achieves the same approximation guarantee.   Furthermore, we initiate the study of approximate maximin fair division under submodular valuations. Specifically, we show that when the valuations of the agents are nonnegative, monotone, and submodular, then a $1/10$-approximate maximin fair allocation is guaranteed to exist. In fact, we show that such an allocation (with a slightly worse approximation guarantee) can be efficiently found by a simple round-robin algorithm. A technical contribution of the paper is to analyze the performance of this combinatorial algorithm by employing the concept of multilinear extensions.|['Siddharth Barman', 'Sanath Kumar Krishna Murthy']|['cs.GT']
2017-03-16T23:31:20Z|2017-03-05T19:12:40Z|http://arxiv.org/abs/1703.01649v1|http://arxiv.org/pdf/1703.01649v1|Fair Allocation of Indivisible Goods to Asymmetric Agents|We study fair allocation of indivisible goods to agents with unequal entitlements. Fair allocation has been the subject of many studies in both divisible and indivisible settings. Our emphasis is on the case where the goods are indivisible and agents have unequal entitlements. This problem is a generalization of the work by Procaccia and Wang wherein the agents are assumed to be symmetric with respect to their entitlements. Although Procaccia and Wang show an almost fair (constant approximation) allocation exists in their setting, our main result is in sharp contrast to their observation. We show that, in some cases with $n$ agents, no allocation can guarantee better than $1/n$ approximation of a fair allocation when the entitlements are not necessarily equal. Furthermore, we devise a simple algorithm that ensures a $1/n$ approximation guarantee. Our second result is for a restricted version of the problem where the valuation of every agent for each good is bounded by the total value he wishes to receive in a fair allocation. Although this assumption might seem w.l.o.g, we show it enables us to find a $1/2$ approximation fair allocation via a greedy algorithm. Finally, we run some experiments on real-world data and show that, in practice, a fair allocation is likely to exist. We also support our experiments by showing positive results for two stochastic variants of the problem, namely stochastic agents and stochastic items.|['Alireza Farhadi', 'Mohammad Ghodsi', 'MohammadTaghi Hajiaghayi', 'Sebastien Lahaie', 'David Pennock', 'Masoud Seddighin', 'Saeed Seddighin', 'Hadi Yami']|['cs.GT']
2017-03-16T23:31:20Z|2017-03-07T13:05:15Z|http://arxiv.org/abs/1703.01599v2|http://arxiv.org/pdf/1703.01599v2|How bad is selfish routing in practice?|Routing games are one of the most successful domains of application of game theory. It is well understood that simple dynamics converge to equilibria, whose performance is nearly optimal regardless of the size of the network or the number of agents. These strong theoretical assertions prompt a natural question: How well do these pen-and-paper calculations agree with the reality of everyday traffic routing? We focus on a semantically rich dataset from Singapore's National Science Experiment that captures detailed information about the daily behavior of thousands of Singaporean students. Using this dataset, we can identify the routes as well as the modes of transportation used by the students, e.g. car (driving or being driven to school) versus bus or metro, estimate source and sink destinations (home-school) and trip duration, as well as their mode-dependent available routes. We quantify both the system and individual optimality. Our estimate of the Empirical Price of Anarchy lies between 1.11 and 1.22. Individually, the typical behavior is consistent from day to day and nearly optimal, with low regret for not deviating to alternative paths.|['Barnabé Monnot', 'Francisco Benita', 'Georgios Piliouras']|['cs.GT']
2017-03-16T23:31:20Z|2017-03-04T02:12:37Z|http://arxiv.org/abs/1703.01380v1|http://arxiv.org/pdf/1703.01380v1|Internalization of Externalities in Interdependent Security: Large   Network Cases|With increasing connectivity among comprising agents or (sub-)systems in large, complex systems, there is a growing interest in understanding interdependent security and dealing with inefficiency in security investments. Making use of a population game model and the well-known Chung-Lu random graph model, we study how one could encourage selfish agents to invest more in security by internalizing the externalities produced by their security investments.   To this end, we first establish an interesting relation between the local minimizers of social cost and the Nash equilibria of a population game with slightly altered costs. Secondly, under a mild technical assumption, we demonstrate that there exists a unique minimizer of social cost and it coincides with the unique Nash equilibrium of the population game. This finding tells us how to modify the private cost functions of selfish agents in order to enhance the overall security and reduce social cost. In addition, it reveals how the sensitivity of overall security to security investments of agents influences their externalities and, consequently, penalties or taxes that should be imposed for internalization of externalities. Finally, we illustrate how the degree distribution of agents influences their security investments and overall security at both the NEs of population games and social optima.|['Richard J. La']|['cs.SI', 'cs.GT']
2017-03-16T23:31:20Z|2017-03-03T12:57:26Z|http://arxiv.org/abs/1703.01138v1|http://arxiv.org/pdf/1703.01138v1|Multiplicative Weights Update with Constant Step-Size in Congestion   Games: Convergence, Limit Cycles and Chaos|"The Multiplicative Weights Update (MWU) method is a ubiquitous meta-algorithm that works as follows: A distribution is maintained on a certain set, and at each step the probability assigned to element $\gamma$ is multiplied by $(1 -\epsilon C(\gamma))>0$ where $C(\gamma)$ is the ""cost"" of element $\gamma$ and then rescaled to ensure that the new values form a distribution. We analyze MWU in congestion games where agents use \textit{arbitrary admissible constants} as learning rates $\epsilon$ and prove convergence to \textit{exact Nash equilibria}. Our proof leverages a novel connection between MWU and the Baum-Welch algorithm, the standard instantiation of the Expectation-Maximization (EM) algorithm for hidden Markov models (HMM). Interestingly, this convergence result does not carry over to the nearly homologous MWU variant where at each step the probability assigned to element $\gamma$ is multiplied by $(1 -\epsilon)^{C(\gamma)}$ even for the most innocuous case of two-agent, two-strategy load balancing games, where such dynamics can provably lead to limit cycles or even chaotic behavior."|['Gerasimos Palaiopanos', 'Ioannis Panageas', 'Georgios Piliouras']|['cs.GT']
2017-03-16T23:31:24Z|2017-03-03T12:10:13Z|http://arxiv.org/abs/1703.01121v1|http://arxiv.org/pdf/1703.01121v1|On Parameterized Complexity of Group Activity Selection Problems on   Social Networks|In Group Activity Selection Problem (GASP), players form coalitions to participate in activities and have preferences over pairs of the form (activity, group size). Recently, Igarashi et al. have initiated the study of group activity selection problems on social networks (gGASP): a group of players can engage in the same activity if the members of the group form a connected subset of the underlying communication structure. Igarashi et al. have primarily focused on Nash stable outcomes, and showed that many associated algorithmic questions are computationally hard even for very simple networks. In this paper we study the parameterized complexity of gGASP with respect to the number of activities as well as with respect to the number of players, for several solution concepts such as Nash stability, individual stability and core stability. The first parameter we consider in the number of activities. For this parameter, we propose an FPT algorithm for Nash stability for the case where the social network is acyclic and obtain a W[1]-hardness result for cliques (i.e., for classic GASP); similar results hold for individual stability. In contrast, finding a core stable outcome is hard even if the number of activities is bounded by a small constant, both for classic GASP and when the social network is a star. Another parameter we study is the number of players. While all solution concepts we consider become polynomial-time computable when this parameter is bounded by a constant, we prove W[1]-hardness results for cliques (i.e., for classic GASP).|['Ayumi Igarashi', 'Robert Bredereck', 'Edith Elkind']|['cs.GT']
2017-03-16T23:31:24Z|2017-03-15T00:36:53Z|http://arxiv.org/abs/1703.00972v2|http://arxiv.org/pdf/1703.00972v2|Eliciting Private User Information for Residential Demand Response|"Residential Demand Response has emerged as a viable tool to alleviate supply and demand imbalances of electricity, particularly during times when the electric grid is strained due a shortage of supply. Demand Response providers bid reduction capacity into the wholesale electricity market by asking their customers under contract to temporarily reduce their consumption in exchange for a monetary incentive. To contribute to the analysis of consumer behavior in response to such incentives, this paper formulates Demand Response as a Mechanism Design problem, where a Demand Response Provider elicits private information of its rational, profit-maximizing customers who derive positive expected utility by participating in reduction events. By designing an incentive compatible and individually rational mechanism to collect users' price elasticities of demand, the Demand Response provider can target the most susceptible users to incentives. We measure reductions by comparing the materialized consumption to the projected consumption, which we model as the ""10-in-10""-baseline, the regulatory standard set by the California Independent System Operator. Due to the suboptimal performance of this baseline, we show, using consumption data of residential customers in California, that Demand Response Providers receive payments for ""virtual reductions"", which exist due to the inaccuracies of the baseline rather than actual reductions. Improving the accuracy of the baseline diminishes the contribution of these virtual reductions."|['Datong P. Zhou', 'Munther A. Dahleh', 'Claire J. Tomlin']|['cs.GT']
2017-03-16T23:31:24Z|2017-03-02T19:35:48Z|http://arxiv.org/abs/1703.00927v1|http://arxiv.org/pdf/1703.00927v1|On the asymptotic behavior of the price of anarchy: Is selfish routing   bad in highly congested networks?|This paper examines the asymptotic behavior of the price of anarchy as a function of the total traffic inflow in nonatomic congestion games with multiple origin-destination pairs. We first show that the price of anarchy may remain bounded away from 1, even in simple three-link parallel networks with convex cost functions. On the other hand, empirical studies show that the price of anarchy is close to 1 in highly congested real-world networks, thus begging the question: under what assumptions can this behavior be justified analytically? To that end, we prove a general result showing that for a large class of cost functions (defined in terms of regular variation and including all polynomials), the price of anarchy converges to 1 in the high congestion limit. In particular, specializing to networks with polynomial costs, we show that this convergence follows a power law whose degree can be computed explicitly.|['Riccardo Colini Baldeschi', 'Roberto Cominetti', 'Panayotis Mertikopoulos', 'Marco Scarsini']|['cs.GT', 'math.OC', 'Primary 91A13, secondary 91A43']
2017-03-16T23:31:24Z|2017-03-02T09:42:47Z|http://arxiv.org/abs/1703.00683v1|http://arxiv.org/pdf/1703.00683v1|Parity Games, Imperfect Information and Structural Complexity|"We address the problem of solving parity games with imperfect information on finite graphs of bounded structural complexity. It is a major open problem whether parity games with perfect information can be solved in PTIME. Restricting the structural complexity of the game arenas, however, often leads to efficient algorithms for parity games. Such results are known for graph classes of bounded tree-width, DAG-width, directed path-width, and entanglement, which we describe in terms of cops and robber games. Conversely, the introduction of imperfect information makes the problem more difficult, it becomes EXPTIME-hard. We analyse the interaction of both approaches.   We use a simple method to measure the amount of ""unawareness""' of a player, the amount of imperfect information. It turns out that if it is unbounded, low structural complexity does not make the problem simpler. It remains EXPTIME-hard or PSPACE-hard even on very simple graphs.   For games with bounded imperfect information we analyse the powerset construction, which is commonly used to convert a game of imperfect information into an equivalent game with perfect information. This construction preserves boundedness of directed path-width and DAG-width, but not of entanglement or of tree-width. Hence, if directed path-width or DAG-width are bounded, parity games with bounded imperfect information can be solved in PTIME. For DAG-width we follow two approaches. One leads to a generalization of the known fact that perfect information parity games are in PTIME if DAG-width is bounded. We prove this theorem for non-monotone DAG-width. The other approach introduces a cops and robbers game (with multiple robbers) on directed graphs, considered Richerby and Thilikos forundirected graphs. We show a tight linear bound for the number of additional cops needed to capture an additional robber."|['Bernd Puchala', 'Roman Rabinovich']|['cs.GT']
2017-03-16T23:31:24Z|2017-03-02T05:36:16Z|http://arxiv.org/abs/1703.00632v1|http://arxiv.org/pdf/1703.00632v1|A Dominant Strategy Truthful, Deterministic Multi-Armed Bandit Mechanism   with Logarithmic Regret|Stochastic multi-armed bandit (MAB) mechanisms are widely used in sponsored search auctions, crowdsourcing, online procurement, etc. Existing stochastic MAB mechanisms with a deterministic payment rule, proposed in the literature, necessarily suffer a regret of $\Omega(T^{2/3})$, where $T$ is the number of time steps. This happens because the existing mechanisms consider the worst case scenario where the means of the agents' stochastic rewards are separated by a very small amount that depends on $T$. We make, and, exploit the crucial observation that in most scenarios, the separation between the agents' rewards is rarely a function of $T$. Moreover, in the case that the rewards of the arms are arbitrarily close, the regret contributed by such sub-optimal arms is minimal. Our idea is to allow the center to indicate the resolution, $\Delta$, with which the agents must be distinguished. This immediately leads us to introduce the notion of $\Delta$-Regret. Using sponsored search auctions as a concrete example (the same idea applies for other applications as well), we propose a dominant strategy incentive compatible (DSIC) and individually rational (IR), deterministic MAB mechanism, based on ideas from the Upper Confidence Bound (UCB) family of MAB algorithms. Remarkably, the proposed mechanism $\Delta$-UCB achieves a $\Delta$-regret of $O(\log T)$ for the case of sponsored search auctions. We first establish the results for single slot sponsored search auctions and then non-trivially extend the results to the case where multiple slots are to be allocated.|['Divya Padmanabhan', 'Satyanath Bhat', 'Prabuchandran K. J.', 'Shirish Shevade', 'Y. Narahari']|['cs.GT']
2017-03-16T23:31:24Z|2017-03-01T20:09:43Z|http://arxiv.org/abs/1703.00484v1|http://arxiv.org/pdf/1703.00484v1|Truth and Regret in Online Scheduling|We consider a scheduling problem where a cloud service provider has multiple units of a resource available over time. Selfish clients submit jobs, each with an arrival time, deadline, length, and value. The service provider's goal is to implement a truthful online mechanism for scheduling jobs so as to maximize the social welfare of the schedule. Recent work shows that under a stochastic assumption on job arrivals, there is a single-parameter family of mechanisms that achieves near-optimal social welfare. We show that given any such family of near-optimal online mechanisms, there exists an online mechanism that in the worst case performs nearly as well as the best of the given mechanisms. Our mechanism is truthful whenever the mechanisms in the given family are truthful and prompt, and achieves optimal (within constant factors) regret.   We model the problem of competing against a family of online scheduling mechanisms as one of learning from expert advice. A primary challenge is that any scheduling decisions we make affect not only the payoff at the current step, but also the resource availability and payoffs in future steps. Furthermore, switching from one algorithm (a.k.a. expert) to another in an online fashion is challenging both because it requires synchronization with the state of the latter algorithm as well as because it affects the incentive structure of the algorithms. We further show how to adapt our algorithm to a non-clairvoyant setting where job lengths are unknown until jobs are run to completion. Once again, in this setting, we obtain truthfulness along with asymptotically optimal regret (within poly-logarithmic factors).|['Shuchi Chawla', 'Nikhil Devanur', 'Janardhan Kulkarni', 'Rad Niazadeh']|['cs.GT', 'cs.AI', 'cs.DS', 'cs.LG']
2017-03-16T23:31:24Z|2017-03-01T14:42:20Z|http://arxiv.org/abs/1703.00320v1|http://arxiv.org/pdf/1703.00320v1|Investigating the Characteristics of One-Sided Matching Mechanisms Under   Various Preferences and Risk Attitudes|One-sided matching mechanisms are fundamental for assigning a set of indivisible objects to a set of self-interested agents when monetary transfers are not allowed. Two widely-studied randomized mechanisms in multiagent settings are the Random Serial Dictatorship (RSD) and the Probabilistic Serial Rule (PS). Both mechanisms require only that agents specify ordinal preferences and have a number of desirable economic and computational properties. However, the induced outcomes of the mechanisms are often incomparable and thus there are challenges when it comes to deciding which mechanism to adopt in practice. In this paper, we first consider the space of general ordinal preferences and provide empirical results on the (in)comparability of RSD and PS. We analyze their respective economic properties under general and lexicographic preferences. We then instantiate utility functions with the goal of gaining insights on the manipulability, efficiency, and envyfreeness of the mechanisms under different risk-attitude models. Our results hold under various preference distribution models, which further confirm the broad use of RSD in most practical applications.|['Hadi Hosseini', 'Kate Larson', 'Robin Cohen']|['cs.GT', 'cs.AI', 'cs.MA', 'I.2.11; J.4']
2017-03-16T23:31:24Z|2017-03-01T10:30:36Z|http://arxiv.org/abs/1703.00216v1|http://arxiv.org/pdf/1703.00216v1|Congestion-Aware Distributed Network Selection for Integrated Cellular   and Wi-Fi Networks|Intelligent network selection plays an important role in achieving an effective data offloading in the integrated cellular and Wi-Fi networks. However, previously proposed network selection schemes mainly focused on offloading as much data traffic to Wi-Fi as possible, without systematically considering the Wi-Fi network congestion and the ping-pong effect, both of which may lead to a poor overall user quality of experience. Thus, in this paper, we study a more practical network selection problem by considering both the impacts of the network congestion and switching penalties. More specifically, we formulate the users' interactions as a Bayesian network selection game (NSG) under the incomplete information of the users' mobilities. We prove that it is a Bayesian potential game and show the existence of a pure Bayesian Nash equilibrium that can be easily reached. We then propose a distributed network selection (DNS) algorithm based on the network congestion statistics obtained from the operator. Furthermore, we show that computing the optimal centralized network allocation is an NP-hard problem, which further justifies our distributed approach. Simulation results show that the DNS algorithm achieves the highest user utility and a good fairness among users, as compared with the on-the-spot offloading and cellular-only benchmark schemes.|['Man Hon Cheung', 'Fen Hou', 'Jianwei Huang', 'Richard Southwell']|['cs.NI', 'cs.GT']
2017-03-16T23:31:24Z|2017-02-28T16:57:49Z|http://arxiv.org/abs/1702.08862v1|http://arxiv.org/pdf/1702.08862v1|Proportional Representation in Vote Streams|We consider elections where the voters come one at a time, in a streaming fashion, and devise space-efficient algorithms which identify an approximate winning committee with respect to common multiwinner proportional representation voting rules; specifically, we consider the Approval-based and the Borda-based variants of both the Chamberlin-- ourant rule and the Monroe rule. We complement our algorithms with lower bounds. Somewhat surprisingly, our results imply that, using space which does not depend on the number of voters it is possible to efficiently identify an approximate representative committee of fixed size over vote streams with huge number of voters.|['Palash Dey', 'Nimrod Talmon', 'Otniel van Handel']|['cs.GT', 'cs.AI', 'cs.CC', 'cs.DS', 'cs.MA']
2017-03-16T23:31:24Z|2017-02-28T14:16:00Z|http://arxiv.org/abs/1702.08794v1|http://arxiv.org/pdf/1702.08794v1|Lowest Unique Bid Auctions with Resubmission Opportunities|The recent online platforms propose multiple items for bidding. The state of the art, however, is limited to the analysis of one item auction without resubmission. In this paper we study multi-item lowest unique bid auctions (LUBA) with resubmission in discrete bid spaces under budget constraints. We show that the game does not have pure Bayes-Nash equilibria (except in very special cases). However, at least one mixed Bayes-Nash equilibria exists for arbitrary number of bidders and items. The equilibrium is explicitly computed for two-bidder setup with resubmission possibilities. In the general setting we propose a distributed strategic learning algorithm to approximate equilibria. Computer simulations indicate that the error quickly decays in few number of steps. When the number of bidders per item follows a Poisson distribution, it is shown that the seller can get a non-negligible revenue on several items, and hence making a partial revelation of the true value of the items. Finally, the attitude of the bidders towards the risk is considered. In contrast to risk-neutral agents who bids very small values, the cumulative distribution and the bidding support of risk-sensitive agents are more distributed.|['Yida Xu', 'Hamidou Tembine']|['cs.GT']
2017-03-16T23:31:28Z|2017-02-28T13:56:57Z|http://arxiv.org/abs/1702.08789v1|http://arxiv.org/pdf/1702.08789v1|Nash and Wardrop equilibria in aggregative games with coupling   constraints|We consider the framework of aggregative games, in which the cost function of each agent depends on his own strategy and on the average population strategy. As first contribution, we investigate the relations between the concepts of Nash and Wardrop equilibrium. By exploiting a characterization of the two equilibria as solutions of variational inequalities, we bound their distance with a decreasing function of the population size. As second contribution, we propose two decentralized algorithms that converge to such equilibria and are capable of coping with constraints coupling the strategies of different agents. Finally, we study the applications of charging of electric vehicles and of route choice on a road network.|['Basilio Gentile', 'Francesca Parise', 'Dario Paccagnan', 'Maryam Kamgarpour', 'John Lygeros']|['cs.SY', 'cs.GT', 'math.OC']
2017-03-16T23:31:28Z|2017-02-27T21:13:57Z|http://arxiv.org/abs/1702.08533v1|http://arxiv.org/pdf/1702.08533v1|Competing Bandits: Learning under Competition|"Most modern systems strive to learn from interactions with users, and many engage in \emph{exploration}: making potentially suboptimal choices for the sake of acquiring new information. We initiate a study of the interplay between \emph{exploration and competition}---how such systems balance the exploration for learning and the competition for users. Here the users play three distinct roles: they are customers that generate revenue, they are sources of data for learning, and they are self-interested agents which choose among the competing systems.   As a model, we consider competition between two multi-armed bandit algorithms faced with the same bandit instance. Users arrive one by one and choose among the two algorithms, so that each algorithm makes progress if and only if it is chosen. We ask whether and to which extent competition incentivizes \emph{innovation}: adoption of better algorithms. We investigate this issue for several models of user response, as we vary the degree of rationality and competitiveness in the model. Effectively, we map out the ""competition vs. innovation"" relationship, a well-studied theme in economics."|['Yishay Mansour', 'Aleksandrs Slivkins', 'Zhiwei Steven Wu']|['cs.GT', 'cs.LG']
2017-03-16T23:31:28Z|2017-02-27T18:07:12Z|http://arxiv.org/abs/1702.08405v1|http://arxiv.org/pdf/1702.08405v1|Game-Theoretic Semantics for ATL+ with Applications to Model Checking|We develop game-theoretic semantics (GTS) for the fragment ATL+ of the full Alternating-time Temporal Logic ATL*, essentially extending a recently introduced GTS for ATL. We first show that the new game-theoretic semantics is equivalent to the standard semantics of ATL+ (based on perfect recall strategies). We then provide an analysis, based on the new semantics, of the memory and time resources needed for model checking ATL+. Based on that, we establish that strategies that use only a very limited amount of memory suffice for ATL+. Furthermore, using the GTS we provide a new algorithm for model checking of ATL+ and identify a natural hierarchy of tractable fragments of ATL+ that extend ATL.|['Valentin Goranko', 'Antti Kuusisto', 'Raine Rönnholm']|['math.LO', 'cs.GT', 'cs.LO', 'F.4.1; I.2.11']
2017-03-16T23:31:28Z|2017-02-27T15:41:56Z|http://arxiv.org/abs/1702.08334v1|http://arxiv.org/pdf/1702.08334v1|Stochastic Stability Analysis of Perturbed Learning Automata with   Constant Step-Size in Strategic-Form Games|This paper considers a class of reinforcement-learning that belongs to the family of Learning Automata and provides a stochastic-stability analysis in strategic-form games. For this class of dynamics, convergence to pure Nash equilibria has been demonstrated only for the fine class of potential games. Prior work primarily provides convergence properties of the dynamics through stochastic approximations, where the asymptotic behavior can be associated with the limit points of an ordinary-differential equation (ODE). However, analyzing global convergence through the ODE-approximation requires the existence of a Lyapunov or a potential function, which naturally restricts the applicabity of these algorithms to a fine class of games. To overcome these limitations, this paper introduces an alternative framework for analyzing stochastic-stability that is based upon an explicit characterization of the (unique) invariant probability measure of the induced Markov chain.|['Georgios C. Chasparis']|['cs.GT']
2017-03-16T23:31:28Z|2017-02-27T13:54:44Z|http://arxiv.org/abs/1702.08286v1|http://arxiv.org/pdf/1702.08286v1|Balancing Lexicographic Fairness and a Utilitarian Objective with   Application to Kidney Exchange|Balancing fairness and efficiency in resource allocation is a classical economic and computational problem. The price of fairness measures the worst-case loss of economic efficiency when using an inefficient but fair allocation rule; for indivisible goods in many settings, this price is unacceptably high. In this work, we propose a hybrid fairness rule that balances a strict lexicographic preference ordering over classes of agents and a utilitarian objective that maximizes economic efficiency. We develop a utility function that favors disadvantaged groups lexicographically; but if cost to overall efficiency becomes too high, it smoothly switches to a utilitarian objective. This rule has only one parameter which is proportional to a bound on the price of fairness, and can be adjusted by policymakers. We apply this rule to kidney exchange, where needy patients swap willing but incompatible donors, and demonstrate on real data from a large exchange that our hybrid rule produces more reliable outcomes than other fairness rules.|['Duncan C. McElfresh', 'John P. Dickerson']|['cs.GT', 'cs.AI', 'I.2.11; J.4']
2017-03-16T23:31:28Z|2017-03-08T03:37:42Z|http://arxiv.org/abs/1703.02091v2|http://arxiv.org/pdf/1703.02091v2|Optimized Cost per Click in Taobao Display Advertising|Taobao, as the largest online retail platform in the world, provides billions of online display advertising impressions for millions of advertisers every day. For commercial purposes, the advertisers bid for specific spots and target crowds to compete for business traffic. The platform chooses the most suitable ads to display in tens of milliseconds. Common pricing methods include cost per mille (CPM) and cost per click (CPC). Traditional advertising systems target certain traits of users and ad placements with fixed bids, essentially regarded as coarse-grained matching of bid and traffic quality. However, the fixed bids set by the advertisers competing for different quality requests cannot fully optimize the advertisers' key requirements. Moreover, the platform has to be responsible for the business revenue and user experience. Thus, we proposed a bid optimizing strategy called optimized cost per click (OCPC) which automatically adjusts the bid to achieve finer matching of bid and traffic quality of page view (PV) request granularity. Our approach optimizes advertisers' demands, platform business revenue and user experience and as a whole improves traffic allocation efficiency. We have validated our approach in Taobao display advertising system in production. The online A/B test shows our algorithm yields substantially better results than previous fixed bid manner.|['Han Zhu', 'Junqi Jin', 'Chang Tan', 'Fei Pan', 'Yifan Zeng', 'Han Li', 'Kun Gai']|['cs.GT', 'stat.ML']
2017-03-16T23:31:28Z|2017-02-26T03:24:31Z|http://arxiv.org/abs/1702.07984v1|http://arxiv.org/abs/1702.07984v1|Collaborative Optimization for Collective Decision-making in Continuous   Spaces|Many societal decision problems lie in high-dimensional continuous spaces not amenable to the voting techniques common for their discrete or single-dimensional counterparts. These problems are typically discretized before running an election or decided upon through negotiation by representatives. We propose a meta-algorithm called \emph{Iterative Local Voting} for collective decision-making in this setting, in which voters are sequentially sampled and asked to modify a candidate solution within some local neighborhood of its current value, as defined by a ball in some chosen norm. In general, such schemes do not converge, or, when they do, the resulting solution does not have a natural description.   We first prove the convergence of this algorithm under appropriate choices of neighborhoods to plausible solutions in certain natural settings: when the voters' utilities can be expressed in terms of some form of distance from their ideal solution, and when these utilities are additively decomposable across dimensions. In many of these cases, we obtain convergence to the societal welfare maximizing solution.   We then describe an experiment in which we test our algorithm for the decision of the U.S. Federal Budget on Mechanical Turk with over 4,000 workers, employing neighborhoods defined by $\mathcal{L}^1, \mathcal{L}^2$ and $\mathcal{L}^\infty$ balls. We make several observations that inform future implementations of such a procedure.|['Nikhil Garg', 'Vijay Kamble', 'Ashish Goel', 'David Marn', 'Kamesh Munagala']|['cs.MA', 'cs.CY', 'cs.GT']
2017-03-16T23:31:28Z|2017-02-25T17:56:07Z|http://arxiv.org/abs/1702.07932v1|http://arxiv.org/pdf/1702.07932v1|The role of quantum correlations in Cop and Robber game|We introduce and study quantized versions of Cop and Robber game. We achieve this by using graph-preserving unitary operations, which are the quantum analogue of stochastic operations preserving the graph. We provide the tight bound for the number of operations required to reach the given state. By extending them to controlled operations, we define a quantum controlled Cop and Robber game, which expands the classical Cop and Robber game, as well as classically controlled quantum Cop and Robber game. In contrast to the typical scheme for introducing quantum games, we assume that both parties can utilise full information about the opponent's strategy. We show that the utilisation of the full knowledge about the opponent's state does not provide the advantage. Moreover, the chances of catching the Robber decreases for classically cop-win graphs. The result does not depend on the chosen model of evolution. On the other hand, the possibility to execute controlled quantum operations allows catching the Robber on almost all classically cop-win graphs. To provide interesting, non-trivial quantized Cop and Robber game, we need to enrich the structure of correlations between the players' systems. This result demonstrates that the ability to utilise quantum controlled operations is significantly stronger that the control restricted operating on classical selecting quantum operations only.|['Adam Glos', 'Jarosław Adam Miszczak']|['quant-ph', 'cs.DM', 'cs.GT', '05C57 (Primary), 91A46, 81P40 (Secondary)', 'G.2.2']
2017-03-16T23:31:28Z|2017-02-25T15:07:59Z|http://arxiv.org/abs/1702.07902v1|http://arxiv.org/pdf/1702.07902v1|Approval Voting with Intransitive Preferences|We extend Approval voting to the settings where voters may have intransitive preferences. The major obstacle to applying Approval voting in these settings is that voters are not able to clearly determine who they should approve or disapprove, due to the intransitivity of their preferences. An approach to address this issue is to apply tournament solutions to help voters make the decision. We study a class of voting systems where first each voter casts a vote defined as a tournament, then a well-defined tournament solution is applied to select the candidates who are assumed to be approved by the voter. Winners are the ones receiving the most approvals. We study axiomatic properties of this class of voting systems and complexity of control and bribery problems for these voting systems.|['Yongjie Yang']|['cs.GT', 'cs.CC', 'cs.DM']
2017-03-16T23:31:28Z|2017-02-25T00:05:57Z|http://arxiv.org/abs/1702.07810v1|http://arxiv.org/pdf/1702.07810v1|A Decomposition of Forecast Error in Prediction Markets|We introduce and analyze sources of error in prediction market forecasts in order to characterize and bound the difference between a security's price and its ground truth value. We consider cost-function-based prediction markets in which an automated market maker adjusts security prices according to the history of trade. We decompose the forecasting error into four components: \emph{sampling error}, occurring because traders only possess noisy estimates of ground truth; \emph{risk-aversion effect}, arising because traders reveal beliefs only through self-interested trade; \emph{market-maker bias}, resulting from the use of a particular market maker (i.e., cost function) to facilitate trade; and finally, \emph{convergence error}, arising because, at any point in time, market prices may still be in flux. Our goal is to understand the tradeoffs between these error components, and how they are influenced by design decisions such as the functional form of the cost function and the amount of liquidity in the market. We specifically consider a model in which traders have exponential utility and exponential-family beliefs drawn with an independent noise relative to ground truth. In this setting, sampling error and risk-aversion effect vanish as the number of traders grows, but there is a tradeoff between the other two components: decreasing the market maker's liquidity results in smaller market-maker bias, but may also slow down convergence. We provide both upper and lower bounds on market-maker bias and convergence error, and demonstrate via numerical simulations that these bounds are tight. Our results yield new insights into the question of how to set the market's liquidity parameter, and into the extent to which markets that enforce coherent prices across securities produce better predictions than markets that price securities independently.|['Miroslav Dudík', 'Sébastien Lahaie', 'Ryan Rogers', 'Jennifer Wortman Vaughan']|['cs.GT']
2017-03-16T23:31:32Z|2017-02-25T00:00:36Z|http://arxiv.org/abs/1702.07806v1|http://arxiv.org/pdf/1702.07806v1|When Does Diversity of User Preferences Improve Outcomes in Selfish   Routing?|"We seek to understand when heterogeneity in user preferences yields improved outcomes in terms of overall cost. That this might be hoped for is based on the common belief that diversity is advantageous in many settings. We investigate this in the context of routing. Our main result is a sharp characterization of the network settings in which diversity always helps, versus those in which it is sometimes harmful.   Specifically, we consider routing games, where diversity arises in the way that users trade-off two criteria (such as time and money, or, in the case of stochastic delays, expectation and variance of delay). We consider both linear and non-linear combinations of the two criteria and view our main contributions as the following: 1) A participant-oriented measure of cost in the presence of user diversity, together with the identification of the natural benchmark: the same cost measure for an appropriately defined average of the diversity. 2) A full characterization of those network topologies for which diversity always helps, for all latency functions and demands. For single-commodity routings, these are series-parallel graphs, while for multi-commodity routings, they are the newly-defined ""block-matching"" networks. The latter comprise a suitable interweaving of multiple series-parallel graphs each connecting a distinct source-sink pair.   While the result for the single-commodity case may seem intuitive in light of the well-known Braess paradox, the two problems are different. But the main technical challenge is to establish the ""only if"" direction of the result for multi-commodity networks. This follows by constructing an instance where diversity hurts, and showing how to embed it in any network which is not block-matching, by carefully exploiting the way the simple source-sink paths of the commodities intersect in the ""non-block-matching"" portion of the network."|['Richard Cole', 'Thanasis Lianeas', 'Evdokia Nikolova']|['cs.GT']
2017-03-16T23:31:32Z|2017-02-24T17:09:22Z|http://arxiv.org/abs/1702.07665v1|http://arxiv.org/pdf/1702.07665v1|Truthful Mechanisms for Delivery with Mobile Agents|"We study the game-theoretic task of selecting mobile agents to deliver multiple items on a network. An instance is given by $m$ messages (physical objects) which have to be transported between specified source-target pairs in a weighted undirected graph, and $k$ mobile heterogeneous agents, each being able to transport one message at a time. Following a recent model by [B\""artschi et al. 2016], each agent $i$ consumes energy proportional to the distance it travels in the graph, where the different rates of energy consumption are given by weight factors $w_i$. We are interested in optimizing or approximating the total energy consumption over all selected agents.   Unlike previous research, we assume the weights to be private values known only to the respective agents. We present three different mechanisms which select, route and pay the agents in a truthful way that guarantees voluntary participation of the agents, while approximating the optimum energy consumption by a constant factor. To this end we analyze a previous structural result and an approximation algorithm given by [B\""artschi et al. 2017]. Finally, we show that for some instances in the case of a single message ($m=1$), the sum of the payments can be bounded in terms of the optimum as well."|['Andreas Bärtschi', 'Daniel Graf', 'Paolo Penna']|['cs.GT', 'cs.DS']
2017-03-16T23:31:32Z|2017-02-24T02:30:15Z|http://arxiv.org/abs/1702.07450v1|http://arxiv.org/pdf/1702.07450v1|Strongly-Typed Agents are Guaranteed to Interact Safely|As artificial agents proliferate, it is becoming increasingly important to ensure that their interactions with one another are well-behaved. In this paper, we formalize a common-sense notion of when algorithms are well-behaved: an algorithm is safe if it does no harm. Motivated by recent progress in deep learning, we focus on the specific case where agents update their actions according to gradient descent. The first result is that gradient descent converges to a Nash equilibrium in safe games.   The paper provides sufficient conditions that guarantee safe interactions. The main contribution is to define strongly-typed agents and show they are guaranteed to interact safely. A series of examples show that strong-typing generalizes certain key features of convexity and is closely related to blind source separation. The analysis introduce a new perspective on classical multilinear games based on tensor decomposition.|['David Balduzzi']|['cs.LG', 'cs.AI', 'cs.GT']
2017-03-16T23:31:32Z|2017-02-24T01:51:48Z|http://arxiv.org/abs/1702.07444v1|http://arxiv.org/pdf/1702.07444v1|Bandits with Movement Costs and Adaptive Pricing|We extend the model of Multi-armed Bandit with unit switching cost to incorporate a metric between the actions. We consider the case where the metric over the actions can be modeled by a complete binary tree, and the distance between two leaves is the size of the subtree of their least common ancestor, which abstracts the case that the actions are points on the continuous interval $[0,1]$ and the switching cost is their distance. In this setting, we give a new algorithm that establishes a regret of $\widetilde{O}(\sqrt{kT} + T/k)$, where $k$ is the number of actions and $T$ is the time horizon. When the set of actions corresponds to whole $[0,1]$ interval we can exploit our method for the task of bandit learning with Lipschitz loss functions, where our algorithm achieves an optimal regret rate of $\widetilde{\Theta}(T^{2/3})$, which is the same rate one obtains when there is no penalty for movements. As our main application, we use our new algorithm to solve an adaptive pricing problem. Specifically, we consider the case of a single seller faced with a stream of patient buyers. Each buyer has a private value and a window of time in which they are interested in buying, and they buy at the lowest price in the window, if it is below their value. We show that with an appropriate discretization of the prices, the seller can achieve a regret of $\widetilde{O}(T^{2/3})$ compared to the best fixed price in hindsight, which outperform the previous regret bound of $\widetilde{O}(T^{3/4})$ for the problem.|['Tomer Koren', 'Roi Livni', 'Yishay Mansour']|['cs.LG', 'cs.GT']
2017-03-16T23:31:32Z|2017-02-23T17:54:28Z|http://arxiv.org/abs/1702.07311v1|http://arxiv.org/abs/1702.07311v1|ERA: A Framework for Economic Resource Allocation for the Cloud|Cloud computing has reached significant maturity from a systems perspective, but currently deployed solutions rely on rather basic economics mechanisms that yield suboptimal allocation of the costly hardware resources. In this paper we present Economic Resource Allocation (ERA), a complete framework for scheduling and pricing cloud resources, aimed at increasing the efficiency of cloud resources usage by allocating resources according to economic principles. The ERA architecture carefully abstracts the underlying cloud infrastructure, enabling the development of scheduling and pricing algorithms independently of the concrete lower-level cloud infrastructure and independently of its concerns. Specifically, ERA is designed as a flexible layer that can sit on top of any cloud system and interfaces with both the cloud resource manager and with the users who reserve resources to run their jobs. The jobs are scheduled based on prices that are dynamically calculated according to the predicted demand. Additionally, ERA provides a key internal API to pluggable algorithmic modules that include scheduling, pricing and demand prediction. We provide a proof-of-concept software and demonstrate the effectiveness of the architecture by testing ERA over both public and private cloud systems -- Azure Batch of Microsoft and Hadoop/YARN. A broader intent of our work is to foster collaborations between economics and system communities. To that end, we have developed a simulation platform via which economics and system experts can test their algorithmic implementations.|['Moshe Babaioff', 'Yishay Mansour', 'Noam Nisan', 'Gali Noti', 'Carlo Curino', 'Nar Ganapathy', 'Ishai Menache', 'Omer Reingold', 'Moshe Tennenholtz', 'Erez Timnat']|['cs.GT', 'cs.DC']
2017-03-16T23:31:32Z|2017-02-23T17:49:40Z|http://arxiv.org/abs/1702.07309v1|http://arxiv.org/pdf/1702.07309v1|Bounding the inefficiency of compromise|Social networks on the Internet have seen an enormous growth recently and play a crucial role in different aspects of today's life. They have facilitated information dissemination in ways that have been beneficial for their users but they are often used strategically in order to spread information that only serves the objectives of particular users. These properties have inspired a revision of classical opinion formation models from sociology using game-theoretic notions and tools. We follow the same modeling approach, focusing on scenarios where the opinion expressed by each user is a compromise between her internal belief and the opinions of a small number of neighbors among her social acquaintances. We formulate simple games that capture this behavior and quantify the inefficiency of equilibria using the well-known notion of the price of anarchy. Our results indicate that compromise comes at a cost that strongly depends on the neighborhood size.|['Ioannis Caragiannis', 'Panagiotis Kanellopoulos', 'Alexandros A. Voudouris']|['cs.GT']
2017-03-16T23:31:32Z|2017-02-23T04:18:53Z|http://arxiv.org/abs/1702.07638v1|http://arxiv.org/pdf/1702.07638v1|Reward-penalty Mechanism for Reverse Supply Chain Network with   Asymmetric Information and Carbon Emission Constraints|We discuss the government's reward and penalty mechanism in the presence of asymmetric information and carbon emission constraint when downstream retailers compete in a reverse supply chain network. Considering five game models which are different in terms of the coordination structure of the reverse supply chain network and power structure of the reward-penalty mechanism: (1) the reverse supply chain network centralized decision-making model; (2) the reverse supply chain network centralized decision-making model with carbon emission constraint; (3) the retailers' competition reverse supply chain network decentralized decision-making model; (4) the retailers' competition reverse supply chain network decentralized decision-making model with carbon emission constraint; (5) the retailers' competition reverse supply chain network decentralized decision-making model with carbon emission constraint and the government's reward-penalty mechanism. Building the participation-incentive contract under each model use the principal-agent theory, and solving the model use the Lagrange multiplier method. We can get the following conclusion: 1) when the government implements the reward-penalty mechanism for carbon emission and recycling simultaneously, the recycling rate as well as the buy-back price offered by the manufacturer are higher than those when the government conducts reward-penalty mechanism exclusively for carbon emission; 2) when the government implements carbon emission constraint, both retailers' selling prices of the new product are higher than those when no carbon emission constraint is forced; 3) there is no certain relationship between the two retailers' selling prices of the new product when the government implements the reward-penalty mechanism only for carbon emission and when it implements the mechanism for carbon emission as well as recycling.|['Xiao-qing Zhang', 'Xi-gang Yuan']|['math.OC', 'cs.GT']
2017-03-16T23:31:32Z|2017-02-22T22:43:45Z|http://arxiv.org/abs/1702.07032v1|http://arxiv.org/pdf/1702.07032v1|On the Complexity of Bundle-Pricing and Simple Mechanisms|We show that the problem of finding an optimal bundle-pricing for a single additive buyer is #P-hard, even when the distributions have support size 2 for each item and the optimal solution is guaranteed to be a simple one: the seller picks a price for the grand bundle and a price for each individual item; the buyer can purchase either the grand bundle at the given price or any bundle of items at their total individual prices. We refer to this simple and natural family of pricing schemes as discounted item-pricings. In addition to the hardness result, we show that when the distributions are i.i.d. with support size 2, a discounted item-pricing can achieve the optimal revenue obtainable by lottery-pricings and it can be found in polynomial time.|['Xi Chen', 'George Matikas', 'Dimitris Paparas', 'Mihalis Yannakakis']|['cs.GT', 'cs.CC', 'cs.DS']
2017-03-16T23:31:32Z|2017-02-22T22:34:57Z|http://arxiv.org/abs/1702.07031v1|http://arxiv.org/pdf/1702.07031v1|Proactive Resource Management in LTE-U Systems: A Deep Learning   Perspective|LTE in unlicensed spectrum (LTE-U) is a promising approach to overcome the wireless spectrum scarcity. However, to reap the benefits of LTE-U, a fair coexistence mechanism with other incumbent WiFi deployments is required. In this paper, a novel deep learning approach is proposed for modeling the resource allocation problem of LTE-U small base stations (SBSs). The proposed approach enables multiple SBSs to proactively perform dynamic channel selection, carrier aggregation, and fractional spectrum access while guaranteeing fairness with existing WiFi networks and other LTE-U operators. Adopting a proactive coexistence mechanism enables future delay-intolerant LTE-U data demands to be served within a given prediction window ahead of their actual arrival time thus avoiding the underutilization of the unlicensed spectrum during off-peak hours while maximizing the total served LTE-U traffic load. To this end, a noncooperative game model is formulated in which SBSs are modeled as Homo Egualis agents that aim at predicting a sequence of future actions and thus achieving long-term equal weighted fairness with WLAN and other LTE-U operators over a given time horizon. The proposed deep learning algorithm is then shown to reach a mixed-strategy Nash equilibrium (NE), when it converges. Simulation results using real data traces show that the proposed scheme can yield up to 28% and 11% gains over a conventional reactive approach and a proportional fair coexistence mechanism, respectively. The results also show that the proposed framework prevents WiFi performance degradation for a densely deployed LTE-U network.|['Ursula Challita', 'Li Dong', 'Walid Saad']|['cs.IT', 'cs.AI', 'cs.GT', 'math.IT']
2017-03-16T23:31:32Z|2017-02-22T18:06:24Z|http://arxiv.org/abs/1702.06922v1|http://arxiv.org/pdf/1702.06922v1|Formation of coalition structures as a non-cooperative game|The paper defines a family of nested non-cooperative simultaneous finite games to study coalition structure formation with intra and inter-coalition externalities. Every game has two outcomes - an allocation of players over coalitions and a payoff profile for every player.   Every game in the family has an equilibrium in mixed strategies. The equilibrium can generate more than one coalition with a presence of intra and inter group externalities. These properties make it different from the Shapley value, strong Nash, coalition-proof equilibrium, core, kernel, nucleolus. The paper demonstrates some applications: non-cooperative cooperation, Bayesian game, stochastic games and construction of a non-cooperative criterion of coalition structure stability for studying focal points. An example demonstrates that a payoff profile in the Prisoners' Dilemma is non-informative to deduce a cooperation of players.|['Dmitry Levando']|['math.OC', 'cs.GT']
2017-03-16T23:31:36Z|2017-02-22T15:39:12Z|http://arxiv.org/abs/1702.06858v1|http://arxiv.org/pdf/1702.06858v1|Emptiness of zero automata is decidable|Zero automata are a probabilistic extension of parity automata on infinite trees. The satisfiability of a certain probabilistic variant of mso, called tmso + zero, reduces to the emptiness problem for zero automata. We introduce a variant of zero automata called nonzero automata. We prove that for every zero automaton there is an equivalent nonzero automaton of quadratic size and the emptiness problem of nonzero automata is decidable, with complexity np. These results imply that tmso + zero has decidable satisfiability.|['Mikolaj Bojańczyk', 'Hugo Gimbert', 'Edon Kelmendi']|['cs.FL', 'cs.GT']
2017-03-16T23:31:36Z|2017-02-25T23:52:16Z|http://arxiv.org/abs/1702.06810v2|http://arxiv.org/pdf/1702.06810v2|Pricing average price advertisement options when underlying spot market   prices are discontinuous|Advertisement (ad) options have been recently studied as a novel guaranteed delivery (GD) system in online advertising. In essence, an ad option is a contract that gives an advertiser a right but not obligation to enter into transactions to purchase ad inventories such as page views or link clicks from a specific slot at one or multiple pre-specified prices in a specific future period. Compared to guaranteed contracts, the advertiser pays a lower upfront fee but can have greater flexibility and more control in advertising. So far ad option studies have been restricted to the situations where the option payoff is determined by the underlying auction payment price at a specific time point and the price evolution over time is assumed to be continuous. The former leads to a biased option payoff calculation and the latter is invalid empirically for many ad slots. This paper discusses a new option pricing framework which can be applied to a general situation. The option payoff is calculated based on the average price over a specific future period. As we use the general mean, our framework contains different payoff functions as special cases. Further, we use jump-diffusion stochastic models to describe the auction payment price movement, which have Markov and price discontinuity properties, and those properties are validated by our statistical investigation of ad auctions from different datasets. In the paper, we propose a general option pricing solution based on Monte Carlo simulation and also give an explicit pricing formula for a special case. The latter is also a generalisation of the option pricing models in some other recent developments.|['Bowei Chen', 'Mohan S. Kankanhalli']|['cs.GT']
2017-03-16T23:31:36Z|2017-02-22T02:07:25Z|http://arxiv.org/abs/1702.06645v1|http://arxiv.org/pdf/1702.06645v1|Resource Sharing Among mmWave Cellular Service Providers in a Vertically   Differentiated Duopoly|With the increasing interest in the use of millimeter wave bands for 5G cellular systems comes renewed interest in resource sharing. Properties of millimeter wave bands such as massive bandwidth, highly directional antennas, high penetration loss, and susceptibility to shadowing, suggest technical advantages to spectrum and infrastructure sharing in millimeter wave cellular networks. However, technical advantages do not necessarily translate to increased profit for service providers, or increased consumer surplus. In this paper, detailed network simulations are used to better understand the economic implications of resource sharing in a vertically differentiated duopoly market for cellular service. The results suggest that resource sharing is less often profitable for millimeter wave service providers compared to microwave cellular service providers, and does not necessarily increase consumer surplus.|['Fraida Fund', 'Shahram Shahsavari', 'Shivendra S. Panwar', 'Elza Erkip', 'Sundeep Rangan']|['cs.NI', 'cs.GT']
2017-03-16T23:31:36Z|2017-02-21T15:30:28Z|http://arxiv.org/abs/1702.06439v1|http://arxiv.org/pdf/1702.06439v1|Admissibility in Concurrent Games|In this paper, we study the notion of admissibility for randomised strategies in concurrent games. Intuitively, an admissible strategy is one where the player plays `as well as possible', because there is no other strategy that dominates it, i.e., that wins (almost surely) against a super set of adversarial strategies. We prove that admissible strategies always exist in concurrent games, and we characterise them precisely. Then, when the objectives of the players are omega-regular, we show how to perform assume-admissible synthesis, i.e., how to compute admissible strategies that win (almost surely) under the hypothesis that the other players play admissible|['Nicolas Basset', 'Gilles Geeraerts', 'Jean-François Raskin', 'Ocan Sankur']|['cs.GT', 'cs.LO']
2017-03-16T23:31:36Z|2017-02-21T15:19:56Z|http://arxiv.org/abs/1702.06436v1|http://arxiv.org/pdf/1702.06436v1|Contract-Theoretic Resource Allocation for Critical Infrastructure   Protection|Critical infrastructure protection (CIP) is envisioned to be one of the most challenging security problems in the coming decade. One key challenge in CIP is the ability to allocate resources, either personnel or cyber, to critical infrastructures with different vulnerability and criticality levels. In this work, a contract-theoretic approach is proposed to solve the problem of resource allocation in critical infrastructure with asymmetric information. A control center (CC) is used to design contracts and offer them to infrastructures' owners. A contract can be seen as an agreement between the CC and infrastructures using which the CC allocates resources and gets rewards in return. Contracts are designed in a way to maximize the CC's benefit and motivate each infrastructure to accept a contract and obtain proper resources for its protection. Infrastructures are defined by both vulnerability levels and criticality levels which are unknown to the CC. Therefore, each infrastructure can claim that it is the most vulnerable or critical to gain more resources. A novel mechanism is developed to handle such an asymmetric information while providing the optimal contract that motivates each infrastructure to reveal its actual type. The necessary and sufficient conditions for such resource allocation contracts under asymmetric information are derived. Simulation results show that the proposed contract-theoretic approach maximizes the CC's utility while ensuring that no infrastructure has an incentive to ask for another contract, despite the lack of exact information at the CC.|['AbdelRahman Eldosouky', 'Walid Saad', 'Charles Kamhoua', 'and Kevin Kwiat']|['cs.CR', 'cs.GT']
2017-03-16T23:31:36Z|2017-02-20T21:54:26Z|http://arxiv.org/abs/1702.06189v1|http://arxiv.org/pdf/1702.06189v1|A Graphical Evolutionary Game Approach to Social Learning|In this work, we study the social learning problem, in which agents of a networked system collaborate to detect the state of the nature based on their private signals. A novel distributed graphical evolutionary game theoretic learning method is proposed. In the proposed game-theoretic method, agents only need to communicate their binary decisions rather than the real-valued beliefs with their neighbors, which endows the method with low communication complexity. Under mean field approximations, we theoretically analyze the steady state equilibria of the game and show that the evolutionarily stable states (ESSs) coincide with the decisions of the benchmark centralized detector. Numerical experiments are implemented to confirm the effectiveness of the proposed game-theoretic learning method.|['Xuanyu Cao', 'K. J. Ray Liu']|['cs.GT']
2017-03-16T23:31:36Z|2017-02-20T17:56:50Z|http://arxiv.org/abs/1703.00807v1|http://arxiv.org/pdf/1703.00807v1|Privacy Management and Optimal Pricing in People-Centric Sensing|With the emerging sensing technologies such as mobile crowdsensing and Internet of Things (IoT), people-centric data can be efficiently collected and used for analytics and optimization purposes. This data is typically required to develop and render people-centric services. In this paper, we address the privacy implication, optimal pricing, and bundling of people-centric services. We first define the inverse correlation between the service quality and privacy level from data analytics perspectives. We then present the profit maximization models of selling standalone, complementary, and substitute services. Specifically, the closed-form solutions of the optimal privacy level and subscription fee are derived to maximize the gross profit of service providers. For interrelated people-centric services, we show that cooperation by service bundling of complementary services is profitable compared to the separate sales but detrimental for substitutes. We also show that the market value of a service bundle is correlated with the degree of contingency between the interrelated services. Finally, we incorporate the profit sharing models from game theory for dividing the bundling profit among the cooperative service providers.|['Mohammad Abu Alsheikh', 'Dusit Niyato', 'Derek Leong', 'Ping Wang', 'Zhu Han']|['cs.GT']
2017-03-16T23:31:36Z|2017-02-22T01:48:29Z|http://arxiv.org/abs/1702.06062v2|http://arxiv.org/pdf/1702.06062v2|Simple vs Optimal Mechanisms in Auctions with Convex Payments|We investigate approximately optimal mechanisms in settings where bidders' utility functions are non-linear; specifically, convex, with respect to payments (such settings arise, for instance, in procurement auctions for energy). We provide constant factor approximation guarantees for mechanisms that are independent of bidders' private information (i.e., prior-free), and for mechanisms that rely to an increasing extent on that information (i.e., detail free). We also describe experiments, which show that for randomly drawn monotone hazard rate distributions, our mechanisms achieve at least 80\% of the optimal revenue, on average. Both our theoretical and experimental results show that in the convex payment setting, it is desirable to allocate across multiple bidders, rather than only to bidders with the highest (virtual) value, as in the traditional quasi-linear utility setting.|['Amy Greenwald', 'Takehiro Oyakawa', 'Vasilis Syrgkanis']|['cs.GT']
2017-03-16T23:31:36Z|2017-02-20T00:34:00Z|http://arxiv.org/abs/1702.05825v1|http://arxiv.org/pdf/1702.05825v1|Sustainable Fair Division|In this paper, I summarize our work on online fair division. In particular, I present two models for online fair division: (1) one existing model for fair division in food banks and (2) one new model for fair division of deceased organs to patients. I further discuss simple mechanisms for these models that allocate the resources as they arrive to agents. In practice, agents are often risk-averse having imperfect information. Within this assumption, I report several interesting axiomatic and complexity results for these mechanisms and conclude with future work.|['Martin Aleksandrov']|['cs.GT']
2017-03-16T23:31:36Z|2017-02-19T18:23:47Z|http://arxiv.org/abs/1702.05778v1|http://arxiv.org/pdf/1702.05778v1|The Absent-Minded Driver Problem Redux|This paper reconsiders the problem of the absent-minded driver who must choose between alternatives with different payoff with imperfect recall and varying degrees of knowledge of the system. The classical absent-minded driver problem represents the case with limited information and it has bearing on the general area of communication and learning, social choice, mechanism design, auctions, theories of knowledge, belief, and rational agency. Within the framework of extensive games, this problem has applications to many artificial intelligence scenarios. It is obvious that the performance of the agent improves as information available increases. It is shown that a non-uniform assignment strategy for successive choices does better than a fixed probability strategy. We consider both classical and quantum approaches to the problem. We argue that the superior performance of quantum decisions with access to entanglement cannot be fairly compared to a classical algorithm. If the cognitive systems of agents are taken to have access to quantum resources, or have a quantum mechanical basis, then that can be leveraged into superior performance.|['Subhash Kak']|['cs.AI', 'cs.GT']
2017-03-16T23:31:40Z|2017-02-18T18:23:17Z|http://arxiv.org/abs/1702.05640v1|http://arxiv.org/pdf/1702.05640v1|Obvious Strategyproofness Needs Monitoring for Good Approximations|Obvious strategyproofness (OSP) is an appealing concept as it allows to maintain incentive compatibility even in the presence of agents that are not fully rational, e.g., those who struggle with contingent reasoning [Li, 2015]. However, it has been shown to impose some limitations, e.g., no OSP mechanism can return a stable matching [Ashlagi and Gonczarowski, 2015].   We here deepen the study of the limitations of OSP mechanisms by looking at their approximation guarantees for basic optimization problems paradigmatic of the area, i.e., machine scheduling and facility location. We prove a number of bounds on the approximation guarantee of OSP mechanisms, which show that OSP can come at a significant cost. However, rather surprisingly, we prove that OSP mechanisms can return optimal solutions when they use monitoring -- a novel mechanism design paradigm that introduces a mild level of scrutiny on agents' declarations [Kovacs et al., 2015].|['Diodato Ferraioli', 'Carmine Ventre']|['cs.GT']
2017-03-16T23:31:40Z|2017-02-17T22:39:37Z|http://arxiv.org/abs/1702.05536v1|http://arxiv.org/pdf/1702.05536v1|Beyond the Hazard Rate: More Perturbation Algorithms for Adversarial   Multi-armed Bandits|Recent work on follow the perturbed leader (FTPL) algorithms for the adversarial multi-armed bandit problem has highlighted the role of the hazard rate of the distribution generating the perturbations. Assuming that the hazard rate is bounded, it is possible to provide regret analyses for a variety of FTPL algorithms for the multi-armed bandit problem. This paper pushes the inquiry into regret bounds for FTPL algorithms beyond the bounded hazard rate condition. There are good reasons to do so: natural distributions such as the uniform and Gaussian violate the condition. We give regret bounds for both bounded support and unbounded support distributions without assuming the hazard rate condition. We also disprove a conjecture that the Gaussian distribution cannot lead to a low-regret algorithm. In fact, it turns out that it leads to near optimal regret, up to logarithmic factors. A key ingredient in our approach is the introduction of a new notion called the generalized hazard rate.|['Zifan Li', 'Ambuj Tewari']|['cs.LG', 'cs.GT', 'stat.ML']
2017-03-16T23:31:40Z|2017-02-17T18:52:11Z|http://arxiv.org/abs/1702.05472v1|http://arxiv.org/pdf/1702.05472v1|Threshold Constraints with Guarantees for Parity Objectives in Markov   Decision Processes|The beyond worst-case synthesis problem was introduced recently by Bruy\`ere et al. [BFRR14]: it aims at building system controllers that provide strict worst-case performance guarantees against an antagonistic environment while ensuring higher expected performance against a stochastic model of the environment. Our work extends the framework of [BFRR14] and follow-up papers, which focused on quantitative objectives, by addressing the case of $\omega$-regular conditions encoded as parity objectives, a natural way to represent functional requirements of systems.   We build strategies that satisfy a main parity objective on all plays, while ensuring a secondary one with sufficient probability. This setting raises new challenges in comparison to quantitative objectives, as one cannot easily mix different strategies without endangering the functional properties of the system. We establish that, for all variants of this problem, deciding the existence of a strategy lies in ${\sf NP} \cap {\sf coNP}$, the same complexity class as classical parity games. Hence, our framework provides additional modeling power while staying in the same complexity class.   [BFRR14] V\'eronique Bruy\`ere, Emmanuel Filiot, Mickael Randour, and Jean-Fran\c{c}ois Raskin. Meet your expectations with guarantees: Beyond worst-case synthesis in quantitative games. In Ernst W. Mayr and Natacha Portier, editors, 31st International Symposium on Theoretical Aspects of Computer Science, STACS 2014, March 5-8, 2014, Lyon, France, volume 25 of LIPIcs, pages 199-213. Schloss Dagstuhl - Leibniz - Zentrum fuer Informatik, 2014.|['Raphaël Berthon', 'Mickael Randour', 'Jean-François Raskin']|['cs.LO', 'cs.AI', 'cs.FL', 'cs.GT', 'math.PR']
2017-03-16T23:31:40Z|2017-02-25T08:57:27Z|http://arxiv.org/abs/1702.05371v2|http://arxiv.org/pdf/1702.05371v2|Distributionally Robust Games, Part I: f-Divergence and Learning|In this paper we introduce the novel framework of distributionally robust games. These are multi-player games where each player models the state of nature using a worst-case distribution, also called adversarial distribution. Thus each player's payoff depends on the other players' decisions and on the decision of a virtual player (nature) who selects an adversarial distribution of scenarios. This paper provides three main contributions. Firstly, the distributionally robust game is formulated using the statistical notions of f-divergence between two distributions, here represented by the adversarial distribution, and the exact distribution. Secondly, the complexity of the problem is significantly reduced by means of triality theory. Thirdly, stochastic Bregman learning algorithms are proposed to speedup the computation of robust equilibria. Finally, the theoretical findings are illustrated in a convex setting and its limitations are tested with a non-convex non-concave function.|['Dario Bauso', 'Jian Gao', 'Hamidou Tembine']|['math.OC', 'cs.GT']
2017-03-16T23:31:40Z|2017-02-25T09:20:25Z|http://arxiv.org/abs/1702.05361v2|http://arxiv.org/pdf/1702.05361v2|Empathy in One-Shot Prisoner Dilemma|Strategic decision making involves affective and cognitive functions like reasoning, cognitive and emotional empathy which may be subject to age and gender differences. However, empathy-related changes in strategic decision-making and their relation to age, gender and neuropsychological functions have not been studied widely. In this article, we study a one-shot prisoner dilemma from a psychological game theory viewpoint. Forty seven participants (28 women and 19 men), aged 18 to 42 years, were tested with a empathy questionnaire and a one-shot prisoner dilemma questionnaire comprising a closiness option with the other participant. The percentage of cooperation and defection decisions was analyzed. A new empathetic payoff model was calculated to fit the observations from the test whether multi-dimensional empathy levels matter in the outcome. A significant level of cooperation is observed in the experimental one-shot game. The collected data suggests that perspective taking, empathic concern and fantasy scale are strongly correlated and have an important effect on cooperative decisions. However, their effect in the payoff is not additive. Mixed scales as well as other non-classified subscales (25+8 out of 47) were observed from the data.|['Giulia Rossi', 'Alain Tcheukam', 'Hamidou Tembine']|['cs.GT']
2017-03-16T23:31:40Z|2017-02-25T09:23:38Z|http://arxiv.org/abs/1702.05355v2|http://arxiv.org/pdf/1702.05355v2|How Much Does Users' Psychology Matter in Engineering Mean-Field-Type   Games|"Until now mean-field-type game theory was not focused on cognitively-plausible models of choices in humans, animals, machines, robots, software-defined and mobile devices strategic interactions. This work presents some effects of users' psychology in mean-field-type games. In addition to the traditional ""material"" payoff modelling, psychological patterns are introduced in order to better capture and understand behaviors that are observed in engineering practice or in experimental settings. The psychological payoff value depends upon choices, mean-field states, mean-field actions, empathy and beliefs. It is shown that the affective empathy enforces mean-field equilibrium payoff equity and improves fairness between the players. It establishes equilibrium systems for such interactive decision-making problems. Basic empathy concepts are illustrated in several important problems in engineering including resource sharing, packet collision minimization, energy markets, and forwarding in Device-to-Device communications. The work conducts also an experiment with 47 people who have to decide whether to cooperate or not. The basic Interpersonal Reactivity Index of empathy metrics were used to measure the empathy distribution of each participant. Android app called Empathizer is developed to analyze systematically the data obtained from the participants. The experimental results reveal that the dominated strategies of the classical game theory are not dominated any more when users' psychology is involved, and a significant level of cooperation is observed among the users who are positively partially empathetic."|['Giulia Rossi', 'Alain Tcheukam', 'Hamidou Tembine']|['cs.GT', 'cs.MA']
2017-03-16T23:31:40Z|2017-02-16T19:23:12Z|http://arxiv.org/abs/1702.05119v1|http://arxiv.org/pdf/1702.05119v1|Evolutionary prisoner's dilemma games coevolving on adaptive networks|We study a model for switching strategies in the Prisoner's Dilemma game on adaptive networks of player pairings that coevolve as players attempt to maximize their return. We use a node-based strategy model with each player following one strategy (cooperate or defect) at a time with all of its neighbors. We improve on the existing pair approximation (PA) model for this system by using approximate master equations (AMEs). We explore the parameter space demonstrating the accuracy of the approximation as compared with simulations. We study two variations of this partner-switching model to investigate the evolution, predict stationary states, and compare the total utilities and other qualitative differences between these two variants.|['Hsuan-Wei Lee', 'Nishant Malik', 'Peter J. Mucha']|['cs.SI', 'cs.GT', '91']
2017-03-16T23:31:40Z|2017-02-16T03:39:07Z|http://arxiv.org/abs/1702.04849v1|http://arxiv.org/pdf/1702.04849v1|Theoretical and Practical Advances on Smoothing for Extensive-Form Games|Sparse iterative methods, in particular first-order methods, are known to be among the most effective in solving large-scale two-player zero-sum extensive-form games. The convergence rates of these methods depend heavily on the properties of the distance-generating function that they are based on. We investigate the acceleration of first-order methods for solving extensive-form games through better design of the dilated entropy function---a class of distance-generating functions related to the domains associated with the extensive-form games. By introducing a new weighting scheme for the dilated entropy function, we develop the first distance-generating function for the strategy spaces of sequential games that has no dependence on the branching factor of the player. This result improves the convergence rate of several first-order methods by a factor of $\Omega(b^dd)$, where $b$ is the branching factor of the player, and $d$ is the depth of the game tree.   Thus far, counterfactual regret minimization methods have been faster in practice, and more popular, than first-order methods despite their theoretically inferior convergence rates. Using our new weighting scheme and practical tuning we show that, for the first time, the excessive gap technique can be made faster than the fastest counterfactual regret minimization algorithm, CFR+, in practice.|['Christian Kroer', 'Kevin Waugh', 'Fatma Kilinc-Karzan', 'Tuomas Sandholm']|['cs.GT', 'cs.AI']
2017-03-16T23:31:40Z|2017-02-16T17:04:36Z|http://arxiv.org/abs/1702.04254v2|http://arxiv.org/pdf/1702.04254v2|"A ""Quantal Regret"" Method for Structural Econometrics in Repeated Games"|"We suggest a general method for inferring players' values from their actions in repeated games. The method extends and improves upon the recent suggestion of (Nekipelov et al., EC 2015) and is based on the assumption that players are more likely to exhibit sequences of actions that have lower regret.   We evaluate this ""quantal regret"" method on two different datasets from experiments of repeated games with controlled player values: those of (Selten and Chmura, AER 2008) on a variety of two-player 2x2 games and our own experiment on ad-auctions (Noti et al., WWW 2014). We find that the quantal regret method is consistently and significantly more precise than either ""classic"" econometric methods that are based on Nash equilibria, or the ""min-regret"" method of (Nekipelov et al., EC 2015)."|['Noam Nisan', 'Gali Noti']|['cs.GT']
2017-03-16T23:31:40Z|2017-02-14T10:13:15Z|http://arxiv.org/abs/1702.04138v1|http://arxiv.org/pdf/1702.04138v1|Agent Failures in All-Pay Auctions|All-pay auctions, a common mechanism for various human and agent interactions, suffers, like many other mechanisms, from the possibility of players' failure to participate in the auction. We model such failures, and fully characterize equilibrium for this class of games, we present a symmetric equilibrium and show that under some conditions the equilibrium is unique. We reveal various properties of the equilibrium, such as the lack of influence of the most-likely-to-participate player on the behavior of the other players. We perform this analysis with two scenarios: the sum-profit model, where the auctioneer obtains the sum of all submitted bids, and the max-profit model of crowdsourcing contests, where the auctioneer can only use the best submissions and thus obtains only the winning bid.   Furthermore, we examine various methods of influencing the probability of participation such as the effects of misreporting one's own probability of participating, and how influencing another player's participation chances changes the player's strategy.|['Yoad Lewenberg', 'Omer Lev', 'Yoram Bachrach', 'Jeffrey S. Rosenschein']|['cs.GT', 'cs.MA']
2017-03-16T23:31:44Z|2017-02-13T20:37:18Z|http://arxiv.org/abs/1702.03978v1|http://arxiv.org/pdf/1702.03978v1|Multicast Capacity Through Perfect Domination|The capacity of wireless networks is a classic and important topic of study. Informally, the capacity of a network is simply the total amount of information which it can transfer. In the context of models of wireless radio networks, this has usually meant the total number of point-to-point messages which can be sent or received in one time step. This definition has seen intensive study in recent years, particularly with respect to more accurate models of radio networks such as the SINR model. This paper is motivated by an obvious fact: radio antennae are (at least traditionally) omnidirectional, and hence point-to-point connections are not necessarily the best definition of capacity. To fix this, we introduce a new definition of capacity as the maximum number of messages which can be received in one round, and show that this is related to a new optimization problem we call the Maximum Perfect Dominated Set (MaxPDS) problem. Using this relationship we give tight upper and lower bounds for approximating the capacity. We also analyze this notion of capacity under game-theoretic constraints, giving tight bounds on both the Price of Anarchy and the Price of Stability.|['Michael Dinitz', 'Naomi Ephraim']|['cs.GT']
2017-03-16T23:31:44Z|2017-02-13T04:29:14Z|http://arxiv.org/abs/1702.03627v1|http://arxiv.org/pdf/1702.03627v1|Mechanism Design in Social Networks|This paper studies an auction design problem for a seller to sell a commodity in a social network, where each individual (the seller or a buyer) can only communicate with her neighbors. The challenge to the seller is to design a mechanism to incentivize the buyers, who are aware of the auction, to further propagate the information to their neighbors so that more buyers will participate in the auction and hence, the seller will be able to make a higher revenue. We propose a novel auction mechanism, called information diffusion mechanism (IDM), which incentivizes the buyers to not only truthfully report their valuations on the commodity to the seller, but also further propagate the auction information to all their neighbors. In comparison, the direct extension of the well-known Vickrey-Clarke-Groves (VCG) mechanism in social networks can also incentivize the information diffusion, but it will decrease the seller's revenue or even lead to a deficit sometimes. The formalization of the problem has not yet been addressed in the literature of mechanism design and our solution is very significant in the presence of large-scale online social networks.|['Bin Li', 'Dong Hao', 'Dengji Zhao', 'Tao Zhou']|['cs.GT']
2017-03-16T23:31:44Z|2017-02-13T04:08:17Z|http://arxiv.org/abs/1702.03620v1|http://arxiv.org/pdf/1702.03620v1|Complexity of mixed equilibria in Boolean games|Boolean games are a succinct representation of strategic games wherein a player seeks to satisfy a formula of propositional logic by selecting a truth assignment to a set of propositional variables under his control.   The framework has proven popular within the multiagent community, however, almost invariably, the work to date has been restricted to the case of pure strategies. Such a focus is highly restrictive as the notion of randomised play is fundamental to the theory of strategic games -- even very simple games can fail to have pure-strategy equilibria, but every finite game has at least one equilibrium in mixed strategies.   To address this, the present work focuses on the complexity of algorithmic problems dealing with mixed strategies in Boolean games. The main result is that the problem of determining whether a two-player game has an equilibrium satisfying a given payoff constraint is NEXP-complete. Based on this result, we then demonstrate that a number of other decision problems, such as the uniqueness of an equilibrium or the satisfaction of a given formula in equilibrium, are either NEXP or coNEXP-complete. The proof techniques developed in the course of this are then used to show that the problem of deciding whether a given profile is in equilibrium is coNP^#P-hard, and the problem of deciding whether a Boolean game has a rational-valued equilibrium is NEXP-hard, and whether a two-player Boolean game has an irrational-valued equilibrium is NEXP-complete. Finally, we show that determining whether the value of a two-player zero-sum game exceeds a given threshold is EXP-complete.|['Egor Ianovski']|['cs.GT']
2017-03-16T23:31:44Z|2017-02-13T03:07:49Z|http://arxiv.org/abs/1702.03615v1|http://arxiv.org/pdf/1702.03615v1|Online Prediction with Selfish Experts|"We consider the problem of binary prediction with expert advice in settings where experts have agency and seek to maximize their credibility. This paper makes three main contributions. First, it defines a model to reason formally about settings with selfish experts, and demonstrates that ""incentive compatible"" (IC) algorithms are closely related to the design of proper scoring rules. Designing a good IC algorithm is easy if the designer's loss function is quadratic, but for other loss functions, novel techniques are required. Second, we design IC algorithms with good performance guarantees for the absolute loss function. Third, we give a formal separation between the power of online prediction with selfish experts and online prediction with honest experts by proving lower bounds for both IC and non-IC algorithms. In particular, with selfish experts and the absolute loss function, there is no (randomized) algorithm for online prediction-IC or otherwise-with asymptotically vanishing regret."|['Tim Roughgarden', 'Okke Schrijvers']|['cs.GT']
2017-03-16T23:31:44Z|2017-02-11T00:29:30Z|http://arxiv.org/abs/1702.04240v1|http://arxiv.org/pdf/1702.04240v1|Prospect Theory for Enhanced Cyber-Physical Security of Drone Delivery   Systems: A Network Interdiction Game|"The use of unmanned aerial vehicles (UAVs) as delivery systems of online goods is rapidly becoming a global norm, as corroborated by Amazon's ""Prime Air"" and Google's ""Project Wing"" projects. However, the real-world deployment of such drone delivery systems faces many cyber-physical security challenges. In this paper, a novel mathematical framework for analyzing and enhancing the security of drone delivery systems is introduced. In this regard, a zero-sum network interdiction game is formulated between a vendor, operating a drone delivery system, and a malicious attacker. In this game, the vendor seeks to find the optimal path that its UAV should follow, to deliver a purchase from the vendor's warehouse to a customer location, to minimize the delivery time. Meanwhile, an attacker seeks to choose an optimal location to interdict the potential paths of the UAVs, so as to inflict cyber or physical damage to it, thus, maximizing its delivery time. First, the Nash equilibrium point of this game is characterized. Then, to capture the subjective behavior of both the vendor and attacker, new notions from prospect theory are incorporated into the game. These notions allow capturing the vendor's and attacker's i) subjective perception of attack success probabilities, and ii) their disparate subjective valuations of the achieved delivery times relative to a certain target delivery time. Simulation results have shown that the subjective decision making of the vendor and attacker leads to adopting risky path selection strategies which inflict delays to the delivery, thus, yielding unexpected delivery times which surpass the target delivery time set by the vendor."|['Anibal Sanjab', 'Walid Saad', 'Tamer Başar']|['cs.GT', 'cs.IT', 'math.IT']
2017-03-16T23:31:44Z|2017-02-10T01:48:40Z|http://arxiv.org/abs/1702.03037v1|http://arxiv.org/pdf/1702.03037v1|Multi-agent Reinforcement Learning in Sequential Social Dilemmas|Matrix games like Prisoner's Dilemma have guided research on social dilemmas for decades. However, they necessarily treat the choice to cooperate or defect as an atomic action. In real-world social dilemmas these choices are temporally extended. Cooperativeness is a property that applies to policies, not elementary actions. We introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. We analyze the dynamics of policies learned by multiple self-interested independent learning agents, each using its own deep Q-network, on two Markov games we introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We characterize how learned behavior in each domain changes as a function of environmental factors including resource abundance. Our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation.|['Joel Z. Leibo', 'Vinicius Zambaldi', 'Marc Lanctot', 'Janusz Marecki', 'Thore Graepel']|['cs.MA', 'cs.AI', 'cs.GT', 'cs.LG']
2017-03-16T23:31:44Z|2017-02-09T23:58:38Z|http://arxiv.org/abs/1703.00899v1|http://arxiv.org/pdf/1703.00899v1|"Addendum to ""A Market Framework for Eliciting Private Data"""|In Waggoner et al. [NIPS 2015], we proposed an elaboration on prediction markets that preserves differential privacy of transactions. This addendum gives a simpler, prediction-market focused construction and proofs. It also shows how to recover forms of a bounded worst-case loss guarantee by introducing a transaction fee.|['Bo Waggoner', 'Rafael Frongillo', 'Jacob Abernethy']|['cs.GT']
2017-03-16T23:31:44Z|2017-02-09T23:48:27Z|http://arxiv.org/abs/1702.03018v1|http://arxiv.org/pdf/1702.03018v1|Counterexamples to conjectures about Subset Takeaway and counting linear   extensions of a Boolean lattice|We study the game known as Subset Takeaway (Chomp on a hypercube), and give unexpected answers to questions of Gale and Neyman. We show that the number of linear extensions of the lattice of a 7-cube is 630470261306055898099742878692134361829979979674711225065761605059425237453564989302659882866111738567871048772795838071474370002961694720 (roughly $6.3 \cdot 10^{137}$).|['Andries E. Brouwer', 'J. Daniel Christensen']|['math.CO', 'cs.GT', '91A46, 06A07']
2017-03-16T23:31:44Z|2017-02-09T17:45:02Z|http://arxiv.org/abs/1702.02915v1|http://arxiv.org/pdf/1702.02915v1|Game-Theoretic Approaches to Energy Trading: A Survey|The global move towards producing and consuming energy in a clean, efficient and economical way, along with the need for meeting rising consumer demand has led to significant advancements in the design of the smart grid infrastructure. However, the potential of the smart grid remains limited without the integration of renewable energy sources. Energy trading is one way forward. This refers to the transfer of energy from an entity producing surplus energy to one with a deficit, within a certain timeframe. In this paper, we present a detailed review of the literature surrounding the application of game theoretic (GT) methods to scenarios in energy trading. Two levels (layers) of trading are identified and the latest achievements within them are summarised. An extensive description of a complete GT-based energy trading framework is presented, including a taxonomy of GT and an introduction to the smart grid architecture with a focus on renewable energy generation and energy storage. Finally, we present a critical evaluation of the current shortcomings and identify areas for future research.|['Matthias Pilz', 'Luluwah Al-Fagih']|['cs.GT']
2017-03-16T23:31:44Z|2017-02-09T06:40:10Z|http://arxiv.org/abs/1702.02722v1|http://arxiv.org/pdf/1702.02722v1|Mobile Data Trading: Behavioral Economics Analysis and Algorithm Design|Motivated by the recently launched mobile data trading markets (e.g., China Mobile Hong Kong's 2nd exChange Market), in this paper we study the mobile data trading problem under the future data demand uncertainty. We introduce a brokerage-based market, where sellers and buyers propose their selling and buying quantities, respectively, to the trading platform that matches the market supply and demand. To understand the users' realistic trading behaviors, a prospect theory (PT) model from behavioral economics is proposed, which includes the widely adopted expected utility theory (EUT) as a special case. Although the PT modeling leads to a challenging non-convex optimization problem, the optimal solution can be characterized by exploiting the unimodal structure of the objective function. Building upon our analysis, we design an algorithm to help estimate the user's risk preference and provide trading recommendations dynamically, considering the latest market and usage information. It is shown in our simulation that the risk preferences have a significant impact on the user's decision and outcome: a risk-averse dominant user can guarantee a higher minimum profit in the trading, while a risk-seeking dominant user can achieve a higher maximum profit. By comparing with the EUT benchmark, it is shown that a PT user with a low reference point is more willing to buy mobile data. Moreover, when the probability of high future data demand is low, a PT user is more willing to buy mobile data due to the probability distortion comparing with an EUT user.|['Junlin Yu', 'Man Hon Cheung', 'Jianwei Huang', 'H. Vincent Poor']|['cs.GT']
2017-03-16T23:31:49Z|2017-02-09T01:35:31Z|http://arxiv.org/abs/1702.04299v1|http://arxiv.org/pdf/1702.04299v1|Cyclic Dominance in the Spatial Coevolutionary Optional Prisoner's   Dilemma Game|This paper studies scenarios of cyclic dominance in a coevolutionary spatial model in which game strategies and links between agents adaptively evolve over time. The Optional Prisoner's Dilemma (OPD) game is employed. The OPD is an extended version of the traditional Prisoner's Dilemma where players have a third option to abstain from playing the game. We adopt an agent-based simulation approach and use Monte Carlo methods to perform the OPD with coevolutionary rules. The necessary conditions to break the scenarios of cyclic dominance are also investigated. This work highlights that cyclic dominance is essential in the sustenance of biodiversity. Moreover, we also discuss the importance of a spatial coevolutionary model in maintaining cyclic dominance in adverse conditions.|"['Marcos Cardinot', 'Josephine Griffith', ""Colm O'Riordan""]"|['cs.GT', 'cs.MA', 'math.DS', 'physics.soc-ph']
2017-03-16T23:31:49Z|2017-02-07T17:38:34Z|http://arxiv.org/abs/1702.02113v1|http://arxiv.org/pdf/1702.02113v1|Rare Nash Equilibria and the Price of Anarchy in Large Static Games|We study a static game played by a finite number of agents, in which agents are assigned independent and identically distributed random types and each agent minimizes its objective function by choosing from a set of admissible actions that depends on its type. The game is anonymous in the sense that the objective function of each agent depends on the actions of other agents only through the empirical distribution of their type-action pairs. We study the asymptotic behavior of Nash equilibria, as the number of agents tends to infinity, first by deriving laws of large numbers characterizes almost sure limit points of Nash equilibria in terms of so-called Cournot-Nash equilibria of an associated nonatomic game. Our main results are large deviation principles that characterize the probability of rare Nash equilibria and associated conditional limit theorems describing the behavior of equilibria conditioned on a rare event. The results cover situations when neither the finite-player game nor the associated nonatomic game has a unique equilibrium. In addition, we study the asymptotic behavior of the price of anarchy, complementing existing worst-case bounds with new probabilistic bounds in the context of congestion games, which are used to model traffic routing in networks.|['Daniel Lacker', 'Kavita Ramanan']|['math.PR', 'cs.GT', 'math.OC']
2017-03-16T23:31:49Z|2017-02-23T13:40:57Z|http://arxiv.org/abs/1702.01953v3|http://arxiv.org/pdf/1702.01953v3|A short proof of correctness of the quasi-polynomial time algorithm for   parity games|Recently Cristian S. Calude, Sanjay Jain, Bakhadyr Khoussainov, Wei Li and Frank Stephan proposed a quasi-polynomial time algorithm for parity games. This paper proposes a short proof of correctness of their algorithm.|['Hugo Gimbert', 'Rasmus Ibsen-Jensen']|['cs.FL', 'cs.GT']
2017-03-16T23:31:49Z|2017-02-06T21:53:09Z|http://arxiv.org/abs/1702.01803v1|http://arxiv.org/pdf/1702.01803v1|A General Framework for Evaluating Callout Mechanisms in Repeated   Auctions|Motivated by online display ad exchanges, we study a setting in which an exchange repeatedly interacts with bidders who have quota, making decisions about which subsets of bidders are called to participate in ad-slot-specific auctions. A bidder with quota cannot respond to more than a certain number of calls per second. In practice, random throttling is the principal solution by which these constraints are enforced. Given the repeated nature of the interaction with its bidders, the exchange has access to data containing information about each bidder's segments of interest. This information can be utilized to design smarter callout mechanisms --- with the potential of improving the exchange's long-term revenue. In this work, we present a general framework for evaluating and comparing the performance of various callout mechanisms using historical auction data only. To measure the impact of a callout mechanism on long-term revenue, we propose a strategic model that captures the repeated interaction between the exchange and bidders. Our model leads us to two metrics for performance: immediate revenue impact and social welfare. Next we present an empirical framework for estimating these two metrics from historical data. For the baseline to compare against, we consider random throttling, as well as a greedy algorithm with certain theoretical guarantees. We propose several natural callout mechanisms and investigate them through our framework on both synthetic and real auction data. We characterize the conditions under which each heuristic performs well and show that, in addition to being computationally faster, in practice our heuristics consistently and significantly outperform the baselines.|['Hossein Azari', 'William D. Heavlin', 'Hoda Heidari', 'Max Lin', 'Sonia Todorova']|['cs.GT']
2017-03-16T23:31:49Z|2017-02-05T15:40:12Z|http://arxiv.org/abs/1702.01416v1|http://arxiv.org/pdf/1702.01416v1|From Bayesian to Crowdsourced Bayesian Auctions|"A strong assumption in Bayesian mechanism design is that the distributions of the players' private types are common knowledge to the designer and the players--the common prior assumption. An important problem that has received a lot of attention in both economics and computer science is to repeatedly weaken this assumption in game theory--the ""Wilson's Doctrine"". In this work we consider, for the first time in the literature, multi-item auctions where the knowledge about the players' value distributions is scattered among the players and the seller. Each one of them privately knows some or none of the value distributions, no constraint is imposed on who knows which distributions, and the seller does not know who knows what. In such an unstructured information setting, we design mechanisms for unit-demand and additive auctions, whose expected revenue approximates that of the optimal Bayesian mechanisms by ""crowdsourcing"" the players' and the seller's knowledge. Our mechanisms are 2-step dominant-strategy truthful and the revenue increases gracefully with the amount of knowledge the players have. In particular, the revenue starts from a constant fraction of the revenue of the best known dominant-strategy truthful Bayesian mechanisms, and approaches 100 percent of the later when the amount of knowledge increases. Our results greatly improve the literature on the relationship between the amount of knowledge in the system and what mechanism design can achieve. In some sense, our results show that the common prior assumption is without much loss of generality in Bayesian auctions if one is willing to give up a fraction of the revenue."|['Jing Chen', 'Bo Li', 'Yingkai Li']|['cs.GT']
2017-03-16T23:31:49Z|2017-02-02T17:37:44Z|http://arxiv.org/abs/1702.01017v1|http://arxiv.org/pdf/1702.01017v1|Emergence of Distributed Coordination in the Kolkata Paise Restaurant   Problem with Finite Information|In this paper, we study a large-scale distributed coordination problem and propose efficient adaptive strategies to solve the problem. The basic problem is to allocate finite number of resources to individual agents such that there is as little congestion as possible and the fraction of unutilized resources is reduced as far as possible. In the absence of a central planner and global information, agents can employ adaptive strategies that uses only finite knowledge about the competitors. In this paper, we show that a combination of finite information sets and reinforcement learning can increase the utilization rate of resources substantially.|['Diptesh Ghosh', 'Anindya S. Chakrabarti']|['cs.GT', 'q-fin.EC']
2017-03-16T23:31:49Z|2017-02-02T11:07:53Z|http://arxiv.org/abs/1702.00616v1|http://arxiv.org/pdf/1702.00616v1|Competitive division of a mixed manna|A mixed manna contains goods (that everyone likes), bads (that everyone dislikes), as well as items that are goods to some agents, but bads or satiated to others.   If all items are goods and utility functions are homothetic, concave (and monotone), the Competitive Equilibrium with Equal Incomes maximizes the Nash product of utilities: hence it is welfarist (determined utility-wise by the feasible set of profiles), single-valued and easy to compute.   We generalize the Gale-Eisenberg Theorem to a mixed manna. The Competitive division is still welfarist and related to the product of utilities or disutilities. If the zero utility profile (before any manna) is Pareto dominated, the competitive profile is unique and still maximizes the product of utilities. If the zero profile is unfeasible, the competitive profiles are the critical points of the product of disutilities on the efficiency frontier, and multiplicity is pervasive. In particular the task of dividing a mixed manna is either good news for everyone, or bad news for everyone.   We refine our results in the practically important case of linear preferences, where the axiomatic comparison between the division of goods and that of bads is especially sharp. When we divide goods and the manna improves, everyone weakly benefits under the competitive rule; but no reasonable rule to divide bads can be similarly Resource Monotonic. Also, the much larger set of Non Envious and Efficient divisions of bads can be disconnected so that it will admit no continuous selection.|['Anna Bogomolnaia', 'Herve Moulin', 'Fedor Sandomirskiy', 'Elena Yanovskaya']|['cs.GT', 'math.OC', '91B32, 91B50, 52A41']
2017-03-16T23:31:49Z|2017-01-31T02:45:57Z|http://arxiv.org/abs/1701.08896v1|http://arxiv.org/pdf/1701.08896v1|On the Role of a Market Maker in Networked Cournot Competition|We study Cournot competition among firms in a networked marketplace that is centrally managed by a market maker. In particular, we study a situation in which a market maker facilitates trade between geographically separate markets via a constrained transport network. Our focus is on understanding the consequences of the design of the market maker and on providing tools for optimal design. To that end we provide a characterisation of the equilibrium outcomes of the game between firms and the market maker. Our results highlight that the equilibrium structure is impacted dramatically by the market maker objective - depending on the objective there may be a unique equilibrium, multiple equilibria, or no equilibria. Further, the game may be a potential game (as in the case of classical Cournot competition) or not. Beyond characterizing the equilibria of the game, we provide an approach for designing the market maker in order to optimize a design objective (e.g., social welfare) at the equilibrium of the game. Additionally, we use our results to explore the value of transport (trade) and the efficiency of the market maker (as compared to a single, aggregate market).|['Desmond Cai', 'Subhonmesh Bose', 'Adam Wierman']|['cs.GT']
2017-03-16T23:31:49Z|2017-01-30T15:18:03Z|http://arxiv.org/abs/1701.08644v1|http://arxiv.org/pdf/1701.08644v1|Security Game with Non-additive Utilities and Multiple Attacker   Resources|There has been significant interest in studying security games for modeling the interplay of attacks and defenses on various systems involving critical infrastructure, financial system security, political campaigns, and civil safeguarding. However, existing security game models typically either assume additive utility functions, or that the attacker can attack only one target. Such assumptions lead to tractable analysis, but miss key inherent dependencies that exist among different targets in current complex networks. In this paper, we generalize the classical security game models to allow for non-additive utility functions. We also allow attackers to be able to attack multiple targets. We examine such a general security game from a theoretical perspective and provide a unified view. In particular, we show that each security game is equivalent to a combinatorial optimization problem over a set system $\varepsilon$, which consists of defender's pure strategy space. The key technique we use is based on the transformation, projection of a polytope, and the elipsoid method. This work settles several open questions in security game domain and significantly extends the state of-the-art of both the polynomial solvable and NP-hard class of the security game.|['Sinong Wang', 'Ness Shroff']|['cs.GT', 'cs.CR']
2017-03-16T23:31:49Z|2017-01-30T12:58:00Z|http://arxiv.org/abs/1701.08573v1|http://arxiv.org/pdf/1701.08573v1|Debunking van Enk-Pike's criterion for quantum games|S. J. van Enk and R. Pike in PRA 66, 024306 (2002), argue that in a quantum game the payoff's obtained are not better than the payoffs obtained in classical games, hence they conclude that the quantum game does not solve the classical game. In this work, we debunk this criterion by showing that a random strategy in a particular quantum (Hawk-Dove) game can give us a better payoff than what is achievable in a classical game. In fact, for a particular pure strategy we get the maximum payoff. Moreover, we provide an analytical solution to the quantum $2\times2$ strategic form Hawk-Dove game using random mixed strategies. The random strategies which we describe are evolutionary stable implying both Pareto optimality and Nash equilibrium.|['Nilesh Vyas', 'Colin Benjamin']|['quant-ph', 'cond-mat.other', 'cs.GT', 'math-ph', 'math.MP']
2017-03-16T23:31:54Z|2017-01-27T19:08:40Z|http://arxiv.org/abs/1702.02090v1|http://arxiv.org/pdf/1702.02090v1|A Bayesian Game without epsilon equilibria|We present a three player Bayesian game for which there is no epsilon equilibria in Borel measurable strategies for small enough epsilon, however there are non-measurable equilibria.|['Robert Samuel Simon', 'Grzegorz Tomkowicz']|['cs.GT', 'math.PR', '91A60']
2017-03-16T23:31:54Z|2017-01-27T16:37:45Z|http://arxiv.org/abs/1701.08108v1|http://arxiv.org/pdf/1701.08108v1|Existence of Evolutionarily Stable Strategies Remains Hard to Decide for   a Wide Range of Payoff Values|"The concept of an evolutionarily stable strategy (ESS), introduced by Smith and Price, is a refinement of Nash equilibrium in 2-player symmetric games in order to explain counter-intuitive natural phenomena, whose existence is not guaranteed in every game. The problem of deciding whether a game possesses an ESS has been shown to be $\Sigma_{2}^{P}$-complete by Conitzer using the preceding important work by Etessami and Lochbihler. The latter, among other results, proved that deciding the existence of ESS is both NP-hard and coNP-hard. In this paper we introduce a ""reduction robustness"" notion and we show that deciding the existence of an ESS remains coNP-hard for a wide range of games even if we arbitrarily perturb within some intervals the payoff values of the game under consideration. In contrast, ESS exist almost surely for large games with random and independent payoffs chosen from the same distribution."|['Themistoklis Melissourgos', 'Paul Spirakis']|['cs.CC', 'cs.GT', '68Q01']
2017-03-16T23:31:54Z|2017-01-27T14:15:12Z|http://arxiv.org/abs/1701.08058v1|http://arxiv.org/pdf/1701.08058v1|Optimal Communication Strategies in Networked Cyber-Physical Systems   with Adversarial Elements|"This paper studies optimal communication and coordination strategies in cyber-physical systems for both defender and attacker within a game-theoretic framework. We model the communication network of a cyber-physical system as a sensor network which involves one single Gaussian source observed by many sensors, subject to additive independent Gaussian observation noises. The sensors communicate with the estimator over a coherent Gaussian multiple access channel. The aim of the receiver is to reconstruct the underlying source with minimum mean squared error. The scenario of interest here is one where some of the sensors are captured by the attacker and they act as the adversary (jammer): they strive to maximize distortion. The receiver (estimator) knows the captured sensors but still cannot simply ignore them due to the multiple access channel, i.e., the outputs of all sensors are summed to generate the estimator input. We show that the ability of transmitter sensors to secretly agree on a random event, that is ""coordination"", plays a key role in the analysis..."|['Emrah Akyol', 'Kenneth Rose', 'Tamer Basar', 'Cedric Langbort']|['cs.GT', 'cs.CR', 'cs.IT', 'cs.MA', 'math.IT']
2017-03-16T23:31:54Z|2017-01-27T12:11:12Z|http://arxiv.org/abs/1701.08023v1|http://arxiv.org/pdf/1701.08023v1|The Condorcet Principle for Multiwinner Elections: From Shortlisting to   Proportionality|We study two notions of stability in multiwinner elections that are based on the Condorcet criterion. The first notion was introduced by Gehrlein: A committee is stable if each committee member is preferred to each non-member by a (possibly weak) majority of voters. The second notion is called local stability (introduced in this paper): A size-$k$ committee is locally stable in an election with $n$ voters if there is no candidate $c$ and no group of more than $\frac{n}{k+1}$ voters such that each voter in this group prefers $c$ to each committee member. We argue that Gehrlein-stable committees are appropriate for shortlisting tasks, and that locally stable committees are better suited for applications that require proportional representation. The goal of this paper is to analyze these notions in detail, explore their compatibility with notions of proportionality, and investigate the computational complexity of related algorithmic tasks.|['Haris Aziz', 'Edith Elkind', 'Piotr Faliszewski', 'Martin Lackner', 'Piotr Skowron']|['cs.GT']
2017-03-16T23:31:54Z|2017-01-27T10:02:13Z|http://arxiv.org/abs/1701.07991v1|http://arxiv.org/pdf/1701.07991v1|A Mood Value for Fair Resource Allocations|In networking and computing, resource allocation is typically addressed using classical sharing protocols as, for instance, the proportional division rule, the max-min fair allocation , or other solutions inspired by cooperative game theory. In this paper, we argue that, describing the resource allocation problem as a cooperative game, such classical resource allocation approaches, as well as associated notions of fairness, show important limitations. We identify in the individual satisfaction rate the key aspect of the challenge of defining a new notion of fairness and, consequently, a resource allocation algorithm more appropriate for the cooperative context. We generalize the concept of user satisfaction considering the set of admissible solutions for bankruptcy games. We adapt the Jain's fairness index to include the new user satisfaction rate. Accordingly, we propose a new allocation rule we call 'Mood Value'. For each user it equalizes our novel game-theoretic definition of user satisfaction with respect to a distribution of the resource. We test the mood value and the new fairness index through extensive simulations showing how they better support the fairness analysis.|['Francesca Fossati', 'Stefano Moretti', 'Stefano Secci']|['cs.NI', 'cs.GT']
2017-03-16T23:31:54Z|2017-01-27T06:47:02Z|http://arxiv.org/abs/1701.07956v1|http://arxiv.org/pdf/1701.07956v1|Simple approximate equilibria in games with many players|We consider $\epsilon$-equilibria notions for constant value of $\epsilon$ in $n$-player $m$-actions games where $m$ is a constant. We focus on the following question: What is the largest grid size over the mixed strategies such that $\epsilon$-equilibrium is guaranteed to exist over this grid.   For Nash equilibrium, we prove that constant grid size (that depends on $\epsilon$ and $m$, but not on $n$) is sufficient to guarantee existence of weak approximate equilibrium. This result implies a polynomial (in the input) algorithm for weak approximate equilibrium.   For approximate Nash equilibrium we introduce a closely related question and prove its \emph{equivalence} to the well-known Beck-Fiala conjecture from discrepancy theory. To the best of our knowledge this is the first result introduces a connection between game theory and discrepancy theory.   For correlated equilibrium, we prove a $O(\frac{1}{\log n})$ lower-bound on the grid size, which matches the known upper bound of $\Omega(\frac{1}{\log n})$. Our result implies an $\Omega(\log n)$ lower bound on the rate of convergence of dynamics (any dynamic) to approximate correlated (and coarse correlated) equilibrium. Again, this lower bound matches the $O(\log n)$ upper bound that is achieved by regret minimizing algorithms.|['Itai Arieli', 'Yakov Babichenko']|['cs.GT']
2017-03-16T23:31:54Z|2017-01-26T08:33:38Z|http://arxiv.org/abs/1701.07614v1|http://arxiv.org/pdf/1701.07614v1|Tight Inefficiency Bounds for Perception-Parameterized Affine Congestion   Games|"Congestion games constitute an important class of non-cooperative games which was introduced by Rosenthal in 1973. In recent years, several extensions of these games were proposed to incorporate aspects that are not captured by the standard model. Examples of such extensions include the incorporation of risk sensitive players, the modeling of altruistic player behavior and the imposition of taxes on the resources. These extensions were studied intensively with the goal to obtain a precise understanding of the inefficiency of equilibria of these games. In this paper, we introduce a new model of congestion games that captures these extensions (and additional ones) in a unifying way. The key idea here is to parameterize both the perceived cost of each player and the social cost function of the system designer. Intuitively, each player perceives the load induced by the other players by an extent of {\rho}, while the system designer estimates that each player perceives the load of all others by an extent of {\sigma}. The above mentioned extensions reduce to special cases of our model by choosing the parameters {\rho} and {\sigma} accordingly. As in most related works, we concentrate on congestion games with affine latency functions here. Despite the fact that we deal with a more general class of congestion games, we manage to derive tight bounds on the price of anarchy and the price of stability for a large range of pa- rameters. Our bounds provide a complete picture of the inefficiency of equilibria for these perception-parameterized congestion games. As a result, we obtain tight bounds on the price of anarchy and the price of stability for the above mentioned extensions. Our results also reveal how one should ""design"" the cost functions of the players in order to reduce the price of anar- chy."|['Pieter Kleer', 'Guido Schäfer']|['cs.GT']
2017-03-16T23:31:54Z|2017-01-25T18:44:48Z|http://arxiv.org/abs/1701.07419v1|http://arxiv.org/abs/1701.07419v1|A gentle introduction to the minimal Naming Game|Social conventions govern countless behaviors all of us engage in every day, from how we greet each other to the languages we speak. But how can shared conventions emerge spontaneously in the absence of a central coordinating authority? The Naming Game model shows that networks of locally interacting individuals can spontaneously self-organize to produce global coordination. Here, we provide a gentle introduction to the main features of the model, from the dynamics observed in homogeneously mixing populations to the role played by more complex social networks, and to how slight modifications of the basic interaction rules give origin to a richer phenomenology in which more conventions can co-exist indefinitely.|['Andrea Baronchelli']|['physics.soc-ph', 'cs.GT', 'cs.MA', 'q-bio.PE']
2017-03-16T23:31:54Z|2017-01-25T13:35:51Z|http://arxiv.org/abs/1701.07304v1|http://arxiv.org/pdf/1701.07304v1|Congestion Games with Complementarities|We study a model of selfish resource allocation that seeks to incorporate dependencies among resources as they exist in modern networked environments. Our model is inspired by utility functions with constant elasticity of substitution (CES) which is a well-studied model in economics. We consider congestion games with different aggregation functions. In particular, we study $L_p$ norms and analyze the existence and complexity of (approximate) pure Nash equilibria. Additionally, we give an almost tight characterization based on monotonicity properties to describe the set of aggregation functions that guarantee the existence of pure Nash equilibria.|['Matthias Feldotto', 'Lennart Leder', 'Alexander Skopalik']|['cs.GT']
2017-03-16T23:31:54Z|2017-01-24T22:28:19Z|http://arxiv.org/abs/1701.07096v1|http://arxiv.org/abs/1701.07096v1|A new quantum scheme for normal-form games|We give a strict mathematical description for a refinement of the Marinatto-Weber quantum game scheme. The model allows the players to choose projector operators that determine the state on which they perform their local operators. The game induced by the scheme generalizes finite strategic form game. In particular, it covers normal representations of extensive games, i.e., strategic games generated by extensive ones. We illustrate our idea with an example of extensive game and prove that rational choices in the classical game and its quantum counterpart may lead to significantly different outcomes.|['Piotr Frąckiewicz']|['cs.GT']
