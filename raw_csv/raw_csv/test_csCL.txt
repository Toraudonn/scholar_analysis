2017-03-16T23:29:09Z|2017-03-15T17:01:20Z|http://arxiv.org/abs/1703.05260v1|http://arxiv.org/pdf/1703.05260v1|InScript: Narrative texts annotated with script information|This paper presents the InScript corpus (Narrative Texts Instantiating Script structure). InScript is a corpus of 1,000 stories centered around 10 different scenarios. Verbs and noun phrases are annotated with event and participant types, respectively. Additionally, the text is annotated with coreference information. The corpus shows rich lexical variation and will serve as a unique resource for the study of the role of script knowledge in natural language processing.|['Ashutosh Modi', 'Tatjana Anikina', 'Simon Ostermann', 'Manfred Pinkal']|['cs.CL', 'cs.AI']
2017-03-16T23:29:09Z|2017-03-15T12:37:22Z|http://arxiv.org/abs/1703.05123v1|http://arxiv.org/pdf/1703.05123v1|Character-based Neural Embeddings for Tweet Clustering|In this paper we show how the performance of tweet clustering can be improved by leveraging character-based neural networks. The proposed approach overcomes the limitations related to the vocabulary explosion in the word-based models and allows for the seamless processing of the multilingual content. Our evaluation results and code are available on-line at https://github.com/vendi12/tweet2vec_ clustering|['Svitlana Vakulenko', 'Lyndon Nixon', 'Mihai Lupu']|['cs.IR', 'cs.CL']
2017-03-16T23:29:09Z|2017-03-15T12:32:34Z|http://arxiv.org/abs/1703.05122v1|http://arxiv.org/pdf/1703.05122v1|Is this word borrowed? An automatic approach to quantify the likeliness   of borrowing in social media|Code-mixing or code-switching are the effortless phenomena of natural switching between two or more languages in a single conversation. Use of a foreign word in a language; however, does not necessarily mean that the speaker is code-switching because often languages borrow lexical items from other languages. If a word is borrowed, it becomes a part of the lexicon of a language; whereas, during code-switching, the speaker is aware that the conversation involves foreign words or phrases. Identifying whether a foreign word used by a bilingual speaker is due to borrowing or code-switching is a fundamental importance to theories of multilingualism, and an essential prerequisite towards the development of language and speech technologies for multilingual communities. In this paper, we present a series of novel computational methods to identify the borrowed likeliness of a word, based on the social media signals. We first propose context based clustering method to sample a set of candidate words from the social media data.Next, we propose three novel and similar metrics based on the usage of these words by the users in different tweets; these metrics were used to score and rank the candidate words indicating their borrowed likeliness. We compare these rankings with a ground truth ranking constructed through a human judgment experiment. The Spearman's rank correlation between the two rankings (nearly 0.62 for all the three metric variants) is more than double the value (0.26) of the most competitive existing baseline reported in the literature. Some other striking observations are, (i) the correlation is higher for the ground truth data elicited from the younger participants (age less than 30) than that from the older participants, and (ii )those participants who use mixed-language for tweeting the least, provide the best signals of borrowing.|['Jasabanta Patro', 'Bidisha Samanta', 'Saurabh Singh', 'Prithwish Mukherjee', 'Monojit Choudhury', 'Animesh Mukherjee']|['cs.CL']
2017-03-16T23:29:09Z|2017-03-15T04:57:17Z|http://arxiv.org/abs/1703.04929v1|http://arxiv.org/pdf/1703.04929v1|SyntaxNet Models for the CoNLL 2017 Shared Task|"We describe a baseline dependency parsing system for the CoNLL2017 Shared Task. This system, which we call ""ParseySaurus,"" uses the DRAGNN framework [Kong et al, 2017] to combine transition-based recurrent parsing and tagging with character-based word representations. On the v1.3 Universal Dependencies Treebanks, the new system outpeforms the publicly available, state-of-the-art ""Parsey's Cousins"" models by 3.47% absolute Labeled Accuracy Score (LAS) across 52 treebanks."|['Chris Alberti', 'Daniel Andor', 'Ivan Bogatyy', 'Michael Collins', 'Dan Gillick', 'Lingpeng Kong', 'Terry Koo', 'Ji Ma', 'Mark Omernick', 'Slav Petrov', 'Chayut Thanapirom', 'Zora Tung', 'David Weiss']|['cs.CL']
2017-03-16T23:29:09Z|2017-03-15T04:00:27Z|http://arxiv.org/abs/1703.04914v1|http://arxiv.org/pdf/1703.04914v1|Ensemble of Neural Classifiers for Scoring Knowledge Base Triples|This paper describes our approach for the triple scoring task at WSDM Cup 2017. The task aims to assign a relevance score for each pair of entities and their types in a knowledge base in order to enhance the ranking results in entity retrieval tasks. We propose an approach wherein the outputs of multiple neural network classifiers are combined using a supervised machine learning model. The experimental results show that our proposed method achieves the best performance in one out of three measures, and performs competitively in the other two measures.|['Ikuya Yamada', 'Motoki Sato', 'Hiroyuki Shindo']|['cs.CL', 'cs.IR']
2017-03-16T23:29:09Z|2017-03-15T03:30:13Z|http://arxiv.org/abs/1703.04908v1|http://arxiv.org/pdf/1703.04908v1|Emergence of Grounded Compositional Language in Multi-Agent Populations|By capturing statistical patterns in large corpora, machine learning has enabled significant advances in natural language processing, including in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply capturing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. Towards this end, we propose a multi-agent learning environment and learning methods that bring about emergence of a basic compositional language. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of non-verbal communication such as pointing and guiding when language communication is unavailable.|['Igor Mordatch', 'Pieter Abbeel']|['cs.AI', 'cs.CL']
2017-03-16T23:29:09Z|2017-03-15T02:26:25Z|http://arxiv.org/abs/1703.04887v1|http://arxiv.org/pdf/1703.04887v1|Improving Neural Machine Translation with Conditional Sequence   Generative Adversarial Nets|This paper proposes a new route for applying the generative adversarial nets (GANs) to NLP tasks (taking the neural machine translation as an instance) and the widespread perspective that GANs can't work well in the NLP area turns out to be unreasonable. In this work, we build a conditional sequence generative adversarial net which comprises of two adversarial sub models, a generative model (generator) which translates the source sentence into the target sentence as the traditional NMT models do and a discriminative model (discriminator) which discriminates the machine-translated target sentence from the human-translated sentence. From the perspective of Turing test, the proposed model is to generate the translation which is indistinguishable from the human-translated one. Experiments show that the proposed model achieves significant improvements than the traditional NMT model. In Chinese-English translation tasks, we obtain up to +2.0 BLEU points improvement. To the best of our knowledge, this is the first time that the quantitative results about the application of GANs in the traditional NLP task is reported. Meanwhile, we present detailed strategies for GAN training. In addition, We find that the discriminator of the proposed model shows great capability in data cleaning.|['Zhen Yang', 'Wei Chen', 'Feng Wang', 'Bo Xu']|['cs.CL']
2017-03-16T23:29:09Z|2017-03-15T01:54:52Z|http://arxiv.org/abs/1703.04879v1|http://arxiv.org/pdf/1703.04879v1|Sparse Named Entity Classification using Factorization Machines|Named entity classification is the task of classifying text-based elements into various categories, including places, names, dates, times, and monetary values. A bottleneck in named entity classification, however, is the data problem of sparseness, because new named entities continually emerge, making it rather difficult to maintain a dictionary for named entity classification. Thus, in this paper, we address the problem of named entity classification using matrix factorization to overcome the problem of feature sparsity. Experimental results show that our proposed model, with fewer features and a smaller size, achieves competitive accuracy to state-of-the-art models.|['Ai Hirata', 'Mamoru Komachi']|['cs.CL']
2017-03-16T23:29:09Z|2017-03-15T00:47:28Z|http://arxiv.org/abs/1703.04854v1|http://arxiv.org/pdf/1703.04854v1|Distributed-Representation Based Hybrid Recommender System with Short   Item Descriptions|"Collaborative filtering (CF) aims to build a model from users' past behaviors and/or similar decisions made by other users, and use the model to recommend items for users. Despite of the success of previous collaborative filtering approaches, they are all based on the assumption that there are sufficient rating scores available for building high-quality recommendation models. In real world applications, however, it is often difficult to collect sufficient rating scores, especially when new items are introduced into the system, which makes the recommendation task challenging. We find that there are often ""short"" texts describing features of items, based on which we can approximate the similarity of items and make recommendation together with rating scores. In this paper we ""borrow"" the idea of vector representation of words to capture the information of short texts and embed it into a matrix factorization framework. We empirically show that our approach is effective by comparing it with state-of-the-art approaches."|['Junhua He', 'Hankz Hankui Zhuo', 'Jarvan Law']|['cs.IR', 'cs.CL']
2017-03-16T23:29:09Z|2017-03-14T23:25:34Z|http://arxiv.org/abs/1703.04826v1|http://arxiv.org/pdf/1703.04826v1|Encoding Sentences with Graph Convolutional Networks for Semantic Role   Labeling|Semantic role labeling (SRL) is the task of identifying the predicate-argument structure of a sentence. It is typically regarded as an important step in the standard natural language processing pipeline, providing information to downstream tasks such as information extraction and question answering. As the semantic representations are closely related to syntactic ones, we exploit syntactic information in our model. We propose a version of graph convolutional networks (GCNs), a recent class of multilayer neural networks operating on graphs, suited to modeling syntactic dependency graphs. GCNs over syntactic dependency trees are used as sentence encoders, producing latent feature representations of words in a sentence and capturing information relevant to predicting the semantic representations. We observe that GCN layers are complementary to LSTM ones: when we stack both GCN and LSTM layers, we obtain a substantial improvement over an already state-of-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009) both for Chinese and English.|['Diego Marcheggiani', 'Ivan Titov']|['cs.CL', 'cs.LG']
2017-03-16T23:29:13Z|2017-03-14T23:09:45Z|http://arxiv.org/abs/1703.04816v1|http://arxiv.org/pdf/1703.04816v1|FastQA: A Simple and Efficient Neural Architecture for Question   Answering|Recent development of large-scale question answering (QA) datasets triggered a substantial amount of research into end-to-end neural architectures for QA. Increasingly complex systems have been conceived without comparison to a simpler neural baseline system that would justify their complexity. In this work, we propose a simple heuristic that guided the development of FastQA, an efficient end-to-end neural model for question answering that is very competitive with existing models. We further demonstrate, that an extended version (FastQAExt) achieves state-of-the-art results on recent benchmark datasets, namely SQuAD, NewsQA and MsMARCO, outperforming most existing models. However, we show that increasing the complexity of FastQA to FastQAExt does not yield any systematic improvements. We argue that the same holds true for most existing systems that are similar to FastQAExt. A manual analysis reveals that our proposed heuristic explains most predictions of our model, which indicates that modeling a simple heuristic is enough to achieve strong performance on extractive QA datasets. The overall strong performance of FastQA puts results of existing, more complex models into perspective.|['Dirk Weissenborn', 'Georg Wiese', 'Laura Seiffe']|['cs.CL', 'cs.AI', 'cs.NE']
2017-03-16T23:29:13Z|2017-03-14T22:28:51Z|http://arxiv.org/abs/1703.04783v1|http://arxiv.org/pdf/1703.04783v1|Multichannel End-to-end Speech Recognition|The field of speech recognition is in the midst of a paradigm shift: end-to-end neural networks are challenging the dominance of hidden Markov models as a core technology. Using an attention mechanism in a recurrent encoder-decoder architecture solves the dynamic time alignment problem, allowing joint end-to-end training of the acoustic and language modeling components. In this paper we extend the end-to-end framework to encompass microphone array signal processing for noise suppression and speech enhancement within the acoustic encoding network. This allows the beamforming components to be optimized jointly within the recognition architecture to improve the end-to-end speech recognition objective. Experiments on the noisy speech benchmarks (CHiME-4 and AMI) show that our multichannel end-to-end system outperformed the attention-based baseline with input from a conventional adaptive beamformer.|['Tsubasa Ochiai', 'Shinji Watanabe', 'Takaaki Hori', 'John R. Hershey']|['cs.SD', 'cs.CL']
2017-03-16T23:29:13Z|2017-03-14T19:14:32Z|http://arxiv.org/abs/1703.04677v1|http://arxiv.org/pdf/1703.04677v1|A computational investigation of sources of variability in sentence   comprehension difficulty in aphasia|We present a computational evaluation of three hypotheses about sources of deficit in sentence comprehension in aphasia: slowed processing, intermittent deficiency, and resource reduction. The ACT-R based Lewis & Vasishth 2005 model is used to implement these three proposals. Slowed processing is implemented as slowed default production-rule firing time; intermittent deficiency as increased random noise in activation of chunks in memory; and resource reduction as reduced goal activation. As data, we considered subject vs. object relatives presented in a self-paced listening modality to 56 individuals with aphasia (IWA) and 46 matched controls. The participants heard the sentences and carried out a picture verification task to decide on an interpretation of the sentence. These response accuracies are used to identify the best parameters (for each participant) that correspond to the three hypotheses mentioned above. We show that controls have more tightly clustered (less variable) parameter values than IWA; specifically, compared to controls, among IWA there are more individuals with low goal activations, high noise, and slow default action times. This suggests that (i) individual patients show differential amounts of deficit along the three dimensions of slowed processing, intermittent deficient, and resource reduction, (ii) overall, there is evidence for all three sources of deficit playing a role, and (iii) IWA have a more variable range of parameter values than controls. In sum, this study contributes a proof of concept of a quantitative implementation of, and evidence for, these three accounts of comprehension deficits in aphasia.|['Paul Mätzig', 'Shravan Vasishth', 'Felix Engelmann', 'David Caplan']|['cs.CL', 'cs.AI']
2017-03-16T23:29:13Z|2017-03-14T18:26:12Z|http://arxiv.org/abs/1703.04650v1|http://arxiv.org/pdf/1703.04650v1|Joint Learning of Correlated Sequence Labelling Tasks Using   Bidirectional Recurrent Neural Networks|The stream of words produced by Automatic Speech Recognition (ASR) systems is devoid of any punctuations and formatting. Most natural language processing applications usually expect segmented and well-formatted texts as input, which is not available in ASR output. This paper proposes a novel technique of jointly modelling multiple correlated tasks such as punctuation and capitalization using bidirectional recurrent neural networks, which leads to improved performance for each of these tasks. This method can be extended for joint modelling of any other correlated multiple sequence labelling tasks.|['Vardaan Pahuja', 'Anirban Laha', 'Shachar Mirkin', 'Vikas Raykar', 'Lili Kotlerman', 'Guy Lev']|['cs.CL']
2017-03-16T23:29:13Z|2017-03-14T17:43:25Z|http://arxiv.org/abs/1703.04617v1|http://arxiv.org/pdf/1703.04617v1|Exploring Question Understanding and Adaptation in Neural-Network-Based   Question Answering|The last several years have seen intensive interest in exploring neural-network-based models for machine comprehension (MC) and question answering (QA). In this paper, we approach the problems by closely modelling questions in a neural network framework. We first introduce syntactic information to help encode questions. We then view and model different types of questions and the information shared among them as an adaptation task and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results over a competitive baseline.|['Junbei Zhang', 'Xiaodan Zhu', 'Qian Chen', 'Lirong Dai', 'Hui Jiang']|['cs.CL']
2017-03-16T23:29:13Z|2017-03-13T17:34:18Z|http://arxiv.org/abs/1703.04498v1|http://arxiv.org/pdf/1703.04498v1|High-Throughput and Language-Agnostic Entity Disambiguation and Linking   on User Generated Data|The Entity Disambiguation and Linking (EDL) task matches entity mentions in text to a unique Knowledge Base (KB) identifier such as a Wikipedia or Freebase id. It plays a critical role in the construction of a high quality information network, and can be further leveraged for a variety of information retrieval and NLP tasks such as text categorization and document tagging. EDL is a complex and challenging problem due to ambiguity of the mentions and real world text being multi-lingual. Moreover, EDL systems need to have high throughput and should be lightweight in order to scale to large datasets and run on off-the-shelf machines. More importantly, these systems need to be able to extract and disambiguate dense annotations from the data in order to enable an Information Retrieval or Extraction task running on the data to be more efficient and accurate. In order to address all these challenges, we present the Lithium EDL system and algorithm - a high-throughput, lightweight, language-agnostic EDL system that extracts and correctly disambiguates 75% more entities than state-of-the-art EDL systems and is significantly faster than them.|['Preeti Bhargava', 'Nemanja Spasojevic', 'Guoning Hu']|['cs.IR', 'cs.AI', 'cs.CL']
2017-03-16T23:29:13Z|2017-03-13T17:13:51Z|http://arxiv.org/abs/1703.04489v1|http://arxiv.org/pdf/1703.04489v1|Reinforcement Learning for Transition-Based Mention Detection|This paper describes an application of reinforcement learning to the mention detection task. We define a novel action-based formulation for the mention detection task, in which a model can flexibly revise past labeling decisions by grouping together tokens and assigning partial mention labels. We devise a method to create mention-level episodes and we train a model by rewarding correctly labeled complete mentions, irrespective of the inner structure created. The model yields results which are on par with a competitive supervised counterpart while being more flexible in terms of achieving targeted behavior through reward modeling and generating internal mention structure, especially on longer mentions.|['Georgiana Dinu', 'Wael Hamza', 'Radu Florian']|['cs.CL', 'cs.AI']
2017-03-16T23:29:13Z|2017-03-13T16:50:36Z|http://arxiv.org/abs/1703.04481v1|http://arxiv.org/pdf/1703.04481v1|Geometrical morphology|We explore inflectional morphology as an example of the relationship of the discrete and the continuous in linguistics. The grammar requests a form of a lexeme by specifying a set of feature values, which corresponds to a corner M of a hypercube in feature value space. The morphology responds to that request by providing a morpheme, or a set of morphemes, whose vector sum is geometrically closest to the corner M. In short, the chosen morpheme $\mu$ is the morpheme (or set of morphemes) that maximizes the inner product of $\mu$ and M.|['John Goldsmith', 'Eric Rosen']|['cs.CL']
2017-03-16T23:29:13Z|2017-03-13T16:36:38Z|http://arxiv.org/abs/1703.04474v1|http://arxiv.org/pdf/1703.04474v1|DRAGNN: A Transition-based Framework for Dynamically Connected Neural   Networks|In this work, we present a compact, modular framework for constructing novel recurrent neural architectures. Our basic module is a new generic unit, the Transition Based Recurrent Unit (TBRU). In addition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and re-cursive tree-structured models. A TBRU can also serve as both an encoder for downstream tasks and as a decoder for its own task simultaneously, resulting in more accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks.|['Lingpeng Kong', 'Chris Alberti', 'Daniel Andor', 'Ivan Bogatyy', 'David Weiss']|['cs.CL']
2017-03-16T23:29:13Z|2017-03-13T14:34:23Z|http://arxiv.org/abs/1703.04417v1|http://arxiv.org/pdf/1703.04417v1|El Lenguaje Natural como Lenguaje Formal|"Formal languages theory is useful for the study of natural language. In particular, it is of interest to study the adequacy of the grammatical formalisms to express syntactic phenomena present in natural language. First, it helps to draw hypothesis about the nature and complexity of the speaker-hearer linguistic competence, a fundamental question in linguistics and other cognitive sciences. Moreover, from an engineering point of view, it allows the knowledge of practical limitations of applications based on those formalisms. In this article I introduce the adequacy problem of grammatical formalisms for natural language, also introducing some formal language theory concepts required for this discussion. Then, I review the formalisms that have been proposed in history, and the arguments that have been given to support or reject their adequacy.   -----   La teor\'ia de lenguajes formales es \'util para el estudio de los lenguajes naturales. En particular, resulta de inter\'es estudiar la adecuaci\'on de los formalismos gramaticales para expresar los fen\'omenos sint\'acticos presentes en el lenguaje natural. Primero, ayuda a trazar hip\'otesis acerca de la naturaleza y complejidad de las competencias ling\""u\'isticas de los hablantes-oyentes del lenguaje, un interrogante fundamental de la ling\""u\'istica y otras ciencias cognitivas. Adem\'as, desde el punto de vista de la ingenier\'ia, permite conocer limitaciones pr\'acticas de las aplicaciones basadas en dichos formalismos. En este art\'iculo hago una introducci\'on al problema de la adecuaci\'on de los formalismos gramaticales para el lenguaje natural, introduciendo tambi\'en algunos conceptos de la teor\'ia de lenguajes formales necesarios para esta discusi\'on. Luego, hago un repaso de los formalismos que han sido propuestos a lo largo de la historia, y de los argumentos que se han dado para sostener o refutar su adecuaci\'on."|['Franco M. Luque']|['cs.CL', 'cs.FL']
2017-03-16T23:29:16Z|2017-03-13T12:28:03Z|http://arxiv.org/abs/1703.04357v1|http://arxiv.org/pdf/1703.04357v1|Nematus: a Toolkit for Neural Machine Translation|We present Nematus, a toolkit for Neural Machine Translation. The toolkit prioritizes high translation accuracy, usability, and extensibility. Nematus has been used to build top-performing submissions to shared translation tasks at WMT and IWSLT, and has been used to train systems for production environments.|['Rico Sennrich', 'Orhan Firat', 'Kyunghyun Cho', 'Alexandra Birch', 'Barry Haddow', 'Julian Hitschler', 'Marcin Junczys-Dowmunt', 'Samuel Läubli', 'Antonio Valerio Miceli Barone', 'Jozef Mokry', 'Maria Nădejde']|['cs.CL']
2017-03-16T23:29:16Z|2017-03-13T11:19:56Z|http://arxiv.org/abs/1703.04336v1|http://arxiv.org/pdf/1703.04336v1|A Visual Representation of Wittgenstein's Tractatus Logico-Philosophicus|In this paper we present a data visualization method together with its potential usefulness in digital humanities and philosophy of language. We compile a multilingual parallel corpus from different versions of Wittgenstein's Tractatus Logico-Philosophicus, including the original in German and translations into English, Spanish, French, and Russian. Using this corpus, we compute a similarity measure between propositions and render a visual network of relations for different languages.|['Anca Bucur', 'Sergiu Nisioi']|['cs.IR', 'cs.CL']
2017-03-16T23:29:16Z|2017-03-13T11:03:40Z|http://arxiv.org/abs/1703.04330v1|http://arxiv.org/pdf/1703.04330v1|Story Cloze Ending Selection Baselines and Data Examination|This paper describes two supervised baseline systems for the Story Cloze Test Shared Task (Mostafazadeh et al., 2016a). We first build a classifier using features based on word embeddings and semantic similarity computation. We further implement a neural LSTM system with different encoding strategies that try to model the relation between the story and the provided endings. Our experiments show that a model using representation features based on average word embedding vectors over the given story words and the candidate ending sentences words, joint with similarity features between the story and candidate ending representations performed better than the neural models. Our best model achieves an accuracy of 72.42, ranking 3rd in the official evaluation.|['Todor Mihaylov', 'Anette Frank']|['cs.CL']
2017-03-16T23:29:16Z|2017-03-13T04:55:19Z|http://arxiv.org/abs/1703.04247v1|http://arxiv.org/pdf/1703.04247v1|DeepFM: A Factorization-Machine based Neural Network for CTR Prediction|"Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \& Deep model from Google, DeepFM has a shared input to its ""wide"" and ""deep"" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data."|['Huifeng Guo', 'Ruiming Tang', 'Yunming Ye', 'Zhenguo Li', 'Xiuqiang He']|['cs.IR', 'cs.CL']
2017-03-16T23:29:16Z|2017-03-14T20:26:32Z|http://arxiv.org/abs/1703.04213v2|http://arxiv.org/pdf/1703.04213v2|MetaPAD: Meta Pattern Discovery from Massive Text Corpora|Mining textual patterns in news, tweets, papers, and many other kinds of text corpora has been an active theme in text mining and NLP research. Previous studies adopt a dependency parsing-based pattern discovery approach. However, the parsing results lose rich context around entities in the patterns, and the process is costly for a corpus of large scale. In this study, we propose a novel typed textual pattern structure, called meta pattern, which is extended to a frequent, informative, and precise subsequence pattern in certain context. We propose an efficient framework, called MetaPAD, which discovers meta patterns from massive corpora with three techniques: (1) it develops a context-aware segmentation method to carefully determine the boundaries of patterns with a learnt pattern quality assessment function, which avoids costly dependency parsing and generates high-quality patterns; (2) it identifies and groups synonymous meta patterns from multiple facets---their types, contexts, and extractions; and (3) it examines type distributions of entities in the instances extracted by each group of patterns, and looks for appropriate type levels to make discovered patterns precise. Experiments demonstrate that our proposed framework discovers high-quality typed textual patterns efficiently from different genres of massive corpora and facilitates information extraction.|['Meng Jiang', 'Jingbo Shang', 'Taylor Cassidy', 'Xiang Ren', 'Lance M. Kaplan', 'Timothy P. Hanratty', 'Jiawei Han']|['cs.CL']
2017-03-16T23:29:16Z|2017-03-12T21:07:54Z|http://arxiv.org/abs/1703.04178v1|http://arxiv.org/pdf/1703.04178v1|Why we have switched from building full-fledged taxonomies to simply   detecting hypernymy relations|The study of taxonomies and hypernymy relations has been extensive on the Natural Language Processing (NLP) literature. However, the evaluation of taxonomy learning approaches has traditionally troublesome, as it mainly relies on ad-hoc experiments, which are hardly reproducible and manually expensive. Partly because of this, current research has been lately focusing on the hypernymy detection task. In this paper we reflect on this trend, analyzing issues related to current evaluation procedures. Finally, we propose three potential avenues for future work so that is-a relations and resources based on them play a more important role in downstream NLP applications.|['Jose Camacho-Collados']|['cs.CL']
2017-03-16T23:29:16Z|2017-03-12T08:11:29Z|http://arxiv.org/abs/1703.04081v1|http://arxiv.org/pdf/1703.04081v1|Feature overwriting as a finite mixture process: Evidence from   comprehension data|"The ungrammatical sentence ""The key to the cabinets are on the table"" is known to lead to an illusion of grammaticality. As discussed in the meta-analysis by Jaeger et al., 2017, faster reading times are observed at the verb are in the agreement-attraction sentence above compared to the equally ungrammatical sentence ""The key to the cabinet are on the table"". One explanation for this facilitation effect is the feature percolation account: the plural feature on cabinets percolates up to the head noun key, leading to the illusion. An alternative account is in terms of cue-based retrieval (Lewis & Vasishth, 2005), which assumes that the non-subject noun cabinets is misretrieved due to a partial feature-match when a dependency completion process at the auxiliary initiates a memory access for a subject with plural marking. We present evidence for yet another explanation for the observed facilitation. Because the second sentence has two nouns with identical number, it is possible that these are, in some proportion of trials, more difficult to keep distinct, leading to slower reading times at the verb in the first sentence above; this is the feature overwriting account of Nairne, 1990. We show that the feature overwriting proposal can be implemented as a finite mixture process. We reanalysed ten published data-sets, fitting hierarchical Bayesian mixture models to these data assuming a two-mixture distribution. We show that in nine out of the ten studies, a mixture distribution corresponding to feature overwriting furnishes a superior fit over both the feature percolation and the cue-based retrieval accounts."|['Shravan Vasishth', 'Lena A. Jaeger', 'Bruno Nicenboim']|['stat.ML', 'cs.CL', 'stat.AP']
2017-03-16T23:29:16Z|2017-03-11T18:20:13Z|http://arxiv.org/abs/1703.04009v1|http://arxiv.org/pdf/1703.04009v1|Automated Hate Speech Detection and the Problem of Offensive Language|A key challenge for automatic hate-speech detection on social media is the separation of hate speech from other instances of offensive language. Lexical detection methods tend to have low precision because they classify all messages containing particular terms as hate speech and previous work using supervised learning has failed to distinguish between the two categories. We used a crowd-sourced hate speech lexicon to collect tweets containing hate speech keywords. We use crowd-sourcing to label a sample of these tweets into three categories: those containing hate speech, only offensive language, and those with neither. We train a multi-class classifier to distinguish between these different categories. Close analysis of the predictions and the errors shows when we can reliably separate hate speech from other offensive language and when this differentiation is more difficult. We find that racist and homophobic tweets are more likely to be classified as hate speech but that sexist tweets are generally classified as offensive. Tweets without explicit hate keywords are also more difficult to classify.|['Thomas Davidson', 'Dana Warmsley', 'Michael Macy', 'Ingmar Weber']|['cs.CL']
2017-03-16T23:29:16Z|2017-03-11T17:14:55Z|http://arxiv.org/abs/1703.04001v1|http://arxiv.org/pdf/1703.04001v1|Language Use Matters: Analysis of the Linguistic Structure of Question   Texts Can Characterize Answerability in Quora|Quora is one of the most popular community Q&A sites of recent times. However, many question posts on this Q&A site often do not get answered. In this paper, we quantify various linguistic activities that discriminates an answered question from an unanswered one. Our central finding is that the way users use language while writing the question text can be a very effective means to characterize answerability. This characterization helps us to predict early if a question remaining unanswered for a specific time period t will eventually be answered or not and achieve an accuracy of 76.26% (t = 1 month) and 68.33% (t = 3 months). Notably, features representing the language use patterns of the users are most discriminative and alone account for an accuracy of 74.18%. We also compare our method with some of the similar works (Dror et al., Yang et al.) achieving a maximum improvement of ~39% in terms of accuracy.|['Suman Kalyan Maity', 'Aman Kharb', 'Animesh Mukherjee']|['cs.CL', 'cs.SI']
2017-03-16T23:29:16Z|2017-03-11T10:05:19Z|http://arxiv.org/abs/1703.03939v1|http://arxiv.org/pdf/1703.03939v1|Ask Me Even More: Dynamic Memory Tensor Networks (Extended Model)|We examine Memory Networks for the task of question answering (QA), under common real world scenario where training examples are scarce and under weakly supervised scenario, that is only extrinsic labels are available for training. We propose extensions for the Dynamic Memory Network (DMN), specifically within the attention mechanism, we call the resulting Neural Architecture as Dynamic Memory Tensor Network (DMTN). Ultimately, we see that our proposed extensions results in over 80% improvement in the number of task passed against the baselined standard DMN and 20% more task passed compared to state-of-the-art End-to-End Memory Network for Facebook's single task weakly trained 1K bAbi dataset.|['Govardana Sachithanandam Ramachandran', 'Ajay Sohmshetty']|['cs.CL', 'cs.LG', 'cs.NE']
2017-03-16T23:29:20Z|2017-03-11T07:37:37Z|http://arxiv.org/abs/1703.04718v1|http://arxiv.org/pdf/1703.04718v1|Extending Automatic Discourse Segmentation for Texts in Spanish to   Catalan|At present, automatic discourse analysis is a relevant research topic in the field of NLP. However, discourse is one of the phenomena most difficult to process. Although discourse parsers have been already developed for several languages, this tool does not exist for Catalan. In order to implement this kind of parser, the first step is to develop a discourse segmenter. In this article we present the first discourse segmenter for texts in Catalan. This segmenter is based on Rhetorical Structure Theory (RST) for Spanish, and uses lexical and syntactic information to translate rules valid for Spanish into rules for Catalan. We have evaluated the system by using a gold standard corpus including manually segmented texts and results are promising.|['Iria da Cunha', 'Eric SanJuan', 'Juan-Manuel Torres-Moreno', 'Irene Castellón']|['cs.CL']
2017-03-16T23:29:20Z|2017-03-11T07:35:28Z|http://arxiv.org/abs/1703.03923v1|http://arxiv.org/pdf/1703.03923v1|A German Corpus for Text Similarity Detection Tasks|Text similarity detection aims at measuring the degree of similarity between a pair of texts. Corpora available for text similarity detection are designed to evaluate the algorithms to assess the paraphrase level among documents. In this paper we present a textual German corpus for similarity detection. The purpose of this corpus is to automatically assess the similarity between a pair of texts and to evaluate different similarity measures, both for whole documents or for individual sentences. Therefore we have calculated several simple measures on our corpus based on a library of similarity functions.|['Juan-Manuel Torres-Moreno', 'Gerardo Sierra', 'Peter Peinl']|['cs.IR', 'cs.CL']
2017-03-16T23:29:20Z|2017-03-11T04:17:46Z|http://arxiv.org/abs/1703.03906v1|http://arxiv.org/pdf/1703.03906v1|Massive Exploration of Neural Machine Translation Architectures|Neural Machine Translation (NMT) has shown remarkable progress over the past few years with production systems now being deployed to end-users. One major drawback of current architectures is that they are expensive to train, typically requiring days to weeks of GPU time to converge. This makes exhaustive hyperparameter search, as is commonly done with other neural network architectures, prohibitively expensive. In this work, we present the first large-scale analysis of NMT architecture hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on the standard WMT English to German translation task. Our experiments lead to novel insights and practical advice for building and extending NMT architectures. As part of this contribution, we release an open-source NMT framework that enables researchers to easily experiment with novel techniques and reproduce state of the art results.|['Denny Britz', 'Anna Goldie', 'Thang Luong', 'Quoc Le']|['cs.CL']
2017-03-16T23:29:20Z|2017-03-10T20:54:11Z|http://arxiv.org/abs/1703.03842v1|http://arxiv.org/pdf/1703.03842v1|Effects of Limiting Memory Capacity on the Behaviour of Exemplar   Dynamics|Exemplar models are a popular class of models used to describe language change. Here we study how limiting the memory capacity of an individual in these models affects the system's behaviour. In particular we demonstrate the effect this change has on the extinction of categories. Previous work in exemplar dynamics has not addressed this question. In order to investigate this, we will inspect a simplified exemplar model. We will prove for the simplified model that all the sound categories but one will always become extinct, whether memory storage is limited or not. However, computer simulations show that changing the number of stored memories alters how fast categories become extinct.|['B. Goodman', 'P. F. Tupper']|['cs.CL', '91F20']
2017-03-16T23:29:20Z|2017-03-10T17:27:38Z|http://arxiv.org/abs/1703.03771v1|http://arxiv.org/pdf/1703.03771v1|Coping with Construals in Broad-Coverage Semantic Annotation of   Adpositions|We consider the semantics of prepositions, revisiting a broad-coverage annotation scheme used for annotating all 4,250 preposition tokens in a 55,000 word corpus of English. Attempts to apply the scheme to adpositions and case markers in other languages, as well as some problematic cases in English, have led us to reconsider the assumption that a preposition's lexical contribution is equivalent to the role/relation that it mediates. Our proposal is to embrace the potential for construal in adposition use, expressing such phenomena directly at the token level to manage complexity and avoid sense proliferation. We suggest a framework to represent both the scene role and the adposition's lexical function so they can be annotated at scale---supporting automatic, statistical processing of domain-general language---and sketch how this representation would inform a constructional analysis.|"['Jena D. Hwang', 'Archna Bhatia', 'Na-Rae Han', ""Tim O'Gorman"", 'Vivek Srikumar', 'Nathan Schneider']"|['cs.CL']
2017-03-16T23:29:20Z|2017-03-10T15:27:45Z|http://arxiv.org/abs/1703.03714v1|http://arxiv.org/pdf/1703.03714v1|Applying the Wizard-of-Oz Technique to Multimodal Human-Robot Dialogue|Our overall program objective is to provide more natural ways for soldiers to interact and communicate with robots, much like how soldiers communicate with other soldiers today. We describe how the Wizard-of-Oz (WOz) method can be applied to multimodal human-robot dialogue in a collaborative exploration task. While the WOz method can help design robot behaviors, traditional approaches place the burden of decisions on a single wizard. In this work, we consider two wizards to stand in for robot navigation and dialogue management software components. The scenario used to elicit data is one in which a human-robot team is tasked with exploring an unknown environment: a human gives verbal instructions from a remote location and the robot follows them, clarifying possible misunderstandings as needed via dialogue. We found the division of labor between wizards to be workable, which holds promise for future software development.|['Matthew Marge', 'Claire Bonial', 'Brendan Byrne', 'Taylor Cassidy', 'A. William Evans', 'Susan G. Hill', 'Clare Voss']|['cs.CL', 'cs.AI', 'cs.HC', 'cs.RO']
2017-03-16T23:29:20Z|2017-03-10T12:59:52Z|http://arxiv.org/abs/1703.03666v1|http://arxiv.org/pdf/1703.03666v1|Comparison of SMT and RBMT; The Requirement of Hybridization for   Marathi-Hindi MT|We present in this paper our work on comparison between Statistical Machine Translation (SMT) and Rule-based machine translation for translation from Marathi to Hindi. Rule Based systems although robust take lots of time to build. On the other hand statistical machine translation systems are easier to create, maintain and improve upon. We describe the development of a basic Marathi-Hindi SMT system and evaluate its performance. Through a detailed error analysis, we, point out the relative strengths and weaknesses of both systems. Effectively, we shall see that even with a small amount of training corpus a statistical machine translation system has many advantages for high quality domain specific machine translation over that of a rule-based counterpart.|['Sreelekha. S', 'Pushpak Bhattacharyya']|['cs.CL']
2017-03-16T23:29:20Z|2017-03-10T11:58:48Z|http://arxiv.org/abs/1703.03640v1|http://arxiv.org/pdf/1703.03640v1|A Study of Metrics of Distance and Correlation Between Ranked Lists for   Compositionality Detection|Compositionality in language refers to how much the meaning of some phrase can be decomposed into the meaning of its constituents and the way these constituents are combined. Based on the premise that substitution by synonyms is meaning-preserving, compositionality can be approximated as the semantic similarity between a phrase and a version of that phrase where words have been replaced by their synonyms. Different ways of representing such phrases exist (e.g., vectors [1] or language models [2]), and the choice of representation affects the measurement of semantic similarity.   We propose a new compositionality detection method that represents phrases as ranked lists of term weights. Our method approximates the semantic similarity between two ranked list representations using a range of well-known distance and correlation metrics. In contrast to most state-of-the-art approaches in compositionality detection, our method is completely unsupervised. Experiments with a publicly available dataset of 1048 human-annotated phrases shows that, compared to strong supervised baselines, our approach provides superior measurement of compositionality using any of the distance and correlation metrics considered.|['Christina Lioma', 'Niels Dalum Hansen']|['cs.CL']
2017-03-16T23:29:20Z|2017-03-10T10:17:27Z|http://arxiv.org/abs/1703.03609v1|http://arxiv.org/abs/1703.03609v1|NetSpam: a Network-based Spam Detection Framework for Reviews in Online   Social Media|Nowadays, a big part of people rely on available content in social media in their decisions (e.g. reviews and feedback on a topic or product). The possibility that anybody can leave a review provide a golden opportunity for spammers to write spam reviews about products and services for different interests. Identifying these spammers and the spam content is a hot topic of research and although a considerable number of studies have been done recently toward this end, but so far the methodologies put forth still barely detect spam reviews, and none of them show the importance of each extracted feature type. In this study, we propose a novel framework, named NetSpam, which utilizes spam features for modeling review datasets as heterogeneous information networks to map spam detection procedure into a classification problem in such networks. Using the importance of spam features help us to obtain better results in terms of different metrics experimented on real-world review datasets from Yelp and Amazon websites. The results show that NetSpam outperforms the existing methods and among four categories of features; including review-behavioral, user-behavioral, reviewlinguistic, user-linguistic, the first type of features performs better than the other categories.|['Saeedreza Shehnepoor', 'Mostafa Salehi', 'Reza Farahbakhsh', 'Noel Crespi']|['cs.SI', 'cs.CL', 'cs.IR', 'physics.soc-ph']
2017-03-16T23:29:20Z|2017-03-09T19:50:00Z|http://arxiv.org/abs/1703.03442v1|http://arxiv.org/pdf/1703.03442v1|The cognitive roots of regularization in language|Regularization occurs when the output a learner produces is less variable than the linguistic data they observed. In an artificial language learning experiment, we show that there exist at least two independent sources of regularization bias in cognition: a domain-general source based on cognitive load and a domain-specific source triggered by linguistic stimuli. Both of these factors modulate how frequency information is encoded and produced, but only the production-side modulations result in regularization (i.e. cause learners to eliminate variation from the observed input). We formalize the definition of regularization as the reduction of entropy and find that entropy measures are better at identifying regularization behavior than frequency-based analyses. We also use a model of cultural transmission to extrapolate from our experimental data in order to predict the amount of regularization which would develop in each experimental condition if the artificial language was transmitted over several generations of learners. Here we find an interaction between cognitive load and linguistic domain, suggesting that the effect of cognitive constraints can become more complex when put into the context of cultural evolution: although learning biases certainly carry information about the course of language evolution, we should not expect a one-to-one correspondence between the micro-level processes that regularize linguistic datasets and the macro-level evolution of linguistic regularity.|['Vanessa Ferdinand', 'Simon Kirby', 'Kenny Smith']|['cs.CL', 'q-bio.NC']
2017-03-16T23:29:24Z|2017-03-09T19:16:14Z|http://arxiv.org/abs/1703.03429v1|http://arxiv.org/pdf/1703.03429v1|What can you do with a rock? Affordance extraction via word embeddings|Autonomous agents must often detect affordances: the set of behaviors enabled by a situation. Affordance detection is particularly helpful in domains with large action spaces, allowing the agent to prune its search space by avoiding futile behaviors. This paper presents a method for affordance extraction via word embeddings trained on a Wikipedia corpus. The resulting word vectors are treated as a common knowledge database which can be queried using linear algebra. We apply this method to a reinforcement learning agent in a text-only environment and show that affordance-based action selection improves performance most of the time. Our method increases the computational complexity of each learning step but significantly reduces the total number of steps needed. In addition, the agent's action selections begin to resemble those a human would choose.|['Nancy Fulda', 'Daniel Ricks', 'Ben Murdoch', 'David Wingate']|['cs.AI', 'cs.CL']
2017-03-16T23:29:24Z|2017-03-09T18:37:50Z|http://arxiv.org/abs/1703.03386v1|http://arxiv.org/pdf/1703.03386v1|Loyalty in Online Communities|Loyalty is an essential component of multi-community engagement. When users have the choice to engage with a variety of different communities, they often become loyal to just one, focusing on that community at the expense of others. However, it is unclear how loyalty is manifested in user behavior, or whether loyalty is encouraged by certain community characteristics.   In this paper we operationalize loyalty as a user-community relation: users loyal to a community consistently prefer it over all others; loyal communities retain their loyal users over time. By exploring this relation using a large dataset of discussion communities from Reddit, we reveal that loyalty is manifested in remarkably consistent behaviors across a wide spectrum of communities. Loyal users employ language that signals collective identity and engage with more esoteric, less popular content, indicating they may play a curational role in surfacing new material. Loyal communities have denser user-user interaction networks and lower rates of triadic closure, suggesting that community-level loyalty is associated with more cohesive interactions and less fragmentation into subgroups. We exploit these general patterns to predict future rates of loyalty. Our results show that a user's propensity to become loyal is apparent from their first interactions with a community, suggesting that some users are intrinsically loyal from the very beginning.|['William L. Hamilton', 'Justine Zhang', 'Cristian Danescu-Niculescu-Mizil', 'Dan Jurafsky', 'Jure Leskovec']|['cs.SI', 'cs.CL']
2017-03-16T23:29:24Z|2017-03-10T08:11:22Z|http://arxiv.org/abs/1703.03200v2|http://arxiv.org/pdf/1703.03200v2|Turkish PoS Tagging by Reducing Sparsity with Morpheme Tags in Small   Datasets|Sparsity is one of the major problems in natural language processing. The problem becomes even more severe in agglutinating languages that are highly prone to be inflected. We deal with sparsity in Turkish by adopting morphological features for part-of-speech tagging. We learn inflectional and derivational morpheme tags in Turkish by using conditional random fields (CRF) and we employ the morpheme tags in part-of-speech (PoS) tagging by using hidden Markov models (HMMs) to mitigate sparsity. Results show that using morpheme tags in PoS tagging helps alleviate the sparsity in emission probabilities. Our model outperforms other hidden Markov model based PoS tagging models for small training datasets in Turkish. We obtain an accuracy of 94.1% in morpheme tagging and 89.2% in PoS tagging on a 5K training dataset.|['Burcu Can', 'Ahmet Üstün', 'Murathan Kurfalı']|['cs.CL']
2017-03-16T23:29:24Z|2017-03-09T06:20:49Z|http://arxiv.org/abs/1703.03149v1|http://arxiv.org/pdf/1703.03149v1|Detecting Sockpuppets in Deceptive Opinion Spam|This paper explores the problem of sockpuppet detection in deceptive opinion spam using authorship attribution and verification approaches. Two methods are explored. The first is a feature subsampling scheme that uses the KL-Divergence on stylistic language models of an author to find discriminative features. The second is a transduction scheme, spy induction that leverages the diversity of authors in the unlabeled test set by sending a set of spies (positive samples) from the training set to retrieve hidden samples in the unlabeled test set using nearest and farthest neighbors. Experiments using ground truth sockpuppet data show the effectiveness of the proposed schemes.|['Marjan Hosseinia', 'Arjun Mukherjee']|['cs.CL']
2017-03-16T23:29:24Z|2017-03-09T04:42:30Z|http://arxiv.org/abs/1703.03130v1|http://arxiv.org/pdf/1703.03130v1|A Structured Self-attentive Sentence Embedding|This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.|['Zhouhan Lin', 'Minwei Feng', 'Cicero Nogueira dos Santos', 'Mo Yu', 'Bing Xiang', 'Bowen Zhou', 'Yoshua Bengio']|['cs.CL', 'cs.AI', 'cs.LG', 'cs.NE']
2017-03-16T23:29:24Z|2017-03-09T01:28:00Z|http://arxiv.org/abs/1703.03097v1|http://arxiv.org/abs/1703.03097v1|Information Extraction in Illicit Domains|Extracting useful entities and attribute values from illicit domains such as human trafficking is a challenging problem with the potential for widespread social impact. Such domains employ atypical language models, have `long tails' and suffer from the problem of concept drift. In this paper, we propose a lightweight, feature-agnostic Information Extraction (IE) paradigm specifically designed for such domains. Our approach uses raw, unlabeled text from an initial corpus, and a few (12-120) seed annotations per domain-specific attribute, to learn robust IE models for unobserved pages and websites. Empirically, we demonstrate that our approach can outperform feature-centric Conditional Random Field baselines by over 18\% F-Measure on five annotated sets of real-world human trafficking datasets in both low-supervision and high-supervision settings. We also show that our approach is demonstrably robust to concept drift, and can be efficiently bootstrapped even in a serial computing environment.|['Mayank Kejriwal', 'Pedro Szekely']|['cs.CL', 'cs.AI']
2017-03-16T23:29:24Z|2017-03-09T01:04:07Z|http://arxiv.org/abs/1703.03091v1|http://arxiv.org/pdf/1703.03091v1|Deep Learning applied to NLP|Convolutional Neural Network (CNNs) are typically associated with Computer Vision. CNNs are responsible for major breakthroughs in Image Classification and are the core of most Computer Vision systems today. More recently CNNs have been applied to problems in Natural Language Processing and gotten some interesting results. In this paper, we will try to explain the basics of CNNs, its different variations and how they have been applied to NLP.|['Marc Moreno Lopez', 'Jugal Kalita']|['cs.CL']
2017-03-16T23:29:24Z|2017-03-08T14:57:10Z|http://arxiv.org/abs/1703.02860v1|http://arxiv.org/pdf/1703.02860v1|Spice up Your Chat: The Intentions and Sentiment Effects of Using Emoji|Emojis, as a new way of conveying nonverbal cues, are widely adopted in computer-mediated communications. In this paper, first from a message sender perspective, we focus on people's motives in using four types of emojis -- positive, neutral, negative, and non-facial. We compare the willingness levels of using these emoji types for seven typical intentions that people usually apply nonverbal cues for in communication. The results of extensive statistical hypothesis tests not only report the popularities of the intentions, but also uncover the subtle differences between emoji types in terms of intended uses. Second, from a perspective of message recipients, we further study the sentiment effects of emojis, as well as their duplications, on verbal messages. Different from previous studies in emoji sentiment, we study the sentiments of emojis and their contexts as a whole. The experiment results indicate that the powers of conveying sentiment are different between four emoji types, and the sentiment effects of emojis vary in the contexts of different valences.|['Tianran Hu', 'Han Guo', 'Hao Sun', 'Thuy-vy Thi Nguyen', 'Jiebo Luo']|['cs.CL', 'cs.HC']
2017-03-16T23:29:24Z|2017-03-08T14:51:09Z|http://arxiv.org/abs/1703.02859v1|http://arxiv.org/pdf/1703.02859v1|A World of Difference: Divergent Word Interpretations among People|Divergent word usages reflect differences among people. In this paper, we present a novel angle for studying word usage divergence -- word interpretations. We propose an approach that quantifies semantic differences in interpretations among different groups of people. The effectiveness of our approach is validated by quantitative evaluations. Experiment results indicate that divergences in word interpretations exist. We further apply the approach to two well studied types of differences between people -- gender and region. The detected words with divergent interpretations reveal the unique features of specific groups of people. For gender, we discover that certain different interests, social attitudes, and characters between males and females are reflected in their divergent interpretations of many words. For region, we find that specific interpretations of certain words reveal the geographical and cultural features of different regions.|['Tianran Hu', 'Ruihua Song', 'Philip Ding', 'Xing Xie', 'Jiebo Luo']|['cs.CL']
2017-03-16T23:29:24Z|2017-03-08T12:53:21Z|http://arxiv.org/abs/1703.02819v1|http://arxiv.org/abs/1703.02819v1|Introduction to Formal Concept Analysis and Its Applications in   Information Retrieval and Related Fields|This paper is a tutorial on Formal Concept Analysis (FCA) and its applications. FCA is an applied branch of Lattice Theory, a mathematical discipline which enables formalisation of concepts as basic units of human thinking and analysing data in the object-attribute form. Originated in early 80s, during the last three decades, it became a popular human-centred tool for knowledge representation and data analysis with numerous applications. Since the tutorial was specially prepared for RuSSIR 2014, the covered FCA topics include Information Retrieval with a focus on visualisation aspects, Machine Learning, Data Mining and Knowledge Discovery, Text Mining and several others.|['Dmitry I. Ignatov']|['cs.IR', 'cs.AI', 'cs.CL', 'cs.DM', 'stat.ML', '68P20, 06B99, 68T30', 'H.3.3; G.2; I.2']
2017-03-16T23:29:28Z|2017-03-07T22:13:17Z|http://arxiv.org/abs/1703.02620v1|http://arxiv.org/pdf/1703.02620v1|Linguistic Knowledge as Memory for Recurrent Neural Networks|Training recurrent neural networks to model long term dependencies is difficult. Hence, we propose to use external linguistic knowledge as an explicit signal to inform the model which memories it should utilize. Specifically, external knowledge is used to augment a sequence with typed edges between arbitrarily distant elements, and the resulting graph is decomposed into directed acyclic subgraphs. We introduce a model that encodes such graphs as explicit memory in recurrent neural networks, and use it to model coreference relations in text. We apply our model to several text comprehension tasks and achieve new state-of-the-art results on all considered benchmarks, including CNN, bAbi, and LAMBADA. On the bAbi QA tasks, our model solves 15 out of the 20 tasks with only 1000 training examples per task. Analysis of the learned representations further demonstrates the ability of our model to encode fine-grained entity information across a document.|['Bhuwan Dhingra', 'Zhilin Yang', 'William W. Cohen', 'Ruslan Salakhutdinov']|['cs.CL']
2017-03-16T23:29:28Z|2017-03-07T19:56:26Z|http://arxiv.org/abs/1703.02573v1|http://arxiv.org/pdf/1703.02573v1|Data Noising as Smoothing in Neural Network Language Models|Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in $n$-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing.|['Ziang Xie', 'Sida I. Wang', 'Jiwei Li', 'Daniel Lévy', 'Aiming Nie', 'Dan Jurafsky', 'Andrew Y. Ng']|['cs.LG', 'cs.CL']
2017-03-16T23:29:28Z|2017-03-07T18:35:33Z|http://arxiv.org/abs/1703.02517v1|http://arxiv.org/pdf/1703.02517v1|Learning opacity in Stratal Maximum Entropy Grammar|Opaque phonological patterns are sometimes claimed to be difficult to learn; specific hypotheses have been advanced about the relative difficulty of particular kinds of opaque processes (Kiparsky 1971, 1973), and the kind of data that will be helpful in learning an opaque pattern (Kiparsky 2000). In this paper, we present a computationally implemented learning theory for one grammatical theory of opacity: a Maximum Entropy version of Stratal OT (Berm\'udez-Otero 1999, Kiparsky 2000), and test it on simplified versions of opaque French tense-lax vowel alternations and the opaque interaction of diphthong raising and flapping in Canadian English. We find that the difficulty of opacity can be influenced by evidence for stratal affiliation: the Canadian English case is easier if the learner encounters application of raising outside the flapping context, or non-application of raising between words (i.e., <life> with a raised vowel; <lie for> with a non-raised vowel).|['Aleksei Nazarov', 'Joe Pater']|['cs.CL']
2017-03-16T23:29:28Z|2017-03-07T18:19:11Z|http://arxiv.org/abs/1703.02507v1|http://arxiv.org/pdf/1703.02507v1|Unsupervised Learning of Sentence Embeddings using Compositional n-Gram   Features|The recent tremendous success of unsupervised word embeddings in a multitude of applications raises the obvious question if similar methods could be derived to improve embeddings (i.e. semantic representations) of word sequences as well. We present a simple but efficient unsupervised objective to train distributed representations of sentences. Our method outperforms the state-of-the-art unsupervised models on most benchmark tasks, and on many tasks even beats supervised models, highlighting the robustness of the produced sentence embeddings.|['Matteo Pagliardini', 'Prakhar Gupta', 'Martin Jaggi']|['cs.CL', 'cs.AI', 'cs.IR', 'I.2.7']
2017-03-16T23:29:28Z|2017-03-07T18:15:57Z|http://arxiv.org/abs/1703.02504v1|http://arxiv.org/pdf/1703.02504v1|Leveraging Large Amounts of Weakly Supervised Data for Multi-Language   Sentiment Classification|This paper presents a novel approach for multi-lingual sentiment classification in short texts. This is a challenging task as the amount of training data in languages other than English is very limited. Previously proposed multi-lingual approaches typically require to establish a correspondence to English for which powerful classifiers are already available. In contrast, our method does not require such supervision. We leverage large amounts of weakly-supervised data in various languages to train a multi-layer convolutional network and demonstrate the importance of using pre-training of such networks. We thoroughly evaluate our approach on various multi-lingual datasets, including the recent SemEval-2016 sentiment prediction benchmark (Task 4), where we achieved state-of-the-art performance. We also compare the performance of our model trained individually for each language to a variant trained for all languages at once. We show that the latter model reaches slightly worse - but still acceptable - performance when compared to the single language model, while benefiting from better generalization properties across languages.|['Jan Deriu', 'Aurelien Lucchi', 'Valeria De Luca', 'Aliaksei Severyn', 'Simon Müller', 'Mark Cieliebak', 'Thomas Hofmann', 'Martin Jaggi']|['cs.CL', 'cs.IR', 'cs.LG', 'I.2.7']
2017-03-16T23:29:28Z|2017-03-07T01:13:39Z|http://arxiv.org/abs/1703.02166v1|http://arxiv.org/pdf/1703.02166v1|Building a Syllable Database to Solve the Problem of Khmer Word   Segmentation|Word segmentation is a basic problem in natural language processing. With the languages having the complex writing system like the Khmer language in Southern of Vietnam, this problem really very intractable, posing the significant challenges. Although there are some experts in Vietnam as well as international having deeply researched this problem, there are still no reasonable results meeting the demand, in particular, no treated thoroughly the ambiguous phenomenon, in the process of Khmer language processing so far. This paper present a solution based on the syllable division into component clusters using two syllable models proposed, thereby building a Khmer syllable database, is still not actually available. This method using a lexical database updated from the online Khmer dictionaries and some supported dictionaries serving role of training data and complementary linguistic characteristics. Each component cluster is labelled and located by the first and last letter to identify entirety a syllable. This approach is workable and the test results achieve high accuracy, eliminate the ambiguity, contribute to solving the problem of word segmentation and applying efficiency in Khmer language processing.|['Nam Tran Van']|['cs.CL']
2017-03-16T23:29:28Z|2017-03-06T22:37:43Z|http://arxiv.org/abs/1703.02136v1|http://arxiv.org/pdf/1703.02136v1|English Conversational Telephone Speech Recognition by Humans and   Machines|One of the most difficult speech recognition tasks is accurate recognition of human to human communication. Advances in deep learning over the last few years have produced major speech recognition improvements on the representative Switchboard conversational corpus. Word error rates that just a few years ago were 14% have dropped to 8.0%, then 6.6% and most recently 5.8%, and are now believed to be within striking range of human performance. This then raises two issues - what IS human performance, and how far down can we still drive speech recognition error rates? A recent paper by Microsoft suggests that we have already achieved human performance. In trying to verify this statement, we performed an independent set of human performance measurements on two conversational tasks and found that human performance may be considerably better than what was earlier reported, giving the community a significantly harder goal to achieve. We also report on our own efforts in this area, presenting a set of acoustic and language modeling techniques that lowered the word error rate of our own English conversational telephone LVCSR system to the level of 5.5%/10.3% on the Switchboard/CallHome subsets of the Hub5 2000 evaluation, which - at least at the writing of this paper - is a new performance milestone (albeit not at what we measure to be human performance!). On the acoustic side, we use a score fusion of three models: one LSTM with multiple feature inputs, a second LSTM trained with speaker-adversarial multi-task learning and a third residual net (ResNet) with 25 convolutional layers and time-dilated convolutions. On the language modeling side, we use word and character LSTMs and convolutional WaveNet-style language models.|['George Saon', 'Gakuto Kurata', 'Tom Sercu', 'Kartik Audhkhasi', 'Samuel Thomas', 'Dimitrios Dimitriadis', 'Xiaodong Cui', 'Bhuvana Ramabhadran', 'Michael Picheny', 'Lynn-Li Lim', 'Bergul Roomi', 'Phil Hall']|['cs.CL']
2017-03-16T23:29:28Z|2017-03-06T18:44:49Z|http://arxiv.org/abs/1703.02019v1|http://arxiv.org/pdf/1703.02019v1|Performing Stance Detection on Twitter Data using Computational   Linguistics Techniques|As humans, we can often detect from a persons utterances if he or she is in favor of or against a given target entity (topic, product, another person, etc). But from the perspective of a computer, we need means to automatically deduce the stance of the tweeter, given just the tweet text. In this paper, we present our results of performing stance detection on twitter data using a supervised approach. We begin by extracting bag-of-words to perform classification using TIMBL, then try and optimize the features to improve stance detection accuracy, followed by extending the dataset with two sets of lexicons - arguing, and MPQA subjectivity; next we explore the MALT parser and construct features using its dependency triples, finally we perform analysis using Scikit-learn Random Forest implementation.|['Gourav G. Shenoy', 'Erika H. Dsouza', 'Sandra Kübler']|['cs.CL', 'I.2.7']
2017-03-16T23:29:28Z|2017-03-06T14:40:09Z|http://arxiv.org/abs/1703.01898v1|http://arxiv.org/pdf/1703.01898v1|Generative and Discriminative Text Classification with Recurrent Neural   Networks|We empirically characterize the performance of discriminative and generative LSTM models for text classification. We find that although RNN-based generative models are more powerful than their bag-of-words ancestors (e.g., they account for conditional dependencies across words in a document), they have higher asymptotic error rates than discriminatively trained RNN models. However we also find that generative models approach their asymptotic error rate more rapidly than their discriminative counterparts---the same pattern that Ng & Jordan (2001) proved holds for linear classification models that make more naive conditional independence assumptions. Building on this finding, we hypothesize that RNN-based generative classification models will be more robust to shifts in the data distribution. This hypothesis is confirmed in a series of experiments in zero-shot and continual learning settings that show that generative models substantially outperform discriminative models.|['Dani Yogatama', 'Chris Dyer', 'Wang Ling', 'Phil Blunsom']|['stat.ML', 'cs.CL', 'cs.LG']
2017-03-16T23:29:28Z|2017-03-06T05:07:12Z|http://arxiv.org/abs/1703.01726v1|http://arxiv.org/pdf/1703.01726v1|A Novel Comprehensive Approach for Estimating Concept Semantic   Similarity in WordNet|"Computation of semantic similarity between concepts is an important foundation for many research works. This paper focuses on IC computing methods and IC measures, which estimate the semantic similarities between concepts by exploiting the topological parameters of the taxonomy. Based on analyzing representative IC computing methods and typical semantic similarity measures, we propose a new hybrid IC computing method. Through adopting the parameter dhyp and lch, we utilize the new IC computing method and propose a novel comprehensive measure of semantic similarity between concepts. An experiment based on WordNet ""is a"" taxonomy has been designed to test representative measures and our measure on benchmark dataset R&G, and the results show that our measure can obviously improve the similarity accuracy. We evaluate the proposed approach by comparing the correlation coefficients between five measures and the artificial data. The results show that our proposal outperforms the previous measures."|['Xiao-gang Zhang', 'Shou-qian Sun', 'Ke-jun Zhang']|['cs.CL']
2017-03-16T23:29:33Z|2017-03-06T04:56:19Z|http://arxiv.org/abs/1703.01725v1|http://arxiv.org/abs/1703.01725v1|Cats and Captions vs. Creators and the Clock: Comparing Multimodal   Content to Context in Predicting Relative Popularity|"The content of today's social media is becoming more and more rich, increasingly mixing text, images, videos, and audio. It is an intriguing research question to model the interplay between these different modes in attracting user attention and engagement. But in order to pursue this study of multimodal content, we must also account for context: timing effects, community preferences, and social factors (e.g., which authors are already popular) also affect the amount of feedback and reaction that social-media posts receive. In this work, we separate out the influence of these non-content factors in several ways. First, we focus on ranking pairs of submissions posted to the same community in quick succession, e.g., within 30 seconds, this framing encourages models to focus on time-agnostic and community-specific content features. Within that setting, we determine the relative performance of author vs. content features. We find that victory usually belongs to ""cats and captions,"" as visual and textual features together tend to outperform identity-based features. Moreover, our experiments show that when considered in isolation, simple unigram text features and deep neural network visual features yield the highest accuracy individually, and that the combination of the two modalities generally leads to the best accuracies overall."|['Jack Hessel', 'Lillian Lee', 'David Mimno']|['cs.SI', 'cs.CL', 'cs.CV', 'physics.soc-ph']
2017-03-16T23:29:33Z|2017-03-06T04:30:12Z|http://arxiv.org/abs/1703.01720v1|http://arxiv.org/pdf/1703.01720v1|Sound-Word2Vec: Learning Word Representations Grounded in Sounds|Sound and vision are the primary modalities that influence how we perceive the world around us. Thus, it is crucial to incorporate information from these modalities into language to help machines interact better with humans. While existing works have explored incorporating visual cues into language embeddings, the task of learning word representations that respect auditory grounding remains under-explored. In this work, we propose a new embedding scheme, sound-word2vec that learns language embeddings by grounding them in sound -- for example, two seemingly unrelated concepts, leaves and paper are closer in our embedding space as they produce similar rustling sounds. We demonstrate that the proposed embeddings perform better than language-only word representations, on two purely textual tasks that require reasoning about aural cues -- sound retrieval and foley-sound discovery. Finally, we analyze nearest neighbors to highlight the unique dependencies captured by sound-w2v as compared to language-only embeddings.|['Ashwin K Vijayakumar', 'Ramakrishna Vedantam', 'Devi Parikh']|['cs.CL', 'cs.AI', 'cs.SD']
2017-03-16T23:29:33Z|2017-03-06T00:38:51Z|http://arxiv.org/abs/1703.01694v1|http://arxiv.org/pdf/1703.01694v1|Word forms - not just their lengths- are optimized for efficient   communication|The inverse relationship between the length of a word and the frequency of its use, first identified by G.K. Zipf in 1935, is a classic empirical law that holds across a wide range of human languages. We demonstrate that length is one aspect of a much more general property of words: how distinctive they are with respect to other words in a language. Distinctiveness plays a critical role in recognizing words in fluent speech, in that it reflects the strength of potential competitors when selecting the best candidate for an ambiguous signal. Phonological information content, a measure of a word's string probability under a statistical model of a language's sound or character sequences, concisely captures distinctiveness. Examining large-scale corpora from 13 languages, we find that distinctiveness significantly outperforms word length as a predictor of frequency. This finding provides evidence that listeners' processing constraints shape fine-grained aspects of word forms across languages.|['Stephan C. Meylan', 'Thomas L. Griffiths']|['cs.CL']
2017-03-16T23:29:33Z|2017-03-05T21:57:25Z|http://arxiv.org/abs/1703.01671v1|http://arxiv.org/pdf/1703.01671v1|Controlling for Unobserved Confounds in Classification Using   Correlational Constraints|As statistical classifiers become integrated into real-world applications, it is important to consider not only their accuracy but also their robustness to changes in the data distribution. In this paper, we consider the case where there is an unobserved confounding variable $z$ that influences both the features $\mathbf{x}$ and the class variable $y$. When the influence of $z$ changes from training to testing data, we find that the classifier accuracy can degrade rapidly. In our approach, we assume that we can predict the value of $z$ at training time with some error. The prediction for $z$ is then fed to Pearl's back-door adjustment to build our model. Because of the attenuation bias caused by measurement error in $z$, standard approaches to controlling for $z$ are ineffective. In response, we propose a method to properly control for the influence of $z$ by first estimating its relationship with the class variable $y$, then updating predictions for $z$ to match that estimated relationship. By adjusting the influence of $z$, we show that we can build a model that exceeds competing baselines on accuracy as well as on robustness over a range of confounding relationships.|['Virgile Landeiro', 'Aron Culotta']|['cs.AI', 'cs.CL']
2017-03-16T23:29:33Z|2017-03-05T16:10:11Z|http://arxiv.org/abs/1703.01619v1|http://arxiv.org/pdf/1703.01619v1|Neural Machine Translation and Sequence-to-sequence Models: A Tutorial|"This tutorial introduces a new and powerful set of techniques variously called ""neural machine translation"" or ""neural sequence-to-sequence models"". These techniques have been used in a number of tasks regarding the handling of human language, and can be a powerful tool in the toolbox of anyone who wants to model sequential data of some sort. The tutorial assumes that the reader knows the basics of math and programming, but does not assume any particular experience with neural networks or natural language processing. It attempts to explain the intuition behind the various methods covered, then delves into them with enough mathematical detail to understand them concretely, and culiminates with a suggestion for an implementation exercise, where readers can test that they understood the content in practice."|['Graham Neubig']|['cs.CL', 'cs.LG', 'stat.ML']
2017-03-16T23:29:33Z|2017-03-05T15:07:10Z|http://arxiv.org/abs/1703.02031v1|http://arxiv.org/pdf/1703.02031v1|Random vector generation of a semantic space|We show how random vectors and random projection can be implemented in the usual vector space model to construct a Euclidean semantic space from a French synonym dictionary. We evaluate theoretically the resulting noise and show the experimental distribution of the similarities of terms in a neighborhood according to the choice of parameters. We also show that the Schmidt orthogonalization process is applicable and can be used to separate homonyms with distinct semantic meanings. Neighboring terms are easily arranged into semantically significant clusters which are well suited to the generation of realistic lists of synonyms and to such applications as word selection for automatic text generation. This process, applicable to any language, can easily be extended to collocations, is extremely fast and can be updated in real time, whenever new synonyms are proposed.|['Jean-François Delpech', 'Sabine Ploux']|['cs.CL']
2017-03-16T23:29:33Z|2017-03-05T04:43:41Z|http://arxiv.org/abs/1703.01557v1|http://arxiv.org/pdf/1703.01557v1|Using Graphs of Classifiers to Impose Declarative Constraints on   Semi-supervised Learning|We propose a general approach to modeling semi-supervised learning (SSL) algorithms. Specifically, we present a declarative language for modeling both traditional supervised classification tasks and many SSL heuristics, including both well-known heuristics such as co-training and novel domain-specific heuristics. In addition to representing individual SSL heuristics, we show that multiple heuristics can be automatically combined using Bayesian optimization methods. We experiment with two classes of tasks, link-based text classification and relation extraction. We show modest improvements on well-studied link-based classification benchmarks, and state-of-the-art results on relation-extraction tasks for two realistic domains.|['Lidong Bing', 'William W. Cohen', 'Bhuwan Dhingra']|['cs.LG', 'cs.CL', 'stat.ML']
2017-03-16T23:29:33Z|2017-03-04T15:55:01Z|http://arxiv.org/abs/1703.01485v1|http://arxiv.org/pdf/1703.01485v1|Lexical Resources for Hindi Marathi MT|In this paper we describe some ways to utilize various lexical resources to improve the quality of statistical machine translation system. We have augmented the training corpus with various lexical resources such as IndoWordnet semantic relation set, function words, kridanta pairs and verb phrases etc. Our research on the usage of lexical resources mainly focused on two ways such as augmenting parallel corpus with more vocabulary and augmenting with various word forms. We have described case studies, evaluations and detailed error analysis for both Marathi to Hindi and Hindi to Marathi machine translation systems. From the evaluations we observed that, there is an incremental growth in the quality of machine translation as the usage of various lexical resources increases. Moreover usage of various lexical resources helps to improve the coverage and quality of machine translation where limited parallel corpus is available.|['Sreelekha S', 'Pushpak Bhattacharyya']|['cs.CL']
2017-03-16T23:29:33Z|2017-03-03T03:14:28Z|http://arxiv.org/abs/1703.01024v1|http://arxiv.org/pdf/1703.01024v1|Exponential Moving Average Model in Parallel Speech Recognition Training|As training data rapid growth, large-scale parallel training with multi-GPUs cluster is widely applied in the neural network model learning currently.We present a new approach that applies exponential moving average method in large-scale parallel training of neural network model. It is a non-interference strategy that the exponential moving average model is not broadcasted to distributed workers to update their local models after model synchronization in the training process, and it is implemented as the final model of the training system. Fully-connected feed-forward neural networks (DNNs) and deep unidirectional Long short-term memory (LSTM) recurrent neural networks (RNNs) are successfully trained with proposed method for large vocabulary continuous speech recognition on Shenma voice search data in Mandarin. The character error rate (CER) of Mandarin speech recognition further degrades than state-of-the-art approaches of parallel training.|['Xu Tian', 'Jun Zhang', 'Zejun Ma', 'Yi He', 'Juan Wei']|['cs.CL']
2017-03-16T23:29:33Z|2017-03-09T05:50:21Z|http://arxiv.org/abs/1703.01008v2|http://arxiv.org/pdf/1703.01008v2|End-to-End Task-Completion Neural Dialogue Systems|This paper presents an end-to-end learning framework for task-completion neural dialogue systems, which leverages supervised and reinforcement learning with various deep-learning models. The system is able to interface with a structured database, and interact with users for assisting them to access information and complete tasks such as booking movie tickets. Our experiments in a movie-ticket booking domain show the proposed system outperforms a modular-based dialogue system and is more robust to noise produced by other components in the system.|['Xiujun Li', 'Yun-Nung Chen', 'Lihong Li', 'Jianfeng Gao']|['cs.CL', 'cs.AI']
2017-03-16T23:29:37Z|2017-03-02T23:58:54Z|http://arxiv.org/abs/1703.00993v1|http://arxiv.org/pdf/1703.00993v1|A Comparative Study of Word Embeddings for Reading Comprehension|The focus of past machine learning research for Reading Comprehension tasks has been primarily on the design of novel deep learning architectures. Here we show that seemingly minor choices made on (1) the use of pre-trained word embeddings, and (2) the representation of out-of-vocabulary tokens at test time, can turn out to have a larger impact than architectural choices on the final performance. We systematically explore several options for these choices, and provide recommendations to researchers working in this area.|['Bhuwan Dhingra', 'Hanxiao Liu', 'Ruslan Salakhutdinov', 'William W. Cohen']|['cs.CL']
2017-03-16T23:29:37Z|2017-03-02T21:23:47Z|http://arxiv.org/abs/1703.00955v1|http://arxiv.org/pdf/1703.00955v1|Controllable Text Generation|Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.|['Zhiting Hu', 'Zichao Yang', 'Xiaodan Liang', 'Ruslan Salakhutdinov', 'Eric P. Xing']|['cs.LG', 'cs.AI', 'cs.CL', 'stat.ML']
2017-03-16T23:29:37Z|2017-03-02T20:55:20Z|http://arxiv.org/abs/1703.00948v1|http://arxiv.org/abs/1703.00948v1|DAWT: Densely Annotated Wikipedia Texts across multiple languages|In this work, we open up the DAWT dataset - Densely Annotated Wikipedia Texts across multiple languages. The annotations include labeled text mentions mapping to entities (represented by their Freebase machine ids) as well as the type of the entity. The data set contains total of 13.6M articles, 5.0B tokens, 13.8M mention entity co-occurrences. DAWT contains 4.8 times more anchor text to entity links than originally present in the Wikipedia markup. Moreover, it spans several languages including English, Spanish, Italian, German, French and Arabic. We also present the methodology used to generate the dataset which enriches Wikipedia markup in order to increase number of links. In addition to the main dataset, we open up several derived datasets including mention entity co-occurrence counts and entity embeddings, as well as mappings between Freebase ids and Wikidata item ids. We also discuss two applications of these datasets and hope that opening them up would prove useful for the Natural Language Processing and Information Retrieval communities, as well as facilitate multi-lingual research.|['Nemanja Spasojevic', 'Preeti Bhargava', 'Guoning Hu']|['cs.IR', 'cs.AI', 'cs.CL', 'cs.SI']
2017-03-16T23:29:37Z|2017-03-02T13:52:47Z|http://arxiv.org/abs/1703.00786v1|http://arxiv.org/pdf/1703.00786v1|A Generic Online Parallel Learning Framework for Large Margin Models|To speed up the training process, many existing systems use parallel technology for online learning algorithms. However, most research mainly focus on stochastic gradient descent (SGD) instead of other algorithms. We propose a generic online parallel learning framework for large margin models, and also analyze our framework on popular large margin algorithms, including MIRA and Structured Perceptron. Our framework is lock-free and easy to implement on existing systems. Experiments show that systems with our framework can gain near linear speed up by increasing running threads, and with no loss in accuracy.|['Shuming Ma', 'Xu Sun']|['cs.CL', 'cs.LG']
2017-03-16T23:29:37Z|2017-03-02T13:49:23Z|http://arxiv.org/abs/1703.00782v1|http://arxiv.org/pdf/1703.00782v1|Lock-Free Parallel Perceptron for Graph-based Dependency Parsing|Dependency parsing is an important NLP task. A popular approach for dependency parsing is structured perceptron. Still, graph-based dependency parsing has the time complexity of $O(n^3)$, and it suffers from slow training. To deal with this problem, we propose a parallel algorithm called parallel perceptron. The parallel algorithm can make full use of a multi-core computer which saves a lot of training time. Based on experiments we observe that dependency parsing with parallel perceptron can achieve 8-fold faster training speed than traditional structured perceptron methods when using 10 threads, and with no loss at all in accuracy.|['Xu Sun', 'Shuming Ma']|['cs.CL']
2017-03-16T23:29:37Z|2017-03-02T03:59:18Z|http://arxiv.org/abs/1703.00607v1|http://arxiv.org/pdf/1703.00607v1|Discovery of Evolving Semantics through Dynamic Word Embedding Learning|During the course of human language evolution, the semantic meanings of words keep evolving with time. The understanding of evolving semantics enables us to capture the true meaning of the words in different usage contexts, and thus is critical for various applications, such as machine translation. While it is naturally promising to study word semantics in a time-aware manner, traditional methods to learn word vector representation do not adequately capture the change over time. To this end, in this paper, we aim at learning time-aware vector representation of words through dynamic word embedding modeling. Specifically, we first propose a method that captures time-specific semantics and across-time alignment simultaneously in a way that is robust to data sparsity. Then, we solve the resulting optimization problem using a scalable coordinate descent method. Finally, we perform the empirical study on New York Times data to learn the temporal embeddings and develop multiple evaluations that illustrate the semantic evolution of words, discovered from news media. Moreover, our qualitative and quantitative tests indicate that the our method not only reliably captures the semantic evolution over time, but also onsistently outperforms state-of-the-art temporal embedding approaches on both semantic accuracy and alignment quality.|['Zijun Yao', 'Yifan Sun', 'Weicong Ding', 'Nikhil Rao', 'Hui Xiong']|['cs.CL', 'stat.ML']
2017-03-16T23:29:37Z|2017-03-02T01:08:10Z|http://arxiv.org/abs/1703.00572v1|http://arxiv.org/pdf/1703.00572v1|Structural Embedding of Syntactic Trees for Machine Comprehension|This paper develops a model that addresses syntactic embedding for machine comprehension, a key task of natural language understanding. Our proposed model, structural embedding of syntactic trees (SEST), takes each word in a sentence, constructs a sequence of syntactic nodes extracted from syntactic parse trees, and encodes the sequence into a vector representation. The learned vector is then incorporated into neural attention models, which allows learning the mapping of syntactic structures between question and context pairs. We evaluate our approach on SQuAD dataset and demonstrate that our model can accurately identify the syntactic boundaries of the sentences and to extract answers that are syntactically coherent over the baseline methods.|['Rui Liu', 'Junjie Hu', 'Wei Wei', 'Zi Yang', 'Eric Nyberg']|['cs.CL']
2017-03-16T23:29:37Z|2017-03-06T19:15:04Z|http://arxiv.org/abs/1703.00565v2|http://arxiv.org/pdf/1703.00565v2|Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ|Scattertext is an open source tool for visualizing linguistic variation between document categories in a language-independent way. The tool presents a scatterplot, where each axis corresponds to the rank-frequency a term occurs in a category of documents. Through a tie-breaking strategy, the tool is able to display thousands of visible term-representing points and find space to legibly label hundreds of them. Scattertext also lends itself to a query-based visualization of how the use of terms with similar embeddings differs between document categories, as well as a visualization for comparing the importance scores of bag-of-words features to univariate metrics.|['Jason S. Kessler']|['cs.CL', 'cs.IR']
2017-03-16T23:29:37Z|2017-03-01T22:37:02Z|http://arxiv.org/abs/1703.00538v1|http://arxiv.org/abs/1703.00538v1|Unsupervised Ensemble Ranking of Terms in Electronic Health Record Notes   Based on Their Importance to Patients|Background: Electronic health record (EHR) notes contain abundant medical jargon that can be difficult for patients to comprehend. One way to help patients is to reduce information overload and help them focus on medical terms that matter most to them.   Objective: The aim of this work was to develop FIT (Finding Important Terms for patients), an unsupervised natural language processing (NLP) system that ranks medical terms in EHR notes based on their importance to patients.   Methods: We built FIT on a new unsupervised ensemble ranking model derived from the biased random walk algorithm to combine heterogeneous information resources for ranking candidate terms from each EHR note. Specifically, FIT integrates four single views for term importance: patient use of medical concepts, document-level term salience, word-occurrence based term relatedness, and topic coherence. It also incorporates partial information of term importance as conveyed by terms' unfamiliarity levels and semantic types. We evaluated FIT on 90 expert-annotated EHR notes and compared it with three benchmark unsupervised ensemble ranking methods.   Results: FIT achieved 0.885 AUC-ROC for ranking candidate terms from EHR notes to identify important terms. When including term identification, the performance of FIT for identifying important terms from EHR notes was 0.813 AUC-ROC. It outperformed the three ensemble rankers for most metrics. Its performance is relatively insensitive to its parameter.   Conclusions: FIT can automatically identify EHR terms important to patients and may help develop personalized interventions to improve quality of care. By using unsupervised learning as well as a robust and flexible framework for information fusion, FIT can be readily applied to other domains and applications.|['Jinying Chen', 'Hong Yu']|['cs.CL', 'I.2.7']
2017-03-16T23:29:37Z|2017-03-01T14:40:22Z|http://arxiv.org/abs/1703.00317v1|http://arxiv.org/pdf/1703.00317v1|Tracing Linguistic Relations in Winning and Losing Sides of Explicit   Opposing Groups|Linguistic relations in oral conversations present how opinions are constructed and developed in a restricted time. The relations bond ideas, arguments, thoughts, and feelings, re-shape them during a speech, and finally build knowledge out of all information provided in the conversation. Speakers share a common interest to discuss. It is expected that each speaker's reply includes duplicated forms of words from previous speakers. However, linguistic adaptation is observed and evolves in a more complex path than just transferring slightly modified versions of common concepts. A conversation aiming a benefit at the end shows an emergent cooperation inducing the adaptation. Not only cooperation, but also competition drives the adaptation or an opposite scenario and one can capture the dynamic process by tracking how the concepts are linguistically linked. To uncover salient complex dynamic events in verbal communications, we attempt to discover self-organized linguistic relations hidden in a conversation with explicitly stated winners and losers. We examine open access data of the United States Supreme Court. Our understanding is crucial in big data research to guide how transition states in opinion mining and decision-making should be modeled and how this required knowledge to guide the model should be pinpointed, by filtering large amount of data.|['Ceyda Sanli', 'Anupam Mondal', 'Erik Cambria']|['cs.CL', 'cs.AI']
2017-03-16T23:29:42Z|2017-03-03T15:33:07Z|http://arxiv.org/abs/1703.00203v2|http://arxiv.org/pdf/1703.00203v2|Frequency patterns of semantic change: Corpus-based evidence of a   near-critical dynamics in language change|It is generally believed that, when a linguistic item acquires a new meaning, its overall frequency of use in the language rises with time with an S-shaped growth curve. Yet, this claim has only been supported by a limited number of case studies. In this paper, we provide the first corpus-based quantitative confirmation of the genericity of the S-curve in language change. Moreover, we uncover another generic pattern, a latency phase of variable duration preceding the S-growth, during which the frequency of use of the semantically expanding word remains low and more or less constant. We also propose a usage-based model of language change supported by cognitive considerations, which predicts that both phases, the latency and the fast S-growth, take place. The driving mechanism is a stochastic dynamics, a random walk in the space of frequency of use. The underlying deterministic dynamics highlights the role of a control parameter, the strength of the cognitive impetus governing the onset of change, which tunes the system at the vicinity of a saddle-node bifurcation. In the neighborhood of the critical point, the latency phase corresponds to the diffusion time over the critical region, and the S-growth to the fast convergence that follows. The duration of the two phases is computed as specific first passage times of the random walk process, leading to distributions that fit well the ones extracted from our dataset. We argue that our results are not specific to the studied corpus, but apply to semantic change in general.|['Quentin Feltgen', 'Benjamin Fagard', 'Jean-Pierre Nadal']|['physics.soc-ph', 'cs.CL']
2017-03-16T23:29:42Z|2017-03-01T01:27:32Z|http://arxiv.org/abs/1703.00099v1|http://arxiv.org/pdf/1703.00099v1|Learning Conversational Systems that Interleave Task and Non-Task   Content|Task-oriented dialog systems have been applied in various tasks, such as automated personal assistants, customer service providers and tutors. These systems work well when users have clear and explicit intentions that are well-aligned to the systems' capabilities. However, they fail if users intentions are not explicit. To address this shortcoming, we propose a framework to interleave non-task content (i.e. everyday social conversation) into task conversations. When the task content fails, the system can still keep the user engaged with the non-task content. We trained a policy using reinforcement learning algorithms to promote long-turn conversation coherence and consistency, so that the system can have smooth transitions between task and non-task content. To test the effectiveness of the proposed framework, we developed a movie promotion dialog system. Experiments with human users indicate that a system that interleaves social and task content achieves a better task success rate and is also rated as more engaging compared to a pure task-oriented system.|['Zhou Yu', 'Alan W Black', 'Alexander I. Rudnicky']|['cs.CL', 'cs.AI', 'cs.HC']
2017-03-16T23:29:42Z|2017-03-01T00:59:17Z|http://arxiv.org/abs/1703.00096v1|http://arxiv.org/pdf/1703.00096v1|Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence   Labelling|Most existing sequence labelling models rely on a fixed decomposition of a target sequence into a sequence of basic units. These methods suffer from two major drawbacks: 1) the set of basic units is fixed, such as the set of words, characters or phonemes in speech recognition, and 2) the decomposition of target sequences is fixed. These drawbacks usually result in sub-optimal performance of modeling sequences. In this pa- per, we extend the popular CTC loss criterion to alleviate these limitations, and propose a new loss function called Gram-CTC. While preserving the advantages of CTC, Gram-CTC automatically learns the best set of basic units (grams), as well as the most suitable decomposition of tar- get sequences. Unlike CTC, Gram-CTC allows the model to output variable number of characters at each time step, which enables the model to capture longer term dependency and improves the computational efficiency. We demonstrate that the proposed Gram-CTC improves CTC in terms of both performance and efficiency on the large vocabulary speech recognition task at multiple scales of data, and that with Gram-CTC we can outperform the state-of-the-art on a standard speech benchmark.|['Hairong Liu', 'Zhenyao Zhu', 'Xiangang Li', 'Sanjeev Satheesh']|['cs.CL', 'cs.LG', 'cs.NE']
2017-03-16T23:29:42Z|2017-02-28T23:54:57Z|http://arxiv.org/abs/1703.00089v1|http://arxiv.org/pdf/1703.00089v1|A Joint Identification Approach for Argumentative Writing Revisions|Prior work on revision identification typically uses a pipeline method: revision extraction is first conducted to identify the locations of revisions and revision classification is then conducted on the identified revisions. Such a setting propagates the errors of the revision extraction step to the revision classification step. This paper proposes an approach that identifies the revision location and the revision type jointly to solve the issue of error propagation. It utilizes a sequence representation of revisions and conducts sequence labeling for revision identification. A mutation-based approach is utilized to update identification sequences. Results demonstrate that our proposed approach yields better performance on both revision location extraction and revision type classification compared to a pipeline baseline.|['Fan Zhang', 'Diane Litman']|['cs.CL']
2017-03-16T23:29:42Z|2017-02-28T20:47:47Z|http://arxiv.org/abs/1703.00050v1|http://arxiv.org/pdf/1703.00050v1|SceneSeer: 3D Scene Design with Natural Language|Designing 3D scenes is currently a creative task that requires significant expertise and effort in using complex 3D design interfaces. This effortful design process starts in stark contrast to the easiness with which people can use language to describe real and imaginary environments. We present SceneSeer: an interactive text to 3D scene generation system that allows a user to design 3D scenes using natural language. A user provides input text from which we extract explicit constraints on the objects that should appear in the scene. Given these explicit constraints, the system then uses a spatial knowledge base learned from an existing database of 3D scenes and 3D object models to infer an arrangement of the objects forming a natural scene matching the input description. Using textual commands the user can then iteratively refine the created scene by adding, removing, replacing, and manipulating objects. We evaluate the quality of 3D scenes generated by SceneSeer in a perceptual evaluation experiment where we compare against manually designed scenes and simpler baselines for 3D scene generation. We demonstrate how the generated scenes can be iteratively refined through simple natural language commands.|['Angel X. Chang', 'Mihail Eric', 'Manolis Savva', 'Christopher D. Manning']|['cs.GR', 'cs.CL', 'cs.HC']
2017-03-16T23:29:42Z|2017-02-28T05:43:10Z|http://arxiv.org/abs/1702.08653v1|http://arxiv.org/pdf/1702.08653v1|Scaffolding Networks for Teaching and Learning to Comprehend|In scaffolding teaching, students are gradually asked questions to build background knowledge, clear up confusions, learn to be attentive, and improve comprehension. Inspired by this approach, we explore methods for teaching machines to learn to reason over text documents through asking questions about the past information. We address three key challenges in teaching and learning to reason: 1) the need for an effective architecture that learns from the information in text and keeps it in memory; 2) the difficulty of self-assessing what is learned at any given point and what is left to be learned; 3) the difficulty of teaching reasoning in a scalable way. To address the first challenge, we present the Scaffolding Network, an attention-based neural network agent that can reason over a dynamic memory. It learns a policy using reinforcement learning to incrementally register new information about concepts and their relations. For the second challenge, we describe a question simulator as part of the scaffolding network that learns to continuously question the agent about the information processed so far. Through questioning, the agent learns to correctly answer as many questions as possible. For the last challenge, we explore training with reduced annotated data. We evaluate on synthetic and real datasets, demonstrating that our model competes well with the state-of-the-art methods, especially when less supervision is used.|['Asli Celikyilmaz', 'Li Deng', 'Lihong Li', 'Chong Wang']|['cs.CL']
2017-03-16T23:29:42Z|2017-02-27T22:25:45Z|http://arxiv.org/abs/1702.08563v1|http://arxiv.org/pdf/1702.08563v1|Improving Machine Learning Ability with Fine-Tuning|Item Response Theory (IRT) allows for measuring ability of Machine Learning models as compared to a human population. However, it is difficult to create a large dataset to train the ability of deep neural network models (DNNs). We propose fine-tuning as a new training process, where a model pre-trained on a large dataset is fine-tuned with a small supplemental training set. Our results show that fine-tuning can improve the ability of a state-of-the-art DNN model for Recognizing Textual Entailment tasks.|['John P. Lalor', 'Hao Wu', 'Hong Yu']|['cs.CL']
2017-03-16T23:29:42Z|2017-02-27T17:19:03Z|http://arxiv.org/abs/1702.08388v1|http://arxiv.org/pdf/1702.08388v1|Stance Classification of Social Media Users in Independence Movements|Social media and data mining are increasingly being used to analyse political and societal issues. Characterisation of users into socio-demographic groups is crucial to improve these analyses. Here we undertake the classification of social media users as supporting or opposing ongoing independence movements in their territories. Independence movements occur in territories whose citizens have conflicting national identities; users with opposing national identities will then support or oppose the sense of being part of an independent nation that differs from the officially recognised country. We describe a methodology that relies on users' self-reported location to build datasets for three territories -- Catalonia, the Basque Country and Scotland -- and we test language-independent classifiers using four types of features. We show the effectiveness of the approach to build large annotated datasets, and the ability to achieve accurate, language-independent classification performances ranging from 85% to 97% for the three territories under study.|['Arkaitz Zubiaga', 'Bo Wang', 'Maria Liakata', 'Rob Procter']|['cs.CL', 'cs.SI']
2017-03-16T23:29:42Z|2017-02-27T14:37:21Z|http://arxiv.org/abs/1702.08303v1|http://arxiv.org/pdf/1702.08303v1|Identifying beneficial task relations for multi-task learning in deep   neural networks|Multi-task learning (MTL) in deep neural networks for NLP has recently received increasing interest due to some compelling benefits, including its potential to efficiently regularize models and to reduce the need for labeled data. While it has brought significant improvements in a number of NLP tasks, mixed results have been reported, and little is known about the conditions under which MTL leads to gains in NLP. This paper sheds light on the specific task relations that can lead to gains from MTL models over single-task setups.|['Joachim Bingel', 'Anders Søgaard']|['cs.CL', 'I.2.7']
2017-03-16T23:29:42Z|2017-02-27T13:38:08Z|http://arxiv.org/abs/1702.08451v1|http://arxiv.org/pdf/1702.08451v1|Approches d'analyse distributionnelle pour améliorer la   désambiguïsation sémantique|Word sense disambiguation (WSD) improves many Natural Language Processing (NLP) applications such as Information Retrieval, Machine Translation or Lexical Simplification. WSD is the ability of determining a word sense among different ones within a polysemic lexical unit taking into account the context. The most straightforward approach uses a semantic proximity measure between the word sense candidates of the target word and those of its context. Such a method very easily entails a combinatorial explosion. In this paper, we propose two methods based on distributional analysis which enable to reduce the exponential complexity without losing the coherence. We present a comparison between the selection of distributional neighbors and the linearly nearest neighbors. The figures obtained show that selecting distributional neighbors leads to better results.|['Mokhtar Billami', 'Núria Gala']|['cs.CL']
2017-03-16T23:29:46Z|2017-02-27T13:37:15Z|http://arxiv.org/abs/1702.08450v1|http://arxiv.org/pdf/1702.08450v1|A Knowledge-Based Approach to Word Sense Disambiguation by   distributional selection and semantic features|Word sense disambiguation improves many Natural Language Processing (NLP) applications such as Information Retrieval, Information Extraction, Machine Translation, or Lexical Simplification. Roughly speaking, the aim is to choose for each word in a text its best sense. One of the most popular method estimates local semantic similarity relatedness between two word senses and then extends it to all words from text. The most direct method computes a rough score for every pair of word senses and chooses the lexical chain that has the best score (we can imagine the exponential complexity that returns this comprehensive approach). In this paper, we propose to use a combinatorial optimization metaheuristic for choosing the nearest neighbors obtained by distributional selection around the word to disambiguate. The test and the evaluation of our method concern a corpus written in French by means of the semantic network BabelNet. The obtained accuracy rate is 78 % on all names and verbs chosen for the evaluation.|['Mokhtar Billami']|['cs.CL']
2017-03-16T23:29:46Z|2017-02-27T10:09:46Z|http://arxiv.org/abs/1702.08217v1|http://arxiv.org/pdf/1702.08217v1|A case study on English-Malayalam Machine Translation|In this paper we present our work on a case study on Statistical Machine Translation (SMT) and Rule based machine translation (RBMT) for translation from English to Malayalam and Malayalam to English. One of the motivations of our study is to make a three way performance comparison, such as, a) SMT and RBMT b) English to Malayalam SMT and Malayalam to English SMT c) English to Malayalam RBMT and Malayalam to English RBMT. We describe the development of English to Malayalam and Malayalam to English baseline phrase based SMT system and the evaluation of its performance compared against the RBMT system. Based on our study the observations are: a) SMT systems outperform RBMT systems, b) In the case of SMT, English - Malayalam systems perform better than that of Malayalam - English systems, c) In the case RBMT, Malayalam to English systems are performing better than English to Malayalam systems. Based on our evaluations and detailed error analysis, we describe the requirements of incorporating morphological processing into the SMT to improve the accuracy of translation.|['Sreelekha S', 'Pushpak Bhattacharyya']|['cs.CL']
2017-03-16T23:29:46Z|2017-02-27T04:16:01Z|http://arxiv.org/abs/1702.08139v1|http://arxiv.org/pdf/1702.08139v1|Improved Variational Autoencoders for Text Modeling using Dilated   Convolutions|Recent work on generative modeling of text has found that variational auto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder's dilation architecture, we control the effective context from previously generated words. In experiments, we find that there is a trade off between the contextual capacity of the decoder and the amount of encoding information used. We show that with the right decoder, VAE can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive experimental result on the use VAE for generative modeling of text. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.|['Zichao Yang', 'Zhiting Hu', 'Ruslan Salakhutdinov', 'Taylor Berg-Kirkpatrick']|['cs.NE', 'cs.CL', 'cs.LG']
2017-03-16T23:29:46Z|2017-02-26T11:22:41Z|http://arxiv.org/abs/1702.08021v1|http://arxiv.org/pdf/1702.08021v1|Friends and Enemies of Clinton and Trump: Using Context for Detecting   Stance in Political Tweets|Stance detection, the task of identifying the speaker's opinion towards a particular target, has attracted the attention of researchers. This paper describes a novel approach for detecting stance in Twitter. We define a set of features in order to consider the context surrounding a target of interest with the final aim of training a model for predicting the stance towards the mentioned targets. In particular, we are interested in investigating political debates in social media. For this reason we evaluated our approach focusing on two targets of the SemEval-2016 Task6 on Detecting stance in tweets, which are related to the political campaign for the 2016 U.S. presidential elections: Hillary Clinton vs. Donald Trump. For the sake of comparison with the state of the art, we evaluated our model against the dataset released in the SemEval-2016 Task 6 shared task competition. Our results outperform the best ones obtained by participating teams, and show that information about enemies and friends of politicians help in detecting stance towards them.|['Mirko Lai', 'Delia Irazú Hernández Farías', 'Viviana Patti', 'Paolo Rosso']|['cs.CL']
2017-03-16T23:29:46Z|2017-02-26T08:07:26Z|http://arxiv.org/abs/1702.07998v1|http://arxiv.org/pdf/1702.07998v1|Detecting (Un)Important Content for Single-Document News Summarization|"We present a robust approach for detecting intrinsic sentence importance in news, by training on two corpora of document-summary pairs. When used for single-document summarization, our approach, combined with the ""beginning of document"" heuristic, outperforms a state-of-the-art summarizer and the beginning-of-article baseline in both automatic and manual evaluations. These results represent an important advance because in the absence of cross-document repetition, single document summarizers for news have not been able to consistently outperform the strong beginning-of-article baseline."|['Yinfei Yang', 'Forrest Sheng Bao', 'Ani Nenkova']|['cs.CL']
2017-03-16T23:29:46Z|2017-02-26T03:19:13Z|http://arxiv.org/abs/1702.07983v1|http://arxiv.org/pdf/1702.07983v1|Maximum-Likelihood Augmented Discrete Generative Adversarial Networks|Despite the successes in capturing continuous distributions, the application of generative adversarial networks (GANs) to discrete settings, like natural language tasks, is rather restricted. The fundamental reason is the difficulty of back-propagation through discrete random variables combined with the inherent instability of the GAN training objective. To address these problems, we propose Maximum-Likelihood Augmented Discrete Generative Adversarial Networks. Instead of directly optimizing the GAN objective, we derive a novel and low-variance objective using the discriminator's output that follows corresponds to the log-likelihood. Compared with the original, the new objective is proved to be consistent in theory and beneficial in practice. The experimental results on various discrete datasets demonstrate the effectiveness of the proposed approach.|['Tong Che', 'Yanran Li', 'Ruixiang Zhang', 'R Devon Hjelm', 'Wenjie Li', 'Yangqiu Song', 'Yoshua Bengio']|['cs.AI', 'cs.CL', 'cs.LG']
2017-03-16T23:29:46Z|2017-02-25T05:16:15Z|http://arxiv.org/abs/1702.07835v1|http://arxiv.org/pdf/1702.07835v1|Critical Survey of the Freely Available Arabic Corpora|The availability of corpora is a major factor in building natural language processing applications. However, the costs of acquiring corpora can prevent some researchers from going further in their endeavours. The ease of access to freely available corpora is urgent needed in the NLP research community especially for language such as Arabic. Currently, there is not easy was to access to a comprehensive and updated list of freely available Arabic corpora. We present in this paper, the results of a recent survey conducted to identify the list of the freely available Arabic corpora and language resources. Our preliminary results showed an initial list of 66 sources. We presents our findings in the various categories studied and we provided the direct links to get the data when possible.|['Wajdi Zaghouani']|['cs.CL']
2017-03-16T23:29:46Z|2017-02-25T03:20:49Z|http://arxiv.org/abs/1702.07826v1|http://arxiv.org/pdf/1702.07826v1|Rationalization: A Neural Machine Translation Approach to Generating   Natural Language Explanations|We introduce AI rationalization, an approach for generating explanations of autonomous system behavior as if a human had done the behavior. We describe a rationalization technique that uses neural machine translation to translate internal state-action representations of the autonomous agent into natural language. We evaluate our technique in the Frogger game environment. The natural language is collected from human players thinking out loud as they play the game. We motivate the use of rationalization as an approach to explanation generation, show the results of experiments on the accuracy of our rationalization technique, and describe future research agenda.|['Brent Harrison', 'Upol Ehsan', 'Mark O. Riedl']|['cs.AI', 'cs.CL', 'cs.HC', 'cs.LG']
2017-03-16T23:29:46Z|2017-03-07T23:09:23Z|http://arxiv.org/abs/1702.07825v2|http://arxiv.org/pdf/1702.07825v2|Deep Voice: Real-time Neural Text-to-Speech|We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.|['Sercan O. Arik', 'Mike Chrzanowski', 'Adam Coates', 'Gregory Diamos', 'Andrew Gibiansky', 'Yongguo Kang', 'Xian Li', 'John Miller', 'Andrew Ng', 'Jonathan Raiman', 'Shubho Sengupta', 'Mohammad Shoeybi']|['cs.CL', 'cs.LG', 'cs.NE', 'cs.SD']
2017-03-16T23:29:46Z|2017-02-24T22:49:13Z|http://arxiv.org/abs/1702.07793v1|http://arxiv.org/pdf/1702.07793v1|Residual Convolutional CTC Networks for Automatic Speech Recognition|Deep learning approaches have been widely used in Automatic Speech Recognition (ASR) and they have achieved a significant accuracy improvement. Especially, Convolutional Neural Networks (CNNs) have been revisited in ASR recently. However, most CNNs used in existing work have less than 10 layers which may not be deep enough to capture all human speech signal information. In this paper, we propose a novel deep and wide CNN architecture denoted as RCNN-CTC, which has residual connections and Connectionist Temporal Classification (CTC) loss function. RCNN-CTC is an end-to-end system which can exploit temporal and spectral structures of speech signals simultaneously. Furthermore, we introduce a CTC-based system combination, which is different from the conventional frame-wise senone-based one. The basic subsystems adopted in the combination are different types and thus mutually complementary to each other. Experimental results show that our proposed single system RCNN-CTC can achieve the lowest word error rate (WER) on WSJ and Tencent Chat data sets, compared to several widely used neural network systems in ASR. In addition, the proposed system combination can offer a further error reduction on these two data sets, resulting in relative WER reductions of $14.91\%$ and $6.52\%$ on WSJ dev93 and Tencent Chat data sets respectively.|['Yisen Wang', 'Xuejiao Deng', 'Songbai Pu', 'Zhiheng Huang']|['cs.CL']
