2017-03-16T23:26:20Z|2017-03-15T13:31:23Z|http://arxiv.org/abs/1703.05144v1|http://arxiv.org/pdf/1703.05144v1|Bergm: Bayesian exponential random graph models in R|The Bergm package provides a comprehensive framework for Bayesian inference using Markov chain Monte Carlo (MCMC) algorithms. It can also supply graphical Bayesian goodness-of-fit procedures that address the issue of model adequacy. The package is simple to use and represents an attractive way of analysing network data as it offers the advantage of a complete probabilistic treatment of uncertainty. Bergm is based on the ergm package and therefore it makes use of the same model set-up and network simulation algorithms. The Bergm package has been continually improved in terms of speed performance over the last years and now offers the end-user a feasible option for carrying out Bayesian inference for networks with several thousands of nodes.|['Alberto Caimo', 'Nial Friel']|['stat.CO']
2017-03-16T23:26:20Z|2017-03-15T10:20:32Z|http://arxiv.org/abs/1703.05060v1|http://arxiv.org/pdf/1703.05060v1|Online Learning for Distribution-Free Prediction|We develop an online learning method for prediction, which is important in problems with large and/or streaming data sets. We formulate the learning approach using a covariance-fitting methodology, and show that the resulting predictor has desirable computational and distribution-free properties: It is implemented online with a runtime that scales linearly in the number of samples; has a constant memory requirement; avoids local minima problems; and prunes away redundant feature dimensions without relying on restrictive assumptions on the data distribution. In conjunction with the split conformal approach, it also produces distribution-free prediction confidence intervals in a computationally efficient manner. The method is demonstrated on both real and synthetic datasets.|['Dave Zachariah', 'Petre Stoica', 'Thomas B. Sch√∂n']|['cs.LG', 'stat.CO', 'stat.ML']
2017-03-16T23:26:20Z|2017-03-15T01:18:57Z|http://arxiv.org/abs/1703.04866v1|http://arxiv.org/pdf/1703.04866v1|Multilevel Sequential Monte Carlo with Dimension-Independent   Likelihood-Informed Proposals|In this article we develop a new sequential Monte Carlo (SMC) method for multilevel (ML) Monte Carlo estimation. In particular, the method can be used to estimate expectations with respect to a target probability distribution over an infinite-dimensional and non-compact space as given, for example, by a Bayesian inverse problem with Gaussian random field prior. Under suitable assumptions the MLSMC method has the optimal $O(\epsilon^{-2})$ bound on the cost to obtain a mean-square error of $O(\epsilon^2)$. The algorithm is accelerated by dimension-independent likelihood-informed (DILI) proposals designed for Gaussian priors, leveraging a novel variation which uses empirical sample covariance information in lieu of Hessian information, hence eliminating the requirement for gradient evaluations. The efficiency of the algorithm is illustrated on two examples: inversion of noisy pressure measurements in a PDE model of Darcy flow to recover the posterior distribution of the permeability field, and inversion of noisy measurements of the solution of an SDE to recover the posterior path measure.|['Alexandros Beskos', 'Ajay Jasra', 'Kody Law', 'Youssef Marzouk', 'Yan Zhou']|['stat.CO']
2017-03-16T23:26:20Z|2017-03-13T16:19:01Z|http://arxiv.org/abs/1703.04467v1|http://arxiv.org/pdf/1703.04467v1|spmoran: An R package for Moran's eigenvector-based spatial regression   analysis|"The objective of this study is illustrating how to use ""spmoran,"" which is an R package for Moran's eigenvector-based spatial regression analysis. spmoran estimates regression models in the presence of spatial dependence, including eigenvector spatial filtering (ESF) and random effects ESF (RE-ESF) models. These models are allowed to have spatially varying coefficients to capture spatial heterogeneity. These ESF and RE-ESF models are suitable to estimate and infer regression coefficients with/without spatial variation. spmoran implements these models in a computationally efficient manner. For the illustration, this study applies ESF and RE-ESF models for a land price analysis."|['Daisuke Murakami']|['stat.OT', 'stat.CO']
2017-03-16T23:26:20Z|2017-03-13T11:19:28Z|http://arxiv.org/abs/1703.04334v1|http://arxiv.org/pdf/1703.04334v1|Probabilistic Matching: Causal Inference under Measurement Errors|The abundance of data produced daily from large variety of sources has boosted the need of novel approaches on causal inference analysis from observational data. Observational data often contain noisy or missing entries. Moreover, causal inference studies may require unobserved high-level information which needs to be inferred from other observed attributes. In such cases, inaccuracies of the applied inference methods will result in noisy outputs. In this study, we propose a novel approach for causal inference when one or more key variables are noisy. Our method utilizes the knowledge about the uncertainty of the real values of key variables in order to reduce the bias induced by noisy measurements. We evaluate our approach in comparison with existing methods both on simulated and real scenarios and we demonstrate that our method reduces the bias and avoids false causal inference conclusions in most cases.|['Fani Tsapeli', 'Peter Tino', 'Mirco Musolesi']|['stat.ME', 'stat.CO', 'stat.ML']
2017-03-16T23:26:20Z|2017-03-11T20:07:06Z|http://arxiv.org/abs/1703.04025v1|http://arxiv.org/pdf/1703.04025v1|Learning Large-Scale Bayesian Networks with the sparsebn Package|Learning graphical models from data is an important problem with wide applications, ranging from genomics to the social sciences. Nowadays datasets typically have upwards of thousands---sometimes tens or hundreds of thousands---of variables and far fewer samples. To meet this challenge, we develop a new R package called sparsebn for learning the structure of large, sparse graphical models with a focus on Bayesian networks. While there are many existing packages for this task within the R ecosystem, this package focuses on the unique setting of learning large networks from high-dimensional data, possibly with interventions. As such, the methods provided place a premium on scalability and consistency in a high-dimensional setting. Furthermore, in the presence of interventions, the methods implemented here achieve the goal of learning a causal network from data. The sparsebn package is open-source and available on CRAN.|['Bryon Aragam', 'Jiaying Gu', 'Qing Zhou']|['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']
2017-03-16T23:26:20Z|2017-03-10T13:43:06Z|http://arxiv.org/abs/1703.03680v1|http://arxiv.org/pdf/1703.03680v1|Strong convergence rates of probabilistic integrators for ordinary   differential equations|Probabilistic integration of a continuous dynamical system is a way of systematically introducing model error, at scales no larger than errors inroduced by standard numerical discretisation, in order to enable thorough exploration of possible responses of the system to inputs. It is thus a potentially useful approach in a number of applications such as forward uncertainty quantification, inverse problems, and data assimilation. We extend the convergence analysis of probabilistic integrators for deterministic ordinary differential equations, as proposed by Conrad et al. (Stat. Comput., 2016), to establish mean-square convergence in the uniform norm on discrete- or continuous-time solutions under relaxed regularity assumptions on the driving vector fields and their induced flows. Specifically, we show that randomised high-order integrators for globally Lipschitz flows and randomised Euler integrators for dissipative vector fields with polynomially-bounded local Lipschitz constants all have the same mean-square convergence rate as their deterministic counterparts, provided that the variance of the integration noise is not of higher order than the corresponding deterministic integrator.|['H. C. Lie', 'A. M. Stuart', 'T. J. Sullivan']|['math.NA', 'math.PR', 'math.ST', 'stat.CO', 'stat.TH', '65L20, 65C99, 37H10, 68W20']
2017-03-16T23:26:20Z|2017-03-09T21:57:00Z|http://arxiv.org/abs/1703.03475v1|http://arxiv.org/pdf/1703.03475v1|Auxiliary Variables for Bayesian Inference in Multi-Class Queueing   Networks|Queue networks describe complex stochastic systems of both theoretical and practical interest. They provide the means to assess alterations, diagnose poor performance and evaluate robustness across sets of interconnected resources. In the present paper, we focus on the underlying continuous-time Markov chains induced by these networks, and we present a flexible method for drawing parameter inference in multi-class Markovian cases with switching and different service disciplines. The approach is directed towards the inferential problem with missing data and introduces a slice sampling technique with mappings to the measurable space of task transitions between service stations. The method deals with time and tractability issues, can handle prior system knowledge and overcomes common restrictions on service rates across existing inferential frameworks. Finally, the proposed algorithm is validated on synthetic data and applied to a real data set, obtained from a service delivery tasking tool implemented in two university hospitals.|['Iker Perez', 'David Hodge', 'Theodore Kypraios']|['stat.CO']
2017-03-16T23:26:20Z|2017-03-09T17:17:39Z|http://arxiv.org/abs/1703.03352v1|http://arxiv.org/pdf/1703.03352v1|A log-linear time algorithm for constrained changepoint detection|Changepoint detection is a central problem in time series and genomic data. For some applications, it is natural to impose constraints on the directions of changes. One example is ChIP-seq data, for which adding an up-down constraint improves peak detection accuracy, but makes the optimization problem more complicated. We show how a recently proposed functional pruning technique can be adapted to solve such constrained changepoint detection problems. This leads to a new algorithm which can solve problems with arbitrary affine constraints on adjacent segment means, and which has empirical time complexity that is log-linear in the amount of data. This algorithm achieves state-of-the-art accuracy in a benchmark of several genomic data sets, and is orders of magnitude faster than existing algorithms that have similar accuracy. Our implementation is available as the PeakSegPDPA function in the coseg R package, https://github.com/tdhock/coseg|['Toby Dylan Hocking', 'Guillem Rigaill', 'Paul Fearnhead', 'Guillaume Bourque']|['stat.CO', 'q-bio.GN', 'stat.ML']
2017-03-16T23:26:20Z|2017-03-10T21:00:26Z|http://arxiv.org/abs/1703.03004v2|http://arxiv.org/pdf/1703.03004v2|New approximation for GARCH parameters estimate|This paper presents a new approach for the optimization of GARCH parameters estimation. Firstly, we propose a method for the localization of the maximum. Thereafter, using the methods of least squares, we make a local approximation for the projection of the likelihood function curve on two dimensional planes by a polynomial of order two which will be used to calculate an estimation of the maximum.|['Yakoub Boularouk', 'Nasr-eddine Hamri']|['stat.CO']
2017-03-16T23:26:24Z|2017-03-08T19:22:11Z|http://arxiv.org/abs/1703.02998v1|http://arxiv.org/pdf/1703.02998v1|A note on quickly sampling a sparse matrix with low rank expectation|"Given matrices $X,Y \in R^{n \times K}$ and $S \in R^{K \times K}$ with positive elements, this paper proposes an algorithm fastRG to sample a sparse matrix $A$ with low rank expectation $E(A) = XSY^T$ and independent Poisson elements. This allows for quickly sampling from a broad class of stochastic blockmodel graphs (degree-corrected, mixed membership, overlapping) all of which are specific parameterizations of the generalized random product graph model defined in Section 2.2. The basic idea of fastRG is to first sample the number of edges $m$ and then sample each edge. The key insight is that because of the the low rank expectation, it is easy to sample individual edges. The naive ""element-wise"" algorithm requires $O(n^2)$ operations to generate the $n\times n$ adjacency matrix $A$. In sparse graphs, where $m = O(n)$, ignoring log terms, fastRG runs in time $O(n)$. An implementation in fastRG is available on github. A computational experiment in Section 2.4 simulates graphs up to $n=10,000,000$ nodes with $m = 100,000,000$ edges. For example, on a graph with $n=500,000$ and $m = 5,000,000$, fastRG runs in less than one second on a 3.5 GHz Intel i5."|['Karl Rohe', 'Jun Tao', 'Xintian Han', 'Norbert Binkiewicz']|['stat.CO']
2017-03-16T23:26:24Z|2017-03-07T18:36:55Z|http://arxiv.org/abs/1703.02518v1|http://arxiv.org/pdf/1703.02518v1|Faster Coordinate Descent via Adaptive Importance Sampling|Coordinate descent methods employ random partial updates of decision variables in order to solve huge-scale convex optimization problems. In this work, we introduce new adaptive rules for the random selection of their updates. By adaptive, we mean that our selection rules are based on the dual residual or the primal-dual gap estimates and can change at each iteration. We theoretically characterize the performance of our selection rules and demonstrate improvements over the state-of-the-art, and extend our theory and algorithms to general convex objectives. Numerical evidence with hinge-loss support vector machines and Lasso confirm that the practice follows the theory.|['Dmytro Perekrestenko', 'Volkan Cevher', 'Martin Jaggi']|['cs.LG', 'cs.CV', 'math.OC', 'stat.CO', 'stat.ML', 'G.1.6']
2017-03-16T23:26:24Z|2017-03-07T15:13:08Z|http://arxiv.org/abs/1703.02428v1|http://arxiv.org/pdf/1703.02428v1|Robust Bayesian Filtering and Smoothing Using Student's t Distribution|State estimation in heavy-tailed process and measurement noise is an important challenge that must be addressed in, e.g., tracking scenarios with agile targets and outlier-corrupted measurements. The performance of the Kalman filter (KF) can deteriorate in such applications because of the close relation to the Gaussian distribution. Therefore, this paper describes the use of Student's t distribution to develop robust, scalable, and simple filtering and smoothing algorithms.   After a discussion of Student's t distribution, exact filtering in linear state-space models with t noise is analyzed. Intermediate approximation steps are used to arrive at filtering and smoothing algorithms that closely resemble the KF and the Rauch-Tung-Striebel (RTS) smoother except for a nonlinear measurement-dependent matrix update. The required approximations are discussed and an undesirable behavior of moment matching for t densities is revealed. A favorable approximation based on minimization of the Kullback-Leibler divergence is presented. Because of its relation to the KF, some properties and algorithmic extensions are inherited by the t filter. Instructive simulation examples demonstrate the performance and robustness of the novel algorithms.|['Michael Roth', 'Tohid Ardeshiri', 'Emre √ñzkan', 'Fredrik Gustafsson']|['stat.ME', 'cs.SY', 'stat.CO']
2017-03-16T23:26:24Z|2017-03-07T15:01:51Z|http://arxiv.org/abs/1703.02419v1|http://arxiv.org/pdf/1703.02419v1|Probabilistic learning of nonlinear dynamical systems using sequential   Monte Carlo|"Probabilistic modeling provides the capability to represent and manipulate uncertainty in data, models, decisions and predictions. We are concerned with the problem of learning probabilistic models of dynamical systems from measured data. Specifically, we consider learning of probabilistic nonlinear state space models. There is no closed-form solution available for this problem, implying that we are forced to use approximations. In this tutorial we will provide a self-contained introduction to one of the state-of-the-art methods---the particle Metropolis-Hastings algorithm---which has proven to offer very practical approximations. This is a Monte Carlo based method, where the so-called particle filter is used to guide a Markov chain Monte Carlo method through the parameter space. One of the key merits of the particle Metropolis-Hastings method is that it is guaranteed to converge to the ""true solution"" under mild assumptions, despite being based on a practical implementation of a particle filter (i.e., using a finite number of particles). We will also provide a motivating numerical example illustrating the method which we have implemented in an in-house developed modeling language, serving the purpose of abstracting away the underlying mathematics of the Monte Carlo approximations from the user. This modeling language will open up the power of sophisticated Monte Carlo methods, including particle Metropolis-Hastings, to a large group of users without requiring them to know all the underlying mathematical details."|['Thomas B. Sch√∂n', 'Andreas Svensson', 'Lawrence Murray', 'Fredrik Lindsten']|['stat.CO', 'cs.LG', 'cs.SY']
2017-03-16T23:26:24Z|2017-03-07T11:48:45Z|http://arxiv.org/abs/1703.02341v1|http://arxiv.org/pdf/1703.02341v1|An automatic adaptive method to combine summary statistics in   approximate Bayesian computation|To infer the parameters of mechanistic models with intractable likelihoods, techniques such as approximate Bayesian computation (ABC) are increasingly being adopted. One of the main disadvantages of ABC in practical situations, however, is that parameter inference must generally rely on summary statistics of the data. This is particularly the case for problems involving high-dimensional data, such as biological imaging experiments. However, some summary statistics contain more information about parameters of interest than others, and it is not always clear how to weight their contributions within the ABC framework. We address this problem by developing an automatic, adaptive algorithm that chooses weights for each summary statistic. Our algorithm aims to maximize the distance between the prior and the approximate posterior by automatically adapting the weights within the ABC distance function. To demonstrate the effectiveness of our algorithm, we apply it to several stochastic models of biochemical reaction networks, and a spatial model of diffusion, and compare our results with existing algorithms.|['Jonathan U Harrison', 'Ruth E Baker']|['stat.CO']
2017-03-16T23:26:24Z|2017-03-13T11:11:48Z|http://arxiv.org/abs/1703.02337v2|http://arxiv.org/pdf/1703.02337v2|A Note on the Convergence of the Gaussian Mean Shift Algorithm|Mean shift (MS) algorithms are popular methods for mode finding in pattern analysis. Each MS algorithm can be phrased as a fixed-point iteration scheme, which operates on a kernel density estimate (KDE) based on some data. The ability of an MS algorithm to obtain the modes of its KDE depends on whether or not the fixed-point scheme converges. The convergence of MS algorithms have recently been proved under some general conditions via first principle arguments. We complement the recent proofs by demonstrating that the MS algorithm operating on a Gaussian KDE can be viewed as an MM (minorization-maximization) algorithm, and thus permits the application of convergence techniques for such constructions. For the Gaussian case, we extend upon the previously results by showing that the fixed-points of the MS algorithm are all stationary points of the KDE in cases where the stationary points may not necessarily be isolated.|['Hien D Nguyen']|['stat.CO']
2017-03-16T23:26:24Z|2017-03-07T09:33:21Z|http://arxiv.org/abs/1703.02293v1|http://arxiv.org/pdf/1703.02293v1|Variable selection for mixed data clustering: a model-based approach|We propose two approaches for selecting variables in latent class analysis (i.e.,mixture model assuming within component independence), which is the common model-based clustering method for mixed data. The first approach consists in optimizing the BIC with a modified version of the EM algorithm. This approach simultaneously performs both model selection and parameter inference. The second approach consists in maximizing the MICL, which considers the clustering task, with an algorithm of alternate optimization. This approach performs model selection without requiring the maximum likelihood estimates for model comparison, then parameter inference is done for the unique selected model. Thus, the benefits of both approaches is to avoid the computation of the maximum likelihood estimates for each model comparison. Moreover, they also avoid the use of the standard algorithms for variable selection which are often suboptimal (e.g. stepwise method) and computationally expensive. The case of data with missing values is also discussed. The interest of both proposed criteria is shown on simulated and real data.|['Matthieu Marbac', 'Mohammed Sedki']|['stat.CO', '62F15']
2017-03-16T23:26:24Z|2017-03-07T07:44:52Z|http://arxiv.org/abs/1703.02251v1|http://arxiv.org/pdf/1703.02251v1|The Maximum Likelihood Degree of Toric Varieties|We study the maximum likelihood degree (ML degree) of toric varieties, known as discrete exponential models in statistics. By introducing scaling coefficients to the monomial parameterization of the toric variety, one can change the ML degree. We show that the ML degree is equal to the degree of the toric variety for generic scalings, while it drops if and only if the scaling vector is in the locus of the principal $A$-determinant. We also illustrate how to compute the ML estimate of a toric variety numerically via homotopy continuation from a scaled toric variety with low ML degree. Throughout, we include examples motivated by algebraic geometry and statistics. We compute the ML degree of rational normal scrolls and a large class of Veronese-type varieties. In addition, we investigate the ML degree of scaled Segre varieties, hierarchical loglinear models, and graphical models.|['Carlos Am√©ndola', 'Nathan Bliss', 'Isaac Burke', 'Courtney R. Gibbons', 'Martin Helmer', 'Serkan Ho≈üten', 'Evan D. Nash', 'Jose Israel Rodriguez', 'Daniel Smolkin']|['math.AG', 'math.ST', 'stat.CO', 'stat.TH', '14Q15, 14M25, 13P15, 62F10']
2017-03-16T23:26:24Z|2017-03-07T06:40:44Z|http://arxiv.org/abs/1703.02237v1|http://arxiv.org/pdf/1703.02237v1|Scalable Collaborative Targeted Learning for High-Dimensional Data|Robust inference of a low-dimensional parameter in a large semi-parametric model relies on external estimators of infinite-dimensional features of the distribution of the data. Typically, only one of the latter is optimized for the sake of constructing a well behaved estimator of the low-dimensional parameter of interest. Optimizing more than one of them for the sake of achieving a better bias-variance trade-off in the estimation of the parameter of interest is the core idea driving the general template of the collaborative targeted minimum loss-based estimation (C-TMLE) procedure. The original implementation/instantiation of the C-TMLE template can be presented as a greedy forward stepwise C-TMLE algorithm. It does not scale well when the number $p$ of covariates increases drastically. This motivates the introduction of a novel instantiation of the C-TMLE template where the covariates are pre-ordered. Its time complexity is $\mathcal{O}(p)$ as opposed to the original $\mathcal{O}(p^2)$, a remarkable gain. We propose two pre-ordering strategies and suggest a rule of thumb to develop other meaningful strategies. Because it is usually unclear a priori which pre-ordering strategy to choose, we also introduce another implementation/instantiation called SL-C-TMLE algorithm that enables the data-driven choice of the better pre-ordering strategy given the problem at hand. Its time complexity is $\mathcal{O}(p)$ as well. The computational burden and relative performance of these algorithms were compared in simulation studies involving fully synthetic data or partially synthetic data based on a real world large electronic health database; and in analyses of three real, large electronic health databases. In all analyses involving electronic health databases, the greedy C-TMLE algorithm is unacceptably slow. Simulation studies indicate our scalable C-TMLE and SL-C-TMLE algorithms work well.|['Cheng Ju', 'Susan Gruber', 'Samuel D. Lendle', 'Antoine Chambaz', 'Jessica M. Franklin', 'Richard Wyss', 'Sebastian Schneeweiss', 'Mark J. van der Laan']|['stat.CO', 'stat.ME']
2017-03-16T23:26:24Z|2017-03-07T02:14:38Z|http://arxiv.org/abs/1703.02177v1|http://arxiv.org/pdf/1703.02177v1|Mixtures of Generalized Hyperbolic Distributions and Mixtures of Skew-t   Distributions for Model-Based Clustering with Incomplete Data|Robust clustering from incomplete data is an important topic because, in many practical situations, real data sets are heavy-tailed, asymmetric, and/or have arbitrary patterns of missing observations. Flexible methods and algorithms for model-based clustering are presented via mixture of the generalized hyperbolic distributions and its limiting case, the mixture of multivariate skew-t distributions. An analytically feasible EM algorithm is formulated for parameter estimation and imputation of missing values for mixture models employing missing at random mechanisms. The proposed methodologies are investigated through a simulation study with varying proportions of synthetic missing values and illustrated using a real dataset. Comparisons are made with those obtained from the traditional mixture of generalized hyperbolic distribution counterparts by filling in the missing data using the mean imputation method.|['Yuhong Wei', 'Paul D. McNicholas']|['stat.ME', 'stat.CO']
2017-03-16T23:26:29Z|2017-03-06T23:48:50Z|http://arxiv.org/abs/1703.02151v1|http://arxiv.org/pdf/1703.02151v1|Computationally Efficient Simulation of Queues: The R Package   queuecomputer|Large networks of queueing systems model important real-world systems such as MapReduce clusters, web-servers, hospitals, call-centers and airport passenger terminals.To model such systems accurately we must infer queueing parameters from data. Unfortunately, for many queueing networks there is no clear way to proceed with parameter inference from data. Approximate Bayesian computation could offer a straight-forward way to infer parameters for such networks if we could simulate data quickly enough.   We present a computationally efficient method for simulating from a very general set of queueing networks with the R package queuecomputer. Remarkable speedups of more than 2 orders of magnitude are observed relative to the popular DES packages simmer and simpy. We replicate output from these packages to validate the package.   The package is modular and integrates well with the popular R package dplyr. Complex queueing networks with tandem, parallel and fork/join topologies can easily be built with these two packages together. We show how to use this package with two examples: a call-centre and an airport terminal.|['Anthony Ebert', 'Paul Wu', 'Kerrie Mengersen', 'Fabrizio Ruggeri']|['stat.CO', 'math.OC']
2017-03-16T23:26:29Z|2017-03-06T19:44:45Z|http://arxiv.org/abs/1703.02081v1|http://arxiv.org/pdf/1703.02081v1|Estimation and prediction in sparse and unbalanced tables|We consider the problem where we have a multi-way table of means, indexed by several factors, where each factor can have a large number of levels. The entry in each cell is the mean of some response, averaged over the observations falling into that cell. Some cells may be very sparsely populated, and in extreme cases, not populated at all. We might still like to estimate an expected response in such cells. We propose here a novel hierarchical ANOVA (HANOVA) representation for such data. Sparse cells will lean more on the lower-order interaction model for the data. These in turn could have components that are poorly represented in the data, in which case they rely on yet lower-order models. Our approach leads to a simple hierarchical algorithm, requiring repeated calculations of sub-table means of modified counts. The algorithm has shown superiority over the unshrinked methods in both simulations and real data sets.|['Qingyuan Zhao', 'Trevor Hastie', 'Daryl Pregibon']|['stat.CO']
2017-03-16T23:26:29Z|2017-03-04T21:50:25Z|http://arxiv.org/abs/1703.01526v1|http://arxiv.org/abs/1703.01526v1|High Accuracy Classification of Parkinson's Disease through Shape   Analysis and Surface Fitting in $^{123}$I-Ioflupane SPECT Imaging|Early and accurate identification of parkinsonian syndromes (PS) involving presynaptic degeneration from non-degenerative variants such as Scans Without Evidence of Dopaminergic Deficit (SWEDD) and tremor disorders, is important for effective patient management as the course, therapy and prognosis differ substantially between the two groups. In this study, we use Single Photon Emission Computed Tomography (SPECT) images from healthy normal, early PD and SWEDD subjects, as obtained from the Parkinson's Progression Markers Initiative (PPMI) database, and process them to compute shape- and surface fitting-based features for the three groups. We use these features to develop and compare various classification models that can discriminate between scans showing dopaminergic deficit, as in PD, from scans without the deficit, as in healthy normal or SWEDD. Along with it, we also compare these features with Striatal Binding Ratio (SBR)-based features, which are well-established and clinically used, by computing a feature importance score using Random forests technique. We observe that the Support Vector Machine (SVM) classifier gave the best performance with an accuracy of 97.29%. These features also showed higher importance than the SBR-based features. We infer from the study that shape analysis and surface fitting are useful and promising methods for extracting discriminatory features that can be used to develop diagnostic models that might have the potential to help clinicians in the diagnostic process.|['R. Prashanth', 'Sumantra Dutta Roy', 'Pravat K. Mandal', 'Shantanu Ghosh']|['stat.AP', 'cs.CV', 'physics.data-an', 'stat.CO', 'stat.ML']
2017-03-16T23:26:29Z|2017-03-04T09:12:42Z|http://arxiv.org/abs/1703.01421v1|http://arxiv.org/pdf/1703.01421v1|$l_0$-estimation of piecewise-constant signals on graphs|We study recovery of piecewise-constant signals over arbitrary graphs by the estimator minimizing an $l_0$-edge-penalized objective. Although exact minimization of this objective may be computationally intractable, we show that the same statistical risk guarantees are achieved by the alpha-expansion algorithm which approximately minimizes this objective in polynomial time. We establish that for graphs with small average vertex degree, these guarantees are rate-optimal in a minimax sense over classes of edge-sparse signals. For application to spatially inhomogeneous graphs, we propose minimization of an edge-weighted variant of this objective where each edge is weighted by its effective resistance or another measure of its contribution to the graph's connectivity. We establish minimax optimality of the resulting estimators over corresponding edge-weighted sparsity classes. We show theoretically that these risk guarantees are not always achieved by the estimator minimizing the $l_1$/total-variation relaxation, and empirically that the $l_0$-based estimates are more accurate in high signal-to-noise settings.|['Zhou Fan', 'Leying Guan']|['stat.ME', 'math.ST', 'stat.CO', 'stat.TH']
2017-03-16T23:26:29Z|2017-03-03T18:07:15Z|http://arxiv.org/abs/1703.01273v1|http://arxiv.org/pdf/1703.01273v1|Estimating Spatial Econometrics Models with Integrated Nested Laplace   Approximation|Integrated Nested Laplace Approximation provides a fast and effective method for marginal inference on Bayesian hierarchical models. This methodology has been implemented in the R-INLA package which permits INLA to be used from within R statistical software. Although INLA is implemented as a general methodology, its use in practice is limited to the models implemented in the R-INLA package.   Spatial autoregressive models are widely used in spatial econometrics but have until now been missing from the R-INLA package. In this paper, we describe the implementation and application of a new class of latent models in INLA made available through R-INLA. This new latent class implements a standard spatial lag model, which is widely used and that can be used to build more complex models in spatial econometrics.   The implementation of this latent model in R-INLA also means that all the other features of INLA can be used for model fitting, model selection and inference in spatial econometrics, as will be shown in this paper. Finally, we will illustrate the use of this new latent model and its applications with two datasets based on Gaussian and binary outcomes.|['Virgilio Gomez-Rubio', 'Roger S. Bivand', 'H√•vard Rue']|['stat.CO']
2017-03-16T23:26:29Z|2017-03-03T16:29:21Z|http://arxiv.org/abs/1703.01234v1|http://arxiv.org/pdf/1703.01234v1|A Bayesian computer model analysis of Robust Bayesian analyses|We harness the power of Bayesian emulation techniques, designed to aid the analysis of complex computer models, to examine the structure of complex Bayesian analyses themselves. These techniques facilitate robust Bayesian analyses and/or sensitivity analyses of complex problems, and hence allow global exploration of the impacts of choices made in both the likelihood and prior specification. We show how previously intractable problems in robustness studies can be overcome using emulation techniques, and how these methods allow other scientists to quickly extract approximations to posterior results corresponding to their own particular subjective specification. The utility and flexibility of our method is demonstrated on a reanalysis of a real application where Bayesian methods were employed to capture beliefs about river flow. We discuss the obvious extensions and directions of future research that such an approach opens up.|['Ian Vernon', 'John Paul Gosling']|['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']
2017-03-16T23:26:29Z|2017-03-03T10:44:47Z|http://arxiv.org/abs/1703.01106v1|http://arxiv.org/pdf/1703.01106v1|Differentially Private Bayesian Learning on Distributed Data|Many applications of machine learning, for example in health care, would benefit from methods that can guarantee privacy of data subjects. Differential privacy (DP) has become established as a standard for protecting learning results, but the proposed algorithms require a single trusted party to have access to the entire data, which is a clear weakness. We consider DP Bayesian learning in a distributed setting, where each party only holds a single sample or a few samples of the data. We propose a novel method for DP learning in this distributed setting, based on a secure multi-party sum function for aggregating summaries from the data holders. Each data holder adds their share of Gaussian noise to make the total computation differentially private using the Gaussian mechanism. We prove that the system can be made secure against a desired number of colluding data owners and robust against faulting data owners. The method builds on an asymptotically optimal and practically efficient DP Bayesian inference with rapidly diminishing extra cost.|['Mikko Heikkil√§', 'Yusuke Okimoto', 'Samuel Kaski', 'Kana Shimizu', 'Antti Honkela']|['stat.ML', 'cs.CR', 'cs.LG', 'stat.CO']
2017-03-16T23:26:29Z|2017-03-02T17:33:58Z|http://arxiv.org/abs/1703.00864v1|http://arxiv.org/pdf/1703.00864v1|The Unreasonable Effectiveness of Random Orthogonal Embeddings|We present a general class of embeddings based on structured random matrices with orthogonal rows which can be applied in many machine learning applications including dimensionality reduction, kernel approximation and locality-sensitive hashing. We show that this class yields improvements over previous state-of-the-art methods either in computational efficiency (while providing similar accuracy) or in accuracy, or both. In particular, we propose the \textit{Orthogonal Johnson-Lindenstrauss Transform} (OJLT) which is as fast as earlier methods yet provably outperforms them in terms of accuracy, leading to a `free lunch' improvement over previous dimensionality reduction mechanisms. We introduce matrices with complex entries that further improve accuracy. Other applications include estimators for certain pointwise nonlinear Gaussian kernels, and speed improvements for approximate nearest-neighbor search in massive datasets with high-dimensional feature vectors.|['Krzysztof Choromanski', 'Mark Rowland', 'Adrian Weller']|['stat.ML', 'stat.CO']
2017-03-16T23:26:29Z|2017-03-01T16:29:12Z|http://arxiv.org/abs/1703.00368v1|http://arxiv.org/pdf/1703.00368v1|Approximate Computational Approaches for Bayesian Sensor Placement in   High Dimensions|Since the cost of installing and maintaining sensors is usually high, sensor locations are always strategically selected. For those aiming at inferring certain quantities of interest (QoI), it is desirable to explore the dependency between sensor measurements and QoI. One of the most popular metric for the dependency is mutual information which naturally measures how much information about one variable can be obtained given the other. However, computing mutual information is always challenging, and the result is unreliable in high dimension. In this paper, we propose an approach to find an approximate lower bound of mutual information and compute it in a lower dimension. Then, sensors are placed where highest mutual information (lower bound) is achieved and QoI is inferred via Bayes rule given sensor measurements. In addition, Bayesian optimization is introduced to provide a continuous mutual information surface over the domain and thus reduce the number of evaluations. A chemical release accident is simulated where multiple sensors are placed to locate the source of the release. The result shows that the proposed approach is both effective and efficient in inferring QoI.|['Xiao Lin', 'Asif Chowdhury', 'Xiaofan Wang', 'Gabriel Terejanu']|['stat.CO']
2017-03-16T23:26:29Z|2017-03-13T17:28:04Z|http://arxiv.org/abs/1702.08896v2|http://arxiv.org/pdf/1702.08896v2|Deep and Hierarchical Implicit Models|Implicit probabilistic models are a flexible class for modeling data. They define a process to simulate observations, and unlike traditional models, they do not require a tractable likelihood function. In this paper, we develop two families of models: hierarchical implicit models and deep implicit models. They combine the idea of implicit densities with hierarchical Bayesian modeling and deep neural networks. The use of implicit models with Bayesian analysis has been limited by our ability to perform accurate and scalable inference. We develop likelihood-free variational inference (LFVI). Key to LFVI is specifying a variational family that is also implicit. This matches the model's flexibility and allows for accurate approximation of the posterior. Our work scales up implicit models to sizes previously not possible and advances their modeling design. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for text generation.|['Dustin Tran', 'Rajesh Ranganath', 'David M. Blei']|['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']
2017-03-16T23:26:32Z|2017-02-28T16:31:21Z|http://arxiv.org/abs/1702.08849v1|http://arxiv.org/pdf/1702.08849v1|Multi-Sensor Multi-object Tracking with the Generalized Labeled   Multi-Bernoulli Filter|This paper proposes an efficient implementation of the multi-sensor generalized labeled multi-Bernoulli (GLMB) filter. The solution exploits the GLMB joint prediction and update together with a new technique for truncating the GLMB filtering density based on Gibbs sampling. The resulting algorithm has quadratic complexity in the number of hypothesized object and linear in the number of measurements of each individual sensors.|['Ba Ngu Vo', 'Ba Tuong Vo']|['stat.CO']
2017-03-16T23:26:32Z|2017-02-28T13:34:02Z|http://arxiv.org/abs/1702.08781v1|http://arxiv.org/pdf/1702.08781v1|General Bayesian inference schemes in infinite mixture models|Bayesian statistical models allow us to formalise our knowledge about the world and reason about our uncertainty, but there is a need for better procedures to accurately encode its complexity. One way to do so is through compositional models, which are formed by combining blocks consisting of simpler models. One can increase the complexity of the compositional model by either stacking more blocks or by using a not-so-simple model as a building block. This thesis is an example of the latter. One first aim is to expand the choice of Bayesian nonparametric (BNP) blocks for constructing tractable compositional models. So far, most of the models that have a Bayesian nonparametric component use a Dirichlet Process or a Pitman-Yor process because of the availability of tractable and compact representations. This thesis shows how to overcome certain intractabilities in order to obtain analogous compact representations for the class of Poisson-Kingman priors which includes the Dirichlet and Pitman-Yor processes.   A major impediment to the widespread use of Bayesian nonparametric building blocks is that inference is often costly, intractable or difficult to carry out. This is an active research area since dealing with the model's infinite dimensional component forbids the direct use of standard simulation-based methods. The main contribution of this thesis is a variety of inference schemes that tackle this problem: Markov chain Monte Carlo and Sequential Monte Carlo methods, which are exact inference schemes since they target the true posterior. The contributions of this thesis, in a larger context, provide general purpose exact inference schemes in the flavour or probabilistic programming: the user is able to choose from a variety of models, focusing only on the modelling part. Indeed, if the wide enough class of Poisson-Kingman priors is used as one of our blocks, this objective is achieved.|['Maria Lomeli']|['stat.CO']
2017-03-16T23:26:32Z|2017-02-28T11:02:04Z|http://arxiv.org/abs/1702.08738v1|http://arxiv.org/pdf/1702.08738v1|Efficient simulation of high dimensional Gaussian vectors|We describe a Markov chain Monte Carlo method to approximately simulate a centered d-dimensional Gaussian vector X with given covariance matrix. The standard Monte Carlo method is based on the Cholesky decomposition, which takes cubic time and has quadratic storage cost in d. In contrast, the storage cost of our algorithm is linear in d. We give a bound on the quadractic Wasserstein distance between the distribution of our sample and the target distribution. Our method can be used to estimate the expectation of h(X), where h is a real-valued function of d variables. Under certain conditions, we show that the mean square error of our method is inversely proportional to its running time. We also prove that, under suitable conditions, our method is faster than the standard Monte Carlo method by a factor nearly proportional to d. A numerical example is given.|['Nabil Kahale']|['stat.CO', '60J22, 65C40']
2017-03-16T23:26:32Z|2017-02-27T22:52:29Z|http://arxiv.org/abs/1702.08572v1|http://arxiv.org/pdf/1702.08572v1|Comparison of Confidence Interval Estimators: an Index Approach|We develop a confidence interval index for comparing confidence interval estimators based on the confidence interval length and coverage probability. We show that the confidence interval index has range of values within the neighborhood of the range of the coverage probability, [0,1]. In addition, a good confidence interval estimator is shown to have an index value approaching 1; and a bad confidence interval has an index value approaching 0. A simulation study is conducted to assess the finite sample performance of the index. Finally, the proposed index is illustrated with a practical example from the literature.|['Richard Minkah', 'Tertius de Wet']|['stat.ME', 'stat.CO', '62F99, 62G99']
2017-03-16T23:26:32Z|2017-02-27T17:43:59Z|http://arxiv.org/abs/1702.08397v1|http://arxiv.org/pdf/1702.08397v1|Forward Event-Chain Monte Carlo: a general rejection-free and   irreversible Markov chain simulation method|This paper considers Event-Chain Monte Carlo simulation schemes in order to design an original irreversible Markov Chain Monte Carlo (MCMC) algorithm for the sampling of complex statistical models. The functioning principles of MCMC sampling methods are firstly recalled, as well as standard Event-Chain Monte Carlo simulation schemes are described. Then, a Forward Event-Chain Monte Carlo sampling methodology is proposed and introduced. This nonreversible MCMC rejection-free simulation algorithm is tested and run for the sampling of high-dimensional ill-conditioned Gaussian statistical distributions. Numerical experiments demonstrate the efficiency of the proposed approach, compared to standard Event-Chain and standard Monte Carlo sampling methods. Accelerations up to several magnitudes are exhibited.|['Manon Michel', 'St√©phane S√©n√©cal']|['stat.CO']
2017-03-16T23:26:32Z|2017-02-27T12:12:46Z|http://arxiv.org/abs/1702.08251v1|http://arxiv.org/pdf/1702.08251v1|Hessian corrections to Hybrid Monte Carlo|A method for the introduction of second-order derivatives of the log likelihood into HMC algorithms is introduced, which does not require the Hessian to be evaluated at each leapfrog step but only at the start and end of trajectories.|['Thomas House']|['stat.CO']
2017-03-16T23:26:32Z|2017-02-27T12:03:01Z|http://arxiv.org/abs/1702.08248v1|http://arxiv.org/pdf/1702.08248v1|Scalable and Distributed Clustering via Lightweight Coresets|Coresets are compact representations of data sets such that models trained on a coreset are provably competitive with models trained on the full data set. As such, they have been successfully used to scale up clustering models to massive data sets. While existing approaches generally only allow for multiplicative approximation errors, we propose a novel notion of coresets called lightweight coresets that allows for both multiplicative and additive errors. We provide a single algorithm to construct light-weight coresets for k-Means clustering, Bregman clustering and maximum likelihood estimation of Gaussian mixture models. The algorithm is substantially faster than existing constructions, embarrassingly parallel and resulting coresets are smaller. In an extensive experimental evaluation, we demonstrate that the proposed method outperforms existing coreset constructions.|['Olivier Bachem', 'Mario Lucic', 'Andreas Krause']|['stat.ML', 'cs.DC', 'cs.DS', 'cs.LG', 'stat.CO']
2017-03-16T23:26:32Z|2017-02-27T08:42:49Z|http://arxiv.org/abs/1702.08188v1|http://arxiv.org/pdf/1702.08188v1|dotCall64: An Efficient Interface to Compiled C/C++ and Fortran Code   Supporting Long Vectors|The R functions .C() and .Fortran() can be used to call compiled C/C++ and Fortran code from R. This so-called foreign function interface is convenient, since it does not require any interactions with the C API of R. However, it does not support long vectors (i.e., vectors of more than 2^31 elements). To overcome this limitation, the R package dotCall64 provides .C64(), which can be used to call compiled C/C++ and Fortran functions. It transparently supports long vectors and does the necessary castings to pass numeric R vectors to 64-bit integer arguments of the compiled code. Moreover, .C64() features a mechanism to avoid unnecessary copies of function arguments, making it efficient in terms of speed and memory usage.|['Florian Gerber', 'Kaspar M√∂singer', 'Reinhard Furrer']|['stat.CO']
2017-03-16T23:26:32Z|2017-02-27T08:33:26Z|http://arxiv.org/abs/1702.08185v1|http://arxiv.org/pdf/1702.08185v1|An update on statistical boosting in biomedicine|Statistical boosting algorithms have triggered a lot of research during the last decade. They combine a powerful machine-learning approach with classical statistical modelling, offering various practical advantages like automated variable selection and implicit regularization of effect estimates. They are extremely flexible, as the underlying base-learners (regression functions defining the type of effect for the explanatory variables) can be combined with any kind of loss function (target function to be optimized, defining the type of regression setting). In this review article, we highlight the most recent methodological developments on statistical boosting regarding variable selection, functional regression and advanced time-to-event modelling. Additionally, we provide a short overview on relevant applications of statistical boosting in biomedicine.|['Andreas Mayr', 'Benjamin Hofner', 'Elisabeth Waldmann', 'Tobias Hepp', 'Olaf Gefeller', 'Matthias Schmid']|['stat.AP', 'stat.CO', 'stat.ML']
2017-03-16T23:26:32Z|2017-02-27T04:17:36Z|http://arxiv.org/abs/1702.08140v1|http://arxiv.org/pdf/1702.08140v1|A mixture model approach to infer land-use influence on point referenced   water quality|The assessment of water quality across space and time is of considerable interest for both agricultural and public health reasons. The standard method to assess the water quality of a catchment, or a group of catchments, usually involves collecting point measurements of water quality and other additional information such as the date and time of measurements, rainfall amounts, the land-use and soil-type of the catchment and the elevation. Some of this auxiliary information will be point data, measured at the exact location, whereas other such as land-use will be areal data often in a compositional format. Two problems arise if analysts try to incorporate this information into a statistical model in order to predict (for example) the influence of land-use on water quality. First is the spatial change of support problem that arises when using areal data to predict outcomes at point locations. Secondly, the physical process driving water quality is not compositional, rather it is the observation process that provides compositional data. In this paper we present an approach that accounts for these two issues by using a latent variable to identify the land-use that most likely influences water quality. This latent variable is used in a spatial mixture model to help estimate the influence of land-use on water quality. We demonstrate the potential of this approach with data from a water quality research study in the Mount Lofty range, in South Australia.|['Adrien Ickowicz', 'Jessica H. Ford', 'Keith R. Hayes']|['stat.AP', 'stat.CO']
2017-03-16T23:26:36Z|2017-02-28T09:59:57Z|http://arxiv.org/abs/1702.08061v2|http://arxiv.org/pdf/1702.08061v2|The Ensemble Kalman Filter: A Signal Processing Perspective|The ensemble Kalman filter (EnKF) is a Monte Carlo based implementation of the Kalman filter (KF) for extremely high-dimensional, possibly nonlinear and non-Gaussian state estimation problems. Its ability to handle state dimensions in the order of millions has made the EnKF a popular algorithm in different geoscientific disciplines. Despite a similarly vital need for scalable algorithms in signal processing, e.g., to make sense of the ever increasing amount of sensor data, the EnKF is hardly discussed in our field.   This self-contained review paper is aimed at signal processing researchers and provides all the knowledge to get started with the EnKF. The algorithm is derived in a KF framework, without the often encountered geoscientific terminology. Algorithmic challenges and required extensions of the EnKF are provided, as well as relations to sigma-point KF and particle filters. The relevant EnKF literature is summarized in an extensive survey and unique simulation examples, including popular benchmark problems, complement the theory with practical insights. The signal processing perspective highlights new directions of research and facilitates the exchange of potentially beneficial ideas, both for the EnKF and high-dimensional nonlinear and non-Gaussian filtering in general.|['Michael Roth', 'Gustaf Hendeby', 'Carsten Fritsche', 'Fredrik Gustafsson']|['stat.ME', 'cs.SY', 'stat.CO']
2017-03-16T23:26:36Z|2017-02-26T17:50:02Z|http://arxiv.org/abs/1702.08446v1|http://arxiv.org/pdf/1702.08446v1|Monte Carlo on manifolds: sampling densities and integrating functions|We describe and analyze some Monte Carlo methods for manifolds in Euclidean space defined by equality and inequality constraints. First, we give an MCMC sampler for probability distributions defined by un-normalized densities on such manifolds. The sampler uses a specific orthogonal projection to the surface that requires only information about the tangent space to the manifold, obtainable from first derivatives of the constraint functions, hence avoiding the need for curvature information or second derivatives. Second, we use the sampler to develop a multi-stage algorithm to compute integrals over such manifolds. We provide single-run error estimates that avoid the need for multiple independent runs. Computational experiments on various test problems show that the algorithms and error estimates work in practice. The method is applied to compute the entropies of different sticky hard sphere systems. These predict the temperature or interaction energy at which loops of hard sticky spheres become preferable to chains.|['Emilio Zappa', 'Miranda Holmes-Cerfon', 'Jonathan Goodman']|['math.NA', 'cond-mat.stat-mech', 'stat.CO']
2017-03-16T23:26:36Z|2017-02-25T17:47:33Z|http://arxiv.org/abs/1702.07930v1|http://arxiv.org/pdf/1702.07930v1|Upper-Bounding the Regularization Constant for Convex Sparse Signal   Reconstruction|Consider reconstructing a signal $x$ by minimizing a weighted sum of a convex differentiable negative log-likelihood (NLL) (data-fidelity) term and a convex regularization term that imposes a convex-set constraint on $x$ and enforces its sparsity using $\ell_1$-norm analysis regularization. We compute upper bounds on the regularization tuning constant beyond which the regularization term overwhelmingly dominates the NLL term so that the set of minimum points of the objective function does not change. Necessary and sufficient conditions for irrelevance of sparse signal regularization and a condition for the existence of finite upper bounds are established. We formulate an optimization problem for finding these bounds when the regularization term can be globally minimized by a feasible $x$ and also develop an alternating direction method of multipliers (ADMM) type method for their computation. Simulation examples show that the derived and empirical bounds match.|['Renliang Gu', 'Aleksandar Dogand≈æiƒá']|['stat.CO', 'math.OC']
2017-03-16T23:26:36Z|2017-02-25T03:46:20Z|http://arxiv.org/abs/1702.07830v1|http://arxiv.org/pdf/1702.07830v1|A Near-Optimal Sampling Strategy for Sparse Recovery of Polynomial Chaos   Expansions|Compressive sampling has become a widely used approach to construct polynomial chaos surrogates when the number of available simulation samples is limited. Originally, these expensive simulation samples would be obtained at random locations in the parameter space. It was later shown that the choice of sample locations could significantly impact the accuracy of resulting surrogates. This motivated new sampling strategies or design-of-experiment approaches, such as coherence-optimal sampling, which aim at improving the coherence property. In this paper, we propose a sampling strategy that can identify near-optimal sample locations that lead to improvement in local-coherence property and also enhancement of cross-correlation properties of measurement matrices. We provide theoretical motivations for the proposed sampling strategy along with several numerical examples that show that our near-optimal sampling strategy produces substantially more accurate results, compared to other sampling strategies.|['Negin Alemazkoor', 'Hadi Meidani']|['stat.CO']
2017-03-16T23:26:36Z|2017-02-24T17:54:23Z|http://arxiv.org/abs/1702.07685v1|http://arxiv.org/pdf/1702.07685v1|ROPE: high-dimensional network modeling with robust control of edge FDR|Network modeling has become increasingly popular for analyzing genomic data, to aid in the interpretation and discovery of possible mechanistic components and therapeutic targets. However, genomic-scale networks are high-dimensional models and are usually estimated from a relatively small number of samples. Therefore, their usefulness is hampered by estimation instability. In addition, the complexity of the models is controlled by one or more penalization (tuning) parameters where small changes to these can lead to vastly different networks, thus making interpretation of models difficult. This necessitates the development of techniques to produce robust network models accompanied by estimation quality assessments.   We introduce Resampling of Penalized Estimates (ROPE): a novel statistical method for robust network modeling. The method utilizes resampling-based network estimation and integrates results from several levels of penalization through a constrained, over-dispersed beta-binomial mixture model. ROPE provides robust False Discovery Rate (FDR) control of network estimates and each edge is assigned a measure of validity, the q-value, corresponding to the FDR-level for which the edge would be included in the network model. We apply ROPE to several simulated data sets as well as genomic data from The Cancer Genome Atlas. We show that ROPE outperforms state-of-the-art methods in terms of FDR control and robust performance across data sets. We illustrate how to use ROPE to make a principled model selection for which genomic associations to study further. ROPE is available as an R package on CRAN.|['Jonatan Kallus', 'Jose Sanchez', 'Alexandra Jauhiainen', 'Sven Nelander', 'Rebecka J√∂rnsten']|['stat.CO']
2017-03-16T23:26:36Z|2017-02-24T17:01:59Z|http://arxiv.org/abs/1702.07662v1|http://arxiv.org/pdf/1702.07662v1|A Network Epidemic Model for Online Community Commissioning Data|Statistical models for network epidemics usually assume a Bernoulli random graph, in which any two nodes have the same probability of being connected. This assumption provides computational simplicity but does not describe real-life networks well. We propose an epidemic model based on the preferential attachment model, which adds nodes sequentially by simple rules to generate a network. A simulation study based on the subsequent Markov Chain Monte Carlo algorithm reveals an identifiability issue with the model parameters, so an alternative parameterisation is suggested. Finally, the model is applied to a set of online commissioning data.|['Clement Lee', 'Andrew Garbett', 'Darren J. Wilkinson']|['stat.CO', 'cs.SI', 'stat.ME']
2017-03-16T23:26:36Z|2017-02-23T21:37:06Z|http://arxiv.org/abs/1702.07400v1|http://arxiv.org/pdf/1702.07400v1|Horseshoe Regularization for Feature Subset Selection|Feature subset selection arises in many high-dimensional applications in machine learning and statistics, such as compressed sensing and genomics. The $\ell_0$ penalty is ideal for this task, the caveat being it requires the NP-hard combinatorial evaluation of all models. A recent area of considerable interest is to develop efficient algorithms to fit models with a non-convex $\ell_\gamma$ penalty for $\gamma\in (0,1)$, which results in sparser models than the convex $\ell_1$ or lasso penalty, but is harder to fit. We propose an alternative, termed the horseshoe regularization penalty for feature subset selection, and demonstrate its theoretical and computational advantages. The distinguishing feature from existing non-convex optimization approaches is a full probabilistic representation of the penalty as the negative of the logarithm of a suitable prior, which in turn enables an efficient expectation-maximization algorithm for optimization and MCMC for uncertainty quantification. In synthetic and real data, the resulting algorithm provides better statistical performance, and the computation requires a fraction of time of state of the art non-convex solvers.|['Anindya Bhadra', 'Jyotishka Datta', 'Nicholas G. Polson', 'Brandon Willard']|['stat.ML', 'stat.CO']
2017-03-16T23:26:36Z|2017-02-23T04:40:41Z|http://arxiv.org/abs/1702.07094v1|http://arxiv.org/pdf/1702.07094v1|BigVAR: Tools for Modeling Sparse High-Dimensional Multivariate Time   Series|The R package BigVAR allows for the simultaneous estimation of high-dimensional time series by applying structured penalties to the conventional vector autoregression (VAR) and vector autoregression with exogenous variables (VARX) frameworks. Our methods can be utilized in many forecasting applications that make use of time-dependent data such as macroeconomics, finance, and internet traffic. Our package extends solution algorithms from the machine learning and signal processing literatures to a time dependent setting: selecting the regularization parameter by sequential cross validation and provides substantial improvements in forecasting performance over conventional methods. We offer a user-friendly interface that utilizes R's s4 object class structure which makes our methodology easily accessible to practicioners.   In this paper, we present an overview of our notation, the models that comprise BigVAR, and the functionality of our package with a detailed example using publicly available macroeconomic data. In addition, we present a simulation study comparing the performance of several procedures that refit the support selected by a BigVAR procedure according to several variants of least squares and conclude that refitting generally degrades forecast performance.|['William Nicholson', 'David Matteson', 'Jacob Bien']|['stat.CO']
2017-03-16T23:26:36Z|2017-02-22T00:43:01Z|http://arxiv.org/abs/1702.06632v1|http://arxiv.org/pdf/1702.06632v1|A Balanced Algorithm for Sampling Abstract Simplicial Complexes|We provide an algorithm for sampling the space of abstract simplicial complexes on a fixed number of vertices that aims to provide a balanced sampling over non-isomorphic complexes. Although sampling uniformly from geometrically distinct complexes is a difficult task with no known analytic algorithm, our generative and descriptive algorithm is designed with heuristics to help balance the combinatorial multiplicities of the states and more widely sample across the space of inequivalent configurations. We provide a formula for the exact probabilities with which this algorithm will produce a requested labeled state, and compare the algorithm to Kahle's multi-parameter model of exponential random simplicial complexes, demonstrating analytically that our algorithm performs better with respect to worst-case probability bounds on a given complex and providing numerical results illustrating the increased sampling efficiency over distinct classes.|['John Lombard']|['stat.CO', 'math.CO', 'math.PR']
2017-03-16T23:26:36Z|2017-02-23T19:01:53Z|http://arxiv.org/abs/1702.06488v2|http://arxiv.org/pdf/1702.06488v2|Distributed Estimation of Principal Eigenspaces|"Principal component analysis (PCA) is fundamental to statistical machine learning. It extracts latent principal factors that contribute to the most variation of the data. When data are stored across multiple machines, however, communication cost can prohibit the computation of PCA in a central location and distributed algorithms for PCA are thus needed. This paper proposes and studies a distributed PCA algorithm: each node machine computes the top $K$ eigenvectors and transmits them to the central server; the central server then aggregates the information from all the node machines and conducts a PCA based on the aggregated information. We investigate the bias and variance for the resulting distributed estimator of the top $K$ eigenvectors. In particular, we show that for distributions with symmetric innovation, the distributed PCA is ""unbiased"". We derive the rate of convergence for distributed PCA estimators, which depends explicitly on the effective rank of covariance, eigen-gap, and the number of machines. We show that when the number of machines is not unreasonably large, the distributed PCA performs as well as the whole sample PCA, even without full access of whole data. The theoretical results are verified by an extensive simulation study. We also extend our analysis to the heterogeneous case where the population covariance matrices are different across local machines but share similar top eigen-structures."|['Jianqing Fan', 'Dong Wang', 'Kaizheng Wang', 'Ziwei Zhu']|['stat.CO', 'math.ST', 'stat.TH']
2017-03-16T23:26:41Z|2017-02-23T14:07:34Z|http://arxiv.org/abs/1702.06407v2|http://arxiv.org/pdf/1702.06407v2|General Semiparametric Shared Frailty Model Estimation and Simulation   with frailtySurv|The R package frailtySurv for simulating and fitting semi-parametric shared frailty models is introduced. frailtySurv implements semi-parametric consistent estimators for a variety of frailty distributions, including gamma, log-normal, inverse Gaussian and power variance function, and provides consistent estimators of the standard errors of the parameters' estimators. The parameters' estimators are asymptotically normally distributed, and therefore statistical inference based on the results of this package, such as hypothesis testing and confidence intervals, can be performed using the normal distribution. Extensive simulations demonstrate the flexibility and correct implementation of the estimator. Two case studies performed with publicly-available datasets demonstrate applicability of the package. In the Diabetic Retinopathy Study, the onset of blindness is clustered by patient, and in a large hard drive failure dataset, failure times are thought to be clustered by the hard drive manufacturer and model.|['John V. Monaco', 'Malka Gorfine', 'Li Hsu']|['stat.CO', 'cs.MS']
2017-03-16T23:26:41Z|2017-02-19T04:08:18Z|http://arxiv.org/abs/1702.05698v1|http://arxiv.org/pdf/1702.05698v1|Online Robust Principal Component Analysis with Change Point Detection|Robust PCA methods are typically batch algorithms which requires loading all observations into memory before processing. This makes them inefficient to process big data. In this paper, we develop an efficient online robust principal component methods, namely online moving window robust principal component analysis (OMWRPCA). Unlike existing algorithms, OMWRPCA can successfully track not only slowly changing subspace but also abruptly changed subspace. By embedding hypothesis testing into the algorithm, OMWRPCA can detect change points of the underlying subspaces. Extensive simulation studies demonstrate the superior performance of OMWRPCA comparing with other state-of-art approach. We also apply the algorithm for real-time background subtraction of surveillance video.|['Wei Xiao', 'Xiaolin Huang', 'Jorge Silva', 'Saba Emrani', 'Arin Chaudhuri']|['cs.LG', 'cs.CV', 'stat.AP', 'stat.CO', 'stat.ML']
2017-03-16T23:26:41Z|2017-02-18T00:04:25Z|http://arxiv.org/abs/1702.05546v1|http://arxiv.org/pdf/1702.05546v1|A Sequential Scheme for Large Scale Bayesian Multiple Testing|The problem of large scale multiple testing arises in many contexts, including testing for pairwise interaction among large numbers of neurons. With advances in technologies, it has become common to record from hundreds of neurons simultaneously, and this number is growing quickly, so that the number of pairwise tests can be very large. It is important to control the rate at which false positives occur. In addition, there is sometimes information that affects the probability of a positive result for any given pair. In the case of neurons, they are more likely to have correlated activity when they are close together, and when they respond similarly to various stimuli. Recently a method was developed to control false positives when covariate information, such as distances between pairs of neurons, is available. This method, however, relies on computationally-intensive Markov Chain Monte Carlo (MCMC). Here we develop an alternative, based on Sequential Monte Carlo, which scales well with the size of the dataset. This scheme considers data items sequentially, with relevant probabilities being updated at each step. Simulation experiments demonstrate that the proposed algorithm delivers results as accurately as the previous MCMC method with only a single pass through the data. We illustrate the method by using it to analyze neural recordings from extrastriate cortex in a macaque monkey. The scripts that implement the proposed algorithm with a synthetic dataset are available online at: https://github.com/robinlau1981/SMC_Multi_Testing.|['Bin Liu', 'Giuseppe Vinci', 'Adam C. Snyder', 'Matthew A. Smith', 'Robert E. Kass']|['stat.CO']
2017-03-16T23:26:41Z|2017-02-17T21:16:40Z|http://arxiv.org/abs/1702.05518v1|http://arxiv.org/pdf/1702.05518v1|Sampling Strategies for Fast Updating of Gaussian Markov Random Fields|Gaussian Markov random fields (GMRFs) are popular for modeling temporal or spatial dependence in large areal datasets due to their ease of interpretation and computational convenience afforded by conditional independence and their sparse precision matrices needed for random variable generation. Using such models inside a Markov chain Monte Carlo algorithm requires repeatedly simulating random fields. This is a nontrivial issue, especially when the full conditional precision matrix depends on parameters that change at each iteration. Typically in Bayesian computation, GMRFs are updated jointly in a block Gibbs sampler or one location at a time in a single-site sampler. The former approach leads to quicker convergence by updating correlated variables all at once, while the latter avoids solving large matrices. Efficient algorithms for sampling Markov random fields have become the focus of much recent research in the machine learning literature, much of which can be useful to statisticians. We briefly review recently proposed approaches with an eye toward implementation for statisticians without expertise in numerical analysis or advanced computing. In particular, we consider a version of block sampling in which the underlying graph can be cut so that conditionally independent sites are all updated together. This algorithm allows a practitioner to parallelize the updating of a subset locations or to take advantage of `vectorized' calculations in a high-level language such as R. Through both simulation and real data application, we demonstrate computational savings that can be achieved versus both traditional single-site updating and block updating, regardless of whether the data are on a regular or irregular lattice. We argue that this easily-implemented sampling routine provides a good compromise between statistical and computational efficiency when working with large datasets.|['D. Andrew Brown', 'Christopher S. McMahan']|['stat.CO']
2017-03-16T23:26:41Z|2017-02-17T18:06:27Z|http://arxiv.org/abs/1702.05462v1|http://arxiv.org/pdf/1702.05462v1|Objective Bayesian Analysis for Change Point Problems|In this paper we present an objective approach to change point analysis. In particular, we look at the problem from two perspectives. The first focuses on the definition of an objective prior when the number of change points is known a priori. The second contribution aims to estimate the number of change points by using an objective approach, recently introduced in the literature, based on losses. The latter considers change point estimation as a model selection exercise. We show the performance of the proposed approach on simulated data and on real data sets.|['Laurentiu Hinoveanu', 'Fabrizio Leisen', 'Cristiano Villa']|['stat.ME', 'math.ST', 'stat.AP', 'stat.CO', 'stat.ML', 'stat.TH']
2017-03-16T23:26:41Z|2017-02-15T11:52:14Z|http://arxiv.org/abs/1702.04561v1|http://arxiv.org/pdf/1702.04561v1|Probing for sparse and fast variable selection with model-based boosting|We present a new variable selection method based on model-based gradient boosting and randomly permuted variables. Model-based boosting is a tool to fit a statistical model while performing variable selection at the same time. A drawback of the fitting lies in the need of multiple model fits on slightly altered data (e.g. cross-validation or bootstrap) to find the optimal number of boosting iterations and prevent overfitting. In our proposed approach, we augment the data set with randomly permuted versions of the true variables, so called shadow variables, and stop the step-wise fitting as soon as such a variable would be added to the model. This allows variable selection in a single fit of the model without requiring further parameter tuning. We show that our probing approach can compete with state-of-the-art selection methods like stability selection in a high-dimensional classification benchmark and apply it on gene expression data for the estimation of riboflavin production of Bacillus subtilis.|['Janek Thomas', 'Tobias Hepp', 'Andreas Mayr', 'Bernd Bischl']|['stat.ML', 'stat.CO']
2017-03-16T23:26:41Z|2017-02-14T21:20:23Z|http://arxiv.org/abs/1702.04391v1|http://arxiv.org/pdf/1702.04391v1|Bootstrap-based inferential improvements in beta autoregressive moving   average model|We consider the issue of performing accurate small sample inference in beta autoregressive moving average model, which is useful for modeling and forecasting continuous variables that assumes values in the interval $(0,1)$. The inferences based on conditional maximum likelihood estimation have good asymptotic properties, but their performances in small samples may be poor. This way, we propose bootstrap bias corrections of the point estimators and different bootstrap strategies for confidence interval improvements. Our Monte Carlo simulations show that finite sample inference based on bootstrap corrections is much more reliable than the usual inferences. We also presented an empirical application.|['Bruna Gregory Palm', 'F√°bio M. Bayer']|['stat.CO']
2017-03-16T23:26:41Z|2017-02-13T17:23:02Z|http://arxiv.org/abs/1702.03891v1|http://arxiv.org/pdf/1702.03891v1|Spatial Models with the Integrated Nested Laplace Approximation within   Markov Chain Monte Carlo|The Integrated Nested Laplace Approximation (INLA) is a convenient way to obtain approximations to the posterior marginals for parameters in Bayesian hierarchical models when the latent effects can be expressed as a Gaussian Markov Random Field (GMRF). In addition, its implementation in the R-INLA package for the R statistical software provides an easy way to fit models using INLA in practice. R-INLA implements a number of widely used latent models, including several spatial models. In addition, R-INLA can fit models in a fraction of the time than other computer intensive methods (e.g. Markov Chain Monte Carlo) take to fit the same model.   Although INLA provides a fast approximation to the marginals of the model parameters, it is difficult to use it with models not implemented in R-INLA. It is also difficult to make multivariate posterior inference on the parameters of the model as INLA focuses on the posterior marginals and not the joint posterior distribution.   In this paper we describe how to use INLA within the Metropolis-Hastings algorithm to fit spatial models and estimate the joint posterior distribution of a reduced number of parameters. We will illustrate the benefits of this new method with two examples on spatial econometrics and disease mapping where complex spatial models with several spatial structures need to be fitted.|['Virgilio G√≥mez-Rubio', 'Francisco Palm√≠-Perales']|['stat.CO']
2017-03-16T23:26:41Z|2017-02-13T08:52:58Z|http://arxiv.org/abs/1702.03673v1|http://arxiv.org/pdf/1702.03673v1|Bayesian Probabilistic Numerical Methods|The emergent field of probabilistic numerics has thus far lacked rigorous statistical principals. This paper establishes Bayesian probabilistic numerical methods as those which can be cast as solutions to certain Bayesian inverse problems, albeit problems that are non-standard. This allows us to establish general conditions under which Bayesian probabilistic numerical methods are well-defined, encompassing both non-linear and non-Gaussian models. For general computation, a numerical approximation scheme is developed and its asymptotic convergence is established. The theoretical development is then extended to pipelines of computation, wherein probabilistic numerical methods are composed to solve more challenging numerical tasks. The contribution highlights an important research frontier at the interface of numerical analysis and uncertainty quantification, with some illustrative applications presented.|['Jon Cockayne', 'Chris Oates', 'Tim Sullivan', 'Mark Girolami']|['stat.ME', 'cs.NA', 'math.NA', 'math.ST', 'stat.CO', 'stat.TH']
2017-03-16T23:26:41Z|2017-02-10T12:26:52Z|http://arxiv.org/abs/1702.03146v1|http://arxiv.org/pdf/1702.03146v1|Analysis of a nonlinear importance sampling scheme for Bayesian   parameter estimation in state-space models|The Bayesian estimation of the unknown parameters of state-space (dynamical) systems has received considerable attention over the past decade, with a handful of powerful algorithms being introduced. In this paper we tackle the theoretical analysis of the recently proposed {\it nonlinear} population Monte Carlo (NPMC). This is an iterative importance sampling scheme whose key features, compared to conventional importance samplers, are (i) the approximate computation of the importance weights (IWs) assigned to the Monte Carlo samples and (ii) the nonlinear transformation of these IWs in order to prevent the degeneracy problem that flaws the performance of conventional importance samplers. The contribution of the present paper is a rigorous proof of convergence of the nonlinear IS (NIS) scheme as the number of Monte Carlo samples, $M$, increases. Our analysis reveals that the NIS approximation errors converge to 0 almost surely and with the optimal Monte Carlo rate of $M^{-\frac{1}{2}}$. Moreover, we prove that this is achieved even when the mean estimation error of the IWs remains constant, a property that has been termed {\it exact approximation} in the Markov chain Monte Carlo literature. We illustrate these theoretical results by means of a computer simulation example involving the estimation of the parameters of a state-space model typically used for target tracking.|['Joaquin Miguez', 'Ines P. Mari√±o', 'Manuel A. Vazquez']|['stat.CO']
2017-03-16T23:26:45Z|2017-02-10T10:44:23Z|http://arxiv.org/abs/1702.03126v1|http://arxiv.org/pdf/1702.03126v1|Computational inference without proposal kernels|Likelihood-free methods, such as approximate Bayesian computation, are powerful tools for practical inference problems with intractable likelihood functions. Markov chain Monte Carlo and sequential Monte Carlo variants of approximate Bayesian computation can be effective techniques for sampling posterior distributions without likelihoods. However, the efficiency of these methods depends crucially on the proposal kernel used to generate proposal posterior samples, and a poor choice can lead to extremely low efficiency. We propose a new method for likelihood-free Bayesian inference based upon ideas from multilevel Monte Carlo. Our method is accurate and does not require proposal kernels, thereby overcoming a key obstacle in the use of likelihood-free approaches in real-world situations.|['David J. Warne', 'Ruth E. Baker', 'Matthew J. Simpson']|['stat.CO', '62F15, 65C05']
2017-03-16T23:26:45Z|2017-02-15T10:14:43Z|http://arxiv.org/abs/1702.03057v2|http://arxiv.org/pdf/1702.03057v2|Unbiased Multi-index Monte Carlo|We introduce a new class of Monte Carlo based approximations of expectations of random variables defined whose laws are not available directly, but only through certain discretisatizations. Sampling from the discretized versions of these laws can typically introduce a bias. In this paper, we show how to remove that bias, by introducing a new version of multi-index Monte Carlo (MIMC) that has the added advantage of reducing the computational effort, relative to i.i.d. sampling from the most precise discretization, for a given level of error. We cover extensions of results regarding variance and optimality criteria for the new approach. We apply the methodology to the problem of computing an unbiased mollified version of the solution of a partial differential equation with random coefficients. A second application concerns the Bayesian inference (the smoothing problem) of an infinite dimensional signal modelled by the solution of a stochastic partial differential equation that is observed on a discrete space grid and at discrete times. Both applications are complemented by numerical simulations.|['Dan Crisan', 'Jeremie Houssineau', 'Ajay Jasra']|['stat.CO']
2017-03-16T23:26:45Z|2017-02-14T04:22:00Z|http://arxiv.org/abs/1702.02707v2|http://arxiv.org/pdf/1702.02707v2|A Fast Algorithm for the Coordinate-wise Minimum Distance Estimation|Application of the minimum distance method to the linear regression model for estimating regression parameters is a difficult and time-consuming process due to the complexity of its distance function, and hence, it is computationally expensive. To deal with the computational cost, this paper proposes a fast algorithm which mainly uses technique of coordinate-wise minimization in order to estimate the regression parameters. R package based on the proposed algorithm and written in Rcpp is available online.|['Jiwoong Kim']|['stat.CO']
2017-03-16T23:26:45Z|2017-02-09T00:11:27Z|http://arxiv.org/abs/1702.02658v1|http://arxiv.org/pdf/1702.02658v1|Estimating the number of clusters using cross-validation|Many clustering methods, including k-means, require the user to specify the number of clusters as an input parameter. A variety of methods have been devised to choose the number of clusters automatically, but they often rely on strong modeling assumptions. This paper proposes a data-driven approach to estimate the number of clusters based on a novel form of cross-validation. The proposed method differs from ordinary cross-validation, because clustering is fundamentally an unsupervised learning problem. Simulation and real data analysis results show that the proposed method outperforms existing methods, especially in high-dimensional settings with heterogeneous or heavy-tailed noise. In a yeast cell cycle dataset, the proposed method finds a parsimonious clustering with interpretable gene groupings.|['Wei Fu', 'Patrick O. Perry']|['stat.ME', 'stat.CO']
2017-03-16T23:26:45Z|2017-02-06T14:01:20Z|http://arxiv.org/abs/1702.01618v1|http://arxiv.org/pdf/1702.01618v1|Learning of state-space models with highly informative observations: a   tempered Sequential Monte Carlo solution|Probabilistic (or Bayesian) modeling and learning offers interesting possibilities for systematic representation of uncertainty based on probability theory. Recent advances in Monte Carlo based methods have made previously intractable problem possible to solve using only the computational power available in a standard personal computer. For probabilistic learning of unknown parameters in nonlinear state-space models, methods based on the particle filter have proven useful. However, a notoriously challenging problem occurs when the observations are highly informative, i.e. when there is very little or no measurement noise present. The particle filter will then struggle in estimating one of the basic component in most parameter learning algorithms, the likelihood p(data parameters). To this end we suggest an algorithm which initially assumes that there is artificial measurement noise present. The variance of this noise is sequentially decreased in an adaptive fashion such that we in the end recover the original problem or possibly a very close approximation of it. Computationally the parameters are learned using a sequential Monte Carlo (SMC) sampler, which gives our proposed method a clear resemblance to the SMC^2 method. Another natural link is also made to the ideas underlying the so-called approximate Bayesian computation (ABC). We provide a theoretical justification (implying convergence results) for the suggested approach. We also illustrate it with numerical examples, and in particular show promising results for a challenging Wiener-Hammerstein benchmark.|['Andreas Svensson', 'Thomas B. Sch√∂n', 'Fredrik Lindsten']|['stat.CO', 'stat.ML']
2017-03-16T23:26:45Z|2017-02-05T15:43:17Z|http://arxiv.org/abs/1702.01418v1|http://arxiv.org/pdf/1702.01418v1|Choosing the number of groups in a latent stochastic block model for   dynamic networks|Latent stochastic block models are flexible statistical models that are widely used in social network analysis. In recent years, efforts have been made to extend these models to temporal dynamic networks, whereby the connections between nodes are observed at a number of different times. In this paper we extend the original stochastic block model by using a Markovian property to describe the evolution of nodes' cluster memberships over time. We recast the problem of clustering the nodes of the network into a model-based context, and show that the integrated completed likelihood can be evaluated analytically for a number of likelihood models. Then, we propose a scalable greedy algorithm to maximise this quantity, thereby estimating both the optimal partition and the ideal number of groups in a single inferential framework. Finally we propose applications of our methodology to both real and artificial datasets.|['Riccardo Rastelli', 'Pierre Latouche', 'Nial Friel']|['stat.ME', 'stat.CO']
2017-03-16T23:26:45Z|2017-02-05T04:55:14Z|http://arxiv.org/abs/1702.01373v1|http://arxiv.org/pdf/1702.01373v1|Exact heat kernel on a hypersphere and its applications in kernel SVM|"Many contemporary statistical learning methods assume a Euclidean feature space, however, the ""curse of dimensionality"" associated with high feature dimensions is particularly severe for the Euclidean distance. This paper presents a method for defining similarity based on hyperspherical geometry and shows that it often improves the performance of support vector machine compared to other competing similarity measures. Specifically, the idea of using heat diffusion on a hypersphere to measure similarity has been proposed and tested by \citet{Lafferty:2015uy}, demonstrating promising results based on an approximate heat kernel, however, the exact hyperspherical heat kernel hitherto remains unknown. In this paper, we derive an exact form of the heat kernel on a unit hypersphere in terms of a uniformly and absolutely convergent series in high-dimensional angular momentum eigenmodes. Being a natural measure of similarity between sample points dwelling on a hypersphere, the exact kernel often shows superior performance in kernel SVM classifications applied to text mining, tumor somatic mutation imputation, and stock market analysis. The improvement in classification accuracy compared with kernels based on Euclidean geometry may arise from ameliorating the curse of dimensionality on compact manifolds."|['Chenchao Zhao', 'Jun S. Song']|['stat.ML', 'q-bio.QM', 'stat.CO']
2017-03-16T23:26:45Z|2017-02-04T18:58:02Z|http://arxiv.org/abs/1702.01326v1|http://arxiv.org/pdf/1702.01326v1|An Algorithm for Computing the Distribution Function of the Generalized   Poisson-Binomial Distribution|The Poisson-binomial distribution is useful in many applied problems in engineering, actuarial science, and data mining. The Poisson-binomial distribution models the distribution of the sum of independent but not identically distributed Bernoulli random variables whose success probabilities vary. In this paper, we extend the Poisson-binomial distribution to the generalized Poisson-binomial (GPB) distribution. The GPB distribution is defined in cases where the Bernoulli variables can take any two arbitrary values instead of 0 and~1. The GPB distribution is useful in many areas such as voting theory, actuarial science, warranty prediction, and probability theory. With few previous works studying the GPB distribution, we derive the probability distribution via the discrete Fourier transform of the characteristic function of the distribution. We develop an efficient algorithm for computing the distribution function, which uses the fast Fourier transform. We test the accuracy of the developed algorithm upon comparing it with enumeration-based exact method and the results from the binomial distribution. We also study the computational time of the algorithm in various parameter settings. Finally, we discus the factors affecting the computational efficiency of this algorithm, and illustrate the use of the software package.|['Man Zhang', 'Yili Hong', 'Narayanaswamy Balakrishnan']|['stat.CO']
2017-03-16T23:26:45Z|2017-02-03T22:07:37Z|http://arxiv.org/abs/1702.01185v1|http://arxiv.org/pdf/1702.01185v1|Basis Adaptive Sample Efficient Polynomial Chaos (BASE-PC)|For a large class of orthogonal basis functions, there has been a recent identification of expansion methods for computing accurate, stable approximations of a quantity of interest. This paper presents, within the context of uncertainty quantification, a practical implementation using basis adaptation, and coherence motivated sampling, which under assumptions has satisfying guarantees. This implementation is referred to as Basis Adaptive Sample Efficient Polynomial Chaos (BASE-PC). A key component of this is the use of anisotropic polynomial order which admits evolving global bases for approximation in an efficient manner, leading to consistently stable approximation for a practical class of smooth functionals. This fully adaptive, non-intrusive method, requires no a priori information of the solution, and has satisfying theoretical guarantees of recovery. A key contribution to stability is the use of a presented correction sampling for coherence-optimal sampling in order to improve stability and accuracy within the adaptive basis scheme. Theoretically, the method may dramatically reduce the impact of dimensionality in function approximation, and numerically the method is demonstrated to perform well on problems with dimension up to 1000.|['Jerrad Hampton', 'Alireza Doostan']|['stat.CO', 'math.PR', 'math.ST', 'stat.TH']
2017-03-16T23:26:45Z|2017-02-03T21:23:46Z|http://arxiv.org/abs/1702.01166v1|http://arxiv.org/pdf/1702.01166v1|Optimal Subsampling for Large Sample Logistic Regression|For massive data, the family of subsampling algorithms is popular to downsize the data volume and reduce computational burden. Existing studies focus on approximating the ordinary least squares estimate in linear regression, where statistical leverage scores are often used to define subsampling probabilities. In this paper, we propose fast subsampling algorithms to efficiently approximate the maximum likelihood estimate in logistic regression. We first establish consistency and asymptotic normality of the estimator from a general subsampling algorithm, and then derive optimal subsampling probabilities that minimize the asymptotic mean squared error of the resultant estimator. An alternative minimization criterion is also proposed to further reduce the computational cost. The optimal subsampling probabilities depend on the full data estimate, so we develop a two-step algorithm to approximate the optimal subsampling procedure. This algorithm is computationally efficient and has a significant reduction in computing time compared to the full data approach. Consistency and asymptotic normality of the estimator from a two-step algorithm are also established. Synthetic and real data sets are used to evaluate the practical performance of the proposed method.|['HaiYing Wang', 'Rong Zhu', 'Ping Ma']|['stat.CO', 'stat.ME', 'stat.ML']
2017-03-16T23:26:49Z|2017-02-02T20:08:42Z|http://arxiv.org/abs/1702.00817v1|http://arxiv.org/abs/1702.00817v1|DCT-like Transform for Image Compression Requires 14 Additions Only|A low-complexity 8-point orthogonal approximate DCT is introduced. The proposed transform requires no multiplications or bit-shift operations. The derived fast algorithm requires only 14 additions, less than any existing DCT approximation. Moreover, in several image compression scenarios, the proposed transform could outperform the well-known signed DCT, as well as state-of-the-art algorithms.|['F. M. Bayer', 'R. J. Cintra']|['cs.MM', 'cs.DS', 'stat.AP', 'stat.CO']
2017-03-16T23:26:49Z|2017-02-01T19:44:14Z|http://arxiv.org/abs/1702.00434v1|http://arxiv.org/pdf/1702.00434v1|Applying Nearest Neighbor Gaussian Processes to Massive Spatial Data   Sets: Forest Canopy Height Prediction Across Tanana Valley Alaska|Light detection and ranging (LiDAR) data provide critical information on the three-dimensional structure of forests. However, collecting wall-to-wall LiDAR data at regional and global scales is cost prohibitive. As a result, studies employing LiDAR data from airborne platforms typically collect data via strip sampling; leaving large swaths of the forest domain unmeasured by the instrument. Frameworks to accommodate incomplete coverage information from LiDAR instruments are essential to advance our understanding of forest structure and begin effectively monitoring forest resource dynamics over time. Here, we define and assess several spatial regression models capable of delivering complete coverage forest canopy height prediction maps with associated uncertainty estimates using sparsely sampled LiDAR data. Despite the sparsity of the LiDAR data considered, the number of observations is large, e.g., n=5x10^6. Computational hurdles associated with developing the desired data products is overcome by using highly scalable hierarchical Nearest Neighbor Gaussian Process (NNGP) models. We outline new Markov chain Monte Carlo (MCMC) algorithms that provide improved convergence and run time over existing algorithms. We also propose a MCMC free hybrid implementation of NNGP. We assess the computational and inferential benefits of these alternate NNGP specifications using simulated data sets and LiDAR data collected over the US Forest Service Tanana Inventory Unit (TIU) in a remote portion of Interior Alaska. The resulting data product is the first statistically robust map of forest canopy for the TIU.|['Andrew O. Finley', 'Abhirup Datta', 'Bruce C. Cook', 'Douglas C. Morton', 'Hans E. Andersen', 'Sudipto Banerjee']|['stat.CO', 'stat.AP']
2017-03-16T23:26:49Z|2017-02-27T17:17:15Z|http://arxiv.org/abs/1702.00428v2|http://arxiv.org/pdf/1702.00428v2|Malliavin-based Multilevel Monte Carlo Estimators for Densities of   Max-stable Processes|We introduce a class of unbiased Monte Carlo estimators for the multivariate density of max-stable fields generated by Gaussian processes. Our estimators take advantage of recent results on exact simulation of max-stable fields combined with identities studied in the Malliavin calculus literature and ideas developed in the multilevel Monte Carlo literature. Our approach allows estimating multivariate densities of max-stable fields with precision $\varepsilon $ at a computational cost of order $O\left( \varepsilon ^{-2}\log \log \log \left( 1/\varepsilon \right) \right) $.|['Jose Blanchet', 'Zhipeng Liu']|['stat.CO', 'math.PR']
2017-03-16T23:26:49Z|2017-02-07T22:13:25Z|http://arxiv.org/abs/1702.00317v2|http://arxiv.org/pdf/1702.00317v2|On SGD's Failure in Practice: Characterizing and Overcoming Stalling|Stochastic Gradient Descent (SGD) is widely used in machine learning problems to efficiently perform empirical risk minimization, yet, in practice, SGD is known to stall before reaching the actual minimizer of the empirical risk. SGD stalling has often been attributed to its sensitivity to the conditioning of the problem; however, as we demonstrate, SGD will stall even when applied to a simple linear regression problem with unity condition number for standard learning rates. Thus, in this work, we numerically demonstrate and mathematically argue that stalling is a crippling and generic limitation of SGD and its variants in practice. Once we have established the problem of stalling, we generalize an existing framework for hedging against its effects, which (1) deters SGD and its variants from stalling, (2) still provides convergence guarantees, and (3) makes SGD and its variants more practical methods for minimization.|['Vivak Patel']|['stat.ML', 'cs.LG', 'math.OC', 'stat.CO', '62L20, 62L12, 90C99', 'G.1.6; G.3; I.2.6']
2017-03-16T23:26:49Z|2017-02-01T10:55:12Z|http://arxiv.org/abs/1702.00204v1|http://arxiv.org/pdf/1702.00204v1|Bayesian model selection for the latent position cluster model for   Social Networks|The latent position cluster model is a popular model for the statistical analysis of network data. This model assumes that there is an underlying latent space in which the actors follow a finite mixture distribution. Moreover, actors which are close in this latent space are more likely to be tied by an edge. This is an appealing approach since it allows the model to cluster actors which consequently provides the practitioner with useful qualitative information. However, exploring the uncertainty in the number of underlying latent components in the mixture distribution is a complex task. The current state-of-the-art is to use an approximate form of BIC for this purpose, where an approximation of the log-likelihood is used instead of the true log-likelihood which is unavailable. The main contribution of this paper is to show that through the use of conjugate prior distributions it is possible to analytically integrate out almost all of the model parameters, leaving a posterior distribution which depends on the allocation vector of the mixture model. This enables posterior inference over the number of components in the latent mixture distribution without using trans- dimensional MCMC algorithms such as reversible jump MCMC. Our approach is compared with the state-of-the-art latentnet (Krivitsky & Handcock 2015) and VBLPCM (Salter-Townshend & Murphy 2013) packages.|['Caitriona Ryan', 'Jason Wyse', 'Nial Friel']|['stat.CO']
2017-03-16T23:26:49Z|2017-01-28T16:31:25Z|http://arxiv.org/abs/1701.08299v1|http://arxiv.org/pdf/1701.08299v1|Computing the aggregate loss distribution based on numerical inversion   of the compound empirical characteristic function of frequency and severity|A non-parametric method for evaluation of the aggregate loss distribution (ALD) by combining and numerically inverting the empirical characteristic functions (CFs) is presented and illustrated. This approach to evaluate ALD is based on purely non-parametric considerations, i.e., based on the empirical CFs of frequency and severity of the claims in the actuarial risk applications. This approach can be, however, naturally generalized to a more complex semi-parametric modeling approach, e.g., by incorporating the generalized Pareto distribution fit of the severity distribution heavy tails, and/or by considering the weighted mixture of the parametric CFs (used to model the expert knowledge) and the empirical CFs (used to incorporate the knowledge based on the historical data - internal and/or external). Here we present a simple and yet efficient method and algorithms for numerical inversion of the CF, suitable for evaluation of the ALDs and the associated measures of interest important for applications, as, e.g., the value at risk (VaR). The presented approach is based on combination of the Gil-Pelaez inversion formulae for deriving the probability distribution (PDF and CDF) from the compound (empirical) CF and the trapezoidal rule used for numerical integration. The applicability of the suggested approach is illustrated by analysis of a well know insurance dataset, the Danish fire loss data.|['Viktor Witkovsky', 'Gejza Wimmer', 'Tomas Duby']|['stat.CO', 'q-fin.RM', 'stat.AP', '91B30, 62G32']
2017-03-16T23:26:49Z|2017-03-03T15:54:59Z|http://arxiv.org/abs/1701.08142v3|http://arxiv.org/pdf/1701.08142v3|Modelling Preference Data with the Wallenius Distribution|The Wallenius distribution is a generalisation of the Hypergeometric distribution where weights are assigned to balls of different colours. This naturally defines a model for ranking categories which can be used for classification purposes. Since, in general, the resulting likelihood is not analytically available, we adopt an approximate Bayesian computational (ABC) approach for estimating the importance of the categories. We illustrate the performance of the estimation procedure on simulated datasets. Finally, we use the new model for analysing two datasets about movies ratings and Italian academic statisticians' journal preferences. The latter is a novel dataset collected by the authors.|['Clara Grazian', 'Fabrizio Leisen', 'Brunero Liseo']|['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']
2017-03-16T23:26:49Z|2017-01-26T19:07:53Z|http://arxiv.org/abs/1701.07844v1|http://arxiv.org/pdf/1701.07844v1|Markov Chain Monte Carlo with the Integrated Nested Laplace   Approximation|The Integrated Nested Laplace Approximation (INLA) has established itself as a widely used method for approximate inference on Bayesian hierarchical models which can be represented as a latent Gaussian model (LGM). INLA is based on producing an accurate approximation to the posterior marginal distributions of the parameters in the model and some other quantities of interest by using repeated approximations to intermediate distributions and integrals that appear in the computation of the posterior marginals.   INLA focuses on models whose latent effects are a Gaussian Markov random field (GMRF). For this reason, we have explored alternative ways of expanding the number of possible models that can be fitted using the INLA methodology. In this paper, we present a novel approach that combines INLA and Markov chain Monte Carlo (MCMC). The aim is to consider a wider range of models that cannot be fitted with INLA unless some of the parameters of the model have been fixed. Hence, conditioning on these parameters the model could be fitted with the R-INLA package. We show how new values of these parameters can be drawn from their posterior by using conditional models fitted with INLA and standard MCMC algorithms, such as Metropolis-Hastings. Hence, this will extend the use of INLA to fit models that can be expressed as a conditional LGM. Also, this new approach can be used to build simpler MCMC samplers for complex models as it allows sampling only on a limited number parameters in the model.   We will demonstrate how our approach can extend the class of models that could benefit from INLA, and how the R-INLA package will ease its implementation. We will go through simple examples of this new approach before we discuss more advanced problems with datasets taken from relevant literature.|['Virgilio G√≥mez-Rubio', 'H√•vard Rue']|['stat.CO']
2017-03-16T23:26:49Z|2017-02-09T12:18:42Z|http://arxiv.org/abs/1701.07787v3|http://arxiv.org/pdf/1701.07787v3|Multi-locus data distinguishes between population growth and multiple   merger coalescents|We introduce a low dimensional function of the site frequency spectrum that is tailor-made for distinguishing coalescent models with multiple mergers from Kingman coalescent models with population growth, and use this function to construct a hypothesis test between these two model classes. The null and alternative sampling distributions of our statistic are intractable, but its low dimensionality renders these distributions amenable to Monte Carlo estimation. We construct kernel density estimates of the sampling distributions based on simulated data, and show that the resulting hypothesis test dramatically improves on the statistical power of a current state-of-the-art method. A key reason for this improvement is the use of multi-locus data, in particular averaging observed site frequency spectra across unlinked loci to reduce sampling variance. We also demonstrate the robustness of our method to nuisance and tuning parameters. Finally we demonstrate that the same kernel density estimates can be used to conduct parameter estimation, and argue that our method is readily generalisable for applications in model selection, parameter inference and experimental design.|['Jere Koskela']|['q-bio.PE', 'q-bio.QM', 'stat.CO', 'stat.ME', '92D15 (Primary), 62M02, 62M05 (Secondary)']
2017-03-16T23:26:49Z|2017-01-25T21:43:03Z|http://arxiv.org/abs/1701.07496v1|http://arxiv.org/pdf/1701.07496v1|Phylogenetic Factor Analysis|Phylogenetic comparative methods explore the relationships between quantitative traits adjusting for shared evolutionary history. This adjustment often occurs through a Brownian diffusion process along the branches of the phylogeny that generates model residuals or the traits themselves. For high-dimensional traits, inferring all pair-wise correlations within the multivariate diffusion is limiting. To circumvent this problem, we propose phylogenetic factor analysis (PFA) that assumes a small unknown number of independent evolutionary factors arise along the phylogeny and these factors generate clusters of dependent traits. Set in a Bayesian framework, PFA provides measures of uncertainty on the factor number and groupings, combines both continuous and discrete traits, integrates over missing measurements and incorporates phylogenetic uncertainty with the help of molecular sequences. We develop Gibbs samplers based on dynamic programming to estimate the PFA posterior distribution, over three-fold faster than for multivariate diffusion and a further order-of-magnitude more efficiently in the presence of latent traits. We further propose a novel marginal likelihood estimator for previously impractical models with discrete data and find that PFA also provides a better fit than multivariate diffusion in evolutionary questions in columbine flower development, placental reproduction transitions and triggerfish fin morphometry.|['Max R. Tolkoff', 'Michael L. Alfaro', 'Guy Baele', 'Philippe Lemey', 'Marc A. Suchard']|['stat.ME', 'stat.AP', 'stat.CO']
2017-03-16T23:26:53Z|2017-01-23T20:26:42Z|http://arxiv.org/abs/1701.06619v1|http://arxiv.org/pdf/1701.06619v1|Bayesian Inference in the Presence of Intractable Normalizing Functions|Models with intractable normalizing functions arise frequently in statistics. Common examples of such models include exponential random graph models for social networks and Markov point processes for ecology and disease modeling. Inference for these models is complicated because the normalizing functions of their probability distributions include the parameters of interest. In Bayesian analysis they result in so-called doubly intractable posterior distributions which pose significant computational challenges. Several Monte Carlo methods have emerged in recent years to address Bayesian inference for such models. We provide a framework for understanding the algorithms and elucidate connections among them. Through multiple simulated and real data examples, we compare and contrast the computational and statistical efficiency of these algorithms and discuss their theoretical bases. Our study provides practical recommendations for practitioners along with directions for future research for MCMC methodologists.|['Jaewoo Park', 'Murali Haran']|['stat.CO', 'stat.AP']
2017-03-16T23:26:53Z|2017-01-20T22:08:49Z|http://arxiv.org/abs/1701.05936v1|http://arxiv.org/pdf/1701.05936v1|The biglasso Package: A Memory- and Computation-Efficient Solver for   Lasso Model Fitting with Big Data in R|Penalized regression models such as the lasso have been extensively applied to analyzing high-dimensional data sets. However, due to memory limitations, existing R packages like glmnet and ncvreg are not capable of fitting lasso-type models for ultrahigh-dimensional, multi-gigabyte data sets that are increasingly seen in many areas such as genetics, genomics, biomedical imaging, and high-frequency finance. In this research, we implement an R package called biglasso that tackles this challenge. biglasso utilizes memory-mapped files to store the massive data on the disk, only reading data into memory when necessary during model fitting, and is thus able to handle out-of-core computation seamlessly. Moreover, it's equipped with newly proposed, more efficient feature screening rules, which substantially accelerate the computation. Benchmarking experiments show that our biglasso package, as compared to existing popular ones like glmnet, is much more memory- and computation-efficient. We further analyze a 31 GB real data set on a laptop with only 16 GB RAM to demonstrate the out-of-core computation capability of biglasso in analyzing massive data sets that cannot be accommodated by existing R packages.|['Yaohui Zeng', 'Patrick Breheny']|['stat.CO', 'stat.ML']
2017-03-16T23:26:53Z|2017-01-20T18:50:50Z|http://arxiv.org/abs/1701.05892v1|http://arxiv.org/pdf/1701.05892v1|Bayesian Static Parameter Estimation for Partially Observed Diffusions   via Multilevel Monte Carlo|In this article we consider static Bayesian parameter estimation for partially observed diffusions that are discretely observed. We work under the assumption that one must resort to discretizing the underlying diffusion process, for instance using the Euler-Maruyama method. Given this assumption, we show how one can use Markov chain Monte Carlo (MCMC) and particularly particle MCMC [Andrieu, C., Doucet, A. and Holenstein, R. (2010). Particle Markov chain Monte Carlo methods (with discussion). J. R. Statist. Soc. Ser. B, 72, 269--342] to implement a new approximation of the multilevel (ML) Monte Carlo (MC) collapsing sum identity. Our approach comprises constructing an approximate coupling of the posterior density of the joint distribution over parameter and hidden variables at two different discretization levels and then correcting by an importance sampling method. The variance of the weights are independent of the length of the observed data set. The utility of such a method is that, for a prescribed level of mean square error, the cost of this MLMC method is provably less than i.i.d. sampling from the posterior associated to the most precise discretization. However the method here comprises using only known and efficient simulation methodologies. The theoretical results are illustrated by inference of the parameters of two prototypical processes given noisy partial observations of the process: the first is an Ornstein Uhlenbeck process and the second is a more general Langevin equation.|['Ajay Jasra', 'Kengo Kamatani', 'Kody J. H. Law', 'Yan Zhou']|['stat.CO', 'math.PR']
2017-03-16T23:26:53Z|2017-01-19T21:31:39Z|http://arxiv.org/abs/1701.05609v1|http://arxiv.org/pdf/1701.05609v1|Confidence Intervals for Finite Difference Solutions|Although applications of Bayesian analysis for numerical quadrature problems have been considered before, it's only very recently that statisticians have focused on the connections between statistics and numerical analysis of differential equations. In line with this very recent trend, we show how certain commonly used finite difference schemes for numerical solutions of ordinary and partial differential equations can be considered in a regression setting. Focusing on this regression framework, we apply a simple Bayesian strategy to obtain confidence intervals for the finite difference solutions. We apply this framework on several examples to show how the confidence intervals are related to truncation error and illustrate the utility of the confidence intervals for the examples considered.|['Majnu John', 'Yihren Wu']|['stat.CO', 'math.NA']
2017-03-16T23:26:53Z|2017-01-19T17:07:21Z|http://arxiv.org/abs/1701.05512v1|http://arxiv.org/pdf/1701.05512v1|Fisher consistency for prior probability shift|We introduce Fisher consistency in the sense of unbiasedness as a criterion to distinguish potentially suitable and unsuitable estimators of prior class probabilities in test datasets under prior probability and more general dataset shift. The usefulness of this unbiasedness concept is demonstrated with three examples of classifiers used for quantification: Adjusted Classify & Count, EM-algorithm and CDE-Iterate. We find that Adjusted Classify & Count and EM-algorithm are Fisher consistent. A counter-example shows that CDE-Iterate is not Fisher consistent and, therefore, cannot be trusted to deliver reliable estimates of class probabilities.|['Dirk Tasche']|['stat.ML', 'cs.LG', 'stat.CO', '62C10']
2017-03-16T23:26:53Z|2017-01-19T10:26:48Z|http://arxiv.org/abs/1701.05358v1|http://arxiv.org/pdf/1701.05358v1|Smooth Transition HYGARCH Model: Stability and Forecasting|HYGARCH process is the commonly used long memory process in modeling the long-rang dependence in volatility.   Financial time series are characterized by transition between phases of different volatility levels. The smooth transition HYGARCH (ST-HYGARCH) model is proposed to model time-varying structure with long memory property. The asymptotic behavior of the second moment is studied and an upper bound for it is derived. A score test is developed to check the smooth transition property. The asymptotic behavior of the proposed model and the score test is examined by simulation. The proposed model is applied to the \textit{S}\&\textit{P}500 indices for some period which show evidence of smooth transition property and demonstrates out-performance of the ST-HYGARCH than HYGARCH in forecasting.|['Ferdous Mohammadi', 'Saeid Rezakhah']|['stat.CO', 'stat.ME', '37JM10, 62P05, 62F03, 62F10']
2017-03-16T23:26:53Z|2017-01-18T16:59:55Z|http://arxiv.org/abs/1701.05146v1|http://arxiv.org/pdf/1701.05146v1|Inference in generative models using the Wasserstein distance|In purely generative models, one can simulate data given parameters but not necessarily evaluate the likelihood. We use Wasserstein distances between empirical distributions of observed data and empirical distributions of synthetic data drawn from such models to estimate their parameters. Previous interest in the Wasserstein distance for statistical inference has been mainly theoretical, due to computational limitations. Thanks to recent advances in numerical transport, the computation of these distances has become feasible, up to controllable approximation errors. We leverage these advances to propose point estimators and quasi-Bayesian distributions for parameter inference, first for independent data. For dependent data, we extend the approach by using delay reconstruction and residual reconstruction techniques. For large data sets, we propose an alternative distance using the Hilbert space-filling curve, which computation scales as $n\log n$ where $n$ is the size of the data. We provide a theoretical study of the proposed estimators, and adaptive Monte Carlo algorithms to approximate them. The approach is illustrated on four examples: a quantile g-and-k distribution, a toggle switch model from systems biology, a Lotka-Volterra model for plankton population sizes and a L\'evy-driven stochastic volatility model.|['Espen Bernton', 'Pierre E. Jacob', 'Mathieu Gerber', 'Christian P. Robert']|['stat.ME', 'math.ST', 'stat.CO', 'stat.TH']
2017-03-16T23:26:53Z|2017-01-18T16:13:24Z|http://arxiv.org/abs/1701.05128v1|http://arxiv.org/pdf/1701.05128v1|A Constructive Approach to High-dimensional Regression|We develop a constructive approach to estimating sparse, high-dimensional linear regression models. The approach is a computational algorithm motivated from the KKT conditions for the $\ell_0$-penalized least squares solutions. It generates a sequence of solutions iteratively, based on support detection using primal and dual information and root finding. We refer to the algorithm as SDAR for brevity. Under a sparse Rieze condition on the design matrix and certain other conditions, we show that with high probability, the $\ell_2$ estimation error of the solution sequence decays exponentially to the minimax error bound in $O(\sqrt{J}\log(R))$ steps; and under a mutual coherence condition and certain other conditions, the $\ell_{\infty}$ estimation error decays to the optimal error bound in $O(\log(R))$ steps, where $J$ is the number of important predictors, $R$ is the relative magnitude of the nonzero target coefficients. Computational complexity analysis shows that the cost of SDAR is $O(np)$ per iteration. Moreover the oracle least squares estimator can be exactly recovered with high probability at the same cost if we know the sparsity level. We also consider an adaptive version of SDAR to make it more practical in applications. Numerical comparisons with Lasso, MCP and greedy methods demonstrate that SDAR is competitive with or outperforms them in accuracy and efficiency.|['Jian Huang', 'Yuling Jiao', 'Yanyan Liu', 'Xiliang Lu']|['stat.CO', 'stat.ME']
2017-03-16T23:26:53Z|2017-01-16T11:28:50Z|http://arxiv.org/abs/1701.04247v1|http://arxiv.org/pdf/1701.04247v1|Nonreversible Langevin Samplers: Splitting Schemes, Analysis and   Implementation|For a given target density, there exist an infinite number of diffusion processes which are ergodic with respect to this density. As observed in a number of papers, samplers based on nonreversible diffusion processes can significantly outperform their reversible counterparts both in terms of asymptotic variance and rate of convergence to equilibrium. In this paper, we take advantage of this in order to construct efficient sampling algorithms based on the Lie-Trotter decomposition of a nonreversible diffusion process into reversible and nonreversible components. We show that samplers based on this scheme can significantly outperform standard MCMC methods, at the cost of introducing some controlled bias. In particular, we prove that numerical integrators constructed according to this decomposition are geometrically ergodic and characterise fully their asymptotic bias and variance, showing that the sampler inherits the good mixing properties of the underlying nonreversible diffusion. This is illustrated further with a number of numerical examples ranging from highly correlated low dimensional distributions, to logistic regression problems in high dimensions as well as inference for spatial models with many latent variables.|['A. B. Duncan', 'G. A. Pavliotis', 'K. C. Zygalakis']|['stat.ME', 'stat.CO']
2017-03-16T23:26:53Z|2017-01-16T11:18:51Z|http://arxiv.org/abs/1701.04244v1|http://arxiv.org/pdf/1701.04244v1|Piecewise Deterministic Markov Processes for Scalable Monte Carlo on   Restricted Domains|Piecewise deterministic Monte Carlo methods (PDMC) consist of a class of continuous-time Markov chain Monte Carlo methods (MCMC) which have recently been shown to hold considerable promise. Being non-reversible, the mixing properties of PDMC methods often significantly outperform classical reversible MCMC competitors. Moreover, in a Bayesian context they can use sub-sampling ideas, so that they need only access one data point per iteration, whilst still maintaining the true posterior distribution as their invariant distribution. However, current methods are limited to parameter spaces of real d-dimensional vectors. We show how these algorithms can be extended to applications involving restricted parameter spaces. In simulations we observe that the resulting algorithm is more efficient than Hamiltonian Monte Carlo for sampling from truncated logistic regression models. The theoretical framework used to justify this extension lays the foundation for the development of other novel PDMC algorithms.|['Joris Bierkens', 'Alexandre Bouchard-C√¥t√©', 'Arnaud Doucet', 'Andrew B. Duncan', 'Paul Fearnhead', 'Gareth Roberts', 'Sebastian J. Vollmer']|['stat.ME', 'stat.CO']
2017-03-16T23:26:57Z|2017-01-14T01:15:23Z|http://arxiv.org/abs/1701.03861v1|http://arxiv.org/pdf/1701.03861v1|Network Inference from a Link-Traced Sample using Approximate Bayesian   Computation|We present a new inference method based on approximate Bayesian computation for estimating parameters governing an entire network based on link-traced samples of that network. To do this, we first take summary statistics from an observed link-traced network sample, such as a recruitment network of subjects in a hard-to-reach population. Then we assume prior distributions, such as multivariate uniform, for the distribution of some parameters governing the structure of the network and behaviour of its nodes. Then, we draw many independent and identically distributed values for these parameters. For each set of values, we simulate a population network, take a link-traced sample from that network, and find the summary statistics for that sample. The statistics from the sample, and the parameters that eventually led to that sample, are collectively treated as a single point. We take a Kernel Density estimate of the points from many simulations, and observe the density across the hyperplane coinciding with the statistic values of the originally observed sample. This density function is treat as a posterior estimate of the paramaters of the network that provided the observed sample.   We also apply this method to a network of precedence citations between legal documents, centered around cases overseen by the Supreme Court of Canada, is observed. The features of certain cases that lead to their frequent citation are inferred, and their effects estimated by ABC. Future work and extensions are also briefly discussed.|['Jack Davis', 'Steven K. Thompson']|['stat.CO', 'cs.SI', 'physics.soc-ph']
2017-03-16T23:26:57Z|2017-03-07T18:41:45Z|http://arxiv.org/abs/1701.03757v2|http://arxiv.org/pdf/1701.03757v2|Deep Probabilistic Programming|We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations---random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.|['Dustin Tran', 'Matthew D. Hoffman', 'Rif A. Saurous', 'Eugene Brevdo', 'Kevin Murphy', 'David M. Blei']|['stat.ML', 'cs.AI', 'cs.LG', 'cs.PL', 'stat.CO']
2017-03-16T23:26:57Z|2017-01-13T14:05:41Z|http://arxiv.org/abs/1701.03675v1|http://arxiv.org/pdf/1701.03675v1|Tutorial in Joint Modeling and Prediction: a Statistical Software for   Correlated Longitudinal Outcomes, Recurrent Events and a Terminal Event|Extensions in the field of joint modeling of correlated data and dynamic predictions improve the development of prognosis research. The R package frailtypack provides estimations of various joint models for longitudinal data and survival events. In particular, it fits models for recurrent events and a terminal event (frailtyPenal), models for two survival outcomes for clustered data (frailtyPenal), models for two types of recurrent events and a terminal event (multivPenal), models for a longitudinal biomarker and a terminal event (longiPenal) and models for a longitudinal biomarker, recurrent events and a terminal event (trivPenal). The estimators are obtained using a standard and penalized maximum likelihood approach, each model function allows to evaluate goodness-of-fit analyses and plots of baseline hazard functions. Finally, the package provides individual dynamic predictions of the terminal event and evaluation of predictive accuracy. This paper presents theoretical models with estimation techniques, applies the methods for predictions and illustrates frailtypack functions details with examples.|['Agnieszka Kr√≥l', 'Audrey Mauguen', 'Yassin Mazroui', 'Alexandre Laurent', 'Stefan Michiels', 'Virginie Rondeau']|['stat.CO']
2017-03-16T23:26:57Z|2017-01-12T21:34:48Z|http://arxiv.org/abs/1701.03512v1|http://arxiv.org/pdf/1701.03512v1|Parallelizing Computation of Expected Values in Recombinant Binomial   Trees|"Recombinant binomial trees are binary trees where each non-leaf node has two child nodes, but adjacent parents share a common child node. Such trees arise in finance when pricing an option. For example, valuation of a European option can be carried out by evaluating the expected value of asset payoffs with respect to random paths in the tree. In many variants of the option valuation problem, a closed form solution cannot be obtained and computational methods are needed. The cost to exactly compute expected values over random paths grows exponentially in the depth of the tree, rendering a serial computation of one branch at a time impractical. We propose a parallelization method that transforms the calculation of the expected value into an ""embarrassingly parallel"" problem by mapping the branches of the binomial tree to the processes in a multiprocessor computing environment. We also propose a parallel Monte Carlo method which takes advantage of the mapping to achieve a reduced variance over the basic Monte Carlo estimator. Performance results from R and Julia implementations of the parallelization method on a distributed computing cluster indicate that both the implementations are scalable, but Julia is significantly faster than a similarly written R code. A simulation study is carried out to verify the convergence and the variance reduction behavior in the proposed Monte Carlo method."|['Sai K. Popuri', 'Andrew M. Raim', 'Nagaraj K. Neerchal', 'Matthias K. Gobbert']|['stat.CO', 'q-fin.CP']
2017-03-16T23:26:57Z|2017-01-12T16:56:29Z|http://arxiv.org/abs/1701.03405v1|http://arxiv.org/pdf/1701.03405v1|New Flexible Compact Covariance Model on a Sphere|We discuss how the kernel convolution approach can be used to accurately approximate the spatial covariance model on a sphere using spherical distances between points. A detailed derivation of the required formulas is provided. The proposed covariance model approximation can be used for non-stationary spatial prediction and simulation in the case when the dataset is large and the covariance model can be estimated separately in the data subsets.|['Alexander Gribov', 'Konstantin Krivoruchko']|['stat.CO']
2017-03-16T23:26:57Z|2017-01-12T08:44:14Z|http://arxiv.org/abs/1701.03267v1|http://arxiv.org/pdf/1701.03267v1|Robust clustering for functional data based on trimming and constraints|"Many clustering algorithms when the data are curves or functions have been recently proposed. However, the presence of contamination in the sample of curves can influence the performance of most of them. In this work we propose a robust, model-based clustering method based on an approximation to the ""density function"" for functional data. The robustness results from the joint application of trimming, for reducing the effect of contaminated observations, and constraints on the variances, for avoiding spurious clusters in the solution. The proposed method has been evaluated through a simulation study. Finally, an application to a real data problem is given."|['Diego Rivera-Garc√≠a', 'Luis Angel Garc√≠a-Escudero', 'Agust√≠n Mayo-Iscar', 'Joaquƒ±n Ortega']|['stat.CO']
2017-03-16T23:26:57Z|2017-01-11T18:51:54Z|http://arxiv.org/abs/1701.03095v1|http://arxiv.org/pdf/1701.03095v1|Bayesian estimation of Differential Transcript Usage from RNA-seq data|Next generation sequencing allows the identification of genes consisting of differentially expressed transcripts, a term which usually refers to changes in the overall expression level. A specific type of differential expression is differential transcript usage (DTU) and targets changes in the relative within gene expression of a transcript. The contribution of this paper is to: (a) extend the use of cjBitSeq to the DTU context, a previously introduced Bayesian model which is originally designed for identifying changes in overall expression levels and (b) propose a Bayesian version of DRIMSeq, a frequentist model for inferring DTU. cjBitSeq is a read based model and performs fully Bayesian inference by MCMC sampling on the space of latent state of each transcript per gene. BayesDRIMSeq is a count based model and estimates the Bayes Factor of a DTU model against a null model using Laplace's approximation. The proposed models are benchmarked against the existing ones using a recent independent simulation study. Our results suggest that the Bayesian methods exhibit similar performance with DRIMSeq in terms of precision/recall but offer better calibration of False Discovery Rate.|['Panagiotis Papastamoulis', 'Magnus Rattray']|['q-bio.GN', 'stat.AP', 'stat.CO']
2017-03-16T23:26:57Z|2017-01-11T13:44:01Z|http://arxiv.org/abs/1701.02969v1|http://arxiv.org/pdf/1701.02969v1|Logit stick-breaking priors for Bayesian density regression|There is an increasing focus in several fields on learning how the distribution of an outcome changes with a set of covariates. Bayesian nonparametric dependent mixture models provide a useful approach to flexibly address this goal, however many representations are characterized by difficult interpretation and intractable computational methods. Motivated by these issues, we describe a flexible formulation for Bayesian density regression, which is defined as a potentially infinite mixture model whose probability weights change with the covariates via a stick-breaking construction relying on a set of logistic regressions. We derive theoretical properties and show that our logit stick-breaking representation can be interpreted as a simple continuation-ratio logistic regression. This result facilitates derivation of three computational methods of routine use in Bayesian inference, covering Markov Chain Monte Carlo via Gibbs sampling, the Expectation Conditional Maximization algorithm for the estimation of posterior modes, and a Variational Bayes approach for scalable inference. The algorithms associated with these methods are analytically derived and made available online at https://github.com/tommasorigon/DLSBP. We additionally compare the three computational strategies in an application to the Old Faithful Geyser dataset.|['Tommaso Rigon', 'Daniele Durante']|['stat.CO']
2017-03-16T23:26:57Z|2017-01-10T11:33:41Z|http://arxiv.org/abs/1701.02522v1|http://arxiv.org/pdf/1701.02522v1|Magnus expansions and pseudospectra of Master Equations|New directions in research on master equations are showcased by example. Magnus expansions, time-varying rates, and pseudospectra are highlighted. Exact eigenvalues are found and contrasted with the large errors produced by standard numerical methods in some cases. Isomerisation provides a running example and an illustrative application to chemical kinetics. We also give a brief example of the totally asymmetric exclusion process.|['Arieh Iserles', 'Shev MacNamara']|['math.NA', 'math.PR', 'stat.CO']
2017-03-16T23:26:57Z|2017-01-09T21:00:46Z|http://arxiv.org/abs/1701.02349v1|http://arxiv.org/pdf/1701.02349v1|MEBoost: Variable Selection in the Presence of Measurement Error|"We present a novel method for variable selection in regression models when covariates are measured with error. The iterative algorithm we propose, MEBoost, follows a path defined by estimating equations that correct for covariate measurement error. Via simulation, we evaluated our method and compare its performance to the recently-proposed Convex Conditioned Lasso (CoCoLasso) and to the ""naive"" Lasso which does not correct for measurement error. Increasing the degree of measurement error increased prediction error and decreased the probability of accurate covariate selection, but this loss of accuracy was least pronounced when using MEBoost. We illustrate the use of MEBoost in practice by analyzing data from the Box Lunch Study, a clinical trial in nutrition where several variables are based on self-report and hence measured with error."|['Benjamin Brown', 'Timothy Weaver', 'Julian Wolfson']|['stat.CO', 'stat.ML']
