2017-03-16T23:32:43Z|2017-03-15T15:10:16Z|http://arxiv.org/abs/1703.05199v1|http://arxiv.org/pdf/1703.05199v1|Optimal Unateness Testers for Real-Valued Functions: Adaptivity Helps|We study the problem of testing unateness of functions $f:\{0,1\}^d \to \mathbb{R}.$ We give a $O(\frac{d}{\epsilon} \cdot \log\frac{d}{\epsilon})$-query nonadaptive tester and a $O(\frac{d}{\epsilon})$-query adaptive tester and show that both testers are optimal for a fixed distance parameter $\epsilon$. Previously known unateness testers worked only for Boolean functions, and their query complexity had worse dependence on the dimension both for the adaptive and the nonadaptive case. Moreover, no lower bounds for testing unateness were known. We also generalize our results to obtain optimal unateness testers for functions $f:[n]^d \to \mathbb{R}$.   Our results establish that adaptivity helps with testing unateness of real-valued functions on domains of the form $\{0,1\}^d$ and, more generally, $[n]^d$. This stands in contrast to the situation for monotonicity testing where there is no adaptivity gap for functions $f:[n]^d \to \mathbb{R}$.|['Roksana Baleshzar', 'Deeparnab Chakrabarty', 'Ramesh Krishnan S. Pallavoor', 'Sofya Raskhodnikova', 'C. Seshadhri']|['cs.DS', 'cs.DM']
2017-03-16T23:32:43Z|2017-03-15T14:01:21Z|http://arxiv.org/abs/1703.05160v1|http://arxiv.org/pdf/1703.05160v1|A New Unbiased and Efficient Class of LSH-Based Samplers and Estimators   for Partition Function Computation in Log-Linear Models|"Log-linear models are arguably the most successful class of graphical models for large-scale applications because of their simplicity and tractability. Learning and inference with these models require calculating the partition function, which is a major bottleneck and intractable for large state spaces. Importance Sampling (IS) and MCMC-based approaches are lucrative. However, the condition of having a ""good"" proposal distribution is often not satisfied in practice.   In this paper, we add a new dimension to efficient estimation via sampling. We propose a new sampling scheme and an unbiased estimator that estimates the partition function accurately in sub-linear time. Our samples are generated in near-constant time using locality sensitive hashing (LSH), and so are correlated and unnormalized. We demonstrate the effectiveness of our proposed approach by comparing the accuracy and speed of estimating the partition function against other state-of-the-art estimation techniques including IS and the efficient variant of Gumbel-Max sampling. With our efficient sampling scheme, we accurately train real-world language models using only 1-2% of computations."|['Ryan Spring', 'Anshumali Shrivastava']|['stat.ML', 'cs.DB', 'cs.DS', 'cs.LG']
2017-03-16T23:32:43Z|2017-03-15T13:51:23Z|http://arxiv.org/abs/1703.05156v1|http://arxiv.org/pdf/1703.05156v1|Complexity Dichotomies for the Minimum F-Overlay Problem|For a (possibly infinite) fixed family of graphs F, we say that a graph G overlays F on a hypergraph H if V(H) is equal to V(G) and the subgraph of G induced by every hyperedge of H contains some member of F as a spanning subgraph.While it is easy to see that the complete graph on  V(H)  overlays F on a hypergraph H whenever the problem admits a solution, the Minimum F-Overlay problem asks for such a graph with the minimum number of edges.This problem allows to generalize some natural problems which may arise in practice. For instance, if the family F contains all connected graphs, then Minimum F-Overlay corresponds to the Minimum Connectivity Inference problem (also known as Subset Interconnection Design problem) introduced for the low-resolution reconstruction of macro-molecular assembly in structural biology, or for the design of networks.Our main contribution is a strong dichotomy result regarding the polynomial vs. NP-hard status with respect to the considered family F. Roughly speaking, we show that the easy cases one can think of (e.g. when edgeless graphs of the right sizes are in F, or if F contains only cliques) are the only families giving rise to a polynomial problem: all others are NP-complete.We then investigate the parameterized complexity of the problem and give similar sufficient conditions on F that give rise to W[1]-hard, W[2]-hard or FPT problems when the parameter is the size of the solution.This yields an FPT/W[1]-hard dichotomy for a relaxed problem, where every hyperedge of H must contain some member of F as a (non necessarily spanning) subgraph.|['Nathann Cohen', 'Frédéric Havet', 'Dorian Mazauric', 'Ignasi Sau', 'Rémi Watrigant']|['cs.DS', 'cs.CC']
2017-03-16T23:32:43Z|2017-03-15T12:06:53Z|http://arxiv.org/abs/1703.05102v1|http://arxiv.org/pdf/1703.05102v1|Algorithms for outerplanar graph roots and graph roots of pathwidth at   most 2|Deciding whether a given graph has a square root is a classical problem that has been studied extensively both from graph theoretic and from algorithmic perspectives. The problem is NP-complete in general, and consequently substantial effort has been dedicated to deciding whether a given graph has a square root that belongs to a particular graph class. There are both polynomial-time solvable and NP-complete cases, depending on the graph class. We contribute with new results in this direction. Given an arbitrary input graph G, we give polynomial-time algorithms to decide whether G has an outerplanar square root, and whether G has a square root that is of pathwidth at most 2.|['Petr A. Golovach', 'Pinar Heggernes', 'Dieter Kratsch', 'Paloma T. Lima', 'Daniel Paulusma']|['cs.DS', 'cs.DM']
2017-03-16T23:32:43Z|2017-03-15T11:57:53Z|http://arxiv.org/abs/1703.05097v1|http://arxiv.org/pdf/1703.05097v1|A cubic-time algorithm for computing the trinet distance between level-1   networks|In evolutionary biology, phylogenetic networks are constructed to represent the evolution of species in which reticulate events are thought to have occurred, such as recombination and hybridization. It is therefore useful to have efficiently computable metrics with which to systematically compare such networks. Through developing an optimal algorithm to enumerate all trinets displayed by a level-1 network (a type of network that is slightly more general than an evolutionary tree), here we propose a cubic-time algorithm to compute the trinet distance between two level-1 networks. Employing simulations, we also present a comparison between the trinet metric and the so-called Robinson-Foulds phylogenetic network metric restricted to level-1 networks. The algorithms described in this paper have been implemented in JAVA and are freely available at https://www.uea.ac.uk/computing/TriLoNet.|['Vincent Moulton', 'James Oldman', 'Taoyang Wu']|['q-bio.PE', 'cs.DM', 'cs.DS']
2017-03-16T23:32:43Z|2017-03-15T06:21:59Z|http://arxiv.org/abs/1703.04954v1|http://arxiv.org/pdf/1703.04954v1|Faster STR-IC-LCS computation via RLE|The constrained LCS problem asks one to find a longest common subsequence of two input strings $A$ and $B$ with some constraints. The STR-IC-LCS problem is a variant of the constrained LCS problem, where the solution must include a given constraint string $C$ as a substring. Given two strings $A$ and $B$ of respective lengths $M$ and $N$, and a constraint string $C$ of length at most $\min\{M, N\}$, the best known algorithm for the STR-IC-LCS problem, proposed by Deorowicz~({\em Inf. Process. Lett.}, 11:423--426, 2012), runs in $O(MN)$ time. In this work, we present an $O(mN + nM)$-time solution to the STR-IC-LCS problem, where $m$ and $n$ denote the sizes of the run-length encodings of $A$ and $B$, respectively. Since $m \leq M$ and $n \leq N$ always hold, our algorithm is always as fast as Deorowicz's algorithm, and is faster when input strings are compressible via RLE.|['Keita Kuboi', 'Yuta Fujishige', 'Shunsuke Inenaga', 'Hideo Bannai', 'Masayuki Takeda']|['cs.DS']
2017-03-16T23:32:43Z|2017-03-14T23:06:33Z|http://arxiv.org/abs/1703.04814v1|http://arxiv.org/pdf/1703.04814v1|Near-Optimal Compression for the Planar Graph Metric|"The Planar Graph Metric Compression Problem is to compactly encode the distances among $k$ nodes in a planar graph of size $n$. Two na\""ive solutions are to store the graph using $O(n)$ bits, or to explicitly store the distance matrix with $O(k^2 \log{n})$ bits. The only lower bounds are from the seminal work of Gavoille, Peleg, Prennes, and Raz [SODA'01], who rule out compressions into a polynomially smaller number of bits, for {\em weighted} planar graphs, but leave a large gap for unweighted planar graphs. For example, when $k=\sqrt{n}$, the upper bound is $O(n)$ and their constructions imply an $\Omega(n^{3/4})$ lower bound. This gap is directly related to other major open questions in labelling schemes, dynamic algorithms, and compact routing.   Our main result is a new compression of the planar graph metric into $\tilde{O}(\min (k^2 , \sqrt{k\cdot n}))$ bits, which is optimal up to log factors. Our data structure breaks an $\Omega(k^2)$ lower bound of Krauthgamer, Nguyen, and Zondiner [SICOMP'14] for compression using minors, and the lower bound of Gavoille et al. for compression of weighted planar graphs. This is an unexpected and decisive proof that weights can make planar graphs inherently more complex. Moreover, we design a new {\em Subset Distance Oracle} for planar graphs with $\tilde O(\sqrt{k\cdot n})$ space, and $\tilde O(n^{3/4})$ query time.   Our work carries strong messages to related fields. In particular, the famous $O(n^{1/2})$ vs. $\Omega(n^{1/3})$ gap for distance labelling schemes in planar graphs {\em cannot} be resolved with the current lower bound techniques."|['Amir Abboud', 'Pawel Gawrychowski', 'Shay Mozes', 'Oren Weimann']|['cs.DS']
2017-03-16T23:32:43Z|2017-03-14T22:16:53Z|http://arxiv.org/abs/1703.04769v1|http://arxiv.org/pdf/1703.04769v1|The Stochastic Container Relocation Problem|"The Container Relocation Problem (CRP) is concerned with finding a sequence of moves of containers that minimizes the number of relocations needed to retrieve all containers, while respecting a given order of retrieval. However, the assumption of knowing the full retrieval order of containers is particularly unrealistic in real operations. This paper studies the stochastic CRP (SCRP), which relaxes this assumption. A new multi-stage stochastic model, called the batch model, is introduced, motivated, and compared with an existing model (the online model). The two main contributions are an optimal algorithm called Pruning-Best-First-Search (PBFS) and a randomized approximate algorithm called PBFS-Approximate with a bounded average error. Both algorithms, applicable in the batch and online models, are based on a new family of lower bounds for which we show some theoretical properties. Moreover, we introduce two new heuristics outperforming the best existing heuristics. Algorithms, bounds and heuristics are tested in an extensive computational section. Finally, based on strong computational evidence, we conjecture the optimality of the ""Leveling"" heuristic in a special ""no information"" case, where at any retrieval stage, any of the remaining containers is equally likely to be retrieved next."|['Virgile Galle', 'Setareh Borjian Boroujeni', 'Vahideh H. Manshadi', 'Cynthia Barnhart', 'Patrick Jaillet']|['cs.DS']
2017-03-16T23:32:43Z|2017-03-14T18:49:57Z|http://arxiv.org/abs/1703.04664v1|http://arxiv.org/pdf/1703.04664v1|Optimal Densification for Fast and Accurate Minwise Hashing|Minwise hashing is a fundamental and one of the most successful hashing algorithm in the literature. Recent advances based on the idea of densification~\cite{Proc:OneHashLSH_ICML14,Proc:Shrivastava_UAI14} have shown that it is possible to compute $k$ minwise hashes, of a vector with $d$ nonzeros, in mere $(d + k)$ computations, a significant improvement over the classical $O(dk)$. These advances have led to an algorithmic improvement in the query complexity of traditional indexing algorithms based on minwise hashing. Unfortunately, the variance of the current densification techniques is unnecessarily high, which leads to significantly poor accuracy compared to vanilla minwise hashing, especially when the data is sparse. In this paper, we provide a novel densification scheme which relies on carefully tailored 2-universal hashes. We show that the proposed scheme is variance-optimal, and without losing the runtime efficiency, it is significantly more accurate than existing densification techniques. As a result, we obtain a significantly efficient hashing scheme which has the same variance and collision probability as minwise hashing. Experimental evaluations on real sparse and high-dimensional datasets validate our claims. We believe that given the significant advantages, our method will replace minwise hashing implementations in practice.|['Anshumali Shrivastava']|['cs.DS', 'cs.LG']
2017-03-16T23:32:43Z|2017-03-13T16:18:01Z|http://arxiv.org/abs/1703.04466v1|http://arxiv.org/pdf/1703.04466v1|Bicriteria Rectilinear Shortest Paths among Rectilinear Obstacles in the   Plane|Given a rectilinear domain $\mathcal{P}$ of $h$ pairwise-disjoint rectilinear obstacles with a total of $n$ vertices in the plane, we study the problem of computing bicriteria rectilinear shortest paths between two points $s$ and $t$ in $\mathcal{P}$. Three types of bicriteria rectilinear paths are considered: minimum-link shortest paths, shortest minimum-link paths, and minimum-cost paths where the cost of a path is a non-decreasing function of both the number of edges and the length of the path. The one-point and two-point path queries are also considered. Algorithms for these problems have been given previously. Our contributions are threefold. First, we find a critical error in all previous algorithms. Second, we correct the error in a not-so-trivial way. Third, we further improve the algorithms so that they are even faster than the previous (incorrect) algorithms when $h$ is relatively small. For example, for the minimum-link shortest paths, we obtain the following results. Our algorithm computes a minimum-link shortest $s$-$t$ path in $O(n+h\log^{3/2} h)$ time. For the one-point queries, we build a data structure of size $O(n+ h\log h)$ in $O(n+h\log^{3/2} h)$ time for a source point $s$, such that given any query point $t$, a minimum-link shortest $s$-$t$ path can be determined in $O(\log n)$ time. For the two-point queries, with $O(n+h^2\log^2 h)$ time and space preprocessing, a minimum-link shortest $s$-$t$ path can be determined in $O(\log n+\log^2 h)$ time for any two query points $s$ and $t$; alternatively, with $O(n+h^2\cdot \log^{2} h \cdot 4^{\sqrt{\log h}})$ time and $O(n+h^2\cdot \log h \cdot 4^{\sqrt{\log h}})$ space preprocessing, we can answer each two-point query in $O(\log n)$ time.|['Haitao Wang']|['cs.CG', 'cs.DS']
2017-03-16T23:32:52Z|2017-03-13T13:31:17Z|http://arxiv.org/abs/1703.04381v1|http://arxiv.org/pdf/1703.04381v1|On the Transformation Capability of Feasible Mechanisms for Programmable   Matter|In this work, we study theoretical models of \emph{programmable matter} systems. The systems under consideration consist of spherical modules, kept together by magnetic forces and able to perform two minimal mechanical operations (or movements): \emph{rotate} around a neighbor and \emph{slide} over a line. In terms of modeling, there are $n$ nodes arranged in a 2-dimensional grid and forming some initial \emph{shape}. The goal is for the initial shape $A$ to \emph{transform} to some target shape $B$ by a sequence of movements. Most of the paper focuses on \emph{transformability} questions, meaning whether it is in principle feasible to transform a given shape to another. We first consider the case in which only rotation is available to the nodes. Our main result is that deciding whether two given shapes $A$ and $B$ can be transformed to each other, is in $\mathbf{P}$. We then insist on rotation only and impose the restriction that the nodes must maintain global connectivity throughout the transformation. We prove that the corresponding transformability question is in $\mathbf{PSPACE}$ and study the problem of determining the minimum \emph{seeds} that can make feasible, otherwise infeasible transformations. Next we allow both rotations and slidings and prove universality: any two connected shapes $A,B$ of the same order, can be transformed to each other without breaking connectivity. The worst-case number of movements of the generic strategy is $\Omega(n^2)$. We improve this to $O(n)$ parallel time, by a pipelining strategy, and prove optimality of both by matching lower bounds. In the last part of the paper, we turn our attention to distributed transformations. The nodes are now distributed processes able to perform communicate-compute-move rounds. We provide distributed algorithms for a general type of transformations.|['Othon Michail', 'George Skretas', 'Paul G. Spirakis']|['cs.DS', 'cs.DC', 'cs.RO']
2017-03-16T23:32:52Z|2017-03-13T02:57:50Z|http://arxiv.org/abs/1703.04230v1|http://arxiv.org/pdf/1703.04230v1|Improved approximation algorithms for $k$-connected $m$-dominating set   problems|A graph is $k$-connected if it has $k$ internally-disjoint paths between every pair of nodes. A subset $S$ of nodes in a graph $G$ is a $k$-connected set if the subgraph $G[S]$ induced by $S$ is $k$-connected; $S$ is an $m$-dominating set if every $v \in V \setminus S$ has at least $m$ neighbors in $S$. If $S$ is both $k$-connected and $m$-dominating then $S$ is a $k$-connected $m$-dominating set, or $(k,m)$-cds for short. In the $k$-Connected $m$-Dominating Set ($(k,m)$-CDS) problem the goal is to find a minimum weight $(k,m)$-cds in a node-weighted graph. We consider the case $m \geq k$ and obtain the following approximation ratios. For unit disc-graphs we obtain ratio $O(k\ln k)$, improving the previous ratio $O(k^2 \ln k)$. For general graphs we obtain the first non-trivial approximation ratio $O(k^2 \ln n)$.|['Zeev Nutov']|['cs.DS']
2017-03-16T23:32:52Z|2017-03-12T17:11:49Z|http://arxiv.org/abs/1703.04143v1|http://arxiv.org/pdf/1703.04143v1|Bernoulli Factories and Black-Box Reductions in Mechanism Design|"We provide a polynomial time reduction from Bayesian incentive compatible mechanism design to Bayesian algorithm design for welfare maximization problems. Unlike prior results, our reduction achieves exact incentive compatibility for problems with multi-dimensional and continuous type spaces. The key technical barrier preventing exact incentive compatibility in prior black-box reductions is that repairing violations of incentive constraints requires understanding the distribution of the mechanism's output. Reductions that instead estimate the output distribution by sampling inevitably suffer from sampling error, which typically precludes exact incentive compatibility.   We overcome this barrier by employing and generalizing the computational model in the literature on Bernoulli Factories. In a Bernoulli factory problem, one is given a function mapping the bias of an ""input coin"" to that of an ""output coin"", and the challenge is to efficiently simulate the output coin given sample access to the input coin. We generalize this to the ""expectations from samples"" computational model, in which an instance is specified by a function mapping the expected values of a set of input distributions to a distribution over outcomes. The challenge is to give a polynomial time algorithm that exactly samples from the distribution over outcomes given only sample access to the input distributions. In this model, we give a polynomial time algorithm for the exponential weights: expected values of the input distributions correspond to the weights of alternatives and we wish to select an alternative with probability proportional to an exponential function of its weight. This algorithm is the key ingredient in designing an incentive compatible mechanism for bipartite matching, which can be used to make the approximately incentive compatible reduction of Hartline et al. (2015) exactly incentive compatible."|['Shaddin Dughmi', 'Jason Hartline', 'Robert Kleinberg', 'Rad Niazadeh']|['cs.GT', 'cs.CC', 'cs.DS', 'math.PR']
2017-03-16T23:32:52Z|2017-03-11T23:16:23Z|http://arxiv.org/abs/1703.04040v1|http://arxiv.org/abs/1703.04040v1|Locality-sensitive hashing of curves|We study data structures for storing a set of polygonal curves in ${\rm R}^d$ such that, given a query curve, we can efficiently retrieve similar curves from the set, where similarity is measured using the discrete Fr\'echet distance or the dynamic time warping distance. To this end we devise the first locality-sensitive hashing schemes for these distance measures. A major challenge is posed by the fact that these distance measures internally optimize the alignment between the curves. We give solutions for different types of alignments including constrained and unconstrained versions. For unconstrained alignments, we improve over a result by Indyk from 2002 for short curves. Let $n$ be the number of input curves and let $m$ be the maximum complexity of a curve in the input. In the particular case where $m \leq \frac{\alpha}{4d} \log n$, for some fixed $\alpha>0$, our solutions imply an approximate near-neighbor data structure for the discrete Fr\'echet distance that uses space in $O(n^{1+\alpha}\log n)$ and achieves query time in $O(n^{\alpha}\log^2 n)$ and constant approximation factor. Furthermore, our solutions provide a trade-off between approximation quality and computational performance: for any parameter $k \in [m]$, we can give a data structure that uses space in $O(2^{2k}m^{k-1} n \log n + nm)$, answers queries in $O( 2^{2k} m^{k}\log n)$ time and achieves approximation factor in $O(m/k)$.|['Anne Driemel', 'Francesco Silvestri']|['cs.CG', 'cs.DS', 'cs.IR', 'F.2.2']
2017-03-16T23:32:52Z|2017-03-11T16:53:04Z|http://arxiv.org/abs/1703.03998v1|http://arxiv.org/pdf/1703.03998v1|The Weighted Matching Approach to Maximum Cardinality Matching|Several papers have achieved time $O(\sqrt n m)$ for cardinality matching, starting from first principles. This results in a long derivation. We simplify the task by employing well-known concepts for maximum weight matching. We use Edmonds' algorithm to derive the structure of shortest augmenting paths. We extend this to a complete algorithm for maximum cardinality matching in time $O(\sqrt n m)$.|['Harold N. Gabow']|['cs.DS']
2017-03-16T23:32:52Z|2017-03-11T12:24:19Z|http://arxiv.org/abs/1703.03963v1|http://arxiv.org/pdf/1703.03963v1|On Solving Travelling Salesman Problem with Vertex Requisitions|We consider the Travelling Salesman Problem with Vertex Requisitions, where for each position of the tour at most two possible vertices are given. It is known that the problem is strongly NP-hard. The proposed algorithm for this problem has less time complexity compared to the previously known one. In particular, almost all feasible instances of the problem are solvable in O(n) time using the new algorithm, where n is the number of vertices. The developed approach also helps in fast enumeration of a neighborhood in the local search and yields an integer programming model with O(n) binary variables for the problem.|['Anton Eremeev', 'Yulia Kovalenko']|['cs.DS']
2017-03-16T23:32:52Z|2017-03-11T03:28:30Z|http://arxiv.org/abs/1703.03900v1|http://arxiv.org/pdf/1703.03900v1|Core Maintenance in Dynamic Graphs: A Parallel Approach based on   Matching|The core number of a vertex is a basic index depicting cohesiveness of a graph, and has been widely used in large-scale graph analytics. In this paper, we study the update of core numbers of vertices in dynamic graphs with edge insertions/deletions, which is known as the core maintenance problem. Different from previous approaches that just focus on the case of single-edge insertion/deletion and sequentially handle the edges when multiple edges are inserted/deleted, we investigate the parallelism in the core maintenance procedure. Specifically, we show that if the inserted/deleted edges constitute a matching, the core number update with respect to each inserted/deleted edge can be handled in parallel. Based on this key observation, we propose parallel algorithms for core maintenance in both cases of edge insertions and deletions. We conduct extensive experiments to evaluate the efficiency, stability, parallelism and scalability of our algorithms on different types of real-world and synthetic graphs. Comparing with sequential approaches, our algorithms can improve the core maintenance efficiency significantly.|['Na Wang', 'Dongxiao Yu', 'Hai Jin', 'Qiang-Sheng Hua', 'Xuanhua Shi', 'Xia Xie']|['cs.DS']
2017-03-16T23:32:52Z|2017-03-10T22:25:56Z|http://arxiv.org/abs/1703.03859v1|http://arxiv.org/abs/1703.03859v1|Markov Chain Lifting and Distributed ADMM|The time to converge to the steady state of a finite Markov chain can be greatly reduced by a lifting operation, which creates a new Markov chain on an expanded state space. For a class of quadratic objectives, we show an analogous behavior where a distributed ADMM algorithm can be seen as a lifting of Gradient Descent algorithm. This provides a deep insight for its faster convergence rate under optimal parameter tuning. We conjecture that this gain is always present, as opposed to the lifting of a Markov chain which sometimes only provides a marginal speedup.|['Guilherme França', 'José Bento']|['stat.ML', 'cs.DS', 'cs.IT', 'cs.LG', 'math.IT', 'math.OC']
2017-03-16T23:32:52Z|2017-03-10T21:51:50Z|http://arxiv.org/abs/1703.03849v1|http://arxiv.org/pdf/1703.03849v1|A note on approximate strengths of edges in a hypergraph|Let $H=(V,E)$ be an edge-weighted hypergraph of rank $r$. Kogan and Krauthgamer extended Bencz\'{u}r and Karger's random sampling scheme for cut sparsification from graphs to hypergraphs. The sampling requires an algorithm for computing the approximate strengths of edges. In this note we extend the algorithm for graphs to hypergraphs and describe a near-linear time algorithm to compute approximate strengths of edges; we build on a sparsification result for hypergraphs from our recent work. Combined with prior results we obtain faster algorithms for finding $(1+\epsilon)$-approximate mincuts when the rank of the hypergraph is small.|['Chandra Chekuri', 'Chao Xu']|['cs.DS']
2017-03-16T23:32:52Z|2017-03-10T09:58:40Z|http://arxiv.org/abs/1703.03603v1|http://arxiv.org/pdf/1703.03603v1|The Densest Subgraph Problem with a Convex/Concave Size Function|In the densest subgraph problem, given an edge-weighted undirected graph $G=(V,E,w)$, we are asked to find $S\subseteq V$ that maximizes the density, i.e., $w(S)/ S $, where $w(S)$ is the sum of weights of the edges in the subgraph induced by $S$. This problem has often been employed in a wide variety of graph mining applications. However, the problem has a drawback; it may happen that the obtained subset is too large or too small in comparison with the size desired in the application at hand. In this study, we address the size issue of the densest subgraph problem by generalizing the density of $S\subseteq V$. Specifically, we introduce the $f$-density of $S\subseteq V$, which is defined as $w(S)/f( S )$, where $f:\mathbb{Z}_{\geq 0}\rightarrow \mathbb{R}_{\geq 0}$ is a monotonically non-decreasing function. In the $f$-densest subgraph problem ($f$-DS), we aim to find $S\subseteq V$ that maximizes the $f$-density $w(S)/f( S )$. Although $f$-DS does not explicitly specify the size of the output subset of vertices, we can handle the above size issue using a convex/concave size function $f$ appropriately. For $f$-DS with convex function $f$, we propose a nearly-linear-time algorithm with a provable approximation guarantee. On the other hand, for $f$-DS with concave function $f$, we propose an LP-based exact algorithm, a flow-based $O( V ^3)$-time exact algorithm for unweighted graphs, and a nearly-linear-time approximation algorithm.|['Yasushi Kawase', 'Atsushi Miyauchi']|['cs.DS', 'cs.DM', 'cs.SI']
2017-03-16T23:32:56Z|2017-03-10T08:35:24Z|http://arxiv.org/abs/1703.03575v1|http://arxiv.org/pdf/1703.03575v1|Crossing the Logarithmic Barrier for Dynamic Boolean Data Structure   Lower Bounds|"This paper proves the first super-logarithmic lower bounds on the cell probe complexity of dynamic boolean (a.k.a. decision) data structure problems, a long-standing milestone in data structure lower bounds.   We introduce a new method for proving dynamic cell probe lower bounds and use it to prove a $\tilde{\Omega}(\log^{1.5} n)$ lower bound on the operational time of a wide range of boolean data structure problems, most notably, on the query time of dynamic range counting over $\mathbb{F}_2$ ([Pat07]). Proving an $\omega(\lg n)$ lower bound for this problem was explicitly posed as one of five important open problems in the late Mihai P\v{a}tra\c{s}cu's obituary [Tho13]. This result also implies the first $\omega(\lg n)$ lower bound for the classical 2D range counting problem, one of the most fundamental data structure problems in computational geometry and spatial databases. We derive similar lower bounds for boolean versions of dynamic polynomial evaluation and 2D rectangle stabbing, and for the (non-boolean) problems of range selection and range median.   Our technical centerpiece is a new way of ""weakly"" simulating dynamic data structures using efficient one-way communication protocols with small advantage over random guessing. This simulation involves a surprising excursion to low-degree (Chebychev) polynomials which may be of independent interest, and offers an entirely new algorithmic angle on the ""cell sampling"" method of Panigrahy et al. [PTW10]."|['Kasper Green Larsen', 'Omri Weinstein', 'Huacheng Yu']|['cs.DS', 'cs.CC', 'cs.CG', 'cs.IT', 'math.IT']
2017-03-16T23:32:56Z|2017-03-09T22:47:10Z|http://arxiv.org/abs/1703.03484v1|http://arxiv.org/pdf/1703.03484v1|Combinatorial Auctions with Online XOS Bidders|In combinatorial auctions, a designer must decide how to allocate a set of indivisible items amongst a set of bidders. Each bidder has a valuation function which gives the utility they obtain from any subset of the items. Our focus is specifically on welfare maximization, where the objective is to maximize the sum of valuations that the bidders place on the items that they were allocated (the valuation functions are assumed to be reported truthfully). We analyze an online problem in which the algorithm is not given the set of bidders in advance. Instead, the bidders are revealed sequentially in a uniformly random order, similarly to secretary problems. The algorithm must make an irrevocable decision about which items to allocate to the current bidder before the next one is revealed. When the valuation functions lie in the class $XOS$ (which includes submodular functions), we provide a black box reduction from offline to online optimization. Specifically, given an $\alpha$-approximation algorithm for offline welfare maximization, we show how to create a $(0.199 \alpha)$-approximation algorithm for the online problem. Our algorithm draws on connections to secretary problems; in fact, we show that the online welfare maximization problem itself can be viewed as a particular kind of secretary problem with nonuniform arrival order.|['Shaddin Dughmi', 'Bryan Wilder']|['cs.GT', 'cs.DS']
2017-03-16T23:32:56Z|2017-03-09T15:46:25Z|http://arxiv.org/abs/1703.03304v1|http://arxiv.org/pdf/1703.03304v1|On low rank-width colorings|We introduce the concept of low rank-width colorings, generalising the notion of low tree-depth colorings introduced by Ne\v{s}et\v{r}il and Ossona de Mendez in [Grad and classes with bounded expansion I. Decompositions. EJC, 2008]. We say that a class $\mathcal{C}$ of graphs admits low rank-width colourings if there exist functions $N\colon \mathbb{N}\rightarrow\mathbb{N}$ and $Q\colon \mathbb{N}\rightarrow\mathbb{N}$ such that for all $p\in \mathbb{N}$, every graph $G\in \mathcal{C}$ can be vertex colored with at most $N(p)$ colors such that the union of any $i\leq p$ color classes induces a subgraph of rank-width at most $Q(i)$.   Graph classes admitting low rank-width colorings strictly generalize graph classes admitting low tree-depth colorings and graph classes of bounded rank-width. We prove that for every graph class $\mathcal{C}$ of bounded expansion and every positive integer $r$, the class $\{G^r\colon G\in \mathcal{C}\}$ of $r$th powers of graphs from $\mathcal{C}$, as well as the classes of unit interval graphs and bipartite permutation graphs admit low rank-width colorings. All of these classes have unbounded rank-width and do not admit low tree-depth colorings. We also show that the classes of interval graphs and permutation graphs do not admit low rank-width colorings. As interesting side properties, we prove that every graph class admitting low rank-width colorings has the Erd\H{o}s-Hajnal property and is $\chi$-bounded.|['O-joung Kwon', 'Michał Pilipczuk', 'Sebastian Siebertz']|['cs.DS', 'math.CO']
2017-03-16T23:32:56Z|2017-03-09T06:08:01Z|http://arxiv.org/abs/1703.03147v1|http://arxiv.org/pdf/1703.03147v1|Juggling Functions Inside a Database|"We define and study the Functional Aggregate Query (FAQ) problem, which captures common computational tasks across a very wide range of domains including relational databases, logic, matrix and tensor computation, probabilistic graphical models, constraint satisfaction, and signal processing. Simply put, an FAQ is a declarative way of defining a new function from a database of input functions.   We present ""InsideOut"", a dynamic programming algorithm, to evaluate an FAQ. The algorithm rewrites the input query into a set of easier-to-compute FAQ sub-queries. Each sub-query is then evaluated using a worst-case optimal relational join algorithm. The topic of designing algorithms to optimally evaluate the classic multiway join problem has seen exciting developments in the past few years. Our framework tightly connects these new ideas in database theory with a vast number of application areas in a coherent manner, showing potentially that a good database engine can be a general-purpose constraint solver, relational data store, graphical model inference engine, and matrix/tensor computation processor all at once.   The InsideOut algorithm is very simple, as shall be described in this paper. Yet, in spite of solving an extremely general problem, its runtime either is as good as or improves upon the best known algorithm for the applications that FAQ specializes to. These corollaries include computational tasks in graphical model inference, matrix/tensor operations, relational joins, and logic. Better yet, InsideOut can be used within any database engine, because it is basically a principled way of rewriting queries. Indeed, it is already part of the LogicBlox database engine, helping efficiently answer traditional database queries, graphical model inference queries, and train a large class of machine learning models inside the database itself."|['Mahmoud Abo Khamis', 'Hung Q. Ngo', 'Atri Rudra']|['cs.DB', 'cs.DS', 'cs.LO']
2017-03-16T23:32:56Z|2017-03-08T21:50:06Z|http://arxiv.org/abs/1703.03048v1|http://arxiv.org/pdf/1703.03048v1|Quickest Visibility Queries in Polygonal Domains|Let $s$ be a point in a polygonal domain $\mathcal{P}$ of $h-1$ holes and $n$ vertices. We consider a quickest visibility query problem. Given a query point $q$ in $\mathcal{P}$, the goal is to find a shortest path in $\mathcal{P}$ to move from $s$ to see $q$ as quickly as possible. Previously, Arkin et al. (SoCG 2015) built a data structure of size $O(n^22^{\alpha(n)}\log n)$ that can answer each query in $O(K\log^2 n)$ time, where $\alpha(n)$ is the inverse Ackermann function and $K$ is the size of the visibility polygon of $q$ in $\mathcal{P}$ (and $K$ can be $\Theta(n)$ in the worst case). In this paper, we present a new data structure of size $O(n\log h + h^2)$ that can answer each query in $O(h\log h\log n)$ time. Our result improves the previous work when $h$ is relatively small. In particular, if $h$ is a constant, then our result even matches the best result for the simple polygon case (i.e., $h=1$), which is optimal. As a by-product, we also have a new algorithm for a shortest-path-to-segment query problem. Given a query line segment $\tau$ in $\mathcal{P}$, the query seeks a shortest path from $s$ to all points of $\tau$. Previously, Arkin et al. gave a data structure of size $O(n^22^{\alpha(n)}\log n)$ that can answer each query in $O(\log^2 n)$ time, and another data structure of size $O(n^3\log n)$ with $O(\log n)$ query time. We present a data structure of size $O(n)$ with query time $O(h\log \frac{n}{h})$, which also favors small values of $h$ and is optimal when $h=O(1)$.|['Haitao Wang']|['cs.CG', 'cs.DS']
2017-03-16T23:32:56Z|2017-03-08T15:16:11Z|http://arxiv.org/abs/1703.02867v1|http://arxiv.org/pdf/1703.02867v1|Electoral District Design via Constrained Clustering|The paper studies the electoral district design problem where municipalities of a state have to be grouped into districts of nearly equal population while obeying certain politically motivated requirements. We develop a general framework for electoral district design that is based on the close connection of constrained geometric clustering and diagrams. The approach is computationally efficient and flexible enough to pursue various conflicting juridical demands for the shape of the districts. We demonstrate the practicability of our methodology for electoral districting in Germany.|['Andreas Brieden', 'Peter Gritzmann', 'Fabian Klemm']|['cs.DS', 'math.CO', '90C90']
2017-03-16T23:32:56Z|2017-03-08T15:16:05Z|http://arxiv.org/abs/1703.02866v1|http://arxiv.org/pdf/1703.02866v1|The Half-integral Erdös-Pósa Property for Non-null Cycles|"A Group Labeled Graph is a pair $(G,\Lambda)$ where $G$ is an oriented graph and $\Lambda$ is a mapping from the arcs of $G$ to elements of a group. A (not necessarily directed) cycle $C$ is called non-null if for any cyclic ordering of the arcs in $C$, the group element obtained by `adding' the labels on forward arcs and `subtracting' the labels on reverse arcs is not the identity element of the group. Non-null cycles in group labeled graphs generalize several well-known graph structures, including odd cycles.   In this paper, we prove that non-null cycles on Group Labeled Graphs have the half-integral Erd\""os-P\'osa property. That is, there is a function $f:{\mathbb N}\to {\mathbb N}$ such that for any $k\in {\mathbb N}$, any group labeled graph $(G,\Lambda)$ has a set of $k$ non-null cycles such that each vertex of $G$ appears in at most two of these cycles or there is a set of at most $f(k)$ vertices that intersects every non-null cycle. Since it is known that non-null cycles do not have the integeral Erd\""os-P\'osa property in general, a half-integral Erd\""os-P\'osa result is the best one could hope for."|['Daniel Lokshtanov', 'M. S. Ramanujan', 'Saket Saurabh']|['cs.DM', 'cs.DS']
2017-03-16T23:32:56Z|2017-03-08T10:56:03Z|http://arxiv.org/abs/1703.02784v1|http://arxiv.org/pdf/1703.02784v1|$K$-Best Solutions of MSO Problems on Tree-Decomposable Graphs|We show that, for any graph optimization problem in which the feasible solutions can be expressed by a formula in monadic second-order logic describing sets of vertices or edges and in which the goal is to minimize the sum of the weights in the selected sets, we can find the $k$ best solutions for $n$-vertex graphs of bounded treewidth in time $\mathcal O(n+k\log n)$. In particular, this applies to the problem of finding the $k$ shortest simple paths between given vertices in directed graphs of bounded treewidth, giving an exponential speedup in the per-path cost over previous algorithms.|['David Eppstein', 'Denis Kurz']|['cs.DS', 'G.2.2']
2017-03-16T23:32:56Z|2017-03-08T04:18:58Z|http://arxiv.org/abs/1703.02693v1|http://arxiv.org/pdf/1703.02693v1|Stream Aggregation Through Order Sampling|This paper introduces a new single-pass reservoir weighted-sampling stream aggregation algorithm, Priority Sample and Hold. PrSH combines aspects of the well-known Sample and Hold algorithm with Priority Sampling. In particular, it achieves a reduced computational cost for rate adaptation in a fixed cache by using a single persistent random variable across the lifetime of each key in the cache. The basic approach can be supplemented with a Sample and Hold pre-sampling stage with a sampling rate adaptation controlled by PrSH. We prove that PrSH provides unbiased estimates of the true aggregates. We analyze the computational complexity of PrSH and its variants, and provide a detailed evaluation of its accuracy on synthetic and trace data. Weighted relative error is reduced by 40% to 65% at sampling rates of 5% to 17%, relative to Adaptive Sample and Hold; there is also substantial improvement for rank queries.|['Nick Duffield', 'Yunhong Xu', 'Liangzhen Xia', 'Nesreen Ahmed', 'Minlan Yu']|['cs.DS']
2017-03-16T23:32:56Z|2017-03-08T03:56:27Z|http://arxiv.org/abs/1703.02690v1|http://arxiv.org/pdf/1703.02690v1|Leveraging Sparsity for Efficient Submodular Data Summarization|The facility location problem is widely used for summarizing large datasets and has additional applications in sensor placement, image retrieval, and clustering. One difficulty of this problem is that submodular optimization algorithms require the calculation of pairwise benefits for all items in the dataset. This is infeasible for large problems, so recent work proposed to only calculate nearest neighbor benefits. One limitation is that several strong assumptions were invoked to obtain provable approximation guarantees. In this paper we establish that these extra assumptions are not necessary---solving the sparsified problem will be almost optimal under the standard assumptions of the problem. We then analyze a different method of sparsification that is a better model for methods such as Locality Sensitive Hashing to accelerate the nearest neighbor computations and extend the use of the problem to a broader family of similarities. We validate our approach by demonstrating that it rapidly generates interpretable summaries.|['Erik M. Lindgren', 'Shanshan Wu', 'Alexandros G. Dimakis']|['stat.ML', 'cs.DS', 'cs.IT', 'cs.LG', 'math.IT']
2017-03-16T23:33:00Z|2017-03-08T03:55:27Z|http://arxiv.org/abs/1703.02689v1|http://arxiv.org/pdf/1703.02689v1|Exact MAP Inference by Avoiding Fractional Vertices|Given a graphical model, one essential problem is MAP inference, that is, finding the most likely configuration of states according to the model. Although this problem is NP-hard, large instances can be solved in practice. A major open question is to explain why this is true. We give a natural condition under which we can provably perform MAP inference in polynomial time. We require that the number of fractional vertices in the LP relaxation exceeding the optimal solution is bounded by a polynomial in the problem size. This resolves an open question by Dimakis, Gohari, and Wainwright. In contrast, for general LP relaxations of integer programs, known techniques can only handle a constant number of fractional vertices whose value exceeds the optimal solution. We experimentally verify this condition and demonstrate how efficient various integer programming methods are at removing fractional solutions.|['Erik M. Lindgren', 'Alexandros G. Dimakis', 'Adam Klivans']|['stat.ML', 'cs.DS', 'cs.IT', 'cs.LG', 'math.IT']
2017-03-16T23:33:00Z|2017-03-07T22:18:35Z|http://arxiv.org/abs/1703.02625v1|http://arxiv.org/pdf/1703.02625v1|On Sampling from Massive Graph Streams|We propose Graph Priority Sampling (GPS), a new paradigm for order-based reservoir sampling from massive streams of graph edges. GPS provides a general way to weight edge sampling according to auxiliary and/or size variables so as to accomplish various estimation goals of graph properties. In the context of subgraph counting, we show how edge sampling weights can be chosen so as to minimize the estimation variance of counts of specified sets of subgraphs. In distinction with many prior graph sampling schemes, GPS separates the functions of edge sampling and subgraph estimation. We propose two estimation frameworks: (1) Post-Stream estimation, to allow GPS to construct a reference sample of edges to support retrospective graph queries, and (2) In-Stream estimation, to allow GPS to obtain lower variance estimates by incrementally updating the subgraph count estimates during stream processing. Unbiasedness of subgraph estimators is established through a new Martingale formulation of graph stream order sampling, which shows that subgraph estimators, written as a product of constituent edge estimators are unbiased, even when computed at different points in the stream. The separation of estimation and sampling enables significant resource savings relative to previous work. We illustrate our framework with applications to triangle and wedge counting. We perform a large-scale experimental study on real-world graphs from various domains and types. GPS achieves high accuracy with less than 1% error for triangle and wedge counting, while storing a small fraction of the graph with average update times of a few microseconds per edge. Notably, for a large Twitter graph with more than 260M edges, GPS accurately estimates triangle counts with less than 1% error, while storing only 40K edges.|['Nesreen K. Ahmed', 'Nick Duffield', 'Theodore Willke', 'Ryan A. Rossi']|['cs.SI', 'cs.DS', 'cs.IR', 'math.ST', 'stat.TH']
2017-03-16T23:33:00Z|2017-03-07T17:35:51Z|http://arxiv.org/abs/1703.02485v1|http://arxiv.org/pdf/1703.02485v1|Certifying coloring algorithms for graphs without long induced paths|Let $P_k$ be a path, $C_k$ a cycle on $k$ vertices, and $K_{k,k}$ a complete bipartite graph with $k$ vertices on each side of the bipartition. We prove that (1) for any integers $k, t>0$ and a graph $H$ there are finitely many subgraph minimal graphs with no induced $P_k$ and $K_{t,t}$ that are not $H$-colorable and (2) for any integer $k>4$ there are finitely many subgraph minimal graphs with no induced $P_k$ that are not $C_{k-2}$-colorable.   The former generalizes the result of Hell and Huang [Complexity of coloring graphs without paths and cycles, Discrete Appl. Math. 216: 211--232 (2017)] and the latter extends a result of Bruce, Hoang, and Sawada [A certifying algorithm for 3-colorability of $P_5$-Free Graphs, ISAAC 2009: 594--604]. Both our results lead to polynomial-time certifying algorithms for the corresponding coloring problems.|['Marcin Kamiński', 'Anna Pstrucha']|['math.CO', 'cs.DS']
2017-03-16T23:33:00Z|2017-03-07T14:48:15Z|http://arxiv.org/abs/1703.02411v1|http://arxiv.org/pdf/1703.02411v1|A Simple Deterministic Distributed MST Algorithm, with Near-Optimal Time   and Message Complexities|Distributed minimum spanning tree (MST) problem is one of the most central and fundamental problems in distributed graph algorithms. Garay et al. \cite{GKP98,KP98} devised an algorithm with running time $O(D + \sqrt{n} \cdot \log^* n)$, where $D$ is the hop-diameter of the input $n$-vertex $m$-edge graph, and with message complexity $O(m + n^{3/2})$. Peleg and Rubinovich \cite{PR99} showed that the running time of the algorithm of \cite{KP98} is essentially tight, and asked if one can achieve near-optimal running time **together with near-optimal message complexity**.   In a recent breakthrough, Pandurangan et al. \cite{PRS16} answered this question in the affirmative, and devised a **randomized** algorithm with time $\tilde{O}(D+ \sqrt{n})$ and message complexity $\tilde{O}(m)$. They asked if such a simultaneous time- and message-optimality can be achieved by a **deterministic** algorithm.   In this paper, building upon the work of \cite{PRS16}, we answer this question in the affirmative, and devise a **deterministic** algorithm that computes MST in time $O((D + \sqrt{n}) \cdot \log n)$, using $O(m \cdot \log n + n \log n \cdot \log^* n)$ messages. The polylogarithmic factors in the time and message complexities of our algorithm are significantly smaller than the respective factors in the result of \cite{PRS16}. Also, our algorithm and its analysis are very **simple** and self-contained, as opposed to rather complicated previous sublinear-time algorithms \cite{GKP98,KP98,E04b,PRS16}.|['Michael Elkin']|['cs.DS']
2017-03-16T23:33:00Z|2017-03-07T13:43:45Z|http://arxiv.org/abs/1703.02375v1|http://arxiv.org/pdf/1703.02375v1|Graph sketching-based Massive Data Clustering|In this paper, we address the problem of recovering arbitrary-shaped data clusters from massive datasets. We present DBMSTClu a new density-based non-parametric method working on a limited number of linear measurements i.e. a sketched version of the similarity graph $G$ between the $N$ objects to cluster. Unlike $k$-means, $k$-medians or $k$-medoids algorithms, it does not fail at distinguishing clusters with particular structures. No input parameter is needed contrarily to DBSCAN or the Spectral Clustering method. DBMSTClu as a graph-based technique relies on the similarity graph $G$ which costs theoretically $O(N^2)$ in memory. However, our algorithm follows the dynamic semi-streaming model by handling $G$ as a stream of edge weight updates and sketches it in one pass over the data into a compact structure requiring $O(\operatorname{poly} \operatorname{log} (N))$ space. Thanks to the property of the Minimum Spanning Tree (MST) for expressing the underlying structure of a graph, our algorithm successfully detects the right number of non-convex clusters by recovering an approximate MST from the graph sketch of $G$. We provide theoretical guarantees on the quality of the clustering partition and also demonstrate its advantage over the existing state-of-the-art on several datasets.|['Anne Morvan', 'Krzysztof Choromanski', 'Cédric Gouy-Pailler', 'Jamal Atif']|['cs.LG', 'cs.DS']
2017-03-16T23:33:00Z|2017-03-07T05:43:56Z|http://arxiv.org/abs/1703.02224v1|http://arxiv.org/pdf/1703.02224v1|Space-efficient K-MER algorithm for generalized suffix tree|Suffix trees have emerged to be very fast for pattern searching yielding O (m) time, where m is the pattern size. Unfortunately their high memory requirements make it impractical to work with huge amounts of data. We present a memory efficient algorithm of a generalized suffix tree which reduces the space size by a factor of 10 when the size of the pattern is known beforehand. Experiments on the chromosomes and Pizza&Chili corpus show significant advantages of our algorithm over standard linear time suffix tree construction in terms of memory usage for pattern searching.|['Freeson Kaniwa', 'Venu Madhav Kuthadi', 'Otlhapile Dinakenyane', 'Heiko Schroeder']|['cs.DS']
2017-03-16T23:33:00Z|2017-03-06T20:28:23Z|http://arxiv.org/abs/1703.02100v1|http://arxiv.org/pdf/1703.02100v1|Guarantees for Greedy Maximization of Non-submodular Functions with   Applications|We investigate the performance of the Greedy algorithm for cardinality constrained maximization of non-submodular nondecreasing set functions. While there are strong theoretical guarantees on the performance of Greedy for maximizing submodular functions, there are few guarantees for non-submodular ones. However, Greedy enjoys strong empirical performance for many important non-submodular functions, e.g., the Bayesian A-optimality objective in experimental design. We prove theoretical guarantees supporting the empirical performance. Our guarantees are characterized by the (generalized) submodularity ratio $\gamma$ and the (generalized) curvature $\alpha$. In particular, we prove that Greedy enjoys a tight approximation guarantee of $\frac{1}{\alpha}(1- e^{-\gamma\alpha})$ for cardinality constrained maximization. In addition, we bound the submodularity ratio and curvature for several important real-world objectives, e.g., the Bayesian A-optimality objective, the determinantal function of a square submatrix and certain linear programs with combinatorial constraints. We experimentally validate our theoretical findings for several real-world applications.|['Andrew An Bian', 'Joachim M. Buhmann', 'Andreas Krause', 'Sebastian Tschiatschek']|['cs.DM', 'cs.AI', 'cs.DS', 'cs.LG', 'math.OC']
2017-03-16T23:33:00Z|2017-03-06T19:01:03Z|http://arxiv.org/abs/1703.02059v1|http://arxiv.org/pdf/1703.02059v1|Cheshire: An Online Algorithm for Activity Maximization in Social   Networks|User engagement in social networks depends critically on the number of online actions their users take in the network. Can we design an algorithm that finds when to incentivize users to take actions to maximize the overall activity in a social network? In this paper, we model the number of online actions over time using multidimensional Hawkes processes, derive an alternate representation of these processes based on stochastic differential equations (SDEs) with jumps and, exploiting this alternate representation, address the above question from the perspective of stochastic optimal control of SDEs with jumps. We find that the optimal level of incentivized actions depends linearly on the current level of overall actions. Moreover, the coefficients of this linear relationship can be found by solving a matrix Riccati differential equation, which can be solved efficiently, and a first order differential equation, which has a closed form solution. As a result, we are able to design an efficient online algorithm, Cheshire, to sample the optimal times of the users' incentivized actions. Experiments on both synthetic and real data gathered from Twitter show that our algorithm is able to consistently maximize the number of online actions more effectively than the state of the art.|['Ali Zarezade', 'Abir De', 'Hamid Rabiee', 'Manuel Gomez Rodriguez']|['stat.ML', 'cs.DS', 'cs.LG', 'cs.SI']
2017-03-16T23:33:00Z|2017-03-07T14:17:51Z|http://arxiv.org/abs/1703.01939v2|http://arxiv.org/pdf/1703.01939v2|Distributed Exact Shortest Paths in Sublinear Time|"The distributed single-source shortest paths problem is one of the most fundamental and central problems in the message-passing distributed computing. Classical Bellman-Ford algorithm solves it in $O(n)$ time, where $n$ is the number of vertices in the input graph $G$. Peleg and Rubinovich (FOCS'99) showed a lower bound of $\tilde{\Omega}(D + \sqrt{n})$ for this problem, where $D$ is the hop-diameter of $G$.   Whether or not this problem can be solved in $o(n)$ time when $D$ is relatively small is a major notorious open question. Despite intensive research \cite{LP13,N14,HKN15,EN16,BKKL16} that yielded near-optimal algorithms for the approximate variant of this problem, no progress was reported for the original problem.   In this paper we answer this question in the affirmative. We devise an algorithm that requires $O((n \log n)^{5/6})$ time, for $D = O(\sqrt{n \log n})$, and $O(D^{1/3} \cdot (n \log n)^{2/3})$ time, for larger $D$. This running time is sublinear in $n$ in almost the entire range of parameters, specifically, for $D = o(n/\log^2 n)$.   We also devise the first algorithm with non-trivial complexity guarantees for computing exact shortest paths in the multipass semi-streaming model of computation.   From the technical viewpoint, our algorithm computes a hopset $G""$ of a skeleton graph $G'$ of $G$ without first computing $G'$ itself. We then conduct a Bellman-Ford exploration in $G' \cup G""$, while computing the required edges of $G'$ on the fly. As a result, our algorithm computes exactly those edges of $G'$ that it really needs, rather than computing approximately the entire $G'$."|['Michael Elkin']|['cs.DS']
2017-03-16T23:33:00Z|2017-03-06T15:03:55Z|http://arxiv.org/abs/1703.01913v1|http://arxiv.org/pdf/1703.01913v1|Near-Optimal Closeness Testing of Discrete Histogram Distributions|We investigate the problem of testing the equivalence between two discrete histograms. A {\em $k$-histogram} over $[n]$ is a probability distribution that is piecewise constant over some set of $k$ intervals over $[n]$. Histograms have been extensively studied in computer science and statistics. Given a set of samples from two $k$-histogram distributions $p, q$ over $[n]$, we want to distinguish (with high probability) between the cases that $p = q$ and $\ p-q\ _1 \geq \epsilon$. The main contribution of this paper is a new algorithm for this testing problem and a nearly matching information-theoretic lower bound. Specifically, the sample complexity of our algorithm matches our lower bound up to a logarithmic factor, improving on previous work by polynomial factors in the relevant parameters. Our algorithmic approach applies in a more general setting and yields improved sample upper bounds for testing closeness of other structured distributions as well.|['Ilias Diakonikolas', 'Daniel M. Kane', 'Vladimir Nikishkin']|['cs.DS', 'cs.IT', 'cs.LG', 'math.IT', 'math.ST', 'stat.TH']
2017-03-16T23:33:04Z|2017-03-06T14:52:58Z|http://arxiv.org/abs/1703.01905v1|http://arxiv.org/pdf/1703.01905v1|A randomized, efficient algorithm for 3SAT|In this paper I present a 3SAT algorithm based on the randomized algorithm of Papadimitriou from 1991, and Schoning from 1991. We also show that this algorithm finds a solution (if it exists) for a 3SAT problem with high probability in polynomial time.|['Cristian Dumitrescu']|['cs.DS']
2017-03-16T23:33:04Z|2017-03-06T12:55:21Z|http://arxiv.org/abs/1703.01847v1|http://arxiv.org/pdf/1703.01847v1|Tight Space-Approximation Tradeoff for the Multi-Pass Streaming Set   Cover Problem|We study the classic set cover problem in the streaming model: the sets that comprise the instance are revealed one by one in a stream and the goal is to solve the problem by making one or few passes over the stream while maintaining a sublinear space $o(mn)$ in the input size; here $m$ denotes the number of the sets and $n$ is the universe size. Notice that in this model, we are mainly concerned with the space requirement of the algorithms and hence do not restrict their computation time.   Our main result is a resolution of the space-approximation tradeoff for the streaming set cover problem: we show that any $\alpha$-approximation algorithm for the set cover problem requires $\widetilde{\Omega}(mn^{1/\alpha})$ space, even if it is allowed polylog${(n)}$ passes over the stream, and even if the sets are arriving in a random order in the stream. This space-approximation tradeoff matches the best known bounds achieved by the recent algorithm of Har-Peled et.al. (PODS 2016) that requires only $O(\alpha)$ passes over the stream in an adversarial order, hence settling the space complexity of approximating the set cover problem in data streams in a quite robust manner. Additionally, our approach yields tight lower bounds for the space complexity of $(1- \epsilon)$-approximating the streaming maximum coverage problem studied in several recent works.|['Sepehr Assadi']|['cs.DS']
2017-03-16T23:33:04Z|2017-03-06T12:06:58Z|http://arxiv.org/abs/1703.01830v1|http://arxiv.org/pdf/1703.01830v1|Decomposable Submodular Function Minimization: Discrete and Continuous|This paper investigates connections between discrete and continuous approaches for decomposable submodular function minimization. We provide improved running time estimates for the state-of-the-art continuous algorithms for the problem using combinatorial arguments. We also provide a systematic experimental comparison of the two types of methods, based on a clear distinction between level-0 and level-1 algorithms.|['Alina Ene', 'Huy L. Nguyen', 'László A. Végh']|['cs.LG', 'cs.DS']
2017-03-16T23:33:04Z|2017-03-06T05:14:07Z|http://arxiv.org/abs/1703.01727v1|http://arxiv.org/pdf/1703.01727v1|Frequent Query Matching in Dynamic Data Warehousing|With the need for flexible and on-demand decision support, Dynamic Data Warehouses (DDW) provide benefits over traditional data warehouses due to their dynamic characteristics in structuring and access mechanism. A DDW is a data framework that accommodates data source changes easily to allow seamless querying to users. Materialized Views (MV) are proven to be an effective methodology to enhance the process of retrieving data from a DDW as results are pre-computed and stored in it. However, due to the static nature of materialized views, the level of dynamicity that can be provided at the MV access layer is restricted. As a result, the collection of materialized views is not compatible with ever-changing reporting requirements. It is important that the MV collection is consistent with current and upcoming queries. The solution to the above problem must consider the following aspects: (a) MV must be matched against an OLAP query in order to recognize whether the MV can answer the query, (b) enable scalability in the MV collection, an intuitive mechanism to prune it and retrieve closely matching MVs must be incorporated, (c) MV collection must be able to evolve in correspondence to the regularly changing user query patterns. Therefore, the primary objective of this paper is to explore these aspects and provide a well-rounded solution for the MV access layer to remove the mismatch between the MV collection and reporting requirements. Our contribution to solve the problem includes a Query Matching Technique, a Domain Matching Technique and Maintenance of the MV collection. We developed an experimental platform using real data-sets to evaluate the effectiveness in terms of performance and precision of the proposed techniques.|['Charles H. Goonetilleke', 'J. Wenny Rahayu', 'Md. Saiful Islam']|['cs.DB', 'cs.DS']
2017-03-16T23:33:04Z|2017-03-05T23:06:03Z|http://arxiv.org/abs/1703.01686v1|http://arxiv.org/pdf/1703.01686v1|Parameterized complexity of finding a spanning tree with minimum reload   cost diameter|We study the minimum diameter spanning tree problem under the reload cost model (DIAMETER-TREE for short) introduced by Wirth and Steffan (2001). In this problem, given an undirected edge-colored graph $G$, reload costs on a path arise at a node where the path uses consecutive edges of different colors. The objective is to find a spanning tree of $G$ of minimum diameter with respect to the reload costs. We initiate a systematic study of the parameterized complexity of the DIAMETER-TREE problem by considering the following parameters: the cost of a solution, and the treewidth and the maximum degree $\Delta$ of the input graph. We prove that DIAMETER-TREE is para-NP-hard for any combination of two of these three parameters, and that it is FPT parameterized by the three of them. We also prove that the problem can be solved in polynomial time on cactus graphs. This result is somehow surprising since we prove DIAMETER-TREE to be NP-hard on graphs of treewidth two, which is best possible as the problem can be trivially solved on forests. When the reload costs satisfy the triangle inequality, Wirth and Steffan (2001) proved that the problem can be solved in polynomial time on graphs with $\Delta = 3$, and Galbiati (2008) proved that it is NP-hard if $\Delta = 4$. Our results show, in particular, that without the requirement of the triangle inequality, the problem is NP-hard if $\Delta = 3$, which is also best possible. Finally, in the case where the reload costs are polynomially bounded by the size of the input graph, we prove that DIAMETER-TREE is in XP and W[1]-hard parameterized by the treewidth plus $\Delta$.|['Julien Baste', 'Didem Gözüpek', 'Christophe Paul', 'Ignasi Sau', 'Mordechai Shalom', 'Dimitrios M. Thilikos']|['cs.DS', 'cs.CC', '05C85, 05C10', 'G.2.2; G.2.3']
2017-03-16T23:33:04Z|2017-03-05T18:24:23Z|http://arxiv.org/abs/1703.01640v1|http://arxiv.org/abs/1703.01640v1|Approximation algorithms for TSP with neighborhoods in the plane|In the Euclidean TSP with neighborhoods (TSPN), we are given a collection of n regions (neighborhoods) and we seek a shortest tour that visits each region. As a generalization of the classical Euclidean TSP, TSPN is also NP-hard. In this paper, we present new approximation results for the TSPN, including (1) a constant-factor approximation algorithm for the case of arbitrary connected neighborhoods having comparable diameters; and (2) a PTAS for the important special case of disjoint unit disk neighborhoods (or nearly disjoint, nearly-unit disks). Our methods also yield improved approximation ratios for various special classes of neighborhoods, which have previously been studied. Further, we give a linear-time O(1)-approximation algorithm for the case of neighborhoods that are (infinite) straight lines.|['Adrian Dumitrescu', 'Joseph S. B. Mitchell']|['cs.CG', 'cs.DS']
2017-03-16T23:33:04Z|2017-03-05T18:12:06Z|http://arxiv.org/abs/1703.01638v1|http://arxiv.org/pdf/1703.01638v1|Conditional Hardness for Sensitivity Problems|In recent years it has become popular to study dynamic problems in a sensitivity setting: Instead of allowing for an arbitrary sequence of updates, the sensitivity model only allows to apply batch updates of small size to the original input data. The sensitivity model is particularly appealing since recent strong conditional lower bounds ruled out fast algorithms for many dynamic problems, such as shortest paths, reachability, or subgraph connectivity.   In this paper we prove conditional lower bounds for sensitivity problems. For example, we show that under the Boolean Matrix Multiplication (BMM) conjecture combinatorial algorithms cannot compute the (4/3 - {\epsilon})-approximate diameter of an undirected unweighted dense graph with truly subcubic preprocessing time and truly subquadratic update/query time. This result is surprising since in the static setting it is not clear whether a reduction from BMM to diameter is possible. We further show under the BMM conjecture that many problems, such as reachability or approximate shortest paths, cannot be solved faster than by recomputation from scratch even after only one or two edge insertions. We give more lower bounds under the Strong Exponential Time Hypothesis and the All Pairs Shortest Paths Conjecture. Many of our lower bounds also hold for static oracle data structures where no sensitivity is required. Finally, we give the first algorithm for the (1 + {\epsilon})-approximate radius, diameter, and eccentricity problems in directed or undirected unweighted graphs in case of single edges failures. The algorithm has a truly subcubic running time for graphs with a truly subquadratic number of edges; it is tight w.r.t. the conditional lower bounds we obtain.|['Monika Henzinger', 'Andrea Lincoln', 'Stefan Neumann', 'Virginia Vassilevska Williams']|['cs.DS', 'F.2.2']
2017-03-16T23:33:04Z|2017-03-05T17:45:59Z|http://arxiv.org/abs/1703.01634v1|http://arxiv.org/pdf/1703.01634v1|Stochastic Online Scheduling on Unrelated Machines|We derive the first performance guarantees for a combinatorial online algorithm that schedules stochastic, non-preemptive jobs on unrelated machines to minimize the expectation of the total weighted completion time. Prior work on unrelated machine scheduling with stochastic jobs was restricted to the offline case, and required sophisticated linear or convex programming relaxations for the assignment of jobs to machines. Our algorithm is purely combinatorial, and therefore it also works for the online setting. As to the techniques applied, this paper shows how the dual fitting technique can be put to work for stochastic and non-preemptive scheduling problems.|['Varun Gupta', 'Benjamin Moseley', 'Marc Uetz', 'Qiaomin Xie']|['cs.DS']
2017-03-16T23:33:04Z|2017-03-05T01:08:29Z|http://arxiv.org/abs/1703.01539v1|http://arxiv.org/pdf/1703.01539v1|Distributed Partial Clustering|Recent years have witnessed an increasing popularity of algorithm design for distributed data, largely due to the fact that massive datasets are often collected and stored in different locations. In the distributed setting communication typically dominates the query processing time. Thus it becomes crucial to design communication efficient algorithms for queries on distributed data. Simultaneously, it has been widely recognized that partial optimizations, where we are allowed to disregard a small part of the data, provide us significantly better solutions. The motivation for disregarded points often arise from noise and other phenomena that are pervasive in large data scenarios.   In this paper we focus on partial clustering problems, $k$-center, $k$-median and $k$-means, in the distributed model, and provide algorithms with communication sublinear of the input size. As a consequence we develop the first algorithms for the partial $k$-median and means objectives that run in subquadratic running time. We also initiate the study of distributed algorithms for clustering uncertain data, where each data point can possibly fall into multiple locations under certain probability distribution.|['Sudipto Guha', 'Yi Li', 'Qin Zhang']|['cs.DS']
2017-03-16T23:33:04Z|2017-03-04T22:56:03Z|http://arxiv.org/abs/1703.01532v1|http://arxiv.org/pdf/1703.01532v1|Using Matching to Detect Infeasibility of Some Integer Programs|A novel matching based heuristic algorithm designed to detect specially formulated infeasible zero-one IPs is presented. The algorithm input is a set of nested doubly stochastic subsystems and a set E of instance defining variables set at zero level. The algorithm deduces additional variables at zero level until either a constraint is violated (the IP is infeasible), or no more variables can be deduced zero (the IP is undecided). All feasible IPs, and all infeasible IPs not detected infeasible are undecided. We successfully apply the algorithm to a small set of specially formulated infeasible zero-one IP instances of the Hamilton cycle decision problem. We show how to model both the graph and subgraph isomorphism decision problems for input to the algorithm. Increased levels of nested doubly stochastic subsystems can be implemented dynamically. The algorithm is designed for parallel processing, and for inclusion of techniques in addition to matching.|['S. J. Gismondi', 'E. R. Swart']|['cs.DS', 'cs.DM']
2017-03-16T23:33:12Z|2017-03-04T19:08:22Z|http://arxiv.org/abs/1703.01507v1|http://arxiv.org/pdf/1703.01507v1|Machine Learning Friendly Set Version of Johnson-Lindenstrauss Lemma|In this paper we make a novel use of the Johnson-Lindenstrauss Lemma. The Lemma has an existential form saying that there exists a JL transformation $f$ of the data points into lower dimensional space such that all of them fall into predefined error range $\delta$. We formulate in this paper a theorem stating that we can choose the target dimensionality in a random projection type JL linear transformation in such a way that with probability $1-\epsilon$ all of them fall into predefined error range $\delta$ for any user-predefined failure probability $\epsilon$. This result is important for applications such a data clustering where we want to have a priori dimensionality reducing transformation instead of trying out a (large) number of them, as with traditional Johnson-Lindenstrauss Lemma.|['Mieczysław A. Kłopotek']|['cs.DS', 'cs.LG']
2017-03-16T23:33:12Z|2017-03-04T15:14:06Z|http://arxiv.org/abs/1703.01475v1|http://arxiv.org/pdf/1703.01475v1|4/3 Rectangle Tiling lower bound|The problem that we consider is the following: given an $n \times n$ array $A$ of positive numbers, find a tiling using at most $p$ rectangles (which means that each array element must be covered by some rectangle and no two rectangles must overlap) that minimizes the maximum weight of any rectangle (the weight of a rectangle is the sum of elements which are covered by it). We prove that it is NP-hard to approximate this problem to within a factor of \textbf{1$\frac{1}{3}$} (the previous best result was $1\frac{1}{4}$).|['Grzegorz Głuch', 'Krzysztof Loryś']|['cs.DS']
2017-03-16T23:33:12Z|2017-03-04T15:13:41Z|http://arxiv.org/abs/1703.01474v1|http://arxiv.org/pdf/1703.01474v1|Sharp bounds for population recovery|The population recovery problem is a basic problem in noisy unsupervised learning that has attracted significant research attention in recent years [WY12,DRWY12, MS13, BIMP13, LZ15,DST16]. A number of different variants of this problem have been studied, often under assumptions on the unknown distribution (such as that it has restricted support size). In this work we study the sample complexity and algorithmic complexity of the most general version of the problem, under both bit-flip noise and erasure noise model. We give essentially matching upper and lower sample complexity bounds for both noise models, and efficient algorithms matching these sample complexity bounds up to polynomial factors.|"['Anindya De', ""Ryan O'Donnell"", 'Rocco Servedio']"|['cs.DS', 'cs.LG', 'math.ST', 'stat.TH']
2017-03-16T23:33:12Z|2017-03-06T09:05:51Z|http://arxiv.org/abs/1703.01166v2|http://arxiv.org/pdf/1703.01166v2|Efficient Network Measurements through Approximated Windows|Many networking applications require timely access to recent network measurements, which can be captured using a sliding window model. Maintaining such measurements is a challenging task due to the fast line speed and scarcity of fast memory in routers. In this work, we study the efficiency factor that can be gained by approximating the window size. That is, we allow the algorithm to dynamically adjust the window size between $W$ and $W(1+\tau)$ where $\tau$ is a small positive parameter. For example, consider the \emph{basic summing} problem of computing the sum of the last $W$ elements in a stream whose items are integers in $\{0,1\ldots,R\}$, where $R=\text{poly}(W)$. While it is known that $\Omega(W\log{R})$ bits are needed in the exact window model, we show that approximate windows allow an exponential space reduction for constant $\tau$.   Specifically, we present a lower bound of $\Omega(\tau^{-1}\log(RW\tau))$ bits for the basic summing problem. Further, an $(1+\epsilon)$ multiplicative approximation of this problem requires $\Omega(\log({W/\epsilon}+\log\log{R}))$ bits for constant $\tau$. Additionally, for $RW\epsilon$ additive approximations, we show an $\Omega(\tau^{-1}\log\lfloor{1+\tau/\epsilon}\rfloor+\log({W/\epsilon}))$ lower bound~\footnote{ We also provide an optimal bound and algorithm for the $\tau<\epsilon$ case.}. For all three settings, we provide memory optimal algorithms that operate in constant time. Finally, we demonstrate the generality of the approximated window model by applying it to counting the number of distinct flows in a sliding window over a network stream. We present an algorithm that solves this problem while requiring asymptotically less space than previous sliding window methods when $\tau=O(1)$.|['Ran Ben Basat', 'Gil Einziger', 'Roy Friedman']|['cs.DS']
2017-03-16T23:33:12Z|2017-03-03T06:41:02Z|http://arxiv.org/abs/1703.01054v1|http://arxiv.org/abs/1703.01054v1|When Hashes Met Wedges: A Distributed Algorithm for Finding High   Similarity Vectors|"Finding similar user pairs is a fundamental task in social networks, with numerous applications in ranking and personalization tasks such as link prediction and tie strength detection. A common manifestation of user similarity is based upon network structure: each user is represented by a vector that represents the user's network connections, where pairwise cosine similarity among these vectors defines user similarity. The predominant task for user similarity applications is to discover all similar pairs that have a pairwise cosine similarity value larger than a given threshold $\tau$. In contrast to previous work where $\tau$ is assumed to be quite close to 1, we focus on recommendation applications where $\tau$ is small, but still meaningful. The all pairs cosine similarity problem is computationally challenging on networks with billions of edges, and especially so for settings with small $\tau$. To the best of our knowledge, there is no practical solution for computing all user pairs with, say $\tau = 0.2$ on large social networks, even using the power of distributed algorithms.   Our work directly addresses this challenge by introducing a new algorithm --- WHIMP --- that solves this problem efficiently in the MapReduce model. The key insight in WHIMP is to combine the ""wedge-sampling"" approach of Cohen-Lewis for approximate matrix multiplication with the SimHash random projection techniques of Charikar. We provide a theoretical analysis of WHIMP, proving that it has near optimal communication costs while maintaining computation cost comparable with the state of the art. We also empirically demonstrate WHIMP's scalability by computing all highly similar pairs on four massive data sets, and show that it accurately finds high similarity pairs. In particular, we note that WHIMP successfully processes the entire Twitter network, which has tens of billions of edges."|['Aneesh Sharma', 'C. Seshadhri', 'Ashish Goel']|['cs.SI', 'cs.DC', 'cs.DS']
2017-03-16T23:33:12Z|2017-03-06T04:41:19Z|http://arxiv.org/abs/1703.01009v2|http://arxiv.org/pdf/1703.01009v2|Optimal Time and Space Construction of Suffix Arrays and LCP Arrays for   Integer Alphabets|Suffix arrays and LCP arrays are one of the most fundamental data structures widely used for various kinds of string processing. Many problems can be solved efficiently by using suffix arrays, or a pair of suffix arrays and LCP arrays. In this paper, we consider two problems for a string of length $N$, the characters of which are represented as integers in $[1, \dots, \sigma]$ for $1 \leq \sigma \leq N$; the string contains $\sigma$ distinct characters, (1) construction of the suffix array and (2) simultaneous construction of both the suffix array and the LCP array. In the word RAM model, we propose algorithms to solve both the problems in $O(N)$ time using $O(1)$ extra words, which are optimal in time and space. Extra words mean the required space except for the space of the input string and output suffix array and LCP array. Our contribution improves the previous most efficient algorithm that runs in $O(N)$ time using $\sigma+O(1)$ extra words for the suffix array construction proposed by [Nong, TOIS 2013], and it improves the previous most efficient solution that runs in $O(N)$ time using $\sigma + O(1)$ extra words for both suffix array and LCP array construction using the combination of [Nong, TOIS 2013] and [Manzini, SWAT 2004].   Another optimal time and space algorithm to construct the suffix array was proposed by [Li et al, arXiv 2016] very recently and independently. Our algorithm is simpler than theirs, and it allows us to solve the second problem in optimal time and space.|['Keisuke Goto']|['cs.DS']
2017-03-16T23:33:12Z|2017-03-02T20:25:04Z|http://arxiv.org/abs/1703.00941v1|http://arxiv.org/pdf/1703.00941v1|On the Fine-grained Complexity of One-Dimensional Dynamic Programming|In this paper, we investigate the complexity of one-dimensional dynamic programming, or more specifically, of the Least-Weight Subsequence (LWS) problem: Given a sequence of $n$ data items together with weights for every pair of the items, the task is to determine a subsequence $S$ minimizing the total weight of the pairs adjacent in $S$. A large number of natural problems can be formulated as LWS problems, yielding obvious $O(n^2)$-time solutions.   In many interesting instances, the $O(n^2)$-many weights can be succinctly represented. Yet except for near-linear time algorithms for some specific special cases, little is known about when an LWS instantiation admits a subquadratic-time algorithm and when it does not. In particular, no lower bounds for LWS instantiations have been known before. In an attempt to remedy this situation, we provide a general approach to study the fine-grained complexity of succinct instantiations of the LWS problem. In particular, given an LWS instantiation we identify a highly parallel core problem that is subquadratically equivalent. This provides either an explanation for the apparent hardness of the problem or an avenue to find improved algorithms as the case may be.   More specifically, we prove subquadratic equivalences between the following pairs (an LWS instantiation and the corresponding core problem) of problems: a low-rank version of LWS and minimum inner product, finding the longest chain of nested boxes and vector domination, and a coin change problem which is closely related to the knapsack problem and (min,+)-convolution. Using these equivalences and known SETH-hardness results for some of the core problems, we deduce tight conditional lower bounds for the corresponding LWS instantiations. We also establish the (min,+)-convolution-hardness of the knapsack problem.|['Marvin Künnemann', 'Ramamohan Paturi', 'Stefan Schneider']|['cs.CC', 'cs.DS']
2017-03-16T23:33:12Z|2017-03-02T18:55:52Z|http://arxiv.org/abs/1703.00900v1|http://arxiv.org/pdf/1703.00900v1|Deterministic Distributed Matching: Simpler, Faster, Better|We present improved deterministic distributed algorithms for a number of well-studied matching problems, which are simpler, faster, more accurate, and/or more general than their known counterparts. The common denominator of these results is a deterministic distributed rounding method for certain linear programs, which is the first such rounding method, to our knowledge. A sampling of our end results is as follows:   -- An $O(\log^2 \Delta \log n)$-round deterministic distributed algorithm for computing a maximal matching, in $n$-node graphs with maximum degree $\Delta$. This is the first improvement in about 20 years over the celebrated $O(\log^4 n)$-round algorithm of Hanckowiak, Karonski, and Panconesi [SODA'98, PODC'99].   -- An $O(\log^2 \Delta \log \frac{1}{\varepsilon} + \log^ * n)$-round deterministic distributed algorithm for a $(2+\varepsilon)$-approximation of maximum matching. This is exponentially faster than the classic $O(\Delta +\log^* n)$-round $2$-approximation of Panconesi and Rizzi [DIST'01]. With some modifications, the algorithm can also find an almost maximal matching which leaves only an $\varepsilon$-fraction of the edges on unmatched nodes.   -- An $O(\log^2 \Delta \log \frac{1}{\varepsilon} \log_{1+\varepsilon} W + \log^ * n)$-round deterministic distributed algorithm for a $(2+\varepsilon)$-approximation of a maximum weighted matching, and also for the more general problem of maximum weighted $b$-matching. Here, $W$ denotes the maximum normalized weight. These improve over the $O(\log^4 n \log_{1+\varepsilon} W)$-round $(6+\varepsilon)$-approximation algorithm of Panconesi and Sozio [DIST'10].|['Manuela Fischer', 'Mohsen Ghaffari']|['cs.DS']
2017-03-16T23:33:12Z|2017-03-02T18:50:33Z|http://arxiv.org/abs/1703.00893v1|http://arxiv.org/pdf/1703.00893v1|Being Robust (in High Dimensions) Can Be Practical|Robust estimation is much more challenging in high dimensions than it is in one dimension: Most techniques either lead to intractable optimization problems or estimators that can tolerate only a tiny fraction of errors. Recent work in theoretical computer science has shown that, in appropriate distributional models, it is possible to robustly estimate the mean and covariance with polynomial time algorithms that can tolerate a constant fraction of corruptions, independent of the dimension. However, the sample and time complexity of these algorithms is prohibitively large for high-dimensional applications. In this work, we address both of these issues by establishing sample complexity bounds that are optimal, up to logarithmic factors, as well as giving various refinements that allow the algorithms to tolerate a much larger fraction of corruptions. Finally, we show on both synthetic and real data that our algorithms have state-of-the-art performance and suddenly make high-dimensional robust estimation a realistic possibility.|['Ilias Diakonikolas', 'Gautam Kamath', 'Daniel M. Kane', 'Jerry Li', 'Ankur Moitra', 'Alistair Stewart']|['cs.LG', 'cs.DS', 'cs.IT', 'math.IT', 'stat.ML']
2017-03-16T23:33:12Z|2017-03-02T15:27:14Z|http://arxiv.org/abs/1703.00830v1|http://arxiv.org/pdf/1703.00830v1|General and Robust Communication-Efficient Algorithms for Distributed   Clustering|As datasets become larger and more distributed, algorithms for distributed clustering have become more and more important. In this work, we present a general framework for designing distributed clustering algorithms that are robust to outliers. Using our framework, we give a distributed approximation algorithm for k-means, k-median, or generally any L_p objective, with z outliers and/or balance constraints, using O(m(k+z)(d+log n)) bits of communication, where m is the number of machines, n is the size of the point set, and d is the dimension. This generalizes and improves over previous work of Bateni et al. and Malkomes et al. As a special case, we achieve the first distributed algorithm for k-median with outliers, answering an open question posed by Malkomes et al. For distributed k-means clustering, we provide the first dimension-dependent communication complexity lower bound for finding the optimal clustering. This improves over the lower bound from Chen et al. which is dimension-agnostic.   Furthermore, we give distributed clustering algorithms which return nearly optimal solutions, provided the data satisfies the approximation stability condition of Balcan et al. or the spectral stability condition of Kumar and Kannan. In certain clustering applications where a local clustering consistent among all machines is sufficient, we show that no communication is necessary if the data satisfies approximation stability.|['Pranjal Awasthi', 'Maria-Florina Balcan', 'Colin White']|['cs.DS', 'cs.DC', 'cs.LG']
2017-03-16T23:33:16Z|2017-03-02T10:26:39Z|http://arxiv.org/abs/1703.00699v1|http://arxiv.org/pdf/1703.00699v1|Exact algorithms for the picking problem|Order picking is the problem of collecting a set of products in a warehouse in a minimum amount of time. It is currently a major bottleneck in supply-chain because of its cost in time and labor force. This article presents two exact and effective algorithms for this problem. Firstly, a sparse formulation in mixed-integer programming is strengthened by preprocessing and valid inequalities. Secondly, a dynamic programming approach generalizing known algorithms for two or three cross-aisles is proposed and evaluated experimentally. Performances of these algorithms are reported and compared with the Traveling Salesman Problem (TSP) solver Concorde.|['Lucie Pansart', 'Nicolas Catusse', 'Hadrien Cambazard']|['cs.DS']
2017-03-16T23:33:16Z|2017-03-02T09:54:11Z|http://arxiv.org/abs/1703.00687v1|http://arxiv.org/pdf/1703.00687v1|Even faster sorting of (not only) integers|In this paper we introduce RADULS2, the fastest parallel sorter based on radix algorithm. It is optimized to process huge amounts of data making use of modern multicore CPUs. The main novelties include: extremely optimized algorithm for handling tiny arrays (up to about a hundred of records) that could appear even billions times as subproblems to handle and improved processing of larger subarrays with better use of non-temporal memory stores.|['Marek Kokot', 'Sebastian Deorowicz', 'Maciej Dlugosz']|['cs.DS', 'cs.DC']
2017-03-16T23:33:16Z|2017-03-02T08:37:37Z|http://arxiv.org/abs/1703.00667v1|http://arxiv.org/pdf/1703.00667v1|A resource-frugal probabilistic dictionary and applications in   bioinformatics|Indexing massive data sets is extremely expensive for large scale problems. In many fields, huge amounts of data are currently generated, however extracting meaningful information from voluminous data sets, such as computing similarity between elements, is far from being trivial. It remains nonetheless a fundamental need. This work proposes a probabilistic data structure based on a minimal perfect hash function for indexing large sets of keys. Our structure out-compete the hash table for construction, query times and for memory usage, in the case of the indexation of a static set. To illustrate the impact of algorithms performances, we provide two applications based on similarity computation between collections of sequences, and for which this calculation is an expensive but required operation. In particular, we show a practical case in which other bioinformatics tools fail to scale up the tested data set or provide lower quality results.|['Camille Marchet', 'Lolita Lecompte', 'Antoine Limasset', 'Lucie Bittner', 'Pierre Peterlongo']|['cs.DS', 'q-bio.QM']
2017-03-16T23:33:16Z|2017-03-02T07:12:12Z|http://arxiv.org/abs/1703.00640v1|http://arxiv.org/pdf/1703.00640v1|Faster truncated integer multiplication|We present new algorithms for computing the low n bits or the high n bits of the product of two n-bit integers. We show that these problems may be solved in asymptotically 75% of the time required to compute the full 2n-bit product, assuming that the underlying integer multiplication algorithm relies on computing cyclic convolutions of real sequences.|['David Harvey']|['cs.SC', 'cs.DS', '68W30 (Primary)', 'G.1.0; F.2.1']
2017-03-16T23:33:16Z|2017-03-02T01:19:26Z|http://arxiv.org/abs/1703.00575v1|http://arxiv.org/pdf/1703.00575v1|On the NP-hardness of scheduling with time restrictions|In a recent paper, Braun, Chung and Graham [1] have addressed a single-processor scheduling problem with time restrictions. Given a fixed integer $B \geq 2$, there is a set of jobs to be processed by a single processor subject to the following B-constraint. For any real $x$, no unit time interval $[x, x+1)$ is allowed to intersect more than $B$ jobs. The problem has been shown to be NP-hard when $B$ is part of the input and left as an open question whether it remains NP-hard or not if $B$ is fixed [1, 5, 7]. This paper contributes to answering this question that we prove the problem is NP-hard even when $B=2$. A PTAS is also presented for any constant $B \geq 2$.|['An Zhang', 'Yong Chen', 'Lin Chen', 'Guangting Chen']|['cs.DS']
2017-03-16T23:33:16Z|2017-03-01T20:09:43Z|http://arxiv.org/abs/1703.00484v1|http://arxiv.org/pdf/1703.00484v1|Truth and Regret in Online Scheduling|We consider a scheduling problem where a cloud service provider has multiple units of a resource available over time. Selfish clients submit jobs, each with an arrival time, deadline, length, and value. The service provider's goal is to implement a truthful online mechanism for scheduling jobs so as to maximize the social welfare of the schedule. Recent work shows that under a stochastic assumption on job arrivals, there is a single-parameter family of mechanisms that achieves near-optimal social welfare. We show that given any such family of near-optimal online mechanisms, there exists an online mechanism that in the worst case performs nearly as well as the best of the given mechanisms. Our mechanism is truthful whenever the mechanisms in the given family are truthful and prompt, and achieves optimal (within constant factors) regret.   We model the problem of competing against a family of online scheduling mechanisms as one of learning from expert advice. A primary challenge is that any scheduling decisions we make affect not only the payoff at the current step, but also the resource availability and payoffs in future steps. Furthermore, switching from one algorithm (a.k.a. expert) to another in an online fashion is challenging both because it requires synchronization with the state of the latter algorithm as well as because it affects the incentive structure of the algorithms. We further show how to adapt our algorithm to a non-clairvoyant setting where job lengths are unknown until jobs are run to completion. Once again, in this setting, we obtain truthfulness along with asymptotically optimal regret (within poly-logarithmic factors).|['Shuchi Chawla', 'Nikhil Devanur', 'Janardhan Kulkarni', 'Rad Niazadeh']|['cs.GT', 'cs.AI', 'cs.DS', 'cs.LG']
2017-03-16T23:33:16Z|2017-03-01T18:51:13Z|http://arxiv.org/abs/1703.00440v1|http://arxiv.org/pdf/1703.00440v1|Fast k-Nearest Neighbour Search via Prioritized DCI|Most exact methods for k-nearest neighbour search suffer from the curse of dimensionality; that is, their query times exhibit exponential dependence on either the ambient or the intrinsic dimensionality. Dynamic Continuous Indexing (DCI) offers a promising way of circumventing the curse by avoiding space partitioning and achieves a query time that grows sublinearly in the intrinsic dimensionality. In this paper, we develop a variant of DCI, which we call Prioritized DCI, and show a further improvement in the dependence on the intrinsic dimensionality compared to standard DCI, thereby improving the performance of DCI on datasets with high intrinsic dimensionality. We also demonstrate empirically that Prioritized DCI compares favourably to standard DCI and Locality-Sensitive Hashing (LSH) both in terms of running time and space consumption at all levels of approximation quality. In particular, relative to LSH, Prioritized DCI reduces the number of distance evaluations by a factor of 5 to 30 and the space consumption by a factor of 47 to 55.|['Ke Li', 'Jitendra Malik']|['cs.LG', 'cs.AI', 'cs.DS', 'cs.IR', 'stat.ML']
2017-03-16T23:33:16Z|2017-03-02T13:01:47Z|http://arxiv.org/abs/1703.00236v2|http://arxiv.org/pdf/1703.00236v2|Algorithms and Bounds for Very Strong Rainbow Coloring|A well-studied coloring problem is to assign colors to the edges of a graph so that, for every pair of vertices, all edges of at least one shortest path between them receive different colors. The minimum number of colors necessary in such a coloring is the strong rainbow connection number ($\src(G)$) of a graph. When proving upper bounds on $\src(G)$, it is natural to prove that a coloring exists where, for every pair of vertices, all edges of \emph{all} shortest paths between them receive different colors. In this paper, we introduce and formally define this more restricted coloring number, which we call very strong rainbow connection number ($\vsrc(G)$), and give the first insights into the complexity of computing it.   We first prove that there is no polynomial-time algorithm to decide whether $\vsrc(G) \leq 3$ nor to approximate $\vsrc(G)$ within a factor $n^{1-\varepsilon}$, unless P$=$NP. We then prove that $\vsrc(G)$ can be determined in polynomial time on cactus graphs, thus making progress on assessing the complexity of computing $\vsrc(G)$ for graphs of bounded treewidth. The latter problem is wide open, even for $\src(G)$. We observe, however, that deciding whether $\vsrc(G) = k$ is fixed-parameter tractable in $k$ and the treewidth of $G$. Finally, we give upper bounds on $\vsrc(G)$ for several graph classes, some of which are proved tight. These imply new upper bounds on $\src(G)$ for these classes as well.|['L. Sunil Chandran', 'Anita Das', 'Davis Issac', 'Erik Jan van Leeuwen']|['cs.DS', 'math.CO']
2017-03-16T23:33:16Z|2017-02-28T21:41:09Z|http://arxiv.org/abs/1703.00066v1|http://arxiv.org/pdf/1703.00066v1|On the Power of Learning from $k$-Wise Queries|Several well-studied models of access to data samples, including statistical queries, local differential privacy and low-communication algorithms rely on queries that provide information about a function of a single sample. (For example, a statistical query (SQ) gives an estimate of $Ex_{x \sim D}[q(x)]$ for any choice of the query function $q$ mapping $X$ to the reals, where $D$ is an unknown data distribution over $X$.) Yet some data analysis algorithms rely on properties of functions that depend on multiple samples. Such algorithms would be naturally implemented using $k$-wise queries each of which is specified by a function $q$ mapping $X^k$ to the reals. Hence it is natural to ask whether algorithms using $k$-wise queries can solve learning problems more efficiently and by how much.   Blum, Kalai and Wasserman (2003) showed that for any weak PAC learning problem over a fixed distribution, the complexity of learning with $k$-wise SQs is smaller than the (unary) SQ complexity by a factor of at most $2^k$. We show that for more general problems over distributions the picture is substantially richer. For every $k$, the complexity of distribution-independent PAC learning with $k$-wise queries can be exponentially larger than learning with $(k+1)$-wise queries. We then give two approaches for simulating a $k$-wise query using unary queries. The first approach exploits the structure of the problem that needs to be solved. It generalizes and strengthens (exponentially) the results of Blum et al.. It allows us to derive strong lower bounds for learning DNF formulas and stochastic constraint satisfaction problems that hold against algorithms using $k$-wise queries. The second approach exploits the $k$-party communication complexity of the $k$-wise query function.|['Vitaly Feldman', 'Badih Ghazi']|['cs.LG', 'cs.DS']
2017-03-16T23:33:16Z|2017-02-28T18:47:57Z|http://arxiv.org/abs/1702.08903v1|http://arxiv.org/pdf/1702.08903v1|Defective Coloring on Classes of Perfect Graphs|In Defective Coloring we are given a graph $G$ and two integers $\chi_d$, $\Delta^*$ and are asked if we can $\chi_d$-color $G$ so that the maximum degree induced by any color class is at most $\Delta^*$. We show that this natural generalization of Coloring is much harder on several basic graph classes. In particular, we show that it is NP-hard on split graphs, even when one of the two parameters $\chi_d$, $\Delta^*$ is set to the smallest possible fixed value that does not trivialize the problem ($\chi_d = 2$ or $\Delta^* = 1$). Together with a simple treewidth-based DP algorithm this completely determines the complexity of the problem also on chordal graphs. We then consider the case of cographs and show that, somewhat surprisingly, Defective Coloring turns out to be one of the few natural problems which are NP-hard on this class. We complement this negative result by showing that Defective Coloring is in P for cographs if either $\chi_d$ or $\Delta^*$ is fixed; that it is in P for trivially perfect graphs; and that it admits a sub-exponential time algorithm for cographs when both $\chi_d$ and $\Delta^*$ are unbounded.|['Rémy Belmonte', 'Michael Lampis', 'Valia Mitsou']|['cs.DS', 'math.CO']
2017-03-16T23:33:20Z|2017-02-28T18:38:41Z|http://arxiv.org/abs/1702.08899v1|http://arxiv.org/pdf/1702.08899v1|Binary Search in Graphs Revisited|"In the classical binary search in a path the aim is to detect an unknown target by asking as few queries as possible, where each query reveals the direction to the target. This binary search algorithm has been recently extended by [Emamjomeh-Zadeh et al., STOC, 2016] to the problem of detecting a target in an arbitrary graph. Similarly to the classical case in the path, the algorithm of Emamjomeh-Zadeh et al. maintains a candidates' set for the target, while each query asks an appropriately chosen vertex-- the ""median""--which minimizes a potential $\Phi$ among the vertices of the candidates' set. In this paper we address three open questions posed by Emamjomeh-Zadeh et al., namely (a) detecting a target when the query response is a direction to an approximately shortest path to the target, (b) detecting a target when querying a vertex that is an approximate median of the current candidates' set (instead of an exact one), and (c) detecting multiple targets, for which to the best of our knowledge no progress has been made so far. We resolve questions (a) and (b) by providing appropriate upper and lower bounds, as well as a new potential $\Gamma$ that guarantees efficient target detection even by querying an approximate median each time. With respect to (c), we initiate a systematic study for detecting two targets in graphs and we identify sufficient conditions on the queries that allow for strong (linear) lower bounds and strong (polylogarithmic) upper bounds for the number of queries. All of our positive results can be derived using our new potential $\Gamma$ that allows querying approximate medians."|['Argyrios Deligkas', 'George B. Mertzios', 'Paul G. Spirakis']|['cs.DS']
2017-03-16T23:33:20Z|2017-02-28T16:57:49Z|http://arxiv.org/abs/1702.08862v1|http://arxiv.org/pdf/1702.08862v1|Proportional Representation in Vote Streams|We consider elections where the voters come one at a time, in a streaming fashion, and devise space-efficient algorithms which identify an approximate winning committee with respect to common multiwinner proportional representation voting rules; specifically, we consider the Approval-based and the Borda-based variants of both the Chamberlin-- ourant rule and the Monroe rule. We complement our algorithms with lower bounds. Somewhat surprisingly, our results imply that, using space which does not depend on the number of voters it is possible to efficiently identify an approximate representative committee of fixed size over vote streams with huge number of voters.|['Palash Dey', 'Nimrod Talmon', 'Otniel van Handel']|['cs.GT', 'cs.AI', 'cs.CC', 'cs.DS', 'cs.MA']
2017-03-16T23:33:20Z|2017-02-28T14:07:42Z|http://arxiv.org/abs/1702.08791v1|http://arxiv.org/pdf/1702.08791v1|Robust Budget Allocation via Continuous Submodular Functions|The optimal allocation of resources for maximizing influence, spread of information or coverage, has gained attention in the past years, in particular in machine learning and data mining. But, in applications, the parameters of the problem are rarely known exactly, and using wrong parameters can lead to undesirable outcomes. We hence revisit a continuous version of the Budget Allocation or Bipartite Influence Maximization problem introduced by Alon et al. (2012) from a robust optimization perspective, where an adversary may choose the least favorable parameters within a confidence set. The resulting problem is a nonconvex-concave saddle point problem (or game). We show that this nonconvex problem can be solved exactly by leveraging connections to continuous submodular functions, and by solving a constrained submodular minimization problem. Although constrained submodular minimization is hard in general, here, we establish conditions under which such a problem can be solved to arbitrary precision $\epsilon$.|['Matthew Staib', 'Stefanie Jegelka']|['cs.LG', 'cs.AI', 'cs.DS', 'cs.SI', 'math.OC']
2017-03-16T23:33:20Z|2017-02-28T10:42:31Z|http://arxiv.org/abs/1702.08734v1|http://arxiv.org/pdf/1702.08734v1|Billion-scale similarity search with GPUs|Similarity search finds application in specialized database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data-parallel tasks, prior approaches are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy.   We propose a design for k-selection that operates at up to 55% of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5x faster than prior GPU state of the art. We apply it in different similarity search scenarios, by proposing optimized design for brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation enables the construction of a high accuracy k-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility.|['Jeff Johnson', 'Matthijs Douze', 'Hervé Jégou']|['cs.CV', 'cs.DB', 'cs.DS']
2017-03-16T23:33:20Z|2017-02-27T21:48:00Z|http://arxiv.org/abs/1702.08545v1|http://arxiv.org/pdf/1702.08545v1|Synergistic Computation of Planar Maxima and Convex Hull|Refinements of the worst case complexity over instances of fixed input size consider the input order or the input structure, but rarely both at the same time. Barbay et al. [2016] described ``synergistic'' solutions on multisets, which take advantage of the input order and the input structure, such as to asymptotically outperform any comparable solution which takes advantage only of one of those features. We consider the extension of their results to the computation of the \textsc{Maxima Set} and the \textsc{Convex Hull} of a set of planar points. After revisiting and improving previous approaches taking advantage only of the input order or of the input structure, we describe synergistic solutions taking optimally advantage of various notions of the input order and input structure in the plane. As intermediate results, we describe and analyze the first adaptive algorithms for \textsc{Merging Maxima} and \textsc{Merging Convex Hulls}.|['Jérémy Barbay', 'Carlos Ochoa']|['cs.DS']
2017-03-16T23:33:20Z|2017-02-27T20:05:43Z|http://arxiv.org/abs/1702.08503v1|http://arxiv.org/pdf/1702.08503v1|SGD Learns the Conjugate Kernel Class of the Network|We show that the standard stochastic gradient decent (SGD) algorithm is guaranteed to learn, in polynomial time, a function that is competitive with the best function in the conjugate kernel space, as defined in Daniely, Frostig and Singer (2016). The result holds for log-depth networks from a rich family of architectures. To the best of our knowledge, it is the first polynomial-time guarantee for the standard neural network learning algorithm for networks of depth $\ge 3$.|['Amit Daniely']|['cs.LG', 'cs.DS', 'stat.ML']
2017-03-16T23:33:20Z|2017-02-27T19:11:44Z|http://arxiv.org/abs/1702.08474v1|http://arxiv.org/pdf/1702.08474v1|The Infinite Server Problem|We study a variant of the $k$-server problem, the infinite server problem, in which infinitely many servers reside initially at a particular point of the metric space and serve a sequence of requests. In the framework of competitive analysis, we show a surprisingly tight connection between this problem and the $(h,k)$-server problem, in which an online algorithm with $k$ servers competes against an offline algorithm with $h$ servers. Specifically, we show that the infinite server problem has bounded competitive ratio if and only if the $(h,k)$-server problem has bounded competitive ratio for some $k=O(h)$. We give a lower bound of $3.146$ for the competitive ratio of the infinite server problem, which implies the same lower bound for the $(h,k)$-server problem even when $k/h \to \infty$ and holds also for the line metric; the previous known bounds were 2.4 for general metric spaces and 2 for the line. For weighted trees and layered graphs we obtain upper bounds, although they depend on the depth. Of particular interest is the infinite server problem on the line, which we show to be equivalent to the seemingly easier case in which all requests are in a fixed bounded interval away from the original position of the servers. This is a special case of a more general reduction from arbitrary metric spaces to bounded subspaces. Unfortunately, classical approaches (double coverage and generalizations, work function algorithm, balancing algorithms) fail even for this special case.|['Christian Coester', 'Elias Koutsoupias', 'Philip Lazos']|['cs.DS', 'F.1.2']
2017-03-16T23:33:20Z|2017-02-27T18:23:28Z|http://arxiv.org/abs/1702.08415v1|http://arxiv.org/pdf/1702.08415v1|An SDP-Based Algorithm for Linear-Sized Spectral Sparsification|For any undirected and weighted graph $G=(V,E,w)$ with $n$ vertices and $m$ edges, we call a sparse subgraph $H$ of $G$, with proper reweighting of the edges, a $(1+\varepsilon)$-spectral sparsifier if \[ (1-\varepsilon)x^{\intercal}L_Gx\leq x^{\intercal} L_{H} x\leq (1+\varepsilon) x^{\intercal} L_Gx \] holds for any $x\in\mathbb{R}^n$, where $L_G$ and $L_{H}$ are the respective Laplacian matrices of $G$ and $H$. Noticing that $\Omega(m)$ time is needed for any algorithm to construct a spectral sparsifier and a spectral sparsifier of $G$ requires $\Omega(n)$ edges, a natural question is to investigate, for any constant $\varepsilon$, if a $(1+\varepsilon)$-spectral sparsifier of $G$ with $O(n)$ edges can be constructed in $\tilde{O}(m)$ time, where the $\tilde{O}$ notation suppresses polylogarithmic factors. All previous constructions on spectral sparsification require either super-linear number of edges or $m^{1+\Omega(1)}$ time.   In this work we answer this question affirmatively by presenting an algorithm that, for any undirected graph $G$ and $\varepsilon>0$, outputs a $(1+\varepsilon)$-spectral sparsifier of $G$ with $O(n/\varepsilon^2)$ edges in $\tilde{O}(m/\varepsilon^{O(1)})$ time. Our algorithm is based on three novel techniques: (1) a new potential function which is much easier to compute yet has similar guarantees as the potential functions used in previous references; (2) an efficient reduction from a two-sided spectral sparsifier to a one-sided spectral sparsifier; (3) constructing a one-sided spectral sparsifier by a semi-definite program.|['Yin Tat Lee', 'He Sun']|['cs.DS', 'cs.LG']
2017-03-16T23:33:20Z|2017-02-27T14:25:36Z|http://arxiv.org/abs/1702.08299v1|http://arxiv.org/pdf/1702.08299v1|Independent Set Size Approximation in Graph Streams|We study the problem of estimating the size of independent sets in a graph $G$ defined by a stream of edges. Our approach relies on the Caro-Wei bound, which expresses the desired quantity in terms of a sum over nodes of the reciprocal of their degrees, denoted by $\beta(G)$. Our results show that $\beta(G)$ can be approximated accurately, based on a provided lower bound on $\beta$. Stronger results are possible when the edges are promised to arrive grouped by an incident node. In this setting, we obtain a value that is at most a logarithmic factor below the true value of $\beta$ and no more than the true independent set size. To justify the form of this bound, we also show an $\Omega(n/\beta)$ lower bound on any algorithm that approximates $\beta$ up to a constant factor.|['Graham Cormode', 'Jacques Dark', 'Christian Konrad']|['cs.DS']
2017-03-16T23:33:20Z|2017-02-27T12:03:01Z|http://arxiv.org/abs/1702.08248v1|http://arxiv.org/pdf/1702.08248v1|Scalable and Distributed Clustering via Lightweight Coresets|Coresets are compact representations of data sets such that models trained on a coreset are provably competitive with models trained on the full data set. As such, they have been successfully used to scale up clustering models to massive data sets. While existing approaches generally only allow for multiplicative approximation errors, we propose a novel notion of coresets called lightweight coresets that allows for both multiplicative and additive errors. We provide a single algorithm to construct light-weight coresets for k-Means clustering, Bregman clustering and maximum likelihood estimation of Gaussian mixture models. The algorithm is substantially faster than existing constructions, embarrassingly parallel and resulting coresets are smaller. In an extensive experimental evaluation, we demonstrate that the proposed method outperforms existing coreset constructions.|['Olivier Bachem', 'Mario Lucic', 'Andreas Krause']|['stat.ML', 'cs.DC', 'cs.DS', 'cs.LG', 'stat.CO']
2017-03-16T23:33:24Z|2017-02-28T01:39:26Z|http://arxiv.org/abs/1702.08247v2|http://arxiv.org/pdf/1702.08247v2|On the Expected Value of the Determinant of Random Sum of Rank-One   Matrices|We present a simple, yet useful result about the expected value of the determinant of random sum of rank-one matrices. Computing such expectations in general may involve a sum over exponentially many terms. Nevertheless, we show that an interesting and useful class of such expectations that arise in, e.g., D-optimal estimation and random graphs can be computed efficiently via computing a single determinant.|['Kasra Khosoussi']|['cs.DS', 'math.PR']
2017-03-16T23:33:24Z|2017-02-27T09:52:17Z|http://arxiv.org/abs/1702.08207v1|http://arxiv.org/pdf/1702.08207v1|Approximation Strategies for Generalized Binary Search in Weighted Trees|We consider the following generalization of the binary search problem. A search strategy is required to locate an unknown target node $t$ in a given tree $T$. Upon querying a node $v$ of the tree, the strategy receives as a reply an indication of the connected component of $T\setminus\{v\}$ containing the target $t$. The cost of querying each node is given by a known non-negative weight function, and the considered objective is to minimize the total query cost for a worst-case choice of the target. Designing an optimal strategy for a weighted tree search instance is known to be strongly NP-hard, in contrast to the unweighted variant of the problem which can be solved optimally in linear time. Here, we show that weighted tree search admits a quasi-polynomial time approximation scheme: for any $0 \textless{} \varepsilon \textless{} 1$, there exists a $(1+\varepsilon)$-approximation strategy with a computation time of $n^{O(\log n / \varepsilon^2)}$. Thus, the problem is not APX-hard, unless $NP \subseteq DTIME(n^{O(\log n)})$. By applying a generic reduction, we obtain as a corollary that the studied problem admits a polynomial-time $O(\sqrt{\log n})$-approximation. This improves previous $\hat O(\log n)$-approximation approaches, where the $\hat O$-notation disregards $O(\mathrm{poly}\log\log n)$-factors.|['Dariusz Dereniowski', 'Adrian Kosowski', 'Przemyslaw Uznanski', 'Mengchuan Zou']|['cs.DS']
2017-03-16T23:33:24Z|2017-02-26T07:12:46Z|http://arxiv.org/abs/1702.08443v1|http://arxiv.org/pdf/1702.08443v1|Elementary Yet Precise Worst-case Analysis of MergeSort, A short version   (SV)|This paper offers two elementary yet precise derivations of an exact formula   \[ W(n) = \sum_{i=1} ^{n} \lceil \lg i \rceil = n \lceil \lg n \rceil - 2^{\lceil \lg n \rceil} + 1 \] for the maximum number $ W(n) $ of comparisons of keys performed by $ {\tt MergeSort} $ on an $ n $-element array. The first of the two, due to its structural regularity, is well worth carefully studying in its own right.   Close smooth bounds on $ W(n) $ are derived. It seems interesting that $ W(n) $ is linear between the points $ n = 2^{\lfloor \lg n \rfloor} $ and it linearly interpolates its own lower bound $ n \lg n - n + 1 $ between these points.|['Marek A. Suchenek']|['cs.DS', 'cs.CC', 'cs.DM', '68W40 Analysis of algorithms', 'F.2.2; G.2.0; G.2.1; G.2.2']
2017-03-16T23:33:24Z|2017-02-26T00:51:38Z|http://arxiv.org/abs/1702.07961v1|http://arxiv.org/pdf/1702.07961v1|An Efficient Multiway Mergesort for GPU Architectures|Sorting is a primitive operation that is a building block for countless algorithms. As such, it is important to design sorting algorithms that approach peak performance on a range of hardware architectures. Graphics Processing Units (GPUs) are particularly attractive architectures as they provides massive parallelism and computing power. However, the intricacies of their compute and memory hierarchies make designing GPU-efficient algorithms challenging. In this work we present GPU Multiway Mergesort (MMS), a new GPU-efficient multiway mergesort algorithm. MMS employs a new partitioning technique that exposes the parallelism needed by modern GPU architectures. To the best of our knowledge, MMS is the first sorting algorithm for the GPU that is asymptotically optimal in terms of global memory accesses and that is completely free of shared memory bank conflicts. We realize an initial implementation of MMS, evaluate its performance on three modern GPU architectures, and compare it to competitive implementations available in state-of-the- art GPU libraries. Despite these implementations being highly optimized, MMS compares favorably, achieving performance improvements for most random inputs. Furthermore, unlike MMS, state-of-the-art algorithms are susceptible to bank conflicts. We find that for certain inputs that cause these algorithms to incur large numbers of bank conflicts, MMS can achieve a 33.7% performance improvement over its fastest competitor. Overall, even though its current implementation is not fully optimized, due to its efficient use of the memory hierarchy, MMS outperforms the fastest comparison-based sorting implementations available to date.|['Henri Casanova', 'John Iacono', 'Ben Karsin', 'Nodari Sitchinava', 'Volker Weichert']|['cs.DS']
2017-03-16T23:33:24Z|2017-02-25T04:13:22Z|http://arxiv.org/abs/1702.07832v1|http://arxiv.org/pdf/1702.07832v1|Constructing Adjacency Arrays from Incidence Arrays|Graph construction, a fundamental operation in a data processing pipeline, is typically done by multiplying the incidence array representations of a graph, $\mathbf{E}_\mathrm{in}$ and $\mathbf{E}_\mathrm{out}$, to produce an adjacency array of the graph, $\mathbf{A}$, that can be processed with a variety of algorithms. This paper provides the mathematical criteria to determine if the product $\mathbf{A} = \mathbf{E}^{\sf T}_\mathrm{out}\mathbf{E}_\mathrm{in}$ will have the required structure of the adjacency array of the graph. The values in the resulting adjacency array are determined by the corresponding addition $\oplus$ and multiplication $\otimes$ operations used to perform the array multiplication. Illustrations of the various results possible from different $\oplus$ and $\otimes$ operations are provided using a small collection of popular music metadata.|['Hayden Jananthan', 'Karia Dibert', 'Jeremy Kepner']|['cs.DS', 'cs.DM', 'math.CO']
2017-03-16T23:33:24Z|2017-02-25T01:24:03Z|http://arxiv.org/abs/1702.07815v1|http://arxiv.org/pdf/1702.07815v1|Subquadratic Algorithms for the Diameter and the Sum of Pairwise   Distances in Planar Graphs|We show how to compute for $n$-vertex planar graphs in $O(n^{11/6}{\rm polylog}(n))$ expected time the diameter and the sum of the pairwise distances. The algorithms work for directed graphs with real weights and no negative cycles. In $O(n^{15/8}{\rm polylog}(n))$ expected time we can also compute the number of pairs of vertices at distance smaller than a given threshold. These are the first algorithms for these problems using time $O(n^c)$ for some constant $c<2$, even when restricted to undirected, unweighted planar graphs.|['Sergio Cabello']|['cs.DS']
2017-03-16T23:33:24Z|2017-02-24T18:59:08Z|http://arxiv.org/abs/1702.07709v1|http://arxiv.org/pdf/1702.07709v1|Computationally Efficient Robust Estimation of Sparse Functionals|Many conventional statistical procedures are extremely sensitive to seemingly minor deviations from modeling assumptions. This problem is exacerbated in modern high-dimensional settings, where the problem dimension can grow with and possibly exceed the sample size. We consider the problem of robust estimation of sparse functionals, and provide a computationally and statistically efficient algorithm in the high-dimensional setting. Our theory identifies a unified set of deterministic conditions under which our algorithm guarantees accurate recovery. By further establishing that these deterministic conditions hold with high-probability for a wide range of statistical models, our theory applies to many problems of considerable interest including sparse mean and covariance estimation; sparse linear regression; and sparse generalized linear models.|['Simon S. Du', 'Sivaraman Balakrishnan', 'Aarti Singh']|['stat.ML', 'cs.DS', 'cs.LG']
2017-03-16T23:33:24Z|2017-02-24T18:30:03Z|http://arxiv.org/abs/1702.07696v1|http://arxiv.org/pdf/1702.07696v1|An Efficient Data Structure for Dynamic Two-Dimensional Reconfiguration|In the presence of dynamic insertions and deletions into a partially reconfigurable FPGA, fragmentation is unavoidable. This poses the challenge of developing efficient approaches to dynamic defragmentation and reallocation. One key aspect is to develop efficient algorithms and data structures that exploit the two-dimensional geometry of a chip, instead of just one. We propose a new method for this task, based on the fractal structure of a quadtree, which allows dynamic segmentation of the chip area, along with dynamically adjusting the necessary communication infrastructure. We describe a number of algorithmic aspects, and present different solutions. We also provide a number of basic simulations that indicate that the theoretical worst-case bound may be pessimistic.|['Sándor P. Fekete', 'Jan-Marc Reinhardt', 'Christian Scheffer']|['cs.DS', 'F.2.2']
2017-03-16T23:33:24Z|2017-02-24T17:18:02Z|http://arxiv.org/abs/1702.07669v1|http://arxiv.org/pdf/1702.07669v1|On problems equivalent to (min,+)-convolution|In the recent years, significant progress has been made in explaining apparent hardness of improving over naive solutions for many fundamental polynomially solvable problems. This came in the form of conditional lower bounds - reductions to one of problems assumed to be hard. These include 3SUM, All-Pairs Shortest Paths, SAT and Orthogonal Vectors, and others.   In the (min,+)-convolution problem, the goal is to compute a sequence $(c[i])^{n-1}_{i=0}$, where $c[k] = \min_{i=0,\ldots,k} \{a[i]+b[k-i]\}$, given sequences $(a[i])^{n-1}_{i=0}$ and $(b[i])_{i=0}^{n-1}$. This can easily be done in $O(n^2)$ time, but no $O(n^{2-\varepsilon})$ algorithm is known for $\varepsilon > 0$. In this paper we undertake a systematic study of the (min,+)-convolution problem as a hardness assumption.   As the first step, we establish equivalence of this problem to a group of other problems, including variants of the classic knapsack problem and problems related to subadditive sequences. The (min,+)-convolution has been used as a building block in algorithms for many problems, notably problems in stringology. It has also already appeared as an ad hoc hardness assumption. We investigate some of these connections and provide new reductions and other results.|['Marek Cygan', 'Marcin Mucha', 'Karol Węgrzycki', 'Michał Włodarczyk']|['cs.DS', 'cs.CC', 'F.1.3; F.2']
2017-03-16T23:33:24Z|2017-02-24T17:09:22Z|http://arxiv.org/abs/1702.07665v1|http://arxiv.org/pdf/1702.07665v1|Truthful Mechanisms for Delivery with Mobile Agents|"We study the game-theoretic task of selecting mobile agents to deliver multiple items on a network. An instance is given by $m$ messages (physical objects) which have to be transported between specified source-target pairs in a weighted undirected graph, and $k$ mobile heterogeneous agents, each being able to transport one message at a time. Following a recent model by [B\""artschi et al. 2016], each agent $i$ consumes energy proportional to the distance it travels in the graph, where the different rates of energy consumption are given by weight factors $w_i$. We are interested in optimizing or approximating the total energy consumption over all selected agents.   Unlike previous research, we assume the weights to be private values known only to the respective agents. We present three different mechanisms which select, route and pay the agents in a truthful way that guarantees voluntary participation of the agents, while approximating the optimum energy consumption by a constant factor. To this end we analyze a previous structural result and an approximation algorithm given by [B\""artschi et al. 2017]. Finally, we show that for some instances in the case of a single message ($m=1$), the sum of the payments can be bounded in terms of the optimum as well."|['Andreas Bärtschi', 'Daniel Graf', 'Paolo Penna']|['cs.GT', 'cs.DS']
2017-03-16T23:33:28Z|2017-03-06T10:54:22Z|http://arxiv.org/abs/1702.07578v2|http://arxiv.org/pdf/1702.07578v2|Fast and Simple Parallel Wavelet Tree and Matrix Construction|The wavelet tree (Grossi et al. [SODA, 2003]) and wavelet matrix (Claude et al. [Inf. Syst., 47:15--32, 2015]) are compact indices for texts over an alphabet $[0,\sigma)$ that support rank, select and access queries in $O(\lg \sigma)$ time. We first present new practical sequential and parallel algorithms for wavelet matrix construction. Their unifying characteristics is that they construct the wavelet matrix bottom-up, i.e., they compute the last level first. We also show that this bottom-up construction can easily be adapted to wavelet trees. In practice, our best sequential algorithm is up to twice as fast as the currently fastest sequential construction algorithm (serialWT), simultaneously saving a factor of 2 in space. On 4 cores, our best parallel algorithm is at least twice as fast as the currently fastest parallel algorithm (recWT), while also using less space. This scales up to 32 cores, where we are about equally fast as recWT, but still use only about 75% of the space. An additional theoretical result shows how to adapt any wavelet tree construction algorithm to the wavelet matrix in the same (asymptotic) time, using only little extra space.|['Johannes Fischer', 'Florian Kurpicz']|['cs.DS']
2017-03-16T23:33:28Z|2017-02-24T13:41:29Z|http://arxiv.org/abs/1702.07577v1|http://arxiv.org/pdf/1702.07577v1|Compression with the tudocomp Framework|We present a framework facilitating the implementation and comparison of text compression algorithms. We evaluate its features by a case study on two novel compression algorithms based on the Lempel-Ziv compression schemes that perform well on highly repetitive texts.|['Patrick Dinklage', 'Johannes Fischer', 'Dominik Köppl', 'Marvin Löbel', 'Kunihiko Sadakane']|['cs.DS']
2017-03-16T23:33:28Z|2017-02-24T03:57:57Z|http://arxiv.org/abs/1702.07458v1|http://arxiv.org/pdf/1702.07458v1|Small-space encoding LCE data structure with constant-time queries|"The \emph{longest common extension} (\emph{LCE}) problem is to preprocess a given string $w$ of length $n$ so that the length of the longest common prefix between suffixes of $w$ that start at any two given positions is answered quickly. In this paper, we present a data structure of $O(z \tau^2 + \frac{n}{\tau})$ words of space which answers LCE queries in $O(1)$ time and can be built in $O(n \log \sigma)$ time, where $1 \leq \tau \leq \sqrt{n}$ is a parameter, $z$ is the size of the Lempel-Ziv 77 factorization of $w$ and $\sigma$ is the alphabet size. This is an \emph{encoding} data structure, i.e., it does not access the input string $w$ when answering queries and thus $w$ can be deleted after preprocessing. On top of this main result, we obtain further results using (variants of) our LCE data structure, which include the following:   - For highly repetitive strings where the $z\tau^2$ term is dominated by $\frac{n}{\tau}$, we obtain a \emph{constant-time and sub-linear space} LCE query data structure.   - Even when the input string is not well compressible via Lempel-Ziv 77 factorization, we still can obtain a \emph{constant-time and sub-linear space} LCE data structure for suitable $\tau$ and for $\sigma \leq 2^{o(\log n)}$.   - The time-space trade-off lower bounds for the LCE problem by Bille et al. [J. Discrete Algorithms, 25:42-50, 2014] and by Kosolobov [CoRR, abs/1611.02891, 2016] can be ""surpassed"" in some cases with our LCE data structure."|['Yuka Tanimura', 'Takaaki Nishimoto', 'Hideo Bannai', 'Shunsuke Inenaga', 'Masayuki Takeda']|['cs.DS']
2017-03-16T23:33:28Z|2017-02-24T01:28:11Z|http://arxiv.org/abs/1702.07435v1|http://arxiv.org/pdf/1702.07435v1|Capacitated Center Problems with Two-Sided Bounds and Outliers|In recent years, the capacitated center problems have attracted a lot of research interest. Given a set of vertices $V$, we want to find a subset of vertices $S$, called centers, such that the maximum cluster radius is minimized. Moreover, each center in $S$ should satisfy some capacity constraint, which could be an upper or lower bound on the number of vertices it can serve. Capacitated $k$-center problems with one-sided bounds (upper or lower) have been well studied in previous work, and a constant factor approximation was obtained.   We are the first to study the capacitated center problem with both capacity lower and upper bounds (with or without outliers). We assume each vertex has a uniform lower bound and a non-uniform upper bound. For the case of opening exactly $k$ centers, we note that a generalization of a recent LP approach can achieve constant factor approximation algorithms for our problems. Our main contribution is a simple combinatorial algorithm for the case where there is no cardinality constraint on the number of open centers. Our combinatorial algorithm is simpler and achieves better constant approximation factor compared to the LP approach.|['Hu Ding', 'Lunjia Hu', 'Lingxiao Huang', 'Jian Li']|['cs.DS']
2017-03-16T23:33:28Z|2017-02-23T21:49:52Z|http://arxiv.org/abs/1702.07403v1|http://arxiv.org/pdf/1702.07403v1|Making Asynchronous Distributed Computations Robust to Noise|We consider the problem of making distributed computations robust to noise, in particular to worst-case (adversarial) corruptions of messages. We give a general distributed interactive coding scheme which simulates any asynchronous distributed protocol while tolerating an optimal corruption of a $\Theta(1/n)$ fraction of all messages while incurring a moderate blowup of $O(n\log^2 n)$ in the communication complexity.   Our result is the first fully distributed interactive coding scheme in which the topology of the communication network is not known in advance. Prior work required either a coordinating node to be connected to all other nodes in the network or assumed a synchronous network in which all nodes already know the complete topology of the network.|['Keren Censor-Hillel', 'Ran Gelles', 'Bernhard Haeupler']|['cs.DS', 'cs.DC']
2017-03-16T23:33:28Z|2017-02-23T16:59:25Z|http://arxiv.org/abs/1702.07292v1|http://arxiv.org/pdf/1702.07292v1|Network Construction with Ordered Constraints|In this paper, we study the problem of constructing a network by observing ordered connectivity constraints, which we define herein. These ordered constraints are made to capture realistic properties of real-world problems that are not reflected in previous, more general models. We give hardness of approximation results and nearly-matching upper bounds for the offline problem, and we study the online problem in both general graphs and restricted sub-classes. In the online problem, for general graphs, we give exponentially better upper bounds than exist for algorithms for general connectivity problems. For the restricted classes of stars and paths we are able to find algorithms with optimal competitive ratios, the latter of which involve analysis using a potential function defined over pq-trees.|['Yi Huang', 'Mano Vikash Janardhanan', 'Lev Reyzin']|['cs.DS']
2017-03-16T23:33:28Z|2017-02-23T10:57:56Z|http://arxiv.org/abs/1702.07172v1|http://arxiv.org/pdf/1702.07172v1|Tight Bounds for Online Coloring of Basic Graph Classes|We resolve a number of long-standing open problems in online graph coloring. More specifically, we develop tight lower bounds on the performance of online algorithms for fundamental graph classes. An important contribution is that our bounds also hold for randomized online algorithms, for which hardly any results were known. Technically, we construct lower bounds for chordal graphs. The constructions then allow us to derive results on the performance of randomized online algorithms for the following further graph classes: trees, planar, bipartite, inductive, bounded-treewidth and disk graphs. It shows that the best competitive ratio of both deterministic and randomized online algorithms is $\Theta(\log n)$, where $n$ is the number of vertices of a graph. Furthermore, we prove that this guarantee cannot be improved if an online algorithm has a lookahead of size $O(n/\log n)$ or access to a reordering buffer of size $n^{1-\epsilon}$, for any $0<\epsilon\leq 1$. A consequence of our results is that, for all of the above mentioned graph classes except bipartite graphs, the natural $\textit{First Fit}$ coloring algorithm achieves an optimal performance, up to constant factors, among deterministic and randomized online algorithms.|['Susanne Albers', 'Sebastian Schraink']|['cs.DS', 'cs.DM']
2017-03-16T23:33:28Z|2017-02-23T08:35:45Z|http://arxiv.org/abs/1702.07134v1|http://arxiv.org/pdf/1702.07134v1|Diverse Weighted Bipartite b-Matching|Bipartite matching, where agents on one side of a market are matched to agents or items on the other, is a classical problem in computer science and economics, with widespread application in healthcare, education, advertising, and general resource allocation. A practitioner's goal is typically to maximize a matching market's economic efficiency, possibly subject to some fairness requirements that promote equal access to resources. A natural balancing act exists between fairness and efficiency in matching markets, and has been the subject of much research.   In this paper, we study a complementary goal---balancing diversity and efficiency---in a generalization of bipartite matching where agents on one side of the market can be matched to sets of agents on the other. Adapting a classical definition of the diversity of a set, we propose a quadratic programming-based approach to solving a supermodular minimization problem that balances diversity and total weight of the solution. We also provide a scalable greedy algorithm with theoretical performance bounds. We then define the price of diversity, a measure of the efficiency loss due to enforcing diversity, and give a worst-case theoretical bound. Finally, we demonstrate the efficacy of our methods on three real-world datasets, and show that the price of diversity is not bad in practice.|['Faez Ahmed', 'John P. Dickerson', 'Mark Fuge']|['cs.DS']
2017-03-16T23:33:28Z|2017-02-22T22:43:45Z|http://arxiv.org/abs/1702.07032v1|http://arxiv.org/pdf/1702.07032v1|On the Complexity of Bundle-Pricing and Simple Mechanisms|We show that the problem of finding an optimal bundle-pricing for a single additive buyer is #P-hard, even when the distributions have support size 2 for each item and the optimal solution is guaranteed to be a simple one: the seller picks a price for the grand bundle and a price for each individual item; the buyer can purchase either the grand bundle at the given price or any bundle of items at their total individual prices. We refer to this simple and natural family of pricing schemes as discounted item-pricings. In addition to the hardness result, we show that when the distributions are i.i.d. with support size 2, a discounted item-pricing can achieve the optimal revenue obtainable by lottery-pricings and it can be found in polynomial time.|['Xi Chen', 'George Matikas', 'Dimitris Paparas', 'Mihalis Yannakakis']|['cs.GT', 'cs.CC', 'cs.DS']
2017-03-16T23:33:28Z|2017-02-22T20:59:22Z|http://arxiv.org/abs/1702.07002v1|http://arxiv.org/pdf/1702.07002v1|Breaking the Bonds of Submodularity: Empirical Estimation of   Approximation Ratios for Monotone Non-Submodular Greedy Maximization|While greedy algorithms have long been observed to perform well on a wide variety of problems, up to now approximation ratios have only been known for their application to problems having submodular objective functions $f$. Since many practical problems have non-submodular $f$, there is a critical need to devise new techniques to bound the performance of greedy algorithms in the case of non-submodularity.   Our primary contribution is the introduction of a novel technique for estimating the approximation ratio of the greedy algorithm for maximization of monotone non-decreasing functions based on the curvature of $f$ without relying on the submodularity constraint. We show that this technique reduces to the classical $(1 - 1/e)$ ratio for submodular functions. Furthermore, we develop an extension of this ratio to the adaptive greedy algorithm, which allows applications to non-submodular stochastic maximization problems. This notably extends support to applications modeling incomplete data with uncertainty. Finally we use this new technique to derive a $(1 - 1/\sqrt{e})$ ratio for a popular problem, Robust Influence Maximization, which is non-submodular and $1/2$ for Adaptive Max-Crawling, which is adaptive non-submodular.|['J. David Smith', 'My T. Thai']|['cs.DS', 'cs.DM', 'F.2.2; G.2']
