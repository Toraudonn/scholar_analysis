2017-03-16T23:24:55Z|2017-03-15T14:47:02Z|http://arxiv.org/abs/1703.05189v1|http://arxiv.org/pdf/1703.05189v1|Student-t Process Quadratures for Filtering of Non-Linear Systems with   Heavy-Tailed Noise|The aim of this article is to design a moment transformation for Student- t distributed random variables, which is able to account for the error in the numerically computed mean. We employ Student-t process quadrature, an instance of Bayesian quadrature, which allows us to treat the integral itself as a random variable whose variance provides information about the incurred integration error. Advantage of the Student- t process quadrature over the traditional Gaussian process quadrature, is that the integral variance depends also on the function values, allowing for a more robust modelling of the integration error. The moment transform is applied in nonlinear sigma-point filtering and evaluated on two numerical examples, where it is shown to outperform the state-of-the-art moment transforms.|['Jakub Prüher', 'Filip Tronarp', 'Toni Karvonen', 'Simo Särkkä', 'Ondřej Straka']|['stat.ME', 'stat.ML']
2017-03-16T23:24:55Z|2017-03-15T14:31:55Z|http://arxiv.org/abs/1703.05175v1|http://arxiv.org/pdf/1703.05175v1|Prototypical Networks for Few-shot Learning|We propose prototypical networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classification can be performed by computing Euclidean distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve state-of-the-art results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to the case of zero-shot learning and achieve state-of-the-art zero-shot results on the CU-Birds dataset.|['Jake Snell', 'Kevin Swersky', 'Richard S. Zemel']|['cs.LG', 'stat.ML']
2017-03-16T23:24:55Z|2017-03-15T14:01:21Z|http://arxiv.org/abs/1703.05160v1|http://arxiv.org/pdf/1703.05160v1|A New Unbiased and Efficient Class of LSH-Based Samplers and Estimators   for Partition Function Computation in Log-Linear Models|"Log-linear models are arguably the most successful class of graphical models for large-scale applications because of their simplicity and tractability. Learning and inference with these models require calculating the partition function, which is a major bottleneck and intractable for large state spaces. Importance Sampling (IS) and MCMC-based approaches are lucrative. However, the condition of having a ""good"" proposal distribution is often not satisfied in practice.   In this paper, we add a new dimension to efficient estimation via sampling. We propose a new sampling scheme and an unbiased estimator that estimates the partition function accurately in sub-linear time. Our samples are generated in near-constant time using locality sensitive hashing (LSH), and so are correlated and unnormalized. We demonstrate the effectiveness of our proposed approach by comparing the accuracy and speed of estimating the partition function against other state-of-the-art estimation techniques including IS and the efficient variant of Gumbel-Max sampling. With our efficient sampling scheme, we accurately train real-world language models using only 1-2% of computations."|['Ryan Spring', 'Anshumali Shrivastava']|['stat.ML', 'cs.DB', 'cs.DS', 'cs.LG']
2017-03-16T23:24:55Z|2017-03-15T11:17:02Z|http://arxiv.org/abs/1703.05082v1|http://arxiv.org/pdf/1703.05082v1|Selective Harvesting over Networks|Active search (AS) on graphs focuses on collecting certain labeled nodes (targets) given global knowledge of the network topology and its edge weights under a query budget. However, in most networks, nodes, topology and edge weights are all initially unknown. We introduce selective harvesting, a variant of AS where the next node to be queried must be chosen among the neighbors of the current queried node set; the available training data for deciding which node to query is restricted to the subgraph induced by the queried set (and their node attributes) and their neighbors (without any node or edge attributes). Therefore, selective harvesting is a sequential decision problem, where we must decide which node to query at each step. A classifier trained in this scenario suffers from a tunnel vision effect: without recourse to independent sampling, the urge to query promising nodes forces classifiers to gather increasingly biased training data, which we show significantly hurts the performance of AS methods and standard classifiers. We find that it is possible to collect a much larger set of targets by using multiple classifiers, not by combining their predictions as an ensemble, but switching between classifiers used at each step, as a way to ease the tunnel vision effect. We discover that switching classifiers collects more targets by (a) diversifying the training data and (b) broadening the choices of nodes that can be queried next. This highlights an exploration, exploitation, and diversification trade-off in our problem that goes beyond the exploration and exploitation duality found in classic sequential decision problems. From these observations we propose D3TS, a method based on multi-armed bandits for non-stationary stochastic processes that enforces classifier diversity, matching or exceeding the performance of competing methods on seven real network datasets in our evaluation.|['Fabricio Murai', 'Diogo Rennó', 'Bruno Ribeiro', 'Gisele L. Pappa', 'Don Towsley', 'Krista Gile']|['cs.SI', 'cs.LG', 'stat.ML', 'I.2.6; E.1']
2017-03-16T23:24:55Z|2017-03-15T11:07:13Z|http://arxiv.org/abs/1703.05080v1|http://arxiv.org/pdf/1703.05080v1|Tuning Free Orthogonal Matching Pursuit|Orthogonal matching pursuit (OMP) is a widely used compressive sensing (CS) algorithm for recovering sparse signals in noisy linear regression models. The performance of OMP depends on its stopping criteria (SC). SC for OMP discussed in literature typically assumes knowledge of either the sparsity of the signal to be estimated $k_0$ or noise variance $\sigma^2$, both of which are unavailable in many practical applications. In this article we develop a modified version of OMP called tuning free OMP or TF-OMP which does not require a SC. TF-OMP is proved to accomplish successful sparse recovery under the usual assumptions on restricted isometry constants (RIC) and mutual coherence of design matrix. TF-OMP is numerically shown to deliver a highly competitive performance in comparison with OMP having \textit{a priori} knowledge of $k_0$ or $\sigma^2$. Greedy algorithm for robust de-noising (GARD) is an OMP like algorithm proposed for efficient estimation in classical overdetermined linear regression models corrupted by sparse outliers. However, GARD requires the knowledge of inlier noise variance which is difficult to estimate. We also produce a tuning free algorithm (TF-GARD) for efficient estimation in the presence of sparse outliers by extending the operating principle of TF-OMP to GARD. TF-GARD is numerically shown to achieve a performance comparable to that of the existing implementation of GARD.|['Sreejith Kallummil', 'Sheetal Kalyani']|['stat.ML', 'cs.IT', 'math.IT']
2017-03-16T23:24:55Z|2017-03-15T10:20:32Z|http://arxiv.org/abs/1703.05060v1|http://arxiv.org/pdf/1703.05060v1|Online Learning for Distribution-Free Prediction|We develop an online learning method for prediction, which is important in problems with large and/or streaming data sets. We formulate the learning approach using a covariance-fitting methodology, and show that the resulting predictor has desirable computational and distribution-free properties: It is implemented online with a runtime that scales linearly in the number of samples; has a constant memory requirement; avoids local minima problems; and prunes away redundant feature dimensions without relying on restrictive assumptions on the data distribution. In conjunction with the split conformal approach, it also produces distribution-free prediction confidence intervals in a computationally efficient manner. The method is demonstrated on both real and synthetic datasets.|['Dave Zachariah', 'Petre Stoica', 'Thomas B. Schön']|['cs.LG', 'stat.CO', 'stat.ML']
2017-03-16T23:24:55Z|2017-03-15T07:46:18Z|http://arxiv.org/abs/1703.04986v1|http://arxiv.org/abs/1703.04986v1|Label Stability in Multiple Instance Learning|We address the problem of \emph{instance label stability} in multiple instance learning (MIL) classifiers. These classifiers are trained only on globally annotated images (bags), but often can provide fine-grained annotations for image pixels or patches (instances). This is interesting for computer aided diagnosis (CAD) and other medical image analysis tasks for which only a coarse labeling is provided. Unfortunately, the instance labels may be unstable. This means that a slight change in training data could potentially lead to abnormalities being detected in different parts of the image, which is undesirable from a CAD point of view. Despite MIL gaining popularity in the CAD literature, this issue has not yet been addressed. We investigate the stability of instance labels provided by several MIL classifiers on 5 different datasets, of which 3 are medical image datasets (breast histopathology, diabetic retinopathy and computed tomography lung images). We propose an unsupervised measure to evaluate instance stability, and demonstrate that a performance-stability trade-off can be made when comparing MIL classifiers.|['Veronika Cheplygina', 'Lauge Sørensen', 'David M. J. Tax', 'Marleen de Bruijne', 'Marco Loog']|['cs.CV', 'stat.ML']
2017-03-16T23:24:55Z|2017-03-15T07:43:10Z|http://arxiv.org/abs/1703.04981v1|http://arxiv.org/pdf/1703.04981v1|Transfer Learning by Asymmetric Image Weighting for Segmentation across   Scanners|Supervised learning has been very successful for automatic segmentation of images from a single scanner. However, several papers report deteriorated performances when using classifiers trained on images from one scanner to segment images from other scanners. We propose a transfer learning classifier that adapts to differences between training and test images. This method uses a weighted ensemble of classifiers trained on individual images. The weight of each classifier is determined by the similarity between its training image and the test image.   We examine three unsupervised similarity measures, which can be used in scenarios where no labeled data from a newly introduced scanner or scanning protocol is available. The measures are based on a divergence, a bag distance, and on estimating the labels with a clustering procedure. These measures are asymmetric. We study whether the asymmetry can improve classification. Out of the three similarity measures, the bag similarity measure is the most robust across different studies and achieves excellent results on four brain tissue segmentation datasets and three white matter lesion segmentation datasets, acquired at different centers and with different scanners and scanning protocols. We show that the asymmetry can indeed be informative, and that computing the similarity from the test image to the training images is more appropriate than the opposite direction.|['Veronika Cheplygina', 'Annegreet van Opbroek', 'M. Arfan Ikram', 'Meike W. Vernooij', 'Marleen de Bruijne']|['cs.CV', 'stat.ML']
2017-03-16T23:24:55Z|2017-03-15T07:41:49Z|http://arxiv.org/abs/1703.04980v1|http://arxiv.org/abs/1703.04980v1|Classification of COPD with Multiple Instance Learning|Chronic obstructive pulmonary disease (COPD) is a lung disease where early detection benefits the survival rate. COPD can be quantified by classifying patches of computed tomography images, and combining patch labels into an overall diagnosis for the image. As labeled patches are often not available, image labels are propagated to the patches, incorrectly labeling healthy patches in COPD patients as being affected by the disease. We approach quantification of COPD from lung images as a multiple instance learning (MIL) problem, which is more suitable for such weakly labeled data. We investigate various MIL assumptions in the context of COPD and show that although a concept region with COPD-related disease patterns is present, considering the whole distribution of lung tissue patches improves the performance. The best method is based on averaging instances and obtains an AUC of 0.742, which is higher than the previously reported best of 0.713 on the same dataset. Using the full training set further increases performance to 0.776, which is significantly higher (DeLong test) than previous results.|['Veronika Cheplygina', 'Lauge Sørensen', 'David M. J. Tax', 'Jesper Holst Pedersen', 'Marco Loog', 'Marleen de Bruijne']|['cs.CV', 'stat.ML']
2017-03-16T23:24:55Z|2017-03-15T05:47:37Z|http://arxiv.org/abs/1703.04943v1|http://arxiv.org/pdf/1703.04943v1|Matched bipartite block model with covariates|Community detection or clustering is a fundamental task in the analysis of network data. Many real networks have a bipartite structure which makes community detection challenging. In this paper, we consider a model which allows for matched communities in the bipartite setting, in addition to node covariates with information about the matching. We derive a simple fast algorithm for fitting the model based on variational inference ideas and show its effectiveness on both simulated and real data. A variation of the model to allow for degree-correction is also considered, in addition to a novel approach to fitting such degree-corrected models.|['Zahra S. Razaee', 'Arash A. Amini', 'Jingyi Jessica Li']|['cs.SI', 'cs.LG', 'stat.ML']
2017-03-16T23:24:59Z|2017-03-15T05:43:48Z|http://arxiv.org/abs/1703.04940v1|http://arxiv.org/pdf/1703.04940v1|Resilience: A Criterion for Learning in the Presence of Arbitrary   Outliers|We introduce a criterion, resilience, which allows properties of a dataset (such as its mean or best low rank approximation) to be robustly computed, even in the presence of a large fraction of arbitrary additional data. Resilience is a weaker condition than most other properties considered so far in the literature, and yet enables robust estimation in a broader variety of settings, including the previously unstudied problem of robust mean estimation in $\ell_p$-norms.|['Jacob Steinhardt', 'Moses Charikar', 'Gregory Valiant']|['cs.LG', 'cs.AI', 'cs.CC', 'cs.CR', 'stat.ML']
2017-03-16T23:24:59Z|2017-03-15T02:34:39Z|http://arxiv.org/abs/1703.04890v1|http://arxiv.org/pdf/1703.04890v1|Riemannian stochastic quasi-Newton algorithm with variance reduction and   its convergence analysis|Stochastic variance reduction algorithms have recently become popular for minimizing the average of a large, but finite number of loss functions. The present paper proposes a Riemannian stochastic quasi-Newton algorithm with variance reduction (R-SQN-VR). The key challenges of averaging, adding, and subtracting multiple gradients are addressed with notions of retraction and vector transport. We present a global convergence analysis and a local convergence rate analysis of R-SQN-VR under some natural assumptions. The proposed algorithm is applied to the Karcher mean computation on the symmetric positive-definite manifold and low-rank matrix completion on the Grassmann manifold. In all cases, the proposed algorithm outperforms the Riemannian stochastic gradient descent and the Riemannian stochastic variance reduction algorithms.|['Hiroyuki Kasai', 'Hiroyuki Sato', 'Bamdev Mishra']|['cs.LG', 'cs.NA', 'math.NA', 'math.OC', 'stat.ML']
2017-03-16T23:24:59Z|2017-03-15T01:16:09Z|http://arxiv.org/abs/1703.04864v1|http://arxiv.org/pdf/1703.04864v1|Optimization for L1-Norm Error Fitting via Data Aggregation|We propose a data aggregation-based algorithm with monotonic convergence to a global optimum for a generalized version of the L1-norm error fitting model with an assumption of the fitting function, by generalizing the previously proposed algorithm by Park and Klabjan (2016). The new algorithm can solve multi-dimensional fitting problems with arbitrary constraints on the fitting coefficients. Any model following the form can be solved optimally using the proposed algorithm. The generalized problem includes popular models such as regression, principal component analysis, and the orthogonal Procrustes problem. The results of the computational experiment show that the proposed algorithms are up to 9,000 times faster than the state-of-the-art benchmarks for the problems and data sets studied.|['Young Woong Park']|['stat.ML']
2017-03-16T23:24:59Z|2017-03-14T23:35:57Z|http://arxiv.org/abs/1703.04832v1|http://arxiv.org/pdf/1703.04832v1|A Random Finite Set Model for Data Clustering|The goal of data clustering is to partition data points into groups to minimize a given objective function. While most existing clustering algorithms treat each data point as vector, in many applications each datum is not a vector but a point pattern or a set of points. Moreover, many existing clustering methods require the user to specify the number of clusters, which is not available in advance. This paper proposes a new class of models for data clustering that addresses set-valued data as well as unknown number of clusters, using a Dirichlet Process mixture of Poisson random finite sets. We also develop an efficient Markov Chain Monte Carlo posterior inference technique that can learn the number of clusters and mixture parameters automatically from the data. Numerical studies are presented to demonstrate the salient features of this new model, in particular its capacity to discover extremely unbalanced clusters in data.|['Dinh Phung', 'Ba-Ngu Bo']|['stat.ML']
2017-03-16T23:24:59Z|2017-03-14T23:20:17Z|http://arxiv.org/abs/1703.04823v1|http://arxiv.org/pdf/1703.04823v1|Classification in biological networks with hypergraphlet kernels|Biological and cellular systems are often modeled as graphs in which vertices represent objects of interest (genes, proteins, drugs) and edges represent relational ties among these objects (binds-to, interacts-with, regulates). This approach has been highly successful owing to the theory, methodology and software that support analysis and learning on graphs. Graphs, however, often suffer from information loss when modeling physical systems due to their inability to accurately represent multiobject relationships. Hypergraphs, a generalization of graphs, provide a framework to mitigate information loss and unify disparate graph-based methodologies. In this paper, we present a hypergraph-based approach for modeling physical systems and formulate vertex classification, edge classification and link prediction problems on (hyper)graphs as instances of vertex classification on (extended, dual) hypergraphs in a semi-supervised setting. We introduce a novel kernel method on vertex- and edge-labeled (colored) hypergraphs for analysis and learning. The method is based on exact and inexact (via hypergraph edit distances) enumeration of small simple hypergraphs, referred to as hypergraphlets, rooted at a vertex of interest. We extensively evaluate this method and show its potential use in a positive-unlabeled setting to estimate the number of missing and false positive links in protein-protein interaction networks.|['Jose Lugo-Martinez', 'Predrag Radivojac']|['stat.ML', 'cs.LG']
2017-03-16T23:24:59Z|2017-03-14T23:05:54Z|http://arxiv.org/abs/1703.04813v1|http://arxiv.org/pdf/1703.04813v1|Learned Optimizers that Scale and Generalize|Learning to learn has emerged as an important direction for achieving artificial intelligence. Two of the primary barriers to its adoption are an inability to scale to larger problems and a limited ability to generalize to new tasks. We introduce a learned gradient descent optimizer that generalizes well to new tasks, and which has significantly reduced memory and computation overhead. We achieve this by introducing a novel hierarchical RNN architecture, with minimal per-parameter overhead, augmented with additional architectural features that mirror the known structure of optimization tasks. We also develop a meta-training ensemble of small, diverse, optimization tasks capturing common properties of loss landscapes. The optimizer learns to out-perform RMSProp/ADAM on problems in this corpus. More importantly, it performs comparably or better when applied to small convolutional neural networks, despite seeing no neural networks in its meta-training set. Finally, it generalizes to train Inception V3 and ResNet V2 architectures on the ImageNet dataset, optimization problems that are of a vastly different scale than those it was trained on.|['Olga Wichrowska', 'Niru Maheswaranathan', 'Matthew W. Hoffman', 'Sergio Gomez Colmenarejo', 'Misha Denil', 'Nando de Freitas', 'Jascha Sohl-Dickstein']|['cs.LG', 'cs.NE', 'stat.ML']
2017-03-16T23:24:59Z|2017-03-14T22:28:27Z|http://arxiv.org/abs/1703.04782v1|http://arxiv.org/pdf/1703.04782v1|Online Learning Rate Adaptation with Hypergradient Descent|"We introduce a general method for improving the convergence rate of gradient-based optimizers that is easy to implement and works well in practice. We analyze the effectiveness of the method by applying it to stochastic gradient descent, stochastic gradient descent with Nesterov momentum, and Adam, showing that it improves upon these commonly used algorithms on a range of optimization problems; in particular the kinds of objective functions that arise frequently in deep neural network training. Our method works by dynamically updating the learning rate during optimization using the gradient with respect to the learning rate of the update rule itself. Computing this ""hypergradient"" needs little additional computation, requires only one extra copy of the original gradient to be stored in memory, and relies upon nothing more than what is provided by reverse-mode automatic differentiation."|['Atilim Gunes Baydin', 'Robert Cornish', 'David Martinez Rubio', 'Mark Schmidt', 'Frank Wood']|['cs.LG', 'stat.ML', '68T05', 'G.1.6; I.2.6']
2017-03-16T23:24:59Z|2017-03-14T22:23:17Z|http://arxiv.org/abs/1703.04778v1|http://arxiv.org/pdf/1703.04778v1|A statistical model for aggregating judgments by incorporating peer   predictions|We propose a probabilistic model to aggregate the answers of respondents answering multiple-choice questions. The model does not assume that everyone has access to the same information, and so does not assume that the consensus answer is correct. Instead, it infers the most probable world state, even if only a minority vote for it. Each respondent is modeled as receiving a signal contingent on the actual world state, and as using this signal to both determine their own answer and predict the answers given by others. By incorporating respondent's predictions of others' answers, the model infers latent parameters corresponding to the prior over world states and the probability of different signals being received in all possible world states, including counterfactual ones. Unlike other probabilistic models for aggregation, our model applies to both single and multiple questions, in which case it estimates each respondent's expertise. The model shows good performance, compared to a number of other probabilistic models, on data from seven studies covering different types of expertise.|['John McCoy', 'Drazen Prelec']|['stat.ML']
2017-03-16T23:24:59Z|2017-03-14T22:21:48Z|http://arxiv.org/abs/1703.04775v1|http://arxiv.org/pdf/1703.04775v1|Discriminate-and-Rectify Encoders: Learning from Image Transformation   Sets|The complexity of a learning task is increased by transformations in the input space that preserve class identity. Visual object recognition for example is affected by changes in viewpoint, scale, illumination or planar transformations. While drastically altering the visual appearance, these changes are orthogonal to recognition and should not be reflected in the representation or feature encoding used for learning. We introduce a framework for weakly supervised learning of image embeddings that are robust to transformations and selective to the class distribution, using sets of transforming examples (orbit sets), deep parametrizations and a novel orbit-based loss. The proposed loss combines a discriminative, contrastive part for orbits with a reconstruction error that learns to rectify orbit transformations. The learned embeddings are evaluated in distance metric-based tasks, such as one-shot classification under geometric transformations, as well as face verification and retrieval under more realistic visual variability. Our results suggest that orbit sets, suitably computed or observed, can be used for efficient, weakly-supervised learning of semantically relevant image embeddings.|['Andrea Tacchetti', 'Stephen Voinea', 'Georgios Evangelopoulos']|['cs.CV', 'cs.LG', 'stat.ML']
2017-03-16T23:24:59Z|2017-03-14T22:13:41Z|http://arxiv.org/abs/1703.04757v1|http://arxiv.org/pdf/1703.04757v1|Convergence of Deep Neural Networks to a Hierarchical Covariance Matrix   Decomposition|We show that in a deep neural network trained with ReLU, the low-lying layers should be replaceable with truncated linearly activated layers. We derive the gradient descent equations in this truncated linear model and demonstrate that --if the distribution of the training data is stationary during training-- the optimal choice for weights in these low-lying layers is the eigenvectors of the covariance matrix of the data. If the training data is random and uniform enough, these eigenvectors can be found using a small fraction of the training data, thus reducing the computational complexity of training. We show how this can be done recursively to form successive, trained layers. At least in the first layer, our tests show that this approach improves classification of images while reducing network size.|['Nima Dehmamy', 'Neda Rohani', 'Aggelos Katsaggelos']|['cs.LG', 'physics.data-an', 'stat.ML']
2017-03-16T23:25:03Z|2017-03-14T21:07:01Z|http://arxiv.org/abs/1703.04730v1|http://arxiv.org/pdf/1703.04730v1|Understanding Black-box Predictions via Influence Functions|How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, identifying the points most responsible for a given prediction. Applying ideas from second-order optimization, we scale up influence functions to modern machine learning settings and show that they can be applied to high-dimensional black-box models, even in non-convex and non-differentiable settings. We give a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for many different purposes: to understand model behavior, debug models and detect dataset errors, and even identify and exploit vulnerabilities to adversarial training-set attacks.|['Pang Wei Koh', 'Percy Liang']|['stat.ML', 'cs.AI', 'cs.LG']
2017-03-16T23:25:03Z|2017-03-14T20:19:08Z|http://arxiv.org/abs/1703.04697v1|http://arxiv.org/pdf/1703.04697v1|On the benefits of output sparsity for multi-label classification|The multi-label classification framework, where each observation can be associated with a set of labels, has generated a tremendous amount of attention over recent years. The modern multi-label problems are typically large-scale in terms of number of observations, features and labels, and the amount of labels can even be comparable with the amount of observations. In this context, different remedies have been proposed to overcome the curse of dimensionality. In this work, we aim at exploiting the output sparsity by introducing a new loss, called the sparse weighted Hamming loss. This proposed loss can be seen as a weighted version of classical ones, where active and inactive labels are weighted separately. Leveraging the influence of sparsity in the loss function, we provide improved generalization bounds for the empirical risk minimizer, a suitable property for large-scale problems. For this new loss, we derive rates of convergence linear in the underlying output-sparsity rather than linear in the number of labels. In practice, minimizing the associated risk can be performed efficiently by using convex surrogates and modern convex optimization algorithms. We provide experiments on various real-world datasets demonstrating the pertinence of our approach when compared to non-weighted techniques.|['Evgenii Chzhen', 'Christophe Denis', 'Mohamed Hebiri', 'Joseph Salmon']|['math.ST', 'cs.LG', 'stat.ML', 'stat.TH']
2017-03-16T23:25:03Z|2017-03-14T20:07:12Z|http://arxiv.org/abs/1703.04691v1|http://arxiv.org/pdf/1703.04691v1|Conditional Time Series Forecasting with Convolutional Neural Networks|We develop a modern deep convolutional neural network for conditional time series forecasting based on the recent WaveNet architecture. The proposed network contains stacks of dilated convolutions that widen the receptive field of the forecast; multiple convolutional filters are applied in parallel to separate time series and allow for the fast processing of data and the exploitation of the correlation structure between the multivariate time series. The performance of the deep convolutional neural network is analyzed on various multivariate time series including commodities data and stock indices and compared to that of the well-known autoregressive model and a fully convolutional network. We show that our network is able to effectively learn dependencies between the series without the need of long historical time series and significantly outperforms the baseline neural forecasting models.|['Anastasia Borovykh', 'Sander Bohte', 'Cornelis W. Oosterlee']|['stat.ML']
2017-03-16T23:25:03Z|2017-03-14T17:23:02Z|http://arxiv.org/abs/1703.04599v1|http://arxiv.org/pdf/1703.04599v1|Generalized Self-Concordant Functions: A Recipe for Newton-Type Methods|We study the smooth structure of convex functions by generalizing a powerful concept so-called self-concordance introduced by Nesterov and Nemirovskii in the early 1990s to a broader class of convex functions, which we call generalized self-concordant functions. This notion allows us to develop a unified framework for designing Newton-type methods to solve convex optimiza- tion problems. The proposed theory provides a mathematical tool to analyze both local and global convergence of Newton-type methods without imposing unverifiable assumptions as long as the un- derlying functionals fall into our generalized self-concordant function class. First, we introduce the class of generalized self-concordant functions, which covers standard self-concordant functions as a special case. Next, we establish several properties and key estimates of this function class, which can be used to design numerical methods. Then, we apply this theory to develop several Newton-type methods for solving a class of smooth convex optimization problems involving the generalized self- concordant functions. We provide an explicit step-size for the damped-step Newton-type scheme which can guarantee a global convergence without performing any globalization strategy. We also prove a local quadratic convergence of this method and its full-step variant without requiring the Lipschitz continuity of the objective Hessian. Then, we extend our result to develop proximal Newton-type methods for a class of composite convex minimization problems involving generalized self-concordant functions. We also achieve both global and local convergence without additional assumption. Finally, we verify our theoretical results via several numerical examples, and compare them with existing methods.|['Tianxiao Sun', 'Quoc Tran-Dinh']|['math.OC', 'stat.ML']
2017-03-16T23:25:03Z|2017-03-13T15:43:00Z|http://arxiv.org/abs/1703.04455v1|http://arxiv.org/pdf/1703.04455v1|Multivariate Gaussian and Student$-t$ Process Regression for   Multi-output Prediction|Gaussian process for vector-valued function model has been shown to be a useful method for multi-output prediction. The existing method for this model is to re-formulate the matrix-variate Gaussian distribution as a multivariate normal distribution. Although it is effective and convenient in many cases, re-formulation is not always workable and difficult to extend because not all matrix-variate distributions can be transformed to related multivariate distributions, such as the case for matrix-variate Student$-t$ distribution. In this paper, we propose a multivariate Gaussian process regression (MV-GPR) as well as multivariate Student$-t$ process regression (MV-TPR) for multi-output prediction, where the model settings, derivations, and computations are all performed in matrix form directly, rather than vectorizing the matrices involved. Compared with independent Gaussian process regression and Student$-t$ process regression models, both MV-GPR and MV-TPR significantly have the outstanding performances in the simulated examples and the corresponding Buy\&Sell strategies can be also shown profitable in stock market investment. In particular, MV-TPR has a distinct investment in NetEase among three Chinese stocks from NASDAQ. From the view of industrial sector, MV-GPR has considerable performances in Industrials, Consumer Goods, Consumer Services, and Financials sectors while MV-TPR can gain maximum profit in Health Care sector.|['Zexun Chen', 'Bo Wang', 'Alexander N. Gorban']|['stat.ML']
2017-03-16T23:25:03Z|2017-03-13T13:45:13Z|http://arxiv.org/abs/1703.04389v1|http://arxiv.org/pdf/1703.04389v1|Bayesian Optimization with Gradients|In recent years, Bayesian optimization has proven successful for global optimization of expensive-to-evaluate multimodal objective functions. However, unlike most optimization methods, Bayesian optimization typically does not use derivative information. In this paper we show how Bayesian optimization can exploit derivative information to decrease the number of objective function evaluations required for good performance. In particular, we develop a novel Bayesian optimization algorithm, the derivative-enabled knowledge-gradient (dKG), for which we show one-step Bayes-optimality, asymptotic consistency, and greater one-step value of information than is possible in the derivative-free setting. Our procedure accommodates noisy and incomplete derivative information, and comes in both sequential and batch forms. We show dKG provides state-of-the-art performance compared to a wide range of optimization procedures with and without gradients, on benchmarks including logistic regression, kernel learning, and k-nearest neighbors.|['Jian Wu', 'Matthias Poloczek', 'Andrew Gordon Wilson', 'Peter I. Frazier']|['stat.ML', 'cs.AI', 'cs.LG', 'math.OC']
2017-03-16T23:25:03Z|2017-03-13T13:27:56Z|http://arxiv.org/abs/1703.04379v1|http://arxiv.org/pdf/1703.04379v1|Langevin Dynamics with Continuous Tempering for High-dimensional   Non-convex Optimization|"Minimizing non-convex and high-dimensional objective functions are challenging, especially when training modern deep neural networks. In this paper, a novel approach is proposed which divides the training process into two consecutive phases to obtain better generalization performance: Bayesian sampling and stochastic optimization. The first phase is to explore the energy landscape and to capture the ""fat"" modes; and the second one is to fine-tune the parameter learned from the first phase. In the Bayesian learning phase, we apply continuous tempering and stochastic approximation into the Langevin dynamics to create an efficient and effective sampler, in which the temperature is adjusted automatically according to the designed ""temperature dynamics"". These strategies can overcome the challenge of early trapping into bad local minima and have achieved remarkable improvements in various types of neural networks as shown in our theoretical analysis and empirical experiments."|['Nanyang Ye', 'Zhanxing Zhu', 'Rafal K. Mantiuk']|['cs.LG', 'stat.ML']
2017-03-16T23:25:03Z|2017-03-13T11:19:52Z|http://arxiv.org/abs/1703.04335v1|http://arxiv.org/pdf/1703.04335v1|Practical Bayesian Optimization for Variable Cost Objectives|We propose a novel Bayesian Optimization approach for black-box functions with an environmental variable whose value determines the tradeoff between evaluation cost and the fidelity of the evaluations. Further, we use a novel approach to sampling support points, allowing faster construction of the acquisition function. This allows us to achieve optimization with lower overheads than previous approaches and is implemented for a more general class of problem. We show this approach to be effective on synthetic and real world benchmark problems.|['Mark McLeod', 'Michael A. Osborne', 'Stephen J. Roberts']|['stat.ML']
2017-03-16T23:25:03Z|2017-03-13T11:19:28Z|http://arxiv.org/abs/1703.04334v1|http://arxiv.org/pdf/1703.04334v1|Probabilistic Matching: Causal Inference under Measurement Errors|The abundance of data produced daily from large variety of sources has boosted the need of novel approaches on causal inference analysis from observational data. Observational data often contain noisy or missing entries. Moreover, causal inference studies may require unobserved high-level information which needs to be inferred from other observed attributes. In such cases, inaccuracies of the applied inference methods will result in noisy outputs. In this study, we propose a novel approach for causal inference when one or more key variables are noisy. Our method utilizes the knowledge about the uncertainty of the real values of key variables in order to reduce the bias induced by noisy measurements. We evaluate our approach in comparison with existing methods both on simulated and real scenarios and we demonstrate that our method reduces the bias and avoids false causal inference conclusions in most cases.|['Fani Tsapeli', 'Peter Tino', 'Mirco Musolesi']|['stat.ME', 'stat.CO', 'stat.ML']
2017-03-16T23:25:03Z|2017-03-13T00:02:48Z|http://arxiv.org/abs/1703.04200v1|http://arxiv.org/pdf/1703.04200v1|Improved multitask learning through synaptic intelligence|Deep learning has led to remarkable advances when applied to problems where the data distribution does not change over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, and solve a diversity of tasks simultaneously. Furthermore, synapses in biological neurons are not simply real-valued scalars, but possess complex molecular machinery enabling non-trivial learning dynamics. In this study, we take a first step toward bringing this biological complexity into artificial neural networks. We introduce a model of intelligent synapses that accumulate task relevant information over time, and exploit this information to efficiently consolidate memories of old tasks to protect them from being overwritten as new tasks are learned. We apply our framework to learning sequences of related classification problems, and show that it dramatically reduces catastrophic forgetting while maintaining computational efficiency.|['Friedemann Zenke', 'Ben Poole', 'Surya Ganguli']|['cs.LG', 'q-bio.NC', 'stat.ML']
2017-03-16T23:25:08Z|2017-03-12T17:29:11Z|http://arxiv.org/abs/1703.04145v1|http://arxiv.org/pdf/1703.04145v1|Robustness from structure: Inference with hierarchical spiking networks   on analog neuromorphic hardware|How spiking networks are able to perform probabilistic inference is an intriguing question, not only for understanding information processing in the brain, but also for transferring these computational principles to neuromorphic silicon circuits. A number of computationally powerful spiking network models have been proposed, but most of them have only been tested, under ideal conditions, in software simulations. Any implementation in an analog, physical system, be it in vivo or in silico, will generally lead to distorted dynamics due to the physical properties of the underlying substrate. In this paper, we discuss several such distortive effects that are difficult or impossible to remove by classical calibration routines or parameter training. We then argue that hierarchical networks of leaky integrate-and-fire neurons can offer the required robustness for physical implementation and demonstrate this with both software simulations and emulation on an accelerated analog neuromorphic device.|['Mihai A. Petrovici', 'Anna Schroeder', 'Oliver Breitwieser', 'Andreas Grübl', 'Johannes Schemmel', 'Karlheinz Meier']|['q-bio.NC', 'cs.NE', 'stat.ML']
2017-03-16T23:25:08Z|2017-03-12T16:29:44Z|http://arxiv.org/abs/1703.04140v1|http://arxiv.org/pdf/1703.04140v1|Multiscale Hierarchical Convolutional Networks|Deep neural network algorithms are difficult to analyze because they lack structure allowing to understand the properties of underlying transforms and invariants. Multiscale hierarchical convolutional networks are structured deep convolutional networks where layers are indexed by progressively higher dimensional attributes, which are learned from training data. Each new layer is computed with multidimensional convolutions along spatial and attribute variables. We introduce an efficient implementation of such networks where the dimensionality is progressively reduced by averaging intermediate layers along attribute indices. Hierarchical networks are tested on CIFAR image data bases where they obtain comparable precisions to state of the art networks, with much fewer parameters. We study some properties of the attributes learned from these databases.|['Jörn-Henrik Jacobsen', 'Edouard Oyallon', 'Stéphane Mallat', 'Arnold W. M. Smeulders']|['cs.LG', 'stat.ML']
2017-03-16T23:25:08Z|2017-03-12T08:18:11Z|http://arxiv.org/abs/1703.04082v1|http://arxiv.org/pdf/1703.04082v1|Sequential Local Learning for Latent Graphical Models|Learning parameters of latent graphical models (GM) is inherently much harder than that of no-latent ones since the latent variables make the corresponding log-likelihood non-concave. Nevertheless, expectation-maximization schemes are popularly used in practice, but they are typically stuck in local optima. In the recent years, the method of moments have provided a refreshing angle for resolving the non-convex issue, but it is applicable to a quite limited class of latent GMs. In this paper, we aim for enhancing its power via enlarging such a class of latent GMs. To this end, we introduce two novel concepts, coined marginalization and conditioning, which can reduce the problem of learning a larger GM to that of a smaller one. More importantly, they lead to a sequential learning framework that repeatedly increases the learning portion of given latent GM, and thus covers a significantly broader and more complicated class of loopy latent GMs which include convolutional and random regular models.|['Sejun Park', 'Eunho Yang', 'Jinwoo Shin']|['cs.LG', 'stat.ML']
2017-03-16T23:25:08Z|2017-03-12T08:11:29Z|http://arxiv.org/abs/1703.04081v1|http://arxiv.org/pdf/1703.04081v1|Feature overwriting as a finite mixture process: Evidence from   comprehension data|"The ungrammatical sentence ""The key to the cabinets are on the table"" is known to lead to an illusion of grammaticality. As discussed in the meta-analysis by Jaeger et al., 2017, faster reading times are observed at the verb are in the agreement-attraction sentence above compared to the equally ungrammatical sentence ""The key to the cabinet are on the table"". One explanation for this facilitation effect is the feature percolation account: the plural feature on cabinets percolates up to the head noun key, leading to the illusion. An alternative account is in terms of cue-based retrieval (Lewis & Vasishth, 2005), which assumes that the non-subject noun cabinets is misretrieved due to a partial feature-match when a dependency completion process at the auxiliary initiates a memory access for a subject with plural marking. We present evidence for yet another explanation for the observed facilitation. Because the second sentence has two nouns with identical number, it is possible that these are, in some proportion of trials, more difficult to keep distinct, leading to slower reading times at the verb in the first sentence above; this is the feature overwriting account of Nairne, 1990. We show that the feature overwriting proposal can be implemented as a finite mixture process. We reanalysed ten published data-sets, fitting hierarchical Bayesian mixture models to these data assuming a two-mixture distribution. We show that in nine out of the ten studies, a mixture distribution corresponding to feature overwriting furnishes a superior fit over both the feature percolation and the cue-based retrieval accounts."|['Shravan Vasishth', 'Lena A. Jaeger', 'Bruno Nicenboim']|['stat.ML', 'cs.CL', 'stat.AP']
2017-03-16T23:25:08Z|2017-03-12T07:19:55Z|http://arxiv.org/abs/1703.04078v1|http://arxiv.org/abs/1703.04078v1|Prostate Cancer Diagnosis using Deep Learning with 3D Multiparametric   MRI|A novel deep learning architecture (XmasNet) based on convolutional neural networks was developed for the classification of prostate cancer lesions, using the 3D multiparametric MRI data provided by the PROSTATEx challenge. End-to-end training was performed for XmasNet, with data augmentation done through 3D rotation and slicing, in order to incorporate the 3D information of the lesion. XmasNet outperformed traditional machine learning models based on engineered features, for both train and test data. For the test data, XmasNet outperformed 69 methods from 33 participating groups and achieved the second highest AUC (0.84) in the PROSTATEx challenge. This study shows the great potential of deep learning for cancer imaging.|['Saifeng Liu', 'Huaixiu Zheng', 'Yesu Feng', 'Wei Li']|['cs.CV', 'stat.ML']
2017-03-16T23:25:08Z|2017-03-12T00:15:32Z|http://arxiv.org/abs/1703.04046v1|http://arxiv.org/pdf/1703.04046v1|DeepSleepNet: a Model for Automatic Sleep Stage Scoring based on Raw   Single-Channel EEG|Objective: The present study proposes a deep learn- ing model, named DeepSleepNet, for automatic sleep stage scoring based on raw single-channel EEG, and a two-step training algorithm used to effectively train such model. Methods: Most of the existing methods rely on hand-engineered features which require prior knowledge about sleep stage scoring. Only a few of them encode the temporal information such as stage transition rules, which is important to correctly identify the next possible sleep stages, into the extracted features. In the proposed model, we utilize Convolutional Neural Networks (CNNs) to extract time-invariant features, and bidirectional Long Short- Term Memory (bidirectional-LSTM) to learn transition rules among sleep stages from EEG epochs. We implement a two-step training algorithm that pre-trains the model with oversampled dataset to alleviate class-imbalance problems and fine-tunes the model with sequences of EEG epochs to encode the temporal information into the model. Results: We applied our model to the F4-EOG (Left) channel from a set of 62 subjects in an open- access database, containing 58600 EEG epochs (~488 hours). The results demonstrated that our model scored the EEG epochs with the accuracy of 86.2% and the macro F1-score of 81.7. Conclusion: Without utilizing any hand-engineered features, our model can achieve a similar sleep stage scoring performance with the highest macro F1-score compared to the state-of-the- art methods. Significance: This study proposes a deep learning model that can automatically learn features from raw single- channel EEG, and accurately score EEG epochs as good as the state-of-the-art hand-engineering methods.|['Akara Supratak', 'Hao Dong', 'Chao Wu', 'Yike Guo']|['stat.ML']
2017-03-16T23:25:08Z|2017-03-11T20:07:06Z|http://arxiv.org/abs/1703.04025v1|http://arxiv.org/pdf/1703.04025v1|Learning Large-Scale Bayesian Networks with the sparsebn Package|Learning graphical models from data is an important problem with wide applications, ranging from genomics to the social sciences. Nowadays datasets typically have upwards of thousands---sometimes tens or hundreds of thousands---of variables and far fewer samples. To meet this challenge, we develop a new R package called sparsebn for learning the structure of large, sparse graphical models with a focus on Bayesian networks. While there are many existing packages for this task within the R ecosystem, this package focuses on the unique setting of learning large networks from high-dimensional data, possibly with interventions. As such, the methods provided place a premium on scalability and consistency in a high-dimensional setting. Furthermore, in the presence of interventions, the methods implemented here achieve the goal of learning a causal network from data. The sparsebn package is open-source and available on CRAN.|['Bryon Aragam', 'Jiaying Gu', 'Qing Zhou']|['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']
2017-03-16T23:25:08Z|2017-03-11T01:18:14Z|http://arxiv.org/abs/1703.03888v1|http://arxiv.org/pdf/1703.03888v1|Segmentation of skin lesions based on fuzzy classification of pixels and   histogram thresholding|This paper proposes an innovative method for segmentation of skin lesions in dermoscopy images developed by the authors, based on fuzzy classification of pixels and histogram thresholding.|['Jose Luis Garcia-Arroyo', 'Begonya Garcia-Zapirain']|['cs.CV', 'cs.AI', 'stat.ML']
2017-03-16T23:25:08Z|2017-03-10T23:26:33Z|http://arxiv.org/abs/1703.03869v1|http://arxiv.org/pdf/1703.03869v1|Deep Learning in Customer Churn Prediction: Unsupervised Feature   Learning on Abstract Company Independent Feature Vectors|As companies increase their efforts in retaining customers, being able to predict accurately ahead of time, whether a customer will churn in the foreseeable future is an extremely powerful tool for any marketing team. The paper describes in depth the application of Deep Learning in the problem of churn prediction. Using abstract feature vectors, that can generated on any subscription based company's user event logs, the paper proves that through the use of the intrinsic property of Deep Neural Networks (learning secondary features in an unsupervised manner), the complete pipeline can be applied to any subscription based company with extremely good churn predictive performance. Furthermore the research documented in the paper was performed for Framed Data (a company that sells churn prediction as a service for other companies) in conjunction with the Data Science Institute at Lancaster University, UK. This paper is the intellectual property of Framed Data.|['Philip Spanoudes', 'Thomson Nguyen']|['cs.LG', 'stat.ML']
2017-03-16T23:25:08Z|2017-03-10T23:02:19Z|http://arxiv.org/abs/1703.03864v1|http://arxiv.org/pdf/1703.03864v1|Evolution Strategies as a Scalable Alternative to Reinforcement Learning|We explore the use of Evolution Strategies, a class of black box optimization algorithms, as an alternative to popular RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using hundreds to thousands of parallel workers, ES can solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training time. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.|['Tim Salimans', 'Jonathan Ho', 'Xi Chen', 'Ilya Sutskever']|['stat.ML', 'cs.AI', 'cs.LG', 'cs.NE']
2017-03-16T23:25:12Z|2017-03-10T22:47:31Z|http://arxiv.org/abs/1703.03863v1|http://arxiv.org/pdf/1703.03863v1|Tuning Over-Relaxed ADMM|The framework of Integral Quadratic Constraints (IQC) reduces the computation of upper bounds on the convergence rate of several optimization algorithms to a semi-definite program (SDP). In the case of over-relaxed Alternating Direction Method of Multipliers (ADMM), an explicit and closed form solution to this SDP was derived in our recent work [1]. The purpose of this paper is twofold. First, we summarize these results. Second, we explore one of its consequences which allows us to obtain general and simple formulas for optimal parameter selection. These results are valid for arbitrary strongly convex objective functions.|['Guilherme França', 'José Bento']|['stat.ML', 'math.DS', 'math.OC']
2017-03-16T23:25:12Z|2017-03-10T22:46:09Z|http://arxiv.org/abs/1703.03862v1|http://arxiv.org/pdf/1703.03862v1|Joint Embedding of Graphs|Feature extraction and dimension reduction for networks is critical in a wide variety of domains. Efficiently and accurately learning features for multiple graphs has important applications in statistical inference on graphs. We propose a method to jointly embed multiple undirected graphs. Given a set of graphs, the joint embedding method identifies a linear subspace spanned by rank one symmetric matrices and projects adjacency matrices of graphs into this subspace. The projection coefficients can be treated as features of the graphs. We also propose a random graph model which generalizes classical random graph model and can be used to model multiple graphs. We show through theory and numerical experiments that under the model, the joint embedding method produces estimates of parameters with small errors. Via simulation experiments, we demonstrate that the joint embedding method produces features which lead to state of the art performance in classifying graphs. Applying the joint embedding method to human brain graphs, we find it extract interpretable features that can be used to predict individual composite creativity index.|['Shangsi Wang', 'Joshua T. Vogelstein', 'Carey E. Priebe']|['stat.AP', 'cs.LG', 'stat.ML']
2017-03-16T23:25:12Z|2017-03-10T22:25:56Z|http://arxiv.org/abs/1703.03859v1|http://arxiv.org/abs/1703.03859v1|Markov Chain Lifting and Distributed ADMM|The time to converge to the steady state of a finite Markov chain can be greatly reduced by a lifting operation, which creates a new Markov chain on an expanded state space. For a class of quadratic objectives, we show an analogous behavior where a distributed ADMM algorithm can be seen as a lifting of Gradient Descent algorithm. This provides a deep insight for its faster convergence rate under optimal parameter tuning. We conjecture that this gain is always present, as opposed to the lifting of a Markov chain which sometimes only provides a marginal speedup.|['Guilherme França', 'José Bento']|['stat.ML', 'cs.DS', 'cs.IT', 'cs.LG', 'math.IT', 'math.OC']
2017-03-16T23:25:12Z|2017-03-10T18:01:20Z|http://arxiv.org/abs/1703.05298v1|http://arxiv.org/pdf/1703.05298v1|Neural Networks for Beginners. A fast implemention in Matlab, Torch,   TensorFlow|This report provides an introduction to some Machine Learning tools within the most common development environments. It mainly focuses on practical problems, skipping any theoretical introduction. It is oriented to both students trying to approach Machine Learning and experts looking for new frameworks.|['Francesco Giannini', 'Vincenzo Laveglia', 'Alessandro Rossi', 'Dario Zanca', 'Andrea Zugarini']|['cs.LG', 'cs.CV', 'cs.MS', 'stat.ML']
2017-03-16T23:25:12Z|2017-03-10T15:35:32Z|http://arxiv.org/abs/1703.03717v1|http://arxiv.org/pdf/1703.03717v1|Right for the Right Reasons: Training Differentiable Models by   Constraining their Explanations|Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.|['Andrew Slavin Ross', 'Michael C. Hughes', 'Finale Doshi-Velez']|['cs.LG', 'stat.ML']
2017-03-16T23:25:12Z|2017-03-10T09:34:37Z|http://arxiv.org/abs/1703.03596v1|http://arxiv.org/pdf/1703.03596v1|High SNR Consistent Compressive Sensing|High signal to noise ratio (SNR) consistency of model selection criteria in linear regression models has attracted a lot of attention recently. However, most of the existing literature on high SNR consistency deals with model order selection. Further, the limited literature available on the high SNR consistency of subset selection procedures (SSPs) is applicable to linear regression with full rank measurement matrices only. Hence, the performance of SSPs used in underdetermined linear models (a.k.a compressive sensing (CS) algorithms) at high SNR is largely unknown. This paper fills this gap by deriving necessary and sufficient conditions for the high SNR consistency of popular CS algorithms like $l_0$-minimization, basis pursuit de-noising or LASSO, orthogonal matching pursuit and Dantzig selector. Necessary conditions analytically establish the high SNR inconsistency of CS algorithms when used with the tuning parameters discussed in literature. Novel tuning parameters with SNR adaptations are developed using the sufficient conditions and the choice of SNR adaptations are discussed analytically using convergence rate analysis. CS algorithms with the proposed tuning parameters are numerically shown to be high SNR consistent and outperform existing tuning parameters in the moderate to high SNR regime.|['Sreejith Kallummil', 'Sheetal Kalyani']|['stat.ML', 'cs.IT', 'math.IT']
2017-03-16T23:25:12Z|2017-03-10T01:42:39Z|http://arxiv.org/abs/1703.03507v1|http://arxiv.org/pdf/1703.03507v1|Decorrelated Jet Substructure Tagging using Adversarial Neural Networks|We describe a strategy for constructing a neural network jet substructure tagger which powerfully discriminates boosted decay signals while remaining largely uncorrelated with the jet mass. This reduces the impact of systematic uncertainties in background modeling while enhancing signal purity, resulting in improved discovery significance relative to existing taggers. The network is trained using an adversarial strategy, resulting in a tagger that learns to balance classification accuracy with decorrelation. As a benchmark scenario, we consider the case where large-radius jets originating from a boosted resonance decay are discriminated from a background of nonresonant quark and gluon jets. We show that in the presence of systematic uncertainties on the background rate, our adversarially-trained, decorrelated tagger considerably outperforms a conventionally trained neural network, despite having a slightly worse signal-background separation power. We generalize the adversarial training technique to include a parametric dependence on the signal hypothesis, training a single network that provides optimized, interpolatable decorrelated jet tagging across a continuous range of hypothetical resonance masses, after training on discrete choices of the signal mass.|['Chase Shimmin', 'Peter Sadowski', 'Pierre Baldi', 'Edison Weik', 'Daniel Whiteson', 'Edward Goul', 'Andreas Søgaard']|['hep-ex', 'physics.data-an', 'stat.ML']
2017-03-16T23:25:12Z|2017-03-10T01:28:15Z|http://arxiv.org/abs/1703.03503v1|http://arxiv.org/pdf/1703.03503v1|Density Level Set Estimation on Manifolds with DBSCAN|DBSCAN is one of the most popular clustering algorithms amongst practitioners, but it has received comparatively less theoretical treatment.   We show that given $\lambda > 0$ and its parameters set under appropriate ranges, DBSCAN estimates the connected components of the $\lambda$-density level set (i.e. $\{ x : f(x) \ge \lambda \}$ where $f$ is the density).   We characterize the regularity of the level set boundaries using parameter $\beta > 0$ and analyze the estimation error under the Hausdorff metric. When the data lies in $\mathbb{R}^D$ we obtain an estimation rate of $O(n^{-1/(2\beta + D)})$, which matches known lower bounds up to logarithmic factors. When the data lies on an embedded unknown $d$-dimensional manifold in $\mathbb{R}^D$, then we obtain an estimation rate of $O(n^{-1/(2\beta + d\cdot \max\{1, \beta \})})$.   Finally, we provide adaptive parameter tuning in order to attain these rates with no a priori knowledge of the intrinsic dimension, density, or $\beta$.|['Heinrich Jiang']|['stat.ML']
2017-03-16T23:25:12Z|2017-03-09T20:43:40Z|http://arxiv.org/abs/1703.03457v1|http://arxiv.org/pdf/1703.03457v1|Parallel Markov Chain Monte Carlo for the Indian Buffet Process|Indian Buffet Process based models are an elegant way for discovering underlying features within a data set, but inference in such models can be slow. Inferring underlying features using Markov chain Monte Carlo either relies on an uncollapsed representation, which leads to poor mixing, or on a collapsed representation, which leads to a quadratic increase in computational complexity. Existing attempts at distributing inference have introduced additional approximation within the inference procedure. In this paper we present a novel algorithm to perform asymptotically exact parallel Markov chain Monte Carlo inference for Indian Buffet Process models. We take advantage of the fact that the features are conditionally independent under the beta-Bernoulli process. Because of this conditional independence, we can partition the features into two parts: one part containing only the finitely many instantiated features and the other part containing the infinite tail of uninstantiated features. For the finite partition, parallel inference is simple given the instantiation of features. But for the infinite tail, performing uncollapsed MCMC leads to poor mixing and hence we collapse out the features. The resulting hybrid sampler, while being parallel, produces samples asymptotically from the true posterior.|['Michael M. Zhang', 'Avinava Dubey', 'Sinead A. Williamson']|['stat.ML']
2017-03-16T23:25:12Z|2017-03-09T20:22:27Z|http://arxiv.org/abs/1703.03454v1|http://arxiv.org/pdf/1703.03454v1|Sample Efficient Feature Selection for Factored MDPs|In reinforcement learning, the state of the real world is often represented by feature vectors. However, not all of the features may be pertinent for solving the current task. We propose Feature Selection Explore and Exploit (FS-EE), an algorithm that automatically selects the necessary features while learning a Factored Markov Decision Process, and prove that under mild assumptions, its sample complexity scales with the in-degree of the dynamics of just the necessary features, rather than the in-degree of all features. This can result in a much better sample complexity when the in-degree of the necessary features is smaller than the in-degree of all features.|['Zhaohan Daniel Guo', 'Emma Brunskill']|['cs.LG', 'stat.ML']
2017-03-16T23:25:17Z|2017-03-15T10:19:13Z|http://arxiv.org/abs/1703.03373v2|http://arxiv.org/pdf/1703.03373v2|mlrMBO: A Modular Framework for Model-Based Optimization of Expensive   Black-Box Functions|We present mlrMBO, a flexible and comprehensive R toolbox for model-based optimization (MBO), also known as Bayesian optimization, which addresses the problem of expensive black-box optimization by approximating the given objective function through a surrogate regression model. It is designed for both single- and multi-objective optimization with mixed continuous, categorical and conditional parameters. Additional features include multi-point batch proposal, parallelization, visualization, logging and error-handling. mlrMBO is implemented in a modular fashion, such that single components can be easily replaced or adapted by the user for specific use cases, e.g., any regression learner from the mlr toolbox for machine learning can be used, and infill criteria and infill optimizers are easily exchangeable. We empirically demonstrate that mlrMBO provides state-of-the-art performance by comparing it on different benchmark scenarios against a wide range of other optimizers, including DiceOptim, rBayesianOptimization, SPOT, SMAC, Spearmint, and Hyperopt.|['Bernd Bischl', 'Jakob Richter', 'Jakob Bossek', 'Daniel Horn', 'Janek Thomas', 'Michel Lang']|['stat.ML']
2017-03-16T23:25:17Z|2017-03-09T17:17:39Z|http://arxiv.org/abs/1703.03352v1|http://arxiv.org/pdf/1703.03352v1|A log-linear time algorithm for constrained changepoint detection|Changepoint detection is a central problem in time series and genomic data. For some applications, it is natural to impose constraints on the directions of changes. One example is ChIP-seq data, for which adding an up-down constraint improves peak detection accuracy, but makes the optimization problem more complicated. We show how a recently proposed functional pruning technique can be adapted to solve such constrained changepoint detection problems. This leads to a new algorithm which can solve problems with arbitrary affine constraints on adjacent segment means, and which has empirical time complexity that is log-linear in the amount of data. This algorithm achieves state-of-the-art accuracy in a benchmark of several genomic data sets, and is orders of magnitude faster than existing algorithms that have similar accuracy. Our implementation is available as the PeakSegPDPA function in the coseg R package, https://github.com/tdhock/coseg|['Toby Dylan Hocking', 'Guillem Rigaill', 'Paul Fearnhead', 'Guillaume Bourque']|['stat.CO', 'q-bio.GN', 'stat.ML']
2017-03-16T23:25:17Z|2017-03-09T17:06:21Z|http://arxiv.org/abs/1703.03722v1|http://arxiv.org/pdf/1703.03722v1|Recovery of Sparse and Low Rank Components of Matrices Using Iterative   Method with Adaptive Thresholding|In this letter, we propose an algorithm for recovery of sparse and low rank components of matrices using an iterative method with adaptive thresholding. In each iteration, the low rank and sparse components are obtained using a thresholding operator. This algorithm is fast and can be implemented easily. We compare it with one of the most common fast methods in which the rank and sparsity are approximated by $\ell_1$ norm. We also apply it to some real applications where the noise is not so sparse. The simulation results show that it has a suitable performance with low run-time.|['Nematollah Zarmehi', 'Farokh Marvasti']|['cs.NA', 'stat.ML']
2017-03-16T23:25:17Z|2017-03-09T10:24:27Z|http://arxiv.org/abs/1703.03216v1|http://arxiv.org/pdf/1703.03216v1|Robust Density Ratio Estimation: Trimming the Likelihood Ratio|Density ratio estimation has become a versatile tool in machine learning community recently. However, due to its unbounded nature, density ratio estimation is vulnerable to corrupted data points, which misleads the estimated ratio toward infinity. In this paper, we present a robust estimator which automatically identifies and trims outliers according to the log likelihood ratio values. Such an estimator has a convex formulation and can be efficiently solved. We analyze the \ell_{2} parameter estimation error of such an estimator under two scenarios motivated by real-world problems. Numerical analysis was conducted to verify the effectiveness of such an estimator.|['Song Liu', 'Akiko Takeda', 'Taiji Suzuki', 'Kenji Fukumizu']|['stat.ML']
2017-03-16T23:25:17Z|2017-03-09T10:11:03Z|http://arxiv.org/abs/1703.03208v1|http://arxiv.org/pdf/1703.03208v1|Compressed Sensing using Generative Models|The goal of compressed sensing is to estimate a vector from an underdetermined system of noisy linear measurements, by making use of prior knowledge on the structure of vectors in the relevant domain. For almost all results in this literature, the structure is represented by sparsity in a well-chosen basis. We show how to achieve guarantees similar to standard compressed sensing but without employing sparsity at all. Instead, we suppose that vectors lie near the range of a generative model $G: \mathbb{R}^k \to \mathbb{R}^n$. Our main theorem is that, if $G$ is $L$-Lipschitz, then roughly $O(k \log L)$ random Gaussian measurements suffice for an $\ell_2/\ell_2$ recovery guarantee. We demonstrate our results using generative models from published variational autoencoder and generative adversarial networks. Our method can use $5$-$10$x fewer measurements than Lasso for the same accuracy.|['Ashish Bora', 'Ajil Jalal', 'Eric Price', 'Alexandros G. Dimakis']|['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']
2017-03-16T23:25:17Z|2017-03-09T07:40:53Z|http://arxiv.org/abs/1703.03167v1|http://arxiv.org/pdf/1703.03167v1|Cross-validation|This text is a survey on cross-validation. We define all classical cross-validation procedures, and we study their properties for two different goals: estimating the risk of a given estimator, and selecting the best estimator among a given family. For the risk estimation problem, we compute the bias (which can also be corrected) and the variance of cross-validation methods. For estimator selection, we first provide a first-order analysis (based on expectations). Then, we explain how to take into account second-order terms (from variance computations, and by taking into account the usefulness of overpenalization). This allows, in the end, to provide some guidelines for choosing the best cross-validation method for a given learning problem.|['Sylvain Arlot']|['math.ST', 'stat.ML', 'stat.TH']
2017-03-16T23:25:17Z|2017-03-08T21:44:12Z|http://arxiv.org/abs/1703.03044v1|http://arxiv.org/pdf/1703.03044v1|A GAMP Based Low Complexity Sparse Bayesian Learning Algorithm|In this paper, we present an algorithm for the sparse signal recovery problem that incorporates damped Gaussian generalized approximate message passing (GGAMP) into Expectation-Maximization (EM)-based sparse Bayesian learning (SBL). In particular, GGAMP is used to implement the E-step in SBL in place of matrix inversion, leveraging the fact that GGAMP is guaranteed to converge with appropriate damping. The resulting GGAMP-SBL algorithm is much more robust to arbitrary measurement matrix $\boldsymbol{A}$ than the standard damped GAMP algorithm while being much lower complexity than the standard SBL algorithm. We then extend the approach from the single measurement vector (SMV) case to the temporally correlated multiple measurement vector (MMV) case, leading to the GGAMP-TSBL algorithm. We verify the robustness and computational advantages of the proposed algorithms through numerical experiments.|['Maher Al-Shoukairi', 'Philip Schniter', 'Bhaskar D. Rao']|['cs.LG', 'stat.ML']
2017-03-16T23:25:17Z|2017-03-08T21:29:52Z|http://arxiv.org/abs/1703.03038v1|http://arxiv.org/abs/1703.03038v1|Parallel Implementation of Efficient Search Schemes for the Inference of   Cancer Progression Models|The emergence and development of cancer is a consequence of the accumulation over time of genomic mutations involving a specific set of genes, which provides the cancer clones with a functional selective advantage. In this work, we model the order of accumulation of such mutations during the progression, which eventually leads to the disease, by means of probabilistic graphic models, i.e., Bayesian Networks (BNs). We investigate how to perform the task of learning the structure of such BNs, according to experimental evidence, adopting a global optimization meta-heuristics. In particular, in this work we rely on Genetic Algorithms, and to strongly reduce the execution time of the inference -- which can also involve multiple repetitions to collect statistically significant assessments of the data -- we distribute the calculations using both multi-threading and a multi-node architecture. The results show that our approach is characterized by good accuracy and specificity; we also demonstrate its feasibility, thanks to a 84x reduction of the overall execution time with respect to a traditional sequential implementation.|['Daniele Ramazzotti', 'Marco S. Nobile', 'Paolo Cazzaniga', 'Giancarlo Mauri', 'Marco Antoniotti']|['cs.LG', 'stat.ML']
2017-03-16T23:25:17Z|2017-03-10T11:09:10Z|http://arxiv.org/abs/1703.03020v2|http://arxiv.org/pdf/1703.03020v2|Spectral Graph Convolutions for Population-based Disease Prediction|Exploiting the wealth of imaging and non-imaging information for disease prediction tasks requires models capable of representing, at the same time, individual features as well as data associations between subjects from potentially large populations. Graphs provide a natural framework for such tasks, yet previous graph-based approaches focus on pairwise similarities without modelling the subjects' individual characteristics and features. On the other hand, relying solely on subject-specific imaging feature vectors fails to model the interaction and similarity between subjects, which can reduce performance. In this paper, we introduce the novel concept of Graph Convolutional Networks (GCN) for brain analysis in populations, combining imaging and non-imaging data. We represent populations as a sparse graph where its vertices are associated with image-based feature vectors and the edges encode phenotypic information. This structure was used to train a GCN model on partially labelled graphs, aiming to infer the classes of unlabelled nodes from the node features and pairwise associations between subjects. We demonstrate the potential of the method on the challenging ADNI and ABIDE databases, as a proof of concept of the benefit from integrating contextual information in classification tasks. This has a clear impact on the quality of the predictions, leading to 69.5% accuracy for ABIDE (outperforming the current state of the art of 66.8%) and 77% for ADNI for prediction of MCI conversion, significantly outperforming standard linear classifiers where only individual features are considered.|['Sarah Parisot', 'Sofia Ira Ktena', 'Enzo Ferrante', 'Matthew Lee', 'Ricardo Guerrerro Moreno', 'Ben Glocker', 'Daniel Rueckert']|['stat.ML', 'cs.LG']
2017-03-16T23:25:17Z|2017-03-08T18:58:20Z|http://arxiv.org/abs/1703.02965v1|http://arxiv.org/pdf/1703.02965v1|Unsupervised Ensemble Regression|Consider a regression problem where there is no labeled data and the only observations are the predictions $f_i(x_j)$ of $m$ experts $f_{i}$ over many samples $x_j$. With no knowledge on the accuracy of the experts, is it still possible to accurately estimate the unknown responses $y_{j}$? Can one still detect the least or most accurate experts? In this work we propose a framework to study these questions, based on the assumption that the $m$ experts have uncorrelated deviations from the optimal predictor. Assuming the first two moments of the response are known, we develop methods to detect the best and worst regressors, and derive U-PCR, a novel principal components approach for unsupervised ensemble regression. We provide theoretical support for U-PCR and illustrate its improved accuracy over the ensemble mean and median on a variety of regression problems.|['Omer Dror', 'Boaz Nadler', 'Erhan Bilal', 'Yuval Kluger']|['stat.ML', 'cs.LG']
2017-03-16T23:25:21Z|2017-03-08T17:00:21Z|http://arxiv.org/abs/1703.02914v1|http://arxiv.org/pdf/1703.02914v1|Dropout Inference in Bayesian Neural Networks with Alpha-divergences|To obtain uncertainty estimates with real-world Bayesian deep learning models, practical inference approximations are needed. Dropout variational inference (VI) for example has been used for machine vision and medical applications, but VI can severely underestimates model uncertainty. Alpha-divergences are alternative divergences to VI's KL objective, which are able to avoid VI's uncertainty underestimation. But these are hard to use in practice: existing techniques can only use Gaussian approximating distributions, and require existing models to be changed radically, thus are of limited use for practitioners. We propose a re-parametrisation of the alpha-divergence objectives, deriving a simple inference technique which, together with dropout, can be easily implemented with existing models by simply changing the loss of the model. We demonstrate improved uncertainty estimates and accuracy compared to VI in dropout networks. We study our model's epistemic uncertainty far away from the data using adversarial images, showing that these can be distinguished from non-adversarial images by examining our model's uncertainty.|['Yingzhen Li', 'Yarin Gal']|['cs.LG', 'stat.ML']
2017-03-16T23:25:21Z|2017-03-08T16:53:57Z|http://arxiv.org/abs/1703.02910v1|http://arxiv.org/pdf/1703.02910v1|Deep Bayesian Active Learning with Image Data|Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).|['Yarin Gal', 'Riashat Islam', 'Zoubin Ghahramani']|['cs.LG', 'cs.CV', 'stat.ML']
2017-03-16T23:25:21Z|2017-03-08T16:28:17Z|http://arxiv.org/abs/1703.02899v1|http://arxiv.org/pdf/1703.02899v1|Model-Based Policy Search for Automatic Tuning of Multivariate PID   Controllers|PID control architectures are widely used in industrial applications. Despite their low number of open parameters, tuning multiple, coupled PID controllers can become tedious in practice. In this paper, we extend PILCO, a model-based policy search framework, to automatically tune multivariate PID controllers purely based on data observed on an otherwise unknown system. The system's state is extended appropriately to frame the PID policy as a static state feedback policy. This renders PID tuning possible as the solution of a finite horizon optimal control problem without further a priori knowledge. The framework is applied to the task of balancing an inverted pendulum on a seven degree-of-freedom robotic arm, thereby demonstrating its capabilities of fast and data-efficient policy learning, even on complex real world problems.|['Andreas Doerr', 'Duy Nguyen-Tuong', 'Alonso Marco', 'Stefan Schaal', 'Sebastian Trimpe']|['cs.LG', 'cs.RO', 'cs.SY', 'stat.ML']
2017-03-16T23:25:21Z|2017-03-08T13:47:17Z|http://arxiv.org/abs/1703.02834v1|http://arxiv.org/pdf/1703.02834v1|Exact Dimensionality Selection for Bayesian PCA|We present a Bayesian model selection approach to estimate the intrinsic dimensionality of a high-dimensional dataset. To this end, we introduce a novel formulation of the probabilisitic principal component analysis model based on a normal-gamma prior distribution. In this context, we exhibit a closed-form expression of the marginal likelihood which allows to infer an optimal number of components. We also propose a heuristic based on the expected shape of the marginal likelihood curve in order to choose the hyperparameters. In non-asymptotic frameworks, we show on simulated data that this exact dimensionality selection approach is competitive with both Bayesian and frequentist state-of-the-art methods.|['Charles Bouveyron', 'Pierre Latouche', 'Pierre-Alexandre Mattei']|['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']
2017-03-16T23:25:21Z|2017-03-08T12:53:21Z|http://arxiv.org/abs/1703.02819v1|http://arxiv.org/abs/1703.02819v1|Introduction to Formal Concept Analysis and Its Applications in   Information Retrieval and Related Fields|This paper is a tutorial on Formal Concept Analysis (FCA) and its applications. FCA is an applied branch of Lattice Theory, a mathematical discipline which enables formalisation of concepts as basic units of human thinking and analysing data in the object-attribute form. Originated in early 80s, during the last three decades, it became a popular human-centred tool for knowledge representation and data analysis with numerous applications. Since the tutorial was specially prepared for RuSSIR 2014, the covered FCA topics include Information Retrieval with a focus on visualisation aspects, Machine Learning, Data Mining and Knowledge Discovery, Text Mining and several others.|['Dmitry I. Ignatov']|['cs.IR', 'cs.AI', 'cs.CL', 'cs.DM', 'stat.ML', '68P20, 06B99, 68T30', 'H.3.3; G.2; I.2']
2017-03-16T23:25:21Z|2017-03-08T09:26:36Z|http://arxiv.org/abs/1703.02757v1|http://arxiv.org/pdf/1703.02757v1|Byzantine-Tolerant Machine Learning|The growth of data, the need for scalability and the complexity of models used in modern machine learning calls for distributed implementations. Yet, as of today, distributed machine learning frameworks have largely ignored the possibility of arbitrary (i.e., Byzantine) failures. In this paper, we study the robustness to Byzantine failures at the fundamental level of stochastic gradient descent (SGD), the heart of most machine learning algorithms. Assuming a set of $n$ workers, up to $f$ of them being Byzantine, we ask how robust can SGD be, without limiting the dimension, nor the size of the parameter space.   We first show that no gradient descent update rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience property of the update rule capturing the basic requirements to guarantee convergence despite $f$ Byzantine workers. We finally propose Krum, an update rule that satisfies the resilience property aforementioned. For a $d$-dimensional learning problem, the time complexity of Krum is $O(n^2 \cdot (d + \log n))$.|['Peva Blanchard', 'El Mahdi El Mhamdi', 'Rachid Guerraoui', 'Julien Stainer']|['cs.DC', 'cs.LG', 'cs.NE', 'math.OC', 'stat.ML']
2017-03-16T23:25:21Z|2017-03-08T06:22:56Z|http://arxiv.org/abs/1703.02724v1|http://arxiv.org/pdf/1703.02724v1|Guaranteed Tensor PCA with Optimality in Statistics and Computation|Tensors, or high-order arrays, attract much attention in recent research. In this paper, we propose a general framework for tensor principal component analysis (tensor PCA), which focuses on the methodology and theory for extracting the hidden low-rank structure from the high-dimensional tensor data. A unified solution is provided for tensor PCA with considerations in both statistical limits and computational costs. The problem exhibits three different phases according to the signal-noise-ratio (SNR). In particular, with strong SNR, we propose a fast spectral power iteration method that achieves the minimax optimal rate of convergence in estimation; with weak SNR, the information-theoretical lower bound shows that it is impossible to have consistent estimation in general; with moderate SNR, we show that the non-convex maximum likelihood estimation provides optimal solution, but with NP-hard computational cost; moreover, under the hardness hypothesis of hypergraphic planted clique detection, there are no polynomial-time algorithms performing consistently in general. Simulation studies show that the proposed spectral power iteration method have good performance under a variety of settings.|['Anru Zhang', 'Dong Xia']|['math.ST', 'cs.LG', 'stat.ME', 'stat.ML', 'stat.TH']
2017-03-16T23:25:21Z|2017-03-08T06:21:46Z|http://arxiv.org/abs/1703.02723v1|http://arxiv.org/pdf/1703.02723v1|Scalable Greedy Feature Selection via Weak Submodularity|Greedy algorithms are widely used for problems in machine learning such as feature selection and set function optimization. Unfortunately, for large datasets, the running time of even greedy algorithms can be quite high. This is because for each greedy step we need to refit a model or calculate a function using the previously selected choices and the new candidate.   Two algorithms that are faster approximations to the greedy forward selection were introduced recently ([Mirzasoleiman et al. 2013, 2015]). They achieve better performance by exploiting distributed computation and stochastic evaluation respectively. Both algorithms have provable performance guarantees for submodular functions.   In this paper we show that divergent from previously held opinion, submodularity is not required to obtain approximation guarantees for these two algorithms. Specifically, we show that a generalized concept of weak submodularity suffices to give multiplicative approximation guarantees. Our result extends the applicability of these algorithms to a larger class of functions. Furthermore, we show that a bounded submodularity ratio can be used to provide data dependent bounds that can sometimes be tighter also for submodular functions. We empirically validate our work by showing superior performance of fast greedy approximations versus several established baselines on artificial and real datasets.|['Rajiv Khanna', 'Ethan Elenberg', 'Alexandros G. Dimakis', 'Sahand Negahban', 'Joydeep Ghosh']|['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']
2017-03-16T23:25:21Z|2017-03-08T06:20:10Z|http://arxiv.org/abs/1703.02721v1|http://arxiv.org/pdf/1703.02721v1|On Approximation Guarantees for Greedy Low Rank Optimization|We provide new approximation guarantees for greedy low rank matrix estimation under standard assumptions of restricted strong convexity and smoothness. Our novel analysis also uncovers previously unknown connections between the low rank estimation and combinatorial optimization, so much so that our bounds are reminiscent of corresponding approximation bounds in submodular maximization. Additionally, we also provide statistical recovery guarantees. Finally, we present empirical comparison of greedy estimation with established baselines on two important real-world problems.|['Rajiv Khanna', 'Ethan Elenberg', 'Alexandros G. Dimakis', 'Sahand Negahban']|['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']
2017-03-16T23:25:21Z|2017-03-08T03:56:27Z|http://arxiv.org/abs/1703.02690v1|http://arxiv.org/pdf/1703.02690v1|Leveraging Sparsity for Efficient Submodular Data Summarization|The facility location problem is widely used for summarizing large datasets and has additional applications in sensor placement, image retrieval, and clustering. One difficulty of this problem is that submodular optimization algorithms require the calculation of pairwise benefits for all items in the dataset. This is infeasible for large problems, so recent work proposed to only calculate nearest neighbor benefits. One limitation is that several strong assumptions were invoked to obtain provable approximation guarantees. In this paper we establish that these extra assumptions are not necessary---solving the sparsified problem will be almost optimal under the standard assumptions of the problem. We then analyze a different method of sparsification that is a better model for methods such as Locality Sensitive Hashing to accelerate the nearest neighbor computations and extend the use of the problem to a broader family of similarities. We validate our approach by demonstrating that it rapidly generates interpretable summaries.|['Erik M. Lindgren', 'Shanshan Wu', 'Alexandros G. Dimakis']|['stat.ML', 'cs.DS', 'cs.IT', 'cs.LG', 'math.IT']
2017-03-16T23:25:25Z|2017-03-08T03:55:27Z|http://arxiv.org/abs/1703.02689v1|http://arxiv.org/pdf/1703.02689v1|Exact MAP Inference by Avoiding Fractional Vertices|Given a graphical model, one essential problem is MAP inference, that is, finding the most likely configuration of states according to the model. Although this problem is NP-hard, large instances can be solved in practice. A major open question is to explain why this is true. We give a natural condition under which we can provably perform MAP inference in polynomial time. We require that the number of fractional vertices in the LP relaxation exceeding the optimal solution is bounded by a polynomial in the problem size. This resolves an open question by Dimakis, Gohari, and Wainwright. In contrast, for general LP relaxations of integer programs, known techniques can only handle a constant number of fractional vertices whose value exceeds the optimal solution. We experimentally verify this condition and demonstrate how efficient various integer programming methods are at removing fractional solutions.|['Erik M. Lindgren', 'Alexandros G. Dimakis', 'Adam Klivans']|['stat.ML', 'cs.DS', 'cs.IT', 'cs.LG', 'math.IT']
2017-03-16T23:25:25Z|2017-03-08T03:21:08Z|http://arxiv.org/abs/1703.02682v1|http://arxiv.org/pdf/1703.02682v1|Sparse Quadratic Logistic Regression in Sub-quadratic Time|We consider support recovery in the quadratic logistic regression setting - where the target depends on both p linear terms $x_i$ and up to $p^2$ quadratic terms $x_i x_j$. Quadratic terms enable prediction/modeling of higher-order effects between features and the target, but when incorporated naively may involve solving a very large regression problem. We consider the sparse case, where at most $s$ terms (linear or quadratic) are non-zero, and provide a new faster algorithm. It involves (a) identifying the weak support (i.e. all relevant variables) and (b) standard logistic regression optimization only on these chosen variables. The first step relies on a novel insight about correlation tests in the presence of non-linearity, and takes $O(pn)$ time for $n$ samples - giving potentially huge computational gains over the naive approach. Motivated by insights from the boolean case, we propose a non-linear correlation test for non-binary finite support case that involves hashing a variable and then correlating with the output variable. We also provide experimental results to demonstrate the effectiveness of our methods.|['Karthikeyan Shanmugam', 'Murat Kocaoglu', 'Alexandros G. Dimakis', 'Sujay Sanghavi']|['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']
2017-03-16T23:25:25Z|2017-03-08T03:07:37Z|http://arxiv.org/abs/1703.02679v1|http://arxiv.org/pdf/1703.02679v1|Performance Bounds for Graphical Record Linkage|Record linkage involves merging records in large, noisy databases to remove duplicate entities. It has become an important area because of its widespread occurrence in bibliometrics, public health, official statistics production, political science, and beyond. Traditional linkage methods directly linking records to one another are computationally infeasible as the number of records grows. As a result, it is increasingly common for researchers to treat record linkage as a clustering task, in which each latent entity is associated with one or more noisy database records. We critically assess performance bounds using the Kullback-Leibler (KL) divergence under a Bayesian record linkage framework, making connections to Kolchin partition models. We provide an upper bound using the KL divergence and a lower bound on the minimum probability of misclassifying a latent entity. We give insights for when our bounds hold using simulated data and provide practical user guidance.|['Rebecca C. Steorts', 'Matt Barnes', 'Willie Neiswanger']|['math.ST', 'cs.IT', 'math.IT', 'stat.ME', 'stat.ML', 'stat.TH']
2017-03-16T23:25:25Z|2017-03-08T02:22:30Z|http://arxiv.org/abs/1703.02674v1|http://arxiv.org/pdf/1703.02674v1|Column Subset Selection via Polynomial Time Dual Volume Sampling|We study dual volume sampling, a method for selecting k columns from an n*m short and wide matrix (n <= k <= m) such that the probability of selection is proportional to the volume of the parallelepiped spanned by the rows of the induced submatrix. This method was studied in [3], who motivated it as a promising method for column subset selection. However, the development of polynomial time sampling algorithms -- exact or approximate -- has been since open. We close this open problem by presenting (i) an exact (randomized) polynomial time sampling algorithm; (ii) its derandomization that samples subsets satisfying the desired properties deterministically; and (iii) an efficient approximate sampling procedure using Markov chains that are provably fast mixing. Our algorithms can thus benefit downstream applications of dual volume sampling, such as column subset selection and experimental design.|['Chengtao Li', 'Stefanie Jegelka', 'Suvrit Sra']|['stat.ML']
2017-03-16T23:25:25Z|2017-03-08T01:45:54Z|http://arxiv.org/abs/1703.02662v1|http://arxiv.org/pdf/1703.02662v1|Structural Data Recognition with Graph Model Boosting|This paper presents a novel method for structural data recognition using a large number of graph models. In general, prevalent methods for structural data recognition have two shortcomings: 1) Only a single model is used to capture structural variation. 2) Naive recognition methods are used, such as the nearest neighbor method. In this paper, we propose strengthening the recognition performance of these models as well as their ability to capture structural variation. The proposed method constructs a large number of graph models and trains decision trees using the models. This paper makes two main contributions. The first is a novel graph model that can quickly perform calculations, which allows us to construct several models in a feasible amount of time. The second contribution is a novel approach to structural data recognition: graph model boosting. Comprehensive structural variations can be captured with a large number of graph models constructed in a boosting framework, and a sophisticated classifier can be formed by aggregating the decision trees. Consequently, we can carry out structural data recognition with powerful recognition capability in the face of comprehensive structural variation. The experiments shows that the proposed method achieves impressive results and outperforms existing methods on datasets of IAM graph database repository.|['Tomo Miyazaki', 'Shinichiro Omachi']|['cs.LG', 'stat.ML']
2017-03-16T23:25:25Z|2017-03-08T00:31:30Z|http://arxiv.org/abs/1703.02647v1|http://arxiv.org/pdf/1703.02647v1|Streaming Weak Submodularity: Interpreting Neural Networks on the Fly|In many machine learning applications, it is important to explain the predictions of a black-box classifier. For example, why does a deep neural network assign an image to a particular class? We cast interpretability of black-box classifiers as a combinatorial maximization problem and propose an efficient streaming algorithm to solve it subject to cardinality constraints. By extending ideas from Badanidiyuru et al. [2014], we provide a constant factor approximation guarantee for our algorithm in the case of random stream order and a weakly submodular objective function. This is the first such theoretical guarantee for this general class of functions, and we also show that no such algorithm exists for a worst case stream order. Our algorithm obtains similar explanations of Inception V3 predictions $10$ times faster than the state-of-the-art LIME framework of Ribeiro et al. [2016].|['Ethan R. Elenberg', 'Alexandros G. Dimakis', 'Moran Feldman', 'Amin Karbasi']|['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']
2017-03-16T23:25:25Z|2017-03-08T00:15:54Z|http://arxiv.org/abs/1703.02645v1|http://arxiv.org/pdf/1703.02645v1|Cost-Optimal Learning of Causal Graphs|We consider the problem of learning a causal graph over a set of variables with interventions. We study the cost-optimal causal graph learning problem: For a given skeleton (undirected version of the causal graph), design the set of interventions with minimum total cost, that can uniquely identify any causal graph with the given skeleton. We show that this problem is solvable in polynomial time. Later, we consider the case when the number of interventions is limited. For this case, we provide polynomial time algorithms when the skeleton is a tree or a clique tree. For a general chordal skeleton, we develop an efficient greedy algorithm, which can be improved when the causal graph skeleton is an interval graph.|['Murat Kocaoglu', 'Alexandros G. Dimakis', 'Sriram Vishwanath']|['cs.AI', 'cs.IT', 'math.IT', 'stat.ML']
2017-03-16T23:25:25Z|2017-03-08T00:04:01Z|http://arxiv.org/abs/1703.02641v1|http://arxiv.org/pdf/1703.02641v1|Don't Fear the Bit Flips: Optimized Coding Strategies for Binary   Classification|After being trained, classifiers must often operate on data that has been corrupted by noise. In this paper, we consider the impact of such noise on the features of binary classifiers. Inspired by tools for classifier robustness, we introduce the same classification probability (SCP) to measure the resulting distortion on the classifier outputs. We introduce a low-complexity estimate of the SCP based on quantization and polynomial multiplication. We also study channel coding techniques based on replication error-correcting codes. In contrast to the traditional channel coding approach, where error-correction is meant to preserve the data and is agnostic to the application, our schemes specifically aim to maximize the SCP (equivalently minimizing the distortion of the classifier output) for the same redundancy overhead.|['Frederic Sala', 'Shahroze Kabir', 'Guy Van den Broeck', 'Lara Dolecek']|['stat.ML', 'cs.LG']
2017-03-16T23:25:25Z|2017-03-07T22:32:06Z|http://arxiv.org/abs/1703.02629v1|http://arxiv.org/pdf/1703.02629v1|Online Learning Without Prior Information|The vast majority of optimization and online learning algorithms today require some prior information about the data (often in the form of bounds on gradients or on the optimal parameter value). When this information is not available, these algorithms require laborious manual tuning of various hyperparameters, motivating the search for algorithms that can adapt to the data with no prior information. We describe a frontier of new lower bounds on the performance of such algorithms, reflecting a tradeoff between a term that depends on the optimal parameter value and a term that depends on the gradients' rate of growth. Further, we construct a family of algorithms whose performance matches any desired point on this frontier, which no previous algorithm reaches.|['Ashok Cutkosky', 'Kwabena Boahen']|['cs.LG', 'stat.ML']
2017-03-16T23:25:25Z|2017-03-09T14:33:52Z|http://arxiv.org/abs/1703.02628v2|http://arxiv.org/pdf/1703.02628v2|Global optimization of Lipschitz functions|"The goal of the paper is to design sequential strategies which lead to efficient optimization of an unknown function under the only assumption that it has a finite Lipschitz constant. We first identify sufficient conditions for the consistency of generic sequential algorithms and formulate the expected minimax rate for their performance. We introduce and analyze a first algorithm called LIPO which assumes the Lipschitz constant to be known. Consistency, minimax rates for LIPO are proved, as well as fast rates under an additional H\""older like condition. An adaptive version of LIPO is also introduced for the more realistic setup where the Lipschitz constant is unknown and has to be estimated along with the optimization. Similar theoretical guarantees are shown to hold for the adaptive LIPO algorithm and a numerical assessment is provided at the end of the paper to illustrate the potential of this strategy with respect to state-of-the-art methods over typical benchmark problems for global optimization."|['Cédric Malherbe', 'Nicolas Vayatis']|['stat.ML']
2017-03-16T23:25:29Z|2017-03-07T22:17:17Z|http://arxiv.org/abs/1703.02624v1|http://arxiv.org/pdf/1703.02624v1|Exploiting Strong Convexity from Data with Primal-Dual First-Order   Algorithms|We consider empirical risk minimization of linear predictors with convex loss functions. Such problems can be reformulated as convex-concave saddle point problems, and thus are well suitable for primal-dual first-order algorithms. However, primal-dual algorithms often require explicit strongly convex regularization in order to obtain fast linear convergence, and the required dual proximal mapping may not admit closed-form or efficient solution. In this paper, we develop both batch and randomized primal-dual algorithms that can exploit strong convexity from data adaptively and are capable of achieving linear convergence even without regularization. We also present dual-free variants of the adaptive primal-dual algorithms that do not require computing the dual proximal mapping, which are especially suitable for logistic regression.|['Jialei Wang', 'Lin Xiao']|['math.OC', 'stat.ML']
2017-03-16T23:25:29Z|2017-03-07T22:14:53Z|http://arxiv.org/abs/1703.02622v1|http://arxiv.org/pdf/1703.02622v1|Online Convex Optimization with Unconstrained Domains and Losses|We propose an online convex optimization algorithm (RescaledExp) that achieves optimal regret in the unconstrained setting without prior knowledge of any bounds on the loss functions. We prove a lower bound showing an exponential separation between the regret of existing algorithms that require a known bound on the loss functions and any algorithm that does not require such knowledge. RescaledExp matches this lower bound asymptotically in the number of iterations. RescaledExp is naturally hyperparameter-free and we demonstrate empirically that it matches prior optimization algorithms that require hyperparameter optimization.|['Ashok Cutkosky', 'Kwabena Boahen']|['cs.LG', 'stat.ML']
2017-03-16T23:25:29Z|2017-03-07T21:18:11Z|http://arxiv.org/abs/1703.02596v1|http://arxiv.org/pdf/1703.02596v1|Customer Life Time Value Prediction Using Embeddings|We describe the Customer Life Time Value (CLTV) prediction system deployed at ASOS.com, a global online fashion retailer. CLTV prediction is an important problem in e-commerce where an accurate estimate of future value allows retailers to effectively allocate marketing spend, identify and nurture high value customers and mitigate exposure to losses. The system at ASOS provides daily estimates of the future value of every customer and is one of the cornerstones of the personalised shopping experience. The state of the art in this domain uses large numbers of handcrafted features and ensemble regressors to forecast value, predict churn and evaluate customer loyalty. We describe our system, which adopts this approach, and our ongoing efforts to further improve it. Recently, domains including language, vision and speech have shown dramatic advances by replacing handcrafted features with features that are learned automatically from data. We show that learning feature representations is a promising extension to the state of the art in CLTV modelling. We propose a novel way to generate embeddings of customers, which addresses the issue of the ever changing product catalogue and obtain a significant improvement over an exhaustive set of handcrafted features.|['Benjamin Paul Chamberlain', 'Angelo Cardoso', 'C. H. Bryan Liu', 'Roberto Pagliari', 'Marc Peter Deisenroth']|['cs.LG', 'cs.CY', 'cs.IR', 'cs.NE', 'stat.ML']
2017-03-16T23:25:29Z|2017-03-07T19:47:22Z|http://arxiv.org/abs/1703.02570v1|http://arxiv.org/pdf/1703.02570v1|Regularising Non-linear Models Using Feature Side-information|Very often features come with their own vectorial descriptions which provide detailed information about their properties. We refer to these vectorial descriptions as feature side-information. In the standard learning scenario, input is represented as a vector of features and the feature side-information is most often ignored or used only for feature selection prior to model fitting. We believe that feature side-information which carries information about features intrinsic property will help improve model prediction if used in a proper way during learning process. In this paper, we propose a framework that allows for the incorporation of the feature side-information during the learning of very general model families to improve the prediction performance. We control the structures of the learned models so that they reflect features similarities as these are defined on the basis of the side-information. We perform experiments on a number of benchmark datasets which show significant predictive performance gains, over a number of baselines, as a result of the exploitation of the side-information.|['Amina Mollaysa', 'Pablo Strasser', 'Alexandros Kalousis']|['cs.LG', 'stat.ML']
2017-03-16T23:25:29Z|2017-03-07T18:54:04Z|http://arxiv.org/abs/1703.02528v1|http://arxiv.org/pdf/1703.02528v1|Stopping GAN Violence: Generative Unadversarial Networks|While the costs of human violence have attracted a great deal of attention from the research community, the effects of the network-on-network (NoN) violence popularised by Generative Adversarial Networks have yet to be addressed. In this work, we quantify the financial, social, spiritual, cultural, grammatical and dermatological impact of this aggression and address the issue by proposing a more peaceful approach which we term Generative Unadversarial Networks (GUNs). Under this framework, we simultaneously train two models: a generator G that does its best to capture whichever data distribution it feels it can manage, and a motivator M that helps G to achieve its dream. Fighting is strictly verboten and both models evolve by learning to respect their differences. The framework is both theoretically and electrically grounded in game theory, and can be viewed as a winner-shares-all two-player game in which both players work as a team to achieve the best score. Experiments show that by working in harmony, the proposed model is able to claim both the moral and log-likelihood high ground. Our work builds on a rich history of carefully argued position-papers, published as anonymous YouTube comments, which prove that the optimal solution to NoN violence is more GUNs.|['Samuel Albanie', 'Sébastien Ehrhardt', 'João F. Henriques']|['stat.ML', 'cs.LG']
2017-03-16T23:25:29Z|2017-03-07T18:53:58Z|http://arxiv.org/abs/1703.02527v1|http://arxiv.org/pdf/1703.02527v1|Online Learning to Rank in Stochastic Click Models|Online learning to rank is an important problem in machine learning, information retrieval, recommender systems, and display advertising. Many provably efficient algorithms have been developed for this problem recently, under specific click models. A click model is a model of how users click on a list of documents. Though these results are significant, the proposed algorithms have limited application because they are designed for specific click models, and do not have guarantees beyond them. To overcome this challenge, we propose a novel algorithm, which we call MergeRank, for learning to rank in a class of click models that satisfy mild technical assumptions. This class encompasses two most fundamental click models, the cascade and position-based models. We derive a gap-dependent upper bound on the expected $n$-step regret of MergeRank and evaluate it on web search queries. We observe that MergeRank performs better than ranked bandits and is more robust than CascadeKL-UCB, an existing algorithm for learning to rank in the cascade model.|['Mohammad Ghavamzadeh', 'Branislav Kveton', 'Csaba Szepesvari', 'Tomas Tunys', 'Zheng Wen', 'Masrour Zoghi']|['cs.LG', 'stat.ML']
2017-03-16T23:25:29Z|2017-03-07T18:36:55Z|http://arxiv.org/abs/1703.02518v1|http://arxiv.org/pdf/1703.02518v1|Faster Coordinate Descent via Adaptive Importance Sampling|Coordinate descent methods employ random partial updates of decision variables in order to solve huge-scale convex optimization problems. In this work, we introduce new adaptive rules for the random selection of their updates. By adaptive, we mean that our selection rules are based on the dual residual or the primal-dual gap estimates and can change at each iteration. We theoretically characterize the performance of our selection rules and demonstrate improvements over the state-of-the-art, and extend our theory and algorithms to general convex objectives. Numerical evidence with hinge-loss support vector machines and Lasso confirm that the practice follows the theory.|['Dmytro Perekrestenko', 'Volkan Cevher', 'Martin Jaggi']|['cs.LG', 'cs.CV', 'math.OC', 'stat.CO', 'stat.ML', 'G.1.6']
2017-03-16T23:25:29Z|2017-03-07T17:52:13Z|http://arxiv.org/abs/1703.02492v1|http://arxiv.org/pdf/1703.02492v1|Online Multilinear Dictionary Learning for Sequential Compressive   Sensing|A method for online tensor dictionary learning is proposed. With the assumption of separable dictionaries, tensor contraction is used to diminish a $N$-way model of $\mathcal{O}\left(L^N\right)$ into a simple matrix equation of $\mathcal{O}\left(NL^2\right)$ with a real-time capability. To avoid numerical instability due to inversion of sparse matrix, a class of stochastic gradient with memory is formulated via a least-square solution to guarantee convergence and robustness. Both gradient descent with exact line search and Newton's method are discussed and realized. Extensions onto how to deal with bad initialization and outliers are also explained in detail. Experiments on two synthetic signals confirms an impressive performance of our proposed method.|['Thiernithi Variddhisaï', 'Danilo Mandic']|['cs.LG', 'stat.ML']
2017-03-16T23:25:29Z|2017-03-12T23:44:34Z|http://arxiv.org/abs/1703.02435v2|http://arxiv.org/pdf/1703.02435v2|Unsupervised learning of phase transitions: from principal component   analysis to variational autoencoders|We employ unsupervised machine learning techniques to learn latent parameters which best describe states of the two-dimensional Ising model and the three-dimensional XY model. These methods range from principal component analysis to artificial neural network based variational autoencoders. The states are sampled using a Monte-Carlo simulation above and below the critical temperature. We find that the predicted latent parameters correspond to the known order parameters. The latent representation of the states of the models in question are clustered, which makes it possible to identify phases without prior knowledge of their existence or the underlying Hamiltonian. Furthermore, we find that the reconstruction loss function can be used as a universal identifier for phase transitions.|['Sebastian Johann Wetzel']|['cond-mat.stat-mech', 'cs.LG', 'stat.ML']
2017-03-16T23:25:29Z|2017-03-07T15:26:38Z|http://arxiv.org/abs/1703.02433v1|http://arxiv.org/pdf/1703.02433v1|An investigation into machine learning approaches for forecasting   spatio-temporal demand in ride-hailing service|In this paper, we present machine learning approaches for characterizing and forecasting the short-term demand for on-demand ride-hailing services. We propose the spatio-temporal estimation of the demand that is a function of variable effects related to traffic, pricing and weather conditions. With respect to the methodology, a single decision tree, bootstrap-aggregated (bagged) decision trees, random forest, boosted decision trees, and artificial neural network for regression have been adapted and systematically compared using various statistics, e.g. R-square, Root Mean Square Error (RMSE), and slope. To better assess the quality of the models, they have been tested on a real case study using the data of DiDi Chuxing, the main on-demand ride hailing service provider in China. In the current study, 199,584 time-slots describing the spatio-temporal ride-hailing demand has been extracted with an aggregated-time interval of 10 mins. All the methods are trained and validated on the basis of two independent samples from this dataset. The results revealed that boosted decision trees provide the best prediction accuracy (RMSE=16.41), while avoiding the risk of over-fitting, followed by artificial neural network (20.09), random forest (23.50), bagged decision trees (24.29) and single decision tree (33.55).|['Ismaïl Saadi', 'Melvin Wong', 'Bilal Farooq', 'Jacques Teller', 'Mario Cools']|['cs.LG', 'stat.ML']
2017-03-16T23:25:33Z|2017-03-07T14:39:15Z|http://arxiv.org/abs/1703.02403v1|http://arxiv.org/pdf/1703.02403v1|On Structured Prediction Theory with Calibrated Convex Surrogate Losses|"We provide novel theoretical insights on structured prediction in the context of efficient convex surrogate loss minimization with consistency guarantees. For any task loss, we construct a convex surrogate that can be optimized via stochastic gradient descent and we prove tight bounds on the so-called ""calibration function"" relating the excess surrogate risk to the actual risk. In contrast to prior related work, we carefully monitor the effect of the exponential number of classes in the learning guarantees as well as on the optimization complexity. As an interesting consequence, we formalize the intuition that some task losses make learning harder than others, and that the classical 0-1 loss is ill-suited for general structured prediction."|['Anton Osokin', 'Francis Bach', 'Simon Lacoste-Julien']|['cs.LG', 'stat.ML']
2017-03-16T23:25:33Z|2017-03-13T16:46:20Z|http://arxiv.org/abs/1703.02379v2|http://arxiv.org/pdf/1703.02379v2|Global Weisfeiler-Lehman Graph Kernels|Most state-of-the-art graph kernels only take local graph properties into account, i.e., the kernel is computed with regard to properties of the neighborhood of vertices or other small substructures only. On the other hand, kernels that do take global graph properties into account may not scale well to large graph databases. Here we propose to start exploring the space between local and global graph kernels, striking the balance between both worlds. Specifically, we introduce a novel graph kernel based on the $k$-dimensional Weisfeiler-Lehman algorithm, and show that it takes local as well as global properties into account. Unfortunately, the $k$-dimensional Weisfeiler-Lehman scales exponentially in $k$. Consequently, we devise a stochastic version of the kernel with provable approximation guarantees using conditional Rademacher averages. On bounded-degree graphs, it can even be computed in constant time. We support our theoretical results with experiments on several graph classification benchmarks, showing that our kernels often outperform the state-of-the-art in terms of classification accuracies.|['Christopher Morris', 'Kristian Kersting', 'Petra Mutzel']|['cs.LG', 'stat.ML']
2017-03-16T23:25:33Z|2017-03-07T10:36:30Z|http://arxiv.org/abs/1703.02317v1|http://arxiv.org/pdf/1703.02317v1|Convolutional Recurrent Neural Networks for Bird Audio Detection|Bird sounds possess distinctive spectral structure which may exhibit small shifts in spectrum depending on the bird species and environmental conditions. In this paper, we propose using convolutional recurrent neural networks on the task of automated bird audio detection in real-life environments. In the proposed method, convolutional layers extract high dimensional, local frequency shift invariant features, while recurrent layers capture longer term dependencies between the features extracted from short time frames. This method achieves 88.5% Area Under ROC Curve (AUC) score on the unseen evaluation data and obtains the second place in the Bird Audio Detection challenge.|['EmreÇakır', 'Sharath Adavanne', 'Giambattista Parascandolo', 'Konstantinos Drossos', 'Tuomas Virtanen']|['cs.SD', 'cs.LG', 'stat.ML']
2017-03-16T23:25:33Z|2017-03-07T10:16:45Z|http://arxiv.org/abs/1703.02310v1|http://arxiv.org/pdf/1703.02310v1|Deep Robust Kalman Filter|A Robust Markov Decision Process (RMDP) is a sequential decision making model that accounts for uncertainty in the parameters of dynamic systems. This uncertainty introduces difficulties in learning an optimal policy, especially for environments with large state spaces. We propose two algorithms, RTD-DQN and Deep-RoK, for solving large-scale RMDPs using nonlinear approximation schemes such as deep neural networks. The RTD-DQN algorithm incorporates the robust Bellman temporal difference error into a robust loss function, yielding robust policies for the agent. The Deep-RoK algorithm is a robust Bayesian method, based on the Extended Kalman Filter (EKF), that accounts for both the uncertainty in the weights of the approximated value function and the uncertainty in the transition probabilities, improving the robustness of the agent. We provide theoretical results for our approach and test the proposed algorithms on a continuous state domain.|['Shirli Di-Castro Shashua', 'Shie Mannor']|['cs.AI', 'cs.LG', 'stat.ML']
2017-03-16T23:25:33Z|2017-03-14T18:52:48Z|http://arxiv.org/abs/1703.02236v2|http://arxiv.org/pdf/1703.02236v2|Propensity score prediction for electronic healthcare databases using   Super Learner and High-dimensional Propensity Score Methods|"The optimal learner for prediction modeling varies depending on the underlying data-generating distribution. Super Learner (SL) is a generic ensemble learning algorithm that uses cross-validation to select among a ""library"" of candidate prediction models. The SL is not restricted to a single prediction model, but uses the strengths of a variety of learning algorithms to adapt to different databases. While the SL has been shown to perform well in a number of settings, it has not been thoroughly evaluated in large electronic healthcare databases that are common in pharmacoepidemiology and comparative effectiveness research. In this study, we applied and evaluated the performance of the SL in its ability to predict treatment assignment using three electronic healthcare databases. We considered a library of algorithms that consisted of both nonparametric and parametric models. We also considered a novel strategy for prediction modeling that combines the SL with the high-dimensional propensity score (hdPS) variable selection algorithm. Predictive performance was assessed using three metrics: the negative log-likelihood, area under the curve (AUC), and time complexity. Results showed that the best individual algorithm, in terms of predictive performance, varied across datasets. The SL was able to adapt to the given dataset and optimize predictive performance relative to any individual learner. Combining the SL with the hdPS was the most consistent prediction method and may be promising for PS estimation and prediction modeling in electronic healthcare databases."|['Cheng Ju', 'Mary Combs', 'Samuel D Lendle', 'Jessica M Franklin', 'Richard Wyss', 'Sebastian Schneeweiss', 'Mark J. van der Laan']|['stat.AP', 'stat.ML']
2017-03-16T23:25:33Z|2017-03-07T04:03:27Z|http://arxiv.org/abs/1703.02205v1|http://arxiv.org/pdf/1703.02205v1|Raw Waveform-based Speech Enhancement by Fully Convolutional Networks|This paper proposes a fully convolutional network (FCN) model for raw waveform-based speech enhancement. The proposed system performs speech enhancement in an end-to-end (i.e. waveform-in and waveform-out) manner, which differs from most existing denoising methods that process the magnitude spectrum (e.g. log power spectrum (LPS)) only. Because the fully connected layers, which are involved in deep neural networks (DNN) and convolutional neural networks (CNN), may not accurately characterize local information of speech signals, especially for high-frequency components, we employed fully convolutional layers to model the waveform. More specifically, FCN only consists convolutional layers and hence the local temporal structures of speech signals can be efficiently and effectively preserved with a relatively small number of weights. Experimental results show that DNN and CNN based models have limited capability to restore high-frequency components of waveforms, thus leading to imperfect intelligibility of enhanced speech. On the other hand, the proposed FCN model can not only well recover the waveforms but also outperform the LPS-based DNN baseline in terms of STOI and PESQ. In addition, the number of model parameters in FCN is roughly only 0.2% compared with that in DNN and CNN.|['Szu-Wei Fu', 'Yu Tsao', 'Xugang Lu', 'Hisashi Kawai']|['stat.ML', 'cs.LG', 'cs.SD']
2017-03-16T23:25:33Z|2017-03-07T02:41:40Z|http://arxiv.org/abs/1703.02185v1|http://arxiv.org/pdf/1703.02185v1|Indoor Localization by Fusing a Group of Fingerprints Based on Random   Forests|Indoor localization based on SIngle Of Fingerprint (SIOF) is rather susceptible to the changing environment, multipath, and non-line-of-sight (NLOS) propagation. Building SIOF is also a very time-consuming process. Recently, we first proposed a GrOup Of Fingerprints (GOOF) to improve the localization accuracy and reduce the burden of building fingerprints. However, the main drawback is the timeliness. In this paper, we propose a novel localization framework by Fusing A Group Of fingerprinTs (FAGOT) based on random forests. In the offline phase, we first build a GOOF from different transformations of the received signals of multiple antennas. Then, we design multiple GOOF strong classifiers based on Random Forests (GOOF-RF) by training each fingerprint in the GOOF. In the online phase, we input the corresponding transformations of the real measurements into these strong classifiers to obtain multiple independent decisions. Finally, we propose a Sliding Window aIded Mode-based (SWIM) fusion algorithm to balance the localization accuracy and time. Our proposed approaches can work better in an unknown indoor scenario. The burden of building fingerprints can also be reduced drastically. We demonstrate the performance of our algorithms through simulations and real experimental data using two Universal Software Radio Peripheral (USRP) platforms.|['Xiansheng Guo', 'Nirwan Ansari', 'Huiyong Li']|['stat.ML', 'cs.CV']
2017-03-16T23:25:33Z|2017-03-07T02:41:07Z|http://arxiv.org/abs/1703.02184v1|http://arxiv.org/pdf/1703.02184v1|Indoor Localization Using Visible Light Via Fusion Of Multiple   Classifiers|A multiple classifiers fusion localization technique using received signal strengths (RSSs) of visible light is proposed, in which the proposed system transmits different intensity modulated sinusoidal signals by LEDs and the signals received by a Photo Diode (PD) placed at various grid points. First, we obtain some {\emph{approximate}} received signal strengths (RSSs) fingerprints by capturing the peaks of power spectral density (PSD) of the received signals at each given grid point. Unlike the existing RSSs based algorithms, several representative machine learning approaches are adopted to train multiple classifiers based on these RSSs fingerprints. The multiple classifiers localization estimators outperform the classical RSS-based LED localization approaches in accuracy and robustness. To further improve the localization performance, two robust fusion localization algorithms, namely, grid independent least square (GI-LS) and grid dependent least square (GD-LS), are proposed to combine the outputs of these classifiers. We also use a singular value decomposition (SVD) based LS (LS-SVD) method to mitigate the numerical stability problem when the prediction matrix is singular. Experiments conducted on intensity modulated direct detection (IM/DD) systems have demonstrated the effectiveness of the proposed algorithms. The experimental results show that the probability of having mean square positioning error (MSPE) of less than 5cm achieved by GD-LS is improved by 93.03\% and 93.15\%, respectively, as compared to those by the RSS ratio (RSSR) and RSS matching methods with the FFT length of 2000.|['Xiansheng Guo', 'Sihua Shao', 'Nirwan Ansari', 'Abdallah Khreishah']|['stat.ML', 'cs.CV']
2017-03-16T23:25:33Z|2017-03-07T00:09:31Z|http://arxiv.org/abs/1703.02156v1|http://arxiv.org/pdf/1703.02156v1|On the Limits of Learning Representations with Label-Based Supervision|Advances in neural network based classifiers have transformed automatic feature learning from a pipe dream of stronger AI to a routine and expected property of practical systems. Since the emergence of AlexNet every winning submission of the ImageNet challenge has employed end-to-end representation learning, and due to the utility of good representations for transfer learning, representation learning has become as an important and distinct task from supervised learning. At present, this distinction is inconsequential, as supervised methods are state-of-the-art in learning transferable representations. But recent work has shown that generative models can also be powerful agents of representation learning. Will the representations learned from these generative methods ever rival the quality of those from their supervised competitors? In this work, we argue in the affirmative, that from an information theoretic perspective, generative models have greater potential for representation learning. Based on several experimentally validated assumptions, we show that supervised learning is upper bounded in its capacity for representation learning in ways that certain generative models, such as Generative Adversarial Networks (GANs) are not. We hope that our analysis will provide a rigorous motivation for further exploration of generative representation learning.|['Jiaming Song', 'Russell Stewart', 'Shengjia Zhao', 'Stefano Ermon']|['cs.LG', 'cs.AI', 'stat.ML']
2017-03-16T23:25:33Z|2017-03-07T00:03:32Z|http://arxiv.org/abs/1703.02155v1|http://arxiv.org/pdf/1703.02155v1|Model-Based Multiple Instance Learning|While Multiple Instance (MI) data are point patterns -- sets or multi-sets of unordered points -- appropriate statistical point pattern models have not been used in MI learning. This article proposes a framework for model-based MI learning using point process theory. Likelihood functions for point pattern data derived from point process theory enable principled yet conceptually transparent extensions of learning tasks, such as classification, novelty detection and clustering, to point pattern data. Furthermore, tractable point pattern models as well as solutions for learning and decision making from point pattern data are developed.|['Ba-Ngu Vo', 'Dinh Phung', 'Quang N. Tran', 'Ba-Tuong Vo']|['stat.ML', 'cs.LG']
