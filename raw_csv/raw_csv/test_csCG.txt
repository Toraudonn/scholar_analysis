2017-03-16T23:30:33Z|2017-03-15T01:00:44Z|http://arxiv.org/abs/1703.04861v1|http://arxiv.org/pdf/1703.04861v1|Robust Non-Rigid Registration With Reweighted Dual Sparsities|Non-rigid registration is challenging because it is ill-posed with high degrees of freedom and is thus sensitive to noise and outliers. We propose a robust non-rigid registration method using reweighted sparsities on position and transformation to estimate the deformations between 3-D shapes. We formulate the energy function with dual sparsities on both the data term and the smoothness term, and define the smoothness constraint using local rigidity. The dual-sparsity based non-rigid registration model is enhanced with a reweighting scheme, and solved by transferring the model into some alternating optimized subproblems which have exact solutions and guaranteed convergence. Experimental results on both public datasets and real scanned datasets show that our method outperforms the state-of-the-art methods and is more robust to noise and outliers than conventional non-rigid registration methods.|['Jingyu Yang', 'Kun Li', 'Yu-Kun Lai', 'Daoliang Guo']|['cs.CV', 'cs.CG', 'cs.GR']
2017-03-16T23:30:33Z|2017-03-14T22:19:50Z|http://arxiv.org/abs/1703.04774v1|http://arxiv.org/pdf/1703.04774v1|Self-Assembly of 4-sided Fractals in the Two-handed Tile Assembly Model|In this paper, we consider the strict self-assembly of fractals in one of the most well-studied models of tile based self-assembling systems known as the Two-handed Tile Assembly Model (2HAM). We are particularly interested in a class of fractals called discrete self-similar fractals (a class of fractals that includes the discrete Sierpinski's carpet). We present a 2HAM system that strictly self-assembles the discrete Sierpinski's carpet with scale factor 1. Moreover, the 2HAM system that we give lends itself to being generalized and we describe how this system can be modified to obtain a 2HAM system that strictly self-assembles one of any fractal from an infinite set of fractals which we call 4-sided fractals. The 2HAM systems we give in this paper are the first examples of systems that strictly self-assemble discrete self-similar fractals at scale factor 1 in a purely growth model of self-assembly. Finally, we give an example of a 3-sided fractal (which is not a tree fractal) that cannot be strictly self-assembled by any 2HAM system.|['Jacob Hendricks', 'Joseph Opseth']|['cs.ET', 'cs.CG']
2017-03-16T23:30:33Z|2017-03-14T22:13:58Z|http://arxiv.org/abs/1703.04758v1|http://arxiv.org/pdf/1703.04758v1|Approximation Schemes for Independent Set and Sparse Subsets of Polygons|We present an $(1+\varepsilon)$-approximation algorithm with quasi-polynomial running time for computing the maximum weight independent set of polygons out of a given set of polygons in the plane (specifically, the running time is $n^{O( \mathrm{poly}( \log n, 1/\varepsilon))}$). Contrasting this, the best known polynomial time algorithm for the problem has an approximation ratio of~$n^{\varepsilon}$. Surprisingly, we can extend the algorithm to the problem of computing the maximum weight subset of the given set of polygons whose intersection graph fulfills some sparsity condition. For example, we show that one can approximate the maximum weight subset of polygons, such that the intersection graph of the subset is planar or does not contain a cycle of length $4$ (i.e., $K_{2,2}$). Our algorithm relies on a recursive partitioning scheme, whose backbone is the existence of balanced cuts with small complexity that intersect polygons from the optimal solution of a small total weight.   For the case of large axis-parallel rectangles, we provide a polynomial time $(1+\varepsilon)$-approximation for the maximum weight independent set. Specifically, we consider the problem where each rectangle has one edge whose length is at least a constant fraction of the length of the corresponding edge of the bounding box of all the input elements. This is now the most general case for which a PTAS is known, and it requires a new and involved partitioning scheme, which should be of independent interest.|['Anna Adamaszek', 'Sariel Har-Peled', 'Andreas Wiese']|['cs.CG']
2017-03-16T23:30:33Z|2017-03-13T16:18:01Z|http://arxiv.org/abs/1703.04466v1|http://arxiv.org/pdf/1703.04466v1|Bicriteria Rectilinear Shortest Paths among Rectilinear Obstacles in the   Plane|Given a rectilinear domain $\mathcal{P}$ of $h$ pairwise-disjoint rectilinear obstacles with a total of $n$ vertices in the plane, we study the problem of computing bicriteria rectilinear shortest paths between two points $s$ and $t$ in $\mathcal{P}$. Three types of bicriteria rectilinear paths are considered: minimum-link shortest paths, shortest minimum-link paths, and minimum-cost paths where the cost of a path is a non-decreasing function of both the number of edges and the length of the path. The one-point and two-point path queries are also considered. Algorithms for these problems have been given previously. Our contributions are threefold. First, we find a critical error in all previous algorithms. Second, we correct the error in a not-so-trivial way. Third, we further improve the algorithms so that they are even faster than the previous (incorrect) algorithms when $h$ is relatively small. For example, for the minimum-link shortest paths, we obtain the following results. Our algorithm computes a minimum-link shortest $s$-$t$ path in $O(n+h\log^{3/2} h)$ time. For the one-point queries, we build a data structure of size $O(n+ h\log h)$ in $O(n+h\log^{3/2} h)$ time for a source point $s$, such that given any query point $t$, a minimum-link shortest $s$-$t$ path can be determined in $O(\log n)$ time. For the two-point queries, with $O(n+h^2\log^2 h)$ time and space preprocessing, a minimum-link shortest $s$-$t$ path can be determined in $O(\log n+\log^2 h)$ time for any two query points $s$ and $t$; alternatively, with $O(n+h^2\cdot \log^{2} h \cdot 4^{\sqrt{\log h}})$ time and $O(n+h^2\cdot \log h \cdot 4^{\sqrt{\log h}})$ space preprocessing, we can answer each two-point query in $O(\log n)$ time.|['Haitao Wang']|['cs.CG', 'cs.DS']
2017-03-16T23:30:33Z|2017-03-13T11:00:53Z|http://arxiv.org/abs/1703.04329v1|http://arxiv.org/pdf/1703.04329v1|Stabbing segments with rectilinear objects|Given a set $S$ of $n$ line segments in the plane, we say that a region $\mathcal{R}\subseteq \mathbb{R}^2$ is a {\em stabber} for $S$ if $\mathcal{R}$ contains exactly one endpoint of each segment of $S$. In this paper we provide optimal or near-optimal algorithms for reporting all combinatorially different stabbers for several shapes of stabbers. Specifically, we consider the case in which the stabber can be described as the intersection of axis-parallel halfplanes (thus the stabbers are halfplanes, strips, quadrants, $3$-sided rectangles, or rectangles). The running times are $O(n)$ (for the halfplane case), $O(n\log n)$ (for strips, quadrants, and 3-sided rectangles), and $O(n^2 \log n)$ (for rectangles).|['Mercè Claverol', 'Delia Garijo', 'Matias Korman', 'Carlos Seara', 'Rodrigo I. Silveira']|['cs.CG']
2017-03-16T23:30:33Z|2017-03-13T08:00:31Z|http://arxiv.org/abs/1703.04283v1|http://arxiv.org/pdf/1703.04283v1|Universal Slope Sets for 1-Bend Planar Drawings|We describe a set of $\Delta -1$ slopes that are universal for 1-bend planar drawings of planar graphs of maximum degree $\Delta \geq 4$; this establishes a new upper bound of $\Delta-1$ on the 1-bend planar slope number. By universal we mean that every planar graph of degree $\Delta$ has a planar drawing with at most one bend per edge and such that the slopes of the segments forming the edges belong to the given set of slopes. This improves over previous results in two ways: Firstly, the best previously known upper bound for the 1-bend planar slope number was $\frac{3}{2} (\Delta -1)$ (the known lower bound being $\frac{3}{4} (\Delta -1)$); secondly, all the known algorithms to construct 1-bend planar drawings with $O(\Delta)$ slopes use a different set of slopes for each graph and can have bad angular resolution, while our algorithm uses a universal set of slopes, which also guarantees that the minimum angle between any two edges incident to a vertex is $\frac{\pi}{(\Delta-1)}$.|['Patrizio Angelini', 'Michael A. Bekos', 'Giuseppe Liotta', 'Fabrizio Montecchiani']|['cs.CG']
2017-03-16T23:30:33Z|2017-03-12T07:21:50Z|http://arxiv.org/abs/1703.04079v1|http://arxiv.org/pdf/1703.04079v1|SurfNet: Generating 3D shape surfaces using deep residual networks|3D shape models are naturally parameterized using vertices and faces, \ie, composed of polygons forming a surface. However, current 3D learning paradigms for predictive and generative tasks using convolutional neural networks focus on a voxelized representation of the object. Lifting convolution operators from the traditional 2D to 3D results in high computational overhead with little additional benefit as most of the geometry information is contained on the surface boundary. Here we study the problem of directly generating the 3D shape surface of rigid and non-rigid shapes using deep convolutional neural networks. We develop a procedure to create consistent `geometry images' representing the shape surface of a category of 3D objects. We then use this consistent representation for category-specific shape surface generation from a parametric representation or an image by developing novel extensions of deep residual networks for the task of geometry image generation. Our experiments indicate that our network learns a meaningful representation of shape surfaces allowing it to interpolate between shape orientations and poses, invent new shape surfaces and reconstruct 3D shape surfaces from previously unseen images.|['Ayan Sinha', 'Asim Unmesh', 'Qixing Huang', 'Karthik Ramani']|['cs.CV', 'cs.CG']
2017-03-16T23:30:33Z|2017-03-11T23:16:23Z|http://arxiv.org/abs/1703.04040v1|http://arxiv.org/abs/1703.04040v1|Locality-sensitive hashing of curves|We study data structures for storing a set of polygonal curves in ${\rm R}^d$ such that, given a query curve, we can efficiently retrieve similar curves from the set, where similarity is measured using the discrete Fr\'echet distance or the dynamic time warping distance. To this end we devise the first locality-sensitive hashing schemes for these distance measures. A major challenge is posed by the fact that these distance measures internally optimize the alignment between the curves. We give solutions for different types of alignments including constrained and unconstrained versions. For unconstrained alignments, we improve over a result by Indyk from 2002 for short curves. Let $n$ be the number of input curves and let $m$ be the maximum complexity of a curve in the input. In the particular case where $m \leq \frac{\alpha}{4d} \log n$, for some fixed $\alpha>0$, our solutions imply an approximate near-neighbor data structure for the discrete Fr\'echet distance that uses space in $O(n^{1+\alpha}\log n)$ and achieves query time in $O(n^{\alpha}\log^2 n)$ and constant approximation factor. Furthermore, our solutions provide a trade-off between approximation quality and computational performance: for any parameter $k \in [m]$, we can give a data structure that uses space in $O(2^{2k}m^{k-1} n \log n + nm)$, answers queries in $O( 2^{2k} m^{k}\log n)$ time and achieves approximation factor in $O(m/k)$.|['Anne Driemel', 'Francesco Silvestri']|['cs.CG', 'cs.DS', 'cs.IR', 'F.2.2']
2017-03-16T23:30:33Z|2017-03-10T13:54:29Z|http://arxiv.org/abs/1703.03687v1|http://arxiv.org/pdf/1703.03687v1|Best Laid Plans of Lions and Men|"We answer the following question dating back to J.E. Littlewood (1885 - 1977): Can two lions catch a man in a bounded area with rectifiable lakes? The lions and the man are all assumed to be points moving with at most unit speed. That the lakes are rectifiable means that their boundaries are finitely long. This requirement is to avoid pathological examples where the man survives forever because any path to the lions is infinitely long. We show that the answer to the question is not always ""yes"" by giving an example of a region $R$ in the plane where the man has a strategy to survive forever. $R$ is a polygonal region with holes and the exterior and interior boundaries are pairwise disjoint, simple polygons. Our construction is the first truly two-dimensional example where the man can survive.   Next, we consider the following game played on the entire plane instead of a bounded area: There is any finite number of unit speed lions and one fast man who can run with speed $1+\varepsilon$ for some value $\varepsilon>0$. Can the man always survive? We answer the question in the affirmative for any constant $\varepsilon>0$."|['Mikkel Abrahamsen', 'Jacob Holm', 'Eva Rotenberg', 'Christian Wulff-Nilsen']|['cs.CG', 'cs.GT']
2017-03-16T23:30:33Z|2017-03-10T08:35:24Z|http://arxiv.org/abs/1703.03575v1|http://arxiv.org/pdf/1703.03575v1|Crossing the Logarithmic Barrier for Dynamic Boolean Data Structure   Lower Bounds|"This paper proves the first super-logarithmic lower bounds on the cell probe complexity of dynamic boolean (a.k.a. decision) data structure problems, a long-standing milestone in data structure lower bounds.   We introduce a new method for proving dynamic cell probe lower bounds and use it to prove a $\tilde{\Omega}(\log^{1.5} n)$ lower bound on the operational time of a wide range of boolean data structure problems, most notably, on the query time of dynamic range counting over $\mathbb{F}_2$ ([Pat07]). Proving an $\omega(\lg n)$ lower bound for this problem was explicitly posed as one of five important open problems in the late Mihai P\v{a}tra\c{s}cu's obituary [Tho13]. This result also implies the first $\omega(\lg n)$ lower bound for the classical 2D range counting problem, one of the most fundamental data structure problems in computational geometry and spatial databases. We derive similar lower bounds for boolean versions of dynamic polynomial evaluation and 2D rectangle stabbing, and for the (non-boolean) problems of range selection and range median.   Our technical centerpiece is a new way of ""weakly"" simulating dynamic data structures using efficient one-way communication protocols with small advantage over random guessing. This simulation involves a surprising excursion to low-degree (Chebychev) polynomials which may be of independent interest, and offers an entirely new algorithmic angle on the ""cell sampling"" method of Panigrahy et al. [PTW10]."|['Kasper Green Larsen', 'Omri Weinstein', 'Huacheng Yu']|['cs.DS', 'cs.CC', 'cs.CG', 'cs.IT', 'math.IT']
2017-03-16T23:30:37Z|2017-03-08T21:50:06Z|http://arxiv.org/abs/1703.03048v1|http://arxiv.org/pdf/1703.03048v1|Quickest Visibility Queries in Polygonal Domains|Let $s$ be a point in a polygonal domain $\mathcal{P}$ of $h-1$ holes and $n$ vertices. We consider a quickest visibility query problem. Given a query point $q$ in $\mathcal{P}$, the goal is to find a shortest path in $\mathcal{P}$ to move from $s$ to see $q$ as quickly as possible. Previously, Arkin et al. (SoCG 2015) built a data structure of size $O(n^22^{\alpha(n)}\log n)$ that can answer each query in $O(K\log^2 n)$ time, where $\alpha(n)$ is the inverse Ackermann function and $K$ is the size of the visibility polygon of $q$ in $\mathcal{P}$ (and $K$ can be $\Theta(n)$ in the worst case). In this paper, we present a new data structure of size $O(n\log h + h^2)$ that can answer each query in $O(h\log h\log n)$ time. Our result improves the previous work when $h$ is relatively small. In particular, if $h$ is a constant, then our result even matches the best result for the simple polygon case (i.e., $h=1$), which is optimal. As a by-product, we also have a new algorithm for a shortest-path-to-segment query problem. Given a query line segment $\tau$ in $\mathcal{P}$, the query seeks a shortest path from $s$ to all points of $\tau$. Previously, Arkin et al. gave a data structure of size $O(n^22^{\alpha(n)}\log n)$ that can answer each query in $O(\log^2 n)$ time, and another data structure of size $O(n^3\log n)$ with $O(\log n)$ query time. We present a data structure of size $O(n)$ with query time $O(h\log \frac{n}{h})$, which also favors small values of $h$ and is optimal when $h=O(1)$.|['Haitao Wang']|['cs.CG', 'cs.DS']
2017-03-16T23:30:37Z|2017-03-08T16:32:05Z|http://arxiv.org/abs/1703.02901v1|http://arxiv.org/pdf/1703.02901v1|Local Equivalence and Intrinsic Metrics between Reeb Graphs|As graphical summaries for topological spaces and maps, Reeb graphs are common objects in the computer graphics or topological data analysis literature. Defining good metrics between these objects has become an important question for applications, where it matters to quantify the extent by which two given Reeb graphs differ. Recent contributions emphasize this aspect, proposing novel distances such as {\em functional distortion} or {\em interleaving} that are provably more discriminative than the so-called {\em bottleneck distance}, being true metrics whereas the latter is only a pseudo-metric. Their main drawback compared to the bottleneck distance is to be comparatively hard (if at all possible) to evaluate. Here we take the opposite view on the problem and show that the bottleneck distance is in fact good enough {\em locally}, in the sense that it is able to discriminate a Reeb graph from any other Reeb graph in a small enough neighborhood, as efficiently as the other metrics do. This suggests considering the {\em intrinsic metrics} induced by these distances, which turn out to be all {\em globally} equivalent. This novel viewpoint on the study of Reeb graphs has a potential impact on applications, where one may not only be interested in discriminating between data but also in interpolating between them.|['Mathieu Carrière', 'Steve Oudot']|['cs.CG', 'math.AT']
2017-03-16T23:30:37Z|2017-03-08T02:12:35Z|http://arxiv.org/abs/1703.02671v1|http://arxiv.org/pdf/1703.02671v1|Symmetric Assembly Puzzles are Hard, Beyond a Few Pieces|We study the complexity of symmetric assembly puzzles: given a collection of simple polygons, can we translate, rotate, and possibly flip them so that their interior-disjoint union is line symmetric? On the negative side, we show that the problem is strongly NP-complete even if the pieces are all polyominos. On the positive side, we show that the problem can be solved in polynomial time if the number of pieces is a fixed constant.|['Erik D. Demaine', 'Matias Korman', 'Jason S. Ku', 'Joseph S. B. Mitchell', 'Yota Otachi', 'André van Renssen', 'Marcel Roeloffzen', 'Ryuhei Uehara', 'Yushi Uno']|['cs.CG']
2017-03-16T23:30:37Z|2017-03-07T23:22:46Z|http://arxiv.org/abs/1703.02637v1|http://arxiv.org/pdf/1703.02637v1|Effective identifiability criteria for tensors and polynomials|A tensor $T$, in a given tensor space, is said to be $h$-identifiable if it admits a unique decomposition as a sum of $h$ rank one tensors. A criterion for $h$-identifiability is called effective if it is satisfied in a dense, open subset of the set of rank $h$ tensors. In this paper we give effective $h$-identifiability criteria for a large class of tensors. We then improve these criteria for some symmetric tensors. For instance, this allows us to give a complete set of effective identifiability criteria for ternary quintic polynomial. Finally, we implement our identifiability algorithms in Macaulay2.|['Alex Massarenti', 'Massimiliano Mella', 'Giovanni Staglianò']|['math.AG', 'cs.CG', '15A69, 15A72, 11P05 (Primary), 14N05, 15A69 (Secondary)']
2017-03-16T23:30:37Z|2017-03-07T08:02:33Z|http://arxiv.org/abs/1703.02261v1|http://arxiv.org/pdf/1703.02261v1|An annotated bibliography on 1-planarity|The notion of 1-planarity is among the most natural and most studied generalizations of graph planarity. A graph is 1-planar if it has an embedding where each edge is crossed by at most another edge. The study of 1-planar graphs dates back to more than fifty years ago and, recently, it has driven increasing attention in the areas of graph theory, graph algorithms, graph drawing, and computational geometry. This annotated bibliography aims to provide a guiding reference to researchers who want to have an overview of the large body of literature about 1-planar graphs. It reviews the current literature covering various research streams about 1-planarity, such as characterization and recognition, combinatorial properties, and geometric representations. As an additional contribution, we offer a list of open problems on 1-planar graphs.|['Stephen Kobourov', 'Giuseppe Liotta', 'Fabrizio Montecchiani']|['cs.CG']
2017-03-16T23:30:37Z|2017-03-06T16:05:05Z|http://arxiv.org/abs/1703.01943v1|http://arxiv.org/pdf/1703.01943v1|Enumeration of $2$-level polytopes|A (convex) polytope $P$ is said to be $2$-level if for every direction of hyperplanes which is facet-defining for $P$, the vertices of $P$ can be covered with two hyperplanes of that direction. The study of these polytopes is motivated by questions in combinatorial optimization and communication complexity, among others. In this paper, we present the first algorithm for enumerating all combinatorial types of $2$-level polytopes of a given dimension $d$, and provide complete experimental results for $d \leqslant 7$. Our approach is inductive: for each fixed $(d-1)$-dimensional $2$-level polytope $P_0$, we enumerate all $d$-dimensional $2$-level polytopes $P$ that have $P_0$ as a facet. This relies on the enumeration of the closed sets of a closure operator over a finite ground set. By varying the prescribed facet $P_0$, we obtain all $2$-level polytopes in dimension $d$.|['Adam Bohn', 'Yuri Faenza', 'Samuel Fiorini', 'Vissarion Fisikopoulos', 'Marco Macchia', 'Kanstantsin Pashkovich']|['math.CO', 'cs.CG', 'cs.DM', 'math.OC', '05A15, 05C17, 52B12, 52B55, 68W05, 90C22']
2017-03-16T23:30:37Z|2017-03-05T23:29:51Z|http://arxiv.org/abs/1703.01691v1|http://arxiv.org/pdf/1703.01691v1|Drawing Planar Graphs with Few Geometric Primitives|We define the visual complexity of a plane graph drawing to be the number of geometric objects needed to represent all its edges. In particular, one object may represent multiple edges (e.g., one needs only one line segment to draw two collinear edges of the same vertex). Let $n$ denote the number of vertices of a graph. We show that trees can be drawn with $3n/4$ straight-line segments on a polynomial grid, and with $n/2$ straight-line segments on a quasi-polynomial grid. Further, we present an algorithm for drawing planar 3-trees with $(8n-17)/3$ segments on an $O(n)\times O(n^2)$ grid. This algorithm can also be used with a small modification to draw maximal outerplanar graphs with $3n/2$ edges on an $O(n)\times O(n^2)$ grid. We also study the problem of drawing maximal planar graphs with circular arcs and provide an algorithm to draw such graphs using only $(5n - 11)/3$ arcs. This provides a significant improvement over the lower bound of $2n$ for line segments for a nontrivial graph class.|['Gregor Hültenschmidt', 'Philipp Kindermann', 'Wouter Meulemans', 'André Schulz']|['cs.CG']
2017-03-16T23:30:37Z|2017-03-05T19:10:17Z|http://arxiv.org/abs/1703.01646v1|http://arxiv.org/pdf/1703.01646v1|A PTAS for TSP with Neighborhoods Among Fat Regions in the Plane|"The Euclidean TSP with neighborhoods (TSPN) problem seeks a shortest tour that visits a given collection of $n$ regions ({\em neighborhoods}). We present the first polynomial-time approximation scheme for TSPN for a set of regions given by arbitrary disjoint fat regions in the plane. This improves substantially upon the known approximation algorithms, and is the first PTAS for TSPN on regions of non-comparable sizes. Our result is based on a novel extension of the $m$-guillotine method. The result applies to regions that are ""fat"" in a very weak sense: each region $P_i$ has area $\Omega([diam(P_i)]^2)$, but is otherwise arbitrary."|['Joseph S. B. Mitchell']|['cs.CG']
2017-03-16T23:30:37Z|2017-03-05T18:24:23Z|http://arxiv.org/abs/1703.01640v1|http://arxiv.org/abs/1703.01640v1|Approximation algorithms for TSP with neighborhoods in the plane|In the Euclidean TSP with neighborhoods (TSPN), we are given a collection of n regions (neighborhoods) and we seek a shortest tour that visits each region. As a generalization of the classical Euclidean TSP, TSPN is also NP-hard. In this paper, we present new approximation results for the TSPN, including (1) a constant-factor approximation algorithm for the case of arbitrary connected neighborhoods having comparable diameters; and (2) a PTAS for the important special case of disjoint unit disk neighborhoods (or nearly disjoint, nearly-unit disks). Our methods also yield improved approximation ratios for various special classes of neighborhoods, which have previously been studied. Further, we give a linear-time O(1)-approximation algorithm for the case of neighborhoods that are (infinite) straight lines.|['Adrian Dumitrescu', 'Joseph S. B. Mitchell']|['cs.CG', 'cs.DS']
2017-03-16T23:30:37Z|2017-03-05T01:41:08Z|http://arxiv.org/abs/1703.01544v1|http://arxiv.org/pdf/1703.01544v1|L-Graphs and Monotone L-Graphs|"In an $\mathsf{L}$-embedding of a graph, each vertex is represented by an $\mathsf{L}$-segment, and two segments intersect each other if and only if the corresponding vertices are adjacent in the graph. If the corner of each $\mathsf{L}$-segment in an $\mathsf{L}$-embedding lies on a straight line, we call it a monotone $\mathsf{L}$-embedding. In this paper we give a full characterization of monotone $\mathsf{L}$-embeddings by introducing a new class of graphs which we call ""non-jumping"" graphs. We show that a graph admits a monotone $\mathsf{L}$-embedding if and only if the graph is a non-jumping graph. Further, we show that outerplanar graphs, convex bipartite graphs, interval graphs, 3-leaf power graphs, and complete graphs are subclasses of non-jumping graphs. Finally, we show that distance-hereditary graphs and $k$-leaf power graphs ($k\le 4$) admit $\mathsf{L}$-embeddings."|['Abu Reyan Ahmed', 'Felice De Luca', 'Sabin Devkota', 'Alon Efrat', 'Md Iqbal Hossain', 'Stephen Kobourov', 'Jixian Li', 'Sammi Abida Salma', 'Eric Welch']|['cs.CG']
2017-03-16T23:30:41Z|2017-03-04T10:59:56Z|http://arxiv.org/abs/1703.01439v1|http://arxiv.org/pdf/1703.01439v1|On the set of optimal homeomorphisms for the natural pseudo-distance   associated with the Lie group S^1|If $\varphi$ and $\psi$ are two continuous real-valued functions defined on a compact topological space $X$ and $G$ is a subgroup of the group of all homeomorphisms of $X$ onto itself, the natural pseudo-distance $d_G(\varphi,\psi)$ is defined as the infimum of $\mathcal{L}(g)=\ \varphi-\psi \circ g \ _\infty$, as $g$ varies in $G$. In this paper, we make a first step towards extending the study of this concept to the case of Lie groups, by assuming $X=G=S^1$. In particular, we study the set of the optimal homeomorphisms for $d_G$, i.e. the elements $\rho_\alpha$ of $S^1$ such that $\mathcal{L}(\rho_\alpha)$ is equal to $d_G(\varphi,\psi)$. As our main results, we give conditions that a homeomorphism has to meet in order to be optimal, and we prove that the set of the optimal homeomorphisms is finite under suitable conditions.|['Alessandro De Gregorio']|['cs.CG', 'math.AT', 'Primary 57S05, Secondary 55N99']
2017-03-16T23:30:41Z|2017-03-01T02:48:12Z|http://arxiv.org/abs/1703.00112v1|http://arxiv.org/pdf/1703.00112v1|Minimum Enclosing Circle of a Set of Static Points with Dynamic Weight   from One Free Point|Given a set $S$ of $n$ static points and a free point $p$ in the Euclidean plane, we study a new variation of the minimum enclosing circle problem, in which a dynamic weight that equals to the reciprocal of the distance from the free point $p$ to the undetermined circle center is included. In this work, we prove the optimal solution of the new problem is unique and lies on the boundary of the farthest-point Voronoi diagram of $S$, once $p$ does not coincide with any vertex of the convex hull of $S$. We propose a tree structure constructed from the boundary of the farthest-point Voronoi diagram and use the hierarchical relationship between edges to locate the optimal solution. The plane could be divide into at most $3n-4$ non-overlapping regions. When $p$ lies in one of the regions, the optimal solution locates at one node or lies on the interior of one edge in the boundary of the farthest-point Voronoi diagram. Moreover, we apply the new variation to calculate the maximum displacement of one point $p$ under the condition that the displacements of points in $S$ are restricted in 2D rigid motion.|['Lei Qiu', 'Yu Zhang', 'Li Zhang']|['cs.CG']
2017-03-16T23:30:41Z|2017-02-28T09:44:01Z|http://arxiv.org/abs/1702.08716v1|http://arxiv.org/pdf/1702.08716v1|On the Relationship between $k$-Planar and $k$-Quasi Planar Graphs|A graph is $k$-planar $(k \geq 1)$ if it can be drawn in the plane such that no edge is crossed more than $k$ times. A graph is $k$-quasi planar $(k \geq 2)$ if it can be drawn in the plane with no $k$ pairwise crossing edges. The families of $k$-planar and $k$-quasi planar graphs have been widely studied in the literature, and several bounds have been proven on their edge density. Nonetheless, only trivial results are known about the relationship between these two graph families. In this paper we prove that, for $k \geq 3$, every $k$-planar graph is $(k+1)$-quasi planar.|['Patrizio Angelini', 'Michael A. Bekos', 'Franz J. Brandenburg', 'Giordano Da Lozzo', 'Giuseppe Di Battista', 'Walter Didimo', 'Giuseppe Liotta', 'Fabrizio Montecchiani', 'Ignaz Rutter']|['cs.CG']
2017-03-16T23:30:41Z|2017-02-28T05:47:10Z|http://arxiv.org/abs/1702.08654v1|http://arxiv.org/pdf/1702.08654v1|An Improved Algorithm for General Position Subset Selection|In the General Position Subset Selection (GPSS) problem, the goal is to find the largest possible subset of a set of points, such that no three of its members are collinear. If $s_{\textrm{GPSS}}$ is the size the optimal solution, $\sqrt{s_{\textrm{GPSS}}}$ is the current best guarantee for the size of the solution obtained using a polynomial time algorithm. In this paper we present an algorithm for GPSS to improve this bound based on the number of collinear pairs of points.|['Ali Gholami Rudi']|['cs.CG', '65D18, 05C69', 'G.2.1; I.3.5; G.2.2']
2017-03-16T23:30:41Z|2017-02-28T02:18:54Z|http://arxiv.org/abs/1702.08607v1|http://arxiv.org/pdf/1702.08607v1|Faster DB-scan and HDB-scan in Low-Dimensional Euclidean Spaces|We present a new algorithm for the widely used density-based clustering method DBscan. Our algorithm computes the DBscan-clustering in $O(n\log n)$ time in $\mathbb{R}^2$, irrespective of the scale parameter $\varepsilon$ (and assuming the second parameter MinPts is set to a fixed constant, as is the case in practice). Experiments show that the new algorithm is not only fast in theory, but that a slightly simplified version is competitive in practice and much less sensitive to the choice of $\varepsilon$ than the original DBscan algorithm. We also present an $O(n\log n)$ randomized algorithm for HDBscan in the plane---HDBscan is a hierarchical version of DBscan introduced recently---and we show how to compute an approximate version of HDBscan in near-linear time in any fixed dimension.|['Mark de Berg', 'Ade Gunawan', 'Marcel Roeloffzen']|['cs.CG']
2017-03-16T23:30:41Z|2017-02-28T01:14:43Z|http://arxiv.org/abs/1702.08593v1|http://arxiv.org/pdf/1702.08593v1|Mind the Gap: A Study in Global Development through Persistent Homology|"The Gapminder project set out to use statistics to dispel simplistic notions about global development. In the same spirit, we use persistent homology, a technique from computational algebraic topology, to explore the relationship between country development and geography. For each country, two statistics, gross domestic product per capita and average life expectancy, were used to quantify the development. Two analyses were performed. The first considers clusters of the countries based on these two statistics, and the second uncovers cycles in the data when combined with geographic network structure. Our analyses reveal that there is not a clear distinction of ""first"" and ""third"" world countries, and we discovered localized development patterns that are invisible in standard representations."|['Andrew Banman', 'Lori Ziegelmeier']|['math.AT', 'cs.CG']
2017-03-16T23:30:41Z|2017-02-27T22:25:57Z|http://arxiv.org/abs/1703.01350v1|http://arxiv.org/pdf/1703.01350v1|Approximate Convex Hulls|We investigate the PPI algorithm as a means for computing ap- proximate convex hull. We explain how the algorithm computes the curvature of points and prove consistency and convergence. We also extend the algorithm to compute approximate convex hulls described in terms of hyperplanes.|['Robert Graham', 'Adam M. Oberman']|['cs.CG', 'math.CO', '05-04']
2017-03-16T23:30:41Z|2017-02-27T17:07:31Z|http://arxiv.org/abs/1702.08380v1|http://arxiv.org/pdf/1702.08380v1|Exploring Increasing-Chord Paths and Trees|A straight-line drawing $\Gamma$ of a graph $G=(V,E)$ is a drawing of $G$ in the Euclidean plane, where every vertex in $G$ is mapped to a distinct point, and every edge in $G$ is mapped to a straight line segment between their endpoints. A path $P$ in $\Gamma$ is called increasing-chord if for every four points (not necessarily vertices) $a,b,c,d$ on $P$ in this order, the Euclidean distance between $b,c$ is at most the Euclidean distance between $a,d$. A spanning tree $T$ rooted at some vertex $r$ in $\Gamma$ is called increasing-chord if $T$ contains an increasing-chord path from $r$ to every vertex in $T$. In this paper we prove that given a vertex $r$ in a straight-line drawing $\Gamma$, it is NP-complete to determine whether $\Gamma$ contains an increasing-chord spanning tree rooted at $r$. We conjecture that finding an increasing-chord path between a pair of vertices in $\Gamma$, which is an intriguing open problem posed by Alamdari et al., is also NP-complete, and show a (non-polynomial) reduction from the 3-SAT problem.|['Yeganeh Bahoo', 'Stephane Durocher', 'Sahar Mehrpour', 'Debajyoti Mondal']|['cs.CG']
2017-03-16T23:30:41Z|2017-02-25T13:55:53Z|http://arxiv.org/abs/1702.07893v1|http://arxiv.org/pdf/1702.07893v1|The Persistent Homotopy Type Distance|We introduce the persistent homotopy type distance dHT to compare real valued functions defined on possibly different homotopy equivalent topological spaces. The underlying idea in the definition of dHT is to measure the minimal shift that is necessary to apply to one of the two functions in order that the sublevel sets of the two functions become homotopically equivalent. This distance is interesting in connection with persistent homology. Indeed, our main result states that dHT still provides an upper bound for the bottleneck distance between the persistence diagrams of the intervening functions. Moreover, because homotopy equivalences are weaker than homeomorphisms, this implies a lifting of the standard stability results provided by the L-infty distance and the natural pseudo-distance dNP. From a different standpoint, we prove that dHT extends the L-infty distance and dNP in two ways. First, we show that, appropriately restricting the category of objects to which dHT applies, it can be made to coincide with the other two distances. Finally, we show that dHT has an interpretation in terms of interleavings that naturally places it in the family of distances used in persistence theory.|['Patrizio Frosini', 'Claudia Landi', 'Facundo Memoli']|['cs.CG', 'math.AT']
2017-03-16T23:30:41Z|2017-02-24T14:06:31Z|http://arxiv.org/abs/1702.07589v1|http://arxiv.org/pdf/1702.07589v1|Generalization of Schnyder woods to orientable surfaces and applications|Schnyder woods are particularly elegant combinatorial structures with numerous applications concerning planar triangulations and more generally 3-connected planar maps. We propose a simple generalization of Schnyder woods from the plane to maps on orientable surfaces of any genus with a special emphasis on the toroidal case. We provide a natural partition of the set of Schnyder woods of a given map into distributive lattices depending on the surface homology. In the toroidal case we show the existence of particular Schnyder woods with some global properties that are useful for optimal encoding or graph drawing purpose.|['Benjamin Lévêque']|['cs.DM', 'cs.CG', 'math.CO']
2017-03-16T23:30:45Z|2017-02-24T12:25:43Z|http://arxiv.org/abs/1702.07555v1|http://arxiv.org/pdf/1702.07555v1|A generalization of crossing families|For a set of points in the plane, a \emph{crossing family} is a set of line segments, each joining two of the points, such that any two line segments cross. We investigate the following generalization of crossing families: a \emph{spoke set} is a set of lines drawn through a point set such that each unbounded region of the induced line arrangement contains at least one point of the point set. We show that every point set has a spoke set of size $\sqrt{\frac{n}{8}}$. We also characterize the matchings obtained by selecting exactly one point in each unbounded region and connecting every such point to the point in the antipodal unbounded region.|['Patrick Schnider']|['cs.CG']
2017-03-16T23:30:45Z|2017-02-23T21:32:10Z|http://arxiv.org/abs/1702.07399v1|http://arxiv.org/pdf/1702.07399v1|An Optimal Algorithm for Computing the Spherical Depth of Points in the   Plane|For a distribution function $F$ on $\mathbb{R}^d$ and a point $q\in \mathbb{R}^d$, the \emph{spherical depth} $\SphD(q;F)$ is defined to be the probability that a point $q$ is contained inside a random closed hyper-ball obtained from a pair of points from $F$. The spherical depth $\SphD(q;S)$ is also defined for an arbitrary data set $S\subseteq \mathbb{R}^d$ and $q\in \mathbb{R}^d$. This definition is based on counting all of the closed hyper-balls, obtained from pairs of points in $S$, that contain $q$. The significant advantage of using the spherical depth in multivariate data analysis is related to its complexity of computation. Unlike most other data depths, the time complexity of the spherical depth grows linearly rather than exponentially in the dimension $d$. The straightforward algorithm for computing the spherical depth in dimension $d$ takes $O(dn^2)$. The main result of this paper is an optimal algorithm that we present for computing the bivariate spherical depth. The algorithm takes $O(n \log n)$ time. By reducing the problem of \textit{Element Uniqueness}, we prove that computing the spherical depth requires $\Omega(n \log n)$ time. Some geometric properties of spherical depth are also investigated in this paper. These properties indicate that \emph{simplicial depth} ($\SD$) (Liu, 1990) is linearly bounded by spherical depth (in particular, $\SphD\geq \frac{2}{3}SD$). To illustrate this relationship between the spherical depth and the simplicial depth, some experimental results are provided. The obtained experimental bound ($\SphD\geq 2\SD$) indicates that, perhaps, a stronger theoretical bound can be achieved.|['David Bremner', 'Rasoul Shahsavarifar']|['cs.CG']
2017-03-16T23:30:45Z|2017-02-22T15:03:08Z|http://arxiv.org/abs/1702.06829v1|http://arxiv.org/pdf/1702.06829v1|A Simple Convex Layers Algorithm|Given a set of $n$ points $P$ in the plane, the first layer $L_1$ of $P$ is formed by the points that appear on $P$'s convex hull. In general, a point belongs to layer $L_i$, if it lies on the convex hull of the set $P \setminus \bigcup_{j<i}\{L_j\}$. The \emph{convex layers problem} is to compute the convex layers $L_i$. Existing algorithms for this problem either do not achieve the optimal $\mathcal{O}\left(n\log n\right)$ runtime and linear space, or are overly complex and difficult to apply in practice. We propose a new algorithm that is both optimal and simple. The simplicity is achieved by independently computing four sets of monotone convex chains in $\mathcal{O}\left(n\log n\right)$ time and linear space. These are then merged in $\mathcal{O}\left(n\log n\right)$ time.|['Raimi A. Rufai', 'Dana S. Richard']|['cs.CG', 'cs.DS', '68W99', 'I.3.5']
2017-03-16T23:30:45Z|2017-02-20T20:14:46Z|http://arxiv.org/abs/1702.06163v1|http://arxiv.org/pdf/1702.06163v1|1-Fan-Bundle-Planar Drawings of Graphs|Edge bundling is an important concept, heavily used for graph visualization purposes. To enable the comparison with other established nearly-planarity models in graph drawing, we formulate a new edge-bundling model which is inspired by the recently introduced fan-planar graphs. In particular, we restrict the bundling to the endsegments of the edges. As in 1-planarity, we call our model 1-fan-bundle-planarity, as we allow at most one crossing per bundle.   For the two variants where we allow either one or, more naturally, both endsegments of each edge to be part of bundles, we present edge density results and consider various recognition questions, not only for general graphs, but also for the outer and 2-layer variants. We conclude with a series of challenging questions.|['Patrizio Angelini', 'Michael A. Bekos', 'Michael Kaufmann', 'Philipp Kindermann', 'Thomas Schneck']|['cs.CG', 'math.CO']
2017-03-16T23:30:45Z|2017-02-20T08:56:40Z|http://arxiv.org/abs/1702.05900v1|http://arxiv.org/pdf/1702.05900v1|$δ$-Greedy $t$-spanner|We introduce a new geometric spanner, $\delta$-Greedy, whose construction is based on a generalization of the known Path-Greedy and Gap-Greedy spanners. The $\delta$-Greedy spanner combines the most desirable properties of geometric spanners both in theory and in practice. More specifically, it has the same theoretical and practical properties as the Path-Greedy spanner: a natural definition, small degree, linear number of edges, low weight, and strong $(1+\varepsilon)$-spanner for every $\varepsilon>0$. The $\delta$-Greedy algorithm is an improvement over the Path-Greedy algorithm with respect to the number of shortest path queries and hence with respect to its construction time. We show how to construct such a spanner for a set of $n$ points in the plane in $O(n^2 \log n)$ time.   The $\delta$-Greedy spanner has an additional parameter, $\delta$, which indicates how close it is to the Path-Greedy spanner on the account of the number of shortest path queries. For $\delta = t$ the output spanner is identical to the Path-Greedy spanner, while the number of shortest path queries is, in practice, linear.   Finally, we show that for a set of $n$ points placed independently at random in a unit square the expected construction time of the $\delta$-Greedy algorithm is $O(n \log n)$. Our analysis indicates that the $\delta$-Greedy spanner gives the best results among the known spanners of expected $O(n \log n)$ time for random point sets. Moreover, the analysis implies that by setting $\delta = t$, the $\delta$-Greedy algorithm provides a spanner identical to the Path-Greedy spanner in expected $O(n \log n)$ time.|['Gali Bar-On', 'Paz Carmi']|['cs.CG']
2017-03-16T23:30:45Z|2017-02-19T15:48:11Z|http://arxiv.org/abs/1702.05760v1|http://arxiv.org/pdf/1702.05760v1|Hypercube LSH for approximate near neighbors|A celebrated technique for finding near neighbors for the angular distance involves using a set of \textit{random} hyperplanes to partition the space into hash regions [Charikar, STOC 2002]. Experiments later showed that using a set of \textit{orthogonal} hyperplanes, thereby partitioning the space into the Voronoi regions induced by a hypercube, leads to even better results [Terasawa and Tanaka, WADS 2007]. However, no theoretical explanation for this improvement was ever given, and it remained unclear how the resulting hypercube hash method scales in high dimensions.   In this work, we provide explicit asymptotics for the collision probabilities when using hypercubes to partition the space. For instance, two near-orthogonal vectors are expected to collide with probability $(\frac{1}{\pi})^{d + o(d)}$ in dimension $d$, compared to $(\frac{1}{2})^d$ when using random hyperplanes. Vectors at angle $\frac{\pi}{3}$ collide with probability $(\frac{\sqrt{3}}{\pi})^{d + o(d)}$, compared to $(\frac{2}{3})^d$ for random hyperplanes, and near-parallel vectors collide with similar asymptotic probabilities in both cases.   For $c$-approximate nearest neighbor searching, this translates to a decrease in the exponent $\rho$ of locality-sensitive hashing (LSH) methods of a factor up to $\log_2(\pi) \approx 1.652$ compared to hyperplane LSH. For $c = 2$, we obtain $\rho \approx 0.302 + o(1)$ for hypercube LSH, improving upon the $\rho \approx 0.377$ for hyperplane LSH. We further describe how to use hypercube LSH in practice, and we consider an example application in the area of lattice algorithms.|['Thijs Laarhoven']|['cs.DS', 'cs.CC', 'cs.CG', 'cs.CR']
2017-03-16T23:30:45Z|2017-02-18T17:02:58Z|http://arxiv.org/abs/1702.05633v1|http://arxiv.org/pdf/1702.05633v1|Approximation Algorithms for Independence and Domination on B$_1$-VPG   and B$_1$-EPG Graphs|A graph $G$ is called B$_k$-VPG (resp., B$_k$-EPG), for some constant $k\geq 0$, if it has a string representation on a grid such that each vertex is an orthogonal path with at most $k$ bends and two vertices are adjacent in $G$ if and only if the corresponding strings intersect (resp., the corresponding strings share at least one grid edge). If two adjacent strings of a B$_k$-VPG graph intersect exactly once, then the graph is called a one-string B$_k$-VPG graph.   In this paper, we study the Maximum Independent Set and Minimum Dominating Set problems on B$_1$-VPG and B$_1$-EPG graphs. We first give a simple $O(\log n)$-approximation algorithm for the Maximum Independent Set problem on B$_1$-VPG graphs, improving the previous $O((\log n)^2)$-approximation algorithm of Lahiri et al. (COCOA 2015). Then, we consider the Minimum Dominating Set problem. We give an $O(1)$-approximation algorithm for this problem on one-string B$_1$-VPG graphs, providing the first constant-factor approximation algorithm for this problem. Moreover, we show that the Minimum Dominating Set problem is APX-hard on B$_1$-EPG graphs, ruling out the possibility of a PTAS unless P=NP. Finally, we give constant-factor approximation algorithms for this problem on two non-trivial subclasses of B$_1$-EPG graphs. To our knowledge, these are the first results for the Minimum Dominating Set problem on B$_1$-EPG graphs, partially answering a question posed by Epstein et al. (WADS 2013).|['Saeed Mehrabi']|['cs.CG']
2017-03-16T23:30:45Z|2017-02-17T16:07:53Z|http://arxiv.org/abs/1702.06188v1|http://arxiv.org/pdf/1702.06188v1|Forest understory trees revealed using sufficiently dense airborne laser   scanning point clouds|Airborne laser scanning (lidar) point clouds can be process to extract tree-level information over large forested landscapes. Existing procedures typically detect more than 90% of overstory trees, yet they barely detect 60% of understory trees because of reduced number of lidar points penetrating the top canopy layer. Although understory trees provide limited financial value, they offer habitat for numerous wildlife species and are important for stand development. Here we model tree identification accuracy according to point cloud density by decomposing lidar point cloud into overstory and multiple understory canopy layers, estimating the fraction of points representing the different layers, and inspecting tree identification accuracy as a function of point density. We show at a density of about 170 pt/m2 understory tree identification accuracy likely plateaus, which we regard as the required point density for reasonable identification of understory trees. Given the advancements of lidar sensor technology, point clouds can feasibly reach the required density to enable effective identification of individual understory trees, ultimately making remote quantification of forest resources more accurate. The layer decomposition methodology can also be adopted for other similar remote sensing or advanced imaging applications such as geological subsurface modelling or biomedical tissue analysis.|['Hamid Hamraz', 'Marco A. Contreras', 'Jun Zhang']|['cs.CV', 'cs.CG']
2017-03-16T23:30:45Z|2017-02-17T14:34:08Z|http://arxiv.org/abs/1702.05358v1|http://arxiv.org/pdf/1702.05358v1|Computational topology of graphs on surfaces|Computational topology is an area that revisits topological problems from an algorithmic point of view, and develops topological tools for improved algorithms. We survey results in computational topology that are concerned with graphs drawn on surfaces. Typical questions include representing surfaces and graphs embedded on them computationally, deciding whether a graph embeds on a surface, solving computational problems related to homotopy, optimizing curves and graphs on surfaces, and solving standard graph algorithm problems more efficiently in the case of surface-embedded graphs.|['Éric Colin de Verdière']|['cs.CG', 'cs.DM', 'cs.DS', 'math.AT', 'math.CO', '68U05, 05C10, 57M15, 68R10', 'F.2.2; G.2.2; I.3.5']
2017-03-16T23:30:45Z|2017-02-17T09:19:02Z|http://arxiv.org/abs/1702.05265v1|http://arxiv.org/pdf/1702.05265v1|T-Shape Visibility Representations of 1-Planar Graphs|A shape visibility representation displays a graph so that each vertex is represented by an orthogonal polygon of a particular shape and for each edge there is a horizontal or vertical line of sight between the polygons assigned to its endvertices. Special shapes are rectangles, L, T, E and H-shapes, and caterpillars. A flat rectangle is a horizontal bar of height $\epsilon>0$. A graph is 1-planar if there is a drawing in the plane such that each edge is crossed at most once and is IC-planar if in addition no two crossing edges share a vertex.   We show that every IC-planar graph has a flat rectangle visibility representation and that every 1-planar graph has a T-shape visibility representation. The representations use quadratic area and can be computed in linear time from a given embedding.|['Franz J. Brandenburg']|['cs.CG', '68R10', 'G.2.2']
2017-03-16T23:30:49Z|2017-02-15T14:59:10Z|http://arxiv.org/abs/1702.04641v1|http://arxiv.org/pdf/1702.04641v1|Filling missing data in point clouds by merging structured and   unstructured point clouds|"Point clouds arising from structured data, mainly as a result of CT scans, provides special properties on the distribution of points and the distances between those. Yet often, the amount of data provided can not compare to unstructured point clouds, i.e. data that arises from 3D light scans or laser scans. This article hereby proposes an approach to extend structured data and enhance the quality by inserting selected points from an unstructured point cloud. The resulting point cloud still has a partial structure that is called ""half-structure"". In this way, missing data that can not be optimally recovered through other surface reconstruction methods can be completed."|['Franziska Lippoldt', 'Hartmut Schwandt']|['cs.CG', 'cs.CV', 'cs.DM', '53A05', 'F.2.2; G.2.1; I.3.5']
2017-03-16T23:30:49Z|2017-02-14T15:20:23Z|http://arxiv.org/abs/1702.04259v1|http://arxiv.org/pdf/1702.04259v1|On the metastable Mabillard-Wagner conjecture|The purpose of this note is to attract attention to the following conjecture (metastable $r$-fold Whitney trick) by clarifying its status as not having a complete proof, in the sense described in the paper.   Assume that $D=D_1\sqcup\ldots\sqcup D_r$ is disjoint union of $r$ disks of dimension $s$, $f:D\to B^d$ a proper PL map such that $f\partial D_1\cap\ldots\cap f\partial D_r=\emptyset$, $rd\ge (r+1)s+3$ and $d\ge s+3$. If the map $$f^r:\partial(D_1\times\ldots\times D_r)\to (B^d)^r-\{(x,x,\ldots,x)\in(B^d)^r\  \ x\in B^d\}$$ extends to $D_1\times\ldots\times D_r$, then there is a PL map $\overline f:D\to B^d$ such that $$\overline f=f \quad\text{on}\quad D_r\cup\partial D\quad\text{and}\quad \overline fD_1\cap\ldots\cap \overline fD_r=\emptyset.$$|['A. Skopenkov']|['math.GT', 'cs.CG', '57Q35, 57R65, 52B99']
2017-03-16T23:30:49Z|2017-02-13T08:54:44Z|http://arxiv.org/abs/1702.03676v1|http://arxiv.org/pdf/1702.03676v1|Epsilon-approximations and epsilon-nets|The use of random samples to approximate properties of geometric configurations has been an influential idea for both combinatorial and algorithmic purposes. This chapter considers two related notions---$\epsilon$-approximations and $\epsilon$-nets---that capture the most important quantitative properties that one would expect from a random sample with respect to an underlying geometric configuration.|['Nabil H. Mustafa', 'Kasturi Varadarajan']|['cs.CG', 'math.CO', 'math.PR']
2017-03-16T23:30:49Z|2017-02-11T01:20:50Z|http://arxiv.org/abs/1702.03364v1|http://arxiv.org/pdf/1702.03364v1|Techniques in Lattice Basis Reduction|The credit on {\it reduction theory} goes back to the work of Lagrange, Gauss, Hermite, Korkin, Zolotarev, and Minkowski. Modern reduction theory is voluminous and includes the work of A. Lenstra, H. Lenstra and L. Lovasz who created the well known LLL algorithm, and many other researchers such as L. Babai and C. P. Schnorr who created significant new variants of basis reduction algorithms. In this paper, we propose and investigate the efficacy of new optimization techniques to be used along with LLL algorithm. The techniques we have proposed are: i) {\it hill climbing (HC)}, ii) {\it lattice diffusion-sub lattice fusion (LDSF)}, and iii) {\it multistage hybrid LDSF-HC}. The first technique relies on the sensitivity of LLL to permutations of the input basis $B$, and optimization ideas over the symmetric group $S_m$ viewed as a metric space. The second technique relies on partitioning the lattice into sublattices, performing basis reduction in the partition sublattice blocks, fusing the sublattices, and repeating. We also point out places where parallel computation can reduce run-times achieving almost linear speedup. The multistage hybrid technique relies on the lattice diffusion and sublattice fusion and hill climbing algorithms.|['Bal K. Khadka', 'Spyros M. Magliveras']|['cs.CG']
2017-03-16T23:30:49Z|2017-02-10T17:54:59Z|http://arxiv.org/abs/1702.03266v1|http://arxiv.org/pdf/1702.03266v1|Two Optimization Problems for Unit Disks|We present an implementation of a recent algorithm to compute shortest-path trees in unit disk graphs in $O(n\log n)$ worst-case time, where $n$ is the number of disks.   In the minimum-separation problem, we are given $n$ unit disks and two points $s$ and $t$, not contained in any of the disks, and we want to compute the minimum number of disks one needs to retain so that any curve connecting $s$ to $t$ intersects some of the retained disks. We present a new algorithm solving this problem in $O(n^2\log^3 n)$ worst-case time and its implementation.|['Sergio Cabello', 'Lazar Milinković']|['cs.CG']
2017-03-16T23:30:49Z|2017-02-10T14:37:00Z|http://arxiv.org/abs/1702.03187v1|http://arxiv.org/pdf/1702.03187v1|On vertices and facets of combinatorial 2-level polytopes|2-level polytopes naturally appear in several areas of pure and applied mathematics, including combinatorial optimization, polyhedral combinatorics, communication complexity, and statistics. In this paper, we present a polyhedral study of 2-level polytopes arising in combinatorial settings. For all the known (to the best of our knowledge) such polytopes P, we show that v(P).f(P) is upper bounded by d2^(d+1). Here v(P) (resp. f(P)) is the number of vertices (resp. facets) of P, and d is its dimension. Whether this holds for all 2-level polytopes was asked in [Bohn et al., ESA 2015], where experimental results showed it true up to dimension 6. The key to most of our proofs is an understanding of the combinatorial structures underlying those polytopes. This leads to a number of results that we believe to be of independent interest: a trade-off formula for the number of cliques and stable sets in a graph; a description of the facets of the base polytope of the 2-sum of matroids; a linear-size description of the base polytope of matroids that are 2-level in terms of cuts of an associated tree. We also give a self-contained proof of the characterization of the last class, a result first obtained by Grande and Sanyal.|['Manuel Aprile', 'Alfonso Cevallos', 'Yuri Faenza']|['math.CO', 'cs.CG']
2017-03-16T23:30:49Z|2017-02-09T14:04:20Z|http://arxiv.org/abs/1702.02838v1|http://arxiv.org/pdf/1702.02838v1|The DTM-signature for a geometric comparison of metric-measure spaces   from samples|In this paper, we introduce the notion of DTM-signature, a measure on R + that can be associated to any metric-measure space. This signature is based on the distance to a measure (DTM) introduced by Chazal, Cohen-Steiner and M\'erigot. It leads to a pseudo-metric between metric-measure spaces, upper-bounded by the Gromov-Wasserstein distance. Under some geometric assumptions, we derive lower bounds for this pseudo-metric. Given two N-samples, we also build an asymptotic statistical test based on the DTM-signature, to reject the hypothesis of equality of the two underlying metric-measure spaces, up to a measure-preserving isometry. We give strong theoretical justifications for this test and propose an algorithm for its implementation.|['Claire Brécheteau']|['cs.CG', 'math.PR', 'math.ST', 'stat.TH']
2017-03-16T23:30:49Z|2017-02-07T01:11:38Z|http://arxiv.org/abs/1702.01836v1|http://arxiv.org/pdf/1702.01836v1|Linear Time Approximation Schemes for Geometric Maximum Coverage|We study approximation algorithms for the following geometric version of the maximum coverage problem: Let $\mathcal{P}$ be a set of $n$ weighted points in the plane. Let $D$ represent a planar object, such as a rectangle, or a disk. We want to place $m$ copies of $D$ such that the sum of the weights of the points in $\mathcal{P}$ covered by these copies is maximized. For any fixed $\varepsilon>0$, we present efficient approximation schemes that can find a $(1-\varepsilon)$-approximation to the optimal solution. In particular, for $m=1$ and for the special case where $D$ is a rectangle, our algorithm runs in time $O(n\log (\frac{1}{\varepsilon}))$, improving on the previous result. For $m>1$ and the rectangular case, our algorithm runs in $O(\frac{n}{\varepsilon}\log (\frac{1}{\varepsilon})+\frac{m}{\varepsilon}\log m +m(\frac{1}{\varepsilon})^{O(\min(\sqrt{m},\frac{1}{\varepsilon}))})$ time. For a more general class of shapes (including disks, polygons with $O(1)$ edges), our algorithm runs in $O(n(\frac{1}{\varepsilon})^{O(1)}+\frac{m}{\epsilon}\log m + m(\frac{1}{\varepsilon})^{O(\min(m,\frac{1}{\varepsilon^2}))})$ time.|['Kai Jin', 'Jian Li', 'Haitao Wang', 'Bowei Zhang', 'Ningye Zhang']|['cs.CG', 'F.2.2']
2017-03-16T23:30:49Z|2017-02-06T21:31:57Z|http://arxiv.org/abs/1702.01799v1|http://arxiv.org/pdf/1702.01799v1|Radial Contour Labeling with Straight Leaders|The usefulness of technical drawings as well as scientific illustrations such as medical drawings of human anatomy essentially depends on the placement of labels that describe all relevant parts of the figure. In order to not spoil or clutter the figure with text, the labels are often placed around the figure and are associated by thin connecting lines to their features, respectively. This labeling technique is known as external label placement.   In this paper we introduce a flexible and general approach for external label placement assuming a contour of the figure prescribing the possible positions of the labels. While much research on external label placement aims for fast labeling procedures for interactive systems, we focus on highest-quality illustrations. Based on interviews with domain experts and a semi-automatic analysis of 202 handmade anatomical drawings, we identify a set of 18 layout quality criteria, naturally not all of equal importance. We design a new geometric label placement algorithm that is based only on the most important criteria. Yet, other criteria can flexibly be included in the algorithm, either as hard constraints not to be violated or as soft constraints whose violation is penalized by a general cost function. We formally prove that our approach yields labelings that satisfy all hard constraints and have minimum overall cost. Introducing several speedup techniques, we further demonstrate how to deploy our approach in practice. In an experimental evaluation on real-world anatomical drawings we show that the resulting labelings are of high quality and can be produced in adequate time.|['Benjamin Niedermann', 'Martin Nöllenburg', 'Ignaz Rutter']|['cs.CG']
2017-03-16T23:30:49Z|2017-02-06T17:38:26Z|http://arxiv.org/abs/1702.01719v1|http://arxiv.org/pdf/1702.01719v1|A 2-Approximation for the Height of Maximal Outerplanar Graph Drawings|In this paper, we study planar drawings of maximal outerplanar graphs with the objective of achieving small height. A recent paper gave an algorithm for such drawings that is within a factor of 4 of the optimum height. In this paper, we substantially improve the approximation factor to become 2. The main ingredient is to define a new parameter of outerplanar graphs (the so-called umbrella depth, obtained by recursively splitting the graph into graphs called umbrellas). We argue that the height of any poly-line drawing must be at least the umbrella depth, and then devise an algorithm that achieves height at most twice the umbrella depth.|['Therese Biedl', 'Philippe Demontigny']|['cs.DS', 'cs.CG']
2017-03-16T23:30:54Z|2017-02-06T08:01:16Z|http://arxiv.org/abs/1702.01524v1|http://arxiv.org/pdf/1702.01524v1|Edge N-Level Sparse Visibility Graphs: Fast Optimal Any-Angle   Pathfinding Using Hierarchical Taut Paths|In the Any-Angle Pathfinding problem, the goal is to find the shortest path between a pair of vertices on a uniform square grid, that is not constrained to any fixed number of possible directions over the grid. Visibility Graphs are a known optimal algorithm for solving the problem with the use of pre-processing. However, Visibility Graphs are known to perform poorly in terms of running time, especially on large, complex maps. In this paper, we introduce two improvements over the Visibility Graph Algorithm to compute optimal paths. Sparse Visibility Graphs (SVGs) are constructed by pruning unnecessary edges from the original Visibility Graph. Edge N-Level Sparse Visibility Graphs (ENLSVGs) is a hierarchical SVG built by iteratively pruning non-taut paths. We also introduce Line-of-Sight Scans, a faster algorithm for building Visibility Graphs over a grid. SVGs run much faster than Visibility Graphs by reducing the average vertex degree. ENLSVGs, a hierarchical algorithm, improves this further, especially on larger maps. On large maps, with the use of pre-processing, these algorithms are orders of magnitude faster than existing algorithms like Visibility Graphs and Theta*.|['Shunhao Oh', 'Hon Wai Leong']|['cs.CG']
2017-03-16T23:30:54Z|2017-02-09T01:46:20Z|http://arxiv.org/abs/1702.01446v2|http://arxiv.org/pdf/1702.01446v2|Efficient Algorithms for k-Regret Minimizing Sets|A regret minimizing set Q is a small size representation of a much larger database P so that user queries executed on Q return answers whose scores are not much worse than those on the full dataset. In particular, a k-regret minimizing set has the property that the regret ratio between the score of the top-1 item in Q and the score of the top-k item in P is minimized, where the score of an item is the inner product of the item's attributes with a user's weight (preference) vector. The problem is challenging because we want to find a single representative set Q whose regret ratio is small with respect to all possible user weight vectors.   We show that k-regret minimization is NP-Complete for all dimensions d >= 3. This settles an open problem from Chester et al. [VLDB 2014], and resolves the complexity status of the problem for all d: the problem is known to have polynomial-time solution for d <= 2. In addition, we propose two new approximation schemes for regret minimization, both with provable guarantees, one based on coresets and another based on hitting sets. We also carry out extensive experimental evaluation, and show that our schemes compute regret-minimizing sets comparable in size to the greedy algorithm proposed in [VLDB 14] but our schemes are significantly faster and scalable to large data sets.|['Pankaj K. Agarwal', 'Nirman Kumar', 'Stavros Sintos', 'Subhash Suri']|['cs.DS', 'cs.CG', 'cs.DB']
2017-03-16T23:30:54Z|2017-02-04T11:51:50Z|http://arxiv.org/abs/1702.01277v1|http://arxiv.org/pdf/1702.01277v1|Geometric Biplane Graphs II: Graph Augmentation|We study biplane graphs drawn on a finite point set $S$ in the plane in general position. This is the family of geometric graphs whose vertex set is $S$ and which can be decomposed into two plane graphs. We show that every sufficiently large point set admits a 5-connected biplane graph and that there are arbitrarily large point sets that do not admit any 6-connected biplane graph. Furthermore, we show that every plane graph (other than a wheel or a fan) can be augmented into a 4-connected biplane graph. However, there are arbitrarily large plane graphs that cannot be augmented to a 5-connected biplane graph by adding pairwise noncrossing edges.|['Alfredo García', 'Ferran Hurtado', 'Matias Korman', 'Inês Matos', 'Maria Saumell', 'Rodrigo I. Silveira', 'Javier Tejel', 'Csaba D. Tóth']|['cs.CG']
2017-03-16T23:30:54Z|2017-02-04T11:51:44Z|http://arxiv.org/abs/1702.01275v1|http://arxiv.org/pdf/1702.01275v1|Geometric Biplane Graphs I: Maximal Graphs|We study biplane graphs drawn on a finite planar point set $S$ in general position. This is the family of geometric graphs whose vertex set is $S$ and can be decomposed into two plane graphs. We show that two maximal biplane graphs---in the sense that no edge can be added while staying biplane---may differ in the number of edges, and we provide an efficient algorithm for adding edges to a biplane graph to make it maximal. We also study extremal properties of maximal biplane graphs such as the maximum number of edges and the largest maximum connectivity over $n$-element point sets.|['Alfredo García', 'Ferran Hurtado', 'Matias Korman', 'Inês Matos', 'Maria Saumell', 'Rodrigo I. Silveira', 'Javier Tejel', 'Csaba D. Tóth']|['cs.CG']
2017-03-16T23:30:54Z|2017-02-02T22:20:25Z|http://arxiv.org/abs/1702.00849v1|http://arxiv.org/pdf/1702.00849v1|On the union complexity of families of axis-parallel rectangles with a   low packing number|Let R be a family of n axis-parallel rectangles with packing number p-1, meaning that among any p of the rectangles, there are two with a non-empty intersection. We show that the union complexity of R is at most O(n+p^2), and that the (<=k)-level complexity of R is at most O(kn+k^2p^2). Both upper bounds are tight.|['Chaya Keller', 'Shakhar Smorodinsky']|['math.CO', 'cs.CG', '52C45, 52C15']
2017-03-16T23:30:54Z|2017-02-01T16:55:41Z|http://arxiv.org/abs/1702.00353v1|http://arxiv.org/pdf/1702.00353v1|The non-cooperative tile assembly model is not intrinsically universal   or capable of bounded Turing machine simulation|The field of algorithmic self-assembly is concerned with the computational and expressive power of nanoscale self-assembling molecular systems. In the well-studied cooperative, or temperature 2, abstract tile assembly model it is known that there is a tile set to simulate any Turing machine and an intrinsically universal tile set that simulates the shapes and dynamics of any instance of the model, up to spatial rescaling. It has been an open question as to whether the seemingly simpler noncooperative, or temperature 1, model is capable of such behaviour. Here we show that this is not the case, by showing that there is no tile set in the noncooperative model that is intrinsically universal, nor one capable of time-bounded Turing machine simulation within a bounded region of the plane.   Although the noncooperative model intuitively seems to lack the complexity and power of the cooperative model it has been exceedingly hard to prove this. One reason is that there have been few tools to analyse the structure of complicated paths in the plane. This paper provides a number of such tools. A second reason is that almost every obvious and small generalisation to the model (e.g. allowing error, 3D, non-square tiles, signals/wires on tiles, tiles that repel each other, parallel synchronous growth) endows it with great computational, and sometimes simulation, power. Our main results show that all of these generalisations provably increase computational and/or simulation power. Our results hold for both deterministic and nondeterministic noncooperative systems. Our first main result stands in stark contrast with the fact that for both the cooperative tile assembly model, and for 3D noncooperative tile assembly, there are respective intrinsically universal tilesets. Our second main result gives a new technique (reduction to simulation) for proving negative results about computation in tile assembly.|['Pierre-Étienne Meunier', 'Damien Woods']|['cs.CC', 'cs.CG', 'cs.DS']
2017-03-16T23:30:54Z|2017-02-01T06:45:40Z|http://arxiv.org/abs/1702.00146v1|http://arxiv.org/pdf/1702.00146v1|Untangling Planar Curves|Any generic closed curve in the plane can be transformed into a simple closed curve by a finite sequence of local transformations called homotopy moves. We prove that simplifying a planar closed curve with $n$ self-crossings requires $\Theta(n^{3/2})$ homotopy moves in the worst case. Our algorithm improves the best previous upper bound $O(n^2)$, which is already implicit in the classical work of Steinitz; the matching lower bound follows from the construction of closed curves with large defect, a topological invariant of generic closed curves introduced by Aicardi and Arnold. Our lower bound also implies that $\Omega(n^{3/2})$ facial electrical transformations are required to reduce any plane graph with treewidth $\Omega(\sqrt{n})$ to a single vertex, matching known upper bounds for rectangular and cylindrical grid graphs. More generally, we prove that transforming one immersion of $k$ circles with at most $n$ self-crossings into another requires $\Theta(n^{3/2} + nk + k^2)$ homotopy moves in the worst case. Finally, we prove that transforming one noncontractible closed curve to another on any orientable surface requires $\Omega(n^2)$ homotopy moves in the worst case; this lower bound is tight if the curve is homotopic to a simple closed curve.|['Hsien-Chih Chang', 'Jeff Erickson']|['cs.CG', 'math.GT']
2017-03-16T23:30:54Z|2017-01-29T19:55:27Z|http://arxiv.org/abs/1701.08423v1|http://arxiv.org/pdf/1701.08423v1|One Size Fits All : Effectiveness of Local Search on Structured Data|In this paper, we analyze the performance of a simple and standard Local Search algorithm for clustering on well behaved data. Since the seminal paper by Ostrovsky, Rabani, Schulman and Swamy [FOCS 2006], much progress has been made to characterize real-world instances. We distinguish the three main definitions -- Distribution Stability (Awasthi, Blum, Sheffet, FOCS 2010) -- Spectral Separability (Kumar, Kannan, FOCS 2010) -- Perturbation Resilience (Bilu, Linial, ICS 2010) We show that Local Search performs well on the instances with the aforementioned stability properties. Specifically, for the $k$-means and $k$-median objective, we show that Local Search exactly recovers the optimal clustering if the dataset is $3+\varepsilon$-perturbation resilient, and is a PTAS for distribution stability and spectral separability. This implies the first PTAS for instances satisfying the spectral separability condition. For the distribution stability condition we also go beyond previous work by showing that the clustering output by the algorithm and the optimal clustering are very similar. This is a significant step toward understanding the success of Local Search heuristics in clustering applications and supports the legitimacy of the stability conditions: They characterize some of the structure of real-world instances that make Local Search a popular heuristic.|['Vincent Cohen-Addad', 'Chris Schwiegelshohn']|['cs.DS', 'cs.CG', 'cs.LG']
2017-03-16T23:30:54Z|2017-01-19T18:01:10Z|http://arxiv.org/abs/1701.05532v1|http://arxiv.org/pdf/1701.05532v1|Tighter Bounds for the Discrepancy of Boxes and Polytopes|Combinatorial discrepancy is a complexity measure of a collection of sets which quantifies how well the sets in the collection can be simultaneously balanced. More precisely, we are given an n-point set $P$, and a collection $\mathcal{F} = \{F_1, ..., F_m\}$ of subsets of $P$, and our goal is color $P$ with two colors, red and blue, so that the largest absolute difference between the number of red elements and the number of blue elements (i.e. the discrepancy) in any $F_i$ is minimized. Combinatorial discrepancy has many applications in mathematics and computer science, including constructions of uniformly distributed point sets, and lower bounds for data structures and private data analysis algorithms.   We investigate the combinatorial discrepancy of geometrically defined set systems, in which $P$ is an n-point set in $d$-dimensional space, and $\mathcal{F}$ is the collection of subsets of $P$ induced by dilations and translations of a fixed convex polytope $B$. Such set systems include sets induced by axis-aligned boxes, whose discrepancy is the subject of the well known Tusnady problem. We prove new discrepancy upper bounds for such set systems by extending the approach based on factorization norms previously used by the author and Matousek. We improve the best known upper bound for the Tusnady problem by a logarithmic factor, using a result of Banaszczyk on signed series of vectors. We extend this improvement to any arbitrary convex polytope B by using a decomposition due to Matousek.|['Aleksandar Nikolov']|['math.CO', 'cs.CG']
2017-03-16T23:30:54Z|2017-01-19T16:24:27Z|http://arxiv.org/abs/1701.05500v1|http://arxiv.org/pdf/1701.05500v1|The number of realizations of a Laman graph|Laman graphs model planar frameworks that are rigid for a general choice of distances between the vertices. There are finitely many ways, up to isometries, to realize a Laman graph in the plane. Such realizations can be seen as solutions of systems of quadratic equations prescribing the distances between pairs of points. Using ideas from algebraic and tropical geometry, we provide a recursion formula for the number of complex solutions of such systems.|['Jose Capco', 'Matteo Gallet', 'Georg Grasegger', 'Christoph Koutschan', 'Niels Lubbes', 'Josef Schicho']|['math.AG', 'cs.CG', 'cs.SC', 'math.CO', '14T05, 14N99, 52C25, 05C99']
2017-03-16T23:30:58Z|2017-01-19T15:33:50Z|http://arxiv.org/abs/1701.05475v1|http://arxiv.org/pdf/1701.05475v1|Irrational Guards are Sometimes Needed|In this paper we study the art gallery problem, which is one of the fundamental problems in computational geometry. The objective is to place a minimum number of guards inside a simple polygon such that the guards together can see the whole polygon. We say that a guard at position $x$ sees a point $y$ if the line segment $xy$ is fully contained in the polygon.   Despite an extensive study of the art gallery problem, it remained an open question whether there are polygons given by integer coordinates that require guard positions with irrational coordinates in any optimal solution. We give a positive answer to this question by constructing a monotone polygon with integer coordinates that can be guarded by three guards only when we allow to place the guards at points with irrational coordinates. Otherwise, four guards are needed. By extending this example, we show that for every $n$, there is polygon which can be guarded by $3n$ guards with irrational coordinates but need $4n$ guards if the coordinates have to be rational. Subsequently, we show that there are rectilinear polygons given by integer coordinates that require guards with irrational coordinates in any optimal solution.|['Mikkel Abrahamsen', 'Anna Adamaszek', 'Tillmann Miltzow']|['cs.CG', 'cs.DM', 'math.CO']
2017-03-16T23:30:58Z|2017-01-19T03:57:28Z|http://arxiv.org/abs/1701.05290v1|http://arxiv.org/pdf/1701.05290v1|Range-efficient consistent sampling and locality-sensitive hashing for   polygons|Locality-sensitive hashing (LSH) is a fundamental technique for similarity search and similarity estimation in high-dimensional spaces. The basic idea is that similar objects should produce hash collisions with probability significantly larger than objects with low similarity. We consider LSH for objects that can be represented as point sets in either one or two dimensions. To make the point sets finite size we consider the subset of points on a grid. Directly applying LSH (e.g. min-wise hashing) to these point sets would require time proportional to the number of points. We seek to achieve time that is much lower than direct approaches.   Technically, we introduce new primitives for range-efficient consistent sampling (of independent interest), and show how to turn such samples into LSH values. Another application of our technique is a data structure for quickly estimating the size of the intersection or union of a set of preprocessed polygons. Curiously, our consistent sampling method uses transformation to a geometric problem.|['Joachim Gudmundsson', 'Rasmus Pagh']|['cs.CG', '68U05', 'F.2.2']
2017-03-16T23:30:58Z|2017-01-19T03:20:51Z|http://arxiv.org/abs/1701.05286v1|http://arxiv.org/pdf/1701.05286v1|Algorithms For Longest Chains In Pseudo- Transitive Graphs|A directed acyclic graph G = (V, E) is pseudo-transitive with respect to a given subset of edges E1, if for any edge ab in E1 and any edge bc in E, we have ac in E. We give algorithms for computing longest chains and demonstrate geometric applications that unify and improves some important past results. (For specific applications see the introduction.)|['Farhad Shahrokhi']|['cs.CG', 'math.CO']
2017-03-16T23:30:58Z|2017-01-18T16:46:08Z|http://arxiv.org/abs/1701.05141v1|http://arxiv.org/pdf/1701.05141v1|The Explicit Corridor Map: A Medial Axis-Based Navigation Mesh for   Multi-Layered Environments|Path planning for walking characters in complicated virtual environments is a fundamental task in simulations and games. In this paper, we present an improved definition of the Explicit Corridor Map (ECM), a navigation mesh that allows efficient path planning and crowd simulation for disk-shaped characters of any radius. The ECM is a medial axis (MA) annotated with nearest-obstacle information. For a planar environment with $n$ obstacle vertices, the ECM has size $O(n)$ and can be computed in $O(n \log n)$ time.   We also introduce multi-layered environments (MLEs), in which multiple planar layers are connected by line segment connections. Typical real-world examples are multi-storey buildings, train stations, and sports stadiums. We define the MA and the ECM for multi-layered environments, based on projected distances on the ground plane. For an MLE with $n$ obstacle points and $k$ connections, the MA has size $O(n)$. We present an improved algorithm that constructs the MA and ECM in $O(n \log n \log k)$ time.   Our implementations show that the ECM can be computed efficiently for large 2D and multi-layered environments, and that it can be used to compute paths within milliseconds. This enables simulations of large virtual crowds of heterogeneous characters in real-time.|['Wouter van Toll', 'Atlas F. Cook IV', 'Marc J. van Kreveld', 'Roland Geraerts']|['cs.CG', 'cs.DS']
2017-03-16T23:30:58Z|2017-01-13T15:08:46Z|http://arxiv.org/abs/1701.03693v1|http://arxiv.org/pdf/1701.03693v1|Multivariate Analysis for Computing Maxima in High Dimensions|We study the problem of computing the \textsc{Maxima} of a set of $n$ $d$-dimensional points. For dimensions 2 and 3, there are algorithms to solve the problem with order-oblivious instance-optimal running time. However, in higher dimensions there is still room for improvements. We present an algorithm sensitive to the structural entropy of the input set, which improves the running time, for large classes of instances, on the best solution for \textsc{Maxima} to date for $d \ge 4$.|['Jérémy Barbay', 'Javiel Rojas']|['cs.CG', 'cs.DS', 'F.2.2']
2017-03-16T23:30:58Z|2017-01-12T16:01:50Z|http://arxiv.org/abs/1701.03388v1|http://arxiv.org/pdf/1701.03388v1|Dynamic and Kinetic Conflict-Free Coloring of Intervals with Respect to   Points|We introduce the dynamic conflict-free coloring problem for a set $S$ of intervals in $\mathbb{R}^1$ with respect to points, where the goal is to maintain a conflict-free coloring for $S$ under insertions and deletions. We investigate trade-offs between the number of colors used and the number of intervals that are recolored upon insertion or deletion of an interval. Our results include:   - a lower bound on the number of recolorings as a function of the number of colors, which implies that with $O(1)$ recolorings per update the worst-case number of colors is $\Omega(\log n/\log\log n)$, and that any strategy using $O(1/\varepsilon)$ colors needs $\Omega(\varepsilon n^{\varepsilon})$ recolorings;   - a coloring strategy that uses $O(\log n)$ colors at the cost of $O(\log n)$ recolorings, and another strategy that uses $O(1/\varepsilon)$ colors at the cost of $O(n^{\varepsilon}/\varepsilon)$ recolorings;   - stronger upper and lower bounds for special cases.   We also consider the kinetic setting where the intervals move continuously (but there are no insertions or deletions); here we show how to maintain a coloring with only four colors at the cost of three recolorings per event and show this is tight.|['Mark de Berg', 'Tim Leijsen', 'André van Renssen', 'Marcel Roeloffzen', 'Aleksandar Markovic', 'Gerhard Woeginger']|['cs.CG']
2017-03-16T23:30:58Z|2017-01-12T04:51:15Z|http://arxiv.org/abs/1701.03230v1|http://arxiv.org/pdf/1701.03230v1|Surface Reconstruction with Data-driven Exemplar Priors|In this paper, we propose a framework to reconstruct 3D models from raw scanned points by learning the prior knowledge of a specific class of objects. Unlike previous work that heuristically specifies particular regularities and defines parametric models, our shape priors are learned directly from existing 3D models under a framework based on affinity propagation. Given a database of 3D models within the same class of objects, we build a comprehensive library of 3D local shape priors. We then formulate the problem to select as-few-as-possible priors from the library, referred to as exemplar priors. These priors are sufficient to represent the 3D shapes of the whole class of objects from where they are generated. By manipulating these priors, we are able to reconstruct geometrically faithful models with the same class of objects from raw point clouds. Our framework can be easily generalized to reconstruct various categories of 3D objects that have more geometrically or topologically complex structures. Comprehensive experiments exhibit the power of our exemplar priors for gracefully solving several problems in 3D shape reconstruction such as preserving sharp features, recovering fine details and so on.|['Oussama Remil', 'Qian Xie', 'Xingyu Xie', 'Kai Xu', 'Jun Wang']|['cs.CG', 'cs.GR']
2017-03-16T23:30:58Z|2017-01-11T04:04:43Z|http://arxiv.org/abs/1701.02843v1|http://arxiv.org/pdf/1701.02843v1|Solve Partial Differential Equations on Manifold From Incomplete   Inter-Point Distance|Solutions of partial differential equations (PDEs) on manifolds have provided important applications in different fields in science and engineering. Existing methods are majorly based on discretization of manifolds as implicit functions, triangle meshes, or point clouds, where the manifold structure is approximated by either zero level set of an implicit function or a set of points. In many applications, manifolds might be only provided as an inter-point distance matrix with possible missing values. This paper discusses a framework to discretize PDEs on manifolds represented as incomplete distance information. Without conducting a time-consuming global coordinates reconstruction, we propose a more efficient strategy by discretizing differential operators only based on point-wisely local reconstruction. Our local reconstruction model is based on the recent advances of low-rank matrix completion theory, where only a very small random portion of distance information is required. This method enables us to conduct analyses of incomplete distance data using solutions of special designed PDEs such as the Laplace-Beltrami (LB) eigen-system. As an application, we demonstrate a new way of manifold reconstruction from an incomplete distance by stitching patches using the spectrum of the LB operator. Intensive numerical experiments demonstrate the effectiveness of the proposed methods.|['Rongjie Lai', 'Jia Li']|['math.NA', 'cs.CG', '65D18, 65D25, 65N25']
2017-03-16T23:30:58Z|2017-01-09T16:17:10Z|http://arxiv.org/abs/1701.02229v1|http://arxiv.org/pdf/1701.02229v1|Searching edges in the overlap of two plane graphs|Consider a pair of plane straight-line graphs, whose edges are colored red and blue, respectively, and let n be the total complexity of both graphs. We present a O(n log n)-time O(n)-space technique to preprocess such pair of graphs, that enables efficient searches among the red-blue intersections along edges of one of the graphs. Our technique has a number of applications to geometric problems. This includes: (1) a solution to the batched red-blue search problem [Dehne et al. 2006] in O(n log n) queries to the oracle; (2) an algorithm to compute the maximum vertical distance between a pair of 3D polyhedral terrains one of which is convex in O(n log n) time, where n is the total complexity of both terrains; (3) an algorithm to construct the Hausdorff Voronoi diagram of a family of point clusters in the plane in O((n+m) log^3 n) time and O(n+m) space, where n is the total number of points in all clusters and m is the number of crossings between all clusters; (4) an algorithm to construct the farthest-color Voronoi diagram of the corners of n axis-aligned rectangles in O(n log^2 n) time; (5) an algorithm to solve the stabbing circle problem for n parallel line segments in the plane in optimal O(n log n) time. All these results are new or improve on the best known algorithms.|['John Iacono', 'Elena Khramtcova', 'Stefan Langerman']|['cs.CG']
2017-03-16T23:30:58Z|2017-01-09T15:18:28Z|http://arxiv.org/abs/1701.02208v1|http://arxiv.org/pdf/1701.02208v1|Barcodes of Towers and a Streaming Algorithm for Persistent Homology|A tower is a sequence of simplicial complexes connected by simplicial maps. We show how to compute a filtration, a sequence of nested simplicial complexes, with the same persistent barcode as the tower. Our approach is based on the coning strategy by Dey et al. (SoCG 2014). We show that a variant of this approach yields a filtration that is asymptotically only marginally larger than the tower and can be efficiently computed by a streaming algorithm, both in theory and in practice. Furthermore, we show that our approach can be combined with a streaming algorithm to compute the barcode of the tower via matrix reduction. The space complexity of the algorithm does not depend on the length of the tower, but the maximal size of any subcomplex within the tower.|['Michael Kerber', 'Hannah Schreiber']|['math.AT', 'cs.CG']
2017-03-16T23:31:02Z|2017-02-22T20:40:08Z|http://arxiv.org/abs/1701.02200v2|http://arxiv.org/pdf/1701.02200v2|Bounding a global red-blue proportion using local conditions|"We study the following local-to-global phenomenon: Let $B$ and $R$ be two finite sets of (blue and red) points in the Euclidean plane $\mathbb{R}^2$. Suppose that in each ""neighborhood"" of a red point, the number of blue points is at least as large as the number of red points. We show that in this case the total number of blue points is at least one fifth of the total number of red points. We also show that this bound is optimal and we generalize the result to arbitrary dimension and arbitrary norm using results from Minkowski arrangements."|['Márton Naszódi', 'Leonardo Martínez Sandoval', 'Shakhar Smorodinsky']|['cs.CG']
2017-03-16T23:31:02Z|2017-01-05T12:00:37Z|http://arxiv.org/abs/1701.06430v1|http://arxiv.org/pdf/1701.06430v1|An Upper Bound of the Minimal Dispersion via Delta Covers|For a point set of $n$ elements in the $d$-dimensional unit cube and a class of test sets we are interested in the largest volume of a test set which does not contain any point. For all natural numbers $n$, $d$ and under the assumption of a $delta$-cover with cardinality $\vert \Gamma_\delta \vert$ we prove that there is a point set, such that the largest volume of such a test set without any point is bounded by $\frac{\log \vert \Gamma_\delta \vert}{n} + \delta$. For axis-parallel boxes on the unit cube this leads to a volume of at most $\frac{4d}{n}\log(\frac{9n}{d})$ and on the torus to $\frac{4d}{n}\log (2n)$.|['Daniel Rudolf']|['cs.CG', 'math.NA', '52B55, 68Q25']
2017-03-16T23:31:02Z|2017-01-04T07:44:19Z|http://arxiv.org/abs/1701.00921v1|http://arxiv.org/pdf/1701.00921v1|Towards Faithful Graph Visualizations|Readability criteria have been addressed as a measurement of the quality of graph visualizations. In this paper, we argue that readability criteria are necessary but not sufficient. We propose a new kind of criteria, namely faithfulness, to evaluate the quality of graph layouts. We introduce a general model for quantify faithfulness, and contrast it with the well established readability criteria. We show examples of common visualization techniques, such as multidimensional scaling, edge bundling and several other visualization metaphors for the study of faithfulness.|['Quan Hoang Nguyen', 'Peter Eades', 'Seok-Hee Hong']|['cs.CG']
2017-03-16T23:31:02Z|2017-01-03T12:41:18Z|http://arxiv.org/abs/1701.00679v1|http://arxiv.org/pdf/1701.00679v1|Removing Depth-Order Cycles Among Triangles: An Efficient Algorithm   Generating Triangular Fragments|We present an algorithm that cuts any collection of n disjoint triangles in R^3 into O(n^{7/4} polylog n) triangular fragments such that all cycles in the depth-order relation are eliminated. The running time of our algorithm is O(n^{3.69}). We also prove a refined bound, which depends on the number, K, of intersections between the projections of the triangle edges onto the xy-plane. More precisely, we show that O(n^{1+\eps} + n^{1/4} K^{3/4} polylog n) fragments suffice to eliminate all cycles. This result extends to xy-monotone surface patches bounded by a constant number of bounded-degree algebraic arcs in general position.|['Mark de Berg']|['cs.CG', 'F.2.2']
2017-03-16T23:31:02Z|2017-01-02T16:45:18Z|http://arxiv.org/abs/1701.00441v1|http://arxiv.org/pdf/1701.00441v1|Collecting a Swarm in a Grid Environment Using Shared, Global Inputs|This paper investigates efficient techniques to collect and concentrate an under-actuated particle swarm despite obstacles. Concentrating a swarm of particles is of critical importance in health-care for targeted drug delivery, where micro-scale particles must be steered to a goal location. Individual particles must be small in order to navigate through micro-vasculature, but decreasing size brings new challenges. Individual particles are too small to contain on-board power or computation and are instead controlled by a global input, such as an applied fluidic flow or electric field.   To make progress, this paper considers a swarm of robots initialized in a grid world in which each position is either free-space or obstacle. This paper provides algorithms that collect all the robots to one position and compares these algorithms on the basis of efficiency and implementation time.|['Arun V. Mahadev', 'Dominik Krupke', 'Jan-Marc Reinhardt', 'Sándor P. Fekete', 'Aaron T. Becker']|['cs.RO', 'cs.CG', 'I.2.11; F.2.2']
2017-03-16T23:31:02Z|2017-01-01T04:49:47Z|http://arxiv.org/abs/1701.00198v1|http://arxiv.org/abs/1701.00198v1|A robust approach for tree segmentation in deciduous forests using   small-footprint airborne LiDAR data|This paper presents a non-parametric approach for segmenting trees from airborne LiDAR data in deciduous forests. Based on the LiDAR point cloud, the approach collects crown information such as steepness and height on-the-fly to delineate crown boundaries, and most importantly, does not require a priori assumptions of crown shape and size. The approach segments trees iteratively starting from the tallest within a given area to the smallest until all trees have been segmented. To evaluate its performance, the approach was applied to the University of Kentucky Robinson Forest, a deciduous closed-canopy forest with complex terrain and vegetation conditions. The approach identified 94% of dominant and co-dominant trees with a false detection rate of 13%. About 62% of intermediate, overtopped, and dead trees were also detected with a false detection rate of 15%. The overall segmentation accuracy was 77%. Correlations of the segmentation scores of the proposed approach with local terrain and stand metrics was not significant, which is likely an indication of the robustness of the approach as results are not sensitive to the differences in terrain and stand structures.|['Hamid Hamraz', 'Marco A. Contreras', 'Jun Zhang']|['cs.CV', 'cs.CE', 'cs.CG']
2017-03-16T23:31:02Z|2016-12-31T21:53:09Z|http://arxiv.org/abs/1701.00169v1|http://arxiv.org/pdf/1701.00169v1|Tree segmentation in multi-story stands within small-footprint airborne   LiDAR data|Airborne LiDAR point cloud of a forest contains three dimensional data, from which vertical stand structure (including information about under-story trees) can be derived. This paper presents a segmentation approach for multi-story stands that strips the point cloud to its canopy layers, identifies individual tree segments within each layer using a DSM-based tree identification method as a building block, and combines the segments of immediate layers in order to fix potential over-segmentation of tree crowns across the layers. We introduce local layering that analyzes the vertical distributions of LiDAR points within their local neighborhoods in order to locally determine the height thresholds for layering the canopy. Unlike the previous work that stripped stiff layers within constrained areas, the local layering method strips flexible (in thickness and elevation) and narrower canopy layers within unconstrained areas. Statistical analyses showed that layering in general strongly improves identifying (specifically under-story) trees for the cost of moderately increasing over-segmentation rate of the identified trees. Combining tree segments across the immediate layers did not seem to improve tree identification accuracy remarkably, suggesting that layers separated canopy layers rather precisely.|['Hamid Hamraz', 'Marco A. Contreras', 'Jun Zhang']|['cs.CV', 'cs.CE', 'cs.CG']
2017-03-16T23:31:02Z|2016-12-31T17:05:53Z|http://arxiv.org/abs/1701.00146v1|http://arxiv.org/pdf/1701.00146v1|Even $1 \times n$ Edge-Matching and Jigsaw Puzzles are Really Hard|We prove the computational intractability of rotating and placing $n$ square tiles into a $1 \times n$ array such that adjacent tiles are compatible--either equal edge colors, as in edge-matching puzzles, or matching tab/pocket shapes, as in jigsaw puzzles. Beyond basic NP-hardness, we prove that it is NP-hard even to approximately maximize the number of placed tiles (allowing blanks), while satisfying the compatibility constraint between nonblank tiles, within a factor of 0.9999999851. (On the other hand, there is an easy $1 \over 2$-approximation.) This is the first (correct) proof of inapproximability for edge-matching and jigsaw puzzles. Along the way, we prove NP-hardness of distinguishing, for a directed graph on $n$ nodes, between having a Hamiltonian path (length $n-1$) and having at most $0.999999284 (n-1)$ edges that form a vertex-disjoint union of paths. We use this gap hardness and gap-preserving reductions to establish similar gap hardness for $1 \times n$ jigsaw and edge-matching puzzles.|['Jeffrey Bosboom', 'Erik D. Demaine', 'Martin L. Demaine', 'Adam Hesterberg', 'Pasin Manurangsi', 'Anak Yodpinyanee']|['cs.CC', 'cs.CG']
2017-03-16T23:31:02Z|2016-12-30T09:33:07Z|http://arxiv.org/abs/1612.09434v1|http://arxiv.org/pdf/1612.09434v1|Data driven estimation of Laplace-Beltrami operator|Approximations of Laplace-Beltrami operators on manifolds through graph Lapla-cians have become popular tools in data analysis and machine learning. These discretized operators usually depend on bandwidth parameters whose tuning remains a theoretical and practical problem. In this paper, we address this problem for the unnormalized graph Laplacian by establishing an oracle inequality that opens the door to a well-founded data-driven procedure for the bandwidth selection. Our approach relies on recent results by Lacour and Massart [LM15] on the so-called Lepski's method.|['Frédéric Chazal', 'Ilaria Giulini', 'Bertrand Michel']|['cs.CG', 'cs.LG', 'math.ST', 'stat.TH']
2017-03-16T23:31:02Z|2017-01-02T14:15:21Z|http://arxiv.org/abs/1612.09277v2|http://arxiv.org/pdf/1612.09277v2|On Planar Greedy Drawings of 3-Connected Planar Graphs|"A graph drawing is $\textit{greedy}$ if, for every ordered pair of vertices $(x,y)$, there is a path from $x$ to $y$ such that the Euclidean distance to $y$ decreases monotonically at every vertex of the path. Greedy drawings support a simple geometric routing scheme, in which any node that has to send a packet to a destination ""greedily"" forwards the packet to any neighbor that is closer to the destination than itself, according to the Euclidean distance in the drawing. In a greedy drawing such a neighbor always exists and hence this routing scheme is guaranteed to succeed.   In 2004 Papadimitriou and Ratajczak stated two conjectures related to greedy drawings. The $\textit{greedy embedding conjecture}$ states that every $3$-connected planar graph admits a greedy drawing. The $\textit{convex greedy embedding conjecture}$ asserts that every $3$-connected planar graph admits a planar greedy drawing in which the faces are delimited by convex polygons. In 2008 the greedy embedding conjecture was settled in the positive by Leighton and Moitra.   In this paper we prove that every $3$-connected planar graph admits a $\textit{planar}$ greedy drawing. Apart from being a strengthening of Leighton and Moitra's result, this theorem constitutes a natural intermediate step towards a proof of the convex greedy embedding conjecture."|"['Giordano Da Lozzo', ""Anthony D'Angelo"", 'Fabrizio Frati']"|['cs.CG']
2017-03-16T23:31:06Z|2016-12-24T09:00:37Z|http://arxiv.org/abs/1612.08153v1|http://arxiv.org/pdf/1612.08153v1|EgoReID: Cross-view Self-Identification and Human Re-identification in   Egocentric and Surveillance Videos|Human identification remains to be one of the challenging tasks in computer vision community due to drastic changes in visual features across different viewpoints, lighting conditions, occlusion, etc. Most of the literature has been focused on exploring human re-identification across viewpoints that are not too drastically different in nature. Cameras usually capture oblique or side views of humans, leaving room for a lot of geometric and visual reasoning. Given the recent popularity of egocentric and top-view vision, re-identification across these two drastically different views can now be explored. Having an egocentric and a top view video, our goal is to identify the cameraman in the content of the top-view video, and also re-identify the people visible in the egocentric video, by matching them to the identities present in the top-view video. We propose a CRF-based method to address the two problems. Our experimental results demonstrates the efficiency of the proposed approach over a variety of video recorded from two views.|['Shervin Ardeshir', 'Sandesh Sharma', 'Ali Broji']|['cs.CV', 'cs.CG']
2017-03-16T23:31:06Z|2016-12-22T00:55:29Z|http://arxiv.org/abs/1612.07405v1|http://arxiv.org/pdf/1612.07405v1|Practical linear-space Approximate Near Neighbors in high dimension|The $c$-approximate Near Neighbor problem in high dimensional spaces has been mainly addressed by Locality Sensitive Hashing (LSH), which offers polynomial dependence on the dimension, query time sublinear in the size of the dataset, and subquadratic space requirement. For practical applications, linear space is typically imperative. Most previous work in the linear space regime focuses on the case that $c$ exceeds $1$ by a constant term. In a recently accepted paper, optimal bounds have been achieved for any $c>1$ \cite{ALRW17}.   Towards practicality, we present a new and simple data structure using linear space and sublinear query time for any $c>1$ including $c\to 1^+$. Given an LSH family of functions for some metric space, we randomly project points to the Hamming cube of dimension $\log n$, where $n$ is the number of input points. The projected space contains strings which serve as keys for buckets containing the input points. The query algorithm simply projects the query point, then examines points which are assigned to the same or nearby vertices on the Hamming cube. We analyze in detail the query time for some standard LSH families.   To illustrate our claim of practicality, we offer an open-source implementation in {\tt C++}, and report on several experiments in dimension up to 1000 and $n$ up to $10^6$. Our algorithm is one to two orders of magnitude faster than brute force search. Experiments confirm the sublinear dependence on $n$ and the linear dependence on the dimension. We have compared against state-of-the-art LSH-based library {\tt FALCONN}: our search is somewhat slower, but memory usage and preprocessing time are significantly smaller.|['Georgia Avarikioti', 'Ioannis Z. Emiris', 'Ioannis Psarros', 'Georgios Samaras']|['cs.CG']
2017-03-16T23:31:06Z|2016-12-21T18:58:17Z|http://arxiv.org/abs/1612.07276v1|http://arxiv.org/pdf/1612.07276v1|Splitting $B_2$-VPG graphs into outer-string and co-comparability graphs|In this paper, we show that any $B_2$-VPG graph (i.e., an intersection graph of orthogonal curves with at most 2 bends) can be decomposed into $O(\log n)$ outerstring graphs or $O(\log^3 n)$ permutation graphs. This leads to better approximation algorithms for hereditary graph problems, such as independent set, clique and clique cover, on $B_2$-VPG graphs.|['Martin Derka', 'Therese Biedl']|['cs.CG', 'cs.DS']
2017-03-16T23:31:06Z|2017-02-27T10:00:30Z|http://arxiv.org/abs/1701.00541v2|http://arxiv.org/pdf/1701.00541v2|Packing Unequal Circles into a Square Container by Partitioning Narrow   Action Spaces and Circle Items|We address the NP-hard problem of finding a non-overlapping dense packing pattern for n Unequal Circle items in a two-dimensional Square Container (PUC-SC) such that the size of the container is minimized. Based on our previous work on an Action Space based Global Optimization (ASGO) that approximates each circle item as a square item to efficiently find the large unoccupied spaces, we propose an optimization algorithm based on the Partitioned Action Space and Partitioned Circle Items (PAS-PCI). The PAS is to partition the narrow action space on the long side to find two equal action spaces to fully utilize the unoccupied spaces. The PCI is to partition the circle items into four groups based on size for the basin hopping strategy. Experiments on two sets of benchmark instances show the effectiveness of the proposed method. In comparison with our previous algorithm ASGO on the 68 tested instances that ASGO published, PAS-PCI not only gains smaller containers in 64 instances and matches the other 4 but also runs faster in most instances. In comparison with the best record of the Packomania website on a total of 98 instances, PAS-PCI finds smaller containers on 82 and matches the other 16. Note that we updated 19 records for (47-48, 51-54, 57, 61-72) that had been kept unchanged since 2013.|['Kun He', 'Mohammed Dosh', 'Shenghao Zou']|['cs.CG', 'cs.DS', '52C26']
2017-03-16T23:31:06Z|2017-01-03T14:51:30Z|http://arxiv.org/abs/1612.06954v2|http://arxiv.org/pdf/1612.06954v2|Colored stochastic dominance problems|In this paper, we study the dominance relation under a stochastic setting. Let $\mathcal{S}$ be a set of $n$ colored stochastic points in $\mathbb{R}^d$, each of which is associated with an existence probability. We investigate the problem of computing the probability that a realization of $\mathcal{S}$ contains inter-color dominances, which we call the \textit{colored stochastic dominance} (CSD) problem. We propose the first algorithm to solve the CSD problem for $d=2$ in $O(n^2 \log^2 n)$ time. On the other hand, we prove that, for $d \geq 3$, even the CSD problem with a restricted color pattern is #P-hard. In addition, even if the existence probabilities are restricted to be $\frac{1}{2}$, the problem remains #P-hard for $d \geq 7$. A simple FPRAS is then provided to approximate the desired probability in any dimension. We also study a variant of the CSD problem in which the dominance relation is considered with respect to not only the standard basis but any orthogonal basis of $\mathbb{R}^d$. Specifically, this variant, which we call the {\em free-basis colored stochastic dominance} (FBCSD) problem, considers the probability that a realization of $\mathcal{S}$ contains inter-color dominances with respect to any orthogonal basis of $\mathbb{R}^d$. We show that the CSD problem is polynomial-time reducible to the FBCSD problem in the same dimension, which proves the #P-hardness of the latter for $d \geq 3$. Conversely, we reduce the FBCSD problem in $\mathbb{R}^2$ to the CSD problem in $\mathbb{R}^2$, by which an $O(n^4 \log^2 n)$ time algorithm for the former is obtained.|['Jie Xue', 'Yuan Li']|['cs.CG', 'F.2.2']
2017-03-16T23:31:06Z|2017-03-09T14:09:06Z|http://arxiv.org/abs/1612.05101v2|http://arxiv.org/pdf/1612.05101v2|Open problem on risk-aware planning in the plane|"We consider the problem of planning a collision-free path of a robot in the presence of risk zones. The robot is allowed to travel in these zones but is penalized in a super-linear fashion for consecutive accumulative time spent there. We recently suggested a natural cost function that balances path length and risk-exposure time. When no risk zones exists, our problem resorts to computing minimal-length paths which is known to be computationally hard in the number of dimensions. It is well known that in two-dimensions computing minimal-length paths can be done efficiently. Thus, a natural question we pose is ""Is our problem computationally hard or not?"" If the problem is hard, we wish to find an approximation algorithm to compute a near-optimal path. If not, then a polynomial-time algorithm should be found."|['Oren Salzman', 'Siddhartha Srinivasa']|['cs.CG']
2017-03-16T23:31:06Z|2016-12-15T00:17:43Z|http://arxiv.org/abs/1612.04890v1|http://arxiv.org/pdf/1612.04890v1|Stochastic closest-pair problem and most-likely nearest-neighbor search   in tree spaces|Let $T$ be a tree space (or tree network) represented by a weighted tree with $t$ vertices, and $S$ be a set of $n$ stochastic points in $T$, each of which has a fixed location with an independent existence probability. We investigate two fundamental problems under such a stochastic setting, the closest-pair problem and the nearest-neighbor search. For the former, we study the computation of the $\ell$-threshold probability and the expectation of the closest-pair distance of a realization of $S$. We propose the first algorithm to compute the $\ell$-threshold probability in $O(t+n\log n+ \min\{tn,n^2\})$ time for any given threshold $\ell$, which immediately results in an $O(t+\min\{tn^3,n^4\})$-time algorithm for computing the expected closest-pair distance. Based on this, we further show that one can compute a $(1+\varepsilon)$-approximation for the expected closest-pair distance in $O(t+\varepsilon^{-1}\min\{tn^2,n^3\})$ time, by arguing that the expected closest-pair distance can be approximated via $O(\varepsilon^{-1}n)$ threshold probability queries. For the latter, we study the $k$ most-likely nearest-neighbor search ($k$-LNN) via a notion called $k$ most-likely Voronoi Diagram ($k$-LVD). We show that the size of the $k$-LVD $\varPsi_T^S$ of $S$ on $T$ is bounded by $O(kn)$ if the existence probabilities of the points in $S$ are constant-far from 0. Furthermore, we establish an $O(kn)$ average-case upper bound for the size of $\varPsi_T^S$, by regarding the existence probabilities as i.i.d. random variables drawn from some fixed distribution. Our results imply the existence of an LVD data structure which answers $k$-LNN queries in $O(\log n+k)$ time using average-case $O(t+k^2n)$ space, and worst-case $O(t+kn^2)$ space if the existence probabilities are constant-far from 0. Finally, we also give an $O(t+ n^2\log n+n^2k)$-time algorithm to construct the LVD data structure.|['Jie Xue', 'Yuan Li']|['cs.CG', 'F.2.2']
2017-03-16T23:31:06Z|2016-12-14T19:33:00Z|http://arxiv.org/abs/1612.04780v1|http://arxiv.org/pdf/1612.04780v1|Minimum Weight Connectivity Augmentation for Planar Straight-Line Graphs|We consider edge insertion and deletion operations that increase the connectivity of a given planar straight-line graph (PSLG), while minimizing the total edge length of the output. We show that every connected PSLG $G=(V,E)$ in general position can be augmented to a 2-connected PSLG $(V,E\cup E^+)$ by adding new edges of total Euclidean length $\ E^+\ \leq 2\ E\ $, and this bound is the best possible. An optimal edge set $E^+$ can be computed in $O( V ^4)$ time; however the problem becomes NP-hard when $G$ is disconnected. Further, there is a sequence of edge insertions and deletions that transforms a connected PSLG $G=(V,E)$ into a planar straight-line cycle $G'=(V,E')$ such that $\ E'\ \leq 2\ {\rm MST}(V)\ $, and the graph remains connected with edge length below $\ E\ +\ {\rm MST}(V)\ $ at all stages. These bounds are the best possible.|['Hugo A. Akitaya', 'Rajasekhar Inkulu', 'Torrie L. Nichols', 'Diane L. Souvaine', 'Csaba D. Tóth', 'Charles R. Winston']|['cs.CG', '05C40, 05C85, 68R10', 'I.3.5']
2017-03-16T23:31:06Z|2016-12-14T19:06:05Z|http://arxiv.org/abs/1612.04774v1|http://arxiv.org/pdf/1612.04774v1|Beam Search for Learning a Deep Convolutional Neural Network of 3D   Shapes|This paper addresses 3D shape recognition. Recent work typically represents a 3D shape as a set of binary variables corresponding to 3D voxels of a uniform 3D grid centered on the shape, and resorts to deep convolutional neural networks(CNNs) for modeling these binary variables. Robust learning of such CNNs is currently limited by the small datasets of 3D shapes available, an order of magnitude smaller than other common datasets in computer vision. Related work typically deals with the small training datasets using a number of ad hoc, hand-tuning strategies. To address this issue, we formulate CNN learning as a beam search aimed at identifying an optimal CNN architecture, namely, the number of layers, nodes, and their connectivity in the network, as well as estimating parameters of such an optimal CNN. Each state of the beam search corresponds to a candidate CNN. Two types of actions are defined to add new convolutional filters or new convolutional layers to a parent CNN, and thus transition to children states. The utility function of each action is efficiently computed by transferring parameter values of the parent CNN to its children, thereby enabling an efficient beam search. Our experimental evaluation on the 3D ModelNet dataset demonstrates that our model pursuit using the beam search yields a CNN with superior performance on 3D shape classification than the state of the art.|['Xu Xu', 'Sinisa Todorovic']|['cs.CV', 'cs.CG']
2017-03-16T23:31:06Z|2016-12-14T10:52:01Z|http://arxiv.org/abs/1612.04571v1|http://arxiv.org/pdf/1612.04571v1|A Refined Analysis of LSH for Well-dispersed Data Points|Near neighbor problems are fundamental in algorithms for high-dimensional Euclidean spaces. While classical approaches suffer from the curse of dimensionality, locality sensitive hashing (LSH) can effectively solve a-approximate r-near neighbor problem, and has been proven to be optimal in the worst case. However, for real-world data sets, LSH can naturally benefit from well-dispersed data and low doubling dimension, leading to significantly improved performance. In this paper, we address this issue and propose a refined analyses for running time of approximating near neighbors queries via LSH. We characterize dispersion of data using N_b, the number of b*r-near pairs among the data points. Combined with optimal data-oblivious LSH scheme, we get a new query time bound depending on N_b and doubling dimension. For many natural scenarios where points are well-dispersed or lying in a low-doubling-dimension space, our result leads to sharper performance than existing worst-case analysis. This paper not only present first rigorous proof on how LSHs make use of the structure of data points, but also provide important insights into parameter setting in the practice of LSH beyond worst case. Besides, the techniques in our analysis involve a generalized version of sphere packing problem, which might be of some independent interest.|['Wenlong Mou', 'Liwei Wang']|['cs.DS', 'cs.CG']
2017-03-16T23:31:10Z|2016-12-13T18:12:44Z|http://arxiv.org/abs/1612.04304v1|http://arxiv.org/pdf/1612.04304v1|Towards a Constructive Version of Banaszczyk's Vector Balancing Theorem|An important theorem of Banaszczyk (Random Structures & Algorithms `98) states that for any sequence of vectors of $\ell_2$ norm at most $1/5$ and any convex body $K$ of Gaussian measure $1/2$ in $\mathbb{R}^n$, there exists a signed combination of these vectors which lands inside $K$. A major open problem is to devise a constructive version of Banaszczyk's vector balancing theorem, i.e. to find an efficient algorithm which constructs the signed combination.   We make progress towards this goal along several fronts. As our first contribution, we show an equivalence between Banaszczyk's theorem and the existence of $O(1)$-subgaussian distributions over signed combinations. For the case of symmetric convex bodies, our equivalence implies the existence of a universal signing algorithm (i.e. independent of the body), which simply samples from the subgaussian sign distribution and checks to see if the associated combination lands inside the body. For asymmetric convex bodies, we provide a novel recentering procedure, which allows us to reduce to the case where the body is symmetric.   As our second main contribution, we show that the above framework can be efficiently implemented when the vectors have length $O(1/\sqrt{\log n})$, recovering Banaszczyk's results under this stronger assumption. More precisely, we use random walk techniques to produce the required $O(1)$-subgaussian signing distributions when the vectors have length $O(1/\sqrt{\log n})$, and use a stochastic gradient ascent method to implement the recentering procedure for asymmetric bodies.|['Daniel Dadush', 'Shashwat Garg', 'Shachar Lovett', 'Aleksandar Nikolov']|['cs.DS', 'cs.CG', 'math.FA', 'math.PR']
2017-03-16T23:31:10Z|2016-12-12T19:12:47Z|http://arxiv.org/abs/1612.03854v1|http://arxiv.org/pdf/1612.03854v1|Crossing Number for Graphs With Bounded Pathwidth|The crossing number is the smallest number of pairwise edge-crossings when drawing a graph into the plane. There are only very few graph classes for which the exact crossing number is known or for which there at least exist constant approximation ratios. Furthermore, up to now, general crossing number computations have never been successfully tackled using bounded width of graph decompositions, like treewidth or pathwidth.   In this paper, we for the first time show that crossing number is tractable (even in linear time) for maximal graphs of bounded pathwidth~3. The technique also shows that the crossing number and the rectilinear (a.k.a. straight-line) crossing number are identical for this graph class, and that we require only an $O(n)\times O(n)$-grid to achieve such a drawing.   Our techniques can further be extended to devise a 2-approximation for general graphs with pathwidth 3, and a $4w^3$-approximation for maximal graphs of pathwidth $w$. This is a constant approximation for bounded pathwidth graphs.|['Therese Biedl', 'Markus Chimani', 'Martin Derka', 'Petra Mutzel']|['cs.CG']
2017-03-16T23:31:10Z|2016-12-12T12:10:02Z|http://arxiv.org/abs/1612.03638v1|http://arxiv.org/pdf/1612.03638v1|Intersection Graphs of Rays and Grounded Segments|We consider several classes of intersection graphs of line segments in the plane and prove new equality and separation results between those classes. In particular, we show that: (1) intersection graphs of grounded segments and intersection graphs of downward rays form the same graph class, (2) not every intersection graph of rays is an intersection graph of downward rays, and (3) not every intersection graph of rays is an outer segment graph. The first result answers an open problem posed by Cabello and Jej\v{c}i\v{c}. The third result confirms a conjecture by Cabello. We thereby completely elucidate the remaining open questions on the containment relations between these classes of segment graphs. We further characterize the complexity of the recognition problems for the classes of outer segment, grounded segment, and ray intersection graphs. We prove that these recognition problems are complete for the existential theory of the reals. This holds even if a 1-string realization is given as additional input.|['Jean Cardinal', 'Stefan Felsner', 'Tillmann Miltzow', 'Casey Tompkins', 'Birgit Vogtenhuber']|['cs.DM', 'cs.CC', 'cs.CG', 'math.CO']
2017-03-16T23:31:10Z|2016-12-09T13:00:53Z|http://arxiv.org/abs/1612.04861v1|http://arxiv.org/pdf/1612.04861v1|Some Counterexamples for Compatible Triangulations|"We consider the conjecture by Aichholzer, Aurenhammer, Hurtado, and Krasser that any two points sets with the same cardinality and the same size convex hull can be triangulated in the ""same"" way, more precisely via \emph{compatible triangulations}. We show counterexamples to various strengthened versions of this conjecture."|['Cody Barnson', 'Dawn Chandler', 'Qiao Chen', 'Christina Chung', 'Andrew Coccimiglio', 'Sean La', 'Lily Li', 'Aïna Linn', 'Anna Lubiw', 'Clare Lyle', 'Shikha Mahajan', 'Gregory Mierzwinski', 'Simon Pratt', 'Yoon Su', 'Yoo', 'Hongbo Zhang', 'Kevin Zhang']|['cs.CG', 'cs.DM', 'math.CO', '68R10', 'F.2.2']
2017-03-16T23:31:10Z|2016-12-09T10:31:04Z|http://arxiv.org/abs/1612.02967v1|http://arxiv.org/pdf/1612.02967v1|Dune-CurvilinearGrid: Parallel Dune Grid Manager for Unstructured   Tetrahedral Curvilinear Meshes|We introduce the dune-curvilineargrid module. The module provides the self-contained, parallel grid manager, as well as the underlying elementary curvilinear geometry module dune-curvilineargeometry. This work is motivated by the need for reliable and scalable electromagnetic design of nanooptical devices. Curvilinear geometries improve both the accuracy of modeling smooth material boundaries, and the h/p-convergence rate of PDE solutions, reducing the necessary computational effort. dune-curvilineargrid provides a large spectrum of features for scalable parallel implementations of Finite Element and Boundary Integral methods over curvilinear tetrahedral geometries, including symbolic polynomial mappings and operations, recursive integration, sparse and dense grid communication, parallel timing and memory footprint diagnostics utilities. It is written in templated C++ using MPI for parallelization and ParMETIS for grid partitioning, and is provided as a module for the DUNE interface. The dune-curvilineargrid grid manager is continuously developed and improved, and so is this documentation. For the most recent version of the documentation, as well as the source code, please refer to the provided repositories and our website.|['Aleksejs Fomins', 'Benedikt Oswald']|['cs.CG', '51P05, 78-04', 'F.2.2; G.1.8; I.3.5']
2017-03-16T23:31:10Z|2016-12-09T04:10:03Z|http://arxiv.org/abs/1612.02905v1|http://arxiv.org/pdf/1612.02905v1|An obstruction to Delaunay triangulations in Riemannian manifolds|Delaunay has shown that the Delaunay complex of a finite set of points $P$ of Euclidean space $\mathbb{R}^m$ triangulates the convex hull of $P$, provided that $P$ satisfies a mild genericity property. Voronoi diagrams and Delaunay complexes can be defined for arbitrary Riemannian manifolds. However, Delaunay's genericity assumption no longer guarantees that the Delaunay complex will yield a triangulation; stronger assumptions on $P$ are required. A natural one is to assume that $P$ is sufficiently dense. Although results in this direction have been claimed, we show that sample density alone is insufficient to ensure that the Delaunay complex triangulates a manifold of dimension greater than 2.|['Jean-Daniel Boissonnat', 'Ramsay Dyer', 'Arijit Ghosh', 'Nikolay Martynchuk']|['cs.CG', 'math.DG', 'math.GT', '57R05 (Primary), 54B15, 53B20 (Secondary)']
2017-03-16T23:31:10Z|2016-12-12T17:12:07Z|http://arxiv.org/abs/1612.02861v2|http://arxiv.org/pdf/1612.02861v2|Multi-Scale Projective Coordinates via Persistent Cohomology of Sparse   Filtrations|We present in this paper a framework for mapping data onto real and complex projective spaces. The resulting projective coordinates provide a multi-scale representation of the data, and capture low dimensional underlying topological features. An initial map is obtained in two steps: First, the persistent cohomology of a sparse filtration is used to compute systems of transition functions for (real and complex) line bundles over neighborhoods of the data. Next, the transition functions are used to produce explicit classifying maps for the induced bundles. A framework for dimensionality reduction in projective space (Principal Projective Components) is also developed, aimed at decreasing the target dimension of the original map. Several examples are provided as well as theorems addressing choices in the construction.|['Jose A. Perea']|['math.AT', 'cs.CG']
2017-03-16T23:31:10Z|2016-12-08T01:57:47Z|http://arxiv.org/abs/1612.02509v1|http://arxiv.org/pdf/1612.02509v1|Geodesics using Waves: Computing Distances using Wave Propagation|In this paper, we present a new method for computing approximate geodesic distances. We introduce the wave method for approximating geodesic distances from a point on a manifold mesh. Our method involves the solution of two linear systems of equations. One system of equations is solved repeatedly to propagate the wave on the entire mesh, and one system is solved once after wave propagation is complete in order to compute the approximate geodesic distances up to an additive constant. However, these systems need to be pre-factored only once, and can be solved efficiently at each iteration. All of our tests required approximately between 300 and 400 iterations, which were completed in a few seconds. Therefore, this method can approximate geodesic distances quickly, and the approximation is highly accurate.|['Ayushi Sinha', 'Michael Kazhdan']|['cs.CG', 'cs.GR', 'I.3.5']
2017-03-16T23:31:10Z|2016-12-07T23:24:33Z|http://arxiv.org/abs/1612.02483v1|http://arxiv.org/pdf/1612.02483v1|High Dimensional Consistent Digital Segments|We consider the problem of digitalizing Euclidean line segments from $\mathbb{R}^d$ to $\mathbb{Z}^d$. Christ {\em et al.} (DCG, 2012) showed how to construct a set of {\em consistent digital segment} (CDS) for $d=2$: a collection of segments connecting any two points in $\mathbb{Z}^2$ that satisfies the natural extension of the Euclidean axioms to $\mathbb{Z}^d$. In this paper we study the construction of CDSs in higher dimensions.   We show that any total order can be used to create a set of {\em consistent digital rays} CDR in $\mathbb{Z}^d$ (a set of rays emanating from a fixed point $p$ that satisfies the extension of the Euclidean axioms). We fully characterize for which total orders the construction holds and study their Hausdorff distance, which in particular positively answers the question posed by Christ {\em et al.}.|['Man-Kwun Chiu', 'Matias Korman']|['cs.CG', 'I.3.5; I.4.1']
2017-03-16T23:31:10Z|2016-12-07T20:44:21Z|http://arxiv.org/abs/1612.02412v1|http://arxiv.org/pdf/1612.02412v1|Shortcuts for the Circle|Let $C$ be the unit circle in $\mathbb{R}^2$. We can view $C$ as a plane graph whose vertices are all the points on $C$, and the distance between any two points on $C$ is the length of the smaller arc between them. We consider a graph augmentation problem on $C$, where we want to place $k\geq 1$ \emph{shortcuts} on $C$ such that the diameter of the resulting graph is minimized.   We analyze for each $k$ with $1\leq k\leq 7$ what the optimal set of shortcuts is. Interestingly, the minimum diameter one can obtain is not a strictly decreasing function of~$k$. For example, with seven shortcuts one cannot obtain a smaller diameter than with six shortcuts. Finally, we prove that the optimal diameter is $2 + \Theta(1/k^{\frac{2}{3}})$ for any~$k$.|['Sang Won Bae', 'Mark de Berg', 'Otfried Cheong', 'Joachim Gudmundsson', 'Christos Levcopoulos']|['math.MG', 'cs.CG']
