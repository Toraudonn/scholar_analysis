2017-03-28T14:08:43Z|2017-03-27T17:52:55Z|http://arxiv.org/abs/1703.09211v1|http://arxiv.org/pdf/1703.09211v1|Coherent Online Video Style Transfer|coher onlin video style transfer|Training a feed-forward network for fast neural style transfer of images is proven to be successful. However, the naive extension to process video frame by frame is prone to producing flickering results. We propose the first end-to-end network for online video style transfer, which generates temporally coherent stylized video sequences in near real-time. Two key ideas include an efficient network by incorporating short-term coherence, and propagating short-term coherence to long-term, which ensures the consistency over larger period of time. Our network can incorporate different image stylization networks. We show that the proposed method clearly outperforms the per-frame baseline both qualitatively and quantitatively. Moreover, it can achieve visually comparable coherence to optimization-based video style transfer, but is three orders of magnitudes faster in runtime.|train feed forward network fast neural style transfer imag proven success howev naiv extens process video frame frame prone produc flicker result propos first end end network onlin video style transfer generat tempor coher styliz video sequenc near real time two key idea includ effici network incorpor short term coher propag short term coher long term ensur consist larger period time network incorpor differ imag stylize network show propos method clear outperform per frame baselin qualit quantit moreov achiev visual compar coher optim base video style transfer three order magnitud faster runtim|['Dongdong Chen', 'Jing Liao', 'Yuan Lu', 'Nenghai Yu', 'Gang Hua']|['cs.CV']
2017-03-28T14:08:43Z|2017-03-27T17:52:18Z|http://arxiv.org/abs/1703.09210v1|http://arxiv.org/pdf/1703.09210v1|StyleBank: An Explicit Representation for Neural Image Style Transfer|stylebank explicit represent neural imag style transfer|We propose StyleBank, which is composed of multiple convolution filter banks and each filter bank explicitly represents one style, for neural image style transfer. To transfer an image to a specific style, the corresponding filter bank is operated on top of the intermediate feature embedding produced by a single auto-encoder. The StyleBank and the auto-encoder are jointly learnt, where the learning is conducted in such a way that the auto-encoder does not encode any style information thanks to the flexibility introduced by the explicit filter bank representation. It also enables us to conduct incremental learning to add a new image style by learning a new filter bank while holding the auto-encoder fixed. The explicit style representation along with the flexible network design enables us to fuse styles at not only the image level, but also the region level. Our method is the first style transfer network that links back to traditional texton mapping methods, and hence provides new understanding on neural style transfer. Our method is easy to train, runs in real-time, and produces results that qualitatively better or at least comparable to existing methods.|propos stylebank compos multipl convolut filter bank filter bank explicit repres one style neural imag style transfer transfer imag specif style correspond filter bank oper top intermedi featur embed produc singl auto encod stylebank auto encod joint learnt learn conduct way auto encod doe encod ani style inform thank flexibl introduc explicit filter bank represent also enabl us conduct increment learn add new imag style learn new filter bank hold auto encod fix explicit style represent along flexibl network design enabl us fuse style onli imag level also region level method first style transfer network link back tradit texton map method henc provid new understand neural style transfer method easi train run real time produc result qualit better least compar exist method|['Dongdong Chen', 'Yuan Lu', 'Jing Liao', 'Nenghai Yu', 'Gang Hua']|['cs.CV']
2017-03-28T14:08:43Z|2017-03-27T17:37:33Z|http://arxiv.org/abs/1703.09200v1|http://arxiv.org/pdf/1703.09200v1|Deep Poincare Map For Robust Medical Image Segmentation|deep poincar map robust medic imag segment|Precise segmentation is a prerequisite for an accurate quantification of the imaged objects. It is a very challenging task in many medical imaging applications due to relatively poor image quality and data scarcity. In this work, we present an innovative segmentation paradigm, named Deep Poincare Map (DPM), by coupling the dynamical system theory with a novel deep learning based approach. Firstly, we model the image segmentation process as a dynamical system, in which limit cycle models the boundary of the region of interest (ROI). Secondly, instead of segmenting the ROI directly, convolutional neural network is employed to predict the vector field of the dynamical system. Finally, the boundary of the ROI is identified using the Poincare map and the flow integration. We demonstrate that our segmentation model can be built using a very limited number of train- ing data. By cross-validation, we can achieve a mean Dice score of 94% compared to the manual delineation (ground truth) of the left ventricle ROI defined by clinical experts on a cardiac MRI dataset. Compared with other state-of-the-art methods, we can conclude that the proposed DPM method is adaptive, accurate and robust. It is straightforward to apply this method for other medical imaging applications.|precis segment prerequisit accur quantif imag object veri challeng task mani medic imag applic due relat poor imag qualiti data scarciti work present innov segment paradigm name deep poincar map dpm coupl dynam system theori novel deep learn base approach first model imag segment process dynam system limit cycl model boundari region interest roi second instead segment roi direct convolut neural network employ predict vector field dynam system final boundari roi identifi use poincar map flow integr demonstr segment model built use veri limit number train ing data cross valid achiev mean dice score compar manual delin ground truth left ventricl roi defin clinic expert cardiac mri dataset compar state art method conclud propos dpm method adapt accur robust straightforward appli method medic imag applic|['Yuanhan Mo', 'Fangde Liu', 'Jingqing Zhang', 'Guang Yang', 'Taigang He', 'Yike Guo']|['cs.CV']
2017-03-28T14:08:43Z|2017-03-27T17:36:33Z|http://arxiv.org/abs/1703.09199v1|http://arxiv.org/pdf/1703.09199v1|Introduction To The Monogenic Signal|introduct monogen signal|The monogenic signal is an image analysis methodology that was introduced by Felsberg and Sommer in 2001 and has been employed for a variety of purposes in image processing and computer vision research. In particular, it has been found to be useful in the analysis of ultrasound imagery in several research scenarios mostly in work done within the BioMedIA lab at Oxford. However, the literature on the monogenic signal can be difficult to penetrate due to the lack of a single resource to explain the various principles from basics. The purpose of this document is therefore to introduce the principles, purpose, applications, and limitations of the methodology. It assumes some background knowledge from the fields of image and signal processing, in particular a good knowledge of Fourier transforms as applied to signals and images. We will not attempt to provide a thorough math- ematical description or derivation of the monogenic signal, but rather focus on developing an intuition for understanding and using the methodology and refer the reader elsewhere for a more mathematical treatment.|monogen signal imag analysi methodolog introduc felsberg sommer employ varieti purpos imag process comput vision research particular found use analysi ultrasound imageri sever research scenario work done within biomedia lab oxford howev literatur monogen signal difficult penetr due lack singl resourc explain various principl basic purpos document therefor introduc principl purpos applic limit methodolog assum background knowledg field imag signal process particular good knowledg fourier transform appli signal imag attempt provid thorough math emat descript deriv monogen signal rather focus develop intuit understand use methodolog refer reader elsewher mathemat treatment|['Christopher P. Bridge']|['cs.CV']
2017-03-28T14:08:43Z|2017-03-27T16:48:03Z|http://arxiv.org/abs/1703.09179v1|http://arxiv.org/pdf/1703.09179v1|Transfer learning for music classification and regression tasks|transfer learn music classif regress task|In this paper, we present a transfer learning approach for music classification and regression tasks. We propose to use a pretrained convnet feature, a concatenated feature vector using activations of feature maps of multiple layers in a trained convolutional network. We show that how this convnet feature can serve as a general-purpose music representation. In the experiment, a convnet is trained for music tagging and then transferred for many music-related classification and regression tasks as well as an audio-related classification task. In experiments, the convnet feature outperforms the baseline MFCC feature in all tasks and many reported approaches of aggregating MFCCs and low- and high-level music features.|paper present transfer learn approach music classif regress task propos use pretrain convnet featur concaten featur vector use activ featur map multipl layer train convolut network show convnet featur serv general purpos music represent experi convnet train music tag transfer mani music relat classif regress task well audio relat classif task experi convnet featur outperform baselin mfcc featur task mani report approach aggreg mfccs low high level music featur|['Keunwoo Choi', 'Gy√∂rgy Fazekas', 'Mark Sandler', 'Kyunghyun Cho']|['cs.CV', 'cs.AI', 'cs.MM', 'cs.SD']
2017-03-28T14:08:43Z|2017-03-27T16:21:26Z|http://arxiv.org/abs/1703.09167v1|http://arxiv.org/pdf/1703.09167v1|A Study on the Extraction and Analysis of a Large Set of Eye Movement   Features during Reading|studi extract analysi larg set eye movement featur dure read|This work presents a study on the extraction and analysis of a set of 101 categories of eye movement features from three types of eye movement events: fixations, saccades, and post-saccadic oscillations. The eye movements were recorded during a reading task. For the categories of features with multiple instances in a recording we extract corresponding feature subtypes by calculating descriptive statistics on the distributions of these instances. A unified framework of detailed descriptions and mathematical formulas are provided for the extraction of the feature set. The analysis of feature values is performed using a large database of eye movement recordings from a normative population of 298 subjects. We demonstrate the central tendency and overall variability of feature values over the experimental population, and more importantly, we quantify the test-retest reliability (repeatability) of each separate feature. The described methods and analysis can provide valuable tools in fields exploring the eye movements, such as in behavioral studies, attention and cognition research, medical research, biometric recognition, and human-computer interaction.|work present studi extract analysi set categori eye movement featur three type eye movement event fixat saccad post saccad oscil eye movement record dure read task categori featur multipl instanc record extract correspond featur subtyp calcul descript statist distribut instanc unifi framework detail descript mathemat formula provid extract featur set analysi featur valu perform use larg databas eye movement record normat popul subject demonstr central tendenc overal variabl featur valu experiment popul import quantifi test retest reliabl repeat separ featur describ method analysi provid valuabl tool field explor eye movement behavior studi attent cognit research medic research biometr recognit human comput interact|['Ioannis Rigas', 'Lee Friedman', 'Oleg Komogortsev']|['cs.CV', 'cs.HC', 'q-bio.QM']
2017-03-28T14:08:43Z|2017-03-27T16:09:20Z|http://arxiv.org/abs/1703.09161v1|http://arxiv.org/pdf/1703.09161v1|A Dynamic Programming Solution to Bounded Dejittering Problems|dynam program solut bound dejitt problem|We propose a dynamic programming solution to image dejittering problems with bounded displacements and obtain efficient algorithms for the removal of line jitter, line pixel jitter, and pixel jitter.|propos dynam program solut imag dejitt problem bound displac obtain effici algorithm remov line jitter line pixel jitter pixel jitter|['Lukas F. Lang']|['math.OC', 'cs.CV']
2017-03-28T14:08:43Z|2017-03-27T15:57:27Z|http://arxiv.org/abs/1703.09157v1|http://arxiv.org/pdf/1703.09157v1|Reweighted Infrared Patch-Tensor Model With Both Non-Local and Local   Priors for Single-Frame Small Target Detection|reweight infrar patch tensor model non local local prior singl frame small target detect|Many state-of-the-art methods have been proposed for infrared small target detection. They work well on the images with homogeneous backgrounds and high-contrast targets. However, when facing highly heterogeneous backgrounds, they would not perform very well, mainly due to: 1) the existence of strong edges and other interfering components, 2) not utilizing the priors fully. Inspired by this, we propose a novel method to exploit both local and non-local priors simultaneously. Firstly, we employ a new infrared patch-tensor (IPT) model to represent the image and preserve its spatial correlations. Exploiting the target sparse prior and background non-local self-correlation prior, the target-background separation is modeled as a robust low-rank tensor recovery problem. Moreover, with the help of the structure tensor and reweighted idea, we design an entry-wise local-structure-adaptive and sparsity enhancing weight to replace the globally constant weighting parameter. The decomposition could be achieved via the element-wise reweighted higher-order robust principal component analysis with an additional convergence condition according to the practical situation of target detection. Extensive experiments demonstrate that our model outperforms the other state-of-the-arts, in particular for the images with very dim targets and heavy clutters.|mani state art method propos infrar small target detect work well imag homogen background high contrast target howev face high heterogen background would perform veri well main due exist strong edg interf compon util prior fulli inspir propos novel method exploit local non local prior simultan first employ new infrar patch tensor ipt model repres imag preserv spatial correl exploit target spars prior background non local self correl prior target background separ model robust low rank tensor recoveri problem moreov help structur tensor reweight idea design entri wise local structur adapt sparsiti enhanc weight replac global constant weight paramet decomposit could achiev via element wise reweight higher order robust princip compon analysi addit converg condit accord practic situat target detect extens experi demonstr model outperform state art particular imag veri dim target heavi clutter|['Yimian Dai', 'Yiquan Wu']|['cs.CV']
2017-03-28T14:08:43Z|2017-03-27T15:31:00Z|http://arxiv.org/abs/1703.09145v1|http://arxiv.org/pdf/1703.09145v1|"Multi-Path Region-Based Convolutional Neural Network for Accurate   Detection of Unconstrained ""Hard Faces"""|multi path region base convolut neural network accur detect unconstrain hard face|"Large-scale variations still pose a challenge in unconstrained face detection. To the best of our knowledge, no current face detection algorithm can detect a face as large as 800 x 800 pixels while simultaneously detecting another one as small as 8 x 8 pixels within a single image with equally high accuracy. We propose a two-stage cascaded face detection framework, Multi-Path Region-based Convolutional Neural Network (MP-RCNN), that seamlessly combines a deep neural network with a classic learning strategy, to tackle this challenge. The first stage is a Multi-Path Region Proposal Network (MP-RPN) that proposes faces at three different scales. It simultaneously utilizes three parallel outputs of the convolutional feature maps to predict multi-scale candidate face regions. The ""atrous"" convolution trick (convolution with up-sampled filters) and a newly proposed sampling layer for ""hard"" examples are embedded in MP-RPN to further boost its performance. The second stage is a Boosted Forests classifier, which utilizes deep facial features pooled from inside the candidate face regions as well as deep contextual features pooled from a larger region surrounding the candidate face regions. This step is included to further remove hard negative samples. Experiments show that this approach achieves state-of-the-art face detection performance on the WIDER FACE dataset ""hard"" partition, outperforming the former best result by 9.6% for the Average Precision."|larg scale variat still pose challeng unconstrain face detect best knowledg current face detect algorithm detect face larg pixel simultan detect anoth one small pixel within singl imag equal high accuraci propos two stage cascad face detect framework multi path region base convolut neural network mp rcnn seamless combin deep neural network classic learn strategi tackl challeng first stage multi path region propos network mp rpn propos face three differ scale simultan util three parallel output convolut featur map predict multi scale candid face region atrous convolut trick convolut sampl filter newli propos sampl layer hard exampl embed mp rpn boost perform second stage boost forest classifi util deep facial featur pool insid candid face region well deep contextu featur pool larger region surround candid face region step includ remov hard negat sampl experi show approach achiev state art face detect perform wider face dataset hard partit outperform former best result averag precis|['Yuguang Liu', 'Martin D. Levine']|['cs.CV']
2017-03-28T14:08:43Z|2017-03-27T15:13:49Z|http://arxiv.org/abs/1703.09137v1|http://arxiv.org/pdf/1703.09137v1|Where to put the Image in an Image Caption Generator|put imag imag caption generat|When a neural language model is used for caption generation, the image information can be fed to the neural network either by directly incorporating it in a recurrent neural network -- conditioning the language model by injecting image features -- or in a layer following the recurrent neural network -- conditioning the language model by merging the image features. While merging implies that visual features are bound at the end of the caption generation process, injecting can bind the visual features at a variety stages. In this paper we empirically show that late binding is superior to early binding in terms of different evaluation metrics. This suggests that the different modalities (visual and linguistic) for caption generation should not be jointly encoded by the RNN; rather, the multimodal integration should be delayed to a subsequent stage. Furthermore, this suggests that recurrent neural networks should not be viewed as actually generating text, but only as encoding it for prediction in a subsequent layer.|neural languag model use caption generat imag inform fed neural network either direct incorpor recurr neural network condit languag model inject imag featur layer follow recurr neural network condit languag model merg imag featur merg impli visual featur bound end caption generat process inject bind visual featur varieti stage paper empir show late bind superior earli bind term differ evalu metric suggest differ modal visual linguist caption generat joint encod rnn rather multimod integr delay subsequ stage furthermor suggest recurr neural network view actual generat text onli encod predict subsequ layer|['Marc Tanti', 'Albert Gatt', 'Kenneth P. Camilleri']|['cs.NE', 'cs.CL', 'cs.CV']
2017-03-28T14:08:48Z|2017-03-27T13:44:26Z|http://arxiv.org/abs/1703.09076v1|http://arxiv.org/pdf/1703.09076v1|Active Convolution: Learning the Shape of Convolution for Image   Classification|activ convolut learn shape convolut imag classif|In recent years, deep learning has achieved great success in many computer vision applications. Convolutional neural networks (CNNs) have lately emerged as a major approach to image classification. Most research on CNNs thus far has focused on developing architectures such as the Inception and residual networks. The convolution layer is the core of the CNN, but few studies have addressed the convolution unit itself. In this paper, we introduce a convolution unit called the active convolution unit (ACU). A new convolution has no fixed shape, because of which we can define any form of convolution. Its shape can be learned through backpropagation during training. Our proposed unit has a few advantages. First, the ACU is a generalization of convolution; it can define not only all conventional convolutions, but also convolutions with fractional pixel coordinates. We can freely change the shape of the convolution, which provides greater freedom to form CNN structures. Second, the shape of the convolution is learned while training and there is no need to tune it by hand. Third, the ACU can learn better than a conventional unit, where we obtained the improvement simply by changing the conventional convolution to an ACU. We tested our proposed method on plain and residual networks, and the results showed significant improvement using our method on various datasets and architectures in comparison with the baseline.|recent year deep learn achiev great success mani comput vision applic convolut neural network cnns late emerg major approach imag classif research cnns thus far focus develop architectur incept residu network convolut layer core cnn studi address convolut unit paper introduc convolut unit call activ convolut unit acu new convolut fix shape becaus defin ani form convolut shape learn backpropag dure train propos unit advantag first acu general convolut defin onli convent convolut also convolut fraction pixel coordin freeli chang shape convolut provid greater freedom form cnn structur second shape convolut learn train need tune hand third acu learn better convent unit obtain improv simpli chang convent convolut acu test propos method plain residu network result show signific improv use method various dataset architectur comparison baselin|['Yunho Jeon', 'Junmo Kim']|['cs.CV']
2017-03-28T14:08:48Z|2017-03-27T12:42:13Z|http://arxiv.org/abs/1703.09039v1|http://arxiv.org/pdf/1703.09039v1|Efficient Processing of Deep Neural Networks: A Tutorial and Survey|effici process deep neural network tutori survey|Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of deep neural network to improve energy-efficiency and throughput without sacrificing performance accuracy or increasing hardware cost are critical to enabling the wide deployment of DNNs in AI systems.   This article aims to provide a comprehensive tutorial and survey about the recent advances towards the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various platforms and architectures that support DNNs, and highlight key trends in recent efficient processing techniques that reduce the computation cost of DNNs either solely via hardware design changes or via joint hardware design and network algorithm changes. It will also summarize various development resources that can enable researchers and practitioners to quickly get started on DNN design, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic co-design, being proposed in academia and industry.   The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand trade-offs between various architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand of recent implementation trends and opportunities.|deep neural network dnns current wide use mani artifici intellig ai applic includ comput vision speech recognit robot dnns deliv state art accuraci mani ai task come cost high comput complex accord techniqu enabl effici process deep neural network improv energi effici throughput without sacrif perform accuraci increas hardwar cost critic enabl wide deploy dnns ai system articl aim provid comprehens tutori survey recent advanc toward goal enabl effici process dnns specif provid overview dnns discuss various platform architectur support dnns highlight key trend recent effici process techniqu reduc comput cost dnns either sole via hardwar design chang via joint hardwar design network algorithm chang also summar various develop resourc enabl research practition quick get start dnn design highlight import benchmark metric design consider use evalu rapid grow number dnn hardwar design option includ algorithm co design propos academia industri reader take away follow concept articl understand key design consider dnns abl evalu differ dnn hardwar implement benchmark comparison metric understand trade various architectur platform abl evalu util various dnn design techniqu effici process understand recent implement trend opportun|['Vivienne Sze', 'Yu-Hsin Chen', 'Tien-Ju Yang', 'Joel Emer']|['cs.CV']
2017-03-28T14:08:48Z|2017-03-27T12:14:07Z|http://arxiv.org/abs/1703.09026v1|http://arxiv.org/pdf/1703.09026v1|Trespassing the Boundaries: Labeling Temporal Bounds for Object   Interactions in Egocentric Video|trespass boundari label tempor bound object interact egocentr video|Manual annotations of temporal bounds for object interactions (i.e. start and end times) are typical training input to recognition, localization and detection algorithms. For three publicly available egocentric datasets, we uncover inconsistencies in ground truth temporal bounds within and across annotators and datasets. We systematically assess the robustness of state-of-the-art approaches to changes in labeled temporal bounds, for object interaction recognition. As boundaries are trespassed, a drop of up to 10% is observed for both Improved Dense Trajectories and Two-Stream Convolutional Neural Network. We demonstrate that such disagreement stems from a limited understanding of the distinct phases of an action, and propose annotating based on the Rubicon Boundaries, inspired by a similarly named cognitive model, for consistent temporal bounds of object interactions. Evaluated on a public dataset, we report a 4% increase in overall accuracy, and an increase in accuracy for 55% of classes when Rubicon Boundaries are used for temporal annotations.|manual annot tempor bound object interact start end time typic train input recognit local detect algorithm three public avail egocentr dataset uncov inconsist ground truth tempor bound within across annot dataset systemat assess robust state art approach chang label tempor bound object interact recognit boundari trespass drop observ improv dens trajectori two stream convolut neural network demonstr disagr stem limit understand distinct phase action propos annot base rubicon boundari inspir similar name cognit model consist tempor bound object interact evalu public dataset report increas overal accuraci increas accuraci class rubicon boundari use tempor annot|['Davide Moltisanti', 'Michael Wray', 'Walterio Mayol-Cuevas', 'Dima Damen']|['cs.CV']
2017-03-28T14:08:48Z|2017-03-27T09:51:55Z|http://arxiv.org/abs/1703.08987v1|http://arxiv.org/pdf/1703.08987v1|Simultaneous Perception and Path Generation Using Fully Convolutional   Neural Networks|simultan percept path generat use fulli convolut neural network|In this work, a novel learning-based approach has been developed to generate driving paths by integrating LIDAR point clouds, GPS-IMU information, and Google driving directions. The system is based on a fully convolutional neural network that jointly learns to carry out perception and path generation from real-world driving sequences and that is trained using automatically generated training examples. Several combinations of input data were tested in order to assess the performance gain provided by specific information modalities. The fully convolutional neural network trained using all the available sensors together with driving directions achieved the best MaxF score of 88.13% when considering a region of interest of 60x60 meters. By considering a smaller region of interest, the agreement between predicted paths and ground-truth increased to 92.60%. The positive results obtained in this work indicate that the proposed system may help fill the gap between low-level scene parsing and behavior-reflex approaches by generating outputs that are close to vehicle control and at the same time human-interpretable.|work novel learn base approach develop generat drive path integr lidar point cloud gps imu inform googl drive direct system base fulli convolut neural network joint learn carri percept path generat real world drive sequenc train use automat generat train exampl sever combin input data test order assess perform gain provid specif inform modal fulli convolut neural network train use avail sensor togeth drive direct achiev best maxf score consid region interest meter consid smaller region interest agreement predict path ground truth increas posit result obtain work indic propos system may help fill gap low level scene pars behavior reflex approach generat output close vehicl control time human interpret|['Luca Caltagirone', 'Mauro Bellone', 'Lennart Svensson', 'Mattias Wahde']|['cs.CV']
2017-03-28T14:08:48Z|2017-03-27T08:23:47Z|http://arxiv.org/abs/1703.08966v1|http://arxiv.org/pdf/1703.08966v1|Mastering Sketching: Adversarial Augmentation for Structured Prediction|master sketch adversari augment structur predict|We present an integral framework for training sketch simplification networks that convert challenging rough sketches into clean line drawings. Our approach augments a simplification network with a discriminator network, training both networks jointly so that the discriminator network discerns whether a line drawing is a real training data or the output of the simplification network, which in turn tries to fool it. This approach has two major advantages. First, because the discriminator network learns the structure in line drawings, it encourages the output sketches of the simplification network to be more similar in appearance to the training sketches. Second, we can also train the simplification network with additional unsupervised data, using the discriminator network as a substitute teacher. Thus, by adding only rough sketches without simplified line drawings, or only line drawings without the original rough sketches, we can improve the quality of the sketch simplification. We show how our framework can be used to train models that significantly outperform the state of the art in the sketch simplification task, despite using the same architecture for inference. We additionally present an approach to optimize for a single image, which improves accuracy at the cost of additional computation time. Finally, we show that, using the same framework, it is possible to train the network to perform the inverse problem, i.e., convert simple line sketches into pencil drawings, which is not possible using the standard mean squared error loss. We validate our framework with two user tests, where our approach is preferred to the state of the art in sketch simplification 92.3% of the time and obtains 1.2 more points on a scale of 1 to 5.|present integr framework train sketch simplif network convert challeng rough sketch clean line draw approach augment simplif network discrimin network train network joint discrimin network discern whether line draw real train data output simplif network turn tri fool approach two major advantag first becaus discrimin network learn structur line draw encourag output sketch simplif network similar appear train sketch second also train simplif network addit unsupervis data use discrimin network substitut teacher thus ad onli rough sketch without simplifi line draw onli line draw without origin rough sketch improv qualiti sketch simplif show framework use train model signific outperform state art sketch simplif task despit use architectur infer addit present approach optim singl imag improv accuraci cost addit comput time final show use framework possibl train network perform invers problem convert simpl line sketch pencil draw possibl use standard mean squar error loss valid framework two user test approach prefer state art sketch simplif time obtain point scale|['Edgar Simo-Serra', 'Satoshi Iizuka', 'Hiroshi Ishikawa']|['cs.CV']
2017-03-28T14:08:48Z|2017-03-27T07:49:43Z|http://arxiv.org/abs/1703.08961v1|http://arxiv.org/pdf/1703.08961v1|Scaling the Scattering Transform: Deep Hybrid Networks|scale scatter transform deep hybrid network|We use the scattering network as a generic and fixed initialization of the first layers of a supervised hybrid deep network. We show that early layers do not necessarily need to be learned, providing the best results to-date with pre-defined representations while being competitive with Deep CNNs. Using a shallow cascade of 1x1 convolutions, which encodes scattering coefficients that correspond to spatial windows of very small sizes, permits to obtain AlexNet accuracy on the imagenet ILSVRC2012. We demonstrate that this local encoding explicitly learns in-variance w.r.t. rotations. Combining scattering networks with a modern ResNet, we achieve a single-crop top 5 error of 11.4% on imagenet ILSVRC2012, comparable to the Resnet-18 architecture, while utilizing only 10 layers. We also find that hybrid architectures can yield excellent performance in the small sample regime, exceeding their end-to-end counterparts, through their ability to incorporate geometrical priors. We demonstrate this on subsets of the CIFAR-10 dataset and by setting a new state-of-the-art on the STL-10 dataset.|use scatter network generic fix initi first layer supervis hybrid deep network show earli layer necessarili need learn provid best result date pre defin represent competit deep cnns use shallow cascad convolut encod scatter coeffici correspond spatial window veri small size permit obtain alexnet accuraci imagenet ilsvrc demonstr local encod explicit learn varianc rotat combin scatter network modern resnet achiev singl crop top error imagenet ilsvrc compar resnet architectur util onli layer also find hybrid architectur yield excel perform small sampl regim exceed end end counterpart abil incorpor geometr prior demonstr subset cifar dataset set new state art stl dataset|['Edouard Oyallon', 'Eugene Belilovsky', 'Sergey Zagoruyko']|['cs.CV', 'cs.LG']
2017-03-28T14:08:48Z|2017-03-27T03:50:51Z|http://arxiv.org/abs/1703.08919v1|http://arxiv.org/pdf/1703.08919v1|MIHash: Online Hashing with Mutual Information|mihash onlin hash mutual inform|Learning-based adaptive hashing methods are widely used for nearest neighbor retrieval. Recently, online hashing methods have demonstrated a good performance-complexity tradeoff by learning hash functions from streaming data. In this paper, we aim to advance the state-of-the-art for online hashing. We first address a key challenge that has often been ignored: the binary codes for indexed data must be recomputed to keep pace with updates to the hash functions. We propose an efficient quality measure for hash functions, based on an information-theoretic quantity, mutual information, and use it successfully as a criterion to eliminate unnecessary hash table updates. Next, we show that mutual information can also be used as an objective in learning hash functions, using gradient-based optimization. Experiments on image retrieval benchmarks (including a 2.5M image dataset) confirm the effectiveness of our formulation, both in reducing hash table recomputations and in learning high-quality hash functions.|learn base adapt hash method wide use nearest neighbor retriev recent onlin hash method demonstr good perform complex tradeoff learn hash function stream data paper aim advanc state art onlin hash first address key challeng often ignor binari code index data must recomput keep pace updat hash function propos effici qualiti measur hash function base inform theoret quantiti mutual inform use success criterion elimin unnecessari hash tabl updat next show mutual inform also use object learn hash function use gradient base optim experi imag retriev benchmark includ imag dataset confirm effect formul reduc hash tabl recomput learn high qualiti hash function|['Fatih Cakir', 'Kun He', 'Sarah Adel Bargal', 'Stan Sclaroff']|['cs.CV']
2017-03-28T14:08:48Z|2017-03-27T03:46:58Z|http://arxiv.org/abs/1703.08917v1|http://arxiv.org/pdf/1703.08917v1|A Visual Measure of Changes to Weighted Self-Organizing Map Patterns|visual measur chang weight self organ map pattern|Estimating output changes by input changes is the main task in causal analysis. In previous work, input and output Self-Organizing Maps (SOMs) were associated for causal analysis of multivariate and nonlinear data. Based on the association, a weight distribution of the output conditional on a given input was obtained over the output map space. Such a weighted SOM pattern of the output changes when the input changes. In order to analyze the change, it is important to measure the difference of the patterns. Many methods have been proposed for the dissimilarity measure of patterns. However, it remains a major challenge when attempting to measure how the patterns change. In this paper, we propose a visualization approach that simplifies the comparison of the difference in terms of the pattern property. Using this approach, the change can be analyzed by integrating colors and star glyph shapes representing the property dissimilarity. Ecological data is used to demonstrate the usefulness of our approach and the experimental results show that our approach provides the change information effectively.|estim output chang input chang main task causal analysi previous work input output self organ map som associ causal analysi multivari nonlinear data base associ weight distribut output condit given input obtain output map space weight som pattern output chang input chang order analyz chang import measur differ pattern mani method propos dissimilar measur pattern howev remain major challeng attempt measur pattern chang paper propos visual approach simplifi comparison differ term pattern properti use approach chang analyz integr color star glyph shape repres properti dissimilar ecolog data use demonstr use approach experiment result show approach provid chang inform effect|['Younjin Chung', 'Joachim Gudmundsson', 'Masahiro Takatsuka']|['cs.CV']
2017-03-28T14:08:48Z|2017-03-27T03:08:58Z|http://arxiv.org/abs/1703.08912v1|http://arxiv.org/pdf/1703.08912v1|Exploiting Color Name Space for Salient Object Detection|exploit color name space salient object detect|In this paper, we will investigate the contribution of color names for salient object detection. Each input image is first converted to the color name space, which is consisted of 11 probabilistic channels. By exploring the topological structure relationship between the figure and the ground, we obtain a saliency map through a linear combination of a set of sequential attention maps. To overcome the limitation of only exploiting the surroundedness cue, two global cues with respect to color names are invoked for guiding the computation of another weighted saliency map. Finally, we integrate the two saliency maps into a unified framework to infer the saliency result. In addition, an improved post-processing procedure is introduced to effectively suppress the background while uniformly highlight the salient objects. Experimental results show that the proposed model produces more accurate saliency maps and performs well against 23 saliency models in terms of three evaluation metrics on three public datasets.|paper investig contribut color name salient object detect input imag first convert color name space consist probabilist channel explor topolog structur relationship figur ground obtain salienc map linear combin set sequenti attent map overcom limit onli exploit surrounded cue two global cue respect color name invok guid comput anoth weight salienc map final integr two salienc map unifi framework infer salienc result addit improv post process procedur introduc effect suppress background uniform highlight salient object experiment result show propos model produc accur salienc map perform well salienc model term three evalu metric three public dataset|['Jing Lou', 'Huan Wang', 'Longtao Chen', 'Qingyuan Xia', 'Wei Zhu', 'Mingwu Ren']|['cs.CV', 'I.4']
2017-03-28T14:08:48Z|2017-03-27T01:44:41Z|http://arxiv.org/abs/1703.08897v1|http://arxiv.org/pdf/1703.08897v1|Transductive Zero-Shot Learning with Adaptive Structural Embedding|transduct zero shot learn adapt structur embed|Zero-shot learning (ZSL) endows the computer vision system with the inferential capability to recognize instances of a new category that has never seen before. Two fundamental challenges in it are visual-semantic embedding and domain adaptation in cross-modality learning and unseen class prediction steps, respectively. To address both challenges, this paper presents two corresponding methods named Adaptive STructural Embedding (ASTE) and Self-PAsed Selective Strategy (SPASS), respectively. Specifically, ASTE formulates the visualsemantic interactions in a latent structural SVM framework to adaptively adjust the slack variables to embody the different reliableness among training instances. In this way, the reliable instances are imposed with small punishments, wheras the less reliable instances are imposed with more severe punishments. Thus, it ensures a more discriminative embedding. On the other hand, SPASS offers a framework to alleviate the domain shift problem in ZSL, which exploits the unseen data in an easy to hard fashion. Particularly, SPASS borrows the idea from selfpaced learning by iteratively selecting the unseen instances from reliable to less reliable to gradually adapt the knowledge from the seen domain to the unseen domain. Subsequently, by combining SPASS and ASTE, we present a self-paced Transductive ASTE (TASTE) method to progressively reinforce the classification capacity. Extensive experiments on three benchmark datasets (i.e., AwA, CUB, and aPY) demonstrate the superiorities of ASTE and TASTE. Furthermore, we also propose a fast training (FT) strategy to improve the efficiency of most of existing ZSL methods. The FT strategy is surprisingly simple and general enough, which can speed up the training time of most existing methods by 4~300 times while holding the previous performance.|zero shot learn zsl endow comput vision system inferenti capabl recogn instanc new categori never seen befor two fundament challeng visual semant embed domain adapt cross modal learn unseen class predict step respect address challeng paper present two correspond method name adapt structur embed ast self pase select strategi spass respect specif ast formul visualsemant interact latent structur svm framework adapt adjust slack variabl embodi differ reliabl among train instanc way reliabl instanc impos small punish whera less reliabl instanc impos sever punish thus ensur discrimin embed hand spass offer framework allevi domain shift problem zsl exploit unseen data easi hard fashion particular spass borrow idea selfpac learn iter select unseen instanc reliabl less reliabl gradual adapt knowledg seen domain unseen domain subsequ combin spass ast present self pace transduct ast tast method progress reinforc classif capac extens experi three benchmark dataset awa cub api demonstr superior ast tast furthermor also propos fast train ft strategi improv effici exist zsl method ft strategi surpris simpl general enough speed train time exist method time hold previous perform|['Yunlong Yu', 'Zhong Ji', 'Jichang Guo', 'Yanwei Pang']|['cs.CV']
2017-03-28T14:08:52Z|2017-03-27T01:36:38Z|http://arxiv.org/abs/1703.08893v1|http://arxiv.org/pdf/1703.08893v1|Transductive Zero-Shot Learning with a Self-training dictionary approach|transduct zero shot learn self train dictionari approach|As an important and challenging problem in computer vision, zero-shot learning (ZSL) aims at automatically recognizing the instances from unseen object classes without training data. To address this problem, ZSL is usually carried out in the following two aspects: 1) capturing the domain distribution connections between seen classes data and unseen classes data; and 2) modeling the semantic interactions between the image feature space and the label embedding space. Motivated by these observations, we propose a bidirectional mapping based semantic relationship modeling scheme that seeks for crossmodal knowledge transfer by simultaneously projecting the image features and label embeddings into a common latent space. Namely, we have a bidirectional connection relationship that takes place from the image feature space to the latent space as well as from the label embedding space to the latent space. To deal with the domain shift problem, we further present a transductive learning approach that formulates the class prediction problem in an iterative refining process, where the object classification capacity is progressively reinforced through bootstrapping-based model updating over highly reliable instances. Experimental results on three benchmark datasets (AwA, CUB and SUN) demonstrate the effectiveness of the proposed approach against the state-of-the-art approaches.|import challeng problem comput vision zero shot learn zsl aim automat recogn instanc unseen object class without train data address problem zsl usual carri follow two aspect captur domain distribut connect seen class data unseen class data model semant interact imag featur space label embed space motiv observ propos bidirect map base semant relationship model scheme seek crossmod knowledg transfer simultan project imag featur label embed common latent space name bidirect connect relationship take place imag featur space latent space well label embed space latent space deal domain shift problem present transduct learn approach formul class predict problem iter refin process object classif capac progress reinforc bootstrap base model updat high reliabl instanc experiment result three benchmark dataset awa cub sun demonstr effect propos approach state art approach|['Yunlong Yu', 'Zhong Ji', 'Xi Li', 'Jichang Guo', 'Zhongfei Zhang', 'Haibin Ling', 'Fei Wu']|['cs.CV']
2017-03-28T14:08:52Z|2017-03-26T20:28:02Z|http://arxiv.org/abs/1703.08866v1|http://arxiv.org/pdf/1703.08866v1|Multi-View Deep Learning for Consistent Semantic Mapping with RGB-D   Cameras|multi view deep learn consist semant map rgb camera|Visual scene understanding is an important capability that enables robots to purposefully act in their environment. In this paper, we propose a novel approach to object-class segmentation from multiple RGB-D views using deep learning. We train a deep neural network to predict object-class semantics that is consistent from several view points in a semi-supervised way. At test time, the semantics predictions of our network can be fused more consistently in semantic keyframe maps than predictions of a network trained on individual views. We base our network architecture on a recent single-view deep learning approach to RGB and depth fusion for semantic object-class segmentation and enhance it with multi-scale loss minimization. We obtain the camera trajectory using RGB-D SLAM and warp the predictions of RGB-D images into ground-truth annotated frames in order to enforce multi-view consistency during training. At test time, predictions from multiple views are fused into keyframes. We propose and analyze several methods for enforcing multi-view consistency during training and testing. We evaluate the benefit of multi-view consistency training and demonstrate that pooling of deep features and fusion over multiple views outperforms single-view baselines on the NYUDv2 benchmark for semantic segmentation. Our end-to-end trained network achieves state-of-the-art performance on the NYUDv2 dataset in single-view segmentation as well as multi-view semantic fusion.|visual scene understand import capabl enabl robot purpos act environ paper propos novel approach object class segment multipl rgb view use deep learn train deep neural network predict object class semant consist sever view point semi supervis way test time semant predict network fuse consist semant keyfram map predict network train individu view base network architectur recent singl view deep learn approach rgb depth fusion semant object class segment enhanc multi scale loss minim obtain camera trajectori use rgb slam warp predict rgb imag ground truth annot frame order enforc multi view consist dure train test time predict multipl view fuse keyfram propos analyz sever method enforc multi view consist dure train test evalu benefit multi view consist train demonstr pool deep featur fusion multipl view outperform singl view baselin nyudv benchmark semant segment end end train network achiev state art perform nyudv dataset singl view segment well multi view semant fusion|['Lingni Ma', 'J√∂rg St√ºckler', 'Christian Kerl', 'Daniel Cremers']|['cs.CV']
2017-03-28T14:08:52Z|2017-03-26T16:20:36Z|http://arxiv.org/abs/1703.08840v1|http://arxiv.org/pdf/1703.08840v1|Inferring The Latent Structure of Human Decision-Making from Raw Visual   Inputs|infer latent structur human decis make raw visual input|The goal of imitation learning is to match example expert behavior, without access to a reinforcement signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are not explicitly modeled. We introduce an extension to the Generative Adversarial Imitation Learning method that can infer the latent structure of human decision-making in an unsupervised way. Our method can not only imitate complex behaviors, but also learn interpretable and meaningful representations. We demonstrate that the approach is applicable to high-dimensional environments including raw visual inputs. In the highway driving domain, we show that a model learned from demonstrations is able to both produce different styles of human-like driving behaviors and accurately anticipate human actions. Our method surpasses various baselines in terms of performance and functionality.|goal imit learn match exampl expert behavior without access reinforc signal expert demonstr provid human howev often show signific variabl due latent factor explicit model introduc extens generat adversari imit learn method infer latent structur human decis make unsupervis way method onli imit complex behavior also learn interpret meaning represent demonstr approach applic high dimension environ includ raw visual input highway drive domain show model learn demonstr abl produc differ style human like drive behavior accur anticip human action method surpass various baselin term perform function|['Yunzhu Li', 'Jiaming Song', 'Stefano Ermon']|['cs.LG', 'cs.AI', 'cs.CV']
2017-03-28T14:08:52Z|2017-03-26T16:18:48Z|http://arxiv.org/abs/1703.08837v1|http://arxiv.org/abs/1703.08837v1|Person Re-Identification by Camera Correlation Aware Feature   Augmentation|person identif camera correl awar featur augment|The challenge of person re-identification (re-id) is to match individual images of the same person captured by different non-overlapping camera views against significant and unknown cross-view feature distortion. While a large number of distance metric/subspace learning models have been developed for re-id, the cross-view transformations they learned are view-generic and thus potentially less effective in quantifying the feature distortion inherent to each camera view. Learning view-specific feature transformations for re-id (i.e., view-specific re-id), an under-studied approach, becomes an alternative resort for this problem. In this work, we formulate a novel view-specific person re-identification framework from the feature augmentation point of view, called Camera coRrelation Aware Feature augmenTation (CRAFT). Specifically, CRAFT performs cross-view adaptation by automatically measuring camera correlation from cross-view visual data distribution and adaptively conducting feature augmentation to transform the original features into a new adaptive space. Through our augmentation framework, view-generic learning algorithms can be readily generalized to learn and optimize view-specific sub-models whilst simultaneously modelling view-generic discrimination information. Therefore, our framework not only inherits the strength of view-generic model learning but also provides an effective way to take into account view specific characteristics. Our CRAFT framework can be extended to jointly learn view-specific feature transformations for person re-id across a large network with more than two cameras, a largely under-investigated but realistic re-id setting. Additionally, we present a domain-generic deep person appearance representation which is designed particularly to be towards view invariant for facilitating cross-view adaptation by CRAFT.|challeng person identif id match individu imag person captur differ non overlap camera view signific unknown cross view featur distort larg number distanc metric subspac learn model develop id cross view transform learn view generic thus potenti less effect quantifi featur distort inher camera view learn view specif featur transform id view specif id studi approach becom altern resort problem work formul novel view specif person identif framework featur augment point view call camera correl awar featur augment craft specif craft perform cross view adapt automat measur camera correl cross view visual data distribut adapt conduct featur augment transform origin featur new adapt space augment framework view generic learn algorithm readili general learn optim view specif sub model whilst simultan model view generic discrimin inform therefor framework onli inherit strength view generic model learn also provid effect way take account view specif characterist craft framework extend joint learn view specif featur transform person id across larg network two camera larg investig realist id set addit present domain generic deep person appear represent design particular toward view invari facilit cross view adapt craft|['Ying-Cong Chen', 'Xiatian Zhu', 'Wei-Shi Zheng', 'Jian-Huang Lai']|['cs.CV']
2017-03-28T14:08:52Z|2017-03-26T16:17:55Z|http://arxiv.org/abs/1703.08836v1|http://arxiv.org/pdf/1703.08836v1|Learned multi-patch similarity|learn multi patch similar|Estimating a depth map from multiple views of a scene is a fundamental task in computer vision. As soon as more than two viewpoints are available, one faces the very basic question how to measure similarity across >2 image patches. Surprisingly, no direct solution exists, instead it is common to fall back to more or less robust averaging of two-view similarities. Encouraged by the success of machine learning, and in particular convolutional neural networks, we propose to learn a matching function which directly maps multiple image patches to a scalar similarity score. Experiments on several multi-view datasets demonstrate that this approach has advantages over methods based on pairwise patch similarity.|estim depth map multipl view scene fundament task comput vision soon two viewpoint avail one face veri basic question measur similar across imag patch surpris direct solut exist instead common fall back less robust averag two view similar encourag success machin learn particular convolut neural network propos learn match function direct map multipl imag patch scalar similar score experi sever multi view dataset demonstr approach advantag method base pairwis patch similar|['Wilfried Hartmann', 'Silvano Galliani', 'Michal Havlena', 'Konrad Schindler', 'Luc Van Gool']|['cs.CV', 'cs.LG']
2017-03-28T14:08:52Z|2017-03-26T06:34:45Z|http://arxiv.org/abs/1703.08774v1|http://arxiv.org/pdf/1703.08774v1|Who Said What: Modeling Individual Labelers Improves Classification|said model individu label improv classif|Data are often labeled by many different experts with each expert only labeling a small fraction of the data and each data point being labeled by several experts. This reduces the workload on individual experts and also gives a better estimate of the unobserved ground truth. When experts disagree, the standard approaches are to treat the majority opinion as the correct label or to model the correct label as a distribution. These approaches, however, do not make any use of potentially valuable information about which expert produced which label. To make use of this extra information, we propose modeling the experts individually and then learning averaging weights for combining them, possibly in sample-specific ways. This allows us to give more weight to more reliable experts and take advantage of the unique strengths of individual experts at classifying certain types of data. Here we show that our approach leads to improvements in computer-aided diagnosis of diabetic retinopathy. We also show that our method performs better than competing algorithms by Welinder and Perona, and by Mnih and Hinton. Our work offers an innovative approach for dealing with the myriad real-world settings that use expert opinions to define labels for training.|data often label mani differ expert expert onli label small fraction data data point label sever expert reduc workload individu expert also give better estim unobserv ground truth expert disagre standard approach treat major opinion correct label model correct label distribut approach howev make ani use potenti valuabl inform expert produc label make use extra inform propos model expert individu learn averag weight combin possibl sampl specif way allow us give weight reliabl expert take advantag uniqu strength individu expert classifi certain type data show approach lead improv comput aid diagnosi diabet retinopathi also show method perform better compet algorithm welind perona mnih hinton work offer innov approach deal myriad real world set use expert opinion defin label train|['Melody Y. Guan', 'Varun Gulshan', 'Andrew M. Dai', 'Geoffrey E. Hinton']|['cs.LG', 'cs.CV']
2017-03-28T14:08:52Z|2017-03-26T05:53:39Z|http://arxiv.org/abs/1703.08772v1|http://arxiv.org/pdf/1703.08772v1|Multivariate Regression with Gross Errors on Manifold-valued Data|multivari regress gross error manifold valu data|We consider the topic of multivariate regression on manifold-valued output, that is, for a multivariate observation, its output response lies on a manifold. Moreover, we propose a new regression model to deal with the presence of grossly corrupted manifold-valued responses, a bottleneck issue commonly encountered in practical scenarios. Our model first takes a correction step on the grossly corrupted responses via geodesic curves on the manifold, and then performs multivariate linear regression on the corrected data. This results in a nonconvex and nonsmooth optimization problem on manifolds. To this end, we propose a dedicated approach named PALMR, by utilizing and extending the proximal alternating linearized minimization techniques. Theoretically, we investigate its convergence property, where it is shown to converge to a critical point under mild conditions. Empirically, we test our model on both synthetic and real diffusion tensor imaging data, and show that our model outperforms other multivariate regression models when manifold-valued responses contain gross errors, and is effective in identifying gross errors.|consid topic multivari regress manifold valu output multivari observ output respons lie manifold moreov propos new regress model deal presenc grossli corrupt manifold valu respons bottleneck issu common encount practic scenario model first take correct step grossli corrupt respons via geodes curv manifold perform multivari linear regress correct data result nonconvex nonsmooth optim problem manifold end propos dedic approach name palmr util extend proxim altern linear minim techniqu theoret investig converg properti shown converg critic point mild condit empir test model synthet real diffus tensor imag data show model outperform multivari regress model manifold valu respons contain gross error effect identifi gross error|['Xiaowei Zhang', 'Xudong Shi', 'Yu Sun', 'Li Cheng']|['stat.ML', 'cs.CV', 'math.OC']
2017-03-28T14:08:52Z|2017-03-26T05:48:38Z|http://arxiv.org/abs/1703.08770v1|http://arxiv.org/pdf/1703.08770v1|SCAN: Structure Correcting Adversarial Network for Chest X-rays Organ   Segmentation|scan structur correct adversari network chest ray organ segment|Chest X-ray (CXR) is one of the most commonly prescribed medical imaging procedures, often with over 2-10x more scans than other imaging modalities such as MRI, CT scan, and PET scans. These voluminous CXR scans place significant workloads on radiologists and medical practitioners. Organ segmentation is a crucial step to obtain effective computer-aided detection on CXR. In this work, we propose Structure Correcting Adversarial Network (SCAN) to segment lung fields and the heart in CXR images. SCAN incorporates a critic network to impose on the convolutional segmentation network the structural regularities emerging from human physiology. During training, the critic network learns to discriminate between the ground truth organ annotations from the masks synthesized by the segmentation network. Through this adversarial process the critic network learns the higher order structures and guides the segmentation model to achieve realistic segmentation outcomes. Extensive experiments show that our method produces highly accurate and natural segmentation. Using only very limited training data available, our model reaches human-level performance without relying on any existing trained model or dataset. Our method also generalizes well to CXR images from a different patient population and disease profiles, surpassing the current state-of-the-art.|chest ray cxr one common prescrib medic imag procedur often scan imag modal mri ct scan pet scan volumin cxr scan place signific workload radiologist medic practition organ segment crucial step obtain effect comput aid detect cxr work propos structur correct adversari network scan segment lung field heart cxr imag scan incorpor critic network impos convolut segment network structur regular emerg human physiolog dure train critic network learn discrimin ground truth organ annot mask synthes segment network adversari process critic network learn higher order structur guid segment model achiev realist segment outcom extens experi show method produc high accur natur segment use onli veri limit train data avail model reach human level perform without reli ani exist train model dataset method also general well cxr imag differ patient popul diseas profil surpass current state art|['Wei Dai', 'Joseph Doyle', 'Xiaodan Liang', 'Hao Zhang', 'Nanqing Dong', 'Yuan Li', 'Eric P. Xing']|['cs.CV']
2017-03-28T14:08:52Z|2017-03-26T05:44:56Z|http://arxiv.org/abs/1703.08769v1|http://arxiv.org/pdf/1703.08769v1|Open Vocabulary Scene Parsing|open vocabulari scene pars|Recognizing arbitrary objects in the wild has been a challenging problem due to the limitations of existing classification models and datasets. In this paper, we propose a new task that aims at parsing scene with a large and open vocabulary, and several evaluation metrics are explored for this problem. Our proposed approach to this problem is a joint image pixel and word concept embeddings framework, where word concepts are connected by semantic relations. We validate the open vocabulary prediction ability of our framework on ADE20K dataset which covers a wide variety of scenes and objects. We further explore the trained joint embedding space to show its interpretability.|recogn arbitrari object wild challeng problem due limit exist classif model dataset paper propos new task aim pars scene larg open vocabulari sever evalu metric explor problem propos approach problem joint imag pixel word concept embed framework word concept connect semant relat valid open vocabulari predict abil framework adek dataset cover wide varieti scene object explor train joint embed space show interpret|['Hang Zhao', 'Xavier Puig', 'Bolei Zhou', 'Sanja Fidler', 'Antonio Torralba']|['cs.CV', 'cs.AI']
2017-03-28T14:08:52Z|2017-03-26T04:15:10Z|http://arxiv.org/abs/1703.08764v1|http://arxiv.org/pdf/1703.08764v1|Structured Learning of Tree Potentials in CRF for Image Segmentation|structur learn tree potenti crf imag segment|We propose a new approach to image segmentation, which exploits the advantages of both conditional random fields (CRFs) and decision trees. In the literature, the potential functions of CRFs are mostly defined as a linear combination of some pre-defined parametric models, and then methods like structured support vector machines (SSVMs) are applied to learn those linear coefficients. We instead formulate the unary and pairwise potentials as nonparametric forests---ensembles of decision trees, and learn the ensemble parameters and the trees in a unified optimization problem within the large-margin framework. In this fashion, we easily achieve nonlinear learning of potential functions on both unary and pairwise terms in CRFs. Moreover, we learn class-wise decision trees for each object that appears in the image. Due to the rich structure and flexibility of decision trees, our approach is powerful in modelling complex data likelihoods and label relationships. The resulting optimization problem is very challenging because it can have exponentially many variables and constraints. We show that this challenging optimization can be efficiently solved by combining a modified column generation and cutting-planes techniques. Experimental results on both binary (Graz-02, Weizmann horse, Oxford flower) and multi-class (MSRC-21, PASCAL VOC 2012) segmentation datasets demonstrate the power of the learned nonlinear nonparametric potentials.|propos new approach imag segment exploit advantag condit random field crfs decis tree literatur potenti function crfs defin linear combin pre defin parametr model method like structur support vector machin ssvms appli learn linear coeffici instead formul unari pairwis potenti nonparametr forest ensembl decis tree learn ensembl paramet tree unifi optim problem within larg margin framework fashion easili achiev nonlinear learn potenti function unari pairwis term crfs moreov learn class wise decis tree object appear imag due rich structur flexibl decis tree approach power model complex data likelihood label relationship result optim problem veri challeng becaus exponenti mani variabl constraint show challeng optim effici solv combin modifi column generat cut plane techniqu experiment result binari graz weizmann hors oxford flower multi class msrc pascal voc segment dataset demonstr power learn nonlinear nonparametr potenti|['Fayao Liu', 'Guosheng Lin', 'Ruizhi Qiao', 'Chunhua Shen']|['cs.CV']
2017-03-28T14:08:55Z|2017-03-25T20:33:45Z|http://arxiv.org/abs/1703.08738v1|http://arxiv.org/pdf/1703.08738v1|Sketch-based Face Editing in Video Using Identity Deformation Transfer|sketch base face edit video use ident deform transfer|We address the problem of using hand-drawn sketch to edit facial identity, such as enlarging the shape or modifying the position of eyes or mouth, in the whole video. This task is formulated as a 3D face model reconstruction and deformation problem. We first introduce a two-stage real-time 3D face model fitting schema to recover facial identity and expressions from the video. We recognize the user's editing intention from the input sketch as a set of facial modifications. A novel identity deformation algorithm is then proposed to transfer these deformations from 2D space to 3D facial identity directly, while preserving the facial expressions. Finally, these changes are propagated to the whole video with the modified identity. Experimental results demonstrate that our method can effectively edit facial identity in video based on the input sketch with high consistency and fidelity.|address problem use hand drawn sketch edit facial ident enlarg shape modifi posit eye mouth whole video task formul face model reconstruct deform problem first introduc two stage real time face model fit schema recov facial ident express video recogn user edit intent input sketch set facial modif novel ident deform algorithm propos transfer deform space facial ident direct preserv facial express final chang propag whole video modifi ident experiment result demonstr method effect edit facial ident video base input sketch high consist fidel|['Long Zhao', 'Fangda Han', 'Mubbasir Kapadia', 'Vladimir Pavlovic', 'Dimitris Metaxas']|['cs.CV']
2017-03-28T14:08:56Z|2017-03-25T16:49:03Z|http://arxiv.org/abs/1703.08710v1|http://arxiv.org/pdf/1703.08710v1|Count-ception: Counting by Fully Convolutional Redundant Counting|count ception count fulli convolut redund count|Counting objects in digital images is a process that should be replaced by machines. This tedious task is time consuming and prone to errors due to fatigue of human annotators. The goal is to have a system that takes as input an image and returns a count of the objects inside and justification for the prediction in the form of object localization. We repose a problem, originally posed by Lempitsky and Zisserman, to instead predict a count map which contains redundant counts based on the receptive field of a smaller regression network. The regression network predicts a count of the objects that exist inside this frame. By processing the image in a fully convolutional way each pixel is going to be accounted for some number of times, the number of windows which include it, which is the size of each window, (i.e., 32x32 = 1024). To recover the true count take the average over the redundant predictions. Our contribution is redundant counting instead of predicting a density map in order to average over errors. We also propose a novel deep neural network architecture adapted from the Inception family of networks called the Count-ception network. Together our approach results in a 20% gain over the state of the art method by Xie, Noble, and Zisserman in 2016.|count object digit imag process replac machin tedious task time consum prone error due fatigu human annot goal system take input imag return count object insid justif predict form object local repos problem origin pose lempitski zisserman instead predict count map contain redund count base recept field smaller regress network regress network predict count object exist insid frame process imag fulli convolut way pixel go account number time number window includ size window recov true count take averag redund predict contribut redund count instead predict densiti map order averag error also propos novel deep neural network architectur adapt incept famili network call count ception network togeth approach result gain state art method xie nobl zisserman|['Joseph Paul Cohen', 'Henry Z. Lo', 'Yoshua Bengio']|['cs.CV', 'cs.LG', 'stat.ML']
2017-03-28T14:08:56Z|2017-03-25T14:36:12Z|http://arxiv.org/abs/1703.08697v1|http://arxiv.org/abs/1703.08697v1|Improving the Accuracy of the CogniLearn System for Cognitive Behavior   Assessment|improv accuraci cognilearn system cognit behavior assess|"HTKS is a game-like cognitive assessment method, designed for children between four and eight years of age. During the HTKS assessment, a child responds to a sequence of requests, such as ""touch your head"" or ""touch your toes"". The cognitive challenge stems from the fact that the children are instructed to interpret these requests not literally, but by touching a different body part than the one stated. In prior work, we have developed the CogniLearn system, that captures data from subjects performing the HTKS game, and analyzes the motion of the subjects. In this paper we propose some specific improvements that make the motion analysis module more accurate. As a result of these improvements, the accuracy in recognizing cases where subjects touch their toes has gone from 76.46% in our previous work to 97.19% in this paper."|htks game like cognit assess method design children four eight year age dure htks assess child respond sequenc request touch head touch toe cognit challeng stem fact children instruct interpret request liter touch differ bodi part one state prior work develop cognilearn system captur data subject perform htks game analyz motion subject paper propos specif improv make motion analysi modul accur result improv accuraci recogn case subject touch toe gone previous work paper|['Amir Ghaderi', 'Srujana Gattupalli', 'Dylan Ebert', 'Ali Sharifara', 'Vassilis Athitsos', 'Fillia Makedon']|['cs.CV']
2017-03-28T14:08:56Z|2017-03-25T06:18:38Z|http://arxiv.org/abs/1703.08653v1|http://arxiv.org/pdf/1703.08653v1|Bayesian Optimization for Refining Object Proposals|bayesian optim refin object propos|We develop a general-purpose algorithm using a Bayesian optimization framework for the efficient refinement of object proposals. While recent research has achieved substantial progress for object localization and related objectives in computer vision, current state-of-the-art object localization procedures are nevertheless encumbered by inefficiency and inaccuracy. We present a novel, computationally efficient method for refining inaccurate bounding-box proposals for a target object using Bayesian optimization. Offline, image features from a convolutional neural network are used to train a model to predict the offset distance of an object proposal from a target object. Online, this model is used in a Bayesian active search to improve inaccurate object proposals. In experiments, we compare our approach to a state-of-the-art bounding-box regression method for localization refinement of pedestrian object proposals. Our method exhibits a substantial improvement for the task of localization refinement over this baseline regression method.|develop general purpos algorithm use bayesian optim framework effici refin object propos recent research achiev substanti progress object local relat object comput vision current state art object local procedur nevertheless encumb ineffici inaccuraci present novel comput effici method refin inaccur bound box propos target object use bayesian optim offlin imag featur convolut neural network use train model predict offset distanc object propos target object onlin model use bayesian activ search improv inaccur object propos experi compar approach state art bound box regress method local refin pedestrian object propos method exhibit substanti improv task local refin baselin regress method|['Anthony D. Rhodes', 'Jordan Witte', 'Melanie Mitchell', 'Bruno Jedynak']|['cs.CV']
2017-03-28T14:08:56Z|2017-03-25T05:51:42Z|http://arxiv.org/abs/1703.08651v1|http://arxiv.org/pdf/1703.08651v1|More is Less: A More Complicated Network with Less Inference Complexity|less complic network less infer complex|In this paper, we present a novel and general network structure towards accelerating the inference process of convolutional neural networks, which is more complicated in network structure yet with less inference complexity. The core idea is to equip each original convolutional layer with another low-cost collaborative layer (LCCL), and the element-wise multiplication of the ReLU outputs of these two parallel layers produces the layer-wise output. The combined layer is potentially more discriminative than the original convolutional layer, and its inference is faster for two reasons: 1) the zero cells of the LCCL feature maps will remain zero after element-wise multiplication, and thus it is safe to skip the calculation of the corresponding high-cost convolution in the original convolutional layer, 2) LCCL is very fast if it is implemented as a 1*1 convolution or only a single filter shared by all channels. Extensive experiments on the CIFAR-10, CIFAR-100 and ILSCRC-2012 benchmarks show that our proposed network structure can accelerate the inference process by 32\% on average with negligible performance drop.|paper present novel general network structur toward acceler infer process convolut neural network complic network structur yet less infer complex core idea equip origin convolut layer anoth low cost collabor layer lccl element wise multipl relu output two parallel layer produc layer wise output combin layer potenti discrimin origin convolut layer infer faster two reason zero cell lccl featur map remain zero element wise multipl thus safe skip calcul correspond high cost convolut origin convolut layer lccl veri fast implement convolut onli singl filter share channel extens experi cifar cifar ilscrc benchmark show propos network structur acceler infer process averag neglig perform drop|['Xuanyi Dong', 'Junshi Huang', 'Yi Yang', 'Shuicheng Yan']|['cs.CV']
2017-03-28T14:08:56Z|2017-03-24T23:50:52Z|http://arxiv.org/abs/1703.08628v1|http://arxiv.org/pdf/1703.08628v1|AMAT: Medial Axis Transform for Natural Images|amat medial axi transform natur imag|The medial axis transform (MAT) is a powerful shape abstraction that has been successfully used in shape editing, matching and retrieval. Despite its long history, the MAT has not found widespread use in tasks involving natural images, due to the lack of a generalization that accommodates color and texture. In this paper we introduce Appearance-MAT (AMAT), by framing the MAT of natural images as a weighted geometric set cover problem. We make the following contributions: i) we extend previous medial point detection methods for color images, by associating each medial point with a local scale; ii) inspired by the invertibility property of the binary MAT, we also associate each medial point with a local encoding that allows us to invert the AMAT, reconstructing the input image; iii) we describe a clustering scheme that takes advantage of the additional scale and appearance information to group individual points into medial branches, providing a shape decomposition of the underlying image regions. In our experiments, we show state-of-the-art performance in medial point detection on Berkeley Medial AXes (BMAX500), a new dataset of medial axes based on the established BSDS500 database. We also measure the quality of reconstructed images from the same dataset, obtained by inverting their computed AMAT. Our approach delivers significantly better reconstruction quality with respect to three baselines, using just 10% of the image pixels. Our code is available at https://github.com/tsogkas/amat.|medial axi transform mat power shape abstract success use shape edit match retriev despit long histori mat found widespread use task involv natur imag due lack general accommod color textur paper introduc appear mat amat frame mat natur imag weight geometr set cover problem make follow contribut extend previous medial point detect method color imag associ medial point local scale ii inspir invert properti binari mat also associ medial point local encod allow us invert amat reconstruct input imag iii describ cluster scheme take advantag addit scale appear inform group individu point medial branch provid shape decomposit imag region experi show state art perform medial point detect berkeley medial axe bmax new dataset medial axe base establish bsds databas also measur qualiti reconstruct imag dataset obtain invert comput amat approach deliv signific better reconstruct qualiti respect three baselin use imag pixel code avail https github com tsogka amat|['Stavros Tsogkas', 'Sven Dickinson']|['cs.CV']
2017-03-28T14:08:56Z|2017-03-24T22:43:05Z|http://arxiv.org/abs/1703.08617v1|http://arxiv.org/pdf/1703.08617v1|Temporal Non-Volume Preserving Approach to Facial Age-Progression and   Age-Invariant Face Recognition|tempor non volum preserv approach facial age progress age invari face recognit|Modeling the long-term facial aging process is extremely challenging due to the presence of large and non-linear variations during the face development stages. In order to efficiently address the problem, this work first decomposes the aging process into multiple short-term stages. Then, a novel generative probabilistic model, named Temporal Non-Volume Preserving (TNVP) transformation, is presented to model the facial aging process at each stage. Unlike Generative Adversarial Networks (GANs), which requires an empirical balance threshold, and Restricted Boltzmann Machines (RBM), an intractable model, our proposed TNVP approach guarantees a tractable density function, exact inference and evaluation for embedding the feature transformations between faces in consecutive stages. Our model shows its advantages not only in capturing the non-linear age related variance in each stage but also producing a smooth synthesis in age progression across faces. Our approach can model any face in the wild provided with only four basic landmark points. Moreover, the structure can be transformed into a deep convolutional network while keeping the advantages of probabilistic models with tractable log-likelihood density estimation. Our method is evaluated in both terms of synthesizing age-progressed faces and cross-age face verification and consistently shows the state-of-the-art results in various face aging databases, i.e. FG-NET, MORPH, AginG Faces in the Wild (AGFW), and Cross-Age Celebrity Dataset (CACD). A large-scale face verification on Megaface challenge 1 is also performed to further show the advantages of our proposed approach.|model long term facial age process extrem challeng due presenc larg non linear variat dure face develop stage order effici address problem work first decompos age process multipl short term stage novel generat probabilist model name tempor non volum preserv tnvp transform present model facial age process stage unlik generat adversari network gan requir empir balanc threshold restrict boltzmann machin rbm intract model propos tnvp approach guarante tractabl densiti function exact infer evalu embed featur transform face consecut stage model show advantag onli captur non linear age relat varianc stage also produc smooth synthesi age progress across face approach model ani face wild provid onli four basic landmark point moreov structur transform deep convolut network keep advantag probabilist model tractabl log likelihood densiti estim method evalu term synthes age progress face cross age face verif consist show state art result various face age databas fg net morph age face wild agfw cross age celebr dataset cacd larg scale face verif megafac challeng also perform show advantag propos approach|['Chi Nhan Duong', 'Kha Gia Quach', 'Khoa Luu', 'T. Hoang Ngan le', 'Marios Savvides']|['cs.CV']
2017-03-28T14:08:56Z|2017-03-24T21:26:16Z|http://arxiv.org/abs/1703.08603v1|http://arxiv.org/pdf/1703.08603v1|Adversarial Examples for Semantic Segmentation and Object Detection|adversari exampl semant segment object detect|It has been well demonstrated that adversarial examples, i.e., natural images with visually imperceptible perturbations added, generally exist for deep networks to fail on image classification. In this paper, we extend adversarial examples to semantic segmentation and object detection which are much more difficult. Our observation is that both segmentation and detection are based on classifying multiple targets on an image (e.g., the basic target is a pixel or a receptive field in segmentation, and an object proposal in detection), which inspires us to optimize a loss function over a set of pixels/proposals for generating adversarial perturbations. Based on this idea, we propose a novel algorithm named Dense Adversary Generation (DAG), which generates a large family of adversarial examples, and applies to a wide range of state-of-the-art deep networks for segmentation and detection. We also find that the adversarial perturbations can be transferred across networks with different training data, based on different architectures, and even for different recognition tasks. In particular, the transferability across networks with the same architecture is more significant than in other cases. Besides, summing up heterogeneous perturbations often leads to better transfer performance, which provides an effective method of black-box adversarial attack.|well demonstr adversari exampl natur imag visual impercept perturb ad general exist deep network fail imag classif paper extend adversari exampl semant segment object detect much difficult observ segment detect base classifi multipl target imag basic target pixel recept field segment object propos detect inspir us optim loss function set pixel propos generat adversari perturb base idea propos novel algorithm name dens adversari generat dag generat larg famili adversari exampl appli wide rang state art deep network segment detect also find adversari perturb transfer across network differ train data base differ architectur even differ recognit task particular transfer across network architectur signific case besid sum heterogen perturb often lead better transfer perform provid effect method black box adversari attack|['Cihang Xie', 'Jianyu Wang', 'Zhishuai Zhang', 'Yuyin Zhou', 'Lingxi Xie', 'Alan Yuille']|['cs.CV']
2017-03-28T14:08:56Z|2017-03-24T19:43:20Z|http://arxiv.org/abs/1703.08580v1|http://arxiv.org/pdf/1703.08580v1|Deep Residual Learning for Instrument Segmentation in Robotic Surgery|deep residu learn instrument segment robot surgeri|Detection, tracking, and pose estimation of surgical instruments are crucial tasks for computer assistance during minimally invasive robotic surgery. In the majority of cases, the first step is the automatic segmentation of surgical tools. Prior work has focused on binary segmentation, where the objective is to label every pixel in an image as tool or background. We improve upon previous work in two major ways. First, we leverage recent techniques such as deep residual learning and dilated convolutions to advance binary-segmentation performance. Second, we extend the approach to multi-class segmentation, which lets us segment different parts of the tool, in addition to background. We demonstrate the performance of this method on the MICCAI Endoscopic Vision Challenge Robotic Instruments dataset.|detect track pose estim surgic instrument crucial task comput assist dure minim invas robot surgeri major case first step automat segment surgic tool prior work focus binari segment object label everi pixel imag tool background improv upon previous work two major way first leverag recent techniqu deep residu learn dilat convolut advanc binari segment perform second extend approach multi class segment let us segment differ part tool addit background demonstr perform method miccai endoscop vision challeng robot instrument dataset|['Daniil Pakhomov', 'Vittal Premachandran', 'Max Allan', 'Mahdi Azizian', 'Nassir Navab']|['cs.CV']
2017-03-28T14:08:56Z|2017-03-24T17:14:58Z|http://arxiv.org/abs/1703.08516v1|http://arxiv.org/pdf/1703.08516v1|Radiomics strategies for risk assessment of tumour failure in   head-and-neck cancer|radiom strategi risk assess tumour failur head neck cancer|Quantitative extraction of high-dimensional mineable data from medical images is a process known as radiomics. Radiomics is foreseen as an essential prognostic tool for cancer risk assessment and the quantification of intratumoural heterogeneity. In this work, 1615 radiomic features (quantifying tumour image intensity, shape, texture) extracted from pre-treatment FDG-PET and CT images of 300 patients from four different cohorts were analyzed for the risk assessment of locoregional recurrences (LR) and distant metastases (DM) in head-and-neck cancer. Prediction models combining radiomic and clinical variables were constructed via random forests and imbalance-adjustment strategies using two of the four cohorts. Independent validation of the prediction and prognostic performance of the models was carried out on the other two cohorts (LR: AUC = 0.69 and CI = 0.67; DM: AUC = 0.86 and CI = 0.88). Furthermore, the results obtained via Kaplan-Meier analysis demonstrated the potential of radiomics for assessing the risk of specific tumour outcomes using multiple stratification groups. This could have important clinical impact, notably by allowing for a better personalization of chemo-radiation treatments for head-and-neck cancer patients from different risk groups.|quantit extract high dimension mineabl data medic imag process known radiom radiom foreseen essenti prognost tool cancer risk assess quantif intratumour heterogen work radiom featur quantifi tumour imag intens shape textur extract pre treatment fdg pet ct imag patient four differ cohort analyz risk assess locoregion recurr lr distant metastas dm head neck cancer predict model combin radiom clinic variabl construct via random forest imbal adjust strategi use two four cohort independ valid predict prognost perform model carri two cohort lr auc ci dm auc ci furthermor result obtain via kaplan meier analysi demonstr potenti radiom assess risk specif tumour outcom use multipl stratif group could import clinic impact notabl allow better person chemo radiat treatment head neck cancer patient differ risk group|['Martin Valli√®res', 'Emily Kay-Rivest', 'L√©o Jean Perrin', 'Xavier Liem', 'Christophe Furstoss', 'Hugo J. W. L. Aerts', 'Nader Khaouam', 'Phuc Felix Nguyen-Tan', 'Chang-Shu Wang', 'Khalil Sultanem', 'Jan Seuntjens', 'Issam El Naqa']|['cs.CV', 'I.2.1; I.2.10; I.4.7; I.4.9; J.3']
2017-03-28T14:09:00Z|2017-03-24T16:41:19Z|http://arxiv.org/abs/1703.08497v1|http://arxiv.org/pdf/1703.08497v1|Local Deep Neural Networks for Age and Gender Classification|local deep neural network age gender classif|Local deep neural networks have been recently introduced for gender recognition. Although, they achieve very good performance they are very computationally expensive to train. In this work, we introduce a simplified version of local deep neural networks which significantly reduces the training time. Instead of using hundreds of patches per image, as suggested by the original method, we propose to use 9 overlapping patches per image which cover the entire face region. This results in a much reduced training time, since just 9 patches are extracted per image instead of hundreds, at the expense of a slightly reduced performance. We tested the proposed modified local deep neural networks approach on the LFW and Adience databases for the task of gender and age classification. For both tasks and both databases the performance is up to 1% lower compared to the original version of the algorithm. We have also investigated which patches are more discriminative for age and gender classification. It turns out that the mouth and eyes regions are useful for age classification, whereas just the eye region is useful for gender classification.|local deep neural network recent introduc gender recognit although achiev veri good perform veri comput expens train work introduc simplifi version local deep neural network signific reduc train time instead use hundr patch per imag suggest origin method propos use overlap patch per imag cover entir face region result much reduc train time sinc patch extract per imag instead hundr expens slight reduc perform test propos modifi local deep neural network approach lfw adienc databas task gender age classif task databas perform lower compar origin version algorithm also investig patch discrimin age gender classif turn mouth eye region use age classif wherea eye region use gender classif|['Zukang Liao', 'Stavros Petridis', 'Maja Pantic']|['cs.CV']
2017-03-28T14:09:00Z|2017-03-24T16:28:57Z|http://arxiv.org/abs/1703.08493v1|http://arxiv.org/pdf/1703.08493v1|Multi-stage Multi-recursive-input Fully Convolutional Networks for   Neuronal Boundary Detection|multi stage multi recurs input fulli convolut network neuron boundari detect|In the field of connectomics, neuroscientists seek to identify cortical connectivity comprehensively. Neuronal boundary detection from the Electron Microscopy (EM) images is often done to assist the automatic reconstruction of neuronal circuit. But the segmentation of EM images is a challenging problem, as it requires the detector to be able to detect both filament-like thin and blob-like thick membrane, while suppressing the ambiguous intracellular structure. In this paper, we propose multi-stage multi-recursive-input fully convolutional networks to address this problem. The multiple recursive inputs for one stage, i.e., the multiple side outputs with different receptive field sizes learned from the lower stage, provide multi-scale contextual boundary information for the consecutive learning. This design is biologically-plausible, as it likes a human visual system to compare different possible segmentation solutions to address the ambiguous boundary issue. Our multi-stage networks are trained end-to-end. It achieves promising results on a public available mouse piriform cortex dataset, which significantly outperforms other competitors.|field connectom neuroscientist seek identifi cortic connect comprehens neuron boundari detect electron microscopi em imag often done assist automat reconstruct neuron circuit segment em imag challeng problem requir detector abl detect filament like thin blob like thick membran suppress ambigu intracellular structur paper propos multi stage multi recurs input fulli convolut network address problem multipl recurs input one stage multipl side output differ recept field size learn lower stage provid multi scale contextu boundari inform consecut learn design biolog plausibl like human visual system compar differ possibl segment solut address ambigu boundari issu multi stage network train end end achiev promis result public avail mous piriform cortex dataset signific outperform competitor|['Wei Shen', 'Bin Wang', 'Yuan Jiang', 'Yan Wang', 'Alan Yuille']|['cs.CV']
2017-03-28T14:09:00Z|2017-03-24T16:27:57Z|http://arxiv.org/abs/1703.08492v1|http://arxiv.org/pdf/1703.08492v1|Content-Based Image Retrieval Based on Late Fusion of Binary and Local   Descriptors|content base imag retriev base late fusion binari local descriptor|One of the challenges in Content-Based Image Retrieval (CBIR) is to reduce the semantic gaps between low-level features and high-level semantic concepts. In CBIR, the images are represented in the feature space and the performance of CBIR depends on the type of selected feature representation. Late fusion also known as visual words integration is applied to enhance the performance of image retrieval. The recent advances in image retrieval diverted the focus of research towards the use of binary descriptors as they are reported computationally efficient. In this paper, we aim to investigate the late fusion of Fast Retina Keypoint (FREAK) and Scale Invariant Feature Transform (SIFT). The late fusion of binary and local descriptor is selected because among binary descriptors, FREAK has shown good results in classification-based problems while SIFT is robust to translation, scaling, rotation and small distortions. The late fusion of FREAK and SIFT integrates the performance of both feature descriptors for an effective image retrieval. Experimental results and comparisons show that the proposed late fusion enhances the performances of image retrieval.|one challeng content base imag retriev cbir reduc semant gap low level featur high level semant concept cbir imag repres featur space perform cbir depend type select featur represent late fusion also known visual word integr appli enhanc perform imag retriev recent advanc imag retriev divert focus research toward use binari descriptor report comput effici paper aim investig late fusion fast retina keypoint freak scale invari featur transform sift late fusion binari local descriptor select becaus among binari descriptor freak shown good result classif base problem sift robust translat scale rotat small distort late fusion freak sift integr perform featur descriptor effect imag retriev experiment result comparison show propos late fusion enhanc perform imag retriev|['Nouman Ali', 'Danish Ali Mazhar', 'Zeshan Iqbal', 'Rehan Ashraf', 'Jawad Ahmed', 'Farrukh Zeeshan Khan']|['cs.CV']
2017-03-28T14:09:00Z|2017-03-24T15:41:01Z|http://arxiv.org/abs/1703.08472v1|http://arxiv.org/pdf/1703.08472v1|Medical Image Retrieval using Deep Convolutional Neural Network|medic imag retriev use deep convolut neural network|With a widespread use of digital imaging data in hospitals, the size of medical image repositories is increasing rapidly. This causes difficulty in managing and querying these large databases leading to the need of content based medical image retrieval (CBMIR) systems. A major challenge in CBMIR systems is the semantic gap that exists between the low level visual information captured by imaging devices and high level semantic information perceived by human. The efficacy of such systems is more crucial in terms of feature representations that can characterize the high-level information completely. In this paper, we propose a framework of deep learning for CBMIR system by using deep Convolutional Neural Network (CNN) that is trained for classification of medical images. An intermodal dataset that contains twenty four classes and five modalities is used to train the network. The learned features and the classification results are used to retrieve medical images. For retrieval, best results are achieved when class based predictions are used. An average classification accuracy of 99.77% and a mean average precision of 0.69 is achieved for retrieval task. The proposed method is best suited to retrieve multimodal medical images for different body organs.|widespread use digit imag data hospit size medic imag repositori increas rapid caus difficulti manag queri larg databas lead need content base medic imag retriev cbmir system major challeng cbmir system semant gap exist low level visual inform captur imag devic high level semant inform perceiv human efficaci system crucial term featur represent character high level inform complet paper propos framework deep learn cbmir system use deep convolut neural network cnn train classif medic imag intermod dataset contain twenti four class five modal use train network learn featur classif result use retriev medic imag retriev best result achiev class base predict use averag classif accuraci mean averag precis achiev retriev task propos method best suit retriev multimod medic imag differ bodi organ|['Adnan Qayyum', 'Syed Muhammad Anwar', 'Muhammad Awais', 'Muhammad Majid']|['cs.CV']
2017-03-28T14:09:00Z|2017-03-27T07:23:05Z|http://arxiv.org/abs/1703.08448v2|http://arxiv.org/pdf/1703.08448v2|Object Region Mining with Adversarial Erasing: A Simple Classification   to Semantic Segmentation Approach|object region mine adversari eras simpl classif semant segment approach|We investigate a principle way to progressively mine discriminative object regions using classification networks to address the weakly-supervised semantic segmentation problems. Classification networks are only responsive to small and sparse discriminative regions from the object of interest, which deviates from the requirement of the segmentation task that needs to localize dense, interior and integral regions for pixel-wise inference. To mitigate this gap, we propose a new adversarial erasing approach for localizing and expanding object regions progressively. Starting with a single small object region, our proposed approach drives the classification network to sequentially discover new and complement object regions by erasing the current mined regions in an adversarial manner. These localized regions eventually constitute a dense and complete object region for learning semantic segmentation. To further enhance the quality of the discovered regions by adversarial erasing, an online prohibitive segmentation learning approach is developed to collaborate with adversarial erasing by providing auxiliary segmentation supervision modulated by the more reliable classification scores. Despite its apparent simplicity, the proposed approach achieves 55.0% and 55.7% mean Intersection-over-Union (mIoU) scores on PASCAL VOC 2012 val and test sets, which are the new state-of-the-arts.|investig principl way progress mine discrimin object region use classif network address weak supervis semant segment problem classif network onli respons small spars discrimin region object interest deviat requir segment task need local dens interior integr region pixel wise infer mitig gap propos new adversari eras approach local expand object region progress start singl small object region propos approach drive classif network sequenti discov new complement object region eras current mine region adversari manner local region eventu constitut dens complet object region learn semant segment enhanc qualiti discov region adversari eras onlin prohibit segment learn approach develop collabor adversari eras provid auxiliari segment supervis modul reliabl classif score despit appar simplic propos approach achiev mean intersect union miou score pascal voc val test set new state art|['Yunchao Wei', 'Jiashi Feng', 'Xiaodan Liang', 'Ming-Ming Cheng', 'Yao Zhao', 'Shuicheng Yan']|['cs.CV']
2017-03-28T14:09:00Z|2017-03-24T12:41:38Z|http://arxiv.org/abs/1703.08388v1|http://arxiv.org/pdf/1703.08388v1|DeepVisage: Making face recognition simple yet with powerful   generalization skills|deepvisag make face recognit simpl yet power general skill|Face recognition (FR) methods report significant performance by adopting the convolutional neural network (CNN) based learning methods. Although CNNs are mostly trained by optimizing the softmax loss, the recent trend shows an improvement of accuracy with different strategies, such as task-specific CNN learning with different loss functions, fine-tuning on target dataset, metric learning and concatenating features from multiple CNNs. Incorporating these tasks obviously requires additional efforts. Moreover, it demotivates the discovery of efficient CNN models for FR which are trained only with identity labels. We focus on this fact and propose an easily trainable and single CNN based FR method. Our CNN model exploits the residual learning framework. Additionally, it uses normalized features to compute the loss. Our extensive experiments show excellent generalization on different datasets. We obtain very competitive and state-of-the-art results on the LFW, IJB-A, YouTube faces and CACD datasets.|face recognit fr method report signific perform adopt convolut neural network cnn base learn method although cnns train optim softmax loss recent trend show improv accuraci differ strategi task specif cnn learn differ loss function fine tune target dataset metric learn concaten featur multipl cnns incorpor task obvious requir addit effort moreov demotiv discoveri effici cnn model fr train onli ident label focus fact propos easili trainabl singl cnn base fr method cnn model exploit residu learn framework addit use normal featur comput loss extens experi show excel general differ dataset obtain veri competit state art result lfw ijb youtub face cacd dataset|['Abul Hasnat', 'Julien Bohn√©', 'St√©phane Gentric', 'Liming Chen']|['cs.CV']
2017-03-28T14:09:00Z|2017-03-24T11:58:14Z|http://arxiv.org/abs/1703.08378v1|http://arxiv.org/pdf/1703.08378v1|Feature Fusion using Extended Jaccard Graph and Stochastic Gradient   Descent for Robot|featur fusion use extend jaccard graph stochast gradient descent robot|Robot vision is a fundamental device for human-robot interaction and robot complex tasks. In this paper, we use Kinect and propose a feature graph fusion (FGF) for robot recognition. Our feature fusion utilizes RGB and depth information to construct fused feature from Kinect. FGF involves multi-Jaccard similarity to compute a robust graph and utilize word embedding method to enhance the recognition results. We also collect DUT RGB-D face dataset and a benchmark datset to evaluate the effectiveness and efficiency of our method. The experimental results illustrate FGF is robust and effective to face and object datasets in robot applications.|robot vision fundament devic human robot interact robot complex task paper use kinect propos featur graph fusion fgf robot recognit featur fusion util rgb depth inform construct fuse featur kinect fgf involv multi jaccard similar comput robust graph util word embed method enhanc recognit result also collect dut rgb face dataset benchmark datset evalu effect effici method experiment result illustr fgf robust effect face object dataset robot applic|['Shenglan Liu', 'Muxin Sun', 'Wei Wang', 'Feilong Wang']|['cs.CV', 'cs.LG', 'cs.RO']
2017-03-28T14:09:00Z|2017-03-24T11:39:26Z|http://arxiv.org/abs/1703.08366v1|http://arxiv.org/pdf/1703.08366v1|A Hybrid Deep Learning Approach for Texture Analysis|hybrid deep learn approach textur analysi|Texture classification is a problem that has various applications such as remote sensing and forest species recognition. Solutions tend to be custom fit to the dataset used but fails to generalize. The Convolutional Neural Network (CNN) in combination with Support Vector Machine (SVM) form a robust selection between powerful invariant feature extractor and accurate classifier. The fusion of experts provides stability in classification rates among different datasets.|textur classif problem various applic remot sens forest speci recognit solut tend custom fit dataset use fail general convolut neural network cnn combin support vector machin svm form robust select power invari featur extractor accur classifi fusion expert provid stabil classif rate among differ dataset|['Hussein Adly', 'Mohamed Moustafa']|['cs.CV']
2017-03-28T14:09:00Z|2017-03-24T11:17:00Z|http://arxiv.org/abs/1703.08359v1|http://arxiv.org/pdf/1703.08359v1|Scalable Person Re-identification on Supervised Smoothed Manifold|scalabl person identif supervis smooth manifold|Most existing person re-identification algorithms either extract robust visual features or learn discriminative metrics for person images. However, the underlying manifold which those images reside on is rarely investigated. That raises a problem that the learned metric is not smooth with respect to the local geometry structure of the data manifold.   In this paper, we study person re-identification with manifold-based affinity learning, which did not receive enough attention from this area. An unconventional manifold-preserving algorithm is proposed, which can 1) make the best use of supervision from training data, whose label information is given as pairwise constraints; 2) scale up to large repositories with low on-line time complexity; and 3) be plunged into most existing algorithms, serving as a generic postprocessing procedure to further boost the identification accuracies. Extensive experimental results on five popular person re-identification benchmarks consistently demonstrate the effectiveness of our method. Especially, on the largest CUHK03 and Market-1501, our method outperforms the state-of-the-art alternatives by a large margin with high efficiency, which is more appropriate for practical applications.|exist person identif algorithm either extract robust visual featur learn discrimin metric person imag howev manifold imag resid rare investig rais problem learn metric smooth respect local geometri structur data manifold paper studi person identif manifold base affin learn receiv enough attent area unconvent manifold preserv algorithm propos make best use supervis train data whose label inform given pairwis constraint scale larg repositori low line time complex plung exist algorithm serv generic postprocess procedur boost identif accuraci extens experiment result five popular person identif benchmark consist demonstr effect method especi largest cuhk market method outperform state art altern larg margin high effici appropri practic applic|['Song Bai', 'Xiang Bai', 'Qi Tian']|['cs.CV']
2017-03-28T14:09:00Z|2017-03-24T10:11:03Z|http://arxiv.org/abs/1703.08338v1|http://arxiv.org/pdf/1703.08338v1|Improving Classification by Improving Labelling: Introducing   Probabilistic Multi-Label Object Interaction Recognition|improv classif improv label introduc probabilist multi label object interact recognit|This work deviates from easy-to-define class boundaries for object interactions. For the task of object interaction recognition, often captured using an egocentric view, we show that semantic ambiguities in verbs and recognising sub-interactions along with concurrent interactions result in legitimate class overlaps (Figure 1). We thus aim to model the mapping between observations and interaction classes, as well as class overlaps, towards a probabilistic multi-label classifier that emulates human annotators. Given a video segment containing an object interaction, we model the probability for a verb, out of a list of possible verbs, to be used to annotate that interaction. The proba- bility is learnt from crowdsourced annotations, and is tested on two public datasets, comprising 1405 video sequences for which we provide annotations on 90 verbs. We outper- form conventional single-label classification by 11% and 6% on the two datasets respectively, and show that learning from annotation probabilities outperforms majority voting and enables discovery of co-occurring labels.|work deviat easi defin class boundari object interact task object interact recognit often captur use egocentr view show semant ambigu verb recognis sub interact along concurr interact result legitim class overlap figur thus aim model map observ interact class well class overlap toward probabilist multi label classifi emul human annot given video segment contain object interact model probabl verb list possibl verb use annot interact proba biliti learnt crowdsourc annot test two public dataset compris video sequenc provid annot verb outper form convent singl label classif two dataset respect show learn annot probabl outperform major vote enabl discoveri co occur label|['Michael Wray', 'Davide Moltisanti', 'Walterio Mayol-Cuevas', 'Dima Damen']|['cs.CV']
2017-03-28T14:09:05Z|2017-03-24T05:54:11Z|http://arxiv.org/abs/1703.08289v1|http://arxiv.org/pdf/1703.08289v1|Deep Direct Regression for Multi-Oriented Scene Text Detection|deep direct regress multi orient scene text detect|In this paper, we first provide a new perspective to divide existing high performance object detection methods into direct and indirect regressions. Direct regression performs boundary regression by predicting the offsets from a given point, while indirect regression predicts the offsets from some bounding box proposals. Then we analyze the drawbacks of the indirect regression, which the recent state-of-the-art detection structures like Faster-RCNN and SSD follows, for multi-oriented scene text detection, and point out the potential superiority of direct regression. To verify this point of view, we propose a deep direct regression based method for multi-oriented scene text detection. Our detection framework is simple and effective with a fully convolutional network and one-step post processing. The fully convolutional network is optimized in an end-to-end way and has bi-task outputs where one is pixel-wise classification between text and non-text, and the other is direct regression to determine the vertex coordinates of quadrilateral text boundaries. The proposed method is particularly beneficial for localizing incidental scene texts. On the ICDAR2015 Incidental Scene Text benchmark, our method achieves the F1-measure of 81%, which is a new state-of-the-art and significantly outperforms previous approaches. On other standard datasets with focused scene texts, our method also reaches the state-of-the-art performance.|paper first provid new perspect divid exist high perform object detect method direct indirect regress direct regress perform boundari regress predict offset given point indirect regress predict offset bound box propos analyz drawback indirect regress recent state art detect structur like faster rcnn ssd follow multi orient scene text detect point potenti superior direct regress verifi point view propos deep direct regress base method multi orient scene text detect detect framework simpl effect fulli convolut network one step post process fulli convolut network optim end end way bi task output one pixel wise classif text non text direct regress determin vertex coordin quadrilater text boundari propos method particular benefici local incident scene text icdar incident scene text benchmark method achiev measur new state art signific outperform previous approach standard dataset focus scene text method also reach state art perform|['Wenhao He', 'Xu-Yao Zhang', 'Fei Yin', 'Cheng-Lin Liu']|['cs.CV']
2017-03-28T14:09:05Z|2017-03-24T03:01:29Z|http://arxiv.org/abs/1703.08274v1|http://arxiv.org/pdf/1703.08274v1|View Adaptive Recurrent Neural Networks for High Performance Human   Action Recognition from Skeleton Data|view adapt recurr neural network high perform human action recognit skeleton data|Skeleton-based human action recognition has recently attracted increasing attention due to the popularity of 3D skeleton data. One main challenge lies in the large view variations in captured human actions. We propose a novel view adaptation scheme to automatically regulate observation viewpoints during the occurrence of an action. Rather than re-positioning the skeletons based on a human defined prior criterion, we design a view adaptive recurrent neural network (RNN) with LSTM architecture, which enables the network itself to adapt to the most suitable observation viewpoints from end to end. Extensive experiment analyses show that the proposed view adaptive RNN model strives to (1) transform the skeletons of various views to much more consistent viewpoints and (2) maintain the continuity of the action rather than transforming every frame to the same position with the same body orientation. Our model achieves state-of-the-art performance on three benchmark datasets. On the current largest NTU RGB+D dataset, our scheme outperforms the state of the art by an impressive 6% gain in accuracy.|skeleton base human action recognit recent attract increas attent due popular skeleton data one main challeng lie larg view variat captur human action propos novel view adapt scheme automat regul observ viewpoint dure occurr action rather posit skeleton base human defin prior criterion design view adapt recurr neural network rnn lstm architectur enabl network adapt suitabl observ viewpoint end end extens experi analys show propos view adapt rnn model strive transform skeleton various view much consist viewpoint maintain continu action rather transform everi frame posit bodi orient model achiev state art perform three benchmark dataset current largest ntu rgb dataset scheme outperform state art impress gain accuraci|['Pengfei Zhang', 'Cuiling Lan', 'Junliang Xing', 'Wenjun Zeng', 'Jianru Xue', 'Nanning Zheng']|['cs.CV']
2017-03-28T14:09:05Z|2017-03-23T22:25:05Z|http://arxiv.org/abs/1703.08245v1|http://arxiv.org/pdf/1703.08245v1|On the Robustness of Convolutional Neural Networks to Internal   Architecture and Weight Perturbations|robust convolut neural network intern architectur weight perturb|Deep convolutional neural networks are generally regarded as robust function approximators. So far, this intuition is based on perturbations to external stimuli such as the images to be classified. Here we explore the robustness of convolutional neural networks to perturbations to the internal weights and architecture of the network itself. We show that convolutional networks are surprisingly robust to a number of internal perturbations in the higher convolutional layers but the bottom convolutional layers are much more fragile. For instance, Alexnet shows less than a 30% decrease in classification performance when randomly removing over 70% of weight connections in the top convolutional or dense layers but performance is almost at chance with the same perturbation in the first convolutional layer. Finally, we suggest further investigations which could continue to inform the robustness of convolutional networks to internal perturbations.|deep convolut neural network general regard robust function approxim far intuit base perturb extern stimuli imag classifi explor robust convolut neural network perturb intern weight architectur network show convolut network surpris robust number intern perturb higher convolut layer bottom convolut layer much fragil instanc alexnet show less decreas classif perform random remov weight connect top convolut dens layer perform almost chanc perturb first convolut layer final suggest investig could continu inform robust convolut network intern perturb|['Nicholas Cheney', 'Martin Schrimpf', 'Gabriel Kreiman']|['cs.LG', 'cs.CV']
2017-03-28T14:09:05Z|2017-03-23T21:25:48Z|http://arxiv.org/abs/1703.08238v1|http://arxiv.org/pdf/1703.08238v1|Semi-Automatic Segmentation and Ultrasonic Characterization of Solid   Breast Lesions|semi automat segment ultrason character solid breast lesion|Characterization of breast lesions is an essential prerequisite to detect breast cancer in an early stage. Automatic segmentation makes this categorization method robust by freeing it from subjectivity and human error. Both spectral and morphometric features are successfully used for differentiating between benign and malignant breast lesions. In this thesis, we used empirical mode decomposition method for semi-automatic segmentation. Sonographic features like ehcogenicity, heterogeneity, FNPA, margin definition, Hurst coefficient, compactness, roundness, aspect ratio, convexity, solidity, form factor were calculated to be used as our characterization parameters. All of these parameters did not give desired comparative results. But some of them namely echogenicity, heterogeneity, margin definition, aspect ratio and convexity gave good results and were used for characterization.|character breast lesion essenti prerequisit detect breast cancer earli stage automat segment make categor method robust free subject human error spectral morphometr featur success use differenti benign malign breast lesion thesi use empir mode decomposit method semi automat segment sonograph featur like ehcogen heterogen fnpa margin definit hurst coeffici compact round aspect ratio convex solid form factor calcul use character paramet paramet give desir compar result name echogen heterogen margin definit aspect ratio convex gave good result use character|['Mohammad Saad Billah', 'Tahmida Binte Mahmud']|['cs.CV']
2017-03-28T14:09:05Z|2017-03-23T16:46:00Z|http://arxiv.org/abs/1703.08136v1|http://arxiv.org/pdf/1703.08136v1|Visually grounded learning of keyword prediction from untranscribed   speech|visual ground learn keyword predict untranscrib speech|"During language acquisition, infants have the benefit of visual cues to ground spoken language. Robots similarly have access to audio and visual sensors. Recent work has shown that images and spoken captions can be mapped into a meaningful common space, allowing images to be retrieved using speech and vice versa. In this setting of images paired with untranscribed spoken captions, we consider whether computer vision systems can be used to obtain textual labels for the speech. Concretely, we use an image-to-words multi-label visual classifier to tag images with soft textual labels, and then train a neural network to map from the speech to these soft targets. We show that the resulting speech system is able to predict which words occur in an utterance---acting as a spoken bag-of-words classifier---without seeing any parallel speech and text. We find that the model often confuses semantically related words, e.g. ""man"" and ""person"", making it even more effective as a semantic keyword spotter."|dure languag acquisit infant benefit visual cue ground spoken languag robot similar access audio visual sensor recent work shown imag spoken caption map meaning common space allow imag retriev use speech vice versa set imag pair untranscrib spoken caption consid whether comput vision system use obtain textual label speech concret use imag word multi label visual classifi tag imag soft textual label train neural network map speech soft target show result speech system abl predict word occur utter act spoken bag word classifi without see ani parallel speech text find model often confus semant relat word man person make even effect semant keyword spotter|['Herman Kamper', 'Shane Settle', 'Gregory Shakhnarovich', 'Karen Livescu']|['cs.CL', 'cs.CV']
2017-03-28T14:09:05Z|2017-03-24T12:12:37Z|http://arxiv.org/abs/1703.08132v2|http://arxiv.org/pdf/1703.08132v2|Weakly Supervised Action Learning with RNN based Fine-to-coarse Modeling|weak supervis action learn rnn base fine coars model|We present an approach for weakly supervised learning of human actions. Given a set of videos and an ordered list of the occurring actions, the goal is to infer start and end frames of the related action classes within the video and to train the respective action classifiers without any need for hand labeled frame boundaries. To address this task, we propose a combination of a discriminative representation of subactions, modeled by a recurrent neural network, and a coarse probabilistic model to allow for a temporal alignment and inference over long sequences. While this system alone already generates good results, we show that the performance can be further improved by approximating the number of subactions to the characteristics of the different action classes. To this end, we adapt the number of subaction classes by iterating realignment and reestimation during training. The proposed system is evaluated on two benchmark datasets, the Breakfast and the Hollywood extended dataset, showing a competitive performance on various weak learning tasks such as temporal action segmentation and action alignment.|present approach weak supervis learn human action given set video order list occur action goal infer start end frame relat action class within video train respect action classifi without ani need hand label frame boundari address task propos combin discrimin represent subact model recurr neural network coars probabilist model allow tempor align infer long sequenc system alon alreadi generat good result show perform improv approxim number subact characterist differ action class end adapt number subact class iter realign reestim dure train propos system evalu two benchmark dataset breakfast hollywood extend dataset show competit perform various weak learn task tempor action segment action align|['Alexander Richard', 'Hilde Kuehne', 'Juergen Gall']|['cs.CV']
2017-03-28T14:09:05Z|2017-03-23T15:57:23Z|http://arxiv.org/abs/1703.08120v1|http://arxiv.org/pdf/1703.08120v1|Recurrent and Contextual Models for Visual Question Answering|recurr contextu model visual question answer|We propose a series of recurrent and contextual neural network models for multiple choice visual question answering on the Visual7W dataset. Motivated by divergent trends in model complexities in the literature, we explore the balance between model expressiveness and simplicity by studying incrementally more complex architectures. We start with LSTM-encoding of input questions and answers; build on this with context generation by LSTM-encodings of neural image and question representations and attention over images; and evaluate the diversity and predictive power of our models and the ensemble thereof. All models are evaluated against a simple baseline inspired by the current state-of-the-art, consisting of involving simple concatenation of bag-of-words and CNN representations for the text and images, respectively. Generally, we observe marked variation in image-reasoning performance between our models not obvious from their overall performance, as well as evidence of dataset bias. Our standalone models achieve accuracies up to $64.6\%$, while the ensemble of all models achieves the best accuracy of $66.67\%$, within $0.5\%$ of the current state-of-the-art for Visual7W.|propos seri recurr contextu neural network model multipl choic visual question answer visualw dataset motiv diverg trend model complex literatur explor balanc model express simplic studi increment complex architectur start lstm encod input question answer build context generat lstm encod neural imag question represent attent imag evalu divers predict power model ensembl thereof model evalu simpl baselin inspir current state art consist involv simpl concaten bag word cnn represent text imag respect general observ mark variat imag reason perform model obvious overal perform well evid dataset bias standalon model achiev accuraci ensembl model achiev best accuraci within current state art visualw|['Abhijit Sharang', 'Eric Lau']|['cs.CL', 'cs.CV']
2017-03-28T14:09:05Z|2017-03-23T15:56:33Z|http://arxiv.org/abs/1703.08119v1|http://arxiv.org/pdf/1703.08119v1|Quality Resilient Deep Neural Networks|qualiti resili deep neural network|"We study deep neural networks for classification of images with quality distortions. We first show that networks fine-tuned on distorted data greatly outperform the original networks when tested on distorted data. However, fine-tuned networks perform poorly on quality distortions that they have not been trained for. We propose a mixture of experts ensemble method that is robust to different types of distortions. The ""experts"" in our model are trained on a particular type of distortion. The output of the model is a weighted sum of the expert models, where the weights are determined by a separate gating network. The gating network is trained to predict optimal weights for a particular distortion type and level. During testing, the network is blind to the distortion level and type, yet can still assign appropriate weights to the expert models. We additionally investigate weight sharing methods for the mixture model and show that improved performance can be achieved with a large reduction in the number of unique network parameters."|studi deep neural network classif imag qualiti distort first show network fine tune distort data great outperform origin network test distort data howev fine tune network perform poor qualiti distort train propos mixtur expert ensembl method robust differ type distort expert model train particular type distort output model weight sum expert model weight determin separ gate network gate network train predict optim weight particular distort type level dure test network blind distort level type yet still assign appropri weight expert model addit investig weight share method mixtur model show improv perform achiev larg reduct number uniqu network paramet|['Samuel Dodge', 'Lina Karam']|['cs.CV']
2017-03-28T14:09:05Z|2017-03-23T14:46:46Z|http://arxiv.org/abs/1703.08089v1|http://arxiv.org/pdf/1703.08089v1|A Bag-of-Words Equivalent Recurrent Neural Network for Action   Recognition|bag word equival recurr neural network action recognit|The traditional bag-of-words approach has found a wide range of applications in computer vision. The standard pipeline consists of a generation of a visual vocabulary, a quantization of the features into histograms of visual words, and a classification step for which usually a support vector machine in combination with a non-linear kernel is used. Given large amounts of data, however, the model suffers from a lack of discriminative power. This applies particularly for action recognition, where the vast amount of video features needs to be subsampled for unsupervised visual vocabulary generation. Moreover, the kernel computation can be very expensive on large datasets. In this work, we propose a recurrent neural network that is equivalent to the traditional bag-of-words approach but enables for the application of discriminative training. The model further allows to incorporate the kernel computation into the neural network directly, solving the complexity issue and allowing to represent the complete classification system within a single network. We evaluate our method on four recent action recognition benchmarks and show that the conventional model as well as sparse coding methods are outperformed.|tradit bag word approach found wide rang applic comput vision standard pipelin consist generat visual vocabulari quantize featur histogram visual word classif step usual support vector machin combin non linear kernel use given larg amount data howev model suffer lack discrimin power appli particular action recognit vast amount video featur need subsampl unsupervis visual vocabulari generat moreov kernel comput veri expens larg dataset work propos recurr neural network equival tradit bag word approach enabl applic discrimin train model allow incorpor kernel comput neural network direct solv complex issu allow repres complet classif system within singl network evalu method four recent action recognit benchmark show convent model well spars code method outperform|['Alexander Richard', 'Juergen Gall']|['cs.CV']
2017-03-28T14:09:05Z|2017-03-23T12:55:34Z|http://arxiv.org/abs/1703.08050v1|http://arxiv.org/pdf/1703.08050v1|Is Second-order Information Helpful for Large-scale Visual Recognition?|second order inform help larg scale visual recognit|By stacking deeper layers of convolutions and nonlinearity, convolutional networks (ConvNets) effectively learn from low-level to high-level features and discriminative representations. Since the end goal of large-scale recognition is to delineate the complex boundaries of thousands of classes in a large-dimensional space, adequate exploration of feature distributions is important for realizing full potentials of ConvNets. However, state-of-the-art works concentrate only on deeper or wider architecture design, while rarely exploring feature statistics higher than first-order. We take a step towards addressing this problem. Our method consists in covariance pooling, instead of the most commonly used first-order pooling, of high-level convolutional features. The main challenges involved are robust covariance estimation given a small sample of large-dimensional features and usage of the manifold structure of covariance matrices. To address these challenges, we present a Matrix Power Normalized Covariance (MPN-COV) method. We develop the forward and backward propagation formulas regarding the nonlinear matrix functions such that MPN-COV can be trained end-to-end. In addition, we analyze both qualitatively and quantitatively its advantage over the widely used Log-Euclidean metric. On the ImageNet 2012 validation set, by combining MPN-COV we achieve over 4%, 3% and 2.5% gains for AlexNet, VGG-M and VGG-16, respectively; integration of MPN-COV into 50-layer ResNet outperforms ResNet-101 and is comparable to ResNet-152, both of which use first-order, global average pooling.|stack deeper layer convolut nonlinear convolut network convnet effect learn low level high level featur discrimin represent sinc end goal larg scale recognit delin complex boundari thousand class larg dimension space adequ explor featur distribut import realiz full potenti convnet howev state art work concentr onli deeper wider architectur design rare explor featur statist higher first order take step toward address problem method consist covari pool instead common use first order pool high level convolut featur main challeng involv robust covari estim given small sampl larg dimension featur usag manifold structur covari matric address challeng present matrix power normal covari mpn cov method develop forward backward propag formula regard nonlinear matrix function mpn cov train end end addit analyz qualit quantit advantag wide use log euclidean metric imagenet valid set combin mpn cov achiev gain alexnet vgg vgg respect integr mpn cov layer resnet outperform resnet compar resnet use first order global averag pool|['Peihua Li', 'Jiangtao Xie', 'Qilong Wang', 'Wangmeng Zuo']|['cs.CV']
2017-03-28T14:09:09Z|2017-03-23T12:19:09Z|http://arxiv.org/abs/1703.08033v1|http://arxiv.org/pdf/1703.08033v1|Generative Adversarial Residual Pairwise Networks for One Shot Learning|generat adversari residu pairwis network one shot learn|Deep neural networks achieve unprecedented performance levels over many tasks and scale well with large quantities of data, but performance in the low-data regime and tasks like one shot learning still lags behind. While recent work suggests many hypotheses from better optimization to more complicated network structures, in this work we hypothesize that having a learnable and more expressive similarity objective is an essential missing component. Towards overcoming that, we propose a network design inspired by deep residual networks that allows the efficient computation of this more expressive pairwise similarity objective. Further, we argue that regularization is key in learning with small amounts of data, and propose an additional generator network based on the Generative Adversarial Networks where the discriminator is our residual pairwise network. This provides a strong regularizer by leveraging the generated data samples. The proposed model can generate plausible variations of exemplars over unseen classes and outperforms strong discriminative baselines for few shot classification tasks. Notably, our residual pairwise network design outperforms previous state-of-theart on the challenging mini-Imagenet dataset for one shot learning by getting over 55% accuracy for the 5-way classification task over unseen classes.|deep neural network achiev unpreced perform level mani task scale well larg quantiti data perform low data regim task like one shot learn still lag behind recent work suggest mani hypothes better optim complic network structur work hypothes learnabl express similar object essenti miss compon toward overcom propos network design inspir deep residu network allow effici comput express pairwis similar object argu regular key learn small amount data propos addit generat network base generat adversari network discrimin residu pairwis network provid strong regular leverag generat data sampl propos model generat plausibl variat exemplar unseen class outperform strong discrimin baselin shot classif task notabl residu pairwis network design outperform previous state theart challeng mini imagenet dataset one shot learn get accuraci way classif task unseen class|['Akshay Mehrotra', 'Ambedkar Dukkipati']|['cs.CV', 'cs.NE']
2017-03-28T14:09:09Z|2017-03-26T02:43:13Z|http://arxiv.org/abs/1703.08025v2|http://arxiv.org/pdf/1703.08025v2|Saliency-guided video classification via adaptively weighted learning|salienc guid video classif via adapt weight learn|Video classification is productive in many practical applications, and the recent deep learning has greatly improved its accuracy. However, existing works often model video frames indiscriminately, but from the view of motion, video frames can be decomposed into salient and non-salient areas naturally. Salient and non-salient areas should be modeled with different networks, for the former present both appearance and motion information, and the latter present static background information. To address this problem, in this paper, video saliency is predicted by optical flow without supervision firstly. Then two streams of 3D CNN are trained individually for raw frames and optical flow on salient areas, and another 2D CNN is trained for raw frames on non-salient areas. For the reason that these three streams play different roles for each class, the weights of each stream are adaptively learned for each class. Experimental results show that saliency-guided modeling and adaptively weighted learning can reinforce each other, and we achieve the state-of-the-art results.|video classif product mani practic applic recent deep learn great improv accuraci howev exist work often model video frame indiscrimin view motion video frame decompos salient non salient area natur salient non salient area model differ network former present appear motion inform latter present static background inform address problem paper video salienc predict optic flow without supervis first two stream cnn train individu raw frame optic flow salient area anoth cnn train raw frame non salient area reason three stream play differ role class weight stream adapt learn class experiment result show salienc guid model adapt weight learn reinforc achiev state art result|['Yunzhen Zhao', 'Yuxin Peng']|['cs.CV']
2017-03-28T14:09:09Z|2017-03-24T08:24:07Z|http://arxiv.org/abs/1703.08014v2|http://arxiv.org/pdf/1703.08014v2|Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse   IMUs|spars inerti poser automat human pose estim spars imus|We address the problem of making human motion capture in the wild more practical by using a small set of inertial sensors attached to the body. Since the problem is heavily under-constrained, previous methods either use a large number of sensors, which is intrusive, or they require additional video input. We take a different approach and constrain the problem by: (i) making use of a realistic statistical body model that includes anthropometric constraints and (ii) using a joint optimization framework to fit the model to orientation and acceleration measurements over multiple frames. The resulting tracker Sparse Inertial Poser (SIP) enables 3D human pose estimation using only 6 sensors (attached to the wrists, lower legs, back and head) and works for arbitrary human motions. Experiments on the recently released TNT15 dataset show that, using the same number of sensors, SIP achieves higher accuracy than the dataset baseline without using any video data. We further demonstrate the effectiveness of SIP on newly recorded challenging motions in outdoor scenarios such as climbing or jumping over a wall.|address problem make human motion captur wild practic use small set inerti sensor attach bodi sinc problem heavili constrain previous method either use larg number sensor intrus requir addit video input take differ approach constrain problem make use realist statist bodi model includ anthropometr constraint ii use joint optim framework fit model orient acceler measur multipl frame result tracker spars inerti poser sip enabl human pose estim use onli sensor attach wrist lower leg back head work arbitrari human motion experi recent releas tnt dataset show use number sensor sip achiev higher accuraci dataset baselin without use ani video data demonstr effect sip newli record challeng motion outdoor scenario climb jump wall|['Timo von Marcard', 'Bodo Rosenhahn', 'Michael J. Black', 'Gerard Pons-Moll']|['cs.CV', 'cs.GR']
2017-03-28T14:09:09Z|2017-03-24T09:30:41Z|http://arxiv.org/abs/1703.08013v2|http://arxiv.org/pdf/1703.08013v2|A learning-based approach to text image retrieval: using CNN features   and improved similarity metrics|learn base approach text imag retriev use cnn featur improv similar metric|Text content can have different visual presentation ways with roughly similar characters. While conventional text image retrieval depends on complex model of OCR-based text recognition and text similarity detection, this paper proposes a new learning-based approach to text image retrieval with the purpose of finding out the original or similar text through a query text image. Firstly, features of text images are extracted by the CNN network to obtain the deep visual representations. Then, the dimension of CNN features is reduced by PCA method to improve the efficiency of similarity detection. Based on that, an improved similarity metrics with article theme relevance filtering is proposed to improve the retrieval accuracy. In experimental procedure, we collect a group of academic papers both including English and Chinese as the text database, and cut them into pieces of text image. A text image with changed text content is used as the query image, experimental results show that the proposed approach has good ability to retrieve the original text content.|text content differ visual present way rough similar charact convent text imag retriev depend complex model ocr base text recognit text similar detect paper propos new learn base approach text imag retriev purpos find origin similar text queri text imag first featur text imag extract cnn network obtain deep visual represent dimens cnn featur reduc pca method improv effici similar detect base improv similar metric articl theme relev filter propos improv retriev accuraci experiment procedur collect group academ paper includ english chines text databas cut piec text imag text imag chang text content use queri imag experiment result show propos approach good abil retriev origin text content|['Mao Tan', 'Si-Ping Yuan', 'Yong-Xin Su']|['cs.CV', 'cs.IR', 'cs.LG']
2017-03-28T14:09:09Z|2017-03-23T11:02:42Z|http://arxiv.org/abs/1703.08001v1|http://arxiv.org/pdf/1703.08001v1|Nonlinear Spectral Image Fusion|nonlinear spectral imag fusion|In this paper we demonstrate that the framework of nonlinear spectral decompositions based on total variation (TV) regularization is very well suited for image fusion as well as more general image manipulation tasks. The well-localized and edge-preserving spectral TV decomposition allows to select frequencies of a certain image to transfer particular features, such as wrinkles in a face, from one image to another. We illustrate the effectiveness of the proposed approach in several numerical experiments, including a comparison to the competing techniques of Poisson image editing, linear osmosis, wavelet fusion and Laplacian pyramid fusion. We conclude that the proposed spectral TV image decomposition framework is a valuable tool for semi- and fully-automatic image editing and fusion.|paper demonstr framework nonlinear spectral decomposit base total variat tv regular veri well suit imag fusion well general imag manipul task well local edg preserv spectral tv decomposit allow select frequenc certain imag transfer particular featur wrinkl face one imag anoth illustr effect propos approach sever numer experi includ comparison compet techniqu poisson imag edit linear osmosi wavelet fusion laplacian pyramid fusion conclud propos spectral tv imag decomposit framework valuabl tool semi fulli automat imag edit fusion|['Martin Benning', 'Michael M√∂ller', 'Raz Z. Nossek', 'Martin Burger', 'Daniel Cremers', 'Guy Gilboa', 'Carola-Bibiane Sch√∂nlieb']|['cs.CV', 'math.NA', '35P30, 62H35, 65M70, 94A08', 'G.1.3; G.1.6; G.1.8; I.4.0; I.4.5']
2017-03-28T14:09:09Z|2017-03-23T11:01:27Z|http://arxiv.org/abs/1703.08000v1|http://arxiv.org/pdf/1703.08000v1|Weakly Supervised Object Localization Using Things and Stuff Transfer|weak supervis object local use thing stuff transfer|We propose to help weakly supervised object localization for classes where location annotations are not available, by transferring things and stuff knowledge from a source set with available annotations. The source and target classes might share similar appearance (e.g. bear fur is similar to cat fur) or appear against similar background (e.g. horse and sheep appear against grass). To exploit this, we acquire three types of knowledge from the source set: a segmentation model trained on both thing and stuff classes; similarity relations between target and source classes; and co-occurrence relations between thing and stuff classes in the source. The segmentation model is used to generate thing and stuff segmentation maps on a target image, while the class similarity and co-occurrence knowledge help refining them. We then incorporate these maps as new cues into a multiple instance learning framework (MIL), propagating the transferred knowledge from the pixel level to the object proposal level. In extensive experiments, we conduct our transfer from the PASCAL Context dataset (source) to the ILSVRC, COCO and PASCAL VOC 2007 datasets (targets). We evaluate our transfer across widely different thing classes, including some that are not similar in appearance, but appear against similar background. The results demonstrate significant improvement over standard MIL, and we outperform the state-of-the-art in the transfer setting.|propos help weak supervis object local class locat annot avail transfer thing stuff knowledg sourc set avail annot sourc target class might share similar appear bear fur similar cat fur appear similar background hors sheep appear grass exploit acquir three type knowledg sourc set segment model train thing stuff class similar relat target sourc class co occurr relat thing stuff class sourc segment model use generat thing stuff segment map target imag class similar co occurr knowledg help refin incorpor map new cue multipl instanc learn framework mil propag transfer knowledg pixel level object propos level extens experi conduct transfer pascal context dataset sourc ilsvrc coco pascal voc dataset target evalu transfer across wide differ thing class includ similar appear appear similar background result demonstr signific improv standard mil outperform state art transfer set|['Miaojing Shi', 'Holger Caesar', 'Vittorio Ferrari']|['cs.CV']
2017-03-28T14:09:09Z|2017-03-23T09:49:37Z|http://arxiv.org/abs/1703.07980v1|http://arxiv.org/pdf/1703.07980v1|Discriminatively Boosted Image Clustering with Fully Convolutional   Auto-Encoders|discrimin boost imag cluster fulli convolut auto encod|Traditional image clustering methods take a two-step approach, feature learning and clustering, sequentially. However, recent research results demonstrated that combining the separated phases in a unified framework and training them jointly can achieve a better performance. In this paper, we first introduce fully convolutional auto-encoders for image feature learning and then propose a unified clustering framework to learn image representations and cluster centers jointly based on a fully convolutional auto-encoder and soft $k$-means scores. At initial stages of the learning procedure, the representations extracted from the auto-encoder may not be very discriminative for latter clustering. We address this issue by adopting a boosted discriminative distribution, where high score assignments are highlighted and low score ones are de-emphasized. With the gradually boosted discrimination, clustering assignment scores are discriminated and cluster purities are enlarged. Experiments on several vision benchmark datasets show that our methods can achieve a state-of-the-art performance.|tradit imag cluster method take two step approach featur learn cluster sequenti howev recent research result demonstr combin separ phase unifi framework train joint achiev better perform paper first introduc fulli convolut auto encod imag featur learn propos unifi cluster framework learn imag represent cluster center joint base fulli convolut auto encod soft mean score initi stage learn procedur represent extract auto encod may veri discrimin latter cluster address issu adopt boost discrimin distribut high score assign highlight low score one de emphas gradual boost discrimin cluster assign score discrimin cluster puriti enlarg experi sever vision benchmark dataset show method achiev state art perform|['Fengfu Li', 'Hong Qiao', 'Bo Zhang', 'Xuanyang Xi']|['cs.CV', 'cs.LG']
2017-03-28T14:09:09Z|2017-03-23T09:06:13Z|http://arxiv.org/abs/1703.07971v1|http://arxiv.org/pdf/1703.07971v1|Image-based Localization using Hourglass Networks|imag base local use hourglass network|In this paper, we propose an encoder-decoder convolutional neural network (CNN) architecture for estimating camera pose (orientation and location) from a single RGB-image. The architecture has a hourglass shape consisting of a chain of convolution and up-convolution layers followed by a regression part. The up-convolution layers are introduced to preserve the fine-grained information of the input image. Following the common practice, we train our model in end-to-end manner utilizing transfer learning from large scale classification data. The experiments demonstrate the performance of the approach on data exhibiting different lighting conditions, reflections, and motion blur. The results indicate a clear improvement over the previous state-of-the-art even when compared to methods that utilize sequence of test frames instead of a single frame.|paper propos encod decod convolut neural network cnn architectur estim camera pose orient locat singl rgb imag architectur hourglass shape consist chain convolut convolut layer follow regress part convolut layer introduc preserv fine grain inform input imag follow common practic train model end end manner util transfer learn larg scale classif data experi demonstr perform approach data exhibit differ light condit reflect motion blur result indic clear improv previous state art even compar method util sequenc test frame instead singl frame|['Iaroslav Melekhov', 'Juha Ylioinas', 'Juho Kannala', 'Esa Rahtu']|['cs.CV']
2017-03-28T14:09:09Z|2017-03-23T07:52:31Z|http://arxiv.org/abs/1703.07957v1|http://arxiv.org/pdf/1703.07957v1|Robust SfM with Little Image Overlap|robust sfm littl imag overlap|Usual Structure-from-Motion (SfM) techniques require at least trifocal overlaps to calibrate cameras and reconstruct a scene. We consider here scenarios of reduced image sets with little overlap, possibly as low as two images at most seeing the same part of the scene. We propose a new method, based on line coplanarity hypotheses, for estimating the relative scale of two independent bifocal calibrations sharing a camera, without the need of any trifocal information or Manhattan-world assumption. We use it to compute SfM in a chain of up-to-scale relative motions. For accuracy, we however also make use of trifocal information for line and/or point features, when present, relaxing usual trifocal constraints. For robustness to wrong assumptions and mismatches, we embed all constraints in a parameterless RANSAC-like approach. Experiments show that we can calibrate datasets that previously could not, and that this wider applicability does not come at the cost of inaccuracy.|usual structur motion sfm techniqu requir least trifoc overlap calibr camera reconstruct scene consid scenario reduc imag set littl overlap possibl low two imag see part scene propos new method base line coplanar hypothes estim relat scale two independ bifoc calibr share camera without need ani trifoc inform manhattan world assumpt use comput sfm chain scale relat motion accuraci howev also make use trifoc inform line point featur present relax usual trifoc constraint robust wrong assumpt mismatch emb constraint parameterless ransac like approach experi show calibr dataset previous could wider applic doe come cost inaccuraci|['Yohann Salaun', 'Renaud Marlet', 'Pascal Monasse']|['cs.CV']
2017-03-28T14:09:09Z|2017-03-23T05:22:22Z|http://arxiv.org/abs/1703.07939v1|http://arxiv.org/pdf/1703.07939v1|Recurrent Multimodal Interaction for Referring Image Segmentation|recurr multimod interact refer imag segment|In this paper we are interested in the problem of image segmentation given natural language descriptions, i.e. referring expressions. Existing works tackle this problem by first modeling images and sentences independently and then segment images by combining these two types of representations. We argue that learning word-to-image interaction is more native in the sense of jointly modeling two modalities for the image segmentation task, and we propose convolutional multimodal LSTM to encode the sequential interactions between individual words, visual information, and spatial information. We show that our proposed model outperforms the baseline model on benchmark datasets. In addition, we analyze the intermediate output of the proposed multimodal LSTM approach and empirically explains how this approach enforces a more effective word-to-image interaction.|paper interest problem imag segment given natur languag descript refer express exist work tackl problem first model imag sentenc independ segment imag combin two type represent argu learn word imag interact nativ sens joint model two modal imag segment task propos convolut multimod lstm encod sequenti interact individu word visual inform spatial inform show propos model outperform baselin model benchmark dataset addit analyz intermedi output propos multimod lstm approach empir explain approach enforc effect word imag interact|['Chenxi Liu', 'Zhe Lin', 'Xiaohui Shen', 'Jimei Yang', 'Xin Lu', 'Alan Yuille']|['cs.CV']
2017-03-28T14:09:13Z|2017-03-23T05:21:24Z|http://arxiv.org/abs/1703.07938v1|http://arxiv.org/pdf/1703.07938v1|Planar Object Tracking in the Wild: A Benchmark|planar object track wild benchmark|Planar object tracking plays an important role in computer vision and related fields. While several benchmarks have been constructed for evaluating state-of-the-art algorithms, there is a lack of video sequences captured in the wild rather than in constrained laboratory environment. In this paper, we present a carefully designed planar object tracking benchmark containing 210 videos of 30 planar objects sampled in the natural environment. In particular, for each object, we shoot seven videos involving various challenging factors, namely scale change, rotation, perspective distortion, motion blur, occlusion, out-of-view, and unconstrained. The ground truth is carefully annotated semi-manually to ensure the quality. Moreover, eleven state-of-the-art algorithms are evaluated on the benchmark using two evaluation metrics, with detailed analysis provided for the evaluation results. We expect the proposed benchmark to benefit future studies on planar object tracking.|planar object track play import role comput vision relat field sever benchmark construct evalu state art algorithm lack video sequenc captur wild rather constrain laboratori environ paper present care design planar object track benchmark contain video planar object sampl natur environ particular object shoot seven video involv various challeng factor name scale chang rotat perspect distort motion blur occlus view unconstrain ground truth care annot semi manual ensur qualiti moreov eleven state art algorithm evalu benchmark use two evalu metric detail analysi provid evalu result expect propos benchmark benefit futur studi planar object track|['Pengpeng Liang', 'Yifan Wu', 'Haibin Ling']|['cs.CV']
2017-03-28T14:09:13Z|2017-03-23T04:25:48Z|http://arxiv.org/abs/1703.07928v1|http://arxiv.org/pdf/1703.07928v1|Guided Perturbations: Self Corrective Behavior in Convolutional Neural   Networks|guid perturb self correct behavior convolut neural network|Convolutional Neural Networks have been a subject of great importance over the past decade and great strides have been made in their utility for producing state of the art performance in many computer vision problems. However, the behavior of deep networks is yet to be fully understood and is still an active area of research. In this work, we present an intriguing behavior: pre-trained CNNs can be made to improve their predictions by structurally perturbing the input. We observe that these perturbations - referred as Guided Perturbations - enable a trained network to improve its prediction performance without any learning or change in network weights. We perform various ablative experiments to understand how these perturbations affect the local context and feature representations. Furthermore, we demonstrate that this idea can improve performance of several existing approaches on semantic segmentation and scene labeling tasks on the PASCAL VOC dataset and supervised classification tasks on MNIST and CIFAR10 datasets.|convolut neural network subject great import past decad great stride made util produc state art perform mani comput vision problem howev behavior deep network yet fulli understood still activ area research work present intrigu behavior pre train cnns made improv predict structur perturb input observ perturb refer guid perturb enabl train network improv predict perform without ani learn chang network weight perform various ablat experi understand perturb affect local context featur represent furthermor demonstr idea improv perform sever exist approach semant segment scene label task pascal voc dataset supervis classif task mnist cifar dataset|['Swami Sankaranarayanan', 'Arpit Jain', 'Ser Nam Lim']|['cs.CV', 'cs.AI', 'stat.ML']
2017-03-28T14:09:13Z|2017-03-23T03:48:08Z|http://arxiv.org/abs/1703.07920v1|http://arxiv.org/pdf/1703.07920v1|Changing Fashion Cultures|chang fashion cultur|The paper presents a novel concept that analyzes and visualizes worldwide fashion trends. Our goal is to reveal cutting-edge fashion trends without displaying an ordinary fashion style. To achieve the fashion-based analysis, we created a new fashion culture database (FCDB), which consists of 76 million geo-tagged images in 16 cosmopolitan cities. By grasping a fashion trend of mixed fashion styles,the paper also proposes an unsupervised fashion trend descriptor (FTD) using a fashion descriptor, a codeword vetor, and temporal analysis. To unveil fashion trends in the FCDB, the temporal analysis in FTD effectively emphasizes consecutive features between two different times. In experiments, we clearly show the analysis of fashion trends and fashion-based city similarity. As the result of large-scale data collection and an unsupervised analyzer, the proposed approach achieves world-level fashion visualization in a time series. The code, model, and FCDB will be publicly available after the construction of the project page.|paper present novel concept analyz visual worldwid fashion trend goal reveal cut edg fashion trend without display ordinari fashion style achiev fashion base analysi creat new fashion cultur databas fcdb consist million geo tag imag cosmopolitan citi grasp fashion trend mix fashion style paper also propos unsupervis fashion trend descriptor ftd use fashion descriptor codeword vetor tempor analysi unveil fashion trend fcdb tempor analysi ftd effect emphas consecut featur two differ time experi clear show analysi fashion trend fashion base citi similar result larg scale data collect unsupervis analyz propos approach achiev world level fashion visual time seri code model fcdb public avail construct project page|['Kaori Abe', 'Teppei Suzuki', 'Shunya Ueta', 'Akio Nakamura', 'Yutaka Satoh', 'Hirokatsu Kataoka']|['cs.CV', 'cs.DB', 'cs.MM']
2017-03-28T14:09:13Z|2017-03-23T02:50:32Z|http://arxiv.org/abs/1703.07910v1|http://arxiv.org/pdf/1703.07910v1|Bidirectional-Convolutional LSTM Based Spectral-Spatial Feature Learning   for Hyperspectral Image Classification|bidirect convolut lstm base spectral spatial featur learn hyperspectr imag classif|This paper proposes a novel deep learning framework named bidirectional-convolutional long short term memory (Bi-CLSTM) network to automatically learn the spectral-spatial feature from hyperspectral images (HSIs). In the network, the issue of spectral feature extraction is considered as a sequence learning problem, and a recurrent connection operator across the spectral domain is used to address it. Meanwhile, inspired from the widely used convolutional neural network (CNN), a convolution operator across the spatial domain is incorporated into the network to extract the spatial feature. Besides, to sufficiently capture the spectral information, a bidirectional recurrent connection is proposed. In the classification phase, the learned features are concatenated into a vector and fed to a softmax classifier via a fully-connected operator. To validate the effectiveness of the proposed Bi-CLSTM framework, we compare it with several state-of-the-art methods, including the CNN framework, on three widely used HSIs. The obtained results show that Bi-CLSTM can improve the classification performance as compared to other methods.|paper propos novel deep learn framework name bidirect convolut long short term memori bi clstm network automat learn spectral spatial featur hyperspectr imag hsis network issu spectral featur extract consid sequenc learn problem recurr connect oper across spectral domain use address meanwhil inspir wide use convolut neural network cnn convolut oper across spatial domain incorpor network extract spatial featur besid suffici captur spectral inform bidirect recurr connect propos classif phase learn featur concaten vector fed softmax classifi via fulli connect oper valid effect propos bi clstm framework compar sever state art method includ cnn framework three wide use hsis obtain result show bi clstm improv classif perform compar method|['Qingshan Liu', 'Feng Zhou', 'Renlong Hang', 'Xiaotong Yuan']|['cs.CV']
2017-03-28T14:09:13Z|2017-03-23T01:51:14Z|http://arxiv.org/abs/1703.08173v1|http://arxiv.org/pdf/1703.08173v1|Single Image Super-resolution with a Parameter Economic Residual-like   Convolutional Neural Network|singl imag super resolut paramet econom residu like convolut neural network|Recent years have witnessed great success of convolutional neural network (CNN) for various problems both in low and high level visions. Especially noteworthy is the residual network which was originally proposed to handle high-level vision problems and enjoys several merits. This paper aims to extend the merits of residual network, such as skip connection induced fast training, for a typical low-level vision problem, i.e., single image super-resolution. In general, the two main challenges of existing deep CNN for supper-resolution lie in the gradient exploding/vanishing problem and large amount of parameters or computational cost as CNN goes deeper. Correspondingly, the skip connections or identity mapping shortcuts are utilized to avoid gradient exploding/vanishing problem. To tackle with the second problem, a parameter economic CNN architecture which has carefully designed width, depth and skip connections was proposed. Different residual-like architectures for image superresolution has also been compared. Experimental results have demonstrated that the proposed CNN model can not only achieve state-of-the-art PSNR and SSIM results for single image super-resolution but also produce visually pleasant results. This paper has extended the mmm 2017 paper with more experiments and explanations.|recent year wit great success convolut neural network cnn various problem low high level vision especi noteworthi residu network origin propos handl high level vision problem enjoy sever merit paper aim extend merit residu network skip connect induc fast train typic low level vision problem singl imag super resolut general two main challeng exist deep cnn supper resolut lie gradient explod vanish problem larg amount paramet comput cost cnn goe deeper correspond skip connect ident map shortcut util avoid gradient explod vanish problem tackl second problem paramet econom cnn architectur care design width depth skip connect propos differ residu like architectur imag superresolut also compar experiment result demonstr propos cnn model onli achiev state art psnr ssim result singl imag super resolut also produc visual pleasant result paper extend mmm paper experi explan|['Yudong Liang', 'Ze Yang', 'Kai Zhang', 'Yihui He', 'Jinjun Wang', 'Nanning Zheng']|['cs.CV']
2017-03-28T14:09:13Z|2017-03-22T23:35:51Z|http://arxiv.org/abs/1703.07886v1|http://arxiv.org/pdf/1703.07886v1|Robust Kronecker-Decomposable Component Analysis for Low Rank Modeling|robust kroneck decompos compon analysi low rank model|Dictionary learning and component analysis are part of one of the most well-studied and active research fields, at the intersection of signal and image processing, computer vision, and statistical machine learning. In dictionary learning, the current methods of choice are arguably K-SVD and its variants, which learn a dictionary (i.e., a decomposition) for sparse coding via Singular Value Decomposition. In robust component analysis, leading methods derive from Principal Component Pursuit (PCP), which recovers a low-rank matrix from sparse corruptions of unknown magnitude and support. While K-SVD is sensitive to the presence of noise and outliers in the training set, PCP does not provide a dictionary that respects the structure of the data (e.g., images), and requires expensive SVD computations when solved by convex relaxation. In this paper, we introduce a new robust decomposition of images by combining ideas from sparse dictionary learning and PCP. We propose a novel Kronecker-decomposable component analysis which is robust to gross corruption, can be used for low-rank modeling, and leverages separability to solve significantly smaller problems. We design an efficient learning algorithm by drawing links with a restricted form of tensor factorization. The effectiveness of the proposed approach is demonstrated on real-world applications, namely background subtraction and image denoising, by performing a thorough comparison with the current state of the art.|dictionari learn compon analysi part one well studi activ research field intersect signal imag process comput vision statist machin learn dictionari learn current method choic arguabl svd variant learn dictionari decomposit spars code via singular valu decomposit robust compon analysi lead method deriv princip compon pursuit pcp recov low rank matrix spars corrupt unknown magnitud support svd sensit presenc nois outlier train set pcp doe provid dictionari respect structur data imag requir expens svd comput solv convex relax paper introduc new robust decomposit imag combin idea spars dictionari learn pcp propos novel kroneck decompos compon analysi robust gross corrupt use low rank model leverag separ solv signific smaller problem design effici learn algorithm draw link restrict form tensor factor effect propos approach demonstr real world applic name background subtract imag denois perform thorough comparison current state art|['Mehdi Bahri', 'Yannis Panagakis', 'Stefanos Zafeiriou']|['stat.ML', 'cs.CV']
2017-03-28T14:09:13Z|2017-03-22T20:00:15Z|http://arxiv.org/abs/1703.07834v1|http://arxiv.org/pdf/1703.07834v1|Large Pose 3D Face Reconstruction from a Single Image via Direct   Volumetric CNN Regression|larg pose face reconstruct singl imag via direct volumetr cnn regress|3D face reconstruction is a fundamental Computer Vision problem of extraordinary difficulty. Current systems often assume the availability of multiple facial images (sometimes from the same subject) as input, and must address a number of methodological challenges such as establishing dense correspondences across large facial poses, expressions, and non-uniform illumination. In general these methods require complex and inefficient pipelines for model building and fitting. In this work, we propose to address many of these limitations by training a Convolutional Neural Network (CNN) on an appropriate dataset consisting of 2D images and 3D facial models or scans. Our CNN works with just a single 2D facial image, does not require accurate alignment nor establishes dense correspondence between images, works for arbitrary facial poses and expressions, and can be used to reconstruct the whole 3D facial geometry (including the non-visible parts of the face) bypassing the construction (during training) and fitting (during testing) of a 3D Morphable Model. We achieve this via a simple CNN architecture that performs direct regression of a volumetric representation of the 3D facial geometry from a single 2D image. We also demonstrate how the related task of facial landmark localization can be incorporated into the proposed framework and help improve reconstruction quality, especially for the cases of large poses and facial expressions. Testing code will be made available online, along with pre-trained models http://aaronsplace.co.uk/papers/jackson2017recon.|face reconstruct fundament comput vision problem extraordinari difficulti current system often assum avail multipl facial imag sometim subject input must address number methodolog challeng establish dens correspond across larg facial pose express non uniform illumin general method requir complex ineffici pipelin model build fit work propos address mani limit train convolut neural network cnn appropri dataset consist imag facial model scan cnn work singl facial imag doe requir accur align establish dens correspond imag work arbitrari facial pose express use reconstruct whole facial geometri includ non visibl part face bypass construct dure train fit dure test morphabl model achiev via simpl cnn architectur perform direct regress volumetr represent facial geometri singl imag also demonstr relat task facial landmark local incorpor propos framework help improv reconstruct qualiti especi case larg pose facial express test code made avail onlin along pre train model http aaronsplac co uk paper jacksonrecon|['Aaron S. Jackson', 'Adrian Bulat', 'Vasileios Argyriou', 'Georgios Tzimiropoulos']|['cs.CV']
2017-03-28T14:09:13Z|2017-03-22T18:51:51Z|http://arxiv.org/abs/1703.07815v1|http://arxiv.org/pdf/1703.07815v1|Cross-View Image Matching for Geo-localization in Urban Environments|cross view imag match geo local urban environ|In this paper, we address the problem of cross-view image geo-localization. Specifically, we aim to estimate the GPS location of a query street view image by finding the matching images in a reference database of geo-tagged bird's eye view images, or vice versa. To this end, we present a new framework for cross-view image geo-localization by taking advantage of the tremendous success of deep convolutional neural networks (CNNs) in image classification and object detection. First, we employ the Faster R-CNN to detect buildings in the query and reference images. Next, for each building in the query image, we retrieve the $k$ nearest neighbors from the reference buildings using a Siamese network trained on both positive matching image pairs and negative pairs. To find the correct NN for each query building, we develop an efficient multiple nearest neighbors matching method based on dominant sets. We evaluate the proposed framework on a new dataset that consists of pairs of street view and bird's eye view images. Experimental results show that the proposed method achieves better geo-localization accuracy than other approaches and is able to generalize to images at unseen locations.|paper address problem cross view imag geo local specif aim estim gps locat queri street view imag find match imag refer databas geo tag bird eye view imag vice versa end present new framework cross view imag geo local take advantag tremend success deep convolut neural network cnns imag classif object detect first employ faster cnn detect build queri refer imag next build queri imag retriev nearest neighbor refer build use siames network train posit match imag pair negat pair find correct nn queri build develop effici multipl nearest neighbor match method base domin set evalu propos framework new dataset consist pair street view bird eye view imag experiment result show propos method achiev better geo local accuraci approach abl general imag unseen locat|['Yicong Tian', 'Chen Chen', 'Mubarak Shah']|['cs.CV']
2017-03-28T14:09:13Z|2017-03-22T18:49:05Z|http://arxiv.org/abs/1703.07814v1|http://arxiv.org/pdf/1703.07814v1|R-C3D: Region Convolutional 3D Network for Temporal Activity Detection|cd region convolut network tempor activ detect|We address the problem of activity detection in continuous, untrimmed video streams. This is a difficult task that requires extracting meaningful spatio-temporal features to capture activities, accurately localizing the start and end times of each activity, and also dealing with very large data volumes. We introduce a new model, Region Convolutional 3D Network (R-C3D), which encodes the video streams using a three-dimensional fully convolutional network, then generates candidate temporal regions containing activities, and finally classifies selected regions into specific activities. Computation is saved due to the sharing of convolutional features between the proposal and the classification pipelines. The entire model is trained end-to-end with jointly optimized localization and classification losses. R-C3D is faster than existing methods (569 frames per second on a single Titan X Maxwell GPU) and achieves state-of-the-art results on THUMOS'14 (10\% absolute improvement). We further demonstrate that our model is a general activity detection framework that does not rely on assumptions about particular dataset properties by evaluating our approach on ActivityNet and Charades.|address problem activ detect continu untrim video stream difficult task requir extract meaning spatio tempor featur captur activ accur local start end time activ also deal veri larg data volum introduc new model region convolut network cd encod video stream use three dimension fulli convolut network generat candid tempor region contain activ final classifi select region specif activ comput save due share convolut featur propos classif pipelin entir model train end end joint optim local classif loss cd faster exist method frame per second singl titan maxwel gpu achiev state art result thumo absolut improv demonstr model general activ detect framework doe reli assumpt particular dataset properti evalu approach activitynet charad|['Huijuan Xu', 'Abir Das', 'Kate Saenko']|['cs.CV']
2017-03-28T14:09:13Z|2017-03-27T13:39:01Z|http://arxiv.org/abs/1703.07737v2|http://arxiv.org/pdf/1703.07737v2|In Defense of the Triplet Loss for Person Re-Identification|defens triplet loss person identif|In the past few years, the field of computer vision has gone through a revolution fueled mainly by the advent of large datasets and the adoption of deep convolutional neural networks for end-to-end learning. The person re-identification subfield is no exception to this, thanks to the notable publication of the Market-1501 and MARS datasets and several strong deep learning approaches. Unfortunately, a prevailing belief in the community seems to be that the triplet loss is inferior to using surrogate losses (classification, verification) followed by a separate metric learning step. We show that, for models trained from scratch as well as pretrained ones, using a variant of the triplet loss to perform end-to-end deep metric learning outperforms any other published method by a large margin.|past year field comput vision gone revolut fuel main advent larg dataset adopt deep convolut neural network end end learn person identif subfield except thank notabl public market mar dataset sever strong deep learn approach unfortun prevail belief communiti seem triplet loss inferior use surrog loss classif verif follow separ metric learn step show model train scratch well pretrain one use variant triplet loss perform end end deep metric learn outperform ani publish method larg margin|['Alexander Hermans', 'Lucas Beyer', 'Bastian Leibe']|['cs.CV', 'cs.NE']
2017-03-28T14:09:17Z|2017-03-22T15:46:49Z|http://arxiv.org/abs/1703.07715v1|http://arxiv.org/pdf/1703.07715v1|Classifying Symmetrical Differences and Temporal Change in Mammography   Using Deep Neural Networks|classifi symmetr differ tempor chang mammographi use deep neural network|We investigate the addition of symmetry and temporal context information to a deep Convolutional Neural Network (CNN) with the purpose of detecting malignant soft tissue lesions in mammography. We employ a simple linear mapping that takes the location of a mass candidate and maps it to either the contra-lateral or prior mammogram and Regions Of Interest (ROI) are extracted around each location. We subsequently explore two different architectures (1) a fusion model employing two datastreams were both ROIs are fed to the network during training and testing and (2) a stage-wise approach where a single ROI CNN is trained on the primary image and subsequently used as feature extractor for both primary and symmetrical or prior ROIs. A 'shallow' Gradient Boosted Tree (GBT) classifier is then trained on the concatenation of these features and used to classify the joint representation. Results shown a significant increase in performance using the first architecture and symmetry information, but only marginal gains in performance using temporal data and the other setting. We feel results are promising and can greatly be improved when more temporal data becomes available.|investig addit symmetri tempor context inform deep convolut neural network cnn purpos detect malign soft tissu lesion mammographi employ simpl linear map take locat mass candid map either contra later prior mammogram region interest roi extract around locat subsequ explor two differ architectur fusion model employ two datastream roi fed network dure train test stage wise approach singl roi cnn train primari imag subsequ use featur extractor primari symmetr prior roi shallow gradient boost tree gbt classifi train concaten featur use classifi joint represent result shown signific increas perform use first architectur symmetri inform onli margin gain perform use tempor data set feel result promis great improv tempor data becom avail|['Thijs Kooi', 'Nico Karssemeijer']|['cs.CV']
2017-03-28T14:09:17Z|2017-03-22T14:45:15Z|http://arxiv.org/abs/1703.07684v1|http://arxiv.org/pdf/1703.07684v1|Predicting Deeper into the Future of Semantic Segmentation|predict deeper futur semant segment|The ability to predict and therefore to anticipate the future is an important attribute of intelligence. It is also of utmost importance in real-time systems, e.g. in robotics or autonomous driving, which depend on visual scene understanding for decision making. While prediction of the raw RGB pixel values in future video frames has been studied in previous work, here we focus on predicting semantic segmentations of future frames. More precisely, given a sequence of semantically segmented video frames, our goal is to predict segmentation maps of not yet observed video frames that lie up to a second or further in the future. We develop an autoregressive convolutional neural network that learns to iteratively generate multiple frames. Our results on the Cityscapes dataset show that directly predicting future segmentations is substantially better than predicting and then segmenting future RGB frames. Our models predict trajectories of cars and pedestrians much more accurately (25%) than baselines that copy the most recent semantic segmentation or warp it using optical flow. Prediction results up to half a second in the future are visually convincing, the mean IoU of predicted segmentations reaching two thirds of the real future segmentations.|abil predict therefor anticip futur import attribut intellig also utmost import real time system robot autonom drive depend visual scene understand decis make predict raw rgb pixel valu futur video frame studi previous work focus predict semant segment futur frame precis given sequenc semant segment video frame goal predict segment map yet observ video frame lie second futur develop autoregress convolut neural network learn iter generat multipl frame result cityscap dataset show direct predict futur segment substanti better predict segment futur rgb frame model predict trajectori car pedestrian much accur baselin copi recent semant segment warp use optic flow predict result half second futur visual convinc mean iou predict segment reach two third real futur segment|['Natalia Neverova', 'Pauline Luc', 'Camille Couprie', 'Jakob Verbeek', 'Yann LeCun']|['cs.CV', 'cs.LG']
2017-03-28T14:09:17Z|2017-03-22T13:48:47Z|http://arxiv.org/abs/1703.07655v1|http://arxiv.org/pdf/1703.07655v1|ASP: Learning to Forget with Adaptive Synaptic Plasticity in Spiking   Neural Networks|asp learn forget adapt synapt plastic spike neural network|"A fundamental feature of learning in animals is the ""ability to forget"" that allows an organism to perceive, model and make decisions from disparate streams of information and adapt to changing environments. Against this backdrop, we present a novel unsupervised learning mechanism ASP (Adaptive Synaptic Plasticity) for improved recognition with Spiking Neural Networks (SNNs) for real time on-line learning in a dynamic environment. We incorporate an adaptive weight decay mechanism with the traditional Spike Timing Dependent Plasticity (STDP) learning to model adaptivity in SNNs. The leak rate of the synaptic weights is modulated based on the temporal correlation between the spiking patterns of the pre- and post-synaptic neurons. This mechanism helps in gradual forgetting of insignificant data while retaining significant, yet old, information. ASP, thus, maintains a balance between forgetting and immediate learning to construct a stable-plastic self-adaptive SNN for continuously changing inputs. We demonstrate that the proposed learning methodology addresses catastrophic forgetting while yielding significantly improved accuracy over the conventional STDP learning method for digit recognition applications. Additionally, we observe that the proposed learning model automatically encodes selective attention towards relevant features in the input data while eliminating the influence of background noise (or denoising) further improving the robustness of the ASP learning."|fundament featur learn anim abil forget allow organ perceiv model make decis dispar stream inform adapt chang environ backdrop present novel unsupervis learn mechan asp adapt synapt plastic improv recognit spike neural network snns real time line learn dynam environ incorpor adapt weight decay mechan tradit spike time depend plastic stdp learn model adapt snns leak rate synapt weight modul base tempor correl spike pattern pre post synapt neuron mechan help gradual forget insignific data retain signific yet old inform asp thus maintain balanc forget immedi learn construct stabl plastic self adapt snn continu chang input demonstr propos learn methodolog address catastroph forget yield signific improv accuraci convent stdp learn method digit recognit applic addit observ propos learn model automat encod select attent toward relev featur input data elimin influenc background nois denois improv robust asp learn|['Priyadarshini Panda', 'Jason M. Allred', 'Shriram Ramanathan', 'Kaushik Roy']|['cs.NE', 'cs.CV']
2017-03-28T14:09:17Z|2017-03-22T13:35:49Z|http://arxiv.org/abs/1703.07645v1|http://arxiv.org/pdf/1703.07645v1|Neural Ctrl-F: Segmentation-free Query-by-String Word Spotting in   Handwritten Manuscript Collections|neural ctrl segment free queri string word spot handwritten manuscript collect|In this paper, we approach the problem of segmentation-free query-by-string word spotting for handwritten documents. In other words, we use methods inspired from computer vision and machine learning to search for words in large collections of digitized manuscripts. In particular, we are interested in historical handwritten texts, which are often far more challenging than modern printed documents. This task is important, as it provides people with a way to quickly find what they are looking for in large collections that are tedious and difficult to read manually. To this end, we introduce an end-to-end trainable model based on deep neural networks that we call Ctrl-F-Net. Given a full manuscript page, the model simultaneously generates region proposals, and embeds these into a distributed word embedding space, where searches are performed. We evaluate the model on common benchmarks for handwritten word spotting, outperforming the previous state-of-the-art segmentation-free approaches by a large margin, and in some cases even segmentation-based approaches. One interesting real-life application of our approach is to help historians to find and count specific words in court records that are related to women's sustenance activities and division of labor. We provide promising preliminary experiments that validate our method on this task.|paper approach problem segment free queri string word spot handwritten document word use method inspir comput vision machin learn search word larg collect digit manuscript particular interest histor handwritten text often far challeng modern print document task import provid peopl way quick find look larg collect tedious difficult read manual end introduc end end trainabl model base deep neural network call ctrl net given full manuscript page model simultan generat region propos emb distribut word embed space search perform evalu model common benchmark handwritten word spot outperform previous state art segment free approach larg margin case even segment base approach one interest real life applic approach help historian find count specif word court record relat women susten activ divis labor provid promis preliminari experi valid method task|['Tomas Wilkinson', 'Jonas Lindstr√∂m', 'Anders Brun']|['cs.CV']
2017-03-28T14:09:17Z|2017-03-22T10:35:58Z|http://arxiv.org/abs/1703.07595v1|http://arxiv.org/pdf/1703.07595v1|Can you tell where in India I am from? Comparing humans and computers on   fine-grained race face classification|tell india compar human comput fine grain race face classif|Faces form the basis for a rich variety of judgments in humans, yet the underlying features remain poorly understood. Although fine-grained distinctions within a race might more strongly constrain possible facial features used by humans than in case of coarse categories such as race or gender, such fine grained distinctions are relatively less studied. Fine-grained race classification is also interesting because even humans may not be perfectly accurate on these tasks. This allows us to compare errors made by humans and machines, in contrast to standard object detection tasks where human performance is nearly perfect. We have developed a novel face database of close to 1650 diverse Indian faces labeled for fine-grained race (South vs North India) as well as for age, weight, height and gender. We then asked close to 130 human subjects who were instructed to categorize each face as belonging toa Northern or Southern state in India. We then compared human performance on this task with that of computational models trained on the ground-truth labels. Our main results are as follows: (1) Humans are highly consistent (average accuracy: 63.6%), with some faces being consistently classified with > 90% accuracy and others consistently misclassified with < 30% accuracy; (2) Models trained on ground-truth labels showed slightly worse performance (average accuracy: 62%) but showed higher accuracy (72.2%) on faces classified with > 80% accuracy by humans. This was true for models trained on simple spatial and intensity measurements extracted from faces as well as deep neural networks trained on race or gender classification; (3) Using overcomplete banks of features derived from each face part, we found that mouth shape was the single largest contributor towards fine-grained race classification, whereas distances between face parts was the strongest predictor of gender.|face form basi rich varieti judgment human yet featur remain poor understood although fine grain distinct within race might strong constrain possibl facial featur use human case coars categori race gender fine grain distinct relat less studi fine grain race classif also interest becaus even human may perfect accur task allow us compar error made human machin contrast standard object detect task human perform near perfect develop novel face databas close divers indian face label fine grain race south vs north india well age weight height gender ask close human subject instruct categor face belong toa northern southern state india compar human perform task comput model train ground truth label main result follow human high consist averag accuraci face consist classifi accuraci consist misclassifi accuraci model train ground truth label show slight wors perform averag accuraci show higher accuraci face classifi accuraci human true model train simpl spatial intens measur extract face well deep neural network train race gender classif use overcomplet bank featur deriv face part found mouth shape singl largest contributor toward fine grain race classif wherea distanc face part strongest predictor gender|['Harish Katti', 'S. P. Arun']|['cs.CV']
2017-03-28T14:09:17Z|2017-03-22T09:25:49Z|http://arxiv.org/abs/1703.07579v1|http://arxiv.org/pdf/1703.07579v1|An End-to-End Approach to Natural Language Object Retrieval via   Context-Aware Deep Reinforcement Learning|end end approach natur languag object retriev via context awar deep reinforc learn|We propose an end-to-end approach to the natural language object retrieval task, which localizes an object within an image according to a natural language description, i.e., referring expression. Previous works divide this problem into two independent stages: first, compute region proposals from the image without the exploration of the language description; second, score the object proposals with regard to the referring expression and choose the top-ranked proposals. The object proposals are generated independently from the referring expression, which makes the proposal generation redundant and even irrelevant to the referred object. In this work, we train an agent with deep reinforcement learning, which learns to move and reshape a bounding box to localize the object according to the referring expression. We incorporate both the spatial and temporal context information into the training procedure. By simultaneously exploiting local visual information, the spatial and temporal context and the referring language a priori, the agent selects an appropriate action to take at each time. A special action is defined to indicate when the agent finds the referred object, and terminate the procedure. We evaluate our model on various datasets, and our algorithm significantly outperforms the compared algorithms. Notably, the accuracy improvement of our method over the recent method GroundeR and SCRC on the ReferItGame dataset are 7.67% and 18.25%, respectively.|propos end end approach natur languag object retriev task local object within imag accord natur languag descript refer express previous work divid problem two independ stage first comput region propos imag without explor languag descript second score object propos regard refer express choos top rank propos object propos generat independ refer express make propos generat redund even irrelev refer object work train agent deep reinforc learn learn move reshap bound box local object accord refer express incorpor spatial tempor context inform train procedur simultan exploit local visual inform spatial tempor context refer languag priori agent select appropri action take time special action defin indic agent find refer object termin procedur evalu model various dataset algorithm signific outperform compar algorithm notabl accuraci improv method recent method grounder scrc referitgam dataset respect|['Fan Wu', 'Zhongwen Xu', 'Yi Yang']|['cs.CV']
2017-03-28T14:09:17Z|2017-03-22T09:03:25Z|http://arxiv.org/abs/1703.07570v1|http://arxiv.org/pdf/1703.07570v1|Deep MANTA: A Coarse-to-fine Many-Task Network for joint 2D and 3D   vehicle analysis from monocular image|deep manta coars fine mani task network joint vehicl analysi monocular imag|In this paper, we present a novel approach, called Deep MANTA (Deep Many-Tasks), for many-task vehicle analysis from a given image. A robust convolutional network is introduced for simultaneous vehicle detection, part localization, visibility characterization and 3D dimension estimation. Its architecture is based on a new coarse-to-fine object proposal that boosts the vehicle detection. Moreover, the Deep MANTA network is able to localize vehicle parts even if these parts are not visible. In the inference, the network's outputs are used by a real time robust pose estimation algorithm for fine orientation estimation and 3D vehicle localization. We show in experiments that our method outperforms monocular state-of-the-art approaches on vehicle detection, orientation and 3D location tasks on the very challenging KITTI benchmark.|paper present novel approach call deep manta deep mani task mani task vehicl analysi given imag robust convolut network introduc simultan vehicl detect part local visibl character dimens estim architectur base new coars fine object propos boost vehicl detect moreov deep manta network abl local vehicl part even part visibl infer network output use real time robust pose estim algorithm fine orient estim vehicl local show experi method outperform monocular state art approach vehicl detect orient locat task veri challeng kitti benchmark|['Florian Chabot', 'Mohamed Chaouch', 'Jaonary Rabarisoa', 'C√©line Teuli√®re', 'Thierry Chateau']|['cs.CV']
2017-03-28T14:09:17Z|2017-03-23T13:56:01Z|http://arxiv.org/abs/1703.07523v2|http://arxiv.org/pdf/1703.07523v2|Deeply-Supervised CNN for Prostate Segmentation|deepli supervis cnn prostat segment|Prostate segmentation from Magnetic Resonance (MR) images plays an important role in image guided interven- tion. However, the lack of clear boundary specifically at the apex and base, and huge variation of shape and texture between the images from different patients make the task very challenging. To overcome these problems, in this paper, we propose a deeply supervised convolutional neural network (CNN) utilizing the convolutional information to accurately segment the prostate from MR images. The proposed model can effectively detect the prostate region with additional deeply supervised layers compared with other approaches. Since some information will be abandoned after convolution, it is necessary to pass the features extracted from early stages to later stages. The experimental results show that significant segmentation accuracy improvement has been achieved by our proposed method compared to other reported approaches.|prostat segment magnet reson mr imag play import role imag guid interven tion howev lack clear boundari specif apex base huge variat shape textur imag differ patient make task veri challeng overcom problem paper propos deepli supervis convolut neural network cnn util convolut inform accur segment prostat mr imag propos model effect detect prostat region addit deepli supervis layer compar approach sinc inform abandon convolut necessari pass featur extract earli stage later stage experiment result show signific segment accuraci improv achiev propos method compar report approach|['Qikui Zhu', 'Bo Du', 'Baris Turkbey', 'Peter L . Choyke', 'Pingkun Yan']|['cs.CV']
2017-03-28T14:09:17Z|2017-03-22T04:40:51Z|http://arxiv.org/abs/1703.07519v1|http://arxiv.org/pdf/1703.07519v1|Joint Intermodal and Intramodal Label Transfers for Extremely Rare or   Unseen Classes|joint intermod intramod label transfer extrem rare unseen class|In this paper, we present a label transfer model from texts to images for image classification tasks. The problem of image classification is often much more challenging than text classification. On one hand, labeled text data is more widely available than the labeled images for classification tasks. On the other hand, text data tends to have natural semantic interpretability, and they are often more directly related to class labels. On the contrary, the image features are not directly related to concepts inherent in class labels. One of our goals in this paper is to develop a model for revealing the functional relationships between text and image features as to directly transfer intermodal and intramodal labels to annotate the images. This is implemented by learning a transfer function as a bridge to propagate the labels between two multimodal spaces. However, the intermodal label transfers could be undermined by blindly transferring the labels of noisy texts to annotate images. To mitigate this problem, we present an intramodal label transfer process, which complements the intermodal label transfer by transferring the image labels instead when relevant text is absent from the source corpus. In addition, we generalize the inter-modal label transfer to zero-shot learning scenario where there are only text examples available to label unseen classes of images without any positive image examples. We evaluate our algorithm on an image classification task and show the effectiveness with respect to the other compared algorithms.|paper present label transfer model text imag imag classif task problem imag classif often much challeng text classif one hand label text data wide avail label imag classif task hand text data tend natur semant interpret often direct relat class label contrari imag featur direct relat concept inher class label one goal paper develop model reveal function relationship text imag featur direct transfer intermod intramod label annot imag implement learn transfer function bridg propag label two multimod space howev intermod label transfer could undermin blind transfer label noisi text annot imag mitig problem present intramod label transfer process complement intermod label transfer transfer imag label instead relev text absent sourc corpus addit general inter modal label transfer zero shot learn scenario onli text exampl avail label unseen class imag without ani posit imag exampl evalu algorithm imag classif task show effect respect compar algorithm|['Guo-Jun Qi', 'Wei Liu', 'Charu Aggarwal', 'Thomas Huang']|['cs.CV']
2017-03-28T14:09:17Z|2017-03-22T04:31:38Z|http://arxiv.org/abs/1703.07514v1|http://arxiv.org/pdf/1703.07514v1|Video Frame Interpolation via Adaptive Convolution|video frame interpol via adapt convolut|Video frame interpolation typically involves two steps: motion estimation and pixel synthesis. Such a two-step approach heavily depends on the quality of motion estimation. This paper presents a robust video frame interpolation method that combines these two steps into a single process. Specifically, our method considers pixel synthesis for the interpolated frame as local convolution over two input frames. The convolution kernel captures both the local motion between the input frames and the coefficients for pixel synthesis. Our method employs a deep fully convolutional neural network to estimate a spatially-adaptive convolution kernel for each pixel. This deep neural network can be directly trained end to end using widely available video data without any difficult-to-obtain ground-truth data like optical flow. Our experiments show that the formulation of video interpolation as a single convolution process allows our method to gracefully handle challenges like occlusion, blur, and abrupt brightness change and enables high-quality video frame interpolation.|video frame interpol typic involv two step motion estim pixel synthesi two step approach heavili depend qualiti motion estim paper present robust video frame interpol method combin two step singl process specif method consid pixel synthesi interpol frame local convolut two input frame convolut kernel captur local motion input frame coeffici pixel synthesi method employ deep fulli convolut neural network estim spatial adapt convolut kernel pixel deep neural network direct train end end use wide avail video data without ani difficult obtain ground truth data like optic flow experi show formul video interpol singl convolut process allow method grace handl challeng like occlus blur abrupt bright chang enabl high qualiti video frame interpol|['Simon Niklaus', 'Long Mai', 'Feng Liu']|['cs.CV']
2017-03-28T14:09:21Z|2017-03-22T04:21:41Z|http://arxiv.org/abs/1703.07511v1|http://arxiv.org/pdf/1703.07511v1|Deep Photo Style Transfer|deep photo style transfer|This paper introduces a deep-learning approach to photographic style transfer that handles a large variety of image content while faithfully transferring the reference style. Our approach builds upon recent work on painterly transfer that separates style from the content of an image by considering different layers of a neural network. However, as is, this approach is not suitable for photorealistic style transfer. Even when both the input and reference images are photographs, the output still exhibits distortions reminiscent of a painting. Our contribution is to constrain the transformation from the input to the output to be locally affine in colorspace, and to express this constraint as a custom CNN layer through which we can backpropagate. We show that this approach successfully suppresses distortion and yields satisfying photorealistic style transfers in a broad variety of scenarios, including transfer of the time of day, weather, season, and artistic edits.|paper introduc deep learn approach photograph style transfer handl larg varieti imag content faith transfer refer style approach build upon recent work painter transfer separ style content imag consid differ layer neural network howev approach suitabl photorealist style transfer even input refer imag photograph output still exhibit distort reminisc paint contribut constrain transform input output local affin colorspac express constraint custom cnn layer backpropag show approach success suppress distort yield satisfi photorealist style transfer broad varieti scenario includ transfer time day weather season artist edit|['Fujun Luan', 'Sylvain Paris', 'Eli Shechtman', 'Kavita Bala']|['cs.CV']
2017-03-28T14:09:21Z|2017-03-22T00:51:14Z|http://arxiv.org/abs/1703.07479v1|http://arxiv.org/pdf/1703.07479v1|Knowledge Transfer for Melanoma Screening with Deep Learning|knowledg transfer melanoma screen deep learn|Knowledge transfer impacts the performance of deep learning -- the state of the art for image classification tasks, including automated melanoma screening. Deep learning's greed for large amounts of training data poses a challenge for medical tasks, which we can alleviate by recycling knowledge from models trained on different tasks, in a scheme called transfer learning. Although much of the best art on automated melanoma screening employs some form of transfer learning, a systematic evaluation was missing. Here we investigate the presence of transfer, from which task the transfer is sourced, and the application of fine tuning (i.e., retraining of the deep learning model after transfer). We also test the impact of picking deeper (and more expensive) models. Our results favor deeper models, pre-trained over ImageNet, with fine-tuning, reaching an AUC of 80.7% and 84.5% for the two skin-lesion datasets evaluated.|knowledg transfer impact perform deep learn state art imag classif task includ autom melanoma screen deep learn greed larg amount train data pose challeng medic task allevi recycl knowledg model train differ task scheme call transfer learn although much best art autom melanoma screen employ form transfer learn systemat evalu miss investig presenc transfer task transfer sourc applic fine tune retrain deep learn model transfer also test impact pick deeper expens model result favor deeper model pre train imagenet fine tune reach auc two skin lesion dataset evalu|['Afonso Menegola', 'Michel Fornaciali', 'Ramon Pires', 'Fl√°via Vasques Bittencourt', 'Sandra Avila', 'Eduardo Valle']|['cs.CV']
2017-03-28T14:09:21Z|2017-03-23T21:37:40Z|http://arxiv.org/abs/1703.07478v2|http://arxiv.org/pdf/1703.07478v2|Spatially-Varying Blur Detection Based on Multiscale Fused and Sorted   Transform Coefficients of Gradient Magnitudes|spatial vari blur detect base multiscal fuse sort transform coeffici gradient magnitud|The detection of spatially-varying blur without having any information about the blur type is a challenging task. In this paper, we propose a novel effective approach to address the blur detection problem from a single image without requiring any knowledge about the blur type, level, or camera settings. Our approach computes blur detection maps based on a novel High-frequency multiscale Fusion and Sort Transform (HiFST) of gradient magnitudes. The evaluations of the proposed approach on a diverse set of blurry images with different blur types, levels, and contents demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods qualitatively and quantitatively.|detect spatial vari blur without ani inform blur type challeng task paper propos novel effect approach address blur detect problem singl imag without requir ani knowledg blur type level camera set approach comput blur detect map base novel high frequenc multiscal fusion sort transform hifst gradient magnitud evalu propos approach divers set blurri imag differ blur type level content demonstr propos algorithm perform favor state art method qualit quantit|['S. Alireza Golestaneh', 'Lina J. Karam']|['cs.CV']
2017-03-28T14:09:21Z|2017-03-22T00:22:49Z|http://arxiv.org/abs/1703.07475v1|http://arxiv.org/pdf/1703.07475v1|PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action   Understanding|pku mmd larg scale benchmark continu multi modal human action understand|Despite the fact that many 3D human activity benchmarks being proposed, most existing action datasets focus on the action recognition tasks for the segmented videos. There is a lack of standard large-scale benchmarks, especially for current popular data-hungry deep learning based methods. In this paper, we introduce a new large scale benchmark (PKU-MMD) for continuous multi-modality 3D human action understanding and cover a wide range of complex human activities with well annotated information. PKU-MMD contains 1076 long video sequences in 51 action categories, performed by 66 subjects in three camera views. It contains almost 20,000 action instances and 5.4 million frames in total. Our dataset also provides multi-modality data sources, including RGB, depth, Infrared Radiation and Skeleton. With different modalities, we conduct extensive experiments on our dataset in terms of two scenarios and evaluate different methods by various metrics, including a new proposed evaluation protocol 2D-AP. We believe this large-scale dataset will benefit future researches on action detection for the community.|despit fact mani human activ benchmark propos exist action dataset focus action recognit task segment video lack standard larg scale benchmark especi current popular data hungri deep learn base method paper introduc new larg scale benchmark pku mmd continu multi modal human action understand cover wide rang complex human activ well annot inform pku mmd contain long video sequenc action categori perform subject three camera view contain almost action instanc million frame total dataset also provid multi modal data sourc includ rgb depth infrar radiat skeleton differ modal conduct extens experi dataset term two scenario evalu differ method various metric includ new propos evalu protocol ap believ larg scale dataset benefit futur research action detect communiti|['Chunhui Liu', 'Yueyu Hu', 'Yanghao Li', 'Sijie Song', 'Jiaying Liu']|['cs.CV']
2017-03-28T14:09:21Z|2017-03-21T23:56:51Z|http://arxiv.org/abs/1703.07473v1|http://arxiv.org/pdf/1703.07473v1|Episode-Based Active Learning with Bayesian Neural Networks|episod base activ learn bayesian neural network|We investigate different strategies for active learning with Bayesian deep neural networks. We focus our analysis on scenarios where new, unlabeled data is obtained episodically, such as commonly encountered in mobile robotics applications. An evaluation of different strategies for acquisition, updating, and final training on the CIFAR-10 dataset shows that incremental network updates with final training on the accumulated acquisition set are essential for best performance, while limiting the amount of required human labeling labor.|investig differ strategi activ learn bayesian deep neural network focus analysi scenario new unlabel data obtain episod common encount mobil robot applic evalu differ strategi acquisit updat final train cifar dataset show increment network updat final train accumul acquisit set essenti best perform limit amount requir human label labor|['Feras Dayoub', 'Niko S√ºnderhauf', 'Peter Corke']|['cs.CV', 'cs.LG', 'stat.ML']
2017-03-28T14:09:21Z|2017-03-24T21:17:05Z|http://arxiv.org/abs/1703.07464v2|http://arxiv.org/pdf/1703.07464v2|No Fuss Distance Metric Learning using Proxies|fuss distanc metric learn use proxi|We address the problem of distance metric learning (DML), defined as learning a distance consistent with a notion of semantic similarity. Traditionally, for this problem supervision is expressed in the form of sets of points that follow an ordinal relationship -- an anchor point $x$ is similar to a set of positive points $Y$, and dissimilar to a set of negative points $Z$, and a loss defined over these distances is minimized.   While the specifics of the optimization differ, in this work we collectively call this type of supervision Triplets and all methods that follow this pattern Triplet-Based methods. These methods are challenging to optimize. A main issue is the need for finding informative triplets, which is usually achieved by a variety of tricks such as increasing the batch size, hard or semi-hard triplet mining, etc, but even with these tricks, the convergence rate of such methods is slow. In this paper we propose to optimize the triplet loss on a different space of triplets, consisting of an anchor data point and similar and dissimilar proxy points. These proxies approximate the original data points, so that a triplet loss over the proxies is a tight upper bound of the original loss. This proxy-based loss is empirically better behaved. As a result, the proxy-loss improves on state-of-art results for three standard zero-shot learning datasets, by up to 15% points, while converging three times as fast as other triplet-based losses.|address problem distanc metric learn dml defin learn distanc consist notion semant similar tradit problem supervis express form set point follow ordin relationship anchor point similar set posit point dissimilar set negat point loss defin distanc minim specif optim differ work collect call type supervis triplet method follow pattern triplet base method method challeng optim main issu need find inform triplet usual achiev varieti trick increas batch size hard semi hard triplet mine etc even trick converg rate method slow paper propos optim triplet loss differ space triplet consist anchor data point similar dissimilar proxi point proxi approxim origin data point triplet loss proxi tight upper bound origin loss proxi base loss empir better behav result proxi loss improv state art result three standard zero shot learn dataset point converg three time fast triplet base loss|['Yair Movshovitz-Attias', 'Alexander Toshev', 'Thomas K. Leung', 'Sergey Ioffe', 'Saurabh Singh']|['cs.CV']
2017-03-28T14:09:21Z|2017-03-21T21:05:21Z|http://arxiv.org/abs/1703.07431v1|http://arxiv.org/pdf/1703.07431v1|IOD-CNN: Integrating Object Detection Networks for Event Recognition|iod cnn integr object detect network event recognit|Many previous methods have showed the importance of considering semantically relevant objects for performing event recognition, yet none of the methods have exploited the power of deep convolutional neural networks to directly integrate relevant object information into a unified network. We present a novel unified deep CNN architecture which integrates architecturally different, yet semantically-related object detection networks to enhance the performance of the event recognition task. Our architecture allows the sharing of the convolutional layers and a fully connected layer which effectively integrates event recognition, rigid object detection and non-rigid object detection.|mani previous method show import consid semant relev object perform event recognit yet none method exploit power deep convolut neural network direct integr relev object inform unifi network present novel unifi deep cnn architectur integr architectur differ yet semant relat object detect network enhanc perform event recognit task architectur allow share convolut layer fulli connect layer effect integr event recognit rigid object detect non rigid object detect|['Sungmin Eum', 'Hyungtae Lee', 'Heesung Kwon', 'David Doermann']|['cs.CV']
2017-03-28T14:09:21Z|2017-03-21T19:40:25Z|http://arxiv.org/abs/1703.07402v1|http://arxiv.org/pdf/1703.07402v1|Simple Online and Realtime Tracking with a Deep Association Metric|simpl onlin realtim track deep associ metric|Simple Online and Realtime Tracking (SORT) is a pragmatic approach to multiple object tracking with a focus on simple, effective algorithms. In this paper, we integrate appearance information to improve the performance of SORT. Due to this extension we are able to track objects through longer periods of occlusions, effectively reducing the number of identity switches. In spirit of the original framework we place much of the computational complexity into an offline pre-training stage where we learn a deep association metric on a large-scale person re-identification dataset. During online application, we establish measurement-to-track associations using nearest neighbor queries in visual appearance space. Experimental evaluation shows that our extensions reduce the number of identity switches by 45%, achieving overall competitive performance at high frame rates.|simpl onlin realtim track sort pragmat approach multipl object track focus simpl effect algorithm paper integr appear inform improv perform sort due extens abl track object longer period occlus effect reduc number ident switch spirit origin framework place much comput complex offlin pre train stage learn deep associ metric larg scale person identif dataset dure onlin applic establish measur track associ use nearest neighbor queri visual appear space experiment evalu show extens reduc number ident switch achiev overal competit perform high frame rate|['Nicolai Wojke', 'Alex Bewley', 'Dietrich Paulus']|['cs.CV']
2017-03-28T14:09:21Z|2017-03-21T17:41:46Z|http://arxiv.org/abs/1703.07334v1|http://arxiv.org/pdf/1703.07334v1|Pop-up SLAM: Semantic Monocular Plane SLAM for Low-texture Environments|pop slam semant monocular plane slam low textur environ|Existing simultaneous localization and mapping (SLAM) algorithms are not robust in challenging low-texture environments because there are only few salient features. The resulting sparse or semi-dense map also conveys little information for motion planning. Though some work utilize plane or scene layout for dense map regularization, they require decent state estimation from other sources. In this paper, we propose real-time monocular plane SLAM to demonstrate that scene understanding could improve both state estimation and dense mapping especially in low-texture environments. The plane measurements come from a pop-up 3D plane model applied to each single image. We also combine planes with point based SLAM to improve robustness. On a public TUM dataset, our algorithm generates a dense semantic 3D model with pixel depth error of 6.2 cm while existing SLAM algorithms fail. On a 60 m long dataset with loops, our method creates a much better 3D model with state estimation error of 0.67%.|exist simultan local map slam algorithm robust challeng low textur environ becaus onli salient featur result spars semi dens map also convey littl inform motion plan though work util plane scene layout dens map regular requir decent state estim sourc paper propos real time monocular plane slam demonstr scene understand could improv state estim dens map especi low textur environ plane measur come pop plane model appli singl imag also combin plane point base slam improv robust public tum dataset algorithm generat dens semant model pixel depth error cm exist slam algorithm fail long dataset loop method creat much better model state estim error|['Shichao Yang', 'Yu Song', 'Michael Kaess', 'Sebastian Scherer']|['cs.CV', 'cs.RO']
2017-03-28T14:09:21Z|2017-03-21T17:37:36Z|http://arxiv.org/abs/1703.07332v1|http://arxiv.org/pdf/1703.07332v1|How far are we from solving the 2D & 3D Face Alignment problem? (and a   dataset of 230,000 3D facial landmarks)|far solv face align problem dataset facial landmark|"This paper investigates how far a very deep neural network is from attaining close to saturating performance on existing 2D and 3D face alignment datasets. To this end, we make the following three contributions: (a) we construct, for the first time, a very strong baseline by combining a state-of-the-art architecture for landmark localization with a state-of-the-art residual block, train it on a very large yet synthetically expanded 2D facial landmark dataset and finally evaluate it on all other 2D facial landmark datasets. (b) We create a guided by 2D landmarks network which converts 2D landmark annotations to 3D and unifies all existing datasets, leading to the creation of LS3D-W, the largest and most challenging 3D facial landmark dataset to date (~230,000 images). (c) Following that, we train a neural network for 3D face alignment and evaluate it on the newly introduced LS3D-W. (d) We further look into the effect of all ""traditional"" factors affecting face alignment performance like large pose, initialization and resolution, and introduce a ""new"" one, namely the size of the network. (e) We show that both 2D and 3D face alignment networks achieve performance of remarkable accuracy which is probably close to saturating the datasets used. Demo code and pre-trained models can be downloaded from http://www.cs.nott.ac.uk/~psxab5/face-alignment/"|paper investig far veri deep neural network attain close satur perform exist face align dataset end make follow three contribut construct first time veri strong baselin combin state art architectur landmark local state art residu block train veri larg yet synthet expand facial landmark dataset final evalu facial landmark dataset creat guid landmark network convert landmark annot unifi exist dataset lead creation lsd largest challeng facial landmark dataset date imag follow train neural network face align evalu newli introduc lsd look effect tradit factor affect face align perform like larg pose initi resolut introduc new one name size network show face align network achiev perform remark accuraci probabl close satur dataset use demo code pre train model download http www cs nott ac uk psxab face align|['Adrian Bulat', 'Georgios Tzimiropoulos']|['cs.CV']
2017-04-07T11:30:13Z|2017-04-06T16:47:56Z|http://arxiv.org/abs/1704.01926v1|http://arxiv.org/pdf/1704.01926v1|Semantically-Guided Video Object Segmentation|semant guid video object segment|This paper tackles the problem of semi-supervised video object segmentation, that is, segmenting an object in a sequence given its mask in the first frame. One of the main challenges in this scenario is the change of appearance of the objects of interest. Their semantics, on the other hand, do not vary. This paper investigates how to take advantage of such invariance via the introduction of a semantic prior that guides the appearance model. Specifically, given the segmentation mask of the first frame of a sequence, we estimate the semantics of the object of interest, and propagate that knowledge throughout the sequence to improve the results based on an appearance model. We present Semantically-Guided Video Object Segmentation (SGV), which improves results over previous state of the art on two different datasets using a variety of evaluation metrics, while running in half a second per frame.|paper tackl problem semi supervis video object segment segment object sequenc given mask first frame one main challeng scenario chang appear object interest semant hand vari paper investig take advantag invari via introduct semant prior guid appear model specif given segment mask first frame sequenc estim semant object interest propag knowledg throughout sequenc improv result base appear model present semant guid video object segment sgv improv result previous state art two differ dataset use varieti evalu metric run half second per frame|['Sergi Caelles', 'Yuhua Chen', 'Jordi Pont-Tuset', 'Luc Van Gool']|['cs.CV']
2017-04-07T11:30:13Z|2017-04-06T16:47:16Z|http://arxiv.org/abs/1704.01925v1|http://arxiv.org/pdf/1704.01925v1|Automated Latent Fingerprint Recognition|autom latent fingerprint recognit|Latent fingerprints are one of the most important and widely used evidence in law enforcement and forensic agencies worldwide. Yet, NIST evaluations show that the performance of state-of-the-art latent recognition systems is far from satisfactory. An automated latent fingerprint recognition system with high accuracy is essential to compare latents found at crime scenes to a large collection of reference prints to generate a candidate list of possible mates. In this paper, we propose an automated latent fingerprint recognition algorithm that utilizes Convolutional Neural Networks (ConvNets) for ridge flow estimation and minutiae descriptor extraction, and extract complementary templates (two minutiae templates and one texture template) to represent the latent. The comparison scores between the latent and a reference print based on the three templates are fused to retrieve a short candidate list from the reference database. Experimental results show that the rank-1 identification accuracies (query latent is matched with its true mate in the reference database) are 64.7% for the NIST SD27 and 75.3% for the WVU latent databases, against a reference database of 100K rolled prints. These results are the best among published papers on latent recognition and competitive with the performance (66.7% and 70.8% rank-1 accuracies on NIST SD27 and WVU DB, respectively) of a leading COTS latent Automated Fingerprint Identification System (AFIS). By score-level (rank-level) fusion of our system with the commercial off-the-shelf (COTS) latent AFIS, the overall rank-1 identification performance can be improved from 64.7% and 75.3% to 73.3% (74.4%) and 76.6% (78.4%) on NIST SD27 and WVU latent databases, respectively.|latent fingerprint one import wide use evid law enforc forens agenc worldwid yet nist evalu show perform state art latent recognit system far satisfactori autom latent fingerprint recognit system high accuraci essenti compar latent found crime scene larg collect refer print generat candid list possibl mate paper propos autom latent fingerprint recognit algorithm util convolut neural network convnet ridg flow estim minutia descriptor extract extract complementari templat two minutia templat one textur templat repres latent comparison score latent refer print base three templat fuse retriev short candid list refer databas experiment result show rank identif accuraci queri latent match true mate refer databas nist sd wvu latent databas refer databas roll print result best among publish paper latent recognit competit perform rank accuraci nist sd wvu db respect lead cot latent autom fingerprint identif system afi score level rank level fusion system commerci shelf cot latent afi overal rank identif perform improv nist sd wvu latent databas respect|['Kai Cao', 'Anil K. Jain']|['cs.CV']
2017-04-07T11:30:13Z|2017-04-06T16:37:15Z|http://arxiv.org/abs/1704.01920v1|http://arxiv.org/pdf/1704.01920v1|Encoder Based Lifelong Learning|encod base lifelong learn|This paper introduces a new lifelong learning solution where a single model is trained for a sequence of tasks. The main challenge that vision systems face in this context is catastrophic forgetting: as they tend to adapt to the most recently seen task, they lose performance on the tasks that were learned previously. Our method aims at preserving the knowledge of the previous tasks while learning a new one by using autoencoders. For each task, an under-complete autoencoder is learned, capturing the features that are crucial for its achievement. When a new task is presented to the system, we prevent the reconstructions of the features with these autoencoders from changing, which has the effect of preserving the information on which the previous tasks are mainly relying. At the same time, the features are given space to adjust to the most recent environment as only their projection into a low dimension submanifold is controlled. The proposed system is evaluated on image classification tasks and shows a reduction of forgetting over the state-of-the-art|paper introduc new lifelong learn solut singl model train sequenc task main challeng vision system face context catastroph forget tend adapt recent seen task lose perform task learn previous method aim preserv knowledg previous task learn new one use autoencod task complet autoencod learn captur featur crucial achiev new task present system prevent reconstruct featur autoencod chang effect preserv inform previous task main reli time featur given space adjust recent environ onli project low dimens submanifold control propos system evalu imag classif task show reduct forget state art|['Amal Rannen Triki', 'Rahaf Aljundi', 'Mathew B. Blaschko', 'Tinne Tuytelaars']|['cs.CV', 'cs.AI', 'stat.ML']
2017-04-07T11:30:13Z|2017-04-06T15:44:29Z|http://arxiv.org/abs/1704.01897v1|http://arxiv.org/abs/1704.01897v1|Online Hashing|onlin hash|Although hash function learning algorithms have achieved great success in recent years, most existing hash models are off-line, which are not suitable for processing sequential or online data. To address this problem, this work proposes an online hash model to accommodate data coming in stream for online learning. Specifically, a new loss function is proposed to measure the similarity loss between a pair of data samples in hamming space. Then, a structured hash model is derived and optimized in a passive-aggressive way. Theoretical analysis on the upper bound of the cumulative loss for the proposed online hash model is provided. Furthermore, we extend our online hashing from a single-model to a multi-model online hashing that trains multiple models so as to retain diverse online hashing models in order to avoid biased update. The competitive efficiency and effectiveness of the proposed online hash models are verified through extensive experiments on several large-scale datasets as compared to related hashing methods.|although hash function learn algorithm achiev great success recent year exist hash model line suitabl process sequenti onlin data address problem work propos onlin hash model accommod data come stream onlin learn specif new loss function propos measur similar loss pair data sampl ham space structur hash model deriv optim passiv aggress way theoret analysi upper bound cumul loss propos onlin hash model provid furthermor extend onlin hash singl model multi model onlin hash train multipl model retain divers onlin hash model order avoid bias updat competit effici effect propos onlin hash model verifi extens experi sever larg scale dataset compar relat hash method|['Long-Kai Huang', 'Qiang Yang', 'Wei-Shi Zheng']|['cs.CV', 'cs.AI']
2017-04-07T11:30:13Z|2017-04-06T15:08:59Z|http://arxiv.org/abs/1704.01880v1|http://arxiv.org/pdf/1704.01880v1|A Convolution Tree with Deconvolution Branches: Exploiting Geometric   Relationships for Single Shot Keypoint Detection|convolut tree deconvolut branch exploit geometr relationship singl shot keypoint detect|Recently, Deep Convolution Networks (DCNNs) have been applied to the task of face alignment and have shown potential for learning improved feature representations. Although deeper layers can capture abstract concepts like pose, it is difficult to capture the geometric relationships among the keypoints in DCNNs. In this paper, we propose a novel convolution-deconvolution network for facial keypoint detection. Our model predicts the 2D locations of the keypoints and their individual visibility along with 3D head pose, while exploiting the spatial relationships among different keypoints. Different from existing approaches of modeling these relationships, we propose learnable transform functions which captures the relationships between keypoints at feature level. However, due to extensive variations in pose, not all of these relationships act at once, and hence we propose, a pose-based routing function which implicitly models the active relationships. Both transform functions and the routing function are implemented through convolutions in a multi-task framework. Our approach presents a single-shot keypoint detection method, making it different from many existing cascade regression-based methods. We also show that learning these relationships significantly improve the accuracy of keypoint detections for in-the-wild face images from challenging datasets such as AFW and AFLW.|recent deep convolut network dcnns appli task face align shown potenti learn improv featur represent although deeper layer captur abstract concept like pose difficult captur geometr relationship among keypoint dcnns paper propos novel convolut deconvolut network facial keypoint detect model predict locat keypoint individu visibl along head pose exploit spatial relationship among differ keypoint differ exist approach model relationship propos learnabl transform function captur relationship keypoint featur level howev due extens variat pose relationship act onc henc propos pose base rout function implicit model activ relationship transform function rout function implement convolut multi task framework approach present singl shot keypoint detect method make differ mani exist cascad regress base method also show learn relationship signific improv accuraci keypoint detect wild face imag challeng dataset afw aflw|['Amit Kumar', 'Rama Chellappa']|['cs.CV']
2017-04-07T11:30:14Z|2017-04-06T12:55:11Z|http://arxiv.org/abs/1704.01811v1|http://arxiv.org/pdf/1704.01811v1|Higher-Order Minimum Cost Lifted Multicuts for Motion Segmentation|higher order minimum cost lift multicut motion segment|Most state-of-the-art motion segmentation algorithms draw their potential from modeling motion differences of local entities such as point trajectories in terms of pairwise potentials in graphical models. Inference in instances of minimum cost multicut problems defined on such graphs al- lows to optimize the number of the resulting segments along with the segment assignment. However, pairwise potentials limit the discriminative power of the employed motion models to translational differences. More complex models such as Euclidean or affine transformations call for higher-order potentials and a tractable inference in the resulting higher-order graphical models. In this paper, we (1) introduce a generalization of the minimum cost lifted multicut problem to hypergraphs, and (2) propose a simple primal feasible heuristic that allows for a reasonably efficient inference in instances of higher-order lifted multicut problem instances defined on point trajectory hypergraphs for motion segmentation. The resulting motion segmentations improve over the state-of-the-art on the FBMS-59 dataset.|state art motion segment algorithm draw potenti model motion differ local entiti point trajectori term pairwis potenti graphic model infer instanc minimum cost multicut problem defin graph al low optim number result segment along segment assign howev pairwis potenti limit discrimin power employ motion model translat differ complex model euclidean affin transform call higher order potenti tractabl infer result higher order graphic model paper introduc general minimum cost lift multicut problem hypergraph propos simpl primal feasibl heurist allow reason effici infer instanc higher order lift multicut problem instanc defin point trajectori hypergraph motion segment result motion segment improv state art fbms dataset|['Margret Keuper']|['cs.CV']
2017-04-07T11:30:14Z|2017-04-06T08:58:39Z|http://arxiv.org/abs/1704.01754v1|http://arxiv.org/pdf/1704.01754v1|Enhance Feature Discrimination for Unsupervised Hashing|enhanc featur discrimin unsupervis hash|We propose a novel approach to improve unsupervised hashing. Specifically, we propose an embedding method to enhance the discriminative property of features before passing them into hashing. We propose a very efficient embedding method: Gaussian Mixture Model embedding (Gemb). Gemb embeds feature vector into a low-dimensional vector using Gaussian Mixture Model. Our experiment shows that the proposed method boosts the hashing performance of many state-of-the-art, e.g. Binary Autoencoder (BA), Iterative Quantization (ITQ), in standard evaluation metrics for the three main benchmark datasets.|propos novel approach improv unsupervis hash specif propos embed method enhanc discrimin properti featur befor pass hash propos veri effici embed method gaussian mixtur model embed gemb gemb emb featur vector low dimension vector use gaussian mixtur model experi show propos method boost hash perform mani state art binari autoencod ba iter quantize itq standard evalu metric three main benchmark dataset|['Tuan Hoang', 'Thanh-Toan Do', 'Dang-Khoa Le Tan', 'Ngai-Man Cheung']|['cs.CV', 'cs.IR']
2017-04-07T11:30:14Z|2017-04-06T08:25:19Z|http://arxiv.org/abs/1704.01745v1|http://arxiv.org/pdf/1704.01745v1|How to Make an Image More Memorable? A Deep Style Transfer Approach|make imag memor deep style transfer approach|"Recent works have shown that it is possible to automatically predict intrinsic image properties like memorability. In this paper, we take a step forward addressing the question: ""Can we make an image more memorable?"". Methods for automatically increasing image memorability would have an impact in many application fields like education, gaming or advertising. Our work is inspired by the popular editing-by-applying-filters paradigm adopted in photo editing applications, like Instagram and Prisma. In this context, the problem of increasing image memorability maps to that of retrieving ""memorabilizing"" filters or style ""seeds"". Still, users generally have to go through most of the available filters before finding the desired solution, thus turning the editing process into a resource and time consuming task. In this work, we show that it is possible to automatically retrieve the best style seeds for a given image, thus remarkably reducing the number of human attempts needed to find a good match. Our approach leverages from recent advances in the field of image synthesis and adopts a deep architecture for generating a memorable picture from a given input image and a style seed. Importantly, to automatically select the best style a novel learning-based solution, also relying on deep models, is proposed. Our experimental evaluation, conducted on publicly available benchmarks, demonstrates the effectiveness of the proposed approach for generating memorable images through automatic style seed selection"|recent work shown possibl automat predict intrins imag properti like memor paper take step forward address question make imag memor method automat increas imag memor would impact mani applic field like educ game advertis work inspir popular edit appli filter paradigm adopt photo edit applic like instagram prisma context problem increas imag memor map retriev memorabil filter style seed still user general go avail filter befor find desir solut thus turn edit process resourc time consum task work show possibl automat retriev best style seed given imag thus remark reduc number human attempt need find good match approach leverag recent advanc field imag synthesi adopt deep architectur generat memor pictur given input imag style seed import automat select best style novel learn base solut also reli deep model propos experiment evalu conduct public avail benchmark demonstr effect propos approach generat memor imag automat style seed select|['Aliaksandr Siarohin', 'Gloria Zen', 'Cveta Majtanovic', 'Xavier Alameda-Pineda', 'Elisa Ricci', 'Nicu Sebe']|['cs.CV']
2017-04-07T11:30:14Z|2017-04-06T08:05:04Z|http://arxiv.org/abs/1704.01740v1|http://arxiv.org/pdf/1704.01740v1|Object-Part Attention Driven Discriminative Localization for   Fine-grained Image Classification|object part attent driven discrimin local fine grain imag classif|Fine-grained image classification is to recognize hundreds of subcategories belonging to the same basic-level category, such as 200 subcategories belonging to bird, and highly challenging due to large variance in same subcategory and small variance among different subcategories. Existing methods generally find where the object or its parts are and then discriminate which subcategory the image belongs to. However, they mainly have two limitations: (1) Relying on object or parts annotations which are heavily labor consuming. (2) Ignoring the spatial relationship between the object and its parts as well as among these parts, both of which are significantly helpful for finding discriminative parts. Therefore, this paper proposes the object-part attention driven discriminative localization (OPADDL) approach for weakly supervised fine-grained image classification, and the main novelties are: (1) Object-part attention model integrates two level attentions: object-level attention localizes objects of images, and part-level attention selects discriminative parts of object. Both are jointly employed to learn multi-view and multi-scale features to enhance their mutual promotion. (2) Object-part spatial model combines two spatial constraints: object spatial constraint ensures selected parts highly representative, and part spatial constraint eliminates redundancy and enhances discrimination of selected parts. Both are jointly employed to exploit the subtle and local differences for distinguishing the subcategories. Importantly, neither objects nor parts annotations are used, which avoids the heavy labor consuming of labeling. Comparing with more than 10 state-of-the-art methods on 3 widely used datasets, our OPADDL approach achieves the best performance.|fine grain imag classif recogn hundr subcategori belong basic level categori subcategori belong bird high challeng due larg varianc subcategori small varianc among differ subcategori exist method general find object part discrimin subcategori imag belong howev main two limit reli object part annot heavili labor consum ignor spatial relationship object part well among part signific help find discrimin part therefor paper propos object part attent driven discrimin local opaddl approach weak supervis fine grain imag classif main novelti object part attent model integr two level attent object level attent local object imag part level attent select discrimin part object joint employ learn multi view multi scale featur enhanc mutual promot object part spatial model combin two spatial constraint object spatial constraint ensur select part high repres part spatial constraint elimin redund enhanc discrimin select part joint employ exploit subtl local differ distinguish subcategori import neither object part annot use avoid heavi labor consum label compar state art method wide use dataset opaddl approach achiev best perform|['Yuxin Peng', 'Xiangteng He', 'Junjie Zhao']|['cs.CV']
2017-04-07T11:30:14Z|2017-04-06T06:09:55Z|http://arxiv.org/abs/1704.01719v1|http://arxiv.org/pdf/1704.01719v1|Beyond triplet loss: a deep quadruplet network for person   re-identification|beyond triplet loss deep quadruplet network person identif|Person re-identification (ReID) is an important task in wide area video surveillance which focuses on identifying people across different cameras. Recently, deep learning networks with a triplet loss become a common framework for person ReID. However, the triplet loss pays main attentions on obtaining correct orders on the training set. It still suffers from a weaker generalization capability from the training set to the testing set, thus resulting in inferior performance. In this paper, we design a quadruplet loss, which can lead to the model output with a larger inter-class variation and a smaller intra-class variation compared to the triplet loss. As a result, our model has a better generalization ability and can achieve a higher performance on the testing set. In particular, a quadruplet deep network using a margin-based online hard negative mining is proposed based on the quadruplet loss for the person ReID. In extensive experiments, the proposed network outperforms most of the state-of-the-art algorithms on representative datasets which clearly demonstrates the effectiveness of our proposed method.|person identif reid import task wide area video surveil focus identifi peopl across differ camera recent deep learn network triplet loss becom common framework person reid howev triplet loss pay main attent obtain correct order train set still suffer weaker general capabl train set test set thus result inferior perform paper design quadruplet loss lead model output larger inter class variat smaller intra class variat compar triplet loss result model better general abil achiev higher perform test set particular quadruplet deep network use margin base onlin hard negat mine propos base quadruplet loss person reid extens experi propos network outperform state art algorithm repres dataset clear demonstr effect propos method|['Weihua Chen', 'Xiaotang Chen', 'Jianguo Zhang', 'Kaiqi Huang']|['cs.CV']
2017-04-07T11:30:17Z|2017-04-06T06:00:14Z|http://arxiv.org/abs/1704.01716v1|http://arxiv.org/pdf/1704.01716v1|Action Representation Using Classifier Decision Boundaries|action represent use classifi decis boundari|Most popular deep learning based models for action recognition are designed to generate separate predictions within their short temporal windows, which are often aggregated by heuristic means to assign an action label to the full video segment. Given that not all frames from a video characterize the underlying action, pooling schemes that impose equal importance to all frames might be unfavorable. In an attempt towards tackling this challenge, we propose a novel pooling scheme, dubbed SVM pooling, based on the notion that among the bag of features generated by a CNN on all temporal windows, there is at least one feature that characterizes the action. To this end, we learn a decision hyperplane that separates this unknown yet useful feature from the rest. Applying multiple instance learning in an SVM setup, we use the parameters of this separating hyperplane as a descriptor for the video. Since these parameters are directly related to the support vectors in a max-margin framework, they serve as robust representations for pooling of the CNN features. We devise a joint optimization objective and an efficient solver that learns these hyperplanes per video and the corresponding action classifiers over the hyperplanes. Showcased experiments on the standard HMDB and UCF101 datasets demonstrate state-of-the-art performance.|popular deep learn base model action recognit design generat separ predict within short tempor window often aggreg heurist mean assign action label full video segment given frame video character action pool scheme impos equal import frame might unfavor attempt toward tackl challeng propos novel pool scheme dub svm pool base notion among bag featur generat cnn tempor window least one featur character action end learn decis hyperplan separ unknown yet use featur rest appli multipl instanc learn svm setup use paramet separ hyperplan descriptor video sinc paramet direct relat support vector max margin framework serv robust represent pool cnn featur devis joint optim object effici solver learn hyperplan per video correspond action classifi hyperplan showcas experi standard hmdb ucf dataset demonstr state art perform|['Jue Wang', 'Anoop Cherian', 'Fatih Porikli', 'Stephen Gould']|['cs.CV']
2017-04-07T11:30:17Z|2017-04-06T04:52:48Z|http://arxiv.org/abs/1704.01705v1|http://arxiv.org/pdf/1704.01705v1|Generate To Adapt: Aligning Domains using Generative Adversarial   Networks|generat adapt align domain use generat adversari network|Visual Domain adaptation is an actively researched problem in Computer Vision. In this work, we propose an approach that leverages unsupervised data to bring the source and target distributions closer in a learned joint feature space. We accomplish this by inducing a symbiotic relationship between the learned embedding and a generative adversarial framework. This is in contrast to methods which use an adversarial framework for realistic data generation and retraining deep models with such data. We show the strength and generality of our method by performing experiments on three different tasks: (1) Digit classification (MNIST, SVHN and USPS datasets) (2) Object recognition using OFFICE dataset and (3) Face recognition using the Celebrity Frontal Profile (CFP) dataset.|visual domain adapt activ research problem comput vision work propos approach leverag unsupervis data bring sourc target distribut closer learn joint featur space accomplish induc symbiot relationship learn embed generat adversari framework contrast method use adversari framework realist data generat retrain deep model data show strength general method perform experi three differ task digit classif mnist svhn usp dataset object recognit use offic dataset face recognit use celebr frontal profil cfp dataset|['Swami Sankaranarayanan', 'Yogesh Balaji', 'Carlos D. Castillo', 'Rama Chellappa']|['cs.CV']
2017-04-07T11:30:17Z|2017-04-05T23:04:43Z|http://arxiv.org/abs/1704.01664v1|http://arxiv.org/pdf/1704.01664v1|The Relative Performance of Ensemble Methods with Deep Convolutional   Neural Networks for Image Classification|relat perform ensembl method deep convolut neural network imag classif|Artificial neural networks have been successfully applied to a variety of machine learning tasks, including image recognition, semantic segmentation, and machine translation. However, few studies fully investigated ensembles of artificial neural networks. In this work, we investigated multiple widely used ensemble methods, including unweighted averaging, majority voting, the Bayes Optimal Classifier, and the (discrete) Super Learner, for image recognition tasks, with deep neural networks as candidate algorithms. We designed several experiments, with the candidate algorithms being the same network structure with different model checkpoints within a single training process, networks with same structure but trained multiple times stochastically, and networks with different structure. In addition, we further studied the over-confidence phenomenon of the neural networks, as well as its impact on the ensemble methods. Across all of our experiments, the Super Learner achieved best performance among all the ensemble methods in this study.|artifici neural network success appli varieti machin learn task includ imag recognit semant segment machin translat howev studi fulli investig ensembl artifici neural network work investig multipl wide use ensembl method includ unweight averag major vote bay optim classifi discret super learner imag recognit task deep neural network candid algorithm design sever experi candid algorithm network structur differ model checkpoint within singl train process network structur train multipl time stochast network differ structur addit studi confid phenomenon neural network well impact ensembl method across experi super learner achiev best perform among ensembl method studi|['Cheng Ju', 'Aur√©lien Bibaut', 'Mark J. van der Laan']|['stat.ML', 'cs.CV', 'cs.LG', 'stat.ME']
2017-04-07T11:30:17Z|2017-04-05T16:36:13Z|http://arxiv.org/abs/1704.01518v1|http://arxiv.org/pdf/1704.01518v1|Generating Descriptions with Grounded and Co-Referenced People|generat descript ground co referenc peopl|Learning how to generate descriptions of images or videos received major interest both in the Computer Vision and Natural Language Processing communities. While a few works have proposed to learn a grounding during the generation process in an unsupervised way (via an attention mechanism), it remains unclear how good the quality of the grounding is and whether it benefits the description quality. In this work we propose a movie description model which learns to generate description and jointly ground (localize) the mentioned characters as well as do visual co-reference resolution between pairs of consecutive sentences/clips. We also propose to use weak localization supervision through character mentions provided in movie descriptions to learn the character grounding. At training time, we first learn how to localize characters by relating their visual appearance to mentions in the descriptions via a semi-supervised approach. We then provide this (noisy) supervision into our description model which greatly improves its performance. Our proposed description model improves over prior work w.r.t. generated description quality and additionally provides grounding and local co-reference resolution. We evaluate it on the MPII Movie Description dataset using automatic and human evaluation measures and using our newly collected grounding and co-reference data for characters.|learn generat descript imag video receiv major interest comput vision natur languag process communiti work propos learn ground dure generat process unsupervis way via attent mechan remain unclear good qualiti ground whether benefit descript qualiti work propos movi descript model learn generat descript joint ground local mention charact well visual co refer resolut pair consecut sentenc clip also propos use weak local supervis charact mention provid movi descript learn charact ground train time first learn local charact relat visual appear mention descript via semi supervis approach provid noisi supervis descript model great improv perform propos descript model improv prior work generat descript qualiti addit provid ground local co refer resolut evalu mpii movi descript dataset use automat human evalu measur use newli collect ground co refer data charact|['Anna Rohrbach', 'Marcus Rohrbach', 'Siyu Tang', 'Seong Joon Oh', 'Bernt Schiele']|['cs.CV']
2017-04-07T11:30:17Z|2017-04-05T16:20:36Z|http://arxiv.org/abs/1704.01510v1|http://arxiv.org/pdf/1704.01510v1|Isotropic reconstruction of 3D fluorescence microscopy images using   convolutional neural networks|isotrop reconstruct fluoresc microscopi imag use convolut neural network|Fluorescence microscopy images usually show severe anisotropy in axial versus lateral resolution. This hampers downstream processing, i.e. the automatic extraction of quantitative biological data. While deconvolution methods and other techniques to address this problem exist, they are either time consuming to apply or limited in their ability to remove anisotropy. We propose a method to recover isotropic resolution from readily acquired anisotropic data. We achieve this using a convolutional neural network that is trained end-to-end from the same anisotropic body of data we later apply the network to. The network effectively learns to restore the full isotropic resolution by restoring the image under a trained, sample specific image prior. We apply our method to $3$ synthetic and $3$ real datasets and show that our results improve on results from deconvolution and state-of-the-art super-resolution techniques. Finally, we demonstrate that a standard 3D segmentation pipeline performs on the output of our network with comparable accuracy as on the full isotropic data.|fluoresc microscopi imag usual show sever anisotropi axial versus later resolut hamper downstream process automat extract quantit biolog data deconvolut method techniqu address problem exist either time consum appli limit abil remov anisotropi propos method recov isotrop resolut readili acquir anisotrop data achiev use convolut neural network train end end anisotrop bodi data later appli network network effect learn restor full isotrop resolut restor imag train sampl specif imag prior appli method synthet real dataset show result improv result deconvolut state art super resolut techniqu final demonstr standard segment pipelin perform output network compar accuraci full isotrop data|['Martin Weigert', 'Loic Royer', 'Florian Jug', 'Gene Myers']|['cs.CV']
2017-04-07T11:30:17Z|2017-04-05T16:06:09Z|http://arxiv.org/abs/1704.01502v1|http://arxiv.org/pdf/1704.01502v1|Weakly Supervised Dense Video Captioning|weak supervis dens video caption|This paper focuses on a novel and challenging vision task, dense video captioning, which aims to automatically describe a video clip with multiple informative and diverse caption sentences. The proposed method is trained without explicit annotation of fine-grained sentence to video region-sequence correspondence, but is only based on weak video-level sentence annotations. It differs from existing video captioning systems in three technical aspects. First, we propose lexical fully convolutional neural networks (Lexical-FCN) with weakly supervised multi-instance multi-label learning to weakly link video regions with lexical labels. Second, we introduce a novel submodular maximization scheme to generate multiple informative and diverse region-sequences based on the Lexical-FCN outputs. A winner-takes-all scheme is adopted to weakly associate sentences to region-sequences in the training phase. Third, a sequence-to-sequence learning based language model is trained with the weakly supervised information obtained through the association process. We show that the proposed method can not only produce informative and diverse dense captions, but also outperform state-of-the-art single video captioning methods by a large margin.|paper focus novel challeng vision task dens video caption aim automat describ video clip multipl inform divers caption sentenc propos method train without explicit annot fine grain sentenc video region sequenc correspond onli base weak video level sentenc annot differ exist video caption system three technic aspect first propos lexic fulli convolut neural network lexic fcn weak supervis multi instanc multi label learn weak link video region lexic label second introduc novel submodular maxim scheme generat multipl inform divers region sequenc base lexic fcn output winner take scheme adopt weak associ sentenc region sequenc train phase third sequenc sequenc learn base languag model train weak supervis inform obtain associ process show propos method onli produc inform divers dens caption also outperform state art singl video caption method larg margin|['Zhiqiang Shen', 'Jianguo Li', 'Zhou Su', 'Minjun Li', 'Yurong Chen', 'Yu-Gang Jiang', 'Xiangyang Xue']|['cs.CV']
2017-04-07T11:30:17Z|2017-04-05T15:12:25Z|http://arxiv.org/abs/1704.01474v1|http://arxiv.org/pdf/1704.01474v1|Convolutional Neural Networks for Page Segmentation of Historical   Document Images|convolut neural network page segment histor document imag|This paper presents a Convolutional Neural Network (CNN) based page segmentation method for handwritten historical document images. We consider page segmentation as a pixel labeling problem, i.e., each pixel is classified as one of the predefined classes. Traditional methods in this area rely on carefully hand-crafted features or large amounts of prior knowledge. In contrast, we propose to learn features from raw image pixels using a CNN. While many researchers focus on developing deep CNN architectures to solve different problems, we train a simple CNN with only one convolution layer. We show that the simple architecture achieves competitive results against other deep architectures on different public datasets. Experiments also demonstrate the effectiveness and superiority of the proposed method compared to previous methods.|paper present convolut neural network cnn base page segment method handwritten histor document imag consid page segment pixel label problem pixel classifi one predefin class tradit method area reli care hand craft featur larg amount prior knowledg contrast propos learn featur raw imag pixel use cnn mani research focus develop deep cnn architectur solv differ problem train simpl cnn onli one convolut layer show simpl architectur achiev competit result deep architectur differ public dataset experi also demonstr effect superior propos method compar previous method|['Kai Chen', 'Mathias Seuret']|['cs.CV', 'cs.LG', 'stat.ML']
2017-04-07T11:30:17Z|2017-04-05T13:53:25Z|http://arxiv.org/abs/1704.01429v1|http://arxiv.org/pdf/1704.01429v1|Non-Convex Weighted Lp Minimization based Group Sparse Representation   Framework for Image Denoising|non convex weight lp minim base group spars represent framework imag denois|Nonlocal image representation or group sparsity has attracted considerable interest in various low-level vision tasks and has led to several state-of-the-art image denoising techniques, such as BM3D, LSSC. In the past, convex optimization with sparsity-promoting convex regularization was usually regarded as a standard scheme for estimating sparse signals in noise. However, using convex regularization can not still obtain the correct sparsity solution under some practical problems including image inverse problems. In this paper we propose a non-convex weighted $\ell_p$ minimization based group sparse representation (GSR) framework for image denoising. To make the proposed scheme tractable and robust, the generalized soft-thresholding (GST) algorithm is adopted to solve the non-convex $\ell_p$ minimization problem. In addition, to improve the accuracy of the nonlocal similar patches selection, an adaptive patch search (APS) scheme is proposed. Experimental results have demonstrated that the proposed approach not only outperforms many state-of-the-art denoising methods such as BM3D and WNNM, but also results in a competitive speed.|nonloc imag represent group sparsiti attract consider interest various low level vision task led sever state art imag denois techniqu bmd lssc past convex optim sparsiti promot convex regular usual regard standard scheme estim spars signal nois howev use convex regular still obtain correct sparsiti solut practic problem includ imag invers problem paper propos non convex weight ell minim base group spars represent gsr framework imag denois make propos scheme tractabl robust general soft threshold gst algorithm adopt solv non convex ell minim problem addit improv accuraci nonloc similar patch select adapt patch search ap scheme propos experiment result demonstr propos approach onli outperform mani state art denois method bmd wnnm also result competit speed|['Qiong Wang', 'Xinggan Zhang', 'Yu Wu', 'Yechao Bai', 'Lan Tang', 'Zhiyuan Zha']|['cs.CV']
2017-04-07T11:30:17Z|2017-04-05T13:49:27Z|http://arxiv.org/abs/1704.01426v1|http://arxiv.org/pdf/1704.01426v1|The UMCD Dataset|umcd dataset|In recent years, the technological improvements of low-cost small-scale Unmanned Aerial Vehicles (UAVs) are promoting an ever-increasing use of them in different tasks. In particular, the use of small-scale UAVs is useful in all these low-altitude tasks in which common UAVs cannot be adopted, such as recurrent comprehensive view of wide environments, frequent monitoring of military areas, real-time classification of static and moving entities (e.g., people, cars, etc.). These tasks can be supported by mosaicking and change detection algorithms achieved at low-altitude. Currently, public datasets for testing these algorithms are not available. This paper presents the UMCD dataset, the first collection of geo-referenced video sequences acquired at low-altitude for mosaicking and change detection purposes. Five reference scenarios are also reported.|recent year technolog improv low cost small scale unman aerial vehicl uav promot ever increas use differ task particular use small scale uav use low altitud task common uav cannot adopt recurr comprehens view wide environ frequent monitor militari area real time classif static move entiti peopl car etc task support mosaick chang detect algorithm achiev low altitud current public dataset test algorithm avail paper present umcd dataset first collect geo referenc video sequenc acquir low altitud mosaick chang detect purpos five refer scenario also report|['Danilo Avola', 'Gian Luca Foresti', 'Niki Martinel', 'Daniele Pannone', 'Claudio Piciarelli']|['cs.CV']
2017-04-07T11:30:17Z|2017-04-05T11:28:25Z|http://arxiv.org/abs/1704.01372v1|http://arxiv.org/pdf/1704.01372v1|On the Relation between Color Image Denoising and Classification|relat color imag denois classif|Large amount of image denoising literature focuses on single channel images and often experimentally validates the proposed methods on tens of images at most. In this paper, we investigate the interaction between denoising and classification on large scale dataset. Inspired by classification models, we propose a novel deep learning architecture for color (multichannel) image denoising and report on thousands of images from ImageNet dataset as well as commonly used imagery. We study the importance of (sufficient) training data, how semantic class information can be traded for improved denoising results. As a result, our method greatly improves PSNR performance by 0.34 - 0.51 dB on average over state-of-the art methods on large scale dataset. We conclude that it is beneficial to incorporate in classification models. On the other hand, we also study how noise affect classification performance. In the end, we come to a number of interesting conclusions, some being counter-intuitive.|larg amount imag denois literatur focus singl channel imag often experiment valid propos method ten imag paper investig interact denois classif larg scale dataset inspir classif model propos novel deep learn architectur color multichannel imag denois report thousand imag imagenet dataset well common use imageri studi import suffici train data semant class inform trade improv denois result result method great improv psnr perform db averag state art method larg scale dataset conclud benefici incorpor classif model hand also studi nois affect classif perform end come number interest conclus counter intuit|['Jiqing Wu', 'Radu Timofte', 'Zhiwu Huang', 'Luc Van Gool']|['cs.CV']
2017-04-07T11:30:22Z|2017-04-05T10:37:42Z|http://arxiv.org/abs/1704.01358v1|http://arxiv.org/pdf/1704.01358v1|Incremental Tube Construction for Human Action Detection|increment tube construct human action detect|Current state-of-the-art action detection systems are tailored for offline batch-processing applications. However, for online applications like human-robot interaction, current systems fall short, either because they only detect one action per video, or because they assume that the entire video is available ahead of time. In this work, we introduce a real-time and online joint-labelling and association algorithm for action detection that can incrementally construct space-time action tubes on the most challenging action videos in which different action categories occur concurrently. In contrast to previous methods, we solve the detection-window association and action labelling problems jointly in a single pass. We demonstrate superior online association accuracy and speed (2.2ms per frame) as compared to the current state-of-the-art offline systems. We further demonstrate that the entire action detection pipeline can easily be made to work effectively in real-time using our action tube construction algorithm.|current state art action detect system tailor offlin batch process applic howev onlin applic like human robot interact current system fall short either becaus onli detect one action per video becaus assum entir video avail ahead time work introduc real time onlin joint label associ algorithm action detect increment construct space time action tube challeng action video differ action categori occur concurr contrast previous method solv detect window associ action label problem joint singl pass demonstr superior onlin associ accuraci speed ms per frame compar current state art offlin system demonstr entir action detect pipelin easili made work effect real time use action tube construct algorithm|['Harkirat S. Behl', 'Michael Sapienza', 'Gurkirt Singh', 'Suman Saha', 'Fabio Cuzzolin', 'Philip H. S. Torr']|['cs.CV']
2017-04-07T11:30:22Z|2017-04-05T09:58:51Z|http://arxiv.org/abs/1704.01344v1|http://arxiv.org/pdf/1704.01344v1|Not All Pixels Are Equal: Difficulty-aware Semantic Segmentation via   Deep Layer Cascade|pixel equal difficulti awar semant segment via deep layer cascad|We propose a novel deep layer cascade (LC) method to improve the accuracy and speed of semantic segmentation. Unlike the conventional model cascade (MC) that is composed of multiple independent models, LC treats a single deep model as a cascade of several sub-models. Earlier sub-models are trained to handle easy and confident regions, and they progressively feed-forward harder regions to the next sub-model for processing. Convolutions are only calculated on these regions to reduce computations. The proposed method possesses several advantages. First, LC classifies most of the easy regions in the shallow stage and makes deeper stage focuses on a few hard regions. Such an adaptive and 'difficulty-aware' learning improves segmentation performance. Second, LC accelerates both training and testing of deep network thanks to early decisions in the shallow stage. Third, in comparison to MC, LC is an end-to-end trainable framework, allowing joint learning of all sub-models. We evaluate our method on PASCAL VOC and Cityscapes datasets, achieving state-of-the-art performance and fast speed.|propos novel deep layer cascad lc method improv accuraci speed semant segment unlik convent model cascad mc compos multipl independ model lc treat singl deep model cascad sever sub model earlier sub model train handl easi confid region progress feed forward harder region next sub model process convolut onli calcul region reduc comput propos method possess sever advantag first lc classifi easi region shallow stage make deeper stage focus hard region adapt difficulti awar learn improv segment perform second lc acceler train test deep network thank earli decis shallow stage third comparison mc lc end end trainabl framework allow joint learn sub model evalu method pascal voc cityscap dataset achiev state art perform fast speed|['Xiaoxiao Li', 'Ziwei Liu', 'Ping Luo', 'Chen Change Loy', 'Xiaoou Tang']|['cs.CV', 'cs.LG']
2017-04-07T11:30:22Z|2017-04-05T08:00:14Z|http://arxiv.org/abs/1704.01297v1|http://arxiv.org/pdf/1704.01297v1|Automated Diagnosis of Epilepsy Employing Multifractal Detrended   Fluctuation Analysis Based Features|autom diagnosi epilepsi employ multifract detrend fluctuat analysi base featur|This contribution reports an application of MultiFractal Detrended Fluctuation Analysis, MFDFA based novel feature extraction technique for automated detection of epilepsy. In fractal geometry, Multifractal Detrended Fluctuation Analysis MFDFA is a popular technique to examine the self-similarity of a nonlinear, chaotic and noisy time series. In the present research work, EEG signals representing healthy, interictal (seizure free) and ictal activities (seizure) are acquired from an existing available database. The acquired EEG signals of different states are at first analyzed using MFDFA. To requisite the time series singularity quantification at local and global scales, a novel set of fourteen different features. Suitable feature ranking employing students t-test has been done to select the most statistically significant features which are henceforth being used as inputs to a support vector machines (SVM) classifier for the classification of different EEG signals. Eight different classification problems have been presented in this paper and it has been observed that the overall classification accuracy using MFDFA based features are reasonably satisfactory for all classification problems. The performance of the proposed method are also found to be quite commensurable and in some cases even better when compared with the results published in existing literature studied on the similar data set.|contribut report applic multifract detrend fluctuat analysi mfdfa base novel featur extract techniqu autom detect epilepsi fractal geometri multifract detrend fluctuat analysi mfdfa popular techniqu examin self similar nonlinear chaotic noisi time seri present research work eeg signal repres healthi interict seizur free ictal activ seizur acquir exist avail databas acquir eeg signal differ state first analyz use mfdfa requisit time seri singular quantif local global scale novel set fourteen differ featur suitabl featur rank employ student test done select statist signific featur henceforth use input support vector machin svm classifi classif differ eeg signal eight differ classif problem present paper observ overal classif accuraci use mfdfa base featur reason satisfactori classif problem perform propos method also found quit commensur case even better compar result publish exist literatur studi similar data set|['S Pratiher', 'S Chatterjee', 'R Bose']|['cs.CV', 'nlin.AO', 'nlin.CD', 'q-bio.QM', 'stat.OT', '92B25 92F99']
2017-04-07T11:30:22Z|2017-04-05T06:58:56Z|http://arxiv.org/abs/1704.01285v1|http://arxiv.org/pdf/1704.01285v1|Smart Mining for Deep Metric Learning|smart mine deep metric learn|To solve deep metric learning problems and producing feature embeddings, current methodologies will commonly use a triplet model to minimise the relative distance between samples from the same class and maximise the relative distance between samples from different classes. Though successful, the training convergence of this triplet model can be compromised by the fact that the vast majority of the training samples will produce gradients with magnitudes that are close to zero. This issue has motivated the development of methods that explore the global structure of the embedding and other methods that explore hard negative/positive mining. The effectiveness of such mining methods is often associated with intractable computational requirements. In this paper, we propose a novel deep metric learning method that combines the triplet model and the global structure of the embedding space. We rely on a smart mining procedure that produces effective training samples for a low computational cost. In addition, we propose an adaptive controller that automatically adjusts the smart mining hyper-parameters and speeds up the convergence of the training process. We show empirically that our proposed method allows for fast and more accurate training of triplet ConvNets than other competing mining methods. Additionally, we show that our method achieves new state-of-the-art embedding results for CUB-200-2011 and Cars196 datasets.|solv deep metric learn problem produc featur embed current methodolog common use triplet model minimis relat distanc sampl class maximis relat distanc sampl differ class though success train converg triplet model compromis fact vast major train sampl produc gradient magnitud close zero issu motiv develop method explor global structur embed method explor hard negat posit mine effect mine method often associ intract comput requir paper propos novel deep metric learn method combin triplet model global structur embed space reli smart mine procedur produc effect train sampl low comput cost addit propos adapt control automat adjust smart mine hyper paramet speed converg train process show empir propos method allow fast accur train triplet convnet compet mine method addit show method achiev new state art embed result cub car dataset|['Vijay B G Kumar', 'Ben Harwood', 'Gustavo Carneiro', 'Ian Reid', 'Tom Drummond']|['cs.CV']
2017-04-07T11:30:22Z|2017-04-05T04:24:41Z|http://arxiv.org/abs/1704.01266v1|http://arxiv.org/pdf/1704.01266v1|Supporting Navigation of Outdoor Shopping Complexes for   Visually-impaired Users through Multi-modal Data Fusion|support navig outdoor shop complex visual impair user multi modal data fusion|Outdoor shopping complexes (OSC) are extremely difficult for people with visual impairment to navigate. Existing GPS devices are mostly designed for roadside navigation and seldom transition well into an OSC-like setting. We report our study on the challenges faced by a blind person in navigating OSC through developing a new mobile application named iExplore. We first report an exploratory study aiming at deriving specific design principles for building this system by learning the unique challenges of the problem. Then we present a methodology that can be used to derive the necessary information for the development of iExplore, followed by experimental validation of the technology by a group of visually impaired users in a local outdoor shopping center. User feedback and other experiments suggest that iExplore, while at its very initial phase, has the potential of filling a practical gap in existing assistive technologies for the visually impaired.|outdoor shop complex osc extrem difficult peopl visual impair navig exist gps devic design roadsid navig seldom transit well osc like set report studi challeng face blind person navig osc develop new mobil applic name iexplor first report exploratori studi aim deriv specif design principl build system learn uniqu challeng problem present methodolog use deriv necessari inform develop iexplor follow experiment valid technolog group visual impair user local outdoor shop center user feedback experi suggest iexplor veri initi phase potenti fill practic gap exist assist technolog visual impair|['Archana Paladugu', 'Parag S. Chandakkar', 'Peng Zhang', 'Baoxin Li']|['cs.CV', 'cs.CY', 'cs.HC']
2017-04-07T11:30:22Z|2017-04-05T04:20:13Z|http://arxiv.org/abs/1704.01264v1|http://arxiv.org/pdf/1704.01264v1|Classification of Diabetic Retinopathy Images Using Multi-Class   Multiple-Instance Learning Based on Color Correlogram Features|classif diabet retinopathi imag use multi class multipl instanc learn base color correlogram featur|All people with diabetes have the risk of developing diabetic retinopathy (DR), a vision-threatening complication. Early detection and timely treatment can reduce the occurrence of blindness due to DR. Computer-aided diagnosis has the potential benefit of improving the accuracy and speed in DR detection. This study is concerned with automatic classification of images with microaneurysm (MA) and neovascularization (NV), two important DR clinical findings. Together with normal images, this presents a 3-class classification problem. We propose a modified color auto-correlogram feature (AutoCC) with low dimensionality that is spectrally tuned towards DR images. Recognizing the fact that the images with or without MA or NV are generally different only in small, localized regions, we propose to employ a multi-class, multiple-instance learning framework for performing the classification task using the proposed feature. Extensive experiments including comparison with a few state-of-art image classification approaches have been performed and the results suggest that the proposed approach is promising as it outperforms other methods by a large margin.|peopl diabet risk develop diabet retinopathi dr vision threaten complic earli detect time treatment reduc occurr blind due dr comput aid diagnosi potenti benefit improv accuraci speed dr detect studi concern automat classif imag microaneurysm neovascular nv two import dr clinic find togeth normal imag present class classif problem propos modifi color auto correlogram featur autocc low dimension spectral tune toward dr imag recogn fact imag without nv general differ onli small local region propos employ multi class multipl instanc learn framework perform classif task use propos featur extens experi includ comparison state art imag classif approach perform result suggest propos approach promis outperform method larg margin|['Ragav Venkatesan', 'Parag S. Chandakkar', 'Baoxin Li']|['cs.CV']
2017-04-07T11:30:22Z|2017-04-05T04:07:07Z|http://arxiv.org/abs/1704.01262v1|http://arxiv.org/pdf/1704.01262v1|Investigating Human Factors in Image Forgery Detection|investig human factor imag forgeri detect|"In today's age of internet and social media, one can find an enormous volume of forged images on-line. These images have been used in the past to convey falsified information and achieve harmful intentions. The spread and the effect of the social media only makes this problem more severe. While creating forged images has become easier due to software advancements, there is no automated algorithm which can reliably detect forgery.   Image forgery detection can be seen as a subset of image understanding problem. Human performance is still the gold-standard for these type of problems when compared to existing state-of-art automated algorithms. We conduct a subjective evaluation test with the aid of eye-tracker to investigate into human factors associated with this problem. We compare the performance of an automated algorithm and humans for forgery detection problem. We also develop an algorithm which uses the data from the evaluation test to predict the difficulty-level of an image (the difficulty-level of an image here denotes how difficult it is for humans to detect forgery in an image. Terms such as ""Easy/difficult image"" will be used in the same context). The experimental results presented in this paper should facilitate development of better algorithms in the future."|today age internet social media one find enorm volum forg imag line imag use past convey falsifi inform achiev harm intent spread effect social media onli make problem sever creat forg imag becom easier due softwar advanc autom algorithm reliabl detect forgeri imag forgeri detect seen subset imag understand problem human perform still gold standard type problem compar exist state art autom algorithm conduct subject evalu test aid eye tracker investig human factor associ problem compar perform autom algorithm human forgeri detect problem also develop algorithm use data evalu test predict difficulti level imag difficulti level imag denot difficult human detect forgeri imag term easi difficult imag use context experiment result present paper facilit develop better algorithm futur|['Parag S. Chandakkar', 'Baoxin Li']|['cs.CV']
2017-04-07T11:30:22Z|2017-04-05T03:38:08Z|http://arxiv.org/abs/1704.01256v1|http://arxiv.org/pdf/1704.01256v1|Improving Vision-based Self-positioning in Intelligent Transportation   Systems via Integrated Lane and Vehicle Detection|improv vision base self posit intellig transport system via integr lane vehicl detect|Traffic congestion is a widespread problem. Dynamic traffic routing systems and congestion pricing are getting importance in recent research. Lane prediction and vehicle density estimation is an important component of such systems. We introduce a novel problem of vehicle self-positioning which involves predicting the number of lanes on the road and vehicle's position in those lanes using videos captured by a dashboard camera. We propose an integrated closed-loop approach where we use the presence of vehicles to aid the task of self-positioning and vice-versa. To incorporate multiple factors and high-level semantic knowledge into the solution, we formulate this problem as a Bayesian framework. In the framework, the number of lanes, the vehicle's position in those lanes and the presence of other vehicles are considered as parameters. We also propose a bounding box selection scheme to reduce the number of false detections and increase the computational efficiency. We show that the number of box proposals decreases by a factor of 6 using the selection approach. It also results in large reduction in the number of false detections. The entire approach is tested on real-world videos and is found to give acceptable results.|traffic congest widespread problem dynam traffic rout system congest price get import recent research lane predict vehicl densiti estim import compon system introduc novel problem vehicl self posit involv predict number lane road vehicl posit lane use video captur dashboard camera propos integr close loop approach use presenc vehicl aid task self posit vice versa incorpor multipl factor high level semant knowledg solut formul problem bayesian framework framework number lane vehicl posit lane presenc vehicl consid paramet also propos bound box select scheme reduc number fals detect increas comput effici show number box propos decreas factor use select approach also result larg reduct number fals detect entir approach test real world video found give accept result|['Parag S. Chandakkar', 'Yilin Wang', 'Baoxin Li']|['cs.CV']
2017-04-07T11:30:22Z|2017-04-05T03:13:01Z|http://arxiv.org/abs/1704.01250v1|http://arxiv.org/pdf/1704.01250v1|Relative Learning from Web Images for Content-adaptive Enhancement|relat learn web imag content adapt enhanc|Personalized and content-adaptive image enhancement can find many applications in the age of social media and mobile computing. This paper presents a relative-learning-based approach, which, unlike previous methods, does not require matching original and enhanced images for training. This allows the use of massive online photo collections to train a ranking model for improved enhancement. We first propose a multi-level ranking model, which is learned from only relatively-labeled inputs that are automatically crawled. Then we design a novel parameter sampling scheme under this model to generate the desired enhancement parameters for a new image. For evaluation, we first verify the effectiveness and the generalization abilities of our approach, using images that have been enhanced/labeled by experts. Then we carry out subjective tests, which show that users prefer images enhanced by our approach over other existing methods.|person content adapt imag enhanc find mani applic age social media mobil comput paper present relat learn base approach unlik previous method doe requir match origin enhanc imag train allow use massiv onlin photo collect train rank model improv enhanc first propos multi level rank model learn onli relat label input automat crawl design novel paramet sampl scheme model generat desir enhanc paramet new imag evalu first verifi effect general abil approach use imag enhanc label expert carri subject test show user prefer imag enhanc approach exist method|['Parag S. Chandakkar', 'Qiongjie Tian', 'Baoxin Li']|['cs.CV']
2017-04-07T11:30:22Z|2017-04-05T03:04:28Z|http://arxiv.org/abs/1704.01249v1|http://arxiv.org/pdf/1704.01249v1|A Structured Approach to Predicting Image Enhancement Parameters|structur approach predict imag enhanc paramet|Social networking on mobile devices has become a commonplace of everyday life. In addition, photo capturing process has become trivial due to the advances in mobile imaging. Hence people capture a lot of photos everyday and they want them to be visually-attractive. This has given rise to automated, one-touch enhancement tools. However, the inability of those tools to provide personalized and content-adaptive enhancement has paved way for machine-learned methods to do the same. The existing typical machine-learned methods heuristically (e.g. kNN-search) predict the enhancement parameters for a new image by relating the image to a set of similar training images. These heuristic methods need constant interaction with the training images which makes the parameter prediction sub-optimal and computationally expensive at test time which is undesired. This paper presents a novel approach to predicting the enhancement parameters given a new image using only its features, without using any training images. We propose to model the interaction between the image features and its corresponding enhancement parameters using the matrix factorization (MF) principles. We also propose a way to integrate the image features in the MF formulation. We show that our approach outperforms heuristic approaches as well as recent approaches in MF and structured prediction on synthetic as well as real-world data of image enhancement.|social network mobil devic becom commonplac everyday life addit photo captur process becom trivial due advanc mobil imag henc peopl captur lot photo everyday want visual attract given rise autom one touch enhanc tool howev inabl tool provid person content adapt enhanc pave way machin learn method exist typic machin learn method heurist knn search predict enhanc paramet new imag relat imag set similar train imag heurist method need constant interact train imag make paramet predict sub optim comput expens test time undesir paper present novel approach predict enhanc paramet given new imag use onli featur without use ani train imag propos model interact imag featur correspond enhanc paramet use matrix factor mf principl also propos way integr imag featur mf formul show approach outperform heurist approach well recent approach mf structur predict synthet well real world data imag enhanc|['Parag S. Chandakkar', 'Baoxin Li']|['cs.CV']
2017-04-07T11:30:26Z|2017-04-05T02:49:30Z|http://arxiv.org/abs/1704.01248v1|http://arxiv.org/pdf/1704.01248v1|A Computational Approach to Relative Aesthetics|comput approach relat aesthet|Computational visual aesthetics has recently become an active research area. Existing state-of-art methods formulate this as a binary classification task where a given image is predicted to be beautiful or not. In many applications such as image retrieval and enhancement, it is more important to rank images based on their aesthetic quality instead of binary-categorizing them. Furthermore, in such applications, it may be possible that all images belong to the same category. Hence determining the aesthetic ranking of the images is more appropriate. To this end, we formulate a novel problem of ranking images with respect to their aesthetic quality. We construct a new dataset of image pairs with relative labels by carefully selecting images from the popular AVA dataset. Unlike in aesthetics classification, there is no single threshold which would determine the ranking order of the images across our entire dataset. We propose a deep neural network based approach that is trained on image pairs by incorporating principles from relative learning. Results show that such relative training procedure allows our network to rank the images with a higher accuracy than a state-of-art network trained on the same set of images using binary labels.|comput visual aesthet recent becom activ research area exist state art method formul binari classif task given imag predict beauti mani applic imag retriev enhanc import rank imag base aesthet qualiti instead binari categor furthermor applic may possibl imag belong categori henc determin aesthet rank imag appropri end formul novel problem rank imag respect aesthet qualiti construct new dataset imag pair relat label care select imag popular ava dataset unlik aesthet classif singl threshold would determin rank order imag across entir dataset propos deep neural network base approach train imag pair incorpor principl relat learn result show relat train procedur allow network rank imag higher accuraci state art network train set imag use binari label|['Parag S. Chandakkar', 'Vijetha Gattupalli', 'Baoxin Li']|['cs.CV']
2017-04-07T11:30:26Z|2017-04-05T02:37:44Z|http://arxiv.org/abs/1704.01246v1|http://arxiv.org/pdf/1704.01246v1|Estimation of Tissue Microstructure Using a Deep Network Inspired by a   Sparse Reconstruction Framework|estim tissu microstructur use deep network inspir spars reconstruct framework|Diffusion magnetic resonance imaging (dMRI) provides a unique tool for noninvasively probing the microstructure of the neuronal tissue. The NODDI model has been a popular approach to the estimation of tissue microstructure in many neuroscience studies. It represents the diffusion signals with three types of diffusion in tissue: intra-cellular, extra-cellular, and cerebrospinal fluid compartments. However, the original NODDI method uses a computationally expensive procedure to fit the model and could require a large number of diffusion gradients for accurate microstructure estimation, which may be impractical for clinical use. Therefore, efforts have been devoted to efficient and accurate NODDI microstructure estimation with a reduced number of diffusion gradients. In this work, we propose a deep network based approach to the NODDI microstructure estimation, which is named Microstructure Estimation using a Deep Network (MEDN). Motivated by the AMICO algorithm which accelerates the computation of NODDI parameters, we formulate the microstructure estimation problem in a dictionary-based framework. The proposed network comprises two cascaded stages. The first stage resembles the solution to a dictionary-based sparse reconstruction problem and the second stage computes the final microstructure using the output of the first stage. The weights in the two stages are jointly learned from training data, which is obtained from training dMRI scans with diffusion gradients that densely sample the q-space. The proposed method was applied to brain dMRI scans, where two shells each with 30 gradient directions (60 diffusion gradients in total) were used. Estimation accuracy with respect to the gold standard was measured and the results demonstrate that MEDN outperforms the competing algorithms.|diffus magnet reson imag dmri provid uniqu tool noninvas probe microstructur neuron tissu noddi model popular approach estim tissu microstructur mani neurosci studi repres diffus signal three type diffus tissu intra cellular extra cellular cerebrospin fluid compart howev origin noddi method use comput expens procedur fit model could requir larg number diffus gradient accur microstructur estim may impract clinic use therefor effort devot effici accur noddi microstructur estim reduc number diffus gradient work propos deep network base approach noddi microstructur estim name microstructur estim use deep network medn motiv amico algorithm acceler comput noddi paramet formul microstructur estim problem dictionari base framework propos network compris two cascad stage first stage resembl solut dictionari base spars reconstruct problem second stage comput final microstructur use output first stage weight two stage joint learn train data obtain train dmri scan diffus gradient dens sampl space propos method appli brain dmri scan two shell gradient direct diffus gradient total use estim accuraci respect gold standard measur result demonstr medn outperform compet algorithm|['Chuyang Ye']|['cs.CV']
2017-04-07T11:30:26Z|2017-04-05T01:28:04Z|http://arxiv.org/abs/1704.01235v1|http://arxiv.org/pdf/1704.01235v1|Joint Regression and Ranking for Image Enhancement|joint regress rank imag enhanc|Research on automated image enhancement has gained momentum in recent years, partially due to the need for easy-to-use tools for enhancing pictures captured by ubiquitous cameras on mobile devices. Many of the existing leading methods employ machine-learning-based techniques, by which some enhancement parameters for a given image are found by relating the image to the training images with known enhancement parameters. While knowing the structure of the parameter space can facilitate search for the optimal solution, none of the existing methods has explicitly modeled and learned that structure. This paper presents an end-to-end, novel joint regression and ranking approach to model the interaction between desired enhancement parameters and images to be processed, employing a Gaussian process (GP). GP allows searching for ideal parameters using only the image features. The model naturally leads to a ranking technique for comparing images in the induced feature space. Comparative evaluation using the ground-truth based on the MIT-Adobe FiveK dataset plus subjective tests on an additional data-set were used to demonstrate the effectiveness of the proposed approach.|research autom imag enhanc gain momentum recent year partial due need easi use tool enhanc pictur captur ubiquit camera mobil devic mani exist lead method employ machin learn base techniqu enhanc paramet given imag found relat imag train imag known enhanc paramet know structur paramet space facilit search optim solut none exist method explicit model learn structur paper present end end novel joint regress rank approach model interact desir enhanc paramet imag process employ gaussian process gp gp allow search ideal paramet use onli imag featur model natur lead rank techniqu compar imag induc featur space compar evalu use ground truth base mit adob fivek dataset plus subject test addit data set use demonstr effect propos approach|['Parag S. Chandakkar', 'Baoxin Li']|['cs.CV']
2017-04-07T11:30:26Z|2017-04-04T23:52:40Z|http://arxiv.org/abs/1704.01222v1|http://arxiv.org/pdf/1704.01222v1|Escape from Cells: Deep Kd-Networks for The Recognition of 3D Point   Cloud Models|escap cell deep kd network recognit point cloud model|We present a new deep learning architecture (called Kd-network) that is designed for 3D model recognition tasks and works with unstructured point clouds. The new architecture performs multiplicative transformations and share parameters of these transformations according to the subdivisions of the point clouds imposed onto them by Kd-trees. Unlike the currently dominant convolutional architectures that usually require rasterization on uniform two-dimensional or three-dimensional grids, Kd-networks do not rely on such grids in any way and therefore avoid poor scaling behaviour. In a series of experiments with popular shape recognition benchmarks, Kd-networks demonstrate competitive performance in a number of shape recognition tasks such as shape classification, shape retrieval and shape part segmentation.|present new deep learn architectur call kd network design model recognit task work unstructur point cloud new architectur perform multipl transform share paramet transform accord subdivis point cloud impos onto kd tree unlik current domin convolut architectur usual requir raster uniform two dimension three dimension grid kd network reli grid ani way therefor avoid poor scale behaviour seri experi popular shape recognit benchmark kd network demonstr competit perform number shape recognit task shape classif shape retriev shape part segment|['Roman Klokov', 'Victor Lempitsky']|['cs.CV']
2017-04-07T11:30:26Z|2017-04-04T21:32:04Z|http://arxiv.org/abs/1704.01194v1|http://arxiv.org/pdf/1704.01194v1|Two Stream LSTM: A Deep Fusion Framework for Human Action Recognition|two stream lstm deep fusion framework human action recognit|In this paper we address the problem of human action recognition from video sequences. Inspired by the exemplary results obtained via automatic feature learning and deep learning approaches in computer vision, we focus our attention towards learning salient spatial features via a convolutional neural network (CNN) and then map their temporal relationship with the aid of Long-Short-Term-Memory (LSTM) networks. Our contribution in this paper is a deep fusion framework that more effectively exploits spatial features from CNNs with temporal features from LSTM models. We also extensively evaluate their strengths and weaknesses. We find that by combining both the sets of features, the fully connected features effectively act as an attention mechanism to direct the LSTM to interesting parts of the convolutional feature sequence. The significance of our fusion method is its simplicity and effectiveness compared to other state-of-the-art methods. The evaluation results demonstrate that this hierarchical multi stream fusion method has higher performance compared to single stream mapping methods allowing it to achieve high accuracy outperforming current state-of-the-art methods in three widely used databases: UCF11, UCFSports, jHMDB.|paper address problem human action recognit video sequenc inspir exemplari result obtain via automat featur learn deep learn approach comput vision focus attent toward learn salient spatial featur via convolut neural network cnn map tempor relationship aid long short term memori lstm network contribut paper deep fusion framework effect exploit spatial featur cnns tempor featur lstm model also extens evalu strength weak find combin set featur fulli connect featur effect act attent mechan direct lstm interest part convolut featur sequenc signific fusion method simplic effect compar state art method evalu result demonstr hierarch multi stream fusion method higher perform compar singl stream map method allow achiev high accuraci outperform current state art method three wide use databas ucf ucfsport jhmdb|['Harshala Gammulle', 'Simon Denman', 'Sridha Sridharan', 'Clinton Fookes']|['cs.CV']
2017-04-07T11:30:26Z|2017-04-04T18:56:53Z|http://arxiv.org/abs/1704.01155v1|http://arxiv.org/pdf/1704.01155v1|Feature Squeezing: Detecting Adversarial Examples in Deep Neural   Networks|featur squeez detect adversari exampl deep neural network|Although deep neural networks (DNNs) have achieved great success in many computer vision tasks, recent studies have shown they are vulnerable to adversarial examples. Such examples, typically generated by adding small but purposeful distortions, can frequently fool DNN models. Previous studies to defend against adversarial examples mostly focused on refining the DNN models. They have either shown limited success or suffer from the expensive computation. We propose a new strategy, \emph{feature squeezing}, that can be used to harden DNN models by detecting adversarial examples. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many different feature vectors in the original space into a single sample. By comparing a DNN model's prediction on the original input with that on the squeezed input, feature squeezing detects adversarial examples with high accuracy and few false positives. This paper explores two instances of feature squeezing: reducing the color bit depth of each pixel and smoothing using a spatial filter. These strategies are straightforward, inexpensive, and complementary to defensive methods that operate on the underlying model, such as adversarial training.|although deep neural network dnns achiev great success mani comput vision task recent studi shown vulner adversari exampl exampl typic generat ad small purpos distort frequent fool dnn model previous studi defend adversari exampl focus refin dnn model either shown limit success suffer expens comput propos new strategi emph featur squeez use harden dnn model detect adversari exampl featur squeez reduc search space avail adversari coalesc sampl correspond mani differ featur vector origin space singl sampl compar dnn model predict origin input squeez input featur squeez detect adversari exampl high accuraci fals posit paper explor two instanc featur squeez reduc color bit depth pixel smooth use spatial filter strategi straightforward inexpens complementari defens method oper model adversari train|['Weilin Xu', 'David Evans', 'Yanjun Qi']|['cs.CV', 'cs.CR', 'cs.LG']
2017-04-07T11:30:26Z|2017-04-04T18:41:47Z|http://arxiv.org/abs/1704.01152v1|http://arxiv.org/pdf/1704.01152v1|Pose2Instance: Harnessing Keypoints for Person Instance Segmentation|poseinst har keypoint person instanc segment|Human keypoints are a well-studied representation of people.We explore how to use keypoint models to improve instance-level person segmentation. The main idea is to harness the notion of a distance transform of oracle provided keypoints or estimated keypoint heatmaps as a prior for person instance segmentation task within a deep neural network. For training and evaluation, we consider all those images from COCO where both instance segmentation and human keypoints annotations are available. We first show how oracle keypoints can boost the performance of existing human segmentation model during inference without any training. Next, we propose a framework to directly learn a deep instance segmentation model conditioned on human pose. Experimental results show that at various Intersection Over Union (IOU) thresholds, in a constrained environment with oracle keypoints, the instance segmentation accuracy achieves 10% to 12% relative improvements over a strong baseline of oracle bounding boxes. In a more realistic environment, without the oracle keypoints, the proposed deep person instance segmentation model conditioned on human pose achieves 3.8% to 10.5% relative improvements comparing with its strongest baseline of a deep network trained only for segmentation.|human keypoint well studi represent peopl explor use keypoint model improv instanc level person segment main idea har notion distanc transform oracl provid keypoint estim keypoint heatmap prior person instanc segment task within deep neural network train evalu consid imag coco instanc segment human keypoint annot avail first show oracl keypoint boost perform exist human segment model dure infer without ani train next propos framework direct learn deep instanc segment model condit human pose experiment result show various intersect union iou threshold constrain environ oracl keypoint instanc segment accuraci achiev relat improv strong baselin oracl bound box realist environ without oracl keypoint propos deep person instanc segment model condit human pose achiev relat improv compar strongest baselin deep network train onli segment|['Subarna Tripathi', 'Maxwell Collins', 'Matthew Brown', 'Serge Belongie']|['cs.CV']
2017-04-07T11:30:26Z|2017-04-04T18:14:02Z|http://arxiv.org/abs/1704.01137v1|http://arxiv.org/pdf/1704.01137v1|DyVEDeep: Dynamic Variable Effort Deep Neural Networks|dyvedeep dynam variabl effort deep neural network|Deep Neural Networks (DNNs) have advanced the state-of-the-art in a variety of machine learning tasks and are deployed in increasing numbers of products and services. However, the computational requirements of training and evaluating large-scale DNNs are growing at a much faster pace than the capabilities of the underlying hardware platforms that they are executed upon. In this work, we propose Dynamic Variable Effort Deep Neural Networks (DyVEDeep) to reduce the computational requirements of DNNs during inference. Previous efforts propose specialized hardware implementations for DNNs, statically prune the network, or compress the weights. Complementary to these approaches, DyVEDeep is a dynamic approach that exploits the heterogeneity in the inputs to DNNs to improve their compute efficiency with comparable classification accuracy. DyVEDeep equips DNNs with dynamic effort mechanisms that, in the course of processing an input, identify how critical a group of computations are to classify the input. DyVEDeep dynamically focuses its compute effort only on the critical computa- tions, while skipping or approximating the rest. We propose 3 effort knobs that operate at different levels of granularity viz. neuron, feature and layer levels. We build DyVEDeep versions for 5 popular image recognition benchmarks - one for CIFAR-10 and four for ImageNet (AlexNet, OverFeat and VGG-16, weight-compressed AlexNet). Across all benchmarks, DyVEDeep achieves 2.1x-2.6x reduction in the number of scalar operations, which translates to 1.8x-2.3x performance improvement over a Caffe-based implementation, with < 0.5% loss in accuracy.|deep neural network dnns advanc state art varieti machin learn task deploy increas number product servic howev comput requir train evalu larg scale dnns grow much faster pace capabl hardwar platform execut upon work propos dynam variabl effort deep neural network dyvedeep reduc comput requir dnns dure infer previous effort propos special hardwar implement dnns static prune network compress weight complementari approach dyvedeep dynam approach exploit heterogen input dnns improv comput effici compar classif accuraci dyvedeep equip dnns dynam effort mechan cours process input identifi critic group comput classifi input dyvedeep dynam focus comput effort onli critic computa tion skip approxim rest propos effort knob oper differ level granular viz neuron featur layer level build dyvedeep version popular imag recognit benchmark one cifar four imagenet alexnet overfeat vgg weight compress alexnet across benchmark dyvedeep achiev reduct number scalar oper translat perform improv caff base implement loss accuraci|['Sanjay Ganapathy', 'Swagath Venkataramani', 'Balaraman Ravindran', 'Anand Raghunathan']|['cs.NE', 'cs.CV', 'cs.LG']
2017-04-07T11:30:26Z|2017-04-04T18:03:55Z|http://arxiv.org/abs/1704.01133v1|http://arxiv.org/pdf/1704.01133v1|Satellite Image-based Localization via Learned Embeddings|satellit imag base local via learn embed|We propose a vision-based method that localizes a ground vehicle using publicly available satellite imagery as the only prior knowledge of the environment. Our approach takes as input a sequence of ground-level images acquired by the vehicle as it navigates, and outputs an estimate of the vehicle's pose relative to a georeferenced satellite image. We overcome the significant viewpoint and appearance variations between the images through a neural multi-view model that learns location-discriminative embeddings in which ground-level images are matched with their corresponding satellite view of the scene. We use this learned function as an observation model in a filtering framework to maintain a distribution over the vehicle's pose. We evaluate our method on different benchmark datasets and demonstrate its ability localize ground-level images in environments novel relative to training, despite the challenges of significant viewpoint and appearance variations.|propos vision base method local ground vehicl use public avail satellit imageri onli prior knowledg environ approach take input sequenc ground level imag acquir vehicl navig output estim vehicl pose relat georeferenc satellit imag overcom signific viewpoint appear variat imag neural multi view model learn locat discrimin embed ground level imag match correspond satellit view scene use learn function observ model filter framework maintain distribut vehicl pose evalu method differ benchmark dataset demonstr abil local ground level imag environ novel relat train despit challeng signific viewpoint appear variat|['Dong-Ki Kim', 'Matthew R. Walter']|['cs.RO', 'cs.CV', 'cs.LG']
2017-04-07T11:30:26Z|2017-04-04T16:15:54Z|http://arxiv.org/abs/1704.01085v1|http://arxiv.org/pdf/1704.01085v1|Deep Depth From Focus|deep depth focus|Depth from Focus (DFF) is one of the classical ill-posed inverse problems in computer vision. Most approaches recover the depth at each pixel based on the focal setting which exhibits maximal sharpness. Yet, it is not obvious how to reliably estimate the sharpness level, particularly in low-textured areas. In this paper, we propose 'Deep Depth From Focus (DDFF)' as the first end-to-end learning approach to this problem. Towards this goal, we create a novel real-scene indoor benchmark composed of 4D light-field images obtained from a plenoptic camera and ground truth depth obtained from a registered RGB-D sensor. Compared to existing benchmarks our dataset is 30 times larger, enabling the use of machine learning for this inverse problem. We compare our results with state-of-the-art DFF methods and we also analyze the effect of several key deep architectural components. These experiments show that DDFFNet achieves state-of-the-art performance in all scenes, reducing depth error by more than 70% wrt classic DFF methods.|depth focus dff one classic ill pose invers problem comput vision approach recov depth pixel base focal set exhibit maxim sharp yet obvious reliabl estim sharp level particular low textur area paper propos deep depth focus ddff first end end learn approach problem toward goal creat novel real scene indoor benchmark compos light field imag obtain plenopt camera ground truth depth obtain regist rgb sensor compar exist benchmark dataset time larger enabl use machin learn invers problem compar result state art dff method also analyz effect sever key deep architectur compon experi show ddffnet achiev state art perform scene reduc depth error wrt classic dff method|['Caner Hazirbas', 'Laura Leal-Taix√©', 'Daniel Cremers']|['cs.CV']
2017-04-07T11:30:30Z|2017-04-04T15:35:31Z|http://arxiv.org/abs/1704.01069v1|http://arxiv.org/pdf/1704.01069v1|ME R-CNN: Multi-Expert Region-based CNN for Object Detection|cnn multi expert region base cnn object detect|"Recent CNN-based object detection methods have drastically improved their performances but still use a single classifier as opposed to ""multiple experts"" in categorizing objects. The main motivation of introducing multi-experts is twofold: i) to allow different experts to specialize in different fundamental object shape priors and ii) to better capture the appearance variations caused by different poses and viewing angles. The proposed approach, referred to as multi-expert Region-based CNN (ME R-CNN), consists of three experts each responsible for objects with particular shapes: horizontally elongated, square-like, and vertically elongated. Each expert is a network with multiple fully connected layers and all the experts are preceded by a shared network which consists of multiple convolutional layers.   On top of using selective search which provides a compact, yet effective set of region of interests (RoIs) for object detection, we augmented the set by also employing the exhaustive search for training. Incorporating the exhaustive search can provide complementary advantages: i) it captures the multitude of neighboring RoIs missed by the selective search, and thus ii) provide significantly larger amount of training examples to achieve the enhanced accuracy."|recent cnn base object detect method drastic improv perform still use singl classifi oppos multipl expert categor object main motiv introduc multi expert twofold allow differ expert special differ fundament object shape prior ii better captur appear variat caus differ pose view angl propos approach refer multi expert region base cnn cnn consist three expert respons object particular shape horizont elong squar like vertic elong expert network multipl fulli connect layer expert preced share network consist multipl convolut layer top use select search provid compact yet effect set region interest roi object detect augment set also employ exhaust search train incorpor exhaust search provid complementari advantag captur multitud neighbor roi miss select search thus ii provid signific larger amount train exampl achiev enhanc accuraci|['Hyungtae Lee', 'Sungmin Eum', 'Heesung Kwon']|['cs.CV']
2017-04-07T11:30:30Z|2017-04-04T15:02:05Z|http://arxiv.org/abs/1704.01047v1|http://arxiv.org/pdf/1704.01047v1|OctNetFusion: Learning Depth Fusion from Data|octnetfus learn depth fusion data|In this paper, we present a learning based approach to depth fusion, i.e., dense 3D reconstruction from multiple depth images. The most common approach to depth fusion is based on averaging truncated signed distance functions, which was originally proposed by Curless and Levoy in 1996. While this method achieves great results, it can not reconstruct surfaces occluded in the input views and requires a large number frames to filter out sensor noise and outliers. Motivated by large 3D model databases and recent advances in deep learning, we present a novel 3D convolutional network architecture that learns to predict an implicit surface representation from the input depth maps. Our learning based fusion approach significantly outperforms the traditional volumetric fusion approach in terms of noise reduction and outlier suppression. By learning the structure of real world 3D objects and scenes, our approach is further able to reconstruct occluded regions and to fill gaps in the reconstruction. We evaluate our approach extensively on both synthetic and real-world datasets for volumetric fusion. Further, we apply our approach to the problem of 3D shape completion from a single view where our approach achieves state-of-the-art results.|paper present learn base approach depth fusion dens reconstruct multipl depth imag common approach depth fusion base averag truncat sign distanc function origin propos curless levoy method achiev great result reconstruct surfac occlud input view requir larg number frame filter sensor nois outlier motiv larg model databas recent advanc deep learn present novel convolut network architectur learn predict implicit surfac represent input depth map learn base fusion approach signific outperform tradit volumetr fusion approach term nois reduct outlier suppress learn structur real world object scene approach abl reconstruct occlud region fill gap reconstruct evalu approach extens synthet real world dataset volumetr fusion appli approach problem shape complet singl view approach achiev state art result|['Gernot Riegler', 'Ali Osman Ulusoy', 'Horst Bischof', 'Andreas Geiger']|['cs.CV']
2017-04-07T11:30:30Z|2017-04-04T14:23:26Z|http://arxiv.org/abs/1704.01472v1|http://arxiv.org/pdf/1704.01472v1|Automatic Breast Ultrasound Image Segmentation: A Survey|automat breast ultrasound imag segment survey|Breast cancer is one of the leading causes of cancer death among women worldwide. In clinical routine, automatic breast ultrasound (BUS) image segmentation is very challenging and essential for cancer diagnosis and treatment planning. Many BUS segmentation approaches have been studied in the last two decades, and have been proved to be effective on private datasets. Currently, the advancement of BUS image segmentation seems to meet its bottleneck. The improvement of the performance is increasingly challenging, and only few new approaches were published in the last several years. It is the time to look at the field by reviewing previous approaches comprehensively and to investigate the future directions. In this paper, we study the basic ideas, theories, pros and cons of the approaches, group them into categories, and extensively review each category in depth by discussing the principles, application issues, and advantages/disadvantages.|breast cancer one lead caus cancer death among women worldwid clinic routin automat breast ultrasound bus imag segment veri challeng essenti cancer diagnosi treatment plan mani bus segment approach studi last two decad prove effect privat dataset current advanc bus imag segment seem meet bottleneck improv perform increas challeng onli new approach publish last sever year time look field review previous approach comprehens investig futur direct paper studi basic idea theori pros con approach group categori extens review categori depth discuss principl applic issu advantag disadvantag|['Min Xian', 'Yingtao Zhang', 'H. D. Cheng', 'Fei Xu', 'Boyu Zhang', 'Jianrui Ding']|['cs.CV', 'cs.LG']
2017-04-07T11:30:30Z|2017-04-04T12:28:12Z|http://arxiv.org/abs/1704.00979v1|http://arxiv.org/pdf/1704.00979v1|Optic Disc and Cup Segmentation Methods for Glaucoma Detection with   Modification of U-Net Convolutional Neural Network|optic disc cup segment method glaucoma detect modif net convolut neural network|Glaucoma is the second leading cause of blindness all over the world, with approximately 60 million cases reported worldwide in 2010. If undiagnosed in time, glaucoma causes irreversible damage to the optic nerve leading to blindness. The optic nerve head examination, which involves measurement of cup-to-disc ratio, is considered one of the most valuable methods of structural diagnosis of the disease. Estimation of cup-to-disc ratio requires segmentation of optic disc and optic cup on eye fundus images and can be performed by modern computer vision algorithms. This work presents universal approach for automatic optic disc and cup segmentation, which is based on deep learning, namely, modification of U-Net convolutional neural network. Our experiments include comparison with the best known methods on publicly available databases DRIONS-DB, RIM-ONE v.3, DRISHTI-GS. For both optic disc and cup segmentation, our method achieves quality comparable to current state-of-the-art methods, outperforming them in terms of the prediction time.|glaucoma second lead caus blind world approxim million case report worldwid undiagnos time glaucoma caus irrevers damag optic nerv lead blind optic nerv head examin involv measur cup disc ratio consid one valuabl method structur diagnosi diseas estim cup disc ratio requir segment optic disc optic cup eye fundus imag perform modern comput vision algorithm work present univers approach automat optic disc cup segment base deep learn name modif net convolut neural network experi includ comparison best known method public avail databas drion db rim one drishti gs optic disc cup segment method achiev qualiti compar current state art method outperform term predict time|['Artem Sevastopolsky']|['cs.CV', 'stat.ML']
2017-04-07T11:30:30Z|2017-04-04T07:29:34Z|http://arxiv.org/abs/1704.01466v1|http://arxiv.org/pdf/1704.01466v1|A Unified Multi-Faceted Video Summarization System|unifi multi facet video summar system|This paper addresses automatic summarization and search in visual data comprising of videos, live streams and image collections in a unified manner. In particular, we propose a framework for multi-faceted summarization which extracts key-frames (image summaries), skims (video summaries) and entity summaries (summarization at the level of entities like objects, scenes, humans and faces in the video). The user can either view these as extractive summarization, or query focused summarization. Our approach first pre-processes the video or image collection once, to extract all important visual features, following which we provide an interactive mechanism to the user to summarize the video based on their choice. We investigate several diversity, coverage and representation models for all these problems, and argue the utility of these different mod- els depending on the application. While most of the prior work on submodular summarization approaches has focused on combining several models and learning weighted mixtures, we focus on the explain-ability of different the diversity, coverage and representation models and their scalability. Most importantly, we also show that we can summarize hours of video data in a few seconds, and our system allows the user to generate summaries of various lengths and types interactively on the fly.|paper address automat summar search visual data compris video live stream imag collect unifi manner particular propos framework multi facet summar extract key frame imag summari skim video summari entiti summari summar level entiti like object scene human face video user either view extract summar queri focus summar approach first pre process video imag collect onc extract import visual featur follow provid interact mechan user summar video base choic investig sever divers coverag represent model problem argu util differ mod el depend applic prior work submodular summar approach focus combin sever model learn weight mixtur focus explain abil differ divers coverag represent model scalabl import also show summar hour video data second system allow user generat summari various length type interact fli|['Anurag Sahoo', 'Vishal Kaushal', 'Khoshrav Doctor', 'Suyash Shetty', 'Rishabh Iyer', 'Ganesh Ramakrishnan']|['cs.CV', 'cs.DM']
2017-04-07T11:30:30Z|2017-04-04T06:28:30Z|http://arxiv.org/abs/1704.00887v1|http://arxiv.org/pdf/1704.00887v1|A Branch-and-Bound Algorithm for Checkerboard Extraction in Camera-Laser   Calibration|branch bound algorithm checkerboard extract camera laser calibr|We address the problem of camera-to-laser-scanner calibration using a checkerboard and multiple image-laser scan pairs. Distinguishing which laser points measure the checkerboard and which lie on the background is essential to any such system. We formulate the checkerboard extraction as a combinatorial optimization problem with a clear cut objective function. We propose a branch-and-bound technique that deterministically and globally optimizes the objective. Unlike what is available in the literature, the proposed method is not heuristic and does not require assumptions such as constraints on the background or relying on discontinuity of the range measurements to partition the data into line segments. The proposed approach is generic and can be applied to both 3D or 2D laser scanners as well as the cases where multiple checkerboards are present. We demonstrate the effectiveness of the proposed approach by providing numerical simulations as well as experimental results.|address problem camera laser scanner calibr use checkerboard multipl imag laser scan pair distinguish laser point measur checkerboard lie background essenti ani system formul checkerboard extract combinatori optim problem clear cut object function propos branch bound techniqu determinist global optim object unlik avail literatur propos method heurist doe requir assumpt constraint background reli discontinu rang measur partit data line segment propos approach generic appli laser scanner well case multipl checkerboard present demonstr effect propos approach provid numer simul well experiment result|['Alireza Khosravian', 'Tat-Jun Chin', 'Ian Reid']|['cs.RO', 'cs.CV']
2017-04-07T11:30:30Z|2017-04-04T03:04:30Z|http://arxiv.org/abs/1704.00860v1|http://arxiv.org/pdf/1704.00860v1|Simultaneous Feature Aggregating and Hashing for Large-scale Image   Search|simultan featur aggreg hash larg scale imag search|In most state-of-the-art hashing-based visual search systems, local image descriptors of an image are first aggregated as a single feature vector. This feature vector is then subjected to a hashing function that produces a binary hash code. In previous work, the aggregating and the hashing processes are designed independently. In this paper, we propose a novel framework where feature aggregating and hashing are designed simultaneously and optimized jointly. Specifically, our joint optimization produces aggregated representations that can be better reconstructed by some binary codes. This leads to more discriminative binary hash codes and improved retrieval accuracy. In addition, we also propose a fast version of the recently-proposed Binary Autoencoder to be used in our proposed framework. We perform extensive retrieval experiments on several benchmark datasets with both SIFT and convolutional features. Our results suggest that the proposed framework achieves significant improvements over the state of the art.|state art hash base visual search system local imag descriptor imag first aggreg singl featur vector featur vector subject hash function produc binari hash code previous work aggreg hash process design independ paper propos novel framework featur aggreg hash design simultan optim joint specif joint optim produc aggreg represent better reconstruct binari code lead discrimin binari hash code improv retriev accuraci addit also propos fast version recent propos binari autoencod use propos framework perform extens retriev experi sever benchmark dataset sift convolut featur result suggest propos framework achiev signific improv state art|['Thanh-Toan Do', 'Dang-Khoa Le Tan', 'Trung T. Pham', 'Ngai-Man Cheung']|['cs.CV']
2017-04-07T11:30:30Z|2017-04-04T01:46:46Z|http://arxiv.org/abs/1704.00848v1|http://arxiv.org/pdf/1704.00848v1|Guided Proofreading of Automatic Segmentations for Connectomics|guid proofread automat segment connectom|Automatic cell image segmentation methods in connectomics produce merge and split errors, which require correction through proofreading. Previous research has identified the visual search for these errors as the bottleneck in interactive proofreading. To aid error correction, we develop two classifiers that automatically recommend candidate merges and splits to the user. These classifiers use a convolutional neural network (CNN) that has been trained with errors in automatic segmentations against expert-labeled ground truth. Our classifiers detect potentially-erroneous regions by considering a large context region around a segmentation boundary. Corrections can then be performed by a user with yes/no decisions, which reduces variation of information 7.5x faster than previous proofreading methods. We also present a fully-automatic mode that uses a probability threshold to make merge/split decisions. Extensive experiments using the automatic approach and comparing performance of novice and expert users demonstrate that our method performs favorably against state-of-the-art proofreading methods on different connectomics datasets.|automat cell imag segment method connectom produc merg split error requir correct proofread previous research identifi visual search error bottleneck interact proofread aid error correct develop two classifi automat recommend candid merg split user classifi use convolut neural network cnn train error automat segment expert label ground truth classifi detect potenti erron region consid larg context region around segment boundari correct perform user yes decis reduc variat inform faster previous proofread method also present fulli automat mode use probabl threshold make merg split decis extens experi use automat approach compar perform novic expert user demonstr method perform favor state art proofread method differ connectom dataset|['Daniel Haehn', 'Verena Kaynig', 'James Tompkin', 'Jeff W. Lichtman', 'Hanspeter Pfister']|['cs.CV']
2017-04-07T11:30:30Z|2017-04-03T23:55:13Z|http://arxiv.org/abs/1704.00834v1|http://arxiv.org/pdf/1704.00834v1|Cascaded Segmentation-Detection Networks for Word-Level Text Spotting|cascad segment detect network word level text spot|"We introduce an algorithm for word-level text spotting that is able to accurately and reliably determine the bounding regions of individual words of text ""in the wild"". Our system is formed by the cascade of two convolutional neural networks. The first network is fully convolutional and is in charge of detecting areas containing text. This results in a very reliable but possibly inaccurate segmentation of the input image. The second network (inspired by the popular YOLO architecture) analyzes each segment produced in the first stage, and predicts oriented rectangular regions containing individual words. No post-processing (e.g. text line grouping) is necessary. With execution time of 450 ms for a 1000-by-560 image on a Titan X GPU, our system achieves the highest score to date among published algorithms on the ICDAR 2015 Incidental Scene Text dataset benchmark."|introduc algorithm word level text spot abl accur reliabl determin bound region individu word text wild system form cascad two convolut neural network first network fulli convolut charg detect area contain text result veri reliabl possibl inaccur segment input imag second network inspir popular yolo architectur analyz segment produc first stage predict orient rectangular region contain individu word post process text line group necessari execut time ms imag titan gpu system achiev highest score date among publish algorithm icdar incident scene text dataset benchmark|['Siyang Qin', 'Roberto Manduchi']|['cs.CV']
2017-04-07T11:30:30Z|2017-04-03T22:40:48Z|http://arxiv.org/abs/1704.00829v1|http://arxiv.org/pdf/1704.00829v1|Online deforestation detection|onlin deforest detect|Deforestation detection using satellite images can make an important contribution to forest management. Current approaches can be broadly divided into those that compare two images taken at similar periods of the year and those that monitor changes by using multiple images taken during the growing season. The CMFDA algorithm described in Zhu et al. (2012) is an algorithm that builds on the latter category by implementing a year-long, continuous, time-series based approach to monitoring images. This algorithm was developed for 30m resolution, 16-day frequency reflectance data from the Landsat satellite. In this work we adapt the algorithm to 1km, 16-day frequency reflectance data from the modis sensor aboard the Terra satellite. The CMFDA algorithm is composed of two submodels which are fitted on a pixel-by-pixel basis. The first estimates the amount of surface reflectance as a function of the day of the year. The second estimates the occurrence of a deforestation event by comparing the last few predicted and real reflectance values. For this comparison, the reflectance observations for six different bands are first combined into a forest index. Real and predicted values of the forest index are then compared and high absolute differences for consecutive observation dates are flagged as deforestation events. Our adapted algorithm also uses the two model framework. However, since the modis 13A2 dataset used, includes reflectance data for different spectral bands than those included in the Landsat dataset, we cannot construct the forest index. Instead we propose two contrasting approaches: a multivariate and an index approach similar to that of CMFDA.|deforest detect use satellit imag make import contribut forest manag current approach broad divid compar two imag taken similar period year monitor chang use multipl imag taken dure grow season cmfda algorithm describ zhu et al algorithm build latter categori implement year long continu time seri base approach monitor imag algorithm develop resolut day frequenc reflect data landsat satellit work adapt algorithm km day frequenc reflect data modi sensor aboard terra satellit cmfda algorithm compos two submodel fit pixel pixel basi first estim amount surfac reflect function day year second estim occurr deforest event compar last predict real reflect valu comparison reflect observ six differ band first combin forest index real predict valu forest index compar high absolut differ consecut observ date flag deforest event adapt algorithm also use two model framework howev sinc modi dataset use includ reflect data differ spectral band includ landsat dataset cannot construct forest index instead propos two contrast approach multivari index approach similar cmfda|['Emiliano Diaz']|['stat.AP', 'cs.CV']
2017-04-07T11:30:34Z|2017-04-03T18:57:42Z|http://arxiv.org/abs/1704.00763v1|http://arxiv.org/pdf/1704.00763v1|AMC: Attention guided Multi-modal Correlation Learning for Image Search|amc attent guid multi modal correl learn imag search|Given a user's query, traditional image search systems rank images according to its relevance to a single modality (e.g., image content or surrounding text). Nowadays, an increasing number of images on the Internet are available with associated meta data in rich modalities (e.g., titles, keywords, tags, etc.), which can be exploited for better similarity measure with queries. In this paper, we leverage visual and textual modalities for image search by learning their correlation with input query. According to the intent of query, attention mechanism can be introduced to adaptively balance the importance of different modalities. We propose a novel Attention guided Multi-modal Correlation (AMC) learning method which consists of a jointly learned hierarchy of intra and inter-attention networks. Conditioned on query's intent, intra-attention networks (i.e., visual intra-attention network and language intra-attention network) attend on informative parts within each modality; a multi-modal inter-attention network promotes the importance of the most query-relevant modalities. In experiments, we evaluate AMC models on the search logs from two real world image search engines and show a significant boost on the ranking of user-clicked images in search results. Additionally, we extend AMC models to caption ranking task on COCO dataset and achieve competitive results compared with recent state-of-the-arts.|given user queri tradit imag search system rank imag accord relev singl modal imag content surround text nowaday increas number imag internet avail associ meta data rich modal titl keyword tag etc exploit better similar measur queri paper leverag visual textual modal imag search learn correl input queri accord intent queri attent mechan introduc adapt balanc import differ modal propos novel attent guid multi modal correl amc learn method consist joint learn hierarchi intra inter attent network condit queri intent intra attent network visual intra attent network languag intra attent network attend inform part within modal multi modal inter attent network promot import queri relev modal experi evalu amc model search log two real world imag search engin show signific boost rank user click imag search result addit extend amc model caption rank task coco dataset achiev competit result compar recent state art|['Kan Chen', 'Trung Bui', 'Fang Chen', 'Zhaowen Wang', 'Ram Nevatia']|['cs.CV']
2017-04-07T11:30:34Z|2017-04-03T18:43:20Z|http://arxiv.org/abs/1704.00758v1|http://arxiv.org/pdf/1704.00758v1|Unsupervised Action Proposal Ranking through Proposal Recombination|unsupervis action propos rank propos recombin|Recently, action proposal methods have played an important role in action recognition tasks, as they reduce the search space dramatically. Most unsupervised action proposal methods tend to generate hundreds of action proposals which include many noisy, inconsistent, and unranked action proposals, while supervised action proposal methods take advantage of predefined object detectors (e.g., human detector) to refine and score the action proposals, but they require thousands of manual annotations to train.   Given the action proposals in a video, the goal of the proposed work is to generate a few better action proposals that are ranked properly. In our approach, we first divide action proposal into sub-proposal and then use Dynamic Programming based graph optimization scheme to select the optimal combinations of sub-proposals from different proposals and assign each new proposal a score. We propose a new unsupervised image-based actioness detector that leverages web images and employs it as one of the node scores in our graph formulation. Moreover, we capture motion information by estimating the number of motion contours within each action proposal patch. The proposed method is an unsupervised method that neither needs bounding box annotations nor video level labels, which is desirable with the current explosion of large-scale action datasets. Our approach is generic and does not depend on a specific action proposal method. We evaluate our approach on several publicly available trimmed and un-trimmed datasets and obtain better performance compared to several proposal ranking methods. In addition, we demonstrate that properly ranked proposals produce significantly better action detection as compared to state-of-the-art proposal based methods.|recent action propos method play import role action recognit task reduc search space dramat unsupervis action propos method tend generat hundr action propos includ mani noisi inconsist unrank action propos supervis action propos method take advantag predefin object detector human detector refin score action propos requir thousand manual annot train given action propos video goal propos work generat better action propos rank proper approach first divid action propos sub propos use dynam program base graph optim scheme select optim combin sub propos differ propos assign new propos score propos new unsupervis imag base actio detector leverag web imag employ one node score graph formul moreov captur motion inform estim number motion contour within action propos patch propos method unsupervis method neither need bound box annot video level label desir current explos larg scale action dataset approach generic doe depend specif action propos method evalu approach sever public avail trim un trim dataset obtain better perform compar sever propos rank method addit demonstr proper rank propos produc signific better action detect compar state art propos base method|['Waqas Sultani', 'Dong Zhang', 'Mubarak Shah']|['cs.CV']
2017-04-07T11:30:34Z|2017-04-03T17:58:07Z|http://arxiv.org/abs/1704.00717v1|http://arxiv.org/pdf/1704.00717v1|It Takes Two to Tango: Towards Theory of AI's Mind|take two tango toward theori ai mind|Theory of Mind is the ability to attribute mental states (beliefs, intents, knowledge, perspectives, etc.) to others and recognize that these mental states may differ from one's own. Theory of Mind is critical to effective communication and to teams demonstrating higher collective performance. To effectively leverage the progress in Artificial Intelligence (AI) to make our lives more productive, it is important for humans and AI to work well together in a team. Traditionally, there has been much emphasis on research to make AI more accurate, and (to a lesser extent) on having it better understand human intentions, tendencies, beliefs, and contexts. The latter involves making AI more human-like and having it develop a theory of our minds.   In this work, we argue that for human-AI teams to be effective, humans must also develop a theory of AI's mind - get to know its strengths, weaknesses, beliefs, and quirks. We instantiate these ideas within the domain of Visual Question Answering (VQA). We find that using just a few examples(50), lay people can be trained to better predict responses and oncoming failures of a complex VQA model. Surprisingly, we find that having access to the model's internal states - its confidence in its top-k predictions, explicit or implicit attention maps which highlight regions in the image (and words in the question) the model is looking at (and listening to) while answering a question about an image - do not help people better predict its behavior|theori mind abil attribut mental state belief intent knowledg perspect etc recogn mental state may differ one theori mind critic effect communic team demonstr higher collect perform effect leverag progress artifici intellig ai make live product import human ai work well togeth team tradit much emphasi research make ai accur lesser extent better understand human intent tendenc belief context latter involv make ai human like develop theori mind work argu human ai team effect human must also develop theori ai mind get know strength weak belief quirk instanti idea within domain visual question answer vqa find use exampl lay peopl train better predict respons oncom failur complex vqa model surpris find access model intern state confid top predict explicit implicit attent map highlight region imag word question model look listen answer question imag help peopl better predict behavior|['Arjun Chandrasekaran', 'Deshraj Yadav', 'Prithvijit Chattopadhyay', 'Viraj Prabhu', 'Devi Parikh']|['cs.CV', 'cs.AI', 'cs.CL']
2017-04-07T11:30:34Z|2017-04-03T17:52:51Z|http://arxiv.org/abs/1704.00710v1|http://arxiv.org/pdf/1704.00710v1|Hierarchical Surface Prediction for 3D Object Reconstruction|hierarch surfac predict object reconstruct|Recently, Convolutional Neural Networks have shown promising results for 3D geometry prediction. They can make predictions from very little input data such as for example a single color image, depth map or a partial 3D volume. A major limitation of such approaches is that they only predict a coarse resolution voxel grid, which does not capture the surface of the objects well. We propose a general framework, called hierarchical surface prediction (HSP), which facilitates prediction of high resolution voxel grids. The main insight is that it is sufficient to predict high resolution voxels around the predicted surfaces. The exterior and interior of the objects can be represented with coarse resolution voxels. This allows us to predict significantly higher resolution voxel grids around the surface, from which triangle meshes can be extracted. Our approach is general and not dependent on a specific input type. In our experiments, we show results for geometry prediction from color images, depth images and shape completion from partial voxel grids. Our analysis shows that the network is able to predict the surface more accurately than a low resolution prediction.|recent convolut neural network shown promis result geometri predict make predict veri littl input data exampl singl color imag depth map partial volum major limit approach onli predict coars resolut voxel grid doe captur surfac object well propos general framework call hierarch surfac predict hsp facilit predict high resolut voxel grid main insight suffici predict high resolut voxel around predict surfac exterior interior object repres coars resolut voxel allow us predict signific higher resolut voxel grid around surfac triangl mesh extract approach general depend specif input type experi show result geometri predict color imag depth imag shape complet partial voxel grid analysi show network abl predict surfac accur low resolut predict|['Christian H√§ne', 'Shubham Tulsiani', 'Jitendra Malik']|['cs.CV']
2017-04-07T11:30:34Z|2017-04-03T17:45:10Z|http://arxiv.org/abs/1704.00705v1|http://arxiv.org/pdf/1704.00705v1|Graph Partitioning with Acyclicity Constraints|graph partit acycl constraint|Graphs are widely used to model execution dependencies in applications. In particular, the NP-complete problem of partitioning a graph under constraints receives enormous attention by researchers because of its applicability in multiprocessor scheduling. We identified the additional constraint of acyclic dependencies between blocks when mapping computer vision and imaging applications to a heterogeneous embedded multiprocessor. Existing algorithms and heuristics do not address this requirement and deliver results that are not applicable for our use-case. In this work, we show that this more constrained version of the graph partitioning problem is NP-complete and present heuristics that achieve a close approximation of the optimal solution found by an exhaustive search for small problem instances and much better scalability for larger instances. In addition, we can show a positive impact on the schedule of a real imaging application that improves communication volume and execution time.|graph wide use model execut depend applic particular np complet problem partit graph constraint receiv enorm attent research becaus applic multiprocessor schedul identifi addit constraint acycl depend block map comput vision imag applic heterogen embed multiprocessor exist algorithm heurist address requir deliv result applic use case work show constrain version graph partit problem np complet present heurist achiev close approxim optim solut found exhaust search small problem instanc much better scalabl larger instanc addit show posit impact schedul real imag applic improv communic volum execut time|['Orlando Moreira', 'Merten Popp', 'Christian Schulz']|['cs.DS', 'cs.CV', 'cs.DC']
2017-04-07T11:30:34Z|2017-04-03T16:44:46Z|http://arxiv.org/abs/1704.00675v1|http://arxiv.org/pdf/1704.00675v1|The 2017 DAVIS Challenge on Video Object Segmentation|davi challeng video object segment|We present the 2017 DAVIS Challenge, a public competition specifically designed for the task of video object segmentation. Following the footsteps of other successful initiatives, such as ILSVRC and PASCAL VOC, which established the avenue of research in the fields of scene classification and semantic segmentation, the DAVIS Challenge comprises a dataset, an evaluation methodology, and a public competition with a dedicated workshop co-located with CVPR 2017. The DAVIS Challenge follows up on the recent publication of DAVIS (Densely-Annotated VIdeo Segmentation), which has fostered the development of several novel state-of-the-art video object segmentation techniques. In this paper we describe the scope of the benchmark, highlight the main characteristics of the dataset and define the evaluation metrics of the competition.|present davi challeng public competit specif design task video object segment follow footstep success initi ilsvrc pascal voc establish avenu research field scene classif semant segment davi challeng compris dataset evalu methodolog public competit dedic workshop co locat cvpr davi challeng follow recent public davi dens annot video segment foster develop sever novel state art video object segment techniqu paper describ scope benchmark highlight main characterist dataset defin evalu metric competit|['Jordi Pont-Tuset', 'Federico Perazzi', 'Sergi Caelles', 'Pablo Arbel√°ez', 'Alex Sorkine-Hornung', 'Luc Van Gool']|['cs.CV']
2017-04-07T11:30:34Z|2017-04-03T15:39:56Z|http://arxiv.org/abs/1704.00648v1|http://arxiv.org/pdf/1704.00648v1|Soft-to-Hard Vector Quantization for End-to-End Learned Compression of   Images and Neural Networks|soft hard vector quantize end end learn compress imag neural network|In this work we present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives state-of-the-art results for both.|work present new approach learn compress represent deep architectur end end train strategi method base soft relax quantize entropi anneal discret counterpart throughout train showcas method two challeng applic imag compress neural network compress task typic approach differ method soft hard quantize approach give state art result|['Eirikur Agustsson', 'Fabian Mentzer', 'Michael Tschannen', 'Lukas Cavigelli', 'Radu Timofte', 'Luca Benini', 'Luc Van Gool']|['cs.LG', 'cs.CV']
2017-04-07T11:30:34Z|2017-04-03T15:34:11Z|http://arxiv.org/abs/1704.00642v1|http://arxiv.org/pdf/1704.00642v1|Local nearest neighbour classification with applications to   semi-supervised learning|local nearest neighbour classif applic semi supervis learn|We derive a new asymptotic expansion for the global excess risk of a local $k$-nearest neighbour classifier, where the choice of $k$ may depend upon the test point. This expansion elucidates conditions under which the dominant contribution to the excess risk comes from the locus of points at which each class label is equally likely to occur, but we also show that if these conditions are not satisfied, the dominant contribution may arise from the tails of the marginal distribution of the features. Moreover, we prove that, provided the $d$-dimensional marginal distribution of the features has a finite $\rho$th moment for some $\rho > 4$ (as well as other regularity conditions), a local choice of $k$ can yield a rate of convergence of the excess risk of $O(n^{-4/(d+4)})$, where $n$ is the sample size, whereas for the standard $k$-nearest neighbour classifier, our theory would require $d \geq 5$ and $\rho > 4d/(d-4)$ finite moments to achieve this rate. Our results motivate a new $k$-nearest neighbour classifier for semi-supervised learning problems, where the unlabelled data are used to obtain an estimate of the marginal feature density, and fewer neighbours are used for classification when this density estimate is small. The potential improvements over the standard $k$-nearest neighbour classifier are illustrated both through our theory and via a simulation study.|deriv new asymptot expans global excess risk local nearest neighbour classifi choic may depend upon test point expans elucid condit domin contribut excess risk come locus point class label equal like occur also show condit satisfi domin contribut may aris tail margin distribut featur moreov prove provid dimension margin distribut featur finit rho th moment rho well regular condit local choic yield rate converg excess risk sampl size wherea standard nearest neighbour classifi theori would requir geq rho finit moment achiev rate result motiv new nearest neighbour classifi semi supervis learn problem unlabel data use obtain estim margin featur densiti fewer neighbour use classif densiti estim small potenti improv standard nearest neighbour classifi illustr theori via simul studi|['Timothy I. Cannings', 'Thomas B. Berrett', 'Richard J. Samworth']|['math.ST', 'cs.CV', 'cs.LG', 'stat.ME', 'stat.TH', '62G20']
2017-04-07T11:30:34Z|2017-04-03T14:29:40Z|http://arxiv.org/abs/1704.00616v1|http://arxiv.org/pdf/1704.00616v1|Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance   for Action Classification and Detection|chain multi stream network exploit pose motion appear action classif detect|General human action recognition requires understanding of various visual cues. In this paper, we propose a network architecture that computes and integrates the most important visual cues for action recognition: pose, motion, and the raw images. For the integration, we introduce a Markov chain model which adds cues successively. The resulting approach is efficient and applicable to action classification as well as to spatial and temporal action localization. The two contributions clearly improve the performance over respective baselines. The overall approach achieves state-of-the-art action classification performance on HMDB51, J-HMDB and NTU RGB+D datasets. Moreover, it yields state-of-the-art spatio-temporal action localization results on UCF101 and J-HMDB.|general human action recognit requir understand various visual cue paper propos network architectur comput integr import visual cue action recognit pose motion raw imag integr introduc markov chain model add cue success result approach effici applic action classif well spatial tempor action local two contribut clear improv perform respect baselin overal approach achiev state art action classif perform hmdb hmdb ntu rgb dataset moreov yield state art spatio tempor action local result ucf hmdb|['Mohammadreza Zolfaghari', 'Gabriel L. Oliveira', 'Nima Sedaghat', 'Thomas Brox']|['cs.CV', 'cs.AI', 'cs.HC', 'cs.MM', 'cs.NE']
2017-04-07T11:30:34Z|2017-04-03T13:21:38Z|http://arxiv.org/abs/1704.00570v1|http://arxiv.org/pdf/1704.00570v1|Spatiotemporal Networks for Video Emotion Recognition|spatiotempor network video emot recognit|Our article presents an audio-visual based multi-modal emotion classification system. Considering the fact of deep learning approaches to facial analysis have recently demonstrated high performance, in our work, we use convolutional neural networks (CNNs) for emotion recognition in video, relying on temporal averaging and pooling operations reminiscent of widely used approaches for the spatial aggregation of information. In respect of time sequence, we extract the feature from audio clips in the video and use RNN to propagate information. In this work, we focus our presentation and experimental analysis on a fusion CNN-RNN architecture for facial expression analysis.|articl present audio visual base multi modal emot classif system consid fact deep learn approach facial analysi recent demonstr high perform work use convolut neural network cnns emot recognit video reli tempor averag pool oper reminisc wide use approach spatial aggreg inform respect time sequenc extract featur audio clip video use rnn propag inform work focus present experiment analysi fusion cnn rnn architectur facial express analysi|['Lijie Fan', 'Yunjie Ke']|['cs.CV']
2017-04-07T11:30:38Z|2017-04-03T11:19:03Z|http://arxiv.org/abs/1704.00529v1|http://arxiv.org/abs/1704.00529v1|3D Object Reconstruction from Hand-Object Interactions|object reconstruct hand object interact|Recent advances have enabled 3d object reconstruction approaches using a single off-the-shelf RGB-D camera. Although these approaches are successful for a wide range of object classes, they rely on stable and distinctive geometric or texture features. Many objects like mechanical parts, toys, household or decorative articles, however, are textureless and characterized by minimalistic shapes that are simple and symmetric. Existing in-hand scanning systems and 3d reconstruction techniques fail for such symmetric objects in the absence of highly distinctive features. In this work, we show that extracting 3d hand motion for in-hand scanning effectively facilitates the reconstruction of even featureless and highly symmetric objects and we present an approach that fuses the rich additional information of hands into a 3d reconstruction pipeline, significantly contributing to the state-of-the-art of in-hand scanning.|recent advanc enabl object reconstruct approach use singl shelf rgb camera although approach success wide rang object class reli stabl distinct geometr textur featur mani object like mechan part toy household decor articl howev textureless character minimalist shape simpl symmetr exist hand scan system reconstruct techniqu fail symmetr object absenc high distinct featur work show extract hand motion hand scan effect facilit reconstruct even featureless high symmetr object present approach fuse rich addit inform hand reconstruct pipelin signific contribut state art hand scan|['Dimitrios Tzionas', 'Juergen Gall']|['cs.CV']
2017-04-07T11:30:38Z|2017-04-03T11:01:47Z|http://arxiv.org/abs/1704.00524v1|http://arxiv.org/pdf/1704.00524v1|Block-Matching Convolutional Neural Network for Image Denoising|block match convolut neural network imag denois|There are two main streams in up-to-date image denoising algorithms: non-local self similarity (NSS) prior based methods and convolutional neural network (CNN) based methods. The NSS based methods are favorable on images with regular and repetitive patterns while the CNN based methods perform better on irregular structures. In this paper, we propose a block-matching convolutional neural network (BMCNN) method that combines NSS prior and CNN. Initially, similar local patches in the input image are integrated into a 3D block. In order to prevent the noise from messing up the block matching, we first apply an existing denoising algorithm on the noisy image. The denoised image is employed as a pilot signal for the block matching, and then denoising function for the block is learned by a CNN structure. Experimental results show that the proposed BMCNN algorithm achieves state-of-the-art performance. In detail, BMCNN can restore both repetitive and irregular structures.|two main stream date imag denois algorithm non local self similar nss prior base method convolut neural network cnn base method nss base method favor imag regular repetit pattern cnn base method perform better irregular structur paper propos block match convolut neural network bmcnn method combin nss prior cnn initi similar local patch input imag integr block order prevent nois mess block match first appli exist denois algorithm noisi imag denois imag employ pilot signal block match denois function block learn cnn structur experiment result show propos bmcnn algorithm achiev state art perform detail bmcnn restor repetit irregular structur|['Byeongyong Ahn', 'Nam Ik Cho']|['cs.CV']
2017-04-07T11:30:38Z|2017-04-03T10:26:45Z|http://arxiv.org/abs/1704.00515v1|http://arxiv.org/abs/1704.00515v1|Capturing Hand Motion with an RGB-D Sensor, Fusing a Generative Model   with Salient Points|captur hand motion rgb sensor fuse generat model salient point|Hand motion capture has been an active research topic in recent years, following the success of full-body pose tracking. Despite similarities, hand tracking proves to be more challenging, characterized by a higher dimensionality, severe occlusions and self-similarity between fingers. For this reason, most approaches rely on strong assumptions, like hands in isolation or expensive multi-camera systems, that limit the practical use. In this work, we propose a framework for hand tracking that can capture the motion of two interacting hands using only a single, inexpensive RGB-D camera. Our approach combines a generative model with collision detection and discriminatively learned salient points. We quantitatively evaluate our approach on 14 new sequences with challenging interactions.|hand motion captur activ research topic recent year follow success full bodi pose track despit similar hand track prove challeng character higher dimension sever occlus self similar finger reason approach reli strong assumpt like hand isol expens multi camera system limit practic use work propos framework hand track captur motion two interact hand use onli singl inexpens rgb camera approach combin generat model collis detect discrimin learn salient point quantit evalu approach new sequenc challeng interact|['Dimitrios Tzionas', 'Abhilash Srikantha', 'Pablo Aponte', 'Juergen Gall']|['cs.CV']
2017-04-07T11:30:38Z|2017-04-03T10:11:10Z|http://arxiv.org/abs/1704.00509v1|http://arxiv.org/pdf/1704.00509v1|Truncating Wide Networks using Binary Tree Architectures|truncat wide network use binari tree architectur|Recent study shows that a wide deep network can obtain accuracy comparable to a deeper but narrower network. Compared to narrower and deeper networks, wide networks employ relatively less number of layers and have various important benefits, such that they have less running time on parallel computing devices, and they are less affected by gradient vanishing problems. However, the parameter size of a wide network can be very large due to use of large width of each layer in the network. In order to keep the benefits of wide networks meanwhile improve the parameter size and accuracy trade-off of wide networks, we propose a binary tree architecture to truncate architecture of wide networks by reducing the width of the networks. More precisely, in the proposed architecture, the width is continuously reduced from lower layers to higher layers in order to increase the expressive capacity of network with a less increase on parameter size. Also, to ease the gradient vanishing problem, features obtained at different layers are concatenated to form the output of our architecture. By employing the proposed architecture on a baseline wide network, we can construct and train a new network with same depth but considerably less number of parameters. In our experimental analyses, we observe that the proposed architecture enables us to obtain better parameter size and accuracy trade-off compared to baseline networks using various benchmark image classification datasets. The results show that our model can decrease the classification error of baseline from 20.43% to 19.22% on Cifar-100 using only 28% of parameters that baseline has. Code is available at https://github.com/ZhangVision/bitnet.|recent studi show wide deep network obtain accuraci compar deeper narrow network compar narrow deeper network wide network employ relat less number layer various import benefit less run time parallel comput devic less affect gradient vanish problem howev paramet size wide network veri larg due use larg width layer network order keep benefit wide network meanwhil improv paramet size accuraci trade wide network propos binari tree architectur truncat architectur wide network reduc width network precis propos architectur width continu reduc lower layer higher layer order increas express capac network less increas paramet size also eas gradient vanish problem featur obtain differ layer concaten form output architectur employ propos architectur baselin wide network construct train new network depth consider less number paramet experiment analys observ propos architectur enabl us obtain better paramet size accuraci trade compar baselin network use various benchmark imag classif dataset result show model decreas classif error baselin cifar use onli paramet baselin code avail https github com zhangvis bitnet|['Yan Zhang', 'Mete Ozay', 'Shuohao Li', 'Takayuki Okatani']|['cs.CV']
2017-04-07T11:30:38Z|2017-04-03T09:40:56Z|http://arxiv.org/abs/1704.00498v1|http://arxiv.org/pdf/1704.00498v1|Convolutional neural networks for segmentation and object detection of   human semen|convolut neural network segment object detect human semen|We compare a set of convolutional neural network (CNN) architectures for the task of segmenting and detecting human sperm cells in an image taken from a semen sample. In contrast to previous work, samples are not stained or washed to allow for full sperm quality analysis, making analysis harder due to clutter. Our results indicate that training on full images is superior to training on patches when class-skew is properly handled. Full image training including up-sampling during training proves to be beneficial in deep CNNs for pixel wise accuracy and detection performance. Predicted sperm cells are found by using connected components on the CNN predictions. We investigate optimization of a threshold parameter on the size of detected components. Our best network achieves 93.87% precision and 91.89% recall on our test dataset after thresholding outperforming a classical mage analysis approach.|compar set convolut neural network cnn architectur task segment detect human sperm cell imag taken semen sampl contrast previous work sampl stain wash allow full sperm qualiti analysi make analysi harder due clutter result indic train full imag superior train patch class skew proper handl full imag train includ sampl dure train prove benefici deep cnns pixel wise accuraci detect perform predict sperm cell found use connect compon cnn predict investig optim threshold paramet size detect compon best network achiev precis recal test dataset threshold outperform classic mage analysi approach|['Malte St√¶r Nissen', 'Oswin Krause', 'Kristian Almstrup', 'S√∏ren Kj√¶rulff', 'Torben Trindk√¶r Nielsen', 'Mads Nielsen']|['cs.CV']
2017-04-07T11:30:38Z|2017-04-03T09:31:01Z|http://arxiv.org/abs/1704.00492v1|http://arxiv.org/abs/1704.00492v1|A Comparison of Directional Distances for Hand Pose Estimation|comparison direct distanc hand pose estim|Benchmarking methods for 3d hand tracking is still an open problem due to the difficulty of acquiring ground truth data. We introduce a new dataset and benchmarking protocol that is insensitive to the accumulative error of other protocols. To this end, we create testing frame pairs of increasing difficulty and measure the pose estimation error separately for each of them. This approach gives new insights and allows to accurately study the performance of each feature or method without employing a full tracking pipeline. Following this protocol, we evaluate various directional distances in the context of silhouette-based 3d hand tracking, expressed as special cases of a generalized Chamfer distance form. An appropriate parameter setup is proposed for each of them, and a comparative study reveals the best performing method in this context.|benchmark method hand track still open problem due difficulti acquir ground truth data introduc new dataset benchmark protocol insensit accumul error protocol end creat test frame pair increas difficulti measur pose estim error separ approach give new insight allow accur studi perform featur method without employ full track pipelin follow protocol evalu various direct distanc context silhouett base hand track express special case general chamfer distanc form appropri paramet setup propos compar studi reveal best perform method context|['Dimitrios Tzionas', 'Juergen Gall']|['cs.CV']
2017-04-07T11:30:38Z|2017-04-03T07:23:37Z|http://arxiv.org/abs/1704.00454v1|http://arxiv.org/pdf/1704.00454v1|Clustering in Hilbert simplex geometry|cluster hilbert simplex geometri|Clustering categorical distributions in the probability simplex is a fundamental primitive often met in applications dealing with histograms or mixtures of multinomials. Traditionally, the differential-geometric structure of the probability simplex has been used either by (i) setting the Riemannian metric tensor to the Fisher information matrix of the categorical distributions, or (ii) defining the information-geometric structure induced by a smooth dissimilarity measure, called a divergence. In this paper, we introduce a novel computationally-friendly non-Riemannian framework for modeling the probability simplex: Hilbert simplex geometry. We discuss the pros and cons of those three statistical modelings, and compare them experimentally for clustering tasks.|cluster categor distribut probabl simplex fundament primit often met applic deal histogram mixtur multinomi tradit differenti geometr structur probabl simplex use either set riemannian metric tensor fisher inform matrix categor distribut ii defin inform geometr structur induc smooth dissimilar measur call diverg paper introduc novel comput friend non riemannian framework model probabl simplex hilbert simplex geometri discuss pros con three statist model compar experiment cluster task|['Frank Nielsen', 'Ke Sun']|['cs.LG', 'cs.CV']
2017-04-07T11:30:38Z|2017-04-03T06:49:46Z|http://arxiv.org/abs/1704.00447v1|http://arxiv.org/pdf/1704.00447v1|Learning a Variational Network for Reconstruction of Accelerated MRI   Data|learn variat network reconstruct acceler mri data|Purpose: To allow fast and high-quality reconstruction of clinical accelerated multi-coil MR data by learning a variational network that combines the mathematical structure of variational models with deep learning.   Theory and Methods: Generalized compressed sensing reconstruction formulated as a variational model is embedded in an unrolled gradient descent scheme. All parameters of this formulation, including the prior model defined by filter kernels and activation functions as well as the data term weights, are learned during an offline training procedure. The learned model can then be applied online to previously unseen data.   Results: The variational network approach is evaluated on a clinical knee imaging protocol. The variational network reconstructions outperform standard reconstruction algorithms in terms of image quality and residual artifacts for all tested acceleration factors and sampling patterns.   Conclusion: Variational network reconstructions preserve the natural appearance of MR images as well as pathologies that were not included in the training data set. Due to its high computational performance, i.e., reconstruction time of 193 ms on a single graphics card, and the omission of parameter tuning once the network is trained, this new approach to image reconstruction can easily be integrated into clinical workflow.|purpos allow fast high qualiti reconstruct clinic acceler multi coil mr data learn variat network combin mathemat structur variat model deep learn theori method general compress sens reconstruct formul variat model embed unrol gradient descent scheme paramet formul includ prior model defin filter kernel activ function well data term weight learn dure offlin train procedur learn model appli onlin previous unseen data result variat network approach evalu clinic knee imag protocol variat network reconstruct outperform standard reconstruct algorithm term imag qualiti residu artifact test acceler factor sampl pattern conclus variat network reconstruct preserv natur appear mr imag well patholog includ train data set due high comput perform reconstruct time ms singl graphic card omiss paramet tune onc network train new approach imag reconstruct easili integr clinic workflow|['Kerstin Hammernik', 'Teresa Klatzer', 'Erich Kobler', 'Michael P Recht', 'Daniel K Sodickson', 'Thomas Pock', 'Florian Knoll']|['cs.CV']
2017-04-07T11:30:38Z|2017-04-03T06:11:43Z|http://arxiv.org/abs/1704.00438v1|http://arxiv.org/pdf/1704.00438v1|A Good Practice Towards Top Performance of Face Recognition: Transferred   Deep Feature Fusion|good practic toward top perform face recognit transfer deep featur fusion|Unconstrained face recognition performance evaluations have traditionally focused on Labeled Faces in the Wild (LFW) dataset for imagery and the YouTubeFaces (YTF) dataset for videos in the last couple of years. Spectacular progress in this field has resulted in a saturation on verification and identification accuracies for those benchmark datasets. In this paper, we propose a unified learning framework named transferred deep feature fusion targeting at the new IARPA Janus Bechmark A (IJB-A) face recognition dataset released by NIST face challenge. The IJB-A dataset includes real-world unconstrained faces from 500 subjects with full pose and illumination variations which are much harder than the LFW and YTF datasets. Inspired by transfer learning, we train two advanced deep convolutional neural networks (DCNN) with two different large datasets in source domain, respectively. By exploring the complementarity of two distinct DCNNs, deep feature fusion is utilized after feature extraction in target domain. Then, template specific linear SVMs is adopted to enhance the discrimination of framework. Finally, multiple matching scores corresponding different templates are merged as the final results. This simple unified framework outperforms the state-of-the-art by a wide margin on IJB-A dataset. Based on the proposed approach, we have submitted our IJB-A results to National Institute of Standards and Technology (NIST) for official evaluation.|unconstrain face recognit perform evalu tradit focus label face wild lfw dataset imageri youtubefac ytf dataset video last coupl year spectacular progress field result satur verif identif accuraci benchmark dataset paper propos unifi learn framework name transfer deep featur fusion target new iarpa janus bechmark ijb face recognit dataset releas nist face challeng ijb dataset includ real world unconstrain face subject full pose illumin variat much harder lfw ytf dataset inspir transfer learn train two advanc deep convolut neural network dcnn two differ larg dataset sourc domain respect explor complementar two distinct dcnns deep featur fusion util featur extract target domain templat specif linear svms adopt enhanc discrimin framework final multipl match score correspond differ templat merg final result simpl unifi framework outperform state art wide margin ijb dataset base propos approach submit ijb result nation institut standard technolog nist offici evalu|['Lin Xiong', 'Jayashree Karlekar', 'Jian Zhao', 'Jiashi Feng', 'Sugiri Pranata', 'Shengmei Shen']|['cs.CV']
2017-04-07T11:30:38Z|2017-04-03T02:12:28Z|http://arxiv.org/abs/1704.00406v1|http://arxiv.org/pdf/1704.00406v1|Sparse Autoencoder for Unsupervised Nucleus Detection and Representation   in Histopathology Images|spars autoencod unsupervis nucleus detect represent histopatholog imag|Histopathology images are crucial to the study of complex diseases such as cancer. The histologic characteristics of nuclei play a key role in disease diagnosis, prognosis and analysis. In this work, we propose a sparse Convolutional Autoencoder (CAE) for fully unsupervised, simultaneous nucleus detection and feature extraction in histopathology tissue images. Our CAE detects and encodes nuclei in image patches in tissue images into sparse feature maps that encode both the location and appearance of nuclei. Our CAE is the first unsupervised detection network for computer vision applications. The pretrained nucleus detection and feature extraction modules in our CAE can be fine-tuned for supervised learning in an end-to-end fashion. We evaluate our method on four datasets and reduce the errors of state-of-the-art methods up to 42%. We are able to achieve comparable performance with only 5% of the fully-supervised annotation cost.|histopatholog imag crucial studi complex diseas cancer histolog characterist nuclei play key role diseas diagnosi prognosi analysi work propos spars convolut autoencod cae fulli unsupervis simultan nucleus detect featur extract histopatholog tissu imag cae detect encod nuclei imag patch tissu imag spars featur map encod locat appear nuclei cae first unsupervis detect network comput vision applic pretrain nucleus detect featur extract modul cae fine tune supervis learn end end fashion evalu method four dataset reduc error state art method abl achiev compar perform onli fulli supervis annot cost|['Le Hou', 'Vu Nguyen', 'Dimitris Samaras', 'Tahsin M. Kurc', 'Yi Gao', 'Tianhao Zhao', 'Joel H. Saltz']|['cs.CV']
2017-04-07T11:30:43Z|2017-04-02T23:58:22Z|http://arxiv.org/abs/1704.00390v1|http://arxiv.org/pdf/1704.00390v1|Geometric loss functions for camera pose regression with deep learning|geometr loss function camera pose regress deep learn|Deep learning has shown to be effective for robust and real-time monocular image relocalisation. In particular, PoseNet is a deep convolutional neural network which learns to regress the 6-DOF camera pose from a single image. It learns to localize using high level features and is robust to difficult lighting, motion blur and unknown camera intrinsics, where point based SIFT registration fails. However, it was trained using a naive loss function, with hyper-parameters which require expensive tuning. In this paper, we give the problem a more fundamental theoretical treatment. We explore a number of novel loss functions for learning camera pose which are based on geometry and scene reprojection error. Additionally we show how to automatically learn an optimal weighting to simultaneously regress position and orientation. By leveraging geometry, we demonstrate that our technique significantly improves PoseNet's performance across datasets ranging from indoor rooms to a small city.|deep learn shown effect robust real time monocular imag relocalis particular posenet deep convolut neural network learn regress dof camera pose singl imag learn local use high level featur robust difficult light motion blur unknown camera intrins point base sift registr fail howev train use naiv loss function hyper paramet requir expens tune paper give problem fundament theoret treatment explor number novel loss function learn camera pose base geometri scene reproject error addit show automat learn optim weight simultan regress posit orient leverag geometri demonstr techniqu signific improv posenet perform across dataset rang indoor room small citi|['Alex Kendall', 'Roberto Cipolla']|['cs.CV']
2017-04-07T11:30:43Z|2017-04-02T23:39:51Z|http://arxiv.org/abs/1704.00389v1|http://arxiv.org/pdf/1704.00389v1|Hidden Two-Stream Convolutional Networks for Action Recognition|hidden two stream convolut network action recognit|Analyzing videos of human actions involves understanding the temporal relationships among video frames. CNNs are the current state-of-the-art methods for action recognition in videos. However, the CNN architectures currently being used have difficulty in capturing these relationships. State-of-the-art action recognition approaches rely on traditional local optical flow estimation methods to pre-compute the motion information for CNNs. Such a two-stage approach is computationally expensive, storage demanding, and not end-to-end trainable. In this paper, we present a novel CNN architecture that implicitly captures motion information. Our method is 10x faster than a two-stage approach, does not need to cache flow information, and is end-to-end trainable. Experimental results on UCF101 and HMDB51 show that it achieves competitive accuracy with the two-stage approaches.|analyz video human action involv understand tempor relationship among video frame cnns current state art method action recognit video howev cnn architectur current use difficulti captur relationship state art action recognit approach reli tradit local optic flow estim method pre comput motion inform cnns two stage approach comput expens storag demand end end trainabl paper present novel cnn architectur implicit captur motion inform method faster two stage approach doe need cach flow inform end end trainabl experiment result ucf hmdb show achiev competit accuraci two stage approach|['Yi Zhu', 'Zhenzhong Lan', 'Shawn Newsam', 'Alexander G. Hauptmann']|['cs.CV', 'cs.LG', 'cs.MM']
2017-04-07T11:30:43Z|2017-04-02T17:56:47Z|http://arxiv.org/abs/1704.00337v1|http://arxiv.org/pdf/1704.00337v1|Dense Multi-view 3D-reconstruction Without Dense Correspondences|dens multi view reconstruct without dens correspond|We introduce a variational method for multi-view shape-from-shading under natural illumination. The key idea is to couple PDE-based solutions for single-image based shape-from-shading problems across multiple images and multiple color channels by means of a variational formulation. Rather than alternatingly solving the individual SFS problems and optimizing the consistency across images and channels which is known to lead to suboptimal results, we propose an efficient solution of the coupled problem by means of an ADMM algorithm. In numerous experiments on both simulated and real imagery, we demonstrate that the proposed fusion of multiple-view reconstruction and shape-from-shading provides highly accurate dense reconstructions without the need to compute dense correspondences. With the proposed variational integration across multiple views shape-from-shading techniques become applicable to challenging real-world reconstruction problems, giving rise to highly detailed geometry even in areas of smooth brightness variation and lacking texture.|introduc variat method multi view shape shade natur illumin key idea coupl pde base solut singl imag base shape shade problem across multipl imag multipl color channel mean variat formul rather altern solv individu sfs problem optim consist across imag channel known lead suboptim result propos effici solut coupl problem mean admm algorithm numer experi simul real imageri demonstr propos fusion multipl view reconstruct shape shade provid high accur dens reconstruct without need comput dens correspond propos variat integr across multipl view shape shade techniqu becom applic challeng real world reconstruct problem give rise high detail geometri even area smooth bright variat lack textur|['Yvain Qu√©au', 'Jean M√©lou', 'Jean-Denis Durou', 'Daniel Cremers']|['cs.CV']
2017-04-07T11:30:43Z|2017-04-02T17:19:02Z|http://arxiv.org/abs/1704.00331v1|http://arxiv.org/pdf/1704.00331v1|Restoration of Images with Wavefront Aberrations|restor imag wavefront aberr|This contribution deals with image restoration in optical systems with coherent illumination, which is an important topic in astronomy, coherent microscopy and radar imaging. Such optical systems suffer from wavefront distortions, which are caused by imperfect imaging components and conditions. Known image restoration algorithms work well for incoherent imaging, they fail in case of coherent images. In this paper a novel wavefront correction algorithm is presented, which allows image restoration under coherent conditions. In most coherent imaging systems, especially in astronomy, the wavefront deformation is known. Using this information, the proposed algorithm allows a high quality restoration even in case of severe wavefront distortions. We present two versions of this algorithm, which are an evolution of the Gerchberg-Saxton and the Hybrid-Input-Output algorithm. The algorithm is verified on simulated and real microscopic images.|contribut deal imag restor optic system coher illumin import topic astronomi coher microscopi radar imag optic system suffer wavefront distort caus imperfect imag compon condit known imag restor algorithm work well incoher imag fail case coher imag paper novel wavefront correct algorithm present allow imag restor coher condit coher imag system especi astronomi wavefront deform known use inform propos algorithm allow high qualiti restor even case sever wavefront distort present two version algorithm evolut gerchberg saxton hybrid input output algorithm algorithm verifi simul real microscop imag|['Claudius Zelenka', 'Reinhard Koch']|['astro-ph.IM', 'cs.CV']
2017-04-07T11:30:43Z|2017-04-02T17:13:55Z|http://arxiv.org/abs/1704.00330v1|http://arxiv.org/pdf/1704.00330v1|Understanding Deep Representations through Random Weights|understand deep represent random weight|We systematically study the deep representation of random weight CNN (convolutional neural network) using the DeCNN (deconvolutional neural network) architecture. We first fix the weights of an untrained CNN, and for each layer of its feature representation, we train a corresponding DeCNN to reconstruct the input image. As compared with the pre-trained CNN, the DeCNN trained on a random weight CNN can reconstruct images more quickly and accurately, no matter which type of random distribution for the CNN's weights. It reveals that every layer of the random CNN can retain photographically accurate information about the image. We then let the DeCNN be untrained, i.e. the overall CNN-DeCNN architecture uses only random weights. Strikingly, we can reconstruct all position information of the image for low layer representations but the colors change. For high layer representations, we can still capture the rough contours of the image. We also change the number of feature maps and the shape of the feature maps and gain more insight on the random function of the CNN-DeCNN structure. Our work reveals that the purely random CNN-DeCNN architecture substantially contributes to the geometric and photometric invariance due to the intrinsic symmetry and invertible structure, but it discards the colormetric information due to the random projection.|systemat studi deep represent random weight cnn convolut neural network use decnn deconvolut neural network architectur first fix weight untrain cnn layer featur represent train correspond decnn reconstruct input imag compar pre train cnn decnn train random weight cnn reconstruct imag quick accur matter type random distribut cnn weight reveal everi layer random cnn retain photograph accur inform imag let decnn untrain overal cnn decnn architectur use onli random weight strike reconstruct posit inform imag low layer represent color chang high layer represent still captur rough contour imag also chang number featur map shape featur map gain insight random function cnn decnn structur work reveal pure random cnn decnn architectur substanti contribut geometr photometr invari due intrins symmetri invert structur discard colormetr inform due random project|['Yao Shu', 'Man Zhu', 'Kun He', 'John Hopcroft', 'Pan Zhou']|['cs.CV']
2017-04-07T11:30:43Z|2017-04-02T16:38:04Z|http://arxiv.org/abs/1704.00326v1|http://arxiv.org/pdf/1704.00326v1|People Counting in Crowded and Outdoor Scenes using an Hybrid   Multi-Camera Approach|peopl count crowd outdoor scene use hybrid multi camera approach|This paper presents two novel approaches for people counting in crowded and open environments that combine the information gathered by multiple views. Multiple camera are used to expand the field of view as well as to mitigate the problem of occlusion that commonly affects the performance of counting methods using single cameras. The first approach is regarded as a direct approach and it attempts to segment and count each individual in the crowd. For such an aim, two head detectors trained with head images are employed: one based on support vector machines and another based on Adaboost perceptron. The second approach, regarded as an indirect approach employs learning algorithms and statistical analysis on the whole crowd to achieve counting. For such an aim, corner points are extracted from groups of people in a foreground image and computed by a learning algorithm which estimates the number of people in the scene. Both approaches count the number of people on the scene and not only on a given image or video frame of the scene. The experimental results obtained on the benchmark PETS2009 video dataset show that proposed indirect method surpasses other methods with improvements of up to 46.7% and provides accurate counting results for the crowded scenes. On the other hand, the direct method shows high error rates due to the fact that the latter has much more complex problems to solve, such as segmentation of heads.|paper present two novel approach peopl count crowd open environ combin inform gather multipl view multipl camera use expand field view well mitig problem occlus common affect perform count method use singl camera first approach regard direct approach attempt segment count individu crowd aim two head detector train head imag employ one base support vector machin anoth base adaboost perceptron second approach regard indirect approach employ learn algorithm statist analysi whole crowd achiev count aim corner point extract group peopl foreground imag comput learn algorithm estim number peopl scene approach count number peopl scene onli given imag video frame scene experiment result obtain benchmark pet video dataset show propos indirect method surpass method improv provid accur count result crowd scene hand direct method show high error rate due fact latter much complex problem solv segment head|['Fabio Dittrich', 'Luiz E. S. de Oliveira', 'Alceu S. Britto Jr.', 'Alessandro L. Koerich']|['cs.CV']
2017-04-07T11:30:43Z|2017-04-02T14:00:48Z|http://arxiv.org/abs/1704.00299v1|http://arxiv.org/pdf/1704.00299v1|Efficient Version-Space Reduction for Visual Tracking|effici version space reduct visual track|Discrminative trackers, employ a classification approach to separate the target from its background. To cope with variations of the target shape and appearance, the classifier is updated online with different samples of the target and the background. Sample selection, labeling and updating the classifier is prone to various sources of errors that drift the tracker. We introduce the use of an efficient version space shrinking strategy to reduce the labeling errors and enhance its sampling strategy by measuring the uncertainty of the tracker about the samples. The proposed tracker, utilize an ensemble of classifiers that represents different hypotheses about the target, diversify them using boosting to provide a larger and more consistent coverage of the version-space and tune the classifiers' weights in voting. The proposed system adjusts the model update rate by promoting the co-training of the short-memory ensemble with a long-memory oracle. The proposed tracker outperformed state-of-the-art trackers on different sequences bearing various tracking challenges.|discrmin tracker employ classif approach separ target background cope variat target shape appear classifi updat onlin differ sampl target background sampl select label updat classifi prone various sourc error drift tracker introduc use effici version space shrink strategi reduc label error enhanc sampl strategi measur uncertainti tracker sampl propos tracker util ensembl classifi repres differ hypothes target diversifi use boost provid larger consist coverag version space tune classifi weight vote propos system adjust model updat rate promot co train short memori ensembl long memori oracl propos tracker outperform state art tracker differ sequenc bear various track challeng|['Kourosh Meshgi', 'Shigeyuki Oba', 'Shin Ishii']|['cs.CV']
2017-04-07T11:30:43Z|2017-04-02T10:38:49Z|http://arxiv.org/abs/1704.00280v1|http://arxiv.org/abs/1704.00280v1|The Stixel world: A medium-level representation of traffic scenes|stixel world medium level represent traffic scene|Recent progress in advanced driver assistance systems and the race towards autonomous vehicles is mainly driven by two factors: (1) increasingly sophisticated algorithms that interpret the environment around the vehicle and react accordingly, and (2) the continuous improvements of sensor technology itself. In terms of cameras, these improvements typically include higher spatial resolution, which as a consequence requires more data to be processed. The trend to add multiple cameras to cover the entire surrounding of the vehicle is not conducive in that matter. At the same time, an increasing number of special purpose algorithms need access to the sensor input data to correctly interpret the various complex situations that can occur, particularly in urban traffic.   By observing those trends, it becomes clear that a key challenge for vision architectures in intelligent vehicles is to share computational resources. We believe this challenge should be faced by introducing a representation of the sensory data that provides compressed and structured access to all relevant visual content of the scene. The Stixel World discussed in this paper is such a representation. It is a medium-level model of the environment that is specifically designed to compress information about obstacles by leveraging the typical layout of outdoor traffic scenes. It has proven useful for a multitude of automotive vision applications, including object detection, tracking, segmentation, and mapping.   In this paper, we summarize the ideas behind the model and generalize it to take into account multiple dense input streams: the image itself, stereo depth maps, and semantic class probability maps that can be generated, e.g., by CNNs. Our generalization is embedded into a novel mathematical formulation for the Stixel model. We further sketch how the free parameters of the model can be learned using structured SVMs.|recent progress advanc driver assist system race toward autonom vehicl main driven two factor increas sophist algorithm interpret environ around vehicl react accord continu improv sensor technolog term camera improv typic includ higher spatial resolut consequ requir data process trend add multipl camera cover entir surround vehicl conduc matter time increas number special purpos algorithm need access sensor input data correct interpret various complex situat occur particular urban traffic observ trend becom clear key challeng vision architectur intellig vehicl share comput resourc believ challeng face introduc represent sensori data provid compress structur access relev visual content scene stixel world discuss paper represent medium level model environ specif design compress inform obstacl leverag typic layout outdoor traffic scene proven use multitud automot vision applic includ object detect track segment map paper summar idea behind model general take account multipl dens input stream imag stereo depth map semant class probabl map generat cnns general embed novel mathemat formul stixel model sketch free paramet model learn use structur svms|['Marius Cordts', 'Timo Rehfeld', 'Lukas Schneider', 'David Pfeiffer', 'Markus Enzweiler', 'Stefan Roth', 'Marc Pollefeys', 'Uwe Franke']|['cs.CV']
2017-04-07T11:30:43Z|2017-04-02T09:44:05Z|http://arxiv.org/abs/1704.00275v1|http://arxiv.org/pdf/1704.00275v1|SAR image despeckling through convolutional neural networks|sar imag despeckl convolut neural network|In this paper we investigate the use of discriminative model learning through Convolutional Neural Networks (CNNs) for SAR image despeckling. The network uses a residual learning strategy, hence it does not recover the filtered image, but the speckle component, which is then subtracted from the noisy one. Training is carried out by considering a large multitemporal SAR image properly despeckled through 3D filtering, in order to approximate a {\em clean} image. Experimental results, both on synthetic and real SAR data, show the method to achieve performance that improve with respect to state-of-the-art techniques.|paper investig use discrimin model learn convolut neural network cnns sar imag despeckl network use residu learn strategi henc doe recov filter imag speckl compon subtract noisi one train carri consid larg multitempor sar imag proper despeckl filter order approxim em clean imag experiment result synthet real sar data show method achiev perform improv respect state art techniqu|['G. Chierchia', 'D. Cozzolino', 'G. Poggi', 'L. Verdoliva']|['cs.CV']
2017-04-07T11:30:43Z|2017-04-02T08:01:30Z|http://arxiv.org/abs/1704.00260v1|http://arxiv.org/pdf/1704.00260v1|Aligned Image-Word Representations Improve Inductive Transfer Across   Vision-Language Tasks|align imag word represent improv induct transfer across vision languag task|A grand goal of computer vision is to build systems that learn visual representations over time that can be applied to many tasks. In this paper, we investigate a vision-language embedding as a core representation and show that it leads to better cross-task transfer than standard multi-task learning. In particular, the task of visual recognition is aligned to the task of visual question answering by forcing each to use the same word-region embeddings. We show this leads to greater inductive transfer from recognition to VQA than standard multitask learning. Visual recognition also improves, especially for categories that have relatively few recognition training labels but appear often in the VQA setting. Thus, our paper takes a small step towards creating more general vision systems by showing the benefit of interpretable, flexible, and trainable core representations.|grand goal comput vision build system learn visual represent time appli mani task paper investig vision languag embed core represent show lead better cross task transfer standard multi task learn particular task visual recognit align task visual question answer forc use word region embed show lead greater induct transfer recognit vqa standard multitask learn visual recognit also improv especi categori relat recognit train label appear often vqa set thus paper take small step toward creat general vision system show benefit interpret flexibl trainabl core represent|['Tanmay Gupta', 'Kevin Shih', 'Saurabh Singh', 'Derek Hoiem']|['cs.CV', 'cs.AI', 'cs.LG', 'cs.NE', 'stat.ML']
2017-04-07T11:30:48Z|2017-04-02T03:12:18Z|http://arxiv.org/abs/1704.00248v1|http://arxiv.org/pdf/1704.00248v1|A-Lamp: Adaptive Layout-Aware Multi-Patch Deep Convolutional Neural   Network for Photo Aesthetic Assessment|lamp adapt layout awar multi patch deep convolut neural network photo aesthet assess|Deep convolutional neural networks (CNN) have recently been shown to generate promising results for aesthetics assessment. However, the performance of these deep CNN methods is often compromised by the constraint that the neural network only takes the fixed-size input. To accommodate this requirement, input images need to be transformed via cropping, warping, or padding, which often alter image composition, reduce image resolution, or cause image distortion. Thus the aesthetics of the original images is impaired because of potential loss of fine grained details and holistic image layout. However, such fine grained details and holistic image layout is critical for evaluating an image's aesthetics. In this paper, we present an Adaptive Layout-Aware Multi-Patch Convolutional Neural Network (A-Lamp CNN) architecture for photo aesthetic assessment. This novel scheme is able to accept arbitrary sized images, and learn from both fined grained details and holistic image layout simultaneously. To enable training on these hybrid inputs, we extend the method by developing a dedicated double-subnet neural network structure, i.e. a Multi-Patch subnet and a Layout-Aware subnet. We further construct an aggregation layer to effectively combine the hybrid features from these two subnets. Extensive experiments on the large-scale aesthetics assessment benchmark (AVA) demonstrate significant performance improvement over the state-of-the-art in photo aesthetic assessment.|deep convolut neural network cnn recent shown generat promis result aesthet assess howev perform deep cnn method often compromis constraint neural network onli take fix size input accommod requir input imag need transform via crop warp pad often alter imag composit reduc imag resolut caus imag distort thus aesthet origin imag impair becaus potenti loss fine grain detail holist imag layout howev fine grain detail holist imag layout critic evalu imag aesthet paper present adapt layout awar multi patch convolut neural network lamp cnn architectur photo aesthet assess novel scheme abl accept arbitrari size imag learn fine grain detail holist imag layout simultan enabl train hybrid input extend method develop dedic doubl subnet neural network structur multi patch subnet layout awar subnet construct aggreg layer effect combin hybrid featur two subnet extens experi larg scale aesthet assess benchmark ava demonstr signific perform improv state art photo aesthet assess|['Shuang Ma', 'Jing Liu', 'Chang Wen Chen']|['cs.CV']
2017-04-07T11:30:48Z|2017-04-01T15:15:38Z|http://arxiv.org/abs/1704.00180v1|http://arxiv.org/abs/1704.00180v1|Complexity-Aware Assignment of Latent Values in Discriminative Models   for Accurate Gesture Recognition|complex awar assign latent valu discrimin model accur gestur recognit|Many of the state-of-the-art algorithms for gesture recognition are based on Conditional Random Fields (CRFs). Successful approaches, such as the Latent-Dynamic CRFs, extend the CRF by incorporating latent variables, whose values are mapped to the values of the labels. In this paper we propose a novel methodology to set the latent values according to the gesture complexity. We use an heuristic that iterates through the samples associated with each label value, stimating their complexity. We then use it to assign the latent values to the label values. We evaluate our method on the task of recognizing human gestures from video streams. The experiments were performed in binary datasets, generated by grouping different labels. Our results demonstrate that our approach outperforms the arbitrary one in many cases, increasing the accuracy by up to 10%.|mani state art algorithm gestur recognit base condit random field crfs success approach latent dynam crfs extend crf incorpor latent variabl whose valu map valu label paper propos novel methodolog set latent valu accord gestur complex use heurist iter sampl associ label valu stimat complex use assign latent valu label valu evalu method task recogn human gestur video stream experi perform binari dataset generat group differ label result demonstr approach outperform arbitrari one mani case increas accuraci|['Manoel Horta Ribeiro', 'Bruno Teixeira', 'Ant√¥nio Ot√°vio Fernandes', 'Wagner Meira Jr.', 'Erickson R. Nascimento']|['cs.CV']
2017-04-07T11:30:48Z|2017-04-01T11:59:41Z|http://arxiv.org/abs/1704.00159v1|http://arxiv.org/pdf/1704.00159v1|Compositional Human Pose Regression|composit human pose regress|Regression based methods are widely used for 3D and 2D human pose estimation, but the performance is not satisfactory. One problem is that the structural information of the pose is not well exploited in the existing methods. In this work, we propose a structure-aware regression approach. It adopts a reparameterized pose representation using bones instead of joints. It exploits the joint connection structure and proposes a compositional loss function that encodes the long range interactions in the pose. It is simple, effective, and general. Comprehensive evaluation validates the effectiveness of our approach. It significantly advances the state-of-the-art on Human3.6M and achieves state-of-the-art results on MPII, in a unified framework for 3D and 2D pose regression.|regress base method wide use human pose estim perform satisfactori one problem structur inform pose well exploit exist method work propos structur awar regress approach adopt reparameter pose represent use bone instead joint exploit joint connect structur propos composit loss function encod long rang interact pose simpl effect general comprehens evalu valid effect approach signific advanc state art human achiev state art result mpii unifi framework pose regress|['Xiao Sun', 'Jiaxiang Shang', 'Shuang Liang', 'Yichen Wei']|['cs.CV']
2017-04-07T11:30:48Z|2017-04-01T08:32:40Z|http://arxiv.org/abs/1704.00138v1|http://arxiv.org/pdf/1704.00138v1|Multiple Instance Detection Network with Online Instance Classifier   Refinement|multipl instanc detect network onlin instanc classifi refin|Of late, weakly supervised object detection is with great importance in object recognition. Based on deep learning, weakly supervised detectors have achieved many promising results. However, compared with fully supervised detection, it is more challenging to train deep network based detectors in a weakly supervised manner. Here we formulate weakly supervised detection as a Multiple Instance Learning (MIL) problem, where instance classifiers (object detectors) are put into the network as hidden nodes. We propose a novel online instance classifier refinement algorithm to integrate MIL and the instance classifier refinement procedure into a single deep network, and train the network end-to-end with only image-level supervision, i.e., without object location information. More precisely, instance labels inferred from weak supervision are propagated to their spatially overlapped instances to refine instance classifier online. The iterative instance classifier refinement procedure is implemented using multiple streams in deep network, where each stream supervises its latter stream. Weakly supervised object detection experiments are carried out on the challenging PASCAL VOC 2007 and 2012 benchmarks. We obtain 47% mAP on VOC 2007 that significantly outperforms the previous state-of-the-art.|late weak supervis object detect great import object recognit base deep learn weak supervis detector achiev mani promis result howev compar fulli supervis detect challeng train deep network base detector weak supervis manner formul weak supervis detect multipl instanc learn mil problem instanc classifi object detector put network hidden node propos novel onlin instanc classifi refin algorithm integr mil instanc classifi refin procedur singl deep network train network end end onli imag level supervis without object locat inform precis instanc label infer weak supervis propag spatial overlap instanc refin instanc classifi onlin iter instanc classifi refin procedur implement use multipl stream deep network stream supervis latter stream weak supervis object detect experi carri challeng pascal voc benchmark obtain map voc signific outperform previous state art|['Peng Tang', 'Xinggang Wang', 'Xiang Bai', 'Wenyu Liu']|['cs.CV']
2017-04-07T11:30:48Z|2017-04-01T06:15:25Z|http://arxiv.org/abs/1704.01088v1|http://arxiv.org/pdf/1704.01088v1|sWSI: A Low-cost and Commercial-quality Whole Slide Imaging System on   Android and iOS Smartphones|swsi low cost commerci qualiti whole slide imag system android io smartphon|In this paper, scalable Whole Slide Imaging (sWSI), a novel high-throughput, cost-effective and robust whole slide imaging system on both Android and iOS platforms is introduced and analyzed. With sWSI, most mainstream smartphone connected to a optical eyepiece of any manually controlled microscope can be automatically controlled to capture sequences of mega-pixel fields of views that are synthesized into giga-pixel virtual slides. Remote servers carry out the majority of computation asynchronously to support clients running at satisfying frame rates without sacrificing image quality nor robustness. A typical 15x15mm sample can be digitized in 30 seconds with 4X or in 3 minutes with 10X object magnification, costing under $1. The virtual slide quality is considered comparable to existing high-end scanners thus satisfying for clinical usage by surveyed pathologies. The scan procedure with features such as supporting magnification up to 100x, recoding z-stacks, specimen-type-neutral and giving real-time feedback, is deemed work-flow-friendly and reliable.|paper scalabl whole slide imag swsi novel high throughput cost effect robust whole slide imag system android io platform introduc analyz swsi mainstream smartphon connect optic eyepiec ani manual control microscop automat control captur sequenc mega pixel field view synthes giga pixel virtual slide remot server carri major comput asynchron support client run satisfi frame rate without sacrif imag qualiti robust typic xmm sampl digit second minut object magnif cost virtual slide qualiti consid compar exist high end scanner thus satisfi clinic usag survey patholog scan procedur featur support magnif recod stack specimen type neutral give real time feedback deem work flow friend reliabl|['Shuoxin Ma', 'Tan Wang']|['physics.bio-ph', 'cs.CV']
2017-04-07T11:30:48Z|2017-04-04T00:50:58Z|http://arxiv.org/abs/1704.00112v2|http://arxiv.org/pdf/1704.00112v2|Configurable, Photorealistic Image Rendering and Ground Truth Synthesis   by Sampling Stochastic Grammars Representing Indoor Scenes|configur photorealist imag render ground truth synthesi sampl stochast grammar repres indoor scene|We propose the configurable rendering of massive quantities of photorealistic images with ground truth for the purposes of training, benchmarking, and diagnosing computer vision models. In contrast to the conventional (crowd-sourced) manual labeling of ground truth for a relatively modest number of RGB-D images captured by Kinect-like sensors, we devise a non-trivial configurable pipeline of algorithms capable of generating a potentially infinite variety of indoor scenes using a stochastic grammar, specifically, one represented by an attributed spatial And-Or graph. We employ physics-based rendering to synthesize photorealistic RGB images while automatically synthesizing detailed, per-pixel ground truth data, including visible surface depth and normal, object identity and material information, as well as illumination. Our pipeline is configurable inasmuch as it enables the precise customization and control of important attributes of the generated scenes. We demonstrate that our generated scenes achieve a performance similar to the NYU v2 Dataset on pre-trained deep learning models. By modifying pipeline components in a controllable manner, we furthermore provide diagnostics on common scene understanding tasks; eg., depth and surface normal prediction, semantic segmentation, etc.|propos configur render massiv quantiti photorealist imag ground truth purpos train benchmark diagnos comput vision model contrast convent crowd sourc manual label ground truth relat modest number rgb imag captur kinect like sensor devis non trivial configur pipelin algorithm capabl generat potenti infinit varieti indoor scene use stochast grammar specif one repres attribut spatial graph employ physic base render synthes photorealist rgb imag automat synthes detail per pixel ground truth data includ visibl surfac depth normal object ident materi inform well illumin pipelin configur inasmuch enabl precis custom control import attribut generat scene demonstr generat scene achiev perform similar nyu dataset pre train deep learn model modifi pipelin compon control manner furthermor provid diagnost common scene understand task eg depth surfac normal predict semant segment etc|['Chenfanfu Jiang', 'Yixin Zhu', 'Siyuan Qi', 'Siyuan Huang', 'Jenny Lin', 'Xingwen Guo', 'Lap-Fai Yu', 'Demetri Terzopoulos', 'Song-Chun Zhu']|['cs.CV', 'stat.ML']
2017-04-07T11:30:48Z|2017-04-01T02:12:40Z|http://arxiv.org/abs/1704.00103v1|http://arxiv.org/pdf/1704.00103v1|SafetyNet: Detecting and Rejecting Adversarial Examples Robustly|safetynet detect reject adversari exampl robust|We describe a method to produce a network where current methods such as DeepFool have great difficulty producing adversarial samples. Our construction suggests some insights into how deep networks work. We provide a reasonable analyses that our construction is difficult to defeat, and show experimentally that our method is hard to defeat using several standard networks and datasets. We use our method to produce a system that can reliably detect whether an image is a picture of a real scene or not. Our system applies to images captured with depth maps (RGBD images) and checks if a pair of image and depth map is consistent. It relies on the relative difficulty of producing naturalistic depth maps for images in post processing. We demonstrate that our system is robust to adversarial examples built from currently known attacking approaches.|describ method produc network current method deepfool great difficulti produc adversari sampl construct suggest insight deep network work provid reason analys construct difficult defeat show experiment method hard defeat use sever standard network dataset use method produc system reliabl detect whether imag pictur real scene system appli imag captur depth map rgbd imag check pair imag depth map consist reli relat difficulti produc naturalist depth map imag post process demonstr system robust adversari exampl built current known attack approach|['Jiajun Lu', 'Theerasit Issaranon', 'David Forsyth']|['cs.CV', 'cs.LG']
2017-04-07T11:30:48Z|2017-04-01T01:55:28Z|http://arxiv.org/abs/1704.00098v1|http://arxiv.org/pdf/1704.00098v1|Customizing First Person Image Through Desired Actions|custom first person imag desir action|This paper studies a problem of inverse visual path planning: creating a visual scene from a first person action. Our conjecture is that the spatial arrangement of a first person visual scene is deployed to afford an action, and therefore, the action can be inversely used to synthesize a new scene such that the action is feasible. As a proof-of-concept, we focus on linking visual experiences induced by walking.   A key innovation of this paper is a concept of ActionTunnel---a 3D virtual tunnel along the future trajectory encoding what the wearer will visually experience as moving into the scene. This connects two distinctive first person images through similar walking paths. Our method takes a first person image with a user defined future trajectory and outputs a new image that can afford the future motion. The image is created by combining present and future ActionTunnels in 3D where the missing pixels in adjoining area are computed by a generative adversarial network. Our work can provide a travel across different first person experiences in diverse real world scenes.|paper studi problem invers visual path plan creat visual scene first person action conjectur spatial arrang first person visual scene deploy afford action therefor action invers use synthes new scene action feasibl proof concept focus link visual experi induc walk key innov paper concept actiontunnel virtual tunnel along futur trajectori encod wearer visual experi move scene connect two distinct first person imag similar walk path method take first person imag user defin futur trajectori output new imag afford futur motion imag creat combin present futur actiontunnel miss pixel adjoin area comput generat adversari network work provid travel across differ first person experi divers real world scene|['Shan Su', 'Jianbo Shi', 'Hyun Soo Park']|['cs.CV']
2017-04-07T11:30:48Z|2017-04-01T00:50:12Z|http://arxiv.org/abs/1704.00090v1|http://arxiv.org/pdf/1704.00090v1|Learning to Predict Indoor Illumination from a Single Image|learn predict indoor illumin singl imag|In this work, we propose a method to infer high dynamic range illumination from a single, limited field-of-view, low dynamic range photograph of an indoor scene. Inferring scene illumination from a single photograph is a challenging problem. The pixel intensities observed in a photograph are a complex function of scene geometry, reflectance properties, and illumination. We introduce an end-to-end solution to this problem and propose a deep neural network that takes the limited field-of-view photo as input and produces an environment map as a panorama and a light mask prediction over the panorama as the output. Our technique does not require special image capture or user input. We preprocess standard low dynamic range panoramas by introducing novel light source detection and warping methods on the panorama, and use the results with corresponding limited field-of-view crops as training data. Our method does not rely on any assumptions on scene appearance, geometry, material properties, or lighting. This allows us to automatically recover high-quality illumination estimates that significantly outperform previous state-of-the-art methods. Consequently, using our illumination estimates for applications like 3D object insertion lead to results that are photo-realistic, which we demonstrate over a large set of examples and via a user study.|work propos method infer high dynam rang illumin singl limit field view low dynam rang photograph indoor scene infer scene illumin singl photograph challeng problem pixel intens observ photograph complex function scene geometri reflect properti illumin introduc end end solut problem propos deep neural network take limit field view photo input produc environ map panorama light mask predict panorama output techniqu doe requir special imag captur user input preprocess standard low dynam rang panorama introduc novel light sourc detect warp method panorama use result correspond limit field view crop train data method doe reli ani assumpt scene appear geometri materi properti light allow us automat recov high qualiti illumin estim signific outperform previous state art method consequ use illumin estim applic like object insert lead result photo realist demonstr larg set exampl via user studi|['Marc-Andr√© Gardner', 'Kalyan Sunkavalli', 'Ersin Yumer', 'Xiaohui Shen', 'Emiliano Gambaretto', 'Christian Gagn√©', 'Jean-Fran√ßois Lalonde']|['cs.CV']
2017-04-07T11:30:48Z|2017-03-31T23:33:06Z|http://arxiv.org/abs/1704.00085v1|http://arxiv.org/pdf/1704.00085v1|Optimal Reconstruction with a Small Number of Views|optim reconstruct small number view|Estimating positions of world points from features observed in images is a key problem in 3D reconstruction, image mosaicking, simultaneous localization and mapping and structure from motion. We consider a special instance in which there is a dominant ground plane $\mathcal{G}$ viewed from a parallel viewing plane $\mathcal{S}$ above it. Such instances commonly arise, for example, in aerial photography.   Consider a world point $g \in \mathcal{G}$ and its worst case reconstruction uncertainty $\varepsilon(g,\mathcal{S})$ obtained by merging \emph{all} possible views of $g$ chosen from $\mathcal{S}$. We first show that one can pick two views $s_p$ and $s_q$ such that the uncertainty $\varepsilon(g,\{s_p,s_q\})$ obtained using only these two views is almost as good as (i.e. within a small constant factor of) $\varepsilon(g,\mathcal{S})$. Next, we extend the result to the entire ground plane $\mathcal{G}$ and show that one can pick a small subset of $\mathcal{S'} \subseteq \mathcal{S}$ (which grows only linearly with the area of $\mathcal{G}$) and still obtain a constant factor approximation, for every point $g \in \mathcal{G}$, to the minimum worst case estimate obtained by merging all views in $\mathcal{S}$.   Our results provide a view selection mechanism with provable performance guarantees which can drastically increase the speed of scene reconstruction algorithms. In addition to theoretical results, we demonstrate their effectiveness in an application where aerial imagery is used for monitoring farms and orchards.|estim posit world point featur observ imag key problem reconstruct imag mosaick simultan local map structur motion consid special instanc domin ground plane mathcal view parallel view plane mathcal abov instanc common aris exampl aerial photographi consid world point mathcal worst case reconstruct uncertainti varepsilon mathcal obtain merg emph possibl view chosen mathcal first show one pick two view uncertainti varepsilon obtain use onli two view almost good within small constant factor varepsilon mathcal next extend result entir ground plane mathcal show one pick small subset mathcal subseteq mathcal grow onli linear area mathcal still obtain constant factor approxim everi point mathcal minimum worst case estim obtain merg view mathcal result provid view select mechan provabl perform guarante drastic increas speed scene reconstruct algorithm addit theoret result demonstr effect applic aerial imageri use monitor farm orchard|['Cheng Peng', 'Volkan Isler']|['cs.CV']
2017-04-07T11:30:52Z|2017-03-31T23:28:20Z|http://arxiv.org/abs/1704.00083v1|http://arxiv.org/pdf/1704.00083v1|Efficient Asymmetric Co-Tracking using Uncertainty Sampling|effici asymmetr co track use uncertainti sampl|Adaptive tracking-by-detection approaches are popular for tracking arbitrary objects. They treat the tracking problem as a classification task and use online learning techniques to update the object model. However, these approaches are heavily invested in the efficiency and effectiveness of their detectors. Evaluating a massive number of samples for each frame (e.g., obtained by a sliding window) forces the detector to trade the accuracy in favor of speed. Furthermore, misclassification of borderline samples in the detector introduce accumulating errors in tracking. In this study, we propose a co-tracking based on the efficient cooperation of two detectors: a rapid adaptive exemplar-based detector and another more sophisticated but slower detector with a long-term memory. The sampling labeling and co-learning of the detectors are conducted by an uncertainty sampling unit, which improves the speed and accuracy of the system. We also introduce a budgeting mechanism which prevents the unbounded growth in the number of examples in the first detector to maintain its rapid response. Experiments demonstrate the efficiency and effectiveness of the proposed tracker against its baselines and its superior performance against state-of-the-art trackers on various benchmark videos.|adapt track detect approach popular track arbitrari object treat track problem classif task use onlin learn techniqu updat object model howev approach heavili invest effici effect detector evalu massiv number sampl frame obtain slide window forc detector trade accuraci favor speed furthermor misclassif borderlin sampl detector introduc accumul error track studi propos co track base effici cooper two detector rapid adapt exemplar base detector anoth sophist slower detector long term memori sampl label co learn detector conduct uncertainti sampl unit improv speed accuraci system also introduc budget mechan prevent unbound growth number exampl first detector maintain rapid respons experi demonstr effici effect propos tracker baselin superior perform state art tracker various benchmark video|['Kourosh Meshgi', 'Maryam Sadat Mirzaei', 'Shigeyuki Oba', 'Shin Ishii']|['cs.CV']
2017-04-07T11:30:52Z|2017-03-31T22:39:32Z|http://arxiv.org/abs/1704.00077v1|http://arxiv.org/abs/1704.00077v1|Geodesic Distance Histogram Feature for Video Segmentation|geodes distanc histogram featur video segment|This paper proposes a geodesic-distance-based feature that encodes global information for improved video segmentation algorithms. The feature is a joint histogram of intensity and geodesic distances, where the geodesic distances are computed as the shortest paths between superpixels via their boundaries. We also incorporate adaptive voting weights and spatial pyramid configurations to include spatial information into the geodesic histogram feature and show that this further improves results. The feature is generic and can be used as part of various algorithms. In experiments, we test the geodesic histogram feature by incorporating it into two existing video segmentation frameworks. This leads to significantly better performance in 3D video segmentation benchmarks on two datasets.|paper propos geodes distanc base featur encod global inform improv video segment algorithm featur joint histogram intens geodes distanc geodes distanc comput shortest path superpixel via boundari also incorpor adapt vote weight spatial pyramid configur includ spatial inform geodes histogram featur show improv result featur generic use part various algorithm experi test geodes histogram featur incorpor two exist video segment framework lead signific better perform video segment benchmark two dataset|['Hieu Le', 'Vu Nguyen', 'Chen-Ping Yu', 'Dimitris Samaras']|['cs.CV']
2017-04-07T11:30:52Z|2017-03-31T19:57:12Z|http://arxiv.org/abs/1704.00036v1|http://arxiv.org/pdf/1704.00036v1|Efficient Registration of Pathological Images: A Joint   PCA/Image-Reconstruction Approach|effici registr patholog imag joint pca imag reconstruct approach|Registration involving one or more images containing pathologies is challenging, as standard image similarity measures and spatial transforms cannot account for common changes due to pathologies. Low-rank/Sparse (LRS) decomposition removes pathologies prior to registration; however, LRS is memory-demanding and slow, which limits its use on larger data sets. Additionally, LRS blurs normal tissue regions, which may degrade registration performance. This paper proposes an efficient alternative to LRS: (1) normal tissue appearance is captured by principal component analysis (PCA) and (2) blurring is avoided by an integrated model for pathology removal and image reconstruction. Results on synthetic and BRATS 2015 data demonstrate its utility.|registr involv one imag contain patholog challeng standard imag similar measur spatial transform cannot account common chang due patholog low rank spars lrs decomposit remov patholog prior registr howev lrs memori demand slow limit use larger data set addit lrs blur normal tissu region may degrad registr perform paper propos effici altern lrs normal tissu appear captur princip compon analysi pca blur avoid integr model patholog remov imag reconstruct result synthet brat data demonstr util|['Xu Han', 'Xiao Yang', 'Stephen Aylward', 'Roland Kwitt', 'Marc Niethammer']|['cs.CV']
2017-04-07T11:30:52Z|2017-03-31T19:50:06Z|http://arxiv.org/abs/1704.00033v1|http://arxiv.org/pdf/1704.00033v1|Transfer of View-manifold Learning to Similarity Perception of Novel   Objects|transfer view manifold learn similar percept novel object|We develop a model of perceptual similarity judgment based on re-training a deep convolution neural network (DCNN) that learns to associate different views of each 3D object to capture the notion of object persistence and continuity in our visual experience. The re-training process effectively performs distance metric learning under the object persistency constraints, to modify the view-manifold of object representations. It reduces the effective distance between the representations of different views of the same object without compromising the distance between those of the views of different objects, resulting in the untangling of the view-manifolds between individual objects within the same category and across categories. This untangling enables the model to discriminate and recognize objects within the same category, independent of viewpoints. We found that this ability is not limited to the trained objects, but transfers to novel objects in both trained and untrained categories, as well as to a variety of completely novel artificial synthetic objects. This transfer in learning suggests the modification of distance metrics in view- manifolds is more general and abstract, likely at the levels of parts, and independent of the specific objects or categories experienced during training. Interestingly, the resulting transformation of feature representation in the deep networks is found to significantly better match human perceptual similarity judgment than AlexNet, suggesting that object persistence could be an important constraint in the development of perceptual similarity judgment in biological neural networks.|develop model perceptu similar judgment base train deep convolut neural network dcnn learn associ differ view object captur notion object persist continu visual experi train process effect perform distanc metric learn object persist constraint modifi view manifold object represent reduc effect distanc represent differ view object without compromis distanc view differ object result untangl view manifold individu object within categori across categori untangl enabl model discrimin recogn object within categori independ viewpoint found abil limit train object transfer novel object train untrain categori well varieti complet novel artifici synthet object transfer learn suggest modif distanc metric view manifold general abstract like level part independ specif object categori experienc dure train interest result transform featur represent deep network found signific better match human perceptu similar judgment alexnet suggest object persist could import constraint develop perceptu similar judgment biolog neural network|['Xingyu Lin', 'Hao Wang', 'Zhihao Li', 'Yimeng Zhang', 'Alan Yuille', 'Tai Sing Lee']|['cs.CV']
2017-04-07T11:30:52Z|2017-03-31T15:47:27Z|http://arxiv.org/abs/1703.10956v1|http://arxiv.org/pdf/1703.10956v1|InverseFaceNet: Deep Single-Shot Inverse Face Rendering From A Single   Image|inversefacenet deep singl shot invers face render singl imag|We introduce InverseFaceNet, a deep convolutional inverse rendering framework for faces that jointly estimates facial pose, shape, expression, reflectance and illumination from a single input image in a single shot. By estimating all these parameters from just a single image, advanced editing possibilities on a single face image, such as appearance editing and relighting, become feasible. Previous learning-based face reconstruction approaches do not jointly recover all dimensions, or are severely limited in terms of visual quality. In contrast, we propose to recover high-quality facial pose, shape, expression, reflectance and illumination using a deep neural network that is trained using a large, synthetically created dataset. Our approach builds on a novel loss function that measures model-space similarity directly in parameter space and significantly improves reconstruction accuracy. In addition, we propose an analysis-by-synthesis breeding approach which iteratively updates the synthetic training corpus based on the distribution of real-world images, and we demonstrate that this strategy outperforms completely synthetically trained networks. Finally, we show high-quality reconstructions and compare our approach to several state-of-the-art approaches.|introduc inversefacenet deep convolut invers render framework face joint estim facial pose shape express reflect illumin singl input imag singl shot estim paramet singl imag advanc edit possibl singl face imag appear edit relight becom feasibl previous learn base face reconstruct approach joint recov dimens sever limit term visual qualiti contrast propos recov high qualiti facial pose shape express reflect illumin use deep neural network train use larg synthet creat dataset approach build novel loss function measur model space similar direct paramet space signific improv reconstruct accuraci addit propos analysi synthesi breed approach iter updat synthet train corpus base distribut real world imag demonstr strategi outperform complet synthet train network final show high qualiti reconstruct compar approach sever state art approach|['Hyeongwoo Kim', 'Michael Zollh√∂fer', 'Ayush Tewari', 'Justus Thies', 'Christian Richardt', 'Christian Theobalt']|['cs.CV']
2017-04-07T11:30:52Z|2017-03-31T14:13:55Z|http://arxiv.org/abs/1703.10908v1|http://arxiv.org/pdf/1703.10908v1|Quicksilver: Fast Predictive Image Registration - a Deep Learning   Approach|quicksilv fast predict imag registr deep learn approach|This paper introduces Quicksilver, a fast deformable image registration method. Quicksilver registration for image-pairs works by patch-wise prediction of a deformation model based directly on image appearance. A deep encoder-decoder network is used as the prediction model. While the prediction strategy is general, we focus on predictions for the Large Deformation Diffeomorphic Metric Mapping (LDDMM) model. Specifically, we predict the momentum-parameterization of LDDMM, which facilitates a patch-wise prediction strategy while maintaining the theoretical properties of LDDMM, such as guaranteed diffeomorphic mappings for sufficiently strong regularization. We also provide a probabilistic version of our prediction network which can be sampled during test time to calculate uncertainties in the predicted deformations. Finally, we introduce a new correction network which greatly increases the prediction accuracy of an already existing prediction network. Experiments are conducted for both atlas-to-image and image-to-image registrations. These experiments show that our method accurately predicts registrations obtained by numerical optimization, is very fast, and achieves state-of-the-art registration results on four standard validation datasets. Quicksilver is freely available as open-source software.|paper introduc quicksilv fast deform imag registr method quicksilv registr imag pair work patch wise predict deform model base direct imag appear deep encod decod network use predict model predict strategi general focus predict larg deform diffeomorph metric map lddmm model specif predict momentum parameter lddmm facilit patch wise predict strategi maintain theoret properti lddmm guarante diffeomorph map suffici strong regular also provid probabilist version predict network sampl dure test time calcul uncertainti predict deform final introduc new correct network great increas predict accuraci alreadi exist predict network experi conduct atlas imag imag imag registr experi show method accur predict registr obtain numer optim veri fast achiev state art registr result four standard valid dataset quicksilv freeli avail open sourc softwar|['Xiao Yang', 'Roland Kwitt', 'Marc Niethammer']|['cs.CV']
2017-04-07T11:30:52Z|2017-03-31T14:05:57Z|http://arxiv.org/abs/1703.10902v1|http://arxiv.org/pdf/1703.10902v1|Fast Predictive Multimodal Image Registration|fast predict multimod imag registr|We introduce a deep encoder-decoder architecture for image deformation prediction from multimodal images. Specifically, we design an image-patch-based deep network that jointly (i) learns an image similarity measure and (ii) the relationship between image patches and deformation parameters. While our method can be applied to general image registration formulations, we focus on the Large Deformation Diffeomorphic Metric Mapping (LDDMM) registration model. By predicting the initial momentum of the shooting formulation of LDDMM, we preserve its mathematical properties and drastically reduce the computation time, compared to optimization-based approaches. Furthermore, we create a Bayesian probabilistic version of the network that allows evaluation of registration uncertainty via sampling of the network at test time. We evaluate our method on a 3D brain MRI dataset using both T1- and T2-weighted images. Our experiments show that our method generates accurate predictions and that learning the similarity measure leads to more consistent registrations than relying on generic multimodal image similarity measures, such as mutual information. Our approach is an order of magnitude faster than optimization-based LDDMM.|introduc deep encod decod architectur imag deform predict multimod imag specif design imag patch base deep network joint learn imag similar measur ii relationship imag patch deform paramet method appli general imag registr formul focus larg deform diffeomorph metric map lddmm registr model predict initi momentum shoot formul lddmm preserv mathemat properti drastic reduc comput time compar optim base approach furthermor creat bayesian probabilist version network allow evalu registr uncertainti via sampl network test time evalu method brain mri dataset use weight imag experi show method generat accur predict learn similar measur lead consist registr reli generic multimod imag similar measur mutual inform approach order magnitud faster optim base lddmm|['Xiao Yang', 'Roland Kwitt', 'Martin Styner', 'Marc Niethammer']|['cs.CV']
2017-04-07T11:30:52Z|2017-03-31T14:05:13Z|http://arxiv.org/abs/1703.10901v1|http://arxiv.org/pdf/1703.10901v1|Unsupervised learning from video to detect foreground objects in single   images|unsupervis learn video detect foreground object singl imag|Unsupervised learning from visual data is one of the most difficult challenges in computer vision, being a fundamental task for understanding how visual recognition works. From a practical point of view, learning from unsupervised visual input has an immense practical value, as very large quantities of unlabeled videos can be collected at low cost. In this paper, we address the task of unsupervised learning to detect and segment foreground objects in single images. We achieve our goal by training a student pathway, consisting of a deep neural network. It learns to predict from a single input image (a video frame) the output for that particular frame, of a teacher pathway that performs unsupervised object discovery in video. Our approach is different from the published literature that performs unsupervised discovery in videos or in collections of images at test time. We move the unsupervised discovery phase during the training stage, while at test time we apply the standard feed-forward processing along the student pathway. This has a dual benefit: firstly, it allows in principle unlimited possibilities of learning and generalization during training, while remaining very fast at testing. Secondly, the student not only becomes able to detect in single images significantly better than its unsupervised video discovery teacher, but it also achieves state of the art results on two important current benchmarks, YouTube Objects and Object Discovery datasets. Moreover, at test time, our system is at least two orders of magnitude faster than other previous methods.|unsupervis learn visual data one difficult challeng comput vision fundament task understand visual recognit work practic point view learn unsupervis visual input immens practic valu veri larg quantiti unlabel video collect low cost paper address task unsupervis learn detect segment foreground object singl imag achiev goal train student pathway consist deep neural network learn predict singl input imag video frame output particular frame teacher pathway perform unsupervis object discoveri video approach differ publish literatur perform unsupervis discoveri video collect imag test time move unsupervis discoveri phase dure train stage test time appli standard feed forward process along student pathway dual benefit first allow principl unlimit possibl learn general dure train remain veri fast test second student onli becom abl detect singl imag signific better unsupervis video discoveri teacher also achiev state art result two import current benchmark youtub object object discoveri dataset moreov test time system least two order magnitud faster previous method|['Ioana Croitoru', 'Simion-Vlad Bogolin', 'Marius Leordeanu']|['cs.CV']
2017-04-07T11:30:52Z|2017-03-31T13:59:31Z|http://arxiv.org/abs/1703.10898v1|http://arxiv.org/pdf/1703.10898v1|Thin-Slicing Network: A Deep Structured Model for Pose Estimation in   Videos|thin slice network deep structur model pose estim video|Deep ConvNets have been shown to be effective for the task of human pose estimation from single images. However, several challenging issues arise in the video-based case such as self-occlusion, motion blur, and uncommon poses with few or no examples in training data sets. Temporal information can provide additional cues about the location of body joints and help to alleviate these issues. In this paper, we propose a deep structured model to estimate a sequence of human poses in unconstrained videos. This model can be efficiently trained in an end-to-end manner and is capable of representing appearance of body joints and their spatio-temporal relationships simultaneously. Domain knowledge about the human body is explicitly incorporated into the network providing effective priors to regularize the skeletal structure and to enforce temporal consistency. The proposed end-to-end architecture is evaluated on two widely used benchmarks (Penn Action dataset and JHMDB dataset) for video-based pose estimation. Our approach significantly outperforms the existing state-of-the-art methods.|deep convnet shown effect task human pose estim singl imag howev sever challeng issu aris video base case self occlus motion blur uncommon pose exampl train data set tempor inform provid addit cue locat bodi joint help allevi issu paper propos deep structur model estim sequenc human pose unconstrain video model effici train end end manner capabl repres appear bodi joint spatio tempor relationship simultan domain knowledg human bodi explicit incorpor network provid effect prior regular skelet structur enforc tempor consist propos end end architectur evalu two wide use benchmark penn action dataset jhmdb dataset video base pose estim approach signific outperform exist state art method|['Jie Song', 'Limin Wang', 'Luc Van Gool', 'Otmar Hilliges']|['cs.CV']
2017-04-07T11:30:52Z|2017-03-31T13:56:35Z|http://arxiv.org/abs/1703.10896v1|http://arxiv.org/pdf/1703.10896v1|BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for   Predicting the 3D Poses of Challenging Objects without Using Depth|bb scalabl accur robust partial occlus method predict pose challeng object without use depth|"We introduce a novel method for 3D object detection and pose estimation from color images only. We first use segmentation to detect the objects of interest in 2D even in presence of partial occlusions and cluttered background. By contrast with recent patch-based methods, we rely on a ""holistic"" approach: We apply to the detected objects a Convolutional Neural Network (CNN) trained to predict their 3D poses in the form of 2D projections of the corners of their 3D bounding boxes for the pose of objects' parts. This, however, is not sufficient for handling objects from the recent T-LESS dataset: These objects exhibit an axis of rotational symmetry, and the similarity of two images of such an object under two different poses makes training the CNN challenging. We solve this problem by restricting the range of poses used for training, and by introducing a classifier to identify the range of a pose at run-time before estimating it. We also use an optional additional step that refines the predicted poses for hand pose estimation. We improve the state-of-the-art on the LINEMOD dataset from 73.7% to 89.3% of correctly registered RGB frames. We are also the first to report results on the Occlusion dataset using color images only. We obtain 54% of frames passing the Pose 6D criterion on average on several sequences of the T-LESS dataset, compared to the 67% of the state-of-the-art on the same sequences which uses both color and depth. The full approach is also scalable, as a single network can be trained for multiple objects simultaneously."|introduc novel method object detect pose estim color imag onli first use segment detect object interest even presenc partial occlus clutter background contrast recent patch base method reli holist approach appli detect object convolut neural network cnn train predict pose form project corner bound box pose object part howev suffici handl object recent less dataset object exhibit axi rotat symmetri similar two imag object two differ pose make train cnn challeng solv problem restrict rang pose use train introduc classifi identifi rang pose run time befor estim also use option addit step refin predict pose hand pose estim improv state art linemod dataset correct regist rgb frame also first report result occlus dataset use color imag onli obtain frame pass pose criterion averag sever sequenc less dataset compar state art sequenc use color depth full approach also scalabl singl network train multipl object simultan|['Mahdi Rad', 'Vincent Lepetit']|['cs.CV']
