2017-03-28T14:06:40Z|2017-03-25T22:50:20Z|http://arxiv.org/abs/1703.08746v1|http://arxiv.org/pdf/1703.08746v1|Proof Verification Can Be Hard!|proof verif hard|The generally accepted wisdom in computational circles is that pure proof verification is a solved problem and that the computationally hard elements and fertile areas of study lie in proof discovery. This wisdom presumably does hold for conventional proof systems such as first-order logic with a standard proof calculus such as natural deduction or resolution. But this folk belief breaks down when we consider more user-friendly/powerful inference rules. One such rule is the restricted {\omega}-rule, which is not even semi-decidable when added to a standard proof calculus of a nice theory. While presumably not a novel result, we feel that the hardness of proof verification is under-appreciated in most communities that deal with proofs. A proof-sketch follows.|general accept wisdom comput circl pure proof verif solv problem comput hard element fertil area studi lie proof discoveri wisdom presum doe hold convent proof system first order logic standard proof calculus natur deduct resolut folk belief break consid user friend power infer rule one rule restrict omega rule even semi decid ad standard proof calculus nice theori presum novel result feel hard proof verif appreci communiti deal proof proof sketch follow|['Naveen Sundar Govindarajulu', 'Selmer Bringsjord']|['cs.LO', 'cs.CC']
2017-03-28T14:06:40Z|2017-03-23T16:50:03Z|http://arxiv.org/abs/1703.08139v1|http://arxiv.org/pdf/1703.08139v1|Optimal lower bounds for universal relation, samplers, and finding   duplicates|optim lower bound univers relat sampler find duplic|In the communication problem $\mathbf{UR}$ (universal relation) [KRW95], Alice and Bob respectively receive $x$ and $y$ in $\{0,1\}^n$ with the promise that $x\neq y$. The last player to receive a message must output an index $i$ such that $x_i\neq y_i$. We prove that the randomized one-way communication complexity of this problem in the public coin model is exactly $\Theta(\min\{n, \log(1/\delta)\log^2(\frac{n}{\log(1/\delta)})\})$ bits for failure probability $\delta$. Our lower bound holds even if promised $\mathop{support}(y)\subset \mathop{support}(x)$. As a corollary, we obtain optimal lower bounds for $\ell_p$-sampling in strict turnstile streams for $0\le p < 2$, as well as for the problem of finding duplicates in a stream. Our lower bounds do not need to use large weights, and hold even if it is promised that $x\in\{0,1\}^n$ at all points in the stream.   Our lower bound demonstrates that any algorithm $\mathcal{A}$ solving sampling problems in turnstile streams in low memory can be used to encode subsets of $[n]$ of certain sizes into a number of bits below the information theoretic minimum. Our encoder makes adaptive queries to $\mathcal{A}$ throughout its execution, but done carefully so as to not violate correctness. This is accomplished by injecting random noise into the encoder's interactions with $\mathcal{A}$, which is loosely motivated by techniques in differential privacy. Our correctness analysis involves understanding the ability of $\mathcal{A}$ to correctly answer adaptive queries which have positive but bounded mutual information with $\mathcal{A}$'s internal randomness, and may be of independent interest in the newly emerging area of adaptive data analysis with a theoretical computer science lens.|communic problem mathbf ur univers relat krw alic bob respect receiv promis neq last player receiv messag must output index neq prove random one way communic complex problem public coin model exact theta min log delta log frac log delta bit failur probabl delta lower bound hold even promis mathop support subset mathop support corollari obtain optim lower bound ell sampl strict turnstil stream le well problem find duplic stream lower bound need use larg weight hold even promis point stream lower bound demonstr ani algorithm mathcal solv sampl problem turnstil stream low memori use encod subset certain size number bit inform theoret minimum encod make adapt queri mathcal throughout execut done care violat correct accomplish inject random nois encod interact mathcal loos motiv techniqu differenti privaci correct analysi involv understand abil mathcal correct answer adapt queri posit bound mutual inform mathcal intern random may independ interest newli emerg area adapt data analysi theoret comput scienc len|['Jelani Nelson', 'Jakub Pachocki', 'Zhengyu Wang']|['cs.CC', 'cs.DS']
2017-03-28T14:06:40Z|2017-03-23T00:00:41Z|http://arxiv.org/abs/1703.07891v1|http://arxiv.org/pdf/1703.07891v1|Width Hierarchies for Quantum and Classical Ordered Binary Decision   Diagrams with Repeated Test|width hierarchi quantum classic order binari decis diagram repeat test|We consider quantum, nondterministic and probabilistic versions of known computational model Ordered Read-$k$-times Branching Programs or Ordered Binary Decision Diagrams with repeated test ($k$-QOBDD, $k$-NOBDD and $k$-POBDD). We show width hierarchy for complexity classes of Boolean function computed by these models and discuss relation between different variants of $k$-OBDD.|consid quantum nondterminist probabilist version known comput model order read time branch program order binari decis diagram repeat test qobdd nobdd pobdd show width hierarchi complex class boolean function comput model discuss relat differ variant obdd|['Kamil Khadiev', 'Rishat Ibrahimov']|['cs.CC', 'quant-ph']
2017-03-28T14:06:40Z|2017-03-22T19:56:27Z|http://arxiv.org/abs/1703.07833v1|http://arxiv.org/pdf/1703.07833v1|Information complexity of the AND function in the two-Party, and   multiparty settings|inform complex function two parti multiparti set|In a recent breakthrough paper [M. Braverman, A. Garg, D. Pankratov, and O. Weinstein, From information to exact communication, STOC'13] Braverman et al. developed a local characterization for the zero-error information complexity in the two party model, and used it to compute the exact internal and external information complexity of the 2-bit AND function, which was then applied to determine the exact asymptotic of randomized communication complexity of the set disjointness problem.   In this article, we extend their results on AND function to the multi-party number-in-hand model by proving that the generalization of their protocol has optimal internal and external information cost for certain distributions. Our proof has new components, and in particular it fixes some minor gaps in the proof of Braverman et al.|recent breakthrough paper braverman garg pankratov weinstein inform exact communic stoc braverman et al develop local character zero error inform complex two parti model use comput exact intern extern inform complex bit function appli determin exact asymptot random communic complex set disjoint problem articl extend result function multi parti number hand model prove general protocol optim intern extern inform cost certain distribut proof new compon particular fix minor gap proof braverman et al|['Yuval Filmus', 'Hamed Hatami', 'Yaqiao Li', 'Suzin You']|['cs.CC']
2017-03-28T14:06:40Z|2017-03-23T07:30:19Z|http://arxiv.org/abs/1703.07768v2|http://arxiv.org/pdf/1703.07768v2|Quantum Communication-Query Tradeoffs|quantum communic queri tradeoff|For any function $f: X \times Y \to Z$, we prove that $Q^{*\text{cc}}(f) \cdot Q^{\text{OIP}}(f) \cdot (\log Q^{\text{OIP}}(f) + \log  Z ) \geq \Omega(\log  X )$. Here, $Q^{*\text{cc}}(f)$ denotes the bounded-error communication complexity of $f$ using an entanglement-assisted two-way qubit channel, and $Q^{\text{OIP}}(f)$ denotes the number of quantum queries needed to determine $x$ with high probability given oracle access to the function $f_x(y) \stackrel{\text{def}}{=} f(x, y)$. We show that this tradeoff is close to the best possible. We also give a generalization of this tradeoff for distributional query complexity.   As an application, we prove an optimal $\Omega(\log q)$ lower bound on the $Q^{*\text{cc}}$ complexity of determining whether $x + y$ is a perfect square, where Alice holds $x \in \mathbf{F}_q$, Bob holds $y \in \mathbf{F}_q$, and $\mathbf{F}_q$ is a finite field of odd characteristic. As another application, we give a new, simpler proof that searching an ordered size-$N$ database requires $\Omega(\log N / \log \log N)$ quantum queries. (It was already known that $\Theta(\log N)$ queries are required.)|ani function time prove text cc cdot text oip cdot log text oip log geq omega log text cc denot bound error communic complex use entangl assist two way qubit channel text oip denot number quantum queri need determin high probabl given oracl access function stackrel text def show tradeoff close best possibl also give general tradeoff distribut queri complex applic prove optim omega log lower bound text cc complex determin whether perfect squar alic hold mathbf bob hold mathbf mathbf finit field odd characterist anoth applic give new simpler proof search order size databas requir omega log log log quantum queri alreadi known theta log queri requir|['William M. Hoza']|['cs.CC', 'quant-ph']
2017-03-28T14:06:40Z|2017-03-22T14:14:35Z|http://arxiv.org/abs/1703.07666v1|http://arxiv.org/pdf/1703.07666v1|Query-to-Communication Lifting for BPP|queri communic lift bpp|For any $n$-bit boolean function $f$, we show that the randomized communication complexity of the composed function $f\circ g^n$, where $g$ is an index gadget, is characterized by the randomized decision tree complexity of $f$. In particular, this means that many query complexity separations involving randomized models (e.g., classical vs. quantum) automatically imply analogous separations in communication complexity.|ani bit boolean function show random communic complex compos function circ index gadget character random decis tree complex particular mean mani queri complex separ involv random model classic vs quantum automat impli analog separ communic complex|['Mika Göös', 'Toniann Pitassi', 'Thomas Watson']|['cs.CC']
2017-03-28T14:06:40Z|2017-03-23T20:55:10Z|http://arxiv.org/abs/1703.07657v2|http://arxiv.org/pdf/1703.07657v2|"A Counterexample to the ""Majority is Least Stable"" Conjecture"|counterexampl major least stabl conjectur|"We exhibit a linear threshold function in 5 variables with strictly smaller noise stability (for small values of the correlation parameter) than the majority function on 5 variables, thereby providing a counterexample to the ""Majority is Least Stable"" Conjecture of Benjamini, Kalai, and Schramm."|exhibit linear threshold function variabl strict smaller nois stabil small valu correl paramet major function variabl therebi provid counterexampl major least stabl conjectur benjamini kalai schramm|['Vishesh Jain']|['cs.CC', 'math.PR']
2017-03-28T14:06:40Z|2017-03-22T04:45:51Z|http://arxiv.org/abs/1703.07521v1|http://arxiv.org/pdf/1703.07521v1|Lifting randomized query complexity to randomized communication   complexity|lift random queri complex random communic complex|We show that for any (partial) query function $f:\{0,1\}^n\rightarrow \{0,1\}$, the randomized communication complexity of $f$ composed with $\mathrm{Index}^n_m$ (with $m= \mathrm{poly}(n)$) is at least the randomized query complexity of $f$ times $\log n$. Here $\mathrm{Index}_m : [m] \times \{0,1\}^m \rightarrow \{0,1\}$ is defined as $\mathrm{Index}_m(x,y)= y_x$ (the $x$th bit of $y$).   Our proof follows on the lines of Raz and Mckenzie [RM99] (and its generalization due to [GPW15]), who showed a lifting theorem for deterministic query complexity to deterministic communication complexity. Our proof deviates from theirs in an important fashion that we consider partitions of rectangles into many sub-rectangles, as opposed to a particular sub-rectangle with desirable properties, as considered by Raz and McKenzie. As a consequence of our main result, some known separations between quantum and classical communication complexities follow from analogous separations in the query world.|show ani partial queri function rightarrow random communic complex compos mathrm index mathrm poli least random queri complex time log mathrm index time rightarrow defin mathrm index th bit proof follow line raz mckenzi rm general due gpw show lift theorem determinist queri complex determinist communic complex proof deviat import fashion consid partit rectangl mani sub rectangl oppos particular sub rectangl desir properti consid raz mckenzi consequ main result known separ quantum classic communic complex follow analog separ queri world|['Anurag Anshu', 'Naresh B. Goud', 'Rahul Jain', 'Srijita Kundu', 'Priyanka Mukhopadhyay']|['cs.CC', 'quant-ph']
2017-03-28T14:06:40Z|2017-03-21T19:48:23Z|http://arxiv.org/abs/1703.07406v1|http://arxiv.org/pdf/1703.07406v1|Subset sum problem in polycyclic groups|subset sum problem polycycl group|We consider a group-theoretic analogue of the classic subset sum problem. It is known that every virtually nilpotent group has polynomial time decidable subset sum problem. In this paper we use subgroup distortion to show that every polycyclic non-virtually-nilpotent group has NP-complete subset sum problem.|consid group theoret analogu classic subset sum problem known everi virtual nilpot group polynomi time decid subset sum problem paper use subgroup distort show everi polycycl non virtual nilpot group np complet subset sum problem|['Andrey Nikolaev', 'Alexander Ushakov']|['math.GR', 'cs.CC', 'math.CO', '03D15, 20F65, 20F10, 20F16']
2017-03-28T14:06:40Z|2017-03-21T12:48:28Z|http://arxiv.org/abs/1703.07184v1|http://arxiv.org/pdf/1703.07184v1|Exact Affine OBDDs|exact affin obdd|We introduce affine OBDD model and we show that exact affine OBDDs can be exponentially narrower than bounded-error quantum and classical OBDDs on some certain problems. Moreover, we consider Las-Vegas quantum and classical automata models and improve the previous gap between deterministic and probabilistic models by a factor of 2 and then follow the same gap for the known most restricted quantum model. Lastly, we follow an exponential gap between exact affine finite automata and Las-Vegas classical and quantum models.|introduc affin obdd model show exact affin obdd exponenti narrow bound error quantum classic obdd certain problem moreov consid las vega quantum classic automata model improv previous gap determinist probabilist model factor follow gap known restrict quantum model last follow exponenti gap exact affin finit automata las vega classic quantum model|['Rishat Ibrahimov', 'Kamil Khadiev', 'Abuzer Yakaryilmaz']|['cs.CC', 'cs.FL', 'quant-ph']
2017-03-28T14:06:44Z|2017-03-23T15:17:23Z|http://arxiv.org/abs/1703.06983v2|http://arxiv.org/pdf/1703.06983v2|Collapsibility to a subcomplex of a given dimension is NP-complete|collaps subcomplex given dimens np complet|In this paper we extend the works of Tancer and of Malgouyres and Franc\'es, showing that $(d,k)$-collapsibility is NP-complete for $d\geq k+2$ except $(2,0)$. By $(d,k)$-collapsibility we mean the following problem: determine whether a given $d$-dimensional simplicial complex can be collapsed to some $k$-dimensional subcomplex. The question of establishing the complexity status of $(d,k)$-collapsibility was asked by Tancer, who proved NP-completeness of $(d,0)$ and $(d,1)$-collapsibility (for $d\geq 3$). Our extended result, together with the known polynomial-time algorithms for $(2,0)$ and $d=k+1$, answers the question completely.|paper extend work tancer malgouyr franc es show collaps np complet geq except collaps mean follow problem determin whether given dimension simplici complex collaps dimension subcomplex question establish complex status collaps ask tancer prove np complet collaps geq extend result togeth known polynomi time algorithm answer question complet|['Giovanni Paolini']|['cs.CG', 'cs.CC', 'math.GT']
2017-03-28T14:06:44Z|2017-03-19T11:24:26Z|http://arxiv.org/abs/1703.06423v1|http://arxiv.org/pdf/1703.06423v1|The Hardness of Embedding Grids and Walls|hard embed grid wall|"The dichotomy conjecture for the parameterized embedding problem states that the problem of deciding whether a given graph $G$ from some class $K$ of ""pattern graphs"" can be embedded into a given graph $H$ (that is, is isomorphic to a subgraph of $H$) is fixed-parameter tractable if $K$ is a class of graphs of bounded tree width and $W[1]$-complete otherwise.   Towards this conjecture, we prove that the embedding problem is $W[1]$-complete if $K$ is the class of all grids or the class of all walls."|dichotomi conjectur parameter embed problem state problem decid whether given graph class pattern graph embed given graph isomorph subgraph fix paramet tractabl class graph bound tree width complet otherwis toward conjectur prove embed problem complet class grid class wall|['Yijia Chen', 'Martin Grohe', 'Bingkai Lin']|['cs.CC']
2017-03-28T14:06:44Z|2017-03-17T17:43:59Z|http://arxiv.org/abs/1703.06127v1|http://arxiv.org/pdf/1703.06127v1|Tusnády's problem, the transference principle, and non-uniform QMC   sampling|tusn dy problem transfer principl non uniform qmc sampl|It is well-known that for every $N \geq 1$ and $d \geq 1$ there exist point sets $x_1, \dots, x_N \in [0,1]^d$ whose discrepancy with respect to the Lebesgue measure is of order at most $(\log N)^{d-1} N^{-1}$. In a more general setting, the first author proved together with Josef Dick that for any normalized measure $\mu$ on $[0,1]^d$ there exist points $x_1, \dots, x_N$ whose discrepancy with respect to $\mu$ is of order at most $(\log N)^{(3d+1)/2} N^{-1}$. The proof used methods from combinatorial mathematics, and in particular a result of Banaszczyk on balancings of vectors. In the present note we use a version of the so-called transference principle together with recent results on the discrepancy of red-blue colorings to show that for any $\mu$ there even exist points having discrepancy of order at most $(\log N)^{d-\frac12} N^{-1}$, which is almost as good as the discrepancy bound in the case of the Lebesgue measure.|well known everi geq geq exist point set dot whose discrep respect lebesgu measur order log general set first author prove togeth josef dick ani normal measur mu exist point dot whose discrep respect mu order log proof use method combinatori mathemat particular result banaszczyk balanc vector present note use version call transfer principl togeth recent result discrep red blue color show ani mu even exist point discrep order log frac almost good discrep bound case lebesgu measur|['Christoph Aistleitner', 'Dmitriy Bilyk', 'Aleksandar Nikolov']|['math.CO', 'cs.CC', 'math.NA', 'math.PR']
2017-03-28T14:06:44Z|2017-03-17T15:08:17Z|http://arxiv.org/abs/1703.06048v1|http://arxiv.org/pdf/1703.06048v1|An FPTAS for the Knapsack Problem with Parametric Weights|fptas knapsack problem parametr weight|In this paper, we investigate the parametric weight knapsack problem, in which the item weights are affine functions of the form $w_i(\lambda) = a_i + \lambda \cdot b_i$ for $i \in \{1,\ldots,n\}$ depending on a real-valued parameter $\lambda$. The aim is to provide a solution for all values of the parameter. It is well-known that any exact algorithm for the problem may need to output an exponential number of knapsack solutions. We present the first fully polynomial-time approximation scheme (FPTAS) for the problem that, for any desired precision $\varepsilon \in (0,1)$, computes $(1-\varepsilon)$-approximate solutions for all values of the parameter. Our FPTAS is based on two different approaches and achieves a running time of $\mathcal{O}(n^3/\varepsilon^2 \cdot \min\{ \log^2 P, n^2 \} \cdot \min\{\log M, n \log (n/\varepsilon) / \log(n \log (n/\varepsilon) )\})$ where $P$ is an upper bound on the optimal profit and $M := \max\{W, n \cdot \max\{a_i,b_i: i \in \{1,\ldots,n\}\}\}$ for a knapsack with capacity $W$.|paper investig parametr weight knapsack problem item weight affin function form lambda lambda cdot ldot depend real valu paramet lambda aim provid solut valu paramet well known ani exact algorithm problem may need output exponenti number knapsack solut present first fulli polynomi time approxim scheme fptas problem ani desir precis varepsilon comput varepsilon approxim solut valu paramet fptas base two differ approach achiev run time mathcal varepsilon cdot min log cdot min log log varepsilon log log varepsilon upper bound optim profit max cdot max ldot knapsack capac|['Michael Holzhauser', 'Sven O. Krumke']|['cs.DS', 'cs.CC', 'math.OC']
2017-03-28T14:06:44Z|2017-03-17T03:58:22Z|http://arxiv.org/abs/1703.05881v1|http://arxiv.org/pdf/1703.05881v1|Complexity of Correspondence Homomorphisms|complex correspond homomorph|Correspondence homomorphisms are both a generalization of standard homomorphisms and a generalization of correspondence colourings. For a fixed target graph $H$, the problem is to decide whether an input graph $G$, with each edge labeled by a pair of permutations of $V(H)$, admits a homomorphism to $H$ 'corresponding' to the labels, in a sense explained below.   We classify the complexity of this problem as a function of the fixed graph $H$. It turns out that there is dichotomy -- each of the problems is polynomial-time solvable or NP-complete. While most graphs $H$ yield NP-complete problems, there are interesting cases of graphs $H$ for which we solve the problem by Gaussian elimination.   We also classify the complexity of the analogous correspondence list homomorphism problems.   In this note we only include the proofs for the case $H$ is reflexive.|correspond homomorph general standard homomorph general correspond colour fix target graph problem decid whether input graph edg label pair permut admit homomorph correspond label sens explain classifi complex problem function fix graph turn dichotomi problem polynomi time solvabl np complet graph yield np complet problem interest case graph solv problem gaussian elimin also classifi complex analog correspond list homomorph problem note onli includ proof case reflex|['Tomas Feder', 'Pavol Hell']|['cs.DM', 'cs.CC', 'math.CO']
2017-03-28T14:06:44Z|2017-03-16T18:18:51Z|http://arxiv.org/abs/1703.05784v1|http://arxiv.org/pdf/1703.05784v1|A Nearly Optimal Lower Bound on the Approximate Degree of AC$^0$|near optim lower bound approxim degre ac|The approximate degree of a Boolean function $f \colon \{-1, 1\}^n \rightarrow \{-1, 1\}$ is the least degree of a real polynomial that approximates $f$ pointwise to error at most $1/3$. We introduce a generic method for increasing the approximate degree of a given function, while preserving its computability by constant-depth circuits.   Specifically, we show how to transform any Boolean function $f$ with approximate degree $d$ into a function $F$ on $O(n \cdot \operatorname{polylog}(n))$ variables with approximate degree at least $D = \Omega(n^{1/3} \cdot d^{2/3})$. In particular, if $d= n^{1-\Omega(1)}$, then $D$ is polynomially larger than $d$. Moreover, if $f$ is computed by a polynomial-size Boolean circuit of constant depth, then so is $F$.   By recursively applying our transformation, for any constant $\delta > 0$ we exhibit an AC$^0$ function of approximate degree $\Omega(n^{1-\delta})$. This improves over the best previous lower bound of $\Omega(n^{2/3})$ due to Aaronson and Shi (J. ACM 2004), and nearly matches the trivial upper bound of $n$ that holds for any function. Our lower bounds also apply to (quasipolynomial-size) DNFs of polylogarithmic width.   We describe several applications of these results. We give:   * For any constant $\delta > 0$, an $\Omega(n^{1-\delta})$ lower bound on the quantum communication complexity of a function in AC$^0$.   * A Boolean function $f$ with approximate degree at least $C(f)^{2-o(1)}$, where $C(f)$ is the certificate complexity of $f$. This separation is optimal up to the $o(1)$ term in the exponent.   * Improved secret sharing schemes with reconstruction procedures in AC$^0$.|approxim degre boolean function colon rightarrow least degre real polynomi approxim pointwis error introduc generic method increas approxim degre given function preserv comput constant depth circuit specif show transform ani boolean function approxim degre function cdot operatornam polylog variabl approxim degre least omega cdot particular omega polynomi larger moreov comput polynomi size boolean circuit constant depth recurs appli transform ani constant delta exhibit ac function approxim degre omega delta improv best previous lower bound omega due aaronson shi acm near match trivial upper bound hold ani function lower bound also appli quasipolynomi size dnfs polylogarithm width describ sever applic result give ani constant delta omega delta lower bound quantum communic complex function ac boolean function approxim degre least certif complex separ optim term expon improv secret share scheme reconstruct procedur ac|['Mark Bun', 'Justin Thaler']|['cs.CC']
2017-03-28T14:06:44Z|2017-03-15T18:00:05Z|http://arxiv.org/abs/1703.05332v1|http://arxiv.org/pdf/1703.05332v1|Complexity of sampling as an order parameter|complex sampl order paramet|We consider the classical complexity of approximately simulating time evolution under spatially local quadratic bosonic Hamiltonians for time $t$. We obtain upper and lower bounds on the scaling of $t$ with the number of bosons, $n$, for which simulation, cast as a sampling problem, is classically efficient and provably hard, respectively. We view these results in the light of classifying phases of physical systems based on parameters in the Hamiltonian and conjecture a link to dynamical phase transitions. In doing so, we combine ideas from mathematical physics and computational complexity to gain insight into the behavior of condensed matter systems.|consid classic complex approxim simul time evolut spatial local quadrat boson hamiltonian time obtain upper lower bound scale number boson simul cast sampl problem classic effici provabl hard respect view result light classifi phase physic system base paramet hamiltonian conjectur link dynam phase transit combin idea mathemat physic comput complex gain insight behavior condens matter system|['Abhinav Deshpande', 'Bill Fefferman', 'Michael Foss-Feig', 'Alexey V. Gorshkov']|['quant-ph', 'cond-mat.quant-gas', 'cs.CC']
2017-03-28T14:06:44Z|2017-03-15T14:21:40Z|http://arxiv.org/abs/1703.05170v1|http://arxiv.org/pdf/1703.05170v1|Busy beavers and Kolmogorov complexity|busi beaver kolmogorov complex|"The idea to find the ""maximal number that can be named"" can be traced back to Archimedes (see his Psammit). From the viewpoint of computation theory the natural question is ""which number can be described by at most n bits""? This question led to the definition of the so-called ""busy beaver"" numbers (introduced by T. Rado). In this note we consider different versions of the busy beaver-like notions defined in terms of Kolmogorov complexity. We show that these versions differ depending on the version of complexity used (plain, prefix, or a priori complexities) and find out how these notions are related, providing matching lower and upper bounds."|idea find maxim number name trace back archimed see psammit viewpoint comput theori natur question number describ bit question led definit call busi beaver number introduc rado note consid differ version busi beaver like notion defin term kolmogorov complex show version differ depend version complex use plain prefix priori complex find notion relat provid match lower upper bound|['Mikhail Andreev']|['cs.CC']
2017-03-28T14:06:44Z|2017-03-15T13:51:23Z|http://arxiv.org/abs/1703.05156v1|http://arxiv.org/pdf/1703.05156v1|Complexity Dichotomies for the Minimum F-Overlay Problem|complex dichotomi minimum overlay problem|For a (possibly infinite) fixed family of graphs F, we say that a graph G overlays F on a hypergraph H if V(H) is equal to V(G) and the subgraph of G induced by every hyperedge of H contains some member of F as a spanning subgraph.While it is easy to see that the complete graph on  V(H)  overlays F on a hypergraph H whenever the problem admits a solution, the Minimum F-Overlay problem asks for such a graph with the minimum number of edges.This problem allows to generalize some natural problems which may arise in practice. For instance, if the family F contains all connected graphs, then Minimum F-Overlay corresponds to the Minimum Connectivity Inference problem (also known as Subset Interconnection Design problem) introduced for the low-resolution reconstruction of macro-molecular assembly in structural biology, or for the design of networks.Our main contribution is a strong dichotomy result regarding the polynomial vs. NP-hard status with respect to the considered family F. Roughly speaking, we show that the easy cases one can think of (e.g. when edgeless graphs of the right sizes are in F, or if F contains only cliques) are the only families giving rise to a polynomial problem: all others are NP-complete.We then investigate the parameterized complexity of the problem and give similar sufficient conditions on F that give rise to W[1]-hard, W[2]-hard or FPT problems when the parameter is the size of the solution.This yields an FPT/W[1]-hard dichotomy for a relaxed problem, where every hyperedge of H must contain some member of F as a (non necessarily spanning) subgraph.|possibl infinit fix famili graph say graph overlay hypergraph equal subgraph induc everi hyperedg contain member span subgraph easi see complet graph overlay hypergraph whenev problem admit solut minimum overlay problem ask graph minimum number edg problem allow general natur problem may aris practic instanc famili contain connect graph minimum overlay correspond minimum connect infer problem also known subset interconnect design problem introduc low resolut reconstruct macro molecular assembl structur biolog design network main contribut strong dichotomi result regard polynomi vs np hard status respect consid famili rough speak show easi case one think edgeless graph right size contain onli cliqu onli famili give rise polynomi problem np complet investig parameter complex problem give similar suffici condit give rise hard hard fpt problem paramet size solut yield fpt hard dichotomi relax problem everi hyperedg must contain member non necessarili span subgraph|['Nathann Cohen', 'Frédéric Havet', 'Dorian Mazauric', 'Ignasi Sau', 'Rémi Watrigant']|['cs.DS', 'cs.CC']
2017-03-28T14:06:44Z|2017-03-15T08:54:12Z|http://arxiv.org/abs/1703.05015v1|http://arxiv.org/pdf/1703.05015v1|Lower Bound and Hierarchies for Quantum Ordered Read-$k$-times Branching   Programs|lower bound hierarchi quantum order read time branch program|We consider quantum version of known computational model Ordered Read-$k$-times Branching Programs or Ordered Binary Decision Diagrams with repeated test ($k$-QOBDD). We get lower bound for quantum $k$-OBDD for $k=o(\sqrt{n})$. This lower bound gives connection between characteristics of model and number of subfunctions for function.   Additionally, we prove the hierarchies for sublinear width bounded error quantum $k$-OBDDs using our lower bounds for $ k=o(\sqrt{n})$. Also we prove hierarchy for polynomial size bounded error quantum $k$-OBDDs constant $k$, and it differs from situation with unbounded error where known that increasing of $k$ does not gives any advantages.   Finally, we discuss relations between different classical and quantum models of $k$-OBDD.|consid quantum version known comput model order read time branch program order binari decis diagram repeat test qobdd get lower bound quantum obdd sqrt lower bound give connect characterist model number subfunct function addit prove hierarchi sublinear width bound error quantum obdd use lower bound sqrt also prove hierarchi polynomi size bound error quantum obdd constant differ situat unbound error known increas doe give ani advantag final discuss relat differ classic quantum model obdd|['Farid Ablayev', 'Kamil Khadiev', 'Aliya Khadieva']|['cs.CC', 'quant-ph']
2017-03-28T14:06:48Z|2017-03-15T05:43:48Z|http://arxiv.org/abs/1703.04940v1|http://arxiv.org/pdf/1703.04940v1|Resilience: A Criterion for Learning in the Presence of Arbitrary   Outliers|resili criterion learn presenc arbitrari outlier|We introduce a criterion, resilience, which allows properties of a dataset (such as its mean or best low rank approximation) to be robustly computed, even in the presence of a large fraction of arbitrary additional data. Resilience is a weaker condition than most other properties considered so far in the literature, and yet enables robust estimation in a broader variety of settings, including the previously unstudied problem of robust mean estimation in $\ell_p$-norms.|introduc criterion resili allow properti dataset mean best low rank approxim robust comput even presenc larg fraction arbitrari addit data resili weaker condit properti consid far literatur yet enabl robust estim broader varieti set includ previous unstudi problem robust mean estim ell norm|['Jacob Steinhardt', 'Moses Charikar', 'Gregory Valiant']|['cs.LG', 'cs.AI', 'cs.CC', 'cs.CR', 'stat.ML']
2017-03-28T14:06:48Z|2017-03-14T17:22:19Z|http://arxiv.org/abs/1703.04598v1|http://arxiv.org/pdf/1703.04598v1|Verification in Staged Tile Self-Assembly|verif stage tile self assembl|We prove the unique assembly and unique shape verification problems, benchmark measures of self-assembly model power, are $\mathrm{coNP}^{\mathrm{NP}}$-hard and contained in $\mathrm{PSPACE}$ (and in $\mathrm{\Pi}^\mathrm{P}_{2s}$ for staged systems with $s$ stages). En route, we prove that unique shape verification problem in the 2HAM is $\mathrm{coNP}^{\mathrm{NP}}$-complete.|prove uniqu assembl uniqu shape verif problem benchmark measur self assembl model power mathrm conp mathrm np hard contain mathrm pspace mathrm pi mathrm stage system stage en rout prove uniqu shape verif problem ham mathrm conp mathrm np complet|['Robert Schweller', 'Andrew Winslow', 'Tim Wylie']|['cs.CC']
2017-03-28T14:06:48Z|2017-03-13T15:47:16Z|http://arxiv.org/abs/1703.04456v1|http://arxiv.org/pdf/1703.04456v1|P=?NP as minimization of degree 4 polynomial, or Grassmann number   problem|np minim degre polynomi grassmann number problem|While the P vs NP problem is mainly being attacked form the point of view of discrete mathematics, this paper propses two reformulations into the field of abstract algebra and of continuous global optimization - which advanced tools might bring new perspectives and approaches to attack this problem. The first one is equivalence of satisfying the 3-SAT problem with the question of reaching zero of a nonnegative degree 4 multivariate polynomial. This continuous search between boolean 0 and 1 values could be attacked using methods of global optimization, suggesting exponential growth of the number of local minima, what might be also a crucial issue for example for adiabatic quantum computers. The second discussed approach is using anti-commuting Grassmann numbers $\theta_i$, making $(A \cdot \textrm{diag}(\theta_i))^n$ nonzero only if $A$ has a Hamilton cycle. Hence, the P$\ne$NP assumption implies exponential growth of matrix representation of Grassmann numbers.|vs np problem main attack form point view discret mathemat paper props two reformul field abstract algebra continu global optim advanc tool might bring new perspect approach attack problem first one equival satisfi sat problem question reach zero nonneg degre multivari polynomi continu search boolean valu could attack use method global optim suggest exponenti growth number local minima might also crucial issu exampl adiabat quantum comput second discuss approach use anti commut grassmann number theta make cdot textrm diag theta nonzero onli hamilton cycl henc ne np assumpt impli exponenti growth matrix represent grassmann number|['Jarek Duda']|['cs.CC']
2017-03-28T14:06:48Z|2017-03-13T09:26:05Z|http://arxiv.org/abs/1703.04300v1|http://arxiv.org/pdf/1703.04300v1|A Note on the Inapproximability of Induced Disjoint Paths|note inapproxim induc disjoint path|We study the inapproximability of the induced disjoint paths problem on an arbitrary $n$-node $m$-edge undirected graph, which is to connect the maximum number of the $k$ source-sink pairs given in the graph via induced disjoint paths. It is known that the problem is NP-hard to approximate within $m^{{1\over 2}-\varepsilon}$ for a general $k$ and any $\varepsilon>0$. In this paper, we prove that the problem is NP-hard to approximate within $n^{1-\varepsilon}$ for a general $k$ and any $\varepsilon>0$ by giving a simple reduction from the independent set problem.|studi inapproxim induc disjoint path problem arbitrari node edg undirect graph connect maximum number sourc sink pair given graph via induc disjoint path known problem np hard approxim within varepsilon general ani varepsilon paper prove problem np hard approxim within varepsilon general ani varepsilon give simpl reduct independ set problem|['Gaoxiu Dong', 'Weidong Chen']|['cs.CC']
2017-03-28T14:06:48Z|2017-03-13T07:50:35Z|http://arxiv.org/abs/1703.04281v1|http://arxiv.org/pdf/1703.04281v1|Affine counter automata|affin counter automata|We introduce an affine generalization of counter automata, and analyze their ability as well as affine finite automata. Our contributions are as follows. We show that there is a promise problem that can be solved by exact affine counter automata but cannot be solved by deterministic counter automata. We also show that a certain promise problem, which is conjectured not to be solved by two-way quantum finite automata in polynomial time, can be solved by Las-Vegas affine finite automata in linear time. Lastly, we show that how a counter helps for affine finite automata by showing that the language $ \mathtt{MANYTWINS} $, which is conjectured not to be recognized by affine, quantum or classical finite state models in polynomial time, can be recognized by affine counter automata with one-sided bounded-error in realtime.|introduc affin general counter automata analyz abil well affin finit automata contribut follow show promis problem solv exact affin counter automata cannot solv determinist counter automata also show certain promis problem conjectur solv two way quantum finit automata polynomi time solv las vega affin finit automata linear time last show counter help affin finit automata show languag mathtt manytwin conjectur recogn affin quantum classic finit state model polynomi time recogn affin counter automata one side bound error realtim|['Masaki Nakanishi', 'Abuzer Yakaryılmaz']|['cs.FL', 'cs.CC', 'quant-ph']
2017-03-28T14:06:48Z|2017-03-12T17:11:49Z|http://arxiv.org/abs/1703.04143v1|http://arxiv.org/pdf/1703.04143v1|Bernoulli Factories and Black-Box Reductions in Mechanism Design|bernoulli factori black box reduct mechan design|"We provide a polynomial time reduction from Bayesian incentive compatible mechanism design to Bayesian algorithm design for welfare maximization problems. Unlike prior results, our reduction achieves exact incentive compatibility for problems with multi-dimensional and continuous type spaces. The key technical barrier preventing exact incentive compatibility in prior black-box reductions is that repairing violations of incentive constraints requires understanding the distribution of the mechanism's output. Reductions that instead estimate the output distribution by sampling inevitably suffer from sampling error, which typically precludes exact incentive compatibility.   We overcome this barrier by employing and generalizing the computational model in the literature on Bernoulli Factories. In a Bernoulli factory problem, one is given a function mapping the bias of an ""input coin"" to that of an ""output coin"", and the challenge is to efficiently simulate the output coin given sample access to the input coin. We generalize this to the ""expectations from samples"" computational model, in which an instance is specified by a function mapping the expected values of a set of input distributions to a distribution over outcomes. The challenge is to give a polynomial time algorithm that exactly samples from the distribution over outcomes given only sample access to the input distributions. In this model, we give a polynomial time algorithm for the exponential weights: expected values of the input distributions correspond to the weights of alternatives and we wish to select an alternative with probability proportional to an exponential function of its weight. This algorithm is the key ingredient in designing an incentive compatible mechanism for bipartite matching, which can be used to make the approximately incentive compatible reduction of Hartline et al. (2015) exactly incentive compatible."|provid polynomi time reduct bayesian incent compat mechan design bayesian algorithm design welfar maxim problem unlik prior result reduct achiev exact incent compat problem multi dimension continu type space key technic barrier prevent exact incent compat prior black box reduct repair violat incent constraint requir understand distribut mechan output reduct instead estim output distribut sampl inevit suffer sampl error typic preclud exact incent compat overcom barrier employ general comput model literatur bernoulli factori bernoulli factori problem one given function map bias input coin output coin challeng effici simul output coin given sampl access input coin general expect sampl comput model instanc specifi function map expect valu set input distribut distribut outcom challeng give polynomi time algorithm exact sampl distribut outcom given onli sampl access input distribut model give polynomi time algorithm exponenti weight expect valu input distribut correspond weight altern wish select altern probabl proport exponenti function weight algorithm key ingredi design incent compat mechan bipartit match use make approxim incent compat reduct hartlin et al exact incent compat|['Shaddin Dughmi', 'Jason Hartline', 'Robert Kleinberg', 'Rad Niazadeh']|['cs.GT', 'cs.CC', 'cs.DS', 'math.PR']
2017-03-28T14:06:48Z|2017-03-10T16:03:26Z|http://arxiv.org/abs/1703.03734v1|http://arxiv.org/pdf/1703.03734v1|On matrices with displacement structure: generalized operators and   faster algorithms|matric displac structur general oper faster algorithm|"For matrices with displacement structure, basic operations like multiplication, inversion, and linear system solving can all be expressed in terms of the following task: evaluate the product $\mathsf{A}\mathsf{B}$, where $\mathsf{A}$ is a structured $n \times n$ matrix of displacement rank $\alpha$, and $\mathsf{B}$ is an arbitrary $n\times\alpha$ matrix. Given $\mathsf{B}$ and a so-called ""generator"" of $\mathsf{A}$, this product is classically computed with a cost ranging from $O(\alpha^2 \mathscr{M}(n))$ to $O(\alpha^2 \mathscr{M}(n)\log(n))$ arithmetic operations, depending on the type of structure of $\mathsf{A}$; here, $\mathscr{M}$ is a cost function for polynomial multiplication. In this paper, we first generalize classical displacement operators, based on block diagonal matrices with companion diagonal blocks, and then design fast algorithms to perform the task above for this extended class of structured matrices. The cost of these algorithms ranges from $O(\alpha^{\omega-1} \mathscr{M}(n))$ to $O(\alpha^{\omega-1} \mathscr{M}(n)\log(n))$, with $\omega$ such that two $n \times n$ matrices over a field can be multiplied using $O(n^\omega)$ field operations. By combining this result with classical randomized regularization techniques, we obtain faster Las Vegas algorithms for structured inversion and linear system solving."|matric displac structur basic oper like multipl invers linear system solv express term follow task evalu product mathsf mathsf mathsf structur time matrix displac rank alpha mathsf arbitrari time alpha matrix given mathsf call generat mathsf product classic comput cost rang alpha mathscr alpha mathscr log arithmet oper depend type structur mathsf mathscr cost function polynomi multipl paper first general classic displac oper base block diagon matric companion diagon block design fast algorithm perform task abov extend class structur matric cost algorithm rang alpha omega mathscr alpha omega mathscr log omega two time matric field multipli use omega field oper combin result classic random regular techniqu obtain faster las vega algorithm structur invers linear system solv|['Alin Bostan', 'Claude-Pierre Jeannerod', 'Christophe Mouilleron', 'Éric Schost']|['cs.SC', 'cs.CC', '68W30, 68Q25, 97H60', 'I.1.2']
2017-03-28T14:06:48Z|2017-03-10T08:35:24Z|http://arxiv.org/abs/1703.03575v1|http://arxiv.org/pdf/1703.03575v1|Crossing the Logarithmic Barrier for Dynamic Boolean Data Structure   Lower Bounds|cross logarithm barrier dynam boolean data structur lower bound|"This paper proves the first super-logarithmic lower bounds on the cell probe complexity of dynamic boolean (a.k.a. decision) data structure problems, a long-standing milestone in data structure lower bounds.   We introduce a new method for proving dynamic cell probe lower bounds and use it to prove a $\tilde{\Omega}(\log^{1.5} n)$ lower bound on the operational time of a wide range of boolean data structure problems, most notably, on the query time of dynamic range counting over $\mathbb{F}_2$ ([Pat07]). Proving an $\omega(\lg n)$ lower bound for this problem was explicitly posed as one of five important open problems in the late Mihai P\v{a}tra\c{s}cu's obituary [Tho13]. This result also implies the first $\omega(\lg n)$ lower bound for the classical 2D range counting problem, one of the most fundamental data structure problems in computational geometry and spatial databases. We derive similar lower bounds for boolean versions of dynamic polynomial evaluation and 2D rectangle stabbing, and for the (non-boolean) problems of range selection and range median.   Our technical centerpiece is a new way of ""weakly"" simulating dynamic data structures using efficient one-way communication protocols with small advantage over random guessing. This simulation involves a surprising excursion to low-degree (Chebychev) polynomials which may be of independent interest, and offers an entirely new algorithmic angle on the ""cell sampling"" method of Panigrahy et al. [PTW10]."|paper prove first super logarithm lower bound cell probe complex dynam boolean decis data structur problem long stand mileston data structur lower bound introduc new method prove dynam cell probe lower bound use prove tild omega log lower bound oper time wide rang boolean data structur problem notabl queri time dynam rang count mathbb pat prove omega lg lower bound problem explicit pose one five import open problem late mihai tra cu obituari tho result also impli first omega lg lower bound classic rang count problem one fundament data structur problem comput geometri spatial databas deriv similar lower bound boolean version dynam polynomi evalu rectangl stab non boolean problem rang select rang median technic centerpiec new way weak simul dynam data structur use effici one way communic protocol small advantag random guess simul involv surpris excurs low degre chebychev polynomi may independ interest offer entir new algorithm angl cell sampl method panigrahi et al ptw|['Kasper Green Larsen', 'Omri Weinstein', 'Huacheng Yu']|['cs.DS', 'cs.CC', 'cs.CG', 'cs.IT', 'math.IT']
2017-03-28T14:06:48Z|2017-03-09T13:45:45Z|http://arxiv.org/abs/1703.03262v1|http://arxiv.org/pdf/1703.03262v1|Does Nash Envy Immunity|doe nash envi immun|The most popular stability notion in games should be Nash equilibrium under the rationality of players who maximize their own payoff individually. In contrast, in many scenarios, players can be (partly) irrational with some unpredictable factors. Hence a strategy profile can be more robust if it is resilient against certain irrational behaviors. In this paper, we propose a stability notion that is resilient against envy. A strategy profile is said to be envy-proof if each player cannot gain a competitive edge with respect to the change in utility over the other players by deviation. Together with Nash equilibrium and another stability notion called immunity, we show how these separate notions are related to each other, whether they exist in games, and whether and when a strategy profile satisfying these notions can be efficiently found. We answer these questions by starting with the general two player game and extend the discussion for the approximate stability and for the corresponding fault-tolerance notions in multi-player games.|popular stabil notion game nash equilibrium ration player maxim payoff individu contrast mani scenario player part irrat unpredict factor henc strategi profil robust resili certain irrat behavior paper propos stabil notion resili envi strategi profil said envi proof player cannot gain competit edg respect chang util player deviat togeth nash equilibrium anoth stabil notion call immun show separ notion relat whether exist game whether strategi profil satisfi notion effici found answer question start general two player game extend discuss approxim stabil correspond fault toler notion multi player game|['Ching-Hua Yu']|['cs.GT', 'cs.CC', 'cs.CR', 'I.2.1; J.4; K.6.0; C.4']
2017-03-28T14:06:48Z|2017-03-08T20:14:27Z|http://arxiv.org/abs/1703.03021v1|http://arxiv.org/pdf/1703.03021v1|A dichotomy theorem for nonuniform CSPs|dichotomi theorem nonuniform csps|In this paper we prove the Dichotomy Conjecture on the complexity of nonuniform constraint satisfaction problems posed by Feder and Vardi.|paper prove dichotomi conjectur complex nonuniform constraint satisfact problem pose feder vardi|['Andrei A. Bulatov']|['cs.CC']
2017-03-28T14:06:52Z|2017-03-07T16:50:43Z|http://arxiv.org/abs/1703.02469v1|http://arxiv.org/pdf/1703.02469v1|Random CNFs are Hard for Cutting Planes|random cnfs hard cut plane|The random k-SAT model is the most important and well-studied distribution over k-SAT instances. It is closely connected to statistical physics; it is used as a testbench for satisfiability algorithms, and average-case hardness over this distribution has also been linked to hardness of approximation via Feige's hypothesis. We prove that any Cutting Planes refutation for random k-SAT requires exponential size, for k that is logarithmic in the number of variables, in the (interesting) regime where the number of clauses guarantees that the formula is unsatisfiable with high probability.|random sat model import well studi distribut sat instanc close connect statist physic use testbench satisfi algorithm averag case hard distribut also link hard approxim via feig hypothesi prove ani cut plane refut random sat requir exponenti size logarithm number variabl interest regim number claus guarante formula unsatisfi high probabl|['Noah Fleming', 'Denis Pankratov', 'Toniann Pitassi', 'Robert Robere']|['cs.CC']
2017-03-28T14:06:52Z|2017-03-07T12:45:26Z|http://arxiv.org/abs/1703.02361v1|http://arxiv.org/pdf/1703.02361v1|On the family of 0/1-polytopes with NP-complete non-adjacency relation|famili polytop np complet non adjac relat|In 1995 T. Matsui considered a special family 0/1-polytopes for which the problem of recognizing the non-adjacency of two arbitrary vertices is NP-complete. In 2012 the author of this paper established that all the polytopes of this family are present as faces in the polytopes associated with the following NP-complete problems: the traveling salesman problem, the 3-satisfiability problem, the knapsack problem, the set covering problem, the partial ordering problem, the cube subgraph problem, and some others. In particular, it follows that for these families the non-adjacency relation is also NP-complete. On the other hand, it is known that the vertex adjacency criterion is polynomial for polytopes of the following NP-complete problems: the maximum independent set problem, the set packing and the set partitioning problem, the three-index assignment problem. It is shown that none of the polytopes of the above-mentioned special family (with the exception of a one-dimensional segment) can be the face of polytopes associated with the problems of the maximum independent set, of a set packing and partitioning, and of 3-assignments.|matsui consid special famili polytop problem recogn non adjac two arbitrari vertic np complet author paper establish polytop famili present face polytop associ follow np complet problem travel salesman problem satisfi problem knapsack problem set cover problem partial order problem cube subgraph problem particular follow famili non adjac relat also np complet hand known vertex adjac criterion polynomi polytop follow np complet problem maximum independ set problem set pack set partit problem three index assign problem shown none polytop abov mention special famili except one dimension segment face polytop associ problem maximum independ set set pack partit assign|['Alexander Maksimenko']|['cs.CC']
2017-03-28T14:06:52Z|2017-03-07T11:22:53Z|http://arxiv.org/abs/1703.02332v1|http://arxiv.org/pdf/1703.02332v1|The Minimum Shared Edges Problem on Grid-like Graphs|minimum share edg problem grid like graph|We study the NP-hard Minimum Shared Edges (MSE) problem on graphs: decide whether it is possible to route $p$ paths from a start vertex to a target vertex in a given graph while using at most $k$ edges more than once. We show that MSE can be decided on bounded grids in linear time when both dimensions are either small or large compared to the number $p$ of paths. On the contrary, we show that MSE remains NP-hard on subgraphs of bounded grids. Finally, we study MSE from a parametrised complexity point of view. It is known that MSE is fixed-parameter tractable with respect to the number $p$ of paths. We show that, under standard complexity-theoretical assumptions, the problem parametrised by the combined parameter $k$, $p$, maximum degree, diameter, and treewidth does not admit a polynomial-size problem kernel, even when restricted to planar graphs.|studi np hard minimum share edg mse problem graph decid whether possibl rout path start vertex target vertex given graph use edg onc show mse decid bound grid linear time dimens either small larg compar number path contrari show mse remain np hard subgraph bound grid final studi mse parametris complex point view known mse fix paramet tractabl respect number path show standard complex theoret assumpt problem parametris combin paramet maximum degre diamet treewidth doe admit polynomi size problem kernel even restrict planar graph|['Till Fluschnik', 'Meike Hatzel', 'Steffen Härtlein', 'Hendrik Molter', 'Henning Seidler']|['cs.CC', '68Q17, 68Q25, 68R10, 05C10', 'F.1.3; F.2.2; G.2.2']
2017-03-28T14:06:52Z|2017-03-06T15:42:36Z|http://arxiv.org/abs/1703.01928v1|http://arxiv.org/pdf/1703.01928v1|On The Complexity of Enumeration|complex enumer|We investigate the relationship between several enumeration complexity classes and focus in particular on the incremental polynomial time and the polynomial delay (IncP and DelayP). We prove, modulo the Exponential Time Hypothesis, that IncP contains a strict hierarchy of subclasses. Since DelayP is included in IncP_1, the first class of the hierarchy, it is separated from IncP. We prove for some algorithms that we can turn an average delay into a worst case delay, suggesting that IncP_1 = DelayP even with a polynomially bounded memory. Finally we relate the uniform generation of solutions to probabilistic enumeration algorithms with polynomial delay.|investig relationship sever enumer complex class focus particular increment polynomi time polynomi delay incp delayp prove modulo exponenti time hypothesi incp contain strict hierarchi subclass sinc delayp includ incp first class hierarchi separ incp prove algorithm turn averag delay worst case delay suggest incp delayp even polynomi bound memori final relat uniform generat solut probabilist enumer algorithm polynomi delay|['Florent Capelli', 'Yann Strozecki']|['cs.CC']
2017-03-28T14:06:52Z|2017-03-05T23:06:03Z|http://arxiv.org/abs/1703.01686v1|http://arxiv.org/pdf/1703.01686v1|Parameterized complexity of finding a spanning tree with minimum reload   cost diameter|parameter complex find span tree minimum reload cost diamet|We study the minimum diameter spanning tree problem under the reload cost model (DIAMETER-TREE for short) introduced by Wirth and Steffan (2001). In this problem, given an undirected edge-colored graph $G$, reload costs on a path arise at a node where the path uses consecutive edges of different colors. The objective is to find a spanning tree of $G$ of minimum diameter with respect to the reload costs. We initiate a systematic study of the parameterized complexity of the DIAMETER-TREE problem by considering the following parameters: the cost of a solution, and the treewidth and the maximum degree $\Delta$ of the input graph. We prove that DIAMETER-TREE is para-NP-hard for any combination of two of these three parameters, and that it is FPT parameterized by the three of them. We also prove that the problem can be solved in polynomial time on cactus graphs. This result is somehow surprising since we prove DIAMETER-TREE to be NP-hard on graphs of treewidth two, which is best possible as the problem can be trivially solved on forests. When the reload costs satisfy the triangle inequality, Wirth and Steffan (2001) proved that the problem can be solved in polynomial time on graphs with $\Delta = 3$, and Galbiati (2008) proved that it is NP-hard if $\Delta = 4$. Our results show, in particular, that without the requirement of the triangle inequality, the problem is NP-hard if $\Delta = 3$, which is also best possible. Finally, in the case where the reload costs are polynomially bounded by the size of the input graph, we prove that DIAMETER-TREE is in XP and W[1]-hard parameterized by the treewidth plus $\Delta$.|studi minimum diamet span tree problem reload cost model diamet tree short introduc wirth steffan problem given undirect edg color graph reload cost path aris node path use consecut edg differ color object find span tree minimum diamet respect reload cost initi systemat studi parameter complex diamet tree problem consid follow paramet cost solut treewidth maximum degre delta input graph prove diamet tree para np hard ani combin two three paramet fpt parameter three also prove problem solv polynomi time cactus graph result somehow surpris sinc prove diamet tree np hard graph treewidth two best possibl problem trivial solv forest reload cost satisfi triangl inequ wirth steffan prove problem solv polynomi time graph delta galbiati prove np hard delta result show particular without requir triangl inequ problem np hard delta also best possibl final case reload cost polynomi bound size input graph prove diamet tree xp hard parameter treewidth plus delta|['Julien Baste', 'Didem Gözüpek', 'Christophe Paul', 'Ignasi Sau', 'Mordechai Shalom', 'Dimitrios M. Thilikos']|['cs.DS', 'cs.CC', '05C85, 05C10', 'G.2.2; G.2.3']
2017-03-28T14:06:52Z|2017-03-03T13:04:46Z|http://arxiv.org/abs/1703.01143v1|http://arxiv.org/pdf/1703.01143v1|Why is it hard to beat $O(n^2)$ for Longest Common Weakly Increasing   Subsequence?|whi hard beat longest common weak increas subsequ|The Longest Common Weakly Increasing Subsequence problem (LCWIS) is a variant of the classic Longest Common Subsequence problem (LCS). Both problems can be solved with simple quadratic time algorithms. A recent line of research led to a number of matching conditional lower bounds for LCS and other related problems. However, the status of LCWIS remained open.   In this paper we show that LCWIS cannot be solved in strongly subquadratic time unless the Strong Exponential Time Hypothesis (SETH) is false.   The ideas which we developed can also be used to obtain a lower bound based on a safer assumption of NC-SETH, i.e. a version of SETH which talks about NC circuits instead of less expressive CNF formulas.|longest common weak increas subsequ problem lcwis variant classic longest common subsequ problem lcs problem solv simpl quadrat time algorithm recent line research led number match condit lower bound lcs relat problem howev status lcwis remain open paper show lcwis cannot solv strong subquadrat time unless strong exponenti time hypothesi seth fals idea develop also use obtain lower bound base safer assumpt nc seth version seth talk nc circuit instead less express cnf formula|['Adam Polak']|['cs.CC']
2017-03-28T14:06:52Z|2017-03-02T20:25:04Z|http://arxiv.org/abs/1703.00941v1|http://arxiv.org/pdf/1703.00941v1|On the Fine-grained Complexity of One-Dimensional Dynamic Programming|fine grain complex one dimension dynam program|In this paper, we investigate the complexity of one-dimensional dynamic programming, or more specifically, of the Least-Weight Subsequence (LWS) problem: Given a sequence of $n$ data items together with weights for every pair of the items, the task is to determine a subsequence $S$ minimizing the total weight of the pairs adjacent in $S$. A large number of natural problems can be formulated as LWS problems, yielding obvious $O(n^2)$-time solutions.   In many interesting instances, the $O(n^2)$-many weights can be succinctly represented. Yet except for near-linear time algorithms for some specific special cases, little is known about when an LWS instantiation admits a subquadratic-time algorithm and when it does not. In particular, no lower bounds for LWS instantiations have been known before. In an attempt to remedy this situation, we provide a general approach to study the fine-grained complexity of succinct instantiations of the LWS problem. In particular, given an LWS instantiation we identify a highly parallel core problem that is subquadratically equivalent. This provides either an explanation for the apparent hardness of the problem or an avenue to find improved algorithms as the case may be.   More specifically, we prove subquadratic equivalences between the following pairs (an LWS instantiation and the corresponding core problem) of problems: a low-rank version of LWS and minimum inner product, finding the longest chain of nested boxes and vector domination, and a coin change problem which is closely related to the knapsack problem and (min,+)-convolution. Using these equivalences and known SETH-hardness results for some of the core problems, we deduce tight conditional lower bounds for the corresponding LWS instantiations. We also establish the (min,+)-convolution-hardness of the knapsack problem.|paper investig complex one dimension dynam program specif least weight subsequ lws problem given sequenc data item togeth weight everi pair item task determin subsequ minim total weight pair adjac larg number natur problem formul lws problem yield obvious time solut mani interest instanc mani weight succinct repres yet except near linear time algorithm specif special case littl known lws instanti admit subquadrat time algorithm doe particular lower bound lws instanti known befor attempt remedi situat provid general approach studi fine grain complex succinct instanti lws problem particular given lws instanti identifi high parallel core problem subquadrat equival provid either explan appar hard problem avenu find improv algorithm case may specif prove subquadrat equival follow pair lws instanti correspond core problem problem low rank version lws minimum inner product find longest chain nest box vector domin coin chang problem close relat knapsack problem min convolut use equival known seth hard result core problem deduc tight condit lower bound correspond lws instanti also establish min convolut hard knapsack problem|['Marvin Künnemann', 'Ramamohan Paturi', 'Stefan Schneider']|['cs.CC', 'cs.DS']
2017-03-28T14:06:52Z|2017-03-01T23:15:54Z|http://arxiv.org/abs/1703.00544v1|http://arxiv.org/pdf/1703.00544v1|Simplified Algorithmic Metatheorems Beyond MSO: Treewidth and   Neighborhood Diversity|simplifi algorithm metatheorem beyond mso treewidth neighborhood divers|This paper settles the computational complexity of model checking of several extensions of the monadic second order (MSO) logic on two classes of graphs: graphs of bounded treewidth and graphs of bounded neighborhood diversity. A classical theorem of Courcelle states that any graph property definable in MSO is decidable in linear time on graphs of bounded treewidth. Algorithmic metatheorems like Courcelle's serve to generalize known positive results on various graph classes. We explore and extend three previously studied MSO extensions: global and local cardinality constraints (CardMSO and MSO-LCC) and optimizing a fair objective function (fairMSO). First, we show how these fragments relate to each other in expressive power and highlight their (non)linearity. On the side of neighborhood diversity, we show that combining the linear variants of local and global cardinality constraints is possible while keeping the linear runtime but removing linearity of either makes this impossible, and we provide a polynomial time algorithm for the hard case. Furthemore, we show that even the combination of the two most powerful fragments is solvable in polynomial time on graphs of bounded treewidth.|paper settl comput complex model check sever extens monad second order mso logic two class graph graph bound treewidth graph bound neighborhood divers classic theorem courcell state ani graph properti defin mso decid linear time graph bound treewidth algorithm metatheorem like courcell serv general known posit result various graph class explor extend three previous studi mso extens global local cardin constraint cardmso mso lcc optim fair object function fairmso first show fragment relat express power highlight non linear side neighborhood divers show combin linear variant local global cardin constraint possibl keep linear runtim remov linear either make imposs provid polynomi time algorithm hard case furthemor show even combin two power fragment solvabl polynomi time graph bound treewidth|['Dušan Knop', 'Martin Koutecký', 'Tomáš Masařík', 'Tomáš Toufar']|['cs.CC', 'cs.LO', '03D15', 'F.2.2']
2017-03-28T14:06:52Z|2017-03-15T08:50:49Z|http://arxiv.org/abs/1703.00242v2|http://arxiv.org/pdf/1703.00242v2|Reordering Method and Hierarchies for Quantum and Classical Ordered   Binary Decision Diagrams|reorder method hierarchi quantum classic order binari decis diagram|"We consider Quantum OBDD model. It is restricted version of read-once Quantum Branching Programs, with respect to ""width"" complexity. It is known that maximal complexity gap between deterministic and quantum model is exponential. But there are few examples of such functions. We present method (called ""reordering""), which allows to build Boolean function $g$ from Boolean Function $f$, such that if for $f$ we have gap between quantum and deterministic OBDD complexity for natural order of variables, then we have almost the same gap for function $g$, but for any order. Using it we construct the total function $REQ$ which deterministic OBDD complexity is $2^{\Omega(n/\log n)}$ and present quantum OBDD of width $O(n^2)$. It is bigger gap for explicit function that was known before for OBDD of width more than linear. Using this result we prove the width hierarchy for complexity classes of Boolean functions for quantum OBDDs.   Additionally, we prove the width hierarchy for complexity classes of Boolean functions for bounded error probabilistic OBDDs. And using ""reordering"" method we extend a hierarchy for $k$-OBDD of polynomial size, for $k=o(n/\log^3n)$. Moreover, we proved a similar hierarchy for bounded error probabilistic $k$-OBDD. And for deterministic and probabilistic $k$-OBDDs of superpolynomial and subexponential size."|consid quantum obdd model restrict version read onc quantum branch program respect width complex known maxim complex gap determinist quantum model exponenti exampl function present method call reorder allow build boolean function boolean function gap quantum determinist obdd complex natur order variabl almost gap function ani order use construct total function req determinist obdd complex omega log present quantum obdd width bigger gap explicit function known befor obdd width linear use result prove width hierarchi complex class boolean function quantum obdd addit prove width hierarchi complex class boolean function bound error probabilist obdd use reorder method extend hierarchi obdd polynomi size log moreov prove similar hierarchi bound error probabilist obdd determinist probabilist obdd superpolynomi subexponenti size|['Kamil Khadiev', 'Aliya Khadieva']|['cs.CC', 'quant-ph']
2017-03-28T14:06:52Z|2017-02-28T20:28:03Z|http://arxiv.org/abs/1703.00043v1|http://arxiv.org/pdf/1703.00043v1|Tree tribes and lower bounds for switching lemmas|tree tribe lower bound switch lemma|We show tight upper and lower bounds for switching lemmas obtained by the action of random $p$-restrictions on boolean functions that can be expressed as decision trees in which every vertex is at a distance of at most $t$ from some leaf, also called $t$-clipped decision trees. More specifically, we show the following:   $\bullet$ If a boolean function $f$ can be expressed as a $t$-clipped decision tree, then under the action of a random $p$-restriction $\rho$, the probability that the smallest depth decision tree for $f _{\rho}$ has depth greater than $d$ is upper bounded by $(4p2^{t})^{d}$.   $\bullet$ For every $t$, there exists a function $g_{t}$ that can be expressed as a $t$-clipped decision tree, such that under the action of a random $p$-restriction $\rho$, the probability that the smallest depth decision tree for $g_{t} _{\rho}$ has depth greater than $d$ is lower bounded by $(c_{0}p2^{t})^{d}$, for $0\leq p\leq c_{p}2^{-t}$ and $0\leq d\leq c_{d}\frac{\log n}{2^{t}\log t}$, where $c_{0},c_{p},c_{d}$ are universal constants.|show tight upper lower bound switch lemma obtain action random restrict boolean function express decis tree everi vertex distanc leaf also call clip decis tree specif show follow bullet boolean function express clip decis tree action random restrict rho probabl smallest depth decis tree rho depth greater upper bound bullet everi exist function express clip decis tree action random restrict rho probabl smallest depth decis tree rho depth greater lower bound leq leq leq leq frac log log univers constant|['Jenish C. Mehta']|['cs.CC']
2017-03-28T14:06:56Z|2017-02-28T16:57:49Z|http://arxiv.org/abs/1702.08862v1|http://arxiv.org/pdf/1702.08862v1|Proportional Representation in Vote Streams|proport represent vote stream|We consider elections where the voters come one at a time, in a streaming fashion, and devise space-efficient algorithms which identify an approximate winning committee with respect to common multiwinner proportional representation voting rules; specifically, we consider the Approval-based and the Borda-based variants of both the Chamberlin-- ourant rule and the Monroe rule. We complement our algorithms with lower bounds. Somewhat surprisingly, our results imply that, using space which does not depend on the number of voters it is possible to efficiently identify an approximate representative committee of fixed size over vote streams with huge number of voters.|consid elect voter come one time stream fashion devis space effici algorithm identifi approxim win committe respect common multiwinn proport represent vote rule specif consid approv base borda base variant chamberlin ourant rule monro rule complement algorithm lower bound somewhat surpris result impli use space doe depend number voter possibl effici identifi approxim repres committe fix size vote stream huge number voter|['Palash Dey', 'Nimrod Talmon', 'Otniel van Handel']|['cs.GT', 'cs.AI', 'cs.CC', 'cs.DS', 'cs.MA']
2017-03-28T14:06:56Z|2017-02-28T15:47:39Z|http://arxiv.org/abs/1702.08830v1|http://arxiv.org/pdf/1702.08830v1|The Complexity of Translationally-Invariant Low-Dimensional Spin   Lattices in 3D|complex translate invari low dimension spin lattic|In this paper, we consider spin systems in three spatial dimensions, and prove that the local Hamiltonian problem for 3D lattices with face-centered cubic unit cells, 4-local translationally-invariant interactions between spin-3/2 particles and open boundary conditions is QMAEXP-complete. We go beyond a mere embedding of past hard 1D history state constructions, and utilize a classical Wang tiling problem as binary counter in order to translate one cube side length into a binary description for the verifier input. We further make use of a recently-developed computational model especially well-suited for history state constructions, and combine it with a specific circuit encoding shown to be universal for quantum computation. These novel techniques allow us to significantly lower the local spin dimension, surpassing the best translationally-invariant result to date by two orders of magnitude (in the number of degrees of freedom per coupling). This brings our models en par with the best non-translationally-invariant construction.|paper consid spin system three spatial dimens prove local hamiltonian problem lattic face center cubic unit cell local translate invari interact spin particl open boundari condit qmaexp complet go beyond mere embed past hard histori state construct util classic wang tile problem binari counter order translat one cube side length binari descript verifi input make use recent develop comput model especi well suit histori state construct combin specif circuit encod shown univers quantum comput novel techniqu allow us signific lower local spin dimens surpass best translate invari result date two order magnitud number degre freedom per coupl bring model en par best non translate invari construct|['Johannes Bausch', 'Stephen Piddock']|['quant-ph', 'cs.CC', '68Q17, 81V70, 68Q10, 82D25']
2017-03-28T14:06:56Z|2017-03-05T02:18:12Z|http://arxiv.org/abs/1702.08662v3|http://arxiv.org/pdf/1702.08662v3|The computational complexity of integer programming with alternations|comput complex integ program altern|We prove that integer programming with three quantifier alternations is $NP$-complete, even for a fixed number of variables. This complements earlier results by Lenstra and Kannan, which together say that integer programming with at most two quantifier alternations can be done in polynomial time for a fixed number of variables. As a byproduct of the proof, we show that for two polytopes $P,Q \subset \mathbb{R}^4$ , counting the projection of integer points in $Q \backslash P$ is $\#P$-complete. This contrasts the 2003 result by Barvinok and Woods, which allows counting in polynomial time the projection of integer points in $P$ and $Q$ separately.|prove integ program three quantifi altern np complet even fix number variabl complement earlier result lenstra kannan togeth say integ program two quantifi altern done polynomi time fix number variabl byproduct proof show two polytop subset mathbb count project integ point backslash complet contrast result barvinok wood allow count polynomi time project integ point separ|['Danny Nguyen', 'Igor Pak']|['math.CO', 'cs.CC', 'cs.DM']
2017-03-28T14:06:56Z|2017-03-05T02:19:58Z|http://arxiv.org/abs/1702.08660v2|http://arxiv.org/pdf/1702.08660v2|Complexity of short generating functions|complex short generat function|We give complexity analysis of the class of short generating functions (GF). Assuming $\#P \not\subseteq FP/poly$, we show that this class is not closed under taking many intersections, unions or projections of GFs, in the sense that these operations can increase the bitlength of coefficients of GFs by a super-polynomial factor. We also prove that truncated theta functions are hard in this class.|give complex analysi class short generat function gf assum subseteq fp poli show class close take mani intersect union project gfs sens oper increas bitlength coeffici gfs super polynomi factor also prove truncat theta function hard class|['Danny Nguyen', 'Igor Pak']|['math.CO', 'cs.CC', 'cs.DM', 'cs.LO', 'math.LO']
2017-03-28T14:06:56Z|2017-02-27T19:46:15Z|http://arxiv.org/abs/1702.08489v1|http://arxiv.org/pdf/1702.08489v1|Depth Separation for Neural Networks|depth separ neural network|Let $f:\mathbb{S}^{d-1}\times \mathbb{S}^{d-1}\to\mathbb{S}$ be a function of the form $f(\mathbf{x},\mathbf{x}') = g(\langle\mathbf{x},\mathbf{x}'\rangle)$ for $g:[-1,1]\to \mathbb{R}$. We give a simple proof that shows that poly-size depth two neural networks with (exponentially) bounded weights cannot approximate $f$ whenever $g$ cannot be approximated by a low degree polynomial. Moreover, for many $g$'s, such as $g(x)=\sin(\pi d^3x)$, the number of neurons must be $2^{\Omega\left(d\log(d)\right)}$. Furthermore, the result holds w.r.t.\ the uniform distribution on $\mathbb{S}^{d-1}\times \mathbb{S}^{d-1}$. As many functions of the above form can be well approximated by poly-size depth three networks with poly-bounded weights, this establishes a separation between depth two and depth three networks w.r.t.\ the uniform distribution on $\mathbb{S}^{d-1}\times \mathbb{S}^{d-1}$.|let mathbb time mathbb mathbb function form mathbf mathbf langl mathbf mathbf rangl mathbb give simpl proof show poli size depth two neural network exponenti bound weight cannot approxim whenev cannot approxim low degre polynomi moreov mani sin pi number neuron must omega left log right furthermor result hold uniform distribut mathbb time mathbb mani function abov form well approxim poli size depth three network poli bound weight establish separ depth two depth three network uniform distribut mathbb time mathbb|['Amit Daniely']|['cs.LG', 'cs.CC', 'stat.ML']
2017-03-28T14:06:56Z|2017-02-27T19:26:15Z|http://arxiv.org/abs/1702.08483v1|http://arxiv.org/pdf/1702.08483v1|The computational landscape of general physical theories|comput landscap general physic theori|The emergence of quantum computers has challenged long-held beliefs about what is efficiently computable given our current physical theories. However, going back to the work of Abrams and Lloyd, changing one aspect of quantum theory can result in yet more dramatic increases in computational power, as well as violations of fundamental physical principles. Here we focus on efficient computation within a framework of general physical theories that make good operational sense. In prior work, Lee and Barrett showed that in any theory satisfying the principle of tomographic locality (roughly, local measurements suffice for tomography of multipartite states) the complexity bound on efficient computation is AWPP. This bound holds independently of whether the principle of causality (roughly, no signalling from the future) is satisfied. In this work we show that this bound is tight: there exists a theory satisfying both the principles of tomographic locality and causality which can efficiently decide everything in AWPP, and in particular can simulate any efficient quantum computation. Thus the class AWPP has a natural physical interpretation: it is precisely the class of problems that can be solved efficiently in tomographically-local theories. This theory is built upon a model of computing involving Turing machines with quasi-probabilities, to wit, machines with transition weights that can be negative but sum to unity over all branches. In analogy with the study of non-local quantum correlations, this leads us to question what physical principles recover the power of quantum computing. Along this line, we give some computational complexity evidence that quantum computation does not achieve the bound of AWPP.|emerg quantum comput challeng long held belief effici comput given current physic theori howev go back work abram lloyd chang one aspect quantum theori result yet dramat increas comput power well violat fundament physic principl focus effici comput within framework general physic theori make good oper sens prior work lee barrett show ani theori satisfi principl tomograph local rough local measur suffic tomographi multipartit state complex bound effici comput awpp bound hold independ whether principl causal rough signal futur satisfi work show bound tight exist theori satisfi principl tomograph local causal effici decid everyth awpp particular simul ani effici quantum comput thus class awpp natur physic interpret precis class problem solv effici tomograph local theori theori built upon model comput involv ture machin quasi probabl wit machin transit weight negat sum uniti branch analog studi non local quantum correl lead us question physic principl recov power quantum comput along line give comput complex evid quantum comput doe achiev bound awpp|['Jonathan Barrett', 'Niel de Beaudrap', 'Matty J. Hoban', 'Ciarán M. Lee']|['quant-ph', 'cs.CC']
2017-03-28T14:06:56Z|2017-02-27T12:21:07Z|http://arxiv.org/abs/1702.08255v1|http://arxiv.org/pdf/1702.08255v1|Learning with Errors is easy with quantum samples|learn error easi quantum sampl|Learning with Errors is one of the fundamental problems in computational learning theory and has in the last years become the cornerstone of post-quantum cryptography. In this work, we study the quantum sample complexity of Learning with Errors and show that there exists an efficient quantum learning algorithm (with polynomial sample and time complexity) for the Learning with Errors problem where the error distribution is the one used in cryptography. While our quantum learning algorithm does not break the LWE-based encryption schemes proposed in the cryptography literature, it does have some interesting implications for cryptography: first, when building an LWE-based scheme, one needs to be careful about the access to the public-key generation algorithm that is given to the adversary; second, our algorithm shows a possible way for attacking LWE-based encryption by using classical samples to approximate the quantum sample state, since then using our quantum learning algorithm would solve LWE.|learn error one fundament problem comput learn theori last year becom cornerston post quantum cryptographi work studi quantum sampl complex learn error show exist effici quantum learn algorithm polynomi sampl time complex learn error problem error distribut one use cryptographi quantum learn algorithm doe break lwe base encrypt scheme propos cryptographi literatur doe interest implic cryptographi first build lwe base scheme one need care access public key generat algorithm given adversari second algorithm show possibl way attack lwe base encrypt use classic sampl approxim quantum sampl state sinc use quantum learn algorithm would solv lwe|['Alex B. Grilo', 'Iordanis Kerenidis']|['quant-ph', 'cs.CC']
2017-03-28T14:06:56Z|2017-02-27T11:24:02Z|http://arxiv.org/abs/1702.08238v1|http://arxiv.org/pdf/1702.08238v1|Consensus Patterns parameterized by input string length is W[1]-hard|consensus pattern parameter input string length hard|We consider the Consensus Patterns problem, where, given a set of input strings, one is asked to extract a long-enough pattern which appears (with some errors) in all strings. We prove that this problem is W[1]-hard when parameterized by the maximum length of input strings.|consid consensus pattern problem given set input string one ask extract long enough pattern appear error string prove problem hard parameter maximum length input string|['Laurent Bulteau']|['cs.CC']
2017-03-28T14:06:56Z|2017-02-27T04:42:03Z|http://arxiv.org/abs/1702.08144v1|http://arxiv.org/pdf/1702.08144v1|Synchronization Problems in Automata without Non-trivial Cycles|synchron problem automata without non trivial cycl|In this paper, we study the computational complexity of various problems related to synchronization of weakly acyclic automata, a subclass of widely studied aperiodic automata. We provide upper and lower bounds on the length of a shortest word synchronizing a weakly acyclic automaton or, more generally, a subset of its states, and show that the problem of approximating this length is hard. We also show inapproximability of the problem of computing the rank of a subset of states in a binary weakly acyclic automaton and prove that several problems related to recognizing a synchronizing subset of states in such automata are NP-complete.|paper studi comput complex various problem relat synchron weak acycl automata subclass wide studi aperiod automata provid upper lower bound length shortest word synchron weak acycl automaton general subset state show problem approxim length hard also show inapproxim problem comput rank subset state binari weak acycl automaton prove sever problem relat recogn synchron subset state automata np complet|['Andrew Ryzhikov']|['cs.FL', 'cs.CC', '68Q17', 'F.1.1; F.1.3; F.2.2']
2017-03-28T14:06:56Z|2017-02-26T20:53:29Z|http://arxiv.org/abs/1702.08084v1|http://arxiv.org/pdf/1702.08084v1|On Algorithmic Statistics for space-bounded algorithms|algorithm statist space bound algorithm|Algorithmic statistics studies explanations of observed data that are good in the algorithmic sense: an explanation should be simple i.e. should have small Kolmogorov complexity and capture all the algorithmically discoverable regularities in the data. However this idea can not be used in practice because Kolmogorov complexity is not computable.   In this paper we develop algorithmic statistics using space-bounded Kolmogorov complexity. We prove an analogue of one of the main result of `classic' algorithmic statistics (about the connection between optimality and randomness deficiences). The main tool of our proof is the Nisan-Wigderson generator.|algorithm statist studi explan observ data good algorithm sens explan simpl small kolmogorov complex captur algorithm discover regular data howev idea use practic becaus kolmogorov complex comput paper develop algorithm statist use space bound kolmogorov complex prove analogu one main result classic algorithm statist connect optim random defici main tool proof nisan wigderson generat|['Alexey Milovanov']|['cs.IT', 'cs.CC', 'math.IT']
2017-03-28T14:07:01Z|2017-03-27T05:15:38Z|http://arxiv.org/abs/1702.08045v2|http://arxiv.org/pdf/1702.08045v2|General Upper Bounds for Gate Complexity and Depth of Reversible   Circuits Consisting of NOT, CNOT and 2-CNOT Gates|general upper bound gate complex depth revers circuit consist cnot cnot gate|The paper discusses the gate complexity and the depth of reversible circuits consisting of NOT, CNOT and 2-CNOT gates in the case, when the number of additional inputs is limited. We study Shannon's gate complexity function $L(n, q)$ and depth function $D(n, q)$ for a reversible circuit implementing a Boolean transformation $f\colon \mathbb Z_2^n \to \mathbb Z_2^n$ with $8n < q \lesssim n2^{n-o(n)}$ additional inputs. The general upper bounds $L(n,q) \lesssim 2^n + 8n2^n \mathop / (\log_2 (q-4n) - \log_2 n - 2)$ and $D(n,q) \lesssim 2^{n+1}(2,5 + \log_2 n - \log_2 (\log_2 (q - 4n) - \log_2 n - 2))$ are proved for this case.|paper discuss gate complex depth revers circuit consist cnot cnot gate case number addit input limit studi shannon gate complex function depth function revers circuit implement boolean transform colon mathbb mathbb lesssim addit input general upper bound lesssim mathop log log lesssim log log log log prove case|['Dmitry V. Zakablukov']|['cs.CC']
2017-03-28T14:07:01Z|2017-02-26T07:12:46Z|http://arxiv.org/abs/1702.08443v1|http://arxiv.org/pdf/1702.08443v1|Elementary Yet Precise Worst-case Analysis of MergeSort, A short version   (SV)|elementari yet precis worst case analysi mergesort short version sv|This paper offers two elementary yet precise derivations of an exact formula   \[ W(n) = \sum_{i=1} ^{n} \lceil \lg i \rceil = n \lceil \lg n \rceil - 2^{\lceil \lg n \rceil} + 1 \] for the maximum number $ W(n) $ of comparisons of keys performed by $ {\tt MergeSort} $ on an $ n $-element array. The first of the two, due to its structural regularity, is well worth carefully studying in its own right.   Close smooth bounds on $ W(n) $ are derived. It seems interesting that $ W(n) $ is linear between the points $ n = 2^{\lfloor \lg n \rfloor} $ and it linearly interpolates its own lower bound $ n \lg n - n + 1 $ between these points.|paper offer two elementari yet precis deriv exact formula sum lceil lg rceil lceil lg rceil lceil lg rceil maximum number comparison key perform tt mergesort element array first two due structur regular well worth care studi right close smooth bound deriv seem interest linear point lfloor lg rfloor linear interpol lower bound lg point|['Marek A. Suchenek']|['cs.DS', 'cs.CC', 'cs.DM', '68W40 Analysis of algorithms', 'F.2.2; G.2.0; G.2.1; G.2.2']
2017-03-28T14:07:01Z|2017-02-25T19:07:15Z|http://arxiv.org/abs/1702.07938v1|http://arxiv.org/pdf/1702.07938v1|Complexity Classification of the Eight-Vertex Model|complex classif eight vertex model|"We prove a complexity dichotomy theorem for the eight-vertex model. For every setting of the parameters of the model, we prove that computing the partition function is either solvable in polynomial time or \#P-hard. The dichotomy criterion is explicit. For tractability, we find some new classes of problems computable in polynomial time. For \#P-hardness, we employ M\""{o}bius transformations to prove the success of interpolations."|prove complex dichotomi theorem eight vertex model everi set paramet model prove comput partit function either solvabl polynomi time hard dichotomi criterion explicit tractabl find new class problem comput polynomi time hard employ bius transform prove success interpol|['Jin-Yi Cai', 'Zhiguo Fu']|['cs.CC']
2017-03-28T14:07:01Z|2017-02-25T15:07:59Z|http://arxiv.org/abs/1702.07902v1|http://arxiv.org/pdf/1702.07902v1|Approval Voting with Intransitive Preferences|approv vote intransit prefer|We extend Approval voting to the settings where voters may have intransitive preferences. The major obstacle to applying Approval voting in these settings is that voters are not able to clearly determine who they should approve or disapprove, due to the intransitivity of their preferences. An approach to address this issue is to apply tournament solutions to help voters make the decision. We study a class of voting systems where first each voter casts a vote defined as a tournament, then a well-defined tournament solution is applied to select the candidates who are assumed to be approved by the voter. Winners are the ones receiving the most approvals. We study axiomatic properties of this class of voting systems and complexity of control and bribery problems for these voting systems.|extend approv vote set voter may intransit prefer major obstacl appli approv vote set voter abl clear determin approv disapprov due intransit prefer approach address issu appli tournament solut help voter make decis studi class vote system first voter cast vote defin tournament well defin tournament solut appli select candid assum approv voter winner one receiv approv studi axiomat properti class vote system complex control briberi problem vote system|['Yongjie Yang']|['cs.GT', 'cs.CC', 'cs.DM']
2017-03-28T14:07:01Z|2017-02-24T17:18:02Z|http://arxiv.org/abs/1702.07669v1|http://arxiv.org/pdf/1702.07669v1|On problems equivalent to (min,+)-convolution|problem equival min convolut|In the recent years, significant progress has been made in explaining apparent hardness of improving over naive solutions for many fundamental polynomially solvable problems. This came in the form of conditional lower bounds - reductions to one of problems assumed to be hard. These include 3SUM, All-Pairs Shortest Paths, SAT and Orthogonal Vectors, and others.   In the (min,+)-convolution problem, the goal is to compute a sequence $(c[i])^{n-1}_{i=0}$, where $c[k] = \min_{i=0,\ldots,k} \{a[i]+b[k-i]\}$, given sequences $(a[i])^{n-1}_{i=0}$ and $(b[i])_{i=0}^{n-1}$. This can easily be done in $O(n^2)$ time, but no $O(n^{2-\varepsilon})$ algorithm is known for $\varepsilon > 0$. In this paper we undertake a systematic study of the (min,+)-convolution problem as a hardness assumption.   As the first step, we establish equivalence of this problem to a group of other problems, including variants of the classic knapsack problem and problems related to subadditive sequences. The (min,+)-convolution has been used as a building block in algorithms for many problems, notably problems in stringology. It has also already appeared as an ad hoc hardness assumption. We investigate some of these connections and provide new reductions and other results.|recent year signific progress made explain appar hard improv naiv solut mani fundament polynomi solvabl problem came form condit lower bound reduct one problem assum hard includ sum pair shortest path sat orthogon vector min convolut problem goal comput sequenc min ldot given sequenc easili done time varepsilon algorithm known varepsilon paper undertak systemat studi min convolut problem hard assumpt first step establish equival problem group problem includ variant classic knapsack problem problem relat subaddit sequenc min convolut use build block algorithm mani problem notabl problem stringolog also alreadi appear ad hoc hard assumpt investig connect provid new reduct result|['Marek Cygan', 'Marcin Mucha', 'Karol Węgrzycki', 'Michał Włodarczyk']|['cs.DS', 'cs.CC', 'F.1.3; F.2']
2017-03-28T14:07:01Z|2017-02-23T18:52:31Z|http://arxiv.org/abs/1702.07339v1|http://arxiv.org/pdf/1702.07339v1|A Converse to Banach's Fixed Point Theorem and its CLS Completeness|convers banach fix point theorem cls complet|Banach's fixed point theorem for contraction maps has been widely used to analyze the convergence of iterative methods in non-convex problems. It is a common experience, however, that iterative maps fail to be globally contracting under the natural metric in their domain, making the applicability of Banach's theorem limited. We explore how generally we can apply Banach's fixed point theorem to establish the convergence of iterative methods when pairing it with carefully designed metrics.   Our first result is a strong converse of Banach's theorem, showing that it is a universal analysis tool for establishing uniqueness of fixed points and for bounding the convergence rate of iterative maps to a unique fixed point. In other words, we show that, whenever an iterative map globally converges to a unique fixed point, there exists a metric under which the iterative map is contracting and which can be used to bound the number of iterations until convergence. We illustrate our approach in the widely used power method, providing a new way of bounding its convergence rate through contraction arguments.   We next consider the computational complexity of Banach's fixed point theorem. Making the proof of our converse theorem constructive, we show that computing a fixed point whose existence is guaranteed by Banach's fixed point theorem is CLS-complete. We thus provide the first natural complete problem for the class CLS, which was defined in [Daskalakis-Papadimitriou 2011] to capture the complexity of problems such as P-matrix LCP, computing KKT-points, and finding mixed Nash equilibria in congestion and network coordination games.|banach fix point theorem contract map wide use analyz converg iter method non convex problem common experi howev iter map fail global contract natur metric domain make applic banach theorem limit explor general appli banach fix point theorem establish converg iter method pair care design metric first result strong convers banach theorem show univers analysi tool establish uniqu fix point bound converg rate iter map uniqu fix point word show whenev iter map global converg uniqu fix point exist metric iter map contract use bound number iter converg illustr approach wide use power method provid new way bound converg rate contract argument next consid comput complex banach fix point theorem make proof convers theorem construct show comput fix point whose exist guarante banach fix point theorem cls complet thus provid first natur complet problem class cls defin daskalaki papadimitriou captur complex problem matrix lcp comput kkt point find mix nash equilibria congest network coordin game|['Constantinos Daskalakis', 'Christos Tzamos', 'Manolis Zampetakis']|['cs.CC', 'cs.LG', 'math.GN', 'stat.ML']
2017-03-28T14:07:01Z|2017-02-23T11:38:36Z|http://arxiv.org/abs/1702.07180v1|http://arxiv.org/pdf/1702.07180v1|Small hitting-sets for tiny arithmetic circuits or: How to turn bad   designs into good|small hit set tini arithmet circuit turn bad design good|We show that if we can design poly($s$)-time hitting-sets for $\Sigma\wedge^a\Sigma\Pi^{O(\log s)}$ circuits of size $s$, where $a=\omega(1)$ is arbitrarily small and the number of variables, or arity $n$, is $O(\log s)$, then we can derandomize blackbox PIT for general circuits in quasipolynomial time. This also establishes that either E$\not\subseteq$\#P/poly or that VP$\ne$VNP. In fact, we show that one only needs a poly($s$)-time hitting-set against individual-degree $a'=\omega(1)$ polynomials that are computable by a size-$s$ arity-$(\log s)$ $\Sigma\Pi\Sigma$ circuit (note: $\Pi$ fanin may be $s$). Alternatively, we claim that, to understand VP one only needs to find hitting-sets, for depth-$3$, that have a small parameterized complexity. Another tiny family of interest is when we restrict the arity $n=\omega(1)$ to be arbitrarily small. We show that if we can design poly($s,\mu(n)$)-time hitting-sets for size-$s$ arity-$n$ $\Sigma\Pi\Sigma\wedge$ circuits (resp.~$\Sigma\wedge^a\Sigma\Pi$), where function $\mu$ is arbitrary, then we can solve PIT for VP in quasipoly-time, and prove the corresponding lower bounds. Our methods are strong enough to prove a surprising {\em arity reduction} for PIT-- to solve the general problem completely it suffices to find a blackbox PIT with time-complexity $sd2^{O(n)}$. We give several examples of ($\log s$)-variate circuits where a new measure (called cone-size) helps in devising poly-time hitting-sets, but the same question for their $s$-variate versions is open till date: For eg., diagonal depth-$3$ circuits, and in general, models that have a {\em small} partial derivative space. We also introduce a new concept, called cone-closed basis isolation, and provide example models where it occurs, or can be achieved by a small shift.|show design poli time hit set sigma wedg sigma pi log circuit size omega arbitrarili small number variabl ariti log derandom blackbox pit general circuit quasipolynomi time also establish either subseteq poli vp ne vnp fact show one onli need poli time hit set individu degre omega polynomi comput size ariti log sigma pi sigma circuit note pi fanin may altern claim understand vp one onli need find hit set depth small parameter complex anoth tini famili interest restrict ariti omega arbitrarili small show design poli mu time hit set size ariti sigma pi sigma wedg circuit resp sigma wedg sigma pi function mu arbitrari solv pit vp quasipoli time prove correspond lower bound method strong enough prove surpris em ariti reduct pit solv general problem complet suffic find blackbox pit time complex sd give sever exampl log variat circuit new measur call cone size help devis poli time hit set question variat version open till date eg diagon depth circuit general model em small partial deriv space also introduc new concept call cone close basi isol provid exampl model occur achiev small shift|['Manindra Agrawal', 'Michael Forbes', 'Sumanta Ghosh', 'Nitin Saxena']|['cs.CC', 'F.1.1; I.1.2; F.1.3']
2017-03-28T14:07:01Z|2017-02-23T08:02:25Z|http://arxiv.org/abs/1702.07128v1|http://arxiv.org/pdf/1702.07128v1|The Facets of the Bases Polytope of a Matroid and Two Consequences|facet base polytop matroid two consequ|Let $M$ to be a matroid defined on a finite set $E$ and $L\subset E$. $L$ is locked in $M$ if $M L$ and $M^* (E\backslash L)$ are 2-connected, and $min\{r(L), r^*(E\backslash L)\} \geq 2$. In this paper, we prove that the nontrivial facets of the bases polytope of $M$ are described by the locked subsets. We deduce that finding the maximum--weight basis of $M$ is a polynomial time problem for matroids with a polynomial number of locked subsets. This class of matroids is closed under 2-sums and contains the class of uniform matroids, the V\'amos matroid and all the excluded minors of 2-sums of uniform matroids. We deduce also a matroid oracle for testing uniformity of matroids after one call of this oracle.|let matroid defin finit set subset lock backslash connect min backslash geq paper prove nontrivi facet base polytop describ lock subset deduc find maximum weight basi polynomi time problem matroid polynomi number lock subset class matroid close sum contain class uniform matroid amo matroid exclud minor sum uniform matroid deduc also matroid oracl test uniform matroid one call oracl|['Brahim Chaourar']|['cs.CC', 'Primary 90C27, Secondary 90C57, 52B40']
2017-03-28T14:07:01Z|2017-02-22T22:43:45Z|http://arxiv.org/abs/1702.07032v1|http://arxiv.org/pdf/1702.07032v1|On the Complexity of Bundle-Pricing and Simple Mechanisms|complex bundl price simpl mechan|We show that the problem of finding an optimal bundle-pricing for a single additive buyer is #P-hard, even when the distributions have support size 2 for each item and the optimal solution is guaranteed to be a simple one: the seller picks a price for the grand bundle and a price for each individual item; the buyer can purchase either the grand bundle at the given price or any bundle of items at their total individual prices. We refer to this simple and natural family of pricing schemes as discounted item-pricings. In addition to the hardness result, we show that when the distributions are i.i.d. with support size 2, a discounted item-pricing can achieve the optimal revenue obtainable by lottery-pricings and it can be found in polynomial time.|show problem find optim bundl price singl addit buyer hard even distribut support size item optim solut guarante simpl one seller pick price grand bundl price individu item buyer purchas either grand bundl given price ani bundl item total individu price refer simpl natur famili price scheme discount item price addit hard result show distribut support size discount item price achiev optim revenu obtain lotteri price found polynomi time|['Xi Chen', 'George Matikas', 'Dimitris Paparas', 'Mihalis Yannakakis']|['cs.GT', 'cs.CC', 'cs.DS']
2017-03-28T14:07:01Z|2017-02-22T20:38:35Z|http://arxiv.org/abs/1702.06997v1|http://arxiv.org/pdf/1702.06997v1|Beyond Talagrand Functions: New Lower Bounds for Testing Monotonicity   and Unateness|beyond talagrand function new lower bound test monoton unat|We prove a lower bound of $\tilde{\Omega}(n^{1/3})$ for the query complexity of any two-sided and adaptive algorithm that tests whether an unknown Boolean function $f:\{0,1\}^n\rightarrow \{0,1\}$ is monotone or far from monotone. This improves the recent bound of $\tilde{\Omega}(n^{1/4})$ for the same problem by Belovs and Blais [BB15]. Our result builds on a new family of random Boolean functions that can be viewed as a two-level extension of Talagrand's random DNFs.   Beyond monotonicity, we also prove a lower bound of $\tilde{\Omega}(\sqrt{n})$ for any two-sided and adaptive algorithm, and a lower bound of $\tilde{\Omega}(n)$ for any one-sided and non-adaptive algorithm for testing unateness, a natural generalization of monotonicity. The latter matches the recent linear upper bounds by Khot and Shinkar [KS15] and by Chakrabarty and Seshadhri [CS16].|prove lower bound tild omega queri complex ani two side adapt algorithm test whether unknown boolean function rightarrow monoton far monoton improv recent bound tild omega problem belov blai bb result build new famili random boolean function view two level extens talagrand random dnfs beyond monoton also prove lower bound tild omega sqrt ani two side adapt algorithm lower bound tild omega ani one side non adapt algorithm test unat natur general monoton latter match recent linear upper bound khot shinkar ks chakrabarti seshadhri cs|['Xi Chen', 'Erik Waingarten', 'Jinyu Xie']|['cs.CC']
2017-03-28T14:07:05Z|2017-02-22T15:29:15Z|http://arxiv.org/abs/1702.06844v1|http://arxiv.org/pdf/1702.06844v1|Parameterized Shifted Combinatorial Optimization|parameter shift combinatori optim|Shifted combinatorial optimization is a new nonlinear optimization framework which is a broad extension of standard combinatorial optimization, involving the choice of several feasible solutions at a time. This framework captures well studied and diverse problems ranging from so-called vulnerability problems to sharing and partitioning problems. In particular, every standard combinatorial optimization problem has its shifted counterpart, which is typically much harder. Already with explicitly given input set the shifted problem may be NP-hard. In this article we initiate a study of the parameterized complexity of this framework. First we show that shifting over an explicitly given set with its cardinality as the parameter may be in XP, FPT or P, depending on the objective function. Second, we study the shifted problem over sets definable in MSO logic (which includes, e.g., the well known MSO partitioning problems). Our main results here are that shifted combinatorial optimization over MSO definable sets is in XP with respect to the MSO formula and the treewidth (or more generally clique-width) of the input graph, and is W[1]-hard even under further severe restrictions.|shift combinatori optim new nonlinear optim framework broad extens standard combinatori optim involv choic sever feasibl solut time framework captur well studi divers problem rang call vulner problem share partit problem particular everi standard combinatori optim problem shift counterpart typic much harder alreadi explicit given input set shift problem may np hard articl initi studi parameter complex framework first show shift explicit given set cardin paramet may xp fpt depend object function second studi shift problem set defin mso logic includ well known mso partit problem main result shift combinatori optim mso defin set xp respect mso formula treewidth general cliqu width input graph hard even sever restrict|['Jakub Gajarský', 'Petr Hliněný', 'Martin Koutecký', 'Shmuel Onn']|['cs.CC']
2017-03-28T14:07:05Z|2017-02-21T23:06:56Z|http://arxiv.org/abs/1702.06616v1|http://arxiv.org/pdf/1702.06616v1|TC^0 circuits for algorithmic problems in nilpotent groups|tc circuit algorithm problem nilpot group|Recently, MacDonald et. al. showed that many algorithmic problems for nilpotent groups including computation of normal forms, the subgroup membership problem, the conjugacy problem, and computation of presentations of subgroups can be done in Logspace. Here we follow their approach and show that all these problems are actually complete for the uniform circuit class TC^0 -- uniformly for all r-generated nilpotent groups of class at most c for fixed r and c.   Moreover, if we allow a certain binary representation of the inputs, then the word problem and computation of normal forms is still in uniform TC^0, while all the other problems we examine are shown to be TC^0-Turing reducible to the problem of computing greatest common divisors and expressing them as a linear combination.|recent macdonald et al show mani algorithm problem nilpot group includ comput normal form subgroup membership problem conjugaci problem comput present subgroup done logspac follow approach show problem actual complet uniform circuit class tc uniform generat nilpot group class fix moreov allow certain binari represent input word problem comput normal form still uniform tc problem examin shown tc ture reduc problem comput greatest common divisor express linear combin|['Alexei Myasnikov', 'Armin Weiß']|['math.GR', 'cs.CC', 'F.2.2; G.2.0']
2017-03-28T14:07:05Z|2017-02-21T18:13:40Z|http://arxiv.org/abs/1702.06503v1|http://arxiv.org/pdf/1702.06503v1|When can Graph Hyperbolicity be computed in Linear Time?|graph hyperbol comput linear time|"Hyperbolicity measures, in terms of (distance) metrics, how close a given graph is to being a tree. Due to its relevance in modeling real-world networks, hyperbolicity has seen intensive research over the last years. Unfortunately, the best known algorithms for computing the hyperbolicity number of a graph (the smaller, the more tree-like) have running time $O(n^4)$, where $n$ is the number of graph vertices. Exploiting the framework of parameterized complexity analysis, we explore possibilities for ""linear-time FPT"" algorithms to compute hyperbolicity. For instance, we show that hyperbolicity can be computed in time $O(2^{O(k)} + n +m)$ ($m$ being the number of graph edges) while at the same time, unless the SETH fails, there is no $2^{o(k)}n^2$-time algorithm."|hyperbol measur term distanc metric close given graph tree due relev model real world network hyperbol seen intens research last year unfortun best known algorithm comput hyperbol number graph smaller tree like run time number graph vertic exploit framework parameter complex analysi explor possibl linear time fpt algorithm comput hyperbol instanc show hyperbol comput time number graph edg time unless seth fail time algorithm|['Till Fluschnik', 'Christian Komusiewicz', 'George B. Mertzios', 'André Nichterlein', 'Rolf Niedermeier', 'Nimrod Talmon']|['cs.CC', 'cs.DS', '05C12, 68R10, 68Q25, 68Q17', 'F.2.2; G.2.2']
2017-03-28T14:07:05Z|2017-02-21T13:06:59Z|http://arxiv.org/abs/1702.06364v1|http://arxiv.org/pdf/1702.06364v1|Linear-Time Tree Containment in Phylogenetic Networks|linear time tree contain phylogenet network|We consider the NP-hard Tree Containment problem that has important applications in phylogenetics. The problem asks if a given leaf-labeled network contains a subdivision of a given leaf-labeled tree. We develop a fast algorithm for the case that the input network is indeed a tree in which multiple leaves might share a label. By combining this algorithm with a generalization of a previously known decomposition scheme, we improve the running time on reticulation visible networks and nearly stable networks to linear time. While these are special classes of networks, they rank among the most general of the previously considered classes.|consid np hard tree contain problem import applic phylogenet problem ask given leaf label network contain subdivis given leaf label tree develop fast algorithm case input network inde tree multipl leav might share label combin algorithm general previous known decomposit scheme improv run time reticul visibl network near stabl network linear time special class network rank among general previous consid class|['Mathias Weller']|['cs.CC', 'cs.DS']
2017-03-28T14:07:05Z|2017-02-23T02:48:22Z|http://arxiv.org/abs/1702.06237v2|http://arxiv.org/pdf/1702.06237v2|Exact tensor completion with sum-of-squares|exact tensor complet sum squar|We obtain the first polynomial-time algorithm for exact tensor completion that improves over the bound implied by reduction to matrix completion. The algorithm recovers an unknown 3-tensor with $r$ incoherent, orthogonal components in $\mathbb R^n$ from $r\cdot \tilde O(n^{1.5})$ randomly observed entries of the tensor. This bound improves over the previous best one of $r\cdot \tilde O(n^{2})$ by reduction to exact matrix completion. Our bound also matches the best known results for the easier problem of approximate tensor completion (Barak & Moitra, 2015).   Our algorithm and analysis extends seminal results for exact matrix completion (Candes & Recht, 2009) to the tensor setting via the sum-of-squares method. The main technical challenge is to show that a small number of randomly chosen monomials are enough to construct a degree-3 polynomial with precisely planted orthogonal global optima over the sphere and that this fact can be certified within the sum-of-squares proof system.|obtain first polynomi time algorithm exact tensor complet improv bound impli reduct matrix complet algorithm recov unknown tensor incoher orthogon compon mathbb cdot tild random observ entri tensor bound improv previous best one cdot tild reduct exact matrix complet bound also match best known result easier problem approxim tensor complet barak moitra algorithm analysi extend semin result exact matrix complet cand recht tensor set via sum squar method main technic challeng show small number random chosen monomi enough construct degre polynomi precis plant orthogon global optima sphere fact certifi within sum squar proof system|['Aaron Potechin', 'David Steurer']|['cs.LG', 'cs.CC', 'cs.DS', 'cs.IT', 'math.IT', 'stat.ML']
2017-03-28T14:07:05Z|2017-02-20T15:39:13Z|http://arxiv.org/abs/1702.06017v1|http://arxiv.org/pdf/1702.06017v1|CLS: New Problems and Completeness|cls new problem complet|The complexity class CLS was introduced by Daskalakis and Papadimitriou with the goal of capturing the complexity of some well-known problems in PPAD$~\cap~$PLS that have resisted, in some cases for decades, attempts to put them in polynomial time. No complete problem was known for CLS, and in previous work, the problems ContractionMap, i.e., the problem of finding an approximate fixpoint of a contraction map, and PLCP, i.e., the problem of solving a P-matrix Linear Complementarity Problem, were identified as prime candidates.   First, we present a new CLS-complete problem MetaMetricContractionMap, which is closely related to the ContractionMap. Second, we introduce EndOfPotentialLine, which captures aspects of PPAD and PLS directly via a monotonic directed path, and show that EndOfPotentialLine is in CLS via a two-way reduction to EndOfMeteredLine. The latter was defined to keep track of how far a vertex is on the PPAD path via a restricted potential function. Third, we reduce PLCP to EndOfPotentialLine, thus making EndOfPotentialLine and EndOfMeteredLine at least as likely to be hard for CLS as PLCP. This last result leverages the monotonic structure of Lemke paths for PLCP problems, making EndOfPotentialLine a likely candidate to capture the exact complexity of PLCP; we note that the structure of Lemke-Howson paths for finding a Nash equilibrium in a two-player game very directly motivated the definition of the complexity class PPAD, which eventually ended up capturing this problem's complexity exactly.|complex class cls introduc daskalaki papadimitriou goal captur complex well known problem ppad cap pls resist case decad attempt put polynomi time complet problem known cls previous work problem contractionmap problem find approxim fixpoint contract map plcp problem solv matrix linear complementar problem identifi prime candid first present new cls complet problem metametriccontractionmap close relat contractionmap second introduc endofpotentiallin captur aspect ppad pls direct via monoton direct path show endofpotentiallin cls via two way reduct endofmeteredlin latter defin keep track far vertex ppad path via restrict potenti function third reduc plcp endofpotentiallin thus make endofpotentiallin endofmeteredlin least like hard cls plcp last result leverag monoton structur lemk path plcp problem make endofpotentiallin like candid captur exact complex plcp note structur lemk howson path find nash equilibrium two player game veri direct motiv definit complex class ppad eventu end captur problem complex exact|['John Fearnley', 'Spencer Gordon', 'Ruta Mehta', 'Rahul Savani']|['cs.CC']
2017-03-28T14:07:05Z|2017-02-20T11:04:29Z|http://arxiv.org/abs/1702.05927v1|http://arxiv.org/pdf/1702.05927v1|How to implement a genuine Parrondo's paradox with quantum walks?|implement genuin parrondo paradox quantum walk|Parrondo's paradox is ubiquitous in games, ratchets and random walks.The apparent paradox, devised by Juan M. R. Parrondo, that two losing games A and B can produce an winning outcome has been adapted in many physical and biological systems to explain their working. However, proposals on demonstrating Parrondo's paradox using quantum walks failed in the asymptotic limits. In this work, we show that instead of a single coin if we consider a two coin initial state which may or may not be entangled, we can observe a genuine Parrondo's paradox with quantum walks. The implications of our results for observing quantum ratchet like behavior using quantum walks is also discussed.|parrondo paradox ubiquit game ratchet random walk appar paradox devis juan parrondo two lose game produc win outcom adapt mani physic biolog system explain work howev propos demonstr parrondo paradox use quantum walk fail asymptot limit work show instead singl coin consid two coin initi state may may entangl observ genuin parrondo paradox quantum walk implic result observ quantum ratchet like behavior use quantum walk also discuss|['Jishnu Rajendran', 'Colin Benjamin']|['quant-ph', 'cond-mat.mes-hall', 'cs.CC']
2017-03-28T14:07:05Z|2017-02-19T15:48:11Z|http://arxiv.org/abs/1702.05760v1|http://arxiv.org/pdf/1702.05760v1|Hypercube LSH for approximate near neighbors|hypercub lsh approxim near neighbor|A celebrated technique for finding near neighbors for the angular distance involves using a set of \textit{random} hyperplanes to partition the space into hash regions [Charikar, STOC 2002]. Experiments later showed that using a set of \textit{orthogonal} hyperplanes, thereby partitioning the space into the Voronoi regions induced by a hypercube, leads to even better results [Terasawa and Tanaka, WADS 2007]. However, no theoretical explanation for this improvement was ever given, and it remained unclear how the resulting hypercube hash method scales in high dimensions.   In this work, we provide explicit asymptotics for the collision probabilities when using hypercubes to partition the space. For instance, two near-orthogonal vectors are expected to collide with probability $(\frac{1}{\pi})^{d + o(d)}$ in dimension $d$, compared to $(\frac{1}{2})^d$ when using random hyperplanes. Vectors at angle $\frac{\pi}{3}$ collide with probability $(\frac{\sqrt{3}}{\pi})^{d + o(d)}$, compared to $(\frac{2}{3})^d$ for random hyperplanes, and near-parallel vectors collide with similar asymptotic probabilities in both cases.   For $c$-approximate nearest neighbor searching, this translates to a decrease in the exponent $\rho$ of locality-sensitive hashing (LSH) methods of a factor up to $\log_2(\pi) \approx 1.652$ compared to hyperplane LSH. For $c = 2$, we obtain $\rho \approx 0.302 + o(1)$ for hypercube LSH, improving upon the $\rho \approx 0.377$ for hyperplane LSH. We further describe how to use hypercube LSH in practice, and we consider an example application in the area of lattice algorithms.|celebr techniqu find near neighbor angular distanc involv use set textit random hyperplan partit space hash region charikar stoc experi later show use set textit orthogon hyperplan therebi partit space voronoi region induc hypercub lead even better result terasawa tanaka wad howev theoret explan improv ever given remain unclear result hypercub hash method scale high dimens work provid explicit asymptot collis probabl use hypercub partit space instanc two near orthogon vector expect collid probabl frac pi dimens compar frac use random hyperplan vector angl frac pi collid probabl frac sqrt pi compar frac random hyperplan near parallel vector collid similar asymptot probabl case approxim nearest neighbor search translat decreas expon rho local sensit hash lsh method factor log pi approx compar hyperplan lsh obtain rho approx hypercub lsh improv upon rho approx hyperplan lsh describ use hypercub lsh practic consid exampl applic area lattic algorithm|['Thijs Laarhoven']|['cs.DS', 'cs.CC', 'cs.CG', 'cs.CR']
2017-03-28T14:07:05Z|2017-02-21T09:07:53Z|http://arxiv.org/abs/1702.05704v2|http://arxiv.org/pdf/1702.05704v2|Computational Complexity of Atomic Chemical Reaction Networks|comput complex atom chemic reaction network|"Informally, a chemical reaction network is ""atomic"" if each reaction may be interpreted as the rearrangement of indivisible units of matter. There are several reasonable definitions formalizing this idea. We investigate the computational complexity of deciding whether a given network is atomic according to each of these definitions.   Our first definition, primitive atomic, which requires each reaction to preserve the total number of atoms, is to shown to be equivalent to mass conservation. Since it is known that it can be decided in polynomial time whether a given chemical reaction network is mass-conserving, the equivalence gives an efficient algorithm to decide primitive atomicity.   Another definition, subset atomic, further requires that all atoms are species. We show that deciding whether a given network is subset atomic is in $\textsf{NP}$, and the problem ""is a network subset atomic with respect to a given atom set"" is strongly $\textsf{NP}$-$\textsf{Complete}$.   A third definition, reachably atomic, studied by Adleman, Gopalkrishnan et al., further requires that each species has a sequence of reactions splitting it into its constituent atoms. We show that there is a polynomial-time algorithm to decide whether a given network is reachably atomic, improving upon the result of Adleman et al. that the problem is decidable. We show that the reachability problem for reachably atomic networks is $\textsf{Pspace}$-$\textsf{Complete}$.   Finally, we demonstrate equivalence relationships between our definitions and some special cases of another existing definition of atomicity due to Gnacadja."|inform chemic reaction network atom reaction may interpret rearrang indivis unit matter sever reason definit formal idea investig comput complex decid whether given network atom accord definit first definit primit atom requir reaction preserv total number atom shown equival mass conserv sinc known decid polynomi time whether given chemic reaction network mass conserv equival give effici algorithm decid primit atom anoth definit subset atom requir atom speci show decid whether given network subset atom textsf np problem network subset atom respect given atom set strong textsf np textsf complet third definit reachabl atom studi adleman gopalkrishnan et al requir speci sequenc reaction split constitu atom show polynomi time algorithm decid whether given network reachabl atom improv upon result adleman et al problem decid show reachabl problem reachabl atom network textsf pspace textsf complet final demonstr equival relationship definit special case anoth exist definit atom due gnacadja|['David Doty', 'Shaopeng Zhu']|['cs.CC', 'F.1.1']
2017-03-28T14:07:05Z|2017-02-18T00:19:02Z|http://arxiv.org/abs/1702.05547v1|http://arxiv.org/pdf/1702.05547v1|Nontrivial Turmites are Turing-universal|nontrivi turmit ture univers|A Turmit is a Turing machine that works over a two-dimensional grid, that is, an agent that moves, reads and writes symbols over the cells of the grid. Its state is an arrow and, depending on the symbol that it reads, it turns to the left or to the right, switching the symbol at the same time. Several symbols are admitted, and the rule is specified by the turning sense that the machine has over each symbol. Turmites are a generalization of Langtons ant, and they present very complex and diverse behaviors. We prove that any Turmite, except for those whose rule does not depend on the symbol, can simulate any Turing Machine. We also prove the P-completeness of prediction their future behavior by explicitly giving a log-space reduction from the Topological Circuit Value Problem. A similar result was already established for Langtons ant; here we use a similar technique but prove a stronger notion of simulation, and for a more general family.|turmit ture machin work two dimension grid agent move read write symbol cell grid state arrow depend symbol read turn left right switch symbol time sever symbol admit rule specifi turn sens machin symbol turmit general langton ant present veri complex divers behavior prove ani turmit except whose rule doe depend symbol simul ani ture machin also prove complet predict futur behavior explicit give log space reduct topolog circuit valu problem similar result alreadi establish langton ant use similar techniqu prove stronger notion simul general famili|['Diego Maldonado', 'Anahí Gajardo', 'Benjamin Hellouin de Menibus', 'Andrés Moreira']|['cs.CC', 'nlin.CG', '68Q17, 68Q05', 'F.1.1; F.1.3']
2017-03-28T14:07:09Z|2017-02-17T23:43:14Z|http://arxiv.org/abs/1702.05543v1|http://arxiv.org/pdf/1702.05543v1|A Fixed-Parameter Perspective on #BIS|fix paramet perspect bis|The complexity of approximately counting independent sets in bipartite graphs (#BIS) is a central open problem in approximate counting, and it is widely believed to be neither easy nor NP-hard. We study several natural parameterised variants of #BIS, both from the polynomial-time and from the fixed-parameter viewpoint: counting independent sets of a given size; counting independent sets with a given number of vertices in one vertex class; and counting maximum independent sets among those with a given number of vertices in one vertex class. Among other things, we show that all these problems are NP-hard to approximate within any polynomial ratio. We also show that the first problem is #W[1]-hard to solve exactly but admits an FPTRAS, and the other two are W[1]-hard to approximate within any polynomial ratio. Finally, we show that when restricted to graphs of bounded degree, all three problems admit exact fixed-parameter algorithms with reasonable time complexity.|complex approxim count independ set bipartit graph bis central open problem approxim count wide believ neither easi np hard studi sever natur parameteris variant bis polynomi time fix paramet viewpoint count independ set given size count independ set given number vertic one vertex class count maximum independ set among given number vertic one vertex class among thing show problem np hard approxim within ani polynomi ratio also show first problem hard solv exact admit fptras two hard approxim within ani polynomi ratio final show restrict graph bound degre three problem admit exact fix paramet algorithm reason time complex|['Radu Curticapean', 'Holger Dell', 'Fedor Fomin', 'Leslie Ann Goldberg', 'John Lapinskas']|['cs.CC', 'F.2.2; G.2.1; G.2.2']
2017-03-28T14:07:09Z|2017-02-17T17:48:41Z|http://arxiv.org/abs/1702.05456v1|http://arxiv.org/pdf/1702.05456v1|LCL problems on grids|lcl problem grid|LCLs or locally checkable labelling problems (e.g. maximal independent set, maximal matching, and vertex colouring) in the LOCAL model of computation are very well-understood in cycles (toroidal 1-dimensional grids): every problem has a complexity of $O(1)$, $\Theta(\log^* n)$, or $\Theta(n)$, and the design of optimal algorithms can be fully automated.   This work develops the complexity theory of LCL problems for toroidal 2-dimensional grids. The complexity classes are the same as in the 1-dimensional case: $O(1)$, $\Theta(\log^* n)$, and $\Theta(n)$. However, given an LCL problem it is undecidable whether its complexity is $\Theta(\log^* n)$ or $\Theta(n)$ in 2-dimensional grids.   Nevertheless, if we correctly guess that the complexity of a problem is $\Theta(\log^* n)$, we can completely automate the design of optimal algorithms. For any problem we can find an algorithm that is of a normal form $A' \circ S_k$, where $A'$ is a finite function, $S_k$ is an algorithm for finding a maximal independent set in $k$th power of the grid, and $k$ is a constant.   With the help of this technique, we study several concrete \lcl{} problems, also in more general settings. For example, for all $d \ge 2$, we prove that:   - $d$-dimensional grids can be $k$-vertex coloured in time $O(\log^* n)$ iff $k \ge 4$,   - $d$-dimensional grids can be $k$-edge coloured in time $O(\log^* n)$ iff $k \ge 2d+1$.   The proof that $3$-colouring of $2$-dimensional grids requires $\Theta(n)$ time introduces a new topological proof technique, which can also be applied to e.g. orientation problems.|lcls local checkabl label problem maxim independ set maxim match vertex colour local model comput veri well understood cycl toroid dimension grid everi problem complex theta log theta design optim algorithm fulli autom work develop complex theori lcl problem toroid dimension grid complex class dimension case theta log theta howev given lcl problem undecid whether complex theta log theta dimension grid nevertheless correct guess complex problem theta log complet autom design optim algorithm ani problem find algorithm normal form circ finit function algorithm find maxim independ set th power grid constant help techniqu studi sever concret lcl problem also general set exampl ge prove dimension grid vertex colour time log iff ge dimension grid edg colour time log iff ge proof colour dimension grid requir theta time introduc new topolog proof techniqu also appli orient problem|['Sebastian Brandt', 'Juho Hirvonen', 'Janne H. Korhonen', 'Tuomo Lempiäinen', 'Patric R. J. Östergård', 'Christopher Purcell', 'Joel Rybicki', 'Jukka Suomela', 'Przemysław Uznański']|['cs.DC', 'cs.CC', 'cs.DS']
2017-03-28T14:07:09Z|2017-02-17T17:20:21Z|http://arxiv.org/abs/1702.05447v1|http://arxiv.org/pdf/1702.05447v1|Counting edge-injective homomorphisms and matchings on restricted graph   classes|count edg inject homomorph match restrict graph class|We consider the parameterized problem of counting all matchings with exactly $k$ edges in a given input graph $G$. This problem is #W[1]-hard (Curticapean, ICALP 2013), so it is unlikely to admit $f(k)\cdot n^{O(1)}$ time algorithms. We show that #W[1]-hardness persists even when the input graph $G$ comes from restricted graph classes, such as line graphs and bipartite graphs of arbitrary constant girth and maximum degree two on one side. To prove the result for line graphs, we observe that $k$-matchings in line graphs can be equivalently viewed as edge-injective homomorphisms from the disjoint union of $k$ paths of length two into (arbitrary) host graphs. Here, a homomorphism from $H$ to $G$ is edge-injective if it maps any two distinct edges of $H$ to distinct edges in $G$. We show that edge-injective homomorphisms from a pattern graph $H$ can be counted in polynomial time if $H$ has bounded vertex-cover number after removing isolated edges. For hereditary classes $\mathcal{H}$ of pattern graphs, we obtain a full complexity dichotomy theorem by proving that counting edge-injective homomorphisms, restricted to patterns from $\mathcal{H}$, is #W[1]-hard if no such bound exists. Our proofs rely on an edge-colored variant of Holant problems and a delicate interpolation argument; both may be of independent interest.|consid parameter problem count match exact edg given input graph problem hard curticapean icalp unlik admit cdot time algorithm show hard persist even input graph come restrict graph class line graph bipartit graph arbitrari constant girth maximum degre two one side prove result line graph observ match line graph equival view edg inject homomorph disjoint union path length two arbitrari host graph homomorph edg inject map ani two distinct edg distinct edg show edg inject homomorph pattern graph count polynomi time bound vertex cover number remov isol edg hereditari class mathcal pattern graph obtain full complex dichotomi theorem prove count edg inject homomorph restrict pattern mathcal hard bound exist proof reli edg color variant holant problem delic interpol argument may independ interest|['Radu Curticapean', 'Holger Dell', 'Marc Roth']|['cs.CC']
2017-03-28T14:07:09Z|2017-02-17T13:07:58Z|http://arxiv.org/abs/1702.05328v1|http://arxiv.org/pdf/1702.05328v1|On algebraic branching programs of small width|algebra branch program small width|In 1979 Valiant showed that the complexity class VP_e of families with polynomially bounded formula size is contained in the class VP_s of families that have algebraic branching programs (ABPs) of polynomially bounded size. Motivated by the problem of separating these classes we study the topological closure VP_e-bar, i.e. the class of polynomials that can be approximated arbitrarily closely by polynomials in VP_e. We describe VP_e-bar with a strikingly simple complete polynomial (in characteristic different from 2) whose recursive definition is similar to the Fibonacci numbers. Further understanding this polynomial seems to be a promising route to new formula lower bounds.   Our methods are rooted in the study of ABPs of small constant width. In 1992 Ben-Or and Cleve showed that formula size is polynomially equivalent to width-3 ABP size. We extend their result (in characteristic different from 2) by showing that approximate formula size is polynomially equivalent to approximate width-2 ABP size. This is surprising because in 2011 Allender and Wang gave explicit polynomials that cannot be computed by width-2 ABPs at all! The details of our construction lead to the aforementioned characterization of VP_e-bar.   As a natural continuation of this work we prove that the class VNP can be described as the class of families that admit a hypercube summation of polynomially bounded dimension over a product of polynomially many affine linear forms. This gives the first separations of algebraic complexity classes from their nondeterministic analogs.|valiant show complex class vp famili polynomi bound formula size contain class vp famili algebra branch program abp polynomi bound size motiv problem separ class studi topolog closur vp bar class polynomi approxim arbitrarili close polynomi vp describ vp bar strike simpl complet polynomi characterist differ whose recurs definit similar fibonacci number understand polynomi seem promis rout new formula lower bound method root studi abp small constant width ben cleve show formula size polynomi equival width abp size extend result characterist differ show approxim formula size polynomi equival approxim width abp size surpris becaus allend wang gave explicit polynomi cannot comput width abp detail construct lead aforement character vp bar natur continu work prove class vnp describ class famili admit hypercub summat polynomi bound dimens product polynomi mani affin linear form give first separ algebra complex class nondeterminist analog|['Karl Bringmann', 'Christian Ikenmeyer', 'Jeroen Zuiddam']|['cs.CC', '68Q15', 'F.1.3']
2017-03-28T14:07:09Z|2017-02-16T23:21:34Z|http://arxiv.org/abs/1702.05183v1|http://arxiv.org/pdf/1702.05183v1|Courcelle's Theorem Made Dynamic|courcell theorem made dynam|Dynamic complexity is concerned with updating the output of a problem when the input is slightly changed. We study the dynamic complexity of model checking a fixed monadic second-order formula over evolving subgraphs of a fixed maximal graph having bounded tree-width; here the subgraph evolves by losing or gaining edges (from the maximal graph). We show that this problem is in DynFO (with LOGSPACE precomputation), via a reduction to a Dyck reachability problem on an acyclic automaton.|dynam complex concern updat output problem input slight chang studi dynam complex model check fix monad second order formula evolv subgraph fix maxim graph bound tree width subgraph evolv lose gain edg maxim graph show problem dynfo logspac precomput via reduct dyck reachabl problem acycl automaton|['Patricia Bouyer-Decitre', 'Vincent Jugé', 'Nicolas Markey']|['cs.CC', 'cs.FL']
2017-03-28T14:07:09Z|2017-02-16T20:02:12Z|http://arxiv.org/abs/1702.05139v1|http://arxiv.org/pdf/1702.05139v1|On the Bit Complexity of Sum-of-Squares Proofs|bit complex sum squar proof|It has often been claimed in recent papers that one can find a degree d Sum-of-Squares proof if one exists via the Ellipsoid algorithm. In [O17], Ryan O'Donnell notes this widely quoted claim is not necessarily true. He presents an example of a polynomial system with bounded coeffcients that admits low-degree proofs of non-negativity, but these proofs necessarily involve numbers with an exponential number of bits, causing the Ellipsoid algorithm to take exponential time. In this paper we obtain both positive and negative results on the bit complexity of SoS proofs. First, we propose a suffcient condition on a polynomial system that implies a bound on the coefficients in an SoS proof. We demonstrate that this sufficient condition is applicable for common use-cases of the SoS algorithm, such as Max-CSP, Balanced Separator, Max- Clique, Max-Bisection, and Unit-Vector constraints. On the negative side, O'Donnell asked whether every polynomial system containing Boolean constraints admits proofs of polynomial bit complexity. We answer this question in the negative, giving a counterexample system and non-negative polynomial which has degree two SoS proofs, but no SoS proof with small coefficients until degree Omega(sqrt(n))|often claim recent paper one find degre sum squar proof one exist via ellipsoid algorithm ryan donnel note wide quot claim necessarili true present exampl polynomi system bound coeffcient admit low degre proof non negat proof necessarili involv number exponenti number bit caus ellipsoid algorithm take exponenti time paper obtain posit negat result bit complex sos proof first propos suffcient condit polynomi system impli bound coeffici sos proof demonstr suffici condit applic common use case sos algorithm max csp balanc separ max cliqu max bisect unit vector constraint negat side donnel ask whether everi polynomi system contain boolean constraint admit proof polynomi bit complex answer question negat give counterexampl system non negat polynomi degre two sos proof sos proof small coeffici degre omega sqrt|['Prasad Raghavendra', 'Benjamin Weitz']|['cs.CC']
2017-03-28T14:07:09Z|2017-02-15T21:21:34Z|http://arxiv.org/abs/1702.04779v1|http://arxiv.org/pdf/1702.04779v1|Compression Complexity|compress complex|The Kolmogorov complexity of x, denoted C(x), is the length of the shortest program that generates x. For such a simple definition, Kolmogorov complexity has a rich and deep theory, as well as applications to a wide variety of topics including learning theory, complexity lower bounds and SAT algorithms.   Kolmogorov complexity typically focuses on decompression, going from the compressed program to the original string. This paper develops a dual notion of compression, the mapping from a string to its compressed version. Typical lossless compression algorithms such as Lempel-Ziv or Huffman Encoding always produce a string that will decompress to the original. We define a general compression concept based on this observation.   For every m, we exhibit a single compression algorithm q of length about m which for n and strings x of length n >= m, the output of q will have length within n-m+O(1) bits of C(x). We also show this bound is tight in a strong way, for every n >= m there is an x of length n with C(x) about m such that no compression program of size slightly less than m can compress x at all.   We also consider a polynomial time-bounded version of compression complexity and show that similar results for this version would rule out cryptographic one-way functions.|kolmogorov complex denot length shortest program generat simpl definit kolmogorov complex rich deep theori well applic wide varieti topic includ learn theori complex lower bound sat algorithm kolmogorov complex typic focus decompress go compress program origin string paper develop dual notion compress map string compress version typic lossless compress algorithm lempel ziv huffman encod alway produc string decompress origin defin general compress concept base observ everi exhibit singl compress algorithm length string length output length within bit also show bound tight strong way everi length compress program size slight less compress also consid polynomi time bound version compress complex show similar result version would rule cryptograph one way function|['Stephen Fenner', 'Lance Fortnow']|['cs.CC']
2017-03-28T14:07:09Z|2017-02-15T19:39:58Z|http://arxiv.org/abs/1702.04748v1|http://arxiv.org/pdf/1702.04748v1|An Improved Dictatorship Test with Perfect Completeness|improv dictatorship test perfect complet|A Boolean function $f:\{0,1\}^n\rightarrow \{0,1\}$ is called a dictator if it depends on exactly one variable i.e $f(x_1, x_2, \ldots, x_n) = x_i$ for some $i\in [n]$. In this work, we study a $k$-query dictatorship test. Dictatorship tests are central in proving many hardness results for constraint satisfaction problems.   The dictatorship test is said to have {\em perfect completeness} if it accepts any dictator function. The {\em soundness} of a test is the maximum probability with which it accepts any function far from a dictator. Our main result is a $k$-query dictatorship test with perfect completeness and soundness $ \frac{2k + 1}{2^k}$, where $k$ is of the form $2^t -1$ for any integer $t > 2$. This improves upon the result of \cite{TY15} which gave a dictatorship test with soundness $ \frac{2k + 3}{2^k}$.|boolean function rightarrow call dictat depend exact one variabl ldot work studi queri dictatorship test dictatorship test central prove mani hard result constraint satisfact problem dictatorship test said em perfect complet accept ani dictat function em sound test maximum probabl accept ani function far dictat main result queri dictatorship test perfect complet sound frac form ani integ improv upon result cite ty gave dictatorship test sound frac|['Amey Bhangale', 'Subhash Khot', 'Devanathan Thiruvenkatachari']|['cs.CC']
2017-03-28T14:07:09Z|2017-02-16T18:09:15Z|http://arxiv.org/abs/1702.04679v2|http://arxiv.org/pdf/1702.04679v2|The complexity of Boolean surjective general-valued CSPs|complex boolean surject general valu csps|Valued constraint satisfaction problems (VCSPs) are discrete optimisation problems with a $\overline{\mathbb{Q}}$-valued objective function given as a sum of fixed-arity functions, where $\overline{\mathbb{Q}}=\mathbb{Q}\cup\{\infty\}$ is the set of extended rationals.   In Boolean surjective VCSPs variables take on labels from $D=\{0,1\}$ and an optimal assignment is required to use both labels from $D$. A classic example is the global min-cut problem in graphs. Building on the work of Uppman, we establish a dichotomy theorem and thus give a complete complexity classification of Boolean surjective VCSPs. The newly discovered tractable case has an interesting structure related to projections of downsets and upsets. Our work generalises the dichotomy for $\{0,\infty\}$-valued constraint languages (corresponding to CSPs) obtained by Creignou and H\'ebrard, and the dichotomy for $\{0,1\}$-valued constraint languages (corresponding to Min-CSPs) obtained by Uppman.|valu constraint satisfact problem vcsps discret optimis problem overlin mathbb valu object function given sum fix ariti function overlin mathbb mathbb cup infti set extend ration boolean surject vcsps variabl take label optim assign requir use label classic exampl global min cut problem graph build work uppman establish dichotomi theorem thus give complet complex classif boolean surject vcsps newli discov tractabl case interest structur relat project downset upset work generalis dichotomi infti valu constraint languag correspond csps obtain creignou ebrard dichotomi valu constraint languag correspond min csps obtain uppman|['Peter Fulla', 'Stanislav Zivny']|['cs.CC', 'cs.DM', 'F.2.0']
2017-03-28T14:07:09Z|2017-02-15T01:05:51Z|http://arxiv.org/abs/1702.04432v1|http://arxiv.org/pdf/1702.04432v1|Vertex isoperimetry and independent set stability for tensor powers of   cliques|vertex isoperimetri independ set stabil tensor power cliqu|The tensor power of the clique on $t$ vertices (denoted by $K_t^n$) is the graph on vertex set $\{1, ..., t\}^n$ such that two vertices $x, y \in \{1, ..., t\}^n$ are connected if and only if $x_i \neq y_i$ for all $i \in \{1, ..., n\}$. Let the density of a subset $S$ of $K_t^n$ to be $\mu(S) := \frac{ S }{t^n}$, and let the vertex boundary of a set $S$ to be vertices which are incident to some vertex of $S$, perhaps including points of $S$. We investigate two similar problems on such graphs.   First, we study the vertex isoperimetry problem. Given a density $\nu \in [0, 1]$ what is the smallest possible density of the vertex boundary of a subset of $K_t^n$ of density $\nu$? Let $\Phi_t(\nu)$ be the infimum of these minimum densities as $n \to \infty$. We find a recursive relation allows one to compute $\Phi_t(\nu)$ in time polynomial to the number of desired bits of precision.   Second, we study given an independent set $I \subseteq K_t^n$ of density $\mu(I) = \frac{1}{t}(1-\epsilon)$, how close it is to a maximum-sized independent set $J$ of density $\frac{1}{t}$. We show that this deviation (measured by $\mu(I \setminus J)$) is at most $4\epsilon^{\frac{\log t}{\log t - \log(t-1)}}$ as long as $\epsilon < 1 - \frac{3}{t} + \frac{2}{t^2}$. This substantially improves on results of Alon, Dinur, Friedgut, and Sudakov (2004) and Ghandehari and Hatami (2008) which had an $O(\epsilon)$ upper bound. We also show the exponent $\frac{\log t}{\log t - \log(t-1)}$ is optimal assuming $n$ tending to infinity and $\epsilon$ tending to $0$. The methods have similarity to recent work by Ellis, Keller, and Lifshitz (2016) in the context of Kneser graphs and other settings.   The author hopes that these results have potential applications in hardness of approximation, particularly in approximate graph coloring and independent set problems.|tensor power cliqu vertic denot graph vertex set two vertic connect onli neq let densiti subset mu frac let vertex boundari set vertic incid vertex perhap includ point investig two similar problem graph first studi vertex isoperimetri problem given densiti nu smallest possibl densiti vertex boundari subset densiti nu let phi nu infimum minimum densiti infti find recurs relat allow one comput phi nu time polynomi number desir bit precis second studi given independ set subseteq densiti mu frac epsilon close maximum size independ set densiti frac show deviat measur mu setminus epsilon frac log log log long epsilon frac frac substanti improv result alon dinur friedgut sudakov ghandehari hatami epsilon upper bound also show expon frac log log log optim assum tend infin epsilon tend method similar recent work elli keller lifshitz context kneser graph set author hope result potenti applic hard approxim particular approxim graph color independ set problem|['Joshua Brakensiek']|['math.CO', 'cs.CC', 'cs.DM']
2017-03-28T14:07:13Z|2017-02-14T18:21:28Z|http://arxiv.org/abs/1702.04322v1|http://arxiv.org/pdf/1702.04322v1|Parameterized Algorithms for Recognizing Monopolar and 2-Subcolorable   Graphs|parameter algorithm recogn monopolar subcolor graph|A graph $G$ is a $(\Pi_A,\Pi_B)$-graph if $V(G)$ can be bipartitioned into $A$ and $B$ such that $G[A]$ satisfies property $\Pi_A$ and $G[B]$ satisfies property $\Pi_B$. The $(\Pi_{A},\Pi_{B})$-Recognition problem is to recognize whether a given graph is a $(\Pi_A,\Pi_B)$-graph. There are many $(\Pi_{A},\Pi_{B})$-Recognition problems, including the recognition problems for bipartite, split, and unipolar graphs. We present efficient algorithms for many cases of $(\Pi_A,\Pi_B)$-Recognition based on a technique which we dub inductive recognition. In particular, we give fixed-parameter algorithms for two NP-hard $(\Pi_{A},\Pi_{B})$-Recognition problems, Monopolar Recognition and 2-Subcoloring. We complement our algorithmic results with several hardness results for $(\Pi_{A},\Pi_{B})$-Recognition.|graph pi pi graph bipartit satisfi properti pi satisfi properti pi pi pi recognit problem recogn whether given graph pi pi graph mani pi pi recognit problem includ recognit problem bipartit split unipolar graph present effici algorithm mani case pi pi recognit base techniqu dub induct recognit particular give fix paramet algorithm two np hard pi pi recognit problem monopolar recognit subcolor complement algorithm result sever hard result pi pi recognit|['Iyad Kanj', 'Christian Komusiewicz', 'Manuel Sorge', 'Erik Jan van Leeuwen']|['cs.CC', 'cs.DS']
2017-03-28T14:07:13Z|2017-02-14T17:23:31Z|http://arxiv.org/abs/1702.04300v1|http://arxiv.org/pdf/1702.04300v1|Optimality condition and complexity analysis for linearly-constrained   optimization without differentiability on the boundary|optim condit complex analysi linear constrain optim without differenti boundari|In this paper we consider the minimization of a continuous function that is potentially not differentiable or not twice differentiable on the boundary of the feasible region. By exploiting an interior point technique, we present first- and second-order optimality conditions for this problem that reduces to classical ones when the derivative on the boundary is available. For this type of problems, existing necessary conditions often rely on the notion of subdifferential or become non-trivially weaker than the KKT condition in the (twice-)differentiable counterpart problems. In contrast, this paper presents a new set of first- and second-order necessary conditions that are derived without the use of subdifferential and reduces to exactly the KKT condition when (twice-)differentiability holds. As a result, these conditions are stronger than some existing ones considered for the discussed minimization problem when only non-negativity constraints are present. To solve for these optimality conditions in the special but important case of linearly constrained problems, we present two novel interior trust-region point algorithms and show that their worst-case computational efficiency in achieving the potentially stronger optimality conditions match the best known complexity bounds. Since this work considers a more general problem than the literature, our results also indicate that best known complexity bounds hold for a wider class of nonlinear programming problems.|paper consid minim continu function potenti differenti twice differenti boundari feasibl region exploit interior point techniqu present first second order optim condit problem reduc classic one deriv boundari avail type problem exist necessari condit often reli notion subdifferenti becom non trivial weaker kkt condit twice differenti counterpart problem contrast paper present new set first second order necessari condit deriv without use subdifferenti reduc exact kkt condit twice differenti hold result condit stronger exist one consid discuss minim problem onli non negat constraint present solv optim condit special import case linear constrain problem present two novel interior trust region point algorithm show worst case comput effici achiev potenti stronger optim condit match best known complex bound sinc work consid general problem literatur result also indic best known complex bound hold wider class nonlinear program problem|['Gabriel Haeser', 'Hongcheng Liu', 'Yinyu Ye']|['cs.CC', 'math.OC', '90C30, 90C51, 90C60, 68Q25']
2017-03-28T14:07:13Z|2017-02-14T03:19:55Z|http://arxiv.org/abs/1702.04059v1|http://arxiv.org/pdf/1702.04059v1|Computing geometric Lorenz attractors with arbitrary precision|comput geometr lorenz attractor arbitrari precis|The Lorenz attractor was introduced in 1963 by E. N. Lorenz as one of the first examples of \emph{strange attractors}. However Lorenz' research was mainly based on (non-rigourous) numerical simulations and, until recently, the proof of the existence of the Lorenz attractor remained elusive. To address that problem some authors introduced geometric Lorenz models and proved that geometric Lorenz models have a strange attractor. In 2002 it was shown that the original Lorenz model behaves like a geometric Lorenz model and thus has a strange attractor. In this paper we show that geometric Lorenz attractors are computable, as well as their physical measures.|lorenz attractor introduc lorenz one first exampl emph strang attractor howev lorenz research main base non rigour numer simul recent proof exist lorenz attractor remain elus address problem author introduc geometr lorenz model prove geometr lorenz model strang attractor shown origin lorenz model behav like geometr lorenz model thus strang attractor paper show geometr lorenz attractor comput well physic measur|['Daniel Graca', 'Cristobal Rojas', 'Ning Zhong']|['math.DS', 'cs.CC', 'nlin.CD']
2017-03-28T14:07:13Z|2017-02-13T10:16:54Z|http://arxiv.org/abs/1702.03700v1|http://arxiv.org/pdf/1702.03700v1|Assortment Optimization under a Single Transition Model|assort optim singl transit model|In this paper, we consider a Markov chain choice model with single transition. In this model, customers arrive at each product with a certain probability. If the arrived product is unavailable, then the seller can recommend a subset of available products to the customer and the customer will purchase one of the recommended products or choose not to purchase with certain transition probabilities. The distinguishing features of the model are that the seller can control which products to recommend depending on the arrived product and that each customer either purchases a product or leaves the market after one transition.   We study the assortment optimization problem under this model. Particularly, we show that this problem is generally NP-Hard even if each product could only transit to at most two products. Despite the complexity of the problem, we provide polynomial time algorithms for several special cases, such as when the transition probabilities are homogeneous with respect to the starting point, or when each product can only transit to one other product. We also provide a tight performance bound for revenue-ordered assortments. In addition, we propose a compact mixed integer program formulation that can solve this problem of large size. Through extensive numerical experiments, we show that the proposed algorithms can solve the problem efficiently and the obtained assortments could significantly improve the revenue of the seller than under the Markov chain choice model.|paper consid markov chain choic model singl transit model custom arriv product certain probabl arriv product unavail seller recommend subset avail product custom custom purchas one recommend product choos purchas certain transit probabl distinguish featur model seller control product recommend depend arriv product custom either purchas product leav market one transit studi assort optim problem model particular show problem general np hard even product could onli transit two product despit complex problem provid polynomi time algorithm sever special case transit probabl homogen respect start point product onli transit one product also provid tight perform bound revenu order assort addit propos compact mix integ program formul solv problem larg size extens numer experi show propos algorithm solv problem effici obtain assort could signific improv revenu seller markov chain choic model|['Kameng Nip', 'Zhenbo Wang', 'Zizhuo Wang']|['math.OC', 'cs.CC']
2017-03-28T14:07:13Z|2017-02-13T04:23:37Z|http://arxiv.org/abs/1702.03625v1|http://arxiv.org/pdf/1702.03625v1|Separation of AC$^0[\oplus]$ Formulas and Circuits|separ ac oplus formula circuit|This paper gives the first separation between the power of {\em formulas} and {\em circuits} of equal depth in the $\mathrm{AC}^0[\oplus]$ basis (unbounded fan-in AND, OR, NOT and MOD$_2$ gates). We show, for all $d(n) \le O(\frac{\log n}{\log\log n})$, that there exist {\em polynomial-size depth-$d$ circuits} that are not equivalent to {\em depth-$d$ formulas of size $n^{o(d)}$} (moreover, this is optimal in that $n^{o(d)}$ cannot be improved to $n^{O(d)}$). This result is obtained by a combination of new lower and upper bounds for {\em Approximate Majorities}, the class of Boolean functions $\{0,1\}^n \to \{0,1\}$ that agree with the Majority function on $3/4$ fraction of inputs.   $\mathrm{AC}^0[\oplus]$ formula lower bound: We show that every depth-$d$ $\mathrm{AC}^0[\oplus]$ formula of size $s$ has a {\em $1/8$-error polynomial approximation} over $\mathbb{F}_2$ of degree $O(\frac{1}{d}\log s)^{d-1}$. This strengthens a classic $O(\log s)^{d-1}$ degree approximation for \underline{circuits} due to Razborov. Since the Majority function has approximate degree $\Theta(\sqrt n)$, this result implies an $\exp(\Omega(dn^{1/2(d-1)}))$ lower bound on the depth-$d$ $\mathrm{AC}^0[\oplus]$ formula size of all Approximate Majority functions for all $d(n) \le O(\log n)$.   Monotone $\mathrm{AC}^0$ circuit upper bound: For all $d(n) \le O(\frac{\log n}{\log\log n})$, we give a randomized construction of depth-$d$ monotone $\mathrm{AC}^0$ circuits (without NOT or MOD$_2$ gates) of size $\exp(O(n^{1/2(d-1)}))$ that compute an Approximate Majority function. This strengthens a construction of \underline{formulas} of size $\exp(O(dn^{1/2(d-1)}))$ due to Amano.|paper give first separ power em formula em circuit equal depth mathrm ac oplus basi unbound fan mod gate show le frac log log log exist em polynomi size depth circuit equival em depth formula size moreov optim cannot improv result obtain combin new lower upper bound em approxim major class boolean function agre major function fraction input mathrm ac oplus formula lower bound show everi depth mathrm ac oplus formula size em error polynomi approxim mathbb degre frac log strengthen classic log degre approxim underlin circuit due razborov sinc major function approxim degre theta sqrt result impli exp omega dn lower bound depth mathrm ac oplus formula size approxim major function le log monoton mathrm ac circuit upper bound le frac log log log give random construct depth monoton mathrm ac circuit without mod gate size exp comput approxim major function strengthen construct underlin formula size exp dn due amano|['Benjamin Rossman', 'Srikanth Srinivasan']|['cs.CC']
2017-03-28T14:07:13Z|2017-02-10T12:43:46Z|http://arxiv.org/abs/1702.03152v1|http://arxiv.org/pdf/1702.03152v1|A Variation of Levin Search for All Well-Defined Problems|variat levin search well defin problem|In 1973, L.A. Levin published an algorithm that solves any inversion problem $\pi$ as quickly as the fastest algorithm $p^*$ computing a solution for $\pi$ in time bounded by $2^{l(p^*)}.t^*$, where $l(p^*)$ is the length of the binary encoding of $p^*$, and $t^*$ is the runtime of $p^*$ plus the time to verify its correctness. In 2002, M. Hutter published an algorithm that solves any well-defined problem $\pi$ as quickly as the fastest algorithm $p^*$ computing a solution for $\pi$ in time bounded by $5.t_{p}(x)+d_p.time_{t_{p}}(x)+c_p$, where $d_p=40.2^{l(p)+l(t_{p})}$ and $c_p=40.2^{l(f)+1}.O(l(f)^2)$, where $l(f)$ is the length of the binary encoding of a proof $f$ that produces a pair $(p,t_p)$, where $t_p(x)$ is a provable time bound on the runtime of the fastest program $p$ provably equivalent to $p^*$. In this paper, we rewrite Levin Search using the ideas of Hutter so that we have a new simple algorithm that solves any well-defined problem $\pi$ as quickly as the fastest algorithm $p^*$ computing a solution for $\pi$ in time bounded by $O(l(f)^2).t_p(x)$.|levin publish algorithm solv ani invers problem pi quick fastest algorithm comput solut pi time bound length binari encod runtim plus time verifi correct hutter publish algorithm solv ani well defin problem pi quick fastest algorithm comput solut pi time bound time length binari encod proof produc pair provabl time bound runtim fastest program provabl equival paper rewrit levin search use idea hutter new simpl algorithm solv ani well defin problem pi quick fastest algorithm comput solut pi time bound|['Fouad B. Chedid']|['cs.CC', 'cs.DS']
2017-03-28T14:07:13Z|2017-02-09T16:50:23Z|http://arxiv.org/abs/1702.02890v1|http://arxiv.org/pdf/1702.02890v1|Answer Set Solving with Bounded Treewidth Revisited|answer set solv bound treewidth revisit|Parameterized algorithms are a way to solve hard problems more efficiently, given that a specific parameter of the input is small. In this paper, we apply this idea to the field of answer set programming (ASP). To this end, we propose two kinds of graph representations of programs to exploit their treewidth as a parameter. Treewidth roughly measures to which extent the internal structure of a program resembles a tree. Our main contribution is the design of parameterized dynamic programming algorithms, which run in linear time if the treewidth and weights of the given program are bounded. Compared to previous work, our algorithms handle the full syntax of ASP. Finally, we report on an empirical evaluation that shows good runtime behaviour for benchmark instances of low treewidth, especially for counting answer sets.|parameter algorithm way solv hard problem effici given specif paramet input small paper appli idea field answer set program asp end propos two kind graph represent program exploit treewidth paramet treewidth rough measur extent intern structur program resembl tree main contribut design parameter dynam program algorithm run linear time treewidth weight given program bound compar previous work algorithm handl full syntax asp final report empir evalu show good runtim behaviour benchmark instanc low treewidth especi count answer set|['Johannes Fichte', 'Markus Hecher', 'Michael Morak', 'Stefan Woltran']|['cs.LO', 'cs.AI', 'cs.CC']
2017-03-28T14:07:13Z|2017-02-09T16:32:26Z|http://arxiv.org/abs/1702.02885v1|http://arxiv.org/pdf/1702.02885v1|Sparse Approximation is Provably Hard under Coherent Dictionaries|spars approxim provabl hard coher dictionari|It is well known that sparse approximation problem is \textsf{NP}-hard under general dictionaries. Several algorithms have been devised and analyzed in the past decade under various assumptions on the \emph{coherence} $\mu$ of the dictionary represented by an $M \times N$ matrix from which a subset of $k$ column vectors is selected. All these results assume $\mu=O(k^{-1})$. This article is an attempt to bridge the big gap between the negative result of \textsf{NP}-hardness under general dictionaries and the positive results under this restrictive assumption. In particular, it suggests that the aforementioned assumption might be asymptotically the best one can make to arrive at any efficient algorithmic result under well-known conjectures of complexity theory. In establishing the results, we make use of a new simple multilayered PCP which is tailored to give a matrix with small coherence combined with our reduction.|well known spars approxim problem textsf np hard general dictionari sever algorithm devis analyz past decad various assumpt emph coher mu dictionari repres time matrix subset column vector select result assum mu articl attempt bridg big gap negat result textsf np hard general dictionari posit result restrict assumpt particular suggest aforement assumpt might asymptot best one make arriv ani effici algorithm result well known conjectur complex theori establish result make use new simpl multilay pcp tailor give matrix small coher combin reduct|['Ali Çivril']|['cs.CC', 'cs.IT', 'math.IT']
2017-03-28T14:07:13Z|2017-02-20T16:33:24Z|http://arxiv.org/abs/1702.02882v4|http://arxiv.org/pdf/1702.02882v4|Improved Inapproximability Results for Steiner Tree via Long Code Based   Reductions|improv inapproxim result steiner tree via long code base reduct|The best algorithm for approximating Steiner tree has performance ratio $\ln(4)+\epsilon \approx 1.386$ [J. Byrka et al., \textit{Proceedings of the 42th Annual ACM Symposium on Theory of Computing (STOC)}, 2010, pp. 583-592], whereas the inapproximability result stays at the factor $\frac{96}{95} \approx 1.0105$ [M. Chleb\'ik and J. Chleb\'ikov\'a, \textit{Proceedings of the 8th Scandinavian Workshop on Algorithm Theory (SWAT)}, 2002, pp. 170-179]. In this article, we take a step forward to bridge this gap and show that there is no polynomial time algorithm approximating Steiner tree with constant ratio better than $\frac{19}{18} \approx 1.0555$ unless \textsf{P = NP}. We also relate the problem to the Unique Games Conjecture by showing that it is \textsf{UG}-hard to find a constant approximation ratio better than $\frac{17}{16} = 1.0625$. In the special case of quasi-bipartite graphs, we prove an inapproximability factor of $\frac{25}{24} \approx 1.0416$ unless \textsf{P = NP}, which improves upon the previous bound of $\frac{128}{127} \approx 1.0078$. The reductions that we present for all the cases are of the same spirit with appropriate modifications. Our main technical contribution is an adaptation of a Set-Cover type reduction in which the Long Code is used to the geometric setting of the problems we consider.|best algorithm approxim steiner tree perform ratio ln epsilon approx byrka et al textit proceed th annual acm symposium theori comput stoc pp wherea inapproxim result stay factor frac approx chleb ik chleb ikov textit proceed th scandinavian workshop algorithm theori swat pp articl take step forward bridg gap show polynomi time algorithm approxim steiner tree constant ratio better frac approx unless textsf np also relat problem uniqu game conjectur show textsf ug hard find constant approxim ratio better frac special case quasi bipartit graph prove inapproxim factor frac approx unless textsf np improv upon previous bound frac approx reduct present case spirit appropri modif main technic contribut adapt set cover type reduct long code use geometr set problem consid|['Ali Çivril']|['cs.CC']
2017-03-28T14:07:13Z|2017-02-09T15:41:48Z|http://arxiv.org/abs/1702.02863v1|http://arxiv.org/pdf/1702.02863v1|Complexity Classification Of The Six-Vertex Model|complex classif six vertex model|We prove a complexity dichotomy theorem for the six-vertex model. For every setting of the parameters of the model, we prove that computing the partition function is either solvable in polynomial time or #P-hard. The dichotomy criterion is explicit.|prove complex dichotomi theorem six vertex model everi set paramet model prove comput partit function either solvabl polynomi time hard dichotomi criterion explicit|['Jin-Yi Cai', 'Zhiguo Fu', 'Mingji Xia']|['cs.CC']
2017-03-28T14:07:17Z|2017-02-09T13:18:08Z|http://arxiv.org/abs/1702.02821v1|http://arxiv.org/pdf/1702.02821v1|Phase Transitions of the Typical Algorithmic Complexity of the Random   Satisfiability Problem Studied with Linear Programming|phase transit typic algorithm complex random satisfi problem studi linear program|"The Boolean Satisfiability problem asks if a Boolean formula is satisfiable by some assignment of the variables or not. It belongs to the NP-complete complexity class and hence no algorithm with polynomial time worst-case complexity is known, i.e., the problem is hard. The K-SAT problem is the subset of the Boolean Satisfiability problem, for which the Boolean formula has the conjunctive normal form with K literals per clause. This problem is still NP-complete for $K \ge 3$. Although the worst case complexity of NP-complete problems is conjectured to be exponential, there might be subsets of the realizations where solutions can typically be found in polynomial time. In fact, random $K$-SAT, with the number of clauses to number of variables ratio $\alpha$ as control parameter, shows a phase transition between a satisfiable phase and an unsatisfiable phase, at which the hardest problems are located. We use here several linear programming approaches to reveal further ""easy-hard"" transition points at which the typical hardness of the problems increases which means that such algorithms can solve the problem on one side efficiently but not beyond this point. For one of these transitions, we observed a coincidence with a structural transition of the literal factor graphs of the problem instances. We also investigated cutting-plane approaches, which often increase the computational efficiency. Also we tried out a mapping to another NP-complete optimization problem using a specific algorithm for that problem. In both cases, no improvement of the performance was observed, i.e., no shift of the easy-hard transition to higher values of $\alpha$."|boolean satisfi problem ask boolean formula satisfi assign variabl belong np complet complex class henc algorithm polynomi time worst case complex known problem hard sat problem subset boolean satisfi problem boolean formula conjunct normal form liter per claus problem still np complet ge although worst case complex np complet problem conjectur exponenti might subset realize solut typic found polynomi time fact random sat number claus number variabl ratio alpha control paramet show phase transit satisfi phase unsatisfi phase hardest problem locat use sever linear program approach reveal easi hard transit point typic hard problem increas mean algorithm solv problem one side effici beyond point one transit observ coincid structur transit liter factor graph problem instanc also investig cut plane approach often increas comput effici also tri map anoth np complet optim problem use specif algorithm problem case improv perform observ shift easi hard transit higher valu alpha|['Hendrik Schawe', 'Roman Bleim', 'Alexander K. Hartmann']|['cond-mat.dis-nn', 'cond-mat.stat-mech', 'cs.AI', 'cs.CC']
2017-03-28T14:07:17Z|2017-02-09T03:51:01Z|http://arxiv.org/abs/1702.02693v1|http://arxiv.org/pdf/1702.02693v1|Dichotomy for Real Holant$^c$ Problems|dichotomi real holant problem|Holant problems capture a class of Sum-of-Product computations such as counting matchings. It is inspired by holographic algorithms and is equivalent to tensor networks, with counting CSP being a special case. A classification for Holant problems is more difficult to prove, not only because it implies a classification for counting CSP, but also due to the deeper reason that there exist more intricate polynomial time tractable problems in the broader framework.   We discover a new family of constraint functions $\mathscr{L}$ which define polynomial time computable counting problems. These do not appear in counting CSP, and no newly discovered tractable constraints can be symmetric. It has a delicate support structure related to error-correcting codes. Local holographic transformations is fundamental in its tractability. We prove a complexity dichotomy theorem for all Holant problems defined by any real valued constraint function set on Boolean variables and contains two 0-1 pinning functions. Previously, dichotomy for the same framework was only known for symmetric constraint functions. he set $\mathscr{L}$ supplies the last piece of tractability. We also prove a dichotomy for a variant of counting CSP as a technical component toward this Holant dichotomy.|holant problem captur class sum product comput count match inspir holograph algorithm equival tensor network count csp special case classif holant problem difficult prove onli becaus impli classif count csp also due deeper reason exist intric polynomi time tractabl problem broader framework discov new famili constraint function mathscr defin polynomi time comput count problem appear count csp newli discov tractabl constraint symmetr delic support structur relat error correct code local holograph transform fundament tractabl prove complex dichotomi theorem holant problem defin ani real valu constraint function set boolean variabl contain two pin function previous dichotomi framework onli known symmetr constraint function set mathscr suppli last piec tractabl also prove dichotomi variant count csp technic compon toward holant dichotomi|['Jin-Yi Cai', 'Pinyan Lu', 'Mingji Xia']|['cs.CC', 'cs.DS']
2017-03-28T14:07:17Z|2017-02-08T12:08:22Z|http://arxiv.org/abs/1702.01666v2|http://arxiv.org/pdf/1702.01666v2|On the Complexity of Estimating Renyi Divergences|complex estim renyi diverg|This paper studies the complexity of estimating Renyi divergences of discrete distributions: $p$ observed from samples and the baseline distribution $q$ known \emph{a priori}. Extending the results of Acharya et al. (SODA'15) on estimating Renyi entropy, we present improved estimation techniques together with upper and lower bounds on the sample complexity.   We show that, contrarily to estimating Renyi entropy where a sublinear (in the alphabet size) number of samples suffices, the sample complexity is heavily dependent on \emph{events occurring unlikely} in $q$, and is unbounded in general (no matter what an estimation technique is used). For any divergence of order bigger than $1$, we provide upper and lower bounds on the number of samples dependent on probabilities of $p$ and $q$. We conclude that the worst-case sample complexity is polynomial in the alphabet size if and only if the probabilities of $q$ are non-negligible.   This gives theoretical insights into heuristics used in applied papers to handle numerical instability, which occurs for small probabilities of $q$. Our result explains that small probabilities should be handled with care not only because of numerical issues, but also because of a blow up in sample complexity.|paper studi complex estim renyi diverg discret distribut observ sampl baselin distribut known emph priori extend result acharya et al soda estim renyi entropi present improv estim techniqu togeth upper lower bound sampl complex show contrarili estim renyi entropi sublinear alphabet size number sampl suffic sampl complex heavili depend emph event occur unlik unbound general matter estim techniqu use ani diverg order bigger provid upper lower bound number sampl depend probabl conclud worst case sampl complex polynomi alphabet size onli probabl non neglig give theoret insight heurist use appli paper handl numer instabl occur small probabl result explain small probabl handl care onli becaus numer issu also becaus blow sampl complex|['Maciej Skorski']|['cs.IT', 'cs.CC', 'math.IT', 'H.1.1']
2017-03-28T14:07:17Z|2017-02-05T21:24:18Z|http://arxiv.org/abs/1702.01454v1|http://arxiv.org/pdf/1702.01454v1|Property Testing of Joint Distributions using Conditional Samples|properti test joint distribut use condit sampl|In this paper, we present the first non-trivial property tester for joint probability distributions in the recently introduced conditional sampling model. The conditional sampling framework provides an oracle for a distribution $\mu$ that takes as input a subset $S$ of the domain $\Omega$ and returns a sample from the distribution $\mu$ conditioned on $S$.For a joint distribution of dimension $n$, we give a $\tilde{\mathcal{O}}(n^3)$-query uniformity tester, a $\tilde{\mathcal{O}}(n^3)$-query identity tester with a known distribution, and a $\tilde{\mathcal{O}}(n^6)$-query tester for testing independence of marginals. Our technique involves an elegant chain rule which can be proved using basic techniques of probability theory, yet powerful enough to avoid the curse of dimensionality.   We also prove a sample complexity lower bound of $\Omega(\sqrt[4]{n})$ for testing uniformity of a joint distribution when the tester is only allowed to condition independently on the marginals. Our technique involves novel relations between Hellinger distance and total variational distance, and may be of independent interest.|paper present first non trivial properti tester joint probabl distribut recent introduc condit sampl model condit sampl framework provid oracl distribut mu take input subset domain omega return sampl distribut mu condit joint distribut dimens give tild mathcal queri uniform tester tild mathcal queri ident tester known distribut tild mathcal queri tester test independ margin techniqu involv eleg chain rule prove use basic techniqu probabl theori yet power enough avoid curs dimension also prove sampl complex lower bound omega sqrt test uniform joint distribut tester onli allow condit independ margin techniqu involv novel relat helling distanc total variat distanc may independ interest|['Rishiraj Bhattacharyya', 'Sourav Chakraborty']|['cs.CC']
2017-03-28T14:07:17Z|2017-02-05T16:21:35Z|http://arxiv.org/abs/1702.01423v1|http://arxiv.org/pdf/1702.01423v1|Deciding Irreducibility/Indecomposability of Feedback Shift Registers is   NP-hard|decid irreduc indecompos feedback shift regist np hard|Feedback shift registers(FSRs) are a fundamental component in electronics and secure communication. An FSR $f$ is said to be reducible if all the output sequences of another FSR $g$ can also be generated by $f$ and the FSR $g$ has less memory than $f$. An FSR is said to be decomposable if it has the same set of output sequences as a cascade connection of two FSRs. It is proved that deciding whether FSRs are irreducible/indecomposable is NP-hard.|feedback shift regist fsrs fundament compon electron secur communic fsr said reduc output sequenc anoth fsr also generat fsr less memori fsr said decompos set output sequenc cascad connect two fsrs prove decid whether fsrs irreduc indecompos np hard|['Lin Wang']|['cs.CC', '68Q25, 94A55, 94C15']
2017-03-28T14:07:17Z|2017-02-03T22:34:34Z|http://arxiv.org/abs/1702.02017v1|http://arxiv.org/pdf/1702.02017v1|Pushing the Bounds for Matrix-Matrix Multiplication|push bound matrix matrix multipl|A tight lower bound for required I/O when computing a matrix-matrix multiplication on a processor with two layers of memory is established. Prior work obtained weaker lower bounds by reasoning about the number of \textit{phases} needed to perform $C:=AB$, where each phase is a series of operations involving $S$ reads and writes to and from fast memory, and $S$ is the size of fast memory. A lower bound on the number of phases was then determined by obtaining an upper bound on the number of scalar multiplications performed per phase. This paper follows the same high level approach, but improves the lower bound by considering $C:=AB+C$ instead of $C:=AB$, and obtains the maximum number of scalar fused multiply-adds (FMAs) per phase instead of scalar additions. Key to obtaining the new result is the decoupling of the per-phase I/O from the size of fast memory. The new lower bound is $2mnk/\sqrt{S}-2S$. The constant for the leading term is an improvement of a factor $4\sqrt{2}$. A theoretical algorithm that attains the lower bound is given, and how the state-of-the-art Goto's algorithm also in some sense meets the lower bound is discussed.|tight lower bound requir comput matrix matrix multipl processor two layer memori establish prior work obtain weaker lower bound reason number textit phase need perform ab phase seri oper involv read write fast memori size fast memori lower bound number phase determin obtain upper bound number scalar multipl perform per phase paper follow high level approach improv lower bound consid ab instead ab obtain maximum number scalar fuse multipli add fmas per phase instead scalar addit key obtain new result decoupl per phase size fast memori new lower bound mnk sqrt constant lead term improv factor sqrt theoret algorithm attain lower bound given state art goto algorithm also sens meet lower bound discuss|['Tyler Michael Smith', 'Robert A. van de Geijn']|['cs.CC']
2017-03-28T14:07:17Z|2017-02-02T18:01:03Z|http://arxiv.org/abs/1702.00767v1|http://arxiv.org/pdf/1702.00767v1|A new Holant dichotomy inspired by quantum computation|new holant dichotomi inspir quantum comput|Holant problems are a framework for the analysis of counting complexity problems on graphs. This framework is simultaneously general enough to encompass many other counting problems on graphs and specific enough to allow the derivation of dichotomy results, partitioning all problem instances into those which can be solved in polynomial time and those which are #P-hard. The Holant framework is based on the theory of holographic algorithms, which was originally inspired by concepts from quantum computation, but this connection appears not to have been explored before.   Here, we employ quantum information theory to explain existing results in a concise way and to derive a dichotomy for a new family of problems, which we call Holant$^+$. This family sits in between the known families of Holant$^*$, for which a full dichotomy is known, and Holant$^c$, for which only a restricted dichotomy is known. Using knowledge from entanglement theory -- both previously existing work and new results of our own -- we prove a full dichotomy theorem for Holant$^+$, which is very similar to the restricted Holant$^c$ dichotomy. Other than the dichotomy for #R$_3$-CSP, ours is the first Holant dichotomy in which the allowed functions are not restricted and in which only a finite number of functions are assumed to be freely available.|holant problem framework analysi count complex problem graph framework simultan general enough encompass mani count problem graph specif enough allow deriv dichotomi result partit problem instanc solv polynomi time hard holant framework base theori holograph algorithm origin inspir concept quantum comput connect appear explor befor employ quantum inform theori explain exist result concis way deriv dichotomi new famili problem call holant famili sit known famili holant full dichotomi known holant onli restrict dichotomi known use knowledg entangl theori previous exist work new result prove full dichotomi theorem holant veri similar restrict holant dichotomi dichotomi csp first holant dichotomi allow function restrict onli finit number function assum freeli avail|['Miriam Backens']|['quant-ph', 'cs.CC']
2017-03-28T14:07:17Z|2017-02-02T07:20:18Z|http://arxiv.org/abs/1702.00558v1|http://arxiv.org/pdf/1702.00558v1|Irreducibility and r-th root finding over finite fields|irreduc th root find finit field|Constructing $r$-th nonresidue over a finite field is a fundamental computational problem. A related problem is to construct an irreducible polynomial of degree $r^e$ (where $r$ is a prime) over a given finite field $\mathbb{F}_q$ of characteristic $p$ (equivalently, constructing the bigger field $\mathbb{F}_{q^{r^e}}$). Both these problems have famous randomized algorithms but the derandomization is an open question. We give some new connections between these two problems and their variants.   In 1897, Stickelberger proved that if a polynomial has an odd number of even degree factors, then its discriminant is a quadratic nonresidue in the field. We give an extension of Stickelberger's Lemma; we construct $r$-th nonresidues from a polynomial $f$ for which there is a $d$, such that, $r d$ and $r\nmid\,$#(irreducible factor of $f(x)$ of degree $d$). Our theorem has the following interesting consequences: (1) we can construct $\mathbb{F}_{q^m}$ in deterministic poly(deg($f$),$m\log q$)-time if $m$ is an $r$-power and $f$ is known; (2) we can find $r$-th roots in $\mathbb{F}_{p^m}$ in deterministic poly($m\log p$)-time if $r$ is constant and $r \gcd(m,p-1)$.   We also discuss a conjecture significantly weaker than the Generalized Riemann hypothesis to get a deterministic poly-time algorithm for $r$-th root finding.|construct th nonresidu finit field fundament comput problem relat problem construct irreduc polynomi degre prime given finit field mathbb characterist equival construct bigger field mathbb problem famous random algorithm derandom open question give new connect two problem variant stickelberg prove polynomi odd number even degre factor discrimin quadrat nonresidu field give extens stickelberg lemma construct th nonresidu polynomi nmid irreduc factor degre theorem follow interest consequ construct mathbb determinist poli deg log time power known find th root mathbb determinist poli log time constant gcd also discuss conjectur signific weaker general riemann hypothesi get determinist poli time algorithm th root find|['Vishwas Bhargava', 'Gábor Ivanyos', 'Rajat Mittal', 'Nitin Saxena']|['cs.CC', 'math.AC', 'math.NT']
2017-03-28T14:07:17Z|2017-02-02T03:23:39Z|http://arxiv.org/abs/1702.00533v1|http://arxiv.org/pdf/1702.00533v1|Complexity results for $k$-domination and $α$-domination problems   and their variants|complex result domin domin problem variant|Let $G=(V, E)$ be a simple and undirected graph. For some integer $k\geq 1$, a set $D\subseteq V$ is said to be a k-dominating set in $G$ if every vertex $v$ of $G$ outside $D$ has at least $k$ neighbors in $D$. Furthermore, for some real number $\alpha$ with $0<\alpha\leq1$, a set $D\subseteq V$ is called an $\alpha$-dominating set in $G$ if every vertex $v$ of $G$ outside $D$ has at least $\alpha\times d_v$ neighbors in $D$, where $d_v$ is the degree of $v$ in $G$. The cardinality of a minimum $k$-dominating set and a minimum $\alpha$-dominating set in $G$ is said to be the $k$-domination number and the $\alpha$-domination number of $G$, respectively. In this paper, we present some approximability and inapproximability results on the problem of finding $k$-domination number and $\alpha$-domination number of some classes of graphs. Moreover, we introduce a generalization of $\alpha$-dominating set which we call an $f$-dominating set. Given a function $f:\mathbb{N}\rightarrow \mathbb{R}$, where $\mathbb{N}=\{1, 2, 3, \ldots\}$, a set $D\subseteq V$ is said to be an $f$-dominating set in $G$ if every vertex $v$ of $G$ outside $D$ has at least $f(d_v)$ neighbors in $D$. We prove NP-hardness of the problem of finding a minimum $f$-dominating set in $G$, for a large family of functions $f$.|let simpl undirect graph integ geq set subseteq said domin set everi vertex outsid least neighbor furthermor real number alpha alpha leq set subseteq call alpha domin set everi vertex outsid least alpha time neighbor degre cardin minimum domin set minimum alpha domin set said domin number alpha domin number respect paper present approxim inapproxim result problem find domin number alpha domin number class graph moreov introduc general alpha domin set call domin set given function mathbb rightarrow mathbb mathbb ldot set subseteq said domin set everi vertex outsid least neighbor prove np hard problem find minimum domin set larg famili function|['Davood Bakhshesh', 'Mohammad Farshi', 'Mahdieh Hasheminezhad']|['cs.CC', 'math.CO', '05C69, 68R05, 68Q25']
2017-03-28T14:07:17Z|2017-02-01T21:54:41Z|http://arxiv.org/abs/1702.00467v1|http://arxiv.org/pdf/1702.00467v1|The Computer Science and Physics of Community Detection: Landscapes,   Phase Transitions, and Hardness|comput scienc physic communiti detect landscap phase transit hard|"Community detection in graphs is the problem of finding groups of vertices which are more densely connected than they are to the rest of the graph. This problem has a long history, but it is currently motivated by social and biological networks. While there are many ways to formalize it, one of the most popular is as an inference problem, where there is a planted ""ground truth"" community structure around which the graph is generated probabilistically. Our task is then to recover the ground truth knowing only the graph.   Recently it was discovered, first heuristically in physics and then rigorously in probability and computer science, that this problem has a phase transition at which it suddenly becomes impossible. Namely, if the graph is too sparse, or the probabilistic process that generates it is too noisy, then no algorithm can find a partition that is correlated with the planted one---or even tell if there are communities, i.e., distinguish the graph from a purely random one with high probability. Above this information-theoretic threshold, there is a second threshold beyond which polynomial-time algorithms are known to succeed; in between, there is a regime in which community detection is possible, but conjectured to be exponentially hard.   For computer scientists, this field offers a wealth of new ideas and open questions, with connections to probability and combinatorics, message-passing algorithms, and random matrix theory. Perhaps more importantly, it provides a window into the cultures of statistical physics and statistical inference, and how those cultures think about distributions of instances, landscapes of solutions, and hardness."|communiti detect graph problem find group vertic dens connect rest graph problem long histori current motiv social biolog network mani way formal one popular infer problem plant ground truth communiti structur around graph generat probabilist task recov ground truth know onli graph recent discov first heurist physic rigor probabl comput scienc problem phase transit sudden becom imposs name graph spars probabilist process generat noisi algorithm find partit correl plant one even tell communiti distinguish graph pure random one high probabl abov inform theoret threshold second threshold beyond polynomi time algorithm known succeed regim communiti detect possibl conjectur exponenti hard comput scientist field offer wealth new idea open question connect probabl combinator messag pass algorithm random matrix theori perhap import provid window cultur statist physic statist infer cultur think distribut instanc landscap solut hard|['Cristopher Moore']|['cs.CC', 'cond-mat.stat-mech', 'cs.SI', 'math.PR', 'physics.soc-ph']
2017-04-07T11:27:57Z|2017-04-06T17:07:10Z|http://arxiv.org/abs/1704.01937v1|http://arxiv.org/pdf/1704.01937v1|Promise Constraint Satisfaction: Algebraic Structure and a Symmetric   Boolean Dichotomy|promis constraint satisfact algebra structur symmetr boolean dichotomi|A classic result due to Schaefer (1978) classifies all constraint satisfaction problems (CSPs) over the Boolean domain as being either in $\mathsf{P}$ or $\mathsf{NP}$-hard. This paper considers a promise-problem variant of CSPs called PCSPs. A PCSP over a finite set of pairs of constraints $\Gamma$ consists of a pair $(\Psi_P, \Psi_Q)$ of CSPs with the same set of variables such that for every $(P, Q) \in \Gamma$, $P(x_{i_1}, ..., x_{i_k})$ is a clause of $\Psi_P$ if and only if $Q(x_{i_1}, ..., x_{i_k})$ is a clause of $\Psi_Q$. The promise problem $\operatorname{PCSP}(\Gamma)$ is to distinguish, given $(\Psi_P, \Psi_Q)$, between the cases $\Psi_P$ is satisfiable and $\Psi_Q$ is unsatisfiable. Many natural problems including approximate graph and hypergraph coloring can be placed in this framework.   This paper is motivated by the pursuit of understanding the computational complexity of Boolean promise CSPs. As our main result, we show that $\operatorname{PCSP}(\Gamma)$ exhibits a dichotomy (it is either polynomial time solvable or $\mathsf{NP}$-hard) when the relations in $\Gamma$ are symmetric and allow for negations of variables. We achieve our dichotomy theorem by extending the weak polymorphism framework of Austrin, Guruswami, and H\aa stad [FOCS '14] which itself is a generalization of the algebraic approach to study CSPs. In both the algorithm and hardness portions of our proof, we incorporate new ideas and techniques not utilized in the CSP case.   Furthermore, we show that the computational complexity of any promise CSP (over arbitrary finite domains) is captured entirely by its weak polymorphisms, a feature known as Galois correspondence, as well as give necessary and sufficient conditions for the structure of this set of weak polymorphisms. Such insights call us to question the existence of a general dichotomy for Boolean PCSPs.|classic result due schaefer classifi constraint satisfact problem csps boolean domain either mathsf mathsf np hard paper consid promis problem variant csps call pcsps pcsp finit set pair constraint gamma consist pair psi psi csps set variabl everi gamma claus psi onli claus psi promis problem operatornam pcsp gamma distinguish given psi psi case psi satisfi psi unsatisfi mani natur problem includ approxim graph hypergraph color place framework paper motiv pursuit understand comput complex boolean promis csps main result show operatornam pcsp gamma exhibit dichotomi either polynomi time solvabl mathsf np hard relat gamma symmetr allow negat variabl achiev dichotomi theorem extend weak polymorph framework austrin guruswami aa stad foc general algebra approach studi csps algorithm hard portion proof incorpor new idea techniqu util csp case furthermor show comput complex ani promis csp arbitrari finit domain captur entir weak polymorph featur known galoi correspond well give necessari suffici condit structur set weak polymorph insight call us question exist general dichotomi boolean pcsps|['Joshua Brakensiek', 'Venkatesan Guruswami']|['cs.CC', 'cs.DM', 'cs.LO']
2017-04-07T11:27:57Z|2017-04-06T16:50:26Z|http://arxiv.org/abs/1704.01929v1|http://arxiv.org/pdf/1704.01929v1|The Matching Problem in General Graphs is in Quasi-NC|match problem general graph quasi nc|We show that the perfect matching problem in general graphs is in Quasi-NC. That is, we give a deterministic parallel algorithm which runs in $O(\log^3 n)$ time on $n^{O(\log^2 n)}$ processors. The result is obtained by a derandomization of the Isolation Lemma for perfect matchings, which was introduced in the classic paper by Mulmuley, Vazirani and Vazirani [1987] to obtain a Randomized NC algorithm.   Our proof extends the framework of Fenner, Gurjar and Thierauf [2016], who proved the analogous result in the special case of bipartite graphs. Compared to that setting, several new ingredients are needed due to the significantly more complex structure of perfect matchings in general graphs. In particular, our proof heavily relies on the laminar structure of the faces of the perfect matching polytope.|show perfect match problem general graph quasi nc give determinist parallel algorithm run log time log processor result obtain derandom isol lemma perfect match introduc classic paper mulmuley vazirani vazirani obtain random nc algorithm proof extend framework fenner gurjar thierauf prove analog result special case bipartit graph compar set sever new ingredi need due signific complex structur perfect match general graph particular proof heavili reli laminar structur face perfect match polytop|['Ola Svensson', 'Jakub Tarnawski']|['cs.CC', 'math.CO']
2017-04-07T11:27:57Z|2017-04-06T16:27:29Z|http://arxiv.org/abs/1704.01914v1|http://arxiv.org/pdf/1704.01914v1|The Proof of CSP Dichotomy Conjecture|proof csp dichotomi conjectur|Many natural combinatorial problems can be expressed as constraint satisfaction problems. This class of problems is known to be NP-complete in general, but certain restrictions on the form of the constraints can ensure tractability. The standard way to parameterize interesting subclasses of the constraint satisfaction problem is via finite constraint languages. The main problem is to classify those subclasses that are solvable in polynomial time and those that are NP-complete. It was conjectured that if a core of a constraint language has a weak near unanimity polymorphism then the corresponding constraint satisfaction problem is tractable, otherwise it is NP-complete.   In the paper we present an algorithm that solves Constraint Satisfaction Problem in polynomial time for constraint languages having a weak near unanimity polymorphism, which proves the remaining part of the conjecture. Also we present the main theorems that explain why the algorithm works. The complete proof will be published online a bit later as the second version of this paper.|mani natur combinatori problem express constraint satisfact problem class problem known np complet general certain restrict form constraint ensur tractabl standard way parameter interest subclass constraint satisfact problem via finit constraint languag main problem classifi subclass solvabl polynomi time np complet conjectur core constraint languag weak near unanim polymorph correspond constraint satisfact problem tractabl otherwis np complet paper present algorithm solv constraint satisfact problem polynomi time constraint languag weak near unanim polymorph prove remain part conjectur also present main theorem explain whi algorithm work complet proof publish onlin bit later second version paper|['Dmitriy Zhuk']|['cs.CC']
2017-04-07T11:27:57Z|2017-04-05T21:56:54Z|http://arxiv.org/abs/1704.01657v1|http://arxiv.org/pdf/1704.01657v1|A Complexity Trichotomy for the Six-Vertex Model|complex trichotomi six vertex model|"We prove a complexity classification theorem that divides the six-vertex model into exactly three types. For every setting of the parameters of the model, the computation of the partition function is precisely: (1) Solvable in polynomial time for every graph, or (2) #P-hard for general graphs but solvable in polynomial time for planar graphs, or (3) #P-hard even for planar graphs. The classification has an explicit criterion. In addition to matchgates and matchgates-transformable signatures, we discover previously unknown families of planar-tractable partition functions by a non-local connection to #CSP, defined in terms of a ""loop space"". For the proof of #P-hardness, we introduce the use of M\""{o}bius transformations as a powerful new tool to prove that certain complexity reductions succeed in polynomial time."|prove complex classif theorem divid six vertex model exact three type everi set paramet model comput partit function precis solvabl polynomi time everi graph hard general graph solvabl polynomi time planar graph hard even planar graph classif explicit criterion addit matchgat matchgat transform signatur discov previous unknown famili planar tractabl partit function non local connect csp defin term loop space proof hard introduc use bius transform power new tool prove certain complex reduct succeed polynomi time|['Jin-Yi Cai', 'Zhiguo Fu', 'Shuai Shao']|['cs.CC']
2017-04-07T11:27:57Z|2017-04-05T16:32:13Z|http://arxiv.org/abs/1704.01514v1|http://arxiv.org/pdf/1704.01514v1|Merlinization of complexity classes above BQP|merlin complex class abov bqp|"We study how complexity classes above BQP, such as postBQP, ${\rm postBQP}_{\rm FP}$, and SBQP, change if we ""Merlinize"" them, i.e., if we allow an extra input quantum state (or classical bit string) given by Merlin as witness. Main results are the following three: First, the Merlinized version of postBQP is equal to PSPACE. Second, if the Merlinized postBQP is restricted in such a way that the postselection probability is equal to all witness states, then the class is equal to PP. Finally, the Merlinization does not change the class SBQP."|studi complex class abov bqp postbqp rm postbqp rm fp sbqp chang merlin allow extra input quantum state classic bit string given merlin wit main result follow three first merlin version postbqp equal pspace second merlin postbqp restrict way postselect probabl equal wit state class equal pp final merlin doe chang class sbqp|['Tomoyuki Morimae', 'Harumichi Nishimura']|['quant-ph', 'cs.CC']
2017-04-07T11:27:57Z|2017-04-05T13:23:09Z|http://arxiv.org/abs/1704.01405v1|http://arxiv.org/pdf/1704.01405v1|Polynomial running times for polynomial-time oracle machines|polynomi run time polynomi time oracl machin|"This paper introduces a more restrictive notion of feasibility of functionals on Baire space than the established one from second-order complexity theory. Thereby making it possible to consider functions on the natural numbers as running times of oracle Turing machines and avoiding second-order polynomials, which are notoriously difficult to handle. Furthermore, all machines that witness this stronger kind of feasibility can be clocked and the different traditions of treating partial operators from computable analysis and second-order complexity theory are equated in a precise sense. The new notion is named ""strong polynomial-time computability"", and proven to be a strictly stronger requirement than polynomial-time computability. It is proven that within the framework for complexity of operators from analysis introduced by Kawamura and Cook the classes of strongly polynomial-time computable operators and polynomial-time computable operators coincide."|paper introduc restrict notion feasibl function bair space establish one second order complex theori therebi make possibl consid function natur number run time oracl ture machin avoid second order polynomi notori difficult handl furthermor machin wit stronger kind feasibl clock differ tradit treat partial oper comput analysi second order complex theori equat precis sens new notion name strong polynomi time comput proven strict stronger requir polynomi time comput proven within framework complex oper analysi introduc kawamura cook class strong polynomi time comput oper polynomi time comput oper coincid|['Akitoshi Kawamura', 'Florian Steinberg']|['cs.CC']
2017-04-07T11:27:57Z|2017-04-04T16:57:18Z|http://arxiv.org/abs/1704.01104v1|http://arxiv.org/pdf/1704.01104v1|Communication Complexity of Correlated Equilibrium in Two-Player Games|communic complex correl equilibrium two player game|We show a communication complexity lower bound for finding a correlated equilibrium of a two-player game. More precisely, we define a two-player $N \times N$ game called the 2-cycle game and show that the randomized communication complexity of finding a 1/poly($N$)-approximate correlated equilibrium of the 2-cycle game is $\Omega(N)$. For small approximation values, this answers an open question of Babichenko and Rubinstein (STOC 2017). Our lower bound is obtained via a direct reduction from the unique set disjointness problem.|show communic complex lower bound find correl equilibrium two player game precis defin two player time game call cycl game show random communic complex find poli approxim correl equilibrium cycl game omega small approxim valu answer open question babichenko rubinstein stoc lower bound obtain via direct reduct uniqu set disjoint problem|['Anat Ganor', 'Karthik C. S.']|['cs.GT', 'cs.CC']
2017-04-07T11:27:57Z|2017-04-04T16:45:29Z|http://arxiv.org/abs/1704.01101v1|http://arxiv.org/pdf/1704.01101v1|On Resource-bounded versions of the van Lambalgen theorem|resourc bound version van lambalgen theorem|"The van Lambalgen theorem is a surprising result in algorithmic information theory concerning the symmetry of relative randomness. It establishes that for any pair of infinite sequences $A$ and $B$, $B$ is Martin-L\""of random and $A$ is Martin-L\""of random relative to $B$ if and only if the interleaved sequence $A \uplus B$ is Martin-L\""of random. This implies that $A$ is relative random to $B$ if and only if $B$ is random relative to $A$ \cite{vanLambalgen}, \cite{Nies09}, \cite{HirschfeldtBook}. This paper studies the validity of this phenomenon for different notions of time-bounded relative randomness.   We prove the classical van Lambalgen theorem using martingales and Kolmogorov compressibility. We establish the failure of relative randomness in these settings, for both time-bounded martingales and time-bounded Kolmogorov complexity. We adapt our classical proofs when applicable to the time-bounded setting, and construct counterexamples when they fail. The mode of failure of the theorem may depend on the notion of time-bounded randomness."|van lambalgen theorem surpris result algorithm inform theori concern symmetri relat random establish ani pair infinit sequenc martin random martin random relat onli interleav sequenc uplus martin random impli relat random onli random relat cite vanlambalgen cite nie cite hirschfeldtbook paper studi valid phenomenon differ notion time bound relat random prove classic van lambalgen theorem use martingal kolmogorov compress establish failur relat random set time bound martingal time bound kolmogorov complex adapt classic proof applic time bound set construct counterexampl fail mode failur theorem may depend notion time bound random|['Diptarka Chakraborty', 'Satyadev Nandakumar', 'Himanshu Shukla']|['cs.CC']
2017-04-07T11:27:57Z|2017-04-03T19:25:23Z|http://arxiv.org/abs/1704.00777v1|http://arxiv.org/pdf/1704.00777v1|The Unbounded-Error Communication Complexity of symmetric XOR functions|unbound error communic complex symmetr xor function|Settling a conjecture of Shi and Zhang, we determine the unbounded-error communication complexity of the symmetric XOR functions up to a poly-logarithmic factor. Our proof is by a simple reduction to an earlier result of Sherstov.|settl conjectur shi zhang determin unbound error communic complex symmetr xor function poli logarithm factor proof simpl reduct earlier result sherstov|['Hamed Hatami', 'Yingjie Qian']|['cs.CC']
2017-04-07T11:27:57Z|2017-04-03T19:07:48Z|http://arxiv.org/abs/1704.00765v1|http://arxiv.org/pdf/1704.00765v1|Quantum Algorithms for Graph Connectivity and Formula Evaluation|quantum algorithm graph connect formula evalu|"We give a new upper bound on the quantum query complexity of deciding $st$-connectivity on certain classes of planar graphs, and show the bound is sometimes exponentially better than previous results. We then show Boolean formula evaluation reduces to deciding connectivity on just such a class of graphs. Applying the algorithm for $st$-connectivity to Boolean formula evaluation problems, we match the $O(\sqrt{N})$ bound on the quantum query complexity of evaluating formulas on $N$ variables, give a quadratic speed-up over the classical query complexity of a certain class of promise Boolean formulas, and show this approach can yield superpolynomial quantum/classical separations. These results indicate that this $st$-connectivity-based approach may be the ""right"" way of looking at quantum algorithms for formula evaluation."|give new upper bound quantum queri complex decid st connect certain class planar graph show bound sometim exponenti better previous result show boolean formula evalu reduc decid connect class graph appli algorithm st connect boolean formula evalu problem match sqrt bound quantum queri complex evalu formula variabl give quadrat speed classic queri complex certain class promis boolean formula show approach yield superpolynomi quantum classic separ result indic st connect base approach may right way look quantum algorithm formula evalu|['Stacey Jeffery', 'Shelby Kimmel']|['quant-ph', 'cs.CC', 'cs.DS']
2017-04-07T11:28:01Z|2017-04-03T17:12:20Z|http://arxiv.org/abs/1704.00690v1|http://arxiv.org/pdf/1704.00690v1|Quantum advantage with shallow circuits|quantum advantag shallow circuit|We prove that constant-depth quantum circuits are more powerful than their classical counterparts. To this end we introduce a non-oracular version of the Bernstein-Vazirani problem which we call the 2D Hidden Linear Function problem. An instance of the problem is specified by a quadratic form q that maps n-bit strings to integers modulo four. The goal is to identify a linear boolean function which describes the action of q on a certain subset of n-bit strings. We prove that any classical probabilistic circuit composed of bounded fan-in gates that solves the 2D Hidden Linear Function problem with high probability must have depth logarithmic in n. In contrast, we show that this problem can be solved with certainty by a constant-depth quantum circuit composed of one- and two-qubit gates acting locally on a two-dimensional grid.|prove constant depth quantum circuit power classic counterpart end introduc non oracular version bernstein vazirani problem call hidden linear function problem instanc problem specifi quadrat form map bit string integ modulo four goal identifi linear boolean function describ action certain subset bit string prove ani classic probabilist circuit compos bound fan gate solv hidden linear function problem high probabl must depth logarithm contrast show problem solv certainti constant depth quantum circuit compos one two qubit gate act local two dimension grid|['Sergey Bravyi', 'David Gosset', 'Robert Koenig']|['quant-ph', 'cs.CC']
2017-04-07T11:28:01Z|2017-04-03T15:12:38Z|http://arxiv.org/abs/1704.00633v1|http://arxiv.org/pdf/1704.00633v1|Optimal lower bounds for universal relation, and for samplers and   finding duplicates in streams|optim lower bound univers relat sampler find duplic stream|In the communication problem $\mathbf{UR}$ (universal relation) [KRW95], Alice and Bob respectively receive $x, y \in\{0,1\}^n$ with the promise that $x\neq y$. The last player to receive a message must output an index $i$ such that $x_i\neq y_i$. We prove that the randomized one-way communication complexity of this problem in the public coin model is exactly $\Theta(\min\{n,\log(1/\delta)\log^2(\frac n{\log(1/\delta)})\})$ for failure probability $\delta$. Our lower bound holds even if promised $\mathop{support}(y)\subset \mathop{support}(x)$. As a corollary, we obtain optimal lower bounds for $\ell_p$-sampling in strict turnstile streams for $0\le p < 2$, as well as for the problem of finding duplicates in a stream. Our lower bounds do not need to use large weights, and hold even if promised $x\in\{0,1\}^n$ at all points in the stream.   We give two different proofs of our main result. The first proof demonstrates that any algorithm $\mathcal A$ solving sampling problems in turnstile streams in low memory can be used to encode subsets of $[n]$ of certain sizes into a number of bits below the information theoretic minimum. Our encoder makes adaptive queries to $\mathcal A$ throughout its execution, but done carefully so as to not violate correctness. This is accomplished by injecting random noise into the encoder's interactions with $\mathcal A$, which is loosely motivated by techniques in differential privacy. Our second proof is via a novel randomized reduction from Augmented Indexing [MNSW98] which needs to interact with $\mathcal A$ adaptively. To handle the adaptivity we identify certain likely interaction patterns and union bound over them to guarantee correct interaction on all of them. To guarantee correctness, it is important that the interaction hides some of its randomness from $\mathcal A$ in the reduction.|communic problem mathbf ur univers relat krw alic bob respect receiv promis neq last player receiv messag must output index neq prove random one way communic complex problem public coin model exact theta min log delta log frac log delta failur probabl delta lower bound hold even promis mathop support subset mathop support corollari obtain optim lower bound ell sampl strict turnstil stream le well problem find duplic stream lower bound need use larg weight hold even promis point stream give two differ proof main result first proof demonstr ani algorithm mathcal solv sampl problem turnstil stream low memori use encod subset certain size number bit inform theoret minimum encod make adapt queri mathcal throughout execut done care violat correct accomplish inject random nois encod interact mathcal loos motiv techniqu differenti privaci second proof via novel random reduct augment index mnsw need interact mathcal adapt handl adapt identifi certain like interact pattern union bound guarante correct interact guarante correct import interact hide random mathcal reduct|['Michael Kapralov', 'Jelani Nelson', 'Jakub Pachocki', 'Zhengyu Wang', 'David P. Woodruff', 'Mobin Yahyazadeh']|['cs.CC', 'cs.DS']
2017-04-07T11:28:01Z|2017-04-03T09:38:33Z|http://arxiv.org/abs/1704.00497v1|http://arxiv.org/pdf/1704.00497v1|UPGMA and the normalized equidistant minimum evolution problem|upgma normal equidist minimum evolut problem|UPGMA (Unweighted Pair Group Method with Arithmetic Mean) is a widely used clustering method. Here we show that UPGMA is a greedy heuristic for the normalized equidistant minimum evolution (NEME) problem, that is, finding a rooted tree that minimizes the minimum evolution score relative to the dissimilarity matrix among all rooted trees with the same leaf-set in which all leaves have the same distance to the root. We prove that the NEME problem is NP-hard. In addition, we present some heuristic and approximation algorithms for solving the NEME problem, including a polynomial time algorithm that yields a binary, rooted tree whose NEME score is within O(log^2 n) of the optimum. We expect that these results to eventually provide further insights into the behavior of the UPGMA algorithm.|upgma unweight pair group method arithmet mean wide use cluster method show upgma greedi heurist normal equidist minimum evolut neme problem find root tree minim minimum evolut score relat dissimilar matrix among root tree leaf set leav distanc root prove neme problem np hard addit present heurist approxim algorithm solv neme problem includ polynomi time algorithm yield binari root tree whose neme score within log optimum expect result eventu provid insight behavior upgma algorithm|['Vincent Moulton', 'Andreas Spillner', 'Taoyang Wu']|['q-bio.PE', 'cs.CC', 'cs.DM']
2017-04-07T11:28:01Z|2017-04-03T08:34:39Z|http://arxiv.org/abs/1704.00468v1|http://arxiv.org/pdf/1704.00468v1|Approximately certifying the restricted isometry property is hard|approxim certifi restrict isometri properti hard|A matrix is said to possess the Restricted Isometry Property (RIP) if it acts as an approximate isometry when restricted to sparse vectors. Previous work has shown it to be NP-hard to determine whether a matrix possess this property, but only in a narrow range of parameters. In this work, we show that it is NP-hard to make this determination for any accuracy parameter, even when we restrict ourselves to instances which are either RIP or far from being RIP. This result implies that it is NP-hard to approximate the range of parameters for which a matrix possesses the Restricted Isometry Property with accuracy better than some constant. Ours is the first work to prove such a claim without any additional assumptions.|matrix said possess restrict isometri properti rip act approxim isometri restrict spars vector previous work shown np hard determin whether matrix possess properti onli narrow rang paramet work show np hard make determin ani accuraci paramet even restrict ourselv instanc either rip far rip result impli np hard approxim rang paramet matrix possess restrict isometri properti accuraci better constant first work prove claim without ani addit assumpt|['Jonathan Weed']|['cs.CC']
2017-04-07T11:28:01Z|2017-04-03T00:44:22Z|http://arxiv.org/abs/1704.00395v1|http://arxiv.org/pdf/1704.00395v1|A Message-Passing Algorithm for Graph Isomorphism|messag pass algorithm graph isomorph|A message-passing procedure for solving the graph isomorphism problem is proposed. The procedure resembles the belief-propagation algorithm in the context of graphical models inference and LDPC decoding. To enable the algorithm, the input graphs are transformed into intermediate canonical representations of bipartite graphs. The matching procedure injects specially designed input patterns to the canonical graphs and runs a message-passing algorithm to generate two output fingerprints that are matched if and only if the input graphs are isomorphic.|messag pass procedur solv graph isomorph problem propos procedur resembl belief propag algorithm context graphic model infer ldpc decod enabl algorithm input graph transform intermedi canon represent bipartit graph match procedur inject special design input pattern canon graph run messag pass algorithm generat two output fingerprint match onli input graph isomorph|['Mohamed Mansour']|['cs.DS', 'cs.CC']
2017-04-07T11:28:01Z|2017-04-02T19:47:06Z|http://arxiv.org/abs/1704.00356v1|http://arxiv.org/pdf/1704.00356v1|Committees providing EJR can be computed efficiently|committe provid ejr comput effici|We identify a whole family of approval-based multi-winner voting rules that satisfy PJR. Moreover, we identify a subfamily of voting rules within this family that satisfy EJR. All these voting rules can be computed in polynomial time as long as the subalgorithms that characterize each rule within the family are polynomial.|identifi whole famili approv base multi winner vote rule satisfi pjr moreov identifi subfamili vote rule within famili satisfi ejr vote rule comput polynomi time long subalgorithm character rule within famili polynomi|['Luis Sánchez-Fernández', 'Edith Elkind', 'Martin Lackner']|['cs.GT', 'cs.CC', '91A35', 'I.2.11; F.2']
2017-04-07T11:28:01Z|2017-04-04T02:04:27Z|http://arxiv.org/abs/1704.00249v2|http://arxiv.org/pdf/1704.00249v2|Complexity of short Presburger arithmetic|complex short presburg arithmet|"We study complexity of short sentences in Presburger arithmetic (Short-PA). Here by ""short"" we mean sentences with a bounded number of variables, quantifiers, inequalities and Boolean operations; the input consists only of the integers involved in the inequalities. We prove that assuming Kannan's partition can be found in polynomial time, the satisfiability of Short-PA sentences can be decided in polynomial time. Furthermore, under the same assumption, we show that the numbers of satisfying assignments of short Presburger sentences can also be computed in polynomial time."|studi complex short sentenc presburg arithmet short pa short mean sentenc bound number variabl quantifi inequ boolean oper input consist onli integ involv inequ prove assum kannan partit found polynomi time satisfi short pa sentenc decid polynomi time furthermor assumpt show number satisfi assign short presburg sentenc also comput polynomi time|['Danny Nguyen', 'Igor Pak']|['math.CO', 'cs.CC', 'cs.DM', 'cs.LO', 'math.LO']
2017-04-07T11:28:01Z|2017-04-01T23:21:06Z|http://arxiv.org/abs/1704.00238v1|http://arxiv.org/pdf/1704.00238v1|Clustering in Hilbert space of a quantum optimization problem|cluster hilbert space quantum optim problem|The solution space of many classical optimization problems breaks up into clusters which are extensively distant from one another in the Hamming metric. Here, we show that an analogous quantum clustering phenomenon takes place in the ground state subspace of a certain quantum optimization problem. This involves extending the notion of clustering to Hilbert space, where the classical Hamming distance is not immediately useful. Quantum clusters correspond to macroscopically distinct subspaces of the full quantum ground state space which grow with the system size. We explicitly demonstrate that such clusters arise in the solution space of random quantum satisfiability (3-QSAT) at its satisfiability transition. We estimate both the number of these clusters and their internal entropy. The former are given by the number of hardcore dimer coverings of the core of the interaction graph, while the latter is related to the underconstrained degrees of freedom not touched by the dimers. We additionally provide new numerical evidence suggesting that the 3-QSAT satisfiability transition may coincide with the product satisfiability transition, which would imply the absence of an intermediate entangled satisfiable phase.|solut space mani classic optim problem break cluster extens distant one anoth ham metric show analog quantum cluster phenomenon take place ground state subspac certain quantum optim problem involv extend notion cluster hilbert space classic ham distanc immedi use quantum cluster correspond macroscop distinct subspac full quantum ground state space grow system size explicit demonstr cluster aris solut space random quantum satisfi qsat satisfi transit estim number cluster intern entropi former given number hardcor dimer cover core interact graph latter relat underconstrain degre freedom touch dimer addit provid new numer evid suggest qsat satisfi transit may coincid product satisfi transit would impli absenc intermedi entangl satisfi phase|['S. C. Morampudi', 'B. Hsu', 'S. L. Sondhi', 'R. Moessner', 'C. R. Laumann']|['quant-ph', 'cond-mat.dis-nn', 'cond-mat.stat-mech', 'cs.CC']
2017-04-07T11:28:01Z|2017-03-29T19:23:01Z|http://arxiv.org/abs/1703.10205v1|http://arxiv.org/pdf/1703.10205v1|A Sharp Tail Bound for the Expander Random Sampler|sharp tail bound expand random sampler|Consider an expander graph in which a $\mu$ fraction of the vertices are marked. A random walk starts at a uniform vertex and at each step continues to a random neighbor. Gillman showed in 1998 that the number of marked vertices seen in a random walk of length $n$ is concentrated around its expectation, $\Phi := \mu n$, independent of the size of the graph. Here we provide a new and sharp tail bound, improving on the existing bounds whenever $\mu$ is not too large.|consid expand graph mu fraction vertic mark random walk start uniform vertex step continu random neighbor gillman show number mark vertic seen random walk length concentr around expect phi mu independ size graph provid new sharp tail bound improv exist bound whenev mu larg|['Shravas Rao', 'Oded Regev']|['math.PR', 'cs.CC']
2017-04-07T11:28:01Z|2017-03-29T17:21:44Z|http://arxiv.org/abs/1703.10146v1|http://arxiv.org/pdf/1703.10146v1|Community detection and stochastic block models: recent developments|communiti detect stochast block model recent develop|The stochastic block model (SBM) is a random graph model with planted clusters. It is widely employed as a canonical model to study clustering and community detection, and provides generally a fertile ground to study the statistical and computational tradeoffs that arise in network and data sciences.   This note surveys the recent developments that establish the fundamental limits for community detection in the SBM, both with respect to information-theoretic and computational thresholds, and for various recovery requirements such as exact, partial and weak recovery (a.k.a., detection). The main results discussed are the phase transitions for exact recovery at the Chernoff-Hellinger threshold, the phase transition for weak recovery at the Kesten-Stigum threshold, the optimal distortion-SNR tradeoff for partial recovery, the learning of the SBM parameters and the gap between information-theoretic and computational thresholds.   The note also covers some of the algorithms developed in the quest of achieving the limits, in particular two-round algorithms via graph-splitting, semi-definite programming, linearized belief propagation, classical and nonbacktracking spectral methods. A few open problems are also discussed.|stochast block model sbm random graph model plant cluster wide employ canon model studi cluster communiti detect provid general fertil ground studi statist comput tradeoff aris network data scienc note survey recent develop establish fundament limit communiti detect sbm respect inform theoret comput threshold various recoveri requir exact partial weak recoveri detect main result discuss phase transit exact recoveri chernoff helling threshold phase transit weak recoveri kesten stigum threshold optim distort snr tradeoff partial recoveri learn sbm paramet gap inform theoret comput threshold note also cover algorithm develop quest achiev limit particular two round algorithm via graph split semi definit program linear belief propag classic nonbacktrack spectral method open problem also discuss|['Emmanuel Abbe']|['math.PR', 'cs.CC', 'cs.IT', 'cs.SI', 'math.IT', 'stat.ML']
2017-04-07T11:28:05Z|2017-03-28T18:01:14Z|http://arxiv.org/abs/1703.09726v1|http://arxiv.org/pdf/1703.09726v1|Ruling out FPT algorithms for Weighted Coloring on forests|rule fpt algorithm weight color forest|Given a graph $G$, a proper $k$-coloring of $G$ is a partition $c = (S_i)_{i\in [1,k]}$ of $V(G)$ into $k$ stable sets $S_1,\ldots, S_{k}$. Given a weight function $w: V(G) \to \mathbb{R}^+$, the weight of a color $S_i$ is defined as $w(i) = \max_{v \in S_i} w(v)$ and the weight of a coloring $c$ as $w(c) = \sum_{i=1}^{k}w(i)$. Guan and Zhu [Inf. Process. Lett., 1997] defined the weighted chromatic number of a pair $(G,w)$, denoted by $\sigma(G,w)$, as the minimum weight of a proper coloring of $G$. For a positive integer $r$, they also defined $\sigma(G,w;r)$ as the minimum of $w(c)$ among all proper $r$-colorings $c$ of $G$.   The complexity of determining $\sigma(G,w)$ when $G$ is a tree was open for almost 20 years, until Ara\'ujo et al. [SIAM J. Discrete Math., 2014] recently proved that the problem cannot be solved in time $n^{o(\log n)}$ on $n$-vertex trees unless the Exponential Time Hypothesis (ETH) fails.   The objective of this article is to provide hardness results for computing $\sigma(G,w)$ and $\sigma(G,w;r)$ when $G$ is a tree or a forest, relying on complexity assumptions weaker than the ETH. Namely, we study the problem from the viewpoint of parameterized complexity, and we assume the weaker hypothesis $FPT \neq W[1]$. Building on the techniques of Ara\'ujo et al., we prove that when $G$ is a forest, computing $\sigma(G,w)$ is $W[1]$-hard parameterized by the size of a largest connected component of $G$, and that computing $\sigma(G,w;r)$ is $W[2]$-hard parameterized by $r$. Our results rule out the existence of $FPT$ algorithms for computing these invariants on trees or forests for many natural choices of the parameter.|given graph proper color partit stabl set ldot given weight function mathbb weight color defin max weight color sum guan zhu inf process lett defin weight chromat number pair denot sigma minimum weight proper color posit integ also defin sigma minimum among proper color complex determin sigma tree open almost year ara ujo et al siam discret math recent prove problem cannot solv time log vertex tree unless exponenti time hypothesi eth fail object articl provid hard result comput sigma sigma tree forest reli complex assumpt weaker eth name studi problem viewpoint parameter complex assum weaker hypothesi fpt neq build techniqu ara ujo et al prove forest comput sigma hard parameter size largest connect compon comput sigma hard parameter result rule exist fpt algorithm comput invari tree forest mani natur choic paramet|['Júlio Araújo', 'Julien Baste', 'Ignasi Sau']|['cs.DS', 'cs.CC', 'math.CO', '05C15', 'G.2.2; F.2.2']
2017-04-07T11:28:05Z|2017-03-25T22:50:20Z|http://arxiv.org/abs/1703.08746v1|http://arxiv.org/pdf/1703.08746v1|Proof Verification Can Be Hard!|proof verif hard|The generally accepted wisdom in computational circles is that pure proof verification is a solved problem and that the computationally hard elements and fertile areas of study lie in proof discovery. This wisdom presumably does hold for conventional proof systems such as first-order logic with a standard proof calculus such as natural deduction or resolution. But this folk belief breaks down when we consider more user-friendly/powerful inference rules. One such rule is the restricted {\omega}-rule, which is not even semi-decidable when added to a standard proof calculus of a nice theory. While presumably not a novel result, we feel that the hardness of proof verification is under-appreciated in most communities that deal with proofs. A proof-sketch follows.|general accept wisdom comput circl pure proof verif solv problem comput hard element fertil area studi lie proof discoveri wisdom presum doe hold convent proof system first order logic standard proof calculus natur deduct resolut folk belief break consid user friend power infer rule one rule restrict omega rule even semi decid ad standard proof calculus nice theori presum novel result feel hard proof verif appreci communiti deal proof proof sketch follow|['Naveen Sundar Govindarajulu', 'Selmer Bringsjord']|['cs.LO', 'cs.CC']
2017-04-07T11:28:05Z|2017-03-23T16:50:03Z|http://arxiv.org/abs/1703.08139v1|http://arxiv.org/pdf/1703.08139v1|Optimal lower bounds for universal relation, samplers, and finding   duplicates|optim lower bound univers relat sampler find duplic|In the communication problem $\mathbf{UR}$ (universal relation) [KRW95], Alice and Bob respectively receive $x$ and $y$ in $\{0,1\}^n$ with the promise that $x\neq y$. The last player to receive a message must output an index $i$ such that $x_i\neq y_i$. We prove that the randomized one-way communication complexity of this problem in the public coin model is exactly $\Theta(\min\{n, \log(1/\delta)\log^2(\frac{n}{\log(1/\delta)})\})$ bits for failure probability $\delta$. Our lower bound holds even if promised $\mathop{support}(y)\subset \mathop{support}(x)$. As a corollary, we obtain optimal lower bounds for $\ell_p$-sampling in strict turnstile streams for $0\le p < 2$, as well as for the problem of finding duplicates in a stream. Our lower bounds do not need to use large weights, and hold even if it is promised that $x\in\{0,1\}^n$ at all points in the stream.   Our lower bound demonstrates that any algorithm $\mathcal{A}$ solving sampling problems in turnstile streams in low memory can be used to encode subsets of $[n]$ of certain sizes into a number of bits below the information theoretic minimum. Our encoder makes adaptive queries to $\mathcal{A}$ throughout its execution, but done carefully so as to not violate correctness. This is accomplished by injecting random noise into the encoder's interactions with $\mathcal{A}$, which is loosely motivated by techniques in differential privacy. Our correctness analysis involves understanding the ability of $\mathcal{A}$ to correctly answer adaptive queries which have positive but bounded mutual information with $\mathcal{A}$'s internal randomness, and may be of independent interest in the newly emerging area of adaptive data analysis with a theoretical computer science lens.|communic problem mathbf ur univers relat krw alic bob respect receiv promis neq last player receiv messag must output index neq prove random one way communic complex problem public coin model exact theta min log delta log frac log delta bit failur probabl delta lower bound hold even promis mathop support subset mathop support corollari obtain optim lower bound ell sampl strict turnstil stream le well problem find duplic stream lower bound need use larg weight hold even promis point stream lower bound demonstr ani algorithm mathcal solv sampl problem turnstil stream low memori use encod subset certain size number bit inform theoret minimum encod make adapt queri mathcal throughout execut done care violat correct accomplish inject random nois encod interact mathcal loos motiv techniqu differenti privaci correct analysi involv understand abil mathcal correct answer adapt queri posit bound mutual inform mathcal intern random may independ interest newli emerg area adapt data analysi theoret comput scienc len|['Jelani Nelson', 'Jakub Pachocki', 'Zhengyu Wang']|['cs.CC', 'cs.DS']
2017-04-07T11:28:05Z|2017-03-23T00:00:41Z|http://arxiv.org/abs/1703.07891v1|http://arxiv.org/pdf/1703.07891v1|Width Hierarchies for Quantum and Classical Ordered Binary Decision   Diagrams with Repeated Test|width hierarchi quantum classic order binari decis diagram repeat test|We consider quantum, nondterministic and probabilistic versions of known computational model Ordered Read-$k$-times Branching Programs or Ordered Binary Decision Diagrams with repeated test ($k$-QOBDD, $k$-NOBDD and $k$-POBDD). We show width hierarchy for complexity classes of Boolean function computed by these models and discuss relation between different variants of $k$-OBDD.|consid quantum nondterminist probabilist version known comput model order read time branch program order binari decis diagram repeat test qobdd nobdd pobdd show width hierarchi complex class boolean function comput model discuss relat differ variant obdd|['Kamil Khadiev', 'Rishat Ibrahimov']|['cs.CC', 'quant-ph']
2017-04-07T11:28:05Z|2017-03-22T19:56:27Z|http://arxiv.org/abs/1703.07833v1|http://arxiv.org/pdf/1703.07833v1|Information complexity of the AND function in the two-Party, and   multiparty settings|inform complex function two parti multiparti set|In a recent breakthrough paper [M. Braverman, A. Garg, D. Pankratov, and O. Weinstein, From information to exact communication, STOC'13] Braverman et al. developed a local characterization for the zero-error information complexity in the two party model, and used it to compute the exact internal and external information complexity of the 2-bit AND function, which was then applied to determine the exact asymptotic of randomized communication complexity of the set disjointness problem.   In this article, we extend their results on AND function to the multi-party number-in-hand model by proving that the generalization of their protocol has optimal internal and external information cost for certain distributions. Our proof has new components, and in particular it fixes some minor gaps in the proof of Braverman et al.|recent breakthrough paper braverman garg pankratov weinstein inform exact communic stoc braverman et al develop local character zero error inform complex two parti model use comput exact intern extern inform complex bit function appli determin exact asymptot random communic complex set disjoint problem articl extend result function multi parti number hand model prove general protocol optim intern extern inform cost certain distribut proof new compon particular fix minor gap proof braverman et al|['Yuval Filmus', 'Hamed Hatami', 'Yaqiao Li', 'Suzin You']|['cs.CC']
2017-04-07T11:28:05Z|2017-03-23T07:30:19Z|http://arxiv.org/abs/1703.07768v2|http://arxiv.org/pdf/1703.07768v2|Quantum Communication-Query Tradeoffs|quantum communic queri tradeoff|For any function $f: X \times Y \to Z$, we prove that $Q^{*\text{cc}}(f) \cdot Q^{\text{OIP}}(f) \cdot (\log Q^{\text{OIP}}(f) + \log  Z ) \geq \Omega(\log  X )$. Here, $Q^{*\text{cc}}(f)$ denotes the bounded-error communication complexity of $f$ using an entanglement-assisted two-way qubit channel, and $Q^{\text{OIP}}(f)$ denotes the number of quantum queries needed to determine $x$ with high probability given oracle access to the function $f_x(y) \stackrel{\text{def}}{=} f(x, y)$. We show that this tradeoff is close to the best possible. We also give a generalization of this tradeoff for distributional query complexity.   As an application, we prove an optimal $\Omega(\log q)$ lower bound on the $Q^{*\text{cc}}$ complexity of determining whether $x + y$ is a perfect square, where Alice holds $x \in \mathbf{F}_q$, Bob holds $y \in \mathbf{F}_q$, and $\mathbf{F}_q$ is a finite field of odd characteristic. As another application, we give a new, simpler proof that searching an ordered size-$N$ database requires $\Omega(\log N / \log \log N)$ quantum queries. (It was already known that $\Theta(\log N)$ queries are required.)|ani function time prove text cc cdot text oip cdot log text oip log geq omega log text cc denot bound error communic complex use entangl assist two way qubit channel text oip denot number quantum queri need determin high probabl given oracl access function stackrel text def show tradeoff close best possibl also give general tradeoff distribut queri complex applic prove optim omega log lower bound text cc complex determin whether perfect squar alic hold mathbf bob hold mathbf mathbf finit field odd characterist anoth applic give new simpler proof search order size databas requir omega log log log quantum queri alreadi known theta log queri requir|['William M. Hoza']|['cs.CC', 'quant-ph']
2017-04-07T11:28:05Z|2017-03-22T14:14:35Z|http://arxiv.org/abs/1703.07666v1|http://arxiv.org/pdf/1703.07666v1|Query-to-Communication Lifting for BPP|queri communic lift bpp|For any $n$-bit boolean function $f$, we show that the randomized communication complexity of the composed function $f\circ g^n$, where $g$ is an index gadget, is characterized by the randomized decision tree complexity of $f$. In particular, this means that many query complexity separations involving randomized models (e.g., classical vs. quantum) automatically imply analogous separations in communication complexity.|ani bit boolean function show random communic complex compos function circ index gadget character random decis tree complex particular mean mani queri complex separ involv random model classic vs quantum automat impli analog separ communic complex|['Mika Göös', 'Toniann Pitassi', 'Thomas Watson']|['cs.CC']
2017-04-07T11:28:05Z|2017-03-23T20:55:10Z|http://arxiv.org/abs/1703.07657v2|http://arxiv.org/pdf/1703.07657v2|"A Counterexample to the ""Majority is Least Stable"" Conjecture"|counterexampl major least stabl conjectur|"We exhibit a linear threshold function in 5 variables with strictly smaller noise stability (for small values of the correlation parameter) than the majority function on 5 variables, thereby providing a counterexample to the ""Majority is Least Stable"" Conjecture of Benjamini, Kalai, and Schramm."|exhibit linear threshold function variabl strict smaller nois stabil small valu correl paramet major function variabl therebi provid counterexampl major least stabl conjectur benjamini kalai schramm|['Vishesh Jain']|['cs.CC', 'math.PR']
2017-04-07T11:28:05Z|2017-03-22T04:45:51Z|http://arxiv.org/abs/1703.07521v1|http://arxiv.org/pdf/1703.07521v1|Lifting randomized query complexity to randomized communication   complexity|lift random queri complex random communic complex|We show that for any (partial) query function $f:\{0,1\}^n\rightarrow \{0,1\}$, the randomized communication complexity of $f$ composed with $\mathrm{Index}^n_m$ (with $m= \mathrm{poly}(n)$) is at least the randomized query complexity of $f$ times $\log n$. Here $\mathrm{Index}_m : [m] \times \{0,1\}^m \rightarrow \{0,1\}$ is defined as $\mathrm{Index}_m(x,y)= y_x$ (the $x$th bit of $y$).   Our proof follows on the lines of Raz and Mckenzie [RM99] (and its generalization due to [GPW15]), who showed a lifting theorem for deterministic query complexity to deterministic communication complexity. Our proof deviates from theirs in an important fashion that we consider partitions of rectangles into many sub-rectangles, as opposed to a particular sub-rectangle with desirable properties, as considered by Raz and McKenzie. As a consequence of our main result, some known separations between quantum and classical communication complexities follow from analogous separations in the query world.|show ani partial queri function rightarrow random communic complex compos mathrm index mathrm poli least random queri complex time log mathrm index time rightarrow defin mathrm index th bit proof follow line raz mckenzi rm general due gpw show lift theorem determinist queri complex determinist communic complex proof deviat import fashion consid partit rectangl mani sub rectangl oppos particular sub rectangl desir properti consid raz mckenzi consequ main result known separ quantum classic communic complex follow analog separ queri world|['Anurag Anshu', 'Naresh B. Goud', 'Rahul Jain', 'Srijita Kundu', 'Priyanka Mukhopadhyay']|['cs.CC', 'quant-ph']
2017-04-07T11:28:05Z|2017-03-21T19:48:23Z|http://arxiv.org/abs/1703.07406v1|http://arxiv.org/pdf/1703.07406v1|Subset sum problem in polycyclic groups|subset sum problem polycycl group|We consider a group-theoretic analogue of the classic subset sum problem. It is known that every virtually nilpotent group has polynomial time decidable subset sum problem. In this paper we use subgroup distortion to show that every polycyclic non-virtually-nilpotent group has NP-complete subset sum problem.|consid group theoret analogu classic subset sum problem known everi virtual nilpot group polynomi time decid subset sum problem paper use subgroup distort show everi polycycl non virtual nilpot group np complet subset sum problem|['Andrey Nikolaev', 'Alexander Ushakov']|['math.GR', 'cs.CC', 'math.CO', '03D15, 20F65, 20F10, 20F16']
2017-04-07T11:28:09Z|2017-03-21T12:48:28Z|http://arxiv.org/abs/1703.07184v1|http://arxiv.org/pdf/1703.07184v1|Exact Affine OBDDs|exact affin obdd|We introduce affine OBDD model and we show that exact affine OBDDs can be exponentially narrower than bounded-error quantum and classical OBDDs on some certain problems. Moreover, we consider Las-Vegas quantum and classical automata models and improve the previous gap between deterministic and probabilistic models by a factor of 2 and then follow the same gap for the known most restricted quantum model. Lastly, we follow an exponential gap between exact affine finite automata and Las-Vegas classical and quantum models.|introduc affin obdd model show exact affin obdd exponenti narrow bound error quantum classic obdd certain problem moreov consid las vega quantum classic automata model improv previous gap determinist probabilist model factor follow gap known restrict quantum model last follow exponenti gap exact affin finit automata las vega classic quantum model|['Rishat Ibrahimov', 'Kamil Khadiev', 'Abuzer Yakaryilmaz']|['cs.CC', 'cs.FL', 'quant-ph']
2017-04-07T11:28:09Z|2017-03-23T15:17:23Z|http://arxiv.org/abs/1703.06983v2|http://arxiv.org/pdf/1703.06983v2|Collapsibility to a subcomplex of a given dimension is NP-complete|collaps subcomplex given dimens np complet|In this paper we extend the works of Tancer and of Malgouyres and Franc\'es, showing that $(d,k)$-collapsibility is NP-complete for $d\geq k+2$ except $(2,0)$. By $(d,k)$-collapsibility we mean the following problem: determine whether a given $d$-dimensional simplicial complex can be collapsed to some $k$-dimensional subcomplex. The question of establishing the complexity status of $(d,k)$-collapsibility was asked by Tancer, who proved NP-completeness of $(d,0)$ and $(d,1)$-collapsibility (for $d\geq 3$). Our extended result, together with the known polynomial-time algorithms for $(2,0)$ and $d=k+1$, answers the question completely.|paper extend work tancer malgouyr franc es show collaps np complet geq except collaps mean follow problem determin whether given dimension simplici complex collaps dimension subcomplex question establish complex status collaps ask tancer prove np complet collaps geq extend result togeth known polynomi time algorithm answer question complet|['Giovanni Paolini']|['cs.CG', 'cs.CC', 'math.GT']
2017-04-07T11:28:09Z|2017-03-19T11:24:26Z|http://arxiv.org/abs/1703.06423v1|http://arxiv.org/pdf/1703.06423v1|The Hardness of Embedding Grids and Walls|hard embed grid wall|"The dichotomy conjecture for the parameterized embedding problem states that the problem of deciding whether a given graph $G$ from some class $K$ of ""pattern graphs"" can be embedded into a given graph $H$ (that is, is isomorphic to a subgraph of $H$) is fixed-parameter tractable if $K$ is a class of graphs of bounded tree width and $W[1]$-complete otherwise.   Towards this conjecture, we prove that the embedding problem is $W[1]$-complete if $K$ is the class of all grids or the class of all walls."|dichotomi conjectur parameter embed problem state problem decid whether given graph class pattern graph embed given graph isomorph subgraph fix paramet tractabl class graph bound tree width complet otherwis toward conjectur prove embed problem complet class grid class wall|['Yijia Chen', 'Martin Grohe', 'Bingkai Lin']|['cs.CC']
2017-04-07T11:28:09Z|2017-03-17T17:43:59Z|http://arxiv.org/abs/1703.06127v1|http://arxiv.org/pdf/1703.06127v1|Tusnády's problem, the transference principle, and non-uniform QMC   sampling|tusn dy problem transfer principl non uniform qmc sampl|It is well-known that for every $N \geq 1$ and $d \geq 1$ there exist point sets $x_1, \dots, x_N \in [0,1]^d$ whose discrepancy with respect to the Lebesgue measure is of order at most $(\log N)^{d-1} N^{-1}$. In a more general setting, the first author proved together with Josef Dick that for any normalized measure $\mu$ on $[0,1]^d$ there exist points $x_1, \dots, x_N$ whose discrepancy with respect to $\mu$ is of order at most $(\log N)^{(3d+1)/2} N^{-1}$. The proof used methods from combinatorial mathematics, and in particular a result of Banaszczyk on balancings of vectors. In the present note we use a version of the so-called transference principle together with recent results on the discrepancy of red-blue colorings to show that for any $\mu$ there even exist points having discrepancy of order at most $(\log N)^{d-\frac12} N^{-1}$, which is almost as good as the discrepancy bound in the case of the Lebesgue measure.|well known everi geq geq exist point set dot whose discrep respect lebesgu measur order log general set first author prove togeth josef dick ani normal measur mu exist point dot whose discrep respect mu order log proof use method combinatori mathemat particular result banaszczyk balanc vector present note use version call transfer principl togeth recent result discrep red blue color show ani mu even exist point discrep order log frac almost good discrep bound case lebesgu measur|['Christoph Aistleitner', 'Dmitriy Bilyk', 'Aleksandar Nikolov']|['math.CO', 'cs.CC', 'math.NA', 'math.PR']
2017-04-07T11:28:09Z|2017-03-17T15:08:17Z|http://arxiv.org/abs/1703.06048v1|http://arxiv.org/pdf/1703.06048v1|An FPTAS for the Knapsack Problem with Parametric Weights|fptas knapsack problem parametr weight|In this paper, we investigate the parametric weight knapsack problem, in which the item weights are affine functions of the form $w_i(\lambda) = a_i + \lambda \cdot b_i$ for $i \in \{1,\ldots,n\}$ depending on a real-valued parameter $\lambda$. The aim is to provide a solution for all values of the parameter. It is well-known that any exact algorithm for the problem may need to output an exponential number of knapsack solutions. We present the first fully polynomial-time approximation scheme (FPTAS) for the problem that, for any desired precision $\varepsilon \in (0,1)$, computes $(1-\varepsilon)$-approximate solutions for all values of the parameter. Our FPTAS is based on two different approaches and achieves a running time of $\mathcal{O}(n^3/\varepsilon^2 \cdot \min\{ \log^2 P, n^2 \} \cdot \min\{\log M, n \log (n/\varepsilon) / \log(n \log (n/\varepsilon) )\})$ where $P$ is an upper bound on the optimal profit and $M := \max\{W, n \cdot \max\{a_i,b_i: i \in \{1,\ldots,n\}\}\}$ for a knapsack with capacity $W$.|paper investig parametr weight knapsack problem item weight affin function form lambda lambda cdot ldot depend real valu paramet lambda aim provid solut valu paramet well known ani exact algorithm problem may need output exponenti number knapsack solut present first fulli polynomi time approxim scheme fptas problem ani desir precis varepsilon comput varepsilon approxim solut valu paramet fptas base two differ approach achiev run time mathcal varepsilon cdot min log cdot min log log varepsilon log log varepsilon upper bound optim profit max cdot max ldot knapsack capac|['Michael Holzhauser', 'Sven O. Krumke']|['cs.DS', 'cs.CC', 'math.OC']
2017-04-07T11:28:09Z|2017-03-17T03:58:22Z|http://arxiv.org/abs/1703.05881v1|http://arxiv.org/pdf/1703.05881v1|Complexity of Correspondence Homomorphisms|complex correspond homomorph|Correspondence homomorphisms are both a generalization of standard homomorphisms and a generalization of correspondence colourings. For a fixed target graph $H$, the problem is to decide whether an input graph $G$, with each edge labeled by a pair of permutations of $V(H)$, admits a homomorphism to $H$ 'corresponding' to the labels, in a sense explained below.   We classify the complexity of this problem as a function of the fixed graph $H$. It turns out that there is dichotomy -- each of the problems is polynomial-time solvable or NP-complete. While most graphs $H$ yield NP-complete problems, there are interesting cases of graphs $H$ for which we solve the problem by Gaussian elimination.   We also classify the complexity of the analogous correspondence list homomorphism problems.   In this note we only include the proofs for the case $H$ is reflexive.|correspond homomorph general standard homomorph general correspond colour fix target graph problem decid whether input graph edg label pair permut admit homomorph correspond label sens explain classifi complex problem function fix graph turn dichotomi problem polynomi time solvabl np complet graph yield np complet problem interest case graph solv problem gaussian elimin also classifi complex analog correspond list homomorph problem note onli includ proof case reflex|['Tomas Feder', 'Pavol Hell']|['cs.DM', 'cs.CC', 'math.CO']
2017-04-07T11:28:09Z|2017-03-16T18:18:51Z|http://arxiv.org/abs/1703.05784v1|http://arxiv.org/pdf/1703.05784v1|A Nearly Optimal Lower Bound on the Approximate Degree of AC$^0$|near optim lower bound approxim degre ac|The approximate degree of a Boolean function $f \colon \{-1, 1\}^n \rightarrow \{-1, 1\}$ is the least degree of a real polynomial that approximates $f$ pointwise to error at most $1/3$. We introduce a generic method for increasing the approximate degree of a given function, while preserving its computability by constant-depth circuits.   Specifically, we show how to transform any Boolean function $f$ with approximate degree $d$ into a function $F$ on $O(n \cdot \operatorname{polylog}(n))$ variables with approximate degree at least $D = \Omega(n^{1/3} \cdot d^{2/3})$. In particular, if $d= n^{1-\Omega(1)}$, then $D$ is polynomially larger than $d$. Moreover, if $f$ is computed by a polynomial-size Boolean circuit of constant depth, then so is $F$.   By recursively applying our transformation, for any constant $\delta > 0$ we exhibit an AC$^0$ function of approximate degree $\Omega(n^{1-\delta})$. This improves over the best previous lower bound of $\Omega(n^{2/3})$ due to Aaronson and Shi (J. ACM 2004), and nearly matches the trivial upper bound of $n$ that holds for any function. Our lower bounds also apply to (quasipolynomial-size) DNFs of polylogarithmic width.   We describe several applications of these results. We give:   * For any constant $\delta > 0$, an $\Omega(n^{1-\delta})$ lower bound on the quantum communication complexity of a function in AC$^0$.   * A Boolean function $f$ with approximate degree at least $C(f)^{2-o(1)}$, where $C(f)$ is the certificate complexity of $f$. This separation is optimal up to the $o(1)$ term in the exponent.   * Improved secret sharing schemes with reconstruction procedures in AC$^0$.|approxim degre boolean function colon rightarrow least degre real polynomi approxim pointwis error introduc generic method increas approxim degre given function preserv comput constant depth circuit specif show transform ani boolean function approxim degre function cdot operatornam polylog variabl approxim degre least omega cdot particular omega polynomi larger moreov comput polynomi size boolean circuit constant depth recurs appli transform ani constant delta exhibit ac function approxim degre omega delta improv best previous lower bound omega due aaronson shi acm near match trivial upper bound hold ani function lower bound also appli quasipolynomi size dnfs polylogarithm width describ sever applic result give ani constant delta omega delta lower bound quantum communic complex function ac boolean function approxim degre least certif complex separ optim term expon improv secret share scheme reconstruct procedur ac|['Mark Bun', 'Justin Thaler']|['cs.CC']
2017-04-07T11:28:09Z|2017-03-15T18:00:05Z|http://arxiv.org/abs/1703.05332v1|http://arxiv.org/pdf/1703.05332v1|Complexity of sampling as an order parameter|complex sampl order paramet|We consider the classical complexity of approximately simulating time evolution under spatially local quadratic bosonic Hamiltonians for time $t$. We obtain upper and lower bounds on the scaling of $t$ with the number of bosons, $n$, for which simulation, cast as a sampling problem, is classically efficient and provably hard, respectively. We view these results in the light of classifying phases of physical systems based on parameters in the Hamiltonian and conjecture a link to dynamical phase transitions. In doing so, we combine ideas from mathematical physics and computational complexity to gain insight into the behavior of condensed matter systems.|consid classic complex approxim simul time evolut spatial local quadrat boson hamiltonian time obtain upper lower bound scale number boson simul cast sampl problem classic effici provabl hard respect view result light classifi phase physic system base paramet hamiltonian conjectur link dynam phase transit combin idea mathemat physic comput complex gain insight behavior condens matter system|['Abhinav Deshpande', 'Bill Fefferman', 'Michael Foss-Feig', 'Alexey V. Gorshkov']|['quant-ph', 'cond-mat.quant-gas', 'cs.CC']
2017-04-07T11:28:09Z|2017-03-15T14:21:40Z|http://arxiv.org/abs/1703.05170v1|http://arxiv.org/pdf/1703.05170v1|Busy beavers and Kolmogorov complexity|busi beaver kolmogorov complex|"The idea to find the ""maximal number that can be named"" can be traced back to Archimedes (see his Psammit). From the viewpoint of computation theory the natural question is ""which number can be described by at most n bits""? This question led to the definition of the so-called ""busy beaver"" numbers (introduced by T. Rado). In this note we consider different versions of the busy beaver-like notions defined in terms of Kolmogorov complexity. We show that these versions differ depending on the version of complexity used (plain, prefix, or a priori complexities) and find out how these notions are related, providing matching lower and upper bounds."|idea find maxim number name trace back archimed see psammit viewpoint comput theori natur question number describ bit question led definit call busi beaver number introduc rado note consid differ version busi beaver like notion defin term kolmogorov complex show version differ depend version complex use plain prefix priori complex find notion relat provid match lower upper bound|['Mikhail Andreev']|['cs.CC']
2017-04-07T11:28:09Z|2017-03-15T13:51:23Z|http://arxiv.org/abs/1703.05156v1|http://arxiv.org/pdf/1703.05156v1|Complexity Dichotomies for the Minimum F-Overlay Problem|complex dichotomi minimum overlay problem|For a (possibly infinite) fixed family of graphs F, we say that a graph G overlays F on a hypergraph H if V(H) is equal to V(G) and the subgraph of G induced by every hyperedge of H contains some member of F as a spanning subgraph.While it is easy to see that the complete graph on  V(H)  overlays F on a hypergraph H whenever the problem admits a solution, the Minimum F-Overlay problem asks for such a graph with the minimum number of edges.This problem allows to generalize some natural problems which may arise in practice. For instance, if the family F contains all connected graphs, then Minimum F-Overlay corresponds to the Minimum Connectivity Inference problem (also known as Subset Interconnection Design problem) introduced for the low-resolution reconstruction of macro-molecular assembly in structural biology, or for the design of networks.Our main contribution is a strong dichotomy result regarding the polynomial vs. NP-hard status with respect to the considered family F. Roughly speaking, we show that the easy cases one can think of (e.g. when edgeless graphs of the right sizes are in F, or if F contains only cliques) are the only families giving rise to a polynomial problem: all others are NP-complete.We then investigate the parameterized complexity of the problem and give similar sufficient conditions on F that give rise to W[1]-hard, W[2]-hard or FPT problems when the parameter is the size of the solution.This yields an FPT/W[1]-hard dichotomy for a relaxed problem, where every hyperedge of H must contain some member of F as a (non necessarily spanning) subgraph.|possibl infinit fix famili graph say graph overlay hypergraph equal subgraph induc everi hyperedg contain member span subgraph easi see complet graph overlay hypergraph whenev problem admit solut minimum overlay problem ask graph minimum number edg problem allow general natur problem may aris practic instanc famili contain connect graph minimum overlay correspond minimum connect infer problem also known subset interconnect design problem introduc low resolut reconstruct macro molecular assembl structur biolog design network main contribut strong dichotomi result regard polynomi vs np hard status respect consid famili rough speak show easi case one think edgeless graph right size contain onli cliqu onli famili give rise polynomi problem np complet investig parameter complex problem give similar suffici condit give rise hard hard fpt problem paramet size solut yield fpt hard dichotomi relax problem everi hyperedg must contain member non necessarili span subgraph|['Nathann Cohen', 'Frédéric Havet', 'Dorian Mazauric', 'Ignasi Sau', 'Rémi Watrigant']|['cs.DS', 'cs.CC']
2017-04-07T11:28:13Z|2017-03-15T08:54:12Z|http://arxiv.org/abs/1703.05015v1|http://arxiv.org/pdf/1703.05015v1|Lower Bound and Hierarchies for Quantum Ordered Read-$k$-times Branching   Programs|lower bound hierarchi quantum order read time branch program|We consider quantum version of known computational model Ordered Read-$k$-times Branching Programs or Ordered Binary Decision Diagrams with repeated test ($k$-QOBDD). We get lower bound for quantum $k$-OBDD for $k=o(\sqrt{n})$. This lower bound gives connection between characteristics of model and number of subfunctions for function.   Additionally, we prove the hierarchies for sublinear width bounded error quantum $k$-OBDDs using our lower bounds for $ k=o(\sqrt{n})$. Also we prove hierarchy for polynomial size bounded error quantum $k$-OBDDs constant $k$, and it differs from situation with unbounded error where known that increasing of $k$ does not gives any advantages.   Finally, we discuss relations between different classical and quantum models of $k$-OBDD.|consid quantum version known comput model order read time branch program order binari decis diagram repeat test qobdd get lower bound quantum obdd sqrt lower bound give connect characterist model number subfunct function addit prove hierarchi sublinear width bound error quantum obdd use lower bound sqrt also prove hierarchi polynomi size bound error quantum obdd constant differ situat unbound error known increas doe give ani advantag final discuss relat differ classic quantum model obdd|['Farid Ablayev', 'Kamil Khadiev', 'Aliya Khadieva']|['cs.CC', 'quant-ph']
2017-04-07T11:28:13Z|2017-03-15T05:43:48Z|http://arxiv.org/abs/1703.04940v1|http://arxiv.org/pdf/1703.04940v1|Resilience: A Criterion for Learning in the Presence of Arbitrary   Outliers|resili criterion learn presenc arbitrari outlier|We introduce a criterion, resilience, which allows properties of a dataset (such as its mean or best low rank approximation) to be robustly computed, even in the presence of a large fraction of arbitrary additional data. Resilience is a weaker condition than most other properties considered so far in the literature, and yet enables robust estimation in a broader variety of settings, including the previously unstudied problem of robust mean estimation in $\ell_p$-norms.|introduc criterion resili allow properti dataset mean best low rank approxim robust comput even presenc larg fraction arbitrari addit data resili weaker condit properti consid far literatur yet enabl robust estim broader varieti set includ previous unstudi problem robust mean estim ell norm|['Jacob Steinhardt', 'Moses Charikar', 'Gregory Valiant']|['cs.LG', 'cs.AI', 'cs.CC', 'cs.CR', 'stat.ML']
2017-04-07T11:28:13Z|2017-03-14T17:22:19Z|http://arxiv.org/abs/1703.04598v1|http://arxiv.org/pdf/1703.04598v1|Verification in Staged Tile Self-Assembly|verif stage tile self assembl|We prove the unique assembly and unique shape verification problems, benchmark measures of self-assembly model power, are $\mathrm{coNP}^{\mathrm{NP}}$-hard and contained in $\mathrm{PSPACE}$ (and in $\mathrm{\Pi}^\mathrm{P}_{2s}$ for staged systems with $s$ stages). En route, we prove that unique shape verification problem in the 2HAM is $\mathrm{coNP}^{\mathrm{NP}}$-complete.|prove uniqu assembl uniqu shape verif problem benchmark measur self assembl model power mathrm conp mathrm np hard contain mathrm pspace mathrm pi mathrm stage system stage en rout prove uniqu shape verif problem ham mathrm conp mathrm np complet|['Robert Schweller', 'Andrew Winslow', 'Tim Wylie']|['cs.CC']
2017-04-07T11:28:13Z|2017-04-05T16:42:57Z|http://arxiv.org/abs/1703.04456v2|http://arxiv.org/pdf/1703.04456v2|P?=NP as minimization of degree 4 polynomial or Grassmann number problem|np minim degre polynomi grassmann number problem|While the P vs NP problem is mainly being approach form the point of view of discrete mathematics, this paper proposes two reformulations into the field of abstract algebra and of continuous global optimization - which advanced tools might bring new perspectives and approaches to approach this question. The first one is equivalence of satisfaction of 3-SAT problem with the question of reaching zero of a nonnegative degree 4 multivariate polynomial (sum of squares), what could be tested from the perspective of algebra by using discriminant. Alternatively, in addition to search in the discrete set $\{0,1\}^n$ of boolean values, this minimization formulation allows to take the search inside the continuous $[0,1]^n$ hypercube, exploiting gradients based on the entire problem, which suggest local search direction. Unless exponential growth of uninteresting local minima, such gradient descent from multiple random initial points might be essentially faster than brute force, or be exploited in a physical realization like adiabatic quantum computer, what might endanger some current cryptography. The second discussed approach is using anti-commuting Grassmann numbers $\theta_i$, making $(A \cdot \textrm{diag}(\theta_i))^n$ nonzero only if $A$ has a Hamilton cycle. Hence, the P$\ne$NP assumption implies exponential growth of matrix representation of Grassmann numbers.|vs np problem main approach form point view discret mathemat paper propos two reformul field abstract algebra continu global optim advanc tool might bring new perspect approach approach question first one equival satisfact sat problem question reach zero nonneg degre multivari polynomi sum squar could test perspect algebra use discrimin altern addit search discret set boolean valu minim formul allow take search insid continu hypercub exploit gradient base entir problem suggest local search direct unless exponenti growth uninterest local minima gradient descent multipl random initi point might essenti faster brute forc exploit physic realize like adiabat quantum comput might endang current cryptographi second discuss approach use anti commut grassmann number theta make cdot textrm diag theta nonzero onli hamilton cycl henc ne np assumpt impli exponenti growth matrix represent grassmann number|['Jarek Duda']|['cs.CC']
2017-04-07T11:28:13Z|2017-03-13T09:26:05Z|http://arxiv.org/abs/1703.04300v1|http://arxiv.org/pdf/1703.04300v1|A Note on the Inapproximability of Induced Disjoint Paths|note inapproxim induc disjoint path|We study the inapproximability of the induced disjoint paths problem on an arbitrary $n$-node $m$-edge undirected graph, which is to connect the maximum number of the $k$ source-sink pairs given in the graph via induced disjoint paths. It is known that the problem is NP-hard to approximate within $m^{{1\over 2}-\varepsilon}$ for a general $k$ and any $\varepsilon>0$. In this paper, we prove that the problem is NP-hard to approximate within $n^{1-\varepsilon}$ for a general $k$ and any $\varepsilon>0$ by giving a simple reduction from the independent set problem.|studi inapproxim induc disjoint path problem arbitrari node edg undirect graph connect maximum number sourc sink pair given graph via induc disjoint path known problem np hard approxim within varepsilon general ani varepsilon paper prove problem np hard approxim within varepsilon general ani varepsilon give simpl reduct independ set problem|['Gaoxiu Dong', 'Weidong Chen']|['cs.CC']
2017-04-07T11:28:13Z|2017-03-13T07:50:35Z|http://arxiv.org/abs/1703.04281v1|http://arxiv.org/pdf/1703.04281v1|Affine counter automata|affin counter automata|We introduce an affine generalization of counter automata, and analyze their ability as well as affine finite automata. Our contributions are as follows. We show that there is a promise problem that can be solved by exact affine counter automata but cannot be solved by deterministic counter automata. We also show that a certain promise problem, which is conjectured not to be solved by two-way quantum finite automata in polynomial time, can be solved by Las-Vegas affine finite automata in linear time. Lastly, we show that how a counter helps for affine finite automata by showing that the language $ \mathtt{MANYTWINS} $, which is conjectured not to be recognized by affine, quantum or classical finite state models in polynomial time, can be recognized by affine counter automata with one-sided bounded-error in realtime.|introduc affin general counter automata analyz abil well affin finit automata contribut follow show promis problem solv exact affin counter automata cannot solv determinist counter automata also show certain promis problem conjectur solv two way quantum finit automata polynomi time solv las vega affin finit automata linear time last show counter help affin finit automata show languag mathtt manytwin conjectur recogn affin quantum classic finit state model polynomi time recogn affin counter automata one side bound error realtim|['Masaki Nakanishi', 'Abuzer Yakaryılmaz']|['cs.FL', 'cs.CC', 'quant-ph']
2017-04-07T11:28:13Z|2017-03-12T17:11:49Z|http://arxiv.org/abs/1703.04143v1|http://arxiv.org/pdf/1703.04143v1|Bernoulli Factories and Black-Box Reductions in Mechanism Design|bernoulli factori black box reduct mechan design|"We provide a polynomial time reduction from Bayesian incentive compatible mechanism design to Bayesian algorithm design for welfare maximization problems. Unlike prior results, our reduction achieves exact incentive compatibility for problems with multi-dimensional and continuous type spaces. The key technical barrier preventing exact incentive compatibility in prior black-box reductions is that repairing violations of incentive constraints requires understanding the distribution of the mechanism's output. Reductions that instead estimate the output distribution by sampling inevitably suffer from sampling error, which typically precludes exact incentive compatibility.   We overcome this barrier by employing and generalizing the computational model in the literature on Bernoulli Factories. In a Bernoulli factory problem, one is given a function mapping the bias of an ""input coin"" to that of an ""output coin"", and the challenge is to efficiently simulate the output coin given sample access to the input coin. We generalize this to the ""expectations from samples"" computational model, in which an instance is specified by a function mapping the expected values of a set of input distributions to a distribution over outcomes. The challenge is to give a polynomial time algorithm that exactly samples from the distribution over outcomes given only sample access to the input distributions. In this model, we give a polynomial time algorithm for the exponential weights: expected values of the input distributions correspond to the weights of alternatives and we wish to select an alternative with probability proportional to an exponential function of its weight. This algorithm is the key ingredient in designing an incentive compatible mechanism for bipartite matching, which can be used to make the approximately incentive compatible reduction of Hartline et al. (2015) exactly incentive compatible."|provid polynomi time reduct bayesian incent compat mechan design bayesian algorithm design welfar maxim problem unlik prior result reduct achiev exact incent compat problem multi dimension continu type space key technic barrier prevent exact incent compat prior black box reduct repair violat incent constraint requir understand distribut mechan output reduct instead estim output distribut sampl inevit suffer sampl error typic preclud exact incent compat overcom barrier employ general comput model literatur bernoulli factori bernoulli factori problem one given function map bias input coin output coin challeng effici simul output coin given sampl access input coin general expect sampl comput model instanc specifi function map expect valu set input distribut distribut outcom challeng give polynomi time algorithm exact sampl distribut outcom given onli sampl access input distribut model give polynomi time algorithm exponenti weight expect valu input distribut correspond weight altern wish select altern probabl proport exponenti function weight algorithm key ingredi design incent compat mechan bipartit match use make approxim incent compat reduct hartlin et al exact incent compat|['Shaddin Dughmi', 'Jason Hartline', 'Robert Kleinberg', 'Rad Niazadeh']|['cs.GT', 'cs.CC', 'cs.DS', 'math.PR']
2017-04-07T11:28:13Z|2017-03-10T16:03:26Z|http://arxiv.org/abs/1703.03734v1|http://arxiv.org/pdf/1703.03734v1|On matrices with displacement structure: generalized operators and   faster algorithms|matric displac structur general oper faster algorithm|"For matrices with displacement structure, basic operations like multiplication, inversion, and linear system solving can all be expressed in terms of the following task: evaluate the product $\mathsf{A}\mathsf{B}$, where $\mathsf{A}$ is a structured $n \times n$ matrix of displacement rank $\alpha$, and $\mathsf{B}$ is an arbitrary $n\times\alpha$ matrix. Given $\mathsf{B}$ and a so-called ""generator"" of $\mathsf{A}$, this product is classically computed with a cost ranging from $O(\alpha^2 \mathscr{M}(n))$ to $O(\alpha^2 \mathscr{M}(n)\log(n))$ arithmetic operations, depending on the type of structure of $\mathsf{A}$; here, $\mathscr{M}$ is a cost function for polynomial multiplication. In this paper, we first generalize classical displacement operators, based on block diagonal matrices with companion diagonal blocks, and then design fast algorithms to perform the task above for this extended class of structured matrices. The cost of these algorithms ranges from $O(\alpha^{\omega-1} \mathscr{M}(n))$ to $O(\alpha^{\omega-1} \mathscr{M}(n)\log(n))$, with $\omega$ such that two $n \times n$ matrices over a field can be multiplied using $O(n^\omega)$ field operations. By combining this result with classical randomized regularization techniques, we obtain faster Las Vegas algorithms for structured inversion and linear system solving."|matric displac structur basic oper like multipl invers linear system solv express term follow task evalu product mathsf mathsf mathsf structur time matrix displac rank alpha mathsf arbitrari time alpha matrix given mathsf call generat mathsf product classic comput cost rang alpha mathscr alpha mathscr log arithmet oper depend type structur mathsf mathscr cost function polynomi multipl paper first general classic displac oper base block diagon matric companion diagon block design fast algorithm perform task abov extend class structur matric cost algorithm rang alpha omega mathscr alpha omega mathscr log omega two time matric field multipli use omega field oper combin result classic random regular techniqu obtain faster las vega algorithm structur invers linear system solv|['Alin Bostan', 'Claude-Pierre Jeannerod', 'Christophe Mouilleron', 'Éric Schost']|['cs.SC', 'cs.CC', '68W30, 68Q25, 97H60', 'I.1.2']
2017-04-07T11:28:13Z|2017-03-10T08:35:24Z|http://arxiv.org/abs/1703.03575v1|http://arxiv.org/pdf/1703.03575v1|Crossing the Logarithmic Barrier for Dynamic Boolean Data Structure   Lower Bounds|cross logarithm barrier dynam boolean data structur lower bound|"This paper proves the first super-logarithmic lower bounds on the cell probe complexity of dynamic boolean (a.k.a. decision) data structure problems, a long-standing milestone in data structure lower bounds.   We introduce a new method for proving dynamic cell probe lower bounds and use it to prove a $\tilde{\Omega}(\log^{1.5} n)$ lower bound on the operational time of a wide range of boolean data structure problems, most notably, on the query time of dynamic range counting over $\mathbb{F}_2$ ([Pat07]). Proving an $\omega(\lg n)$ lower bound for this problem was explicitly posed as one of five important open problems in the late Mihai P\v{a}tra\c{s}cu's obituary [Tho13]. This result also implies the first $\omega(\lg n)$ lower bound for the classical 2D range counting problem, one of the most fundamental data structure problems in computational geometry and spatial databases. We derive similar lower bounds for boolean versions of dynamic polynomial evaluation and 2D rectangle stabbing, and for the (non-boolean) problems of range selection and range median.   Our technical centerpiece is a new way of ""weakly"" simulating dynamic data structures using efficient one-way communication protocols with small advantage over random guessing. This simulation involves a surprising excursion to low-degree (Chebychev) polynomials which may be of independent interest, and offers an entirely new algorithmic angle on the ""cell sampling"" method of Panigrahy et al. [PTW10]."|paper prove first super logarithm lower bound cell probe complex dynam boolean decis data structur problem long stand mileston data structur lower bound introduc new method prove dynam cell probe lower bound use prove tild omega log lower bound oper time wide rang boolean data structur problem notabl queri time dynam rang count mathbb pat prove omega lg lower bound problem explicit pose one five import open problem late mihai tra cu obituari tho result also impli first omega lg lower bound classic rang count problem one fundament data structur problem comput geometri spatial databas deriv similar lower bound boolean version dynam polynomi evalu rectangl stab non boolean problem rang select rang median technic centerpiec new way weak simul dynam data structur use effici one way communic protocol small advantag random guess simul involv surpris excurs low degre chebychev polynomi may independ interest offer entir new algorithm angl cell sampl method panigrahi et al ptw|['Kasper Green Larsen', 'Omri Weinstein', 'Huacheng Yu']|['cs.DS', 'cs.CC', 'cs.CG', 'cs.IT', 'math.IT']
2017-04-07T11:28:13Z|2017-03-09T13:45:45Z|http://arxiv.org/abs/1703.03262v1|http://arxiv.org/pdf/1703.03262v1|Does Nash Envy Immunity|doe nash envi immun|The most popular stability notion in games should be Nash equilibrium under the rationality of players who maximize their own payoff individually. In contrast, in many scenarios, players can be (partly) irrational with some unpredictable factors. Hence a strategy profile can be more robust if it is resilient against certain irrational behaviors. In this paper, we propose a stability notion that is resilient against envy. A strategy profile is said to be envy-proof if each player cannot gain a competitive edge with respect to the change in utility over the other players by deviation. Together with Nash equilibrium and another stability notion called immunity, we show how these separate notions are related to each other, whether they exist in games, and whether and when a strategy profile satisfying these notions can be efficiently found. We answer these questions by starting with the general two player game and extend the discussion for the approximate stability and for the corresponding fault-tolerance notions in multi-player games.|popular stabil notion game nash equilibrium ration player maxim payoff individu contrast mani scenario player part irrat unpredict factor henc strategi profil robust resili certain irrat behavior paper propos stabil notion resili envi strategi profil said envi proof player cannot gain competit edg respect chang util player deviat togeth nash equilibrium anoth stabil notion call immun show separ notion relat whether exist game whether strategi profil satisfi notion effici found answer question start general two player game extend discuss approxim stabil correspond fault toler notion multi player game|['Ching-Hua Yu']|['cs.GT', 'cs.CC', 'cs.CR', 'I.2.1; J.4; K.6.0; C.4']
2017-04-07T11:28:18Z|2017-04-06T17:23:07Z|http://arxiv.org/abs/1703.03021v2|http://arxiv.org/pdf/1703.03021v2|A dichotomy theorem for nonuniform CSPs|dichotomi theorem nonuniform csps|In this paper we prove the Dichotomy Conjecture on the complexity of nonuniform constraint satisfaction problems posed by Feder and Vardi.|paper prove dichotomi conjectur complex nonuniform constraint satisfact problem pose feder vardi|['Andrei A. Bulatov']|['cs.CC']
2017-04-07T11:28:18Z|2017-03-07T16:50:43Z|http://arxiv.org/abs/1703.02469v1|http://arxiv.org/pdf/1703.02469v1|Random CNFs are Hard for Cutting Planes|random cnfs hard cut plane|The random k-SAT model is the most important and well-studied distribution over k-SAT instances. It is closely connected to statistical physics; it is used as a testbench for satisfiability algorithms, and average-case hardness over this distribution has also been linked to hardness of approximation via Feige's hypothesis. We prove that any Cutting Planes refutation for random k-SAT requires exponential size, for k that is logarithmic in the number of variables, in the (interesting) regime where the number of clauses guarantees that the formula is unsatisfiable with high probability.|random sat model import well studi distribut sat instanc close connect statist physic use testbench satisfi algorithm averag case hard distribut also link hard approxim via feig hypothesi prove ani cut plane refut random sat requir exponenti size logarithm number variabl interest regim number claus guarante formula unsatisfi high probabl|['Noah Fleming', 'Denis Pankratov', 'Toniann Pitassi', 'Robert Robere']|['cs.CC']
2017-04-07T11:28:18Z|2017-03-07T12:45:26Z|http://arxiv.org/abs/1703.02361v1|http://arxiv.org/pdf/1703.02361v1|On the family of 0/1-polytopes with NP-complete non-adjacency relation|famili polytop np complet non adjac relat|In 1995 T. Matsui considered a special family 0/1-polytopes for which the problem of recognizing the non-adjacency of two arbitrary vertices is NP-complete. In 2012 the author of this paper established that all the polytopes of this family are present as faces in the polytopes associated with the following NP-complete problems: the traveling salesman problem, the 3-satisfiability problem, the knapsack problem, the set covering problem, the partial ordering problem, the cube subgraph problem, and some others. In particular, it follows that for these families the non-adjacency relation is also NP-complete. On the other hand, it is known that the vertex adjacency criterion is polynomial for polytopes of the following NP-complete problems: the maximum independent set problem, the set packing and the set partitioning problem, the three-index assignment problem. It is shown that none of the polytopes of the above-mentioned special family (with the exception of a one-dimensional segment) can be the face of polytopes associated with the problems of the maximum independent set, of a set packing and partitioning, and of 3-assignments.|matsui consid special famili polytop problem recogn non adjac two arbitrari vertic np complet author paper establish polytop famili present face polytop associ follow np complet problem travel salesman problem satisfi problem knapsack problem set cover problem partial order problem cube subgraph problem particular follow famili non adjac relat also np complet hand known vertex adjac criterion polynomi polytop follow np complet problem maximum independ set problem set pack set partit problem three index assign problem shown none polytop abov mention special famili except one dimension segment face polytop associ problem maximum independ set set pack partit assign|['Alexander Maksimenko']|['cs.CC']
2017-04-07T11:28:18Z|2017-03-07T11:22:53Z|http://arxiv.org/abs/1703.02332v1|http://arxiv.org/pdf/1703.02332v1|The Minimum Shared Edges Problem on Grid-like Graphs|minimum share edg problem grid like graph|We study the NP-hard Minimum Shared Edges (MSE) problem on graphs: decide whether it is possible to route $p$ paths from a start vertex to a target vertex in a given graph while using at most $k$ edges more than once. We show that MSE can be decided on bounded grids in linear time when both dimensions are either small or large compared to the number $p$ of paths. On the contrary, we show that MSE remains NP-hard on subgraphs of bounded grids. Finally, we study MSE from a parametrised complexity point of view. It is known that MSE is fixed-parameter tractable with respect to the number $p$ of paths. We show that, under standard complexity-theoretical assumptions, the problem parametrised by the combined parameter $k$, $p$, maximum degree, diameter, and treewidth does not admit a polynomial-size problem kernel, even when restricted to planar graphs.|studi np hard minimum share edg mse problem graph decid whether possibl rout path start vertex target vertex given graph use edg onc show mse decid bound grid linear time dimens either small larg compar number path contrari show mse remain np hard subgraph bound grid final studi mse parametris complex point view known mse fix paramet tractabl respect number path show standard complex theoret assumpt problem parametris combin paramet maximum degre diamet treewidth doe admit polynomi size problem kernel even restrict planar graph|['Till Fluschnik', 'Meike Hatzel', 'Steffen Härtlein', 'Hendrik Molter', 'Henning Seidler']|['cs.CC', '68Q17, 68Q25, 68R10, 05C10', 'F.1.3; F.2.2; G.2.2']
2017-04-07T11:28:18Z|2017-03-06T15:42:36Z|http://arxiv.org/abs/1703.01928v1|http://arxiv.org/pdf/1703.01928v1|On The Complexity of Enumeration|complex enumer|We investigate the relationship between several enumeration complexity classes and focus in particular on the incremental polynomial time and the polynomial delay (IncP and DelayP). We prove, modulo the Exponential Time Hypothesis, that IncP contains a strict hierarchy of subclasses. Since DelayP is included in IncP_1, the first class of the hierarchy, it is separated from IncP. We prove for some algorithms that we can turn an average delay into a worst case delay, suggesting that IncP_1 = DelayP even with a polynomially bounded memory. Finally we relate the uniform generation of solutions to probabilistic enumeration algorithms with polynomial delay.|investig relationship sever enumer complex class focus particular increment polynomi time polynomi delay incp delayp prove modulo exponenti time hypothesi incp contain strict hierarchi subclass sinc delayp includ incp first class hierarchi separ incp prove algorithm turn averag delay worst case delay suggest incp delayp even polynomi bound memori final relat uniform generat solut probabilist enumer algorithm polynomi delay|['Florent Capelli', 'Yann Strozecki']|['cs.CC']
2017-04-07T11:28:18Z|2017-03-05T23:06:03Z|http://arxiv.org/abs/1703.01686v1|http://arxiv.org/pdf/1703.01686v1|Parameterized complexity of finding a spanning tree with minimum reload   cost diameter|parameter complex find span tree minimum reload cost diamet|We study the minimum diameter spanning tree problem under the reload cost model (DIAMETER-TREE for short) introduced by Wirth and Steffan (2001). In this problem, given an undirected edge-colored graph $G$, reload costs on a path arise at a node where the path uses consecutive edges of different colors. The objective is to find a spanning tree of $G$ of minimum diameter with respect to the reload costs. We initiate a systematic study of the parameterized complexity of the DIAMETER-TREE problem by considering the following parameters: the cost of a solution, and the treewidth and the maximum degree $\Delta$ of the input graph. We prove that DIAMETER-TREE is para-NP-hard for any combination of two of these three parameters, and that it is FPT parameterized by the three of them. We also prove that the problem can be solved in polynomial time on cactus graphs. This result is somehow surprising since we prove DIAMETER-TREE to be NP-hard on graphs of treewidth two, which is best possible as the problem can be trivially solved on forests. When the reload costs satisfy the triangle inequality, Wirth and Steffan (2001) proved that the problem can be solved in polynomial time on graphs with $\Delta = 3$, and Galbiati (2008) proved that it is NP-hard if $\Delta = 4$. Our results show, in particular, that without the requirement of the triangle inequality, the problem is NP-hard if $\Delta = 3$, which is also best possible. Finally, in the case where the reload costs are polynomially bounded by the size of the input graph, we prove that DIAMETER-TREE is in XP and W[1]-hard parameterized by the treewidth plus $\Delta$.|studi minimum diamet span tree problem reload cost model diamet tree short introduc wirth steffan problem given undirect edg color graph reload cost path aris node path use consecut edg differ color object find span tree minimum diamet respect reload cost initi systemat studi parameter complex diamet tree problem consid follow paramet cost solut treewidth maximum degre delta input graph prove diamet tree para np hard ani combin two three paramet fpt parameter three also prove problem solv polynomi time cactus graph result somehow surpris sinc prove diamet tree np hard graph treewidth two best possibl problem trivial solv forest reload cost satisfi triangl inequ wirth steffan prove problem solv polynomi time graph delta galbiati prove np hard delta result show particular without requir triangl inequ problem np hard delta also best possibl final case reload cost polynomi bound size input graph prove diamet tree xp hard parameter treewidth plus delta|['Julien Baste', 'Didem Gözüpek', 'Christophe Paul', 'Ignasi Sau', 'Mordechai Shalom', 'Dimitrios M. Thilikos']|['cs.DS', 'cs.CC', '05C85, 05C10', 'G.2.2; G.2.3']
2017-04-07T11:28:18Z|2017-03-03T13:04:46Z|http://arxiv.org/abs/1703.01143v1|http://arxiv.org/pdf/1703.01143v1|Why is it hard to beat $O(n^2)$ for Longest Common Weakly Increasing   Subsequence?|whi hard beat longest common weak increas subsequ|The Longest Common Weakly Increasing Subsequence problem (LCWIS) is a variant of the classic Longest Common Subsequence problem (LCS). Both problems can be solved with simple quadratic time algorithms. A recent line of research led to a number of matching conditional lower bounds for LCS and other related problems. However, the status of LCWIS remained open.   In this paper we show that LCWIS cannot be solved in strongly subquadratic time unless the Strong Exponential Time Hypothesis (SETH) is false.   The ideas which we developed can also be used to obtain a lower bound based on a safer assumption of NC-SETH, i.e. a version of SETH which talks about NC circuits instead of less expressive CNF formulas.|longest common weak increas subsequ problem lcwis variant classic longest common subsequ problem lcs problem solv simpl quadrat time algorithm recent line research led number match condit lower bound lcs relat problem howev status lcwis remain open paper show lcwis cannot solv strong subquadrat time unless strong exponenti time hypothesi seth fals idea develop also use obtain lower bound base safer assumpt nc seth version seth talk nc circuit instead less express cnf formula|['Adam Polak']|['cs.CC']
2017-04-07T11:28:18Z|2017-03-02T20:25:04Z|http://arxiv.org/abs/1703.00941v1|http://arxiv.org/pdf/1703.00941v1|On the Fine-grained Complexity of One-Dimensional Dynamic Programming|fine grain complex one dimension dynam program|In this paper, we investigate the complexity of one-dimensional dynamic programming, or more specifically, of the Least-Weight Subsequence (LWS) problem: Given a sequence of $n$ data items together with weights for every pair of the items, the task is to determine a subsequence $S$ minimizing the total weight of the pairs adjacent in $S$. A large number of natural problems can be formulated as LWS problems, yielding obvious $O(n^2)$-time solutions.   In many interesting instances, the $O(n^2)$-many weights can be succinctly represented. Yet except for near-linear time algorithms for some specific special cases, little is known about when an LWS instantiation admits a subquadratic-time algorithm and when it does not. In particular, no lower bounds for LWS instantiations have been known before. In an attempt to remedy this situation, we provide a general approach to study the fine-grained complexity of succinct instantiations of the LWS problem. In particular, given an LWS instantiation we identify a highly parallel core problem that is subquadratically equivalent. This provides either an explanation for the apparent hardness of the problem or an avenue to find improved algorithms as the case may be.   More specifically, we prove subquadratic equivalences between the following pairs (an LWS instantiation and the corresponding core problem) of problems: a low-rank version of LWS and minimum inner product, finding the longest chain of nested boxes and vector domination, and a coin change problem which is closely related to the knapsack problem and (min,+)-convolution. Using these equivalences and known SETH-hardness results for some of the core problems, we deduce tight conditional lower bounds for the corresponding LWS instantiations. We also establish the (min,+)-convolution-hardness of the knapsack problem.|paper investig complex one dimension dynam program specif least weight subsequ lws problem given sequenc data item togeth weight everi pair item task determin subsequ minim total weight pair adjac larg number natur problem formul lws problem yield obvious time solut mani interest instanc mani weight succinct repres yet except near linear time algorithm specif special case littl known lws instanti admit subquadrat time algorithm doe particular lower bound lws instanti known befor attempt remedi situat provid general approach studi fine grain complex succinct instanti lws problem particular given lws instanti identifi high parallel core problem subquadrat equival provid either explan appar hard problem avenu find improv algorithm case may specif prove subquadrat equival follow pair lws instanti correspond core problem problem low rank version lws minimum inner product find longest chain nest box vector domin coin chang problem close relat knapsack problem min convolut use equival known seth hard result core problem deduc tight condit lower bound correspond lws instanti also establish min convolut hard knapsack problem|['Marvin Künnemann', 'Ramamohan Paturi', 'Stefan Schneider']|['cs.CC', 'cs.DS']
2017-04-07T11:28:18Z|2017-03-01T23:15:54Z|http://arxiv.org/abs/1703.00544v1|http://arxiv.org/pdf/1703.00544v1|Simplified Algorithmic Metatheorems Beyond MSO: Treewidth and   Neighborhood Diversity|simplifi algorithm metatheorem beyond mso treewidth neighborhood divers|This paper settles the computational complexity of model checking of several extensions of the monadic second order (MSO) logic on two classes of graphs: graphs of bounded treewidth and graphs of bounded neighborhood diversity. A classical theorem of Courcelle states that any graph property definable in MSO is decidable in linear time on graphs of bounded treewidth. Algorithmic metatheorems like Courcelle's serve to generalize known positive results on various graph classes. We explore and extend three previously studied MSO extensions: global and local cardinality constraints (CardMSO and MSO-LCC) and optimizing a fair objective function (fairMSO). First, we show how these fragments relate to each other in expressive power and highlight their (non)linearity. On the side of neighborhood diversity, we show that combining the linear variants of local and global cardinality constraints is possible while keeping the linear runtime but removing linearity of either makes this impossible, and we provide a polynomial time algorithm for the hard case. Furthemore, we show that even the combination of the two most powerful fragments is solvable in polynomial time on graphs of bounded treewidth.|paper settl comput complex model check sever extens monad second order mso logic two class graph graph bound treewidth graph bound neighborhood divers classic theorem courcell state ani graph properti defin mso decid linear time graph bound treewidth algorithm metatheorem like courcell serv general known posit result various graph class explor extend three previous studi mso extens global local cardin constraint cardmso mso lcc optim fair object function fairmso first show fragment relat express power highlight non linear side neighborhood divers show combin linear variant local global cardin constraint possibl keep linear runtim remov linear either make imposs provid polynomi time algorithm hard case furthemor show even combin two power fragment solvabl polynomi time graph bound treewidth|['Dušan Knop', 'Martin Koutecký', 'Tomáš Masařík', 'Tomáš Toufar']|['cs.CC', 'cs.LO', '03D15', 'F.2.2']
2017-04-07T11:28:18Z|2017-03-15T08:50:49Z|http://arxiv.org/abs/1703.00242v2|http://arxiv.org/pdf/1703.00242v2|Reordering Method and Hierarchies for Quantum and Classical Ordered   Binary Decision Diagrams|reorder method hierarchi quantum classic order binari decis diagram|"We consider Quantum OBDD model. It is restricted version of read-once Quantum Branching Programs, with respect to ""width"" complexity. It is known that maximal complexity gap between deterministic and quantum model is exponential. But there are few examples of such functions. We present method (called ""reordering""), which allows to build Boolean function $g$ from Boolean Function $f$, such that if for $f$ we have gap between quantum and deterministic OBDD complexity for natural order of variables, then we have almost the same gap for function $g$, but for any order. Using it we construct the total function $REQ$ which deterministic OBDD complexity is $2^{\Omega(n/\log n)}$ and present quantum OBDD of width $O(n^2)$. It is bigger gap for explicit function that was known before for OBDD of width more than linear. Using this result we prove the width hierarchy for complexity classes of Boolean functions for quantum OBDDs.   Additionally, we prove the width hierarchy for complexity classes of Boolean functions for bounded error probabilistic OBDDs. And using ""reordering"" method we extend a hierarchy for $k$-OBDD of polynomial size, for $k=o(n/\log^3n)$. Moreover, we proved a similar hierarchy for bounded error probabilistic $k$-OBDD. And for deterministic and probabilistic $k$-OBDDs of superpolynomial and subexponential size."|consid quantum obdd model restrict version read onc quantum branch program respect width complex known maxim complex gap determinist quantum model exponenti exampl function present method call reorder allow build boolean function boolean function gap quantum determinist obdd complex natur order variabl almost gap function ani order use construct total function req determinist obdd complex omega log present quantum obdd width bigger gap explicit function known befor obdd width linear use result prove width hierarchi complex class boolean function quantum obdd addit prove width hierarchi complex class boolean function bound error probabilist obdd use reorder method extend hierarchi obdd polynomi size log moreov prove similar hierarchi bound error probabilist obdd determinist probabilist obdd superpolynomi subexponenti size|['Kamil Khadiev', 'Aliya Khadieva']|['cs.CC', 'quant-ph']
2017-04-07T11:28:22Z|2017-02-28T20:28:03Z|http://arxiv.org/abs/1703.00043v1|http://arxiv.org/pdf/1703.00043v1|Tree tribes and lower bounds for switching lemmas|tree tribe lower bound switch lemma|We show tight upper and lower bounds for switching lemmas obtained by the action of random $p$-restrictions on boolean functions that can be expressed as decision trees in which every vertex is at a distance of at most $t$ from some leaf, also called $t$-clipped decision trees. More specifically, we show the following:   $\bullet$ If a boolean function $f$ can be expressed as a $t$-clipped decision tree, then under the action of a random $p$-restriction $\rho$, the probability that the smallest depth decision tree for $f _{\rho}$ has depth greater than $d$ is upper bounded by $(4p2^{t})^{d}$.   $\bullet$ For every $t$, there exists a function $g_{t}$ that can be expressed as a $t$-clipped decision tree, such that under the action of a random $p$-restriction $\rho$, the probability that the smallest depth decision tree for $g_{t} _{\rho}$ has depth greater than $d$ is lower bounded by $(c_{0}p2^{t})^{d}$, for $0\leq p\leq c_{p}2^{-t}$ and $0\leq d\leq c_{d}\frac{\log n}{2^{t}\log t}$, where $c_{0},c_{p},c_{d}$ are universal constants.|show tight upper lower bound switch lemma obtain action random restrict boolean function express decis tree everi vertex distanc leaf also call clip decis tree specif show follow bullet boolean function express clip decis tree action random restrict rho probabl smallest depth decis tree rho depth greater upper bound bullet everi exist function express clip decis tree action random restrict rho probabl smallest depth decis tree rho depth greater lower bound leq leq leq leq frac log log univers constant|['Jenish C. Mehta']|['cs.CC']
2017-04-07T11:28:22Z|2017-02-28T16:57:49Z|http://arxiv.org/abs/1702.08862v1|http://arxiv.org/pdf/1702.08862v1|Proportional Representation in Vote Streams|proport represent vote stream|We consider elections where the voters come one at a time, in a streaming fashion, and devise space-efficient algorithms which identify an approximate winning committee with respect to common multiwinner proportional representation voting rules; specifically, we consider the Approval-based and the Borda-based variants of both the Chamberlin-- ourant rule and the Monroe rule. We complement our algorithms with lower bounds. Somewhat surprisingly, our results imply that, using space which does not depend on the number of voters it is possible to efficiently identify an approximate representative committee of fixed size over vote streams with huge number of voters.|consid elect voter come one time stream fashion devis space effici algorithm identifi approxim win committe respect common multiwinn proport represent vote rule specif consid approv base borda base variant chamberlin ourant rule monro rule complement algorithm lower bound somewhat surpris result impli use space doe depend number voter possibl effici identifi approxim repres committe fix size vote stream huge number voter|['Palash Dey', 'Nimrod Talmon', 'Otniel van Handel']|['cs.GT', 'cs.AI', 'cs.CC', 'cs.DS', 'cs.MA']
2017-04-07T11:28:22Z|2017-02-28T15:47:39Z|http://arxiv.org/abs/1702.08830v1|http://arxiv.org/pdf/1702.08830v1|The Complexity of Translationally-Invariant Low-Dimensional Spin   Lattices in 3D|complex translate invari low dimension spin lattic|In this paper, we consider spin systems in three spatial dimensions, and prove that the local Hamiltonian problem for 3D lattices with face-centered cubic unit cells, 4-local translationally-invariant interactions between spin-3/2 particles and open boundary conditions is QMAEXP-complete. We go beyond a mere embedding of past hard 1D history state constructions, and utilize a classical Wang tiling problem as binary counter in order to translate one cube side length into a binary description for the verifier input. We further make use of a recently-developed computational model especially well-suited for history state constructions, and combine it with a specific circuit encoding shown to be universal for quantum computation. These novel techniques allow us to significantly lower the local spin dimension, surpassing the best translationally-invariant result to date by two orders of magnitude (in the number of degrees of freedom per coupling). This brings our models en par with the best non-translationally-invariant construction.|paper consid spin system three spatial dimens prove local hamiltonian problem lattic face center cubic unit cell local translate invari interact spin particl open boundari condit qmaexp complet go beyond mere embed past hard histori state construct util classic wang tile problem binari counter order translat one cube side length binari descript verifi input make use recent develop comput model especi well suit histori state construct combin specif circuit encod shown univers quantum comput novel techniqu allow us signific lower local spin dimens surpass best translate invari result date two order magnitud number degre freedom per coupl bring model en par best non translate invari construct|['Johannes Bausch', 'Stephen Piddock']|['quant-ph', 'cs.CC', '68Q17, 81V70, 68Q10, 82D25']
2017-04-07T11:28:22Z|2017-03-05T02:18:12Z|http://arxiv.org/abs/1702.08662v3|http://arxiv.org/pdf/1702.08662v3|The computational complexity of integer programming with alternations|comput complex integ program altern|We prove that integer programming with three quantifier alternations is $NP$-complete, even for a fixed number of variables. This complements earlier results by Lenstra and Kannan, which together say that integer programming with at most two quantifier alternations can be done in polynomial time for a fixed number of variables. As a byproduct of the proof, we show that for two polytopes $P,Q \subset \mathbb{R}^4$ , counting the projection of integer points in $Q \backslash P$ is $\#P$-complete. This contrasts the 2003 result by Barvinok and Woods, which allows counting in polynomial time the projection of integer points in $P$ and $Q$ separately.|prove integ program three quantifi altern np complet even fix number variabl complement earlier result lenstra kannan togeth say integ program two quantifi altern done polynomi time fix number variabl byproduct proof show two polytop subset mathbb count project integ point backslash complet contrast result barvinok wood allow count polynomi time project integ point separ|['Danny Nguyen', 'Igor Pak']|['math.CO', 'cs.CC', 'cs.CG', 'cs.DM']
2017-04-07T11:28:22Z|2017-03-05T02:19:58Z|http://arxiv.org/abs/1702.08660v2|http://arxiv.org/pdf/1702.08660v2|Complexity of short generating functions|complex short generat function|We give complexity analysis of the class of short generating functions (GF). Assuming $\#P \not\subseteq FP/poly$, we show that this class is not closed under taking many intersections, unions or projections of GFs, in the sense that these operations can increase the bitlength of coefficients of GFs by a super-polynomial factor. We also prove that truncated theta functions are hard in this class.|give complex analysi class short generat function gf assum subseteq fp poli show class close take mani intersect union project gfs sens oper increas bitlength coeffici gfs super polynomi factor also prove truncat theta function hard class|['Danny Nguyen', 'Igor Pak']|['math.CO', 'cs.CC', 'cs.DM', 'cs.LO', 'math.LO']
2017-04-07T11:28:22Z|2017-02-27T19:46:15Z|http://arxiv.org/abs/1702.08489v1|http://arxiv.org/pdf/1702.08489v1|Depth Separation for Neural Networks|depth separ neural network|Let $f:\mathbb{S}^{d-1}\times \mathbb{S}^{d-1}\to\mathbb{S}$ be a function of the form $f(\mathbf{x},\mathbf{x}') = g(\langle\mathbf{x},\mathbf{x}'\rangle)$ for $g:[-1,1]\to \mathbb{R}$. We give a simple proof that shows that poly-size depth two neural networks with (exponentially) bounded weights cannot approximate $f$ whenever $g$ cannot be approximated by a low degree polynomial. Moreover, for many $g$'s, such as $g(x)=\sin(\pi d^3x)$, the number of neurons must be $2^{\Omega\left(d\log(d)\right)}$. Furthermore, the result holds w.r.t.\ the uniform distribution on $\mathbb{S}^{d-1}\times \mathbb{S}^{d-1}$. As many functions of the above form can be well approximated by poly-size depth three networks with poly-bounded weights, this establishes a separation between depth two and depth three networks w.r.t.\ the uniform distribution on $\mathbb{S}^{d-1}\times \mathbb{S}^{d-1}$.|let mathbb time mathbb mathbb function form mathbf mathbf langl mathbf mathbf rangl mathbb give simpl proof show poli size depth two neural network exponenti bound weight cannot approxim whenev cannot approxim low degre polynomi moreov mani sin pi number neuron must omega left log right furthermor result hold uniform distribut mathbb time mathbb mani function abov form well approxim poli size depth three network poli bound weight establish separ depth two depth three network uniform distribut mathbb time mathbb|['Amit Daniely']|['cs.LG', 'cs.CC', 'stat.ML']
2017-04-07T11:28:22Z|2017-02-27T19:26:15Z|http://arxiv.org/abs/1702.08483v1|http://arxiv.org/pdf/1702.08483v1|The computational landscape of general physical theories|comput landscap general physic theori|The emergence of quantum computers has challenged long-held beliefs about what is efficiently computable given our current physical theories. However, going back to the work of Abrams and Lloyd, changing one aspect of quantum theory can result in yet more dramatic increases in computational power, as well as violations of fundamental physical principles. Here we focus on efficient computation within a framework of general physical theories that make good operational sense. In prior work, Lee and Barrett showed that in any theory satisfying the principle of tomographic locality (roughly, local measurements suffice for tomography of multipartite states) the complexity bound on efficient computation is AWPP. This bound holds independently of whether the principle of causality (roughly, no signalling from the future) is satisfied. In this work we show that this bound is tight: there exists a theory satisfying both the principles of tomographic locality and causality which can efficiently decide everything in AWPP, and in particular can simulate any efficient quantum computation. Thus the class AWPP has a natural physical interpretation: it is precisely the class of problems that can be solved efficiently in tomographically-local theories. This theory is built upon a model of computing involving Turing machines with quasi-probabilities, to wit, machines with transition weights that can be negative but sum to unity over all branches. In analogy with the study of non-local quantum correlations, this leads us to question what physical principles recover the power of quantum computing. Along this line, we give some computational complexity evidence that quantum computation does not achieve the bound of AWPP.|emerg quantum comput challeng long held belief effici comput given current physic theori howev go back work abram lloyd chang one aspect quantum theori result yet dramat increas comput power well violat fundament physic principl focus effici comput within framework general physic theori make good oper sens prior work lee barrett show ani theori satisfi principl tomograph local rough local measur suffic tomographi multipartit state complex bound effici comput awpp bound hold independ whether principl causal rough signal futur satisfi work show bound tight exist theori satisfi principl tomograph local causal effici decid everyth awpp particular simul ani effici quantum comput thus class awpp natur physic interpret precis class problem solv effici tomograph local theori theori built upon model comput involv ture machin quasi probabl wit machin transit weight negat sum uniti branch analog studi non local quantum correl lead us question physic principl recov power quantum comput along line give comput complex evid quantum comput doe achiev bound awpp|['Jonathan Barrett', 'Niel de Beaudrap', 'Matty J. Hoban', 'Ciarán M. Lee']|['quant-ph', 'cs.CC']
2017-04-07T11:28:22Z|2017-02-27T12:21:07Z|http://arxiv.org/abs/1702.08255v1|http://arxiv.org/pdf/1702.08255v1|Learning with Errors is easy with quantum samples|learn error easi quantum sampl|Learning with Errors is one of the fundamental problems in computational learning theory and has in the last years become the cornerstone of post-quantum cryptography. In this work, we study the quantum sample complexity of Learning with Errors and show that there exists an efficient quantum learning algorithm (with polynomial sample and time complexity) for the Learning with Errors problem where the error distribution is the one used in cryptography. While our quantum learning algorithm does not break the LWE-based encryption schemes proposed in the cryptography literature, it does have some interesting implications for cryptography: first, when building an LWE-based scheme, one needs to be careful about the access to the public-key generation algorithm that is given to the adversary; second, our algorithm shows a possible way for attacking LWE-based encryption by using classical samples to approximate the quantum sample state, since then using our quantum learning algorithm would solve LWE.|learn error one fundament problem comput learn theori last year becom cornerston post quantum cryptographi work studi quantum sampl complex learn error show exist effici quantum learn algorithm polynomi sampl time complex learn error problem error distribut one use cryptographi quantum learn algorithm doe break lwe base encrypt scheme propos cryptographi literatur doe interest implic cryptographi first build lwe base scheme one need care access public key generat algorithm given adversari second algorithm show possibl way attack lwe base encrypt use classic sampl approxim quantum sampl state sinc use quantum learn algorithm would solv lwe|['Alex B. Grilo', 'Iordanis Kerenidis']|['quant-ph', 'cs.CC']
2017-04-07T11:28:22Z|2017-02-27T11:24:02Z|http://arxiv.org/abs/1702.08238v1|http://arxiv.org/pdf/1702.08238v1|Consensus Patterns parameterized by input string length is W[1]-hard|consensus pattern parameter input string length hard|We consider the Consensus Patterns problem, where, given a set of input strings, one is asked to extract a long-enough pattern which appears (with some errors) in all strings. We prove that this problem is W[1]-hard when parameterized by the maximum length of input strings.|consid consensus pattern problem given set input string one ask extract long enough pattern appear error string prove problem hard parameter maximum length input string|['Laurent Bulteau']|['cs.CC']
2017-04-07T11:28:22Z|2017-02-27T04:42:03Z|http://arxiv.org/abs/1702.08144v1|http://arxiv.org/pdf/1702.08144v1|Synchronization Problems in Automata without Non-trivial Cycles|synchron problem automata without non trivial cycl|In this paper, we study the computational complexity of various problems related to synchronization of weakly acyclic automata, a subclass of widely studied aperiodic automata. We provide upper and lower bounds on the length of a shortest word synchronizing a weakly acyclic automaton or, more generally, a subset of its states, and show that the problem of approximating this length is hard. We also show inapproximability of the problem of computing the rank of a subset of states in a binary weakly acyclic automaton and prove that several problems related to recognizing a synchronizing subset of states in such automata are NP-complete.|paper studi comput complex various problem relat synchron weak acycl automata subclass wide studi aperiod automata provid upper lower bound length shortest word synchron weak acycl automaton general subset state show problem approxim length hard also show inapproxim problem comput rank subset state binari weak acycl automaton prove sever problem relat recogn synchron subset state automata np complet|['Andrew Ryzhikov']|['cs.FL', 'cs.CC', '68Q17', 'F.1.1; F.1.3; F.2.2']
2017-04-07T11:28:26Z|2017-02-26T20:53:29Z|http://arxiv.org/abs/1702.08084v1|http://arxiv.org/pdf/1702.08084v1|On Algorithmic Statistics for space-bounded algorithms|algorithm statist space bound algorithm|Algorithmic statistics studies explanations of observed data that are good in the algorithmic sense: an explanation should be simple i.e. should have small Kolmogorov complexity and capture all the algorithmically discoverable regularities in the data. However this idea can not be used in practice because Kolmogorov complexity is not computable.   In this paper we develop algorithmic statistics using space-bounded Kolmogorov complexity. We prove an analogue of one of the main result of `classic' algorithmic statistics (about the connection between optimality and randomness deficiences). The main tool of our proof is the Nisan-Wigderson generator.|algorithm statist studi explan observ data good algorithm sens explan simpl small kolmogorov complex captur algorithm discover regular data howev idea use practic becaus kolmogorov complex comput paper develop algorithm statist use space bound kolmogorov complex prove analogu one main result classic algorithm statist connect optim random defici main tool proof nisan wigderson generat|['Alexey Milovanov']|['cs.IT', 'cs.CC', 'math.IT']
2017-04-07T11:28:26Z|2017-03-27T05:15:38Z|http://arxiv.org/abs/1702.08045v2|http://arxiv.org/pdf/1702.08045v2|General Upper Bounds for Gate Complexity and Depth of Reversible   Circuits Consisting of NOT, CNOT and 2-CNOT Gates|general upper bound gate complex depth revers circuit consist cnot cnot gate|The paper discusses the gate complexity and the depth of reversible circuits consisting of NOT, CNOT and 2-CNOT gates in the case, when the number of additional inputs is limited. We study Shannon's gate complexity function $L(n, q)$ and depth function $D(n, q)$ for a reversible circuit implementing a Boolean transformation $f\colon \mathbb Z_2^n \to \mathbb Z_2^n$ with $8n < q \lesssim n2^{n-o(n)}$ additional inputs. The general upper bounds $L(n,q) \lesssim 2^n + 8n2^n \mathop / (\log_2 (q-4n) - \log_2 n - 2)$ and $D(n,q) \lesssim 2^{n+1}(2,5 + \log_2 n - \log_2 (\log_2 (q - 4n) - \log_2 n - 2))$ are proved for this case.|paper discuss gate complex depth revers circuit consist cnot cnot gate case number addit input limit studi shannon gate complex function depth function revers circuit implement boolean transform colon mathbb mathbb lesssim addit input general upper bound lesssim mathop log log lesssim log log log log prove case|['Dmitry V. Zakablukov']|['cs.CC']
2017-04-07T11:28:26Z|2017-02-26T07:12:46Z|http://arxiv.org/abs/1702.08443v1|http://arxiv.org/pdf/1702.08443v1|Elementary Yet Precise Worst-case Analysis of MergeSort, A short version   (SV)|elementari yet precis worst case analysi mergesort short version sv|This paper offers two elementary yet precise derivations of an exact formula   \[ W(n) = \sum_{i=1} ^{n} \lceil \lg i \rceil = n \lceil \lg n \rceil - 2^{\lceil \lg n \rceil} + 1 \] for the maximum number $ W(n) $ of comparisons of keys performed by $ {\tt MergeSort} $ on an $ n $-element array. The first of the two, due to its structural regularity, is well worth carefully studying in its own right.   Close smooth bounds on $ W(n) $ are derived. It seems interesting that $ W(n) $ is linear between the points $ n = 2^{\lfloor \lg n \rfloor} $ and it linearly interpolates its own lower bound $ n \lg n - n + 1 $ between these points.|paper offer two elementari yet precis deriv exact formula sum lceil lg rceil lceil lg rceil lceil lg rceil maximum number comparison key perform tt mergesort element array first two due structur regular well worth care studi right close smooth bound deriv seem interest linear point lfloor lg rfloor linear interpol lower bound lg point|['Marek A. Suchenek']|['cs.DS', 'cs.CC', 'cs.DM', '68W40 Analysis of algorithms', 'F.2.2; G.2.0; G.2.1; G.2.2']
2017-04-07T11:28:26Z|2017-03-19T21:36:55Z|http://arxiv.org/abs/1702.07938v2|http://arxiv.org/pdf/1702.07938v2|Complexity Classification of the Eight-Vertex Model|complex classif eight vertex model|"We prove a complexity dichotomy theorem for the eight-vertex model. For every setting of the parameters of the model, we prove that computing the partition function is either solvable in polynomial time or \#P-hard. The dichotomy criterion is explicit. For tractability, we find some new classes of problems computable in polynomial time. For \#P-hardness, we employ M\""{o}bius transformations to prove the success of interpolations."|prove complex dichotomi theorem eight vertex model everi set paramet model prove comput partit function either solvabl polynomi time hard dichotomi criterion explicit tractabl find new class problem comput polynomi time hard employ bius transform prove success interpol|['Jin-Yi Cai', 'Zhiguo Fu']|['cs.CC', 'cond-mat.stat-mech']
2017-04-07T11:28:26Z|2017-02-25T15:07:59Z|http://arxiv.org/abs/1702.07902v1|http://arxiv.org/pdf/1702.07902v1|Approval Voting with Intransitive Preferences|approv vote intransit prefer|We extend Approval voting to the settings where voters may have intransitive preferences. The major obstacle to applying Approval voting in these settings is that voters are not able to clearly determine who they should approve or disapprove, due to the intransitivity of their preferences. An approach to address this issue is to apply tournament solutions to help voters make the decision. We study a class of voting systems where first each voter casts a vote defined as a tournament, then a well-defined tournament solution is applied to select the candidates who are assumed to be approved by the voter. Winners are the ones receiving the most approvals. We study axiomatic properties of this class of voting systems and complexity of control and bribery problems for these voting systems.|extend approv vote set voter may intransit prefer major obstacl appli approv vote set voter abl clear determin approv disapprov due intransit prefer approach address issu appli tournament solut help voter make decis studi class vote system first voter cast vote defin tournament well defin tournament solut appli select candid assum approv voter winner one receiv approv studi axiomat properti class vote system complex control briberi problem vote system|['Yongjie Yang']|['cs.GT', 'cs.CC', 'cs.DM']
2017-04-07T11:28:26Z|2017-02-24T17:18:02Z|http://arxiv.org/abs/1702.07669v1|http://arxiv.org/pdf/1702.07669v1|On problems equivalent to (min,+)-convolution|problem equival min convolut|In the recent years, significant progress has been made in explaining apparent hardness of improving over naive solutions for many fundamental polynomially solvable problems. This came in the form of conditional lower bounds - reductions to one of problems assumed to be hard. These include 3SUM, All-Pairs Shortest Paths, SAT and Orthogonal Vectors, and others.   In the (min,+)-convolution problem, the goal is to compute a sequence $(c[i])^{n-1}_{i=0}$, where $c[k] = \min_{i=0,\ldots,k} \{a[i]+b[k-i]\}$, given sequences $(a[i])^{n-1}_{i=0}$ and $(b[i])_{i=0}^{n-1}$. This can easily be done in $O(n^2)$ time, but no $O(n^{2-\varepsilon})$ algorithm is known for $\varepsilon > 0$. In this paper we undertake a systematic study of the (min,+)-convolution problem as a hardness assumption.   As the first step, we establish equivalence of this problem to a group of other problems, including variants of the classic knapsack problem and problems related to subadditive sequences. The (min,+)-convolution has been used as a building block in algorithms for many problems, notably problems in stringology. It has also already appeared as an ad hoc hardness assumption. We investigate some of these connections and provide new reductions and other results.|recent year signific progress made explain appar hard improv naiv solut mani fundament polynomi solvabl problem came form condit lower bound reduct one problem assum hard includ sum pair shortest path sat orthogon vector min convolut problem goal comput sequenc min ldot given sequenc easili done time varepsilon algorithm known varepsilon paper undertak systemat studi min convolut problem hard assumpt first step establish equival problem group problem includ variant classic knapsack problem problem relat subaddit sequenc min convolut use build block algorithm mani problem notabl problem stringolog also alreadi appear ad hoc hard assumpt investig connect provid new reduct result|['Marek Cygan', 'Marcin Mucha', 'Karol Węgrzycki', 'Michał Włodarczyk']|['cs.DS', 'cs.CC', 'F.1.3; F.2']
2017-04-07T11:28:26Z|2017-04-05T20:25:27Z|http://arxiv.org/abs/1702.07339v2|http://arxiv.org/pdf/1702.07339v2|A Converse to Banach's Fixed Point Theorem and its CLS Completeness|convers banach fix point theorem cls complet|Banach's fixed point theorem for contraction maps has been widely used to analyze the convergence of iterative methods in non-convex problems. It is a common experience, however, that iterative maps fail to be globally contracting under the natural metric in their domain, making the applicability of Banach's theorem limited. We explore how generally we can apply Banach's fixed point theorem to establish the convergence of iterative methods when pairing it with carefully designed metrics.   Our first result is a strong converse of Banach's theorem, showing that it is a universal analysis tool for establishing uniqueness of fixed points and for bounding the convergence rate of iterative maps to a unique fixed point. In other words, we show that, whenever an iterative map globally converges to a unique fixed point, there exists a metric under which the iterative map is contracting and which can be used to bound the number of iterations until convergence. We illustrate our approach in the widely used power method, providing a new way of bounding its convergence rate through contraction arguments.   We next consider the computational complexity of Banach's fixed point theorem. Making the proof of our converse theorem constructive, we show that computing a fixed point whose existence is guaranteed by Banach's fixed point theorem is CLS-complete. We thus provide the first natural complete problem for the class CLS, which was defined in [Daskalakis-Papadimitriou 2011] to capture the complexity of problems such as P-matrix LCP, computing KKT-points, and finding mixed Nash equilibria in congestion and network coordination games.|banach fix point theorem contract map wide use analyz converg iter method non convex problem common experi howev iter map fail global contract natur metric domain make applic banach theorem limit explor general appli banach fix point theorem establish converg iter method pair care design metric first result strong convers banach theorem show univers analysi tool establish uniqu fix point bound converg rate iter map uniqu fix point word show whenev iter map global converg uniqu fix point exist metric iter map contract use bound number iter converg illustr approach wide use power method provid new way bound converg rate contract argument next consid comput complex banach fix point theorem make proof convers theorem construct show comput fix point whose exist guarante banach fix point theorem cls complet thus provid first natur complet problem class cls defin daskalaki papadimitriou captur complex problem matrix lcp comput kkt point find mix nash equilibria congest network coordin game|['Constantinos Daskalakis', 'Christos Tzamos', 'Manolis Zampetakis']|['cs.CC', 'cs.LG', 'math.GN', 'stat.ML']
2017-04-07T11:28:26Z|2017-02-23T11:38:36Z|http://arxiv.org/abs/1702.07180v1|http://arxiv.org/pdf/1702.07180v1|Small hitting-sets for tiny arithmetic circuits or: How to turn bad   designs into good|small hit set tini arithmet circuit turn bad design good|We show that if we can design poly($s$)-time hitting-sets for $\Sigma\wedge^a\Sigma\Pi^{O(\log s)}$ circuits of size $s$, where $a=\omega(1)$ is arbitrarily small and the number of variables, or arity $n$, is $O(\log s)$, then we can derandomize blackbox PIT for general circuits in quasipolynomial time. This also establishes that either E$\not\subseteq$\#P/poly or that VP$\ne$VNP. In fact, we show that one only needs a poly($s$)-time hitting-set against individual-degree $a'=\omega(1)$ polynomials that are computable by a size-$s$ arity-$(\log s)$ $\Sigma\Pi\Sigma$ circuit (note: $\Pi$ fanin may be $s$). Alternatively, we claim that, to understand VP one only needs to find hitting-sets, for depth-$3$, that have a small parameterized complexity. Another tiny family of interest is when we restrict the arity $n=\omega(1)$ to be arbitrarily small. We show that if we can design poly($s,\mu(n)$)-time hitting-sets for size-$s$ arity-$n$ $\Sigma\Pi\Sigma\wedge$ circuits (resp.~$\Sigma\wedge^a\Sigma\Pi$), where function $\mu$ is arbitrary, then we can solve PIT for VP in quasipoly-time, and prove the corresponding lower bounds. Our methods are strong enough to prove a surprising {\em arity reduction} for PIT-- to solve the general problem completely it suffices to find a blackbox PIT with time-complexity $sd2^{O(n)}$. We give several examples of ($\log s$)-variate circuits where a new measure (called cone-size) helps in devising poly-time hitting-sets, but the same question for their $s$-variate versions is open till date: For eg., diagonal depth-$3$ circuits, and in general, models that have a {\em small} partial derivative space. We also introduce a new concept, called cone-closed basis isolation, and provide example models where it occurs, or can be achieved by a small shift.|show design poli time hit set sigma wedg sigma pi log circuit size omega arbitrarili small number variabl ariti log derandom blackbox pit general circuit quasipolynomi time also establish either subseteq poli vp ne vnp fact show one onli need poli time hit set individu degre omega polynomi comput size ariti log sigma pi sigma circuit note pi fanin may altern claim understand vp one onli need find hit set depth small parameter complex anoth tini famili interest restrict ariti omega arbitrarili small show design poli mu time hit set size ariti sigma pi sigma wedg circuit resp sigma wedg sigma pi function mu arbitrari solv pit vp quasipoli time prove correspond lower bound method strong enough prove surpris em ariti reduct pit solv general problem complet suffic find blackbox pit time complex sd give sever exampl log variat circuit new measur call cone size help devis poli time hit set question variat version open till date eg diagon depth circuit general model em small partial deriv space also introduc new concept call cone close basi isol provid exampl model occur achiev small shift|['Manindra Agrawal', 'Michael Forbes', 'Sumanta Ghosh', 'Nitin Saxena']|['cs.CC', 'F.1.1; I.1.2; F.1.3']
2017-04-07T11:28:26Z|2017-02-23T08:02:25Z|http://arxiv.org/abs/1702.07128v1|http://arxiv.org/pdf/1702.07128v1|The Facets of the Bases Polytope of a Matroid and Two Consequences|facet base polytop matroid two consequ|Let $M$ to be a matroid defined on a finite set $E$ and $L\subset E$. $L$ is locked in $M$ if $M L$ and $M^* (E\backslash L)$ are 2-connected, and $min\{r(L), r^*(E\backslash L)\} \geq 2$. In this paper, we prove that the nontrivial facets of the bases polytope of $M$ are described by the locked subsets. We deduce that finding the maximum--weight basis of $M$ is a polynomial time problem for matroids with a polynomial number of locked subsets. This class of matroids is closed under 2-sums and contains the class of uniform matroids, the V\'amos matroid and all the excluded minors of 2-sums of uniform matroids. We deduce also a matroid oracle for testing uniformity of matroids after one call of this oracle.|let matroid defin finit set subset lock backslash connect min backslash geq paper prove nontrivi facet base polytop describ lock subset deduc find maximum weight basi polynomi time problem matroid polynomi number lock subset class matroid close sum contain class uniform matroid amo matroid exclud minor sum uniform matroid deduc also matroid oracl test uniform matroid one call oracl|['Brahim Chaourar']|['cs.CC', 'Primary 90C27, Secondary 90C57, 52B40']
2017-04-07T11:28:26Z|2017-02-22T22:43:45Z|http://arxiv.org/abs/1702.07032v1|http://arxiv.org/pdf/1702.07032v1|On the Complexity of Bundle-Pricing and Simple Mechanisms|complex bundl price simpl mechan|We show that the problem of finding an optimal bundle-pricing for a single additive buyer is #P-hard, even when the distributions have support size 2 for each item and the optimal solution is guaranteed to be a simple one: the seller picks a price for the grand bundle and a price for each individual item; the buyer can purchase either the grand bundle at the given price or any bundle of items at their total individual prices. We refer to this simple and natural family of pricing schemes as discounted item-pricings. In addition to the hardness result, we show that when the distributions are i.i.d. with support size 2, a discounted item-pricing can achieve the optimal revenue obtainable by lottery-pricings and it can be found in polynomial time.|show problem find optim bundl price singl addit buyer hard even distribut support size item optim solut guarante simpl one seller pick price grand bundl price individu item buyer purchas either grand bundl given price ani bundl item total individu price refer simpl natur famili price scheme discount item price addit hard result show distribut support size discount item price achiev optim revenu obtain lotteri price found polynomi time|['Xi Chen', 'George Matikas', 'Dimitris Paparas', 'Mihalis Yannakakis']|['cs.GT', 'cs.CC', 'cs.DS']
2017-04-07T11:28:30Z|2017-02-22T20:38:35Z|http://arxiv.org/abs/1702.06997v1|http://arxiv.org/pdf/1702.06997v1|Beyond Talagrand Functions: New Lower Bounds for Testing Monotonicity   and Unateness|beyond talagrand function new lower bound test monoton unat|We prove a lower bound of $\tilde{\Omega}(n^{1/3})$ for the query complexity of any two-sided and adaptive algorithm that tests whether an unknown Boolean function $f:\{0,1\}^n\rightarrow \{0,1\}$ is monotone or far from monotone. This improves the recent bound of $\tilde{\Omega}(n^{1/4})$ for the same problem by Belovs and Blais [BB15]. Our result builds on a new family of random Boolean functions that can be viewed as a two-level extension of Talagrand's random DNFs.   Beyond monotonicity, we also prove a lower bound of $\tilde{\Omega}(\sqrt{n})$ for any two-sided and adaptive algorithm, and a lower bound of $\tilde{\Omega}(n)$ for any one-sided and non-adaptive algorithm for testing unateness, a natural generalization of monotonicity. The latter matches the recent linear upper bounds by Khot and Shinkar [KS15] and by Chakrabarty and Seshadhri [CS16].|prove lower bound tild omega queri complex ani two side adapt algorithm test whether unknown boolean function rightarrow monoton far monoton improv recent bound tild omega problem belov blai bb result build new famili random boolean function view two level extens talagrand random dnfs beyond monoton also prove lower bound tild omega sqrt ani two side adapt algorithm lower bound tild omega ani one side non adapt algorithm test unat natur general monoton latter match recent linear upper bound khot shinkar ks chakrabarti seshadhri cs|['Xi Chen', 'Erik Waingarten', 'Jinyu Xie']|['cs.CC']
2017-04-07T11:28:30Z|2017-02-22T15:29:15Z|http://arxiv.org/abs/1702.06844v1|http://arxiv.org/pdf/1702.06844v1|Parameterized Shifted Combinatorial Optimization|parameter shift combinatori optim|Shifted combinatorial optimization is a new nonlinear optimization framework which is a broad extension of standard combinatorial optimization, involving the choice of several feasible solutions at a time. This framework captures well studied and diverse problems ranging from so-called vulnerability problems to sharing and partitioning problems. In particular, every standard combinatorial optimization problem has its shifted counterpart, which is typically much harder. Already with explicitly given input set the shifted problem may be NP-hard. In this article we initiate a study of the parameterized complexity of this framework. First we show that shifting over an explicitly given set with its cardinality as the parameter may be in XP, FPT or P, depending on the objective function. Second, we study the shifted problem over sets definable in MSO logic (which includes, e.g., the well known MSO partitioning problems). Our main results here are that shifted combinatorial optimization over MSO definable sets is in XP with respect to the MSO formula and the treewidth (or more generally clique-width) of the input graph, and is W[1]-hard even under further severe restrictions.|shift combinatori optim new nonlinear optim framework broad extens standard combinatori optim involv choic sever feasibl solut time framework captur well studi divers problem rang call vulner problem share partit problem particular everi standard combinatori optim problem shift counterpart typic much harder alreadi explicit given input set shift problem may np hard articl initi studi parameter complex framework first show shift explicit given set cardin paramet may xp fpt depend object function second studi shift problem set defin mso logic includ well known mso partit problem main result shift combinatori optim mso defin set xp respect mso formula treewidth general cliqu width input graph hard even sever restrict|['Jakub Gajarský', 'Petr Hliněný', 'Martin Koutecký', 'Shmuel Onn']|['cs.CC']
2017-04-07T11:28:31Z|2017-02-21T23:06:56Z|http://arxiv.org/abs/1702.06616v1|http://arxiv.org/pdf/1702.06616v1|TC^0 circuits for algorithmic problems in nilpotent groups|tc circuit algorithm problem nilpot group|Recently, MacDonald et. al. showed that many algorithmic problems for nilpotent groups including computation of normal forms, the subgroup membership problem, the conjugacy problem, and computation of presentations of subgroups can be done in Logspace. Here we follow their approach and show that all these problems are actually complete for the uniform circuit class TC^0 -- uniformly for all r-generated nilpotent groups of class at most c for fixed r and c.   Moreover, if we allow a certain binary representation of the inputs, then the word problem and computation of normal forms is still in uniform TC^0, while all the other problems we examine are shown to be TC^0-Turing reducible to the problem of computing greatest common divisors and expressing them as a linear combination.|recent macdonald et al show mani algorithm problem nilpot group includ comput normal form subgroup membership problem conjugaci problem comput present subgroup done logspac follow approach show problem actual complet uniform circuit class tc uniform generat nilpot group class fix moreov allow certain binari represent input word problem comput normal form still uniform tc problem examin shown tc ture reduc problem comput greatest common divisor express linear combin|['Alexei Myasnikov', 'Armin Weiß']|['math.GR', 'cs.CC', 'F.2.2; G.2.0']
2017-04-07T11:28:31Z|2017-02-21T18:13:40Z|http://arxiv.org/abs/1702.06503v1|http://arxiv.org/pdf/1702.06503v1|When can Graph Hyperbolicity be computed in Linear Time?|graph hyperbol comput linear time|"Hyperbolicity measures, in terms of (distance) metrics, how close a given graph is to being a tree. Due to its relevance in modeling real-world networks, hyperbolicity has seen intensive research over the last years. Unfortunately, the best known algorithms for computing the hyperbolicity number of a graph (the smaller, the more tree-like) have running time $O(n^4)$, where $n$ is the number of graph vertices. Exploiting the framework of parameterized complexity analysis, we explore possibilities for ""linear-time FPT"" algorithms to compute hyperbolicity. For instance, we show that hyperbolicity can be computed in time $O(2^{O(k)} + n +m)$ ($m$ being the number of graph edges) while at the same time, unless the SETH fails, there is no $2^{o(k)}n^2$-time algorithm."|hyperbol measur term distanc metric close given graph tree due relev model real world network hyperbol seen intens research last year unfortun best known algorithm comput hyperbol number graph smaller tree like run time number graph vertic exploit framework parameter complex analysi explor possibl linear time fpt algorithm comput hyperbol instanc show hyperbol comput time number graph edg time unless seth fail time algorithm|['Till Fluschnik', 'Christian Komusiewicz', 'George B. Mertzios', 'André Nichterlein', 'Rolf Niedermeier', 'Nimrod Talmon']|['cs.CC', 'cs.DS', '05C12, 68R10, 68Q25, 68Q17', 'F.2.2; G.2.2']
2017-04-07T11:28:31Z|2017-02-21T13:06:59Z|http://arxiv.org/abs/1702.06364v1|http://arxiv.org/pdf/1702.06364v1|Linear-Time Tree Containment in Phylogenetic Networks|linear time tree contain phylogenet network|We consider the NP-hard Tree Containment problem that has important applications in phylogenetics. The problem asks if a given leaf-labeled network contains a subdivision of a given leaf-labeled tree. We develop a fast algorithm for the case that the input network is indeed a tree in which multiple leaves might share a label. By combining this algorithm with a generalization of a previously known decomposition scheme, we improve the running time on reticulation visible networks and nearly stable networks to linear time. While these are special classes of networks, they rank among the most general of the previously considered classes.|consid np hard tree contain problem import applic phylogenet problem ask given leaf label network contain subdivis given leaf label tree develop fast algorithm case input network inde tree multipl leav might share label combin algorithm general previous known decomposit scheme improv run time reticul visibl network near stabl network linear time special class network rank among general previous consid class|['Mathias Weller']|['cs.CC', 'cs.DS']
2017-04-07T11:28:31Z|2017-02-23T02:48:22Z|http://arxiv.org/abs/1702.06237v2|http://arxiv.org/pdf/1702.06237v2|Exact tensor completion with sum-of-squares|exact tensor complet sum squar|We obtain the first polynomial-time algorithm for exact tensor completion that improves over the bound implied by reduction to matrix completion. The algorithm recovers an unknown 3-tensor with $r$ incoherent, orthogonal components in $\mathbb R^n$ from $r\cdot \tilde O(n^{1.5})$ randomly observed entries of the tensor. This bound improves over the previous best one of $r\cdot \tilde O(n^{2})$ by reduction to exact matrix completion. Our bound also matches the best known results for the easier problem of approximate tensor completion (Barak & Moitra, 2015).   Our algorithm and analysis extends seminal results for exact matrix completion (Candes & Recht, 2009) to the tensor setting via the sum-of-squares method. The main technical challenge is to show that a small number of randomly chosen monomials are enough to construct a degree-3 polynomial with precisely planted orthogonal global optima over the sphere and that this fact can be certified within the sum-of-squares proof system.|obtain first polynomi time algorithm exact tensor complet improv bound impli reduct matrix complet algorithm recov unknown tensor incoher orthogon compon mathbb cdot tild random observ entri tensor bound improv previous best one cdot tild reduct exact matrix complet bound also match best known result easier problem approxim tensor complet barak moitra algorithm analysi extend semin result exact matrix complet cand recht tensor set via sum squar method main technic challeng show small number random chosen monomi enough construct degre polynomi precis plant orthogon global optima sphere fact certifi within sum squar proof system|['Aaron Potechin', 'David Steurer']|['cs.LG', 'cs.CC', 'cs.DS', 'cs.IT', 'math.IT', 'stat.ML']
2017-04-07T11:28:31Z|2017-02-20T15:39:13Z|http://arxiv.org/abs/1702.06017v1|http://arxiv.org/pdf/1702.06017v1|CLS: New Problems and Completeness|cls new problem complet|The complexity class CLS was introduced by Daskalakis and Papadimitriou with the goal of capturing the complexity of some well-known problems in PPAD$~\cap~$PLS that have resisted, in some cases for decades, attempts to put them in polynomial time. No complete problem was known for CLS, and in previous work, the problems ContractionMap, i.e., the problem of finding an approximate fixpoint of a contraction map, and PLCP, i.e., the problem of solving a P-matrix Linear Complementarity Problem, were identified as prime candidates.   First, we present a new CLS-complete problem MetaMetricContractionMap, which is closely related to the ContractionMap. Second, we introduce EndOfPotentialLine, which captures aspects of PPAD and PLS directly via a monotonic directed path, and show that EndOfPotentialLine is in CLS via a two-way reduction to EndOfMeteredLine. The latter was defined to keep track of how far a vertex is on the PPAD path via a restricted potential function. Third, we reduce PLCP to EndOfPotentialLine, thus making EndOfPotentialLine and EndOfMeteredLine at least as likely to be hard for CLS as PLCP. This last result leverages the monotonic structure of Lemke paths for PLCP problems, making EndOfPotentialLine a likely candidate to capture the exact complexity of PLCP; we note that the structure of Lemke-Howson paths for finding a Nash equilibrium in a two-player game very directly motivated the definition of the complexity class PPAD, which eventually ended up capturing this problem's complexity exactly.|complex class cls introduc daskalaki papadimitriou goal captur complex well known problem ppad cap pls resist case decad attempt put polynomi time complet problem known cls previous work problem contractionmap problem find approxim fixpoint contract map plcp problem solv matrix linear complementar problem identifi prime candid first present new cls complet problem metametriccontractionmap close relat contractionmap second introduc endofpotentiallin captur aspect ppad pls direct via monoton direct path show endofpotentiallin cls via two way reduct endofmeteredlin latter defin keep track far vertex ppad path via restrict potenti function third reduc plcp endofpotentiallin thus make endofpotentiallin endofmeteredlin least like hard cls plcp last result leverag monoton structur lemk path plcp problem make endofpotentiallin like candid captur exact complex plcp note structur lemk howson path find nash equilibrium two player game veri direct motiv definit complex class ppad eventu end captur problem complex exact|['John Fearnley', 'Spencer Gordon', 'Ruta Mehta', 'Rahul Savani']|['cs.CC']
2017-04-07T11:28:31Z|2017-02-20T11:04:29Z|http://arxiv.org/abs/1702.05927v1|http://arxiv.org/pdf/1702.05927v1|How to implement a genuine Parrondo's paradox with quantum walks?|implement genuin parrondo paradox quantum walk|Parrondo's paradox is ubiquitous in games, ratchets and random walks.The apparent paradox, devised by Juan M. R. Parrondo, that two losing games A and B can produce an winning outcome has been adapted in many physical and biological systems to explain their working. However, proposals on demonstrating Parrondo's paradox using quantum walks failed in the asymptotic limits. In this work, we show that instead of a single coin if we consider a two coin initial state which may or may not be entangled, we can observe a genuine Parrondo's paradox with quantum walks. The implications of our results for observing quantum ratchet like behavior using quantum walks is also discussed.|parrondo paradox ubiquit game ratchet random walk appar paradox devis juan parrondo two lose game produc win outcom adapt mani physic biolog system explain work howev propos demonstr parrondo paradox use quantum walk fail asymptot limit work show instead singl coin consid two coin initi state may may entangl observ genuin parrondo paradox quantum walk implic result observ quantum ratchet like behavior use quantum walk also discuss|['Jishnu Rajendran', 'Colin Benjamin']|['quant-ph', 'cond-mat.mes-hall', 'cs.CC']
2017-04-07T11:28:31Z|2017-02-19T15:48:11Z|http://arxiv.org/abs/1702.05760v1|http://arxiv.org/pdf/1702.05760v1|Hypercube LSH for approximate near neighbors|hypercub lsh approxim near neighbor|A celebrated technique for finding near neighbors for the angular distance involves using a set of \textit{random} hyperplanes to partition the space into hash regions [Charikar, STOC 2002]. Experiments later showed that using a set of \textit{orthogonal} hyperplanes, thereby partitioning the space into the Voronoi regions induced by a hypercube, leads to even better results [Terasawa and Tanaka, WADS 2007]. However, no theoretical explanation for this improvement was ever given, and it remained unclear how the resulting hypercube hash method scales in high dimensions.   In this work, we provide explicit asymptotics for the collision probabilities when using hypercubes to partition the space. For instance, two near-orthogonal vectors are expected to collide with probability $(\frac{1}{\pi})^{d + o(d)}$ in dimension $d$, compared to $(\frac{1}{2})^d$ when using random hyperplanes. Vectors at angle $\frac{\pi}{3}$ collide with probability $(\frac{\sqrt{3}}{\pi})^{d + o(d)}$, compared to $(\frac{2}{3})^d$ for random hyperplanes, and near-parallel vectors collide with similar asymptotic probabilities in both cases.   For $c$-approximate nearest neighbor searching, this translates to a decrease in the exponent $\rho$ of locality-sensitive hashing (LSH) methods of a factor up to $\log_2(\pi) \approx 1.652$ compared to hyperplane LSH. For $c = 2$, we obtain $\rho \approx 0.302 + o(1)$ for hypercube LSH, improving upon the $\rho \approx 0.377$ for hyperplane LSH. We further describe how to use hypercube LSH in practice, and we consider an example application in the area of lattice algorithms.|celebr techniqu find near neighbor angular distanc involv use set textit random hyperplan partit space hash region charikar stoc experi later show use set textit orthogon hyperplan therebi partit space voronoi region induc hypercub lead even better result terasawa tanaka wad howev theoret explan improv ever given remain unclear result hypercub hash method scale high dimens work provid explicit asymptot collis probabl use hypercub partit space instanc two near orthogon vector expect collid probabl frac pi dimens compar frac use random hyperplan vector angl frac pi collid probabl frac sqrt pi compar frac random hyperplan near parallel vector collid similar asymptot probabl case approxim nearest neighbor search translat decreas expon rho local sensit hash lsh method factor log pi approx compar hyperplan lsh obtain rho approx hypercub lsh improv upon rho approx hyperplan lsh describ use hypercub lsh practic consid exampl applic area lattic algorithm|['Thijs Laarhoven']|['cs.DS', 'cs.CC', 'cs.CG', 'cs.CR']
2017-04-07T11:28:31Z|2017-02-21T09:07:53Z|http://arxiv.org/abs/1702.05704v2|http://arxiv.org/pdf/1702.05704v2|Computational Complexity of Atomic Chemical Reaction Networks|comput complex atom chemic reaction network|"Informally, a chemical reaction network is ""atomic"" if each reaction may be interpreted as the rearrangement of indivisible units of matter. There are several reasonable definitions formalizing this idea. We investigate the computational complexity of deciding whether a given network is atomic according to each of these definitions.   Our first definition, primitive atomic, which requires each reaction to preserve the total number of atoms, is to shown to be equivalent to mass conservation. Since it is known that it can be decided in polynomial time whether a given chemical reaction network is mass-conserving, the equivalence gives an efficient algorithm to decide primitive atomicity.   Another definition, subset atomic, further requires that all atoms are species. We show that deciding whether a given network is subset atomic is in $\textsf{NP}$, and the problem ""is a network subset atomic with respect to a given atom set"" is strongly $\textsf{NP}$-$\textsf{Complete}$.   A third definition, reachably atomic, studied by Adleman, Gopalkrishnan et al., further requires that each species has a sequence of reactions splitting it into its constituent atoms. We show that there is a polynomial-time algorithm to decide whether a given network is reachably atomic, improving upon the result of Adleman et al. that the problem is decidable. We show that the reachability problem for reachably atomic networks is $\textsf{Pspace}$-$\textsf{Complete}$.   Finally, we demonstrate equivalence relationships between our definitions and some special cases of another existing definition of atomicity due to Gnacadja."|inform chemic reaction network atom reaction may interpret rearrang indivis unit matter sever reason definit formal idea investig comput complex decid whether given network atom accord definit first definit primit atom requir reaction preserv total number atom shown equival mass conserv sinc known decid polynomi time whether given chemic reaction network mass conserv equival give effici algorithm decid primit atom anoth definit subset atom requir atom speci show decid whether given network subset atom textsf np problem network subset atom respect given atom set strong textsf np textsf complet third definit reachabl atom studi adleman gopalkrishnan et al requir speci sequenc reaction split constitu atom show polynomi time algorithm decid whether given network reachabl atom improv upon result adleman et al problem decid show reachabl problem reachabl atom network textsf pspace textsf complet final demonstr equival relationship definit special case anoth exist definit atom due gnacadja|['David Doty', 'Shaopeng Zhu']|['cs.CC', 'F.1.1']
2017-04-07T11:28:35Z|2017-02-18T00:19:02Z|http://arxiv.org/abs/1702.05547v1|http://arxiv.org/pdf/1702.05547v1|Nontrivial Turmites are Turing-universal|nontrivi turmit ture univers|A Turmit is a Turing machine that works over a two-dimensional grid, that is, an agent that moves, reads and writes symbols over the cells of the grid. Its state is an arrow and, depending on the symbol that it reads, it turns to the left or to the right, switching the symbol at the same time. Several symbols are admitted, and the rule is specified by the turning sense that the machine has over each symbol. Turmites are a generalization of Langtons ant, and they present very complex and diverse behaviors. We prove that any Turmite, except for those whose rule does not depend on the symbol, can simulate any Turing Machine. We also prove the P-completeness of prediction their future behavior by explicitly giving a log-space reduction from the Topological Circuit Value Problem. A similar result was already established for Langtons ant; here we use a similar technique but prove a stronger notion of simulation, and for a more general family.|turmit ture machin work two dimension grid agent move read write symbol cell grid state arrow depend symbol read turn left right switch symbol time sever symbol admit rule specifi turn sens machin symbol turmit general langton ant present veri complex divers behavior prove ani turmit except whose rule doe depend symbol simul ani ture machin also prove complet predict futur behavior explicit give log space reduct topolog circuit valu problem similar result alreadi establish langton ant use similar techniqu prove stronger notion simul general famili|['Diego Maldonado', 'Anahí Gajardo', 'Benjamin Hellouin de Menibus', 'Andrés Moreira']|['cs.CC', 'nlin.CG', '68Q17, 68Q05', 'F.1.1; F.1.3']
2017-04-07T11:28:35Z|2017-02-17T23:43:14Z|http://arxiv.org/abs/1702.05543v1|http://arxiv.org/pdf/1702.05543v1|A Fixed-Parameter Perspective on #BIS|fix paramet perspect bis|The complexity of approximately counting independent sets in bipartite graphs (#BIS) is a central open problem in approximate counting, and it is widely believed to be neither easy nor NP-hard. We study several natural parameterised variants of #BIS, both from the polynomial-time and from the fixed-parameter viewpoint: counting independent sets of a given size; counting independent sets with a given number of vertices in one vertex class; and counting maximum independent sets among those with a given number of vertices in one vertex class. Among other things, we show that all these problems are NP-hard to approximate within any polynomial ratio. We also show that the first problem is #W[1]-hard to solve exactly but admits an FPTRAS, and the other two are W[1]-hard to approximate within any polynomial ratio. Finally, we show that when restricted to graphs of bounded degree, all three problems admit exact fixed-parameter algorithms with reasonable time complexity.|complex approxim count independ set bipartit graph bis central open problem approxim count wide believ neither easi np hard studi sever natur parameteris variant bis polynomi time fix paramet viewpoint count independ set given size count independ set given number vertic one vertex class count maximum independ set among given number vertic one vertex class among thing show problem np hard approxim within ani polynomi ratio also show first problem hard solv exact admit fptras two hard approxim within ani polynomi ratio final show restrict graph bound degre three problem admit exact fix paramet algorithm reason time complex|['Radu Curticapean', 'Holger Dell', 'Fedor Fomin', 'Leslie Ann Goldberg', 'John Lapinskas']|['cs.CC', 'F.2.2; G.2.1; G.2.2']
2017-04-07T11:28:35Z|2017-02-17T17:48:41Z|http://arxiv.org/abs/1702.05456v1|http://arxiv.org/pdf/1702.05456v1|LCL problems on grids|lcl problem grid|LCLs or locally checkable labelling problems (e.g. maximal independent set, maximal matching, and vertex colouring) in the LOCAL model of computation are very well-understood in cycles (toroidal 1-dimensional grids): every problem has a complexity of $O(1)$, $\Theta(\log^* n)$, or $\Theta(n)$, and the design of optimal algorithms can be fully automated.   This work develops the complexity theory of LCL problems for toroidal 2-dimensional grids. The complexity classes are the same as in the 1-dimensional case: $O(1)$, $\Theta(\log^* n)$, and $\Theta(n)$. However, given an LCL problem it is undecidable whether its complexity is $\Theta(\log^* n)$ or $\Theta(n)$ in 2-dimensional grids.   Nevertheless, if we correctly guess that the complexity of a problem is $\Theta(\log^* n)$, we can completely automate the design of optimal algorithms. For any problem we can find an algorithm that is of a normal form $A' \circ S_k$, where $A'$ is a finite function, $S_k$ is an algorithm for finding a maximal independent set in $k$th power of the grid, and $k$ is a constant.   With the help of this technique, we study several concrete \lcl{} problems, also in more general settings. For example, for all $d \ge 2$, we prove that:   - $d$-dimensional grids can be $k$-vertex coloured in time $O(\log^* n)$ iff $k \ge 4$,   - $d$-dimensional grids can be $k$-edge coloured in time $O(\log^* n)$ iff $k \ge 2d+1$.   The proof that $3$-colouring of $2$-dimensional grids requires $\Theta(n)$ time introduces a new topological proof technique, which can also be applied to e.g. orientation problems.|lcls local checkabl label problem maxim independ set maxim match vertex colour local model comput veri well understood cycl toroid dimension grid everi problem complex theta log theta design optim algorithm fulli autom work develop complex theori lcl problem toroid dimension grid complex class dimension case theta log theta howev given lcl problem undecid whether complex theta log theta dimension grid nevertheless correct guess complex problem theta log complet autom design optim algorithm ani problem find algorithm normal form circ finit function algorithm find maxim independ set th power grid constant help techniqu studi sever concret lcl problem also general set exampl ge prove dimension grid vertex colour time log iff ge dimension grid edg colour time log iff ge proof colour dimension grid requir theta time introduc new topolog proof techniqu also appli orient problem|['Sebastian Brandt', 'Juho Hirvonen', 'Janne H. Korhonen', 'Tuomo Lempiäinen', 'Patric R. J. Östergård', 'Christopher Purcell', 'Joel Rybicki', 'Jukka Suomela', 'Przemysław Uznański']|['cs.DC', 'cs.CC', 'cs.DS']
2017-04-07T11:28:35Z|2017-02-17T17:20:21Z|http://arxiv.org/abs/1702.05447v1|http://arxiv.org/pdf/1702.05447v1|Counting edge-injective homomorphisms and matchings on restricted graph   classes|count edg inject homomorph match restrict graph class|We consider the parameterized problem of counting all matchings with exactly $k$ edges in a given input graph $G$. This problem is #W[1]-hard (Curticapean, ICALP 2013), so it is unlikely to admit $f(k)\cdot n^{O(1)}$ time algorithms. We show that #W[1]-hardness persists even when the input graph $G$ comes from restricted graph classes, such as line graphs and bipartite graphs of arbitrary constant girth and maximum degree two on one side. To prove the result for line graphs, we observe that $k$-matchings in line graphs can be equivalently viewed as edge-injective homomorphisms from the disjoint union of $k$ paths of length two into (arbitrary) host graphs. Here, a homomorphism from $H$ to $G$ is edge-injective if it maps any two distinct edges of $H$ to distinct edges in $G$. We show that edge-injective homomorphisms from a pattern graph $H$ can be counted in polynomial time if $H$ has bounded vertex-cover number after removing isolated edges. For hereditary classes $\mathcal{H}$ of pattern graphs, we obtain a full complexity dichotomy theorem by proving that counting edge-injective homomorphisms, restricted to patterns from $\mathcal{H}$, is #W[1]-hard if no such bound exists. Our proofs rely on an edge-colored variant of Holant problems and a delicate interpolation argument; both may be of independent interest.|consid parameter problem count match exact edg given input graph problem hard curticapean icalp unlik admit cdot time algorithm show hard persist even input graph come restrict graph class line graph bipartit graph arbitrari constant girth maximum degre two one side prove result line graph observ match line graph equival view edg inject homomorph disjoint union path length two arbitrari host graph homomorph edg inject map ani two distinct edg distinct edg show edg inject homomorph pattern graph count polynomi time bound vertex cover number remov isol edg hereditari class mathcal pattern graph obtain full complex dichotomi theorem prove count edg inject homomorph restrict pattern mathcal hard bound exist proof reli edg color variant holant problem delic interpol argument may independ interest|['Radu Curticapean', 'Holger Dell', 'Marc Roth']|['cs.CC']
2017-04-07T11:28:35Z|2017-02-17T13:07:58Z|http://arxiv.org/abs/1702.05328v1|http://arxiv.org/pdf/1702.05328v1|On algebraic branching programs of small width|algebra branch program small width|In 1979 Valiant showed that the complexity class VP_e of families with polynomially bounded formula size is contained in the class VP_s of families that have algebraic branching programs (ABPs) of polynomially bounded size. Motivated by the problem of separating these classes we study the topological closure VP_e-bar, i.e. the class of polynomials that can be approximated arbitrarily closely by polynomials in VP_e. We describe VP_e-bar with a strikingly simple complete polynomial (in characteristic different from 2) whose recursive definition is similar to the Fibonacci numbers. Further understanding this polynomial seems to be a promising route to new formula lower bounds.   Our methods are rooted in the study of ABPs of small constant width. In 1992 Ben-Or and Cleve showed that formula size is polynomially equivalent to width-3 ABP size. We extend their result (in characteristic different from 2) by showing that approximate formula size is polynomially equivalent to approximate width-2 ABP size. This is surprising because in 2011 Allender and Wang gave explicit polynomials that cannot be computed by width-2 ABPs at all! The details of our construction lead to the aforementioned characterization of VP_e-bar.   As a natural continuation of this work we prove that the class VNP can be described as the class of families that admit a hypercube summation of polynomially bounded dimension over a product of polynomially many affine linear forms. This gives the first separations of algebraic complexity classes from their nondeterministic analogs.|valiant show complex class vp famili polynomi bound formula size contain class vp famili algebra branch program abp polynomi bound size motiv problem separ class studi topolog closur vp bar class polynomi approxim arbitrarili close polynomi vp describ vp bar strike simpl complet polynomi characterist differ whose recurs definit similar fibonacci number understand polynomi seem promis rout new formula lower bound method root studi abp small constant width ben cleve show formula size polynomi equival width abp size extend result characterist differ show approxim formula size polynomi equival approxim width abp size surpris becaus allend wang gave explicit polynomi cannot comput width abp detail construct lead aforement character vp bar natur continu work prove class vnp describ class famili admit hypercub summat polynomi bound dimens product polynomi mani affin linear form give first separ algebra complex class nondeterminist analog|['Karl Bringmann', 'Christian Ikenmeyer', 'Jeroen Zuiddam']|['cs.CC', '68Q15', 'F.1.3']
2017-04-07T11:28:35Z|2017-02-16T23:21:34Z|http://arxiv.org/abs/1702.05183v1|http://arxiv.org/pdf/1702.05183v1|Courcelle's Theorem Made Dynamic|courcell theorem made dynam|Dynamic complexity is concerned with updating the output of a problem when the input is slightly changed. We study the dynamic complexity of model checking a fixed monadic second-order formula over evolving subgraphs of a fixed maximal graph having bounded tree-width; here the subgraph evolves by losing or gaining edges (from the maximal graph). We show that this problem is in DynFO (with LOGSPACE precomputation), via a reduction to a Dyck reachability problem on an acyclic automaton.|dynam complex concern updat output problem input slight chang studi dynam complex model check fix monad second order formula evolv subgraph fix maxim graph bound tree width subgraph evolv lose gain edg maxim graph show problem dynfo logspac precomput via reduct dyck reachabl problem acycl automaton|['Patricia Bouyer-Decitre', 'Vincent Jugé', 'Nicolas Markey']|['cs.CC', 'cs.FL']
2017-04-07T11:28:35Z|2017-02-16T20:02:12Z|http://arxiv.org/abs/1702.05139v1|http://arxiv.org/pdf/1702.05139v1|On the Bit Complexity of Sum-of-Squares Proofs|bit complex sum squar proof|It has often been claimed in recent papers that one can find a degree d Sum-of-Squares proof if one exists via the Ellipsoid algorithm. In [O17], Ryan O'Donnell notes this widely quoted claim is not necessarily true. He presents an example of a polynomial system with bounded coeffcients that admits low-degree proofs of non-negativity, but these proofs necessarily involve numbers with an exponential number of bits, causing the Ellipsoid algorithm to take exponential time. In this paper we obtain both positive and negative results on the bit complexity of SoS proofs. First, we propose a suffcient condition on a polynomial system that implies a bound on the coefficients in an SoS proof. We demonstrate that this sufficient condition is applicable for common use-cases of the SoS algorithm, such as Max-CSP, Balanced Separator, Max- Clique, Max-Bisection, and Unit-Vector constraints. On the negative side, O'Donnell asked whether every polynomial system containing Boolean constraints admits proofs of polynomial bit complexity. We answer this question in the negative, giving a counterexample system and non-negative polynomial which has degree two SoS proofs, but no SoS proof with small coefficients until degree Omega(sqrt(n))|often claim recent paper one find degre sum squar proof one exist via ellipsoid algorithm ryan donnel note wide quot claim necessarili true present exampl polynomi system bound coeffcient admit low degre proof non negat proof necessarili involv number exponenti number bit caus ellipsoid algorithm take exponenti time paper obtain posit negat result bit complex sos proof first propos suffcient condit polynomi system impli bound coeffici sos proof demonstr suffici condit applic common use case sos algorithm max csp balanc separ max cliqu max bisect unit vector constraint negat side donnel ask whether everi polynomi system contain boolean constraint admit proof polynomi bit complex answer question negat give counterexampl system non negat polynomi degre two sos proof sos proof small coeffici degre omega sqrt|['Prasad Raghavendra', 'Benjamin Weitz']|['cs.CC']
2017-04-07T11:28:35Z|2017-02-15T21:21:34Z|http://arxiv.org/abs/1702.04779v1|http://arxiv.org/pdf/1702.04779v1|Compression Complexity|compress complex|The Kolmogorov complexity of x, denoted C(x), is the length of the shortest program that generates x. For such a simple definition, Kolmogorov complexity has a rich and deep theory, as well as applications to a wide variety of topics including learning theory, complexity lower bounds and SAT algorithms.   Kolmogorov complexity typically focuses on decompression, going from the compressed program to the original string. This paper develops a dual notion of compression, the mapping from a string to its compressed version. Typical lossless compression algorithms such as Lempel-Ziv or Huffman Encoding always produce a string that will decompress to the original. We define a general compression concept based on this observation.   For every m, we exhibit a single compression algorithm q of length about m which for n and strings x of length n >= m, the output of q will have length within n-m+O(1) bits of C(x). We also show this bound is tight in a strong way, for every n >= m there is an x of length n with C(x) about m such that no compression program of size slightly less than m can compress x at all.   We also consider a polynomial time-bounded version of compression complexity and show that similar results for this version would rule out cryptographic one-way functions.|kolmogorov complex denot length shortest program generat simpl definit kolmogorov complex rich deep theori well applic wide varieti topic includ learn theori complex lower bound sat algorithm kolmogorov complex typic focus decompress go compress program origin string paper develop dual notion compress map string compress version typic lossless compress algorithm lempel ziv huffman encod alway produc string decompress origin defin general compress concept base observ everi exhibit singl compress algorithm length string length output length within bit also show bound tight strong way everi length compress program size slight less compress also consid polynomi time bound version compress complex show similar result version would rule cryptograph one way function|['Stephen Fenner', 'Lance Fortnow']|['cs.CC']
2017-04-07T11:28:35Z|2017-02-15T19:39:58Z|http://arxiv.org/abs/1702.04748v1|http://arxiv.org/pdf/1702.04748v1|An Improved Dictatorship Test with Perfect Completeness|improv dictatorship test perfect complet|A Boolean function $f:\{0,1\}^n\rightarrow \{0,1\}$ is called a dictator if it depends on exactly one variable i.e $f(x_1, x_2, \ldots, x_n) = x_i$ for some $i\in [n]$. In this work, we study a $k$-query dictatorship test. Dictatorship tests are central in proving many hardness results for constraint satisfaction problems.   The dictatorship test is said to have {\em perfect completeness} if it accepts any dictator function. The {\em soundness} of a test is the maximum probability with which it accepts any function far from a dictator. Our main result is a $k$-query dictatorship test with perfect completeness and soundness $ \frac{2k + 1}{2^k}$, where $k$ is of the form $2^t -1$ for any integer $t > 2$. This improves upon the result of \cite{TY15} which gave a dictatorship test with soundness $ \frac{2k + 3}{2^k}$.|boolean function rightarrow call dictat depend exact one variabl ldot work studi queri dictatorship test dictatorship test central prove mani hard result constraint satisfact problem dictatorship test said em perfect complet accept ani dictat function em sound test maximum probabl accept ani function far dictat main result queri dictatorship test perfect complet sound frac form ani integ improv upon result cite ty gave dictatorship test sound frac|['Amey Bhangale', 'Subhash Khot', 'Devanathan Thiruvenkatachari']|['cs.CC']
2017-04-07T11:28:35Z|2017-02-16T18:09:15Z|http://arxiv.org/abs/1702.04679v2|http://arxiv.org/pdf/1702.04679v2|The complexity of Boolean surjective general-valued CSPs|complex boolean surject general valu csps|Valued constraint satisfaction problems (VCSPs) are discrete optimisation problems with a $\overline{\mathbb{Q}}$-valued objective function given as a sum of fixed-arity functions, where $\overline{\mathbb{Q}}=\mathbb{Q}\cup\{\infty\}$ is the set of extended rationals.   In Boolean surjective VCSPs variables take on labels from $D=\{0,1\}$ and an optimal assignment is required to use both labels from $D$. A classic example is the global min-cut problem in graphs. Building on the work of Uppman, we establish a dichotomy theorem and thus give a complete complexity classification of Boolean surjective VCSPs. The newly discovered tractable case has an interesting structure related to projections of downsets and upsets. Our work generalises the dichotomy for $\{0,\infty\}$-valued constraint languages (corresponding to CSPs) obtained by Creignou and H\'ebrard, and the dichotomy for $\{0,1\}$-valued constraint languages (corresponding to Min-CSPs) obtained by Uppman.|valu constraint satisfact problem vcsps discret optimis problem overlin mathbb valu object function given sum fix ariti function overlin mathbb mathbb cup infti set extend ration boolean surject vcsps variabl take label optim assign requir use label classic exampl global min cut problem graph build work uppman establish dichotomi theorem thus give complet complex classif boolean surject vcsps newli discov tractabl case interest structur relat project downset upset work generalis dichotomi infti valu constraint languag correspond csps obtain creignou ebrard dichotomi valu constraint languag correspond min csps obtain uppman|['Peter Fulla', 'Stanislav Zivny']|['cs.CC', 'cs.DM', 'F.2.0']
