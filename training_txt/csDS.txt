2017-03-28T14:09:26Z|2017-03-27T13:57:31Z|http://arxiv.org/abs/1703.09083v1|http://arxiv.org/pdf/1703.09083v1|The weighted stable matching problem|weight stabl match problem|We study the stable matching problem in non-bipartite graphs with incomplete but strict preference lists, where the edges have weights and the goal is to compute a stable matching of minimum or maximum weight. This problem is known to be NP-hard in general. Our contribution is two fold: a polyhedral characterization and an approximation algorithm. Previously Chen et al. have shown that the stable matching polytope is integral if and only if the subgraph obtained after running phase one of Irving's algorithm is bipartite. We improve upon this result by showing that there are instances where this subgraph might not be bipartite but one can further eliminate some edges and arrive at a bipartite subgraph. Our elimination procedure ensures that the set of stable matchings remains the same, and thus the stable matching polytope of the final subgraph contains the incidence vectors of all stable matchings of our original graph. This allows us to characterize a larger class of instances for which the weighted stable matching problem is polynomial-time solvable. We also show that our edge elimination procedure is best possible, meaning that if the subgraph we arrive at is not bipartite, then there is no bipartite subgraph that has the same set of stable matchings as the original graph. We complement these results with a $2$-approximation algorithm for the minimum weight stable matching problem for instances where each agent has at most two possible partners in any stable matching. This is the first approximation result for any class of instances with general weights.|studi stabl match problem non bipartit graph incomplet strict prefer list edg weight goal comput stabl match minimum maximum weight problem known np hard general contribut two fold polyhedr character approxim algorithm previous chen et al shown stabl match polytop integr onli subgraph obtain run phase one irv algorithm bipartit improv upon result show instanc subgraph might bipartit one elimin edg arriv bipartit subgraph elimin procedur ensur set stabl match remain thus stabl match polytop final subgraph contain incid vector stabl match origin graph allow us character larger class instanc weight stabl match problem polynomi time solvabl also show edg elimin procedur best possibl mean subgraph arriv bipartit bipartit subgraph set stabl match origin graph complement result approxim algorithm minimum weight stabl match problem instanc agent two possibl partner ani stabl match first approxim result ani class instanc general weight|['Linda Farczadi', 'Natália Guričanová']|['cs.GT', 'cs.DS']
2017-03-28T14:09:26Z|2017-03-27T06:50:30Z|http://arxiv.org/abs/1703.08950v1|http://arxiv.org/pdf/1703.08950v1|Gene tree species tree reconciliation with gene conversion|gene tree speci tree reconcili gene convers|Gene tree/species tree reconciliation is a recent decisive progress in phylo-genetic methods, accounting for the possible differences between gene histories and species histories. Reconciliation consists in explaining these differences by gene-scale events such as duplication, loss, transfer, which translates mathematically into a mapping between gene tree nodes and species tree nodes or branches. Gene conversion is a very frequent biological event, which results in the replacement of a gene by a copy of another from the same species and in the same gene tree. Including this event in reconciliations has never been attempted because this changes as well the solutions as the methods to construct reconciliations. Standard algorithms based on dynamic programming become ineffective. We propose here a novel mathematical framework including gene conversion as an evolutionary event in gene tree/species tree reconciliation. We describe a randomized algorithm giving in polynomial running time a reconciliation minimizing the number of duplications, losses and conversions. We show that the space of reconciliations includes an analog of the Last Common Ancestor reconciliation, but is not limited to it. Our algorithm outputs any optimal reconciliation with non null probability. We argue that this study opens a wide research avenue on including gene conversion in reconciliation, which can be important for biology.|gene tree speci tree reconcili recent decis progress phylo genet method account possibl differ gene histori speci histori reconcili consist explain differ gene scale event duplic loss transfer translat mathemat map gene tree node speci tree node branch gene convers veri frequent biolog event result replac gene copi anoth speci gene tree includ event reconcili never attempt becaus chang well solut method construct reconcili standard algorithm base dynam program becom ineffect propos novel mathemat framework includ gene convers evolutionari event gene tree speci tree reconcili describ random algorithm give polynomi run time reconcili minim number duplic loss convers show space reconcili includ analog last common ancestor reconcili limit algorithm output ani optim reconcili non null probabl argu studi open wide research avenu includ gene convers reconcili import biolog|['Damir Hasic', 'Eric Tannier']|['q-bio.QM', 'cs.DS']
2017-03-28T14:09:26Z|2017-03-27T05:48:36Z|http://arxiv.org/abs/1703.08940v1|http://arxiv.org/pdf/1703.08940v1|Tree Edit Distance Cannot be Computed in Strongly Subcubic Time (unless   APSP can)|tree edit distanc cannot comput strong subcub time unless apsp|The edit distance between two rooted ordered trees with $n$ nodes labeled from an alphabet~$\Sigma$ is the minimum cost of transforming one tree into the other by a sequence of elementary operations consisting of deleting and relabeling existing nodes, as well as inserting new nodes. Tree edit distance is a well known generalization of string edit distance. The fastest known algorithm for tree edit distance runs in cubic $O(n^3)$ time and is based on a similar dynamic programming solution as string edit distance. In this paper we show that a truly subcubic $O(n^{3-\varepsilon})$ time algorithm for tree edit distance is unlikely: For $ \Sigma  = \Omega(n)$, a truly subcubic algorithm for tree edit distance implies a truly subcubic algorithm for the all pairs shortest paths problem. For $ \Sigma  = O(1)$, a truly subcubic algorithm for tree edit distance implies an $O(n^{k-\varepsilon})$ algorithm for finding a maximum weight $k$-clique.   Thus, while in terms of upper bounds string edit distance and tree edit distance are highly related, in terms of lower bounds string edit distance exhibits the hardness of the strong exponential time hypothesis [Backurs, Indyk STOC'15] whereas tree edit distance exhibits the hardness of all pairs shortest paths. Our result provides a matching conditional lower bound for one of the last remaining classic dynamic programming problems.|edit distanc two root order tree node label alphabet sigma minimum cost transform one tree sequenc elementari oper consist delet relabel exist node well insert new node tree edit distanc well known general string edit distanc fastest known algorithm tree edit distanc run cubic time base similar dynam program solut string edit distanc paper show truli subcub varepsilon time algorithm tree edit distanc unlik sigma omega truli subcub algorithm tree edit distanc impli truli subcub algorithm pair shortest path problem sigma truli subcub algorithm tree edit distanc impli varepsilon algorithm find maximum weight cliqu thus term upper bound string edit distanc tree edit distanc high relat term lower bound string edit distanc exhibit hard strong exponenti time hypothesi backur indyk stoc wherea tree edit distanc exhibit hard pair shortest path result provid match condit lower bound one last remain classic dynam program problem|['Karl Bringmann', 'Paweł Gawrychowski', 'Shay Mozes', 'Oren Weimann']|['cs.DS']
2017-03-28T14:09:26Z|2017-03-27T05:09:06Z|http://arxiv.org/abs/1703.08931v1|http://arxiv.org/pdf/1703.08931v1|Palindromic Decompositions with Gaps and Errors|palindrom decomposit gap error|Identifying palindromes in sequences has been an interesting line of research in combinatorics on words and also in computational biology, after the discovery of the relation of palindromes in the DNA sequence with the HIV virus. Efficient algorithms for the factorization of sequences into palindromes and maximal palindromes have been devised in recent years. We extend these studies by allowing gaps in decompositions and errors in palindromes, and also imposing a lower bound to the length of acceptable palindromes.   We first present an algorithm for obtaining a palindromic decomposition of a string of length n with the minimal total gap length in time O(n log n * g) and space O(n g), where g is the number of allowed gaps in the decomposition. We then consider a decomposition of the string in maximal \delta-palindromes (i.e. palindromes with \delta errors under the edit or Hamming distance) and g allowed gaps. We present an algorithm to obtain such a decomposition with the minimal total gap length in time O(n (g + \delta)) and space O(n g).|identifi palindrom sequenc interest line research combinator word also comput biolog discoveri relat palindrom dna sequenc hiv virus effici algorithm factor sequenc palindrom maxim palindrom devis recent year extend studi allow gap decomposit error palindrom also impos lower bound length accept palindrom first present algorithm obtain palindrom decomposit string length minim total gap length time log space number allow gap decomposit consid decomposit string maxim delta palindrom palindrom delta error edit ham distanc allow gap present algorithm obtain decomposit minim total gap length time delta space|['Michał Adamczyk', 'Mai Alzamel', 'Panagiotis Charalampopoulos', 'Costas S. Iliopoulos', 'Jakub Radoszewski']|['cs.DS']
2017-03-28T14:09:26Z|2017-03-26T09:03:07Z|http://arxiv.org/abs/1703.08790v1|http://arxiv.org/pdf/1703.08790v1|Steiner Point Removal --- Distant Terminals Don't (Really) Bother|steiner point remov distant termin realli bother|Given a weighted graph $G=(V,E,w)$ with a set of $k$ terminals $T\subset V$, the Steiner Point Removal problem seeks for a minor of the graph with vertex set $T$, such that the distance between every pair of terminals is preserved within a small multiplicative distortion. Kamma, Krauthgamer and Nguyen (SODA 2014, SICOMP 2015) used a ball-growing algorithm to show that the distortion is at most $\mathcal{O}(\log^5 k)$ for general graphs.   In this paper, we improve the distortion bound to $\mathcal{O}(\log^2 k)$. The improvement is achieved based on a known algorithm that constructs terminal-distance exact-preservation minor with $\mathcal{O}(k^4)$ (which is independent of $ V $) vertices, and also two tail bounds on the sum of independent exponential random variables, which allow us to show that it is unlikely for a non-terminal being contracted to a distant terminal.|given weight graph set termin subset steiner point remov problem seek minor graph vertex set distanc everi pair termin preserv within small multipl distort kamma krauthgam nguyen soda sicomp use ball grow algorithm show distort mathcal log general graph paper improv distort bound mathcal log improv achiev base known algorithm construct termin distanc exact preserv minor mathcal independ vertic also two tail bound sum independ exponenti random variabl allow us show unlik non termin contract distant termin|['Yun Kuen Cheung']|['cs.DS', 'cs.DM', 'math.CO', 'math.PR']
2017-03-28T14:09:26Z|2017-03-25T15:03:49Z|http://arxiv.org/abs/1703.08702v1|http://arxiv.org/pdf/1703.08702v1|Randomized Load Balancing on Networks with Stochastic Inputs|random load balanc network stochast input|Iterative load balancing algorithms for indivisible tokens have been studied intensively in the past. Complementing previous worst-case analyses, we study an average-case scenario where the load inputs are drawn from a fixed probability distribution. For cycles, tori, hypercubes and expanders, we obtain almost matching upper and lower bounds on the discrepancy, the difference between the maximum and the minimum load. Our bounds hold for a variety of probability distributions including the uniform and binomial distribution but also distributions with unbounded range such as the Poisson and geometric distribution. For graphs with slow convergence like cycles and tori, our results demonstrate a substantial difference between the convergence in the worst- and average-case. An important ingredient in our analysis is new upper bound on the t-step transition probability of a general Markov chain, which is derived by invoking the evolving set process.|iter load balanc algorithm indivis token studi intens past complement previous worst case analys studi averag case scenario load input drawn fix probabl distribut cycl tori hypercub expand obtain almost match upper lower bound discrep differ maximum minimum load bound hold varieti probabl distribut includ uniform binomi distribut also distribut unbound rang poisson geometr distribut graph slow converg like cycl tori result demonstr substanti differ converg worst averag case import ingredi analysi new upper bound step transit probabl general markov chain deriv invok evolv set process|['Leran Cai', 'Thomas Sauerwald']|['cs.DC', 'cs.DS', 'G.3']
2017-03-28T14:09:26Z|2017-03-25T07:27:32Z|http://arxiv.org/abs/1703.08658v1|http://arxiv.org/pdf/1703.08658v1|Maximizing the area of intersection of rectangles|maxim area intersect rectangl|This paper attacks the following problem. We are given a large number $N$ of rectangles in the plane, each with horizontal and vertical sides, and also a number $r<N$. The given list of $N$ rectangles may contain duplicates. The problem is to find $r$ of these rectangles, such that, if they are discarded, then the intersection of the remaining $(N-r)$ rectangles has an intersection with as large an area as possible. We will find an upper bound, depending only on $N$ and $r$, and not on the particular data presented, for the number of steps needed to run the algorithm on (a mathematical model of) a computer. In fact our algorithm is able to determine, for each $s\le r$, $s$ rectangles from the given list of $N$ rectangles, such that the remaining $(N-s)$ rectangles have as large an area as possible, and this takes hardly any more time than taking care only of the case $s=r$. Our algorithm extends to $d$-dimensional rectangles. Our method is to exhaustively examine all possible intersections---this is much faster than it sounds, because we do not need to examine all $\binom Ns$ subsets in order to find all possible intersection rectangles. For an extreme example, suppose the rectangles are nested, for example concentric squares of distinct sizes, then the only intersections examined are the smallest $s+1$ rectangles.|paper attack follow problem given larg number rectangl plane horizont vertic side also number given list rectangl may contain duplic problem find rectangl discard intersect remain rectangl intersect larg area possibl find upper bound depend onli particular data present number step need run algorithm mathemat model comput fact algorithm abl determin le rectangl given list rectangl remain rectangl larg area possibl take hard ani time take care onli case algorithm extend dimension rectangl method exhaust examin possibl intersect much faster sound becaus need examin binom ns subset order find possibl intersect rectangl extrem exampl suppos rectangl nest exampl concentr squar distinct size onli intersect examin smallest rectangl|['David B. A. Epstein', 'Mike Paterson']|['cs.DS', 'F.2.2']
2017-03-28T14:09:26Z|2017-03-24T20:21:52Z|http://arxiv.org/abs/1703.08589v1|http://arxiv.org/pdf/1703.08589v1|Polynomial-Time Methods to Solve Unimodular Quadratic Programs With   Performance Guarantees|polynomi time method solv unimodular quadrat program perform guarante|We develop polynomial-time heuristic methods to solve unimodular quadratic programs (UQPs) approximately, which are known to be NP-hard. In the UQP framework, we maximize a quadratic function of a vector of complex variables with unit modulus. Several problems in active sensing and wireless communication applications boil down to UQP. With this motivation, we present three new heuristic methods with polynomial-time complexity to solve the UQP approximately. The first method is called dominant-eigenvector-matching; here the solution is picked that matches the complex arguments of the dominant eigenvector of the Hermitian matrix in the UQP formulation. We also provide a performance guarantee for this method. The second method, a greedy strategy, is shown to provide a performance guarantee of (1-1/e) with respect to the optimal objective value given that the objective function possesses a property called string submodularity. The third heuristic method is called row-swap greedy strategy, which is an extension to the greedy strategy and utilizes certain properties of the UQP to provide a better performance than the greedy strategy at the expense of an increase in computational complexity. We present numerical results to demonstrate the performance of these heuristic methods, and also compare the performance of these methods against a standard heuristic method called semidefinite relaxation.|develop polynomi time heurist method solv unimodular quadrat program uqp approxim known np hard uqp framework maxim quadrat function vector complex variabl unit modulus sever problem activ sens wireless communic applic boil uqp motiv present three new heurist method polynomi time complex solv uqp approxim first method call domin eigenvector match solut pick match complex argument domin eigenvector hermitian matrix uqp formul also provid perform guarante method second method greedi strategi shown provid perform guarante respect optim object valu given object function possess properti call string submodular third heurist method call row swap greedi strategi extens greedi strategi util certain properti uqp provid better perform greedi strategi expens increas comput complex present numer result demonstr perform heurist method also compar perform method standard heurist method call semidefinit relax|['Shankarachary Ragi', 'Edwin K. P. Chong', 'Hans D. Mittelmann']|['math.OC', 'cs.DS']
2017-03-28T14:09:26Z|2017-03-24T17:10:23Z|http://arxiv.org/abs/1703.08511v1|http://arxiv.org/pdf/1703.08511v1|ALLSAT compressed with wildcards. Part 2: All k-models of a BDD|allsat compress wildcard part model bdd|If f is a Boolean function given by a BDD then it is well known how to calculate the number of models (i.e. bitstrings x with f(x)=1). Let  x  be the number of 1's in x. How to calculate the number of k-models x (i.e. having  x =k) is lesser known; we review a nice method due to Knuth. The main topic however is enumeration (=generation) as opposed to counting. Again, that ALL models can be enumerated in polynomial total time, is well known. Apparently new is the fact that also all k-models (for any fixed k) can be enumerated in polynomial total time. Using suitable wildcards this can be achieved in a compressed format.|boolean function given bdd well known calcul number model bitstr let number calcul number model lesser known review nice method due knuth main topic howev enumer generat oppos count model enumer polynomi total time well known appar new fact also model ani fix enumer polynomi total time use suitabl wildcard achiev compress format|['Marcel Wild']|['cs.DS']
2017-03-28T14:09:26Z|2017-03-24T14:44:04Z|http://arxiv.org/abs/1703.08433v1|http://arxiv.org/pdf/1703.08433v1|Metric random matchings with applications|metric random match applic|Let $(\{1,2,\ldots,n\},d)$ be a metric space. We analyze the expected value and the variance of $\sum_{i=1}^{\lfloor n/2\rfloor}\,d({\boldsymbol{\pi}}(2i-1),{\boldsymbol{\pi}}(2i))$ for a uniformly random permutation ${\boldsymbol{\pi}}$ of $\{1,2,\ldots,n\}$, leading to the following results: (I) Consider the problem of finding a point in $\{1,2,\ldots,n\}$ with the minimum sum of distances to all points. We show that this problem has a randomized algorithm that (1) always outputs a $(2+\epsilon)$-approximate solution in expected $O(n/\epsilon^2)$ time and that (2) inherits Indyk's~\cite{Ind99, Ind00} algorithm to output a $(1+\epsilon)$-approximate solution in $O(n/\epsilon^2)$ time with probability $\Omega(1)$, where $\epsilon\in(0,1)$. (II) The average distance in $(\{1,2,\ldots,n\},d)$ can be approximated in $O(n/\epsilon)$ time to within a multiplicative factor in $[\,1/2-\epsilon,1\,]$ with probability $1/2+\Omega(1)$, where $\epsilon>0$. (III) Assume $d$ to be a graph metric. Then the average distance in $(\{1,2,\ldots,n\},d)$ can be approximated in $O(n)$ time to within a multiplicative factor in $[\,1-\epsilon,1+\epsilon\,]$ with probability $1/2+\Omega(1)$, where $\epsilon=\omega(1/n^{1/4})$.|let ldot metric space analyz expect valu varianc sum lfloor rfloor boldsymbol pi boldsymbol pi uniform random permut boldsymbol pi ldot lead follow result consid problem find point ldot minimum sum distanc point show problem random algorithm alway output epsilon approxim solut expect epsilon time inherit indyk cite ind ind algorithm output epsilon approxim solut epsilon time probabl omega epsilon ii averag distanc ldot approxim epsilon time within multipl factor epsilon probabl omega epsilon iii assum graph metric averag distanc ldot approxim time within multipl factor epsilon epsilon probabl omega epsilon omega|['Ching-Lueh Chang']|['cs.DS']
2017-03-28T14:09:30Z|2017-03-24T02:59:51Z|http://arxiv.org/abs/1703.08273v1|http://arxiv.org/pdf/1703.08273v1|An Asymptotically Tighter Bound on Sampling for Frequent Itemsets Mining|asymptot tighter bound sampl frequent itemset mine|In this paper we present a new error bound on sampling algorithms for frequent itemsets mining. We show that the new bound is asymptotically tighter than the state-of-art bounds, i.e., given the chosen samples, for small enough error probability, the new error bound is roughly half of the existing bounds. Based on the new bound, we give a new approximation algorithm, which is much simpler compared to the existing approximation algorithms, but can also guarantee the worst approximation error with precomputed sample size. We also give an algorithm which can approximate the top-$k$ frequent itemsets with high accuracy and efficiency.|paper present new error bound sampl algorithm frequent itemset mine show new bound asymptot tighter state art bound given chosen sampl small enough error probabl new error bound rough half exist bound base new bound give new approxim algorithm much simpler compar exist approxim algorithm also guarante worst approxim error precomput sampl size also give algorithm approxim top frequent itemset high accuraci effici|['Shiyu Ji', 'Kun Wan']|['cs.DS', 'cs.DB']
2017-03-28T14:09:30Z|2017-03-23T16:50:03Z|http://arxiv.org/abs/1703.08139v1|http://arxiv.org/pdf/1703.08139v1|Optimal lower bounds for universal relation, samplers, and finding   duplicates|optim lower bound univers relat sampler find duplic|In the communication problem $\mathbf{UR}$ (universal relation) [KRW95], Alice and Bob respectively receive $x$ and $y$ in $\{0,1\}^n$ with the promise that $x\neq y$. The last player to receive a message must output an index $i$ such that $x_i\neq y_i$. We prove that the randomized one-way communication complexity of this problem in the public coin model is exactly $\Theta(\min\{n, \log(1/\delta)\log^2(\frac{n}{\log(1/\delta)})\})$ bits for failure probability $\delta$. Our lower bound holds even if promised $\mathop{support}(y)\subset \mathop{support}(x)$. As a corollary, we obtain optimal lower bounds for $\ell_p$-sampling in strict turnstile streams for $0\le p < 2$, as well as for the problem of finding duplicates in a stream. Our lower bounds do not need to use large weights, and hold even if it is promised that $x\in\{0,1\}^n$ at all points in the stream.   Our lower bound demonstrates that any algorithm $\mathcal{A}$ solving sampling problems in turnstile streams in low memory can be used to encode subsets of $[n]$ of certain sizes into a number of bits below the information theoretic minimum. Our encoder makes adaptive queries to $\mathcal{A}$ throughout its execution, but done carefully so as to not violate correctness. This is accomplished by injecting random noise into the encoder's interactions with $\mathcal{A}$, which is loosely motivated by techniques in differential privacy. Our correctness analysis involves understanding the ability of $\mathcal{A}$ to correctly answer adaptive queries which have positive but bounded mutual information with $\mathcal{A}$'s internal randomness, and may be of independent interest in the newly emerging area of adaptive data analysis with a theoretical computer science lens.|communic problem mathbf ur univers relat krw alic bob respect receiv promis neq last player receiv messag must output index neq prove random one way communic complex problem public coin model exact theta min log delta log frac log delta bit failur probabl delta lower bound hold even promis mathop support subset mathop support corollari obtain optim lower bound ell sampl strict turnstil stream le well problem find duplic stream lower bound need use larg weight hold even promis point stream lower bound demonstr ani algorithm mathcal solv sampl problem turnstil stream low memori use encod subset certain size number bit inform theoret minimum encod make adapt queri mathcal throughout execut done care violat correct accomplish inject random nois encod interact mathcal loos motiv techniqu differenti privaci correct analysi involv understand abil mathcal correct answer adapt queri posit bound mutual inform mathcal intern random may independ interest newli emerg area adapt data analysi theoret comput scienc len|['Jelani Nelson', 'Jakub Pachocki', 'Zhengyu Wang']|['cs.CC', 'cs.DS']
2017-03-28T14:09:30Z|2017-03-23T12:32:10Z|http://arxiv.org/abs/1703.08041v1|http://arxiv.org/pdf/1703.08041v1|Resolving the Complexity of Some Fundamental Problems in Computational   Social Choice|resolv complex fundament problem comput social choic|This thesis is in the area called computational social choice which is an intersection area of algorithms and social choice theory.|thesi area call comput social choic intersect area algorithm social choic theori|['Palash Dey']|['cs.DS', 'cs.AI', 'cs.MA']
2017-03-28T14:09:30Z|2017-03-23T08:37:54Z|http://arxiv.org/abs/1703.07964v1|http://arxiv.org/abs/1703.07964v1|Minimum Cuts and Shortest Cycles in Directed Planar Graphs via   Noncrossing Shortest Paths|minimum cut shortest cycl direct planar graph via noncross shortest path|Let $G$ be an $n$-node simple directed planar graph with nonnegative edge weights. We study the fundamental problems of computing (1) a global cut of $G$ with minimum weight and (2) a~cycle of $G$ with minimum weight. The best previously known algorithm for the former problem, running in $O(n\log^3 n)$ time, can be obtained from the algorithm of \Lacki, Nussbaum, Sankowski, and Wulff-Nilsen for single-source all-sinks maximum flows. The best previously known result for the latter problem is the $O(n\log^3 n)$-time algorithm of Wulff-Nilsen. By exploiting duality between the two problems in planar graphs, we solve both problems in $O(n\log n\log\log n)$ time via a divide-and-conquer algorithm that finds a shortest non-degenerate cycle. The kernel of our result is an $O(n\log\log n)$-time algorithm for computing noncrossing shortest paths among nodes well ordered on a common face of a directed plane graph, which is extended from the algorithm of Italiano, Nussbaum, Sankowski, and Wulff-Nilsen for an undirected plane graph.|let node simpl direct planar graph nonneg edg weight studi fundament problem comput global cut minimum weight cycl minimum weight best previous known algorithm former problem run log time obtain algorithm lacki nussbaum sankowski wulff nilsen singl sourc sink maximum flow best previous known result latter problem log time algorithm wulff nilsen exploit dualiti two problem planar graph solv problem log log log time via divid conquer algorithm find shortest non degener cycl kernel result log log time algorithm comput noncross shortest path among node well order common face direct plane graph extend algorithm italiano nussbaum sankowski wulff nilsen undirect plane graph|['Hung-Chun Liang', 'Hsueh-I Lu']|['cs.DS', '05C38, 05C10, 05C85, 68P05']
2017-03-28T14:09:30Z|2017-03-22T21:52:05Z|http://arxiv.org/abs/1703.07867v1|http://arxiv.org/pdf/1703.07867v1|Distance-sensitive hashing|distanc sensit hash|"We initiate the study of distance-sensitive hashing, a generalization of locality-sensitive hashing that seeks a family of hash functions such that the probability of two points having the same hash value is a given function of the distance between them. More precisely, given a distance space $(X, \text{dist})$ and a ""collision probability function"" (CPF) $f\colon \mathbb{R}\rightarrow [0,1]$ we seek a distribution over pairs of functions $(h,g)$ such that for every pair of points $x, y \in X$ the collision probability is $\Pr[h(x)=g(y)] = f(\text{dist}(x,y))$. Locality-sensitive hashing is the study of how fast a CPF can decrease as the distance grows. For many spaces $f$ can be made exponentially decreasing even if we restrict attention to the symmetric case where $g=h$. In this paper we study how asymmetry makes it possible to achieve CPFs that are, for example, increasing or unimodal. Our original motivation comes from annulus queries where we are interested in searching for points at distance approximately $r$ from a query point, but we believe that distance-sensitive hashing is of interest beyond this application."|initi studi distanc sensit hash general local sensit hash seek famili hash function probabl two point hash valu given function distanc precis given distanc space text dist collis probabl function cpf colon mathbb rightarrow seek distribut pair function everi pair point collis probabl pr text dist local sensit hash studi fast cpf decreas distanc grow mani space made exponenti decreas even restrict attent symmetr case paper studi asymmetri make possibl achiev cpfs exampl increas unimod origin motiv come annulus queri interest search point distanc approxim queri point believ distanc sensit hash interest beyond applic|['Martin Aumüller', 'Tobias Christiani', 'Rasmus Pagh', 'Francesco Silvestri']|['cs.DS', 'H.3.3']
2017-03-28T14:09:30Z|2017-03-22T16:28:17Z|http://arxiv.org/abs/1703.07734v1|http://arxiv.org/pdf/1703.07734v1|On the Probe Complexity of Local Computation Algorithms|probe complex local comput algorithm|"The Local Computation Algorithms (LCA) model is a computational model aimed at problem instances with huge inputs and output. For graph problems, the input graph is accessed using probes: strong probes (SP) specify a vertex $v$ and receive as a reply a list of $v$'s neighbors, and weak probes (WP) specify a vertex $v$ and a port number $i$ and receive as a reply $v$'s $i^{th}$ neighbor. Given a local query (e.g., ""is a certain vertex in the vertex cover of the input graph?""), an LCA should compute the corresponding local output (e.g., ""yes"" or ""no"") while making only a small number of probes, with the requirement that all local outputs form a single global solution (e.g., a legal vertex cover). We study the probe complexity of LCAs that are required to work on graphs that may have arbitrarily large degrees. In particular, such LCAs are expected to probe the graph a number of times that is significantly smaller than the maximum, average, or even minimum degree.   For weak probes, we focus on the weak coloring problem. Among our results we show a separation between weak 3-coloring and weak 2-coloring for deterministic LCAs: $\log^* n + O(1)$ weak probes suffice for weak 3-coloring, but $\Omega\left(\frac{\log n}{\log\log n}\right)$ weak probes are required for weak 2-coloring.   For strong probes, we consider randomized LCAs for vertex cover and maximal/maximum matching. Our negative results include showing that there are graphs for which finding a \emph{maximal} matching requires $\Omega(\sqrt{n})$ strong probes. On the positive side, we design a randomized LCA that finds a $(1-\epsilon)$ approximation to \emph{maximum} matching in regular graphs, and uses $\frac{1}{\epsilon }^{O\left( \frac{1}{\epsilon ^2}\right)}$ probes, independently of the number of vertices and of their degrees."|local comput algorithm lca model comput model aim problem instanc huge input output graph problem input graph access use probe strong probe sp specifi vertex receiv repli list neighbor weak probe wp specifi vertex port number receiv repli th neighbor given local queri certain vertex vertex cover input graph lca comput correspond local output yes make onli small number probe requir local output form singl global solut legal vertex cover studi probe complex lcas requir work graph may arbitrarili larg degre particular lcas expect probe graph number time signific smaller maximum averag even minimum degre weak probe focus weak color problem among result show separ weak color weak color determinist lcas log weak probe suffic weak color omega left frac log log log right weak probe requir weak color strong probe consid random lcas vertex cover maxim maximum match negat result includ show graph find emph maxim match requir omega sqrt strong probe posit side design random lca find epsilon approxim emph maximum match regular graph use frac epsilon left frac epsilon right probe independ number vertic degre|['Uriel Feige', 'Boaz Patt-Shamir', 'Shai Vardi']|['cs.DS']
2017-03-28T14:09:30Z|2017-03-22T12:50:15Z|http://arxiv.org/abs/1703.07625v1|http://arxiv.org/pdf/1703.07625v1|Clustering for Different Scales of Measurement - the Gap-Ratio Weighted   K-means Algorithm|cluster differ scale measur gap ratio weight mean algorithm|This paper describes a method for clustering data that are spread out over large regions and which dimensions are on different scales of measurement. Such an algorithm was developed to implement a robotics application consisting in sorting and storing objects in an unsupervised way. The toy dataset used to validate such application consists of Lego bricks of different shapes and colors. The uncontrolled lighting conditions together with the use of RGB color features, respectively involve data with a large spread and different levels of measurement between data dimensions. To overcome the combination of these two characteristics in the data, we have developed a new weighted K-means algorithm, called gap-ratio K-means, which consists in weighting each dimension of the feature space before running the K-means algorithm. The weight associated with a feature is proportional to the ratio of the biggest gap between two consecutive data points, and the average of all the other gaps. This method is compared with two other variants of K-means on the Lego bricks clustering problem as well as two other common classification datasets.|paper describ method cluster data spread larg region dimens differ scale measur algorithm develop implement robot applic consist sort store object unsupervis way toy dataset use valid applic consist lego brick differ shape color uncontrol light condit togeth use rgb color featur respect involv data larg spread differ level measur data dimens overcom combin two characterist data develop new weight mean algorithm call gap ratio mean consist weight dimens featur space befor run mean algorithm weight associ featur proport ratio biggest gap two consecut data point averag gap method compar two variant mean lego brick cluster problem well two common classif dataset|['Joris Guérin', 'Olivier Gibaru', 'Stéphane Thiery', 'Eric Nyiri']|['cs.LG', 'cs.DS', 'stat.ML']
2017-03-28T14:09:30Z|2017-03-21T21:05:27Z|http://arxiv.org/abs/1703.07432v1|http://arxiv.org/pdf/1703.07432v1|Efficient PAC Learning from the Crowd|effici pac learn crowd|In recent years crowdsourcing has become the method of choice for gathering labeled training data for learning algorithms. Standard approaches to crowdsourcing view the process of acquiring labeled data separately from the process of learning a classifier from the gathered data. This can give rise to computational and statistical challenges. For example, in most cases there are no known computationally efficient learning algorithms that are robust to the high level of noise that exists in crowdsourced data, and efforts to eliminate noise through voting often require a large number of queries per example.   In this paper, we show how by interleaving the process of labeling and learning, we can attain computational efficiency with much less overhead in the labeling cost. In particular, we consider the realizable setting where there exists a true target function in $\mathcal{F}$ and consider a pool of labelers. When a noticeable fraction of the labelers are perfect, and the rest behave arbitrarily, we show that any $\mathcal{F}$ that can be efficiently learned in the traditional realizable PAC model can be learned in a computationally efficient manner by querying the crowd, despite high amounts of noise in the responses. Moreover, we show that this can be done while each labeler only labels a constant number of examples and the number of labels requested per example, on average, is a constant. When no perfect labelers exist, a related task is to find a set of the labelers which are good but not perfect. We show that we can identify all good labelers, when at least the majority of labelers are good.|recent year crowdsourc becom method choic gather label train data learn algorithm standard approach crowdsourc view process acquir label data separ process learn classifi gather data give rise comput statist challeng exampl case known comput effici learn algorithm robust high level nois exist crowdsourc data effort elimin nois vote often requir larg number queri per exampl paper show interleav process label learn attain comput effici much less overhead label cost particular consid realiz set exist true target function mathcal consid pool label notic fraction label perfect rest behav arbitrarili show ani mathcal effici learn tradit realiz pac model learn comput effici manner queri crowd despit high amount nois respons moreov show done label onli label constant number exampl number label request per exampl averag constant perfect label exist relat task find set label good perfect show identifi good label least major label good|['Pranjal Awasthi', 'Avrim Blum', 'Nika Haghtalab', 'Yishay Mansour']|['cs.LG', 'cs.DS']
2017-03-28T14:09:30Z|2017-03-21T20:28:52Z|http://arxiv.org/abs/1703.07417v1|http://arxiv.org/pdf/1703.07417v1|Approximating k-spanners in the LOCAL model|approxim spanner local model|Graph spanners have been studied extensively, and have many applications in algorithms, distributed systems, and computer networks. For many of these application, we want distributed constructions of spanners, i.e., algorithms which use only local information. Dinitz and Krauthgamer (PODC 2011) provided a distributed approximation algorithm for 2-spanners in the LOCAL model with polylogarithmic running time, but the question of whether a similar algorithm exists for k-spanners with k > 2 remained open. In this paper, we show that a similar algorithm also works for cases where k > 2.|graph spanner studi extens mani applic algorithm distribut system comput network mani applic want distribut construct spanner algorithm use onli local inform dinitz krauthgam podc provid distribut approxim algorithm spanner local model polylogarithm run time question whether similar algorithm exist spanner remain open paper show similar algorithm also work case|['Michael Dinitz', 'Yasamin Nazari']|['cs.DS', 'cs.DC', 'math.CO']
2017-03-28T14:09:30Z|2017-03-21T17:53:50Z|http://arxiv.org/abs/1703.07340v1|http://arxiv.org/pdf/1703.07340v1|Construction of Directed 2K Graphs|construct direct graph|We study the problem of constructing synthetic graphs that resemble real-world directed graphs in terms of their degree correlations. We define the problem of directed 2K construction (D2K) that takes as input the directed degree sequence (DDS) and a joint degree and attribute matrix (JDAM) so as to capture degree correlation specifically in directed graphs. We provide necessary and sufficient conditions to decide whether a target D2K is realizable, and we design an efficient algorithm that creates realizations with that target D2K. We evaluate our algorithm in creating synthetic graphs that target real-world directed graphs (such as Twitter) and we show that it brings significant benefits compared to state-of-the-art approaches.|studi problem construct synthet graph resembl real world direct graph term degre correl defin problem direct construct dk take input direct degre sequenc dds joint degre attribut matrix jdam captur degre correl specif direct graph provid necessari suffici condit decid whether target dk realiz design effici algorithm creat realize target dk evalu algorithm creat synthet graph target real world direct graph twitter show bring signific benefit compar state art approach|['Bálint Tillman', 'Athina Markopoulou', 'Carter T. Butts', 'Minas Gjoka']|['cs.SI', 'cs.DS']
2017-03-28T14:09:34Z|2017-03-21T15:57:42Z|http://arxiv.org/abs/1703.07290v1|http://arxiv.org/pdf/1703.07290v1|Just-in-Time Batch Scheduling Problem with Two-dimensional Bin Packing   Constraints|time batch schedul problem two dimension bin pack constraint|This paper introduces and approximately solves a multi-component problem where small rectangular items are produced from large rectangular bins via guillotine cuts. An item is characterized by its width, height, due date, and earliness and tardiness penalties per unit time. Each item induces a cost that is proportional to its earliness and tardiness. Items cut from the same bin form a batch, whose processing and completion times depend on its assigned items. The items of a batch have the completion time of their bin. The objective is to find a cutting plan that minimizes the weighted sum of earliness and tardiness penalties. We address this problem via a constraint programming based heuristic (CP) and an agent based modelling heuristic (AB). CP is an impact-based search strategy, implemented in the general-purpose solver IBM CP Optimizer. AB is constructive. It builds a solution through repeated negotiations between the set of agents representing the items and the set representing the bins. The agents cooperate to minimize the weighted earliness-tardiness penalties. The computational investigation shows that CP outperforms AB on small-sized instances while the opposite prevails for larger instances.|paper introduc approxim solv multi compon problem small rectangular item produc larg rectangular bin via guillotin cut item character width height due date earli tardi penalti per unit time item induc cost proport earli tardi item cut bin form batch whose process complet time depend assign item item batch complet time bin object find cut plan minim weight sum earli tardi penalti address problem via constraint program base heurist cp agent base model heurist ab cp impact base search strategi implement general purpos solver ibm cp optim ab construct build solut repeat negoti set agent repres item set repres bin agent cooper minim weight earli tardi penalti comput investig show cp outperform ab small size instanc opposit prevail larger instanc|"['S. Polyakovskiy', 'A. Makarowsky', ""R. M'Hallah""]"|['cs.DS']
2017-03-28T14:09:34Z|2017-03-21T14:46:39Z|http://arxiv.org/abs/1703.07247v1|http://arxiv.org/pdf/1703.07247v1|A Note on the Tree Augmentation Problem|note tree augment problem|"In the Tree Augmentation problem we are given a tree $T=(V,F)$ and an additional set $E \subseteq V \times V$ of edges, called ""links"", with positive integer costs $\{c_e:e \in E\}$. The goal is to augment $T$ by a minimum cost set of links $J \subseteq E$ such that $T \cup J$ is $2$-edge-connected. Let $M$ denote the maximum cost of a link. Recently, Adjiashvili introduced a novel LP for the problem and used it to break the natural $2$-approximation barrier for instances when $M$ is a constant. Specifically, his algorithm computes a $1.96418+\epsilon$ approximate solution in time $n^{O(M/\epsilon^2)}$. Using a slightly weaker LP we achieve ratio $\frac{12}{7}+\epsilon$ for arbitrary costs and ratio $1.6+\epsilon$ for unit costs in time $2^{O(M/\epsilon^2)}$."|tree augment problem given tree addit set subseteq time edg call link posit integ cost goal augment minimum cost set link subseteq cup edg connect let denot maximum cost link recent adjiashvili introduc novel lp problem use break natur approxim barrier instanc constant specif algorithm comput epsilon approxim solut time epsilon use slight weaker lp achiev ratio frac epsilon arbitrari cost ratio epsilon unit cost time epsilon|['Zeev Nutov']|['cs.DS']
2017-03-28T14:09:34Z|2017-03-21T14:39:35Z|http://arxiv.org/abs/1703.07244v1|http://arxiv.org/pdf/1703.07244v1|A Hybrid Feasibility Constraints-Guided Search to the Two-Dimensional   Bin Packing Problem with Due Dates|hybrid feasibl constraint guid search two dimension bin pack problem due date|The two-dimensional non-oriented bin packing problem with due dates packs a set of rectangular items, which may be rotated by 90 degrees, into identical rectangular bins. The bins have equal processing times. An item's lateness is the difference between its due date and the completion time of its bin. The problem packs all items without overlap as to minimize maximum lateness Lmax.   The paper proposes a tight lower bound that enhances an existing bound on Lmax for 24.07% of the benchmark instances and matches it in 30.87% cases. In addition, it models the problem using mixed integer programming (MIP), and solves small-sized instances exactly using CPLEX. It approximately solves larger-sized instances using a two-stage heuristic. The first stage constructs an initial solution via a first-fit heuristic that applies an iterative constraint programming (CP)-based neighborhood search. The second stage, which is iterative too, approximately solves a series of assignment low-level MIPs that are guided by feasibility constraints. It then enhances the solution via a high-level random local search. The approximate approach improves existing upper bounds by 27.45% on average, and obtains the optimum for 33.93% of the instances. Overall, the exact and approximate approaches identify the optimum for 39.07% cases.   The proposed approach is applicable to complex problems. It applies CP and MIP sequentially, while exploring their advantages, and hybridizes heuristic search with MIP. It embeds a new lookahead strategy that guards against infeasible search directions and constrains the search to improving directions only; thus, differs from traditional lookahead beam searches.|two dimension non orient bin pack problem due date pack set rectangular item may rotat degre ident rectangular bin bin equal process time item late differ due date complet time bin problem pack item without overlap minim maximum late lmax paper propos tight lower bound enhanc exist bound lmax benchmark instanc match case addit model problem use mix integ program mip solv small size instanc exact use cplex approxim solv larger size instanc use two stage heurist first stage construct initi solut via first fit heurist appli iter constraint program cp base neighborhood search second stage iter approxim solv seri assign low level mip guid feasibl constraint enhanc solut via high level random local search approxim approach improv exist upper bound averag obtain optimum instanc overal exact approxim approach identifi optimum case propos approach applic complex problem appli cp mip sequenti explor advantag hybrid heurist search mip emb new lookahead strategi guard infeas search direct constrain search improv direct onli thus differ tradit lookahead beam search|"['S. Polyakovskiy', ""R. M'Hallah""]"|['cs.DS']
2017-03-28T14:09:34Z|2017-03-21T09:37:16Z|http://arxiv.org/abs/1703.07107v1|http://arxiv.org/pdf/1703.07107v1|On the Interplay between Strong Regularity and Graph Densification|interplay strong regular graph densif|In this paper we analyze the practical implications of Szemer\'edi's regularity lemma in the preservation of metric information contained in large graphs. To this end, we present a heuristic algorithm to find regular partitions. Our experiments show that this method is quite robust to the natural sparsification of proximity graphs. In addition, this robustness can be enforced by graph densification.|paper analyz practic implic szemer edi regular lemma preserv metric inform contain larg graph end present heurist algorithm find regular partit experi show method quit robust natur sparsif proxim graph addit robust enforc graph densif|['Marco Fiorucci', 'Alessandro Torcinovich', 'Manuel Curado', 'Francisco Escolano', 'Marcello Pelillo']|['cs.DS', 'cs.CV']
2017-03-28T14:09:34Z|2017-03-20T11:17:39Z|http://arxiv.org/abs/1703.06680v1|http://arxiv.org/pdf/1703.06680v1|Parallel Sort-Based Matching for Data Distribution Management on   Shared-Memory Multiprocessors|parallel sort base match data distribut manag share memori multiprocessor|In this paper we consider the problem of identifying intersections between two sets of d-dimensional axis-parallel rectangles. This is a common problem that arises in many agent-based simulation studies, and is of central importance in the context of High Level Architecture (HLA), where it is at the core of the Data Distribution Management (DDM) service. Several realizations of the DDM service have been proposed; however, many of them are either inefficient or inherently sequential. These are serious limitations since multicore processors are now ubiquitous, and DDM algorithms -- being CPU-intensive -- could benefit from additional computing power. We propose a parallel version of the Sort-Based Matching algorithm for shared-memory multiprocessors. Sort-Based Matching is one of the most efficient serial algorithms for the DDM problem, but is quite difficult to parallelize due to data dependencies. We describe the algorithm and compute its asymptotic running time; we complete the analysis by assessing its performance and scalability through extensive experiments on two commodity multicore systems based on a dual socket Intel Xeon processor, and a single socket Intel Core i7 processor.|paper consid problem identifi intersect two set dimension axi parallel rectangl common problem aris mani agent base simul studi central import context high level architectur hla core data distribut manag ddm servic sever realize ddm servic propos howev mani either ineffici inher sequenti serious limit sinc multicor processor ubiquit ddm algorithm cpu intens could benefit addit comput power propos parallel version sort base match algorithm share memori multiprocessor sort base match one effici serial algorithm ddm problem quit difficult parallel due data depend describ algorithm comput asymptot run time complet analysi assess perform scalabl extens experi two commod multicor system base dual socket intel xeon processor singl socket intel core processor|"['Moreno Marzolla', ""Gabriele D'Angelo""]"|['cs.DC', 'cs.DS', 'cs.MA']
2017-03-28T14:09:34Z|2017-03-20T09:34:51Z|http://arxiv.org/abs/1703.06644v1|http://arxiv.org/pdf/1703.06644v1|Reoptimization of the Closest Substring Problem under Pattern Length   Modification|reoptim closest substr problem pattern length modif|This study investigates whether reoptimization can help in solving the closest substring problem. We are dealing with the following reoptimization scenario. Suppose, we have an optimal l-length closest substring of a given set of sequences S. How can this information be beneficial in obtaining an (l+k)-length closest substring for S? In this study, we show that the problem is still computationally hard even with k=1. We present greedy approximation algorithms that make use of the given information and prove that it has an additive error that grows as the parameter k increases. Furthermore, we present hard instances for each algorithm to show that the computed approximation ratio is tight. We also show that we can slightly improve the running-time of the existing polynomial-time approximation scheme (PTAS) for the original problem through reoptimization.|studi investig whether reoptim help solv closest substr problem deal follow reoptim scenario suppos optim length closest substr given set sequenc inform benefici obtain length closest substr studi show problem still comput hard even present greedi approxim algorithm make use given inform prove addit error grow paramet increas furthermor present hard instanc algorithm show comput approxim ratio tight also show slight improv run time exist polynomi time approxim scheme ptas origin problem reoptim|['Jhoirene B. Clemente', 'Henry N. Adorna']|['cs.DS', '68W25', 'G.2.1']
2017-03-28T14:09:34Z|2017-03-18T18:12:17Z|http://arxiv.org/abs/1703.06327v1|http://arxiv.org/pdf/1703.06327v1|Spectrum Estimation from a Few Entries|spectrum estim entri|Singular values of a data in a matrix form provide insights on the structure of the data, the effective dimensionality, and the choice of hyper-parameters on higher-level data analysis tools. However, in many practical applications such as collaborative filtering and network analysis, we only get a partial observation. Under such scenarios, we consider the fundamental problem of recovering spectral properties of the underlying matrix from a sampling of its entries. We are particularly interested in directly recovering the spectrum, which is the set of singular values, and also in sample-efficient approaches for recovering a spectral sum function, which is an aggregate sum of the same function applied to each of the singular values. We propose first estimating the Schatten $k$-norms of a matrix, and then applying Chebyshev approximation to the spectral sum function or applying moment matching in Wasserstein distance to recover the singular values. The main technical challenge is in accurately estimating the Schatten norms from a sampling of a matrix. We introduce a novel unbiased estimator based on counting small structures in a graph and provide guarantees that match its empirical performance. Our theoretical analysis shows that Schatten norms can be recovered accurately from strictly smaller number of samples compared to what is needed to recover the underlying low-rank matrix. Numerical experiments suggest that we significantly improve upon a competing approach of using matrix completion methods.|singular valu data matrix form provid insight structur data effect dimension choic hyper paramet higher level data analysi tool howev mani practic applic collabor filter network analysi onli get partial observ scenario consid fundament problem recov spectral properti matrix sampl entri particular interest direct recov spectrum set singular valu also sampl effici approach recov spectral sum function aggreg sum function appli singular valu propos first estim schatten norm matrix appli chebyshev approxim spectral sum function appli moment match wasserstein distanc recov singular valu main technic challeng accur estim schatten norm sampl matrix introduc novel unbias estim base count small structur graph provid guarante match empir perform theoret analysi show schatten norm recov accur strict smaller number sampl compar need recov low rank matrix numer experi suggest signific improv upon compet approach use matrix complet method|['Ashish Khetan', 'Sewoong Oh']|['stat.ML', 'cs.DS', 'cs.LG', 'cs.NA']
2017-03-28T14:09:34Z|2017-03-18T17:21:14Z|http://arxiv.org/abs/1703.06320v1|http://arxiv.org/pdf/1703.06320v1|Hardware-Efficient Schemes of Quaternion Multiplying Units for 2D   Discrete Quaternion Fourier Transform Processors|hardwar effici scheme quaternion multipli unit discret quaternion fourier transform processor|In this paper, we offer and discuss three efficient structural solutions for the hardware-oriented implementation of discrete quaternion Fourier transform basic operations with reduced implementation complexities. The first solution: a scheme for calculating sq product, the second solution: a scheme for calculating qt product, and the third solution: a scheme for calculating sqt product, where s is a so-called i-quaternion, t is an j-quaternion, and q is an usual quaternion. The direct multiplication of two usual quaternions requires 16 real multiplications (or two-operand multipliers in the case of fully parallel hardware implementation) and 12 real additions (or binary adders). At the same time, our solutions allow to design the computation units, which consume only 6 multipliers plus 6 two input adders for implementation of sq or qt basic operations and 9 binary multipliers plus 6 two-input adders and 4 four-input adders for implementation of sqt basic operation.|paper offer discuss three effici structur solut hardwar orient implement discret quaternion fourier transform basic oper reduc implement complex first solut scheme calcul sq product second solut scheme calcul qt product third solut scheme calcul sqt product call quaternion quaternion usual quaternion direct multipl two usual quaternion requir real multipl two operand multipli case fulli parallel hardwar implement real addit binari adder time solut allow design comput unit consum onli multipli plus two input adder implement sq qt basic oper binari multipli plus two input adder four input adder implement sqt basic oper|['Aleksandr Cariow', 'Galina Cariowa', 'Marina Chicheva']|['cs.DS', 'cs.AR', '65T50, 15A04, 15A66, 15A66, 15A69, 03D15, 65Y20, 65Y10', 'F.2.1; I.1.2; C.1.4; C.3']
2017-03-28T14:09:34Z|2017-03-18T00:44:38Z|http://arxiv.org/abs/1703.06227v1|http://arxiv.org/pdf/1703.06227v1|Discriminative Distance-Based Network Indices and the Tiny-World   Property|discrimin distanc base network indic tini world properti|Distance-based indices, including closeness centrality, average path length, eccentricity and average eccentricity, are important tools for network analysis. In these indices, the distance between two vertices is measured by the size of shortest paths between them. However, this measure has shortcomings. A well-studied shortcoming is that extending it to disconnected graphs (and also directed graphs) is controversial. The second shortcoming is that when this measure is used in real-world networks, a huge number of vertices may have exactly the same closeness/eccentricity scores. The third shortcoming is that in many applications, the distance between two vertices not only depends on the size of shortest paths, but also on the number of shortest paths between them. In this paper, we develop a new distance measure between vertices of a graph that yields discriminative distance-based centrality indices. This measure is proportional to the size of shortest paths and inversely proportional to the number of shortest paths. We present algorithms for exact computation of the proposed discriminative indices. We then develop randomized algorithms that precisely estimate average discriminative path length and average discriminative eccentricity and show that they give $(\epsilon,\delta)$-approximations of these indices. Finally, we preform extensive experiments over several real-world networks from different domains and show that compared to the traditional indices, discriminative indices have usually much more discriminability. Our experiments reveal that real-world networks have usually a tiny average discriminative path length, bounded by a constant (e.g., 2). We refer to this property as the tiny-world property.|distanc base indic includ close central averag path length eccentr averag eccentr import tool network analysi indic distanc two vertic measur size shortest path howev measur shortcom well studi shortcom extend disconnect graph also direct graph controversi second shortcom measur use real world network huge number vertic may exact close eccentr score third shortcom mani applic distanc two vertic onli depend size shortest path also number shortest path paper develop new distanc measur vertic graph yield discrimin distanc base central indic measur proport size shortest path invers proport number shortest path present algorithm exact comput propos discrimin indic develop random algorithm precis estim averag discrimin path length averag discrimin eccentr show give epsilon delta approxim indic final preform extens experi sever real world network differ domain show compar tradit indic discrimin indic usual much discrimin experi reveal real world network usual tini averag discrimin path length bound constant refer properti tini world properti|['Mostafa Haghir Chehreghani', 'Albert Bifet', 'Talel Abdessalem']|['cs.DS', 'cs.SI']
2017-03-28T14:09:34Z|2017-03-17T17:31:01Z|http://arxiv.org/abs/1703.06733v1|http://arxiv.org/pdf/1703.06733v1|Discovering Relaxed Sound Workflow Nets using Integer Linear Programming|discov relax sound workflow net use integ linear program|Process mining is concerned with the analysis, understanding and improvement of business processes. Process discovery, i.e. discovering a process model based on an event log, is considered the most challenging process mining task. State-of-the-art process discovery algorithms only discover local control-flow patterns and are unable to discover complex, non-local patterns. Region theory based techniques, i.e. an established class of process discovery techniques, do allow for discovering such patterns. However, applying region theory directly results in complex, over-fitting models, which is less desirable. Moreover, region theory does not cope with guarantees provided by state-of-the-art process discovery algorithms, both w.r.t. structural and behavioural properties of the discovered process models. In this paper we present an ILP-based process discovery approach, based on region theory, that guarantees to discover relaxed sound workflow nets. Moreover, we devise a filtering algorithm, based on the internal working of the ILP-formulation, that is able to cope with the presence of infrequent behaviour. We have extensively evaluated the technique using different event logs with different levels of exceptional behaviour. Our experiments show that the presented approach allow us to leverage the inherent shortcomings of existing region-based approaches. The techniques presented are implemented and readily available in the HybridILPMiner package in the open-source process mining tool-kits ProM and RapidProM.|process mine concern analysi understand improv busi process process discoveri discov process model base event log consid challeng process mine task state art process discoveri algorithm onli discov local control flow pattern unabl discov complex non local pattern region theori base techniqu establish class process discoveri techniqu allow discov pattern howev appli region theori direct result complex fit model less desir moreov region theori doe cope guarante provid state art process discoveri algorithm structur behaviour properti discov process model paper present ilp base process discoveri approach base region theori guarante discov relax sound workflow net moreov devis filter algorithm base intern work ilp formul abl cope presenc infrequ behaviour extens evalu techniqu use differ event log differ level except behaviour experi show present approach allow us leverag inher shortcom exist region base approach techniqu present implement readili avail hybridilpmin packag open sourc process mine tool kit prom rapidprom|['S. J. van Zelst', 'B. F. van Dongen', 'W. M. P. van der Aalst', 'H. M. W. Verbeek']|['cs.DS']
2017-03-28T14:09:38Z|2017-03-17T16:18:31Z|http://arxiv.org/abs/1703.06074v1|http://arxiv.org/pdf/1703.06074v1|Robust Assignments with Vulnerable Nodes|robust assign vulner node|Various real-life planning problems require making upfront decisions before all parameters of the problem have been disclosed. An important special case of such problem especially arises in scheduling and staff rostering problems, where a set of tasks needs to be assigned to an available set of resources (personnel or machines), in a way that each task is assigned to one resource, while no task is allowed to share a resource with another task. In its nominal form, the resulting computational problem reduces to the well-known assignment problem that can be modeled as matching problems on bipartite graphs.   In recent work \cite{adjiashvili_bindewald_michaels_icalp2016}, a new robust model for the assignment problem was introduced that can deal with situations in which certain resources, i.e.\ nodes or edges of the underlying bipartite graph, are vulnerable and may become unavailable after a solution has been chosen. In the original version from \cite{adjiashvili_bindewald_michaels_icalp2016} the resources subject to uncertainty are the edges of the underlying bipartite graph.   In this follow-up work, we complement our previous study by considering nodes as being vulnerable, instead of edges. The goal is now to choose a minimum-cost collection of nodes such that, if any vulnerable node becomes unavailable, the remaining part of the solution still contains sufficient nodes to perform all tasks. From a practical point of view, such type of unavailability is interesting as it is typically caused e.g.\ by an employee's sickness, or machine failure. We present algorithms and hardness of approximation results for several variants of the problem.|various real life plan problem requir make upfront decis befor paramet problem disclos import special case problem especi aris schedul staff roster problem set task need assign avail set resourc personnel machin way task assign one resourc task allow share resourc anoth task nomin form result comput problem reduc well known assign problem model match problem bipartit graph recent work cite adjiashvili bindewald michael icalp new robust model assign problem introduc deal situat certain resourc node edg bipartit graph vulner may becom unavail solut chosen origin version cite adjiashvili bindewald michael icalp resourc subject uncertainti edg bipartit graph follow work complement previous studi consid node vulner instead edg goal choos minimum cost collect node ani vulner node becom unavail remain part solut still contain suffici node perform task practic point view type unavail interest typic caus employe sick machin failur present algorithm hard approxim result sever variant problem|['David Adjiashvili', 'Viktor Bindewald', 'Dennis Michaels']|['cs.DS', 'cs.DM', '90C27', 'I.1.2; G.2.2; G.1.6']
2017-03-28T14:09:38Z|2017-03-17T16:08:23Z|http://arxiv.org/abs/1703.06065v1|http://arxiv.org/pdf/1703.06065v1|Block CUR : Decomposing Large Distributed Matrices|block cur decompos larg distribut matric|A common problem in large-scale data analysis is to approximate a matrix using a combination of specifically sampled rows and columns, known as CUR decomposition. Unfortunately, in many real-world environments, the ability to sample specific individual rows or columns of the matrix is limited by either system constraints or cost. In this paper, we consider matrix approximation by sampling predefined blocks of columns (or rows) from the matrix. This regime is commonly found when data is distributed across multiple nodes in a compute cluster, where such blocks correspond to columns (or rows) of the matrix stored on the same node, which can be retrieved with much less overhead than retrieving individual columns stored across different nodes. We propose a novel algorithm for sampling useful column blocks and provide guarantees for the quality of the approximation. We demonstrate the practical utility of this algorithm for computing the block CUR decomposition of large matrices in a distributed setting using Apache Spark. Using our proposed block CUR algorithms, we can achieve a significant speed-up compared to a regular CUR decomposition with the same quality of approximation.|common problem larg scale data analysi approxim matrix use combin specif sampl row column known cur decomposit unfortun mani real world environ abil sampl specif individu row column matrix limit either system constraint cost paper consid matrix approxim sampl predefin block column row matrix regim common found data distribut across multipl node comput cluster block correspond column row matrix store node retriev much less overhead retriev individu column store across differ node propos novel algorithm sampl use column block provid guarante qualiti approxim demonstr practic util algorithm comput block cur decomposit larg matric distribut set use apach spark use propos block cur algorithm achiev signific speed compar regular cur decomposit qualiti approxim|['Urvashi Oswal', 'Swayambhoo Jain', 'Kevin S. Xu', 'Brian Eriksson']|['stat.ML', 'cs.DC', 'cs.DS', 'cs.LG']
2017-03-28T14:09:38Z|2017-03-17T15:56:50Z|http://arxiv.org/abs/1703.06061v1|http://arxiv.org/pdf/1703.06061v1|Approximation ratio of RePair|approxim ratio repair|In a seminal paper of Charikar et al.~on the smallest grammar problem, the authors derive upper and lower bounds on the approximation ratios for several grammar-based compressors. Here we improve the lower bound for the famous {\sf RePair} algorithm from $\Omega(\sqrt{\log n})$ to $\Omega(\log n/\log\log n)$. The family of words used in our proof is defined over a binary alphabet, while the lower bound from Charikar et al. needs an alphabet of logarithmic size in the length of the provided words.|semin paper charikar et al smallest grammar problem author deriv upper lower bound approxim ratio sever grammar base compressor improv lower bound famous sf repair algorithm omega sqrt log omega log log log famili word use proof defin binari alphabet lower bound charikar et al need alphabet logarithm size length provid word|['Danny Hucke', 'Artur Jez', 'Markus Lohrey']|['cs.DS', 'F.2.2, E.4']
2017-03-28T14:09:38Z|2017-03-21T12:29:29Z|http://arxiv.org/abs/1703.06053v2|http://arxiv.org/pdf/1703.06053v2|Fast Non-Monotone Submodular Maximisation Subject to a Matroid   Constraint|fast non monoton submodular maximis subject matroid constraint|In this work we present the first practical $\left(\frac{1}{e}-\epsilon\right)$-approximation algorithm to maximise a general non-negative submodular function subject to a matroid constraint. Our algorithm is based on combining the decreasing-threshold procedure of Badanidiyuru and Vondrak (SODA 2014) with a smoother version of the measured continuous greedy algorithm of Feldman et al. (FOCS 2011). This enables us to obtain an algorithm that requires $O(\frac{nr^2}{\epsilon^4} \big(\frac{a+b}{a}\big)^2 \log^2({\frac{n}{\epsilon}}))$ value oracle calls, where $n$ is the cardinality of the ground set, $r$ is the matroid rank, and $ b, a \in \mathbb{R}^+$ are the absolute values of the minimum and maximum marginal values that the function $f$ can take i.e.: $ -b \leq f_S(i) \leq a$, for all $i\in E$ and $S\subseteq E$, (here, $E$ is the ground set). The additional value oracle calls with respect to the work of Badanidiyuru and Vondrak come from the greater spread in the sampling of the multilinear extension that the possibility of negative marginal values introduce.|work present first practic left frac epsilon right approxim algorithm maximis general non negat submodular function subject matroid constraint algorithm base combin decreas threshold procedur badanidiyuru vondrak soda smoother version measur continu greedi algorithm feldman et al foc enabl us obtain algorithm requir frac nr epsilon big frac big log frac epsilon valu oracl call cardin ground set matroid rank mathbb absolut valu minimum maximum margin valu function take leq leq subseteq ground set addit valu oracl call respect work badanidiyuru vondrak come greater spread sampl multilinear extens possibl negat margin valu introduc|['Pau Segui-Gasco', 'Hyo-Sang Shin']|['cs.DS']
2017-03-28T14:09:38Z|2017-03-17T15:08:17Z|http://arxiv.org/abs/1703.06048v1|http://arxiv.org/pdf/1703.06048v1|An FPTAS for the Knapsack Problem with Parametric Weights|fptas knapsack problem parametr weight|In this paper, we investigate the parametric weight knapsack problem, in which the item weights are affine functions of the form $w_i(\lambda) = a_i + \lambda \cdot b_i$ for $i \in \{1,\ldots,n\}$ depending on a real-valued parameter $\lambda$. The aim is to provide a solution for all values of the parameter. It is well-known that any exact algorithm for the problem may need to output an exponential number of knapsack solutions. We present the first fully polynomial-time approximation scheme (FPTAS) for the problem that, for any desired precision $\varepsilon \in (0,1)$, computes $(1-\varepsilon)$-approximate solutions for all values of the parameter. Our FPTAS is based on two different approaches and achieves a running time of $\mathcal{O}(n^3/\varepsilon^2 \cdot \min\{ \log^2 P, n^2 \} \cdot \min\{\log M, n \log (n/\varepsilon) / \log(n \log (n/\varepsilon) )\})$ where $P$ is an upper bound on the optimal profit and $M := \max\{W, n \cdot \max\{a_i,b_i: i \in \{1,\ldots,n\}\}\}$ for a knapsack with capacity $W$.|paper investig parametr weight knapsack problem item weight affin function form lambda lambda cdot ldot depend real valu paramet lambda aim provid solut valu paramet well known ani exact algorithm problem may need output exponenti number knapsack solut present first fulli polynomi time approxim scheme fptas problem ani desir precis varepsilon comput varepsilon approxim solut valu paramet fptas base two differ approach achiev run time mathcal varepsilon cdot min log cdot min log log varepsilon log log varepsilon upper bound optim profit max cdot max ldot knapsack capac|['Michael Holzhauser', 'Sven O. Krumke']|['cs.DS', 'cs.CC', 'math.OC']
2017-03-28T14:09:38Z|2017-03-17T14:53:55Z|http://arxiv.org/abs/1703.06040v1|http://arxiv.org/pdf/1703.06040v1|Towards a Topology-Shape-Metrics Framework for Ortho-Radial Drawings|toward topolog shape metric framework ortho radial draw|Ortho-Radial drawings are a generalization of orthogonal drawings to grids that are formed by concentric circles and straight-line spokes emanating from the circles' center. Such drawings have applications in schematic graph layouts, e.g., for metro maps and destination maps.   A plane graph is a planar graph with a fixed planar embedding. We give a combinatorial characterization of the plane graphs that admit a planar ortho-radial drawing without bends. Previously, such a characterization was only known for paths, cycles, and theta graphs, and in the special case of rectangular drawings for cubic graphs, where the contour of each face is required to be a rectangle.   The characterization is expressed in terms of an ortho-radial representation that, similar to Tamassia's orthogonal representations for orthogonal drawings describes such a drawing combinatorially in terms of angles around vertices and bends on the edges. In this sense our characterization can be seen as a first step towards generalizing the Topology-Shape-Metrics framework of Tamassia to ortho-radial drawings.|ortho radial draw general orthogon draw grid form concentr circl straight line spoke eman circl center draw applic schemat graph layout metro map destin map plane graph planar graph fix planar embed give combinatori character plane graph admit planar ortho radial draw without bend previous character onli known path cycl theta graph special case rectangular draw cubic graph contour face requir rectangl character express term ortho radial represent similar tamassia orthogon represent orthogon draw describ draw combinatori term angl around vertic bend edg sens character seen first step toward general topolog shape metric framework tamassia ortho radial draw|['Lukas Barth', 'Benjamin Niedermann', 'Ignaz Rutter', 'Matthias Wolf']|['cs.DM', 'cs.DS']
2017-03-28T14:09:38Z|2017-03-17T12:57:18Z|http://arxiv.org/abs/1703.05997v1|http://arxiv.org/pdf/1703.05997v1|Connection Scan Algorithm|connect scan algorithm|We introduce the Connection Scan Algorithm (CSA) to efficiently answer queries to timetable information systems. The input consists, in the simplest setting, of a source position and a desired target position. The output consist is a sequence of vehicles such as trains or buses that a traveler should take to get from the source to the target. We study several problem variations such as the earliest arrival and profile problems. We present algorithm variants that only optimize the arrival time or additionally optimize the number of transfers in the Pareto sense. An advantage of CSA is that is can easily adjust to changes in the timetable, allowing the easy incorporation of known vehicle delays. We additionally introduce the Minimum Expected Arrival Time (MEAT) problem to handle possible, uncertain, future vehicle delays. We present a solution to the MEAT problem that is based upon CSA. Finally, we extend CSA using the multilevel overlay paradigm to answer complex queries on nation-wide integrated timetables with trains and buses.|introduc connect scan algorithm csa effici answer queri timet inform system input consist simplest set sourc posit desir target posit output consist sequenc vehicl train buse travel take get sourc target studi sever problem variat earliest arriv profil problem present algorithm variant onli optim arriv time addit optim number transfer pareto sens advantag csa easili adjust chang timet allow easi incorpor known vehicl delay addit introduc minimum expect arriv time meat problem handl possibl uncertain futur vehicl delay present solut meat problem base upon csa final extend csa use multilevel overlay paradigm answer complex queri nation wide integr timet train buse|['Julian Dibbelt', 'Thomas Pajor', 'Ben Strasser', 'Dorothea Wagner']|['cs.DS']
2017-03-28T14:09:38Z|2017-03-16T13:10:29Z|http://arxiv.org/abs/1703.05598v1|http://arxiv.org/pdf/1703.05598v1|Linear-Time Algorithm for Maximum-Cardinality Matching on   Cocomparability Graphs|linear time algorithm maximum cardin match cocompar graph|Finding maximum-cardinality matchings in undirected graphs is arguably one of the most central graph problems. For general m-edge and n-vertex graphs, it is well-known to be solvable in $O(m \sqrt{n})$ time. We develop the first linear-time algorithm to find maximum-cardinality matchings on cocomparability graphs, a prominent subclass of perfect graphs that contains interval graphs as well as permutation graphs. Our algorithm is based on the recently discovered Lexicographic Depth First Search (LDFS).|find maximum cardin match undirect graph arguabl one central graph problem general edg vertex graph well known solvabl sqrt time develop first linear time algorithm find maximum cardin match cocompar graph promin subclass perfect graph contain interv graph well permut graph algorithm base recent discov lexicograph depth first search ldfs|['George B. Mertzios', 'André Nichterlein', 'Rolf Niedermeier']|['cs.DS', 'F.2.2']
2017-03-28T14:09:38Z|2017-03-16T11:39:22Z|http://arxiv.org/abs/1703.05568v1|http://arxiv.org/pdf/1703.05568v1|Quantum Spectral Clustering through a Biased Phase Estimation Algorithm|quantum spectral cluster bias phase estim algorithm|In this brief paper, we go through the theoretical steps of the spectral clustering on quantum computers by employing the phase estimation and the amplitude amplification algorithms. To speed-up the amplitude amplification, we introduce a biased version of the phase estimation algorithm. In addition, when the circuit representation of a data matrix of order $N$ is produced through an ancilla based circuit in which the matrix is written as a sum of $L$ number of Householder matrices; we show that the computational complexity of the whole process is bound by $O(c2^mLN)$ number of quantum gates. Here, $m$ represents the number of qubits involved in the phase register of the phase estimation algorithm and $c$ represents the number of trials done to find the best clustering.|brief paper go theoret step spectral cluster quantum comput employ phase estim amplitud amplif algorithm speed amplitud amplif introduc bias version phase estim algorithm addit circuit represent data matrix order produc ancilla base circuit matrix written sum number household matric show comput complex whole process bound mln number quantum gate repres number qubit involv phase regist phase estim algorithm repres number trial done find best cluster|['Ammar Daskin']|['quant-ph', 'cs.DS']
2017-03-28T14:09:38Z|2017-03-16T11:09:25Z|http://arxiv.org/abs/1703.05559v1|http://arxiv.org/pdf/1703.05559v1|Improving TSP tours using dynamic programming over tree decomposition|improv tsp tour use dynam program tree decomposit|Given a traveling salesman problem (TSP) tour $H$ in graph $G$ a $k$-move is an operation which removes $k$ edges from $H$, and adds $k$ edges of $G$ so that a new tour $H'$ is formed. The popular $k$-OPT heuristics for TSP finds a local optimum by starting from an arbitrary tour $H$ and then improving it by a sequence of $k$-moves.   Until 2016, the only known algorithm to find an improving $k$-move for a given tour was the naive solution in time $O(n^k)$. At ICALP'16 de Berg, Buchin, Jansen and Woeginger showed an $O(n^{\lfloor 2/3k \rfloor+1})$-time algorithm.   We show an algorithm which runs in $O(n^{(1/4+\epsilon_k)k})$ time, where $\lim \epsilon_k = 0$. We are able to show that it improves over the state of the art for every $k=5,\ldots,10$. For the most practically relevant case $k=5$ we provide a slightly refined algorithm running in $O(n^{3.4})$ time. We also show that for the $k=4$ case, improving over the $O(n^3)$-time algorithm of de Berg et al. would be a major breakthrough: an $O(n^{3-\epsilon})$-time algorithm for any $\epsilon>0$ would imply an $O(n^{3-\delta})$-time algorithm for the ALL PAIRS SHORTEST PATHS problem, for some $\delta>0$.|given travel salesman problem tsp tour graph move oper remov edg add edg new tour form popular opt heurist tsp find local optimum start arbitrari tour improv sequenc move onli known algorithm find improv move given tour naiv solut time icalp de berg buchin jansen woeging show lfloor rfloor time algorithm show algorithm run epsilon time lim epsilon abl show improv state art everi ldot practic relev case provid slight refin algorithm run time also show case improv time algorithm de berg et al would major breakthrough epsilon time algorithm ani epsilon would impli delta time algorithm pair shortest path problem delta|['Marek Cygan', 'Lukasz Kowalik', 'Arkadiusz Socala']|['cs.DS']
2017-03-28T14:09:43Z|2017-03-16T08:52:21Z|http://arxiv.org/abs/1703.05509v1|http://arxiv.org/pdf/1703.05509v1|VieM v1.00 -- Vienna Mapping and Sparse Quadratic Assignment User Guide|viem vienna map spars quadrat assign user guid|This paper severs as a user guide to the mapping framework VieM (Vienna Mapping and Sparse Quadratic Assignment). We give a rough overview of the techniques used within the framework and describe the user interface as well as the file formats used.|paper sever user guid map framework viem vienna map spars quadrat assign give rough overview techniqu use within framework describ user interfac well file format use|['Christian Schulz', 'Jesper Larsson Träff']|['cs.DC', 'cs.DS', 'math.CO']
2017-03-28T14:09:43Z|2017-03-16T08:10:30Z|http://arxiv.org/abs/1703.05496v1|http://arxiv.org/pdf/1703.05496v1|Data Delivery by Mobile Agents with Energy Constraints over a fixed path|data deliveri mobil agent energi constraint fix path|We consider $k$ mobile agents of limited energy that are initially located at vertices of an edge-weighted graph $G$ and have to collectively deliver data from a source vertex $s$ to a target vertex $t$. The data are to be collected by an agent reaching $s$, who can carry and then hand them over another agent etc., until some agent with the data reaches $t$. The data can be carried only over a fixed $s-t$ path of $G$; each agent has an initial energy budget and each time it passes an edge, it consumes the edge's weights in energy units and stalls if its energy is not anymore sufficient to move. The main result of this paper is a 3-approximation polynomial time algorithm for the data delivery problem over a fixed $s-t$ path in the graph, for identical initial energy budgets and at most one allowed data hand-over per agent.|consid mobil agent limit energi initi locat vertic edg weight graph collect deliv data sourc vertex target vertex data collect agent reach carri hand anoth agent etc agent data reach data carri onli fix path agent initi energi budget time pass edg consum edg weight energi unit stall energi anymor suffici move main result paper approxim polynomi time algorithm data deliveri problem fix path graph ident initi energi budget one allow data hand per agent|['Aristotelis Giannakos', 'Mhand Hifi', 'Gregory Karagiorgos']|['cs.DS']
2017-03-28T14:09:43Z|2017-03-15T23:02:07Z|http://arxiv.org/abs/1703.05418v1|http://arxiv.org/pdf/1703.05418v1|A Local Algorithm for the Sparse Spanning Graph Problem|local algorithm spars span graph problem|Constructing a sparse \emph{spanning subgraph} is a fundamental primitive in graph theory. In this paper, we study this problem in the Centralized Local model, where the goal is to decide whether an edge is part of the spanning subgraph by examining only a small part of the input; yet, answers must be globally consistent and independent of prior queries.   Unfortunately, maximally sparse spanning subgraphs, i.e., spanning trees, cannot be constructed efficiently in this model. Therefore, we settle for a spanning subgraph containing at most $(1+\varepsilon)n$ edges (where $n$ is the number of vertices and $\varepsilon$ is a given approximation/sparsity parameter). We achieve query complexity of $\tilde{O}(poly(\Delta/\varepsilon)n^{2/3})$,\footnote{$\tilde{O}$-notation hides polylogarithmic factors in $n$.} where $\Delta$ is the maximum degree of the input graph. Our algorithm is the first to do so on arbitrary graphs. Moreover, we achieve the additional property that our algorithm outputs a \emph{spanner,} i.e., distances are approximately preserved. With high probability, for each deleted edge there is a path of $O(poly(\Delta/\varepsilon)\log^2 n)$ hops in the output that connects its endpoints.|construct spars emph span subgraph fundament primit graph theori paper studi problem central local model goal decid whether edg part span subgraph examin onli small part input yet answer must global consist independ prior queri unfortun maxim spars span subgraph span tree cannot construct effici model therefor settl span subgraph contain varepsilon edg number vertic varepsilon given approxim sparsiti paramet achiev queri complex tild poli delta varepsilon footnot tild notat hide polylogarithm factor delta maximum degre input graph algorithm first arbitrari graph moreov achiev addit properti algorithm output emph spanner distanc approxim preserv high probabl delet edg path poli delta varepsilon log hop output connect endpoint|['Christoph Lenzen', 'Reut Levi']|['cs.DS']
2017-03-28T14:09:43Z|2017-03-15T15:10:16Z|http://arxiv.org/abs/1703.05199v1|http://arxiv.org/pdf/1703.05199v1|Optimal Unateness Testers for Real-Valued Functions: Adaptivity Helps|optim unat tester real valu function adapt help|We study the problem of testing unateness of functions $f:\{0,1\}^d \to \mathbb{R}.$ We give a $O(\frac{d}{\epsilon} \cdot \log\frac{d}{\epsilon})$-query nonadaptive tester and a $O(\frac{d}{\epsilon})$-query adaptive tester and show that both testers are optimal for a fixed distance parameter $\epsilon$. Previously known unateness testers worked only for Boolean functions, and their query complexity had worse dependence on the dimension both for the adaptive and the nonadaptive case. Moreover, no lower bounds for testing unateness were known. We also generalize our results to obtain optimal unateness testers for functions $f:[n]^d \to \mathbb{R}$.   Our results establish that adaptivity helps with testing unateness of real-valued functions on domains of the form $\{0,1\}^d$ and, more generally, $[n]^d$. This stands in contrast to the situation for monotonicity testing where there is no adaptivity gap for functions $f:[n]^d \to \mathbb{R}$.|studi problem test unat function mathbb give frac epsilon cdot log frac epsilon queri nonadapt tester frac epsilon queri adapt tester show tester optim fix distanc paramet epsilon previous known unat tester work onli boolean function queri complex wors depend dimens adapt nonadapt case moreov lower bound test unat known also general result obtain optim unat tester function mathbb result establish adapt help test unat real valu function domain form general stand contrast situat monoton test adapt gap function mathbb|['Roksana Baleshzar', 'Deeparnab Chakrabarty', 'Ramesh Krishnan S. Pallavoor', 'Sofya Raskhodnikova', 'C. Seshadhri']|['cs.DS', 'cs.DM']
2017-03-28T14:09:43Z|2017-03-15T14:01:21Z|http://arxiv.org/abs/1703.05160v1|http://arxiv.org/pdf/1703.05160v1|A New Unbiased and Efficient Class of LSH-Based Samplers and Estimators   for Partition Function Computation in Log-Linear Models|new unbias effici class lsh base sampler estim partit function comput log linear model|"Log-linear models are arguably the most successful class of graphical models for large-scale applications because of their simplicity and tractability. Learning and inference with these models require calculating the partition function, which is a major bottleneck and intractable for large state spaces. Importance Sampling (IS) and MCMC-based approaches are lucrative. However, the condition of having a ""good"" proposal distribution is often not satisfied in practice.   In this paper, we add a new dimension to efficient estimation via sampling. We propose a new sampling scheme and an unbiased estimator that estimates the partition function accurately in sub-linear time. Our samples are generated in near-constant time using locality sensitive hashing (LSH), and so are correlated and unnormalized. We demonstrate the effectiveness of our proposed approach by comparing the accuracy and speed of estimating the partition function against other state-of-the-art estimation techniques including IS and the efficient variant of Gumbel-Max sampling. With our efficient sampling scheme, we accurately train real-world language models using only 1-2% of computations."|log linear model arguabl success class graphic model larg scale applic becaus simplic tractabl learn infer model requir calcul partit function major bottleneck intract larg state space import sampl mcmc base approach lucrat howev condit good propos distribut often satisfi practic paper add new dimens effici estim via sampl propos new sampl scheme unbias estim estim partit function accur sub linear time sampl generat near constant time use local sensit hash lsh correl unnorm demonstr effect propos approach compar accuraci speed estim partit function state art estim techniqu includ effici variant gumbel max sampl effici sampl scheme accur train real world languag model use onli comput|['Ryan Spring', 'Anshumali Shrivastava']|['stat.ML', 'cs.DB', 'cs.DS', 'cs.LG']
2017-03-28T14:09:43Z|2017-03-15T13:51:23Z|http://arxiv.org/abs/1703.05156v1|http://arxiv.org/pdf/1703.05156v1|Complexity Dichotomies for the Minimum F-Overlay Problem|complex dichotomi minimum overlay problem|For a (possibly infinite) fixed family of graphs F, we say that a graph G overlays F on a hypergraph H if V(H) is equal to V(G) and the subgraph of G induced by every hyperedge of H contains some member of F as a spanning subgraph.While it is easy to see that the complete graph on  V(H)  overlays F on a hypergraph H whenever the problem admits a solution, the Minimum F-Overlay problem asks for such a graph with the minimum number of edges.This problem allows to generalize some natural problems which may arise in practice. For instance, if the family F contains all connected graphs, then Minimum F-Overlay corresponds to the Minimum Connectivity Inference problem (also known as Subset Interconnection Design problem) introduced for the low-resolution reconstruction of macro-molecular assembly in structural biology, or for the design of networks.Our main contribution is a strong dichotomy result regarding the polynomial vs. NP-hard status with respect to the considered family F. Roughly speaking, we show that the easy cases one can think of (e.g. when edgeless graphs of the right sizes are in F, or if F contains only cliques) are the only families giving rise to a polynomial problem: all others are NP-complete.We then investigate the parameterized complexity of the problem and give similar sufficient conditions on F that give rise to W[1]-hard, W[2]-hard or FPT problems when the parameter is the size of the solution.This yields an FPT/W[1]-hard dichotomy for a relaxed problem, where every hyperedge of H must contain some member of F as a (non necessarily spanning) subgraph.|possibl infinit fix famili graph say graph overlay hypergraph equal subgraph induc everi hyperedg contain member span subgraph easi see complet graph overlay hypergraph whenev problem admit solut minimum overlay problem ask graph minimum number edg problem allow general natur problem may aris practic instanc famili contain connect graph minimum overlay correspond minimum connect infer problem also known subset interconnect design problem introduc low resolut reconstruct macro molecular assembl structur biolog design network main contribut strong dichotomi result regard polynomi vs np hard status respect consid famili rough speak show easi case one think edgeless graph right size contain onli cliqu onli famili give rise polynomi problem np complet investig parameter complex problem give similar suffici condit give rise hard hard fpt problem paramet size solut yield fpt hard dichotomi relax problem everi hyperedg must contain member non necessarili span subgraph|['Nathann Cohen', 'Frédéric Havet', 'Dorian Mazauric', 'Ignasi Sau', 'Rémi Watrigant']|['cs.DS', 'cs.CC']
2017-03-28T14:09:43Z|2017-03-15T12:06:53Z|http://arxiv.org/abs/1703.05102v1|http://arxiv.org/pdf/1703.05102v1|Algorithms for outerplanar graph roots and graph roots of pathwidth at   most 2|algorithm outerplanar graph root graph root pathwidth|Deciding whether a given graph has a square root is a classical problem that has been studied extensively both from graph theoretic and from algorithmic perspectives. The problem is NP-complete in general, and consequently substantial effort has been dedicated to deciding whether a given graph has a square root that belongs to a particular graph class. There are both polynomial-time solvable and NP-complete cases, depending on the graph class. We contribute with new results in this direction. Given an arbitrary input graph G, we give polynomial-time algorithms to decide whether G has an outerplanar square root, and whether G has a square root that is of pathwidth at most 2.|decid whether given graph squar root classic problem studi extens graph theoret algorithm perspect problem np complet general consequ substanti effort dedic decid whether given graph squar root belong particular graph class polynomi time solvabl np complet case depend graph class contribut new result direct given arbitrari input graph give polynomi time algorithm decid whether outerplanar squar root whether squar root pathwidth|['Petr A. Golovach', 'Pinar Heggernes', 'Dieter Kratsch', 'Paloma T. Lima', 'Daniel Paulusma']|['cs.DS', 'cs.DM']
2017-03-28T14:09:43Z|2017-03-15T11:57:53Z|http://arxiv.org/abs/1703.05097v1|http://arxiv.org/pdf/1703.05097v1|A cubic-time algorithm for computing the trinet distance between level-1   networks|cubic time algorithm comput trinet distanc level network|In evolutionary biology, phylogenetic networks are constructed to represent the evolution of species in which reticulate events are thought to have occurred, such as recombination and hybridization. It is therefore useful to have efficiently computable metrics with which to systematically compare such networks. Through developing an optimal algorithm to enumerate all trinets displayed by a level-1 network (a type of network that is slightly more general than an evolutionary tree), here we propose a cubic-time algorithm to compute the trinet distance between two level-1 networks. Employing simulations, we also present a comparison between the trinet metric and the so-called Robinson-Foulds phylogenetic network metric restricted to level-1 networks. The algorithms described in this paper have been implemented in JAVA and are freely available at https://www.uea.ac.uk/computing/TriLoNet.|evolutionari biolog phylogenet network construct repres evolut speci reticul event thought occur recombin hybrid therefor use effici comput metric systemat compar network develop optim algorithm enumer trinet display level network type network slight general evolutionari tree propos cubic time algorithm comput trinet distanc two level network employ simul also present comparison trinet metric call robinson fould phylogenet network metric restrict level network algorithm describ paper implement java freeli avail https www uea ac uk comput trilonet|['Vincent Moulton', 'James Oldman', 'Taoyang Wu']|['q-bio.PE', 'cs.DM', 'cs.DS']
2017-03-28T14:09:43Z|2017-03-15T06:21:59Z|http://arxiv.org/abs/1703.04954v1|http://arxiv.org/pdf/1703.04954v1|Faster STR-IC-LCS computation via RLE|faster str ic lcs comput via rle|The constrained LCS problem asks one to find a longest common subsequence of two input strings $A$ and $B$ with some constraints. The STR-IC-LCS problem is a variant of the constrained LCS problem, where the solution must include a given constraint string $C$ as a substring. Given two strings $A$ and $B$ of respective lengths $M$ and $N$, and a constraint string $C$ of length at most $\min\{M, N\}$, the best known algorithm for the STR-IC-LCS problem, proposed by Deorowicz~({\em Inf. Process. Lett.}, 11:423--426, 2012), runs in $O(MN)$ time. In this work, we present an $O(mN + nM)$-time solution to the STR-IC-LCS problem, where $m$ and $n$ denote the sizes of the run-length encodings of $A$ and $B$, respectively. Since $m \leq M$ and $n \leq N$ always hold, our algorithm is always as fast as Deorowicz's algorithm, and is faster when input strings are compressible via RLE.|constrain lcs problem ask one find longest common subsequ two input string constraint str ic lcs problem variant constrain lcs problem solut must includ given constraint string substr given two string respect length constraint string length min best known algorithm str ic lcs problem propos deorowicz em inf process lett run mn time work present mn nm time solut str ic lcs problem denot size run length encod respect sinc leq leq alway hold algorithm alway fast deorowicz algorithm faster input string compress via rle|['Keita Kuboi', 'Yuta Fujishige', 'Shunsuke Inenaga', 'Hideo Bannai', 'Masayuki Takeda']|['cs.DS']
2017-03-28T14:09:43Z|2017-03-14T23:06:33Z|http://arxiv.org/abs/1703.04814v1|http://arxiv.org/pdf/1703.04814v1|Near-Optimal Compression for the Planar Graph Metric|near optim compress planar graph metric|"The Planar Graph Metric Compression Problem is to compactly encode the distances among $k$ nodes in a planar graph of size $n$. Two na\""ive solutions are to store the graph using $O(n)$ bits, or to explicitly store the distance matrix with $O(k^2 \log{n})$ bits. The only lower bounds are from the seminal work of Gavoille, Peleg, Prennes, and Raz [SODA'01], who rule out compressions into a polynomially smaller number of bits, for {\em weighted} planar graphs, but leave a large gap for unweighted planar graphs. For example, when $k=\sqrt{n}$, the upper bound is $O(n)$ and their constructions imply an $\Omega(n^{3/4})$ lower bound. This gap is directly related to other major open questions in labelling schemes, dynamic algorithms, and compact routing.   Our main result is a new compression of the planar graph metric into $\tilde{O}(\min (k^2 , \sqrt{k\cdot n}))$ bits, which is optimal up to log factors. Our data structure breaks an $\Omega(k^2)$ lower bound of Krauthgamer, Nguyen, and Zondiner [SICOMP'14] for compression using minors, and the lower bound of Gavoille et al. for compression of weighted planar graphs. This is an unexpected and decisive proof that weights can make planar graphs inherently more complex. Moreover, we design a new {\em Subset Distance Oracle} for planar graphs with $\tilde O(\sqrt{k\cdot n})$ space, and $\tilde O(n^{3/4})$ query time.   Our work carries strong messages to related fields. In particular, the famous $O(n^{1/2})$ vs. $\Omega(n^{1/3})$ gap for distance labelling schemes in planar graphs {\em cannot} be resolved with the current lower bound techniques."|planar graph metric compress problem compact encod distanc among node planar graph size two na ive solut store graph use bit explicit store distanc matrix log bit onli lower bound semin work gavoill peleg prenn raz soda rule compress polynomi smaller number bit em weight planar graph leav larg gap unweight planar graph exampl sqrt upper bound construct impli omega lower bound gap direct relat major open question label scheme dynam algorithm compact rout main result new compress planar graph metric tild min sqrt cdot bit optim log factor data structur break omega lower bound krauthgam nguyen zondin sicomp compress use minor lower bound gavoill et al compress weight planar graph unexpect decis proof weight make planar graph inher complex moreov design new em subset distanc oracl planar graph tild sqrt cdot space tild queri time work carri strong messag relat field particular famous vs omega gap distanc label scheme planar graph em cannot resolv current lower bound techniqu|['Amir Abboud', 'Pawel Gawrychowski', 'Shay Mozes', 'Oren Weimann']|['cs.DS']
2017-03-28T14:09:47Z|2017-03-14T22:16:53Z|http://arxiv.org/abs/1703.04769v1|http://arxiv.org/pdf/1703.04769v1|The Stochastic Container Relocation Problem|stochast contain reloc problem|"The Container Relocation Problem (CRP) is concerned with finding a sequence of moves of containers that minimizes the number of relocations needed to retrieve all containers, while respecting a given order of retrieval. However, the assumption of knowing the full retrieval order of containers is particularly unrealistic in real operations. This paper studies the stochastic CRP (SCRP), which relaxes this assumption. A new multi-stage stochastic model, called the batch model, is introduced, motivated, and compared with an existing model (the online model). The two main contributions are an optimal algorithm called Pruning-Best-First-Search (PBFS) and a randomized approximate algorithm called PBFS-Approximate with a bounded average error. Both algorithms, applicable in the batch and online models, are based on a new family of lower bounds for which we show some theoretical properties. Moreover, we introduce two new heuristics outperforming the best existing heuristics. Algorithms, bounds and heuristics are tested in an extensive computational section. Finally, based on strong computational evidence, we conjecture the optimality of the ""Leveling"" heuristic in a special ""no information"" case, where at any retrieval stage, any of the remaining containers is equally likely to be retrieved next."|contain reloc problem crp concern find sequenc move contain minim number reloc need retriev contain respect given order retriev howev assumpt know full retriev order contain particular unrealist real oper paper studi stochast crp scrp relax assumpt new multi stage stochast model call batch model introduc motiv compar exist model onlin model two main contribut optim algorithm call prune best first search pbfs random approxim algorithm call pbfs approxim bound averag error algorithm applic batch onlin model base new famili lower bound show theoret properti moreov introduc two new heurist outperform best exist heurist algorithm bound heurist test extens comput section final base strong comput evid conjectur optim level heurist special inform case ani retriev stage ani remain contain equal like retriev next|['Virgile Galle', 'Setareh Borjian Boroujeni', 'Vahideh H. Manshadi', 'Cynthia Barnhart', 'Patrick Jaillet']|['cs.DS']
2017-03-28T14:09:47Z|2017-03-14T18:49:57Z|http://arxiv.org/abs/1703.04664v1|http://arxiv.org/pdf/1703.04664v1|Optimal Densification for Fast and Accurate Minwise Hashing|optim densif fast accur minwis hash|Minwise hashing is a fundamental and one of the most successful hashing algorithm in the literature. Recent advances based on the idea of densification~\cite{Proc:OneHashLSH_ICML14,Proc:Shrivastava_UAI14} have shown that it is possible to compute $k$ minwise hashes, of a vector with $d$ nonzeros, in mere $(d + k)$ computations, a significant improvement over the classical $O(dk)$. These advances have led to an algorithmic improvement in the query complexity of traditional indexing algorithms based on minwise hashing. Unfortunately, the variance of the current densification techniques is unnecessarily high, which leads to significantly poor accuracy compared to vanilla minwise hashing, especially when the data is sparse. In this paper, we provide a novel densification scheme which relies on carefully tailored 2-universal hashes. We show that the proposed scheme is variance-optimal, and without losing the runtime efficiency, it is significantly more accurate than existing densification techniques. As a result, we obtain a significantly efficient hashing scheme which has the same variance and collision probability as minwise hashing. Experimental evaluations on real sparse and high-dimensional datasets validate our claims. We believe that given the significant advantages, our method will replace minwise hashing implementations in practice.|minwis hash fundament one success hash algorithm literatur recent advanc base idea densif cite proc onehashlsh icml proc shrivastava uai shown possibl comput minwis hash vector nonzero mere comput signific improv classic dk advanc led algorithm improv queri complex tradit index algorithm base minwis hash unfortun varianc current densif techniqu unnecessarili high lead signific poor accuraci compar vanilla minwis hash especi data spars paper provid novel densif scheme reli care tailor univers hash show propos scheme varianc optim without lose runtim effici signific accur exist densif techniqu result obtain signific effici hash scheme varianc collis probabl minwis hash experiment evalu real spars high dimension dataset valid claim believ given signific advantag method replac minwis hash implement practic|['Anshumali Shrivastava']|['cs.DS', 'cs.LG']
2017-03-28T14:09:47Z|2017-03-13T16:18:01Z|http://arxiv.org/abs/1703.04466v1|http://arxiv.org/pdf/1703.04466v1|Bicriteria Rectilinear Shortest Paths among Rectilinear Obstacles in the   Plane|bicriteria rectilinear shortest path among rectilinear obstacl plane|Given a rectilinear domain $\mathcal{P}$ of $h$ pairwise-disjoint rectilinear obstacles with a total of $n$ vertices in the plane, we study the problem of computing bicriteria rectilinear shortest paths between two points $s$ and $t$ in $\mathcal{P}$. Three types of bicriteria rectilinear paths are considered: minimum-link shortest paths, shortest minimum-link paths, and minimum-cost paths where the cost of a path is a non-decreasing function of both the number of edges and the length of the path. The one-point and two-point path queries are also considered. Algorithms for these problems have been given previously. Our contributions are threefold. First, we find a critical error in all previous algorithms. Second, we correct the error in a not-so-trivial way. Third, we further improve the algorithms so that they are even faster than the previous (incorrect) algorithms when $h$ is relatively small. For example, for the minimum-link shortest paths, we obtain the following results. Our algorithm computes a minimum-link shortest $s$-$t$ path in $O(n+h\log^{3/2} h)$ time. For the one-point queries, we build a data structure of size $O(n+ h\log h)$ in $O(n+h\log^{3/2} h)$ time for a source point $s$, such that given any query point $t$, a minimum-link shortest $s$-$t$ path can be determined in $O(\log n)$ time. For the two-point queries, with $O(n+h^2\log^2 h)$ time and space preprocessing, a minimum-link shortest $s$-$t$ path can be determined in $O(\log n+\log^2 h)$ time for any two query points $s$ and $t$; alternatively, with $O(n+h^2\cdot \log^{2} h \cdot 4^{\sqrt{\log h}})$ time and $O(n+h^2\cdot \log h \cdot 4^{\sqrt{\log h}})$ space preprocessing, we can answer each two-point query in $O(\log n)$ time.|given rectilinear domain mathcal pairwis disjoint rectilinear obstacl total vertic plane studi problem comput bicriteria rectilinear shortest path two point mathcal three type bicriteria rectilinear path consid minimum link shortest path shortest minimum link path minimum cost path cost path non decreas function number edg length path one point two point path queri also consid algorithm problem given previous contribut threefold first find critic error previous algorithm second correct error trivial way third improv algorithm even faster previous incorrect algorithm relat small exampl minimum link shortest path obtain follow result algorithm comput minimum link shortest path log time one point queri build data structur size log log time sourc point given ani queri point minimum link shortest path determin log time two point queri log time space preprocess minimum link shortest path determin log log time ani two queri point altern cdot log cdot sqrt log time cdot log cdot sqrt log space preprocess answer two point queri log time|['Haitao Wang']|['cs.CG', 'cs.DS']
2017-03-28T14:09:47Z|2017-03-13T13:31:17Z|http://arxiv.org/abs/1703.04381v1|http://arxiv.org/pdf/1703.04381v1|On the Transformation Capability of Feasible Mechanisms for Programmable   Matter|transform capabl feasibl mechan programm matter|In this work, we study theoretical models of \emph{programmable matter} systems. The systems under consideration consist of spherical modules, kept together by magnetic forces and able to perform two minimal mechanical operations (or movements): \emph{rotate} around a neighbor and \emph{slide} over a line. In terms of modeling, there are $n$ nodes arranged in a 2-dimensional grid and forming some initial \emph{shape}. The goal is for the initial shape $A$ to \emph{transform} to some target shape $B$ by a sequence of movements. Most of the paper focuses on \emph{transformability} questions, meaning whether it is in principle feasible to transform a given shape to another. We first consider the case in which only rotation is available to the nodes. Our main result is that deciding whether two given shapes $A$ and $B$ can be transformed to each other, is in $\mathbf{P}$. We then insist on rotation only and impose the restriction that the nodes must maintain global connectivity throughout the transformation. We prove that the corresponding transformability question is in $\mathbf{PSPACE}$ and study the problem of determining the minimum \emph{seeds} that can make feasible, otherwise infeasible transformations. Next we allow both rotations and slidings and prove universality: any two connected shapes $A,B$ of the same order, can be transformed to each other without breaking connectivity. The worst-case number of movements of the generic strategy is $\Omega(n^2)$. We improve this to $O(n)$ parallel time, by a pipelining strategy, and prove optimality of both by matching lower bounds. In the last part of the paper, we turn our attention to distributed transformations. The nodes are now distributed processes able to perform communicate-compute-move rounds. We provide distributed algorithms for a general type of transformations.|work studi theoret model emph programm matter system system consider consist spheric modul kept togeth magnet forc abl perform two minim mechan oper movement emph rotat around neighbor emph slide line term model node arrang dimension grid form initi emph shape goal initi shape emph transform target shape sequenc movement paper focus emph transform question mean whether principl feasibl transform given shape anoth first consid case onli rotat avail node main result decid whether two given shape transform mathbf insist rotat onli impos restrict node must maintain global connect throughout transform prove correspond transform question mathbf pspace studi problem determin minimum emph seed make feasibl otherwis infeas transform next allow rotat slide prove univers ani two connect shape order transform without break connect worst case number movement generic strategi omega improv parallel time pipelin strategi prove optim match lower bound last part paper turn attent distribut transform node distribut process abl perform communic comput move round provid distribut algorithm general type transform|['Othon Michail', 'George Skretas', 'Paul G. Spirakis']|['cs.DS', 'cs.DC', 'cs.RO']
2017-03-28T14:09:47Z|2017-03-13T02:57:50Z|http://arxiv.org/abs/1703.04230v1|http://arxiv.org/pdf/1703.04230v1|Improved approximation algorithms for $k$-connected $m$-dominating set   problems|improv approxim algorithm connect domin set problem|A graph is $k$-connected if it has $k$ internally-disjoint paths between every pair of nodes. A subset $S$ of nodes in a graph $G$ is a $k$-connected set if the subgraph $G[S]$ induced by $S$ is $k$-connected; $S$ is an $m$-dominating set if every $v \in V \setminus S$ has at least $m$ neighbors in $S$. If $S$ is both $k$-connected and $m$-dominating then $S$ is a $k$-connected $m$-dominating set, or $(k,m)$-cds for short. In the $k$-Connected $m$-Dominating Set ($(k,m)$-CDS) problem the goal is to find a minimum weight $(k,m)$-cds in a node-weighted graph. We consider the case $m \geq k$ and obtain the following approximation ratios. For unit disc-graphs we obtain ratio $O(k\ln k)$, improving the previous ratio $O(k^2 \ln k)$. For general graphs we obtain the first non-trivial approximation ratio $O(k^2 \ln n)$.|graph connect intern disjoint path everi pair node subset node graph connect set subgraph induc connect domin set everi setminus least neighbor connect domin connect domin set cds short connect domin set cds problem goal find minimum weight cds node weight graph consid case geq obtain follow approxim ratio unit disc graph obtain ratio ln improv previous ratio ln general graph obtain first non trivial approxim ratio ln|['Zeev Nutov']|['cs.DS']
2017-03-28T14:09:47Z|2017-03-12T17:11:49Z|http://arxiv.org/abs/1703.04143v1|http://arxiv.org/pdf/1703.04143v1|Bernoulli Factories and Black-Box Reductions in Mechanism Design|bernoulli factori black box reduct mechan design|"We provide a polynomial time reduction from Bayesian incentive compatible mechanism design to Bayesian algorithm design for welfare maximization problems. Unlike prior results, our reduction achieves exact incentive compatibility for problems with multi-dimensional and continuous type spaces. The key technical barrier preventing exact incentive compatibility in prior black-box reductions is that repairing violations of incentive constraints requires understanding the distribution of the mechanism's output. Reductions that instead estimate the output distribution by sampling inevitably suffer from sampling error, which typically precludes exact incentive compatibility.   We overcome this barrier by employing and generalizing the computational model in the literature on Bernoulli Factories. In a Bernoulli factory problem, one is given a function mapping the bias of an ""input coin"" to that of an ""output coin"", and the challenge is to efficiently simulate the output coin given sample access to the input coin. We generalize this to the ""expectations from samples"" computational model, in which an instance is specified by a function mapping the expected values of a set of input distributions to a distribution over outcomes. The challenge is to give a polynomial time algorithm that exactly samples from the distribution over outcomes given only sample access to the input distributions. In this model, we give a polynomial time algorithm for the exponential weights: expected values of the input distributions correspond to the weights of alternatives and we wish to select an alternative with probability proportional to an exponential function of its weight. This algorithm is the key ingredient in designing an incentive compatible mechanism for bipartite matching, which can be used to make the approximately incentive compatible reduction of Hartline et al. (2015) exactly incentive compatible."|provid polynomi time reduct bayesian incent compat mechan design bayesian algorithm design welfar maxim problem unlik prior result reduct achiev exact incent compat problem multi dimension continu type space key technic barrier prevent exact incent compat prior black box reduct repair violat incent constraint requir understand distribut mechan output reduct instead estim output distribut sampl inevit suffer sampl error typic preclud exact incent compat overcom barrier employ general comput model literatur bernoulli factori bernoulli factori problem one given function map bias input coin output coin challeng effici simul output coin given sampl access input coin general expect sampl comput model instanc specifi function map expect valu set input distribut distribut outcom challeng give polynomi time algorithm exact sampl distribut outcom given onli sampl access input distribut model give polynomi time algorithm exponenti weight expect valu input distribut correspond weight altern wish select altern probabl proport exponenti function weight algorithm key ingredi design incent compat mechan bipartit match use make approxim incent compat reduct hartlin et al exact incent compat|['Shaddin Dughmi', 'Jason Hartline', 'Robert Kleinberg', 'Rad Niazadeh']|['cs.GT', 'cs.CC', 'cs.DS', 'math.PR']
2017-03-28T14:09:47Z|2017-03-11T23:16:23Z|http://arxiv.org/abs/1703.04040v1|http://arxiv.org/abs/1703.04040v1|Locality-sensitive hashing of curves|local sensit hash curv|We study data structures for storing a set of polygonal curves in ${\rm R}^d$ such that, given a query curve, we can efficiently retrieve similar curves from the set, where similarity is measured using the discrete Fr\'echet distance or the dynamic time warping distance. To this end we devise the first locality-sensitive hashing schemes for these distance measures. A major challenge is posed by the fact that these distance measures internally optimize the alignment between the curves. We give solutions for different types of alignments including constrained and unconstrained versions. For unconstrained alignments, we improve over a result by Indyk from 2002 for short curves. Let $n$ be the number of input curves and let $m$ be the maximum complexity of a curve in the input. In the particular case where $m \leq \frac{\alpha}{4d} \log n$, for some fixed $\alpha>0$, our solutions imply an approximate near-neighbor data structure for the discrete Fr\'echet distance that uses space in $O(n^{1+\alpha}\log n)$ and achieves query time in $O(n^{\alpha}\log^2 n)$ and constant approximation factor. Furthermore, our solutions provide a trade-off between approximation quality and computational performance: for any parameter $k \in [m]$, we can give a data structure that uses space in $O(2^{2k}m^{k-1} n \log n + nm)$, answers queries in $O( 2^{2k} m^{k}\log n)$ time and achieves approximation factor in $O(m/k)$.|studi data structur store set polygon curv rm given queri curv effici retriev similar curv set similar measur use discret fr echet distanc dynam time warp distanc end devis first local sensit hash scheme distanc measur major challeng pose fact distanc measur intern optim align curv give solut differ type align includ constrain unconstrain version unconstrain align improv result indyk short curv let number input curv let maximum complex curv input particular case leq frac alpha log fix alpha solut impli approxim near neighbor data structur discret fr echet distanc use space alpha log achiev queri time alpha log constant approxim factor furthermor solut provid trade approxim qualiti comput perform ani paramet give data structur use space log nm answer queri log time achiev approxim factor|['Anne Driemel', 'Francesco Silvestri']|['cs.CG', 'cs.DS', 'cs.IR', 'F.2.2']
2017-03-28T14:09:47Z|2017-03-11T16:53:04Z|http://arxiv.org/abs/1703.03998v1|http://arxiv.org/pdf/1703.03998v1|The Weighted Matching Approach to Maximum Cardinality Matching|weight match approach maximum cardin match|Several papers have achieved time $O(\sqrt n m)$ for cardinality matching, starting from first principles. This results in a long derivation. We simplify the task by employing well-known concepts for maximum weight matching. We use Edmonds' algorithm to derive the structure of shortest augmenting paths. We extend this to a complete algorithm for maximum cardinality matching in time $O(\sqrt n m)$.|sever paper achiev time sqrt cardin match start first principl result long deriv simplifi task employ well known concept maximum weight match use edmond algorithm deriv structur shortest augment path extend complet algorithm maximum cardin match time sqrt|['Harold N. Gabow']|['cs.DS']
2017-03-28T14:09:47Z|2017-03-11T12:24:19Z|http://arxiv.org/abs/1703.03963v1|http://arxiv.org/pdf/1703.03963v1|On Solving Travelling Salesman Problem with Vertex Requisitions|solv travel salesman problem vertex requisit|We consider the Travelling Salesman Problem with Vertex Requisitions, where for each position of the tour at most two possible vertices are given. It is known that the problem is strongly NP-hard. The proposed algorithm for this problem has less time complexity compared to the previously known one. In particular, almost all feasible instances of the problem are solvable in O(n) time using the new algorithm, where n is the number of vertices. The developed approach also helps in fast enumeration of a neighborhood in the local search and yields an integer programming model with O(n) binary variables for the problem.|consid travel salesman problem vertex requisit posit tour two possibl vertic given known problem strong np hard propos algorithm problem less time complex compar previous known one particular almost feasibl instanc problem solvabl time use new algorithm number vertic develop approach also help fast enumer neighborhood local search yield integ program model binari variabl problem|['Anton Eremeev', 'Yulia Kovalenko']|['cs.DS']
2017-03-28T14:09:47Z|2017-03-11T03:28:30Z|http://arxiv.org/abs/1703.03900v1|http://arxiv.org/pdf/1703.03900v1|Core Maintenance in Dynamic Graphs: A Parallel Approach based on   Matching|core mainten dynam graph parallel approach base match|The core number of a vertex is a basic index depicting cohesiveness of a graph, and has been widely used in large-scale graph analytics. In this paper, we study the update of core numbers of vertices in dynamic graphs with edge insertions/deletions, which is known as the core maintenance problem. Different from previous approaches that just focus on the case of single-edge insertion/deletion and sequentially handle the edges when multiple edges are inserted/deleted, we investigate the parallelism in the core maintenance procedure. Specifically, we show that if the inserted/deleted edges constitute a matching, the core number update with respect to each inserted/deleted edge can be handled in parallel. Based on this key observation, we propose parallel algorithms for core maintenance in both cases of edge insertions and deletions. We conduct extensive experiments to evaluate the efficiency, stability, parallelism and scalability of our algorithms on different types of real-world and synthetic graphs. Comparing with sequential approaches, our algorithms can improve the core maintenance efficiency significantly.|core number vertex basic index depict cohes graph wide use larg scale graph analyt paper studi updat core number vertic dynam graph edg insert delet known core mainten problem differ previous approach focus case singl edg insert delet sequenti handl edg multipl edg insert delet investig parallel core mainten procedur specif show insert delet edg constitut match core number updat respect insert delet edg handl parallel base key observ propos parallel algorithm core mainten case edg insert delet conduct extens experi evalu effici stabil parallel scalabl algorithm differ type real world synthet graph compar sequenti approach algorithm improv core mainten effici signific|['Na Wang', 'Dongxiao Yu', 'Hai Jin', 'Qiang-Sheng Hua', 'Xuanhua Shi', 'Xia Xie']|['cs.DS']
2017-03-28T14:09:51Z|2017-03-11T02:10:17Z|http://arxiv.org/abs/1703.06113v1|http://arxiv.org/pdf/1703.06113v1|Toward an enumeration of unlabeled trees|toward enumer unlabel tree|We present an algorithm that, on input $n$, lists every unlabeled tree of order $n$.|present algorithm input list everi unlabel tree order|['Pedro Recuero']|['cs.DS', 'math.CO', '05C05']
2017-03-28T14:09:51Z|2017-03-10T22:25:56Z|http://arxiv.org/abs/1703.03859v1|http://arxiv.org/abs/1703.03859v1|Markov Chain Lifting and Distributed ADMM|markov chain lift distribut admm|The time to converge to the steady state of a finite Markov chain can be greatly reduced by a lifting operation, which creates a new Markov chain on an expanded state space. For a class of quadratic objectives, we show an analogous behavior where a distributed ADMM algorithm can be seen as a lifting of Gradient Descent algorithm. This provides a deep insight for its faster convergence rate under optimal parameter tuning. We conjecture that this gain is always present, as opposed to the lifting of a Markov chain which sometimes only provides a marginal speedup.|time converg steadi state finit markov chain great reduc lift oper creat new markov chain expand state space class quadrat object show analog behavior distribut admm algorithm seen lift gradient descent algorithm provid deep insight faster converg rate optim paramet tune conjectur gain alway present oppos lift markov chain sometim onli provid margin speedup|['Guilherme França', 'José Bento']|['stat.ML', 'cs.DS', 'cs.IT', 'cs.LG', 'math.IT', 'math.OC']
2017-03-28T14:09:51Z|2017-03-10T21:51:50Z|http://arxiv.org/abs/1703.03849v1|http://arxiv.org/pdf/1703.03849v1|A note on approximate strengths of edges in a hypergraph|note approxim strength edg hypergraph|Let $H=(V,E)$ be an edge-weighted hypergraph of rank $r$. Kogan and Krauthgamer extended Bencz\'{u}r and Karger's random sampling scheme for cut sparsification from graphs to hypergraphs. The sampling requires an algorithm for computing the approximate strengths of edges. In this note we extend the algorithm for graphs to hypergraphs and describe a near-linear time algorithm to compute approximate strengths of edges; we build on a sparsification result for hypergraphs from our recent work. Combined with prior results we obtain faster algorithms for finding $(1+\epsilon)$-approximate mincuts when the rank of the hypergraph is small.|let edg weight hypergraph rank kogan krauthgam extend bencz karger random sampl scheme cut sparsif graph hypergraph sampl requir algorithm comput approxim strength edg note extend algorithm graph hypergraph describ near linear time algorithm comput approxim strength edg build sparsif result hypergraph recent work combin prior result obtain faster algorithm find epsilon approxim mincut rank hypergraph small|['Chandra Chekuri', 'Chao Xu']|['cs.DS']
2017-03-28T14:09:51Z|2017-03-10T09:58:40Z|http://arxiv.org/abs/1703.03603v1|http://arxiv.org/pdf/1703.03603v1|The Densest Subgraph Problem with a Convex/Concave Size Function|densest subgraph problem convex concav size function|In the densest subgraph problem, given an edge-weighted undirected graph $G=(V,E,w)$, we are asked to find $S\subseteq V$ that maximizes the density, i.e., $w(S)/ S $, where $w(S)$ is the sum of weights of the edges in the subgraph induced by $S$. This problem has often been employed in a wide variety of graph mining applications. However, the problem has a drawback; it may happen that the obtained subset is too large or too small in comparison with the size desired in the application at hand. In this study, we address the size issue of the densest subgraph problem by generalizing the density of $S\subseteq V$. Specifically, we introduce the $f$-density of $S\subseteq V$, which is defined as $w(S)/f( S )$, where $f:\mathbb{Z}_{\geq 0}\rightarrow \mathbb{R}_{\geq 0}$ is a monotonically non-decreasing function. In the $f$-densest subgraph problem ($f$-DS), we aim to find $S\subseteq V$ that maximizes the $f$-density $w(S)/f( S )$. Although $f$-DS does not explicitly specify the size of the output subset of vertices, we can handle the above size issue using a convex/concave size function $f$ appropriately. For $f$-DS with convex function $f$, we propose a nearly-linear-time algorithm with a provable approximation guarantee. On the other hand, for $f$-DS with concave function $f$, we propose an LP-based exact algorithm, a flow-based $O( V ^3)$-time exact algorithm for unweighted graphs, and a nearly-linear-time approximation algorithm.|densest subgraph problem given edg weight undirect graph ask find subseteq maxim densiti sum weight edg subgraph induc problem often employ wide varieti graph mine applic howev problem drawback may happen obtain subset larg small comparison size desir applic hand studi address size issu densest subgraph problem general densiti subseteq specif introduc densiti subseteq defin mathbb geq rightarrow mathbb geq monoton non decreas function densest subgraph problem ds aim find subseteq maxim densiti although ds doe explicit specifi size output subset vertic handl abov size issu use convex concav size function appropri ds convex function propos near linear time algorithm provabl approxim guarante hand ds concav function propos lp base exact algorithm flow base time exact algorithm unweight graph near linear time approxim algorithm|['Yasushi Kawase', 'Atsushi Miyauchi']|['cs.DS', 'cs.DM', 'cs.SI']
2017-03-28T14:09:51Z|2017-03-10T08:35:24Z|http://arxiv.org/abs/1703.03575v1|http://arxiv.org/pdf/1703.03575v1|Crossing the Logarithmic Barrier for Dynamic Boolean Data Structure   Lower Bounds|cross logarithm barrier dynam boolean data structur lower bound|"This paper proves the first super-logarithmic lower bounds on the cell probe complexity of dynamic boolean (a.k.a. decision) data structure problems, a long-standing milestone in data structure lower bounds.   We introduce a new method for proving dynamic cell probe lower bounds and use it to prove a $\tilde{\Omega}(\log^{1.5} n)$ lower bound on the operational time of a wide range of boolean data structure problems, most notably, on the query time of dynamic range counting over $\mathbb{F}_2$ ([Pat07]). Proving an $\omega(\lg n)$ lower bound for this problem was explicitly posed as one of five important open problems in the late Mihai P\v{a}tra\c{s}cu's obituary [Tho13]. This result also implies the first $\omega(\lg n)$ lower bound for the classical 2D range counting problem, one of the most fundamental data structure problems in computational geometry and spatial databases. We derive similar lower bounds for boolean versions of dynamic polynomial evaluation and 2D rectangle stabbing, and for the (non-boolean) problems of range selection and range median.   Our technical centerpiece is a new way of ""weakly"" simulating dynamic data structures using efficient one-way communication protocols with small advantage over random guessing. This simulation involves a surprising excursion to low-degree (Chebychev) polynomials which may be of independent interest, and offers an entirely new algorithmic angle on the ""cell sampling"" method of Panigrahy et al. [PTW10]."|paper prove first super logarithm lower bound cell probe complex dynam boolean decis data structur problem long stand mileston data structur lower bound introduc new method prove dynam cell probe lower bound use prove tild omega log lower bound oper time wide rang boolean data structur problem notabl queri time dynam rang count mathbb pat prove omega lg lower bound problem explicit pose one five import open problem late mihai tra cu obituari tho result also impli first omega lg lower bound classic rang count problem one fundament data structur problem comput geometri spatial databas deriv similar lower bound boolean version dynam polynomi evalu rectangl stab non boolean problem rang select rang median technic centerpiec new way weak simul dynam data structur use effici one way communic protocol small advantag random guess simul involv surpris excurs low degre chebychev polynomi may independ interest offer entir new algorithm angl cell sampl method panigrahi et al ptw|['Kasper Green Larsen', 'Omri Weinstein', 'Huacheng Yu']|['cs.DS', 'cs.CC', 'cs.CG', 'cs.IT', 'math.IT']
2017-03-28T14:09:51Z|2017-03-09T22:47:10Z|http://arxiv.org/abs/1703.03484v1|http://arxiv.org/pdf/1703.03484v1|Combinatorial Auctions with Online XOS Bidders|combinatori auction onlin xos bidder|In combinatorial auctions, a designer must decide how to allocate a set of indivisible items amongst a set of bidders. Each bidder has a valuation function which gives the utility they obtain from any subset of the items. Our focus is specifically on welfare maximization, where the objective is to maximize the sum of valuations that the bidders place on the items that they were allocated (the valuation functions are assumed to be reported truthfully). We analyze an online problem in which the algorithm is not given the set of bidders in advance. Instead, the bidders are revealed sequentially in a uniformly random order, similarly to secretary problems. The algorithm must make an irrevocable decision about which items to allocate to the current bidder before the next one is revealed. When the valuation functions lie in the class $XOS$ (which includes submodular functions), we provide a black box reduction from offline to online optimization. Specifically, given an $\alpha$-approximation algorithm for offline welfare maximization, we show how to create a $(0.199 \alpha)$-approximation algorithm for the online problem. Our algorithm draws on connections to secretary problems; in fact, we show that the online welfare maximization problem itself can be viewed as a particular kind of secretary problem with nonuniform arrival order.|combinatori auction design must decid alloc set indivis item amongst set bidder bidder valuat function give util obtain ani subset item focus specif welfar maxim object maxim sum valuat bidder place item alloc valuat function assum report truth analyz onlin problem algorithm given set bidder advanc instead bidder reveal sequenti uniform random order similar secretari problem algorithm must make irrevoc decis item alloc current bidder befor next one reveal valuat function lie class xos includ submodular function provid black box reduct offlin onlin optim specif given alpha approxim algorithm offlin welfar maxim show creat alpha approxim algorithm onlin problem algorithm draw connect secretari problem fact show onlin welfar maxim problem view particular kind secretari problem nonuniform arriv order|['Shaddin Dughmi', 'Bryan Wilder']|['cs.GT', 'cs.DS']
2017-03-28T14:09:51Z|2017-03-09T15:46:25Z|http://arxiv.org/abs/1703.03304v1|http://arxiv.org/pdf/1703.03304v1|On low rank-width colorings|low rank width color|We introduce the concept of low rank-width colorings, generalising the notion of low tree-depth colorings introduced by Ne\v{s}et\v{r}il and Ossona de Mendez in [Grad and classes with bounded expansion I. Decompositions. EJC, 2008]. We say that a class $\mathcal{C}$ of graphs admits low rank-width colourings if there exist functions $N\colon \mathbb{N}\rightarrow\mathbb{N}$ and $Q\colon \mathbb{N}\rightarrow\mathbb{N}$ such that for all $p\in \mathbb{N}$, every graph $G\in \mathcal{C}$ can be vertex colored with at most $N(p)$ colors such that the union of any $i\leq p$ color classes induces a subgraph of rank-width at most $Q(i)$.   Graph classes admitting low rank-width colorings strictly generalize graph classes admitting low tree-depth colorings and graph classes of bounded rank-width. We prove that for every graph class $\mathcal{C}$ of bounded expansion and every positive integer $r$, the class $\{G^r\colon G\in \mathcal{C}\}$ of $r$th powers of graphs from $\mathcal{C}$, as well as the classes of unit interval graphs and bipartite permutation graphs admit low rank-width colorings. All of these classes have unbounded rank-width and do not admit low tree-depth colorings. We also show that the classes of interval graphs and permutation graphs do not admit low rank-width colorings. As interesting side properties, we prove that every graph class admitting low rank-width colorings has the Erd\H{o}s-Hajnal property and is $\chi$-bounded.|introduc concept low rank width color generalis notion low tree depth color introduc ne et il ossona de mendez grad class bound expans decomposit ejc say class mathcal graph admit low rank width colour exist function colon mathbb rightarrow mathbb colon mathbb rightarrow mathbb mathbb everi graph mathcal vertex color color union ani leq color class induc subgraph rank width graph class admit low rank width color strict general graph class admit low tree depth color graph class bound rank width prove everi graph class mathcal bound expans everi posit integ class colon mathcal th power graph mathcal well class unit interv graph bipartit permut graph admit low rank width color class unbound rank width admit low tree depth color also show class interv graph permut graph admit low rank width color interest side properti prove everi graph class admit low rank width color erd hajnal properti chi bound|['O-joung Kwon', 'Michał Pilipczuk', 'Sebastian Siebertz']|['cs.DS', 'math.CO']
2017-03-28T14:09:51Z|2017-03-09T06:08:01Z|http://arxiv.org/abs/1703.03147v1|http://arxiv.org/pdf/1703.03147v1|Juggling Functions Inside a Database|juggl function insid databas|"We define and study the Functional Aggregate Query (FAQ) problem, which captures common computational tasks across a very wide range of domains including relational databases, logic, matrix and tensor computation, probabilistic graphical models, constraint satisfaction, and signal processing. Simply put, an FAQ is a declarative way of defining a new function from a database of input functions.   We present ""InsideOut"", a dynamic programming algorithm, to evaluate an FAQ. The algorithm rewrites the input query into a set of easier-to-compute FAQ sub-queries. Each sub-query is then evaluated using a worst-case optimal relational join algorithm. The topic of designing algorithms to optimally evaluate the classic multiway join problem has seen exciting developments in the past few years. Our framework tightly connects these new ideas in database theory with a vast number of application areas in a coherent manner, showing potentially that a good database engine can be a general-purpose constraint solver, relational data store, graphical model inference engine, and matrix/tensor computation processor all at once.   The InsideOut algorithm is very simple, as shall be described in this paper. Yet, in spite of solving an extremely general problem, its runtime either is as good as or improves upon the best known algorithm for the applications that FAQ specializes to. These corollaries include computational tasks in graphical model inference, matrix/tensor operations, relational joins, and logic. Better yet, InsideOut can be used within any database engine, because it is basically a principled way of rewriting queries. Indeed, it is already part of the LogicBlox database engine, helping efficiently answer traditional database queries, graphical model inference queries, and train a large class of machine learning models inside the database itself."|defin studi function aggreg queri faq problem captur common comput task across veri wide rang domain includ relat databas logic matrix tensor comput probabilist graphic model constraint satisfact signal process simpli put faq declar way defin new function databas input function present insideout dynam program algorithm evalu faq algorithm rewrit input queri set easier comput faq sub queri sub queri evalu use worst case optim relat join algorithm topic design algorithm optim evalu classic multiway join problem seen excit develop past year framework tight connect new idea databas theori vast number applic area coher manner show potenti good databas engin general purpos constraint solver relat data store graphic model infer engin matrix tensor comput processor onc insideout algorithm veri simpl shall describ paper yet spite solv extrem general problem runtim either good improv upon best known algorithm applic faq special corollari includ comput task graphic model infer matrix tensor oper relat join logic better yet insideout use within ani databas engin becaus basic principl way rewrit queri inde alreadi part logicblox databas engin help effici answer tradit databas queri graphic model infer queri train larg class machin learn model insid databas|['Mahmoud Abo Khamis', 'Hung Q. Ngo', 'Atri Rudra']|['cs.DB', 'cs.DS', 'cs.LO']
2017-03-28T14:09:51Z|2017-03-08T21:50:06Z|http://arxiv.org/abs/1703.03048v1|http://arxiv.org/pdf/1703.03048v1|Quickest Visibility Queries in Polygonal Domains|quickest visibl queri polygon domain|Let $s$ be a point in a polygonal domain $\mathcal{P}$ of $h-1$ holes and $n$ vertices. We consider a quickest visibility query problem. Given a query point $q$ in $\mathcal{P}$, the goal is to find a shortest path in $\mathcal{P}$ to move from $s$ to see $q$ as quickly as possible. Previously, Arkin et al. (SoCG 2015) built a data structure of size $O(n^22^{\alpha(n)}\log n)$ that can answer each query in $O(K\log^2 n)$ time, where $\alpha(n)$ is the inverse Ackermann function and $K$ is the size of the visibility polygon of $q$ in $\mathcal{P}$ (and $K$ can be $\Theta(n)$ in the worst case). In this paper, we present a new data structure of size $O(n\log h + h^2)$ that can answer each query in $O(h\log h\log n)$ time. Our result improves the previous work when $h$ is relatively small. In particular, if $h$ is a constant, then our result even matches the best result for the simple polygon case (i.e., $h=1$), which is optimal. As a by-product, we also have a new algorithm for a shortest-path-to-segment query problem. Given a query line segment $\tau$ in $\mathcal{P}$, the query seeks a shortest path from $s$ to all points of $\tau$. Previously, Arkin et al. gave a data structure of size $O(n^22^{\alpha(n)}\log n)$ that can answer each query in $O(\log^2 n)$ time, and another data structure of size $O(n^3\log n)$ with $O(\log n)$ query time. We present a data structure of size $O(n)$ with query time $O(h\log \frac{n}{h})$, which also favors small values of $h$ and is optimal when $h=O(1)$.|let point polygon domain mathcal hole vertic consid quickest visibl queri problem given queri point mathcal goal find shortest path mathcal move see quick possibl previous arkin et al socg built data structur size alpha log answer queri log time alpha invers ackermann function size visibl polygon mathcal theta worst case paper present new data structur size log answer queri log log time result improv previous work relat small particular constant result even match best result simpl polygon case optim product also new algorithm shortest path segment queri problem given queri line segment tau mathcal queri seek shortest path point tau previous arkin et al gave data structur size alpha log answer queri log time anoth data structur size log log queri time present data structur size queri time log frac also favor small valu optim|['Haitao Wang']|['cs.CG', 'cs.DS']
2017-03-28T14:09:51Z|2017-03-08T15:16:11Z|http://arxiv.org/abs/1703.02867v1|http://arxiv.org/pdf/1703.02867v1|Electoral District Design via Constrained Clustering|elector district design via constrain cluster|The paper studies the electoral district design problem where municipalities of a state have to be grouped into districts of nearly equal population while obeying certain politically motivated requirements. We develop a general framework for electoral district design that is based on the close connection of constrained geometric clustering and diagrams. The approach is computationally efficient and flexible enough to pursue various conflicting juridical demands for the shape of the districts. We demonstrate the practicability of our methodology for electoral districting in Germany.|paper studi elector district design problem municip state group district near equal popul obey certain polit motiv requir develop general framework elector district design base close connect constrain geometr cluster diagram approach comput effici flexibl enough pursu various conflict jurid demand shape district demonstr practic methodolog elector district germani|['Andreas Brieden', 'Peter Gritzmann', 'Fabian Klemm']|['cs.DS', 'math.CO', '90C90']
2017-03-28T14:09:55Z|2017-03-08T15:16:05Z|http://arxiv.org/abs/1703.02866v1|http://arxiv.org/pdf/1703.02866v1|The Half-integral Erdös-Pósa Property for Non-null Cycles|half integr erd sa properti non null cycl|"A Group Labeled Graph is a pair $(G,\Lambda)$ where $G$ is an oriented graph and $\Lambda$ is a mapping from the arcs of $G$ to elements of a group. A (not necessarily directed) cycle $C$ is called non-null if for any cyclic ordering of the arcs in $C$, the group element obtained by `adding' the labels on forward arcs and `subtracting' the labels on reverse arcs is not the identity element of the group. Non-null cycles in group labeled graphs generalize several well-known graph structures, including odd cycles.   In this paper, we prove that non-null cycles on Group Labeled Graphs have the half-integral Erd\""os-P\'osa property. That is, there is a function $f:{\mathbb N}\to {\mathbb N}$ such that for any $k\in {\mathbb N}$, any group labeled graph $(G,\Lambda)$ has a set of $k$ non-null cycles such that each vertex of $G$ appears in at most two of these cycles or there is a set of at most $f(k)$ vertices that intersects every non-null cycle. Since it is known that non-null cycles do not have the integeral Erd\""os-P\'osa property in general, a half-integral Erd\""os-P\'osa result is the best one could hope for."|group label graph pair lambda orient graph lambda map arc element group necessarili direct cycl call non null ani cyclic order arc group element obtain ad label forward arc subtract label revers arc ident element group non null cycl group label graph general sever well known graph structur includ odd cycl paper prove non null cycl group label graph half integr erd os osa properti function mathbb mathbb ani mathbb ani group label graph lambda set non null cycl vertex appear two cycl set vertic intersect everi non null cycl sinc known non null cycl integer erd os osa properti general half integr erd os osa result best one could hope|['Daniel Lokshtanov', 'M. S. Ramanujan', 'Saket Saurabh']|['cs.DM', 'cs.DS']
2017-03-28T14:09:55Z|2017-03-08T10:56:03Z|http://arxiv.org/abs/1703.02784v1|http://arxiv.org/pdf/1703.02784v1|$K$-Best Solutions of MSO Problems on Tree-Decomposable Graphs|best solut mso problem tree decompos graph|We show that, for any graph optimization problem in which the feasible solutions can be expressed by a formula in monadic second-order logic describing sets of vertices or edges and in which the goal is to minimize the sum of the weights in the selected sets, we can find the $k$ best solutions for $n$-vertex graphs of bounded treewidth in time $\mathcal O(n+k\log n)$. In particular, this applies to the problem of finding the $k$ shortest simple paths between given vertices in directed graphs of bounded treewidth, giving an exponential speedup in the per-path cost over previous algorithms.|show ani graph optim problem feasibl solut express formula monad second order logic describ set vertic edg goal minim sum weight select set find best solut vertex graph bound treewidth time mathcal log particular appli problem find shortest simpl path given vertic direct graph bound treewidth give exponenti speedup per path cost previous algorithm|['David Eppstein', 'Denis Kurz']|['cs.DS', 'G.2.2']
2017-03-28T14:09:55Z|2017-03-08T04:18:58Z|http://arxiv.org/abs/1703.02693v1|http://arxiv.org/pdf/1703.02693v1|Stream Aggregation Through Order Sampling|stream aggreg order sampl|This paper introduces a new single-pass reservoir weighted-sampling stream aggregation algorithm, Priority Sample and Hold. PrSH combines aspects of the well-known Sample and Hold algorithm with Priority Sampling. In particular, it achieves a reduced computational cost for rate adaptation in a fixed cache by using a single persistent random variable across the lifetime of each key in the cache. The basic approach can be supplemented with a Sample and Hold pre-sampling stage with a sampling rate adaptation controlled by PrSH. We prove that PrSH provides unbiased estimates of the true aggregates. We analyze the computational complexity of PrSH and its variants, and provide a detailed evaluation of its accuracy on synthetic and trace data. Weighted relative error is reduced by 40% to 65% at sampling rates of 5% to 17%, relative to Adaptive Sample and Hold; there is also substantial improvement for rank queries.|paper introduc new singl pass reservoir weight sampl stream aggreg algorithm prioriti sampl hold prsh combin aspect well known sampl hold algorithm prioriti sampl particular achiev reduc comput cost rate adapt fix cach use singl persist random variabl across lifetim key cach basic approach supplement sampl hold pre sampl stage sampl rate adapt control prsh prove prsh provid unbias estim true aggreg analyz comput complex prsh variant provid detail evalu accuraci synthet trace data weight relat error reduc sampl rate relat adapt sampl hold also substanti improv rank queri|['Nick Duffield', 'Yunhong Xu', 'Liangzhen Xia', 'Nesreen Ahmed', 'Minlan Yu']|['cs.DS']
2017-03-28T14:09:55Z|2017-03-08T03:56:27Z|http://arxiv.org/abs/1703.02690v1|http://arxiv.org/pdf/1703.02690v1|Leveraging Sparsity for Efficient Submodular Data Summarization|leverag sparsiti effici submodular data summar|The facility location problem is widely used for summarizing large datasets and has additional applications in sensor placement, image retrieval, and clustering. One difficulty of this problem is that submodular optimization algorithms require the calculation of pairwise benefits for all items in the dataset. This is infeasible for large problems, so recent work proposed to only calculate nearest neighbor benefits. One limitation is that several strong assumptions were invoked to obtain provable approximation guarantees. In this paper we establish that these extra assumptions are not necessary---solving the sparsified problem will be almost optimal under the standard assumptions of the problem. We then analyze a different method of sparsification that is a better model for methods such as Locality Sensitive Hashing to accelerate the nearest neighbor computations and extend the use of the problem to a broader family of similarities. We validate our approach by demonstrating that it rapidly generates interpretable summaries.|facil locat problem wide use summar larg dataset addit applic sensor placement imag retriev cluster one difficulti problem submodular optim algorithm requir calcul pairwis benefit item dataset infeas larg problem recent work propos onli calcul nearest neighbor benefit one limit sever strong assumpt invok obtain provabl approxim guarante paper establish extra assumpt necessari solv sparsifi problem almost optim standard assumpt problem analyz differ method sparsif better model method local sensit hash acceler nearest neighbor comput extend use problem broader famili similar valid approach demonstr rapid generat interpret summari|['Erik M. Lindgren', 'Shanshan Wu', 'Alexandros G. Dimakis']|['stat.ML', 'cs.DS', 'cs.IT', 'cs.LG', 'math.IT']
2017-03-28T14:09:55Z|2017-03-08T03:55:27Z|http://arxiv.org/abs/1703.02689v1|http://arxiv.org/pdf/1703.02689v1|Exact MAP Inference by Avoiding Fractional Vertices|exact map infer avoid fraction vertic|Given a graphical model, one essential problem is MAP inference, that is, finding the most likely configuration of states according to the model. Although this problem is NP-hard, large instances can be solved in practice. A major open question is to explain why this is true. We give a natural condition under which we can provably perform MAP inference in polynomial time. We require that the number of fractional vertices in the LP relaxation exceeding the optimal solution is bounded by a polynomial in the problem size. This resolves an open question by Dimakis, Gohari, and Wainwright. In contrast, for general LP relaxations of integer programs, known techniques can only handle a constant number of fractional vertices whose value exceeds the optimal solution. We experimentally verify this condition and demonstrate how efficient various integer programming methods are at removing fractional solutions.|given graphic model one essenti problem map infer find like configur state accord model although problem np hard larg instanc solv practic major open question explain whi true give natur condit provabl perform map infer polynomi time requir number fraction vertic lp relax exceed optim solut bound polynomi problem size resolv open question dimaki gohari wainwright contrast general lp relax integ program known techniqu onli handl constant number fraction vertic whose valu exceed optim solut experiment verifi condit demonstr effici various integ program method remov fraction solut|['Erik M. Lindgren', 'Alexandros G. Dimakis', 'Adam Klivans']|['stat.ML', 'cs.DS', 'cs.IT', 'cs.LG', 'math.IT']
2017-03-28T14:09:55Z|2017-03-07T22:18:35Z|http://arxiv.org/abs/1703.02625v1|http://arxiv.org/pdf/1703.02625v1|On Sampling from Massive Graph Streams|sampl massiv graph stream|We propose Graph Priority Sampling (GPS), a new paradigm for order-based reservoir sampling from massive streams of graph edges. GPS provides a general way to weight edge sampling according to auxiliary and/or size variables so as to accomplish various estimation goals of graph properties. In the context of subgraph counting, we show how edge sampling weights can be chosen so as to minimize the estimation variance of counts of specified sets of subgraphs. In distinction with many prior graph sampling schemes, GPS separates the functions of edge sampling and subgraph estimation. We propose two estimation frameworks: (1) Post-Stream estimation, to allow GPS to construct a reference sample of edges to support retrospective graph queries, and (2) In-Stream estimation, to allow GPS to obtain lower variance estimates by incrementally updating the subgraph count estimates during stream processing. Unbiasedness of subgraph estimators is established through a new Martingale formulation of graph stream order sampling, which shows that subgraph estimators, written as a product of constituent edge estimators are unbiased, even when computed at different points in the stream. The separation of estimation and sampling enables significant resource savings relative to previous work. We illustrate our framework with applications to triangle and wedge counting. We perform a large-scale experimental study on real-world graphs from various domains and types. GPS achieves high accuracy with less than 1% error for triangle and wedge counting, while storing a small fraction of the graph with average update times of a few microseconds per edge. Notably, for a large Twitter graph with more than 260M edges, GPS accurately estimates triangle counts with less than 1% error, while storing only 40K edges.|propos graph prioriti sampl gps new paradigm order base reservoir sampl massiv stream graph edg gps provid general way weight edg sampl accord auxiliari size variabl accomplish various estim goal graph properti context subgraph count show edg sampl weight chosen minim estim varianc count specifi set subgraph distinct mani prior graph sampl scheme gps separ function edg sampl subgraph estim propos two estim framework post stream estim allow gps construct refer sampl edg support retrospect graph queri stream estim allow gps obtain lower varianc estim increment updat subgraph count estim dure stream process unbiased subgraph estim establish new martingal formul graph stream order sampl show subgraph estim written product constitu edg estim unbias even comput differ point stream separ estim sampl enabl signific resourc save relat previous work illustr framework applic triangl wedg count perform larg scale experiment studi real world graph various domain type gps achiev high accuraci less error triangl wedg count store small fraction graph averag updat time microsecond per edg notabl larg twitter graph edg gps accur estim triangl count less error store onli edg|['Nesreen K. Ahmed', 'Nick Duffield', 'Theodore Willke', 'Ryan A. Rossi']|['cs.SI', 'cs.DS', 'cs.IR', 'math.ST', 'stat.TH']
2017-03-28T14:09:55Z|2017-03-07T17:35:51Z|http://arxiv.org/abs/1703.02485v1|http://arxiv.org/pdf/1703.02485v1|Certifying coloring algorithms for graphs without long induced paths|certifi color algorithm graph without long induc path|Let $P_k$ be a path, $C_k$ a cycle on $k$ vertices, and $K_{k,k}$ a complete bipartite graph with $k$ vertices on each side of the bipartition. We prove that (1) for any integers $k, t>0$ and a graph $H$ there are finitely many subgraph minimal graphs with no induced $P_k$ and $K_{t,t}$ that are not $H$-colorable and (2) for any integer $k>4$ there are finitely many subgraph minimal graphs with no induced $P_k$ that are not $C_{k-2}$-colorable.   The former generalizes the result of Hell and Huang [Complexity of coloring graphs without paths and cycles, Discrete Appl. Math. 216: 211--232 (2017)] and the latter extends a result of Bruce, Hoang, and Sawada [A certifying algorithm for 3-colorability of $P_5$-Free Graphs, ISAAC 2009: 594--604]. Both our results lead to polynomial-time certifying algorithms for the corresponding coloring problems.|let path cycl vertic complet bipartit graph vertic side bipartit prove ani integ graph finit mani subgraph minim graph induc color ani integ finit mani subgraph minim graph induc color former general result hell huang complex color graph without path cycl discret appl math latter extend result bruce hoang sawada certifi algorithm color free graph isaac result lead polynomi time certifi algorithm correspond color problem|['Marcin Kamiński', 'Anna Pstrucha']|['math.CO', 'cs.DS']
2017-03-28T14:09:55Z|2017-03-07T14:48:15Z|http://arxiv.org/abs/1703.02411v1|http://arxiv.org/pdf/1703.02411v1|A Simple Deterministic Distributed MST Algorithm, with Near-Optimal Time   and Message Complexities|simpl determinist distribut mst algorithm near optim time messag complex|Distributed minimum spanning tree (MST) problem is one of the most central and fundamental problems in distributed graph algorithms. Garay et al. \cite{GKP98,KP98} devised an algorithm with running time $O(D + \sqrt{n} \cdot \log^* n)$, where $D$ is the hop-diameter of the input $n$-vertex $m$-edge graph, and with message complexity $O(m + n^{3/2})$. Peleg and Rubinovich \cite{PR99} showed that the running time of the algorithm of \cite{KP98} is essentially tight, and asked if one can achieve near-optimal running time **together with near-optimal message complexity**.   In a recent breakthrough, Pandurangan et al. \cite{PRS16} answered this question in the affirmative, and devised a **randomized** algorithm with time $\tilde{O}(D+ \sqrt{n})$ and message complexity $\tilde{O}(m)$. They asked if such a simultaneous time- and message-optimality can be achieved by a **deterministic** algorithm.   In this paper, building upon the work of \cite{PRS16}, we answer this question in the affirmative, and devise a **deterministic** algorithm that computes MST in time $O((D + \sqrt{n}) \cdot \log n)$, using $O(m \cdot \log n + n \log n \cdot \log^* n)$ messages. The polylogarithmic factors in the time and message complexities of our algorithm are significantly smaller than the respective factors in the result of \cite{PRS16}. Also, our algorithm and its analysis are very **simple** and self-contained, as opposed to rather complicated previous sublinear-time algorithms \cite{GKP98,KP98,E04b,PRS16}.|distribut minimum span tree mst problem one central fundament problem distribut graph algorithm garay et al cite gkp kp devis algorithm run time sqrt cdot log hop diamet input vertex edg graph messag complex peleg rubinovich cite pr show run time algorithm cite kp essenti tight ask one achiev near optim run time togeth near optim messag complex recent breakthrough pandurangan et al cite prs answer question affirm devis random algorithm time tild sqrt messag complex tild ask simultan time messag optim achiev determinist algorithm paper build upon work cite prs answer question affirm devis determinist algorithm comput mst time sqrt cdot log use cdot log log cdot log messag polylogarithm factor time messag complex algorithm signific smaller respect factor result cite prs also algorithm analysi veri simpl self contain oppos rather complic previous sublinear time algorithm cite gkp kp eb prs|['Michael Elkin']|['cs.DS']
2017-03-28T14:09:55Z|2017-03-23T13:11:52Z|http://arxiv.org/abs/1703.02375v2|http://arxiv.org/pdf/1703.02375v2|Graph sketching-based Massive Data Clustering|graph sketch base massiv data cluster|In this paper, we address the problem of recovering arbitrary-shaped data clusters from massive datasets. We present DBMSTClu a new density-based non-parametric method working on a limited number of linear measurements i.e. a sketched version of the similarity graph $G$ between the $N$ objects to cluster. Unlike $k$-means, $k$-medians or $k$-medoids algorithms, it does not fail at distinguishing clusters with particular structures. No input parameter is needed contrarily to DBSCAN or the Spectral Clustering method. DBMSTClu as a graph-based technique relies on the similarity graph $G$ which costs theoretically $O(N^2)$ in memory. However, our algorithm follows the dynamic semi-streaming model by handling $G$ as a stream of edge weight updates and sketches it in one pass over the data into a compact structure requiring $O(N \operatorname{poly} \operatorname{log} (N))$ space. Thanks to the property of the Minimum Spanning Tree (MST) for expressing the underlying structure of a graph, our algorithm successfully detects the right number of non-convex clusters by recovering an approximate MST from the graph sketch of $G$. We provide theoretical guarantees on the quality of the clustering partition and also demonstrate its advantage over the existing state-of-the-art on several datasets.|paper address problem recov arbitrari shape data cluster massiv dataset present dbmstclu new densiti base non parametr method work limit number linear measur sketch version similar graph object cluster unlik mean median medoid algorithm doe fail distinguish cluster particular structur input paramet need contrarili dbscan spectral cluster method dbmstclu graph base techniqu reli similar graph cost theoret memori howev algorithm follow dynam semi stream model handl stream edg weight updat sketch one pass data compact structur requir operatornam poli operatornam log space thank properti minimum span tree mst express structur graph algorithm success detect right number non convex cluster recov approxim mst graph sketch provid theoret guarante qualiti cluster partit also demonstr advantag exist state art sever dataset|['Anne Morvan', 'Krzysztof Choromanski', 'Cédric Gouy-Pailler', 'Jamal Atif']|['cs.LG', 'cs.DS']
2017-03-28T14:09:55Z|2017-03-07T05:43:56Z|http://arxiv.org/abs/1703.02224v1|http://arxiv.org/pdf/1703.02224v1|Space-efficient K-MER algorithm for generalized suffix tree|space effici mer algorithm general suffix tree|Suffix trees have emerged to be very fast for pattern searching yielding O (m) time, where m is the pattern size. Unfortunately their high memory requirements make it impractical to work with huge amounts of data. We present a memory efficient algorithm of a generalized suffix tree which reduces the space size by a factor of 10 when the size of the pattern is known beforehand. Experiments on the chromosomes and Pizza&Chili corpus show significant advantages of our algorithm over standard linear time suffix tree construction in terms of memory usage for pattern searching.|suffix tree emerg veri fast pattern search yield time pattern size unfortun high memori requir make impract work huge amount data present memori effici algorithm general suffix tree reduc space size factor size pattern known beforehand experi chromosom pizza chili corpus show signific advantag algorithm standard linear time suffix tree construct term memori usag pattern search|['Freeson Kaniwa', 'Venu Madhav Kuthadi', 'Otlhapile Dinakenyane', 'Heiko Schroeder']|['cs.DS']
2017-03-28T14:09:59Z|2017-03-06T20:28:23Z|http://arxiv.org/abs/1703.02100v1|http://arxiv.org/pdf/1703.02100v1|Guarantees for Greedy Maximization of Non-submodular Functions with   Applications|guarante greedi maxim non submodular function applic|We investigate the performance of the Greedy algorithm for cardinality constrained maximization of non-submodular nondecreasing set functions. While there are strong theoretical guarantees on the performance of Greedy for maximizing submodular functions, there are few guarantees for non-submodular ones. However, Greedy enjoys strong empirical performance for many important non-submodular functions, e.g., the Bayesian A-optimality objective in experimental design. We prove theoretical guarantees supporting the empirical performance. Our guarantees are characterized by the (generalized) submodularity ratio $\gamma$ and the (generalized) curvature $\alpha$. In particular, we prove that Greedy enjoys a tight approximation guarantee of $\frac{1}{\alpha}(1- e^{-\gamma\alpha})$ for cardinality constrained maximization. In addition, we bound the submodularity ratio and curvature for several important real-world objectives, e.g., the Bayesian A-optimality objective, the determinantal function of a square submatrix and certain linear programs with combinatorial constraints. We experimentally validate our theoretical findings for several real-world applications.|investig perform greedi algorithm cardin constrain maxim non submodular nondecreas set function strong theoret guarante perform greedi maxim submodular function guarante non submodular one howev greedi enjoy strong empir perform mani import non submodular function bayesian optim object experiment design prove theoret guarante support empir perform guarante character general submodular ratio gamma general curvatur alpha particular prove greedi enjoy tight approxim guarante frac alpha gamma alpha cardin constrain maxim addit bound submodular ratio curvatur sever import real world object bayesian optim object determinant function squar submatrix certain linear program combinatori constraint experiment valid theoret find sever real world applic|['Andrew An Bian', 'Joachim M. Buhmann', 'Andreas Krause', 'Sebastian Tschiatschek']|['cs.DM', 'cs.AI', 'cs.DS', 'cs.LG', 'math.OC']
2017-03-28T14:09:59Z|2017-03-06T19:01:03Z|http://arxiv.org/abs/1703.02059v1|http://arxiv.org/pdf/1703.02059v1|Cheshire: An Online Algorithm for Activity Maximization in Social   Networks|cheshir onlin algorithm activ maxim social network|User engagement in social networks depends critically on the number of online actions their users take in the network. Can we design an algorithm that finds when to incentivize users to take actions to maximize the overall activity in a social network? In this paper, we model the number of online actions over time using multidimensional Hawkes processes, derive an alternate representation of these processes based on stochastic differential equations (SDEs) with jumps and, exploiting this alternate representation, address the above question from the perspective of stochastic optimal control of SDEs with jumps. We find that the optimal level of incentivized actions depends linearly on the current level of overall actions. Moreover, the coefficients of this linear relationship can be found by solving a matrix Riccati differential equation, which can be solved efficiently, and a first order differential equation, which has a closed form solution. As a result, we are able to design an efficient online algorithm, Cheshire, to sample the optimal times of the users' incentivized actions. Experiments on both synthetic and real data gathered from Twitter show that our algorithm is able to consistently maximize the number of online actions more effectively than the state of the art.|user engag social network depend critic number onlin action user take network design algorithm find incentiv user take action maxim overal activ social network paper model number onlin action time use multidimension hawk process deriv altern represent process base stochast differenti equat sdes jump exploit altern represent address abov question perspect stochast optim control sdes jump find optim level incentiv action depend linear current level overal action moreov coeffici linear relationship found solv matrix riccati differenti equat solv effici first order differenti equat close form solut result abl design effici onlin algorithm cheshir sampl optim time user incentiv action experi synthet real data gather twitter show algorithm abl consist maxim number onlin action effect state art|['Ali Zarezade', 'Abir De', 'Hamid Rabiee', 'Manuel Gomez Rodriguez']|['stat.ML', 'cs.DS', 'cs.LG', 'cs.SI']
2017-03-28T14:09:59Z|2017-03-07T14:17:51Z|http://arxiv.org/abs/1703.01939v2|http://arxiv.org/pdf/1703.01939v2|Distributed Exact Shortest Paths in Sublinear Time|distribut exact shortest path sublinear time|"The distributed single-source shortest paths problem is one of the most fundamental and central problems in the message-passing distributed computing. Classical Bellman-Ford algorithm solves it in $O(n)$ time, where $n$ is the number of vertices in the input graph $G$. Peleg and Rubinovich (FOCS'99) showed a lower bound of $\tilde{\Omega}(D + \sqrt{n})$ for this problem, where $D$ is the hop-diameter of $G$.   Whether or not this problem can be solved in $o(n)$ time when $D$ is relatively small is a major notorious open question. Despite intensive research \cite{LP13,N14,HKN15,EN16,BKKL16} that yielded near-optimal algorithms for the approximate variant of this problem, no progress was reported for the original problem.   In this paper we answer this question in the affirmative. We devise an algorithm that requires $O((n \log n)^{5/6})$ time, for $D = O(\sqrt{n \log n})$, and $O(D^{1/3} \cdot (n \log n)^{2/3})$ time, for larger $D$. This running time is sublinear in $n$ in almost the entire range of parameters, specifically, for $D = o(n/\log^2 n)$.   We also devise the first algorithm with non-trivial complexity guarantees for computing exact shortest paths in the multipass semi-streaming model of computation.   From the technical viewpoint, our algorithm computes a hopset $G""$ of a skeleton graph $G'$ of $G$ without first computing $G'$ itself. We then conduct a Bellman-Ford exploration in $G' \cup G""$, while computing the required edges of $G'$ on the fly. As a result, our algorithm computes exactly those edges of $G'$ that it really needs, rather than computing approximately the entire $G'$."|distribut singl sourc shortest path problem one fundament central problem messag pass distribut comput classic bellman ford algorithm solv time number vertic input graph peleg rubinovich foc show lower bound tild omega sqrt problem hop diamet whether problem solv time relat small major notori open question despit intens research cite lp hkn en bkkl yield near optim algorithm approxim variant problem progress report origin problem paper answer question affirm devis algorithm requir log time sqrt log cdot log time larger run time sublinear almost entir rang paramet specif log also devis first algorithm non trivial complex guarante comput exact shortest path multipass semi stream model comput technic viewpoint algorithm comput hopset skeleton graph without first comput conduct bellman ford explor cup comput requir edg fli result algorithm comput exact edg realli need rather comput approxim entir|['Michael Elkin']|['cs.DS']
2017-03-28T14:09:59Z|2017-03-06T15:03:55Z|http://arxiv.org/abs/1703.01913v1|http://arxiv.org/pdf/1703.01913v1|Near-Optimal Closeness Testing of Discrete Histogram Distributions|near optim close test discret histogram distribut|We investigate the problem of testing the equivalence between two discrete histograms. A {\em $k$-histogram} over $[n]$ is a probability distribution that is piecewise constant over some set of $k$ intervals over $[n]$. Histograms have been extensively studied in computer science and statistics. Given a set of samples from two $k$-histogram distributions $p, q$ over $[n]$, we want to distinguish (with high probability) between the cases that $p = q$ and $\ p-q\ _1 \geq \epsilon$. The main contribution of this paper is a new algorithm for this testing problem and a nearly matching information-theoretic lower bound. Specifically, the sample complexity of our algorithm matches our lower bound up to a logarithmic factor, improving on previous work by polynomial factors in the relevant parameters. Our algorithmic approach applies in a more general setting and yields improved sample upper bounds for testing closeness of other structured distributions as well.|investig problem test equival two discret histogram em histogram probabl distribut piecewis constant set interv histogram extens studi comput scienc statist given set sampl two histogram distribut want distinguish high probabl case geq epsilon main contribut paper new algorithm test problem near match inform theoret lower bound specif sampl complex algorithm match lower bound logarithm factor improv previous work polynomi factor relev paramet algorithm approach appli general set yield improv sampl upper bound test close structur distribut well|['Ilias Diakonikolas', 'Daniel M. Kane', 'Vladimir Nikishkin']|['cs.DS', 'cs.IT', 'cs.LG', 'math.IT', 'math.ST', 'stat.TH']
2017-03-28T14:09:59Z|2017-03-24T15:42:05Z|http://arxiv.org/abs/1703.01905v2|http://arxiv.org/pdf/1703.01905v2|A randomized, efficient algorithm for 3SAT|random effici algorithm sat|In this paper I present a 3SAT algorithm based on the randomized algorithm of Papadimitriou from 1991, and Schoning from 1991. We also show that this algorithm finds a solution (if it exists) for a 3SAT problem with high probability in polynomial time.|paper present sat algorithm base random algorithm papadimitriou schone also show algorithm find solut exist sat problem high probabl polynomi time|['Cristian Dumitrescu']|['cs.DS']
2017-03-28T14:09:59Z|2017-03-06T12:55:21Z|http://arxiv.org/abs/1703.01847v1|http://arxiv.org/pdf/1703.01847v1|Tight Space-Approximation Tradeoff for the Multi-Pass Streaming Set   Cover Problem|tight space approxim tradeoff multi pass stream set cover problem|We study the classic set cover problem in the streaming model: the sets that comprise the instance are revealed one by one in a stream and the goal is to solve the problem by making one or few passes over the stream while maintaining a sublinear space $o(mn)$ in the input size; here $m$ denotes the number of the sets and $n$ is the universe size. Notice that in this model, we are mainly concerned with the space requirement of the algorithms and hence do not restrict their computation time.   Our main result is a resolution of the space-approximation tradeoff for the streaming set cover problem: we show that any $\alpha$-approximation algorithm for the set cover problem requires $\widetilde{\Omega}(mn^{1/\alpha})$ space, even if it is allowed polylog${(n)}$ passes over the stream, and even if the sets are arriving in a random order in the stream. This space-approximation tradeoff matches the best known bounds achieved by the recent algorithm of Har-Peled et.al. (PODS 2016) that requires only $O(\alpha)$ passes over the stream in an adversarial order, hence settling the space complexity of approximating the set cover problem in data streams in a quite robust manner. Additionally, our approach yields tight lower bounds for the space complexity of $(1- \epsilon)$-approximating the streaming maximum coverage problem studied in several recent works.|studi classic set cover problem stream model set compris instanc reveal one one stream goal solv problem make one pass stream maintain sublinear space mn input size denot number set univers size notic model main concern space requir algorithm henc restrict comput time main result resolut space approxim tradeoff stream set cover problem show ani alpha approxim algorithm set cover problem requir widetild omega mn alpha space even allow polylog pass stream even set arriv random order stream space approxim tradeoff match best known bound achiev recent algorithm har pele et al pod requir onli alpha pass stream adversari order henc settl space complex approxim set cover problem data stream quit robust manner addit approach yield tight lower bound space complex epsilon approxim stream maximum coverag problem studi sever recent work|['Sepehr Assadi']|['cs.DS']
2017-03-28T14:09:59Z|2017-03-06T12:06:58Z|http://arxiv.org/abs/1703.01830v1|http://arxiv.org/pdf/1703.01830v1|Decomposable Submodular Function Minimization: Discrete and Continuous|decompos submodular function minim discret continu|This paper investigates connections between discrete and continuous approaches for decomposable submodular function minimization. We provide improved running time estimates for the state-of-the-art continuous algorithms for the problem using combinatorial arguments. We also provide a systematic experimental comparison of the two types of methods, based on a clear distinction between level-0 and level-1 algorithms.|paper investig connect discret continu approach decompos submodular function minim provid improv run time estim state art continu algorithm problem use combinatori argument also provid systemat experiment comparison two type method base clear distinct level level algorithm|['Alina Ene', 'Huy L. Nguyen', 'László A. Végh']|['cs.LG', 'cs.DS']
2017-03-28T14:09:59Z|2017-03-06T05:14:07Z|http://arxiv.org/abs/1703.01727v1|http://arxiv.org/pdf/1703.01727v1|Frequent Query Matching in Dynamic Data Warehousing|frequent queri match dynam data wareh|With the need for flexible and on-demand decision support, Dynamic Data Warehouses (DDW) provide benefits over traditional data warehouses due to their dynamic characteristics in structuring and access mechanism. A DDW is a data framework that accommodates data source changes easily to allow seamless querying to users. Materialized Views (MV) are proven to be an effective methodology to enhance the process of retrieving data from a DDW as results are pre-computed and stored in it. However, due to the static nature of materialized views, the level of dynamicity that can be provided at the MV access layer is restricted. As a result, the collection of materialized views is not compatible with ever-changing reporting requirements. It is important that the MV collection is consistent with current and upcoming queries. The solution to the above problem must consider the following aspects: (a) MV must be matched against an OLAP query in order to recognize whether the MV can answer the query, (b) enable scalability in the MV collection, an intuitive mechanism to prune it and retrieve closely matching MVs must be incorporated, (c) MV collection must be able to evolve in correspondence to the regularly changing user query patterns. Therefore, the primary objective of this paper is to explore these aspects and provide a well-rounded solution for the MV access layer to remove the mismatch between the MV collection and reporting requirements. Our contribution to solve the problem includes a Query Matching Technique, a Domain Matching Technique and Maintenance of the MV collection. We developed an experimental platform using real data-sets to evaluate the effectiveness in terms of performance and precision of the proposed techniques.|need flexibl demand decis support dynam data warehous ddw provid benefit tradit data warehous due dynam characterist structur access mechan ddw data framework accommod data sourc chang easili allow seamless queri user materi view mv proven effect methodolog enhanc process retriev data ddw result pre comput store howev due static natur materi view level dynam provid mv access layer restrict result collect materi view compat ever chang report requir import mv collect consist current upcom queri solut abov problem must consid follow aspect mv must match olap queri order recogn whether mv answer queri enabl scalabl mv collect intuit mechan prune retriev close match mvs must incorpor mv collect must abl evolv correspond regular chang user queri pattern therefor primari object paper explor aspect provid well round solut mv access layer remov mismatch mv collect report requir contribut solv problem includ queri match techniqu domain match techniqu mainten mv collect develop experiment platform use real data set evalu effect term perform precis propos techniqu|['Charles H. Goonetilleke', 'J. Wenny Rahayu', 'Md. Saiful Islam']|['cs.DB', 'cs.DS']
2017-03-28T14:09:59Z|2017-03-05T23:06:03Z|http://arxiv.org/abs/1703.01686v1|http://arxiv.org/pdf/1703.01686v1|Parameterized complexity of finding a spanning tree with minimum reload   cost diameter|parameter complex find span tree minimum reload cost diamet|We study the minimum diameter spanning tree problem under the reload cost model (DIAMETER-TREE for short) introduced by Wirth and Steffan (2001). In this problem, given an undirected edge-colored graph $G$, reload costs on a path arise at a node where the path uses consecutive edges of different colors. The objective is to find a spanning tree of $G$ of minimum diameter with respect to the reload costs. We initiate a systematic study of the parameterized complexity of the DIAMETER-TREE problem by considering the following parameters: the cost of a solution, and the treewidth and the maximum degree $\Delta$ of the input graph. We prove that DIAMETER-TREE is para-NP-hard for any combination of two of these three parameters, and that it is FPT parameterized by the three of them. We also prove that the problem can be solved in polynomial time on cactus graphs. This result is somehow surprising since we prove DIAMETER-TREE to be NP-hard on graphs of treewidth two, which is best possible as the problem can be trivially solved on forests. When the reload costs satisfy the triangle inequality, Wirth and Steffan (2001) proved that the problem can be solved in polynomial time on graphs with $\Delta = 3$, and Galbiati (2008) proved that it is NP-hard if $\Delta = 4$. Our results show, in particular, that without the requirement of the triangle inequality, the problem is NP-hard if $\Delta = 3$, which is also best possible. Finally, in the case where the reload costs are polynomially bounded by the size of the input graph, we prove that DIAMETER-TREE is in XP and W[1]-hard parameterized by the treewidth plus $\Delta$.|studi minimum diamet span tree problem reload cost model diamet tree short introduc wirth steffan problem given undirect edg color graph reload cost path aris node path use consecut edg differ color object find span tree minimum diamet respect reload cost initi systemat studi parameter complex diamet tree problem consid follow paramet cost solut treewidth maximum degre delta input graph prove diamet tree para np hard ani combin two three paramet fpt parameter three also prove problem solv polynomi time cactus graph result somehow surpris sinc prove diamet tree np hard graph treewidth two best possibl problem trivial solv forest reload cost satisfi triangl inequ wirth steffan prove problem solv polynomi time graph delta galbiati prove np hard delta result show particular without requir triangl inequ problem np hard delta also best possibl final case reload cost polynomi bound size input graph prove diamet tree xp hard parameter treewidth plus delta|['Julien Baste', 'Didem Gözüpek', 'Christophe Paul', 'Ignasi Sau', 'Mordechai Shalom', 'Dimitrios M. Thilikos']|['cs.DS', 'cs.CC', '05C85, 05C10', 'G.2.2; G.2.3']
2017-03-28T14:09:59Z|2017-03-05T18:24:23Z|http://arxiv.org/abs/1703.01640v1|http://arxiv.org/abs/1703.01640v1|Approximation algorithms for TSP with neighborhoods in the plane|approxim algorithm tsp neighborhood plane|In the Euclidean TSP with neighborhoods (TSPN), we are given a collection of n regions (neighborhoods) and we seek a shortest tour that visits each region. As a generalization of the classical Euclidean TSP, TSPN is also NP-hard. In this paper, we present new approximation results for the TSPN, including (1) a constant-factor approximation algorithm for the case of arbitrary connected neighborhoods having comparable diameters; and (2) a PTAS for the important special case of disjoint unit disk neighborhoods (or nearly disjoint, nearly-unit disks). Our methods also yield improved approximation ratios for various special classes of neighborhoods, which have previously been studied. Further, we give a linear-time O(1)-approximation algorithm for the case of neighborhoods that are (infinite) straight lines.|euclidean tsp neighborhood tspn given collect region neighborhood seek shortest tour visit region general classic euclidean tsp tspn also np hard paper present new approxim result tspn includ constant factor approxim algorithm case arbitrari connect neighborhood compar diamet ptas import special case disjoint unit disk neighborhood near disjoint near unit disk method also yield improv approxim ratio various special class neighborhood previous studi give linear time approxim algorithm case neighborhood infinit straight line|['Adrian Dumitrescu', 'Joseph S. B. Mitchell']|['cs.CG', 'cs.DS']
2017-03-28T14:10:04Z|2017-03-05T18:12:06Z|http://arxiv.org/abs/1703.01638v1|http://arxiv.org/pdf/1703.01638v1|Conditional Hardness for Sensitivity Problems|condit hard sensit problem|In recent years it has become popular to study dynamic problems in a sensitivity setting: Instead of allowing for an arbitrary sequence of updates, the sensitivity model only allows to apply batch updates of small size to the original input data. The sensitivity model is particularly appealing since recent strong conditional lower bounds ruled out fast algorithms for many dynamic problems, such as shortest paths, reachability, or subgraph connectivity.   In this paper we prove conditional lower bounds for sensitivity problems. For example, we show that under the Boolean Matrix Multiplication (BMM) conjecture combinatorial algorithms cannot compute the (4/3 - {\epsilon})-approximate diameter of an undirected unweighted dense graph with truly subcubic preprocessing time and truly subquadratic update/query time. This result is surprising since in the static setting it is not clear whether a reduction from BMM to diameter is possible. We further show under the BMM conjecture that many problems, such as reachability or approximate shortest paths, cannot be solved faster than by recomputation from scratch even after only one or two edge insertions. We give more lower bounds under the Strong Exponential Time Hypothesis and the All Pairs Shortest Paths Conjecture. Many of our lower bounds also hold for static oracle data structures where no sensitivity is required. Finally, we give the first algorithm for the (1 + {\epsilon})-approximate radius, diameter, and eccentricity problems in directed or undirected unweighted graphs in case of single edges failures. The algorithm has a truly subcubic running time for graphs with a truly subquadratic number of edges; it is tight w.r.t. the conditional lower bounds we obtain.|recent year becom popular studi dynam problem sensit set instead allow arbitrari sequenc updat sensit model onli allow appli batch updat small size origin input data sensit model particular appeal sinc recent strong condit lower bound rule fast algorithm mani dynam problem shortest path reachabl subgraph connect paper prove condit lower bound sensit problem exampl show boolean matrix multipl bmm conjectur combinatori algorithm cannot comput epsilon approxim diamet undirect unweight dens graph truli subcub preprocess time truli subquadrat updat queri time result surpris sinc static set clear whether reduct bmm diamet possibl show bmm conjectur mani problem reachabl approxim shortest path cannot solv faster recomput scratch even onli one two edg insert give lower bound strong exponenti time hypothesi pair shortest path conjectur mani lower bound also hold static oracl data structur sensit requir final give first algorithm epsilon approxim radius diamet eccentr problem direct undirect unweight graph case singl edg failur algorithm truli subcub run time graph truli subquadrat number edg tight condit lower bound obtain|['Monika Henzinger', 'Andrea Lincoln', 'Stefan Neumann', 'Virginia Vassilevska Williams']|['cs.DS', 'F.2.2']
2017-03-28T14:10:04Z|2017-03-05T17:45:59Z|http://arxiv.org/abs/1703.01634v1|http://arxiv.org/pdf/1703.01634v1|Stochastic Online Scheduling on Unrelated Machines|stochast onlin schedul unrel machin|We derive the first performance guarantees for a combinatorial online algorithm that schedules stochastic, non-preemptive jobs on unrelated machines to minimize the expectation of the total weighted completion time. Prior work on unrelated machine scheduling with stochastic jobs was restricted to the offline case, and required sophisticated linear or convex programming relaxations for the assignment of jobs to machines. Our algorithm is purely combinatorial, and therefore it also works for the online setting. As to the techniques applied, this paper shows how the dual fitting technique can be put to work for stochastic and non-preemptive scheduling problems.|deriv first perform guarante combinatori onlin algorithm schedul stochast non preemptiv job unrel machin minim expect total weight complet time prior work unrel machin schedul stochast job restrict offlin case requir sophist linear convex program relax assign job machin algorithm pure combinatori therefor also work onlin set techniqu appli paper show dual fit techniqu put work stochast non preemptiv schedul problem|['Varun Gupta', 'Benjamin Moseley', 'Marc Uetz', 'Qiaomin Xie']|['cs.DS']
2017-03-28T14:10:04Z|2017-03-05T01:08:29Z|http://arxiv.org/abs/1703.01539v1|http://arxiv.org/pdf/1703.01539v1|Distributed Partial Clustering|distribut partial cluster|Recent years have witnessed an increasing popularity of algorithm design for distributed data, largely due to the fact that massive datasets are often collected and stored in different locations. In the distributed setting communication typically dominates the query processing time. Thus it becomes crucial to design communication efficient algorithms for queries on distributed data. Simultaneously, it has been widely recognized that partial optimizations, where we are allowed to disregard a small part of the data, provide us significantly better solutions. The motivation for disregarded points often arise from noise and other phenomena that are pervasive in large data scenarios.   In this paper we focus on partial clustering problems, $k$-center, $k$-median and $k$-means, in the distributed model, and provide algorithms with communication sublinear of the input size. As a consequence we develop the first algorithms for the partial $k$-median and means objectives that run in subquadratic running time. We also initiate the study of distributed algorithms for clustering uncertain data, where each data point can possibly fall into multiple locations under certain probability distribution.|recent year wit increas popular algorithm design distribut data larg due fact massiv dataset often collect store differ locat distribut set communic typic domin queri process time thus becom crucial design communic effici algorithm queri distribut data simultan wide recogn partial optim allow disregard small part data provid us signific better solut motiv disregard point often aris nois phenomena pervas larg data scenario paper focus partial cluster problem center median mean distribut model provid algorithm communic sublinear input size consequ develop first algorithm partial median mean object run subquadrat run time also initi studi distribut algorithm cluster uncertain data data point possibl fall multipl locat certain probabl distribut|['Sudipto Guha', 'Yi Li', 'Qin Zhang']|['cs.DS']
2017-03-28T14:10:04Z|2017-03-04T22:56:03Z|http://arxiv.org/abs/1703.01532v1|http://arxiv.org/pdf/1703.01532v1|Using Matching to Detect Infeasibility of Some Integer Programs|use match detect infeas integ program|A novel matching based heuristic algorithm designed to detect specially formulated infeasible zero-one IPs is presented. The algorithm input is a set of nested doubly stochastic subsystems and a set E of instance defining variables set at zero level. The algorithm deduces additional variables at zero level until either a constraint is violated (the IP is infeasible), or no more variables can be deduced zero (the IP is undecided). All feasible IPs, and all infeasible IPs not detected infeasible are undecided. We successfully apply the algorithm to a small set of specially formulated infeasible zero-one IP instances of the Hamilton cycle decision problem. We show how to model both the graph and subgraph isomorphism decision problems for input to the algorithm. Increased levels of nested doubly stochastic subsystems can be implemented dynamically. The algorithm is designed for parallel processing, and for inclusion of techniques in addition to matching.|novel match base heurist algorithm design detect special formul infeas zero one ip present algorithm input set nest doubli stochast subsystem set instanc defin variabl set zero level algorithm deduc addit variabl zero level either constraint violat ip infeas variabl deduc zero ip undecid feasibl ip infeas ip detect infeas undecid success appli algorithm small set special formul infeas zero one ip instanc hamilton cycl decis problem show model graph subgraph isomorph decis problem input algorithm increas level nest doubli stochast subsystem implement dynam algorithm design parallel process inclus techniqu addit match|['S. J. Gismondi', 'E. R. Swart']|['cs.DS', 'cs.DM']
2017-03-28T14:10:04Z|2017-03-04T19:08:22Z|http://arxiv.org/abs/1703.01507v1|http://arxiv.org/pdf/1703.01507v1|Machine Learning Friendly Set Version of Johnson-Lindenstrauss Lemma|machin learn friend set version johnson lindenstrauss lemma|In this paper we make a novel use of the Johnson-Lindenstrauss Lemma. The Lemma has an existential form saying that there exists a JL transformation $f$ of the data points into lower dimensional space such that all of them fall into predefined error range $\delta$. We formulate in this paper a theorem stating that we can choose the target dimensionality in a random projection type JL linear transformation in such a way that with probability $1-\epsilon$ all of them fall into predefined error range $\delta$ for any user-predefined failure probability $\epsilon$. This result is important for applications such a data clustering where we want to have a priori dimensionality reducing transformation instead of trying out a (large) number of them, as with traditional Johnson-Lindenstrauss Lemma.|paper make novel use johnson lindenstrauss lemma lemma existenti form say exist jl transform data point lower dimension space fall predefin error rang delta formul paper theorem state choos target dimension random project type jl linear transform way probabl epsilon fall predefin error rang delta ani user predefin failur probabl epsilon result import applic data cluster want priori dimension reduc transform instead tri larg number tradit johnson lindenstrauss lemma|['Mieczysław A. Kłopotek']|['cs.DS', 'cs.LG']
2017-03-28T14:10:04Z|2017-03-04T15:14:06Z|http://arxiv.org/abs/1703.01475v1|http://arxiv.org/pdf/1703.01475v1|4/3 Rectangle Tiling lower bound|rectangl tile lower bound|The problem that we consider is the following: given an $n \times n$ array $A$ of positive numbers, find a tiling using at most $p$ rectangles (which means that each array element must be covered by some rectangle and no two rectangles must overlap) that minimizes the maximum weight of any rectangle (the weight of a rectangle is the sum of elements which are covered by it). We prove that it is NP-hard to approximate this problem to within a factor of \textbf{1$\frac{1}{3}$} (the previous best result was $1\frac{1}{4}$).|problem consid follow given time array posit number find tile use rectangl mean array element must cover rectangl two rectangl must overlap minim maximum weight ani rectangl weight rectangl sum element cover prove np hard approxim problem within factor textbf frac previous best result frac|['Grzegorz Głuch', 'Krzysztof Loryś']|['cs.DS']
2017-03-28T14:10:04Z|2017-03-04T15:13:41Z|http://arxiv.org/abs/1703.01474v1|http://arxiv.org/pdf/1703.01474v1|Sharp bounds for population recovery|sharp bound popul recoveri|The population recovery problem is a basic problem in noisy unsupervised learning that has attracted significant research attention in recent years [WY12,DRWY12, MS13, BIMP13, LZ15,DST16]. A number of different variants of this problem have been studied, often under assumptions on the unknown distribution (such as that it has restricted support size). In this work we study the sample complexity and algorithmic complexity of the most general version of the problem, under both bit-flip noise and erasure noise model. We give essentially matching upper and lower sample complexity bounds for both noise models, and efficient algorithms matching these sample complexity bounds up to polynomial factors.|popul recoveri problem basic problem noisi unsupervis learn attract signific research attent recent year wy drwi ms bimp lz dst number differ variant problem studi often assumpt unknown distribut restrict support size work studi sampl complex algorithm complex general version problem bit flip nois erasur nois model give essenti match upper lower sampl complex bound nois model effici algorithm match sampl complex bound polynomi factor|"['Anindya De', ""Ryan O'Donnell"", 'Rocco Servedio']"|['cs.DS', 'cs.LG', 'math.ST', 'stat.TH']
2017-03-28T14:10:04Z|2017-03-06T09:05:51Z|http://arxiv.org/abs/1703.01166v2|http://arxiv.org/pdf/1703.01166v2|Efficient Network Measurements through Approximated Windows|effici network measur approxim window|Many networking applications require timely access to recent network measurements, which can be captured using a sliding window model. Maintaining such measurements is a challenging task due to the fast line speed and scarcity of fast memory in routers. In this work, we study the efficiency factor that can be gained by approximating the window size. That is, we allow the algorithm to dynamically adjust the window size between $W$ and $W(1+\tau)$ where $\tau$ is a small positive parameter. For example, consider the \emph{basic summing} problem of computing the sum of the last $W$ elements in a stream whose items are integers in $\{0,1\ldots,R\}$, where $R=\text{poly}(W)$. While it is known that $\Omega(W\log{R})$ bits are needed in the exact window model, we show that approximate windows allow an exponential space reduction for constant $\tau$.   Specifically, we present a lower bound of $\Omega(\tau^{-1}\log(RW\tau))$ bits for the basic summing problem. Further, an $(1+\epsilon)$ multiplicative approximation of this problem requires $\Omega(\log({W/\epsilon}+\log\log{R}))$ bits for constant $\tau$. Additionally, for $RW\epsilon$ additive approximations, we show an $\Omega(\tau^{-1}\log\lfloor{1+\tau/\epsilon}\rfloor+\log({W/\epsilon}))$ lower bound~\footnote{ We also provide an optimal bound and algorithm for the $\tau<\epsilon$ case.}. For all three settings, we provide memory optimal algorithms that operate in constant time. Finally, we demonstrate the generality of the approximated window model by applying it to counting the number of distinct flows in a sliding window over a network stream. We present an algorithm that solves this problem while requiring asymptotically less space than previous sliding window methods when $\tau=O(1)$.|mani network applic requir time access recent network measur captur use slide window model maintain measur challeng task due fast line speed scarciti fast memori router work studi effici factor gain approxim window size allow algorithm dynam adjust window size tau tau small posit paramet exampl consid emph basic sum problem comput sum last element stream whose item integ ldot text poli known omega log bit need exact window model show approxim window allow exponenti space reduct constant tau specif present lower bound omega tau log rw tau bit basic sum problem epsilon multipl approxim problem requir omega log epsilon log log bit constant tau addit rw epsilon addit approxim show omega tau log lfloor tau epsilon rfloor log epsilon lower bound footnot also provid optim bound algorithm tau epsilon case three set provid memori optim algorithm oper constant time final demonstr general approxim window model appli count number distinct flow slide window network stream present algorithm solv problem requir asymptot less space previous slide window method tau|['Ran Ben Basat', 'Gil Einziger', 'Roy Friedman']|['cs.DS']
2017-03-28T14:10:04Z|2017-03-03T06:41:02Z|http://arxiv.org/abs/1703.01054v1|http://arxiv.org/abs/1703.01054v1|When Hashes Met Wedges: A Distributed Algorithm for Finding High   Similarity Vectors|hash met wedg distribut algorithm find high similar vector|"Finding similar user pairs is a fundamental task in social networks, with numerous applications in ranking and personalization tasks such as link prediction and tie strength detection. A common manifestation of user similarity is based upon network structure: each user is represented by a vector that represents the user's network connections, where pairwise cosine similarity among these vectors defines user similarity. The predominant task for user similarity applications is to discover all similar pairs that have a pairwise cosine similarity value larger than a given threshold $\tau$. In contrast to previous work where $\tau$ is assumed to be quite close to 1, we focus on recommendation applications where $\tau$ is small, but still meaningful. The all pairs cosine similarity problem is computationally challenging on networks with billions of edges, and especially so for settings with small $\tau$. To the best of our knowledge, there is no practical solution for computing all user pairs with, say $\tau = 0.2$ on large social networks, even using the power of distributed algorithms.   Our work directly addresses this challenge by introducing a new algorithm --- WHIMP --- that solves this problem efficiently in the MapReduce model. The key insight in WHIMP is to combine the ""wedge-sampling"" approach of Cohen-Lewis for approximate matrix multiplication with the SimHash random projection techniques of Charikar. We provide a theoretical analysis of WHIMP, proving that it has near optimal communication costs while maintaining computation cost comparable with the state of the art. We also empirically demonstrate WHIMP's scalability by computing all highly similar pairs on four massive data sets, and show that it accurately finds high similarity pairs. In particular, we note that WHIMP successfully processes the entire Twitter network, which has tens of billions of edges."|find similar user pair fundament task social network numer applic rank person task link predict tie strength detect common manifest user similar base upon network structur user repres vector repres user network connect pairwis cosin similar among vector defin user similar predomin task user similar applic discov similar pair pairwis cosin similar valu larger given threshold tau contrast previous work tau assum quit close focus recommend applic tau small still meaning pair cosin similar problem comput challeng network billion edg especi set small tau best knowledg practic solut comput user pair say tau larg social network even use power distribut algorithm work direct address challeng introduc new algorithm whimp solv problem effici mapreduc model key insight whimp combin wedg sampl approach cohen lewi approxim matrix multipl simhash random project techniqu charikar provid theoret analysi whimp prove near optim communic cost maintain comput cost compar state art also empir demonstr whimp scalabl comput high similar pair four massiv data set show accur find high similar pair particular note whimp success process entir twitter network ten billion edg|['Aneesh Sharma', 'C. Seshadhri', 'Ashish Goel']|['cs.SI', 'cs.DC', 'cs.DS']
2017-03-28T14:10:04Z|2017-03-06T04:41:19Z|http://arxiv.org/abs/1703.01009v2|http://arxiv.org/pdf/1703.01009v2|Optimal Time and Space Construction of Suffix Arrays and LCP Arrays for   Integer Alphabets|optim time space construct suffix array lcp array integ alphabet|Suffix arrays and LCP arrays are one of the most fundamental data structures widely used for various kinds of string processing. Many problems can be solved efficiently by using suffix arrays, or a pair of suffix arrays and LCP arrays. In this paper, we consider two problems for a string of length $N$, the characters of which are represented as integers in $[1, \dots, \sigma]$ for $1 \leq \sigma \leq N$; the string contains $\sigma$ distinct characters, (1) construction of the suffix array and (2) simultaneous construction of both the suffix array and the LCP array. In the word RAM model, we propose algorithms to solve both the problems in $O(N)$ time using $O(1)$ extra words, which are optimal in time and space. Extra words mean the required space except for the space of the input string and output suffix array and LCP array. Our contribution improves the previous most efficient algorithm that runs in $O(N)$ time using $\sigma+O(1)$ extra words for the suffix array construction proposed by [Nong, TOIS 2013], and it improves the previous most efficient solution that runs in $O(N)$ time using $\sigma + O(1)$ extra words for both suffix array and LCP array construction using the combination of [Nong, TOIS 2013] and [Manzini, SWAT 2004].   Another optimal time and space algorithm to construct the suffix array was proposed by [Li et al, arXiv 2016] very recently and independently. Our algorithm is simpler than theirs, and it allows us to solve the second problem in optimal time and space.|suffix array lcp array one fundament data structur wide use various kind string process mani problem solv effici use suffix array pair suffix array lcp array paper consid two problem string length charact repres integ dot sigma leq sigma leq string contain sigma distinct charact construct suffix array simultan construct suffix array lcp array word ram model propos algorithm solv problem time use extra word optim time space extra word mean requir space except space input string output suffix array lcp array contribut improv previous effici algorithm run time use sigma extra word suffix array construct propos nong toi improv previous effici solut run time use sigma extra word suffix array lcp array construct use combin nong toi manzini swat anoth optim time space algorithm construct suffix array propos li et al arxiv veri recent independ algorithm simpler allow us solv second problem optim time space|['Keisuke Goto']|['cs.DS']
2017-04-07T11:30:58Z|2017-04-06T14:45:40Z|http://arxiv.org/abs/1704.01869v1|http://arxiv.org/pdf/1704.01869v1|Randomized Linear Programming Solves the Discounted Markov Decision   Problem In Nearly-Linear Running Time|random linear program solv discount markov decis problem near linear run time|We propose a randomized linear programming algorithm for approximating the optimal policy of the discounted Markov decision problem. By leveraging the value-policy duality, the algorithm adaptively samples state transitions and makes exponentiated primal-dual updates. We show that it finds an $\epsilon$-optimal policy using nearly-linear running time in the worst case. For Markov decision processes that are ergodic under every stationary policy, we show that the algorithm finds an $\epsilon$-optimal policy using running time linear in the total number of state-action pairs, which is sublinear in the input size. These results provide new complexity benchmarks for solving stochastic dynamic programs.|propos random linear program algorithm approxim optim polici discount markov decis problem leverag valu polici dualiti algorithm adapt sampl state transit make exponenti primal dual updat show find epsilon optim polici use near linear run time worst case markov decis process ergod everi stationari polici show algorithm find epsilon optim polici use run time linear total number state action pair sublinear input size result provid new complex benchmark solv stochast dynam program|['Mengdi Wang']|['math.OC', 'cs.DS']
2017-04-07T11:30:58Z|2017-04-06T14:36:58Z|http://arxiv.org/abs/1704.01862v1|http://arxiv.org/pdf/1704.01862v1|Approximate Clustering with Same-Cluster Queries|approxim cluster cluster queri|"Ashtiani et al. proposed a Semi-Supervised Active Clustering framework (SSAC), where the learner is allowed to make adaptive queries to a domain expert. The queries are of the kind ""do two given points belong to the same optimal cluster?"" There are many clustering contexts where such same-cluster queries are feasible. Ashtiani et al. exhibited the power of such queries by showing that any instance of the $k$-means clustering problem, with additional margin assumption, can be solved efficiently if one is allowed $O(k^2 \log{k} + k \log{n})$ same-cluster queries. This is interesting since the $k$-means problem, even with the margin assumption, is $\mathsf{NP}$-hard.   In this paper, we extend the work of Ashtiani et al. to the approximation setting showing that a few of such same-cluster queries enables one to get a polynomial-time $(1 + \varepsilon)$-approximation algorithm for the $k$-means problem without any margin assumption on the input dataset. Again, this is interesting since the $k$-means problem is $\mathsf{NP}$-hard to approximate within a factor $(1 + c)$ for a fixed constant $0 < c < 1$. The number of same-cluster queries used is $\textrm{poly}(k/\varepsilon)$ which is independent of the size $n$ of the dataset. Our algorithm is based on the $D^2$-sampling technique, also known as the $k$-means++ seeding algorithm. We also give a conditional lower bound on the number of same-cluster queries showing that if the Exponential Time Hypothesis (ETH) holds, then any such efficient query algorithm needs to make $\Omega \left(\frac{k}{poly \log k} \right)$ same-cluster queries. Another result we show with respect to the $k$-means++ seeding algorithm is that a small modification to the $k$-means++ seeding algorithm within the SSAC framework converts it to a constant factor approximation algorithm instead of the well known $O(\log{k})$-approximation algorithm."|ashtiani et al propos semi supervis activ cluster framework ssac learner allow make adapt queri domain expert queri kind two given point belong optim cluster mani cluster context cluster queri feasibl ashtiani et al exhibit power queri show ani instanc mean cluster problem addit margin assumpt solv effici one allow log log cluster queri interest sinc mean problem even margin assumpt mathsf np hard paper extend work ashtiani et al approxim set show cluster queri enabl one get polynomi time varepsilon approxim algorithm mean problem without ani margin assumpt input dataset interest sinc mean problem mathsf np hard approxim within factor fix constant number cluster queri use textrm poli varepsilon independ size dataset algorithm base sampl techniqu also known mean seed algorithm also give condit lower bound number cluster queri show exponenti time hypothesi eth hold ani effici queri algorithm need make omega left frac poli log right cluster queri anoth result show respect mean seed algorithm small modif mean seed algorithm within ssac framework convert constant factor approxim algorithm instead well known log approxim algorithm|['Nir Ailon', 'Anup Bhattacharya', 'Ragesh Jaiswal', 'Amit Kumar']|['cs.DS']
2017-04-07T11:30:58Z|2017-04-06T01:04:24Z|http://arxiv.org/abs/1704.01676v1|http://arxiv.org/abs/1704.01676v1|Multi-Personality Partitioning for Heterogeneous Systems|multi person partit heterogen system|Design flows use graph partitioning both as a precursor to place and route for single devices, and to divide netlists or task graphs among multiple devices. Partitioners have accommodated FPGA heterogeneity via multi-resource constraints, but have not yet exploited the corresponding ability to implement some computations in multiple ways (e.g., LUTs vs. DSP blocks), which could enable a superior solution. This paper introduces multi-personality graph partitioning, which incorporates aspects of resource mapping into partitioning. We present a modified multi-level KLFM partitioning algorithm that also performs heterogeneous resource mapping for nodes with multiple potential implementations (multiple personalities). We evaluate several variants of our multi-personality FPGA circuit partitioner using 21 circuits and benchmark graphs, and show that dynamic resource mapping improves cut size on average by 27% over static mapping for these circuits. We further show that it improves deviation from target resource utilizations by 50% over post-partitioning resource mapping.|design flow use graph partit precursor place rout singl devic divid netlist task graph among multipl devic partition accommod fpga heterogen via multi resourc constraint yet exploit correspond abil implement comput multipl way lut vs dsp block could enabl superior solut paper introduc multi person graph partit incorpor aspect resourc map partit present modifi multi level klfm partit algorithm also perform heterogen resourc map node multipl potenti implement multipl person evalu sever variant multi person fpga circuit partition use circuit benchmark graph show dynam resourc map improv cut size averag static map circuit show improv deviat target resourc util post partit resourc map|['Anthony Gregerson', 'Aman Chadha', 'Katherine Morrow']|['cs.DC', 'cs.DS']
2017-04-07T11:30:58Z|2017-04-05T21:03:53Z|http://arxiv.org/abs/1704.01652v1|http://arxiv.org/pdf/1704.01652v1|Greed is Good: Near-Optimal Submodular Maximization via Greedy   Optimization|greed good near optim submodular maxim via greedi optim|It is known that greedy methods perform well for maximizing monotone submodular functions. At the same time, such methods perform poorly in the face of non-monotonicity. In this paper, we show - arguably, surprisingly - that invoking the classical greedy algorithm $O(\sqrt{k})$-times leads to the (currently) fastest deterministic algorithm, called Repeated Greedy, for maximizing a general submodular function subject to $k$-independent system constraints. Repeated Greedy achieves $(1 + O(1/\sqrt{k}))k$ approximation using $O(nr\sqrt{k})$ function evaluations (here, $n$ and $r$ denote the size of the ground set and the maximum size of a feasible solution, respectively). We then show that by a careful sampling procedure, we can run the greedy algorithm only once and obtain the (currently) fastest randomized algorithm, called Sample Greedy, for maximizing a submodular function subject to $k$-extendible system constraints (a subclass of $k$-independent system constrains). Sample Greedy achieves $(k + 3)$-approximation with only $O(nr/k)$ function evaluations. Finally, we derive an almost matching lower bound, and show that no polynomial time algorithm can have an approximation ratio smaller than $ k + 1/2 - \varepsilon$. To further support our theoretical results, we compare the performance of Repeated Greedy and Sample Greedy with prior art in a concrete application (movie recommendation). We consistently observe that while Sample Greedy achieves practically the same utility as the best baseline, it performs at least two orders of magnitude faster.|known greedi method perform well maxim monoton submodular function time method perform poor face non monoton paper show arguabl surpris invok classic greedi algorithm sqrt time lead current fastest determinist algorithm call repeat greedi maxim general submodular function subject independ system constraint repeat greedi achiev sqrt approxim use nr sqrt function evalu denot size ground set maximum size feasibl solut respect show care sampl procedur run greedi algorithm onli onc obtain current fastest random algorithm call sampl greedi maxim submodular function subject extend system constraint subclass independ system constrain sampl greedi achiev approxim onli nr function evalu final deriv almost match lower bound show polynomi time algorithm approxim ratio smaller varepsilon support theoret result compar perform repeat greedi sampl greedi prior art concret applic movi recommend consist observ sampl greedi achiev practic util best baselin perform least two order magnitud faster|['Moran Feldman', 'Christopher Harshaw', 'Amin Karbasi']|['cs.LG', 'cs.DS']
2017-04-07T11:30:58Z|2017-04-05T20:30:27Z|http://arxiv.org/abs/1704.01646v1|http://arxiv.org/pdf/1704.01646v1|Streaming Pattern Matching with d Wildcards|stream pattern match wildcard|In the pattern matching with $d$ wildcards problem one is given a text $T$ of length $n$ and a pattern $P$ of length $m$ that contains $d$ wildcard characters, each denoted by a special symbol $'?'$. A wildcard character matches any other character. The goal is to establish for each $m$-length substring of $T$ whether it matches $P$. In the streaming model variant of the pattern matching with $d$ wildcards problem the text $T$ arrives one character at a time and the goal is to report, before the next character arrives, if the last $m$ characters match $P$ while using only $o(m)$ words of space.   In this paper we introduce two new algorithms for the $d$ wildcard pattern matching problem in the streaming model. The first is a randomized Monte Carlo algorithm that is parameterized by a constant $0\leq \delta \leq 1$. This algorithm uses $\tilde{O}(d^{1-\delta})$ amortized time per character and $\tilde{O}(d^{1+\delta})$ words of space. The second algorithm, which is used as a black box in the first algorithm, is a randomized Monte Carlo algorithm which uses $O(d+\log m)$ worst-case time per character and $O(d\log m)$ words of space.|pattern match wildcard problem one given text length pattern length contain wildcard charact denot special symbol wildcard charact match ani charact goal establish length substr whether match stream model variant pattern match wildcard problem text arriv one charact time goal report befor next charact arriv last charact match use onli word space paper introduc two new algorithm wildcard pattern match problem stream model first random mont carlo algorithm parameter constant leq delta leq algorithm use tild delta amort time per charact tild delta word space second algorithm use black box first algorithm random mont carlo algorithm use log worst case time per charact log word space|['Shay Golan', 'Tsvi Kopelowitz', 'Ely Porat']|['cs.DS']
2017-04-07T11:30:58Z|2017-04-05T14:54:28Z|http://arxiv.org/abs/1704.01460v1|http://arxiv.org/pdf/1704.01460v1|Comparison Based Nearest Neighbor Search|comparison base nearest neighbor search|We consider machine learning in a comparison-based setting where we are given a set of points in a metric space, but we have no access to the actual distances between the points. Instead, we can only ask an oracle whether the distance between two points $i$ and $j$ is smaller than the distance between the points $i$ and $k$. We are concerned with data structures and algorithms to find nearest neighbors based on such comparisons. We focus on a simple yet effective algorithm that recursively splits the space by first selecting two random pivot points and then assigning all other points to the closer of the two (comparison tree). We prove that if the metric space satisfies certain expansion conditions, then with high probability the height of the comparison tree is logarithmic in the number of points, leading to efficient search performance. We also provide an upper bound for the failure probability to return the true nearest neighbor. Experiments show that the comparison tree is competitive with algorithms that have access to the actual distance values, and needs less triplet comparisons than other competitors.|consid machin learn comparison base set given set point metric space access actual distanc point instead onli ask oracl whether distanc two point smaller distanc point concern data structur algorithm find nearest neighbor base comparison focus simpl yet effect algorithm recurs split space first select two random pivot point assign point closer two comparison tree prove metric space satisfi certain expans condit high probabl height comparison tree logarithm number point lead effici search perform also provid upper bound failur probabl return true nearest neighbor experi show comparison tree competit algorithm access actual distanc valu need less triplet comparison competitor|['Siavash Haghiri', 'Debarghya Ghoshdastidar', 'Ulrike von Luxburg']|['stat.ML', 'cs.DS', 'cs.LG']
2017-04-07T11:30:58Z|2017-04-05T08:45:22Z|http://arxiv.org/abs/1704.01311v1|http://arxiv.org/pdf/1704.01311v1|Optimal trade-offs for pattern matching with $k$ mismatches|optim trade pattern match mismatch|Given a pattern of length $m$ and a text of length $n$, the goal in $k$-mismatch pattern matching is to compute, for every $m$-substring of the text, the exact Hamming distance to the pattern or report that it exceeds $k$. This can be solved in either $\widetilde{O}(n \sqrt{k})$ time as shown by Amir et al. [J. Algorithms 2004] or $\widetilde{O}((m + k^2) \cdot n/m)$ time due to a result of Clifford et al. [SODA 2016]. We provide a smooth time trade-off between these two bounds by designing an algorithm working in time $\widetilde{O}( (m + k \sqrt{m}) \cdot n/m)$. We complement this with a matching conditional lower bound, showing that a significantly faster combinatorial algorithm is not possible, unless the combinatorial matrix multiplication conjecture fails.|given pattern length text length goal mismatch pattern match comput everi substr text exact ham distanc pattern report exceed solv either widetild sqrt time shown amir et al algorithm widetild cdot time due result clifford et al soda provid smooth time trade two bound design algorithm work time widetild sqrt cdot complement match condit lower bound show signific faster combinatori algorithm possibl unless combinatori matrix multipl conjectur fail|['Paweł Gawrychowski', 'Przemysław Uznański']|['cs.DS']
2017-04-07T11:30:58Z|2017-04-05T03:21:54Z|http://arxiv.org/abs/1704.01254v1|http://arxiv.org/pdf/1704.01254v1|Local Flow Partitioning for Faster Edge Connectivity|local flow partit faster edg connect|We study the problem of computing a minimum cut in a simple, undirected graph and give a deterministic $O(m \log^2 n \log\log^2 n)$ time algorithm. This improves both on the best previously known deterministic running time of $O(m \log^{12} n)$ (Kawarabayashi and Thorup, STOC 2015) and the best previously known randomized running time of $O(m \log^{3} n)$ (Karger, J.ACM 2000) for this problem, though Karger's algorithm can be further applied to weighted graphs.   Our approach is using the Kawarabayashi and Thorup graph compression technique, which repeatedly finds low-conductance cuts. To find these cuts they use a diffusion-based local algorithm. We use instead a flow-based local algorithm and suitably adjust their framework to work with our flow-based subroutine. Both flow and diffusion based methods have a long history of being applied to finding low conductance cuts. Diffusion algorithms have several variants that are naturally local while it is more complicated to make flow methods local. Some prior work has proven nice properties for local flow based algorithms with respect to improving or cleaning up low conductance cuts. Our flow subroutine, however, is the first that is both local and produces low conductance cuts. Thus, it may be of independent interest.|studi problem comput minimum cut simpl undirect graph give determinist log log log time algorithm improv best previous known determinist run time log kawarabayashi thorup stoc best previous known random run time log karger acm problem though karger algorithm appli weight graph approach use kawarabayashi thorup graph compress techniqu repeat find low conduct cut find cut use diffus base local algorithm use instead flow base local algorithm suitabl adjust framework work flow base subroutin flow diffus base method long histori appli find low conduct cut diffus algorithm sever variant natur local complic make flow method local prior work proven nice properti local flow base algorithm respect improv clean low conduct cut flow subroutin howev first local produc low conduct cut thus may independ interest|['Monika Henzinger', 'Satish Rao', 'Di Wang']|['cs.DS']
2017-04-07T11:30:58Z|2017-04-04T23:29:54Z|http://arxiv.org/abs/1704.01218v1|http://arxiv.org/pdf/1704.01218v1|Storing complex data sharing policies with the Min Mask Sketch|store complex data share polici min mask sketch|"More data is currently being collected and shared by software applications than ever before. In many cases, the user is asked if either all or none of their data can be shared. We hypothesize that in some cases, users would like to share data in more complex ways. In order to implement the sharing of data using more complicated privacy preferences, complex data sharing policies must be used. These complex sharing policies require more space to store than a simple ""all or nothing"" approach to data sharing. In this paper, we present a new probabilistic data structure, called the Min Mask Sketch, to efficiently store these complex data sharing policies. We describe an implementation for the Min Mask Sketch in PostgreSQL and analyze the practicality and feasibility of using a probabilistic data structure for storing complex data sharing policies."|data current collect share softwar applic ever befor mani case user ask either none data share hypothes case user would like share data complex way order implement share data use complic privaci prefer complex data share polici must use complex share polici requir space store simpl noth approach data share paper present new probabilist data structur call min mask sketch effici store complex data share polici describ implement min mask sketch postgresql analyz practic feasibl use probabilist data structur store complex data share polici|['Stephen Smart', 'Christan Grant']|['cs.DS', 'cs.DB']
2017-04-07T11:30:58Z|2017-04-04T21:52:15Z|http://arxiv.org/abs/1704.01200v1|http://arxiv.org/pdf/1704.01200v1|The integrality gap of the Goemans--Linial SDP relaxation for Sparsest   Cut is at least a constant multiple of $\sqrt{\log n}$|integr gap goeman linial sdp relax sparsest cut least constant multipl sqrt log|We prove that the integrality gap of the Goemans--Linial semidefinite programming relaxation for the Sparsest Cut Problem is $\Omega(\sqrt{\log n})$ on inputs with $n$ vertices.|prove integr gap goeman linial semidefinit program relax sparsest cut problem omega sqrt log input vertic|['Assaf Naor', 'Robert Young']|['cs.DS']
2017-04-07T11:31:02Z|2017-04-04T15:48:55Z|http://arxiv.org/abs/1704.01077v1|http://arxiv.org/pdf/1704.01077v1|Computing top-k Closeness Centrality Faster in Unweighted Graphs|comput top close central faster unweight graph|Given a connected graph $G=(V,E)$, the closeness centrality of a vertex $v$ is defined as $\frac{n-1}{\sum_{w \in V} d(v,w)}$. This measure is widely used in the analysis of real-world complex networks, and the problem of selecting the $k$ most central vertices has been deeply analysed in the last decade. However, this problem is computationally not easy, especially for large networks: in the first part of the paper, we prove that it is not solvable in time $\O( E ^{2-\epsilon})$ on directed graphs, for any constant $\epsilon>0$, under reasonable complexity assumptions. Furthermore, we propose a new algorithm for selecting the $k$ most central nodes in a graph: we experimentally show that this algorithm improves significantly both the textbook algorithm, which is based on computing the distance between all pairs of vertices, and the state of the art. For example, we are able to compute the top $k$ nodes in few dozens of seconds in real-world networks with millions of nodes and edges. Finally, as a case study, we compute the $10$ most central actors in the IMDB collaboration network, where two actors are linked if they played together in a movie, and in the Wikipedia citation network, which contains a directed edge from a page $p$ to a page $q$ if $p$ contains a link to $q$.|given connect graph close central vertex defin frac sum measur wide use analysi real world complex network problem select central vertic deepli analys last decad howev problem comput easi especi larg network first part paper prove solvabl time epsilon direct graph ani constant epsilon reason complex assumpt furthermor propos new algorithm select central node graph experiment show algorithm improv signific textbook algorithm base comput distanc pair vertic state art exampl abl comput top node dozen second real world network million node edg final case studi comput central actor imdb collabor network two actor link play togeth movi wikipedia citat network contain direct edg page page contain link|['Elisabetta Bergamini', 'Michele Borassi', 'Pierluigi Crescenzi', 'Andrea Marino', 'Henning Meyerhenke']|['cs.DS']
2017-04-07T11:31:02Z|2017-04-04T14:04:37Z|http://arxiv.org/abs/1704.01023v1|http://arxiv.org/pdf/1704.01023v1|On the Combinatorial Power of the Weisfeiler-Lehman Algorithm|combinatori power weisfeil lehman algorithm|The classical Weisfeiler-Lehman method WL[2] uses edge colors to produce a powerful graph invariant. It is at least as powerful in its ability to distinguish non-isomorphic graphs as the most prominent algebraic graph invariants. It determines not only the spectrum of a graph, and the angles between standard basis vectors and the eigenspaces, but even the angles between projections of standard basis vectors into the eigenspaces. Here, we investigate the combinatorial power of WL[2]. For sufficiently large k, WL[k] determines all combinatorial properties of a graph. Many traditionally used combinatorial invariants are determined by WL[k] for small k. We focus on two fundamental invariants, the num- ber of cycles Cp of length p, and the number of cliques Kp of size p. We show that WL[2] determines the number of cycles of lengths up to 6, but not those of length 8. Also, WL[2] does not determine the number of 4-cliques.|classic weisfeil lehman method wl use edg color produc power graph invari least power abil distinguish non isomorph graph promin algebra graph invari determin onli spectrum graph angl standard basi vector eigenspac even angl project standard basi vector eigenspac investig combinatori power wl suffici larg wl determin combinatori properti graph mani tradit use combinatori invari determin wl small focus two fundament invari num ber cycl cp length number cliqu kp size show wl determin number cycl length length also wl doe determin number cliqu|['Martin Fürer']|['cs.DS', 'F.2.2; G.2.2']
2017-04-07T11:31:02Z|2017-04-06T05:07:10Z|http://arxiv.org/abs/1704.01396v2|http://arxiv.org/pdf/1704.01396v2|A new algorithm for Solving 3-CNF-SAT problem|new algorithm solv cnf sat problem|NP-Complete problems have an important attribute that if one NP-Complete problem can be solved in polynomial time, all NP-Complete problems will have a polynomial solution. The 3-CNF-SAT problem is a NP-Complete problem and the primary method to solve it checks all values of the truth table. This task is of the {\Omega}(2^n) time order. This paper shows that by changing the viewpoint towards the problem, it is possible to know if a 3-CNF-SAT problem is satisfiable in time O(n^10) or not? In this paper, the value of all clauses are considered as false. With this presumption, any of the values inside the truth table can be shown in string form in order to define the set of compatible clauses for each of the strings. So, rather than processing strings, their clauses will be processed implicating that instead of 2^n strings, (O(n^3)) clauses are to be processed; therefore, the time and space complexity of the algorithm would be polynomial.|np complet problem import attribut one np complet problem solv polynomi time np complet problem polynomi solut cnf sat problem np complet problem primari method solv check valu truth tabl task omega time order paper show chang viewpoint toward problem possibl know cnf sat problem satisfi time paper valu claus consid fals presumpt ani valu insid truth tabl shown string form order defin set compat claus string rather process string claus process implic instead string claus process therefor time space complex algorithm would polynomi|['Belal Qasemi']|['cs.DS']
2017-04-07T11:31:02Z|2017-04-04T07:56:27Z|http://arxiv.org/abs/1704.00908v1|http://arxiv.org/pdf/1704.00908v1|(1, k)-Swap Local Search for Maximum Clique Problem|swap local search maximum cliqu problem|Given simple undirected graph G = (V, E), the Maximum Clique Problem(MCP) is that of finding a maximum-cardinality subset Q of V such that any two vertices in Q are adjacent. We present a modified local search algorithm for this problem. Our algorithm build some maximal solution and can determine in polynomial time if a maximal solution can be improved by replacing a single vertex with k, k > 1, others. We test our algorithms on DIMACS[5], Sloane[15], BHOSLIB[1], Iovanella[8] and our random instances.|given simpl undirect graph maximum cliqu problem mcp find maximum cardin subset ani two vertic adjac present modifi local search algorithm problem algorithm build maxim solut determin polynomi time maxim solut improv replac singl vertex test algorithm dimac sloan bhoslib iovanella random instanc|['Lavnikevich Nikolay']|['math.OC', 'cs.DS', '05C85']
2017-04-07T11:31:02Z|2017-04-04T07:26:03Z|http://arxiv.org/abs/1704.00899v1|http://arxiv.org/pdf/1704.00899v1|Dynamic Rank Maximal Matchings|dynam rank maxim match|We consider the problem of matching applicants to posts where applicants have preferences over posts. Thus the input to our problem is a bipartite graph G = (A U P,E), where A denotes a set of applicants, P is a set of posts, and there are ranks on edges which denote the preferences of applicants over posts. A matching M in G is called rank-maximal if it matches the maximum number of applicants to their rank 1 posts, subject to this the maximum number of applicants to their rank 2 posts, and so on.   We consider this problem in a dynamic setting, where vertices and edges can be added and deleted at any point. Let n and m be the number of vertices and edges in an instance G, and r be the maximum rank used by any rank-maximal matching in G. We give a simple O(r(m+n))-time algorithm to update an existing rank-maximal matching under each of these changes. When r = o(n), this is faster than recomputing a rank-maximal matching completely using a known algorithm like that of Irving et al., which takes time O(min((r + n, r*sqrt(n))m).|consid problem match applic post applic prefer post thus input problem bipartit graph denot set applic set post rank edg denot prefer applic post match call rank maxim match maximum number applic rank post subject maximum number applic rank post consid problem dynam set vertic edg ad delet ani point let number vertic edg instanc maximum rank use ani rank maxim match give simpl time algorithm updat exist rank maxim match chang faster recomput rank maxim match complet use known algorithm like irv et al take time min sqrt|['Prajakta Nimbhorkar', 'Arvind Rameshwar V']|['cs.DM', 'cs.DS']
2017-04-07T11:31:02Z|2017-04-03T22:53:58Z|http://arxiv.org/abs/1704.00830v1|http://arxiv.org/pdf/1704.00830v1|Locally Self-Adjusting Skip Graphs|local self adjust skip graph|We present a distributed self-adjusting algorithm for skip graphs that minimizes the average routing costs between arbitrary communication pairs by performing topological adaptation to the communication pattern. Our algorithm is fully decentralized, conforms to the $\mathcal{CONGEST}$ model (i.e. uses $O(\log n)$ bit messages), and requires $O(\log n)$ bits of memory for each node, where $n$ is the total number of nodes. Upon each communication request, our algorithm first establishes communication by using the standard skip graph routing, and then locally and partially reconstructs the skip graph topology to perform topological adaptation. We propose a computational model for such algorithms, as well as a yardstick (working set property) to evaluate them. Our working set property can also be used to evaluate self-adjusting algorithms for other graph classes where multiple tree-like subgraphs overlap (e.g. hypercube networks). We derive a lower bound of the amortized routing cost for any algorithm that follows our model and serves an unknown sequence of communication requests. We show that the routing cost of our algorithm is at most a constant factor more than the amortized routing cost of any algorithm conforming to our computational model. We also show that the expected transformation cost for our algorithm is at most a logarithmic factor more than the amortized routing cost of any algorithm conforming to our computational model.|present distribut self adjust algorithm skip graph minim averag rout cost arbitrari communic pair perform topolog adapt communic pattern algorithm fulli decentr conform mathcal congest model use log bit messag requir log bit memori node total number node upon communic request algorithm first establish communic use standard skip graph rout local partial reconstruct skip graph topolog perform topolog adapt propos comput model algorithm well yardstick work set properti evalu work set properti also use evalu self adjust algorithm graph class multipl tree like subgraph overlap hypercub network deriv lower bound amort rout cost ani algorithm follow model serv unknown sequenc communic request show rout cost algorithm constant factor amort rout cost ani algorithm conform comput model also show expect transform cost algorithm logarithm factor amort rout cost ani algorithm conform comput model|['Sikder Huq', 'Sukumar Ghosh']|['cs.DC', 'cs.DS']
2017-04-07T11:31:02Z|2017-04-03T20:56:55Z|http://arxiv.org/abs/1704.00807v1|http://arxiv.org/abs/1704.00807v1|Synchronization Strings: Codes for Insertions and Deletions Approaching   the Singleton Bound|synchron string code insert delet approach singleton bound|We introduce synchronization strings as a novel way of efficiently dealing with synchronization errors, i.e., insertions and deletions. Synchronization errors are strictly more general and much harder to deal with than commonly considered half-errors, i.e., symbol corruptions and erasures. For every $\epsilon >0$, synchronization strings allow to index a sequence with an $\epsilon^{-O(1)}$ size alphabet such that one can efficiently transform $k$ synchronization errors into $(1+\epsilon)k$ half-errors. This powerful new technique has many applications. In this paper, we focus on designing insdel codes, i.e., error correcting block codes (ECCs) for insertion deletion channels.   While ECCs for both half-errors and synchronization errors have been intensely studied, the later has largely resisted progress. Indeed, it took until 1999 for the first insdel codes with constant rate, constant distance, and constant alphabet size to be constructed by Schulman and Zuckerman. Insdel codes for asymptotically large or small noise rates were given in 2016 by Guruswami et al. but these codes are still polynomially far from the optimal rate-distance tradeoff. This makes the understanding of insdel codes up to this work equivalent to what was known for regular ECCs after Forney introduced concatenated codes in his doctoral thesis 50 years ago.   A direct application of our synchronization strings based indexing method gives a simple black-box construction which transforms any ECC into an equally efficient insdel code with a slightly larger alphabet size. This instantly transfers much of the highly developed understanding for regular ECCs over large constant alphabets into the realm of insdel codes. Most notably, we obtain efficient insdel codes which get arbitrarily close to the optimal rate-distance tradeoff given by the Singleton bound for the complete noise spectrum.|introduc synchron string novel way effici deal synchron error insert delet synchron error strict general much harder deal common consid half error symbol corrupt erasur everi epsilon synchron string allow index sequenc epsilon size alphabet one effici transform synchron error epsilon half error power new techniqu mani applic paper focus design insdel code error correct block code ecc insert delet channel ecc half error synchron error intens studi later larg resist progress inde took first insdel code constant rate constant distanc constant alphabet size construct schulman zuckerman insdel code asymptot larg small nois rate given guruswami et al code still polynomi far optim rate distanc tradeoff make understand insdel code work equival known regular ecc forney introduc concaten code doctor thesi year ago direct applic synchron string base index method give simpl black box construct transform ani ecc equal effici insdel code slight larger alphabet size instant transfer much high develop understand regular ecc larg constant alphabet realm insdel code notabl obtain effici insdel code get arbitrarili close optim rate distanc tradeoff given singleton bound complet nois spectrum|['Bernhard Haeupler', 'Amirbehshad Shahrasbi']|['cs.IT', 'cs.DS', 'math.IT']
2017-04-07T11:31:02Z|2017-04-03T19:07:48Z|http://arxiv.org/abs/1704.00765v1|http://arxiv.org/pdf/1704.00765v1|Quantum Algorithms for Graph Connectivity and Formula Evaluation|quantum algorithm graph connect formula evalu|"We give a new upper bound on the quantum query complexity of deciding $st$-connectivity on certain classes of planar graphs, and show the bound is sometimes exponentially better than previous results. We then show Boolean formula evaluation reduces to deciding connectivity on just such a class of graphs. Applying the algorithm for $st$-connectivity to Boolean formula evaluation problems, we match the $O(\sqrt{N})$ bound on the quantum query complexity of evaluating formulas on $N$ variables, give a quadratic speed-up over the classical query complexity of a certain class of promise Boolean formulas, and show this approach can yield superpolynomial quantum/classical separations. These results indicate that this $st$-connectivity-based approach may be the ""right"" way of looking at quantum algorithms for formula evaluation."|give new upper bound quantum queri complex decid st connect certain class planar graph show bound sometim exponenti better previous result show boolean formula evalu reduc decid connect class graph appli algorithm st connect boolean formula evalu problem match sqrt bound quantum queri complex evalu formula variabl give quadrat speed classic queri complex certain class promis boolean formula show approach yield superpolynomi quantum classic separ result indic st connect base approach may right way look quantum algorithm formula evalu|['Stacey Jeffery', 'Shelby Kimmel']|['quant-ph', 'cs.CC', 'cs.DS']
2017-04-07T11:31:02Z|2017-04-03T17:45:10Z|http://arxiv.org/abs/1704.00705v1|http://arxiv.org/pdf/1704.00705v1|Graph Partitioning with Acyclicity Constraints|graph partit acycl constraint|Graphs are widely used to model execution dependencies in applications. In particular, the NP-complete problem of partitioning a graph under constraints receives enormous attention by researchers because of its applicability in multiprocessor scheduling. We identified the additional constraint of acyclic dependencies between blocks when mapping computer vision and imaging applications to a heterogeneous embedded multiprocessor. Existing algorithms and heuristics do not address this requirement and deliver results that are not applicable for our use-case. In this work, we show that this more constrained version of the graph partitioning problem is NP-complete and present heuristics that achieve a close approximation of the optimal solution found by an exhaustive search for small problem instances and much better scalability for larger instances. In addition, we can show a positive impact on the schedule of a real imaging application that improves communication volume and execution time.|graph wide use model execut depend applic particular np complet problem partit graph constraint receiv enorm attent research becaus applic multiprocessor schedul identifi addit constraint acycl depend block map comput vision imag applic heterogen embed multiprocessor exist algorithm heurist address requir deliv result applic use case work show constrain version graph partit problem np complet present heurist achiev close approxim optim solut found exhaust search small problem instanc much better scalabl larger instanc addit show posit impact schedul real imag applic improv communic volum execut time|['Orlando Moreira', 'Merten Popp', 'Christian Schulz']|['cs.DS', 'cs.CV', 'cs.DC']
2017-04-07T11:31:02Z|2017-04-03T17:16:39Z|http://arxiv.org/abs/1704.00693v1|http://arxiv.org/pdf/1704.00693v1|Loop Tiling in Large-Scale Stencil Codes at Run-time with OPS|loop tile larg scale stencil code run time op|Stencil computations occur in a multitude of scientific simulations and therefore have been the subject of many domain-specific languages including the OPS (Oxford Parallel library for Structured meshes) DSL embedded in C/C++/Fortran. OPS is currently used in several large partial differential equations (PDE) applications, and has been used as a vehicle to experiment with, and deploy performance improving optimisations. The key common bottleneck in most stencil codes is data movement, and other research has shown that improving data locality through optimisations that schedule across loops do particularly well. However, in many large PDE applications it is not possible to apply such optimisations through a compiler because in larger-scale codes, there are a huge number of options, execution paths and data per grid point, many dependent on run-time parameters, and the code is distributed across a number of different compilation units. In this paper, we adapt the data locality improving optimisation called iteration space slicing for use in large OPS apps, relying on run-time analysis and delayed execution. We observe speedups of 2x on the Cloverleaf 2D/3D proxy application, which contain 83/141 loops respectively. The approach is generally applicable to any stencil DSL that provides per loop data access information.|stencil comput occur multitud scientif simul therefor subject mani domain specif languag includ op oxford parallel librari structur mesh dsl embed fortran op current use sever larg partial differenti equat pde applic use vehicl experi deploy perform improv optimis key common bottleneck stencil code data movement research shown improv data local optimis schedul across loop particular well howev mani larg pde applic possibl appli optimis compil becaus larger scale code huge number option execut path data per grid point mani depend run time paramet code distribut across number differ compil unit paper adapt data local improv optimis call iter space slice use larg op app reli run time analysi delay execut observ speedup cloverleaf proxi applic contain loop respect approach general applic ani stencil dsl provid per loop data access inform|['Istvan Z Reguly', 'Gihan R Mudalige', 'Mike B Giles']|['cs.PF', 'cs.DC', 'cs.DS']
2017-04-07T11:31:07Z|2017-04-03T15:12:38Z|http://arxiv.org/abs/1704.00633v1|http://arxiv.org/pdf/1704.00633v1|Optimal lower bounds for universal relation, and for samplers and   finding duplicates in streams|optim lower bound univers relat sampler find duplic stream|In the communication problem $\mathbf{UR}$ (universal relation) [KRW95], Alice and Bob respectively receive $x, y \in\{0,1\}^n$ with the promise that $x\neq y$. The last player to receive a message must output an index $i$ such that $x_i\neq y_i$. We prove that the randomized one-way communication complexity of this problem in the public coin model is exactly $\Theta(\min\{n,\log(1/\delta)\log^2(\frac n{\log(1/\delta)})\})$ for failure probability $\delta$. Our lower bound holds even if promised $\mathop{support}(y)\subset \mathop{support}(x)$. As a corollary, we obtain optimal lower bounds for $\ell_p$-sampling in strict turnstile streams for $0\le p < 2$, as well as for the problem of finding duplicates in a stream. Our lower bounds do not need to use large weights, and hold even if promised $x\in\{0,1\}^n$ at all points in the stream.   We give two different proofs of our main result. The first proof demonstrates that any algorithm $\mathcal A$ solving sampling problems in turnstile streams in low memory can be used to encode subsets of $[n]$ of certain sizes into a number of bits below the information theoretic minimum. Our encoder makes adaptive queries to $\mathcal A$ throughout its execution, but done carefully so as to not violate correctness. This is accomplished by injecting random noise into the encoder's interactions with $\mathcal A$, which is loosely motivated by techniques in differential privacy. Our second proof is via a novel randomized reduction from Augmented Indexing [MNSW98] which needs to interact with $\mathcal A$ adaptively. To handle the adaptivity we identify certain likely interaction patterns and union bound over them to guarantee correct interaction on all of them. To guarantee correctness, it is important that the interaction hides some of its randomness from $\mathcal A$ in the reduction.|communic problem mathbf ur univers relat krw alic bob respect receiv promis neq last player receiv messag must output index neq prove random one way communic complex problem public coin model exact theta min log delta log frac log delta failur probabl delta lower bound hold even promis mathop support subset mathop support corollari obtain optim lower bound ell sampl strict turnstil stream le well problem find duplic stream lower bound need use larg weight hold even promis point stream give two differ proof main result first proof demonstr ani algorithm mathcal solv sampl problem turnstil stream low memori use encod subset certain size number bit inform theoret minimum encod make adapt queri mathcal throughout execut done care violat correct accomplish inject random nois encod interact mathcal loos motiv techniqu differenti privaci second proof via novel random reduct augment index mnsw need interact mathcal adapt handl adapt identifi certain like interact pattern union bound guarante correct interact guarante correct import interact hide random mathcal reduct|['Michael Kapralov', 'Jelani Nelson', 'Jakub Pachocki', 'Zhengyu Wang', 'David P. Woodruff', 'Mobin Yahyazadeh']|['cs.CC', 'cs.DS']
2017-04-07T11:31:07Z|2017-04-03T13:15:59Z|http://arxiv.org/abs/1704.00565v1|http://arxiv.org/pdf/1704.00565v1|Dynamic Planar Embeddings of Dynamic Graphs|dynam planar embed dynam graph|We present an algorithm to support the dynamic embedding in the plane of a dynamic graph. An edge can be inserted across a face between two vertices on the face boundary (we call such a vertex pair linkable), and edges can be deleted. The planar embedding can also be changed locally by flipping components that are connected to the rest of the graph by at most two vertices.   Given vertices $u,v$, linkable$(u,v)$ decides whether $u$ and $v$ are linkable in the current embedding, and if so, returns a list of suggestions for the placement of $(u,v)$ in the embedding. For non-linkable vertices $u,v$, we define a new query, one-flip-linkable$(u,v)$ providing a suggestion for a flip that will make them linkable if one exists. We support all updates and queries in O(log$^2 n$) time. Our time bounds match those of Italiano et al. for a static (flipless) embedding of a dynamic graph.   Our new algorithm is simpler, exploiting that the complement of a spanning tree of a connected plane graph is a spanning tree of the dual graph. The primal and dual trees are interpreted as having the same Euler tour, and a main idea of the new algorithm is an elegant interaction between top trees over the two trees via their common Euler tour.|present algorithm support dynam embed plane dynam graph edg insert across face two vertic face boundari call vertex pair linkabl edg delet planar embed also chang local flip compon connect rest graph two vertic given vertic linkabl decid whether linkabl current embed return list suggest placement embed non linkabl vertic defin new queri one flip linkabl provid suggest flip make linkabl one exist support updat queri log time time bound match italiano et al static flipless embed dynam graph new algorithm simpler exploit complement span tree connect plane graph span tree dual graph primal dual tree interpret euler tour main idea new algorithm eleg interact top tree two tree via common euler tour|['Jacob Holm', 'Eva Rotenberg']|['cs.DS', 'cs.CG']
2017-04-07T11:31:07Z|2017-04-03T10:22:32Z|http://arxiv.org/abs/1704.00513v1|http://arxiv.org/pdf/1704.00513v1|Optimizing Communication by Compression for Multi-GPU Scalable   Breadth-First Searches|optim communic compress multi gpu scalabl breadth first search|The Breadth First Search (BFS) algorithm is the foundation and building block of many higher graph-based operations such as spanning trees, shortest paths and betweenness centrality. The importance of this algorithm increases each day due to it is a key requirement for many data structures which are becoming popular nowadays. When the BFS algorithm is parallelized by distributing the graph between several processors the interconnection network limits the performance. Hence, improvements on this area may benefit the overall performance of the algorithm.   This work presents an alternative compression scheme for communications in distributed BFS processing. It focuses on BFS processors using General-Purpose Graphics Processing Units.|breadth first search bfs algorithm foundat build block mani higher graph base oper span tree shortest path central import algorithm increas day due key requir mani data structur becom popular nowaday bfs algorithm parallel distribut graph sever processor interconnect network limit perform henc improv area may benefit overal perform algorithm work present altern compress scheme communic distribut bfs process focus bfs processor use general purpos graphic process unit|['Julian Romera']|['cs.DC', 'cs.DS', 'cs.PF', '68P30, 05C50', 'H.3.4; I.2.8']
2017-04-07T11:31:07Z|2017-04-03T00:44:22Z|http://arxiv.org/abs/1704.00395v1|http://arxiv.org/pdf/1704.00395v1|A Message-Passing Algorithm for Graph Isomorphism|messag pass algorithm graph isomorph|A message-passing procedure for solving the graph isomorphism problem is proposed. The procedure resembles the belief-propagation algorithm in the context of graphical models inference and LDPC decoding. To enable the algorithm, the input graphs are transformed into intermediate canonical representations of bipartite graphs. The matching procedure injects specially designed input patterns to the canonical graphs and runs a message-passing algorithm to generate two output fingerprints that are matched if and only if the input graphs are isomorphic.|messag pass procedur solv graph isomorph problem propos procedur resembl belief propag algorithm context graphic model infer ldpc decod enabl algorithm input graph transform intermedi canon represent bipartit graph match procedur inject special design input pattern canon graph run messag pass algorithm generat two output fingerprint match onli input graph isomorph|['Mohamed Mansour']|['cs.DS', 'cs.CC']
2017-04-07T11:31:07Z|2017-04-02T19:34:22Z|http://arxiv.org/abs/1704.00355v1|http://arxiv.org/pdf/1704.00355v1|Local Guarantees in Graph Cuts and Clustering|local guarante graph cut cluster|Correlation Clustering is an elegant model that captures fundamental graph cut problems such as Min $s-t$ Cut, Multiway Cut, and Multicut, extensively studied in combinatorial optimization. Here, we are given a graph with edges labeled $+$ or $-$ and the goal is to produce a clustering that agrees with the labels as much as possible: $+$ edges within clusters and $-$ edges across clusters. The classical approach towards Correlation Clustering (and other graph cut problems) is to optimize a global objective. We depart from this and study local objectives: minimizing the maximum number of disagreements for edges incident on a single node, and the analogous max min agreements objective. This naturally gives rise to a family of basic min-max graph cut problems. A prototypical representative is Min Max $s-t$ Cut: find an $s-t$ cut minimizing the largest number of cut edges incident on any node. We present the following results: $(1)$ an $O(\sqrt{n})$-approximation for the problem of minimizing the maximum total weight of disagreement edges incident on any node (thus providing the first known approximation for the above family of min-max graph cut problems), $(2)$ a remarkably simple $7$-approximation for minimizing local disagreements in complete graphs (improving upon the previous best known approximation of $48$), and $(3)$ a $1/(2+\varepsilon)$-approximation for maximizing the minimum total weight of agreement edges incident on any node, hence improving upon the $1/(4+\varepsilon)$-approximation that follows from the study of approximate pure Nash equilibria in cut and party affiliation games.|correl cluster eleg model captur fundament graph cut problem min cut multiway cut multicut extens studi combinatori optim given graph edg label goal produc cluster agre label much possibl edg within cluster edg across cluster classic approach toward correl cluster graph cut problem optim global object depart studi local object minim maximum number disagr edg incid singl node analog max min agreement object natur give rise famili basic min max graph cut problem prototyp repres min max cut find cut minim largest number cut edg incid ani node present follow result sqrt approxim problem minim maximum total weight disagr edg incid ani node thus provid first known approxim abov famili min max graph cut problem remark simpl approxim minim local disagr complet graph improv upon previous best known approxim varepsilon approxim maxim minimum total weight agreement edg incid ani node henc improv upon varepsilon approxim follow studi approxim pure nash equilibria cut parti affili game|['Moses Charikar', 'Neha Gupta', 'Roy Schwartz']|['cs.DS']
2017-04-07T11:31:07Z|2017-04-01T09:21:56Z|http://arxiv.org/abs/1704.00145v1|http://arxiv.org/pdf/1704.00145v1|Inverse Fractional Knapsack Problem with Profits and Costs Modification|invers fraction knapsack problem profit cost modif|We address in this paper the problem of modifying both profits and costs of a fractional knapsack problem optimally such that a prespecified solution becomes an optimal solution with prespect to new parameters. This problem is called the inverse fractional knapsack problem. Concerning the $l_1$-norm, we first prove that the problem is NP-hard. The problem can be however solved in quadratic time if we only modify profit parameters. Additionally, we develop a quadratic-time algorithm that solves the inverse fractional knapsack problem under $l_\infty$-norm.|address paper problem modifi profit cost fraction knapsack problem optim prespecifi solut becom optim solut prespect new paramet problem call invers fraction knapsack problem concern norm first prove problem np hard problem howev solv quadrat time onli modifi profit paramet addit develop quadrat time algorithm solv invers fraction knapsack problem infti norm|['Kien Trung Nguyen', 'Huynh Duc Quoc']|['math.OC', 'cs.DS']
2017-04-07T11:31:07Z|2017-03-31T10:33:24Z|http://arxiv.org/abs/1703.10840v1|http://arxiv.org/pdf/1703.10840v1|Treewidth distance on phylogenetic trees|treewidth distanc phylogenet tree|"In this article we study the treewidth of the \emph{display graph}, an auxiliary graph structure obtained from the fusion of phylogenetic (i.e., evolutionary) trees at their leaves. Earlier work has shown that the treewidth of the display graph is bounded if the trees are in some formal sense topologically similar. Here we further expand upon this relationship. We analyse a number of reduction rules which are commonly used in the phylogenetics literature to obtain fixed parameter tractable algorithms. In some cases (the \emph{subtree} reduction) the reduction rules behave similarly with respect to treewidth, while others (the \emph{cluster} reduction) behave very differently, and the behaviour of the \emph{chain reduction} is particularly intriguing because of its link with graph separators and forbidden minors. We also show that the gap between treewidth and Tree Bisection and Reconnect (TBR) distance can be infinitely large, and that unlike, for example, planar graphs the treewidth of the display graph can be as much as linear in its number of vertices. On a slightly different note we show that if a display graph is formed from the fusion of a phylogenetic network and a tree, rather than from two trees, the treewidth of the display graph is bounded whenever the tree can be topologically embedded (""displayed"") within the network. This opens the door to the formulation of the display problem in Monadic Second Order Logic (MSOL). A number of other auxiliary results are given. We conclude with a discussion and list a number of open problems."|articl studi treewidth emph display graph auxiliari graph structur obtain fusion phylogenet evolutionari tree leav earlier work shown treewidth display graph bound tree formal sens topolog similar expand upon relationship analys number reduct rule common use phylogenet literatur obtain fix paramet tractabl algorithm case emph subtre reduct reduct rule behav similar respect treewidth emph cluster reduct behav veri differ behaviour emph chain reduct particular intrigu becaus link graph separ forbidden minor also show gap treewidth tree bisect reconnect tbr distanc infinit larg unlik exampl planar graph treewidth display graph much linear number vertic slight differ note show display graph form fusion phylogenet network tree rather two tree treewidth display graph bound whenev tree topolog embed display within network open door formul display problem monad second order logic msol number auxiliari result given conclud discuss list number open problem|['Steven Kelk', 'Georgios Stamoulis', 'Taoyang Wu']|['cs.DM', 'cs.DS', 'q-bio.PE']
2017-04-07T11:31:07Z|2017-03-31T02:03:36Z|http://arxiv.org/abs/1703.10731v1|http://arxiv.org/pdf/1703.10731v1|An analysis of budgeted parallel search on conditional Galton-Watson   trees|analysi budget parallel search condit galton watson tree|Recently Avis and Jordan have demonstrated the efficiency of a simple technique called budgeting for the parallelization of a number of tree search algorithms. The idea is to limit the amount of work that a processor performs before it terminates its search and returns any unexplored nodes to a master process. This limit is set by a critical budget parameter which determines the overhead of the process. In this paper we study the behaviour of the budget parameter on conditional Galton-Watson trees obtaining asymptotically tight bounds on this overhead. We present empirical results to show that this bound is surprisingly accurate in practice.|recent avi jordan demonstr effici simpl techniqu call budget parallel number tree search algorithm idea limit amount work processor perform befor termin search return ani unexplor node master process limit set critic budget paramet determin overhead process paper studi behaviour budget paramet condit galton watson tree obtain asymptot tight bound overhead present empir result show bound surpris accur practic|['David Avis', 'Luc Devroye']|['cs.DS', 'cs.DC']
2017-04-07T11:31:07Z|2017-03-30T18:42:52Z|http://arxiv.org/abs/1703.10633v1|http://arxiv.org/pdf/1703.10633v1|Light spanners for bounded treewidth graphs imply light spanners for   $H$-minor-free graphs|light spanner bound treewidth graph impli light spanner minor free graph|Grigni and Hung~\cite{GH12} conjectured that H-minor-free graphs have $(1+\epsilon)$-spanners that are light, that is, of weight $g( H ,\epsilon)$ times the weight of the minimum spanning tree for some function $g$. This conjecture implies the {\em efficient} polynomial-time approximation scheme (PTAS) of the traveling salesperson problem in $H$-minor free graphs; that is, a PTAS whose running time is of the form $2^{f(\epsilon)}n^{O(1)}$ for some function $f$. The state of the art PTAS for TSP in H-minor-free-graphs has running time $n^{1/\epsilon^c}$. We take a further step toward proving this conjecture by showing that if the bounded treewidth graphs have light greedy spanners, then the conjecture is true. We also prove that the greedy spanner of a bounded pathwidth graph is light and discuss the possibility of extending our proof to bounded treewidth graphs.|grigni hung cite gh conjectur minor free graph epsilon spanner light weight epsilon time weight minimum span tree function conjectur impli em effici polynomi time approxim scheme ptas travel salesperson problem minor free graph ptas whose run time form epsilon function state art ptas tsp minor free graph run time epsilon take step toward prove conjectur show bound treewidth graph light greedi spanner conjectur true also prove greedi spanner bound pathwidth graph light discuss possibl extend proof bound treewidth graph|['Glencora Borradaile', 'Hung Le']|['cs.DS', 'cs.DM', 'F.2.2; G.2.2']
2017-04-07T11:31:07Z|2017-03-30T18:26:53Z|http://arxiv.org/abs/1703.10628v1|http://arxiv.org/pdf/1703.10628v1|Study on Resource Efficiency of Distributed Graph Processing|studi resourc effici distribut graph process|Graphs may be used to represent many different problem domains -- a concrete example is that of detecting communities in social networks, which are represented as graphs. With big data and more sophisticated applications becoming widespread in recent years, graph processing has seen an emergence of requirements pertaining data volume and volatility. This multidisciplinary study presents a review of relevant distributed graph processing systems. Herein they are presented in groups defined by common traits (distributed processing paradigm, type of graph operations, among others), with an overview of each system's strengths and weaknesses. The set of systems is then narrowed down to a set of two, upon which quantitative analysis was performed. For this quantitative comparison of systems, focus was cast on evaluating the performance of algorithms for the problem of detecting communities. To help further understand the evaluations performed, a background is provided on graph clustering.|graph may use repres mani differ problem domain concret exampl detect communiti social network repres graph big data sophist applic becom widespread recent year graph process seen emerg requir pertain data volum volatil multidisciplinari studi present review relev distribut graph process system herein present group defin common trait distribut process paradigm type graph oper among overview system strength weak set system narrow set two upon quantit analysi perform quantit comparison system focus cast evalu perform algorithm problem detect communiti help understand evalu perform background provid graph cluster|['Miguel E. Coimbra', 'Alexandre P. Francisco', 'Luis Veiga']|['cs.DC', 'cs.DS']
2017-04-07T11:31:13Z|2017-03-30T17:44:45Z|http://arxiv.org/abs/1703.10594v1|http://arxiv.org/pdf/1703.10594v1|The Dynamics of Rank-Maximal and Popular Matchings|dynam rank maxim popular match|Given a bipartite graph, where the two sets of vertices are applicants and posts and ranks on the edges represent preferences of applicants over posts, a {\em rank-maximal} matching is one in which the maximum number of applicants is matched to their rank one posts and subject to this condition, the maximum number of applicants is matched to their rank two posts, and so on. A rank-maximal matching can be computed in $O(\min(C \sqrt{n},n) m)$ time, where $n$ denotes the number of applicants, $m$ the number of edges and $C$ the maximum rank of an edge in an optimal solution \cite{rankmax}.   We study the dynamic version of the problem in which a new applicant or post may be added to the graph and we would like to maintain a rank-maximal matching. We show that after the arrival of one vertex, we are always able to update the existing rank-maximal matching in $O(m)$ time. Moreover, we are able to do that in such a way that the number of alterations is minimized. The time bound can be considered optimal under the circumstances, as improving it would imply a better running time for the rank-maximal matching problem.   As a by-product we show also the analogous result for the dynamic version of the (one-sided) popular matching problem.|given bipartit graph two set vertic applic post rank edg repres prefer applic post em rank maxim match one maximum number applic match rank one post subject condit maximum number applic match rank two post rank maxim match comput min sqrt time denot number applic number edg maximum rank edg optim solut cite rankmax studi dynam version problem new applic post may ad graph would like maintain rank maxim match show arriv one vertex alway abl updat exist rank maxim match time moreov abl way number alter minim time bound consid optim circumst improv would impli better run time rank maxim match problem product show also analog result dynam version one side popular match problem|['Pratik Ghosal', 'Adam Kunysz', 'Katarzyna Paluch']|['cs.DS']
2017-04-07T11:31:13Z|2017-03-30T16:58:11Z|http://arxiv.org/abs/1703.10565v1|http://arxiv.org/pdf/1703.10565v1|Fleet management for autonomous vehicles: Online PDP under special   constraints|fleet manag autonom vehicl onlin pdp special constraint|The VIPAFLEET project consists in developing models and algorithms for man- aging a fleet of Individual Public Autonomous Vehicles (VIPA). Hereby, we consider a fleet of cars distributed at specified stations in an industrial area to supply internal transportation, where the cars can be used in different modes of circulation (tram mode, elevator mode, taxi mode). One goal is to develop and implement suitable algorithms for each mode in order to satisfy all the requests under an economic point of view by minimizing the total tour length. The innovative idea and challenge of the project is to develop and install a dynamic fleet management system that allows the operator to switch between the different modes within the different periods of the day according to the dynamic transportation demands of the users. We model the underlying online transportation system and propose a correspond- ing fleet management framework, to handle modes, demands and commands. We consider two modes of circulation, tram and elevator mode, propose for each mode appropriate on- line algorithms and evaluate their performance, both in terms of competitive analysis and practical behavior.|vipafleet project consist develop model algorithm man age fleet individu public autonom vehicl vipa herebi consid fleet car distribut specifi station industri area suppli intern transport car use differ mode circul tram mode elev mode taxi mode one goal develop implement suitabl algorithm mode order satisfi request econom point view minim total tour length innov idea challeng project develop instal dynam fleet manag system allow oper switch differ mode within differ period day accord dynam transport demand user model onlin transport system propos correspond ing fleet manag framework handl mode demand command consid two mode circul tram elev mode propos mode appropri line algorithm evalu perform term competit analysi practic behavior|['Sahar Bsaybes', 'Alain Quilliot', 'Annegret K. Wagler']|['cs.DM', 'cs.DS']
2017-04-07T11:31:13Z|2017-03-30T12:52:46Z|http://arxiv.org/abs/1703.10446v1|http://arxiv.org/pdf/1703.10446v1|Gelly-Scheduling: Distributed Graph Processing for Network Service   Placement|gelli schedul distribut graph process network servic placement|Community network micro-clouds (CNMCs) have seen an increase in the last fifteen years. Their members contact nodes which operate Internet proxies, web servers, user file storage and video streaming services, to name a few. Detecting communities of nodes with properties (such as co-location) and assessing node eligibility for service placement is thus a key-factor in optimizing the experience of users. We present an approach for community finding using a label propagation graph algorithm to address the multi-objective challenge of optimizing service placement in CNMCs. Herein we: i) highlight the applicability of leader election heuristics which are important for service placement in community networks and scheduler-dependent scenarios; ii) present a novel decentralized solution designed as a scalable alternative for the problem of service placement, which has mostly seen computational approaches based on centralization.|communiti network micro cloud cnmcs seen increas last fifteen year member contact node oper internet proxi web server user file storag video stream servic name detect communiti node properti co locat assess node elig servic placement thus key factor optim experi user present approach communiti find use label propag graph algorithm address multi object challeng optim servic placement cnmcs herein highlight applic leader elect heurist import servic placement communiti network schedul depend scenario ii present novel decentr solut design scalabl altern problem servic placement seen comput approach base central|['Miguel E. Coimbra', 'Alexandre P. Francisco', 'Luis Veiga']|['cs.DC', 'cs.DS']
2017-04-07T11:31:13Z|2017-03-30T11:25:13Z|http://arxiv.org/abs/1703.10415v1|http://arxiv.org/pdf/1703.10415v1|A Polynomial-time Algorithm to Achieve Extended Justified Representation|polynomi time algorithm achiev extend justifi represent|We consider a committee voting setting in which each voter approves of a subset of candidates and based on the approvals, a target number of candidates are to be selected. In particular we focus on the axiomatic property called extended justified representation (EJR). Although a committee satisfying EJR is guaranteed to exist, the computational complexity of finding such a committee has been an open problem and explicitly mentioned in multiple recent papers. We settle the complexity of finding a committee satisfying EJR by presenting a polynomial-time algorithm for the problem. Our algorithmic approach may be useful for constructing other voting rules in multi-winner voting.|consid committe vote set voter approv subset candid base approv target number candid select particular focus axiomat properti call extend justifi represent ejr although committe satisfi ejr guarante exist comput complex find committe open problem explicit mention multipl recent paper settl complex find committe satisfi ejr present polynomi time algorithm problem algorithm approach may use construct vote rule multi winner vote|['Haris Aziz', 'Shenwei Huang']|['cs.GT', 'cs.DS', '91A12, 68Q15', 'F.2; J.4']
2017-04-07T11:31:13Z|2017-03-30T09:39:22Z|http://arxiv.org/abs/1703.10380v1|http://arxiv.org/pdf/1703.10380v1|Finding Even Cycles Faster via Capped k-Walks|find even cycl faster via cap walk|"In this paper, we consider the problem of finding a cycle of length $2k$ (a $C_{2k}$) in an undirected graph $G$ with $n$ nodes and $m$ edges for constant $k\ge2$. A classic result by Bondy and Simonovits [J.Comb.Th.'74] implies that if $m \ge100k n^{1+1/k}$, then $G$ contains a $C_{2k}$, further implying that one needs to consider only graphs with $m = O(n^{1+1/k})$.   Previously the best known algorithms were an $O(n^2)$ algorithm due to Yuster and Zwick [J.Disc.Math'97] as well as a $O(m^{2-(1+\lceil k/2\rceil^{-1})/(k+1)})$ algorithm by Alon et al. [Algorithmica'97].   We present an algorithm that uses $O(m^{2k/(k+1)})$ time and finds a $C_{2k}$ if one exists. This bound is $O(n^2)$ exactly when $m=\Theta(n^{1+1/k})$. For $4$-cycles our new bound coincides with Alon et al., while for every $k>2$ our bound yields a polynomial improvement in $m$.   Yuster and Zwick noted that it is ""plausible to conjecture that $O(n^2)$ is the best possible bound in terms of $n$"". We show ""conditional optimality"": if this hypothesis holds then our $O(m^{2k/(k+1)})$ algorithm is tight as well. Furthermore, a folklore reduction implies that no combinatorial algorithm can determine if a graph contains a $6$-cycle in time $O(m^{3/2-\epsilon})$ for any $\epsilon>0$ under the widely believed combinatorial BMM conjecture. Coupled with our main result, this gives tight bounds for finding $6$-cycles combinatorially and also separates the complexity of finding $4$- and $6$-cycles giving evidence that the exponent of $m$ in the running time should indeed increase with $k$.   The key ingredient in our algorithm is a new notion of capped $k$-walks, which are walks of length $k$ that visit only nodes according to a fixed ordering. Our main technical contribution is an involved analysis proving several properties of such walks which may be of independent interest."|paper consid problem find cycl length undirect graph node edg constant ge classic result bondi simonovit comb th impli gek contain impli one need consid onli graph previous best known algorithm algorithm due yuster zwick disc math well lceil rceil algorithm alon et al algorithmica present algorithm use time find one exist bound exact theta cycl new bound coincid alon et al everi bound yield polynomi improv yuster zwick note plausibl conjectur best possibl bound term show condit optim hypothesi hold algorithm tight well furthermor folklor reduct impli combinatori algorithm determin graph contain cycl time epsilon ani epsilon wide believ combinatori bmm conjectur coupl main result give tight bound find cycl combinatori also separ complex find cycl give evid expon run time inde increas key ingredi algorithm new notion cap walk walk length visit onli node accord fix order main technic contribut involv analysi prove sever properti walk may independ interest|['Søren Dahlgaard', 'Mathias Bæk Tejs Knudsen', 'Morten Stöckel']|['cs.DS']
2017-04-07T11:31:13Z|2017-03-30T02:47:26Z|http://arxiv.org/abs/1703.10293v1|http://arxiv.org/pdf/1703.10293v1|Preserving Distances in Very Faulty Graphs|preserv distanc veri faulti graph|Preservers and additive spanners are sparse (hence cheap to store) subgraphs that preserve the distances between given pairs of nodes exactly or with some small additive error, respectively. Since real-world networks are prone to failures, it makes sense to study fault-tolerant versions of the above structures. This turns out to be a surprisingly difficult task. For every small but arbitrary set of edge or vertex failures, the preservers and spanners need to contain {\em replacement paths} around the faulted set. In this paper we make substantial progress on fault tolerant preservers and additive spanners:   (1) We present the first truly sub-quadratic size single-pair preservers in unweighted (possibly directed) graphs for \emph{any} fixed number $f$ of faults. Our result indeed generalizes to the single-source case, and can be used to build new fault-tolerant additive spanners (for all pairs).   (2) The size of the above single-pair preservers is $O(n^{2-g(f)})$ for some positive function $g$, and grows to $O(n^2)$ for increasing $f$. We show that this is necessary even in undirected unweighted graphs, and even if you allow for a small additive error: If you aim at size $O(n^{2-\epsilon})$ for $\epsilon>0$, then the additive error has to be $\Omega(\eps f)$. This surprisingly matches known upper bounds in the literature.   (3) For weighted graphs, we provide matching upper and lower bounds for the single pair case. Namely, the size of the preserver is $\Theta(n^2)$ for $f\geq 2$ in both directed and undirected graphs, while for $f=1$ the size is $\Theta(n)$ in undirected graphs. For directed graphs, we have a superlinear upper bound and a matching lower bound.   Most of our lower bounds extend to the distance oracle setting, where rather than a subgraph we ask for any compact data structure.|preserv addit spanner spars henc cheap store subgraph preserv distanc given pair node exact small addit error respect sinc real world network prone failur make sens studi fault toler version abov structur turn surpris difficult task everi small arbitrari set edg vertex failur preserv spanner need contain em replac path around fault set paper make substanti progress fault toler preserv addit spanner present first truli sub quadrat size singl pair preserv unweight possibl direct graph emph ani fix number fault result inde general singl sourc case use build new fault toler addit spanner pair size abov singl pair preserv posit function grow increas show necessari even undirect unweight graph even allow small addit error aim size epsilon epsilon addit error omega ep surpris match known upper bound literatur weight graph provid match upper lower bound singl pair case name size preserv theta geq direct undirect graph size theta undirect graph direct graph superlinear upper bound match lower bound lower bound extend distanc oracl set rather subgraph ask ani compact data structur|['Greg Bodwin', 'Fabrizio Grandoni', 'Merav Parter', 'Virginia Vassilevska Williams']|['cs.DS']
2017-04-07T11:31:13Z|2017-03-29T20:20:18Z|http://arxiv.org/abs/1703.10232v1|http://arxiv.org/pdf/1703.10232v1|Recursive Method for the Solution of Systems of Linear Equations|recurs method solut system linear equat|New solution method for the systems of linear equations in commutative integral domains is proposed. Its complexity is the same that the complexity of the matrix multiplication.|new solut method system linear equat commut integr domain propos complex complex matrix multipl|['Gennadi Malaschonok']|['cs.DS', 'cs.SC']
2017-04-07T11:31:13Z|2017-04-04T14:53:34Z|http://arxiv.org/abs/1703.10127v2|http://arxiv.org/pdf/1703.10127v2|Priv'IT: Private and Sample Efficient Identity Testing|priv privat sampl effici ident test|We develop differentially private hypothesis testing methods for the small sample regime. Given a sample $\cal D$ from a categorical distribution $p$ over some domain $\Sigma$, an explicitly described distribution $q$ over $\Sigma$, some privacy parameter $\varepsilon$, accuracy parameter $\alpha$, and requirements $\beta_{\rm I}$ and $\beta_{\rm II}$ for the type I and type II errors of our test, the goal is to distinguish between $p=q$ and $d_{\rm{TV}}(p,q) \geq \alpha$.   We provide theoretical bounds for the sample size $ {\cal D} $ so that our method both satisfies $(\varepsilon,0)$-differential privacy, and guarantees $\beta_{\rm I}$ and $\beta_{\rm II}$ type I and type II errors. We show that differential privacy may come for free in some regimes of parameters, and we always beat the sample complexity resulting from running the $\chi^2$-test with noisy counts, or standard approaches such as repetition for endowing non-private $\chi^2$-style statistics with differential privacy guarantees. We experimentally compare the sample complexity of our method to that of recently proposed methods for private hypothesis testing.|develop differenti privat hypothesi test method small sampl regim given sampl cal categor distribut domain sigma explicit describ distribut sigma privaci paramet varepsilon accuraci paramet alpha requir beta rm beta rm ii type type ii error test goal distinguish rm tv geq alpha provid theoret bound sampl size cal method satisfi varepsilon differenti privaci guarante beta rm beta rm ii type type ii error show differenti privaci may come free regim paramet alway beat sampl complex result run chi test noisi count standard approach repetit endow non privat chi style statist differenti privaci guarante experiment compar sampl complex method recent propos method privat hypothesi test|['Bryan Cai', 'Constantinos Daskalakis', 'Gautam Kamath']|['cs.DS', 'cs.CR', 'cs.IT', 'cs.LG', 'math.IT', 'math.ST', 'stat.TH']
2017-04-07T11:31:13Z|2017-03-29T14:12:42Z|http://arxiv.org/abs/1703.10049v1|http://arxiv.org/pdf/1703.10049v1|Flight Tour Planning with Recharging Optimization for Battery-operated   Autonomous Drones|flight tour plan recharg optim batteri oper autonom drone|Autonomous drones (also known as unmanned aerial vehicles) have several advantages over ground vehicles, including agility, swiftness, and energy-efficiency, and hence are convenient for light-weight delivery and substitutions for manned missions in remote operations. It is expected that autonomous drones will be deployed in diverse applications in near future. Typical drones are electric vehicles, powered by on-board batteries. This paper presents several contributions for automated battery-operated drone management systems: (1) We conduct an empirical study to model the battery performance of drones, considering various flight scenarios. (2) We study a joint problem of flight tour planning with recharging optimization for drones with an objective to complete a tour mission for a set of sites of interest. This problem captures diverse applications of delivery and remote operations by drones. (3) We implemented our optimization algorithms in an intelligent drone management system.|autonom drone also known unman aerial vehicl sever advantag ground vehicl includ agil swift energi effici henc conveni light weight deliveri substitut man mission remot oper expect autonom drone deploy divers applic near futur typic drone electr vehicl power board batteri paper present sever contribut autom batteri oper drone manag system conduct empir studi model batteri perform drone consid various flight scenario studi joint problem flight tour plan recharg optim drone object complet tour mission set site interest problem captur divers applic deliveri remot oper drone implement optim algorithm intellig drone manag system|['Chien-Ming Tseng', 'Chi-Kin Chau', 'Khaled Elbassioni', 'Majid Khonji']|['cs.RO', 'cs.DS']
2017-04-07T11:31:13Z|2017-03-29T13:41:35Z|http://arxiv.org/abs/1703.10031v1|http://arxiv.org/pdf/1703.10031v1|Asymptotic Enumeration of Compacted Binary Trees|asymptot enumer compact binari tree|A compacted tree is a graph created from a binary tree such that repeatedly occurring subtrees in the original tree are represented by pointers to existing ones, and hence every subtree is unique. Such representations form a special class of directed acyclic graphs. We are interested in the asymptotic number of compacted trees of given size, where the size of a compacted tree is given by the number of its internal nodes. Due to its superexponential growth this problem poses many difficulties. Therefore we restrict our investigations to compacted trees of bounded right height, which is the maximal number of edges going to the right on any path from the root to a leaf.   We solve the asymptotic counting problem for this class as well as a closely related, further simplified class.   For this purpose, we develop a calculus on exponential generating functions for compacted trees of bounded right height and for relaxed trees of bounded right height, which differ from compacted trees by dropping the above described uniqueness condition. This enables us to derive a recursively defined sequence of differential equations for the exponential generating functions. The coefficients can then be determined by performing a singularity analysis of the solutions of these differential equations.   Our main results are the computation of the asymptotic numbers of relaxed as well as compacted trees of bounded right height and given size, when the size tends to infinity.|compact tree graph creat binari tree repeat occur subtre origin tree repres pointer exist one henc everi subtre uniqu represent form special class direct acycl graph interest asymptot number compact tree given size size compact tree given number intern node due superexponenti growth problem pose mani difficulti therefor restrict investig compact tree bound right height maxim number edg go right ani path root leaf solv asymptot count problem class well close relat simplifi class purpos develop calculus exponenti generat function compact tree bound right height relax tree bound right height differ compact tree drop abov describ uniqu condit enabl us deriv recurs defin sequenc differenti equat exponenti generat function coeffici determin perform singular analysi solut differenti equat main result comput asymptot number relax well compact tree bound right height given size size tend infin|['Antoine Genitrini', 'Bernhard Gittenberger', 'Manuel Kauers', 'Michael Wallner']|['math.CO', 'cs.DM', 'cs.DS', '05C30, 05A16, 05C20, 05C05', 'G.2.1; G.2.2']
2017-04-07T11:31:17Z|2017-03-29T13:18:52Z|http://arxiv.org/abs/1703.10023v1|http://arxiv.org/pdf/1703.10023v1|Engineering DFS-Based Graph Algorithms|engin dfs base graph algorithm|Depth-first search (DFS) is the basis for many efficient graph algorithms. We introduce general techniques for the efficient implementation of DFS-based graph algorithms and exemplify them on three algorithms for computing strongly connected components. The techniques lead to speed-ups by a factor of two to three compared to the implementations provided by LEDA and BOOST.   We have obtained similar speed-ups for biconnected components algorithms. We also compare the graph data types of LEDA and BOOST.|depth first search dfs basi mani effici graph algorithm introduc general techniqu effici implement dfs base graph algorithm exemplifi three algorithm comput strong connect compon techniqu lead speed factor two three compar implement provid leda boost obtain similar speed biconnect compon algorithm also compar graph data type leda boost|['Kurt Mehlhorn', 'Stefan Näher', 'Peter Sanders']|['cs.DS', 'E.1; F.2.2; G.2.2; I.2.8']
2017-04-07T11:31:17Z|2017-03-29T09:31:47Z|http://arxiv.org/abs/1703.09947v1|http://arxiv.org/pdf/1703.09947v1|Efficient Private ERM for Smooth Objectives|effici privat erm smooth object|In this paper, we consider efficient differentially private empirical risk minimization from the viewpoint of optimization algorithms. For strongly convex and smooth objectives, we prove that gradient descent with output perturbation not only achieves nearly optimal utility, but also significantly improves the running time of previous state-of-the-art private optimization algorithms, for both $\epsilon$-DP and $(\epsilon, \delta)$-DP. For non-convex but smooth objectives, we propose an RRPSGD (Random Round Private Stochastic Gradient Descent) algorithm, which provably converges to a stationary point with privacy guarantee. Besides the expected utility bounds, we also provide guarantees in high probability form. Experiments demonstrate that our algorithm consistently outperforms existing method in both utility and running time.|paper consid effici differenti privat empir risk minim viewpoint optim algorithm strong convex smooth object prove gradient descent output perturb onli achiev near optim util also signific improv run time previous state art privat optim algorithm epsilon dp epsilon delta dp non convex smooth object propos rrpsgd random round privat stochast gradient descent algorithm provabl converg stationari point privaci guarante besid expect util bound also provid guarante high probabl form experi demonstr algorithm consist outperform exist method util run time|['Jiaqi Zhang', 'Kai Zheng', 'Wenlong Mou', 'Liwei Wang']|['cs.LG', 'cs.DS', 'stat.ML']
2017-04-07T11:31:17Z|2017-03-28T18:01:14Z|http://arxiv.org/abs/1703.09726v1|http://arxiv.org/pdf/1703.09726v1|Ruling out FPT algorithms for Weighted Coloring on forests|rule fpt algorithm weight color forest|Given a graph $G$, a proper $k$-coloring of $G$ is a partition $c = (S_i)_{i\in [1,k]}$ of $V(G)$ into $k$ stable sets $S_1,\ldots, S_{k}$. Given a weight function $w: V(G) \to \mathbb{R}^+$, the weight of a color $S_i$ is defined as $w(i) = \max_{v \in S_i} w(v)$ and the weight of a coloring $c$ as $w(c) = \sum_{i=1}^{k}w(i)$. Guan and Zhu [Inf. Process. Lett., 1997] defined the weighted chromatic number of a pair $(G,w)$, denoted by $\sigma(G,w)$, as the minimum weight of a proper coloring of $G$. For a positive integer $r$, they also defined $\sigma(G,w;r)$ as the minimum of $w(c)$ among all proper $r$-colorings $c$ of $G$.   The complexity of determining $\sigma(G,w)$ when $G$ is a tree was open for almost 20 years, until Ara\'ujo et al. [SIAM J. Discrete Math., 2014] recently proved that the problem cannot be solved in time $n^{o(\log n)}$ on $n$-vertex trees unless the Exponential Time Hypothesis (ETH) fails.   The objective of this article is to provide hardness results for computing $\sigma(G,w)$ and $\sigma(G,w;r)$ when $G$ is a tree or a forest, relying on complexity assumptions weaker than the ETH. Namely, we study the problem from the viewpoint of parameterized complexity, and we assume the weaker hypothesis $FPT \neq W[1]$. Building on the techniques of Ara\'ujo et al., we prove that when $G$ is a forest, computing $\sigma(G,w)$ is $W[1]$-hard parameterized by the size of a largest connected component of $G$, and that computing $\sigma(G,w;r)$ is $W[2]$-hard parameterized by $r$. Our results rule out the existence of $FPT$ algorithms for computing these invariants on trees or forests for many natural choices of the parameter.|given graph proper color partit stabl set ldot given weight function mathbb weight color defin max weight color sum guan zhu inf process lett defin weight chromat number pair denot sigma minimum weight proper color posit integ also defin sigma minimum among proper color complex determin sigma tree open almost year ara ujo et al siam discret math recent prove problem cannot solv time log vertex tree unless exponenti time hypothesi eth fail object articl provid hard result comput sigma sigma tree forest reli complex assumpt weaker eth name studi problem viewpoint parameter complex assum weaker hypothesi fpt neq build techniqu ara ujo et al prove forest comput sigma hard parameter size largest connect compon comput sigma hard parameter result rule exist fpt algorithm comput invari tree forest mani natur choic paramet|['Júlio Araújo', 'Julien Baste', 'Ignasi Sau']|['cs.DS', 'cs.CC', 'math.CO', '05C15', 'G.2.2; F.2.2']
2017-04-07T11:31:17Z|2017-03-28T12:16:50Z|http://arxiv.org/abs/1703.09533v1|http://arxiv.org/pdf/1703.09533v1|Routing in Polygons with Holes|rout polygon hole|Sending a message through an unknown network is a difficult problem. In this paper, we consider the case in which, during a preprocessing phase, we assign a label and a routing table to each node. The routing strategy must then decide where to send the package using only the label of the target node and the routing table of the node the message is currently at.   In this paper, we present the first routing scheme for the particular case in which the network is defined by the visibility graph of a polygon with $n$ vertices and $h$ holes. For any $\varepsilon > 0$ the routing scheme provides stretch at most $1+\varepsilon$. The labels have $O(\log n)$ bits and the corresponding routing tables are of size $O(\varepsilon^{-1}(h+1)\log n)$. The preprocessing time is $O(n^2\log n+(h+1)n^2+\varepsilon^{-1}(h+1)n)$ and can be improved to $O(n^2+\varepsilon^{-1}n)$ for simple polygons.|send messag unknown network difficult problem paper consid case dure preprocess phase assign label rout tabl node rout strategi must decid send packag use onli label target node rout tabl node messag current paper present first rout scheme particular case network defin visibl graph polygon vertic hole ani varepsilon rout scheme provid stretch varepsilon label log bit correspond rout tabl size varepsilon log preprocess time log varepsilon improv varepsilon simpl polygon|['Matias Korman', 'Wolfgang Mulzer', 'André van Renssen', 'Marcel Roeloffzen', 'Paul Seiferth', 'Yannik Stein', 'Birgit Vogtenhuber', 'Max Willert']|['cs.CG', 'cs.DS']
2017-04-07T11:31:17Z|2017-03-28T07:20:30Z|http://arxiv.org/abs/1703.09421v1|http://arxiv.org/pdf/1703.09421v1|Edge-matching Problems with Rotations|edg match problem rotat|Edge-matching problems, also called edge matching puzzles, are abstractions of placement problems with neighborhood conditions. Pieces with colored edges have to be placed on a board such that adjacent edges have the same color. The problem has gained interest recently with the (now terminated) Eternity~II puzzle, and new complexity results. In this paper we consider a number of settings which differ in size of the puzzles and the manipulations allowed on the pieces. We investigate the effect of allowing rotations of the pieces on the complexity of the problem, an aspect that is only marginally treated so far. We show that some problems have polynomial time algorithms while others are NP-complete. Especially we show that allowing rotations in one-row puzzles makes the problem NP-hard. The proofs of the hardness result uses a large number of colors. This is essential because we also show that this problem (and another related one) is fixed-parameter tractable, where the relevant parameter is the number of colors.|edg match problem also call edg match puzzl abstract placement problem neighborhood condit piec color edg place board adjac edg color problem gain interest recent termin etern ii puzzl new complex result paper consid number set differ size puzzl manipul allow piec investig effect allow rotat piec complex problem aspect onli margin treat far show problem polynomi time algorithm np complet especi show allow rotat one row puzzl make problem np hard proof hard result use larg number color essenti becaus also show problem anoth relat one fix paramet tractabl relev paramet number color|['Martin Ebbesen', 'Paul Fischer', 'Carsten Witt']|['cs.DS']
2017-04-07T11:31:17Z|2017-03-27T22:10:13Z|http://arxiv.org/abs/1703.09324v1|http://arxiv.org/pdf/1703.09324v1|Algorithmic interpretations of fractal dimension|algorithm interpret fractal dimens|We study algorithmic problems on subsets of Euclidean space of low fractal dimension. These spaces are the subject of intensive study in various branches of mathematics, including geometry, topology, and measure theory. There are several well-studied notions of fractal dimension for sets and measures in Euclidean space. We consider a definition of fractal dimension for finite metric spaces which agrees with standard notions used to empirically estimate the fractal dimension of various sets. We define the fractal dimension of some metric space to be the infimum $\delta>0$, such that for any $\epsilon > 0$, for any ball $B$ of radius $r\geq 2\epsilon$, and for any $\epsilon $-net $N$ (that is, for any maximal $\epsilon $-packing), we have $ B\cap N =O((r/\epsilon)^\delta)$.   Using this definition we obtain faster algorithms for a plethora of classical problems on sets of low fractal dimension in Euclidean space. Our results apply to exact and fixed-parameter algorithms, approximation schemes, and spanner constructions. Interestingly, the dependence of the performance of these algorithms on the fractal dimension nearly matches the currently best-known dependence on the standard Euclidean dimension. Thus, when the fractal dimension is strictly smaller than the ambient dimension, our results yield improved solutions in all of these settings.|studi algorithm problem subset euclidean space low fractal dimens space subject intens studi various branch mathemat includ geometri topolog measur theori sever well studi notion fractal dimens set measur euclidean space consid definit fractal dimens finit metric space agre standard notion use empir estim fractal dimens various set defin fractal dimens metric space infimum delta ani epsilon ani ball radius geq epsilon ani epsilon net ani maxim epsilon pack cap epsilon delta use definit obtain faster algorithm plethora classic problem set low fractal dimens euclidean space result appli exact fix paramet algorithm approxim scheme spanner construct interest depend perform algorithm fractal dimens near match current best known depend standard euclidean dimens thus fractal dimens strict smaller ambient dimens result yield improv solut set|['Anastasios Sidiropoulos', 'Vijay Sridhar']|['cs.DS', 'F.2.2']
2017-04-07T11:31:17Z|2017-03-27T20:52:29Z|http://arxiv.org/abs/1703.09307v1|http://arxiv.org/pdf/1703.09307v1|Fluid Communities: A Community Detection Algorithm|fluid communiti communiti detect algorithm|Community detection algorithms are a family of unsupervised graph mining algorithms which group vertices into clusters (i.e., communities). These algorithms provide insight into both the structure of a network and the entities that compose it. In this paper we propose a novel community detection algorithm based on the simple idea of fluids interacting in an environment, expanding and contracting. The fluid communities algorithm is based on the efficient propagation method, which makes it very competitive in computational cost and scalability. At the same time, the quality of its results is close to that of current state-of-the-art community detection algorithms. An interesting novelty of the fluid communities algorithm is that it is the first propagation-based method capable of identifying a variable number of communities within a graph.|communiti detect algorithm famili unsupervis graph mine algorithm group vertic cluster communiti algorithm provid insight structur network entiti compos paper propos novel communiti detect algorithm base simpl idea fluid interact environ expand contract fluid communiti algorithm base effici propag method make veri competit comput cost scalabl time qualiti result close current state art communiti detect algorithm interest novelti fluid communiti algorithm first propag base method capabl identifi variabl number communiti within graph|['Ferran Parés', 'Dario Garcia-Gasulla', 'Armand Vilalta', 'Jonatan Moreno', 'Eduard Ayguadé', 'Jesús Labarta', 'Ulises Cortés', 'Toyotaro Suzumura']|['cs.DS', 'cs.SI', 'physics.soc-ph']
2017-04-07T11:31:17Z|2017-03-27T13:57:31Z|http://arxiv.org/abs/1703.09083v1|http://arxiv.org/pdf/1703.09083v1|The weighted stable matching problem|weight stabl match problem|We study the stable matching problem in non-bipartite graphs with incomplete but strict preference lists, where the edges have weights and the goal is to compute a stable matching of minimum or maximum weight. This problem is known to be NP-hard in general. Our contribution is two fold: a polyhedral characterization and an approximation algorithm. Previously Chen et al. have shown that the stable matching polytope is integral if and only if the subgraph obtained after running phase one of Irving's algorithm is bipartite. We improve upon this result by showing that there are instances where this subgraph might not be bipartite but one can further eliminate some edges and arrive at a bipartite subgraph. Our elimination procedure ensures that the set of stable matchings remains the same, and thus the stable matching polytope of the final subgraph contains the incidence vectors of all stable matchings of our original graph. This allows us to characterize a larger class of instances for which the weighted stable matching problem is polynomial-time solvable. We also show that our edge elimination procedure is best possible, meaning that if the subgraph we arrive at is not bipartite, then there is no bipartite subgraph that has the same set of stable matchings as the original graph. We complement these results with a $2$-approximation algorithm for the minimum weight stable matching problem for instances where each agent has at most two possible partners in any stable matching. This is the first approximation result for any class of instances with general weights.|studi stabl match problem non bipartit graph incomplet strict prefer list edg weight goal comput stabl match minimum maximum weight problem known np hard general contribut two fold polyhedr character approxim algorithm previous chen et al shown stabl match polytop integr onli subgraph obtain run phase one irv algorithm bipartit improv upon result show instanc subgraph might bipartit one elimin edg arriv bipartit subgraph elimin procedur ensur set stabl match remain thus stabl match polytop final subgraph contain incid vector stabl match origin graph allow us character larger class instanc weight stabl match problem polynomi time solvabl also show edg elimin procedur best possibl mean subgraph arriv bipartit bipartit subgraph set stabl match origin graph complement result approxim algorithm minimum weight stabl match problem instanc agent two possibl partner ani stabl match first approxim result ani class instanc general weight|['Linda Farczadi', 'Natália Guričanová']|['cs.GT', 'cs.DS']
2017-04-07T11:31:17Z|2017-03-27T06:50:30Z|http://arxiv.org/abs/1703.08950v1|http://arxiv.org/pdf/1703.08950v1|Gene tree species tree reconciliation with gene conversion|gene tree speci tree reconcili gene convers|Gene tree/species tree reconciliation is a recent decisive progress in phylo-genetic methods, accounting for the possible differences between gene histories and species histories. Reconciliation consists in explaining these differences by gene-scale events such as duplication, loss, transfer, which translates mathematically into a mapping between gene tree nodes and species tree nodes or branches. Gene conversion is a very frequent biological event, which results in the replacement of a gene by a copy of another from the same species and in the same gene tree. Including this event in reconciliations has never been attempted because this changes as well the solutions as the methods to construct reconciliations. Standard algorithms based on dynamic programming become ineffective. We propose here a novel mathematical framework including gene conversion as an evolutionary event in gene tree/species tree reconciliation. We describe a randomized algorithm giving in polynomial running time a reconciliation minimizing the number of duplications, losses and conversions. We show that the space of reconciliations includes an analog of the Last Common Ancestor reconciliation, but is not limited to it. Our algorithm outputs any optimal reconciliation with non null probability. We argue that this study opens a wide research avenue on including gene conversion in reconciliation, which can be important for biology.|gene tree speci tree reconcili recent decis progress phylo genet method account possibl differ gene histori speci histori reconcili consist explain differ gene scale event duplic loss transfer translat mathemat map gene tree node speci tree node branch gene convers veri frequent biolog event result replac gene copi anoth speci gene tree includ event reconcili never attempt becaus chang well solut method construct reconcili standard algorithm base dynam program becom ineffect propos novel mathemat framework includ gene convers evolutionari event gene tree speci tree reconcili describ random algorithm give polynomi run time reconcili minim number duplic loss convers show space reconcili includ analog last common ancestor reconcili limit algorithm output ani optim reconcili non null probabl argu studi open wide research avenu includ gene convers reconcili import biolog|['Damir Hasic', 'Eric Tannier']|['q-bio.QM', 'cs.DS']
2017-04-07T11:31:17Z|2017-03-27T05:48:36Z|http://arxiv.org/abs/1703.08940v1|http://arxiv.org/pdf/1703.08940v1|Tree Edit Distance Cannot be Computed in Strongly Subcubic Time (unless   APSP can)|tree edit distanc cannot comput strong subcub time unless apsp|The edit distance between two rooted ordered trees with $n$ nodes labeled from an alphabet~$\Sigma$ is the minimum cost of transforming one tree into the other by a sequence of elementary operations consisting of deleting and relabeling existing nodes, as well as inserting new nodes. Tree edit distance is a well known generalization of string edit distance. The fastest known algorithm for tree edit distance runs in cubic $O(n^3)$ time and is based on a similar dynamic programming solution as string edit distance. In this paper we show that a truly subcubic $O(n^{3-\varepsilon})$ time algorithm for tree edit distance is unlikely: For $ \Sigma  = \Omega(n)$, a truly subcubic algorithm for tree edit distance implies a truly subcubic algorithm for the all pairs shortest paths problem. For $ \Sigma  = O(1)$, a truly subcubic algorithm for tree edit distance implies an $O(n^{k-\varepsilon})$ algorithm for finding a maximum weight $k$-clique.   Thus, while in terms of upper bounds string edit distance and tree edit distance are highly related, in terms of lower bounds string edit distance exhibits the hardness of the strong exponential time hypothesis [Backurs, Indyk STOC'15] whereas tree edit distance exhibits the hardness of all pairs shortest paths. Our result provides a matching conditional lower bound for one of the last remaining classic dynamic programming problems.|edit distanc two root order tree node label alphabet sigma minimum cost transform one tree sequenc elementari oper consist delet relabel exist node well insert new node tree edit distanc well known general string edit distanc fastest known algorithm tree edit distanc run cubic time base similar dynam program solut string edit distanc paper show truli subcub varepsilon time algorithm tree edit distanc unlik sigma omega truli subcub algorithm tree edit distanc impli truli subcub algorithm pair shortest path problem sigma truli subcub algorithm tree edit distanc impli varepsilon algorithm find maximum weight cliqu thus term upper bound string edit distanc tree edit distanc high relat term lower bound string edit distanc exhibit hard strong exponenti time hypothesi backur indyk stoc wherea tree edit distanc exhibit hard pair shortest path result provid match condit lower bound one last remain classic dynam program problem|['Karl Bringmann', 'Paweł Gawrychowski', 'Shay Mozes', 'Oren Weimann']|['cs.DS']
2017-04-07T11:31:21Z|2017-03-27T05:09:06Z|http://arxiv.org/abs/1703.08931v1|http://arxiv.org/pdf/1703.08931v1|Palindromic Decompositions with Gaps and Errors|palindrom decomposit gap error|Identifying palindromes in sequences has been an interesting line of research in combinatorics on words and also in computational biology, after the discovery of the relation of palindromes in the DNA sequence with the HIV virus. Efficient algorithms for the factorization of sequences into palindromes and maximal palindromes have been devised in recent years. We extend these studies by allowing gaps in decompositions and errors in palindromes, and also imposing a lower bound to the length of acceptable palindromes.   We first present an algorithm for obtaining a palindromic decomposition of a string of length n with the minimal total gap length in time O(n log n * g) and space O(n g), where g is the number of allowed gaps in the decomposition. We then consider a decomposition of the string in maximal \delta-palindromes (i.e. palindromes with \delta errors under the edit or Hamming distance) and g allowed gaps. We present an algorithm to obtain such a decomposition with the minimal total gap length in time O(n (g + \delta)) and space O(n g).|identifi palindrom sequenc interest line research combinator word also comput biolog discoveri relat palindrom dna sequenc hiv virus effici algorithm factor sequenc palindrom maxim palindrom devis recent year extend studi allow gap decomposit error palindrom also impos lower bound length accept palindrom first present algorithm obtain palindrom decomposit string length minim total gap length time log space number allow gap decomposit consid decomposit string maxim delta palindrom palindrom delta error edit ham distanc allow gap present algorithm obtain decomposit minim total gap length time delta space|['Michał Adamczyk', 'Mai Alzamel', 'Panagiotis Charalampopoulos', 'Costas S. Iliopoulos', 'Jakub Radoszewski']|['cs.DS']
2017-04-07T11:31:21Z|2017-03-26T09:03:07Z|http://arxiv.org/abs/1703.08790v1|http://arxiv.org/pdf/1703.08790v1|Steiner Point Removal --- Distant Terminals Don't (Really) Bother|steiner point remov distant termin realli bother|Given a weighted graph $G=(V,E,w)$ with a set of $k$ terminals $T\subset V$, the Steiner Point Removal problem seeks for a minor of the graph with vertex set $T$, such that the distance between every pair of terminals is preserved within a small multiplicative distortion. Kamma, Krauthgamer and Nguyen (SODA 2014, SICOMP 2015) used a ball-growing algorithm to show that the distortion is at most $\mathcal{O}(\log^5 k)$ for general graphs.   In this paper, we improve the distortion bound to $\mathcal{O}(\log^2 k)$. The improvement is achieved based on a known algorithm that constructs terminal-distance exact-preservation minor with $\mathcal{O}(k^4)$ (which is independent of $ V $) vertices, and also two tail bounds on the sum of independent exponential random variables, which allow us to show that it is unlikely for a non-terminal being contracted to a distant terminal.|given weight graph set termin subset steiner point remov problem seek minor graph vertex set distanc everi pair termin preserv within small multipl distort kamma krauthgam nguyen soda sicomp use ball grow algorithm show distort mathcal log general graph paper improv distort bound mathcal log improv achiev base known algorithm construct termin distanc exact preserv minor mathcal independ vertic also two tail bound sum independ exponenti random variabl allow us show unlik non termin contract distant termin|['Yun Kuen Cheung']|['cs.DS', 'cs.DM', 'math.CO', 'math.PR']
2017-04-07T11:31:21Z|2017-03-25T15:03:49Z|http://arxiv.org/abs/1703.08702v1|http://arxiv.org/pdf/1703.08702v1|Randomized Load Balancing on Networks with Stochastic Inputs|random load balanc network stochast input|Iterative load balancing algorithms for indivisible tokens have been studied intensively in the past. Complementing previous worst-case analyses, we study an average-case scenario where the load inputs are drawn from a fixed probability distribution. For cycles, tori, hypercubes and expanders, we obtain almost matching upper and lower bounds on the discrepancy, the difference between the maximum and the minimum load. Our bounds hold for a variety of probability distributions including the uniform and binomial distribution but also distributions with unbounded range such as the Poisson and geometric distribution. For graphs with slow convergence like cycles and tori, our results demonstrate a substantial difference between the convergence in the worst- and average-case. An important ingredient in our analysis is new upper bound on the t-step transition probability of a general Markov chain, which is derived by invoking the evolving set process.|iter load balanc algorithm indivis token studi intens past complement previous worst case analys studi averag case scenario load input drawn fix probabl distribut cycl tori hypercub expand obtain almost match upper lower bound discrep differ maximum minimum load bound hold varieti probabl distribut includ uniform binomi distribut also distribut unbound rang poisson geometr distribut graph slow converg like cycl tori result demonstr substanti differ converg worst averag case import ingredi analysi new upper bound step transit probabl general markov chain deriv invok evolv set process|['Leran Cai', 'Thomas Sauerwald']|['cs.DC', 'cs.DS', 'G.3']
2017-04-07T11:31:21Z|2017-03-25T07:27:32Z|http://arxiv.org/abs/1703.08658v1|http://arxiv.org/pdf/1703.08658v1|Maximizing the area of intersection of rectangles|maxim area intersect rectangl|This paper attacks the following problem. We are given a large number $N$ of rectangles in the plane, each with horizontal and vertical sides, and also a number $r<N$. The given list of $N$ rectangles may contain duplicates. The problem is to find $r$ of these rectangles, such that, if they are discarded, then the intersection of the remaining $(N-r)$ rectangles has an intersection with as large an area as possible. We will find an upper bound, depending only on $N$ and $r$, and not on the particular data presented, for the number of steps needed to run the algorithm on (a mathematical model of) a computer. In fact our algorithm is able to determine, for each $s\le r$, $s$ rectangles from the given list of $N$ rectangles, such that the remaining $(N-s)$ rectangles have as large an area as possible, and this takes hardly any more time than taking care only of the case $s=r$. Our algorithm extends to $d$-dimensional rectangles. Our method is to exhaustively examine all possible intersections---this is much faster than it sounds, because we do not need to examine all $\binom Ns$ subsets in order to find all possible intersection rectangles. For an extreme example, suppose the rectangles are nested, for example concentric squares of distinct sizes, then the only intersections examined are the smallest $s+1$ rectangles.|paper attack follow problem given larg number rectangl plane horizont vertic side also number given list rectangl may contain duplic problem find rectangl discard intersect remain rectangl intersect larg area possibl find upper bound depend onli particular data present number step need run algorithm mathemat model comput fact algorithm abl determin le rectangl given list rectangl remain rectangl larg area possibl take hard ani time take care onli case algorithm extend dimension rectangl method exhaust examin possibl intersect much faster sound becaus need examin binom ns subset order find possibl intersect rectangl extrem exampl suppos rectangl nest exampl concentr squar distinct size onli intersect examin smallest rectangl|['David B. A. Epstein', 'Mike Paterson']|['cs.DS', 'F.2.2']
2017-04-07T11:31:21Z|2017-03-24T20:21:52Z|http://arxiv.org/abs/1703.08589v1|http://arxiv.org/pdf/1703.08589v1|Polynomial-Time Methods to Solve Unimodular Quadratic Programs With   Performance Guarantees|polynomi time method solv unimodular quadrat program perform guarante|We develop polynomial-time heuristic methods to solve unimodular quadratic programs (UQPs) approximately, which are known to be NP-hard. In the UQP framework, we maximize a quadratic function of a vector of complex variables with unit modulus. Several problems in active sensing and wireless communication applications boil down to UQP. With this motivation, we present three new heuristic methods with polynomial-time complexity to solve the UQP approximately. The first method is called dominant-eigenvector-matching; here the solution is picked that matches the complex arguments of the dominant eigenvector of the Hermitian matrix in the UQP formulation. We also provide a performance guarantee for this method. The second method, a greedy strategy, is shown to provide a performance guarantee of (1-1/e) with respect to the optimal objective value given that the objective function possesses a property called string submodularity. The third heuristic method is called row-swap greedy strategy, which is an extension to the greedy strategy and utilizes certain properties of the UQP to provide a better performance than the greedy strategy at the expense of an increase in computational complexity. We present numerical results to demonstrate the performance of these heuristic methods, and also compare the performance of these methods against a standard heuristic method called semidefinite relaxation.|develop polynomi time heurist method solv unimodular quadrat program uqp approxim known np hard uqp framework maxim quadrat function vector complex variabl unit modulus sever problem activ sens wireless communic applic boil uqp motiv present three new heurist method polynomi time complex solv uqp approxim first method call domin eigenvector match solut pick match complex argument domin eigenvector hermitian matrix uqp formul also provid perform guarante method second method greedi strategi shown provid perform guarante respect optim object valu given object function possess properti call string submodular third heurist method call row swap greedi strategi extens greedi strategi util certain properti uqp provid better perform greedi strategi expens increas comput complex present numer result demonstr perform heurist method also compar perform method standard heurist method call semidefinit relax|['Shankarachary Ragi', 'Edwin K. P. Chong', 'Hans D. Mittelmann']|['math.OC', 'cs.DS']
2017-04-07T11:31:21Z|2017-03-24T17:10:23Z|http://arxiv.org/abs/1703.08511v1|http://arxiv.org/pdf/1703.08511v1|ALLSAT compressed with wildcards. Part 2: All k-models of a BDD|allsat compress wildcard part model bdd|If f is a Boolean function given by a BDD then it is well known how to calculate the number of models (i.e. bitstrings x with f(x)=1). Let  x  be the number of 1's in x. How to calculate the number of k-models x (i.e. having  x =k) is lesser known; we review a nice method due to Knuth. The main topic however is enumeration (=generation) as opposed to counting. Again, that ALL models can be enumerated in polynomial total time, is well known. Apparently new is the fact that also all k-models (for any fixed k) can be enumerated in polynomial total time. Using suitable wildcards this can be achieved in a compressed format.|boolean function given bdd well known calcul number model bitstr let number calcul number model lesser known review nice method due knuth main topic howev enumer generat oppos count model enumer polynomi total time well known appar new fact also model ani fix enumer polynomi total time use suitabl wildcard achiev compress format|['Marcel Wild']|['cs.DS']
2017-04-07T11:31:21Z|2017-03-24T14:44:04Z|http://arxiv.org/abs/1703.08433v1|http://arxiv.org/pdf/1703.08433v1|Metric random matchings with applications|metric random match applic|Let $(\{1,2,\ldots,n\},d)$ be a metric space. We analyze the expected value and the variance of $\sum_{i=1}^{\lfloor n/2\rfloor}\,d({\boldsymbol{\pi}}(2i-1),{\boldsymbol{\pi}}(2i))$ for a uniformly random permutation ${\boldsymbol{\pi}}$ of $\{1,2,\ldots,n\}$, leading to the following results: (I) Consider the problem of finding a point in $\{1,2,\ldots,n\}$ with the minimum sum of distances to all points. We show that this problem has a randomized algorithm that (1) always outputs a $(2+\epsilon)$-approximate solution in expected $O(n/\epsilon^2)$ time and that (2) inherits Indyk's~\cite{Ind99, Ind00} algorithm to output a $(1+\epsilon)$-approximate solution in $O(n/\epsilon^2)$ time with probability $\Omega(1)$, where $\epsilon\in(0,1)$. (II) The average distance in $(\{1,2,\ldots,n\},d)$ can be approximated in $O(n/\epsilon)$ time to within a multiplicative factor in $[\,1/2-\epsilon,1\,]$ with probability $1/2+\Omega(1)$, where $\epsilon>0$. (III) Assume $d$ to be a graph metric. Then the average distance in $(\{1,2,\ldots,n\},d)$ can be approximated in $O(n)$ time to within a multiplicative factor in $[\,1-\epsilon,1+\epsilon\,]$ with probability $1/2+\Omega(1)$, where $\epsilon=\omega(1/n^{1/4})$.|let ldot metric space analyz expect valu varianc sum lfloor rfloor boldsymbol pi boldsymbol pi uniform random permut boldsymbol pi ldot lead follow result consid problem find point ldot minimum sum distanc point show problem random algorithm alway output epsilon approxim solut expect epsilon time inherit indyk cite ind ind algorithm output epsilon approxim solut epsilon time probabl omega epsilon ii averag distanc ldot approxim epsilon time within multipl factor epsilon probabl omega epsilon iii assum graph metric averag distanc ldot approxim time within multipl factor epsilon epsilon probabl omega epsilon omega|['Ching-Lueh Chang']|['cs.DS']
2017-04-07T11:31:21Z|2017-03-24T02:59:51Z|http://arxiv.org/abs/1703.08273v1|http://arxiv.org/pdf/1703.08273v1|An Asymptotically Tighter Bound on Sampling for Frequent Itemsets Mining|asymptot tighter bound sampl frequent itemset mine|In this paper we present a new error bound on sampling algorithms for frequent itemsets mining. We show that the new bound is asymptotically tighter than the state-of-art bounds, i.e., given the chosen samples, for small enough error probability, the new error bound is roughly half of the existing bounds. Based on the new bound, we give a new approximation algorithm, which is much simpler compared to the existing approximation algorithms, but can also guarantee the worst approximation error with precomputed sample size. We also give an algorithm which can approximate the top-$k$ frequent itemsets with high accuracy and efficiency.|paper present new error bound sampl algorithm frequent itemset mine show new bound asymptot tighter state art bound given chosen sampl small enough error probabl new error bound rough half exist bound base new bound give new approxim algorithm much simpler compar exist approxim algorithm also guarante worst approxim error precomput sampl size also give algorithm approxim top frequent itemset high accuraci effici|['Shiyu Ji', 'Kun Wan']|['cs.DS', 'cs.DB']
2017-04-07T11:31:21Z|2017-03-23T16:50:03Z|http://arxiv.org/abs/1703.08139v1|http://arxiv.org/pdf/1703.08139v1|Optimal lower bounds for universal relation, samplers, and finding   duplicates|optim lower bound univers relat sampler find duplic|In the communication problem $\mathbf{UR}$ (universal relation) [KRW95], Alice and Bob respectively receive $x$ and $y$ in $\{0,1\}^n$ with the promise that $x\neq y$. The last player to receive a message must output an index $i$ such that $x_i\neq y_i$. We prove that the randomized one-way communication complexity of this problem in the public coin model is exactly $\Theta(\min\{n, \log(1/\delta)\log^2(\frac{n}{\log(1/\delta)})\})$ bits for failure probability $\delta$. Our lower bound holds even if promised $\mathop{support}(y)\subset \mathop{support}(x)$. As a corollary, we obtain optimal lower bounds for $\ell_p$-sampling in strict turnstile streams for $0\le p < 2$, as well as for the problem of finding duplicates in a stream. Our lower bounds do not need to use large weights, and hold even if it is promised that $x\in\{0,1\}^n$ at all points in the stream.   Our lower bound demonstrates that any algorithm $\mathcal{A}$ solving sampling problems in turnstile streams in low memory can be used to encode subsets of $[n]$ of certain sizes into a number of bits below the information theoretic minimum. Our encoder makes adaptive queries to $\mathcal{A}$ throughout its execution, but done carefully so as to not violate correctness. This is accomplished by injecting random noise into the encoder's interactions with $\mathcal{A}$, which is loosely motivated by techniques in differential privacy. Our correctness analysis involves understanding the ability of $\mathcal{A}$ to correctly answer adaptive queries which have positive but bounded mutual information with $\mathcal{A}$'s internal randomness, and may be of independent interest in the newly emerging area of adaptive data analysis with a theoretical computer science lens.|communic problem mathbf ur univers relat krw alic bob respect receiv promis neq last player receiv messag must output index neq prove random one way communic complex problem public coin model exact theta min log delta log frac log delta bit failur probabl delta lower bound hold even promis mathop support subset mathop support corollari obtain optim lower bound ell sampl strict turnstil stream le well problem find duplic stream lower bound need use larg weight hold even promis point stream lower bound demonstr ani algorithm mathcal solv sampl problem turnstil stream low memori use encod subset certain size number bit inform theoret minimum encod make adapt queri mathcal throughout execut done care violat correct accomplish inject random nois encod interact mathcal loos motiv techniqu differenti privaci correct analysi involv understand abil mathcal correct answer adapt queri posit bound mutual inform mathcal intern random may independ interest newli emerg area adapt data analysi theoret comput scienc len|['Jelani Nelson', 'Jakub Pachocki', 'Zhengyu Wang']|['cs.CC', 'cs.DS']
2017-04-07T11:31:21Z|2017-03-23T12:32:10Z|http://arxiv.org/abs/1703.08041v1|http://arxiv.org/pdf/1703.08041v1|Resolving the Complexity of Some Fundamental Problems in Computational   Social Choice|resolv complex fundament problem comput social choic|This thesis is in the area called computational social choice which is an intersection area of algorithms and social choice theory.|thesi area call comput social choic intersect area algorithm social choic theori|['Palash Dey']|['cs.DS', 'cs.AI', 'cs.MA']
2017-04-07T11:31:25Z|2017-03-23T08:37:54Z|http://arxiv.org/abs/1703.07964v1|http://arxiv.org/abs/1703.07964v1|Minimum Cuts and Shortest Cycles in Directed Planar Graphs via   Noncrossing Shortest Paths|minimum cut shortest cycl direct planar graph via noncross shortest path|Let $G$ be an $n$-node simple directed planar graph with nonnegative edge weights. We study the fundamental problems of computing (1) a global cut of $G$ with minimum weight and (2) a~cycle of $G$ with minimum weight. The best previously known algorithm for the former problem, running in $O(n\log^3 n)$ time, can be obtained from the algorithm of \Lacki, Nussbaum, Sankowski, and Wulff-Nilsen for single-source all-sinks maximum flows. The best previously known result for the latter problem is the $O(n\log^3 n)$-time algorithm of Wulff-Nilsen. By exploiting duality between the two problems in planar graphs, we solve both problems in $O(n\log n\log\log n)$ time via a divide-and-conquer algorithm that finds a shortest non-degenerate cycle. The kernel of our result is an $O(n\log\log n)$-time algorithm for computing noncrossing shortest paths among nodes well ordered on a common face of a directed plane graph, which is extended from the algorithm of Italiano, Nussbaum, Sankowski, and Wulff-Nilsen for an undirected plane graph.|let node simpl direct planar graph nonneg edg weight studi fundament problem comput global cut minimum weight cycl minimum weight best previous known algorithm former problem run log time obtain algorithm lacki nussbaum sankowski wulff nilsen singl sourc sink maximum flow best previous known result latter problem log time algorithm wulff nilsen exploit dualiti two problem planar graph solv problem log log log time via divid conquer algorithm find shortest non degener cycl kernel result log log time algorithm comput noncross shortest path among node well order common face direct plane graph extend algorithm italiano nussbaum sankowski wulff nilsen undirect plane graph|['Hung-Chun Liang', 'Hsueh-I Lu']|['cs.DS', '05C38, 05C10, 05C85, 68P05']
2017-04-07T11:31:25Z|2017-03-22T21:52:05Z|http://arxiv.org/abs/1703.07867v1|http://arxiv.org/pdf/1703.07867v1|Distance-sensitive hashing|distanc sensit hash|"We initiate the study of distance-sensitive hashing, a generalization of locality-sensitive hashing that seeks a family of hash functions such that the probability of two points having the same hash value is a given function of the distance between them. More precisely, given a distance space $(X, \text{dist})$ and a ""collision probability function"" (CPF) $f\colon \mathbb{R}\rightarrow [0,1]$ we seek a distribution over pairs of functions $(h,g)$ such that for every pair of points $x, y \in X$ the collision probability is $\Pr[h(x)=g(y)] = f(\text{dist}(x,y))$. Locality-sensitive hashing is the study of how fast a CPF can decrease as the distance grows. For many spaces $f$ can be made exponentially decreasing even if we restrict attention to the symmetric case where $g=h$. In this paper we study how asymmetry makes it possible to achieve CPFs that are, for example, increasing or unimodal. Our original motivation comes from annulus queries where we are interested in searching for points at distance approximately $r$ from a query point, but we believe that distance-sensitive hashing is of interest beyond this application."|initi studi distanc sensit hash general local sensit hash seek famili hash function probabl two point hash valu given function distanc precis given distanc space text dist collis probabl function cpf colon mathbb rightarrow seek distribut pair function everi pair point collis probabl pr text dist local sensit hash studi fast cpf decreas distanc grow mani space made exponenti decreas even restrict attent symmetr case paper studi asymmetri make possibl achiev cpfs exampl increas unimod origin motiv come annulus queri interest search point distanc approxim queri point believ distanc sensit hash interest beyond applic|['Martin Aumüller', 'Tobias Christiani', 'Rasmus Pagh', 'Francesco Silvestri']|['cs.DS', 'H.3.3']
2017-04-07T11:31:25Z|2017-03-22T16:28:17Z|http://arxiv.org/abs/1703.07734v1|http://arxiv.org/pdf/1703.07734v1|On the Probe Complexity of Local Computation Algorithms|probe complex local comput algorithm|"The Local Computation Algorithms (LCA) model is a computational model aimed at problem instances with huge inputs and output. For graph problems, the input graph is accessed using probes: strong probes (SP) specify a vertex $v$ and receive as a reply a list of $v$'s neighbors, and weak probes (WP) specify a vertex $v$ and a port number $i$ and receive as a reply $v$'s $i^{th}$ neighbor. Given a local query (e.g., ""is a certain vertex in the vertex cover of the input graph?""), an LCA should compute the corresponding local output (e.g., ""yes"" or ""no"") while making only a small number of probes, with the requirement that all local outputs form a single global solution (e.g., a legal vertex cover). We study the probe complexity of LCAs that are required to work on graphs that may have arbitrarily large degrees. In particular, such LCAs are expected to probe the graph a number of times that is significantly smaller than the maximum, average, or even minimum degree.   For weak probes, we focus on the weak coloring problem. Among our results we show a separation between weak 3-coloring and weak 2-coloring for deterministic LCAs: $\log^* n + O(1)$ weak probes suffice for weak 3-coloring, but $\Omega\left(\frac{\log n}{\log\log n}\right)$ weak probes are required for weak 2-coloring.   For strong probes, we consider randomized LCAs for vertex cover and maximal/maximum matching. Our negative results include showing that there are graphs for which finding a \emph{maximal} matching requires $\Omega(\sqrt{n})$ strong probes. On the positive side, we design a randomized LCA that finds a $(1-\epsilon)$ approximation to \emph{maximum} matching in regular graphs, and uses $\frac{1}{\epsilon }^{O\left( \frac{1}{\epsilon ^2}\right)}$ probes, independently of the number of vertices and of their degrees."|local comput algorithm lca model comput model aim problem instanc huge input output graph problem input graph access use probe strong probe sp specifi vertex receiv repli list neighbor weak probe wp specifi vertex port number receiv repli th neighbor given local queri certain vertex vertex cover input graph lca comput correspond local output yes make onli small number probe requir local output form singl global solut legal vertex cover studi probe complex lcas requir work graph may arbitrarili larg degre particular lcas expect probe graph number time signific smaller maximum averag even minimum degre weak probe focus weak color problem among result show separ weak color weak color determinist lcas log weak probe suffic weak color omega left frac log log log right weak probe requir weak color strong probe consid random lcas vertex cover maxim maximum match negat result includ show graph find emph maxim match requir omega sqrt strong probe posit side design random lca find epsilon approxim emph maximum match regular graph use frac epsilon left frac epsilon right probe independ number vertic degre|['Uriel Feige', 'Boaz Patt-Shamir', 'Shai Vardi']|['cs.DS']
2017-04-07T11:31:25Z|2017-03-22T12:50:15Z|http://arxiv.org/abs/1703.07625v1|http://arxiv.org/pdf/1703.07625v1|Clustering for Different Scales of Measurement - the Gap-Ratio Weighted   K-means Algorithm|cluster differ scale measur gap ratio weight mean algorithm|This paper describes a method for clustering data that are spread out over large regions and which dimensions are on different scales of measurement. Such an algorithm was developed to implement a robotics application consisting in sorting and storing objects in an unsupervised way. The toy dataset used to validate such application consists of Lego bricks of different shapes and colors. The uncontrolled lighting conditions together with the use of RGB color features, respectively involve data with a large spread and different levels of measurement between data dimensions. To overcome the combination of these two characteristics in the data, we have developed a new weighted K-means algorithm, called gap-ratio K-means, which consists in weighting each dimension of the feature space before running the K-means algorithm. The weight associated with a feature is proportional to the ratio of the biggest gap between two consecutive data points, and the average of all the other gaps. This method is compared with two other variants of K-means on the Lego bricks clustering problem as well as two other common classification datasets.|paper describ method cluster data spread larg region dimens differ scale measur algorithm develop implement robot applic consist sort store object unsupervis way toy dataset use valid applic consist lego brick differ shape color uncontrol light condit togeth use rgb color featur respect involv data larg spread differ level measur data dimens overcom combin two characterist data develop new weight mean algorithm call gap ratio mean consist weight dimens featur space befor run mean algorithm weight associ featur proport ratio biggest gap two consecut data point averag gap method compar two variant mean lego brick cluster problem well two common classif dataset|['Joris Guérin', 'Olivier Gibaru', 'Stéphane Thiery', 'Eric Nyiri']|['cs.LG', 'cs.DS', 'stat.ML']
2017-04-07T11:31:25Z|2017-03-21T21:05:27Z|http://arxiv.org/abs/1703.07432v1|http://arxiv.org/pdf/1703.07432v1|Efficient PAC Learning from the Crowd|effici pac learn crowd|In recent years crowdsourcing has become the method of choice for gathering labeled training data for learning algorithms. Standard approaches to crowdsourcing view the process of acquiring labeled data separately from the process of learning a classifier from the gathered data. This can give rise to computational and statistical challenges. For example, in most cases there are no known computationally efficient learning algorithms that are robust to the high level of noise that exists in crowdsourced data, and efforts to eliminate noise through voting often require a large number of queries per example.   In this paper, we show how by interleaving the process of labeling and learning, we can attain computational efficiency with much less overhead in the labeling cost. In particular, we consider the realizable setting where there exists a true target function in $\mathcal{F}$ and consider a pool of labelers. When a noticeable fraction of the labelers are perfect, and the rest behave arbitrarily, we show that any $\mathcal{F}$ that can be efficiently learned in the traditional realizable PAC model can be learned in a computationally efficient manner by querying the crowd, despite high amounts of noise in the responses. Moreover, we show that this can be done while each labeler only labels a constant number of examples and the number of labels requested per example, on average, is a constant. When no perfect labelers exist, a related task is to find a set of the labelers which are good but not perfect. We show that we can identify all good labelers, when at least the majority of labelers are good.|recent year crowdsourc becom method choic gather label train data learn algorithm standard approach crowdsourc view process acquir label data separ process learn classifi gather data give rise comput statist challeng exampl case known comput effici learn algorithm robust high level nois exist crowdsourc data effort elimin nois vote often requir larg number queri per exampl paper show interleav process label learn attain comput effici much less overhead label cost particular consid realiz set exist true target function mathcal consid pool label notic fraction label perfect rest behav arbitrarili show ani mathcal effici learn tradit realiz pac model learn comput effici manner queri crowd despit high amount nois respons moreov show done label onli label constant number exampl number label request per exampl averag constant perfect label exist relat task find set label good perfect show identifi good label least major label good|['Pranjal Awasthi', 'Avrim Blum', 'Nika Haghtalab', 'Yishay Mansour']|['cs.LG', 'cs.DS']
2017-04-07T11:31:25Z|2017-03-31T18:05:45Z|http://arxiv.org/abs/1703.07417v2|http://arxiv.org/pdf/1703.07417v2|Approximating k-spanners in the LOCAL model|approxim spanner local model|Graph spanners have been studied extensively, and have many applications in algorithms, distributed systems, and computer networks. For many of these application, we want distributed constructions of spanners, i.e., algorithms which use only local information. Dinitz and Krauthgamer (PODC 2011) provided a distributed approximation algorithm for 2-spanners in the LOCAL model with polylogarithmic running time, but the question of whether a similar algorithm exists for k-spanners with k > 2 remained open. In this paper, we show that a similar algorithm also works for cases where k > 2.|graph spanner studi extens mani applic algorithm distribut system comput network mani applic want distribut construct spanner algorithm use onli local inform dinitz krauthgam podc provid distribut approxim algorithm spanner local model polylogarithm run time question whether similar algorithm exist spanner remain open paper show similar algorithm also work case|['Michael Dinitz', 'Yasamin Nazari']|['cs.DS', 'cs.DC', 'math.CO']
2017-04-07T11:31:25Z|2017-03-21T17:53:50Z|http://arxiv.org/abs/1703.07340v1|http://arxiv.org/pdf/1703.07340v1|Construction of Directed 2K Graphs|construct direct graph|We study the problem of constructing synthetic graphs that resemble real-world directed graphs in terms of their degree correlations. We define the problem of directed 2K construction (D2K) that takes as input the directed degree sequence (DDS) and a joint degree and attribute matrix (JDAM) so as to capture degree correlation specifically in directed graphs. We provide necessary and sufficient conditions to decide whether a target D2K is realizable, and we design an efficient algorithm that creates realizations with that target D2K. We evaluate our algorithm in creating synthetic graphs that target real-world directed graphs (such as Twitter) and we show that it brings significant benefits compared to state-of-the-art approaches.|studi problem construct synthet graph resembl real world direct graph term degre correl defin problem direct construct dk take input direct degre sequenc dds joint degre attribut matrix jdam captur degre correl specif direct graph provid necessari suffici condit decid whether target dk realiz design effici algorithm creat realize target dk evalu algorithm creat synthet graph target real world direct graph twitter show bring signific benefit compar state art approach|['Bálint Tillman', 'Athina Markopoulou', 'Carter T. Butts', 'Minas Gjoka']|['cs.SI', 'cs.DS']
2017-04-07T11:31:25Z|2017-03-21T15:57:42Z|http://arxiv.org/abs/1703.07290v1|http://arxiv.org/pdf/1703.07290v1|Just-in-Time Batch Scheduling Problem with Two-dimensional Bin Packing   Constraints|time batch schedul problem two dimension bin pack constraint|This paper introduces and approximately solves a multi-component problem where small rectangular items are produced from large rectangular bins via guillotine cuts. An item is characterized by its width, height, due date, and earliness and tardiness penalties per unit time. Each item induces a cost that is proportional to its earliness and tardiness. Items cut from the same bin form a batch, whose processing and completion times depend on its assigned items. The items of a batch have the completion time of their bin. The objective is to find a cutting plan that minimizes the weighted sum of earliness and tardiness penalties. We address this problem via a constraint programming based heuristic (CP) and an agent based modelling heuristic (AB). CP is an impact-based search strategy, implemented in the general-purpose solver IBM CP Optimizer. AB is constructive. It builds a solution through repeated negotiations between the set of agents representing the items and the set representing the bins. The agents cooperate to minimize the weighted earliness-tardiness penalties. The computational investigation shows that CP outperforms AB on small-sized instances while the opposite prevails for larger instances.|paper introduc approxim solv multi compon problem small rectangular item produc larg rectangular bin via guillotin cut item character width height due date earli tardi penalti per unit time item induc cost proport earli tardi item cut bin form batch whose process complet time depend assign item item batch complet time bin object find cut plan minim weight sum earli tardi penalti address problem via constraint program base heurist cp agent base model heurist ab cp impact base search strategi implement general purpos solver ibm cp optim ab construct build solut repeat negoti set agent repres item set repres bin agent cooper minim weight earli tardi penalti comput investig show cp outperform ab small size instanc opposit prevail larger instanc|"['S. Polyakovskiy', 'A. Makarowsky', ""R. M'Hallah""]"|['cs.DS']
2017-04-07T11:31:25Z|2017-03-21T14:46:39Z|http://arxiv.org/abs/1703.07247v1|http://arxiv.org/pdf/1703.07247v1|A Note on the Tree Augmentation Problem|note tree augment problem|"In the Tree Augmentation problem we are given a tree $T=(V,F)$ and an additional set $E \subseteq V \times V$ of edges, called ""links"", with positive integer costs $\{c_e:e \in E\}$. The goal is to augment $T$ by a minimum cost set of links $J \subseteq E$ such that $T \cup J$ is $2$-edge-connected. Let $M$ denote the maximum cost of a link. Recently, Adjiashvili introduced a novel LP for the problem and used it to break the natural $2$-approximation barrier for instances when $M$ is a constant. Specifically, his algorithm computes a $1.96418+\epsilon$ approximate solution in time $n^{O(M/\epsilon^2)}$. Using a slightly weaker LP we achieve ratio $\frac{12}{7}+\epsilon$ for arbitrary costs and ratio $1.6+\epsilon$ for unit costs in time $2^{O(M/\epsilon^2)}$."|tree augment problem given tree addit set subseteq time edg call link posit integ cost goal augment minimum cost set link subseteq cup edg connect let denot maximum cost link recent adjiashvili introduc novel lp problem use break natur approxim barrier instanc constant specif algorithm comput epsilon approxim solut time epsilon use slight weaker lp achiev ratio frac epsilon arbitrari cost ratio epsilon unit cost time epsilon|['Zeev Nutov']|['cs.DS']
2017-04-07T11:31:25Z|2017-03-21T14:39:35Z|http://arxiv.org/abs/1703.07244v1|http://arxiv.org/pdf/1703.07244v1|A Hybrid Feasibility Constraints-Guided Search to the Two-Dimensional   Bin Packing Problem with Due Dates|hybrid feasibl constraint guid search two dimension bin pack problem due date|The two-dimensional non-oriented bin packing problem with due dates packs a set of rectangular items, which may be rotated by 90 degrees, into identical rectangular bins. The bins have equal processing times. An item's lateness is the difference between its due date and the completion time of its bin. The problem packs all items without overlap as to minimize maximum lateness Lmax.   The paper proposes a tight lower bound that enhances an existing bound on Lmax for 24.07% of the benchmark instances and matches it in 30.87% cases. In addition, it models the problem using mixed integer programming (MIP), and solves small-sized instances exactly using CPLEX. It approximately solves larger-sized instances using a two-stage heuristic. The first stage constructs an initial solution via a first-fit heuristic that applies an iterative constraint programming (CP)-based neighborhood search. The second stage, which is iterative too, approximately solves a series of assignment low-level MIPs that are guided by feasibility constraints. It then enhances the solution via a high-level random local search. The approximate approach improves existing upper bounds by 27.45% on average, and obtains the optimum for 33.93% of the instances. Overall, the exact and approximate approaches identify the optimum for 39.07% cases.   The proposed approach is applicable to complex problems. It applies CP and MIP sequentially, while exploring their advantages, and hybridizes heuristic search with MIP. It embeds a new lookahead strategy that guards against infeasible search directions and constrains the search to improving directions only; thus, differs from traditional lookahead beam searches.|two dimension non orient bin pack problem due date pack set rectangular item may rotat degre ident rectangular bin bin equal process time item late differ due date complet time bin problem pack item without overlap minim maximum late lmax paper propos tight lower bound enhanc exist bound lmax benchmark instanc match case addit model problem use mix integ program mip solv small size instanc exact use cplex approxim solv larger size instanc use two stage heurist first stage construct initi solut via first fit heurist appli iter constraint program cp base neighborhood search second stage iter approxim solv seri assign low level mip guid feasibl constraint enhanc solut via high level random local search approxim approach improv exist upper bound averag obtain optimum instanc overal exact approxim approach identifi optimum case propos approach applic complex problem appli cp mip sequenti explor advantag hybrid heurist search mip emb new lookahead strategi guard infeas search direct constrain search improv direct onli thus differ tradit lookahead beam search|"['S. Polyakovskiy', ""R. M'Hallah""]"|['cs.DS']
2017-04-07T11:31:30Z|2017-03-21T09:37:16Z|http://arxiv.org/abs/1703.07107v1|http://arxiv.org/pdf/1703.07107v1|On the Interplay between Strong Regularity and Graph Densification|interplay strong regular graph densif|In this paper we analyze the practical implications of Szemer\'edi's regularity lemma in the preservation of metric information contained in large graphs. To this end, we present a heuristic algorithm to find regular partitions. Our experiments show that this method is quite robust to the natural sparsification of proximity graphs. In addition, this robustness can be enforced by graph densification.|paper analyz practic implic szemer edi regular lemma preserv metric inform contain larg graph end present heurist algorithm find regular partit experi show method quit robust natur sparsif proxim graph addit robust enforc graph densif|['Marco Fiorucci', 'Alessandro Torcinovich', 'Manuel Curado', 'Francisco Escolano', 'Marcello Pelillo']|['cs.DS', 'cs.CV']
2017-04-07T11:31:30Z|2017-03-20T11:17:39Z|http://arxiv.org/abs/1703.06680v1|http://arxiv.org/pdf/1703.06680v1|Parallel Sort-Based Matching for Data Distribution Management on   Shared-Memory Multiprocessors|parallel sort base match data distribut manag share memori multiprocessor|In this paper we consider the problem of identifying intersections between two sets of d-dimensional axis-parallel rectangles. This is a common problem that arises in many agent-based simulation studies, and is of central importance in the context of High Level Architecture (HLA), where it is at the core of the Data Distribution Management (DDM) service. Several realizations of the DDM service have been proposed; however, many of them are either inefficient or inherently sequential. These are serious limitations since multicore processors are now ubiquitous, and DDM algorithms -- being CPU-intensive -- could benefit from additional computing power. We propose a parallel version of the Sort-Based Matching algorithm for shared-memory multiprocessors. Sort-Based Matching is one of the most efficient serial algorithms for the DDM problem, but is quite difficult to parallelize due to data dependencies. We describe the algorithm and compute its asymptotic running time; we complete the analysis by assessing its performance and scalability through extensive experiments on two commodity multicore systems based on a dual socket Intel Xeon processor, and a single socket Intel Core i7 processor.|paper consid problem identifi intersect two set dimension axi parallel rectangl common problem aris mani agent base simul studi central import context high level architectur hla core data distribut manag ddm servic sever realize ddm servic propos howev mani either ineffici inher sequenti serious limit sinc multicor processor ubiquit ddm algorithm cpu intens could benefit addit comput power propos parallel version sort base match algorithm share memori multiprocessor sort base match one effici serial algorithm ddm problem quit difficult parallel due data depend describ algorithm comput asymptot run time complet analysi assess perform scalabl extens experi two commod multicor system base dual socket intel xeon processor singl socket intel core processor|"['Moreno Marzolla', ""Gabriele D'Angelo""]"|['cs.DC', 'cs.DS', 'cs.MA']
2017-04-07T11:31:30Z|2017-03-20T09:34:51Z|http://arxiv.org/abs/1703.06644v1|http://arxiv.org/pdf/1703.06644v1|Reoptimization of the Closest Substring Problem under Pattern Length   Modification|reoptim closest substr problem pattern length modif|This study investigates whether reoptimization can help in solving the closest substring problem. We are dealing with the following reoptimization scenario. Suppose, we have an optimal l-length closest substring of a given set of sequences S. How can this information be beneficial in obtaining an (l+k)-length closest substring for S? In this study, we show that the problem is still computationally hard even with k=1. We present greedy approximation algorithms that make use of the given information and prove that it has an additive error that grows as the parameter k increases. Furthermore, we present hard instances for each algorithm to show that the computed approximation ratio is tight. We also show that we can slightly improve the running-time of the existing polynomial-time approximation scheme (PTAS) for the original problem through reoptimization.|studi investig whether reoptim help solv closest substr problem deal follow reoptim scenario suppos optim length closest substr given set sequenc inform benefici obtain length closest substr studi show problem still comput hard even present greedi approxim algorithm make use given inform prove addit error grow paramet increas furthermor present hard instanc algorithm show comput approxim ratio tight also show slight improv run time exist polynomi time approxim scheme ptas origin problem reoptim|['Jhoirene B. Clemente', 'Henry N. Adorna']|['cs.DS', '68W25', 'G.2.1']
2017-04-07T11:31:30Z|2017-03-18T18:12:17Z|http://arxiv.org/abs/1703.06327v1|http://arxiv.org/pdf/1703.06327v1|Spectrum Estimation from a Few Entries|spectrum estim entri|Singular values of a data in a matrix form provide insights on the structure of the data, the effective dimensionality, and the choice of hyper-parameters on higher-level data analysis tools. However, in many practical applications such as collaborative filtering and network analysis, we only get a partial observation. Under such scenarios, we consider the fundamental problem of recovering spectral properties of the underlying matrix from a sampling of its entries. We are particularly interested in directly recovering the spectrum, which is the set of singular values, and also in sample-efficient approaches for recovering a spectral sum function, which is an aggregate sum of the same function applied to each of the singular values. We propose first estimating the Schatten $k$-norms of a matrix, and then applying Chebyshev approximation to the spectral sum function or applying moment matching in Wasserstein distance to recover the singular values. The main technical challenge is in accurately estimating the Schatten norms from a sampling of a matrix. We introduce a novel unbiased estimator based on counting small structures in a graph and provide guarantees that match its empirical performance. Our theoretical analysis shows that Schatten norms can be recovered accurately from strictly smaller number of samples compared to what is needed to recover the underlying low-rank matrix. Numerical experiments suggest that we significantly improve upon a competing approach of using matrix completion methods.|singular valu data matrix form provid insight structur data effect dimension choic hyper paramet higher level data analysi tool howev mani practic applic collabor filter network analysi onli get partial observ scenario consid fundament problem recov spectral properti matrix sampl entri particular interest direct recov spectrum set singular valu also sampl effici approach recov spectral sum function aggreg sum function appli singular valu propos first estim schatten norm matrix appli chebyshev approxim spectral sum function appli moment match wasserstein distanc recov singular valu main technic challeng accur estim schatten norm sampl matrix introduc novel unbias estim base count small structur graph provid guarante match empir perform theoret analysi show schatten norm recov accur strict smaller number sampl compar need recov low rank matrix numer experi suggest signific improv upon compet approach use matrix complet method|['Ashish Khetan', 'Sewoong Oh']|['stat.ML', 'cs.DS', 'cs.LG', 'cs.NA']
2017-04-07T11:31:30Z|2017-03-18T17:21:14Z|http://arxiv.org/abs/1703.06320v1|http://arxiv.org/pdf/1703.06320v1|Hardware-Efficient Schemes of Quaternion Multiplying Units for 2D   Discrete Quaternion Fourier Transform Processors|hardwar effici scheme quaternion multipli unit discret quaternion fourier transform processor|In this paper, we offer and discuss three efficient structural solutions for the hardware-oriented implementation of discrete quaternion Fourier transform basic operations with reduced implementation complexities. The first solution: a scheme for calculating sq product, the second solution: a scheme for calculating qt product, and the third solution: a scheme for calculating sqt product, where s is a so-called i-quaternion, t is an j-quaternion, and q is an usual quaternion. The direct multiplication of two usual quaternions requires 16 real multiplications (or two-operand multipliers in the case of fully parallel hardware implementation) and 12 real additions (or binary adders). At the same time, our solutions allow to design the computation units, which consume only 6 multipliers plus 6 two input adders for implementation of sq or qt basic operations and 9 binary multipliers plus 6 two-input adders and 4 four-input adders for implementation of sqt basic operation.|paper offer discuss three effici structur solut hardwar orient implement discret quaternion fourier transform basic oper reduc implement complex first solut scheme calcul sq product second solut scheme calcul qt product third solut scheme calcul sqt product call quaternion quaternion usual quaternion direct multipl two usual quaternion requir real multipl two operand multipli case fulli parallel hardwar implement real addit binari adder time solut allow design comput unit consum onli multipli plus two input adder implement sq qt basic oper binari multipli plus two input adder four input adder implement sqt basic oper|['Aleksandr Cariow', 'Galina Cariowa', 'Marina Chicheva']|['cs.DS', 'cs.AR', '65T50, 15A04, 15A66, 15A66, 15A69, 03D15, 65Y20, 65Y10', 'F.2.1; I.1.2; C.1.4; C.3']
2017-04-07T11:31:30Z|2017-03-18T00:44:38Z|http://arxiv.org/abs/1703.06227v1|http://arxiv.org/pdf/1703.06227v1|Discriminative Distance-Based Network Indices and the Tiny-World   Property|discrimin distanc base network indic tini world properti|Distance-based indices, including closeness centrality, average path length, eccentricity and average eccentricity, are important tools for network analysis. In these indices, the distance between two vertices is measured by the size of shortest paths between them. However, this measure has shortcomings. A well-studied shortcoming is that extending it to disconnected graphs (and also directed graphs) is controversial. The second shortcoming is that when this measure is used in real-world networks, a huge number of vertices may have exactly the same closeness/eccentricity scores. The third shortcoming is that in many applications, the distance between two vertices not only depends on the size of shortest paths, but also on the number of shortest paths between them. In this paper, we develop a new distance measure between vertices of a graph that yields discriminative distance-based centrality indices. This measure is proportional to the size of shortest paths and inversely proportional to the number of shortest paths. We present algorithms for exact computation of the proposed discriminative indices. We then develop randomized algorithms that precisely estimate average discriminative path length and average discriminative eccentricity and show that they give $(\epsilon,\delta)$-approximations of these indices. Finally, we preform extensive experiments over several real-world networks from different domains and show that compared to the traditional indices, discriminative indices have usually much more discriminability. Our experiments reveal that real-world networks have usually a tiny average discriminative path length, bounded by a constant (e.g., 2). We refer to this property as the tiny-world property.|distanc base indic includ close central averag path length eccentr averag eccentr import tool network analysi indic distanc two vertic measur size shortest path howev measur shortcom well studi shortcom extend disconnect graph also direct graph controversi second shortcom measur use real world network huge number vertic may exact close eccentr score third shortcom mani applic distanc two vertic onli depend size shortest path also number shortest path paper develop new distanc measur vertic graph yield discrimin distanc base central indic measur proport size shortest path invers proport number shortest path present algorithm exact comput propos discrimin indic develop random algorithm precis estim averag discrimin path length averag discrimin eccentr show give epsilon delta approxim indic final preform extens experi sever real world network differ domain show compar tradit indic discrimin indic usual much discrimin experi reveal real world network usual tini averag discrimin path length bound constant refer properti tini world properti|['Mostafa Haghir Chehreghani', 'Albert Bifet', 'Talel Abdessalem']|['cs.DS', 'cs.SI']
2017-04-07T11:31:30Z|2017-03-17T17:31:01Z|http://arxiv.org/abs/1703.06733v1|http://arxiv.org/pdf/1703.06733v1|Discovering Relaxed Sound Workflow Nets using Integer Linear Programming|discov relax sound workflow net use integ linear program|Process mining is concerned with the analysis, understanding and improvement of business processes. Process discovery, i.e. discovering a process model based on an event log, is considered the most challenging process mining task. State-of-the-art process discovery algorithms only discover local control-flow patterns and are unable to discover complex, non-local patterns. Region theory based techniques, i.e. an established class of process discovery techniques, do allow for discovering such patterns. However, applying region theory directly results in complex, over-fitting models, which is less desirable. Moreover, region theory does not cope with guarantees provided by state-of-the-art process discovery algorithms, both w.r.t. structural and behavioural properties of the discovered process models. In this paper we present an ILP-based process discovery approach, based on region theory, that guarantees to discover relaxed sound workflow nets. Moreover, we devise a filtering algorithm, based on the internal working of the ILP-formulation, that is able to cope with the presence of infrequent behaviour. We have extensively evaluated the technique using different event logs with different levels of exceptional behaviour. Our experiments show that the presented approach allow us to leverage the inherent shortcomings of existing region-based approaches. The techniques presented are implemented and readily available in the HybridILPMiner package in the open-source process mining tool-kits ProM and RapidProM.|process mine concern analysi understand improv busi process process discoveri discov process model base event log consid challeng process mine task state art process discoveri algorithm onli discov local control flow pattern unabl discov complex non local pattern region theori base techniqu establish class process discoveri techniqu allow discov pattern howev appli region theori direct result complex fit model less desir moreov region theori doe cope guarante provid state art process discoveri algorithm structur behaviour properti discov process model paper present ilp base process discoveri approach base region theori guarante discov relax sound workflow net moreov devis filter algorithm base intern work ilp formul abl cope presenc infrequ behaviour extens evalu techniqu use differ event log differ level except behaviour experi show present approach allow us leverag inher shortcom exist region base approach techniqu present implement readili avail hybridilpmin packag open sourc process mine tool kit prom rapidprom|['S. J. van Zelst', 'B. F. van Dongen', 'W. M. P. van der Aalst', 'H. M. W. Verbeek']|['cs.DS']
2017-04-07T11:31:30Z|2017-03-17T16:18:31Z|http://arxiv.org/abs/1703.06074v1|http://arxiv.org/pdf/1703.06074v1|Robust Assignments with Vulnerable Nodes|robust assign vulner node|Various real-life planning problems require making upfront decisions before all parameters of the problem have been disclosed. An important special case of such problem especially arises in scheduling and staff rostering problems, where a set of tasks needs to be assigned to an available set of resources (personnel or machines), in a way that each task is assigned to one resource, while no task is allowed to share a resource with another task. In its nominal form, the resulting computational problem reduces to the well-known assignment problem that can be modeled as matching problems on bipartite graphs.   In recent work \cite{adjiashvili_bindewald_michaels_icalp2016}, a new robust model for the assignment problem was introduced that can deal with situations in which certain resources, i.e.\ nodes or edges of the underlying bipartite graph, are vulnerable and may become unavailable after a solution has been chosen. In the original version from \cite{adjiashvili_bindewald_michaels_icalp2016} the resources subject to uncertainty are the edges of the underlying bipartite graph.   In this follow-up work, we complement our previous study by considering nodes as being vulnerable, instead of edges. The goal is now to choose a minimum-cost collection of nodes such that, if any vulnerable node becomes unavailable, the remaining part of the solution still contains sufficient nodes to perform all tasks. From a practical point of view, such type of unavailability is interesting as it is typically caused e.g.\ by an employee's sickness, or machine failure. We present algorithms and hardness of approximation results for several variants of the problem.|various real life plan problem requir make upfront decis befor paramet problem disclos import special case problem especi aris schedul staff roster problem set task need assign avail set resourc personnel machin way task assign one resourc task allow share resourc anoth task nomin form result comput problem reduc well known assign problem model match problem bipartit graph recent work cite adjiashvili bindewald michael icalp new robust model assign problem introduc deal situat certain resourc node edg bipartit graph vulner may becom unavail solut chosen origin version cite adjiashvili bindewald michael icalp resourc subject uncertainti edg bipartit graph follow work complement previous studi consid node vulner instead edg goal choos minimum cost collect node ani vulner node becom unavail remain part solut still contain suffici node perform task practic point view type unavail interest typic caus employe sick machin failur present algorithm hard approxim result sever variant problem|['David Adjiashvili', 'Viktor Bindewald', 'Dennis Michaels']|['cs.DS', 'cs.DM', '90C27', 'I.1.2; G.2.2; G.1.6']
2017-04-07T11:31:30Z|2017-03-17T16:08:23Z|http://arxiv.org/abs/1703.06065v1|http://arxiv.org/pdf/1703.06065v1|Block CUR : Decomposing Large Distributed Matrices|block cur decompos larg distribut matric|A common problem in large-scale data analysis is to approximate a matrix using a combination of specifically sampled rows and columns, known as CUR decomposition. Unfortunately, in many real-world environments, the ability to sample specific individual rows or columns of the matrix is limited by either system constraints or cost. In this paper, we consider matrix approximation by sampling predefined blocks of columns (or rows) from the matrix. This regime is commonly found when data is distributed across multiple nodes in a compute cluster, where such blocks correspond to columns (or rows) of the matrix stored on the same node, which can be retrieved with much less overhead than retrieving individual columns stored across different nodes. We propose a novel algorithm for sampling useful column blocks and provide guarantees for the quality of the approximation. We demonstrate the practical utility of this algorithm for computing the block CUR decomposition of large matrices in a distributed setting using Apache Spark. Using our proposed block CUR algorithms, we can achieve a significant speed-up compared to a regular CUR decomposition with the same quality of approximation.|common problem larg scale data analysi approxim matrix use combin specif sampl row column known cur decomposit unfortun mani real world environ abil sampl specif individu row column matrix limit either system constraint cost paper consid matrix approxim sampl predefin block column row matrix regim common found data distribut across multipl node comput cluster block correspond column row matrix store node retriev much less overhead retriev individu column store across differ node propos novel algorithm sampl use column block provid guarante qualiti approxim demonstr practic util algorithm comput block cur decomposit larg matric distribut set use apach spark use propos block cur algorithm achiev signific speed compar regular cur decomposit qualiti approxim|['Urvashi Oswal', 'Swayambhoo Jain', 'Kevin S. Xu', 'Brian Eriksson']|['stat.ML', 'cs.DC', 'cs.DS', 'cs.LG']
2017-04-07T11:31:30Z|2017-03-17T15:56:50Z|http://arxiv.org/abs/1703.06061v1|http://arxiv.org/pdf/1703.06061v1|Approximation ratio of RePair|approxim ratio repair|In a seminal paper of Charikar et al.~on the smallest grammar problem, the authors derive upper and lower bounds on the approximation ratios for several grammar-based compressors. Here we improve the lower bound for the famous {\sf RePair} algorithm from $\Omega(\sqrt{\log n})$ to $\Omega(\log n/\log\log n)$. The family of words used in our proof is defined over a binary alphabet, while the lower bound from Charikar et al. needs an alphabet of logarithmic size in the length of the provided words.|semin paper charikar et al smallest grammar problem author deriv upper lower bound approxim ratio sever grammar base compressor improv lower bound famous sf repair algorithm omega sqrt log omega log log log famili word use proof defin binari alphabet lower bound charikar et al need alphabet logarithm size length provid word|['Danny Hucke', 'Artur Jez', 'Markus Lohrey']|['cs.DS', 'F.2.2, E.4']
2017-04-07T11:31:35Z|2017-03-21T12:29:29Z|http://arxiv.org/abs/1703.06053v2|http://arxiv.org/pdf/1703.06053v2|Fast Non-Monotone Submodular Maximisation Subject to a Matroid   Constraint|fast non monoton submodular maximis subject matroid constraint|In this work we present the first practical $\left(\frac{1}{e}-\epsilon\right)$-approximation algorithm to maximise a general non-negative submodular function subject to a matroid constraint. Our algorithm is based on combining the decreasing-threshold procedure of Badanidiyuru and Vondrak (SODA 2014) with a smoother version of the measured continuous greedy algorithm of Feldman et al. (FOCS 2011). This enables us to obtain an algorithm that requires $O(\frac{nr^2}{\epsilon^4} \big(\frac{a+b}{a}\big)^2 \log^2({\frac{n}{\epsilon}}))$ value oracle calls, where $n$ is the cardinality of the ground set, $r$ is the matroid rank, and $ b, a \in \mathbb{R}^+$ are the absolute values of the minimum and maximum marginal values that the function $f$ can take i.e.: $ -b \leq f_S(i) \leq a$, for all $i\in E$ and $S\subseteq E$, (here, $E$ is the ground set). The additional value oracle calls with respect to the work of Badanidiyuru and Vondrak come from the greater spread in the sampling of the multilinear extension that the possibility of negative marginal values introduce.|work present first practic left frac epsilon right approxim algorithm maximis general non negat submodular function subject matroid constraint algorithm base combin decreas threshold procedur badanidiyuru vondrak soda smoother version measur continu greedi algorithm feldman et al foc enabl us obtain algorithm requir frac nr epsilon big frac big log frac epsilon valu oracl call cardin ground set matroid rank mathbb absolut valu minimum maximum margin valu function take leq leq subseteq ground set addit valu oracl call respect work badanidiyuru vondrak come greater spread sampl multilinear extens possibl negat margin valu introduc|['Pau Segui-Gasco', 'Hyo-Sang Shin']|['cs.DS']
2017-04-07T11:31:35Z|2017-03-17T15:08:17Z|http://arxiv.org/abs/1703.06048v1|http://arxiv.org/pdf/1703.06048v1|An FPTAS for the Knapsack Problem with Parametric Weights|fptas knapsack problem parametr weight|In this paper, we investigate the parametric weight knapsack problem, in which the item weights are affine functions of the form $w_i(\lambda) = a_i + \lambda \cdot b_i$ for $i \in \{1,\ldots,n\}$ depending on a real-valued parameter $\lambda$. The aim is to provide a solution for all values of the parameter. It is well-known that any exact algorithm for the problem may need to output an exponential number of knapsack solutions. We present the first fully polynomial-time approximation scheme (FPTAS) for the problem that, for any desired precision $\varepsilon \in (0,1)$, computes $(1-\varepsilon)$-approximate solutions for all values of the parameter. Our FPTAS is based on two different approaches and achieves a running time of $\mathcal{O}(n^3/\varepsilon^2 \cdot \min\{ \log^2 P, n^2 \} \cdot \min\{\log M, n \log (n/\varepsilon) / \log(n \log (n/\varepsilon) )\})$ where $P$ is an upper bound on the optimal profit and $M := \max\{W, n \cdot \max\{a_i,b_i: i \in \{1,\ldots,n\}\}\}$ for a knapsack with capacity $W$.|paper investig parametr weight knapsack problem item weight affin function form lambda lambda cdot ldot depend real valu paramet lambda aim provid solut valu paramet well known ani exact algorithm problem may need output exponenti number knapsack solut present first fulli polynomi time approxim scheme fptas problem ani desir precis varepsilon comput varepsilon approxim solut valu paramet fptas base two differ approach achiev run time mathcal varepsilon cdot min log cdot min log log varepsilon log log varepsilon upper bound optim profit max cdot max ldot knapsack capac|['Michael Holzhauser', 'Sven O. Krumke']|['cs.DS', 'cs.CC', 'math.OC']
2017-04-07T11:31:35Z|2017-03-17T14:53:55Z|http://arxiv.org/abs/1703.06040v1|http://arxiv.org/pdf/1703.06040v1|Towards a Topology-Shape-Metrics Framework for Ortho-Radial Drawings|toward topolog shape metric framework ortho radial draw|Ortho-Radial drawings are a generalization of orthogonal drawings to grids that are formed by concentric circles and straight-line spokes emanating from the circles' center. Such drawings have applications in schematic graph layouts, e.g., for metro maps and destination maps.   A plane graph is a planar graph with a fixed planar embedding. We give a combinatorial characterization of the plane graphs that admit a planar ortho-radial drawing without bends. Previously, such a characterization was only known for paths, cycles, and theta graphs, and in the special case of rectangular drawings for cubic graphs, where the contour of each face is required to be a rectangle.   The characterization is expressed in terms of an ortho-radial representation that, similar to Tamassia's orthogonal representations for orthogonal drawings describes such a drawing combinatorially in terms of angles around vertices and bends on the edges. In this sense our characterization can be seen as a first step towards generalizing the Topology-Shape-Metrics framework of Tamassia to ortho-radial drawings.|ortho radial draw general orthogon draw grid form concentr circl straight line spoke eman circl center draw applic schemat graph layout metro map destin map plane graph planar graph fix planar embed give combinatori character plane graph admit planar ortho radial draw without bend previous character onli known path cycl theta graph special case rectangular draw cubic graph contour face requir rectangl character express term ortho radial represent similar tamassia orthogon represent orthogon draw describ draw combinatori term angl around vertic bend edg sens character seen first step toward general topolog shape metric framework tamassia ortho radial draw|['Lukas Barth', 'Benjamin Niedermann', 'Ignaz Rutter', 'Matthias Wolf']|['cs.DM', 'cs.DS']
2017-04-07T11:31:35Z|2017-03-17T12:57:18Z|http://arxiv.org/abs/1703.05997v1|http://arxiv.org/pdf/1703.05997v1|Connection Scan Algorithm|connect scan algorithm|We introduce the Connection Scan Algorithm (CSA) to efficiently answer queries to timetable information systems. The input consists, in the simplest setting, of a source position and a desired target position. The output consist is a sequence of vehicles such as trains or buses that a traveler should take to get from the source to the target. We study several problem variations such as the earliest arrival and profile problems. We present algorithm variants that only optimize the arrival time or additionally optimize the number of transfers in the Pareto sense. An advantage of CSA is that is can easily adjust to changes in the timetable, allowing the easy incorporation of known vehicle delays. We additionally introduce the Minimum Expected Arrival Time (MEAT) problem to handle possible, uncertain, future vehicle delays. We present a solution to the MEAT problem that is based upon CSA. Finally, we extend CSA using the multilevel overlay paradigm to answer complex queries on nation-wide integrated timetables with trains and buses.|introduc connect scan algorithm csa effici answer queri timet inform system input consist simplest set sourc posit desir target posit output consist sequenc vehicl train buse travel take get sourc target studi sever problem variat earliest arriv profil problem present algorithm variant onli optim arriv time addit optim number transfer pareto sens advantag csa easili adjust chang timet allow easi incorpor known vehicl delay addit introduc minimum expect arriv time meat problem handl possibl uncertain futur vehicl delay present solut meat problem base upon csa final extend csa use multilevel overlay paradigm answer complex queri nation wide integr timet train buse|['Julian Dibbelt', 'Thomas Pajor', 'Ben Strasser', 'Dorothea Wagner']|['cs.DS']
2017-04-07T11:31:35Z|2017-03-16T13:10:29Z|http://arxiv.org/abs/1703.05598v1|http://arxiv.org/pdf/1703.05598v1|Linear-Time Algorithm for Maximum-Cardinality Matching on   Cocomparability Graphs|linear time algorithm maximum cardin match cocompar graph|Finding maximum-cardinality matchings in undirected graphs is arguably one of the most central graph problems. For general m-edge and n-vertex graphs, it is well-known to be solvable in $O(m \sqrt{n})$ time. We develop the first linear-time algorithm to find maximum-cardinality matchings on cocomparability graphs, a prominent subclass of perfect graphs that contains interval graphs as well as permutation graphs. Our algorithm is based on the recently discovered Lexicographic Depth First Search (LDFS).|find maximum cardin match undirect graph arguabl one central graph problem general edg vertex graph well known solvabl sqrt time develop first linear time algorithm find maximum cardin match cocompar graph promin subclass perfect graph contain interv graph well permut graph algorithm base recent discov lexicograph depth first search ldfs|['George B. Mertzios', 'André Nichterlein', 'Rolf Niedermeier']|['cs.DS', 'F.2.2']
2017-04-07T11:31:35Z|2017-03-16T11:39:22Z|http://arxiv.org/abs/1703.05568v1|http://arxiv.org/pdf/1703.05568v1|Quantum Spectral Clustering through a Biased Phase Estimation Algorithm|quantum spectral cluster bias phase estim algorithm|In this brief paper, we go through the theoretical steps of the spectral clustering on quantum computers by employing the phase estimation and the amplitude amplification algorithms. To speed-up the amplitude amplification, we introduce a biased version of the phase estimation algorithm. In addition, when the circuit representation of a data matrix of order $N$ is produced through an ancilla based circuit in which the matrix is written as a sum of $L$ number of Householder matrices; we show that the computational complexity of the whole process is bound by $O(c2^mLN)$ number of quantum gates. Here, $m$ represents the number of qubits involved in the phase register of the phase estimation algorithm and $c$ represents the number of trials done to find the best clustering.|brief paper go theoret step spectral cluster quantum comput employ phase estim amplitud amplif algorithm speed amplitud amplif introduc bias version phase estim algorithm addit circuit represent data matrix order produc ancilla base circuit matrix written sum number household matric show comput complex whole process bound mln number quantum gate repres number qubit involv phase regist phase estim algorithm repres number trial done find best cluster|['Ammar Daskin']|['quant-ph', 'cs.DS']
2017-04-07T11:31:35Z|2017-03-16T11:09:25Z|http://arxiv.org/abs/1703.05559v1|http://arxiv.org/pdf/1703.05559v1|Improving TSP tours using dynamic programming over tree decomposition|improv tsp tour use dynam program tree decomposit|Given a traveling salesman problem (TSP) tour $H$ in graph $G$ a $k$-move is an operation which removes $k$ edges from $H$, and adds $k$ edges of $G$ so that a new tour $H'$ is formed. The popular $k$-OPT heuristics for TSP finds a local optimum by starting from an arbitrary tour $H$ and then improving it by a sequence of $k$-moves.   Until 2016, the only known algorithm to find an improving $k$-move for a given tour was the naive solution in time $O(n^k)$. At ICALP'16 de Berg, Buchin, Jansen and Woeginger showed an $O(n^{\lfloor 2/3k \rfloor+1})$-time algorithm.   We show an algorithm which runs in $O(n^{(1/4+\epsilon_k)k})$ time, where $\lim \epsilon_k = 0$. We are able to show that it improves over the state of the art for every $k=5,\ldots,10$. For the most practically relevant case $k=5$ we provide a slightly refined algorithm running in $O(n^{3.4})$ time. We also show that for the $k=4$ case, improving over the $O(n^3)$-time algorithm of de Berg et al. would be a major breakthrough: an $O(n^{3-\epsilon})$-time algorithm for any $\epsilon>0$ would imply an $O(n^{3-\delta})$-time algorithm for the ALL PAIRS SHORTEST PATHS problem, for some $\delta>0$.|given travel salesman problem tsp tour graph move oper remov edg add edg new tour form popular opt heurist tsp find local optimum start arbitrari tour improv sequenc move onli known algorithm find improv move given tour naiv solut time icalp de berg buchin jansen woeging show lfloor rfloor time algorithm show algorithm run epsilon time lim epsilon abl show improv state art everi ldot practic relev case provid slight refin algorithm run time also show case improv time algorithm de berg et al would major breakthrough epsilon time algorithm ani epsilon would impli delta time algorithm pair shortest path problem delta|['Marek Cygan', 'Lukasz Kowalik', 'Arkadiusz Socala']|['cs.DS']
2017-04-07T11:31:35Z|2017-03-16T08:52:21Z|http://arxiv.org/abs/1703.05509v1|http://arxiv.org/pdf/1703.05509v1|VieM v1.00 -- Vienna Mapping and Sparse Quadratic Assignment User Guide|viem vienna map spars quadrat assign user guid|This paper severs as a user guide to the mapping framework VieM (Vienna Mapping and Sparse Quadratic Assignment). We give a rough overview of the techniques used within the framework and describe the user interface as well as the file formats used.|paper sever user guid map framework viem vienna map spars quadrat assign give rough overview techniqu use within framework describ user interfac well file format use|['Christian Schulz', 'Jesper Larsson Träff']|['cs.DC', 'cs.DS', 'math.CO']
2017-04-07T11:31:35Z|2017-03-16T08:10:30Z|http://arxiv.org/abs/1703.05496v1|http://arxiv.org/pdf/1703.05496v1|Data Delivery by Mobile Agents with Energy Constraints over a fixed path|data deliveri mobil agent energi constraint fix path|We consider $k$ mobile agents of limited energy that are initially located at vertices of an edge-weighted graph $G$ and have to collectively deliver data from a source vertex $s$ to a target vertex $t$. The data are to be collected by an agent reaching $s$, who can carry and then hand them over another agent etc., until some agent with the data reaches $t$. The data can be carried only over a fixed $s-t$ path of $G$; each agent has an initial energy budget and each time it passes an edge, it consumes the edge's weights in energy units and stalls if its energy is not anymore sufficient to move. The main result of this paper is a 3-approximation polynomial time algorithm for the data delivery problem over a fixed $s-t$ path in the graph, for identical initial energy budgets and at most one allowed data hand-over per agent.|consid mobil agent limit energi initi locat vertic edg weight graph collect deliv data sourc vertex target vertex data collect agent reach carri hand anoth agent etc agent data reach data carri onli fix path agent initi energi budget time pass edg consum edg weight energi unit stall energi anymor suffici move main result paper approxim polynomi time algorithm data deliveri problem fix path graph ident initi energi budget one allow data hand per agent|['Aristotelis Giannakos', 'Mhand Hifi', 'Gregory Karagiorgos']|['cs.DS']
2017-04-07T11:31:35Z|2017-03-15T23:02:07Z|http://arxiv.org/abs/1703.05418v1|http://arxiv.org/pdf/1703.05418v1|A Local Algorithm for the Sparse Spanning Graph Problem|local algorithm spars span graph problem|Constructing a sparse \emph{spanning subgraph} is a fundamental primitive in graph theory. In this paper, we study this problem in the Centralized Local model, where the goal is to decide whether an edge is part of the spanning subgraph by examining only a small part of the input; yet, answers must be globally consistent and independent of prior queries.   Unfortunately, maximally sparse spanning subgraphs, i.e., spanning trees, cannot be constructed efficiently in this model. Therefore, we settle for a spanning subgraph containing at most $(1+\varepsilon)n$ edges (where $n$ is the number of vertices and $\varepsilon$ is a given approximation/sparsity parameter). We achieve query complexity of $\tilde{O}(poly(\Delta/\varepsilon)n^{2/3})$,\footnote{$\tilde{O}$-notation hides polylogarithmic factors in $n$.} where $\Delta$ is the maximum degree of the input graph. Our algorithm is the first to do so on arbitrary graphs. Moreover, we achieve the additional property that our algorithm outputs a \emph{spanner,} i.e., distances are approximately preserved. With high probability, for each deleted edge there is a path of $O(poly(\Delta/\varepsilon)\log^2 n)$ hops in the output that connects its endpoints.|construct spars emph span subgraph fundament primit graph theori paper studi problem central local model goal decid whether edg part span subgraph examin onli small part input yet answer must global consist independ prior queri unfortun maxim spars span subgraph span tree cannot construct effici model therefor settl span subgraph contain varepsilon edg number vertic varepsilon given approxim sparsiti paramet achiev queri complex tild poli delta varepsilon footnot tild notat hide polylogarithm factor delta maximum degre input graph algorithm first arbitrari graph moreov achiev addit properti algorithm output emph spanner distanc approxim preserv high probabl delet edg path poli delta varepsilon log hop output connect endpoint|['Christoph Lenzen', 'Reut Levi']|['cs.DS']
2017-04-07T11:31:39Z|2017-03-15T15:10:16Z|http://arxiv.org/abs/1703.05199v1|http://arxiv.org/pdf/1703.05199v1|Optimal Unateness Testers for Real-Valued Functions: Adaptivity Helps|optim unat tester real valu function adapt help|We study the problem of testing unateness of functions $f:\{0,1\}^d \to \mathbb{R}.$ We give a $O(\frac{d}{\epsilon} \cdot \log\frac{d}{\epsilon})$-query nonadaptive tester and a $O(\frac{d}{\epsilon})$-query adaptive tester and show that both testers are optimal for a fixed distance parameter $\epsilon$. Previously known unateness testers worked only for Boolean functions, and their query complexity had worse dependence on the dimension both for the adaptive and the nonadaptive case. Moreover, no lower bounds for testing unateness were known. We also generalize our results to obtain optimal unateness testers for functions $f:[n]^d \to \mathbb{R}$.   Our results establish that adaptivity helps with testing unateness of real-valued functions on domains of the form $\{0,1\}^d$ and, more generally, $[n]^d$. This stands in contrast to the situation for monotonicity testing where there is no adaptivity gap for functions $f:[n]^d \to \mathbb{R}$.|studi problem test unat function mathbb give frac epsilon cdot log frac epsilon queri nonadapt tester frac epsilon queri adapt tester show tester optim fix distanc paramet epsilon previous known unat tester work onli boolean function queri complex wors depend dimens adapt nonadapt case moreov lower bound test unat known also general result obtain optim unat tester function mathbb result establish adapt help test unat real valu function domain form general stand contrast situat monoton test adapt gap function mathbb|['Roksana Baleshzar', 'Deeparnab Chakrabarty', 'Ramesh Krishnan S. Pallavoor', 'Sofya Raskhodnikova', 'C. Seshadhri']|['cs.DS', 'cs.DM']
2017-04-07T11:31:39Z|2017-03-15T14:01:21Z|http://arxiv.org/abs/1703.05160v1|http://arxiv.org/pdf/1703.05160v1|A New Unbiased and Efficient Class of LSH-Based Samplers and Estimators   for Partition Function Computation in Log-Linear Models|new unbias effici class lsh base sampler estim partit function comput log linear model|"Log-linear models are arguably the most successful class of graphical models for large-scale applications because of their simplicity and tractability. Learning and inference with these models require calculating the partition function, which is a major bottleneck and intractable for large state spaces. Importance Sampling (IS) and MCMC-based approaches are lucrative. However, the condition of having a ""good"" proposal distribution is often not satisfied in practice.   In this paper, we add a new dimension to efficient estimation via sampling. We propose a new sampling scheme and an unbiased estimator that estimates the partition function accurately in sub-linear time. Our samples are generated in near-constant time using locality sensitive hashing (LSH), and so are correlated and unnormalized. We demonstrate the effectiveness of our proposed approach by comparing the accuracy and speed of estimating the partition function against other state-of-the-art estimation techniques including IS and the efficient variant of Gumbel-Max sampling. With our efficient sampling scheme, we accurately train real-world language models using only 1-2% of computations."|log linear model arguabl success class graphic model larg scale applic becaus simplic tractabl learn infer model requir calcul partit function major bottleneck intract larg state space import sampl mcmc base approach lucrat howev condit good propos distribut often satisfi practic paper add new dimens effici estim via sampl propos new sampl scheme unbias estim estim partit function accur sub linear time sampl generat near constant time use local sensit hash lsh correl unnorm demonstr effect propos approach compar accuraci speed estim partit function state art estim techniqu includ effici variant gumbel max sampl effici sampl scheme accur train real world languag model use onli comput|['Ryan Spring', 'Anshumali Shrivastava']|['stat.ML', 'cs.DB', 'cs.DS', 'cs.LG']
2017-04-07T11:31:39Z|2017-03-15T13:51:23Z|http://arxiv.org/abs/1703.05156v1|http://arxiv.org/pdf/1703.05156v1|Complexity Dichotomies for the Minimum F-Overlay Problem|complex dichotomi minimum overlay problem|For a (possibly infinite) fixed family of graphs F, we say that a graph G overlays F on a hypergraph H if V(H) is equal to V(G) and the subgraph of G induced by every hyperedge of H contains some member of F as a spanning subgraph.While it is easy to see that the complete graph on  V(H)  overlays F on a hypergraph H whenever the problem admits a solution, the Minimum F-Overlay problem asks for such a graph with the minimum number of edges.This problem allows to generalize some natural problems which may arise in practice. For instance, if the family F contains all connected graphs, then Minimum F-Overlay corresponds to the Minimum Connectivity Inference problem (also known as Subset Interconnection Design problem) introduced for the low-resolution reconstruction of macro-molecular assembly in structural biology, or for the design of networks.Our main contribution is a strong dichotomy result regarding the polynomial vs. NP-hard status with respect to the considered family F. Roughly speaking, we show that the easy cases one can think of (e.g. when edgeless graphs of the right sizes are in F, or if F contains only cliques) are the only families giving rise to a polynomial problem: all others are NP-complete.We then investigate the parameterized complexity of the problem and give similar sufficient conditions on F that give rise to W[1]-hard, W[2]-hard or FPT problems when the parameter is the size of the solution.This yields an FPT/W[1]-hard dichotomy for a relaxed problem, where every hyperedge of H must contain some member of F as a (non necessarily spanning) subgraph.|possibl infinit fix famili graph say graph overlay hypergraph equal subgraph induc everi hyperedg contain member span subgraph easi see complet graph overlay hypergraph whenev problem admit solut minimum overlay problem ask graph minimum number edg problem allow general natur problem may aris practic instanc famili contain connect graph minimum overlay correspond minimum connect infer problem also known subset interconnect design problem introduc low resolut reconstruct macro molecular assembl structur biolog design network main contribut strong dichotomi result regard polynomi vs np hard status respect consid famili rough speak show easi case one think edgeless graph right size contain onli cliqu onli famili give rise polynomi problem np complet investig parameter complex problem give similar suffici condit give rise hard hard fpt problem paramet size solut yield fpt hard dichotomi relax problem everi hyperedg must contain member non necessarili span subgraph|['Nathann Cohen', 'Frédéric Havet', 'Dorian Mazauric', 'Ignasi Sau', 'Rémi Watrigant']|['cs.DS', 'cs.CC']
2017-04-07T11:31:39Z|2017-03-15T12:06:53Z|http://arxiv.org/abs/1703.05102v1|http://arxiv.org/pdf/1703.05102v1|Algorithms for outerplanar graph roots and graph roots of pathwidth at   most 2|algorithm outerplanar graph root graph root pathwidth|Deciding whether a given graph has a square root is a classical problem that has been studied extensively both from graph theoretic and from algorithmic perspectives. The problem is NP-complete in general, and consequently substantial effort has been dedicated to deciding whether a given graph has a square root that belongs to a particular graph class. There are both polynomial-time solvable and NP-complete cases, depending on the graph class. We contribute with new results in this direction. Given an arbitrary input graph G, we give polynomial-time algorithms to decide whether G has an outerplanar square root, and whether G has a square root that is of pathwidth at most 2.|decid whether given graph squar root classic problem studi extens graph theoret algorithm perspect problem np complet general consequ substanti effort dedic decid whether given graph squar root belong particular graph class polynomi time solvabl np complet case depend graph class contribut new result direct given arbitrari input graph give polynomi time algorithm decid whether outerplanar squar root whether squar root pathwidth|['Petr A. Golovach', 'Pinar Heggernes', 'Dieter Kratsch', 'Paloma T. Lima', 'Daniel Paulusma']|['cs.DS', 'cs.DM']
2017-04-07T11:31:39Z|2017-03-15T11:57:53Z|http://arxiv.org/abs/1703.05097v1|http://arxiv.org/pdf/1703.05097v1|A cubic-time algorithm for computing the trinet distance between level-1   networks|cubic time algorithm comput trinet distanc level network|In evolutionary biology, phylogenetic networks are constructed to represent the evolution of species in which reticulate events are thought to have occurred, such as recombination and hybridization. It is therefore useful to have efficiently computable metrics with which to systematically compare such networks. Through developing an optimal algorithm to enumerate all trinets displayed by a level-1 network (a type of network that is slightly more general than an evolutionary tree), here we propose a cubic-time algorithm to compute the trinet distance between two level-1 networks. Employing simulations, we also present a comparison between the trinet metric and the so-called Robinson-Foulds phylogenetic network metric restricted to level-1 networks. The algorithms described in this paper have been implemented in JAVA and are freely available at https://www.uea.ac.uk/computing/TriLoNet.|evolutionari biolog phylogenet network construct repres evolut speci reticul event thought occur recombin hybrid therefor use effici comput metric systemat compar network develop optim algorithm enumer trinet display level network type network slight general evolutionari tree propos cubic time algorithm comput trinet distanc two level network employ simul also present comparison trinet metric call robinson fould phylogenet network metric restrict level network algorithm describ paper implement java freeli avail https www uea ac uk comput trilonet|['Vincent Moulton', 'James Oldman', 'Taoyang Wu']|['q-bio.PE', 'cs.DM', 'cs.DS']
2017-04-07T11:31:39Z|2017-03-15T06:21:59Z|http://arxiv.org/abs/1703.04954v1|http://arxiv.org/pdf/1703.04954v1|Faster STR-IC-LCS computation via RLE|faster str ic lcs comput via rle|The constrained LCS problem asks one to find a longest common subsequence of two input strings $A$ and $B$ with some constraints. The STR-IC-LCS problem is a variant of the constrained LCS problem, where the solution must include a given constraint string $C$ as a substring. Given two strings $A$ and $B$ of respective lengths $M$ and $N$, and a constraint string $C$ of length at most $\min\{M, N\}$, the best known algorithm for the STR-IC-LCS problem, proposed by Deorowicz~({\em Inf. Process. Lett.}, 11:423--426, 2012), runs in $O(MN)$ time. In this work, we present an $O(mN + nM)$-time solution to the STR-IC-LCS problem, where $m$ and $n$ denote the sizes of the run-length encodings of $A$ and $B$, respectively. Since $m \leq M$ and $n \leq N$ always hold, our algorithm is always as fast as Deorowicz's algorithm, and is faster when input strings are compressible via RLE.|constrain lcs problem ask one find longest common subsequ two input string constraint str ic lcs problem variant constrain lcs problem solut must includ given constraint string substr given two string respect length constraint string length min best known algorithm str ic lcs problem propos deorowicz em inf process lett run mn time work present mn nm time solut str ic lcs problem denot size run length encod respect sinc leq leq alway hold algorithm alway fast deorowicz algorithm faster input string compress via rle|['Keita Kuboi', 'Yuta Fujishige', 'Shunsuke Inenaga', 'Hideo Bannai', 'Masayuki Takeda']|['cs.DS']
2017-04-07T11:31:39Z|2017-03-14T23:06:33Z|http://arxiv.org/abs/1703.04814v1|http://arxiv.org/pdf/1703.04814v1|Near-Optimal Compression for the Planar Graph Metric|near optim compress planar graph metric|"The Planar Graph Metric Compression Problem is to compactly encode the distances among $k$ nodes in a planar graph of size $n$. Two na\""ive solutions are to store the graph using $O(n)$ bits, or to explicitly store the distance matrix with $O(k^2 \log{n})$ bits. The only lower bounds are from the seminal work of Gavoille, Peleg, Prennes, and Raz [SODA'01], who rule out compressions into a polynomially smaller number of bits, for {\em weighted} planar graphs, but leave a large gap for unweighted planar graphs. For example, when $k=\sqrt{n}$, the upper bound is $O(n)$ and their constructions imply an $\Omega(n^{3/4})$ lower bound. This gap is directly related to other major open questions in labelling schemes, dynamic algorithms, and compact routing.   Our main result is a new compression of the planar graph metric into $\tilde{O}(\min (k^2 , \sqrt{k\cdot n}))$ bits, which is optimal up to log factors. Our data structure breaks an $\Omega(k^2)$ lower bound of Krauthgamer, Nguyen, and Zondiner [SICOMP'14] for compression using minors, and the lower bound of Gavoille et al. for compression of weighted planar graphs. This is an unexpected and decisive proof that weights can make planar graphs inherently more complex. Moreover, we design a new {\em Subset Distance Oracle} for planar graphs with $\tilde O(\sqrt{k\cdot n})$ space, and $\tilde O(n^{3/4})$ query time.   Our work carries strong messages to related fields. In particular, the famous $O(n^{1/2})$ vs. $\Omega(n^{1/3})$ gap for distance labelling schemes in planar graphs {\em cannot} be resolved with the current lower bound techniques."|planar graph metric compress problem compact encod distanc among node planar graph size two na ive solut store graph use bit explicit store distanc matrix log bit onli lower bound semin work gavoill peleg prenn raz soda rule compress polynomi smaller number bit em weight planar graph leav larg gap unweight planar graph exampl sqrt upper bound construct impli omega lower bound gap direct relat major open question label scheme dynam algorithm compact rout main result new compress planar graph metric tild min sqrt cdot bit optim log factor data structur break omega lower bound krauthgam nguyen zondin sicomp compress use minor lower bound gavoill et al compress weight planar graph unexpect decis proof weight make planar graph inher complex moreov design new em subset distanc oracl planar graph tild sqrt cdot space tild queri time work carri strong messag relat field particular famous vs omega gap distanc label scheme planar graph em cannot resolv current lower bound techniqu|['Amir Abboud', 'Pawel Gawrychowski', 'Shay Mozes', 'Oren Weimann']|['cs.DS']
2017-04-07T11:31:39Z|2017-03-14T22:16:53Z|http://arxiv.org/abs/1703.04769v1|http://arxiv.org/pdf/1703.04769v1|The Stochastic Container Relocation Problem|stochast contain reloc problem|"The Container Relocation Problem (CRP) is concerned with finding a sequence of moves of containers that minimizes the number of relocations needed to retrieve all containers, while respecting a given order of retrieval. However, the assumption of knowing the full retrieval order of containers is particularly unrealistic in real operations. This paper studies the stochastic CRP (SCRP), which relaxes this assumption. A new multi-stage stochastic model, called the batch model, is introduced, motivated, and compared with an existing model (the online model). The two main contributions are an optimal algorithm called Pruning-Best-First-Search (PBFS) and a randomized approximate algorithm called PBFS-Approximate with a bounded average error. Both algorithms, applicable in the batch and online models, are based on a new family of lower bounds for which we show some theoretical properties. Moreover, we introduce two new heuristics outperforming the best existing heuristics. Algorithms, bounds and heuristics are tested in an extensive computational section. Finally, based on strong computational evidence, we conjecture the optimality of the ""Leveling"" heuristic in a special ""no information"" case, where at any retrieval stage, any of the remaining containers is equally likely to be retrieved next."|contain reloc problem crp concern find sequenc move contain minim number reloc need retriev contain respect given order retriev howev assumpt know full retriev order contain particular unrealist real oper paper studi stochast crp scrp relax assumpt new multi stage stochast model call batch model introduc motiv compar exist model onlin model two main contribut optim algorithm call prune best first search pbfs random approxim algorithm call pbfs approxim bound averag error algorithm applic batch onlin model base new famili lower bound show theoret properti moreov introduc two new heurist outperform best exist heurist algorithm bound heurist test extens comput section final base strong comput evid conjectur optim level heurist special inform case ani retriev stage ani remain contain equal like retriev next|['Virgile Galle', 'Setareh Borjian Boroujeni', 'Vahideh H. Manshadi', 'Cynthia Barnhart', 'Patrick Jaillet']|['cs.DS']
2017-04-07T11:31:39Z|2017-03-14T18:49:57Z|http://arxiv.org/abs/1703.04664v1|http://arxiv.org/pdf/1703.04664v1|Optimal Densification for Fast and Accurate Minwise Hashing|optim densif fast accur minwis hash|Minwise hashing is a fundamental and one of the most successful hashing algorithm in the literature. Recent advances based on the idea of densification~\cite{Proc:OneHashLSH_ICML14,Proc:Shrivastava_UAI14} have shown that it is possible to compute $k$ minwise hashes, of a vector with $d$ nonzeros, in mere $(d + k)$ computations, a significant improvement over the classical $O(dk)$. These advances have led to an algorithmic improvement in the query complexity of traditional indexing algorithms based on minwise hashing. Unfortunately, the variance of the current densification techniques is unnecessarily high, which leads to significantly poor accuracy compared to vanilla minwise hashing, especially when the data is sparse. In this paper, we provide a novel densification scheme which relies on carefully tailored 2-universal hashes. We show that the proposed scheme is variance-optimal, and without losing the runtime efficiency, it is significantly more accurate than existing densification techniques. As a result, we obtain a significantly efficient hashing scheme which has the same variance and collision probability as minwise hashing. Experimental evaluations on real sparse and high-dimensional datasets validate our claims. We believe that given the significant advantages, our method will replace minwise hashing implementations in practice.|minwis hash fundament one success hash algorithm literatur recent advanc base idea densif cite proc onehashlsh icml proc shrivastava uai shown possibl comput minwis hash vector nonzero mere comput signific improv classic dk advanc led algorithm improv queri complex tradit index algorithm base minwis hash unfortun varianc current densif techniqu unnecessarili high lead signific poor accuraci compar vanilla minwis hash especi data spars paper provid novel densif scheme reli care tailor univers hash show propos scheme varianc optim without lose runtim effici signific accur exist densif techniqu result obtain signific effici hash scheme varianc collis probabl minwis hash experiment evalu real spars high dimension dataset valid claim believ given signific advantag method replac minwis hash implement practic|['Anshumali Shrivastava']|['cs.DS', 'cs.LG']
2017-04-07T11:31:39Z|2017-03-13T16:18:01Z|http://arxiv.org/abs/1703.04466v1|http://arxiv.org/pdf/1703.04466v1|Bicriteria Rectilinear Shortest Paths among Rectilinear Obstacles in the   Plane|bicriteria rectilinear shortest path among rectilinear obstacl plane|Given a rectilinear domain $\mathcal{P}$ of $h$ pairwise-disjoint rectilinear obstacles with a total of $n$ vertices in the plane, we study the problem of computing bicriteria rectilinear shortest paths between two points $s$ and $t$ in $\mathcal{P}$. Three types of bicriteria rectilinear paths are considered: minimum-link shortest paths, shortest minimum-link paths, and minimum-cost paths where the cost of a path is a non-decreasing function of both the number of edges and the length of the path. The one-point and two-point path queries are also considered. Algorithms for these problems have been given previously. Our contributions are threefold. First, we find a critical error in all previous algorithms. Second, we correct the error in a not-so-trivial way. Third, we further improve the algorithms so that they are even faster than the previous (incorrect) algorithms when $h$ is relatively small. For example, for the minimum-link shortest paths, we obtain the following results. Our algorithm computes a minimum-link shortest $s$-$t$ path in $O(n+h\log^{3/2} h)$ time. For the one-point queries, we build a data structure of size $O(n+ h\log h)$ in $O(n+h\log^{3/2} h)$ time for a source point $s$, such that given any query point $t$, a minimum-link shortest $s$-$t$ path can be determined in $O(\log n)$ time. For the two-point queries, with $O(n+h^2\log^2 h)$ time and space preprocessing, a minimum-link shortest $s$-$t$ path can be determined in $O(\log n+\log^2 h)$ time for any two query points $s$ and $t$; alternatively, with $O(n+h^2\cdot \log^{2} h \cdot 4^{\sqrt{\log h}})$ time and $O(n+h^2\cdot \log h \cdot 4^{\sqrt{\log h}})$ space preprocessing, we can answer each two-point query in $O(\log n)$ time.|given rectilinear domain mathcal pairwis disjoint rectilinear obstacl total vertic plane studi problem comput bicriteria rectilinear shortest path two point mathcal three type bicriteria rectilinear path consid minimum link shortest path shortest minimum link path minimum cost path cost path non decreas function number edg length path one point two point path queri also consid algorithm problem given previous contribut threefold first find critic error previous algorithm second correct error trivial way third improv algorithm even faster previous incorrect algorithm relat small exampl minimum link shortest path obtain follow result algorithm comput minimum link shortest path log time one point queri build data structur size log log time sourc point given ani queri point minimum link shortest path determin log time two point queri log time space preprocess minimum link shortest path determin log log time ani two queri point altern cdot log cdot sqrt log time cdot log cdot sqrt log space preprocess answer two point queri log time|['Haitao Wang']|['cs.CG', 'cs.DS']
