2017-03-28T14:01:45Z|2017-03-27T17:50:53Z|http://arxiv.org/abs/1703.09207v1|http://arxiv.org/pdf/1703.09207v1|Fairness in Criminal Justice Risk Assessments: The State of the Art|fair crimin justic risk assess state art|Objectives: Discussions of fairness in criminal justice risk assessments typically lack conceptual precision. Rhetoric too often substitutes for careful analysis. In this paper, we seek to clarify the tradeoffs between different kinds of fairness and between fairness and accuracy.   Methods: We draw on the existing literatures in criminology, computer science and statistics to provide an integrated examination of fairness and accuracy in criminal justice risk assessments. We also provide an empirical illustration using data from arraignments.   Results: We show that there are at least six kinds of fairness, some of which are incompatible with one another and with accuracy.   Conclusions: Except in trivial cases, it is impossible to maximize accuracy and fairness at the same time, and impossible simultaneously to satisfy all kinds of fairness. In practice, a major complication is different base rates across different legally protected groups. There is a need to consider challenging tradeoffs.|object discuss fair crimin justic risk assess typic lack conceptu precis rhetor often substitut care analysi paper seek clarifi tradeoff differ kind fair fair accuraci method draw exist literatur criminolog comput scienc statist provid integr examin fair accuraci crimin justic risk assess also provid empir illustr use data arraign result show least six kind fair incompat one anoth accuraci conclus except trivial case imposs maxim accuraci fair time imposs simultan satisfi kind fair practic major complic differ base rate across differ legal protect group need consid challeng tradeoff|['Richard Berk', 'Hoda Heidari', 'Shahin Jabbari', 'Michael Kearns', 'Aaron Roth']|['stat.ML']
2017-03-28T14:01:45Z|2017-03-27T17:45:07Z|http://arxiv.org/abs/1703.09202v1|http://arxiv.org/pdf/1703.09202v1|Biologically inspired protection of deep networks from adversarial   attacks|biolog inspir protect deep network adversari attack|Inspired by biophysical principles underlying nonlinear dendritic computation in neural circuits, we develop a scheme to train deep neural networks to make them robust to adversarial attacks. Our scheme generates highly nonlinear, saturated neural networks that achieve state of the art performance on gradient based adversarial examples on MNIST, despite never being exposed to adversarially chosen examples during training. Moreover, these networks exhibit unprecedented robustness to targeted, iterative schemes for generating adversarial examples, including second-order methods. We further identify principles governing how these networks achieve their robustness, drawing on methods from information geometry. We find these networks progressively create highly flat and compressed internal representations that are sensitive to very few input dimensions, while still solving the task. Moreover, they employ highly kurtotic weight distributions, also found in the brain, and we demonstrate how such kurtosis can protect even linear classifiers from adversarial attack.|inspir biophys principl nonlinear dendrit comput neural circuit develop scheme train deep neural network make robust adversari attack scheme generat high nonlinear satur neural network achiev state art perform gradient base adversari exampl mnist despit never expos adversari chosen exampl dure train moreov network exhibit unpreced robust target iter scheme generat adversari exampl includ second order method identifi principl govern network achiev robust draw method inform geometri find network progress creat high flat compress intern represent sensit veri input dimens still solv task moreov employ high kurtot weight distribut also found brain demonstr kurtosi protect even linear classifi adversari attack|['Aran Nayebi', 'Surya Ganguli']|['stat.ML', 'cs.LG', 'q-bio.NC']
2017-03-28T14:01:45Z|2017-03-27T17:25:02Z|http://arxiv.org/abs/1703.09194v1|http://arxiv.org/pdf/1703.09194v1|Sticking the Landing: An Asymptotically Zero-Variance Gradient Estimator   for Variational Inference|stick land asymptot zero varianc gradient estim variat infer|We propose a simple and general variant of the standard reparameterized gradient estimator for the variational evidence lower bound. Specifically, we remove a part of the total derivative with respect to the variational parameters that corresponds to the score function. Removing this term produces an unbiased gradient estimator whose variance approaches zero as the approximate posterior approaches the exact posterior. We analyze the behavior of this gradient estimator theoretically and empirically, and generalize it to more complex variational distributions such as mixtures and importance-weighted posteriors.|propos simpl general variant standard reparameter gradient estim variat evid lower bound specif remov part total deriv respect variat paramet correspond score function remov term produc unbias gradient estim whose varianc approach zero approxim posterior approach exact posterior analyz behavior gradient estim theoret empir general complex variat distribut mixtur import weight posterior|['Geoffrey Roeder', 'Yuhuai Wu', 'David Duvenaud']|['stat.ML', 'cs.LG']
2017-03-28T14:01:45Z|2017-03-27T16:16:35Z|http://arxiv.org/abs/1703.09165v1|http://arxiv.org/pdf/1703.09165v1|PWLS-ULTRA: An Efficient Clustering and Learning-Based Approach for   Low-Dose 3D CT Image Reconstruction|pwls ultra effici cluster learn base approach low dose ct imag reconstruct|The development of computed tomography (CT) image reconstruction methods that significantly reduce patient radiation exposure while maintaining high image quality is an important area of research in low-dose CT (LDCT) imaging. We propose a new penalized weighted least squares (PWLS) reconstruction method that exploits regularization based on an efficient Union of Learned TRAnsforms (PWLS-ULTRA). The union of square transforms is pre-learned from numerous image patches extracted from a dataset of CT images or volumes. The proposed PWLS-based cost function is optimized by alternating between an image update step, and a sparse coding and clustering step. The CT image update step is accelerated by a relaxed linearized augmented Lagrangian method with ordered-subsets that reduces the number of forward and backward projections. Simulations with 2D and 3D axial CT scans of the XCAT phantom and 3D helical chest scans show that for low-dose levels, the proposed method significantly improves the quality of reconstructed images compared to PWLS reconstruction with a nonadaptive edge-preserving regularizer (PWLS-EP). PWLS with regularization based on a union of learned transforms leads to better image reconstructions than using a single learned square transform or a learned overcomplete synthesis dictionary. We also incorporate patch-based weights in PWLS-ULTRA that enhance image quality and help improve image resolution uniformity.|develop comput tomographi ct imag reconstruct method signific reduc patient radiat exposur maintain high imag qualiti import area research low dose ct ldct imag propos new penal weight least squar pwls reconstruct method exploit regular base effici union learn transform pwls ultra union squar transform pre learn numer imag patch extract dataset ct imag volum propos pwls base cost function optim altern imag updat step spars code cluster step ct imag updat step acceler relax linear augment lagrangian method order subset reduc number forward backward project simul axial ct scan xcat phantom helic chest scan show low dose level propos method signific improv qualiti reconstruct imag compar pwls reconstruct nonadapt edg preserv regular pwls ep pwls regular base union learn transform lead better imag reconstruct use singl learn squar transform learn overcomplet synthesi dictionari also incorpor patch base weight pwls ultra enhanc imag qualiti help improv imag resolut uniform|['Xuehang Zheng', 'Saiprasad Ravishankar', 'Yong Long', 'Jeffrey A. Fessler']|['stat.ML']
2017-03-28T14:01:45Z|2017-03-27T14:38:15Z|http://arxiv.org/abs/1703.09112v1|http://arxiv.org/pdf/1703.09112v1|Sparse Multi-Output Gaussian Processes for Medical Time Series   Prediction|spars multi output gaussian process medic time seri predict|In real-time monitoring of hospital patients, high-quality inference of patients' health status using all information available from clinical covariates and lab tests are essential to enable successful medical interventions and improve patient outcomes. In this work, we develop and explore a Bayesian nonparametric model based on Gaussian process (GP) regression for hospital patient monitoring. Our method, MedGP, incorporates 24 clinical and lab covariates and supports a rich reference data set from which the relationships between these observed covariates may be inferred and exploited for high-quality inference of patient state over time. To do this, we develop a highly structured sparse GP kernel to enable tractable computation over tens of thousands of time points while estimating correlations among clinical covariates, patients, and periodicity in high-dimensional time series measurements of physiological signals. We apply MedGP to data from hundreds of thousands of patients treated at the Hospital of the University of Pennsylvania. MedGP has a number of benefits over current methods, including (i) not requiring an alignment of the time series data, (ii) quantifying confidence intervals in the predictions, (iii) exploiting a vast and rich database of patients, and (iv) providing interpretable relationships among clinical covariates. We evaluate and compare results from MedGP on the task of online state prediction for three different patient subgroups.|real time monitor hospit patient high qualiti infer patient health status use inform avail clinic covari lab test essenti enabl success medic intervent improv patient outcom work develop explor bayesian nonparametr model base gaussian process gp regress hospit patient monitor method medgp incorpor clinic lab covari support rich refer data set relationship observ covari may infer exploit high qualiti infer patient state time develop high structur spars gp kernel enabl tractabl comput ten thousand time point estim correl among clinic covari patient period high dimension time seri measur physiolog signal appli medgp data hundr thousand patient treat hospit univers pennsylvania medgp number benefit current method includ requir align time seri data ii quantifi confid interv predict iii exploit vast rich databas patient iv provid interpret relationship among clinic covari evalu compar result medgp task onlin state predict three differ patient subgroup|['Li-Fang Cheng', 'Gregory Darnell', 'Corey Chivers', 'Michael E Draugelis', 'Kai Li', 'Barbara E Engelhardt']|['stat.ML']
2017-03-28T14:01:45Z|2017-03-27T10:03:27Z|http://arxiv.org/abs/1703.08991v1|http://arxiv.org/pdf/1703.08991v1|Multilabel Classification with R Package mlr|multilabel classif packag mlr|We implemented several multilabel classification algorithms in the machine learning package mlr. The implemented methods are binary relevance, classifier chains, nested stacking, dependent binary relevance and stacking, which can be used with any base learner that is accessible in mlr. Moreover, there is access to the multilabel classification versions of randomForestSRC and rFerns. All these methods can be easily compared by different implemented multilabel performance measures and resampling methods in the standardized mlr framework. In a benchmark experiment with several multilabel datasets, the performance of the different methods is evaluated.|implement sever multilabel classif algorithm machin learn packag mlr implement method binari relev classifi chain nest stack depend binari relev stack use ani base learner access mlr moreov access multilabel classif version randomforestsrc rfern method easili compar differ implement multilabel perform measur resampl method standard mlr framework benchmark experi sever multilabel dataset perform differ method evalu|['Philipp Probst', 'Quay Au', 'Giuseppe Casalicchio', 'Clemens Stachl', 'Bernd Bischl']|['stat.ML']
2017-03-28T14:01:45Z|2017-03-27T08:45:57Z|http://arxiv.org/abs/1703.08972v1|http://arxiv.org/pdf/1703.08972v1|Thompson Sampling for Linear-Quadratic Control Problems|thompson sampl linear quadrat control problem|We consider the exploration-exploitation tradeoff in linear quadratic (LQ) control problems, where the state dynamics is linear and the cost function is quadratic in states and controls. We analyze the regret of Thompson sampling (TS) (a.k.a. posterior-sampling for reinforcement learning) in the frequentist setting, i.e., when the parameters characterizing the LQ dynamics are fixed. Despite the empirical and theoretical success in a wide range of problems from multi-armed bandit to linear bandit, we show that when studying the frequentist regret TS in control problems, we need to trade-off the frequency of sampling optimistic parameters and the frequency of switches in the control policy. This results in an overall regret of $O(T^{2/3})$, which is significantly worse than the regret $O(\sqrt{T})$ achieved by the optimism-in-face-of-uncertainty algorithm in LQ control problems.|consid explor exploit tradeoff linear quadrat lq control problem state dynam linear cost function quadrat state control analyz regret thompson sampl ts posterior sampl reinforc learn frequentist set paramet character lq dynam fix despit empir theoret success wide rang problem multi arm bandit linear bandit show studi frequentist regret ts control problem need trade frequenc sampl optimist paramet frequenc switch control polici result overal regret signific wors regret sqrt achiev optim face uncertainti algorithm lq control problem|['Marc Abeille', 'Alessandro Lazaric']|['stat.ML']
2017-03-28T14:01:45Z|2017-03-27T05:41:03Z|http://arxiv.org/abs/1703.08937v1|http://arxiv.org/pdf/1703.08937v1|A Scale Free Algorithm for Stochastic Bandits with Bounded Kurtosis|scale free algorithm stochast bandit bound kurtosi|Existing strategies for finite-armed stochastic bandits mostly depend on a parameter of scale that must be known in advance. Sometimes this is in the form of a bound on the payoffs, or the knowledge of a variance or subgaussian parameter. The notable exceptions are the analysis of Gaussian bandits with unknown mean and variance by Cowan and Katehakis [2015] and of uniform distributions with unknown support [Cowan and Katehakis, 2015]. The results derived in these specialised cases are generalised here to the non-parametric setup, where the learner knows only a bound on the kurtosis of the noise, which is a scale free measure of the extremity of outliers.|exist strategi finit arm stochast bandit depend paramet scale must known advanc sometim form bound payoff knowledg varianc subgaussian paramet notabl except analysi gaussian bandit unknown mean varianc cowan katehaki uniform distribut unknown support cowan katehaki result deriv specialis case generalis non parametr setup learner know onli bound kurtosi nois scale free measur extrem outlier|['Tor Lattimore']|['stat.ML']
2017-03-28T14:01:45Z|2017-03-26T16:01:28Z|http://arxiv.org/abs/1703.08831v1|http://arxiv.org/pdf/1703.08831v1|Token-based Function Computation with Memory|token base function comput memori|"In distributed function computation, each node has an initial value and the goal is to compute a function of these values in a distributed manner. In this paper, we propose a novel token-based approach to compute a wide class of target functions to which we refer as ""Token-based function Computation with Memory"" (TCM) algorithm. In this approach, node values are attached to tokens and travel across the network. Each pair of travelling tokens would coalesce when they meet, forming a token with a new value as a function of the original token values. In contrast to the Coalescing Random Walk (CRW) algorithm, where token movement is governed by random walk, meeting of tokens in our scheme is accelerated by adopting a novel chasing mechanism. We proved that, compared to the CRW algorithm, the TCM algorithm results in a reduction of time complexity by a factor of at least $\sqrt{n/\log(n)}$ in Erd\""os-Renyi and complete graphs, and by a factor of $\log(n)/\log(\log(n))$ in torus networks. Simulation results show that there is at least a constant factor improvement in the message complexity of TCM algorithm in all considered topologies. Robustness of the CRW and TCM algorithms in the presence of node failure is analyzed. We show that their robustness can be improved by running multiple instances of the algorithms in parallel."|distribut function comput node initi valu goal comput function valu distribut manner paper propos novel token base approach comput wide class target function refer token base function comput memori tcm algorithm approach node valu attach token travel across network pair travel token would coalesc meet form token new valu function origin token valu contrast coalesc random walk crw algorithm token movement govern random walk meet token scheme acceler adopt novel chase mechan prove compar crw algorithm tcm algorithm result reduct time complex factor least sqrt log erd os renyi complet graph factor log log log torus network simul result show least constant factor improv messag complex tcm algorithm consid topolog robust crw tcm algorithm presenc node failur analyz show robust improv run multipl instanc algorithm parallel|['Saber Salehkaleybar', 'S. Jamaloddin Golestani']|['cs.DC', 'stat.ML']
2017-03-28T14:01:45Z|2017-03-26T13:29:25Z|http://arxiv.org/abs/1703.08816v1|http://arxiv.org/pdf/1703.08816v1|Uncertainty Quantification in the Classification of High Dimensional   Data|uncertainti quantif classif high dimension data|Classification of high dimensional data finds wide-ranging applications. In many of these applications equipping the resulting classification with a measure of uncertainty may be as important as the classification itself. In this paper we introduce, develop algorithms for, and investigate the properties of, a variety of Bayesian models for the task of binary classification; via the posterior distribution on the classification labels, these methods automatically give measures of uncertainty. The methods are all based around the graph formulation of semi-supervised learning.   We provide a unified framework which brings together a variety of methods which have been introduced in different communities within the mathematical sciences. We study probit classification, generalize the level-set method for Bayesian inverse problems to the classification setting, and generalize the Ginzburg-Landau optimization-based classifier to a Bayesian setting; we also show that the probit and level set approaches are natural relaxations of the harmonic function approach.   We introduce efficient numerical methods, suited to large data-sets, for both MCMC-based sampling as well as gradient-based MAP estimation. Through numerical experiments we study classification accuracy and uncertainty quantification for our models; these experiments showcase a suite of datasets commonly used to evaluate graph-based semi-supervised learning algorithms.|classif high dimension data find wide rang applic mani applic equip result classif measur uncertainti may import classif paper introduc develop algorithm investig properti varieti bayesian model task binari classif via posterior distribut classif label method automat give measur uncertainti method base around graph formul semi supervis learn provid unifi framework bring togeth varieti method introduc differ communiti within mathemat scienc studi probit classif general level set method bayesian invers problem classif set general ginzburg landau optim base classifi bayesian set also show probit level set approach natur relax harmon function approach introduc effici numer method suit larg data set mcmc base sampl well gradient base map estim numer experi studi classif accuraci uncertainti quantif model experi showcas suit dataset common use evalu graph base semi supervis learn algorithm|['Andrea L. Bertozzi', 'Xiyang Luo', 'Andrew M. Stuart', 'Konstantinos C. Zygalakis']|['cs.LG', 'stat.ML']
2017-03-28T14:01:49Z|2017-03-26T05:53:39Z|http://arxiv.org/abs/1703.08772v1|http://arxiv.org/pdf/1703.08772v1|Multivariate Regression with Gross Errors on Manifold-valued Data|multivari regress gross error manifold valu data|We consider the topic of multivariate regression on manifold-valued output, that is, for a multivariate observation, its output response lies on a manifold. Moreover, we propose a new regression model to deal with the presence of grossly corrupted manifold-valued responses, a bottleneck issue commonly encountered in practical scenarios. Our model first takes a correction step on the grossly corrupted responses via geodesic curves on the manifold, and then performs multivariate linear regression on the corrected data. This results in a nonconvex and nonsmooth optimization problem on manifolds. To this end, we propose a dedicated approach named PALMR, by utilizing and extending the proximal alternating linearized minimization techniques. Theoretically, we investigate its convergence property, where it is shown to converge to a critical point under mild conditions. Empirically, we test our model on both synthetic and real diffusion tensor imaging data, and show that our model outperforms other multivariate regression models when manifold-valued responses contain gross errors, and is effective in identifying gross errors.|consid topic multivari regress manifold valu output multivari observ output respons lie manifold moreov propos new regress model deal presenc grossli corrupt manifold valu respons bottleneck issu common encount practic scenario model first take correct step grossli corrupt respons via geodes curv manifold perform multivari linear regress correct data result nonconvex nonsmooth optim problem manifold end propos dedic approach name palmr util extend proxim altern linear minim techniqu theoret investig converg properti shown converg critic point mild condit empir test model synthet real diffus tensor imag data show model outperform multivari regress model manifold valu respons contain gross error effect identifi gross error|['Xiaowei Zhang', 'Xudong Shi', 'Yu Sun', 'Li Cheng']|['stat.ML', 'cs.CV', 'math.OC']
2017-03-28T14:01:49Z|2017-03-25T20:06:10Z|http://arxiv.org/abs/1703.08737v1|http://arxiv.org/pdf/1703.08737v1|Learning to Predict: A Fast Re-constructive Method to Generate   Multimodal Embeddings|learn predict fast construct method generat multimod embed|"Integrating visual and linguistic information into a single multimodal representation is an unsolved problem with wide-reaching applications to both natural language processing and computer vision. In this paper, we present a simple method to build multimodal representations by learning a language-to-vision mapping and using its output to build multimodal embeddings. In this sense, our method provides a cognitively plausible way of building representations, consistent with the inherently re-constructive and associative nature of human memory. Using seven benchmark concept similarity tests we show that the mapped vectors not only implicitly encode multimodal information, but also outperform strong unimodal baselines and state-of-the-art multimodal methods, thus exhibiting more ""human-like"" judgments---particularly in zero-shot settings."|integr visual linguist inform singl multimod represent unsolv problem wide reach applic natur languag process comput vision paper present simpl method build multimod represent learn languag vision map use output build multimod embed sens method provid cognit plausibl way build represent consist inher construct associ natur human memori use seven benchmark concept similar test show map vector onli implicit encod multimod inform also outperform strong unimod baselin state art multimod method thus exhibit human like judgment particular zero shot set|['Guillem Collell', 'Teddy Zhang', 'Marie-Francine Moens']|['stat.ML']
2017-03-28T14:01:49Z|2017-03-25T18:45:55Z|http://arxiv.org/abs/1703.08729v1|http://arxiv.org/pdf/1703.08729v1|Solving SDPs for synchronization and MaxCut problems via the   Grothendieck inequality|solv sdps synchron maxcut problem via grothendieck inequ|A number of statistical estimation problems can be addressed by semidefinite programs (SDP). While SDPs are solvable in polynomial time using interior point methods, in practice generic SDP solvers do not scale well to high-dimensional problems. In order to cope with this problem, Burer and Monteiro proposed a non-convex rank-constrained formulation, which has good performance in practice but is still poorly understood theoretically.   In this paper we study the rank-constrained version of SDPs arising in MaxCut and in synchronization problems. We establish a Grothendieck-type inequality that proves that all the local maxima and dangerous saddle points are within a small multiplicative gap from the global maximum. We use this structural information to prove that SDPs can be solved within a known accuracy, by applying the Riemannian trust-region method to this non-convex problem, while constraining the rank to be of order one. For the MaxCut problem, our inequality implies that any local maximizer of the rank-constrained SDP provides a $ (1 - 1/(k-1)) \times 0.878$ approximation of the MaxCut, when the rank is fixed to $k$.   We then apply our results to data matrices generated according to the Gaussian ${\mathbb Z}_2$ synchronization problem, and the two-groups stochastic block model with large bounded degree. We prove that the error achieved by local maximizers undergoes a phase transition at the same threshold as for information-theoretically optimal methods.|number statist estim problem address semidefinit program sdp sdps solvabl polynomi time use interior point method practic generic sdp solver scale well high dimension problem order cope problem burer monteiro propos non convex rank constrain formul good perform practic still poor understood theoret paper studi rank constrain version sdps aris maxcut synchron problem establish grothendieck type inequ prove local maxima danger saddl point within small multipl gap global maximum use structur inform prove sdps solv within known accuraci appli riemannian trust region method non convex problem constrain rank order one maxcut problem inequ impli ani local maxim rank constrain sdp provid time approxim maxcut rank fix appli result data matric generat accord gaussian mathbb synchron problem two group stochast block model larg bound degre prove error achiev local maxim undergo phase transit threshold inform theoret optim method|['Song Mei', 'Theodor Misiakiewicz', 'Andrea Montanari', 'Roberto I. Oliveira']|['math.OC', 'stat.ML']
2017-03-28T14:01:49Z|2017-03-25T16:49:03Z|http://arxiv.org/abs/1703.08710v1|http://arxiv.org/pdf/1703.08710v1|Count-ception: Counting by Fully Convolutional Redundant Counting|count ception count fulli convolut redund count|Counting objects in digital images is a process that should be replaced by machines. This tedious task is time consuming and prone to errors due to fatigue of human annotators. The goal is to have a system that takes as input an image and returns a count of the objects inside and justification for the prediction in the form of object localization. We repose a problem, originally posed by Lempitsky and Zisserman, to instead predict a count map which contains redundant counts based on the receptive field of a smaller regression network. The regression network predicts a count of the objects that exist inside this frame. By processing the image in a fully convolutional way each pixel is going to be accounted for some number of times, the number of windows which include it, which is the size of each window, (i.e., 32x32 = 1024). To recover the true count take the average over the redundant predictions. Our contribution is redundant counting instead of predicting a density map in order to average over errors. We also propose a novel deep neural network architecture adapted from the Inception family of networks called the Count-ception network. Together our approach results in a 20% gain over the state of the art method by Xie, Noble, and Zisserman in 2016.|count object digit imag process replac machin tedious task time consum prone error due fatigu human annot goal system take input imag return count object insid justif predict form object local repos problem origin pose lempitski zisserman instead predict count map contain redund count base recept field smaller regress network regress network predict count object exist insid frame process imag fulli convolut way pixel go account number time number window includ size window recov true count take averag redund predict contribut redund count instead predict densiti map order averag error also propos novel deep neural network architectur adapt incept famili network call count ception network togeth approach result gain state art method xie nobl zisserman|['Joseph Paul Cohen', 'Henry Z. Lo', 'Yoshua Bengio']|['cs.CV', 'cs.LG', 'stat.ML']
2017-03-28T14:01:49Z|2017-03-25T15:37:09Z|http://arxiv.org/abs/1703.08705v1|http://arxiv.org/pdf/1703.08705v1|Comparing Rule-Based and Deep Learning Models for Patient Phenotyping|compar rule base deep learn model patient phenotyp|Objective: We investigate whether deep learning techniques for natural language processing (NLP) can be used efficiently for patient phenotyping. Patient phenotyping is a classification task for determining whether a patient has a medical condition, and is a crucial part of secondary analysis of healthcare data. We assess the performance of deep learning algorithms and compare them with classical NLP approaches.   Materials and Methods: We compare convolutional neural networks (CNNs), n-gram models, and approaches based on cTAKES that extract pre-defined medical concepts from clinical notes and use them to predict patient phenotypes. The performance is tested on 10 different phenotyping tasks using 1,610 discharge summaries extracted from the MIMIC-III database.   Results: CNNs outperform other phenotyping algorithms in all 10 tasks. The average F1-score of our model is 76 (PPV of 83, and sensitivity of 71) with our model having an F1-score up to 37 points higher than alternative approaches. We additionally assess the interpretability of our model by presenting a method that extracts the most salient phrases for a particular prediction.   Conclusion: We show that NLP methods based on deep learning improve the performance of patient phenotyping. Our CNN-based algorithm automatically learns the phrases associated with each patient phenotype. As such, it reduces the annotation complexity for clinical domain experts, who are normally required to develop task-specific annotation rules and identify relevant phrases. Our method performs well in terms of both performance and interpretability, which indicates that deep learning is an effective approach to patient phenotyping based on clinicians' notes.|object investig whether deep learn techniqu natur languag process nlp use effici patient phenotyp patient phenotyp classif task determin whether patient medic condit crucial part secondari analysi healthcar data assess perform deep learn algorithm compar classic nlp approach materi method compar convolut neural network cnns gram model approach base ctake extract pre defin medic concept clinic note use predict patient phenotyp perform test differ phenotyp task use discharg summari extract mimic iii databas result cnns outperform phenotyp algorithm task averag score model ppv sensit model score point higher altern approach addit assess interpret model present method extract salient phrase particular predict conclus show nlp method base deep learn improv perform patient phenotyp cnn base algorithm automat learn phrase associ patient phenotyp reduc annot complex clinic domain expert normal requir develop task specif annot rule identifi relev phrase method perform well term perform interpret indic deep learn effect approach patient phenotyp base clinician note|['Sebastian Gehrmann', 'Franck Dernoncourt', 'Yeran Li', 'Eric T. Carlson', 'Joy T. Wu', 'Jonathan Welt', 'John Foote Jr.', 'Edward T. Moseley', 'David W. Grant', 'Patrick D. Tyler', 'Leo Anthony Celi']|['cs.CL', 'cs.AI', 'cs.NE', 'stat.ML']
2017-03-28T14:01:49Z|2017-03-24T22:54:17Z|http://arxiv.org/abs/1703.08619v1|http://arxiv.org/pdf/1703.08619v1|Binarsity: a penalization for one-hot encoded features|binars penal one hot encod featur|This paper deals with the problem of large-scale linear supervised learning in settings where a large number of continuous features are available. We propose to combine the well-known trick of one-hot encoding of continuous features with a new penalization called binarsity. In each group of binary features coming from the one-hot encoding of a single raw continuous feature, this penalization uses total-variation regularization together with an extra linear constraint to avoid collinearity within groups. Non-asymptotic oracle inequalities for generalized linear models are proposed, and numerical experiments illustrate the good performances of our approach on several datasets. It is also noteworthy that our method has a numerical complexity comparable to standard $\ell_1$ penalization.|paper deal problem larg scale linear supervis learn set larg number continu featur avail propos combin well known trick one hot encod continu featur new penal call binars group binari featur come one hot encod singl raw continu featur penal use total variat regular togeth extra linear constraint avoid collinear within group non asymptot oracl inequ general linear model propos numer experi illustr good perform approach sever dataset also noteworthi method numer complex compar standard ell penal|['Mokhtar Z. Alaya', 'Simon Bussy', 'Stéphane Gaïffas', 'Agathe Guilloux']|['stat.ML']
2017-03-28T14:01:49Z|2017-03-24T19:45:24Z|http://arxiv.org/abs/1703.08581v1|http://arxiv.org/pdf/1703.08581v1|Sequence-to-Sequence Models Can Directly Transcribe Foreign Speech|sequenc sequenc model direct transcrib foreign speech|We present a recurrent encoder-decoder deep neural network architecture that directly translates speech in one language into text in another. The model does not explicitly transcribe the speech into text in the source language, nor does it require supervision from the ground truth source language transcription during training. We apply a slightly modified sequence-to-sequence with attention architecture that has previously been used for speech recognition and show that it can be repurposed for this more complex task, illustrating the power of attention-based models. A single model trained end-to-end obtains state-of-the-art performance on the Fisher Callhome Spanish-English speech translation task, outperforming a cascade of independently trained sequence-to-sequence speech recognition and machine translation models by 1.8 BLEU points on the Fisher test set. In addition, we find that making use of the training data in both languages by multi-task training sequence-to-sequence speech translation and recognition models with a shared encoder network can improve performance by a further 1.4 BLEU points.|present recurr encod decod deep neural network architectur direct translat speech one languag text anoth model doe explicit transcrib speech text sourc languag doe requir supervis ground truth sourc languag transcript dure train appli slight modifi sequenc sequenc attent architectur previous use speech recognit show repurpos complex task illustr power attent base model singl model train end end obtain state art perform fisher callhom spanish english speech translat task outperform cascad independ train sequenc sequenc speech recognit machin translat model bleu point fisher test set addit find make use train data languag multi task train sequenc sequenc speech translat recognit model share encod network improv perform bleu point|['Ron J. Weiss', 'Jan Chorowski', 'Navdeep Jaitly', 'Yonghui Wu', 'Zhifeng Chen']|['cs.CL', 'cs.LG', 'stat.ML']
2017-03-28T14:01:49Z|2017-03-24T17:17:45Z|http://arxiv.org/abs/1703.08520v1|http://arxiv.org/pdf/1703.08520v1|Rejection-free Ensemble MCMC with applications to Factorial Hidden   Markov Models|reject free ensembl mcmc applic factori hidden markov model|"Bayesian inference for complex models is challenging due to the need to explore high-dimensional spaces and multimodality and standard Monte Carlo samplers can have difficulties effectively exploring the posterior. We introduce a general purpose rejection-free ensemble Markov Chain Monte Carlo (MCMC) technique to improve on existing poorly mixing samplers. This is achieved by combining parallel tempering and an auxiliary variable move to exchange information between the chains. We demonstrate this ensemble MCMC scheme on Bayesian inference in Factorial Hidden Markov Models. This high-dimensional inference problem is difficult due to the exponentially sized latent variable space. Existing sampling approaches mix slowly and can get trapped in local modes. We show that the performance of these samplers is improved by our rejection-free ensemble technique and that the method is attractive and ""easy-to-use"" since no parameter tuning is required."|bayesian infer complex model challeng due need explor high dimension space multimod standard mont carlo sampler difficulti effect explor posterior introduc general purpos reject free ensembl markov chain mont carlo mcmc techniqu improv exist poor mix sampler achiev combin parallel temper auxiliari variabl move exchang inform chain demonstr ensembl mcmc scheme bayesian infer factori hidden markov model high dimension infer problem difficult due exponenti size latent variabl space exist sampl approach mix slowli get trap local mode show perform sampler improv reject free ensembl techniqu method attract easi use sinc paramet tune requir|['Kaspar Märtens', 'Michalis K Titsias', 'Christopher Yau']|['stat.CO', 'stat.ME', 'stat.ML']
2017-03-28T14:01:49Z|2017-03-24T14:49:58Z|http://arxiv.org/abs/1703.08544v1|http://arxiv.org/pdf/1703.08544v1|D.TRUMP: Data-mining Textual Responses to Uncover Misconception Patterns|trump data mine textual respons uncov misconcept pattern|An important, yet largely unstudied, problem in student data analysis is to detect misconceptions from students' responses to open-response questions. Misconception detection enables instructors to deliver more targeted feedback on the misconceptions exhibited by many students in their class, thus improving the quality of instruction. In this paper, we propose D.TRUMP, a new natural language processing-based framework to detect the common misconceptions among students' textual responses to short-answer questions. We propose a probabilistic model for students' textual responses involving misconceptions and experimentally validate it on a real-world student-response dataset. Experimental results show that D.TRUMP excels at classifying whether a response exhibits one or more misconceptions. More importantly, it can also automatically detect the common misconceptions exhibited across responses from multiple students to multiple questions; this property is especially important at large scale, since instructors will no longer need to manually specify all possible misconceptions that students might exhibit.|import yet larg unstudi problem student data analysi detect misconcept student respons open respons question misconcept detect enabl instructor deliv target feedback misconcept exhibit mani student class thus improv qualiti instruct paper propos trump new natur languag process base framework detect common misconcept among student textual respons short answer question propos probabilist model student textual respons involv misconcept experiment valid real world student respons dataset experiment result show trump excel classifi whether respons exhibit one misconcept import also automat detect common misconcept exhibit across respons multipl student multipl question properti especi import larg scale sinc instructor longer need manual specifi possibl misconcept student might exhibit|['Joshua J. Michalenko', 'Andrew S. Lan', 'Richard G. Baraniuk']|['stat.ML', 'cs.CL']
2017-03-28T14:01:49Z|2017-03-24T13:29:52Z|http://arxiv.org/abs/1703.08403v1|http://arxiv.org/pdf/1703.08403v1|Asymmetric Learning Vector Quantization for Efficient Nearest Neighbor   Classification in Dynamic Time Warping Spaces|asymmetr learn vector quantize effici nearest neighbor classif dynam time warp space|The nearest neighbor method together with the dynamic time warping (DTW) distance is one of the most popular approaches in time series classification. This method suffers from high storage and computation requirements for large training sets. As a solution to both drawbacks, this article extends learning vector quantization (LVQ) from Euclidean spaces to DTW spaces. The proposed LVQ scheme uses asymmetric weighted averaging as update rule. Empirical results exhibited superior performance of asymmetric generalized LVQ (GLVQ) over other state-of-the-art prototype generation methods for nearest neighbor classification.|nearest neighbor method togeth dynam time warp dtw distanc one popular approach time seri classif method suffer high storag comput requir larg train set solut drawback articl extend learn vector quantize lvq euclidean space dtw space propos lvq scheme use asymmetr weight averag updat rule empir result exhibit superior perform asymmetr general lvq glvq state art prototyp generat method nearest neighbor classif|['Brijnesh Jain', 'David Schultz']|['cs.LG', 'stat.ML']
2017-03-28T14:01:53Z|2017-03-24T12:07:34Z|http://arxiv.org/abs/1703.08383v1|http://arxiv.org/pdf/1703.08383v1|Smart Augmentation - Learning an Optimal Data Augmentation Strategy|smart augment learn optim data augment strategi|A recurring problem faced when training neural networks is that there is typically not enough data to maximize the generalization capability of deep neural networks(DNN). There are many techniques to address this, including data augmentation, dropout, and transfer learning. In this paper, we introduce an additional method which we call Smart Augmentation and we show how to use it to increase the accuracy and reduce overfitting on a target network. Smart Augmentation works by creating a network that learns how to generate augmented data during the training process of a target network in a way that reduces that networks loss. This allows us to learn augmentations that minimize the error of that network.   Smart Augmentation has shown the potential to increase accuracy by demonstrably significant measures on all datasets tested. In addition, it has shown potential to achieve similar or improved performance levels with significantly smaller network sizes in a number of tested cases.|recur problem face train neural network typic enough data maxim general capabl deep neural network dnn mani techniqu address includ data augment dropout transfer learn paper introduc addit method call smart augment show use increas accuraci reduc overfit target network smart augment work creat network learn generat augment data dure train process target network way reduc network loss allow us learn augment minim error network smart augment shown potenti increas accuraci demonstr signific measur dataset test addit shown potenti achiev similar improv perform level signific smaller network size number test case|['Joseph Lemley', 'Shabab Bazrafkan', 'Peter Corcoran']|['cs.AI', 'cs.LG', 'stat.ML']
2017-03-28T14:01:53Z|2017-03-24T02:31:23Z|http://arxiv.org/abs/1703.08267v1|http://arxiv.org/abs/1703.08267v1|A Nonconvex Splitting Method for Symmetric Nonnegative Matrix   Factorization: Convergence Analysis and Optimality|nonconvex split method symmetr nonneg matrix factor converg analysi optim|Symmetric nonnegative matrix factorization (SymNMF) has important applications in data analytics problems such as document clustering, community detection and image segmentation. In this paper, we propose a novel nonconvex variable splitting method for solving SymNMF. The proposed algorithm is guaranteed to converge to the set of Karush-Kuhn-Tucker (KKT) points of the nonconvex SymNMF problem. Furthermore, it achieves a global sublinear convergence rate. We also show that the algorithm can be efficiently implemented in parallel. Further, sufficient conditions are provided which guarantee the global and local optimality of the obtained solutions. Extensive numerical results performed on both synthetic and real data sets suggest that the proposed algorithm converges quickly to a local minimum solution.|symmetr nonneg matrix factor symnmf import applic data analyt problem document cluster communiti detect imag segment paper propos novel nonconvex variabl split method solv symnmf propos algorithm guarante converg set karush kuhn tucker kkt point nonconvex symnmf problem furthermor achiev global sublinear converg rate also show algorithm effici implement parallel suffici condit provid guarante global local optim obtain solut extens numer result perform synthet real data set suggest propos algorithm converg quick local minimum solut|['Songtao Lu', 'Mingyi Hong', 'Zhengdao Wang']|['math.OC', 'stat.ML']
2017-03-28T14:01:53Z|2017-03-23T23:27:12Z|http://arxiv.org/abs/1703.08251v1|http://arxiv.org/pdf/1703.08251v1|The Dependence of Machine Learning on Electronic Medical Record Quality|depend machin learn electron medic record qualiti|There is growing interest in applying machine learning methods to Electronic Medical Records (EMR). Across different institutions, however, EMR quality can vary widely. This work investigated the impact of this disparity on the performance of three advanced machine learning algorithms: logistic regression, multilayer perceptron, and recurrent neural network. The EMR disparity was emulated using different permutations of the EMR collected at Children's Hospital Los Angeles (CHLA) Pediatric Intensive Care Unit (PICU) and Cardiothoracic Intensive Care Unit (CTICU). The algorithms were trained using patients from the PICU to predict in-ICU mortality for patients in a held out set of PICU and CTICU patients. The disparate patient populations between the PICU and CTICU provide an estimate of generalization errors across different ICUs. We quantified and evaluated the generalization of these algorithms on varying EMR size, input types, and fidelity of data.|grow interest appli machin learn method electron medic record emr across differ institut howev emr qualiti vari wide work investig impact dispar perform three advanc machin learn algorithm logist regress multilay perceptron recurr neural network emr dispar emul use differ permut emr collect children hospit los angel chla pediatr intens care unit picu cardiothorac intens care unit cticu algorithm train use patient picu predict icu mortal patient held set picu cticu patient dispar patient popul picu cticu provid estim general error across differ icus quantifi evalu general algorithm vari emr size input type fidel data|['Long Ho', 'David Ledbetter', 'Melissa Aczon', 'Randall Wetzel']|['stat.ML']
2017-03-28T14:01:53Z|2017-03-23T15:35:33Z|http://arxiv.org/abs/1703.08110v1|http://arxiv.org/pdf/1703.08110v1|Training Mixture Models at Scale via Coresets|train mixtur model scale via coreset|How can we train a statistical mixture model on a massive data set? In this paper, we show how to construct coresets for mixtures of Gaussians and natural generalizations. A coreset is a weighted subset of the data, which guarantees that models fitting the coreset also provide a good fit for the original data set. We show that, perhaps surprisingly, Gaussian mixtures admit coresets of size polynomial in dimension and the number of mixture components, while being independent of the data set size. Hence, one can compute a $(1+ \varepsilon)$-approximation for the optimal model on a significantly smaller data set. More importantly, such coresets can be efficiently constructed both in distributed and streaming settings. Our results rely on a novel reduction of statistical estimation to problems in computational geometry and new complexity results for mixtures of Gaussians. As a by-product of our analysis, we prove that the pseudo-dimension of arbitrary mixtures of Gaussians is polynomial in the ambient dimension. Empirical evaluation on several real-world datasets suggest that our coreset-based approach enables significant reduction in training-time with negligible approximation error.|train statist mixtur model massiv data set paper show construct coreset mixtur gaussian natur general coreset weight subset data guarante model fit coreset also provid good fit origin data set show perhap surpris gaussian mixtur admit coreset size polynomi dimens number mixtur compon independ data set size henc one comput varepsilon approxim optim model signific smaller data set import coreset effici construct distribut stream set result reli novel reduct statist estim problem comput geometri new complex result mixtur gaussian product analysi prove pseudo dimens arbitrari mixtur gaussian polynomi ambient dimens empir evalu sever real world dataset suggest coreset base approach enabl signific reduct train time neglig approxim error|['Mario Lucic', 'Matthew Faulkner', 'Andreas Krause', 'Dan Feldman']|['stat.ML']
2017-03-28T14:01:53Z|2017-03-23T14:29:29Z|http://arxiv.org/abs/1703.08085v1|http://arxiv.org/pdf/1703.08085v1|Unifying Framework for Crowd-sourcing via Graphon Estimation|unifi framework crowd sourc via graphon estim|We consider the question of inferring true answers associated with tasks based on potentially noisy answers obtained through a micro-task crowd-sourcing platform such as Amazon Mechanical Turk. We propose a generic, non-parametric model for this setting: for a given task $i$, $1\leq i \leq T$, the response of worker $j$, $1\leq j\leq W$ for this task is correct with probability $F_{ij}$, where matrix $F = [F_{ij}]_{i\leq T, j\leq W}$ may satisfy one of a collection of regularity conditions including low rank, which can express the popular Dawid-Skene model; piecewise constant, which occurs when there is finitely many worker and task types; monotonic under permutation, when there is some ordering of worker skills and task difficulties; or Lipschitz with respect to an associated latent non-parametric function. This model, contains most, if not all, of the previously proposed models to the best of our knowledge.   We show that the question of estimating the true answers to tasks can be reduced to solving the Graphon estimation problem, for which there has been much recent progress. By leveraging these techniques, we provide a crowdsourcing inference algorithm along with theoretical bounds on the fraction of incorrectly estimated tasks. Subsequently, we have a solution for inferring the true answers for tasks using noisy answers collected from crowd-sourcing platform under a significantly larger class of models. Concretely, we establish that if the $(i,j)$th element of $F$, $F_{ij}$, is equal to a Lipschitz continuous function over latent features associated with the task $i$ and worker $j$ for all $i, j$, then all task answers can be inferred correctly with high probability by soliciting $\tilde{O}(\ln(T)^{3/2})$ responses per task even without any knowledge of the Lipschitz function, task and worker features, or the matrix $F$.|consid question infer true answer associ task base potenti noisi answer obtain micro task crowd sourc platform amazon mechan turk propos generic non parametr model set given task leq leq respons worker leq leq task correct probabl ij matrix ij leq leq may satisfi one collect regular condit includ low rank express popular dawid skene model piecewis constant occur finit mani worker task type monoton permut order worker skill task difficulti lipschitz respect associ latent non parametr function model contain previous propos model best knowledg show question estim true answer task reduc solv graphon estim problem much recent progress leverag techniqu provid crowdsourc infer algorithm along theoret bound fraction incorrect estim task subsequ solut infer true answer task use noisi answer collect crowd sourc platform signific larger class model concret establish th element ij equal lipschitz continu function latent featur associ task worker task answer infer correct high probabl solicit tild ln respons per task even without ani knowledg lipschitz function task worker featur matrix|['Christina E. Lee', 'Devavrat Shah']|['stat.ML']
2017-03-28T14:01:53Z|2017-03-23T13:41:47Z|http://arxiv.org/abs/1703.08065v1|http://arxiv.org/pdf/1703.08065v1|Robustness of Maximum Correntropy Estimation Against Large Outliers|robust maximum correntropi estim larg outlier|The maximum correntropy criterion (MCC) has recently been successfully applied in robust regression, classification and adaptive filtering, where the correntropy is maximized instead of minimizing the well-known mean square error (MSE) to improve the robustness with respect to outliers (or impulsive noises). Considerable efforts have been devoted to develop various robust adaptive algorithms under MCC, but so far little insight has been gained as to how the optimal solution will be affected by outliers. In this work, we study this problem in the context of parameter estimation for a simple linear errors-in-variables (EIV) model where all variables are scalar. Under certain conditions, we derive an upper bound on the absolute value of the estimation error and show that the optimal solution under MCC can be very close to the true value of the unknown parameter even with outliers (whose values can be arbitrarily large) in both input and output variables. An illustrative example is presented to verify and clarify the theory.|maximum correntropi criterion mcc recent success appli robust regress classif adapt filter correntropi maxim instead minim well known mean squar error mse improv robust respect outlier impuls nois consider effort devot develop various robust adapt algorithm mcc far littl insight gain optim solut affect outlier work studi problem context paramet estim simpl linear error variabl eiv model variabl scalar certain condit deriv upper bound absolut valu estim error show optim solut mcc veri close true valu unknown paramet even outlier whose valu arbitrarili larg input output variabl illustr exampl present verifi clarifi theori|['Badong Chen', 'Lei Xing', 'Haiquan Zhao', 'Bin Xu', 'Jose C. Principe']|['stat.ML']
2017-03-28T14:01:53Z|2017-03-23T13:00:14Z|http://arxiv.org/abs/1703.08052v1|http://arxiv.org/pdf/1703.08052v1|Dynamic Bernoulli Embeddings for Language Evolution|dynam bernoulli embed languag evolut|Word embeddings are a powerful approach for unsupervised analysis of language. Recently, Rudolph et al. (2016) developed exponential family embeddings, which cast word embeddings in a probabilistic framework. Here, we develop dynamic embeddings, building on exponential family embeddings to capture how the meanings of words change over time. We use dynamic embeddings to analyze three large collections of historical texts: the U.S. Senate speeches from 1858 to 2009, the history of computer science ACM abstracts from 1951 to 2014, and machine learning papers on the Arxiv from 2007 to 2015. We find dynamic embeddings provide better fits than classical embeddings and capture interesting patterns about how language changes.|word embed power approach unsupervis analysi languag recent rudolph et al develop exponenti famili embed cast word embed probabilist framework develop dynam embed build exponenti famili embed captur mean word chang time use dynam embed analyz three larg collect histor text senat speech histori comput scienc acm abstract machin learn paper arxiv find dynam embed provid better fit classic embed captur interest pattern languag chang|['Maja Rudolph', 'David Blei']|['stat.ML', 'cs.CL']
2017-03-28T14:01:53Z|2017-03-23T12:17:00Z|http://arxiv.org/abs/1703.08031v1|http://arxiv.org/pdf/1703.08031v1|Distribution of Gaussian Process Arc Lengths|distribut gaussian process arc length|We present the first treatment of the arc length of the Gaussian Process (GP) with more than a single output dimension. GPs are commonly used for tasks such as trajectory modelling, where path length is a crucial quantity of interest. Previously, only paths in one dimension have been considered, with no theoretical consideration of higher dimensional problems. We fill the gap in the existing literature by deriving the moments of the arc length for a stationary GP with multiple output dimensions. A new method is used to derive the mean of a one-dimensional GP over a finite interval, by considering the distribution of the arc length integrand. This technique is used to derive an approximate distribution over the arc length of a vector valued GP in $\mathbb{R}^n$ by moment matching the distribution. Numerical simulations confirm our theoretical derivations.|present first treatment arc length gaussian process gp singl output dimens gps common use task trajectori model path length crucial quantiti interest previous onli path one dimens consid theoret consider higher dimension problem fill gap exist literatur deriv moment arc length stationari gp multipl output dimens new method use deriv mean one dimension gp finit interv consid distribut arc length integrand techniqu use deriv approxim distribut arc length vector valu gp mathbb moment match distribut numer simul confirm theoret deriv|['Justin D. Bewsher', 'Alessandra Tosi', 'Michael A. Osborne', 'Stephen J. Roberts']|['stat.ML']
2017-03-28T14:01:53Z|2017-03-23T07:13:28Z|http://arxiv.org/abs/1703.07948v1|http://arxiv.org/pdf/1703.07948v1|Fast Stochastic Variance Reduced Gradient Method with Momentum   Acceleration for Machine Learning|fast stochast varianc reduc gradient method momentum acceler machin learn|Recently, research on accelerated stochastic gradient descent methods (e.g., SVRG) has made exciting progress (e.g., linear convergence for strongly convex problems). However, the best-known methods (e.g., Katyusha) requires at least two auxiliary variables and two momentum parameters. In this paper, we propose a fast stochastic variance reduction gradient (FSVRG) method, in which we design a novel update rule with the Nesterov's momentum and incorporate the technique of growing epoch size. FSVRG has only one auxiliary variable and one momentum weight, and thus it is much simpler and has much lower per-iteration complexity. We prove that FSVRG achieves linear convergence for strongly convex problems and the optimal $\mathcal{O}(1/T^2)$ convergence rate for non-strongly convex problems, where $T$ is the number of outer-iterations. We also extend FSVRG to directly solve the problems with non-smooth component functions, such as SVM. Finally, we empirically study the performance of FSVRG for solving various machine learning problems such as logistic regression, ridge regression, Lasso and SVM. Our results show that FSVRG outperforms the state-of-the-art stochastic methods, including Katyusha.|recent research acceler stochast gradient descent method svrg made excit progress linear converg strong convex problem howev best known method katyusha requir least two auxiliari variabl two momentum paramet paper propos fast stochast varianc reduct gradient fsvrg method design novel updat rule nesterov momentum incorpor techniqu grow epoch size fsvrg onli one auxiliari variabl one momentum weight thus much simpler much lower per iter complex prove fsvrg achiev linear converg strong convex problem optim mathcal converg rate non strong convex problem number outer iter also extend fsvrg direct solv problem non smooth compon function svm final empir studi perform fsvrg solv various machin learn problem logist regress ridg regress lasso svm result show fsvrg outperform state art stochast method includ katyusha|['Fanhua Shang', 'Yuanyuan Liu', 'James Cheng', 'Jiacheng Zhuo']|['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']
2017-03-28T14:01:53Z|2017-03-23T05:23:34Z|http://arxiv.org/abs/1703.07940v1|http://arxiv.org/pdf/1703.07940v1|Unsupervised Basis Function Adaptation for Reinforcement Learning|unsupervis basi function adapt reinforc learn|When using reinforcement learning (RL) algorithms to evaluate a policy it is common, given a large state space, to introduce some form of approximation architecture for the value function (VF). The exact form of this architecture can have a significant effect on the accuracy of the VF estimate, however, and determining a suitable approximation architecture can often be a highly complex task. Consequently there is a large amount of interest in the potential for allowing RL algorithms to adaptively generate (i.e. to learn) approximation architectures.   We investigate a method of adapting approximation architectures which uses feedback regarding the frequency with which an agent has visited certain states to guide which areas of the state space to approximate with greater detail. We introduce an algorithm based upon this idea which adapts a state aggregation approximation architecture on-line.   Assuming $S$ states, we demonstrate theoretically that - provided the following relatively non-restrictive assumptions are satisfied: (a) the number of cells $X$ in the state aggregation architecture is of order $\sqrt{S}\ln{S}\log_2{S}$ or greater, (b) the policy and transition function are close to deterministic, and (c) the prior for the transition function is uniformly distributed - our algorithm can guarantee, assuming we use an appropriate scoring function to measure VF error, error which is arbitrarily close to zero as $S$ becomes large. It is able to do this despite having only $O(X\log_2{S})$ space complexity (and negligible time complexity). We conclude by generating a set of empirical results which support the theoretical results.|use reinforc learn rl algorithm evalu polici common given larg state space introduc form approxim architectur valu function vf exact form architectur signific effect accuraci vf estim howev determin suitabl approxim architectur often high complex task consequ larg amount interest potenti allow rl algorithm adapt generat learn approxim architectur investig method adapt approxim architectur use feedback regard frequenc agent visit certain state guid area state space approxim greater detail introduc algorithm base upon idea adapt state aggreg approxim architectur line assum state demonstr theoret provid follow relat non restrict assumpt satisfi number cell state aggreg architectur order sqrt ln log greater polici transit function close determinist prior transit function uniform distribut algorithm guarante assum use appropri score function measur vf error error arbitrarili close zero becom larg abl despit onli log space complex neglig time complex conclud generat set empir result support theoret result|['Edward Barker', 'Charl Ras']|['cs.LG', 'cs.AI', 'stat.ML']
2017-03-28T14:01:57Z|2017-03-23T04:25:48Z|http://arxiv.org/abs/1703.07928v1|http://arxiv.org/pdf/1703.07928v1|Guided Perturbations: Self Corrective Behavior in Convolutional Neural   Networks|guid perturb self correct behavior convolut neural network|Convolutional Neural Networks have been a subject of great importance over the past decade and great strides have been made in their utility for producing state of the art performance in many computer vision problems. However, the behavior of deep networks is yet to be fully understood and is still an active area of research. In this work, we present an intriguing behavior: pre-trained CNNs can be made to improve their predictions by structurally perturbing the input. We observe that these perturbations - referred as Guided Perturbations - enable a trained network to improve its prediction performance without any learning or change in network weights. We perform various ablative experiments to understand how these perturbations affect the local context and feature representations. Furthermore, we demonstrate that this idea can improve performance of several existing approaches on semantic segmentation and scene labeling tasks on the PASCAL VOC dataset and supervised classification tasks on MNIST and CIFAR10 datasets.|convolut neural network subject great import past decad great stride made util produc state art perform mani comput vision problem howev behavior deep network yet fulli understood still activ area research work present intrigu behavior pre train cnns made improv predict structur perturb input observ perturb refer guid perturb enabl train network improv predict perform without ani learn chang network weight perform various ablat experi understand perturb affect local context featur represent furthermor demonstr idea improv perform sever exist approach semant segment scene label task pascal voc dataset supervis classif task mnist cifar dataset|['Swami Sankaranarayanan', 'Arpit Jain', 'Ser Nam Lim']|['cs.CV', 'cs.AI', 'stat.ML']
2017-03-28T14:01:57Z|2017-03-23T03:17:14Z|http://arxiv.org/abs/1703.07915v1|http://arxiv.org/pdf/1703.07915v1|Perspective: Energy Landscapes for Machine Learning|perspect energi landscap machin learn|Machine learning techniques are being increasingly used as flexible non-linear fitting and prediction tools in the physical sciences. Fitting functions that exhibit multiple solutions as local minima can be analysed in terms of the corresponding machine learning landscape. Methods to explore and visualise molecular potential energy landscapes can be applied to these machine learning landscapes to gain new insight into the solution space involved in training and the nature of the corresponding predictions. In particular, we can define quantities analogous to molecular structure, thermodynamics, and kinetics, and relate these emergent properties to the structure of the underlying landscape. This Perspective aims to describe these analogies with examples from recent applications, and suggest avenues for new interdisciplinary research.|machin learn techniqu increas use flexibl non linear fit predict tool physic scienc fit function exhibit multipl solut local minima analys term correspond machin learn landscap method explor visualis molecular potenti energi landscap appli machin learn landscap gain new insight solut space involv train natur correspond predict particular defin quantiti analog molecular structur thermodynam kinet relat emerg properti structur landscap perspect aim describ analog exampl recent applic suggest avenu new interdisciplinari research|['Andrew J. Ballard', 'Ritankar Das', 'Stefano Martiniani', 'Dhagash Mehta', 'Levent Sagun', 'Jacob D. Stevenson', 'David J. Wales']|['stat.ML', 'cond-mat.dis-nn', 'cs.LG']
2017-03-28T14:01:57Z|2017-03-23T02:40:36Z|http://arxiv.org/abs/1703.07909v1|http://arxiv.org/pdf/1703.07909v1|Data Driven Exploratory Attacks on Black Box Classifiers in Adversarial   Domains|data driven exploratori attack black box classifi adversari domain|While modern day web applications aim to create impact at the civilization level, they have become vulnerable to adversarial activity, where the next cyber-attack can take any shape and can originate from anywhere. The increasing scale and sophistication of attacks, has prompted the need for a data driven solution, with machine learning forming the core of many cybersecurity systems. Machine learning was not designed with security in mind, and the essential assumption of stationarity, requiring that the training and testing data follow similar distributions, is violated in an adversarial domain. In this paper, an adversary's view point of a classification based system, is presented. Based on a formal adversarial model, the Seed-Explore-Exploit framework is presented, for simulating the generation of data driven and reverse engineering attacks on classifiers. Experimental evaluation, on 10 real world datasets and using the Google Cloud Prediction Platform, demonstrates the innate vulnerability of classifiers and the ease with which evasion can be carried out, without any explicit information about the classifier type, the training data or the application domain. The proposed framework, algorithms and empirical evaluation, serve as a white hat analysis of the vulnerabilities, and aim to foster the development of secure machine learning frameworks.|modern day web applic aim creat impact civil level becom vulner adversari activ next cyber attack take ani shape origin anywher increas scale sophist attack prompt need data driven solut machin learn form core mani cybersecur system machin learn design secur mind essenti assumpt stationar requir train test data follow similar distribut violat adversari domain paper adversari view point classif base system present base formal adversari model seed explor exploit framework present simul generat data driven revers engin attack classifi experiment evalu real world dataset use googl cloud predict platform demonstr innat vulner classifi eas evas carri without ani explicit inform classifi type train data applic domain propos framework algorithm empir evalu serv white hat analysi vulner aim foster develop secur machin learn framework|['Tegjyot Singh Sethi', 'Mehmed Kantardzic']|['stat.ML', 'cs.CR', 'cs.LG']
2017-03-28T14:01:57Z|2017-03-23T01:30:17Z|http://arxiv.org/abs/1703.07904v1|http://arxiv.org/pdf/1703.07904v1|Cross-Validation with Confidence|cross valid confid|Cross-validation is one of the most popular model selection methods in statistics and machine learning. Despite its wide applicability, traditional cross-validation methods tend to select overfitting models, unless the ratio between the training and testing sample sizes is much smaller than conventional choices. We argue that such an overfitting tendency of cross-validation is due to the ignorance of the uncertainty in the testing sample. Starting from this observation, we develop a new, statistically principled inference tool based on cross-validation that takes into account the uncertainty in the testing sample. This new method outputs a small set of highly competitive candidate models containing the best one with guaranteed probability. As a consequence, our method can achieve consistent variable selection in a classical linear regression setting, for which existing cross-validation methods require unconventional split ratios. We demonstrate the performance of the proposed method in several simulated and real data examples.|cross valid one popular model select method statist machin learn despit wide applic tradit cross valid method tend select overfit model unless ratio train test sampl size much smaller convent choic argu overfit tendenc cross valid due ignor uncertainti test sampl start observ develop new statist principl infer tool base cross valid take account uncertainti test sampl new method output small set high competit candid model contain best one guarante probabl consequ method achiev consist variabl select classic linear regress set exist cross valid method requir unconvent split ratio demonstr perform propos method sever simul real data exampl|['Jing Lei']|['stat.ME', 'stat.ML']
2017-03-28T14:01:57Z|2017-03-22T23:35:51Z|http://arxiv.org/abs/1703.07886v1|http://arxiv.org/pdf/1703.07886v1|Robust Kronecker-Decomposable Component Analysis for Low Rank Modeling|robust kroneck decompos compon analysi low rank model|Dictionary learning and component analysis are part of one of the most well-studied and active research fields, at the intersection of signal and image processing, computer vision, and statistical machine learning. In dictionary learning, the current methods of choice are arguably K-SVD and its variants, which learn a dictionary (i.e., a decomposition) for sparse coding via Singular Value Decomposition. In robust component analysis, leading methods derive from Principal Component Pursuit (PCP), which recovers a low-rank matrix from sparse corruptions of unknown magnitude and support. While K-SVD is sensitive to the presence of noise and outliers in the training set, PCP does not provide a dictionary that respects the structure of the data (e.g., images), and requires expensive SVD computations when solved by convex relaxation. In this paper, we introduce a new robust decomposition of images by combining ideas from sparse dictionary learning and PCP. We propose a novel Kronecker-decomposable component analysis which is robust to gross corruption, can be used for low-rank modeling, and leverages separability to solve significantly smaller problems. We design an efficient learning algorithm by drawing links with a restricted form of tensor factorization. The effectiveness of the proposed approach is demonstrated on real-world applications, namely background subtraction and image denoising, by performing a thorough comparison with the current state of the art.|dictionari learn compon analysi part one well studi activ research field intersect signal imag process comput vision statist machin learn dictionari learn current method choic arguabl svd variant learn dictionari decomposit spars code via singular valu decomposit robust compon analysi lead method deriv princip compon pursuit pcp recov low rank matrix spars corrupt unknown magnitud support svd sensit presenc nois outlier train set pcp doe provid dictionari respect structur data imag requir expens svd comput solv convex relax paper introduc new robust decomposit imag combin idea spars dictionari learn pcp propos novel kroneck decompos compon analysi robust gross corrupt use low rank model leverag separ solv signific smaller problem design effici learn algorithm draw link restrict form tensor factor effect propos approach demonstr real world applic name background subtract imag denois perform thorough comparison current state art|['Mehdi Bahri', 'Yannis Panagakis', 'Stefanos Zafeiriou']|['stat.ML', 'cs.CV']
2017-03-28T14:01:57Z|2017-03-22T19:33:54Z|http://arxiv.org/abs/1703.07830v1|http://arxiv.org/abs/1703.07830v1|Randomized Kernel Methods for Least-Squares Support Vector Machines|random kernel method least squar support vector machin|The least-squares support vector machine is a frequently used kernel method for non-linear regression and classification tasks. Here we discuss several approximation algorithms for the least-squares support vector machine classifier. The proposed methods are based on randomized block kernel matrices, and we show that they provide good accuracy and reliable scaling for multi-class classification problems with relatively large data sets. Also, we present several numerical experiments that illustrate the practical applicability of the proposed methods.|least squar support vector machin frequent use kernel method non linear regress classif task discuss sever approxim algorithm least squar support vector machin classifi propos method base random block kernel matric show provid good accuraci reliabl scale multi class classif problem relat larg data set also present sever numer experi illustr practic applic propos method|['M. Andrecut']|['cs.LG', 'physics.data-an', 'stat.ML']
2017-03-28T14:01:57Z|2017-03-22T17:53:27Z|http://arxiv.org/abs/1703.07771v1|http://arxiv.org/pdf/1703.07771v1|Multitask Learning and Benchmarking with Clinical Time Series Data|multitask learn benchmark clinic time seri data|Health care is one of the most exciting frontiers in data mining and machine learning. Successful adoption of electronic health records (EHRs) created an explosion in digital clinical data available for analysis, but progress in machine learning for healthcare research has been difficult to measure because of the absence of publicly available benchmark data sets. To address this problem, we propose four clinical prediction benchmarks using data derived from the publicly available Medical Information Mart for Intensive Care (MIMIC-III) database. These tasks cover a range of clinical problems including modeling risk of mortality, forecasting length of stay, detecting physiologic decline, and phenotype classification. We formulate a heterogeneous multitask problem where the goal is to jointly learn multiple clinically relevant prediction tasks based on the same time series data. To address this problem, we propose a novel recurrent neural network (RNN) architecture that leverages the correlations between the various tasks to learn a better predictive model. We validate the proposed neural architecture on this benchmark, and demonstrate that it outperforms strong baselines, including single task RNNs.|health care one excit frontier data mine machin learn success adopt electron health record ehr creat explos digit clinic data avail analysi progress machin learn healthcar research difficult measur becaus absenc public avail benchmark data set address problem propos four clinic predict benchmark use data deriv public avail medic inform mart intens care mimic iii databas task cover rang clinic problem includ model risk mortal forecast length stay detect physiolog declin phenotyp classif formul heterogen multitask problem goal joint learn multipl clinic relev predict task base time seri data address problem propos novel recurr neural network rnn architectur leverag correl various task learn better predict model valid propos neural architectur benchmark demonstr outperform strong baselin includ singl task rnns|['Hrayr Harutyunyan', 'Hrant Khachatrian', 'David C. Kale', 'Aram Galstyan']|['stat.ML', 'cs.LG']
2017-03-28T14:01:57Z|2017-03-22T17:27:57Z|http://arxiv.org/abs/1703.07758v1|http://arxiv.org/pdf/1703.07758v1|S-Concave Distributions: Towards Broader Distributions for   Noise-Tolerant and Sample-Efficient Learning Algorithms|concav distribut toward broader distribut nois toler sampl effici learn algorithm|We provide new results concerning noise-tolerant and sample-efficient learning algorithms under $s$-concave distributions over $\mathbb{R}^n$ for $-\frac{1}{2n+3}\le s\le 0$. The new class of $s$-concave distributions is a broad and natural generalization of log-concavity, and includes many important additional distributions, e.g., the Pareto distribution and $t$-distribution. This class has been studied in the context of efficient sampling, integration, and optimization, but much remains unknown concerning the geometry of this class of distributions and their applications in the context of learning.   The challenge is that unlike the commonly used distributions in learning (uniform or more generally log-concave distributions), this broader class is not closed under the marginalization operator and many such distributions are fat-tailed. In this work, we introduce new convex geometry tools to study the properties of s-concave distributions and use these properties to provide bounds on quantities of interest to learning including the probability of disagreement between two halfspaces, disagreement outside a band, and disagreement coefficient. We use these results to significantly generalize prior results for margin-based active learning, disagreement-based active learning, and passively learning of intersections of halfspaces.   Our analysis of geometric properties of s-concave distributions might be of independent interest to optimization more broadly.|provid new result concern nois toler sampl effici learn algorithm concav distribut mathbb frac le le new class concav distribut broad natur general log concav includ mani import addit distribut pareto distribut distribut class studi context effici sampl integr optim much remain unknown concern geometri class distribut applic context learn challeng unlik common use distribut learn uniform general log concav distribut broader class close margin oper mani distribut fat tail work introduc new convex geometri tool studi properti concav distribut use properti provid bound quantiti interest learn includ probabl disagr two halfspac disagr outsid band disagr coeffici use result signific general prior result margin base activ learn disagr base activ learn passiv learn intersect halfspac analysi geometr properti concav distribut might independ interest optim broad|['Maria-Florina Balcan', 'Hongyang Zhang']|['stat.ML', 'cs.AI', 'cs.LG']
2017-03-28T14:01:57Z|2017-03-22T17:17:16Z|http://arxiv.org/abs/1703.07754v1|http://arxiv.org/pdf/1703.07754v1|Direct Acoustics-to-Word Models for English Conversational Speech   Recognition|direct acoust word model english convers speech recognit|Recent work on end-to-end automatic speech recognition (ASR) has shown that the connectionist temporal classification (CTC) loss can be used to convert acoustics to phone or character sequences. Such systems are used with a dictionary and separately-trained Language Model (LM) to produce word sequences. However, they are not truly end-to-end in the sense of mapping acoustics directly to words without an intermediate phone representation. In this paper, we present the first results employing direct acoustics-to-word CTC models on two well-known public benchmark tasks: Switchboard and CallHome. These models do not require an LM or even a decoder at run-time and hence recognize speech with minimal complexity. However, due to the large number of word output units, CTC word models require orders of magnitude more data to train reliably compared to traditional systems. We present some techniques to mitigate this issue. Our CTC word model achieves a word error rate of 13.0%/18.8% on the Hub5-2000 Switchboard/CallHome test sets without any LM or decoder compared with 9.6%/16.0% for phone-based CTC with a 4-gram LM. We also present rescoring results on CTC word model lattices to quantify the performance benefits of a LM, and contrast the performance of word and phone CTC models.|recent work end end automat speech recognit asr shown connectionist tempor classif ctc loss use convert acoust phone charact sequenc system use dictionari separ train languag model lm produc word sequenc howev truli end end sens map acoust direct word without intermedi phone represent paper present first result employ direct acoust word ctc model two well known public benchmark task switchboard callhom model requir lm even decod run time henc recogn speech minim complex howev due larg number word output unit ctc word model requir order magnitud data train reliabl compar tradit system present techniqu mitig issu ctc word model achiev word error rate hub switchboard callhom test set without ani lm decod compar phone base ctc gram lm also present rescor result ctc word model lattic quantifi perform benefit lm contrast perform word phone ctc model|['Kartik Audhkhasi', 'Bhuvana Ramabhadran', 'George Saon', 'Michael Picheny', 'David Nahamoo']|['cs.CL', 'cs.NE', 'stat.ML']
2017-03-28T14:01:57Z|2017-03-22T15:34:23Z|http://arxiv.org/abs/1703.07710v1|http://arxiv.org/pdf/1703.07710v1|UBEV - A More Practical Algorithm for Episodic RL with Near-Optimal PAC   and Regret Guarantees|ubev practic algorithm episod rl near optim pac regret guarante|We present UBEV, a simple and efficient reinforcement learning algorithm for fixed-horizon episodic Markov decision processes. The main contribution is a proof that UBEV enjoys a sample-complexity bound that holds for all accuracy levels simultaneously with high probability, and matches the lower bound except for logarithmic terms and one factor of the horizon. A consequence of the fact that our sample-complexity bound holds for all accuracy levels is that the new algorithm achieves a sub-linear regret of O(sqrt(SAT)), which is the first time the dependence on the size of the state space has provably appeared inside the square root. A brief empirical evaluation shows that UBEV is practically superior to existing algorithms with known sample-complexity guarantees.|present ubev simpl effici reinforc learn algorithm fix horizon episod markov decis process main contribut proof ubev enjoy sampl complex bound hold accuraci level simultan high probabl match lower bound except logarithm term one factor horizon consequ fact sampl complex bound hold accuraci level new algorithm achiev sub linear regret sqrt sat first time depend size state space provabl appear insid squar root brief empir evalu show ubev practic superior exist algorithm known sampl complex guarante|['Christoph Dann', 'Tor Lattimore', 'Emma Brunskill']|['cs.LG', 'cs.AI', 'stat.ML']
2017-03-28T14:02:02Z|2017-03-22T15:11:55Z|http://arxiv.org/abs/1703.07698v1|http://arxiv.org/pdf/1703.07698v1|Characterization of Deterministic and Probabilistic Sampling Patterns   for Finite Completability of Low Tensor-Train Rank Tensor|character determinist probabilist sampl pattern finit complet low tensor train rank tensor|In this paper, we analyze the fundamental conditions for low-rank tensor completion given the separation or tensor-train (TT) rank, i.e., ranks of unfoldings. We exploit the algebraic structure of the TT decomposition to obtain the deterministic necessary and sufficient conditions on the locations of the samples to ensure finite completability. Specifically, we propose an algebraic geometric analysis on the TT manifold that can incorporate the whole rank vector simultaneously in contrast to the existing approach based on the Grassmannian manifold that can only incorporate one rank component. Our proposed technique characterizes the algebraic independence of a set of polynomials defined based on the sampling pattern and the TT decomposition, which is instrumental to obtaining the deterministic condition on the sampling pattern for finite completability. In addition, based on the proposed analysis, assuming that the entries of the tensor are sampled independently with probability $p$, we derive a lower bound on the sampling probability $p$, or equivalently, the number of sampled entries that ensures finite completability with high probability. Moreover, we also provide the deterministic and probabilistic conditions for unique completability.|paper analyz fundament condit low rank tensor complet given separ tensor train tt rank rank unfold exploit algebra structur tt decomposit obtain determinist necessari suffici condit locat sampl ensur finit complet specif propos algebra geometr analysi tt manifold incorpor whole rank vector simultan contrast exist approach base grassmannian manifold onli incorpor one rank compon propos techniqu character algebra independ set polynomi defin base sampl pattern tt decomposit instrument obtain determinist condit sampl pattern finit complet addit base propos analysi assum entri tensor sampl independ probabl deriv lower bound sampl probabl equival number sampl entri ensur finit complet high probabl moreov also provid determinist probabilist condit uniqu complet|['Morteza Ashraphijuo', 'Xiaodong Wang']|['cs.LG', 'cs.IT', 'math.AG', 'math.IT', 'stat.ML']
2017-03-28T14:02:02Z|2017-03-22T12:50:15Z|http://arxiv.org/abs/1703.07625v1|http://arxiv.org/pdf/1703.07625v1|Clustering for Different Scales of Measurement - the Gap-Ratio Weighted   K-means Algorithm|cluster differ scale measur gap ratio weight mean algorithm|This paper describes a method for clustering data that are spread out over large regions and which dimensions are on different scales of measurement. Such an algorithm was developed to implement a robotics application consisting in sorting and storing objects in an unsupervised way. The toy dataset used to validate such application consists of Lego bricks of different shapes and colors. The uncontrolled lighting conditions together with the use of RGB color features, respectively involve data with a large spread and different levels of measurement between data dimensions. To overcome the combination of these two characteristics in the data, we have developed a new weighted K-means algorithm, called gap-ratio K-means, which consists in weighting each dimension of the feature space before running the K-means algorithm. The weight associated with a feature is proportional to the ratio of the biggest gap between two consecutive data points, and the average of all the other gaps. This method is compared with two other variants of K-means on the Lego bricks clustering problem as well as two other common classification datasets.|paper describ method cluster data spread larg region dimens differ scale measur algorithm develop implement robot applic consist sort store object unsupervis way toy dataset use valid applic consist lego brick differ shape color uncontrol light condit togeth use rgb color featur respect involv data larg spread differ level measur data dimens overcom combin two characterist data develop new weight mean algorithm call gap ratio mean consist weight dimens featur space befor run mean algorithm weight associ featur proport ratio biggest gap two consecut data point averag gap method compar two variant mean lego brick cluster problem well two common classif dataset|['Joris Guérin', 'Olivier Gibaru', 'Stéphane Thiery', 'Eric Nyiri']|['cs.LG', 'cs.DS', 'stat.ML']
2017-03-28T14:02:02Z|2017-03-22T11:53:53Z|http://arxiv.org/abs/1703.07608v1|http://arxiv.org/pdf/1703.07608v1|Deep Exploration via Randomized Value Functions|deep explor via random valu function|We study the use of randomized value functions to guide deep exploration in reinforcement learning. This offers an elegant means for synthesizing statistically and computationally efficient exploration with common practical approaches to value function learning. We present several reinforcement learning algorithms that leverage randomized value functions and demonstrate their efficacy through computational studies. We also prove a regret bound that establishes statistical efficiency with a tabular representation.|studi use random valu function guid deep explor reinforc learn offer eleg mean synthes statist comput effici explor common practic approach valu function learn present sever reinforc learn algorithm leverag random valu function demonstr efficaci comput studi also prove regret bound establish statist effici tabular represent|['Ian Osband', 'Daniel Russo', 'Zheng Wen', 'Benjamin Van Roy']|['stat.ML', 'cs.AI', 'cs.LG']
2017-03-28T14:02:02Z|2017-03-22T11:48:11Z|http://arxiv.org/abs/1703.07607v1|http://arxiv.org/pdf/1703.07607v1|A probabilistic approach to emission-line galaxy classification|probabilist approach emiss line galaxi classif|This work employs a Gaussian mixture model (GMM) to jointly analyse two traditional emission-line classification schemes of galaxy ionization sources: the Baldwin-Phillips-Terlevich (BPT) and W$_{H\alpha}$ vs. [NII]/H$\alpha$ (WHAN) diagrams, using spectroscopic data from the Sloan Digital Sky Survey Data Release 7 and SEAGal/STARLIGHT datasets. We apply a GMM to empirically define classes of galaxies in a three-dimensional space spanned by the log [OIII]/H\beta, log [NII]/H\alpha, and log EW(H{\alpha}) optical parameters. The best-fit GMM based on several statistical criteria consists of four Gaussian components (GCs), which are capable to explain up to 97 per cent of the data variance. Using elements of information theory, we compare each GC to their respective astronomical counterpart. GC1 and GC4 are associated with star-forming galaxies, suggesting the need to define a new starburst subgroup. GC2 is associated with BPT's Active Galaxy Nuclei (AGN) class and WHAN's weak AGN class. GC3 is associated with BPT's composite class and WHAN's strong AGN class. Conversely, there is no statistical evidence -- based on GMMs -- for the existence of a Seyfert/LINER dichotomy in our sample. We demonstrate the potential of our methodology to recover/unravel different objects inside the wilderness of astronomical datasets, without lacking the ability to convey physically interpretable results; hence being a precious tool for maximum exploitation of the ever-increasing astronomical surveys. The probabilistic classifications from the GMM analysis are publicly available within the COINtoolbox (https://cointoolbox.github.io/GMM_Catalogue/)|work employ gaussian mixtur model gmm joint analys two tradit emiss line classif scheme galaxi ionize sourc baldwin phillip terlevich bpt alpha vs nii alpha whan diagram use spectroscop data sloan digit sky survey data releas seagal starlight dataset appli gmm empir defin class galaxi three dimension space span log oiii beta log nii alpha log ew alpha optic paramet best fit gmm base sever statist criteria consist four gaussian compon gcs capabl explain per cent data varianc use element inform theori compar gc respect astronom counterpart gc gc associ star form galaxi suggest need defin new starburst subgroup gc associ bpt activ galaxi nuclei agn class whan weak agn class gc associ bpt composit class whan strong agn class convers statist evid base gmms exist seyfert liner dichotomi sampl demonstr potenti methodolog recov unravel differ object insid wilder astronom dataset without lack abil convey physic interpret result henc precious tool maximum exploit ever increas astronom survey probabilist classif gmm analysi public avail within cointoolbox https cointoolbox github io gmm catalogu|['R. S. de Souza', 'M. L. L. Dantas', 'M. V. Costa-Duarte', 'E. D. Feigelson', 'M. Killedar', 'P. -Y. Lablanche', 'R. Vilalta', 'A. Krone-Martins', 'R. Beck', 'F. Gieseke']|['astro-ph.GA', 'astro-ph.IM', 'stat.ML']
2017-03-28T14:02:02Z|2017-03-22T10:40:40Z|http://arxiv.org/abs/1703.07596v1|http://arxiv.org/pdf/1703.07596v1|Testing and Learning on Distributions with Symmetric Noise Invariance|test learn distribut symmetr nois invari|Kernel embeddings of distributions and the Maximum Mean Discrepancy (MMD), the resulting distance between distributions, are useful tools for fully nonparametric two-sample testing and learning on distributions. However, it is rarely that all possible differences between samples are of interest -- discovered differences can be due to different types of measurement noise, data collection artefacts or other irrelevant sources of variability. We propose distances between distributions which encode invariance to additive symmetric noise, aimed at testing whether the assumed true underlying processes differ. Moreover, we construct invariant features of distributions, leading to learning algorithms robust to the impairment of the input distributions with symmetric additive noise. Such features lend themselves to a straightforward neural network implementation and can thus also be learned given a supervised signal.|kernel embed distribut maximum mean discrep mmd result distanc distribut use tool fulli nonparametr two sampl test learn distribut howev rare possibl differ sampl interest discov differ due differ type measur nois data collect artefact irrelev sourc variabl propos distanc distribut encod invari addit symmetr nois aim test whether assum true process differ moreov construct invari featur distribut lead learn algorithm robust impair input distribut symmetr addit nois featur lend themselv straightforward neural network implement thus also learn given supervis signal|['Ho Chung Leon Law', 'Christopher Yau', 'Dino Sejdinovic']|['stat.ML']
2017-03-28T14:02:02Z|2017-03-22T03:26:32Z|http://arxiv.org/abs/1703.07506v1|http://arxiv.org/abs/1703.07506v1|LogitBoost autoregressive networks|logitboost autoregress network|Multivariate binary distributions can be decomposed into products of univariate conditional distributions. Recently popular approaches have modeled these conditionals through neural networks with sophisticated weight-sharing structures. It is shown that state-of-the-art performance on several standard benchmark datasets can actually be achieved by training separate probability estimators for each dimension. In that case, model training can be trivially parallelized over data dimensions. On the other hand, complexity control has to be performed for each learned conditional distribution. Three possible methods are considered and experimentally compared. The estimator that is employed for each conditional is LogitBoost. Similarities and differences between the proposed approach and autoregressive models based on neural networks are discussed in detail.|multivari binari distribut decompos product univari condit distribut recent popular approach model condit neural network sophist weight share structur shown state art perform sever standard benchmark dataset actual achiev train separ probabl estim dimens case model train trivial parallel data dimens hand complex control perform learn condit distribut three possibl method consid experiment compar estim employ condit logitboost similar differ propos approach autoregress model base neural network discuss detail|['Marc Goessling']|['stat.ML', 'cs.LG']
2017-03-28T14:02:02Z|2017-03-21T23:56:51Z|http://arxiv.org/abs/1703.07473v1|http://arxiv.org/pdf/1703.07473v1|Episode-Based Active Learning with Bayesian Neural Networks|episod base activ learn bayesian neural network|We investigate different strategies for active learning with Bayesian deep neural networks. We focus our analysis on scenarios where new, unlabeled data is obtained episodically, such as commonly encountered in mobile robotics applications. An evaluation of different strategies for acquisition, updating, and final training on the CIFAR-10 dataset shows that incremental network updates with final training on the accumulated acquisition set are essential for best performance, while limiting the amount of required human labeling labor.|investig differ strategi activ learn bayesian deep neural network focus analysi scenario new unlabel data obtain episod common encount mobil robot applic evalu differ strategi acquisit updat final train cifar dataset show increment network updat final train accumul acquisit set essenti best perform limit amount requir human label labor|['Feras Dayoub', 'Niko Sünderhauf', 'Peter Corke']|['cs.CV', 'cs.LG', 'stat.ML']
2017-03-28T14:02:02Z|2017-03-21T18:05:31Z|http://arxiv.org/abs/1703.07370v1|http://arxiv.org/pdf/1703.07370v1|REBAR: Low-variance, unbiased gradient estimates for discrete latent   variable models|rebar low varianc unbias gradient estim discret latent variabl model|Learning in models with discrete latent variables is challenging due to high variance gradient estimators. Generally, approaches have relied on control variates to reduce the variance of the REINFORCE estimator. Recent work (Jang et al. 2016, Maddison et al. 2016) has taken a different approach, introducing a continuous relaxation of discrete variables to produce low-variance, but biased, gradient estimates. In this work, we combine the two approaches through a novel control variate that produces low-variance, unbiased gradient estimates. We present encouraging preliminary results on a toy problem and on learning sigmoid belief networks.|learn model discret latent variabl challeng due high varianc gradient estim general approach reli control variat reduc varianc reinforc estim recent work jang et al maddison et al taken differ approach introduc continu relax discret variabl produc low varianc bias gradient estim work combin two approach novel control variat produc low varianc unbias gradient estim present encourag preliminari result toy problem learn sigmoid belief network|['George Tucker', 'Andriy Mnih', 'Chris J. Maddison', 'Jascha Sohl-Dickstein']|['cs.LG', 'stat.ML']
2017-03-28T14:02:02Z|2017-03-21T18:00:02Z|http://arxiv.org/abs/1703.07355v1|http://arxiv.org/abs/1703.07355v1|An Army of Me: Sockpuppets in Online Discussion Communities|armi sockpuppet onlin discuss communiti|"In online discussion communities, users can interact and share information and opinions on a wide variety of topics. However, some users may create multiple identities, or sockpuppets, and engage in undesired behavior by deceiving others or manipulating discussions. In this work, we study sockpuppetry across nine discussion communities, and show that sockpuppets differ from ordinary users in terms of their posting behavior, linguistic traits, as well as social network structure. Sockpuppets tend to start fewer discussions, write shorter posts, use more personal pronouns such as ""I"", and have more clustered ego-networks. Further, pairs of sockpuppets controlled by the same individual are more likely to interact on the same discussion at the same time than pairs of ordinary users. Our analysis suggests a taxonomy of deceptive behavior in discussion communities. Pairs of sockpuppets can vary in their deceptiveness, i.e., whether they pretend to be different users, or their supportiveness, i.e., if they support arguments of other sockpuppets controlled by the same user. We apply these findings to a series of prediction tasks, notably, to identify whether a pair of accounts belongs to the same underlying user or not. Altogether, this work presents a data-driven view of deception in online discussion communities and paves the way towards the automatic detection of sockpuppets."|onlin discuss communiti user interact share inform opinion wide varieti topic howev user may creat multipl ident sockpuppet engag undesir behavior deceiv manipul discuss work studi sockpuppetri across nine discuss communiti show sockpuppet differ ordinari user term post behavior linguist trait well social network structur sockpuppet tend start fewer discuss write shorter post use person pronoun cluster ego network pair sockpuppet control individu like interact discuss time pair ordinari user analysi suggest taxonomi decept behavior discuss communiti pair sockpuppet vari decept whether pretend differ user support support argument sockpuppet control user appli find seri predict task notabl identifi whether pair account belong user altogeth work present data driven view decept onlin discuss communiti pave way toward automat detect sockpuppet|['Srijan Kumar', 'Justin Cheng', 'Jure Leskovec', 'V. S. Subrahmanian']|['cs.SI', 'cs.CY', 'physics.soc-ph', 'stat.AP', 'stat.ML']
2017-03-28T14:02:02Z|2017-03-21T17:58:03Z|http://arxiv.org/abs/1703.07345v1|http://arxiv.org/pdf/1703.07345v1|On The Projection Operator to A Three-view Cardinality Constrained Set|project oper three view cardin constrain set|The cardinality constraint is an intrinsic way to restrict the solution structure in many domains, for example, sparse learning, feature selection, and compressed sensing. To solve a cardinality constrained problem, the key challenge is to solve the projection onto the cardinality constraint set, which is NP-hard in general when there exist multiple overlapped cardiaiality constraints. In this paper, we consider the scenario where overlapped cardinality constraints satisfy a Three-view Cardinality Structure (TVCS), which reflects the natural restriction in many applications, such as identification of gene regulatory networks and task-worker assignment problem. We cast the projection onto the TVCS set into a linear programming, and prove that its solution can be obtained by finding an integer solution to such linear programming. We further prove that such integer solution can be found with the complexity proportional to the problem scale. We finally use synthetic experiments and two interesting applications in bioinformatics and crowdsourcing to validate the proposed TVCS model and method.|cardin constraint intrins way restrict solut structur mani domain exampl spars learn featur select compress sens solv cardin constrain problem key challeng solv project onto cardin constraint set np hard general exist multipl overlap cardiaial constraint paper consid scenario overlap cardin constraint satisfi three view cardin structur tvcs reflect natur restrict mani applic identif gene regulatori network task worker assign problem cast project onto tvcs set linear program prove solut obtain find integ solut linear program prove integ solut found complex proport problem scale final use synthet experi two interest applic bioinformat crowdsourc valid propos tvcs model method|['Haichuan Yang', 'Shupeng Gui', 'Chuyang Ke', 'Daniel Stefankovic', 'Ryohei Fujimaki', 'Ji Liu']|['cs.LG', 'stat.ML']
2017-03-28T14:02:06Z|2017-03-21T16:39:28Z|http://arxiv.org/abs/1703.07305v1|http://arxiv.org/abs/1703.07305v1|Targeting Bayes factors with direct-path non-equilibrium thermodynamic   integration|target bay factor direct path non equilibrium thermodynam integr|Thermodynamic integration (TI) for computing marginal likelihoods is based on an inverse annealing path from the prior to the posterior distribution. In many cases, the resulting estimator suffers from high variability, which particularly stems from the prior regime. When comparing complex models with differences in a comparatively small number of parameters, intrinsic errors from sampling fluctuations may outweigh the differences in the log marginal likelihood estimates. In the present article, we propose a thermodynamic integration scheme that directly targets the log Bayes factor. The method is based on a modified annealing path between the posterior distributions of the two models compared, which systematically avoids the high variance prior regime. We combine this scheme with the concept of non-equilibrium TI to minimise discretisation errors from numerical integration. Results obtained on Bayesian regression models applied to standard benchmark data, and a complex hierarchical model applied to biopathway inference, demonstrate a significant reduction in estimator variance over state-of-the-art TI methods.|thermodynam integr ti comput margin likelihood base invers anneal path prior posterior distribut mani case result estim suffer high variabl particular stem prior regim compar complex model differ compar small number paramet intrins error sampl fluctuat may outweigh differ log margin likelihood estim present articl propos thermodynam integr scheme direct target log bay factor method base modifi anneal path posterior distribut two model compar systemat avoid high varianc prior regim combin scheme concept non equilibrium ti minimis discretis error numer integr result obtain bayesian regress model appli standard benchmark data complex hierarch model appli biopathway infer demonstr signific reduct estim varianc state art ti method|['Marco Grzegorczyk', 'Andrej Aderhold', 'Dirk Husmeier']|['stat.ME', 'stat.ML']
2017-03-28T14:02:06Z|2017-03-21T15:42:38Z|http://arxiv.org/abs/1703.07285v1|http://arxiv.org/pdf/1703.07285v1|From safe screening rules to working sets for faster Lasso-type solvers|safe screen rule work set faster lasso type solver|Convex sparsity-promoting regularizations are ubiquitous in modern statistical learning. By construction, they yield solutions with few non-zero coefficients, which correspond to saturated constraints in the dual optimization formulation. Working set (WS) strategies are generic optimization techniques that consist in solving simpler problems that only consider a subset of constraints, whose indices form the WS. Working set methods therefore involve two nested iterations: the outer loop corresponds to the definition of the WS and the inner loop calls a solver for the subproblems. For the Lasso estimator a WS is a set of features, while for a Group Lasso it refers to a set of groups. In practice, WS are generally small in this context so the associated feature Gram matrix can fit in memory. Here we show that the Gauss-Southwell rule (a greedy strategy for block coordinate descent techniques) leads to fast solvers in this case. Combined with a working set strategy based on an aggressive use of so-called Gap Safe screening rules, we propose a solver achieving state-of-the-art performance on sparse learning problems. Results are presented on Lasso and multi-task Lasso estimators.|convex sparsiti promot regular ubiquit modern statist learn construct yield solut non zero coeffici correspond satur constraint dual optim formul work set ws strategi generic optim techniqu consist solv simpler problem onli consid subset constraint whose indic form ws work set method therefor involv two nest iter outer loop correspond definit ws inner loop call solver subproblem lasso estim ws set featur group lasso refer set group practic ws general small context associ featur gram matrix fit memori show gauss southwel rule greedi strategi block coordin descent techniqu lead fast solver case combin work set strategi base aggress use call gap safe screen rule propos solver achiev state art perform spars learn problem result present lasso multi task lasso estim|['Mathurin Massias', 'Alexandre Gramfort', 'Joseph Salmon']|['stat.ML', 'cs.LG', 'math.OC', 'stat.CO']
2017-03-28T14:02:06Z|2017-03-22T17:08:40Z|http://arxiv.org/abs/1703.07255v2|http://arxiv.org/pdf/1703.07255v2|ZM-Net: Real-time Zero-shot Image Manipulation Network|zm net real time zero shot imag manipul network|Many problems in image processing and computer vision (e.g. colorization, style transfer) can be posed as 'manipulating' an input image into a corresponding output image given a user-specified guiding signal. A holy-grail solution towards generic image manipulation should be able to efficiently alter an input image with any personalized signals (even signals unseen during training), such as diverse paintings and arbitrary descriptive attributes. However, existing methods are either inefficient to simultaneously process multiple signals (let alone generalize to unseen signals), or unable to handle signals from other modalities. In this paper, we make the first attempt to address the zero-shot image manipulation task. We cast this problem as manipulating an input image according to a parametric model whose key parameters can be conditionally generated from any guiding signal (even unseen ones). To this end, we propose the Zero-shot Manipulation Net (ZM-Net), a fully-differentiable architecture that jointly optimizes an image-transformation network (TNet) and a parameter network (PNet). The PNet learns to generate key transformation parameters for the TNet given any guiding signal while the TNet performs fast zero-shot image manipulation according to both signal-dependent parameters from the PNet and signal-invariant parameters from the TNet itself. Extensive experiments show that our ZM-Net can perform high-quality image manipulation conditioned on different forms of guiding signals (e.g. style images and attributes) in real-time (tens of milliseconds per image) even for unseen signals. Moreover, a large-scale style dataset with over 20,000 style images is also constructed to promote further research.|mani problem imag process comput vision color style transfer pose manipul input imag correspond output imag given user specifi guid signal holi grail solut toward generic imag manipul abl effici alter input imag ani person signal even signal unseen dure train divers paint arbitrari descript attribut howev exist method either ineffici simultan process multipl signal let alon general unseen signal unabl handl signal modal paper make first attempt address zero shot imag manipul task cast problem manipul input imag accord parametr model whose key paramet condit generat ani guid signal even unseen one end propos zero shot manipul net zm net fulli differenti architectur joint optim imag transform network tnet paramet network pnet pnet learn generat key transform paramet tnet given ani guid signal tnet perform fast zero shot imag manipul accord signal depend paramet pnet signal invari paramet tnet extens experi show zm net perform high qualiti imag manipul condit differ form guid signal style imag attribut real time ten millisecond per imag even unseen signal moreov larg scale style dataset style imag also construct promot research|['Hao Wang', 'Xiaodan Liang', 'Hao Zhang', 'Dit-Yan Yeung', 'Eric P. Xing']|['cs.CV', 'cs.AI', 'cs.GR', 'cs.LG', 'stat.ML']
2017-03-28T14:02:06Z|2017-03-21T13:02:35Z|http://arxiv.org/abs/1703.07198v1|http://arxiv.org/pdf/1703.07198v1|Overcoming model simplifications when quantifying predictive uncertainty|overcom model simplif quantifi predict uncertainti|It is generally accepted that all models are wrong -- the difficulty is determining which are useful. Here, a useful model is considered as one that is capable of combining data and expert knowledge, through an inversion or calibration process, to adequately characterize the uncertainty in predictions of interest. This paper derives conditions that specify which simplified models are useful and how they should be calibrated. To start, the notion of an optimal simplification is defined. This relates the model simplifications to the nature of the data and predictions, and determines when a standard probabilistic calibration scheme is capable of accurately characterizing uncertainty. Furthermore, two additional conditions are defined for suboptimal models that determine when the simplifications can be safely ignored. The first allows a suboptimally simplified model to be used in a way that replicates the performance of an optimal model. This is achieved through the judicial selection of a prior term for the calibration process that explicitly includes the nature of the data, predictions and modelling simplifications. The second considers the dependency structure between the predictions and the available data to gain insights into when the simplifications can be overcome by using the right calibration data. Furthermore, the derived conditions are related to the commonly used calibration schemes based on Tikhonov and subspace regularization. To allow concrete insights to be obtained, the analysis is performed under a linear expansion of the model equations and where the predictive uncertainty is characterized via second order moments only.|general accept model wrong difficulti determin use use model consid one capabl combin data expert knowledg invers calibr process adequ character uncertainti predict interest paper deriv condit specifi simplifi model use calibr start notion optim simplif defin relat model simplif natur data predict determin standard probabilist calibr scheme capabl accur character uncertainti furthermor two addit condit defin suboptim model determin simplif safe ignor first allow suboptim simplifi model use way replic perform optim model achiev judici select prior term calibr process explicit includ natur data predict model simplif second consid depend structur predict avail data gain insight simplif overcom use right calibr data furthermor deriv condit relat common use calibr scheme base tikhonov subspac regular allow concret insight obtain analysi perform linear expans model equat predict uncertainti character via second order moment onli|['George M. Mathews', 'John Vial']|['stat.ML', 'math.PR', 'physics.comp-ph', 'physics.geo-ph', 'stat.ME', '62F15, 62C10, 68U05, 93E12, 93B11, 62P12']
2017-03-28T14:02:06Z|2017-03-21T12:33:19Z|http://arxiv.org/abs/1703.07169v1|http://arxiv.org/pdf/1703.07169v1|A Deterministic Global Optimization Method for Variational Inference|determinist global optim method variat infer|Variational inference methods for latent variable statistical models have gained popularity because they are relatively fast, can handle large data sets, and have deterministic convergence guarantees. However, in practice it is unclear whether the fixed point identified by the variational inference algorithm is a local or a global optimum. Here, we propose a method for constructing iterative optimization algorithms for variational inference problems that are guaranteed to converge to the $\epsilon$-global variational lower bound on the log-likelihood. We derive inference algorithms for two variational approximations to a standard Bayesian Gaussian mixture model (BGMM). We present a minimal data set for empirically testing convergence and show that a variational inference algorithm frequently converges to a local optimum while our algorithm always converges to the globally optimal variational lower bound. We characterize the loss incurred by choosing a non-optimal variational approximation distribution suggesting that selection of the approximating variational distribution deserves as much attention as the selection of the original statistical model for a given data set.|variat infer method latent variabl statist model gain popular becaus relat fast handl larg data set determinist converg guarante howev practic unclear whether fix point identifi variat infer algorithm local global optimum propos method construct iter optim algorithm variat infer problem guarante converg epsilon global variat lower bound log likelihood deriv infer algorithm two variat approxim standard bayesian gaussian mixtur model bgmm present minim data set empir test converg show variat infer algorithm frequent converg local optimum algorithm alway converg global optim variat lower bound character loss incur choos non optim variat approxim distribut suggest select approxim variat distribut deserv much attent select origin statist model given data set|['Hachem Saddiki', 'Andrew C. Trapp', 'Patrick Flaherty']|['stat.ME', 'stat.ML']
2017-03-28T14:02:06Z|2017-03-21T10:34:59Z|http://arxiv.org/abs/1703.07131v1|http://arxiv.org/pdf/1703.07131v1|Knowledge distillation using unlabeled mismatched images|knowledg distil use unlabel mismatch imag|Current approaches for Knowledge Distillation (KD) either directly use training data or sample from the training data distribution. In this paper, we demonstrate effectiveness of 'mismatched' unlabeled stimulus to perform KD for image classification networks. For illustration, we consider scenarios where this is a complete absence of training data, or mismatched stimulus has to be used for augmenting a small amount of training data. We demonstrate that stimulus complexity is a key factor for distillation's good performance. Our examples include use of various datasets for stimulating MNIST and CIFAR teachers.|current approach knowledg distil kd either direct use train data sampl train data distribut paper demonstr effect mismatch unlabel stimulus perform kd imag classif network illustr consid scenario complet absenc train data mismatch stimulus use augment small amount train data demonstr stimulus complex key factor distil good perform exampl includ use various dataset stimul mnist cifar teacher|['Mandar Kulkarni', 'Kalpesh Patil', 'Shirish Karande']|['cs.CV', 'cs.LG', 'stat.ML']
2017-03-28T14:02:06Z|2017-03-21T05:08:33Z|http://arxiv.org/abs/1703.07056v1|http://arxiv.org/pdf/1703.07056v1|Stochastic Primal Dual Coordinate Method with Non-Uniform Sampling Based   on Optimality Violations|stochast primal dual coordin method non uniform sampl base optim violat|We study primal-dual type stochastic optimization algorithms with non-uniform sampling. Our main theoretical contribution in this paper is to present a convergence analysis of Stochastic Primal Dual Coordinate (SPDC) Method with arbitrary sampling. Based on this theoretical framework, we propose Optimality Violation-based Sampling SPDC (ovsSPDC), a non-uniform sampling method based on Optimality Violation. We also propose two efficient heuristic variants of ovsSPDC called ovsSDPC+ and ovsSDPC++. Through intensive numerical experiments, we demonstrate that the proposed method and its variants are faster than other state-of-the-art primal-dual type stochastic optimization methods.|studi primal dual type stochast optim algorithm non uniform sampl main theoret contribut paper present converg analysi stochast primal dual coordin spdc method arbitrari sampl base theoret framework propos optim violat base sampl spdc ovsspdc non uniform sampl method base optim violat also propos two effici heurist variant ovsspdc call ovssdpc ovssdpc intens numer experi demonstr propos method variant faster state art primal dual type stochast optim method|['Atsushi Shibagaki', 'Ichiro Takeuchi']|['stat.ML', 'cs.LG', 'math.OC']
2017-03-28T14:02:06Z|2017-03-21T04:11:13Z|http://arxiv.org/abs/1703.07047v1|http://arxiv.org/pdf/1703.07047v1|High-Resolution Breast Cancer Screening with Multi-View Deep   Convolutional Neural Networks|high resolut breast cancer screen multi view deep convolut neural network|Recent advances in deep learning for object recognition in natural images has prompted a surge of interest in applying a similar set of techniques to medical images. Most of the initial attempts largely focused on replacing the input to such a deep convolutional neural network from a natural image to a medical image. This, however, does not take into consideration the fundamental differences between these two types of data. More specifically, detection or recognition of an anomaly in medical images depends significantly on fine details, unlike object recognition in natural images where coarser, more global structures matter more. This difference makes it inadequate to use the existing deep convolutional neural networks architectures, which were developed for natural images, because they rely on heavily downsampling an image to a much lower resolution to reduce the memory requirements. This hides details necessary to make accurate predictions for medical images. Furthermore, a single exam in medical imaging often comes with a set of different views which must be seamlessly fused in order to reach a correct conclusion. In our work, we propose to use a multi-view deep convolutional neural network that handles a set of more than one high-resolution medical image. We evaluate this network on large-scale mammography-based breast cancer screening (BI-RADS prediction) using 103 thousand images. We focus on investigating the impact of training set sizes and image sizes on the prediction accuracy. Our results highlight that performance clearly increases with the size of training set, and that the best performance can only be achieved using the images in the original resolution. This suggests the future direction of medical imaging research using deep neural networks is to utilize as much data as possible with the least amount of potentially harmful preprocessing.|recent advanc deep learn object recognit natur imag prompt surg interest appli similar set techniqu medic imag initi attempt larg focus replac input deep convolut neural network natur imag medic imag howev doe take consider fundament differ two type data specif detect recognit anomali medic imag depend signific fine detail unlik object recognit natur imag coarser global structur matter differ make inadequ use exist deep convolut neural network architectur develop natur imag becaus reli heavili downsampl imag much lower resolut reduc memori requir hide detail necessari make accur predict medic imag furthermor singl exam medic imag often come set differ view must seamless fuse order reach correct conclus work propos use multi view deep convolut neural network handl set one high resolut medic imag evalu network larg scale mammographi base breast cancer screen bi rad predict use thousand imag focus investig impact train set size imag size predict accuraci result highlight perform clear increas size train set best perform onli achiev use imag origin resolut suggest futur direct medic imag research use deep neural network util much data possibl least amount potenti harm preprocess|['Krzysztof J. Geras', 'Stacey Wolfson', 'S. Gene Kim', 'Linda Moy', 'Kyunghyun Cho']|['cs.CV', 'cs.LG', 'stat.ML']
2017-03-28T14:02:06Z|2017-03-21T02:08:05Z|http://arxiv.org/abs/1703.07027v1|http://arxiv.org/pdf/1703.07027v1|Nonparametric Variational Auto-encoders for Hierarchical Representation   Learning|nonparametr variat auto encod hierarch represent learn|The recently developed variational autoencoders (VAEs) have proved to be an effective confluence of the rich representational power of neural networks with Bayesian methods. However, most work on VAEs use a rather simple prior over the latent variables such as standard normal distribution, thereby restricting its applications to relatively simple phenomena. In this work, we propose hierarchical nonparametric variational autoencoders, which combines tree-structured Bayesian nonparametric priors with VAEs, to enable infinite flexibility of the latent representation space. Both the neural parameters and Bayesian priors are learned jointly using tailored variational inference. The resulting model induces a hierarchical structure of latent semantic concepts underlying the data corpus, and infers accurate representations of data instances. We apply our model in video representation learning. Our method is able to discover highly interpretable activity hierarchies, and obtain improved clustering accuracy and generalization capacity based on the learned rich representations.|recent develop variat autoencod vae prove effect confluenc rich represent power neural network bayesian method howev work vae use rather simpl prior latent variabl standard normal distribut therebi restrict applic relat simpl phenomena work propos hierarch nonparametr variat autoencod combin tree structur bayesian nonparametr prior vae enabl infinit flexibl latent represent space neural paramet bayesian prior learn joint use tailor variat infer result model induc hierarch structur latent semant concept data corpus infer accur represent data instanc appli model video represent learn method abl discov high interpret activ hierarchi obtain improv cluster accuraci general capac base learn rich represent|['Prasoon Goyal', 'Zhiting Hu', 'Xiaodan Liang', 'Chenyu Wang', 'Eric Xing']|['cs.LG', 'stat.ML']
2017-03-28T14:02:06Z|2017-03-21T02:04:30Z|http://arxiv.org/abs/1703.07026v1|http://arxiv.org/pdf/1703.07026v1|Cross-modal Deep Metric Learning with Multi-task Regularization|cross modal deep metric learn multi task regular|DNN-based cross-modal retrieval has become a research hotspot, by which users can search results across various modalities like image and text. However, existing methods mainly focus on the pairwise correlation and reconstruction error of labeled data. They ignore the semantically similar and dissimilar constraints between different modalities, and cannot take advantage of unlabeled data. This paper proposes Cross-modal Deep Metric Learning with Multi-task Regularization (CDMLMR), which integrates quadruplet ranking loss and semi-supervised contrastive loss for modeling cross-modal semantic similarity in a unified multi-task learning architecture. The quadruplet ranking loss can model the semantically similar and dissimilar constraints to preserve cross-modal relative similarity ranking information. The semi-supervised contrastive loss is able to maximize the semantic similarity on both labeled and unlabeled data. Compared to the existing methods, CDMLMR exploits not only the similarity ranking information but also unlabeled cross-modal data, and thus boosts cross-modal retrieval accuracy.|dnn base cross modal retriev becom research hotspot user search result across various modal like imag text howev exist method main focus pairwis correl reconstruct error label data ignor semant similar dissimilar constraint differ modal cannot take advantag unlabel data paper propos cross modal deep metric learn multi task regular cdmlmr integr quadruplet rank loss semi supervis contrast loss model cross modal semant similar unifi multi task learn architectur quadruplet rank loss model semant similar dissimilar constraint preserv cross modal relat similar rank inform semi supervis contrast loss abl maxim semant similar label unlabel data compar exist method cdmlmr exploit onli similar rank inform also unlabel cross modal data thus boost cross modal retriev accuraci|['Xin Huang', 'Yuxin Peng']|['cs.LG', 'cs.CV', 'stat.ML']
2017-03-28T14:02:09Z|2017-03-20T22:32:36Z|http://arxiv.org/abs/1703.06990v1|http://arxiv.org/pdf/1703.06990v1|Metalearning for Feature Selection|metalearn featur select|"A general formulation of optimization problems in which various candidate solutions may use different feature-sets is presented, encompassing supervised classification, automated program learning and other cases. A novel characterization of the concept of a ""good quality feature"" for such an optimization problem is provided; and a proposal regarding the integration of quality based feature selection into metalearning is suggested, wherein the quality of a feature for a problem is estimated using knowledge about related features in the context of related problems. Results are presented regarding extensive testing of this ""feature metalearning"" approach on supervised text classification problems; it is demonstrated that, in this context, feature metalearning can provide significant and sometimes dramatic speedup over standard feature selection heuristics."|general formul optim problem various candid solut may use differ featur set present encompass supervis classif autom program learn case novel character concept good qualiti featur optim problem provid propos regard integr qualiti base featur select metalearn suggest wherein qualiti featur problem estim use knowledg relat featur context relat problem result present regard extens test featur metalearn approach supervis text classif problem demonstr context featur metalearn provid signific sometim dramat speedup standard featur select heurist|['Ben Goertzel', 'Nil Geisweiller', 'Chris Poulin']|['cs.LG', 'stat.ML']
2017-03-28T14:02:09Z|2017-03-20T21:29:18Z|http://arxiv.org/abs/1703.06975v1|http://arxiv.org/pdf/1703.06975v1|Learning to Generate Samples from Noise through Infusion Training|learn generat sampl nois infus train|In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net|work investig novel train procedur learn generat model transit oper markov chain appli repeat unstructur random nois sampl denois sampl match target distribut train set novel train procedur learn progress denois oper involv sampl slight differ chain model chain use generat absenc denois target train chain infus inform train target exampl would like chain reach high probabl thus learn transit oper abl produc qualiti vari sampl small number step experi show competit result compar sampl generat basic generat adversari net|['Florian Bordes', 'Sina Honari', 'Pascal Vincent']|['stat.ML', 'cs.LG']
2017-03-28T14:02:09Z|2017-03-20T19:26:00Z|http://arxiv.org/abs/1703.06934v1|http://arxiv.org/pdf/1703.06934v1|Ensemble representation learning: an analysis of fitness and survival   for wrapper-based genetic programming methods|ensembl represent learn analysi fit surviv wrapper base genet program method|Recently we proposed a general, ensemble-based feature engineering wrapper (FEW) that was paired with a number of machine learning methods to solve regression problems. Here, we adapt FEW for supervised classification and perform a thorough analysis of fitness and survival methods within this framework. Our tests demonstrate that two fitness metrics, one introduced as an adaptation of the silhouette score, outperform the more commonly used Fisher criterion. We analyze survival methods and demonstrate that $\epsilon$-lexicase survival works best across our test problems, followed by random survival which outperforms both tournament and deterministic crowding. We conduct hyper-parameter optimization for several classification methods using a large set of problems to benchmark the ability of FEW to improve data representations. The results show that FEW can improve the best classifier performance on several problems. We show that FEW generates readable and meaningful features for a biomedical problem with different ML pairings.|recent propos general ensembl base featur engin wrapper pair number machin learn method solv regress problem adapt supervis classif perform thorough analysi fit surviv method within framework test demonstr two fit metric one introduc adapt silhouett score outperform common use fisher criterion analyz surviv method demonstr epsilon lexicas surviv work best across test problem follow random surviv outperform tournament determinist crowd conduct hyper paramet optim sever classif method use larg set problem benchmark abil improv data represent result show improv best classifi perform sever problem show generat readabl meaning featur biomed problem differ ml pair|['William La Cava', 'Jason H. Moore']|['cs.NE', 'cs.LG', 'stat.ML']
2017-03-28T14:02:09Z|2017-03-22T07:44:55Z|http://arxiv.org/abs/1703.06891v2|http://arxiv.org/pdf/1703.06891v2|Dance Dance Convolution|danc danc convolut|Dance Dance Revolution (DDR) is a popular rhythm-based video game. Players perform steps on a dance platform in synchronization with music as directed by on-screen step charts. While many step charts are available in standardized packs, users may grow tired of existing charts, or wish to dance to a song for which no chart exists. We introduce the task of learning to choreograph. Given a raw audio track, the goal is to produce a new step chart. This task decomposes naturally into two subtasks: deciding when to place steps and deciding which steps to select. For the step placement task, we combine recurrent and convolutional neural networks to ingest spectrograms of low-level audio features to predict steps, conditioned on chart difficulty. For step selection, we present a conditional LSTM generative model that substantially outperforms n-gram and fixed-window approaches.|danc danc revolut ddr popular rhythm base video game player perform step danc platform synchron music direct screen step chart mani step chart avail standard pack user may grow tire exist chart wish danc song chart exist introduc task learn choreograph given raw audio track goal produc new step chart task decompos natur two subtask decid place step decid step select step placement task combin recurr convolut neural network ingest spectrogram low level audio featur predict step condit chart difficulti step select present condit lstm generat model substanti outperform gram fix window approach|['Chris Donahue', 'Zachary C. Lipton', 'Julian McAuley']|['cs.LG', 'cs.MM', 'cs.NE', 'cs.SD', 'stat.ML']
2017-03-28T14:02:09Z|2017-03-20T17:21:19Z|http://arxiv.org/abs/1703.06857v1|http://arxiv.org/pdf/1703.06857v1|Deep Neural Networks Do Not Recognize Negative Images|deep neural network recogn negat imag|"Deep Neural Networks (DNNs) have achieved remarkable performance on a variety of pattern-recognition tasks, particularly visual classification problems, where new algorithms reported to achieve or even surpass the human performance. In this paper, we test the state-of-the-art DNNs with negative images and show that the accuracy drops to the level of random classification. This leads us to the conjecture that the DNNs, which are merely trained on raw data, do not recognize the semantics of the objects, but rather memorize the inputs. We suggest that negative images can be thought as ""semantic adversarial examples"", which we define as transformed inputs that semantically represent the same objects, but the model does not classify them correctly."|deep neural network dnns achiev remark perform varieti pattern recognit task particular visual classif problem new algorithm report achiev even surpass human perform paper test state art dnns negat imag show accuraci drop level random classif lead us conjectur dnns mere train raw data recogn semant object rather memor input suggest negat imag thought semant adversari exampl defin transform input semant repres object model doe classifi correct|['Hossein Hosseini', 'Radha Poovendran']|['cs.CV', 'cs.LG', 'stat.ML']
2017-03-28T14:02:09Z|2017-03-20T17:18:57Z|http://arxiv.org/abs/1703.06856v1|http://arxiv.org/pdf/1703.06856v1|Counterfactual Fairness|counterfactu fair|Machine learning has matured to the point to where it is now being considered to automate decisions in loan lending, employee hiring, and predictive policing. In many of these scenarios however, previous decisions have been made that are unfairly biased against certain subpopulations (e.g., those of a particular race, gender, or sexual orientation). Because this past data is often biased, machine learning predictors must account for this to avoid perpetuating discriminatory practices (or incidentally making new ones). In this paper, we develop a framework for modeling fairness in any dataset using tools from counterfactual inference. We propose a definition called counterfactual fairness that captures the intuition that a decision is fair towards an individual if it gives the same predictions in (a) the observed world and (b) a world where the individual had always belonged to a different demographic group, other background causes of the outcome being equal. We demonstrate our framework on two real-world problems: fair prediction of law school success, and fair modeling of an individual's criminality in policing data.|machin learn matur point consid autom decis loan lend employe hire predict polic mani scenario howev previous decis made unfair bias certain subpopul particular race gender sexual orient becaus past data often bias machin learn predictor must account avoid perpetu discriminatori practic incident make new one paper develop framework model fair ani dataset use tool counterfactu infer propos definit call counterfactu fair captur intuit decis fair toward individu give predict observ world world individu alway belong differ demograph group background caus outcom equal demonstr framework two real world problem fair predict law school success fair model individu crimin polic data|['Matt J. Kusner', 'Joshua R. Loftus', 'Chris Russell', 'Ricardo Silva']|['stat.ML', 'cs.CY', 'cs.LG']
2017-03-28T14:02:09Z|2017-03-20T15:43:10Z|http://arxiv.org/abs/1703.06807v1|http://arxiv.org/pdf/1703.06807v1|Variance Reduced Stochastic Gradient Descent with Sufficient Decrease|varianc reduc stochast gradient descent suffici decreas|The sufficient decrease technique has been widely used in deterministic optimization, even for non-convex optimization problems, such as line-search techniques. Motivated by those successes, we propose a novel sufficient decrease framework for a class of variance reduced stochastic gradient descent (VR-SGD) methods such as SVRG and SAGA. In order to make sufficient decrease for stochastic optimization, we design a new sufficient decrease criterion. We then introduce a coefficient \theta to satisfy the sufficient decrease property, which takes the decisions to shrink, expand or move in the opposite direction (i.e., \theta x for the variable x), and give two specific update rules for Lasso and ridge regression. Moreover, we analyze the convergence properties of our algorithms for strongly convex problems, which show that both of our algorithms attain linear convergence rates. We also provide the convergence guarantees of both of our algorithms for non-strongly convex problems. Our experimental results further verify that our algorithms achieve better performance than their counterparts.|suffici decreas techniqu wide use determinist optim even non convex optim problem line search techniqu motiv success propos novel suffici decreas framework class varianc reduc stochast gradient descent vr sgd method svrg saga order make suffici decreas stochast optim design new suffici decreas criterion introduc coeffici theta satisfi suffici decreas properti take decis shrink expand move opposit direct theta variabl give two specif updat rule lasso ridg regress moreov analyz converg properti algorithm strong convex problem show algorithm attain linear converg rate also provid converg guarante algorithm non strong convex problem experiment result verifi algorithm achiev better perform counterpart|['Fanhua Shang', 'Yuanyuan Liu', 'James Cheng', 'Kelvin Kai Wing Ng', 'Yuichi Yoshida']|['cs.LG', 'math.OC', 'stat.ML']
2017-03-28T14:02:09Z|2017-03-20T14:42:27Z|http://arxiv.org/abs/1703.06777v1|http://arxiv.org/pdf/1703.06777v1|On the Use of Default Parameter Settings in the Empirical Evaluation of   Classification Algorithms|use default paramet set empir evalu classif algorithm|We demonstrate that, for a range of state-of-the-art machine learning algorithms, the differences in generalisation performance obtained using default parameter settings and using parameters tuned via cross-validation can be similar in magnitude to the differences in performance observed between state-of-the-art and uncompetitive learning systems. This means that fair and rigorous evaluation of new learning algorithms requires performance comparison against benchmark methods with best-practice model selection procedures, rather than using default parameter settings. We investigate the sensitivity of three key machine learning algorithms (support vector machine, random forest and rotation forest) to their default parameter settings, and provide guidance on determining sensible default parameter values for implementations of these algorithms. We also conduct an experimental comparison of these three algorithms on 121 classification problems and find that, perhaps surprisingly, rotation forest is significantly more accurate on average than both random forest and a support vector machine.|demonstr rang state art machin learn algorithm differ generalis perform obtain use default paramet set use paramet tune via cross valid similar magnitud differ perform observ state art uncompetit learn system mean fair rigor evalu new learn algorithm requir perform comparison benchmark method best practic model select procedur rather use default paramet set investig sensit three key machin learn algorithm support vector machin random forest rotat forest default paramet set provid guidanc determin sensibl default paramet valu implement algorithm also conduct experiment comparison three algorithm classif problem find perhap surpris rotat forest signific accur averag random forest support vector machin|['Anthony Bagnall', 'Gavin C. Cawley']|['cs.LG', 'stat.ML']
2017-03-28T14:02:09Z|2017-03-20T14:02:11Z|http://arxiv.org/abs/1703.06749v1|http://arxiv.org/pdf/1703.06749v1|Efficient variational Bayesian neural network ensembles for outlier   detection|effici variat bayesian neural network ensembl outlier detect|In this work we perform outlier detection using ensembles of neural networks obtained by variational approximation of the posterior in a Bayesian neural network setting. The variational parameters are obtained by sampling from the true posterior by gradient descent. We show our outlier detection results are better than those obtained using other efficient ensembling methods.|work perform outlier detect use ensembl neural network obtain variat approxim posterior bayesian neural network set variat paramet obtain sampl true posterior gradient descent show outlier detect result better obtain use effici ensembl method|['Nick Pawlowski', 'Miguel Jaques', 'Ben Glocker']|['stat.ML', 'cs.LG']
2017-03-28T14:02:09Z|2017-03-20T12:09:53Z|http://arxiv.org/abs/1703.06700v1|http://arxiv.org/pdf/1703.06700v1|Independence clustering (without a matrix)|independ cluster without matrix|The independence clustering problem is considered in the following formulation: given a set $S$ of random variables, it is required to find the finest partitioning $\{U_1,\dots,U_k\}$ of $S$ into clusters such that the clusters $U_1,\dots,U_k$ are mutually independent. Since mutual independence is the target, pairwise similarity measurements are of no use, and thus traditional clustering algorithms are inapplicable. The distribution of the random variables in $S$ is, in general, unknown, but a sample is available. Thus, the problem is cast in terms of time series. Two forms of sampling are considered: i.i.d.\ and stationary time series, with the main emphasis being on the latter, more general, case. A consistent, computationally tractable algorithm for each of the settings is proposed, and a number of open directions for further research are outlined.|independ cluster problem consid follow formul given set random variabl requir find finest partit dot cluster cluster dot mutual independ sinc mutual independ target pairwis similar measur use thus tradit cluster algorithm inapplic distribut random variabl general unknown sampl avail thus problem cast term time seri two form sampl consid stationari time seri main emphasi latter general case consist comput tractabl algorithm set propos number open direct research outlin|['Daniil Ryabko']|['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']
2017-03-28T14:02:14Z|2017-03-20T11:44:00Z|http://arxiv.org/abs/1703.06692v1|http://arxiv.org/pdf/1703.06692v1|QMDP-Net: Deep Learning for Planning under Partial Observability|qmdp net deep learn plan partial observ|This paper introduces QMDP-net, a neural network architecture for planning under partial observability. The QMDP-net combines the strengths of model-free learning and model-based planning. It is a recurrent policy network, but it represents a policy by connecting a model with a planning algorithm that solves the model, thus embedding the solution structure of planning in the network architecture. The QMDP-net is fully differentiable and allows end-to-end training. We train a QMDP-net over a set of different environments so that it can generalize over new ones. In preliminary experiments, QMDP-net showed strong performance on several robotic tasks in simulation. Interestingly, it also sometimes outperformed the QMDP algorithm, which generated the data for learning, because of QMDP-net's robustness resulting from end-to-end learning.|paper introduc qmdp net neural network architectur plan partial observ qmdp net combin strength model free learn model base plan recurr polici network repres polici connect model plan algorithm solv model thus embed solut structur plan network architectur qmdp net fulli differenti allow end end train train qmdp net set differ environ general new one preliminari experi qmdp net show strong perform sever robot task simul interest also sometim outperform qmdp algorithm generat data learn becaus qmdp net robust result end end learn|['Peter Karkus', 'David Hsu', 'Wee Sun Lee']|['cs.AI', 'cs.LG', 'cs.NE', 'stat.ML']
2017-03-28T14:02:14Z|2017-03-20T11:26:04Z|http://arxiv.org/abs/1703.06686v1|http://arxiv.org/pdf/1703.06686v1|Copula Index for Detecting Dependence and Monotonicity between   Stochastic Signals|copula index detect depend monoton stochast signal|This paper introduces a nonparametric copula-based approach for detecting the strength and monotonicity of linear and nonlinear statistical dependence between bivariate continuous, discrete or hybrid random variables and stochastic signals, termed CIM. We show that CIM satisfies the data processing inequality and is consequently a self-equitable metric. Simulation results using synthetic datasets reveal that the CIM compares favorably to other state-of-the-art statistical dependency metrics, including the Maximal Information Coefficient (MIC), Randomized Dependency Coefficient (RDC), distance Correlation (dCor), Copula correlation (Ccor), and Copula Statistic (CoS) in both statistical power and sample size requirements. Simulations using real world data highlight the importance of understanding the monotonicity of the dependence structure.|paper introduc nonparametr copula base approach detect strength monoton linear nonlinear statist depend bivari continu discret hybrid random variabl stochast signal term cim show cim satisfi data process inequ consequ self equit metric simul result use synthet dataset reveal cim compar favor state art statist depend metric includ maxim inform coeffici mic random depend coeffici rdc distanc correl dcor copula correl ccor copula statist cos statist power sampl size requir simul use real world data highlight import understand monoton depend structur|['Kiran Karra', 'Lamine Mili']|['stat.ML', 'q-bio.QM']
2017-03-28T14:02:14Z|2017-03-19T23:28:39Z|http://arxiv.org/abs/1703.06537v1|http://arxiv.org/pdf/1703.06537v1|A Controlled Set-Up Experiment to Establish Personalized Baselines for   Real-Life Emotion Recognition|control set experi establish person baselin real life emot recognit|We design, conduct and present the results of a highly personalized baseline emotion recognition experiment, which aims to set reliable ground-truth estimates for the subject's emotional state for real-life prediction under similar conditions using a small number of physiological sensors. We also propose an adaptive stimuli-selection mechanism that would use the user's feedback as guide for future stimuli selection in the controlled-setup experiment and generate optimal ground-truth personalized sessions systematically. Initial results are very promising (85% accuracy) and variable importance analysis shows that only a few features, which are easy-to-implement in portable devices, would suffice to predict the subject's emotional state.|design conduct present result high person baselin emot recognit experi aim set reliabl ground truth estim subject emot state real life predict similar condit use small number physiolog sensor also propos adapt stimuli select mechan would use user feedback guid futur stimuli select control setup experi generat optim ground truth person session systemat initi result veri promis accuraci variabl import analysi show onli featur easi implement portabl devic would suffic predict subject emot state|['Varvara Kollia', 'Noureddine Tayebi']|['stat.ML', 'cs.HC']
2017-03-28T14:02:14Z|2017-03-19T22:23:01Z|http://arxiv.org/abs/1703.06528v1|http://arxiv.org/pdf/1703.06528v1|Universal Consistency and Robustness of Localized Support Vector   Machines|univers consist robust local support vector machin|The massive amount of available data potentially used to discover patters in machine learning is a challenge for kernel based algorithms with respect to runtime and storage capacities. Local approaches might help to relieve these issues. From a statistical point of view local approaches allow additionally to deal with different structures in the data in different ways. This paper analyses properties of localized kernel based, non-parametric statistical machine learning methods, in particular of support vector machines (SVMs) and methods close to them. We will show there that locally learnt kernel methods are universal consistent. Furthermore, we give an upper bound for the maxbias in order to show statistical robustness of the proposed method.|massiv amount avail data potenti use discov patter machin learn challeng kernel base algorithm respect runtim storag capac local approach might help reliev issu statist point view local approach allow addit deal differ structur data differ way paper analys properti local kernel base non parametr statist machin learn method particular support vector machin svms method close show local learnt kernel method univers consist furthermor give upper bound maxbia order show statist robust propos method|['Florian Dumpert']|['stat.ML', '62G08, 62G20, 62G35']
2017-03-28T14:02:14Z|2017-03-19T21:06:51Z|http://arxiv.org/abs/1703.06513v1|http://arxiv.org/pdf/1703.06513v1|Bernoulli Rank-$1$ Bandits for Click Feedback|bernoulli rank bandit click feedback|"The probability that a user will click a search result depends both on its relevance and its position on the results page. The position based model explains this behavior by ascribing to every item an attraction probability, and to every position an examination probability. To be clicked, a result must be both attractive and examined. The probabilities of an item-position pair being clicked thus form the entries of a rank-$1$ matrix. We propose the learning problem of a Bernoulli rank-$1$ bandit where at each step, the learning agent chooses a pair of row and column arms, and receives the product of their Bernoulli-distributed values as a reward. This is a special case of the stochastic rank-$1$ bandit problem considered in recent work that proposed an elimination based algorithm Rank1Elim, and showed that Rank1Elim's regret scales linearly with the number of rows and columns on ""benign"" instances. These are the instances where the minimum of the average row and column rewards $\mu$ is bounded away from zero. The issue with Rank1Elim is that it fails to be competitive with straightforward bandit strategies as $\mu \rightarrow 0$. In this paper we propose Rank1ElimKL which simply replaces the (crude) confidence intervals of Rank1Elim with confidence intervals based on Kullback-Leibler (KL) divergences, and with the help of a novel result concerning the scaling of KL divergences we prove that with this change, our algorithm will be competitive no matter the value of $\mu$. Experiments with synthetic data confirm that on benign instances the performance of Rank1ElimKL is significantly better than that of even Rank1Elim, while experiments with models derived from real data confirm that the improvements are significant across the board, regardless of whether the data is benign or not."|probabl user click search result depend relev posit result page posit base model explain behavior ascrib everi item attract probabl everi posit examin probabl click result must attract examin probabl item posit pair click thus form entri rank matrix propos learn problem bernoulli rank bandit step learn agent choos pair row column arm receiv product bernoulli distribut valu reward special case stochast rank bandit problem consid recent work propos elimin base algorithm rankelim show rankelim regret scale linear number row column benign instanc instanc minimum averag row column reward mu bound away zero issu rankelim fail competit straightforward bandit strategi mu rightarrow paper propos rankelimkl simpli replac crude confid interv rankelim confid interv base kullback leibler kl diverg help novel result concern scale kl diverg prove chang algorithm competit matter valu mu experi synthet data confirm benign instanc perform rankelimkl signific better even rankelim experi model deriv real data confirm improv signific across board regardless whether data benign|['Sumeet Katariya', 'Branislav Kveton', 'Csaba Szepesvári', 'Claire Vernade', 'Zheng Wen']|['cs.LG', 'stat.ML']
2017-03-28T14:02:14Z|2017-03-19T17:45:29Z|http://arxiv.org/abs/1703.06476v1|http://arxiv.org/pdf/1703.06476v1|Practical Coreset Constructions for Machine Learning|practic coreset construct machin learn|We investigate coresets - succinct, small summaries of large data sets - so that solutions found on the summary are provably competitive with solution found on the full data set. We provide an overview over the state-of-the-art in coreset construction for machine learning. In Section 2, we present both the intuition behind and a theoretically sound framework to construct coresets for general problems and apply it to $k$-means clustering. In Section 3 we summarize existing coreset construction algorithms for a variety of machine learning problems such as maximum likelihood estimation of mixture models, Bayesian non-parametric models, principal component analysis, regression and general empirical risk minimization.|investig coreset succinct small summari larg data set solut found summari provabl competit solut found full data set provid overview state art coreset construct machin learn section present intuit behind theoret sound framework construct coreset general problem appli mean cluster section summar exist coreset construct algorithm varieti machin learn problem maximum likelihood estim mixtur model bayesian non parametr model princip compon analysi regress general empir risk minim|['Olivier Bachem', 'Mario Lucic', 'Andreas Krause']|['stat.ML']
2017-03-28T14:02:14Z|2017-03-18T18:12:17Z|http://arxiv.org/abs/1703.06327v1|http://arxiv.org/pdf/1703.06327v1|Spectrum Estimation from a Few Entries|spectrum estim entri|Singular values of a data in a matrix form provide insights on the structure of the data, the effective dimensionality, and the choice of hyper-parameters on higher-level data analysis tools. However, in many practical applications such as collaborative filtering and network analysis, we only get a partial observation. Under such scenarios, we consider the fundamental problem of recovering spectral properties of the underlying matrix from a sampling of its entries. We are particularly interested in directly recovering the spectrum, which is the set of singular values, and also in sample-efficient approaches for recovering a spectral sum function, which is an aggregate sum of the same function applied to each of the singular values. We propose first estimating the Schatten $k$-norms of a matrix, and then applying Chebyshev approximation to the spectral sum function or applying moment matching in Wasserstein distance to recover the singular values. The main technical challenge is in accurately estimating the Schatten norms from a sampling of a matrix. We introduce a novel unbiased estimator based on counting small structures in a graph and provide guarantees that match its empirical performance. Our theoretical analysis shows that Schatten norms can be recovered accurately from strictly smaller number of samples compared to what is needed to recover the underlying low-rank matrix. Numerical experiments suggest that we significantly improve upon a competing approach of using matrix completion methods.|singular valu data matrix form provid insight structur data effect dimension choic hyper paramet higher level data analysi tool howev mani practic applic collabor filter network analysi onli get partial observ scenario consid fundament problem recov spectral properti matrix sampl entri particular interest direct recov spectrum set singular valu also sampl effici approach recov spectral sum function aggreg sum function appli singular valu propos first estim schatten norm matrix appli chebyshev approxim spectral sum function appli moment match wasserstein distanc recov singular valu main technic challeng accur estim schatten norm sampl matrix introduc novel unbias estim base count small structur graph provid guarante match empir perform theoret analysi show schatten norm recov accur strict smaller number sampl compar need recov low rank matrix numer experi suggest signific improv upon compet approach use matrix complet method|['Ashish Khetan', 'Sewoong Oh']|['stat.ML', 'cs.DS', 'cs.LG', 'cs.NA']
2017-03-28T14:02:14Z|2017-03-18T17:49:42Z|http://arxiv.org/abs/1703.06324v1|http://arxiv.org/pdf/1703.06324v1|Deep Tensor Encoding|deep tensor encod|Learning an encoding of feature vectors in terms of an over-complete dictionary or a probabilistic information geometric (Fisher vectors) construct is wide-spread in statistical signal processing and computer vision. In content based information retrieval using deep-learning classifiers, such encodings are learnt on the flattened last layer, without adherence to the multi-linear structure of the underlying feature tensor. We illustrate a variety of feature encodings incl. sparse dictionary coding and Fisher vectors along with proposing that a structured tensor factorization scheme enables us to perform retrieval that is at par, in terms of average precision, with Fisher vector encoded image signatures. In short, we illustrate how structural constraints increase retrieval fidelity.|learn encod featur vector term complet dictionari probabilist inform geometr fisher vector construct wide spread statist signal process comput vision content base inform retriev use deep learn classifi encod learnt flatten last layer without adher multi linear structur featur tensor illustr varieti featur encod incl spars dictionari code fisher vector along propos structur tensor factor scheme enabl us perform retriev par term averag precis fisher vector encod imag signatur short illustr structur constraint increas retriev fidel|['B Sengupta', 'E Vasquez', 'Y Qian']|['cs.IR', 'cs.LG', 'stat.ML']
2017-03-28T14:02:14Z|2017-03-18T08:38:51Z|http://arxiv.org/abs/1703.06272v1|http://arxiv.org/pdf/1703.06272v1|An Automated Auto-encoder Correlation-based Health-Monitoring and   Prognostic Method for Machine Bearings|autom auto encod correl base health monitor prognost method machin bear|This paper studies an intelligent ultimate technique for health-monitoring and prognostic of common rotary machine components, particularly bearings. During a run-to-failure experiment, rich unsupervised features from vibration sensory data are extracted by a trained sparse auto-encoder. Then, the correlation of the extracted attributes of the initial samples (presumably healthy at the beginning of the test) with the succeeding samples is calculated and passed through a moving-average filter. The normalized output is named auto-encoder correlation-based (AEC) rate which stands for an informative attribute of the system depicting its health status and precisely identifying the degradation starting point. We show that AEC technique well-generalizes in several run-to-failure tests. AEC collects rich unsupervised features form the vibration data fully autonomous. We demonstrate the superiority of the AEC over many other state-of-the-art approaches for the health monitoring and prognostic of machine bearings.|paper studi intellig ultim techniqu health monitor prognost common rotari machin compon particular bear dure run failur experi rich unsupervis featur vibrat sensori data extract train spars auto encod correl extract attribut initi sampl presum healthi begin test succeed sampl calcul pass move averag filter normal output name auto encod correl base aec rate stand inform attribut system depict health status precis identifi degrad start point show aec techniqu well general sever run failur test aec collect rich unsupervis featur form vibrat data fulli autonom demonstr superior aec mani state art approach health monitor prognost machin bear|['Ramin M. Hasani', 'Guodong Wang', 'Radu Grosu']|['cs.LG', 'cs.NE', 'stat.ML']
2017-03-28T14:02:14Z|2017-03-25T13:19:14Z|http://arxiv.org/abs/1703.06270v3|http://arxiv.org/pdf/1703.06270v3|SIM-CE: An Advanced Simulink Platform for Studying the Brain of   Caenorhabditis elegans|sim ce advanc simulink platform studi brain caenorhabd elegan|We introduce SIM-CE, an advanced, user-friendly modeling and simulation environment in Simulink for performing multi-scale behavioral analysis of the nervous system of Caenorhabditis elegans (C. elegans). SIM-CE contains an implementation of the mathematical models of C. elegans's neurons and synapses, in Simulink, which can be easily extended and particularized by the user. The Simulink model is able to capture both complex dynamics of ion channels and additional biophysical detail such as intracellular calcium concentration. We demonstrate the performance of SIM-CE by carrying out neuronal, synaptic and neural-circuit-level behavioral simulations. Such environment enables the user to capture unknown properties of the neural circuits, test hypotheses and determine the origin of many behavioral plasticities exhibited by the worm.|introduc sim ce advanc user friend model simul environ simulink perform multi scale behavior analysi nervous system caenorhabd elegan elegan sim ce contain implement mathemat model elegan neuron synaps simulink easili extend particular user simulink model abl captur complex dynam ion channel addit biophys detail intracellular calcium concentr demonstr perform sim ce carri neuron synapt neural circuit level behavior simul environ enabl user captur unknown properti neural circuit test hypothes determin origin mani behavior plastic exhibit worm|['Ramin M. Hasani', 'Victoria Beneder', 'Magdalena Fuchs', 'David Lung', 'Radu Grosu']|['q-bio.NC', 'cs.NE', 'q-bio.QM', 'stat.ML']
2017-03-28T14:02:18Z|2017-03-18T03:28:40Z|http://arxiv.org/abs/1703.06240v1|http://arxiv.org/pdf/1703.06240v1|Multi-fidelity Bayesian Optimisation with Continuous Approximations|multi fidel bayesian optimis continu approxim|Bandit methods for black-box optimisation, such as Bayesian optimisation, are used in a variety of applications including hyper-parameter tuning and experiment design. Recently, \emph{multi-fidelity} methods have garnered considerable attention since function evaluations have become increasingly expensive in such applications. Multi-fidelity methods use cheap approximations to the function of interest to speed up the overall optimisation process. However, most multi-fidelity methods assume only a finite number of approximations. In many practical applications however, a continuous spectrum of approximations might be available. For instance, when tuning an expensive neural network, one might choose to approximate the cross validation performance using less data $N$ and/or few training iterations $T$. Here, the approximations are best viewed as arising out of a continuous two dimensional space $(N,T)$. In this work, we develop a Bayesian optimisation method, BOCA, for this setting. We characterise its theoretical properties and show that it achieves better regret than than strategies which ignore the approximations. BOCA outperforms several other baselines in synthetic and real experiments.|bandit method black box optimis bayesian optimis use varieti applic includ hyper paramet tune experi design recent emph multi fidel method garner consider attent sinc function evalu becom increas expens applic multi fidel method use cheap approxim function interest speed overal optimis process howev multi fidel method assum onli finit number approxim mani practic applic howev continu spectrum approxim might avail instanc tune expens neural network one might choos approxim cross valid perform use less data train iter approxim best view aris continu two dimension space work develop bayesian optimis method boca set characteris theoret properti show achiev better regret strategi ignor approxim boca outperform sever baselin synthet real experi|['Kirthevasan Kandasamy', 'Gautam Dasarathy', 'Jeff Schneider', 'Barnabas Poczos']|['stat.ML']
2017-03-28T14:02:18Z|2017-03-18T00:59:40Z|http://arxiv.org/abs/1703.06229v1|http://arxiv.org/pdf/1703.06229v1|Curriculum Dropout|curriculum dropout|"Dropout is a very effective way of regularizing neural networks. Stochastically ""dropping out"" units with a certain probability discourages over-specific co-adaptations of feature detectors, preventing overfitting and improving network generalization. Besides, Dropout can be interpreted as an approximate model aggregation technique, where an exponential number of smaller networks are averaged in order to get a more powerful ensemble. In this paper, we show that using a fixed dropout probability during training is a suboptimal choice. We thus propose a time scheduling for the probability of retaining neurons in the network. This induces an adaptive regularization scheme that smoothly increases the difficulty of the optimization problem. This idea of ""starting easy"" and adaptively increasing the difficulty of the learning problem has its roots in curriculum learning and allows one to train better models. Indeed, we prove that our optimization strategy implements a very general curriculum scheme, by gradually adding noise to both the input and intermediate feature representations within the network architecture. Experiments on seven image classification datasets and different network architectures show that our method, named Curriculum Dropout, frequently yields to better generalization and, at worst, performs just as well as the standard Dropout method."|dropout veri effect way regular neural network stochast drop unit certain probabl discourag specif co adapt featur detector prevent overfit improv network general besid dropout interpret approxim model aggreg techniqu exponenti number smaller network averag order get power ensembl paper show use fix dropout probabl dure train suboptim choic thus propos time schedul probabl retain neuron network induc adapt regular scheme smooth increas difficulti optim problem idea start easi adapt increas difficulti learn problem root curriculum learn allow one train better model inde prove optim strategi implement veri general curriculum scheme gradual ad nois input intermedi featur represent within network architectur experi seven imag classif dataset differ network architectur show method name curriculum dropout frequent yield better general worst perform well standard dropout method|['Pietro Morerio', 'Jacopo Cavazza', 'Riccardo Volpi', 'Rene Vidal', 'Vittorio Murino']|['cs.NE', 'cs.LG', 'stat.ML']
2017-03-28T14:02:18Z|2017-03-18T00:08:59Z|http://arxiv.org/abs/1703.06222v1|http://arxiv.org/pdf/1703.06222v1|A Unified Treatment of Multiple Testing with Prior Knowledge|unifi treatment multipl test prior knowledg|A significant literature has arisen to study ways to employing prior knowledge to improve power and precision of multiple testing procedures. Some common forms of prior knowledge may include (a) a priori beliefs about which hypotheses are null, modeled by non-uniform prior weights; (b) differing importances of hypotheses, modeled by differing penalties for false discoveries; (c) partitions of the hypotheses into known groups, indicating (dis)similarity of hypotheses; and (d) knowledge of independence, positive dependence or arbitrary dependence between hypotheses or groups, allowing for more aggressive or conservative procedures. We present a general framework for global null testing and false discovery rate (FDR) control that allows the scientist to incorporate all four types of prior knowledge (a)-(d) simultaneously. We unify a number of existing procedures, generalize the conditions under which they are known to work, and simplify their proofs of FDR control under independence, positive and arbitrary dependence. We also present an algorithmic framework that strictly generalizes and unifies the classic algorithms of Benjamini and Hochberg [3] and Simes [25], algorithms that guard against unknown dependence [7, 9], algorithms that employ prior weights [17, 15], algorithms that use penalty weights [4], algorithms that incorporate null-proportion adaptivity [26, 27], and algorithms that make use of multiple arbitrary partitions into groups [1]. Unlike this previous work, we can simultaneously incorporate all of the four types of prior knowledge, combined with all of the three forms of dependence.|signific literatur arisen studi way employ prior knowledg improv power precis multipl test procedur common form prior knowledg may includ priori belief hypothes null model non uniform prior weight differ import hypothes model differ penalti fals discoveri partit hypothes known group indic dis similar hypothes knowledg independ posit depend arbitrari depend hypothes group allow aggress conserv procedur present general framework global null test fals discoveri rate fdr control allow scientist incorpor four type prior knowledg simultan unifi number exist procedur general condit known work simplifi proof fdr control independ posit arbitrari depend also present algorithm framework strict general unifi classic algorithm benjamini hochberg sime algorithm guard unknown depend algorithm employ prior weight algorithm use penalti weight algorithm incorpor null proport adapt algorithm make use multipl arbitrari partit group unlik previous work simultan incorpor four type prior knowledg combin three form depend|['Aaditya Ramdas', 'Rina Foygel Barber', 'Martin J. Wainwright', 'Michael I. Jordan']|['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']
2017-03-28T14:02:18Z|2017-03-17T23:52:14Z|http://arxiv.org/abs/1703.06217v1|http://arxiv.org/pdf/1703.06217v1|Deciding How to Decide: Dynamic Routing in Artificial Neural Networks|decid decid dynam rout artifici neural network|We propose and systematically evaluate three strategies for training dynamically-routed artificial neural networks: graphs of learned transformations through which different input signals may take different paths. Though some approaches have advantages over others, the resulting networks are often qualitatively similar. We find that, in dynamically-routed networks trained to classify images, layers and branches become specialized to process distinct categories of images. Additionally, given a fixed computational budget, dynamically-routed networks tend to perform better than comparable statically-routed networks.|propos systemat evalu three strategi train dynam rout artifici neural network graph learn transform differ input signal may take differ path though approach advantag result network often qualit similar find dynam rout network train classifi imag layer branch becom special process distinct categori imag addit given fix comput budget dynam rout network tend perform better compar static rout network|['Mason McGill', 'Pietro Perona']|['stat.ML', 'cs.CV', 'cs.LG', 'cs.NE']
2017-03-28T14:02:18Z|2017-03-17T19:24:09Z|http://arxiv.org/abs/1703.06177v1|http://arxiv.org/pdf/1703.06177v1|On Consistency of Graph-based Semi-supervised Learning|consist graph base semi supervis learn|Graph-based semi-supervised learning is one of the most popular methods in machine learning. Some of its theoretical properties such as bounds for the generalization error and the convergence of the graph Laplacian regularizer have been studied in computer science and statistics literatures. However, a fundamental statistical property, the consistency of the estimator from this method has not been proved. In this article, we study the consistency problem under a non-parametric framework. We prove the consistency of graph-based learning in the case that the estimated scores are enforced to be equal to the observed responses for the labeled data. The sample sizes of both labeled and unlabeled data are allowed to grow in this result. When the estimated scores are not required to be equal to the observed responses, a tuning parameter is used to balance the loss function and the graph Laplacian regularizer. We give a counterexample demonstrating that the estimator for this case can be inconsistent. The theoretical findings are supported by numerical studies.|graph base semi supervis learn one popular method machin learn theoret properti bound general error converg graph laplacian regular studi comput scienc statist literatur howev fundament statist properti consist estim method prove articl studi consist problem non parametr framework prove consist graph base learn case estim score enforc equal observ respons label data sampl size label unlabel data allow grow result estim score requir equal observ respons tune paramet use balanc loss function graph laplacian regular give counterexampl demonstr estim case inconsist theoret find support numer studi|['Chengan Du', 'Yunpeng Zhao']|['stat.ML']
2017-03-28T14:02:18Z|2017-03-17T17:50:44Z|http://arxiv.org/abs/1703.06131v1|http://arxiv.org/pdf/1703.06131v1|Inference via low-dimensional couplings|infer via low dimension coupl|"Integration against an intractable probability measure is among the fundamental challenges of statistical inference, particularly in the Bayesian setting. A principled approach to this problem seeks a deterministic coupling of the measure of interest with a tractable ""reference"" measure (e.g., a standard Gaussian). This coupling is induced by a transport map, and enables direct simulation from the desired measure simply by evaluating the transport map at samples from the reference. Yet characterizing such a map---e.g., representing and evaluating it---grows challenging in high dimensions. The central contribution of this paper is to establish a link between the Markov properties of the target measure and the existence of certain low-dimensional couplings, induced by transport maps that are sparse or decomposable. Our analysis not only facilitates the construction of couplings in high-dimensional settings, but also suggests new inference methodologies. For instance, in the context of nonlinear and non-Gaussian state space models, we describe new online and single-pass variational algorithms that characterize the full posterior distribution of the sequential inference problem using operations only slightly more complex than regular filtering."|integr intract probabl measur among fundament challeng statist infer particular bayesian set principl approach problem seek determinist coupl measur interest tractabl refer measur standard gaussian coupl induc transport map enabl direct simul desir measur simpli evalu transport map sampl refer yet character map repres evalu grow challeng high dimens central contribut paper establish link markov properti target measur exist certain low dimension coupl induc transport map spars decompos analysi onli facilit construct coupl high dimension set also suggest new infer methodolog instanc context nonlinear non gaussian state space model describ new onlin singl pass variat algorithm character full posterior distribut sequenti infer problem use oper onli slight complex regular filter|['Alessio Spantini', 'Daniele Bigoni', 'Youssef Marzouk']|['stat.ME', 'stat.CO', 'stat.ML']
2017-03-28T14:02:18Z|2017-03-17T17:09:15Z|http://arxiv.org/abs/1703.06104v1|http://arxiv.org/pdf/1703.06104v1|Nonconvex One-bit Single-label Multi-label Learning|nonconvex one bit singl label multi label learn|We study an extreme scenario in multi-label learning where each training instance is endowed with a single one-bit label out of multiple labels. We formulate this problem as a non-trivial special case of one-bit rank-one matrix sensing and develop an efficient non-convex algorithm based on alternating power iteration. The proposed algorithm is able to recover the underlying low-rank matrix model with linear convergence. For a rank-$k$ model with $d_1$ features and $d_2$ classes, the proposed algorithm achieves $O(\epsilon)$ recovery error after retrieving $O(k^{1.5}d_1 d_2/\epsilon)$ one-bit labels within $O(kd)$ memory. Our bound is nearly optimal in the order of $O(1/\epsilon)$. This significantly improves the state-of-the-art sampling complexity of one-bit multi-label learning. We perform experiments to verify our theory and evaluate the performance of the proposed algorithm.|studi extrem scenario multi label learn train instanc endow singl one bit label multipl label formul problem non trivial special case one bit rank one matrix sens develop effici non convex algorithm base altern power iter propos algorithm abl recov low rank matrix model linear converg rank model featur class propos algorithm achiev epsilon recoveri error retriev epsilon one bit label within kd memori bound near optim order epsilon signific improv state art sampl complex one bit multi label learn perform experi verifi theori evalu perform propos algorithm|['Shuang Qiu', 'Tingjin Luo', 'Jieping Ye', 'Ming Lin']|['stat.ML', 'cs.LG']
2017-03-28T14:02:18Z|2017-03-17T17:09:14Z|http://arxiv.org/abs/1703.06103v1|http://arxiv.org/pdf/1703.06103v1|Modeling Relational Data with Graph Convolutional Networks|model relat data graph convolut network|Knowledge bases play a crucial role in many applications, for example question answering and information retrieval. Despite the great effort invested in creating and maintaining them, even the largest representatives (e.g., Yago, DBPedia or Wikidata) are highly incomplete. We introduce relational graph convolutional networks (R-GCNs) and apply them to two standard knowledge base completion tasks: link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing attributes of entities). R-GCNs are a generalization of graph convolutional networks, a recent class of neural networks operating on graphs, and are developed specifically to deal with highly multi-relational data, characteristic of realistic knowledge bases. Our methods achieve competitive results on standard benchmarks for both tasks.|knowledg base play crucial role mani applic exampl question answer inform retriev despit great effort invest creat maintain even largest repres yago dbpedia wikidata high incomplet introduc relat graph convolut network gcns appli two standard knowledg base complet task link predict recoveri miss fact subject predic object tripl entiti classif recoveri miss attribut entiti gcns general graph convolut network recent class neural network oper graph develop specif deal high multi relat data characterist realist knowledg base method achiev competit result standard benchmark task|['Michael Schlichtkrull', 'Thomas N. Kipf', 'Peter Bloem', 'Rianne van den Berg', 'Ivan Titov', 'Max Welling']|['stat.ML', 'cs.AI', 'cs.DB', 'cs.LG']
2017-03-28T14:02:18Z|2017-03-17T16:08:23Z|http://arxiv.org/abs/1703.06065v1|http://arxiv.org/pdf/1703.06065v1|Block CUR : Decomposing Large Distributed Matrices|block cur decompos larg distribut matric|A common problem in large-scale data analysis is to approximate a matrix using a combination of specifically sampled rows and columns, known as CUR decomposition. Unfortunately, in many real-world environments, the ability to sample specific individual rows or columns of the matrix is limited by either system constraints or cost. In this paper, we consider matrix approximation by sampling predefined blocks of columns (or rows) from the matrix. This regime is commonly found when data is distributed across multiple nodes in a compute cluster, where such blocks correspond to columns (or rows) of the matrix stored on the same node, which can be retrieved with much less overhead than retrieving individual columns stored across different nodes. We propose a novel algorithm for sampling useful column blocks and provide guarantees for the quality of the approximation. We demonstrate the practical utility of this algorithm for computing the block CUR decomposition of large matrices in a distributed setting using Apache Spark. Using our proposed block CUR algorithms, we can achieve a significant speed-up compared to a regular CUR decomposition with the same quality of approximation.|common problem larg scale data analysi approxim matrix use combin specif sampl row column known cur decomposit unfortun mani real world environ abil sampl specif individu row column matrix limit either system constraint cost paper consid matrix approxim sampl predefin block column row matrix regim common found data distribut across multipl node comput cluster block correspond column row matrix store node retriev much less overhead retriev individu column store across differ node propos novel algorithm sampl use column block provid guarante qualiti approxim demonstr practic util algorithm comput block cur decomposit larg matric distribut set use apach spark use propos block cur algorithm achiev signific speed compar regular cur decomposit qualiti approxim|['Urvashi Oswal', 'Swayambhoo Jain', 'Kevin S. Xu', 'Brian Eriksson']|['stat.ML', 'cs.DC', 'cs.DS', 'cs.LG']
2017-03-28T14:02:18Z|2017-03-17T14:59:17Z|http://arxiv.org/abs/1703.06043v1|http://arxiv.org/pdf/1703.06043v1|Pattern representation and recognition with accelerated analog   neuromorphic systems|pattern represent recognit acceler analog neuromorph system|Despite being originally inspired by the central nervous system, artificial neural networks have diverged from their biological archetypes as they have been remodeled to fit particular tasks. In this paper, we review several possibilites to reverse map these architectures to biologically more realistic spiking networks with the aim of emulating them on fast, low-power neuromorphic hardware. Since many of these devices employ analog components, which cannot be perfectly controlled, finding ways to compensate for the resulting effects represents a key challenge. Here, we discuss three different strategies to address this problem: the addition of auxiliary network components for stabilizing activity, the utilization of inherently robust architectures and a training method for hardware-emulated networks that functions without perfect knowledge of the system's dynamics and parameters. For all three scenarios, we corroborate our theoretical considerations with experimental results on accelerated analog neuromorphic platforms.|despit origin inspir central nervous system artifici neural network diverg biolog archetyp remodel fit particular task paper review sever possibilit revers map architectur biolog realist spike network aim emul fast low power neuromorph hardwar sinc mani devic employ analog compon cannot perfect control find way compens result effect repres key challeng discuss three differ strategi address problem addit auxiliari network compon stabil activ util inher robust architectur train method hardwar emul network function without perfect knowledg system dynam paramet three scenario corrobor theoret consider experiment result acceler analog neuromorph platform|['Mihai A. Petrovici', 'Sebastian Schmitt', 'Johann Klähn', 'David Stöckel', 'Anna Schroeder', 'Guillaume Bellec', 'Johannes Bill', 'Oliver Breitwieser', 'Ilja Bytschok', 'Andreas Grübl', 'Maurice Güttler', 'Andreas Hartel', 'Stephan Hartmann', 'Dan Husmann', 'Kai Husmann', 'Sebastian Jeltsch', 'Vitali Karasenko', 'Mitja Kleider', 'Christoph Koke', 'Alexander Kononov', 'Christian Mauch', 'Paul Müller', 'Johannes Partzsch', 'Thomas Pfeil', 'Stefan Schiefer', 'Stefan Scholze', 'Anand Subramoney', 'Vasilis Thanasoulis', 'Bernhard Vogginger', 'Robert Legenstein', 'Wolfgang Maass', 'René Schüffny', 'Christian Mayr', 'Johannes Schemmel', 'Karlheinz Meier']|['q-bio.NC', 'cs.NE', 'stat.ML']
2017-03-28T14:02:22Z|2017-03-16T23:33:24Z|http://arxiv.org/abs/1703.05849v1|http://arxiv.org/pdf/1703.05849v1|Causal Inference through the Method of Direct Estimation|causal infer method direct estim|The intersection of causal inference and machine learning is a rapidly advancing field. We propose a new approach, the method of direct estimation, that draws on both traditions in order to obtain nonparametric estimates of treatment effects. The approach focuses on estimating the effect of fluctuations in a treatment variable on an outcome. A tensor-spline implementation enables rich interactions between functional bases allowing for the approach to capture treatment/covariate interactions. We show how new innovations in Bayesian sparse modeling readily handle the proposed framework, and then document its performance in simulation and applied examples. Furthermore we show how the method of direct estimation can easily extend to structural estimators commonly used in a variety of disciplines, like instrumental variables, mediation analysis, and sequential g-estimation.|intersect causal infer machin learn rapid advanc field propos new approach method direct estim draw tradit order obtain nonparametr estim treatment effect approach focus estim effect fluctuat treatment variabl outcom tensor spline implement enabl rich interact function base allow approach captur treatment covari interact show new innov bayesian spars model readili handl propos framework document perform simul appli exampl furthermor show method direct estim easili extend structur estim common use varieti disciplin like instrument variabl mediat analysi sequenti estim|['Marc Ratkovic', 'Dustin Tingley']|['stat.ML', 'stat.ME', '62G08, 46N30, 62P20, 62P25']
2017-03-28T14:02:22Z|2017-03-16T22:37:55Z|http://arxiv.org/abs/1703.05841v1|http://arxiv.org/pdf/1703.05841v1|Adaptivity to Noise Parameters in Nonparametric Active Learning|adapt nois paramet nonparametr activ learn|This work addresses various open questions in the theory of active learning for nonparametric classification. Our contributions are both statistical and algorithmic: -We establish new minimax-rates for active learning under common \textit{noise conditions}. These rates display interesting transitions -- due to the interaction between noise \textit{smoothness and margin} -- not present in the passive setting. Some such transitions were previously conjectured, but remained unconfirmed. -We present a generic algorithmic strategy for adaptivity to unknown noise smoothness and margin; our strategy achieves optimal rates in many general situations; furthermore, unlike in previous work, we avoid the need for \textit{adaptive confidence sets}, resulting in strictly milder distributional requirements.|work address various open question theori activ learn nonparametr classif contribut statist algorithm establish new minimax rate activ learn common textit nois condit rate display interest transit due interact nois textit smooth margin present passiv set transit previous conjectur remain unconfirm present generic algorithm strategi adapt unknown nois smooth margin strategi achiev optim rate mani general situat furthermor unlik previous work avoid need textit adapt confid set result strict milder distribut requir|['Andrea Locatelli', 'Alexandra Carpentier', 'Samory Kpotufe']|['stat.ML']
2017-03-28T14:02:22Z|2017-03-24T20:28:16Z|http://arxiv.org/abs/1703.05840v2|http://arxiv.org/pdf/1703.05840v2|Conditional Accelerated Lazy Stochastic Gradient Descent|condit acceler lazi stochast gradient descent|In this work we introduce a conditional accelerated lazy stochastic gradient descent algorithm with optimal number of calls to a stochastic first-order oracle and convergence rate $O\left(\frac{1}{\varepsilon^2}\right)$ improving over the projection-free, Online Frank-Wolfe based stochastic gradient descent of Hazan and Kale [2012] with convergence rate $O\left(\frac{1}{\varepsilon^4}\right)$.|work introduc condit acceler lazi stochast gradient descent algorithm optim number call stochast first order oracl converg rate left frac varepsilon right improv project free onlin frank wolf base stochast gradient descent hazan kale converg rate left frac varepsilon right|['Guanghui Lan', 'Sebastian Pokutta', 'Yi Zhou', 'Daniel Zink']|['cs.LG', 'stat.ML', '90C25', 'G.1.6']
2017-03-28T14:02:22Z|2017-03-16T18:25:21Z|http://arxiv.org/abs/1703.05785v1|http://arxiv.org/pdf/1703.05785v1|Low-rank and Sparse NMF for Joint Endmembers' Number Estimation and   Blind Unmixing of Hyperspectral Images|low rank spars nmf joint endmemb number estim blind unmix hyperspectr imag|Estimation of the number of endmembers existing in a scene constitutes a critical task in the hyperspectral unmixing process. The accuracy of this estimate plays a crucial role in subsequent unsupervised unmixing steps i.e., the derivation of the spectral signatures of the endmembers (endmembers' extraction) and the estimation of the abundance fractions of the pixels. A common practice amply followed in literature is to treat endmembers' number estimation and unmixing, independently as two separate tasks, providing the outcome of the former as input to the latter. In this paper, we go beyond this computationally demanding strategy. More precisely, we set forth a multiple constrained optimization framework, which encapsulates endmembers' number estimation and unsupervised unmixing in a single task. This is attained by suitably formulating the problem via a low-rank and sparse nonnegative matrix factorization rationale, where low-rankness is promoted with the use of a sophisticated $\ell_2/\ell_1$ norm penalty term. An alternating proximal algorithm is then proposed for minimizing the emerging cost function. The results obtained by simulated and real data experiments verify the effectiveness of the proposed approach.|estim number endmemb exist scene constitut critic task hyperspectr unmix process accuraci estim play crucial role subsequ unsupervis unmix step deriv spectral signatur endmemb endmemb extract estim abund fraction pixel common practic ampli follow literatur treat endmemb number estim unmix independ two separ task provid outcom former input latter paper go beyond comput demand strategi precis set forth multipl constrain optim framework encapsul endmemb number estim unsupervis unmix singl task attain suitabl formul problem via low rank spars nonneg matrix factor rational low rank promot use sophist ell ell norm penalti term altern proxim algorithm propos minim emerg cost function result obtain simul real data experi verifi effect propos approach|['Paris V. Giampouras', 'Athanasios A. Rontogiannis', 'Konstantinos D. Koutroumbas']|['cs.CV', 'stat.ML']
2017-03-28T14:02:22Z|2017-03-16T16:00:00Z|http://arxiv.org/abs/1703.05687v1|http://arxiv.org/pdf/1703.05687v1|Gaussian process regression for forecasting battery state of health|gaussian process regress forecast batteri state health|Accurately predicting the future capacity and remaining useful life of batteries is necessary to ensure reliable system operation and to minimise maintenance costs. The complex nature of battery degradation has meant that mechanistic modelling of capacity fade has thus far remained intractable; however, with the advent of cloud-connected devices, data from cells in various applications is becoming increasingly available, and the feasibility of data-driven methods for battery prognostics is increasing. Here we propose Gaussian process (GP) regression for forecasting battery state of health, and highlight various advantages of GPs over other data-driven and mechanistic approaches. GPs are a type of Bayesian non-parametric method, and hence can model complex systems whilst handling uncertainty in a principled manner. Prior information can be exploited by GPs in a variety of ways: explicit mean functions can be used if the functional form of the underlying degradation model is available, and multiple-output GPs can effectively exploit correlations between data from different cells. We demonstrate the predictive capability of GPs for short-term and long-term (remaining useful life) forecasting on a selection of capacity vs. cycle datasets from lithium-ion cells.|accur predict futur capac remain use life batteri necessari ensur reliabl system oper minimis mainten cost complex natur batteri degrad meant mechanist model capac fade thus far remain intract howev advent cloud connect devic data cell various applic becom increas avail feasibl data driven method batteri prognost increas propos gaussian process gp regress forecast batteri state health highlight various advantag gps data driven mechanist approach gps type bayesian non parametr method henc model complex system whilst handl uncertainti principl manner prior inform exploit gps varieti way explicit mean function use function form degrad model avail multipl output gps effect exploit correl data differ cell demonstr predict capabl gps short term long term remain use life forecast select capac vs cycl dataset lithium ion cell|['Robert R. Richardson', 'Michael A. Osborne', 'David A. Howey']|['stat.AP', 'stat.ML', '62P30', 'J.2; G.3']
2017-03-28T14:02:22Z|2017-03-16T15:14:48Z|http://arxiv.org/abs/1703.05667v1|http://arxiv.org/pdf/1703.05667v1|End-to-End Learning for Structured Prediction Energy Networks|end end learn structur predict energi network|Structured Prediction Energy Networks (Belanger and McCallum, 2016) (SPENs) are a simple, yet expressive family of structured prediction models. An energy function over candidate structured outputs is given by a deep network, and predictions are formed by gradient-based optimization. Unfortunately, we have struggled to apply the structured SVM (SSVM) learning method of Belanger and McCallum, 2016 to applications with more complex structure than multi-label classification. In general, SSVMs are unreliable whenever exact energy minimization is intractable. In response, we present end-to-end learning for SPENs, where the energy function is discriminatively trained by back-propagating through gradient-based prediction. This paper presents a collection of methods necessary to apply the technique to problems with complex structure. For example, we avoid vanishing gradients when learning SPENs for convex relaxations of discrete prediction problems and explicitly train models such that energy minimization converges quickly in practice. Using end-to-end learning, we demonstrate the power of SPENs on 7-Scenes depth image denoising and CoNLL-2005 semantic role labeling tasks. In both, we outperform competitive baselines that employ more simplistic energy functions, but perform exact energy minimization. In particular, for denoising we achieve 40 PSNR, outperforming the previous state-of-the-art of 36.|structur predict energi network belang mccallum spen simpl yet express famili structur predict model energi function candid structur output given deep network predict form gradient base optim unfortun struggl appli structur svm ssvm learn method belang mccallum applic complex structur multi label classif general ssvms unreli whenev exact energi minim intract respons present end end learn spen energi function discrimin train back propag gradient base predict paper present collect method necessari appli techniqu problem complex structur exampl avoid vanish gradient learn spen convex relax discret predict problem explicit train model energi minim converg quick practic use end end learn demonstr power spen scene depth imag denois conll semant role label task outperform competit baselin employ simplist energi function perform exact energi minim particular denois achiev psnr outperform previous state art|['David Belanger', 'Bishan Yang', 'Andrew McCallum']|['stat.ML', 'cs.LG']
2017-03-28T14:02:22Z|2017-03-16T09:52:48Z|http://arxiv.org/abs/1703.05537v1|http://arxiv.org/pdf/1703.05537v1|Shift Aggregate Extract Networks|shift aggreg extract network|"We introduce an architecture based on deep hierarchical decompositions to learn effective representations of large graphs. Our framework extends classic R-decompositions used in kernel methods, enabling nested ""part-of-part"" relations. Unlike recursive neural networks, which unroll a template on input graphs directly, we unroll a neural network template over the decomposition hierarchy, allowing us to deal with the high degree variability that typically characterize social network graphs. Deep hierarchical decompositions are also amenable to domain compression, a technique that reduces both space and time complexity by exploiting symmetries. We show empirically that our approach is competitive with current state-of-the-art graph classification methods, particularly when dealing with social network datasets."|introduc architectur base deep hierarch decomposit learn effect represent larg graph framework extend classic decomposit use kernel method enabl nest part part relat unlik recurs neural network unrol templat input graph direct unrol neural network templat decomposit hierarchi allow us deal high degre variabl typic character social network graph deep hierarch decomposit also amen domain compress techniqu reduc space time complex exploit symmetri show empir approach competit current state art graph classif method particular deal social network dataset|['Francesco Orsini', 'Daniele Baracchi', 'Paolo Frasconi']|['cs.LG', 'stat.ML']
2017-03-28T14:02:22Z|2017-03-16T01:37:25Z|http://arxiv.org/abs/1703.05452v1|http://arxiv.org/pdf/1703.05452v1|Efficient Online Learning for Optimizing Value of Information: Theory   and Application to Interactive Troubleshooting|effici onlin learn optim valu inform theori applic interact troubleshoot|We consider the optimal value of information (VoI) problem, where the goal is to sequentially select a set of tests with a minimal cost, so that one can efficiently make the best decision based on the observed outcomes. Existing algorithms are either heuristics with no guarantees, or scale poorly (with exponential run time in terms of the number of available tests). Moreover, these methods assume a known distribution over the test outcomes, which is often not the case in practice. We propose an efficient sampling-based online learning framework to address the above issues. First, assuming the distribution over hypotheses is known, we propose a dynamic hypothesis enumeration strategy, which allows efficient information gathering with strong theoretical guarantees. We show that with sufficient amount of samples, one can identify a near-optimal decision with high probability. Second, when the parameters of the hypotheses distribution are unknown, we propose an algorithm which learns the parameters progressively via posterior sampling in an online fashion. We further establish a rigorous bound on the expected regret. We demonstrate the effectiveness of our approach on a real-world interactive troubleshooting application and show that one can efficiently make high-quality decisions with low cost.|consid optim valu inform voi problem goal sequenti select set test minim cost one effici make best decis base observ outcom exist algorithm either heurist guarante scale poor exponenti run time term number avail test moreov method assum known distribut test outcom often case practic propos effici sampl base onlin learn framework address abov issu first assum distribut hypothes known propos dynam hypothesi enumer strategi allow effici inform gather strong theoret guarante show suffici amount sampl one identifi near optim decis high probabl second paramet hypothes distribut unknown propos algorithm learn paramet progress via posterior sampl onlin fashion establish rigor bound expect regret demonstr effect approach real world interact troubleshoot applic show one effici make high qualiti decis low cost|['Yuxin Chen', 'Jean-Michel Renders', 'Morteza Haghir Chehreghani', 'Andreas Krause']|['cs.AI', 'cs.LG', 'stat.ML']
2017-03-28T14:02:22Z|2017-03-16T01:31:33Z|http://arxiv.org/abs/1703.05449v1|http://arxiv.org/pdf/1703.05449v1|Minimax Regret Bounds for Reinforcement Learning|minimax regret bound reinforc learn|"We consider the problem of efficient exploration in finite horizon MDPs.We show that an optimistic modification to model-based value iteration, can achieve a regret bound $\tilde{O}( \sqrt{HSAT} + H^2S^2A+H\sqrt{T})$ where $H$ is the time horizon, $S$ the number of states, $A$ the number of actions and $T$ the time elapsed. This result improves over the best previous known bound $\tilde{O}(HS \sqrt{AT})$ achieved by the UCRL2 algorithm.The key significance of our new results is that when $T\geq H^3S^3A$ and $SA\geq H$, it leads to a regret of $\tilde{O}(\sqrt{HSAT})$ that matches the established lower bounds of $\Omega(\sqrt{HSAT})$ up to a logarithmic factor. Our analysis contain two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in $S$), and we use ""exploration bonuses"" based on Bernstein's inequality, together with using a recursive -Bellman-type- Law of Total Variance (to improve scaling in $H$)."|consid problem effici explor finit horizon mdps show optimist modif model base valu iter achiev regret bound tild sqrt hsat sqrt time horizon number state number action time elaps result improv best previous known bound tild hs sqrt achiev ucrl algorithm key signific new result geq sa geq lead regret tild sqrt hsat match establish lower bound omega sqrt hsat logarithm factor analysi contain two key insight use care applic concentr inequ optim valu function whole rather transit probabl improv scale use explor bonus base bernstein inequ togeth use recurs bellman type law total varianc improv scale|['Mohammad Gheshlaghi Azar', 'Ian Osband', 'Rémi Munos']|['stat.ML', 'cs.AI', 'cs.LG']
2017-03-28T14:02:22Z|2017-03-15T23:58:19Z|http://arxiv.org/abs/1703.05430v1|http://arxiv.org/pdf/1703.05430v1|Cost-complexity pruning of random forests|cost complex prune random forest|Random forests perform bootstrap-aggregation by sampling the training samples with replacement. This enables the evaluation of out-of-bag error which serves as a internal cross-validation mechanism. Our motivation lies in using the unsampled training samples to improve each decision tree in the ensemble. We study the effect of using the out-of-bag samples to improve the generalization error first of the decision trees and second the random forest by post-pruning. A preliminary empirical study on four UCI repository datasets show consistent decrease in the size of the forests without considerable loss in accuracy.|random forest perform bootstrap aggreg sampl train sampl replac enabl evalu bag error serv intern cross valid mechan motiv lie use unsampl train sampl improv decis tree ensembl studi effect use bag sampl improv general error first decis tree second random forest post prune preliminari empir studi four uci repositori dataset show consist decreas size forest without consider loss accuraci|['Kiran Bangalore Ravi', 'Jean Serra']|['stat.ML', 'cs.LG']
2017-04-07T11:22:56Z|2017-04-06T17:18:02Z|http://arxiv.org/abs/1704.01942v1|http://arxiv.org/pdf/1704.01942v1|ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models|activi visual explor industri scale deep neural network model|While deep learning models have achieved state-of-the-art accuracies for many prediction tasks, understanding these models remains a challenge. Despite the recent interest in developing visual tools to help users interpret deep learning models, the complexity and wide variety of models deployed in industry, and the large-scale datasets that they used, pose unique design challenges that are inadequately addressed by existing work. Through participatory design sessions with over 15 researchers and engineers at Facebook, we have developed, deployed, and iteratively improved ActiVis, an interactive visualization system for interpreting large-scale deep learning models and results. By tightly integrating multiple coordinated views, such as a computation graph overview of the model architecture, and a neuron activation view for pattern discovery and comparison, users can explore complex deep neural network models at both the instance- and subset-level. ActiVis has been deployed on Facebook's machine learning platform. We present case studies with Facebook researchers and engineers, and usage scenarios of how ActiVis may work with different models.|deep learn model achiev state art accuraci mani predict task understand model remain challeng despit recent interest develop visual tool help user interpret deep learn model complex wide varieti model deploy industri larg scale dataset use pose uniqu design challeng inadequ address exist work participatori design session research engin facebook develop deploy iter improv activi interact visual system interpret larg scale deep learn model result tight integr multipl coordin view comput graph overview model architectur neuron activ view pattern discoveri comparison user explor complex deep neural network model instanc subset level activi deploy facebook machin learn platform present case studi facebook research engin usag scenario activi may work differ model|['Minsuk Kahng', 'Pierre Andrews', 'Aditya Kalro', 'Duen Horng Chau']|['cs.HC', 'stat.ML']
2017-04-07T11:22:56Z|2017-04-06T16:37:15Z|http://arxiv.org/abs/1704.01920v1|http://arxiv.org/pdf/1704.01920v1|Encoder Based Lifelong Learning|encod base lifelong learn|This paper introduces a new lifelong learning solution where a single model is trained for a sequence of tasks. The main challenge that vision systems face in this context is catastrophic forgetting: as they tend to adapt to the most recently seen task, they lose performance on the tasks that were learned previously. Our method aims at preserving the knowledge of the previous tasks while learning a new one by using autoencoders. For each task, an under-complete autoencoder is learned, capturing the features that are crucial for its achievement. When a new task is presented to the system, we prevent the reconstructions of the features with these autoencoders from changing, which has the effect of preserving the information on which the previous tasks are mainly relying. At the same time, the features are given space to adjust to the most recent environment as only their projection into a low dimension submanifold is controlled. The proposed system is evaluated on image classification tasks and shows a reduction of forgetting over the state-of-the-art|paper introduc new lifelong learn solut singl model train sequenc task main challeng vision system face context catastroph forget tend adapt recent seen task lose perform task learn previous method aim preserv knowledg previous task learn new one use autoencod task complet autoencod learn captur featur crucial achiev new task present system prevent reconstruct featur autoencod chang effect preserv inform previous task main reli time featur given space adjust recent environ onli project low dimens submanifold control propos system evalu imag classif task show reduct forget state art|['Amal Rannen Triki', 'Rahaf Aljundi', 'Mathew B. Blaschko', 'Tinne Tuytelaars']|['cs.CV', 'cs.AI', 'stat.ML']
2017-04-07T11:22:56Z|2017-04-06T16:32:50Z|http://arxiv.org/abs/1704.01918v1|http://arxiv.org/pdf/1704.01918v1|Cooperative network localization using hybrid range and angle   measurements|cooper network local use hybrid rang angl measur|A reliable, accurate, and affordable positioning service is highly required in wireless networks. In this paper, a novel distributed message passing algorithm is proposed to solve the problem of cooperative network localization using both distance and angle of arrival estimates. This hybrid approach combines the distance and angle observations to reduce the uncertainty in localizing the network nodes. A statistical problem formulations is employed and approximate minimum mean square error (MMSE) estimates of the node locations are found. Numerical results are presented to show the improvement in localization performance compared to existing distance-only and angle-only localization methods. Moreover, the proposed algorithm improves the identifiability of the localization problem compared to range-only or angle-only localization techniques. That is, it can solve the problem with fewer anchor nodes and fewer connections in the network.|reliabl accur afford posit servic high requir wireless network paper novel distribut messag pass algorithm propos solv problem cooper network local use distanc angl arriv estim hybrid approach combin distanc angl observ reduc uncertainti local network node statist problem formul employ approxim minimum mean squar error mmse estim node locat found numer result present show improv local perform compar exist distanc onli angl onli local method moreov propos algorithm improv identifi local problem compar rang onli angl onli local techniqu solv problem fewer anchor node fewer connect network|['Hassan Naseri', 'Visa Koivunen']|['cs.IT', 'math.IT', 'stat.ML']
2017-04-07T11:22:56Z|2017-04-06T15:43:08Z|http://arxiv.org/abs/1704.01896v1|http://arxiv.org/pdf/1704.01896v1|Statistical Efficiency of Compositional Nonparametric Prediction|statist effici composit nonparametr predict|In this paper, we propose a compositional nonparametric method in which a model is expressed as a labeled binary tree of $2k+1$ nodes, where each node is either a summation, a multiplication, or the application of one of the $q$ basis functions to one of the $p$ covariates. We show that in order to recover a labeled binary tree from a given dataset, the sufficient number of samples is $O(k\log(pq)+\log(k!))$, and the necessary number of samples is $\Omega(k\log (pq)-\log(k!))$. We implement our method for regression as a greedy search algorithm, and demonstrate its effectiveness with two synthetic data sets and one real-world experiment.|paper propos composit nonparametr method model express label binari tree node node either summat multipl applic one basi function one covari show order recov label binari tree given dataset suffici number sampl log pq log necessari number sampl omega log pq log implement method regress greedi search algorithm demonstr effect two synthet data set one real world experi|['Yixi Xu', 'Jean Honorio', 'Xiao Wang']|['stat.ML', 'cs.LG']
2017-04-07T11:22:56Z|2017-04-06T14:48:44Z|http://arxiv.org/abs/1704.01871v1|http://arxiv.org/pdf/1704.01871v1|Massive Data Clustering in Moderate Dimensions from the Dual Spaces of   Observation and Attribute Data Clouds|massiv data cluster moder dimens dual space observ attribut data cloud|Cluster analysis of very high dimensional data can benefit from the properties of such high dimensionality. Informally expressed, in this work, our focus is on the analogous situation when the dimensionality is moderate to small, relative to a massively sized set of observations. Mathematically expressed, these are the dual spaces of observations and attributes. The point cloud of observations is in attribute space, and the point cloud of attributes is in observation space. In this paper, we begin by summarizing various perspectives related to methodologies that are used in multivariate analytics. We draw on these to establish an efficient clustering processing pipeline, both partitioning and hierarchical clustering.|cluster analysi veri high dimension data benefit properti high dimension inform express work focus analog situat dimension moder small relat massiv size set observ mathemat express dual space observ attribut point cloud observ attribut space point cloud attribut observ space paper begin summar various perspect relat methodolog use multivari analyt draw establish effici cluster process pipelin partit hierarch cluster|['Fionn Murtagh']|['stat.ML', '62H30, 91C20', 'H.3.3; I.5.3']
2017-04-07T11:22:56Z|2017-04-06T14:39:54Z|http://arxiv.org/abs/1704.01864v1|http://arxiv.org/pdf/1704.01864v1|Robust Causal Estimation in the Large-Sample Limit without Strict   Faithfulness|robust causal estim larg sampl limit without strict faith|Causal effect estimation from observational data is an important and much studied research topic. The instrumental variable (IV) and local causal discovery (LCD) patterns are canonical examples of settings where a closed-form expression exists for the causal effect of one variable on another, given the presence of a third variable. Both rely on faithfulness to infer that the latter only influences the target effect via the cause variable. In reality, it is likely that this assumption only holds approximately and that there will be at least some form of weak interaction. This brings about the paradoxical situation that, in the large-sample limit, no predictions are made, as detecting the weak edge invalidates the setting. We introduce an alternative approach by replacing strict faithfulness with a prior that reflects the existence of many 'weak' (irrelevant) and 'strong' interactions. We obtain a posterior distribution over the target causal effect estimator which shows that, in many cases, we can still make good estimates. We demonstrate the approach in an application on a simple linear-Gaussian setting, using the MultiNest sampling algorithm, and compare it with established techniques to show our method is robust even when strict faithfulness is violated.|causal effect estim observ data import much studi research topic instrument variabl iv local causal discoveri lcd pattern canon exampl set close form express exist causal effect one variabl anoth given presenc third variabl reli faith infer latter onli influenc target effect via caus variabl realiti like assumpt onli hold approxim least form weak interact bring paradox situat larg sampl limit predict made detect weak edg invalid set introduc altern approach replac strict faith prior reflect exist mani weak irrelev strong interact obtain posterior distribut target causal effect estim show mani case still make good estim demonstr approach applic simpl linear gaussian set use multinest sampl algorithm compar establish techniqu show method robust even strict faith violat|['Ioan Gabriel Bucur', 'Tom Claassen', 'Tom Heskes']|['stat.ML', 'cs.AI', 'stat.ME']
2017-04-07T11:22:56Z|2017-04-06T14:29:10Z|http://arxiv.org/abs/1704.01858v1|http://arxiv.org/pdf/1704.01858v1|An Online Hierarchical Algorithm for Extreme Clustering|onlin hierarch algorithm extrem cluster|Many modern clustering methods scale well to a large number of data items, N, but not to a large number of clusters, K. This paper introduces PERCH, a new non-greedy algorithm for online hierarchical clustering that scales to both massive N and K--a problem setting we term extreme clustering. Our algorithm efficiently routes new data points to the leaves of an incrementally-built tree. Motivated by the desire for both accuracy and speed, our approach performs tree rotations for the sake of enhancing subtree purity and encouraging balancedness. We prove that, under a natural separability assumption, our non-greedy algorithm will produce trees with perfect dendrogram purity regardless of online data arrival order. Our experiments demonstrate that PERCH constructs more accurate trees than other tree-building clustering algorithms and scales well with both N and K, achieving a higher quality clustering than the strongest flat clustering competitor in nearly half the time.|mani modern cluster method scale well larg number data item larg number cluster paper introduc perch new non greedi algorithm onlin hierarch cluster scale massiv problem set term extrem cluster algorithm effici rout new data point leav increment built tree motiv desir accuraci speed approach perform tree rotat sake enhanc subtre puriti encourag balanced prove natur separ assumpt non greedi algorithm produc tree perfect dendrogram puriti regardless onlin data arriv order experi demonstr perch construct accur tree tree build cluster algorithm scale well achiev higher qualiti cluster strongest flat cluster competitor near half time|['Ari Kobren', 'Nicholas Monath', 'Akshay Krishnamurthy', 'Andrew McCallum']|['cs.LG', 'stat.ML']
2017-04-07T11:22:56Z|2017-04-06T04:35:40Z|http://arxiv.org/abs/1704.01704v1|http://arxiv.org/pdf/1704.01704v1|Adequacy of the Gradient-Descent Method for Classifier Evasion Attacks|adequaci gradient descent method classifi evas attack|Despite the wide use of machine learning in adversarial settings including computer security, recent studies have demonstrated vulnerabilities to evasion attacks---carefully crafted adversarial samples that closely resemble legitimate instances, but cause misclassification. In this paper, (1) we analyse the effectiveness of the gradient-descent method---the leading approach for generating adversarial samples---against non-linear support vector machines, and conclude that carefully reduced kernel smoothness can significantly increase robustness to the attack; (2) we propose a quantity similar to margin that can efficiently predict potential susceptibility to gradient-descent attack, before the attack is launched; and (3) we design a new adversarial sample construction algorithm based on optimising the multiplicative ratio of class decision functions. Our results demonstrate that the new method not only increases the attack's success rate, but also achieves success with less perturbation.|despit wide use machin learn adversari set includ comput secur recent studi demonstr vulner evas attack care craft adversari sampl close resembl legitim instanc caus misclassif paper analys effect gradient descent method lead approach generat adversari sampl non linear support vector machin conclud care reduc kernel smooth signific increas robust attack propos quantiti similar margin effici predict potenti suscept gradient descent attack befor attack launch design new adversari sampl construct algorithm base optimis multipl ratio class decis function result demonstr new method onli increas attack success rate also achiev success less perturb|['Yi Han', 'Benjamin I. P. Rubinstein']|['cs.CR', 'cs.LG', 'stat.ML']
2017-04-07T11:22:56Z|2017-04-06T04:02:35Z|http://arxiv.org/abs/1704.01701v1|http://arxiv.org/pdf/1704.01701v1|Learning Certifiably Optimal Rule Lists for Categorical Data|learn certifi optim rule list categor data|We present the design and implementation of a custom discrete optimization technique for building rule lists over a categorical feature space. Our algorithm provides the optimal solution, with a certificate of optimality. By leveraging algorithmic bounds, efficient data structures, and computational reuse, we achieve several orders of magnitude speedup in time and a massive reduction of memory consumption. We demonstrate that our approach produces optimal rule lists on practical problems in seconds. This framework is a novel alternative to CART and other decision tree methods.|present design implement custom discret optim techniqu build rule list categor featur space algorithm provid optim solut certif optim leverag algorithm bound effici data structur comput reus achiev sever order magnitud speedup time massiv reduct memori consumpt demonstr approach produc optim rule list practic problem second framework novel altern cart decis tree method|['Elaine Angelino', 'Nicholas Larus-Stone', 'Daniel Alabi', 'Margo Seltzer', 'Cynthia Rudin']|['stat.ML', 'cs.LG']
2017-04-07T11:22:56Z|2017-04-06T03:34:29Z|http://arxiv.org/abs/1704.01700v1|http://arxiv.org/pdf/1704.01700v1|Accelerated Stochastic Quasi-Newton Optimization on Riemann Manifolds|acceler stochast quasi newton optim riemann manifold|We propose L-BFGS and trust-region algorithms on Riemann manifolds that use stochastic variance reduction techniques to speed up convergence. For the stochastic L-BFGS algorithm we analyze and prove linear convergence rates for geodesically convex problems on the manifold, without resorting to linesearch methods designed to satisfy Wolfe conditions on the step size. To the best of our knowledge our trust-region method with stochastic variance reduction techniques is the first of its kind in the literature. We conduct experiments on Karcher mean computation for positive definite matrices and computation of leading eigenvectors for both synthetic and real data matrices, and demonstrate notably better performance than recently-proposed first-order stochastic optimization methods on Riemann manifolds, as well as standard trust-region manifold optimization techniques.|propos bfgs trust region algorithm riemann manifold use stochast varianc reduct techniqu speed converg stochast bfgs algorithm analyz prove linear converg rate geodes convex problem manifold without resort linesearch method design satisfi wolf condit step size best knowledg trust region method stochast varianc reduct techniqu first kind literatur conduct experi karcher mean comput posit definit matric comput lead eigenvector synthet real data matric demonstr notabl better perform recent propos first order stochast optim method riemann manifold well standard trust region manifold optim techniqu|['Anirban Roychowdhury', 'Srinivasan Parthasarathy']|['math.OC', 'math.DG', 'stat.ML']
2017-04-07T11:22:59Z|2017-04-05T23:04:43Z|http://arxiv.org/abs/1704.01664v1|http://arxiv.org/pdf/1704.01664v1|The Relative Performance of Ensemble Methods with Deep Convolutional   Neural Networks for Image Classification|relat perform ensembl method deep convolut neural network imag classif|Artificial neural networks have been successfully applied to a variety of machine learning tasks, including image recognition, semantic segmentation, and machine translation. However, few studies fully investigated ensembles of artificial neural networks. In this work, we investigated multiple widely used ensemble methods, including unweighted averaging, majority voting, the Bayes Optimal Classifier, and the (discrete) Super Learner, for image recognition tasks, with deep neural networks as candidate algorithms. We designed several experiments, with the candidate algorithms being the same network structure with different model checkpoints within a single training process, networks with same structure but trained multiple times stochastically, and networks with different structure. In addition, we further studied the over-confidence phenomenon of the neural networks, as well as its impact on the ensemble methods. Across all of our experiments, the Super Learner achieved best performance among all the ensemble methods in this study.|artifici neural network success appli varieti machin learn task includ imag recognit semant segment machin translat howev studi fulli investig ensembl artifici neural network work investig multipl wide use ensembl method includ unweight averag major vote bay optim classifi discret super learner imag recognit task deep neural network candid algorithm design sever experi candid algorithm network structur differ model checkpoint within singl train process network structur train multipl time stochast network differ structur addit studi confid phenomenon neural network well impact ensembl method across experi super learner achiev best perform among ensembl method studi|['Cheng Ju', 'Aurélien Bibaut', 'Mark J. van der Laan']|['stat.ML', 'cs.CV', 'cs.LG', 'stat.ME']
2017-04-07T11:22:59Z|2017-04-05T18:49:56Z|http://arxiv.org/abs/1704.01605v1|http://arxiv.org/pdf/1704.01605v1|Nonnegative/binary matrix factorization with a D-Wave quantum annealer|nonneg binari matrix factor wave quantum anneal|D-Wave quantum annealers represent a novel computational architecture and have attracted significant interest, but have been used for few real-world computations. Machine learning has been identified as an area where quantum annealing may be useful. Here, we show that the D-Wave 2X can be effectively used as part of an unsupervised machine learning method. This method can be used to analyze large datasets. The D-Wave only limits the number of features that can be extracted from the dataset. We apply this method to learn the features from a set of facial images.|wave quantum anneal repres novel comput architectur attract signific interest use real world comput machin learn identifi area quantum anneal may use show wave effect use part unsupervis machin learn method method use analyz larg dataset wave onli limit number featur extract dataset appli method learn featur set facial imag|"[""Daniel O'Malley"", 'Velimir V. Vesselinov', 'Boian S. Alexandrov', 'Ludmil B. Alexandrov']"|['cs.LG', 'quant-ph', 'stat.ML']
2017-04-07T11:22:59Z|2017-04-05T18:21:26Z|http://arxiv.org/abs/1704.01574v1|http://arxiv.org/pdf/1704.01574v1|Bag-of-Words Method Applied to Accelerometer Measurements for the   Purpose of Classification and Energy Estimation|bag word method appli acceleromet measur purpos classif energi estim|Accelerometer measurements are the prime type of sensor information most think of when seeking to measure physical activity. On the market, there are many fitness measuring devices which aim to track calories burned and steps counted through the use of accelerometers. These measurements, though good enough for the average consumer, are noisy and unreliable in terms of the precision of measurement needed in a scientific setting. The contribution of this paper is an innovative and highly accurate regression method which uses an intermediary two-stage classification step to better direct the regression of energy expenditure values from accelerometer counts.   We show that through an additional unsupervised layer of intermediate feature construction, we can leverage latent patterns within accelerometer counts to provide better grounds for activity classification than expert-constructed timeseries features. For this, our approach utilizes a mathematical model originating in natural language processing, the bag-of-words model, that has in the past years been appearing in diverse disciplines outside of the natural language processing field such as image processing. Further emphasizing the natural language connection to stochastics, we use a gaussian mixture model to learn the dictionary upon which the bag-of-words model is built. Moreover, we show that with the addition of these features, we're able to improve regression root mean-squared error of energy expenditure by approximately 1.4 units over existing state-of-the-art methods.|acceleromet measur prime type sensor inform think seek measur physic activ market mani fit measur devic aim track calori burn step count use acceleromet measur though good enough averag consum noisi unreli term precis measur need scientif set contribut paper innov high accur regress method use intermediari two stage classif step better direct regress energi expenditur valu acceleromet count show addit unsupervis layer intermedi featur construct leverag latent pattern within acceleromet count provid better ground activ classif expert construct timeseri featur approach util mathemat model origin natur languag process bag word model past year appear divers disciplin outsid natur languag process field imag process emphas natur languag connect stochast use gaussian mixtur model learn dictionari upon bag word model built moreov show addit featur abl improv regress root mean squar error energi expenditur approxim unit exist state art method|['Kevin M. Amaral', 'Ping Chen', 'Scott Crouter', 'Wei Ding']|['cs.LG', 'stat.ML']
2017-04-07T11:23:00Z|2017-04-05T17:47:25Z|http://arxiv.org/abs/1704.01547v1|http://arxiv.org/pdf/1704.01547v1|"Comment on ""Biologically inspired protection of deep networks from   adversarial attacks"""|comment biolog inspir protect deep network adversari attack|A recent paper suggests that Deep Neural Networks can be protected from gradient-based adversarial perturbations by driving the network activations into a highly saturated regime. Here we analyse such saturated networks and show that the attacks fail due to numerical limitations in the gradient computations. A simple stabilisation of the gradient estimates enables successful and efficient attacks. Thus, it has yet to be shown that the robustness observed in highly saturated networks is not simply due to numerical limitations.|recent paper suggest deep neural network protect gradient base adversari perturb drive network activ high satur regim analys satur network show attack fail due numer limit gradient comput simpl stabilis gradient estim enabl success effici attack thus yet shown robust observ high satur network simpli due numer limit|['Wieland Brendel', 'Matthias Bethge']|['stat.ML', 'cs.LG', 'q-bio.NC']
2017-04-07T11:23:00Z|2017-04-05T16:54:20Z|http://arxiv.org/abs/1704.01523v1|http://arxiv.org/pdf/1704.01523v1|MIT at SemEval-2017 Task 10: Relation Extraction with Convolutional   Neural Networks|mit semev task relat extract convolut neural network|Over 50 million scholarly articles have been published: they constitute a unique repository of knowledge. In particular, one may infer from them relations between scientific concepts, such as synonyms and hyponyms. Artificial neural networks have been recently explored for relation extraction. In this work, we continue this line of work and present a system based on a convolutional neural network to extract relations. Our model ranked first in the SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific articles (subtask C).|million scholar articl publish constitut uniqu repositori knowledg particular one may infer relat scientif concept synonym hyponym artifici neural network recent explor relat extract work continu line work present system base convolut neural network extract relat model rank first semev task sciencei relat extract scientif articl subtask|['Ji Young Lee', 'Franck Dernoncourt', 'Peter Szolovits']|['cs.CL', 'cs.AI', 'cs.NE', 'stat.ML']
2017-04-07T11:23:00Z|2017-04-05T15:12:25Z|http://arxiv.org/abs/1704.01474v1|http://arxiv.org/pdf/1704.01474v1|Convolutional Neural Networks for Page Segmentation of Historical   Document Images|convolut neural network page segment histor document imag|This paper presents a Convolutional Neural Network (CNN) based page segmentation method for handwritten historical document images. We consider page segmentation as a pixel labeling problem, i.e., each pixel is classified as one of the predefined classes. Traditional methods in this area rely on carefully hand-crafted features or large amounts of prior knowledge. In contrast, we propose to learn features from raw image pixels using a CNN. While many researchers focus on developing deep CNN architectures to solve different problems, we train a simple CNN with only one convolution layer. We show that the simple architecture achieves competitive results against other deep architectures on different public datasets. Experiments also demonstrate the effectiveness and superiority of the proposed method compared to previous methods.|paper present convolut neural network cnn base page segment method handwritten histor document imag consid page segment pixel label problem pixel classifi one predefin class tradit method area reli care hand craft featur larg amount prior knowledg contrast propos learn featur raw imag pixel use cnn mani research focus develop deep cnn architectur solv differ problem train simpl cnn onli one convolut layer show simpl architectur achiev competit result deep architectur differ public dataset experi also demonstr effect superior propos method compar previous method|['Kai Chen', 'Mathias Seuret']|['cs.CV', 'cs.LG', 'stat.ML']
2017-04-07T11:23:00Z|2017-04-05T14:54:28Z|http://arxiv.org/abs/1704.01460v1|http://arxiv.org/pdf/1704.01460v1|Comparison Based Nearest Neighbor Search|comparison base nearest neighbor search|We consider machine learning in a comparison-based setting where we are given a set of points in a metric space, but we have no access to the actual distances between the points. Instead, we can only ask an oracle whether the distance between two points $i$ and $j$ is smaller than the distance between the points $i$ and $k$. We are concerned with data structures and algorithms to find nearest neighbors based on such comparisons. We focus on a simple yet effective algorithm that recursively splits the space by first selecting two random pivot points and then assigning all other points to the closer of the two (comparison tree). We prove that if the metric space satisfies certain expansion conditions, then with high probability the height of the comparison tree is logarithmic in the number of points, leading to efficient search performance. We also provide an upper bound for the failure probability to return the true nearest neighbor. Experiments show that the comparison tree is competitive with algorithms that have access to the actual distance values, and needs less triplet comparisons than other competitors.|consid machin learn comparison base set given set point metric space access actual distanc point instead onli ask oracl whether distanc two point smaller distanc point concern data structur algorithm find nearest neighbor base comparison focus simpl yet effect algorithm recurs split space first select two random pivot point assign point closer two comparison tree prove metric space satisfi certain expans condit high probabl height comparison tree logarithm number point lead effici search perform also provid upper bound failur probabl return true nearest neighbor experi show comparison tree competit algorithm access actual distanc valu need less triplet comparison competitor|['Siavash Haghiri', 'Debarghya Ghoshdastidar', 'Ulrike von Luxburg']|['stat.ML', 'cs.DS', 'cs.LG']
2017-04-07T11:23:00Z|2017-04-05T14:23:53Z|http://arxiv.org/abs/1704.01445v1|http://arxiv.org/pdf/1704.01445v1|Bayesian Inference of Log Determinants|bayesian infer log determin|The log-determinant of a kernel matrix appears in a variety of machine learning problems, ranging from determinantal point processes and generalized Markov random fields, through to the training of Gaussian processes. Exact calculation of this term is often intractable when the size of the kernel matrix exceeds a few thousand. In the spirit of probabilistic numerics, we reinterpret the problem of computing the log-determinant as a Bayesian inference problem. In particular, we combine prior knowledge in the form of bounds from matrix theory and evidence derived from stochastic trace estimation to obtain probabilistic estimates for the log-determinant and its associated uncertainty within a given computational budget. Beyond its novelty and theoretic appeal, the performance of our proposal is competitive with state-of-the-art approaches to approximating the log-determinant, while also quantifying the uncertainty due to budget-constrained evidence.|log determin kernel matrix appear varieti machin learn problem rang determinant point process general markov random field train gaussian process exact calcul term often intract size kernel matrix exceed thousand spirit probabilist numer reinterpret problem comput log determin bayesian infer problem particular combin prior knowledg form bound matrix theori evid deriv stochast trace estim obtain probabilist estim log determin associ uncertainti within given comput budget beyond novelti theoret appeal perform propos competit state art approach approxim log determin also quantifi uncertainti due budget constrain evid|['Jack Fitzsimons', 'Kurt Cutajar', 'Michael Osborne', 'Stephen Roberts', 'Maurizio Filippone']|['stat.ML', 'cs.NA', 'stat.CO']
2017-04-07T11:23:00Z|2017-04-05T13:54:29Z|http://arxiv.org/abs/1704.01430v1|http://arxiv.org/pdf/1704.01430v1|Detecting confounding in multivariate linear models via spectral   analysis|detect confound multivari linear model via spectral analysi|We study a model where one target variable Y is correlated with a vector X:=(X_1,...,X_d) of predictor variables being potential causes of Y. We describe a method that infers to what extent the statistical dependences between X and Y are due to the influence of X on Y and to what extent due to a hidden common cause (confounder) of X and Y. The method relies on concentration of measure results for large dimensions d and an independence assumption stating that, in the absence of confounding, the vector of regression coefficients describing the influence of each X on Y typically has `generic orientation' relative to the eigenspaces of the covariance matrix of X. For the special case of a scalar confounder we show that confounding typically spoils this generic orientation in a characteristic way that can be used to quantitatively estimate the amount of confounding.|studi model one target variabl correl vector predictor variabl potenti caus describ method infer extent statist depend due influenc extent due hidden common caus confound method reli concentr measur result larg dimens independ assumpt state absenc confound vector regress coeffici describ influenc typic generic orient relat eigenspac covari matrix special case scalar confound show confound typic spoil generic orient characterist way use quantit estim amount confound|['Dominik Janzing', 'Bernhard Schoelkopf']|['stat.ML']
2017-04-07T11:23:00Z|2017-04-05T12:34:20Z|http://arxiv.org/abs/1704.01382v1|http://arxiv.org/pdf/1704.01382v1|On the construction of probabilistic Newton-type algorithms|construct probabilist newton type algorithm|It has recently been shown that many of the existing quasi-Newton algorithms can be formulated as learning algorithms, capable of learning local models of the cost functions. Importantly, this understanding allows us to safely start assembling probabilistic Newton-type algorithms, applicable in situations where we only have access to noisy observations of the cost function and its derivatives. This is where our interest lies.   We make contributions to the use of the non-parametric and probabilistic Gaussian process models in solving these stochastic optimisation problems. Specifically, we present a new algorithm that unites these approximations together with recent probabilistic line search routines to deliver a probabilistic quasi-Newton approach.   We also show that the probabilistic optimisation algorithms deliver promising results on challenging nonlinear system identification problems where the very nature of the problem is such that we can only access the cost function and its derivative via noisy observations, since there are no closed-form expressions available.|recent shown mani exist quasi newton algorithm formul learn algorithm capabl learn local model cost function import understand allow us safe start assembl probabilist newton type algorithm applic situat onli access noisi observ cost function deriv interest lie make contribut use non parametr probabilist gaussian process model solv stochast optimis problem specif present new algorithm unit approxim togeth recent probabilist line search routin deliv probabilist quasi newton approach also show probabilist optimis algorithm deliv promis result challeng nonlinear system identif problem veri natur problem onli access cost function deriv via noisi observ sinc close form express avail|['Adrian G. Wills', 'Thomas B. Schön']|['stat.ML']
2017-04-07T11:23:04Z|2017-04-05T08:48:01Z|http://arxiv.org/abs/1704.01312v1|http://arxiv.org/pdf/1704.01312v1|On Generalization and Regularization in Deep Learning|general regular deep learn|Why do large neural network generalize so well on complex tasks such as image classification or speech recognition? What exactly is the role regularization for them? These are arguably among the most important open questions in machine learning today. In a recent and thought provoking paper [C. Zhang et al.] several authors performed a number of numerical experiments that hint at the need for novel theoretical concepts to account for this phenomenon. The paper stirred quit a lot of excitement among the machine learning community but at the same time it created some confusion as discussions on OpenReview.net testifies. The aim of this pedagogical paper is to make this debate accessible to a wider audience of data scientists without advanced theoretical knowledge in statistical learning. The focus here is on explicit mathematical definitions and on a discussion of relevant concepts, not on proofs for which we provide references.|whi larg neural network general well complex task imag classif speech recognit exact role regular arguabl among import open question machin learn today recent thought provok paper zhang et al sever author perform number numer experi hint need novel theoret concept account phenomenon paper stir quit lot excit among machin learn communiti time creat confus discuss openreview net testifi aim pedagog paper make debat access wider audienc data scientist without advanc theoret knowledg statist learn focus explicit mathemat definit discuss relev concept proof provid refer|['Pirmin Lemberger']|['stat.ML', 'cs.LG', 'math.ST', 'stat.TH', '62-01']
2017-04-07T11:23:04Z|2017-04-05T06:39:51Z|http://arxiv.org/abs/1704.01280v1|http://arxiv.org/pdf/1704.01280v1|Revisiting the problem of audio-based hit song prediction using   convolutional neural networks|revisit problem audio base hit song predict use convolut neural network|Being able to predict whether a song can be a hit has impor- tant applications in the music industry. Although it is true that the popularity of a song can be greatly affected by exter- nal factors such as social and commercial influences, to which degree audio features computed from musical signals (whom we regard as internal factors) can predict song popularity is an interesting research question on its own. Motivated by the recent success of deep learning techniques, we attempt to ex- tend previous work on hit song prediction by jointly learning the audio features and prediction models using deep learning. Specifically, we experiment with a convolutional neural net- work model that takes the primitive mel-spectrogram as the input for feature learning, a more advanced JYnet model that uses an external song dataset for supervised pre-training and auto-tagging, and the combination of these two models. We also consider the inception model to characterize audio infor- mation in different scales. Our experiments suggest that deep structures are indeed more accurate than shallow structures in predicting the popularity of either Chinese or Western Pop songs in Taiwan. We also use the tags predicted by JYnet to gain insights into the result of different models.|abl predict whether song hit impor tant applic music industri although true popular song great affect exter nal factor social commerci influenc degre audio featur comput music signal regard intern factor predict song popular interest research question motiv recent success deep learn techniqu attempt ex tend previous work hit song predict joint learn audio featur predict model use deep learn specif experi convolut neural net work model take primit mel spectrogram input featur learn advanc jynet model use extern song dataset supervis pre train auto tag combin two model also consid incept model character audio infor mation differ scale experi suggest deep structur inde accur shallow structur predict popular either chines western pop song taiwan also use tag predict jynet gain insight result differ model|['Li-Chia Yang', 'Szu-Yu Chou', 'Jen-Yu Liu', 'Yi-Hsuan Yang', 'Yi-An Chen']|['cs.SD', 'cs.LG', 'stat.ML']
2017-04-07T11:23:04Z|2017-04-05T03:26:41Z|http://arxiv.org/abs/1704.01255v1|http://arxiv.org/pdf/1704.01255v1|Linear Additive Markov Processes|linear addit markov process|We introduce LAMP: the Linear Additive Markov Process. Transitions in LAMP may be influenced by states visited in the distant history of the process, but unlike higher-order Markov processes, LAMP retains an efficient parametrization. LAMP also allows the specific dependence on history to be learned efficiently from data. We characterize some theoretical properties of LAMP, including its steady-state and mixing time. We then give an algorithm based on alternating minimization to learn LAMP models from data. Finally, we perform a series of real-world experiments to show that LAMP is more powerful than first-order Markov processes, and even holds its own against deep sequential models (LSTMs) with a negligible increase in parameter complexity.|introduc lamp linear addit markov process transit lamp may influenc state visit distant histori process unlik higher order markov process lamp retain effici parametr lamp also allow specif depend histori learn effici data character theoret properti lamp includ steadi state mix time give algorithm base altern minim learn lamp model data final perform seri real world experi show lamp power first order markov process even hold deep sequenti model lstms neglig increas paramet complex|['Ravi Kumar', 'Maithra Raghu', 'Tamas Sarlos', 'Andrew Tomkins']|['cs.LG', 'stat.ML']
2017-04-07T11:23:04Z|2017-04-05T00:09:37Z|http://arxiv.org/abs/1704.01223v1|http://arxiv.org/pdf/1704.01223v1|Greedy Sampling of Graph Signals|greedi sampl graph signal|Sampling is a fundamental topic in graph signal processing, having found applications in estimation, clustering, and video compression. In contrast to traditional signal processing, the irregularity of the signal domain makes selecting a sampling set non-trivial and hard to analyze. Indeed, though conditions for graph signal interpolation from noiseless samples exist, they do not lead to a unique sampling set. Thus, the presence of noise makes sampling set selection a hard combinatorial problem. Although greedy sampling schemes have become ubiquitous in practice, they have no performance guarantee. This work takes a twofold approach to address this issue. First, universal performance bounds are derived for the interpolation of stochastic graph signals from noisy samples. In contrast to currently available bounds, they are not restricted to specific sampling schemes and hold for any sampling sets. Second, this paper provides near-optimal guarantees for greedy sampling by introducing the concept of approximate submodularity and updating the classical greedy bound. It then provides explicit bounds on the approximate supermodularity of the interpolation mean-square error showing that it can be optimized with worst-case guarantees using greedy search even though it is not supermodular. Simulations illustrate the derived bound for different graph models and show an application of graph signal sampling to reduce the complexity of kernel principal component analysis.|sampl fundament topic graph signal process found applic estim cluster video compress contrast tradit signal process irregular signal domain make select sampl set non trivial hard analyz inde though condit graph signal interpol noiseless sampl exist lead uniqu sampl set thus presenc nois make sampl set select hard combinatori problem although greedi sampl scheme becom ubiquit practic perform guarante work take twofold approach address issu first univers perform bound deriv interpol stochast graph signal noisi sampl contrast current avail bound restrict specif sampl scheme hold ani sampl set second paper provid near optim guarante greedi sampl introduc concept approxim submodular updat classic greedi bound provid explicit bound approxim supermodular interpol mean squar error show optim worst case guarante use greedi search even though supermodular simul illustr deriv bound differ graph model show applic graph signal sampl reduc complex kernel princip compon analysi|['Luiz F. O. Chamon', 'Alejandro Ribeiro']|['cs.IT', 'cs.SI', 'math.IT', 'stat.ML']
2017-04-07T11:23:05Z|2017-04-04T20:07:26Z|http://arxiv.org/abs/1704.01168v1|http://arxiv.org/pdf/1704.01168v1|Learning Approximately Objective Priors|learn approxim object prior|In modern probabilistic learning we often wish to perform automatic inference for Bayesian models. However, informative prior distributions can be costly and difficult to elicit, and, as a consequence, flat priors are often chosen with the hope that they are reasonably uninformative. Objective priors such as the Jeffreys and reference priors are generally preferable over flat priors but are not tractable to derive for many models of interest. We address this issue by proposing techniques for learning reference prior approximations: we select a parametric family and optimize a lower bound on the reference prior objective to find the member of the family that serves as a good approximation. Moreover, optimization can be made derivation-free via differentiable Monte Carlo expectations. We experimentally demonstrate the method's effectiveness by recovering Jeffreys priors and learning the Variational Autoencoder's reference prior.|modern probabilist learn often wish perform automat infer bayesian model howev inform prior distribut cost difficult elicit consequ flat prior often chosen hope reason uninform object prior jeffrey refer prior general prefer flat prior tractabl deriv mani model interest address issu propos techniqu learn refer prior approxim select parametr famili optim lower bound refer prior object find member famili serv good approxim moreov optim made deriv free via differenti mont carlo expect experiment demonstr method effect recov jeffrey prior learn variat autoencod refer prior|['Eric Nalisnick', 'Padhraic Smyth']|['stat.ML', 'stat.CO']
2017-04-07T11:23:05Z|2017-04-04T16:18:07Z|http://arxiv.org/abs/1704.01087v1|http://arxiv.org/pdf/1704.01087v1|Probabilistic Search for Structured Data via Probabilistic Programming   and Nonparametric Bayes|probabilist search structur data via probabilist program nonparametr bay|Databases are widespread, yet extracting relevant data can be difficult. Without substantial domain knowledge, multivariate search queries often return sparse or uninformative results. This paper introduces an approach for searching structured data based on probabilistic programming and nonparametric Bayes. Users specify queries in a probabilistic language that combines standard SQL database search operators with an information theoretic ranking function called predictive relevance. Predictive relevance can be calculated by a fast sparse matrix algorithm based on posterior samples from CrossCat, a nonparametric Bayesian model for high-dimensional, heterogeneously-typed data tables. The result is a flexible search technique that applies to a broad class of information retrieval problems, which we integrate into BayesDB, a probabilistic programming platform for probabilistic data analysis. This paper demonstrates applications to databases of US colleges, global macroeconomic indicators of public health, and classic cars. We found that human evaluators often prefer the results from probabilistic search to results from a standard baseline.|databas widespread yet extract relev data difficult without substanti domain knowledg multivari search queri often return spars uninform result paper introduc approach search structur data base probabilist program nonparametr bay user specifi queri probabilist languag combin standard sql databas search oper inform theoret rank function call predict relev predict relev calcul fast spars matrix algorithm base posterior sampl crosscat nonparametr bayesian model high dimension heterogen type data tabl result flexibl search techniqu appli broad class inform retriev problem integr bayesdb probabilist program platform probabilist data analysi paper demonstr applic databas us colleg global macroeconom indic public health classic car found human evalu often prefer result probabilist search result standard baselin|['Feras Saad', 'Leonardo Casarsa', 'Vikash Mansinghka']|['cs.AI', 'cs.DB', 'cs.LG', 'stat.ML']
2017-04-07T11:23:05Z|2017-04-04T15:56:55Z|http://arxiv.org/abs/1704.01079v1|http://arxiv.org/pdf/1704.01079v1|Homotopy Parametric Simplex Method for Sparse Learning|homotopi parametr simplex method spars learn|High dimensional sparse learning has imposed a great computational challenge to large scale data analysis. In this paper, we are interested in a broad class of sparse learning approaches formulated as linear programs parametrized by a {\em regularization factor}, and solve them by the parametric simplex method (PSM). Our parametric simplex method offers significant advantages over other competing methods: (1) PSM naturally obtains the complete solution path for all values of the regularization parameter; (2) PSM provides a high precision dual certificate stopping criterion; (3) PSM yields sparse solutions through very few iterations, and the solution sparsity significantly reduces the computational cost per iteration. Particularly, we demonstrate the superiority of PSM over various sparse learning approaches, including Dantzig selector for sparse linear regression, LAD-Lasso for sparse robust linear regression, CLIME for sparse precision matrix estimation, sparse differential network estimation, and sparse Linear Programming Discriminant (LPD) analysis. We then provide sufficient conditions under which PSM always outputs sparse solutions such that its computational performance can be significantly boosted. Thorough numerical experiments are provided to demonstrate the outstanding performance of the PSM method.|high dimension spars learn impos great comput challeng larg scale data analysi paper interest broad class spars learn approach formul linear program parametr em regular factor solv parametr simplex method psm parametr simplex method offer signific advantag compet method psm natur obtain complet solut path valu regular paramet psm provid high precis dual certif stop criterion psm yield spars solut veri iter solut sparsiti signific reduc comput cost per iter particular demonstr superior psm various spars learn approach includ dantzig selector spars linear regress lad lasso spars robust linear regress clime spars precis matrix estim spars differenti network estim spars linear program discrimin lpd analysi provid suffici condit psm alway output spars solut comput perform signific boost thorough numer experi provid demonstr outstand perform psm method|['Haotian Pang', 'Tuo Zhao', 'Robert Vanderbei', 'Han Liu']|['cs.LG', 'math.OC', 'stat.ML']
2017-04-07T11:23:05Z|2017-04-04T14:46:00Z|http://arxiv.org/abs/1704.01041v1|http://arxiv.org/pdf/1704.01041v1|Polynomial Time and Sample Complexity for Non-Gaussian Component   Analysis: Spectral Methods|polynomi time sampl complex non gaussian compon analysi spectral method|The problem of Non-Gaussian Component Analysis (NGCA) is about finding a maximal low-dimensional subspace $E$ in $\mathbb{R}^n$ so that data points projected onto $E$ follow a non-gaussian distribution. Although this is an appropriate model for some real world data analysis problems, there has been little progress on this problem over the last decade.   In this paper, we attempt to address this state of affairs in two ways. First, we give a new characterization of standard gaussian distributions in high-dimensions, which lead to effective tests for non-gaussianness. Second, we propose a simple algorithm, \emph{Reweighted PCA}, as a method for solving the NGCA problem. We prove that for a general unknown non-gaussian distribution, this algorithm recovers at least one direction in $E$, with sample and time complexity depending polynomially on the dimension of the ambient space. We conjecture that the algorithm actually recovers the entire $E$.|problem non gaussian compon analysi ngca find maxim low dimension subspac mathbb data point project onto follow non gaussian distribut although appropri model real world data analysi problem littl progress problem last decad paper attempt address state affair two way first give new character standard gaussian distribut high dimens lead effect test non gaussian second propos simpl algorithm emph reweight pca method solv ngca problem prove general unknown non gaussian distribut algorithm recov least one direct sampl time complex depend polynomi dimens ambient space conjectur algorithm actual recov entir|['Yan Shuo Tan', 'Roman Vershynin']|['cs.LG', 'math.PR', 'stat.ML', '68Q87']
2017-04-07T11:23:05Z|2017-04-04T12:28:12Z|http://arxiv.org/abs/1704.00979v1|http://arxiv.org/pdf/1704.00979v1|Optic Disc and Cup Segmentation Methods for Glaucoma Detection with   Modification of U-Net Convolutional Neural Network|optic disc cup segment method glaucoma detect modif net convolut neural network|Glaucoma is the second leading cause of blindness all over the world, with approximately 60 million cases reported worldwide in 2010. If undiagnosed in time, glaucoma causes irreversible damage to the optic nerve leading to blindness. The optic nerve head examination, which involves measurement of cup-to-disc ratio, is considered one of the most valuable methods of structural diagnosis of the disease. Estimation of cup-to-disc ratio requires segmentation of optic disc and optic cup on eye fundus images and can be performed by modern computer vision algorithms. This work presents universal approach for automatic optic disc and cup segmentation, which is based on deep learning, namely, modification of U-Net convolutional neural network. Our experiments include comparison with the best known methods on publicly available databases DRIONS-DB, RIM-ONE v.3, DRISHTI-GS. For both optic disc and cup segmentation, our method achieves quality comparable to current state-of-the-art methods, outperforming them in terms of the prediction time.|glaucoma second lead caus blind world approxim million case report worldwid undiagnos time glaucoma caus irrevers damag optic nerv lead blind optic nerv head examin involv measur cup disc ratio consid one valuabl method structur diagnosi diseas estim cup disc ratio requir segment optic disc optic cup eye fundus imag perform modern comput vision algorithm work present univers approach automat optic disc cup segment base deep learn name modif net convolut neural network experi includ comparison best known method public avail databas drion db rim one drishti gs optic disc cup segment method achiev qualiti compar current state art method outperform term predict time|['Artem Sevastopolsky']|['cs.CV', 'stat.ML']
2017-04-07T11:23:05Z|2017-04-04T11:40:20Z|http://arxiv.org/abs/1704.00963v1|http://arxiv.org/pdf/1704.00963v1|Bayesian optimization with virtual derivative sign observations|bayesian optim virtual deriv sign observ|Bayesian optimization (BO) is a global optimization strategy designed to find the minimum of expensive black-box functions $g$ typically defined on a continuous sets of $\mathcal{R}^d$. Using a Gaussian process (GP) as a surrogate model for the objective and an acquisition function to systematically search its domain, BO strategies aim to minimize the amount of samples required to find the minimum of $g$. Although currently available acquisition functions address this goal with different degree of success, an over-exploration effect of the contour of $g$ is typically observed. This is due to the myopic nature of most acquisitions that greedily try to over-reduce uncertainty in the border of the search domain. In most real problems, however, like the configuration of machine learning algorithms, the function domain is conservatively large and with a high probability the global minimum is not at the boundary. We propose a method to incorporate this knowledge into the searching process by adding virtual derivative observations at the borders of the search space. We use the properties of GP models that allow us to easily impose conditions on the partial derivatives of the objective. The method is applicable with any acquisition function, it is easy to use and consistently reduces the number of evaluations required to find the minimum of $g$ irrespective of the acquisition used. We illustrate the benefits our approach in a simulation study with a battery of objective functions.|bayesian optim bo global optim strategi design find minimum expens black box function typic defin continu set mathcal use gaussian process gp surrog model object acquisit function systemat search domain bo strategi aim minim amount sampl requir find minimum although current avail acquisit function address goal differ degre success explor effect contour typic observ due myopic natur acquisit greedili tri reduc uncertainti border search domain real problem howev like configur machin learn algorithm function domain conserv larg high probabl global minimum boundari propos method incorpor knowledg search process ad virtual deriv observ border search space use properti gp model allow us easili impos condit partial deriv object method applic ani acquisit function easi use consist reduc number evalu requir find minimum irrespect acquisit use illustr benefit approach simul studi batteri object function|['Eero Siivola', 'Aki Vehtari', 'Jarno Vanhatalo', 'Javier González']|['stat.ML', 'stat.CO', 'stat.ME']
2017-04-07T11:23:09Z|2017-04-03T22:28:25Z|http://arxiv.org/abs/1704.00828v1|http://arxiv.org/pdf/1704.00828v1|A Probabilistic Linear Genetic Programming with Stochastic Context-Free   Grammar for solving Symbolic Regression problems|probabilist linear genet program stochast context free grammar solv symbol regress problem|Traditional Linear Genetic Programming (LGP) algorithms are based only on the selection mechanism to guide the search. Genetic operators combine or mutate random portions of the individuals, without knowing if the result will lead to a fitter individual. Probabilistic Model Building Genetic Programming (PMB-GP) methods were proposed to overcome this issue through a probability model that captures the structure of the fit individuals and use it to sample new individuals. This work proposes the use of LGP with a Stochastic Context-Free Grammar (SCFG), that has a probability distribution that is updated according to selected individuals. We proposed a method for adapting the grammar into the linear representation of LGP. Tests performed with the proposed probabilistic method, and with two hybrid approaches, on several symbolic regression benchmark problems show that the results are statistically better than the obtained by the traditional LGP.|tradit linear genet program lgp algorithm base onli select mechan guid search genet oper combin mutat random portion individu without know result lead fitter individu probabilist model build genet program pmb gp method propos overcom issu probabl model captur structur fit individu use sampl new individu work propos use lgp stochast context free grammar scfg probabl distribut updat accord select individu propos method adapt grammar linear represent lgp test perform propos probabilist method two hybrid approach sever symbol regress benchmark problem show result statist better obtain tradit lgp|['Léo Françoso Dal Piccol Sotto', 'Vinícius Veloso de Melo']|['cs.NE', 'math.PR', 'stat.ML']
2017-04-07T11:23:09Z|2017-04-03T20:16:58Z|http://arxiv.org/abs/1704.00794v1|http://arxiv.org/pdf/1704.00794v1|Time Series Cluster Kernel for Learning Similarities between   Multivariate Time Series with Missing Data|time seri cluster kernel learn similar multivari time seri miss data|Similarity-based approaches represent a promising direction for time series analysis. However, many such methods rely on parameter tuning and have shortcomings if the time series are multivariate (MTS) and contain missing data. In this paper, we address these challenges within the powerful context of kernel methods by proposing the robust \emph{time series cluster kernel} (TCK). The approach taken is to leverage the missing data handling properties of Gaussian mixture models (GMM) augmented with informative prior distributions. An ensemble learning approach is exploited to ensure robustness to parameters by combining the clustering results of many GMM to form the final kernel.   We evaluate the TCK on synthetic and real data and compare to other state-of-the-art techniques. The experimental results demonstrate that the TCK is robust to parameter choices, provides competitive results for MTS without missing data and outstanding results for missing data.|similar base approach repres promis direct time seri analysi howev mani method reli paramet tune shortcom time seri multivari mts contain miss data paper address challeng within power context kernel method propos robust emph time seri cluster kernel tck approach taken leverag miss data handl properti gaussian mixtur model gmm augment inform prior distribut ensembl learn approach exploit ensur robust paramet combin cluster result mani gmm form final kernel evalu tck synthet real data compar state art techniqu experiment result demonstr tck robust paramet choic provid competit result mts without miss data outstand result miss data|['Karl Øyvind Mikalsen', 'Filippo Maria Bianchi', 'Cristina Soguero-Ruiz', 'Robert Jenssen']|['stat.ML', 'cs.LG']
2017-04-07T11:23:09Z|2017-04-03T19:16:06Z|http://arxiv.org/abs/1704.00773v1|http://arxiv.org/pdf/1704.00773v1|A comparative study of counterfactual estimators|compar studi counterfactu estim|We provide a comparative study of several widely used off-policy estimators (Empirical Average, Basic Importance Sampling and Normalized Importance Sampling), detailing the different regimes where they are individually suboptimal. We then exhibit properties optimal estimators should possess. In the case where examples have been gathered using multiple policies, we show that fused estimators dominate basic ones but can still be improved.|provid compar studi sever wide use polici estim empir averag basic import sampl normal import sampl detail differ regim individu suboptim exhibit properti optim estim possess case exampl gather use multipl polici show fuse estim domin basic one still improv|['Thomas Nedelec', 'Nicolas Le Roux', 'Vianney Perchet']|['stat.ML', 'cs.LG']
2017-04-07T11:23:09Z|2017-04-03T19:08:54Z|http://arxiv.org/abs/1704.00767v1|http://arxiv.org/pdf/1704.00767v1|Geometric Insights into Support Vector Machine Behavior using the KKT   Conditions|geometr insight support vector machin behavior use kkt condit|The Support Vector Machine (SVM) is a powerful and widely used classification algorithm. Its performance is well known to be impacted by a tuning parameter which is frequently selected by cross-validation. This paper uses the Karush-Kuhn-Tucker conditions to provide rigorous mathematical proof for new insights into the behavior of SVM in the large and small tuning parameter regimes. These insights provide perhaps unexpected relationships between SVM and naive Bayes and maximal data piling directions. We explore how characteristics of the training data affect the behavior of SVM in many cases including: balanced vs. unbalanced classes, low vs. high dimension, separable vs. non-separable data. These results present a simple explanation of SVM's behavior as a function of the tuning parameter. We also elaborate on the geometry of complete data piling directions in high dimensional space. The results proved in this paper suggest important implications for tuning SVM with cross-validation.|support vector machin svm power wide use classif algorithm perform well known impact tune paramet frequent select cross valid paper use karush kuhn tucker condit provid rigor mathemat proof new insight behavior svm larg small tune paramet regim insight provid perhap unexpect relationship svm naiv bay maxim data pile direct explor characterist train data affect behavior svm mani case includ balanc vs unbalanc class low vs high dimens separ vs non separ data result present simpl explan svm behavior function tune paramet also elabor geometri complet data pile direct high dimension space result prove paper suggest import implic tune svm cross valid|['Iain Carmichael', 'J. S. Marron']|['stat.ML', 'cs.LG']
2017-04-07T11:23:09Z|2017-04-03T18:37:12Z|http://arxiv.org/abs/1704.00756v1|http://arxiv.org/pdf/1704.00756v1|Multi-Advisor Reinforcement Learning|multi advisor reinforc learn|This article deals with a novel branch of Separation of Concerns, called Multi-Advisor Reinforcement Learning (MAd-RL), where a single-agent RL problem is distributed to $n$ learners, called advisors. Each advisor tries to solve the problem with a different focus. Their advice is then communicated to an aggregator, which is in control of the system. For the local training, three off-policy bootstrapping methods are proposed and analysed: local-max bootstraps with the local greedy action, rand-policy bootstraps with respect to the random policy, and agg-policy bootstraps with respect to the aggregator's greedy policy. MAd-RL is positioned as a generalisation of Reinforcement Learning with Ensemble methods. An experiment is held on a simplified version of the Ms. Pac-Man Atari game. The results confirm the theoretical relative strengths and weaknesses of each method.|articl deal novel branch separ concern call multi advisor reinforc learn mad rl singl agent rl problem distribut learner call advisor advisor tri solv problem differ focus advic communic aggreg control system local train three polici bootstrap method propos analys local max bootstrap local greedi action rand polici bootstrap respect random polici agg polici bootstrap respect aggreg greedi polici mad rl posit generalis reinforc learn ensembl method experi held simplifi version ms pac man atari game result confirm theoret relat strength weak method|['Romain Laroche', 'Mehdi Fatemi', 'Joshua Romoff', 'Harm van Seijen']|['cs.LG', 'cs.AI', 'stat.ML']
2017-04-07T11:23:09Z|2017-04-03T17:49:02Z|http://arxiv.org/abs/1704.00708v1|http://arxiv.org/pdf/1704.00708v1|No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified   Geometric Analysis|spurious local minima nonconvex low rank problem unifi geometr analysi|In this paper we develop a new framework that captures the common landscape underlying the common non-convex low-rank matrix problems including matrix sensing, matrix completion and robust PCA. In particular, we show for all above problems (including asymmetric cases): 1) all local minima are also globally optimal; 2) no high-order saddle points exists. These results explain why simple algorithms such as stochastic gradient descent have global converge, and efficiently optimize these non-convex objective functions in practice. Our framework connects and simplifies the existing analyses on optimization landscapes for matrix sensing and symmetric matrix completion. The framework naturally leads to new results for asymmetric matrix completion and robust PCA.|paper develop new framework captur common landscap common non convex low rank matrix problem includ matrix sens matrix complet robust pca particular show abov problem includ asymmetr case local minima also global optim high order saddl point exist result explain whi simpl algorithm stochast gradient descent global converg effici optim non convex object function practic framework connect simplifi exist analys optim landscap matrix sens symmetr matrix complet framework natur lead new result asymmetr matrix complet robust pca|['Rong Ge', 'Chi Jin', 'Yi Zheng']|['cs.LG', 'math.OC', 'stat.ML']
2017-04-07T11:23:09Z|2017-04-03T15:33:10Z|http://arxiv.org/abs/1704.01184v1|http://arxiv.org/pdf/1704.01184v1|On the Unreported-Profile-is-Negative Assumption for Predictive   Cheminformatics|unreport profil negat assumpt predict cheminformat|The study of compound-target binding profiles has been a central theme in cheminformatics. For data repositories that only provide positive binding profiles, a popular assumption is that all unreported profiles are negative. In this paper, we caution audience not to take such assumptions for granted. Under a problem setting where binding profiles are used as features to train predictive models, we present empirical evidence that (1) predictive performance degrades when the assumption fails and (2) explicit recovery of unreported profiles improves predictive performance. In particular, we propose a joint framework of profile recovery and supervised learning, which shows further performance improvement. Our study not only calls for more careful treatment of unreported profiles in cheminformatics, but also initiates a new machine learning problem which we called Learning with Positive and Unknown Features.|studi compound target bind profil central theme cheminformat data repositori onli provid posit bind profil popular assumpt unreport profil negat paper caution audienc take assumpt grant problem set bind profil use featur train predict model present empir evid predict perform degrad assumpt fail explicit recoveri unreport profil improv predict perform particular propos joint framework profil recoveri supervis learn show perform improv studi onli call care treatment unreport profil cheminformat also initi new machin learn problem call learn posit unknown featur|['Chao Lan', 'Sai Nivedita Chandrasekaran', 'Jun Huan']|['cs.LG', 'physics.chem-ph', 'stat.ML']
2017-04-07T11:23:09Z|2017-04-03T15:25:47Z|http://arxiv.org/abs/1704.00637v1|http://arxiv.org/pdf/1704.00637v1|Semi-Supervised Generation with Cluster-aware Generative Models|semi supervis generat cluster awar generat model|Deep generative models trained with large amounts of unlabelled data have proven to be powerful within the domain of unsupervised learning. Many real life data sets contain a small amount of labelled data points, that are typically disregarded when training generative models. We propose the Cluster-aware Generative Model, that uses unlabelled information to infer a latent representation that models the natural clustering of the data, and additional labelled data points to refine this clustering. The generative performances of the model significantly improve when labelled information is exploited, obtaining a log-likelihood of -79.38 nats on permutation invariant MNIST, while also achieving competitive semi-supervised classification accuracies. The model can also be trained fully unsupervised, and still improve the log-likelihood performance with respect to related methods.|deep generat model train larg amount unlabel data proven power within domain unsupervis learn mani real life data set contain small amount label data point typic disregard train generat model propos cluster awar generat model use unlabel inform infer latent represent model natur cluster data addit label data point refin cluster generat perform model signific improv label inform exploit obtain log likelihood nat permut invari mnist also achiev competit semi supervis classif accuraci model also train fulli unsupervis still improv log likelihood perform respect relat method|['Lars Maaløe', 'Marco Fraccaro', 'Ole Winther']|['stat.ML', 'cs.AI', 'cs.LG']
2017-04-07T11:23:09Z|2017-04-03T12:03:39Z|http://arxiv.org/abs/1704.00541v1|http://arxiv.org/pdf/1704.00541v1|Dictionary-based Tensor Canonical Polyadic Decomposition|dictionari base tensor canon polyad decomposit|To ensure interpretability of extracted sources in tensor decomposition, we introduce in this paper a dictionary-based tensor canonical polyadic decomposition which enforces one factor to belong exactly to a known dictionary. A new formulation of sparse coding is proposed which enables high dimensional tensors dictionary-based canonical polyadic decomposition. The benefits of using a dictionary in tensor decomposition models are explored both in terms of parameter identifiability and estimation accuracy. This is illustrated on the decomposition of simulated data and on the unmixing of hyperspectral images.|ensur interpret extract sourc tensor decomposit introduc paper dictionari base tensor canon polyad decomposit enforc one factor belong exact known dictionari new formul spars code propos enabl high dimension tensor dictionari base canon polyad decomposit benefit use dictionari tensor decomposit model explor term paramet identifi estim accuraci illustr decomposit simul data unmix hyperspectr imag|['Jérémy E. Cohen', 'Nicolas Gillis']|['stat.ML']
2017-04-07T11:23:09Z|2017-04-03T10:40:15Z|http://arxiv.org/abs/1704.00520v1|http://arxiv.org/pdf/1704.00520v1|Efficient acquisition rules for model-based approximate Bayesian   computation|effici acquisit rule model base approxim bayesian comput|Approximate Bayesian computation (ABC) is a method for Bayesian inference when the likelihood is unavailable but simulating from the model is possible. However, many ABC algorithms require a large number of simulations, which can be costly. To reduce the computational cost, surrogate models and Bayesian optimisation (BO) have been proposed. Bayesian optimisation enables one to intelligently decide where to evaluate the model next, but standard BO strategies are designed for optimisation and not specifically for ABC inference. Our paper addresses this gap in the literature. We propose a new acquisition rule that selects the next evaluation where the uncertainty in the posterior distribution is largest. Experiments show that the proposed method often produces the most accurate approximations, especially in high-dimensional cases or in the presence of strong prior information, compared to common alternatives.|approxim bayesian comput abc method bayesian infer likelihood unavail simul model possibl howev mani abc algorithm requir larg number simul cost reduc comput cost surrog model bayesian optimis bo propos bayesian optimis enabl one intellig decid evalu model next standard bo strategi design optimis specif abc infer paper address gap literatur propos new acquisit rule select next evalu uncertainti posterior distribut largest experi show propos method often produc accur approxim especi high dimension case presenc strong prior inform compar common altern|['Marko Järvenpää', 'Michael U. Gutmann', 'Aki Vehtari', 'Pekka Marttinen']|['stat.ML', 'stat.CO', 'stat.ME']
2017-04-07T11:23:13Z|2017-04-03T10:25:22Z|http://arxiv.org/abs/1704.00514v1|http://arxiv.org/pdf/1704.00514v1|Multi-Task Learning of Keyphrase Boundary Classification|multi task learn keyphras boundari classif|Keyphrase boundary classification (KBC) is the task of detecting keyphrases in scientific articles and labelling them with respect to predefined types. Although important in practice, this task is so far underexplored, partly due to the lack of labelled data. To overcome this, we explore several auxiliary tasks, including semantic super-sense tagging and identification of multi-word expressions, and cast the task as a multi-task learning problem with deep recurrent neural networks. Our multi-task models perform significantly better than previous state of the art approaches on two scientific KBC datasets, particularly for long keyphrases.|keyphras boundari classif kbc task detect keyphras scientif articl label respect predefin type although import practic task far underexplor part due lack label data overcom explor sever auxiliari task includ semant super sens tag identif multi word express cast task multi task learn problem deep recurr neural network multi task model perform signific better previous state art approach two scientif kbc dataset particular long keyphras|['Isabelle Augenstein', 'Anders Søgaard']|['cs.CL', 'cs.AI', 'stat.ML']
2017-04-07T11:23:13Z|2017-04-02T23:32:14Z|http://arxiv.org/abs/1704.00387v1|http://arxiv.org/pdf/1704.00387v1|Identifying networks with common organizational principles|identifi network common organiz principl|Many complex systems can be represented as networks, and the problem of network comparison is becoming increasingly relevant. There are many techniques for network comparison, from simply comparing network summary statistics to sophisticated but computationally costly alignment-based approaches. Yet it remains challenging to accurately cluster networks that are of a different size and density, but hypothesized to be structurally similar. In this paper, we address this problem by introducing a new network comparison methodology that is aimed at identifying common organizational principles in networks. The methodology is simple, intuitive and applicable in a wide variety of settings ranging from the functional classification of proteins to tracking the evolution of a world trade network.|mani complex system repres network problem network comparison becom increas relev mani techniqu network comparison simpli compar network summari statist sophist comput cost align base approach yet remain challeng accur cluster network differ size densiti hypothes structur similar paper address problem introduc new network comparison methodolog aim identifi common organiz principl network methodolog simpl intuit applic wide varieti set rang function classif protein track evolut world trade network|['Anatol E. Wegner', 'Luis Ospina-Forero', 'Robert E. Gaunt', 'Charlotte M. Deane', 'Gesine Reinert']|['stat.ML', 'cs.SI', 'physics.soc-ph']
2017-04-07T11:23:13Z|2017-04-02T21:32:34Z|http://arxiv.org/abs/1704.00367v1|http://arxiv.org/pdf/1704.00367v1|Provable Inductive Robust PCA via Iterative Hard Thresholding|provabl induct robust pca via iter hard threshold|The robust PCA problem, wherein, given an input data matrix that is the superposition of a low-rank matrix and a sparse matrix, we aim to separate out the low-rank and sparse components, is a well-studied problem in machine learning. One natural question that arises is that, as in the inductive setting, if features are provided as input as well, can we hope to do better? Answering this in the affirmative, the main goal of this paper is to study the robust PCA problem while incorporating feature information. In contrast to previous works in which recovery guarantees are based on the convex relaxation of the problem, we propose a simple iterative algorithm based on hard-thresholding of appropriate residuals. Under weaker assumptions than previous works, we prove the global convergence of our iterative procedure; moreover, it admits a much faster convergence rate and lesser computational complexity per iteration. In practice, through systematic synthetic and real data simulations, we confirm our theoretical findings regarding improvements obtained by using feature information.|robust pca problem wherein given input data matrix superposit low rank matrix spars matrix aim separ low rank spars compon well studi problem machin learn one natur question aris induct set featur provid input well hope better answer affirm main goal paper studi robust pca problem incorpor featur inform contrast previous work recoveri guarante base convex relax problem propos simpl iter algorithm base hard threshold appropri residu weaker assumpt previous work prove global converg iter procedur moreov admit much faster converg rate lesser comput complex per iter practic systemat synthet real data simul confirm theoret find regard improv obtain use featur inform|['U. N. Niranjan', 'Arun Rajkumar', 'Theja Tulabandhula']|['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']
2017-04-07T11:23:13Z|2017-04-02T08:01:30Z|http://arxiv.org/abs/1704.00260v1|http://arxiv.org/pdf/1704.00260v1|Aligned Image-Word Representations Improve Inductive Transfer Across   Vision-Language Tasks|align imag word represent improv induct transfer across vision languag task|A grand goal of computer vision is to build systems that learn visual representations over time that can be applied to many tasks. In this paper, we investigate a vision-language embedding as a core representation and show that it leads to better cross-task transfer than standard multi-task learning. In particular, the task of visual recognition is aligned to the task of visual question answering by forcing each to use the same word-region embeddings. We show this leads to greater inductive transfer from recognition to VQA than standard multitask learning. Visual recognition also improves, especially for categories that have relatively few recognition training labels but appear often in the VQA setting. Thus, our paper takes a small step towards creating more general vision systems by showing the benefit of interpretable, flexible, and trainable core representations.|grand goal comput vision build system learn visual represent time appli mani task paper investig vision languag embed core represent show lead better cross task transfer standard multi task learn particular task visual recognit align task visual question answer forc use word region embed show lead greater induct transfer recognit vqa standard multitask learn visual recognit also improv especi categori relat recognit train label appear often vqa set thus paper take small step toward creat general vision system show benefit interpret flexibl trainabl core represent|['Tanmay Gupta', 'Kevin Shih', 'Saurabh Singh', 'Derek Hoiem']|['cs.CV', 'cs.AI', 'cs.LG', 'cs.NE', 'stat.ML']
2017-04-07T11:23:13Z|2017-04-01T19:29:21Z|http://arxiv.org/abs/1704.00217v1|http://arxiv.org/pdf/1704.00217v1|Adversarial Connective-exploiting Networks for Implicit Discourse   Relation Classification|adversari connect exploit network implicit discours relat classif|Implicit discourse relation classification is of great challenge due to the lack of connectives as strong linguistic cues, which motivates the use of annotated implicit connectives to improve the recognition. We propose a feature imitation framework in which an implicit relation network is driven to learn from another neural network with access to connectives, and thus encouraged to extract similarly salient features for accurate classification. We develop an adversarial model to enable an adaptive imitation scheme through competition between the implicit network and a rival feature discriminator. Our method effectively transfers discriminability of connectives to the implicit features, and achieves state-of-the-art performance on the PDTB benchmark.|implicit discours relat classif great challeng due lack connect strong linguist cue motiv use annot implicit connect improv recognit propos featur imit framework implicit relat network driven learn anoth neural network access connect thus encourag extract similar salient featur accur classif develop adversari model enabl adapt imit scheme competit implicit network rival featur discrimin method effect transfer discrimin connect implicit featur achiev state art perform pdtb benchmark|['Lianhui Qin', 'Zhisong Zhang', 'Hai Zhao', 'Zhiting Hu', 'Eric P. Xing']|['cs.CL', 'cs.AI', 'cs.LG', 'stat.ML']
2017-04-07T11:23:13Z|2017-04-01T03:53:08Z|http://arxiv.org/abs/1704.00116v1|http://arxiv.org/pdf/1704.00116v1|Stochastic L-BFGS Revisited: Improved Convergence Rates and Practical   Acceleration Strategies|stochast bfgs revisit improv converg rate practic acceler strategi|We revisit the stochastic limited-memory BFGS (L-BFGS) algorithm. By proposing a new framework for analyzing convergence, we theoretically improve the (linear) convergence rates and computational complexities of the stochastic L-BFGS algorithms in previous works. In addition, we propose several practical acceleration strategies to speed up the empirical performance of such algorithms. We also provide theoretical analyses for most of the strategies. Experiments on large-scale logistic and ridge regression problems demonstrate that our proposed strategies yield significant improvements via-\`a-vis competing state-of-the-art algorithms.|revisit stochast limit memori bfgs bfgs algorithm propos new framework analyz converg theoret improv linear converg rate comput complex stochast bfgs algorithm previous work addit propos sever practic acceler strategi speed empir perform algorithm also provid theoret analys strategi experi larg scale logist ridg regress problem demonstr propos strategi yield signific improv via vis compet state art algorithm|['Renbo Zhao', 'William B. Haskell', 'Vincent Y. F. Tan']|['math.OC', 'cs.IT', 'math.IT', 'stat.ML']
2017-04-07T11:23:13Z|2017-04-04T00:50:58Z|http://arxiv.org/abs/1704.00112v2|http://arxiv.org/pdf/1704.00112v2|Configurable, Photorealistic Image Rendering and Ground Truth Synthesis   by Sampling Stochastic Grammars Representing Indoor Scenes|configur photorealist imag render ground truth synthesi sampl stochast grammar repres indoor scene|We propose the configurable rendering of massive quantities of photorealistic images with ground truth for the purposes of training, benchmarking, and diagnosing computer vision models. In contrast to the conventional (crowd-sourced) manual labeling of ground truth for a relatively modest number of RGB-D images captured by Kinect-like sensors, we devise a non-trivial configurable pipeline of algorithms capable of generating a potentially infinite variety of indoor scenes using a stochastic grammar, specifically, one represented by an attributed spatial And-Or graph. We employ physics-based rendering to synthesize photorealistic RGB images while automatically synthesizing detailed, per-pixel ground truth data, including visible surface depth and normal, object identity and material information, as well as illumination. Our pipeline is configurable inasmuch as it enables the precise customization and control of important attributes of the generated scenes. We demonstrate that our generated scenes achieve a performance similar to the NYU v2 Dataset on pre-trained deep learning models. By modifying pipeline components in a controllable manner, we furthermore provide diagnostics on common scene understanding tasks; eg., depth and surface normal prediction, semantic segmentation, etc.|propos configur render massiv quantiti photorealist imag ground truth purpos train benchmark diagnos comput vision model contrast convent crowd sourc manual label ground truth relat modest number rgb imag captur kinect like sensor devis non trivial configur pipelin algorithm capabl generat potenti infinit varieti indoor scene use stochast grammar specif one repres attribut spatial graph employ physic base render synthes photorealist rgb imag automat synthes detail per pixel ground truth data includ visibl surfac depth normal object ident materi inform well illumin pipelin configur inasmuch enabl precis custom control import attribut generat scene demonstr generat scene achiev perform similar nyu dataset pre train deep learn model modifi pipelin compon control manner furthermor provid diagnost common scene understand task eg depth surfac normal predict semant segment etc|['Chenfanfu Jiang', 'Yixin Zhu', 'Siyuan Qi', 'Siyuan Huang', 'Jenny Lin', 'Xingwen Guo', 'Lap-Fai Yu', 'Demetri Terzopoulos', 'Song-Chun Zhu']|['cs.CV', 'stat.ML']
2017-04-07T11:23:13Z|2017-03-31T21:13:08Z|http://arxiv.org/abs/1704.00060v1|http://arxiv.org/pdf/1704.00060v1|Exploiting gradients and Hessians in Bayesian optimization and Bayesian   quadrature|exploit gradient hessian bayesian optim bayesian quadratur|An exciting branch of machine learning research focuses on methods for learning, optimizing, and integrating unknown functions that are difficult or costly to evaluate. A popular Bayesian approach to this problem uses a Gaussian process (GP) to construct a posterior distribution over the function of interest given a set of observed measurements, and selects new points to evaluate using the statistics of this posterior. Here we extend these methods to exploit derivative information from the unknown function. We describe methods for Bayesian optimization (BO) and Bayesian quadrature (BQ) in settings where first and second derivatives may be evaluated along with the function itself. We perform sampling-based inference in order to incorporate uncertainty over hyperparameters, and show that both hyperparameter and function uncertainty decrease much more rapidly when using derivative information. Moreover, we introduce techniques for overcoming ill-conditioning issues that have plagued earlier methods for gradient-enhanced Gaussian processes and kriging. We illustrate the efficacy of these methods using applications to real and simulated Bayesian optimization and quadrature problems, and show that exploting derivatives can provide substantial gains over standard methods.|excit branch machin learn research focus method learn optim integr unknown function difficult cost evalu popular bayesian approach problem use gaussian process gp construct posterior distribut function interest given set observ measur select new point evalu use statist posterior extend method exploit deriv inform unknown function describ method bayesian optim bo bayesian quadratur bq set first second deriv may evalu along function perform sampl base infer order incorpor uncertainti hyperparamet show hyperparamet function uncertainti decreas much rapid use deriv inform moreov introduc techniqu overcom ill condit issu plagu earlier method gradient enhanc gaussian process krige illustr efficaci method use applic real simul bayesian optim quadratur problem show explot deriv provid substanti gain standard method|['Anqi Wu', 'Mikio C. Aoi', 'Jonathan W. Pillow']|['stat.ML']
2017-04-07T11:23:13Z|2017-03-31T19:25:00Z|http://arxiv.org/abs/1704.00028v1|http://arxiv.org/pdf/1704.00028v1|Improved Training of Wasserstein GANs|improv train wasserstein gan|Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes significant progress toward stable training of GANs, but can still generate low-quality samples or fail to converge in some settings. We find that these training failures are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to pathological behavior. We propose an alternative method for enforcing the Lipschitz constraint: instead of clipping weights, penalize the norm of the gradient of the critic with respect to its input. Our proposed method converges faster and generates higher-quality samples than WGAN with weight clipping. Finally, our method enables very stable GAN training: for the first time, we can train a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data.|generat adversari network gan power generat model suffer train instabl recent propos wasserstein gan wgan make signific progress toward stabl train gan still generat low qualiti sampl fail converg set find train failur often due use weight clip wgan enforc lipschitz constraint critic lead patholog behavior propos altern method enforc lipschitz constraint instead clip weight penal norm gradient critic respect input propos method converg faster generat higher qualiti sampl wgan weight clip final method enabl veri stabl gan train first time train wide varieti gan architectur almost hyperparamet tune includ layer resnet languag model discret data|['Ishaan Gulrajani', 'Faruk Ahmed', 'Martin Arjovsky', 'Vincent Dumoulin', 'Aaron Courville']|['cs.LG', 'stat.ML']
2017-04-07T11:23:13Z|2017-03-31T18:55:48Z|http://arxiv.org/abs/1704.00023v1|http://arxiv.org/pdf/1704.00023v1|On the Reliable Detection of Concept Drift from Streaming Unlabeled Data|reliabl detect concept drift stream unlabel data|Classifiers deployed in the real world operate in a dynamic environment, where the data distribution can change over time. These changes, referred to as concept drift, can cause the predictive performance of the classifier to drop over time, thereby making it obsolete. To be of any real use, these classifiers need to detect drifts and be able to adapt to them, over time. Detecting drifts has traditionally been approached as a supervised task, with labeled data constantly being used for validating the learned model. Although effective in detecting drifts, these techniques are impractical, as labeling is a difficult, costly and time consuming activity. On the other hand, unsupervised change detection techniques are unreliable, as they produce a large number of false alarms. The inefficacy of the unsupervised techniques stems from the exclusion of the characteristics of the learned classifier, from the detection process. In this paper, we propose the Margin Density Drift Detection (MD3) algorithm, which tracks the number of samples in the uncertainty region of a classifier, as a metric to detect drift. The MD3 algorithm is a distribution independent, application independent, model independent, unsupervised and incremental algorithm for reliably detecting drifts from data streams. Experimental evaluation on 6 drift induced datasets and 4 additional datasets from the cybersecurity domain demonstrates that the MD3 approach can reliably detect drifts, with significantly fewer false alarms compared to unsupervised feature based drift detectors. The reduced false alarms enables the signaling of drifts only when they are most likely to affect classification performance. As such, the MD3 approach leads to a detection scheme which is credible, label efficient and general in its applicability.|classifi deploy real world oper dynam environ data distribut chang time chang refer concept drift caus predict perform classifi drop time therebi make obsolet ani real use classifi need detect drift abl adapt time detect drift tradit approach supervis task label data constant use valid learn model although effect detect drift techniqu impract label difficult cost time consum activ hand unsupervis chang detect techniqu unreli produc larg number fals alarm inefficaci unsupervis techniqu stem exclus characterist learn classifi detect process paper propos margin densiti drift detect md algorithm track number sampl uncertainti region classifi metric detect drift md algorithm distribut independ applic independ model independ unsupervis increment algorithm reliabl detect drift data stream experiment evalu drift induc dataset addit dataset cybersecur domain demonstr md approach reliabl detect drift signific fewer fals alarm compar unsupervis featur base drift detector reduc fals alarm enabl signal drift onli like affect classif perform md approach lead detect scheme credibl label effici general applic|['Tegjyot Singh Sethi', 'Mehmed Kantardzic']|['stat.ML', 'cs.AI', 'cs.LG']
2017-04-07T11:23:17Z|2017-03-31T17:27:12Z|http://arxiv.org/abs/1703.10993v1|http://arxiv.org/pdf/1703.10993v1|Catalyst Acceleration for Gradient-Based Non-Convex Optimization|catalyst acceler gradient base non convex optim|We introduce a generic acceleration scheme to accelerate gradient-based convex optimization algorithms to solve possibly nonconvex optimization problems. The proposed approach extends the Catalyst acceleration for convex problems and allows one to venture into possibly nonconvex optimization problems without sacrificing the rate of convergence to stationary points. We present promising experimental results for sparse matrix factorization and for learning neural networks.|introduc generic acceler scheme acceler gradient base convex optim algorithm solv possibl nonconvex optim problem propos approach extend catalyst acceler convex problem allow one ventur possibl nonconvex optim problem without sacrif rate converg stationari point present promis experiment result spars matrix factor learn neural network|['Courtney Paquette', 'Hongzhou Lin', 'Dmitriy Drusvyatskiy', 'Julien Mairal', 'Zaid Harchaoui']|['stat.ML', 'math.OC']
2017-04-07T11:23:17Z|2017-03-31T15:40:24Z|http://arxiv.org/abs/1703.10951v1|http://arxiv.org/pdf/1703.10951v1|Comparison of multi-task convolutional neural network (MT-CNN) and a few   other methods for toxicity prediction|comparison multi task convolut neural network mt cnn method toxic predict|Toxicity analysis and prediction are of paramount importance to human health and environmental protection. Existing computational methods are built from a wide variety of descriptors and regressors, which makes their performance analysis difficult. For example, deep neural network (DNN), a successful approach in many occasions, acts like a black box and offers little conceptual elegance or physical understanding. The present work constructs a common set of microscopic descriptors based on established physical models for charges, surface areas and free energies to assess the performance of multi-task convolutional neural network (MT-CNN) architectures and a few other approaches, including random forest (RF) and gradient boosting decision tree (GBDT), on an equal footing. Comparison is also given to convolutional neural network (CNN) and non-convolutional deep neural network (DNN) algorithms. Four benchmark toxicity data sets (i.e., endpoints) are used to evaluate various approaches. Extensive numerical studies indicate that the present MT-CNN architecture is able to outperform the state-of-the-art methods.|toxic analysi predict paramount import human health environment protect exist comput method built wide varieti descriptor regressor make perform analysi difficult exampl deep neural network dnn success approach mani occas act like black box offer littl conceptu eleg physic understand present work construct common set microscop descriptor base establish physic model charg surfac area free energi assess perform multi task convolut neural network mt cnn architectur approach includ random forest rf gradient boost decis tree gbdt equal foot comparison also given convolut neural network cnn non convolut deep neural network dnn algorithm four benchmark toxic data set endpoint use evalu various approach extens numer studi indic present mt cnn architectur abl outperform state art method|['Kedi Wu', 'Guo-Wei Wei']|['q-bio.QM', 'cs.LG', 'stat.ML']
2017-04-07T11:23:17Z|2017-03-31T15:13:54Z|http://arxiv.org/abs/1703.10936v1|http://arxiv.org/pdf/1703.10936v1|Prediction of infectious disease epidemics via weighted density   ensembles|predict infecti diseas epidem via weight densiti ensembl|Accurate and reliable predictions of infectious disease dynamics can be valuable to public health organizations that plan interventions to decrease or prevent disease transmission. A great variety of models have been developed for this task, using different model structures, covariates, and targets for prediction. Experience has shown that the performance of these models varies; some tend to do better or worse in different seasons or at different points within a season. Ensemble methods combine multiple models to obtain a single prediction that leverages the strengths of each model. We considered a range of ensemble methods that each form a predictive density for a target of interest as a weighted sum of the predictive densities from component models. In the simplest case, equal weight is assigned to each component model; in the most complex case, the weights vary with the region, prediction target, week of the season when the predictions are made, a measure of component model uncertainty, and recent observations of disease incidence. We applied these methods to predict measures of influenza season timing and severity in the United States, both at the national and regional levels, using three component models. We trained the models on retrospective predictions from 14 seasons (1997/1998 - 2010/2011) and evaluated each model's prospective, out-of-sample performance in the five subsequent influenza seasons. In this test phase, the ensemble methods showed overall performance that was similar to the best of the component models, but offered more consistent performance across seasons than the component models. Ensemble methods offer the potential to deliver more reliable predictions to public health decision makers.|accur reliabl predict infecti diseas dynam valuabl public health organ plan intervent decreas prevent diseas transmiss great varieti model develop task use differ model structur covari target predict experi shown perform model vari tend better wors differ season differ point within season ensembl method combin multipl model obtain singl predict leverag strength model consid rang ensembl method form predict densiti target interest weight sum predict densiti compon model simplest case equal weight assign compon model complex case weight vari region predict target week season predict made measur compon model uncertainti recent observ diseas incid appli method predict measur influenza season time sever unit state nation region level use three compon model train model retrospect predict season evalu model prospect sampl perform five subsequ influenza season test phase ensembl method show overal perform similar best compon model offer consist perform across season compon model ensembl method offer potenti deliv reliabl predict public health decis maker|['Evan L. Ray', 'Nicholas G. Reich']|['stat.ML']
2017-04-07T11:23:17Z|2017-03-31T15:13:33Z|http://arxiv.org/abs/1703.10935v1|http://arxiv.org/pdf/1703.10935v1|The Risk of Machine Learning|risk machin learn|Many applied settings in empirical economics involve simultaneous estimation of a large number of parameters. In particular, applied economists are often interested in estimating the effects of many-valued treatments (like teacher effects or location effects), treatment effects for many groups, and prediction models with many regressors. In these settings, machine learning methods that combine regularized estimation and data-driven choices of regularization parameters are useful to avoid over-fitting. In this article, we analyze the performance of a class of machine learning estimators that includes ridge, lasso and pretest in contexts that require simultaneous estimation of many parameters. Our analysis aims to provide guidance to applied researchers on (i) the choice between regularized estimators in practice and (ii) data-driven selection of regularization parameters. To address (i), we characterize the risk (mean squared error) of regularized estimators and derive their relative performance as a function of simple features of the data generating process. To address (ii), we show that data-driven choices of regularization parameters, based on Stein's unbiased risk estimate or on cross-validation, yield estimators with risk uniformly close to the risk attained under the optimal (unfeasible) choice of regularization parameters. We use data from recent examples in the empirical economics literature to illustrate the practical applicability of our results.|mani appli set empir econom involv simultan estim larg number paramet particular appli economist often interest estim effect mani valu treatment like teacher effect locat effect treatment effect mani group predict model mani regressor set machin learn method combin regular estim data driven choic regular paramet use avoid fit articl analyz perform class machin learn estim includ ridg lasso pretest context requir simultan estim mani paramet analysi aim provid guidanc appli research choic regular estim practic ii data driven select regular paramet address character risk mean squar error regular estim deriv relat perform function simpl featur data generat process address ii show data driven choic regular paramet base stein unbias risk estim cross valid yield estim risk uniform close risk attain optim unfeas choic regular paramet use data recent exampl empir econom literatur illustr practic applic result|['Alberto Abadie', 'Maximilian Kasy']|['stat.ML']
2017-04-07T11:23:17Z|2017-03-31T13:11:06Z|http://arxiv.org/abs/1703.10887v1|http://arxiv.org/pdf/1703.10887v1|Bi-class classification of humpback whale sound units against complex   background noise with Deep Convolution Neural Network|bi class classif humpback whale sound unit complex background nois deep convolut neural network|Automatically detecting sound units of humpback whales in complex time-varying background noises is a current challenge for scientists. In this paper, we explore the applicability of Convolution Neural Network (CNN) method for this task. In the evaluation stage, we present 6 bi-class classification experimentations of whale sound detection against different background noise types (e.g., rain, wind). In comparison to classical FFT-based representation like spectrograms, we showed that the use of image-based pretrained CNN features brought higher performance to classify whale sounds and background noise.|automat detect sound unit humpback whale complex time vari background nois current challeng scientist paper explor applic convolut neural network cnn method task evalu stage present bi class classif experiment whale sound detect differ background nois type rain wind comparison classic fft base represent like spectrogram show use imag base pretrain cnn featur brought higher perform classifi whale sound background nois|['Cazau Dorian', 'Riwal Lefort', 'Julien Bonnel', 'Jean-Luc Zarader', 'Olivier Adam']|['stat.ML', 'cs.LG', 'cs.SD']
2017-04-07T11:23:17Z|2017-03-31T10:05:00Z|http://arxiv.org/abs/1703.10827v1|http://arxiv.org/pdf/1703.10827v1|Intraoperative margin assessment of human breast tissue in optical   coherence tomography images using deep neural networks|intraop margin assess human breast tissu optic coher tomographi imag use deep neural network|Objective: In this work, we perform margin assessment of human breast tissue from optical coherence tomography (OCT) images using deep neural networks (DNNs). This work simulates an intraoperative setting for breast cancer lumpectomy. Methods: To train the DNNs, we use both the state-of-the-art methods (Weight Decay and DropOut) and a newly introduced regularization method based on function norms. Commonly used methods can fail when only a small database is available. The use of a function norm introduces a direct control over the complexity of the function with the aim of diminishing the risk of overfitting. Results: As neither the code nor the data of previous results are publicly available, the obtained results are compared with reported results in the literature for a conservative comparison. Moreover, our method is applied to locally collected data on several data configurations. The reported results are the average over the different trials. Conclusion: The experimental results show that the use of DNNs yields significantly better results than other techniques when evaluated in terms of sensitivity, specificity, F1 score, G-mean and Matthews correlation coefficient. Function norm regularization yielded higher and more robust results than competing methods. Significance: We have demonstrated a system that shows high promise for (partially) automated margin assessment of human breast tissue, Equal error rate (EER) is reduced from approximately 12\% (the lowest reported in the literature) to 5\%\,--\,a 58\% reduction. The method is computationally feasible for intraoperative application (less than 2 seconds per image).|object work perform margin assess human breast tissu optic coher tomographi oct imag use deep neural network dnns work simul intraop set breast cancer lumpectomi method train dnns use state art method weight decay dropout newli introduc regular method base function norm common use method fail onli small databas avail use function norm introduc direct control complex function aim diminish risk overfit result neither code data previous result public avail obtain result compar report result literatur conserv comparison moreov method appli local collect data sever data configur report result averag differ trial conclus experiment result show use dnns yield signific better result techniqu evalu term sensit specif score mean matthew correl coeffici function norm regular yield higher robust result compet method signific demonstr system show high promis partial autom margin assess human breast tissu equal error rate eer reduc approxim lowest report literatur reduct method comput feasibl intraop applic less second per imag|['Amal Rannen Triki', 'Matthew B. Blaschko', 'Yoon Mo Jung', 'Seungri Song', 'Hyun Ju Han', 'Seung Il Kim', 'Chulmin Joo']|['stat.ML']
2017-04-07T11:23:17Z|2017-03-31T06:20:26Z|http://arxiv.org/abs/1704.00607v1|http://arxiv.org/pdf/1704.00607v1|A New Measure of Conditional Dependence for Causal Structural Learning|new measur condit depend causal structur learn|Measuring the dependencies among the variables of a network is of great interest to many disciplines. This paper studies the limitations of the existing dependencies measures such as their shortcomings in detecting direct influences or their lack of ability for group selection in order to have effective interventions and introduces a new statistical influence measure to overcome them. This measure is inspired by Dobrushin's coefficients and has been developed based on the paradigm that the conditional distribution of the variable of interest given all the direct causes will not change by intervening on other variables in the system. We show the advantageous of this measure over the related measures in the literature. Moreover, we establish the connection between our measure and the integral probability metric (IPM) that helps to develop estimators for our measure with lower complexity compared to the other relevant information theoretic based measures. At the end, we show the performance of this measure through a numerical simulation.|measur depend among variabl network great interest mani disciplin paper studi limit exist depend measur shortcom detect direct influenc lack abil group select order effect intervent introduc new statist influenc measur overcom measur inspir dobrushin coeffici develop base paradigm condit distribut variabl interest given direct caus chang interven variabl system show advantag measur relat measur literatur moreov establish connect measur integr probabl metric ipm help develop estim measur lower complex compar relev inform theoret base measur end show perform measur numer simul|['Jalal Etesami', 'Kun Zhang', 'Negar Kiyavash']|['stat.ML', 'cs.LG']
2017-04-07T11:23:17Z|2017-03-31T05:26:05Z|http://arxiv.org/abs/1703.10761v1|http://arxiv.org/pdf/1703.10761v1|Universal Scalable Robust Solvers from Computational Information Games   and fast eigenspace adapted Multiresolution Analysis|univers scalabl robust solver comput inform game fast eigenspac adapt multiresolut analysi|We show how the discovery of robust scalable numerical solvers for arbitrary bounded linear operators can be automated as a Game Theory problem by reformulating the process of computing with partial information and limited resources as that of playing underlying hierarchies of adversarial information games. When the solution space is a Banach space $B$ endowed with a quadratic norm $\ \cdot\ $, the optimal measure (mixed strategy) for such games (e.g. the adversarial recovery of $u\in B$, given partial measurements $[\phi_i, u]$ with $\phi_i\in B^*$, using relative error in $\ \cdot\ $-norm as a loss) is a centered Gaussian field $\xi$ solely determined by the norm $\ \cdot\ $, whose conditioning (on measurements) produces optimal bets. When measurements are hierarchical, the process of conditioning this Gaussian field produces a hierarchy of elementary bets (gamblets). These gamblets generalize the notion of Wavelets and Wannier functions in the sense that they are adapted to the norm $\ \cdot\ $ and induce a multi-resolution decomposition of $B$ that is adapted to the eigensubspaces of the operator defining the norm $\ \cdot\ $. When the operator is localized, we show that the resulting gamblets are localized both in space and frequency and introduce the Fast Gamblet Transform (FGT) with rigorous accuracy and (near-linear) complexity estimates. As the FFT can be used to solve and diagonalize arbitrary PDEs with constant coefficients, the FGT can be used to decompose a wide range of continuous linear operators (including arbitrary continuous linear bijections from $H^s_0$ to $H^{-s}$ or to $L^2$) into a sequence of independent linear systems with uniformly bounded condition numbers and leads to $\mathcal{O}(N \operatorname{polylog} N)$ solvers and eigenspace adapted Multiresolution Analysis (resulting in near linear complexity approximation of all eigensubspaces).|show discoveri robust scalabl numer solver arbitrari bound linear oper autom game theori problem reformul process comput partial inform limit resourc play hierarchi adversari inform game solut space banach space endow quadrat norm cdot optim measur mix strategi game adversari recoveri given partial measur phi phi use relat error cdot norm loss center gaussian field xi sole determin norm cdot whose condit measur produc optim bet measur hierarch process condit gaussian field produc hierarchi elementari bet gamblet gamblet general notion wavelet wannier function sens adapt norm cdot induc multi resolut decomposit adapt eigensubspac oper defin norm cdot oper local show result gamblet local space frequenc introduc fast gamblet transform fgt rigor accuraci near linear complex estim fft use solv diagon arbitrari pdes constant coeffici fgt use decompos wide rang continu linear oper includ arbitrari continu linear biject sequenc independ linear system uniform bound condit number lead mathcal operatornam polylog solver eigenspac adapt multiresolut analysi result near linear complex approxim eigensubspac|['Houman Owhadi', 'Clint Scovel']|['math.NA', 'math.AP', 'stat.ML', '68T99, 65T60, 65M55, 65N55, 65F99, 65N75, 62C99, 62C20, 62C10,\n  42C40, 60G42, 68Q25, 15A18, 35Q91']
2017-04-07T11:23:17Z|2017-03-31T03:50:03Z|http://arxiv.org/abs/1704.00003v1|http://arxiv.org/pdf/1704.00003v1|Spectral Methods for Nonparametric Models|spectral method nonparametr model|Nonparametric models are versatile, albeit computationally expensive, tool for modeling mixture models. In this paper, we introduce spectral methods for the two most popular nonparametric models: the Indian Buffet Process (IBP) and the Hierarchical Dirichlet Process (HDP). We show that using spectral methods for the inference of nonparametric models are computationally and statistically efficient. In particular, we derive the lower-order moments of the IBP and the HDP, propose spectral algorithms for both models, and provide reconstruction guarantees for the algorithms. For the HDP, we further show that applying hierarchical models on dataset with hierarchical structure, which can be solved with the generalized spectral HDP, produces better solutions to that of flat models regarding likelihood performance.|nonparametr model versatil albeit comput expens tool model mixtur model paper introduc spectral method two popular nonparametr model indian buffet process ibp hierarch dirichlet process hdp show use spectral method infer nonparametr model comput statist effici particular deriv lower order moment ibp hdp propos spectral algorithm model provid reconstruct guarante algorithm hdp show appli hierarch model dataset hierarch structur solv general spectral hdp produc better solut flat model regard likelihood perform|['Hsiao-Yu Fish Tung', 'Chao-Yuan Wu', 'Manzil Zaheer', 'Alexander J. Smola']|['cs.LG', 'stat.ML']
2017-04-07T11:23:17Z|2017-03-31T03:21:32Z|http://arxiv.org/abs/1703.10740v1|http://arxiv.org/pdf/1703.10740v1|Fundamental Conditions for Low-CP-Rank Tensor Completion|fundament condit low cp rank tensor complet|We consider the problem of low canonical polyadic (CP) rank tensor completion. A completion is a tensor whose entries agree with the observed entries and its rank matches the given CP rank. We analyze the manifold structure corresponding to the tensors with the given rank and define a set of polynomials based on the sampling pattern and CP decomposition. Then, we show that finite completability of the sampled tensor is equivalent to having a certain number of algebraically independent polynomials among the defined polynomials. Our proposed approach results in characterizing the maximum number of algebraically independent polynomials in terms of a simple geometric structure of the sampling pattern, and therefore we obtain the deterministic necessary and sufficient condition on the sampling pattern for finite completability of the sampled tensor. Moreover, assuming that the entries of the tensor are sampled independently with probability $p$ and using the mentioned deterministic analysis, we propose a combinatorial method to derive a lower bound on the sampling probability $p$, or equivalently, the number of sampled entries that guarantees finite completability with high probability. We also show that the existing result for the matrix completion problem can be used to obtain a loose lower bound on the sampling probability $p$. In addition, we obtain deterministic and probabilistic conditions for unique completability. It is seen that the number of samples required for finite or unique completability obtained by the proposed analysis on the CP manifold is orders-of-magnitude lower than that is obtained by the existing analysis on the Grassmannian manifold.|consid problem low canon polyad cp rank tensor complet complet tensor whose entri agre observ entri rank match given cp rank analyz manifold structur correspond tensor given rank defin set polynomi base sampl pattern cp decomposit show finit complet sampl tensor equival certain number algebra independ polynomi among defin polynomi propos approach result character maximum number algebra independ polynomi term simpl geometr structur sampl pattern therefor obtain determinist necessari suffici condit sampl pattern finit complet sampl tensor moreov assum entri tensor sampl independ probabl use mention determinist analysi propos combinatori method deriv lower bound sampl probabl equival number sampl entri guarante finit complet high probabl also show exist result matrix complet problem use obtain loos lower bound sampl probabl addit obtain determinist probabilist condit uniqu complet seen number sampl requir finit uniqu complet obtain propos analysi cp manifold order magnitud lower obtain exist analysi grassmannian manifold|['Morteza Ashraphijuo', 'Xiaodong Wang']|['cs.LG', 'cs.NA', 'math.NA', 'stat.ML']
2017-04-07T11:23:21Z|2017-03-31T00:50:37Z|http://arxiv.org/abs/1703.10722v1|http://arxiv.org/pdf/1703.10722v1|Factorization tricks for LSTM networks|factor trick lstm network|"We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is ""matrix factorization by design"" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the state-of the art perplexity. On the One Billion Word Benchmark we improve single model perplexity down to 24.29."|present two simpl way reduc number paramet acceler train larg long short term memori lstm network first one matrix factor design lstm matrix product two smaller matric second one partit lstm matrix input state independ group approach allow us train larg lstm network signific faster state art perplex one billion word benchmark improv singl model perplex|['Oleksii Kuchaiev', 'Boris Ginsburg']|['cs.CL', 'cs.NE', 'stat.ML']
2017-04-07T11:23:21Z|2017-03-31T00:13:33Z|http://arxiv.org/abs/1703.10717v1|http://arxiv.org/pdf/1703.10717v1|BEGAN: Boundary Equilibrium Generative Adversarial Networks|began boundari equilibrium generat adversari network|We propose a new equilibrium enforcing method paired with a loss derived from the Wasserstein distance for training auto-encoder based Generative Adversarial Networks. This method balances the generator and discriminator during training. Additionally, it provides a new approximate convergence measure, fast and stable training and high visual quality. We also derive a way of controlling the trade-off between image diversity and visual quality. We focus on the image generation task, setting a new milestone in visual quality, even at higher resolutions. This is achieved while using a relatively simple model architecture and a standard training procedure.|propos new equilibrium enforc method pair loss deriv wasserstein distanc train auto encod base generat adversari network method balanc generat discrimin dure train addit provid new approxim converg measur fast stabl train high visual qualiti also deriv way control trade imag divers visual qualiti focus imag generat task set new mileston visual qualiti even higher resolut achiev use relat simpl model architectur standard train procedur|['David Berthelot', 'Tom Schumm', 'Luke Metz']|['cs.LG', 'stat.ML']
2017-04-07T11:23:21Z|2017-03-30T20:25:01Z|http://arxiv.org/abs/1703.10663v1|http://arxiv.org/pdf/1703.10663v1|Near Perfect Protein Multi-Label Classification with Deep Neural   Networks|near perfect protein multi label classif deep neural network|Artificial neural networks (ANNs) have gained a well-deserved popularity among machine learning tools upon their recent successful applications in image- and sound processing and classification problems. ANNs have also been applied for predicting the family or function of a protein, knowing its residue sequence. Here we present two new ANNs with multi-label classification ability, showing impressive accuracy when classifying protein sequences into 698 UniProt families (AUC=99.99%) and 983 Gene Ontology classes (AUC=99.45%).|artifici neural network ann gain well deserv popular among machin learn tool upon recent success applic imag sound process classif problem ann also appli predict famili function protein know residu sequenc present two new ann multi label classif abil show impress accuraci classifi protein sequenc uniprot famili auc gene ontolog class auc|['Balazs Szalkai', 'Vince Grolmusz']|['q-bio.BM', 'cs.LG', 'stat.ML']
2017-04-07T11:23:21Z|2017-03-30T19:51:03Z|http://arxiv.org/abs/1703.10651v1|http://arxiv.org/pdf/1703.10651v1|What-If Reasoning with Counterfactual Gaussian Processes|reason counterfactu gaussian process|"Answering ""What if?"" questions is important in many domains. For example, would a patient's disease progression slow down if I were to give them a dose of drug A? Ideally, we answer our question using an experiment, but this is not always possible (e.g., it may be unethical). As an alternative, we can use non-experimental data to learn models that make counterfactual predictions of what we would observe had we run an experiment. In this paper, we propose a model to make counterfactual predictions about how continuous-time trajectories (time series) respond to sequences of actions taken in continuous-time. We develop our model within the potential outcomes framework of Neyman and Rubin. One challenge is that the assumptions commonly made to learn potential outcome (counterfactual) models from observational data are not applicable in continuous-time as-is. We therefore propose a model using marked point processes and Gaussian processes, and develop alternative assumptions that allow us to learn counterfactual models from continuous-time observational data. We evaluate our approach on two tasks from health care: disease trajectory prediction and personalized treatment planning."|answer question import mani domain exampl would patient diseas progress slow give dose drug ideal answer question use experi alway possibl may uneth altern use non experiment data learn model make counterfactu predict would observ run experi paper propos model make counterfactu predict continu time trajectori time seri respond sequenc action taken continu time develop model within potenti outcom framework neyman rubin one challeng assumpt common made learn potenti outcom counterfactu model observ data applic continu time therefor propos model use mark point process gaussian process develop altern assumpt allow us learn counterfactu model continu time observ data evalu approach two task health care diseas trajectori predict person treatment plan|['Peter Schulam', 'Suchi Saria']|['stat.ML', 'cs.AI', 'cs.LG']
2017-04-07T11:23:21Z|2017-03-30T18:09:43Z|http://arxiv.org/abs/1703.10622v1|http://arxiv.org/pdf/1703.10622v1|Diving into the shallows: a computational perspective on large-scale   shallow learning|dive shallow comput perspect larg scale shallow learn|Remarkable success of deep neural networks has not been easy to analyze theoretically. It has been particularly hard to disentangle relative significance of architecture and optimization in achieving accurate classification on large datasets. On the flip side, shallow methods have encountered obstacles in scaling to large data, despite excellent performance on smaller datasets, and extensive theoretical analysis. Practical methods, such as variants of gradient descent used so successfully in deep learning, seem to perform below par when applied to kernel methods. This difficulty has sometimes been attributed to the limitations of shallow architecture.   In this paper we identify a basic limitation in gradient descent-based optimization in conjunctions with smooth kernels. An analysis demonstrates that only a vanishingly small fraction of the function space is reachable after a fixed number of iterations drastically limiting its power and resulting in severe over-regularization. The issue is purely algorithmic, persisting even in the limit of infinite data.   To address this issue, we introduce EigenPro iteration, based on a simple preconditioning scheme using a small number of approximately computed eigenvectors. It turns out that even this small amount of approximate second-order information results in significant improvement of performance for large-scale kernel methods. Using EigenPro in conjunction with stochastic gradient descent we demonstrate scalable state-of-the-art results for kernel methods on a modest computational budget.   Finally, these results indicate a need for a broader computational perspective on modern large-scale learning to complement more traditional statistical and convergence analyses. In particular, systematic analysis concentrating on the approximation power of algorithms with a fixed computation budget will lead to progress both in theory and practice.|remark success deep neural network easi analyz theoret particular hard disentangl relat signific architectur optim achiev accur classif larg dataset flip side shallow method encount obstacl scale larg data despit excel perform smaller dataset extens theoret analysi practic method variant gradient descent use success deep learn seem perform par appli kernel method difficulti sometim attribut limit shallow architectur paper identifi basic limit gradient descent base optim conjunct smooth kernel analysi demonstr onli vanish small fraction function space reachabl fix number iter drastic limit power result sever regular issu pure algorithm persist even limit infinit data address issu introduc eigenpro iter base simpl precondit scheme use small number approxim comput eigenvector turn even small amount approxim second order inform result signific improv perform larg scale kernel method use eigenpro conjunct stochast gradient descent demonstr scalabl state art result kernel method modest comput budget final result indic need broader comput perspect modern larg scale learn complement tradit statist converg analys particular systemat analysi concentr approxim power algorithm fix comput budget lead progress theori practic|['Siyuan Ma', 'Mikhail Belkin']|['stat.ML', 'cs.LG']
2017-04-07T11:23:21Z|2017-03-30T17:58:31Z|http://arxiv.org/abs/1703.10603v1|http://arxiv.org/pdf/1703.10603v1|Atomic Convolutional Networks for Predicting Protein-Ligand Binding   Affinity|atom convolut network predict protein ligand bind affin|Empirical scoring functions based on either molecular force fields or cheminformatics descriptors are widely used, in conjunction with molecular docking, during the early stages of drug discovery to predict potency and binding affinity of a drug-like molecule to a given target. These models require expert-level knowledge of physical chemistry and biology to be encoded as hand-tuned parameters or features rather than allowing the underlying model to select features in a data-driven procedure. Here, we develop a general 3-dimensional spatial convolution operation for learning atomic-level chemical interactions directly from atomic coordinates and demonstrate its application to structure-based bioactivity prediction. The atomic convolutional neural network is trained to predict the experimentally determined binding affinity of a protein-ligand complex by direct calculation of the energy associated with the complex, protein, and ligand given the crystal structure of the binding pose. Non-covalent interactions present in the complex that are absent in the protein-ligand sub-structures are identified and the model learns the interaction strength associated with these features. We test our model by predicting the binding free energy of a subset of protein-ligand complexes found in the PDBBind dataset and compare with state-of-the-art cheminformatics and machine learning-based approaches. We find that all methods achieve experimental accuracy and that atomic convolutional networks either outperform or perform competitively with the cheminformatics based methods. Unlike all previous protein-ligand prediction systems, atomic convolutional networks are end-to-end and fully-differentiable. They represent a new data-driven, physics-based deep learning model paradigm that offers a strong foundation for future improvements in structure-based bioactivity prediction.|empir score function base either molecular forc field cheminformat descriptor wide use conjunct molecular dock dure earli stage drug discoveri predict potenc bind affin drug like molecul given target model requir expert level knowledg physic chemistri biolog encod hand tune paramet featur rather allow model select featur data driven procedur develop general dimension spatial convolut oper learn atom level chemic interact direct atom coordin demonstr applic structur base bioactiv predict atom convolut neural network train predict experiment determin bind affin protein ligand complex direct calcul energi associ complex protein ligand given crystal structur bind pose non coval interact present complex absent protein ligand sub structur identifi model learn interact strength associ featur test model predict bind free energi subset protein ligand complex found pdbbind dataset compar state art cheminformat machin learn base approach find method achiev experiment accuraci atom convolut network either outperform perform competit cheminformat base method unlik previous protein ligand predict system atom convolut network end end fulli differenti repres new data driven physic base deep learn model paradigm offer strong foundat futur improv structur base bioactiv predict|['Joseph Gomes', 'Bharath Ramsundar', 'Evan N. Feinberg', 'Vijay S. Pande']|['cs.LG', 'physics.chem-ph', 'stat.ML']
2017-04-07T11:23:21Z|2017-03-30T16:02:25Z|http://arxiv.org/abs/1703.10545v1|http://arxiv.org/pdf/1703.10545v1|FairJudge: Trustworthy User Prediction in Rating Platforms|fairjudg trustworthi user predict rate platform|Rating platforms enable large-scale collection of user opinion about items (products, other users, etc.). However, many untrustworthy users give fraudulent ratings for excessive monetary gains. In the paper, we present FairJudge, a system to identify such fraudulent users. We propose three metrics: (i) the fairness of a user that quantifies how trustworthy the user is in rating the products, (ii) the reliability of a rating that measures how reliable the rating is, and (iii) the goodness of a product that measures the quality of the product. Intuitively, a user is fair if it provides reliable ratings that are close to the goodness of the product. We formulate a mutually recursive definition of these metrics, and further address cold start problems and incorporate behavioral properties of users and products in the formulation. We propose an iterative algorithm, FairJudge, to predict the values of the three metrics. We prove that FairJudge is guaranteed to converge in a bounded number of iterations, with linear time complexity. By conducting five different experiments on five rating platforms, we show that FairJudge significantly outperforms nine existing algorithms in predicting fair and unfair users. We reported the 100 most unfair users in the Flipkart network to their review fraud investigators, and 80 users were correctly identified (80% accuracy). The FairJudge algorithm is already being deployed at Flipkart.|rate platform enabl larg scale collect user opinion item product user etc howev mani untrustworthi user give fraudul rate excess monetari gain paper present fairjudg system identifi fraudul user propos three metric fair user quantifi trustworthi user rate product ii reliabl rate measur reliabl rate iii good product measur qualiti product intuit user fair provid reliabl rate close good product formul mutual recurs definit metric address cold start problem incorpor behavior properti user product formul propos iter algorithm fairjudg predict valu three metric prove fairjudg guarante converg bound number iter linear time complex conduct five differ experi five rate platform show fairjudg signific outperform nine exist algorithm predict fair unfair user report unfair user flipkart network review fraud investig user correct identifi accuraci fairjudg algorithm alreadi deploy flipkart|['Srijan Kumar', 'Bryan Hooi', 'Disha Makhija', 'Mohit Kumar', 'Christos Faloutsos', 'V. S. Subrahamanian']|['cs.SI', 'cs.AI', 'stat.ML']
2017-04-07T11:23:21Z|2017-03-30T15:41:10Z|http://arxiv.org/abs/1703.10534v1|http://arxiv.org/pdf/1703.10534v1|The Informativeness of k-Means for Learning Gaussian Mixture Models|inform mean learn gaussian mixtur model|The learning of Gaussian mixture models (GMMs) is a classical problem in machine learning and applied statistics. This can also be interpreted as a clustering problem. Indeed, given data samples independently generated from a GMM, we would like to find the correct target clustering of the samples according to which Gaussian they were generated from. Despite the large number of algorithms designed to find the correct target clustering, many practitioners prefer to use the k-means algorithm because of its simplicity. k-means tries to find an optimal clustering which minimizes the sum of squared distances between each point and its cluster center. In this paper, we provide sufficient conditions for the closeness of any optimal clustering and the correct target clustering of the samples which are independently generated from a GMM. Moreover, to achieve significantly faster running time and reduced memory usage, we show that under weaker conditions on the GMM, any optimal clustering for the samples with reduced dimensionality is also close to the correct target clustering. These results provide intuition for the informativeness of k-means as an algorithm for learning a GMM, further substantiating the conclusions in Kumar and Kannan [2010]. We verify the correctness of our theorems using numerical experiments and show, using datasets with reduced dimensionality, significant speed ups for the time required to perform clustering.|learn gaussian mixtur model gmms classic problem machin learn appli statist also interpret cluster problem inde given data sampl independ generat gmm would like find correct target cluster sampl accord gaussian generat despit larg number algorithm design find correct target cluster mani practition prefer use mean algorithm becaus simplic mean tri find optim cluster minim sum squar distanc point cluster center paper provid suffici condit close ani optim cluster correct target cluster sampl independ generat gmm moreov achiev signific faster run time reduc memori usag show weaker condit gmm ani optim cluster sampl reduc dimension also close correct target cluster result provid intuit inform mean algorithm learn gmm substanti conclus kumar kannan verifi correct theorem use numer experi show use dataset reduc dimension signific speed time requir perform cluster|['Zhaoqiang Liu', 'Vincent Y. F. Tan']|['stat.ML', 'cs.IT', 'cs.LG', 'math.IT', 'stat.ME']
2017-04-07T11:23:21Z|2017-03-30T15:12:02Z|http://arxiv.org/abs/1703.10513v1|http://arxiv.org/pdf/1703.10513v1|On Bayesian Exponentially Embedded Family for Model Order Selection|bayesian exponenti embed famili model order select|In this paper, we derive a Bayesian model order selection rule by using the exponentially embedded family method, termed Bayesian EEF. Unlike many other Bayesian model selection methods, the Bayesian EEF can use vague proper priors and improper noninformative priors to be objective in the elicitation of parameter priors. Moreover, the penalty term of the rule is shown to be the sum of half of the parameter dimension and the estimated mutual information between parameter and observed data. This helps to reveal the EEF mechanism in selecting model orders and may provide new insights into the open problems of choosing an optimal penalty term for model order selection and choosing a good prior from information theoretic viewpoints. The important example of linear model order selection is given to illustrate the algorithms and arguments. Lastly, the Bayesian EEF that uses Jeffreys prior coincides with the EEF rule derived by frequentist strategies. This shows another interesting relationship between the frequentist and Bayesian philosophies for model selection.|paper deriv bayesian model order select rule use exponenti embed famili method term bayesian eef unlik mani bayesian model select method bayesian eef use vagu proper prior improp noninform prior object elicit paramet prior moreov penalti term rule shown sum half paramet dimens estim mutual inform paramet observ data help reveal eef mechan select model order may provid new insight open problem choos optim penalti term model order select choos good prior inform theoret viewpoint import exampl linear model order select given illustr algorithm argument last bayesian eef use jeffrey prior coincid eef rule deriv frequentist strategi show anoth interest relationship frequentist bayesian philosophi model select|['Zhenghan Zhu', 'Steven Kay']|['stat.ML', 'cs.LG']
2017-04-07T11:23:21Z|2017-03-30T12:46:46Z|http://arxiv.org/abs/1703.10444v1|http://arxiv.org/pdf/1703.10444v1|On Fundamental Limits of Robust Learning|fundament limit robust learn|We consider the problems of robust PAC learning from distributed and streaming data, which may contain malicious errors and outliers, and analyze their fundamental complexity questions. In particular, we establish lower bounds on the communication complexity for distributed robust learning performed on multiple machines, and on the space complexity for robust learning from streaming data on a single machine. These results demonstrate that gaining robustness of learning algorithms is usually at the expense of increased complexities. As far as we know, this work gives the first complexity results for distributed and online robust PAC learning.|consid problem robust pac learn distribut stream data may contain malici error outlier analyz fundament complex question particular establish lower bound communic complex distribut robust learn perform multipl machin space complex robust learn stream data singl machin result demonstr gain robust learn algorithm usual expens increas complex far know work give first complex result distribut onlin robust pac learn|['Jiashi Feng']|['cs.LG', 'stat.ML']
2017-04-07T11:23:26Z|2017-03-30T11:49:27Z|http://arxiv.org/abs/1703.10423v1|http://arxiv.org/abs/1703.10423v1|Minimum energy path calculations with Gaussian process regression|minimum energi path calcul gaussian process regress|The calculation of minimum energy paths for transitions such as atomic and/or spin re-arrangements is an important task in many contexts and can often be used to determine the mechanism and rate of transitions. An important challenge is to reduce the computational effort in such calculations, especially when ab initio or electron density functional calculations are used to evaluate the energy since they can require large computational effort. Gaussian process regression is used here to reduce significantly the number of energy evaluations needed to find minimum energy paths of atomic rearrangements. By using results of previous calculations to construct an approximate energy surface and then converge to the minimum energy path on that surface in each Gaussian process iteration, the number of energy evaluations is reduced significantly as compared with regular nudged elastic band calculations. For a test problem involving rearrangements of a heptamer island on a crystal surface, the number of energy evaluations is reduced to less than a fifth. The scaling of the computational effort with the number of degrees of freedom as well as various possible further improvements to this approach are discussed.|calcul minimum energi path transit atom spin arrang import task mani context often use determin mechan rate transit import challeng reduc comput effort calcul especi ab initio electron densiti function calcul use evalu energi sinc requir larg comput effort gaussian process regress use reduc signific number energi evalu need find minimum energi path atom rearrang use result previous calcul construct approxim energi surfac converg minimum energi path surfac gaussian process iter number energi evalu reduc signific compar regular nudg elast band calcul test problem involv rearrang heptam island crystal surfac number energi evalu reduc less fifth scale comput effort number degre freedom well various possibl improv approach discuss|['Olli-Pekka Koistinen', 'Emile Maras', 'Aki Vehtari', 'Hannes Jónsson']|['physics.chem-ph', 'physics.atm-clus', 'physics.comp-ph', 'stat.CO', 'stat.ML']
2017-04-07T11:23:26Z|2017-03-30T08:59:24Z|http://arxiv.org/abs/1703.10893v1|http://arxiv.org/pdf/1703.10893v1|Audio-Visual Speech Enhancement based on Multimodal Deep Convolutional   Neural Network|audio visual speech enhanc base multimod deep convolut neural network|Speech enhancement (SE) aims to reduce noise in speech signals. Most SE techniques focus on addressing audio information only.In this work, inspired by multimodal learning, which utilizes data from different modalities, and the recent success of convolutional neural networks (CNNs) in SE, we propose an audio-visual deep CNN (AVDCNN) SE model, which incorporates audio and visual streams into a unified network model.In the proposed AVDCNN SE model,audio and visual features are first processed using individual CNNs, and then, fused into a joint network to generate enhanced speech at an output layer. The AVDCNN model is trained in an end-to-end manner, and parameters are jointly learned through back-propagation. We evaluate enhanced speech using five objective criteria. Results show that the AVDCNN yields notably better performance as compared to an audio-only CNN-based SE model, confirming the effectiveness of integrating visual information into the SE process.|speech enhanc se aim reduc nois speech signal se techniqu focus address audio inform onli work inspir multimod learn util data differ modal recent success convolut neural network cnns se propos audio visual deep cnn avdcnn se model incorpor audio visual stream unifi network model propos avdcnn se model audio visual featur first process use individu cnns fuse joint network generat enhanc speech output layer avdcnn model train end end manner paramet joint learn back propag evalu enhanc speech use five object criteria result show avdcnn yield notabl better perform compar audio onli cnn base se model confirm effect integr visual inform se process|['Jen-Cheng Hou', 'Syu-Siang Wang', 'Ying-Hui Lai', 'Jen-Chun Lin', 'Yu Tsao', 'Hsiu-Wen Chang', 'Hsin-Min Wang']|['cs.SD', 'cs.MM', 'stat.ML']
2017-04-07T11:23:26Z|2017-03-30T08:37:14Z|http://arxiv.org/abs/1703.10355v1|http://arxiv.org/pdf/1703.10355v1|From Deep to Shallow: Transformations of Deep Rectifier Networks|deep shallow transform deep rectifi network|In this paper, we introduce transformations of deep rectifier networks, enabling the conversion of deep rectifier networks into shallow rectifier networks. We subsequently prove that any rectifier net of any depth can be represented by a maximum of a number of functions that can be realized by a shallow network with a single hidden layer. The transformations of both deep rectifier nets and deep residual nets are conducted to demonstrate the advantages of the residual nets over the conventional neural nets and the advantages of the deep neural nets over the shallow neural nets. In summary, for two rectifier nets with different depths but with same total number of hidden units, the corresponding single hidden layer representation of the deeper net is much more complex than the corresponding single hidden representation of the shallower net. Similarly, for a residual net and a conventional rectifier net with the same structure except for the skip connections in the residual net, the corresponding single hidden layer representation of the residual net is much more complex than the corresponding single hidden layer representation of the conventional net.|paper introduc transform deep rectifi network enabl convers deep rectifi network shallow rectifi network subsequ prove ani rectifi net ani depth repres maximum number function realiz shallow network singl hidden layer transform deep rectifi net deep residu net conduct demonstr advantag residu net convent neural net advantag deep neural net shallow neural net summari two rectifi net differ depth total number hidden unit correspond singl hidden layer represent deeper net much complex correspond singl hidden represent shallow net similar residu net convent rectifi net structur except skip connect residu net correspond singl hidden layer represent residu net much complex correspond singl hidden layer represent convent net|['Senjian An', 'Farid Boussaid', 'Mohammed Bennamoun', 'Jiankun Hu']|['cs.LG', 'stat.ML']
2017-04-07T11:23:26Z|2017-03-30T07:50:15Z|http://arxiv.org/abs/1703.10342v1|http://arxiv.org/pdf/1703.10342v1|Efficient Benchmarking of Algorithm Configuration Procedures via   Model-Based Surrogates|effici benchmark algorithm configur procedur via model base surrog|The optimization of algorithm (hyper-)parameters is crucial for achieving peak performance across a wide range of domains, ranging from deep neural networks to solvers for hard combinatorial problems. The resulting algorithm configuration (AC) problem has attracted much attention from the machine learning community. However, the proper evaluation of new AC procedures is hindered by two key hurdles. First, AC benchmarks are hard to set up. Second and even more significantly, they are computationally expensive: a single run of an AC procedure involves many costly runs of the target algorithm whose performance is to be optimized in a given AC benchmark scenario. One common workaround is to optimize cheap-to-evaluate artificial benchmark functions (e.g., Branin) instead of actual algorithms; however, these have different properties than realistic AC problems. Here, we propose an alternative benchmarking approach that is similarly cheap to evaluate but much closer to the original AC problem: replacing expensive benchmarks by surrogate benchmarks constructed from AC benchmarks. These surrogate benchmarks approximate the response surface corresponding to true target algorithm performance using a regression model, and the original and surrogate benchmark share the same (hyper-)parameter space. In our experiments, we construct and evaluate surrogate benchmarks for hyperparameter optimization as well as for AC problems that involve performance optimization of solvers for hard combinatorial problems, drawing training data from the runs of existing AC procedures. We show that our surrogate benchmarks capture overall important characteristics of the AC scenarios, such as high- and low-performing regions, from which they were derived, while being much easier to use and orders of magnitude cheaper to evaluate.|optim algorithm hyper paramet crucial achiev peak perform across wide rang domain rang deep neural network solver hard combinatori problem result algorithm configur ac problem attract much attent machin learn communiti howev proper evalu new ac procedur hinder two key hurdl first ac benchmark hard set second even signific comput expens singl run ac procedur involv mani cost run target algorithm whose perform optim given ac benchmark scenario one common workaround optim cheap evalu artifici benchmark function branin instead actual algorithm howev differ properti realist ac problem propos altern benchmark approach similar cheap evalu much closer origin ac problem replac expens benchmark surrog benchmark construct ac benchmark surrog benchmark approxim respons surfac correspond true target algorithm perform use regress model origin surrog benchmark share hyper paramet space experi construct evalu surrog benchmark hyperparamet optim well ac problem involv perform optim solver hard combinatori problem draw train data run exist ac procedur show surrog benchmark captur overal import characterist ac scenario high low perform region deriv much easier use order magnitud cheaper evalu|['Katharina Eggensperger', 'Marius Lindauer', 'Holger H. Hoos', 'Frank Hutter', 'Kevin Leyton-Brown']|['cs.AI', 'stat.ML']
2017-04-07T11:23:26Z|2017-03-29T20:17:30Z|http://arxiv.org/abs/1703.10230v1|http://arxiv.org/pdf/1703.10230v1|Numerical Gaussian Processes for Time-dependent and Non-linear Partial   Differential Equations|numer gaussian process time depend non linear partial differenti equat|We introduce the concept of numerical Gaussian processes, which we define as Gaussian processes with covariance functions resulting from temporal discretization of time-dependent partial differential equations. Numerical Gaussian processes, by construction, are designed to deal with cases where: (1) all we observe are noisy data on black-box initial conditions, and (2) we are interested in quantifying the uncertainty associated with such noisy data in our solutions to time-dependent partial differential equations. Our method circumvents the need for spatial discretization of the differential operators by proper placement of Gaussian process priors. This is an attempt to construct structured and data-efficient learning machines, which are explicitly informed by the underlying physics that possibly generated the observed data. The effectiveness of the proposed approach is demonstrated through several benchmark problems involving linear and nonlinear time-dependent operators. In all examples, we are able to recover accurate approximations of the latent solutions, and consistently propagate uncertainty, even in cases involving very long time integration.|introduc concept numer gaussian process defin gaussian process covari function result tempor discret time depend partial differenti equat numer gaussian process construct design deal case observ noisi data black box initi condit interest quantifi uncertainti associ noisi data solut time depend partial differenti equat method circumv need spatial discret differenti oper proper placement gaussian process prior attempt construct structur data effici learn machin explicit inform physic possibl generat observ data effect propos approach demonstr sever benchmark problem involv linear nonlinear time depend oper exampl abl recov accur approxim latent solut consist propag uncertainti even case involv veri long time integr|['Maziar Raissi', 'Paris Perdikaris', 'George Em Karniadakis']|['stat.ML', 'cs.NA', 'math.AP', 'math.DS', 'math.NA', '65C20, 68T05, 65M75']
2017-04-07T11:23:26Z|2017-03-29T17:21:44Z|http://arxiv.org/abs/1703.10146v1|http://arxiv.org/pdf/1703.10146v1|Community detection and stochastic block models: recent developments|communiti detect stochast block model recent develop|The stochastic block model (SBM) is a random graph model with planted clusters. It is widely employed as a canonical model to study clustering and community detection, and provides generally a fertile ground to study the statistical and computational tradeoffs that arise in network and data sciences.   This note surveys the recent developments that establish the fundamental limits for community detection in the SBM, both with respect to information-theoretic and computational thresholds, and for various recovery requirements such as exact, partial and weak recovery (a.k.a., detection). The main results discussed are the phase transitions for exact recovery at the Chernoff-Hellinger threshold, the phase transition for weak recovery at the Kesten-Stigum threshold, the optimal distortion-SNR tradeoff for partial recovery, the learning of the SBM parameters and the gap between information-theoretic and computational thresholds.   The note also covers some of the algorithms developed in the quest of achieving the limits, in particular two-round algorithms via graph-splitting, semi-definite programming, linearized belief propagation, classical and nonbacktracking spectral methods. A few open problems are also discussed.|stochast block model sbm random graph model plant cluster wide employ canon model studi cluster communiti detect provid general fertil ground studi statist comput tradeoff aris network data scienc note survey recent develop establish fundament limit communiti detect sbm respect inform theoret comput threshold various recoveri requir exact partial weak recoveri detect main result discuss phase transit exact recoveri chernoff helling threshold phase transit weak recoveri kesten stigum threshold optim distort snr tradeoff partial recoveri learn sbm paramet gap inform theoret comput threshold note also cover algorithm develop quest achiev limit particular two round algorithm via graph split semi definit program linear belief propag classic nonbacktrack spectral method open problem also discuss|['Emmanuel Abbe']|['math.PR', 'cs.CC', 'cs.IT', 'cs.SI', 'math.IT', 'stat.ML']
2017-04-07T11:23:26Z|2017-03-29T13:43:52Z|http://arxiv.org/abs/1703.10034v1|http://arxiv.org/pdf/1703.10034v1|Probabilistic Line Searches for Stochastic Optimization|probabilist line search stochast optim|In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization. Our method retains a Gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the Wolfe conditions to monitor the descent. The algorithm has very low computational cost, and no user-controlled parameters. Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent.|determinist optim line search standard tool ensur stabil effici onli stochast gradient avail direct equival far formul becaus uncertain gradient allow strict sequenc decis collaps search space construct probabilist line search combin structur exist determinist method notion bayesian optim method retain gaussian process surrog univari optim object use probabilist belief wolf condit monitor descent algorithm veri low comput cost user control paramet experi show effect remov need defin learn rate stochast gradient descent|['Maren Mahsereci', 'Philipp Hennig']|['cs.LG', 'stat.ML']
2017-04-07T11:23:26Z|2017-03-29T12:46:59Z|http://arxiv.org/abs/1703.10010v1|http://arxiv.org/pdf/1703.10010v1|Optimal Policies for Observing Time Series and Related Restless Bandit   Problems|optim polici observ time seri relat restless bandit problem|The trade-off between the cost of acquiring and processing data, and uncertainty due to a lack of data is fundamental in machine learning. A basic instance of this trade-off is the problem of deciding when to make noisy and costly observations of a discrete-time Gaussian random walk, so as to minimise the posterior variance plus observation costs. We present the first proof that a simple policy, which observes when the posterior variance exceeds a threshold, is optimal for this problem. The proof generalises to a wide range of cost functions other than the posterior variance.   This result implies that optimal policies for linear-quadratic-Gaussian control with costly observations have a threshold structure. It also implies that the restless bandit problem of observing multiple such time series, has a well-defined Whittle index. We discuss computation of that index, give closed-form formulae for it, and compare the performance of the associated index policy with heuristic policies.   The proof is based on a new verification theorem that demonstrates threshold structure for Markov decision processes, and on the relation between binary sequences known as mechanical words and the dynamics of discontinuous nonlinear maps, which frequently arise in physics, control and biology.|trade cost acquir process data uncertainti due lack data fundament machin learn basic instanc trade problem decid make noisi cost observ discret time gaussian random walk minimis posterior varianc plus observ cost present first proof simpl polici observ posterior varianc exceed threshold optim problem proof generalis wide rang cost function posterior varianc result impli optim polici linear quadrat gaussian control cost observ threshold structur also impli restless bandit problem observ multipl time seri well defin whittl index discuss comput index give close form formula compar perform associ index polici heurist polici proof base new verif theorem demonstr threshold structur markov decis process relat binari sequenc known mechan word dynam discontinu nonlinear map frequent aris physic control biolog|['Christopher R. Dance', 'Tomi Silander']|['stat.ML']
2017-04-07T11:23:26Z|2017-03-29T11:28:55Z|http://arxiv.org/abs/1703.09975v1|http://arxiv.org/pdf/1703.09975v1|Improving Spectral Clustering using the Asymptotic Value of the   Normalised Cut|improv spectral cluster use asymptot valu normalis cut|Spectral clustering is a popular and versatile clustering method based on a relaxation of the normalised graph cut objective. Despite its popularity, however, there is no single agreed upon method for tuning the important scaling parameter, nor for determining automatically the number of clusters to extract. Popular heuristics exist, but corresponding theoretical results are scarce. In this paper we investigate the asymptotic value of the normalised cut for an increasing sample assumed to arise from an underlying probability distribution, and based on this result provide recommendations for improving spectral clustering methodology. A corresponding algorithm is proposed with strong empirical performance.|spectral cluster popular versatil cluster method base relax normalis graph cut object despit popular howev singl agre upon method tune import scale paramet determin automat number cluster extract popular heurist exist correspond theoret result scarc paper investig asymptot valu normalis cut increas sampl assum aris probabl distribut base result provid recommend improv spectral cluster methodolog correspond algorithm propos strong empir perform|['David Hofmeyr']|['stat.ML']
2017-04-07T11:23:26Z|2017-03-29T10:17:57Z|http://arxiv.org/abs/1703.09956v1|http://arxiv.org/pdf/1703.09956v1|Marginal likelihood based model comparison in Fuzzy Bayesian Learning|margin likelihood base model comparison fuzzi bayesian learn|In a recent paper [1] we introduced the Fuzzy Bayesian Learning (FBL) paradigm where expert opinions can be encoded in the form of fuzzy rule bases and the hyper-parameters of the fuzzy sets can be learned from data using a Bayesian approach. The present paper extends this work for selecting the most appropriate rule base among a set of competing alternatives, which best explains the data, by calculating the model evidence or marginal likelihood. We explain why this is an attractive alternative over simply minimizing a mean squared error metric of prediction and show the validity of the proposition using synthetic examples and a real world case study in the financial services sector.|recent paper introduc fuzzi bayesian learn fbl paradigm expert opinion encod form fuzzi rule base hyper paramet fuzzi set learn data use bayesian approach present paper extend work select appropri rule base among set compet altern best explain data calcul model evid margin likelihood explain whi attract altern simpli minim mean squar error metric predict show valid proposit use synthet exampl real world case studi financi servic sector|['Indranil Pan', 'Dirk Bester']|['stat.ML']
2017-04-07T11:23:30Z|2017-03-29T09:31:47Z|http://arxiv.org/abs/1703.09947v1|http://arxiv.org/pdf/1703.09947v1|Efficient Private ERM for Smooth Objectives|effici privat erm smooth object|In this paper, we consider efficient differentially private empirical risk minimization from the viewpoint of optimization algorithms. For strongly convex and smooth objectives, we prove that gradient descent with output perturbation not only achieves nearly optimal utility, but also significantly improves the running time of previous state-of-the-art private optimization algorithms, for both $\epsilon$-DP and $(\epsilon, \delta)$-DP. For non-convex but smooth objectives, we propose an RRPSGD (Random Round Private Stochastic Gradient Descent) algorithm, which provably converges to a stationary point with privacy guarantee. Besides the expected utility bounds, we also provide guarantees in high probability form. Experiments demonstrate that our algorithm consistently outperforms existing method in both utility and running time.|paper consid effici differenti privat empir risk minim viewpoint optim algorithm strong convex smooth object prove gradient descent output perturb onli achiev near optim util also signific improv run time previous state art privat optim algorithm epsilon dp epsilon delta dp non convex smooth object propos rrpsgd random round privat stochast gradient descent algorithm provabl converg stationari point privaci guarante besid expect util bound also provid guarante high probabl form experi demonstr algorithm consist outperform exist method util run time|['Jiaqi Zhang', 'Kai Zheng', 'Wenlong Mou', 'Liwei Wang']|['cs.LG', 'cs.DS', 'stat.ML']
2017-04-07T11:23:30Z|2017-03-29T08:32:21Z|http://arxiv.org/abs/1703.09930v1|http://arxiv.org/pdf/1703.09930v1|Adaptive Gaussian process approximation for Bayesian inference with   expensive likelihood functions|adapt gaussian process approxim bayesian infer expens likelihood function|We consider Bayesian inference problems with computationally intensive likelihood functions. We propose a Gaussian process (GP) based method to approximate the joint distribution of the unknown parameters and the data. In particular, we write the joint density approximately as a product of an approximate posterior density and an exponentiated GP surrogate. We then provide an adaptive algorithm to construct such an approximation, where an active learning method is used to choose the design points. With numerical examples, we illustrate that the proposed method has competitive performance against existing approaches for Bayesian computation.|consid bayesian infer problem comput intens likelihood function propos gaussian process gp base method approxim joint distribut unknown paramet data particular write joint densiti approxim product approxim posterior densiti exponenti gp surrog provid adapt algorithm construct approxim activ learn method use choos design point numer exampl illustr propos method competit perform exist approach bayesian comput|['Hongqiao Wang', 'Jinglai Li']|['stat.CO', 'stat.ML']
2017-04-07T11:23:30Z|2017-03-28T21:41:13Z|http://arxiv.org/abs/1703.09813v1|http://arxiv.org/pdf/1703.09813v1|Gradient-based Regularization Parameter Selection for Problems with   Non-smooth Penalty Functions|gradient base regular paramet select problem non smooth penalti function|In high-dimensional and/or non-parametric regression problems, regularization (or penalization) is used to control model complexity and induce desired structure. Each penalty has a weight parameter that indicates how strongly the structure corresponding to that penalty should be enforced. Typically the parameters are chosen to minimize the error on a separate validation set using a simple grid search or a gradient-free optimization method. It is more efficient to tune parameters if the gradient can be determined, but this is often difficult for problems with non-smooth penalty functions. Here we show that for many penalized regression problems, the validation loss is actually smooth almost-everywhere with respect to the penalty parameters. We can therefore apply a modified gradient descent algorithm to tune parameters. Through simulation studies on example regression problems, we find that increasing the number of penalty parameters and tuning them using our method can decrease the generalization error.|high dimension non parametr regress problem regular penal use control model complex induc desir structur penalti weight paramet indic strong structur correspond penalti enforc typic paramet chosen minim error separ valid set use simpl grid search gradient free optim method effici tune paramet gradient determin often difficult problem non smooth penalti function show mani penal regress problem valid loss actual smooth almost everywher respect penalti paramet therefor appli modifi gradient descent algorithm tune paramet simul studi exampl regress problem find increas number penalti paramet tune use method decreas general error|['Jean Feng', 'Noah Simon']|['stat.ML']
2017-04-07T11:23:30Z|2017-03-28T19:57:30Z|http://arxiv.org/abs/1703.09775v1|http://arxiv.org/pdf/1703.09775v1|Deep scattering transform applied to note onset detection and instrument   recognition|deep scatter transform appli note onset detect instrument recognit|Automatic Music Transcription (AMT) is one of the oldest and most well-studied problems in the field of music information retrieval. Within this challenging research field, onset detection and instrument recognition take important places in transcription systems, as they respectively help to determine exact onset times of notes and to recognize the corresponding instrument sources. The aim of this study is to explore the usefulness of multiscale scattering operators for these two tasks on plucked string instrument and piano music. After resuming the theoretical background and illustrating the key features of this sound representation method, we evaluate its performances comparatively to other classical sound representations. Using both MIDI-driven datasets with real instrument samples and real musical pieces, scattering is proved to outperform other sound representations for these AMT subtasks, putting forward its richer sound representation and invariance properties.|automat music transcript amt one oldest well studi problem field music inform retriev within challeng research field onset detect instrument recognit take import place transcript system respect help determin exact onset time note recogn correspond instrument sourc aim studi explor use multiscal scatter oper two task pluck string instrument piano music resum theoret background illustr key featur sound represent method evalu perform compar classic sound represent use midi driven dataset real instrument sampl real music piec scatter prove outperform sound represent amt subtask put forward richer sound represent invari properti|['D. Cazau', 'G. Revillon', 'O. Adam']|['stat.ML', 'cs.SD']
2017-04-07T11:23:30Z|2017-03-28T19:56:47Z|http://arxiv.org/abs/1703.09772v1|http://arxiv.org/pdf/1703.09772v1|Particle Filtering for PLCA model with Application to Music   Transcription|particl filter plca model applic music transcript|Automatic Music Transcription (AMT) consists in automatically estimating the notes in an audio recording, through three attributes: onset time, duration and pitch. Probabilistic Latent Component Analysis (PLCA) has become very popular for this task. PLCA is a spectrogram factorization method, able to model a magnitude spectrogram as a linear combination of spectral vectors from a dictionary. Such methods use the Expectation-Maximization (EM) algorithm to estimate the parameters of the acoustic model. This algorithm presents well-known inherent defaults (local convergence, initialization dependency), making EM-based systems limited in their applications to AMT, particularly in regards to the mathematical form and number of priors. To overcome such limits, we propose in this paper to employ a different estimation framework based on Particle Filtering (PF), which consists in sampling the posterior distribution over larger parameter ranges. This framework proves to be more robust in parameter estimation, more flexible and unifying in the integration of prior knowledge in the system. Note-level transcription accuracies of 61.8 $\%$ and 59.5 $\%$ were achieved on evaluation sound datasets of two different instrument repertoires, including the classical piano (from MAPS dataset) and the marovany zither, and direct comparisons to previous PLCA-based approaches are provided. Steps for further development are also outlined.|automat music transcript amt consist automat estim note audio record three attribut onset time durat pitch probabilist latent compon analysi plca becom veri popular task plca spectrogram factor method abl model magnitud spectrogram linear combin spectral vector dictionari method use expect maxim em algorithm estim paramet acoust model algorithm present well known inher default local converg initi depend make em base system limit applic amt particular regard mathemat form number prior overcom limit propos paper employ differ estim framework base particl filter pf consist sampl posterior distribut larger paramet rang framework prove robust paramet estim flexibl unifi integr prior knowledg system note level transcript accuraci achiev evalu sound dataset two differ instrument repertoir includ classic piano map dataset marovani zither direct comparison previous plca base approach provid step develop also outlin|['D. Cazau', 'G. Revillon', 'W. Yuancheng', 'O. Adam']|['stat.ML']
2017-04-07T11:23:30Z|2017-03-28T19:42:16Z|http://arxiv.org/abs/1703.09766v1|http://arxiv.org/pdf/1703.09766v1|Unifying the Stochastic Spectral Descent for Restricted Boltzmann   Machines with Bernoulli or Gaussian Inputs|unifi stochast spectral descent restrict boltzmann machin bernoulli gaussian input|Stochastic gradient descent based algorithms are typically used as the general optimization tools for most deep learning models. A Restricted Boltzmann Machine (RBM) is a probabilistic generative model that can be stacked to construct deep architectures. For RBM with Bernoulli inputs, non-Euclidean algorithm such as stochastic spectral descent (SSD) has been specifically designed to speed up the convergence with improved use of the gradient estimation by sampling methods. However, the existing algorithm and corresponding theoretical justification depend on the assumption that the possible configurations of inputs are finite, like binary variables. The purpose of this paper is to generalize SSD for Gaussian RBM being capable of mod- eling continuous data, regardless of the previous assumption. We propose the gradient descent methods in non-Euclidean space of parameters, via de- riving the upper bounds of logarithmic partition function for RBMs based on Schatten-infinity norm. We empirically show that the advantage and improvement of SSD over stochastic gradient descent (SGD).|stochast gradient descent base algorithm typic use general optim tool deep learn model restrict boltzmann machin rbm probabilist generat model stack construct deep architectur rbm bernoulli input non euclidean algorithm stochast spectral descent ssd specif design speed converg improv use gradient estim sampl method howev exist algorithm correspond theoret justif depend assumpt possibl configur input finit like binari variabl purpos paper general ssd gaussian rbm capabl mod ele continu data regardless previous assumpt propos gradient descent method non euclidean space paramet via de rive upper bound logarithm partit function rbms base schatten infin norm empir show advantag improv ssd stochast gradient descent sgd|['Kai Fan']|['stat.ML']
2017-04-07T11:23:30Z|2017-03-28T16:13:23Z|http://arxiv.org/abs/1703.09700v1|http://arxiv.org/pdf/1703.09700v1|Inverse Reinforcement Learning from Summary Data|invers reinforc learn summari data|Inverse reinforcement learning (IRL) aims to explain observed complex behavior by fitting reinforcement learning models to behavioral data. However, traditional IRL methods are only applicable when the observations are in the form of state-action paths. This is a problem in many real-world modelling settings, where only more limited observations are easily available. To address this issue, we extend the traditional IRL problem formulation. We call this new formulation the inverse reinforcement learning from summary data (IRL-SD) problem, where instead of state-action paths, only summaries of the paths are observed. We propose exact and approximate methods for both maximum likelihood and full posterior estimation for IRL-SD problems. Through case studies we compare these methods, demonstrating that the approximate methods can be used to solve moderate-sized IRL-SD problems in reasonable time.|invers reinforc learn irl aim explain observ complex behavior fit reinforc learn model behavior data howev tradit irl method onli applic observ form state action path problem mani real world model set onli limit observ easili avail address issu extend tradit irl problem formul call new formul invers reinforc learn summari data irl sd problem instead state action path onli summari path observ propos exact approxim method maximum likelihood full posterior estim irl sd problem case studi compar method demonstr approxim method use solv moder size irl sd problem reason time|['Antti Kangasrääsiö', 'Samuel Kaski']|['cs.LG', 'cs.AI', 'stat.ML']
2017-04-07T11:23:30Z|2017-03-28T15:56:53Z|http://arxiv.org/abs/1703.09646v1|http://arxiv.org/pdf/1703.09646v1|Hybrid Clustering based on Content and Connection Structure using Joint   Nonnegative Matrix Factorization|hybrid cluster base content connect structur use joint nonneg matrix factor|We present a hybrid method for latent information discovery on the data sets containing both text content and connection structure based on constrained low rank approximation. The new method jointly optimizes the Nonnegative Matrix Factorization (NMF) objective function for text clustering and the Symmetric NMF (SymNMF) objective function for graph clustering. We propose an effective algorithm for the joint NMF objective function, based on a block coordinate descent (BCD) framework. The proposed hybrid method discovers content associations via latent connections found using SymNMF. The method can also be applied with a natural conversion of the problem when a hypergraph formulation is used or the content is associated with hypergraph edges.   Experimental results show that by simultaneously utilizing both content and connection structure, our hybrid method produces higher quality clustering results compared to the other NMF clustering methods that uses content alone (standard NMF) or connection structure alone (SymNMF). We also present some interesting applications to several types of real world data such as citation recommendations of papers. The hybrid method proposed in this paper can also be applied to general data expressed with both feature space vectors and pairwise similarities and can be extended to the case with multiple feature spaces or multiple similarity measures.|present hybrid method latent inform discoveri data set contain text content connect structur base constrain low rank approxim new method joint optim nonneg matrix factor nmf object function text cluster symmetr nmf symnmf object function graph cluster propos effect algorithm joint nmf object function base block coordin descent bcd framework propos hybrid method discov content associ via latent connect found use symnmf method also appli natur convers problem hypergraph formul use content associ hypergraph edg experiment result show simultan util content connect structur hybrid method produc higher qualiti cluster result compar nmf cluster method use content alon standard nmf connect structur alon symnmf also present interest applic sever type real world data citat recommend paper hybrid method propos paper also appli general data express featur space vector pairwis similar extend case multipl featur space multipl similar measur|['Rundong Du', 'Barry Drake', 'Haesun Park']|['cs.LG', 'stat.ML']
2017-04-07T11:23:30Z|2017-03-28T15:28:28Z|http://arxiv.org/abs/1703.09631v1|http://arxiv.org/pdf/1703.09631v1|Algebraic Variety Models for High-Rank Matrix Completion|algebra varieti model high rank matrix complet|"We consider a generalization of low-rank matrix completion to the case where the data belongs to an algebraic variety, i.e. each data point is a solution to a system of polynomial equations. In this case the original matrix is possibly high-rank, but it becomes low-rank after mapping each column to a higher dimensional space of monomial features. Many well-studied extensions of linear models, including affine subspaces and their union, can be described by a variety model. In addition, varieties can be used to model a richer class of nonlinear quadratic and higher degree curves and surfaces. We study the sampling requirements for matrix completion under a variety model with a focus on a union of affine subspaces. We also propose an efficient matrix completion algorithm that minimizes a convex or non-convex surrogate of the rank of the matrix of monomial features. Our algorithm uses the well-known ""kernel trick"" to avoid working directly with the high-dimensional monomial matrix. We show the proposed algorithm is able to recover synthetically generated data up to the predicted sampling complexity bounds. The proposed algorithm also outperforms standard low rank matrix completion and subspace clustering techniques in experiments with real data."|consid general low rank matrix complet case data belong algebra varieti data point solut system polynomi equat case origin matrix possibl high rank becom low rank map column higher dimension space monomi featur mani well studi extens linear model includ affin subspac union describ varieti model addit varieti use model richer class nonlinear quadrat higher degre curv surfac studi sampl requir matrix complet varieti model focus union affin subspac also propos effici matrix complet algorithm minim convex non convex surrog rank matrix monomi featur algorithm use well known kernel trick avoid work direct high dimension monomi matrix show propos algorithm abl recov synthet generat data predict sampl complex bound propos algorithm also outperform standard low rank matrix complet subspac cluster techniqu experi real data|['Greg Ongie', 'Rebecca Willett', 'Robert D. Nowak', 'Laura Balzano']|['stat.ML']
2017-04-07T11:23:30Z|2017-03-28T14:01:57Z|http://arxiv.org/abs/1703.09580v1|http://arxiv.org/pdf/1703.09580v1|Early Stopping without a Validation Set|earli stop without valid set|Early stopping is a widely used technique to prevent poor generalization performance when training an over-expressive model by means of gradient-based optimization. To find a good point to halt the optimizer, a common practice is to split the dataset into a training and a smaller validation set to obtain an ongoing estimate of the generalization performance. In this paper we propose a novel early stopping criterion which is based on fast-to-compute, local statistics of the computed gradients and entirely removes the need for a held-out validation set. Our experiments show that this is a viable approach in the setting of least-squares and logistic regression as well as neural networks.|earli stop wide use techniqu prevent poor general perform train express model mean gradient base optim find good point halt optim common practic split dataset train smaller valid set obtain ongo estim general perform paper propos novel earli stop criterion base fast comput local statist comput gradient entir remov need held valid set experi show viabl approach set least squar logist regress well neural network|['Maren Mahsereci', 'Lukas Balles', 'Christoph Lassner', 'Philipp Hennig']|['cs.LG', 'stat.ML']
2017-04-07T11:23:34Z|2017-03-28T12:10:45Z|http://arxiv.org/abs/1703.09528v1|http://arxiv.org/pdf/1703.09528v1|A Nonparametric Bayesian Clustering to Discover Latent Covariance   Structure of Multiple Time Series|nonparametr bayesian cluster discov latent covari structur multipl time seri|Analyzing time series data is important to predict future events and changes in finance, manufacturing and administrative decisions. Gaussian processes (GPs) solve regression and classification problems by choosing appropriate kernels capturing covariance structure of data. In time series analysis, GP based regression methods recently demonstrate competitive performance by decomposing temporal covariance structure. Such covariance structure decomposition allows exploiting shared parameters over a set of multiple but selected time series. In this paper, we propose an efficient variational inference algorithm for nonparametric clustering over multiple GP covariance structures. We handle multiple time series by placing an Indian Buffet Process (IBP) prior on the presence of the additive shared kernels. We propose a new variational inference algorithm to learn the nonparametric Bayesian models for the clustering and regression problems. Experiments are conducted on both synthetic data sets and real world data sets, showing promising results in term of structure discoveries. In addition, our model learns GP kernels faster but still preserves a good predictive performance.|analyz time seri data import predict futur event chang financ manufactur administr decis gaussian process gps solv regress classif problem choos appropri kernel captur covari structur data time seri analysi gp base regress method recent demonstr competit perform decompos tempor covari structur covari structur decomposit allow exploit share paramet set multipl select time seri paper propos effici variat infer algorithm nonparametr cluster multipl gp covari structur handl multipl time seri place indian buffet process ibp prior presenc addit share kernel propos new variat infer algorithm learn nonparametr bayesian model cluster regress problem experi conduct synthet data set real world data set show promis result term structur discoveri addit model learn gp kernel faster still preserv good predict perform|['Anh Tong', 'Jaesik Choi']|['stat.ML']
2017-04-07T11:23:34Z|2017-03-29T10:40:02Z|http://arxiv.org/abs/1703.09477v2|http://arxiv.org/pdf/1703.09477v2|Convergence of the Forward-Backward Algorithm: Beyond the Worst Case   with the Help of Geometry|converg forward backward algorithm beyond worst case help geometri|We provide a comprehensive study of the convergence of forward-backward algorithm under suitable geometric conditions leading to fast rates. We present several new results and collect in a unified view a variety of results scattered in the literature, often providing simplified proofs. Novel contributions include the analysis of infinite dimensional convex minimization problems, allowing the case where minimizers might not exist. Further, we analyze the relation between different geometric conditions, and discuss novel connections with a priori conditions in linear inverse problems, including source conditions, restricted isometry properties and partial smoothness.|provid comprehens studi converg forward backward algorithm suitabl geometr condit lead fast rate present sever new result collect unifi view varieti result scatter literatur often provid simplifi proof novel contribut includ analysi infinit dimension convex minim problem allow case minim might exist analyz relat differ geometr condit discuss novel connect priori condit linear invers problem includ sourc condit restrict isometri properti partial smooth|['Guillaume Garrigos', 'Lorenzo Rosasco', 'Silvia Villa']|['math.OC', 'stat.ML']
2017-04-07T11:23:34Z|2017-03-28T04:42:11Z|http://arxiv.org/abs/1703.09397v1|http://arxiv.org/pdf/1703.09397v1|Solving Non-parametric Inverse Problem in Continuous Markov Random Field   using Loopy Belief Propagation|solv non parametr invers problem continu markov random field use loopi belief propag|In this paper, we address the inverse problem, or the statistical machine learning problem, in Markov random fields with a non-parametric pair-wise energy function with continuous variables. The inverse problem is formulated by maximum likelihood estimation. The exact treatment of maximum likelihood estimation is intractable because of two problems: (1) it includes the evaluation of the partition function and (2) it is formulated in the form of functional optimization. We avoid Problem (1) by using Bethe approximation. Bethe approximation is an approximation technique equivalent to the loopy belief propagation. Problem (2) can be solved by using orthonormal function expansion. Orthonormal function expansion can reduce a functional optimization problem to a function optimization problem. Our method can provide an analytic form of the solution of the inverse problem within the framework of Bethe approximation.|paper address invers problem statist machin learn problem markov random field non parametr pair wise energi function continu variabl invers problem formul maximum likelihood estim exact treatment maximum likelihood estim intract becaus two problem includ evalu partit function formul form function optim avoid problem use beth approxim beth approxim approxim techniqu equival loopi belief propag problem solv use orthonorm function expans orthonorm function expans reduc function optim problem function optim problem method provid analyt form solut invers problem within framework beth approxim|['Muneki Yasuda', 'Shun Kataoka']|['stat.ML', 'cond-mat.dis-nn', 'cs.LG']
2017-04-07T11:23:34Z|2017-03-27T21:05:15Z|http://arxiv.org/abs/1703.09310v1|http://arxiv.org/pdf/1703.09310v1|Adaptive Simulation-based Training of AI Decision-makers using Bayesian   Optimization|adapt simul base train ai decis maker use bayesian optim|This work studies how an AI-controlled dog-fighting agent with tunable decision-making parameters can learn to optimize performance against an intelligent adversary, as measured by a stochastic objective function evaluated on simulated combat engagements. Gaussian process Bayesian optimization (GPBO) techniques are developed to automatically learn global Gaussian Process (GP) surrogate models, which provide statistical performance predictions in both explored and unexplored areas of the parameter space. This allows a learning engine to sample full-combat simulations at parameter values that are most likely to optimize performance and also provide highly informative data points for improving future predictions. However, standard GPBO methods do not provide a reliable surrogate model for the highly volatile objective functions found in aerial combat, and thus do not reliably identify global maxima. These issues are addressed by novel Repeat Sampling (RS) and Hybrid Repeat/Multi-point Sampling (HRMS) techniques. Simulation studies show that HRMS improves the accuracy of GP surrogate models, allowing AI decision-makers to more accurately predict performance and efficiently tune parameters.|work studi ai control dog fight agent tunabl decis make paramet learn optim perform intellig adversari measur stochast object function evalu simul combat engag gaussian process bayesian optim gpbo techniqu develop automat learn global gaussian process gp surrog model provid statist perform predict explor unexplor area paramet space allow learn engin sampl full combat simul paramet valu like optim perform also provid high inform data point improv futur predict howev standard gpbo method provid reliabl surrog model high volatil object function found aerial combat thus reliabl identifi global maxima issu address novel repeat sampl rs hybrid repeat multi point sampl hrms techniqu simul studi show hrms improv accuraci gp surrog model allow ai decis maker accur predict perform effici tune paramet|['Brett W. Israelsen', 'Nisar Ahmed', 'Kenneth Center', 'Roderick Green', 'Winston Bennett Jr']|['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']
2017-04-07T11:23:34Z|2017-03-27T18:07:32Z|http://arxiv.org/abs/1703.09244v1|http://arxiv.org/pdf/1703.09244v1|Adversarial Source Identification Game with Corrupted Training|adversari sourc identif game corrupt train|We study a variant of the source identification game with training data in which part of the training data is corrupted by an attacker. In the addressed scenario, the defender aims at deciding whether a test sequence has been drawn according to a discrete memoryless source $X \sim P_X$, whose statistics are known to him through the observation of a training sequence generated by $X$. In order to undermine the correct decision under the alternative hypothesis that the test sequence has not been drawn from $X$, the attacker can modify a sequence produced by a source $Y \sim P_Y$ up to a certain distortion, and corrupt the training sequence either by adding some fake samples or by replacing some samples with fake ones. We derive the unique rationalizable equilibrium of the two versions of the game in the asymptotic regime and by assuming that the defender bases its decision by relying only on the first order statistics of the test and the training sequences. By mimicking Stein's lemma, we derive the best achievable performance for the defender when the first type error probability is required to tend to zero exponentially fast with an arbitrarily small, yet positive, error exponent. We then use such a result to analyze the ultimate distinguishability of any two sources as a function of the allowed distortion and the fraction of corrupted samples injected into the training sequence.|studi variant sourc identif game train data part train data corrupt attack address scenario defend aim decid whether test sequenc drawn accord discret memoryless sourc sim whose statist known observ train sequenc generat order undermin correct decis altern hypothesi test sequenc drawn attack modifi sequenc produc sourc sim certain distort corrupt train sequenc either ad fake sampl replac sampl fake one deriv uniqu rationaliz equilibrium two version game asymptot regim assum defend base decis reli onli first order statist test train sequenc mimick stein lemma deriv best achiev perform defend first type error probabl requir tend zero exponenti fast arbitrarili small yet posit error expon use result analyz ultim distinguish ani two sourc function allow distort fraction corrupt sampl inject train sequenc|['Mauro Barni', 'Benedetta Tondi']|['cs.CR', 'stat.ML']
2017-04-07T11:23:34Z|2017-03-27T17:50:53Z|http://arxiv.org/abs/1703.09207v1|http://arxiv.org/pdf/1703.09207v1|Fairness in Criminal Justice Risk Assessments: The State of the Art|fair crimin justic risk assess state art|Objectives: Discussions of fairness in criminal justice risk assessments typically lack conceptual precision. Rhetoric too often substitutes for careful analysis. In this paper, we seek to clarify the tradeoffs between different kinds of fairness and between fairness and accuracy.   Methods: We draw on the existing literatures in criminology, computer science and statistics to provide an integrated examination of fairness and accuracy in criminal justice risk assessments. We also provide an empirical illustration using data from arraignments.   Results: We show that there are at least six kinds of fairness, some of which are incompatible with one another and with accuracy.   Conclusions: Except in trivial cases, it is impossible to maximize accuracy and fairness at the same time, and impossible simultaneously to satisfy all kinds of fairness. In practice, a major complication is different base rates across different legally protected groups. There is a need to consider challenging tradeoffs.|object discuss fair crimin justic risk assess typic lack conceptu precis rhetor often substitut care analysi paper seek clarifi tradeoff differ kind fair fair accuraci method draw exist literatur criminolog comput scienc statist provid integr examin fair accuraci crimin justic risk assess also provid empir illustr use data arraign result show least six kind fair incompat one anoth accuraci conclus except trivial case imposs maxim accuraci fair time imposs simultan satisfi kind fair practic major complic differ base rate across differ legal protect group need consid challeng tradeoff|['Richard Berk', 'Hoda Heidari', 'Shahin Jabbari', 'Michael Kearns', 'Aaron Roth']|['stat.ML']
2017-04-07T11:23:34Z|2017-03-27T17:45:07Z|http://arxiv.org/abs/1703.09202v1|http://arxiv.org/pdf/1703.09202v1|Biologically inspired protection of deep networks from adversarial   attacks|biolog inspir protect deep network adversari attack|Inspired by biophysical principles underlying nonlinear dendritic computation in neural circuits, we develop a scheme to train deep neural networks to make them robust to adversarial attacks. Our scheme generates highly nonlinear, saturated neural networks that achieve state of the art performance on gradient based adversarial examples on MNIST, despite never being exposed to adversarially chosen examples during training. Moreover, these networks exhibit unprecedented robustness to targeted, iterative schemes for generating adversarial examples, including second-order methods. We further identify principles governing how these networks achieve their robustness, drawing on methods from information geometry. We find these networks progressively create highly flat and compressed internal representations that are sensitive to very few input dimensions, while still solving the task. Moreover, they employ highly kurtotic weight distributions, also found in the brain, and we demonstrate how such kurtosis can protect even linear classifiers from adversarial attack.|inspir biophys principl nonlinear dendrit comput neural circuit develop scheme train deep neural network make robust adversari attack scheme generat high nonlinear satur neural network achiev state art perform gradient base adversari exampl mnist despit never expos adversari chosen exampl dure train moreov network exhibit unpreced robust target iter scheme generat adversari exampl includ second order method identifi principl govern network achiev robust draw method inform geometri find network progress creat high flat compress intern represent sensit veri input dimens still solv task moreov employ high kurtot weight distribut also found brain demonstr kurtosi protect even linear classifi adversari attack|['Aran Nayebi', 'Surya Ganguli']|['stat.ML', 'cs.LG', 'q-bio.NC']
2017-04-07T11:23:34Z|2017-03-28T21:22:19Z|http://arxiv.org/abs/1703.09194v2|http://arxiv.org/pdf/1703.09194v2|Sticking the Landing: An Asymptotically Zero-Variance Gradient Estimator   for Variational Inference|stick land asymptot zero varianc gradient estim variat infer|We propose a simple and general variant of the standard reparameterized gradient estimator for the variational evidence lower bound. Specifically, we remove a part of the total derivative with respect to the variational parameters that corresponds to the score function. Removing this term produces an unbiased gradient estimator whose variance approaches zero as the approximate posterior approaches the exact posterior. We analyze the behavior of this gradient estimator theoretically and empirically, and generalize it to more complex variational distributions such as mixtures and importance-weighted posteriors.|propos simpl general variant standard reparameter gradient estim variat evid lower bound specif remov part total deriv respect variat paramet correspond score function remov term produc unbias gradient estim whose varianc approach zero approxim posterior approach exact posterior analyz behavior gradient estim theoret empir general complex variat distribut mixtur import weight posterior|['Geoffrey Roeder', 'Yuhuai Wu', 'David Duvenaud']|['stat.ML', 'cs.LG']
2017-04-07T11:23:34Z|2017-03-27T16:16:35Z|http://arxiv.org/abs/1703.09165v1|http://arxiv.org/pdf/1703.09165v1|PWLS-ULTRA: An Efficient Clustering and Learning-Based Approach for   Low-Dose 3D CT Image Reconstruction|pwls ultra effici cluster learn base approach low dose ct imag reconstruct|The development of computed tomography (CT) image reconstruction methods that significantly reduce patient radiation exposure while maintaining high image quality is an important area of research in low-dose CT (LDCT) imaging. We propose a new penalized weighted least squares (PWLS) reconstruction method that exploits regularization based on an efficient Union of Learned TRAnsforms (PWLS-ULTRA). The union of square transforms is pre-learned from numerous image patches extracted from a dataset of CT images or volumes. The proposed PWLS-based cost function is optimized by alternating between an image update step, and a sparse coding and clustering step. The CT image update step is accelerated by a relaxed linearized augmented Lagrangian method with ordered-subsets that reduces the number of forward and backward projections. Simulations with 2D and 3D axial CT scans of the XCAT phantom and 3D helical chest scans show that for low-dose levels, the proposed method significantly improves the quality of reconstructed images compared to PWLS reconstruction with a nonadaptive edge-preserving regularizer (PWLS-EP). PWLS with regularization based on a union of learned transforms leads to better image reconstructions than using a single learned square transform or a learned overcomplete synthesis dictionary. We also incorporate patch-based weights in PWLS-ULTRA that enhance image quality and help improve image resolution uniformity.|develop comput tomographi ct imag reconstruct method signific reduc patient radiat exposur maintain high imag qualiti import area research low dose ct ldct imag propos new penal weight least squar pwls reconstruct method exploit regular base effici union learn transform pwls ultra union squar transform pre learn numer imag patch extract dataset ct imag volum propos pwls base cost function optim altern imag updat step spars code cluster step ct imag updat step acceler relax linear augment lagrangian method order subset reduc number forward backward project simul axial ct scan xcat phantom helic chest scan show low dose level propos method signific improv qualiti reconstruct imag compar pwls reconstruct nonadapt edg preserv regular pwls ep pwls regular base union learn transform lead better imag reconstruct use singl learn squar transform learn overcomplet synthesi dictionari also incorpor patch base weight pwls ultra enhanc imag qualiti help improv imag resolut uniform|['Xuehang Zheng', 'Saiprasad Ravishankar', 'Yong Long', 'Jeffrey A. Fessler']|['stat.ML']
2017-04-07T11:23:34Z|2017-03-27T14:38:15Z|http://arxiv.org/abs/1703.09112v1|http://arxiv.org/pdf/1703.09112v1|Sparse Multi-Output Gaussian Processes for Medical Time Series   Prediction|spars multi output gaussian process medic time seri predict|In real-time monitoring of hospital patients, high-quality inference of patients' health status using all information available from clinical covariates and lab tests are essential to enable successful medical interventions and improve patient outcomes. In this work, we develop and explore a Bayesian nonparametric model based on Gaussian process (GP) regression for hospital patient monitoring. Our method, MedGP, incorporates 24 clinical and lab covariates and supports a rich reference data set from which the relationships between these observed covariates may be inferred and exploited for high-quality inference of patient state over time. To do this, we develop a highly structured sparse GP kernel to enable tractable computation over tens of thousands of time points while estimating correlations among clinical covariates, patients, and periodicity in high-dimensional time series measurements of physiological signals. We apply MedGP to data from hundreds of thousands of patients treated at the Hospital of the University of Pennsylvania. MedGP has a number of benefits over current methods, including (i) not requiring an alignment of the time series data, (ii) quantifying confidence intervals in the predictions, (iii) exploiting a vast and rich database of patients, and (iv) providing interpretable relationships among clinical covariates. We evaluate and compare results from MedGP on the task of online state prediction for three different patient subgroups.|real time monitor hospit patient high qualiti infer patient health status use inform avail clinic covari lab test essenti enabl success medic intervent improv patient outcom work develop explor bayesian nonparametr model base gaussian process gp regress hospit patient monitor method medgp incorpor clinic lab covari support rich refer data set relationship observ covari may infer exploit high qualiti infer patient state time develop high structur spars gp kernel enabl tractabl comput ten thousand time point estim correl among clinic covari patient period high dimension time seri measur physiolog signal appli medgp data hundr thousand patient treat hospit univers pennsylvania medgp number benefit current method includ requir align time seri data ii quantifi confid interv predict iii exploit vast rich databas patient iv provid interpret relationship among clinic covari evalu compar result medgp task onlin state predict three differ patient subgroup|['Li-Fang Cheng', 'Gregory Darnell', 'Corey Chivers', 'Michael E Draugelis', 'Kai Li', 'Barbara E Engelhardt']|['stat.ML']
