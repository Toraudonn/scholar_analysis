2017-03-28T14:05:16Z|2017-03-27T16:48:03Z|http://arxiv.org/abs/1703.09179v1|http://arxiv.org/pdf/1703.09179v1|Transfer learning for music classification and regression tasks|transfer learn music classif regress task|In this paper, we present a transfer learning approach for music classification and regression tasks. We propose to use a pretrained convnet feature, a concatenated feature vector using activations of feature maps of multiple layers in a trained convolutional network. We show that how this convnet feature can serve as a general-purpose music representation. In the experiment, a convnet is trained for music tagging and then transferred for many music-related classification and regression tasks as well as an audio-related classification task. In experiments, the convnet feature outperforms the baseline MFCC feature in all tasks and many reported approaches of aggregating MFCCs and low- and high-level music features.|paper present transfer learn approach music classif regress task propos use pretrain convnet featur concaten featur vector use activ featur map multipl layer train convolut network show convnet featur serv general purpos music represent experi convnet train music tag transfer mani music relat classif regress task well audio relat classif task experi convnet featur outperform baselin mfcc featur task mani report approach aggreg mfccs low high level music featur|['Keunwoo Choi', 'György Fazekas', 'Mark Sandler', 'Kyunghyun Cho']|['cs.CV', 'cs.AI', 'cs.MM', 'cs.SD']
2017-03-28T14:05:16Z|2017-03-27T06:19:38Z|http://arxiv.org/abs/1703.08944v1|http://arxiv.org/abs/1703.08944v1|Intelligent bidirectional rapidly-exploring random trees for optimal   motion planning in complex cluttered environments|intellig bidirect rapid explor random tree optim motion plan complex clutter environ|The sampling based motion planning algorithm known as Rapidly-exploring Random Trees (RRT) has gained the attention of many researchers due to their computational efficiency and effectiveness. Recently, a variant of RRT called RRT* has been proposed that ensures asymptotic optimality. Subsequently its bidirectional version has also been introduced in the literature known as Bidirectional-RRT* (B-RRT*). We introduce a new variant called Intelligent Bidirectional-RRT* (IB-RRT*) which is an improved variant of the optimal RRT* and bidirectional version of RRT* (B-RRT*) algorithms and is specially designed for complex cluttered environments. IB-RRT* utilizes the bidirectional trees approach and introduces intelligent sample insertion heuristic for fast convergence to the optimal path solution using uniform sampling heuristics. The proposed algorithm is evaluated theoretically and experimental results are presented that compares IB-RRT* with RRT* and B-RRT*. Moreover, experimental results demonstrate the superior efficiency of IB-RRT* in comparison with RRT* and B-RRT in complex cluttered environments.|sampl base motion plan algorithm known rapid explor random tree rrt gain attent mani research due comput effici effect recent variant rrt call rrt propos ensur asymptot optim subsequ bidirect version also introduc literatur known bidirect rrt rrt introduc new variant call intellig bidirect rrt ib rrt improv variant optim rrt bidirect version rrt rrt algorithm special design complex clutter environ ib rrt util bidirect tree approach introduc intellig sampl insert heurist fast converg optim path solut use uniform sampl heurist propos algorithm evalu theoret experiment result present compar ib rrt rrt rrt moreov experiment result demonstr superior effici ib rrt comparison rrt rrt complex clutter environ|['Ahmed Hussain Qureshi', 'Yasar Ayaz']|['cs.RO', 'cs.AI']
2017-03-28T14:05:16Z|2017-03-27T04:03:56Z|http://arxiv.org/abs/1703.08922v1|http://arxiv.org/pdf/1703.08922v1|On Automating the Doctrine of Double Effect|autom doctrin doubl effect|The doctrine of double effect ($\mathcal{DDE}$) is a long-studied ethical principle that governs when actions that have both positive and negative effects are to be allowed. The goal in this paper is to automate $\mathcal{DDE}$. We briefly present $\mathcal{DDE}$, and use a first-order modal logic, the deontic cognitive event calculus, as our framework to formalize the doctrine. We present formalizations of increasingly stronger versions of the principle, including what is known as the doctrine of triple effect. We then use our framework to simulate successfully scenarios that have been used to test for the presence of the principle in human subjects. Our framework can be used in two different modes: One can use it to build $\mathcal{DDE}$-compliant autonomous systems from scratch, or one can use it to verify that a given AI system is $\mathcal{DDE}$-compliant, by applying a $\mathcal{DDE}$ layer on an existing system or model. For the latter mode, the underlying AI system can be built using any architecture (planners, deep neural networks, bayesian networks, knowledge-representation systems, or a hybrid); as long as the system exposes a few parameters in its model, such verification is possible. The role of the $\mathcal{DDE}$ layer here is akin to a (dynamic or static) software verifier that examines existing software modules. Finally, we end by presenting initial work on how one can apply our $\mathcal{DDE}$ layer to the STRIPS-style planning model, and to a modified POMDP model.|doctrin doubl effect mathcal dde long studi ethic principl govern action posit negat effect allow goal paper autom mathcal dde briefli present mathcal dde use first order modal logic deontic cognit event calculus framework formal doctrin present formal increas stronger version principl includ known doctrin tripl effect use framework simul success scenario use test presenc principl human subject framework use two differ mode one use build mathcal dde compliant autonom system scratch one use verifi given ai system mathcal dde compliant appli mathcal dde layer exist system model latter mode ai system built use ani architectur planner deep neural network bayesian network knowledg represent system hybrid long system expos paramet model verif possibl role mathcal dde layer akin dynam static softwar verifi examin exist softwar modul final end present initi work one appli mathcal dde layer strip style plan model modifi pomdp model|['Naveen Sundar Govindarajulu', 'Selmer Bringsjord']|['cs.AI', 'cs.LO', 'cs.RO']
2017-03-28T14:05:16Z|2017-03-26T19:39:50Z|http://arxiv.org/abs/1703.08862v1|http://arxiv.org/pdf/1703.08862v1|Socially Aware Motion Planning with Deep Reinforcement Learning|social awar motion plan deep reinforc learn|For robotic vehicles to navigate safely and efficiently in pedestrian-rich environments, it is important to model subtle human behaviors and navigation rules. However, while instinctive to humans, socially compliant navigation is still difficult to quantify due to the stochasticity in people's behaviors. Existing works are mostly focused on using feature-matching techniques to describe and imitate human paths, but often do not generalize well since the feature values can vary from person to person, and even run to run. This work notes that while it is challenging to directly specify the details of what to do (precise mechanisms of human navigation), it is straightforward to specify what not to do (violations of social norms). Specifically, using deep reinforcement learning, this work develops a time-efficient navigation policy that respects common social norms. The proposed method is shown to enable fully autonomous navigation of a robotic vehicle moving at human walking speed in an environment with many pedestrians.|robot vehicl navig safe effici pedestrian rich environ import model subtl human behavior navig rule howev instinct human social compliant navig still difficult quantifi due stochast peopl behavior exist work focus use featur match techniqu describ imit human path often general well sinc featur valu vari person person even run run work note challeng direct specifi detail precis mechan human navig straightforward specifi violat social norm specif use deep reinforc learn work develop time effici navig polici respect common social norm propos method shown enabl fulli autonom navig robot vehicl move human walk speed environ mani pedestrian|['Yu Fan Chen', 'Michael Everett', 'Miao Liu', 'Jonathan P. How']|['cs.RO', 'cs.AI', 'cs.HC']
2017-03-28T14:05:16Z|2017-03-26T16:20:36Z|http://arxiv.org/abs/1703.08840v1|http://arxiv.org/pdf/1703.08840v1|Inferring The Latent Structure of Human Decision-Making from Raw Visual   Inputs|infer latent structur human decis make raw visual input|The goal of imitation learning is to match example expert behavior, without access to a reinforcement signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are not explicitly modeled. We introduce an extension to the Generative Adversarial Imitation Learning method that can infer the latent structure of human decision-making in an unsupervised way. Our method can not only imitate complex behaviors, but also learn interpretable and meaningful representations. We demonstrate that the approach is applicable to high-dimensional environments including raw visual inputs. In the highway driving domain, we show that a model learned from demonstrations is able to both produce different styles of human-like driving behaviors and accurately anticipate human actions. Our method surpasses various baselines in terms of performance and functionality.|goal imit learn match exampl expert behavior without access reinforc signal expert demonstr provid human howev often show signific variabl due latent factor explicit model introduc extens generat adversari imit learn method infer latent structur human decis make unsupervis way method onli imit complex behavior also learn interpret meaning represent demonstr approach applic high dimension environ includ raw visual input highway drive domain show model learn demonstr abl produc differ style human like drive behavior accur anticip human action method surpass various baselin term perform function|['Yunzhu Li', 'Jiaming Song', 'Stefano Ermon']|['cs.LG', 'cs.AI', 'cs.CV']
2017-03-28T14:05:16Z|2017-03-26T15:26:34Z|http://arxiv.org/abs/1703.08825v1|http://arxiv.org/pdf/1703.08825v1|Surrogate Model of Multi-Period Flexibility from a Home Energy   Management System|surrog model multi period flexibl home energi manag system|Near-future electric distribution grids operation will have to rely on demand-side flexibility, both by implementation of demand response strategies and by taking advantage of the intelligent management of increasingly common small-scale energy storage. Home energy management systems (HEMS) will play a crucial role on the flexibility provision to both system operators and market players like aggregators. Modeling multi-period flexibility from residential consumers (HEMS flexibility), such as battery storage and electric water heater, while complying with internal constraints (comfort levels, data privacy) and uncertainty is a complex task. This paper describes a computational method that is capable of efficiently define and learn the feasible flexibility set from controllable resources connected to a HEMS. An Evolutionary Particle Swarm Optimization (EPSO) algorithm is adopted and reshaped to derive a set of feasible temporal trajectories for the residential net-load, considering storage, flexible appliances, and predefined costumer preferences, as well as load and photovoltaic (PV) forecast uncertainty. A support vector data description (SVDD) algorithm is used to build models capable of classifying feasible and unfeasible HEMS operating trajectories upon request from an optimization/control algorithm operated by a DSO or market player.|near futur electr distribut grid oper reli demand side flexibl implement demand respons strategi take advantag intellig manag increas common small scale energi storag home energi manag system hem play crucial role flexibl provis system oper market player like aggreg model multi period flexibl residenti consum hem flexibl batteri storag electr water heater compli intern constraint comfort level data privaci uncertainti complex task paper describ comput method capabl effici defin learn feasibl flexibl set control resourc connect hem evolutionari particl swarm optim epso algorithm adopt reshap deriv set feasibl tempor trajectori residenti net load consid storag flexibl applianc predefin costum prefer well load photovolta pv forecast uncertainti support vector data descript svdd algorithm use build model capabl classifi feasibl unfeas hem oper trajectori upon request optim control algorithm oper dso market player|['Rui Pinto', 'Ricardo Bessa', 'Manuel Matos']|['cs.NE', 'cs.AI']
2017-03-28T14:05:16Z|2017-03-26T05:44:56Z|http://arxiv.org/abs/1703.08769v1|http://arxiv.org/pdf/1703.08769v1|Open Vocabulary Scene Parsing|open vocabulari scene pars|Recognizing arbitrary objects in the wild has been a challenging problem due to the limitations of existing classification models and datasets. In this paper, we propose a new task that aims at parsing scene with a large and open vocabulary, and several evaluation metrics are explored for this problem. Our proposed approach to this problem is a joint image pixel and word concept embeddings framework, where word concepts are connected by semantic relations. We validate the open vocabulary prediction ability of our framework on ADE20K dataset which covers a wide variety of scenes and objects. We further explore the trained joint embedding space to show its interpretability.|recogn arbitrari object wild challeng problem due limit exist classif model dataset paper propos new task aim pars scene larg open vocabulari sever evalu metric explor problem propos approach problem joint imag pixel word concept embed framework word concept connect semant relat valid open vocabulari predict abil framework adek dataset cover wide varieti scene object explor train joint embed space show interpret|['Hang Zhao', 'Xavier Puig', 'Bolei Zhou', 'Sanja Fidler', 'Antonio Torralba']|['cs.CV', 'cs.AI']
2017-03-28T14:05:16Z|2017-03-26T03:47:54Z|http://arxiv.org/abs/1703.08762v1|http://arxiv.org/pdf/1703.08762v1|Team Formation for Scheduling Educational Material in Massive Online   Classes|team format schedul educ materi massiv onlin class|Whether teaching in a classroom or a Massive Online Open Course it is crucial to present the material in a way that benefits the audience as a whole. We identify two important tasks to solve towards this objective, 1 group students so that they can maximally benefit from peer interaction and 2 find an optimal schedule of the educational material for each group. Thus, in this paper, we solve the problem of team formation and content scheduling for education. Given a time frame d, a set of students S with their required need to learn different activities T and given k as the number of desired groups, we study the problem of finding k group of students. The goal is to teach students within time frame d such that their potential for learning is maximized and find the best schedule for each group. We show this problem to be NP-hard and develop a polynomial algorithm for it. We show our algorithm to be effective both on synthetic as well as a real data set. For our experiments, we use real data on students' grades in a Computer Science department. As part of our contribution, we release a semi-synthetic dataset that mimics the properties of the real data.|whether teach classroom massiv onlin open cours crucial present materi way benefit audienc whole identifi two import task solv toward object group student maxim benefit peer interact find optim schedul educ materi group thus paper solv problem team format content schedul educ given time frame set student requir need learn differ activ given number desir group studi problem find group student goal teach student within time frame potenti learn maxim find best schedul group show problem np hard develop polynomi algorithm show algorithm effect synthet well real data set experi use real data student grade comput scienc depart part contribut releas semi synthet dataset mimic properti real data|['Sanaz Bahargam', 'Dóra Erdos', 'Azer Bestavros', 'Evimaria Terzi']|['cs.AI']
2017-03-28T14:05:16Z|2017-03-25T15:37:09Z|http://arxiv.org/abs/1703.08705v1|http://arxiv.org/pdf/1703.08705v1|Comparing Rule-Based and Deep Learning Models for Patient Phenotyping|compar rule base deep learn model patient phenotyp|Objective: We investigate whether deep learning techniques for natural language processing (NLP) can be used efficiently for patient phenotyping. Patient phenotyping is a classification task for determining whether a patient has a medical condition, and is a crucial part of secondary analysis of healthcare data. We assess the performance of deep learning algorithms and compare them with classical NLP approaches.   Materials and Methods: We compare convolutional neural networks (CNNs), n-gram models, and approaches based on cTAKES that extract pre-defined medical concepts from clinical notes and use them to predict patient phenotypes. The performance is tested on 10 different phenotyping tasks using 1,610 discharge summaries extracted from the MIMIC-III database.   Results: CNNs outperform other phenotyping algorithms in all 10 tasks. The average F1-score of our model is 76 (PPV of 83, and sensitivity of 71) with our model having an F1-score up to 37 points higher than alternative approaches. We additionally assess the interpretability of our model by presenting a method that extracts the most salient phrases for a particular prediction.   Conclusion: We show that NLP methods based on deep learning improve the performance of patient phenotyping. Our CNN-based algorithm automatically learns the phrases associated with each patient phenotype. As such, it reduces the annotation complexity for clinical domain experts, who are normally required to develop task-specific annotation rules and identify relevant phrases. Our method performs well in terms of both performance and interpretability, which indicates that deep learning is an effective approach to patient phenotyping based on clinicians' notes.|object investig whether deep learn techniqu natur languag process nlp use effici patient phenotyp patient phenotyp classif task determin whether patient medic condit crucial part secondari analysi healthcar data assess perform deep learn algorithm compar classic nlp approach materi method compar convolut neural network cnns gram model approach base ctake extract pre defin medic concept clinic note use predict patient phenotyp perform test differ phenotyp task use discharg summari extract mimic iii databas result cnns outperform phenotyp algorithm task averag score model ppv sensit model score point higher altern approach addit assess interpret model present method extract salient phrase particular predict conclus show nlp method base deep learn improv perform patient phenotyp cnn base algorithm automat learn phrase associ patient phenotyp reduc annot complex clinic domain expert normal requir develop task specif annot rule identifi relev phrase method perform well term perform interpret indic deep learn effect approach patient phenotyp base clinician note|['Sebastian Gehrmann', 'Franck Dernoncourt', 'Yeran Li', 'Eric T. Carlson', 'Joy T. Wu', 'Jonathan Welt', 'John Foote Jr.', 'Edward T. Moseley', 'David W. Grant', 'Patrick D. Tyler', 'Leo Anthony Celi']|['cs.CL', 'cs.AI', 'cs.NE', 'stat.ML']
2017-03-28T14:05:16Z|2017-03-24T15:43:39Z|http://arxiv.org/abs/1703.08475v1|http://arxiv.org/pdf/1703.08475v1|Overcoming Catastrophic Forgetting by Incremental Moment Matching|overcom catastroph forget increment moment match|Catastrophic forgetting is a problem which refers to losing the information of the first task after training from the second task in continual learning of neural networks. To resolve this problem, we propose the incremental moment matching (IMM), which uses the Bayesian neural network framework. IMM assumes that the posterior distribution of parameters of neural networks is approximated with Gaussian distribution and incrementally matches the moment of the posteriors, which are trained for the first and second task, respectively. To make our Gaussian assumption reasonable, the IMM procedure utilizes various transfer learning techniques including weight transfer, L2-norm of old and new parameters, and a newly proposed variant of dropout using old parameters. We analyze our methods on the MNIST and CIFAR-10 datasets, and then evaluate them on a real-world life-log dataset collected using Google Glass. Experimental results show that IMM produces state-of-the-art performance in a variety of datasets.|catastroph forget problem refer lose inform first task train second task continu learn neural network resolv problem propos increment moment match imm use bayesian neural network framework imm assum posterior distribut paramet neural network approxim gaussian distribut increment match moment posterior train first second task respect make gaussian assumpt reason imm procedur util various transfer learn techniqu includ weight transfer norm old new paramet newli propos variant dropout use old paramet analyz method mnist cifar dataset evalu real world life log dataset collect use googl glass experiment result show imm produc state art perform varieti dataset|['Sang-Woo Lee', 'Jin-Hwa Kim', 'Jung-Woo Ha', 'Byoung-Tak Zhang']|['cs.LG', 'cs.AI']
2017-03-28T14:05:20Z|2017-03-24T14:40:31Z|http://arxiv.org/abs/1703.08428v1|http://arxiv.org/abs/1703.08428v1|Calendar.help: Designing a Workflow-Based Scheduling Agent with Humans   in the Loop|calendar help design workflow base schedul agent human loop|Although information workers may complain about meetings, they are an essential part of their work life. Consequently, busy people spend a significant amount of time scheduling meetings. We present Calendar.help, a system that provides fast, efficient scheduling through structured workflows. Users interact with the system via email, delegating their scheduling needs to the system as if it were a human personal assistant. Common scheduling scenarios are broken down using well-defined workflows and completed as a series of microtasks that are automated when possible and executed by a human otherwise. Unusual scenarios fall back to a trained human assistant who executes them as unstructured macrotasks. We describe the iterative approach we used to develop Calendar.help, and share the lessons learned from scheduling thousands of meetings during a year of real-world deployments. Our findings provide insight into how complex information tasks can be broken down into repeatable components that can be executed efficiently to improve productivity.|although inform worker may complain meet essenti part work life consequ busi peopl spend signific amount time schedul meet present calendar help system provid fast effici schedul structur workflow user interact system via email deleg schedul need system human person assist common schedul scenario broken use well defin workflow complet seri microtask autom possibl execut human otherwis unusu scenario fall back train human assist execut unstructur macrotask describ iter approach use develop calendar help share lesson learn schedul thousand meet dure year real world deploy find provid insight complex inform task broken repeat compon execut effici improv product|['Justin Cranshaw', 'Emad Elwany', 'Todd Newman', 'Rafal Kocielnik', 'Bowen Yu', 'Sandeep Soni', 'Jaime Teevan', 'Andrés Monroy-Hernández']|['cs.HC', 'cs.AI', 'cs.CL']
2017-03-28T14:05:20Z|2017-03-24T13:00:52Z|http://arxiv.org/abs/1703.08397v1|http://arxiv.org/pdf/1703.08397v1|Reasoning by Cases in Structured Argumentation|reason case structur argument|We extend the $ASPIC^+$ framework for structured argumentation so as to allow applications of the reasoning by cases inference scheme for defeasible arguments. Given an argument with conclusion `$A$ or $B$', an argument based on $A$ with conclusion $C$, and an argument based on $B$ with conclusion $C$, we allow the construction of an argument with conclusion $C$. We show how our framework leads to different results than other approaches in non-monotonic logic for dealing with disjunctive information, such as disjunctive default theory or approaches based on the OR-rule (which allows to derive a defeasible rule `If ($A$ or $B$) then $C$', given two defeasible rules `If $A$ then $C$' and `If $B$ then $C$'). We raise new questions regarding the subtleties of reasoning defeasibly with disjunctive information, and show that its formalization is more intricate than one would presume.|extend aspic framework structur argument allow applic reason case infer scheme defeas argument given argument conclus argument base conclus argument base conclus allow construct argument conclus show framework lead differ result approach non monoton logic deal disjunct inform disjunct default theori approach base rule allow deriv defeas rule given two defeas rule rais new question regard subtleti reason defeas disjunct inform show formal intric one would presum|['Mathieu Beirlaen', 'Jesse Heyninck', 'Christian Straßer']|['cs.AI', '68T27', 'I.2.3; I.2.4']
2017-03-28T14:05:20Z|2017-03-24T12:07:34Z|http://arxiv.org/abs/1703.08383v1|http://arxiv.org/pdf/1703.08383v1|Smart Augmentation - Learning an Optimal Data Augmentation Strategy|smart augment learn optim data augment strategi|A recurring problem faced when training neural networks is that there is typically not enough data to maximize the generalization capability of deep neural networks(DNN). There are many techniques to address this, including data augmentation, dropout, and transfer learning. In this paper, we introduce an additional method which we call Smart Augmentation and we show how to use it to increase the accuracy and reduce overfitting on a target network. Smart Augmentation works by creating a network that learns how to generate augmented data during the training process of a target network in a way that reduces that networks loss. This allows us to learn augmentations that minimize the error of that network.   Smart Augmentation has shown the potential to increase accuracy by demonstrably significant measures on all datasets tested. In addition, it has shown potential to achieve similar or improved performance levels with significantly smaller network sizes in a number of tested cases.|recur problem face train neural network typic enough data maxim general capabl deep neural network dnn mani techniqu address includ data augment dropout transfer learn paper introduc addit method call smart augment show use increas accuraci reduc overfit target network smart augment work creat network learn generat augment data dure train process target network way reduc network loss allow us learn augment minim error network smart augment shown potenti increas accuraci demonstr signific measur dataset test addit shown potenti achiev similar improv perform level signific smaller network size number test case|['Joseph Lemley', 'Shabab Bazrafkan', 'Peter Corcoran']|['cs.AI', 'cs.LG', 'stat.ML']
2017-03-28T14:05:20Z|2017-03-24T01:59:11Z|http://arxiv.org/abs/1703.08262v1|http://arxiv.org/pdf/1703.08262v1|Supervisor Synthesis of POMDP based on Automata Learning|supervisor synthesi pomdp base automata learn|As a general and thus popular model for autonomous systems, partially observable Markov decision process (POMDP) can capture uncertainties from different sources like sensing noises, actuation errors, and uncertain environments. However, its comprehensiveness makes the planning and control in POMDP difficult. Traditional POMDP planning problems target to find the optimal policy to maximize the expectation of accumulated rewards. But for safety critical applications, guarantees of system performance described by formal specifications are desired, which motivates us to consider formal methods to synthesize supervisor for POMDP. With system specifications given by Probabilistic Computation Tree Logic (PCTL), we propose a supervisory control framework with a type of deterministic finite automata (DFA), za-DFA, as the controller form. While the existing work mainly relies on optimization techniques to learn fixed-size finite state controllers (FSCs), we develop an $L^*$ learning based algorithm to determine both space and transitions of za-DFA. Membership queries and different oracles for conjectures are defined. The learning algorithm is sound and complete. An example is given in detailed steps to illustrate the supervisor synthesis algorithm.|general thus popular model autonom system partial observ markov decis process pomdp captur uncertainti differ sourc like sens nois actuat error uncertain environ howev comprehens make plan control pomdp difficult tradit pomdp plan problem target find optim polici maxim expect accumul reward safeti critic applic guarante system perform describ formal specif desir motiv us consid formal method synthes supervisor pomdp system specif given probabilist comput tree logic pctl propos supervisori control framework type determinist finit automata dfa za dfa control form exist work main reli optim techniqu learn fix size finit state control fscs develop learn base algorithm determin space transit za dfa membership queri differ oracl conjectur defin learn algorithm sound complet exampl given detail step illustr supervisor synthesi algorithm|['Xiaobin Zhang', 'Bo Wu', 'Hai Lin']|['cs.SY', 'cs.AI', 'cs.FL']
2017-03-28T14:05:20Z|2017-03-23T17:07:14Z|http://arxiv.org/abs/1703.08144v1|http://arxiv.org/pdf/1703.08144v1|Note Value Recognition for Rhythm Transcription Using a Markov Random   Field Model for Musical Scores and Performances of Piano Music|note valu recognit rhythm transcript use markov random field model music score perform piano music|This paper presents a statistical method for music transcription that can estimate score times of note onsets and offsets from polyphonic MIDI performance signals. Because performed note durations can deviate largely from score-indicated values, previous methods had the problem of not being able to accurately estimate offset score times (or note values) and thus could only output incomplete musical scores. Based on observations that the pitch context and onset score times are influential on the configuration of note values, we construct a context-tree model that provides prior distributions of note values using these features and combine it with a performance model in the framework of Markov random fields. Evaluation results showed that our method reduces the average error rate by around 40 percent compared to existing/simple methods. We also confirmed that, in our model, the score model plays a more important role than the performance model, and it automatically captures the voice structure by unsupervised learning.|paper present statist method music transcript estim score time note onset offset polyphon midi perform signal becaus perform note durat deviat larg score indic valu previous method problem abl accur estim offset score time note valu thus could onli output incomplet music score base observ pitch context onset score time influenti configur note valu construct context tree model provid prior distribut note valu use featur combin perform model framework markov random field evalu result show method reduc averag error rate around percent compar exist simpl method also confirm model score model play import role perform model automat captur voic structur unsupervis learn|['Eita Nakamura', 'Kazuyoshi Yoshii', 'Simon Dixon']|['cs.AI', 'cs.SD']
2017-03-28T14:05:20Z|2017-03-23T15:15:26Z|http://arxiv.org/abs/1703.08098v1|http://arxiv.org/pdf/1703.08098v1|An overview of embedding models of entities and relationships for   knowledge base completion|overview embed model entiti relationship knowledg base complet|Knowledge bases of real-world facts about entities and their relationships are useful resources for a variety of natural language processing tasks. However, because knowledge bases are typically incomplete, it is useful to be able to perform knowledge base completion, i.e., predict whether a relationship not in the knowledge base is likely to be true. This article presents an overview of embedding models of entities and relationships for knowledge base completion, with up-to-date experimental results on two standard evaluation tasks of link prediction (i.e. entity prediction) and triple classification.|knowledg base real world fact entiti relationship use resourc varieti natur languag process task howev becaus knowledg base typic incomplet use abl perform knowledg base complet predict whether relationship knowledg base like true articl present overview embed model entiti relationship knowledg base complet date experiment result two standard evalu task link predict entiti predict tripl classif|['Dat Quoc Nguyen']|['cs.CL', 'cs.AI', 'cs.IR']
2017-03-28T14:05:20Z|2017-03-23T12:32:10Z|http://arxiv.org/abs/1703.08041v1|http://arxiv.org/pdf/1703.08041v1|Resolving the Complexity of Some Fundamental Problems in Computational   Social Choice|resolv complex fundament problem comput social choic|This thesis is in the area called computational social choice which is an intersection area of algorithms and social choice theory.|thesi area call comput social choic intersect area algorithm social choic theori|['Palash Dey']|['cs.DS', 'cs.AI', 'cs.MA']
2017-03-28T14:05:20Z|2017-03-23T10:44:18Z|http://arxiv.org/abs/1703.07994v1|http://arxiv.org/pdf/1703.07994v1|Containment for Rule-Based Ontology-Mediated Queries|contain rule base ontolog mediat queri|Many efforts have been dedicated to identifying restrictions on ontologies expressed as tuple-generating dependencies (tgds), a.k.a. existential rules, that lead to the decidability for the problem of answering ontology-mediated queries (OMQs). This has given rise to three families of formalisms: guarded, non-recursive, and sticky sets of tgds. In this work, we study the containment problem for OMQs expressed in such formalisms, which is a key ingredient for solving static analysis tasks associated with them. Our main contribution is the development of specially tailored techniques for OMQ containment under the classes of tgds stated above. This enables us to obtain sharp complexity bounds for the problems at hand. We also apply our techniques to pinpoint the complexity of problems associated with two emerging applications of OMQ containment: distribution over components and UCQ rewritability of OMQs.|mani effort dedic identifi restrict ontolog express tupl generat depend tgds existenti rule lead decid problem answer ontolog mediat queri omq given rise three famili formal guard non recurs sticki set tgds work studi contain problem omq express formal key ingredi solv static analysi task associ main contribut develop special tailor techniqu omq contain class tgds state abov enabl us obtain sharp complex bound problem hand also appli techniqu pinpoint complex problem associ two emerg applic omq contain distribut compon ucq rewrit omq|['Pablo Barcelo', 'Gerald Berger', 'Andreas Pieris']|['cs.DB', 'cs.AI', 'cs.LO']
2017-03-28T14:05:20Z|2017-03-23T07:13:28Z|http://arxiv.org/abs/1703.07948v1|http://arxiv.org/pdf/1703.07948v1|Fast Stochastic Variance Reduced Gradient Method with Momentum   Acceleration for Machine Learning|fast stochast varianc reduc gradient method momentum acceler machin learn|Recently, research on accelerated stochastic gradient descent methods (e.g., SVRG) has made exciting progress (e.g., linear convergence for strongly convex problems). However, the best-known methods (e.g., Katyusha) requires at least two auxiliary variables and two momentum parameters. In this paper, we propose a fast stochastic variance reduction gradient (FSVRG) method, in which we design a novel update rule with the Nesterov's momentum and incorporate the technique of growing epoch size. FSVRG has only one auxiliary variable and one momentum weight, and thus it is much simpler and has much lower per-iteration complexity. We prove that FSVRG achieves linear convergence for strongly convex problems and the optimal $\mathcal{O}(1/T^2)$ convergence rate for non-strongly convex problems, where $T$ is the number of outer-iterations. We also extend FSVRG to directly solve the problems with non-smooth component functions, such as SVM. Finally, we empirically study the performance of FSVRG for solving various machine learning problems such as logistic regression, ridge regression, Lasso and SVM. Our results show that FSVRG outperforms the state-of-the-art stochastic methods, including Katyusha.|recent research acceler stochast gradient descent method svrg made excit progress linear converg strong convex problem howev best known method katyusha requir least two auxiliari variabl two momentum paramet paper propos fast stochast varianc reduct gradient fsvrg method design novel updat rule nesterov momentum incorpor techniqu grow epoch size fsvrg onli one auxiliari variabl one momentum weight thus much simpler much lower per iter complex prove fsvrg achiev linear converg strong convex problem optim mathcal converg rate non strong convex problem number outer iter also extend fsvrg direct solv problem non smooth compon function svm final empir studi perform fsvrg solv various machin learn problem logist regress ridg regress lasso svm result show fsvrg outperform state art stochast method includ katyusha|['Fanhua Shang', 'Yuanyuan Liu', 'James Cheng', 'Jiacheng Zhuo']|['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']
2017-03-28T14:05:20Z|2017-03-23T05:23:34Z|http://arxiv.org/abs/1703.07940v1|http://arxiv.org/pdf/1703.07940v1|Unsupervised Basis Function Adaptation for Reinforcement Learning|unsupervis basi function adapt reinforc learn|When using reinforcement learning (RL) algorithms to evaluate a policy it is common, given a large state space, to introduce some form of approximation architecture for the value function (VF). The exact form of this architecture can have a significant effect on the accuracy of the VF estimate, however, and determining a suitable approximation architecture can often be a highly complex task. Consequently there is a large amount of interest in the potential for allowing RL algorithms to adaptively generate (i.e. to learn) approximation architectures.   We investigate a method of adapting approximation architectures which uses feedback regarding the frequency with which an agent has visited certain states to guide which areas of the state space to approximate with greater detail. We introduce an algorithm based upon this idea which adapts a state aggregation approximation architecture on-line.   Assuming $S$ states, we demonstrate theoretically that - provided the following relatively non-restrictive assumptions are satisfied: (a) the number of cells $X$ in the state aggregation architecture is of order $\sqrt{S}\ln{S}\log_2{S}$ or greater, (b) the policy and transition function are close to deterministic, and (c) the prior for the transition function is uniformly distributed - our algorithm can guarantee, assuming we use an appropriate scoring function to measure VF error, error which is arbitrarily close to zero as $S$ becomes large. It is able to do this despite having only $O(X\log_2{S})$ space complexity (and negligible time complexity). We conclude by generating a set of empirical results which support the theoretical results.|use reinforc learn rl algorithm evalu polici common given larg state space introduc form approxim architectur valu function vf exact form architectur signific effect accuraci vf estim howev determin suitabl approxim architectur often high complex task consequ larg amount interest potenti allow rl algorithm adapt generat learn approxim architectur investig method adapt approxim architectur use feedback regard frequenc agent visit certain state guid area state space approxim greater detail introduc algorithm base upon idea adapt state aggreg approxim architectur line assum state demonstr theoret provid follow relat non restrict assumpt satisfi number cell state aggreg architectur order sqrt ln log greater polici transit function close determinist prior transit function uniform distribut algorithm guarante assum use appropri score function measur vf error error arbitrarili close zero becom larg abl despit onli log space complex neglig time complex conclud generat set empir result support theoret result|['Edward Barker', 'Charl Ras']|['cs.LG', 'cs.AI', 'stat.ML']
2017-03-28T14:05:24Z|2017-03-23T04:26:46Z|http://arxiv.org/abs/1703.07929v1|http://arxiv.org/pdf/1703.07929v1|Diversification-Based Learning in Computing and Optimization|diversif base learn comput optim|Diversification-Based Learning (DBL) derives from a collection of principles and methods introduced in the field of metaheuristics that have broad applications in computing and optimization. We show that the DBL framework goes significantly beyond that of the more recent Opposition-based learning (OBL) framework introduced in Tizhoosh (2005), which has become the focus of numerous research initiatives in machine learning and metaheuristic optimization. We unify and extend earlier proposals in metaheuristic search (Glover, 1997, Glover and Laguna, 1997) to give a collection of approaches that are more flexible and comprehensive than OBL for creating intensification and diversification strategies in metaheuristic search. We also describe potential applications of DBL to various subfields of machine learning and optimization.|diversif base learn dbl deriv collect principl method introduc field metaheurist broad applic comput optim show dbl framework goe signific beyond recent opposit base learn obl framework introduc tizhoosh becom focus numer research initi machin learn metaheurist optim unifi extend earlier propos metaheurist search glover glover laguna give collect approach flexibl comprehens obl creat intensif diversif strategi metaheurist search also describ potenti applic dbl various subfield machin learn optim|['Fred Glover', 'Jin-Kao Hao']|['cs.AI']
2017-03-28T14:05:24Z|2017-03-23T04:25:48Z|http://arxiv.org/abs/1703.07928v1|http://arxiv.org/pdf/1703.07928v1|Guided Perturbations: Self Corrective Behavior in Convolutional Neural   Networks|guid perturb self correct behavior convolut neural network|Convolutional Neural Networks have been a subject of great importance over the past decade and great strides have been made in their utility for producing state of the art performance in many computer vision problems. However, the behavior of deep networks is yet to be fully understood and is still an active area of research. In this work, we present an intriguing behavior: pre-trained CNNs can be made to improve their predictions by structurally perturbing the input. We observe that these perturbations - referred as Guided Perturbations - enable a trained network to improve its prediction performance without any learning or change in network weights. We perform various ablative experiments to understand how these perturbations affect the local context and feature representations. Furthermore, we demonstrate that this idea can improve performance of several existing approaches on semantic segmentation and scene labeling tasks on the PASCAL VOC dataset and supervised classification tasks on MNIST and CIFAR10 datasets.|convolut neural network subject great import past decad great stride made util produc state art perform mani comput vision problem howev behavior deep network yet fulli understood still activ area research work present intrigu behavior pre train cnns made improv predict structur perturb input observ perturb refer guid perturb enabl train network improv predict perform without ani learn chang network weight perform various ablat experi understand perturb affect local context featur represent furthermor demonstr idea improv perform sever exist approach semant segment scene label task pascal voc dataset supervis classif task mnist cifar dataset|['Swami Sankaranarayanan', 'Arpit Jain', 'Ser Nam Lim']|['cs.CV', 'cs.AI', 'stat.ML']
2017-03-28T14:05:24Z|2017-03-22T19:08:48Z|http://arxiv.org/abs/1703.07822v1|http://arxiv.org/pdf/1703.07822v1|Information-theoretic Model Identification and Policy Search using   Physics Engines with Application to Robotic Manipulation|inform theoret model identif polici search use physic engin applic robot manipul|We consider the problem of a robot learning the mechanical properties of objects through physical interaction with the object, and introduce a practical, data-efficient approach for identifying the motion models of these objects. The proposed method utilizes a physics engine, where the robot seeks to identify the inertial and friction parameters of the object by simulating its motion under different values of the parameters and identifying those that result in a simulation which matches the observed real motions. The problem is solved in a Bayesian optimization framework. The same framework is used for both identifying the model of an object online and searching for a policy that would minimize a given cost function according to the identified model. Experimental results both in simulation and using a real robot indicate that the proposed method outperforms state-of-the-art model-free reinforcement learning approaches.|consid problem robot learn mechan properti object physic interact object introduc practic data effici approach identifi motion model object propos method util physic engin robot seek identifi inerti friction paramet object simul motion differ valu paramet identifi result simul match observ real motion problem solv bayesian optim framework framework use identifi model object onlin search polici would minim given cost function accord identifi model experiment result simul use real robot indic propos method outperform state art model free reinforc learn approach|['Shaojun Zhu', 'Andrew Kimmel', 'Abdeslam Boularias']|['cs.RO', 'cs.AI', 'cs.LG']
2017-03-28T14:05:24Z|2017-03-22T18:20:07Z|http://arxiv.org/abs/1703.07805v1|http://arxiv.org/abs/1703.07805v1|Supervised Typing of Big Graphs using Semantic Embeddings|supervis type big graph use semant embed|We propose a supervised algorithm for generating type embeddings in the same semantic vector space as a given set of entity embeddings. The algorithm is agnostic to the derivation of the underlying entity embeddings. It does not require any manual feature engineering, generalizes well to hundreds of types and achieves near-linear scaling on Big Graphs containing many millions of triples and instances by virtue of an incremental execution. We demonstrate the utility of the embeddings on a type recommendation task, outperforming a non-parametric feature-agnostic baseline while achieving 15x speedup and near-constant memory usage on a full partition of DBpedia. Using state-of-the-art visualization, we illustrate the agreement of our extensionally derived DBpedia type embeddings with the manually curated domain ontology. Finally, we use the embeddings to probabilistically cluster about 4 million DBpedia instances into 415 types in the DBpedia ontology.|propos supervis algorithm generat type embed semant vector space given set entiti embed algorithm agnost deriv entiti embed doe requir ani manual featur engin general well hundr type achiev near linear scale big graph contain mani million tripl instanc virtu increment execut demonstr util embed type recommend task outperform non parametr featur agnost baselin achiev speedup near constant memori usag full partit dbpedia use state art visual illustr agreement extension deriv dbpedia type embed manual curat domain ontolog final use embed probabilist cluster million dbpedia instanc type dbpedia ontolog|['Mayank Kejriwal', 'Pedro Szekely']|['cs.CL', 'cs.AI']
2017-03-28T14:05:24Z|2017-03-22T17:27:57Z|http://arxiv.org/abs/1703.07758v1|http://arxiv.org/pdf/1703.07758v1|S-Concave Distributions: Towards Broader Distributions for   Noise-Tolerant and Sample-Efficient Learning Algorithms|concav distribut toward broader distribut nois toler sampl effici learn algorithm|We provide new results concerning noise-tolerant and sample-efficient learning algorithms under $s$-concave distributions over $\mathbb{R}^n$ for $-\frac{1}{2n+3}\le s\le 0$. The new class of $s$-concave distributions is a broad and natural generalization of log-concavity, and includes many important additional distributions, e.g., the Pareto distribution and $t$-distribution. This class has been studied in the context of efficient sampling, integration, and optimization, but much remains unknown concerning the geometry of this class of distributions and their applications in the context of learning.   The challenge is that unlike the commonly used distributions in learning (uniform or more generally log-concave distributions), this broader class is not closed under the marginalization operator and many such distributions are fat-tailed. In this work, we introduce new convex geometry tools to study the properties of s-concave distributions and use these properties to provide bounds on quantities of interest to learning including the probability of disagreement between two halfspaces, disagreement outside a band, and disagreement coefficient. We use these results to significantly generalize prior results for margin-based active learning, disagreement-based active learning, and passively learning of intersections of halfspaces.   Our analysis of geometric properties of s-concave distributions might be of independent interest to optimization more broadly.|provid new result concern nois toler sampl effici learn algorithm concav distribut mathbb frac le le new class concav distribut broad natur general log concav includ mani import addit distribut pareto distribut distribut class studi context effici sampl integr optim much remain unknown concern geometri class distribut applic context learn challeng unlik common use distribut learn uniform general log concav distribut broader class close margin oper mani distribut fat tail work introduc new convex geometri tool studi properti concav distribut use properti provid bound quantiti interest learn includ probabl disagr two halfspac disagr outsid band disagr coeffici use result signific general prior result margin base activ learn disagr base activ learn passiv learn intersect halfspac analysi geometr properti concav distribut might independ interest optim broad|['Maria-Florina Balcan', 'Hongyang Zhang']|['stat.ML', 'cs.AI', 'cs.LG']
2017-03-28T14:05:24Z|2017-03-24T01:00:03Z|http://arxiv.org/abs/1703.07726v3|http://arxiv.org/pdf/1703.07726v3|\$1 Today or \$2 Tomorrow? The Answer is in Your Facebook Likes|today tomorrow answer facebook like|"In economics and psychology, delay discounting is often used to characterize how individuals choose between a smaller immediate reward and a larger delayed reward. People with higher delay discounting rate (DDR) often choose smaller but more immediate rewards (a ""today person""). In contrast, people with a lower discounting rate often choose a larger future rewards (a ""tomorrow person""). Since the ability to modulate the desire of immediate gratification for long term rewards plays an important role in our decision-making, the lower discounting rate often predicts better social, academic and health outcomes. In contrast, the higher discounting rate is often associated with problematic behaviors such as alcohol/drug abuse, pathological gambling and credit card default. Thus, research on understanding and moderating delay discounting has the potential to produce substantial societal benefits."|econom psycholog delay discount often use character individu choos smaller immedi reward larger delay reward peopl higher delay discount rate ddr often choos smaller immedi reward today person contrast peopl lower discount rate often choos larger futur reward tomorrow person sinc abil modul desir immedi gratif long term reward play import role decis make lower discount rate often predict better social academ health outcom contrast higher discount rate often associ problemat behavior alcohol drug abus patholog gambl credit card default thus research understand moder delay discount potenti produc substanti societ benefit|['Tao Ding', 'Warren K. Bickel', 'Shimei Pan']|['cs.AI', 'cs.CY', 'cs.SI']
2017-03-28T14:05:24Z|2017-03-22T15:34:23Z|http://arxiv.org/abs/1703.07710v1|http://arxiv.org/pdf/1703.07710v1|UBEV - A More Practical Algorithm for Episodic RL with Near-Optimal PAC   and Regret Guarantees|ubev practic algorithm episod rl near optim pac regret guarante|We present UBEV, a simple and efficient reinforcement learning algorithm for fixed-horizon episodic Markov decision processes. The main contribution is a proof that UBEV enjoys a sample-complexity bound that holds for all accuracy levels simultaneously with high probability, and matches the lower bound except for logarithmic terms and one factor of the horizon. A consequence of the fact that our sample-complexity bound holds for all accuracy levels is that the new algorithm achieves a sub-linear regret of O(sqrt(SAT)), which is the first time the dependence on the size of the state space has provably appeared inside the square root. A brief empirical evaluation shows that UBEV is practically superior to existing algorithms with known sample-complexity guarantees.|present ubev simpl effici reinforc learn algorithm fix horizon episod markov decis process main contribut proof ubev enjoy sampl complex bound hold accuraci level simultan high probabl match lower bound except logarithm term one factor horizon consequ fact sampl complex bound hold accuraci level new algorithm achiev sub linear regret sqrt sat first time depend size state space provabl appear insid squar root brief empir evalu show ubev practic superior exist algorithm known sampl complex guarante|['Christoph Dann', 'Tor Lattimore', 'Emma Brunskill']|['cs.LG', 'cs.AI', 'stat.ML']
2017-03-28T14:05:24Z|2017-03-22T11:53:53Z|http://arxiv.org/abs/1703.07608v1|http://arxiv.org/pdf/1703.07608v1|Deep Exploration via Randomized Value Functions|deep explor via random valu function|We study the use of randomized value functions to guide deep exploration in reinforcement learning. This offers an elegant means for synthesizing statistically and computationally efficient exploration with common practical approaches to value function learning. We present several reinforcement learning algorithms that leverage randomized value functions and demonstrate their efficacy through computational studies. We also prove a regret bound that establishes statistical efficiency with a tabular representation.|studi use random valu function guid deep explor reinforc learn offer eleg mean synthes statist comput effici explor common practic approach valu function learn present sever reinforc learn algorithm leverag random valu function demonstr efficaci comput studi also prove regret bound establish statist effici tabular represent|['Ian Osband', 'Daniel Russo', 'Zheng Wen', 'Benjamin Van Roy']|['stat.ML', 'cs.AI', 'cs.LG']
2017-03-28T14:05:24Z|2017-03-21T23:29:47Z|http://arxiv.org/abs/1703.07469v1|http://arxiv.org/pdf/1703.07469v1|RobustFill: Neural Program Learning under Noisy I/O|robustfil neural program learn noisi|The problem of automatically generating a computer program from some specification has been studied since the early days of AI. Recently, two competing approaches for automatic program learning have received significant attention: (1) neural program synthesis, where a neural network is conditioned on input/output (I/O) examples and learns to generate a program, and (2) neural program induction, where a neural network generates new outputs directly using a latent program representation.   Here, for the first time, we directly compare both approaches on a large-scale, real-world learning task. We additionally contrast to rule-based program synthesis, which uses hand-crafted semantics to guide the program generation. Our neural models use a modified attention RNN to allow encoding of variable-sized sets of I/O pairs. Our best synthesis model achieves 92% accuracy on a real-world test set, compared to the 34% accuracy of the previous best neural synthesis approach. The synthesis model also outperforms a comparable induction model on this task, but we more importantly demonstrate that the strength of each approach is highly dependent on the evaluation metric and end-user application. Finally, we show that we can train our neural models to remain very robust to the type of noise expected in real-world data (e.g., typos), while a highly-engineered rule-based system fails entirely.|problem automat generat comput program specif studi sinc earli day ai recent two compet approach automat program learn receiv signific attent neural program synthesi neural network condit input output exampl learn generat program neural program induct neural network generat new output direct use latent program represent first time direct compar approach larg scale real world learn task addit contrast rule base program synthesi use hand craft semant guid program generat neural model use modifi attent rnn allow encod variabl size set pair best synthesi model achiev accuraci real world test set compar accuraci previous best neural synthesi approach synthesi model also outperform compar induct model task import demonstr strength approach high depend evalu metric end user applic final show train neural model remain veri robust type nois expect real world data typo high engin rule base system fail entir|['Jacob Devlin', 'Jonathan Uesato', 'Surya Bhupatiraju', 'Rishabh Singh', 'Abdel-rahman Mohamed', 'Pushmeet Kohli']|['cs.AI']
2017-03-28T14:05:24Z|2017-03-21T19:12:35Z|http://arxiv.org/abs/1703.07394v1|http://arxiv.org/pdf/1703.07394v1|Deep Learning for Explicitly Modeling Optimization Landscapes|deep learn explicit model optim landscap|In all but the most trivial optimization problems, the structure of the solutions exhibit complex interdependencies between the input parameters. Decades of research with stochastic search techniques has shown the benefit of explicitly modeling the interactions between sets of parameters and the overall quality of the solutions discovered. We demonstrate a novel method, based on learning deep networks, to model the global landscapes of optimization problems. To represent the search space concisely and accurately, the deep networks must encode information about the underlying parameter interactions and their contributions to the quality of the solution. Once the networks are trained, the networks are probed to reveal parameter combinations with high expected performance with respect to the optimization task. These estimates are used to initialize fast, randomized, local search algorithms, which in turn expose more information about the search space that is subsequently used to refine the models. We demonstrate the technique on multiple optimization problems that have arisen in a variety of real-world domains, including: packing, graphics, job scheduling, layout and compression. The problems include combinatoric search spaces, discontinuous and highly non-linear spaces, and span binary, higher-cardinality discrete, as well as continuous parameters. Strengths, limitations, and extensions of the approach are extensively discussed and demonstrated.|trivial optim problem structur solut exhibit complex interdepend input paramet decad research stochast search techniqu shown benefit explicit model interact set paramet overal qualiti solut discov demonstr novel method base learn deep network model global landscap optim problem repres search space concis accur deep network must encod inform paramet interact contribut qualiti solut onc network train network probe reveal paramet combin high expect perform respect optim task estim use initi fast random local search algorithm turn expos inform search space subsequ use refin model demonstr techniqu multipl optim problem arisen varieti real world domain includ pack graphic job schedul layout compress problem includ combinator search space discontinu high non linear space span binari higher cardin discret well continu paramet strength limit extens approach extens discuss demonstr|['Shumeet Baluja']|['cs.NE', 'cs.AI', 'cs.LG']
2017-03-28T14:05:28Z|2017-03-21T18:34:34Z|http://arxiv.org/abs/1703.07384v1|http://arxiv.org/pdf/1703.07384v1|Ontology Based Pivoted normalization using Vector Based Approach for   information Retrieval|ontolog base pivot normal use vector base approach inform retriev|The proposed methodology is procedural i.e. it follows finite number of steps that extracts relevant documents according to users query. It is based on principles of Data Mining for analyzing web data. Data Mining first adapts integration of data to generate warehouse. Then, it extracts useful information with the help of algorithm. The task of representing extracted documents is done by using Vector Based Statistical Approach that represents each document in set of Terms.|propos methodolog procedur follow finit number step extract relev document accord user queri base principl data mine analyz web data data mine first adapt integr data generat warehous extract use inform help algorithm task repres extract document done use vector base statist approach repres document set term|['Vishal Jain', 'Dr. Mayank Singh']|['cs.IR', 'cs.AI']
2017-03-28T14:05:28Z|2017-03-21T18:29:05Z|http://arxiv.org/abs/1703.07381v1|http://arxiv.org/pdf/1703.07381v1|Improving Statistical Multimedia Information Retrieval Model by using   Ontology|improv statist multimedia inform retriev model use ontolog|A typical IR system that delivers and stores information is affected by problem of matching between user query and available content on web. Use of Ontology represents the extracted terms in form of network graph consisting of nodes, edges, index terms etc. The above mentioned IR approaches provide relevance thus satisfying users query. The paper also emphasis on analyzing multimedia documents and performs calculation for extracted terms using different statistical formulas. The proposed model developed reduces semantic gap and satisfies user needs efficiently.|typic ir system deliv store inform affect problem match user queri avail content web use ontolog repres extract term form network graph consist node edg index term etc abov mention ir approach provid relev thus satisfi user queri paper also emphasi analyz multimedia document perform calcul extract term use differ statist formula propos model develop reduc semant gap satisfi user need effici|['Gagandeep Singh Narula', 'Vishal Jain']|['cs.IR', 'cs.AI']
2017-03-28T14:05:28Z|2017-03-22T00:24:03Z|http://arxiv.org/abs/1703.07326v2|http://arxiv.org/pdf/1703.07326v2|One-Shot Imitation Learning|one shot imit learn|Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning.   Specifically, we consider the setting where there is a very large set of tasks, and each task has many instantiations. For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states. At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks. A neural net is trained that takes as input one demonstration and the current state (which initially is the initial state of the other demonstration of the pair), and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. At test time, a demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. The use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks.   Videos available at https://bit.ly/one-shot-imitation .|imit learn common appli solv differ task isol usual requir either care featur engin signific number sampl far desir ideal robot abl learn veri demonstr ani given task instant general new situat task without requir task specif engin paper propos meta learn framework achiev capabl call one shot imit learn specif consid set veri larg set task task mani instanti exampl task could stack block tabl singl tower anoth task could place block tabl two block tower etc case differ instanc task would consist differ set block differ initi state train time algorithm present pair demonstr subset task neural net train take input one demonstr current state initi initi state demonstr pair output action goal result sequenc state action match close possibl second demonstr test time demonstr singl instanc new task present neural net expect perform well new instanc new task use soft attent allow model general condit task unseen train data anticip train model much greater varieti task set obtain general system turn ani demonstr robust polici accomplish overwhelm varieti task video avail https bit ly one shot imit|['Yan Duan', 'Marcin Andrychowicz', 'Bradly C. Stadie', 'Jonathan Ho', 'Jonas Schneider', 'Ilya Sutskever', 'Pieter Abbeel', 'Wojciech Zaremba']|['cs.AI', 'cs.LG', 'cs.NE', 'cs.RO']
2017-03-28T14:05:28Z|2017-03-22T17:08:40Z|http://arxiv.org/abs/1703.07255v2|http://arxiv.org/pdf/1703.07255v2|ZM-Net: Real-time Zero-shot Image Manipulation Network|zm net real time zero shot imag manipul network|Many problems in image processing and computer vision (e.g. colorization, style transfer) can be posed as 'manipulating' an input image into a corresponding output image given a user-specified guiding signal. A holy-grail solution towards generic image manipulation should be able to efficiently alter an input image with any personalized signals (even signals unseen during training), such as diverse paintings and arbitrary descriptive attributes. However, existing methods are either inefficient to simultaneously process multiple signals (let alone generalize to unseen signals), or unable to handle signals from other modalities. In this paper, we make the first attempt to address the zero-shot image manipulation task. We cast this problem as manipulating an input image according to a parametric model whose key parameters can be conditionally generated from any guiding signal (even unseen ones). To this end, we propose the Zero-shot Manipulation Net (ZM-Net), a fully-differentiable architecture that jointly optimizes an image-transformation network (TNet) and a parameter network (PNet). The PNet learns to generate key transformation parameters for the TNet given any guiding signal while the TNet performs fast zero-shot image manipulation according to both signal-dependent parameters from the PNet and signal-invariant parameters from the TNet itself. Extensive experiments show that our ZM-Net can perform high-quality image manipulation conditioned on different forms of guiding signals (e.g. style images and attributes) in real-time (tens of milliseconds per image) even for unseen signals. Moreover, a large-scale style dataset with over 20,000 style images is also constructed to promote further research.|mani problem imag process comput vision color style transfer pose manipul input imag correspond output imag given user specifi guid signal holi grail solut toward generic imag manipul abl effici alter input imag ani person signal even signal unseen dure train divers paint arbitrari descript attribut howev exist method either ineffici simultan process multipl signal let alon general unseen signal unabl handl signal modal paper make first attempt address zero shot imag manipul task cast problem manipul input imag accord parametr model whose key paramet condit generat ani guid signal even unseen one end propos zero shot manipul net zm net fulli differenti architectur joint optim imag transform network tnet paramet network pnet pnet learn generat key transform paramet tnet given ani guid signal tnet perform fast zero shot imag manipul accord signal depend paramet pnet signal invari paramet tnet extens experi show zm net perform high qualiti imag manipul condit differ form guid signal style imag attribut real time ten millisecond per imag even unseen signal moreov larg scale style dataset style imag also construct promot research|['Hao Wang', 'Xiaodan Liang', 'Hao Zhang', 'Dit-Yan Yeung', 'Eric P. Xing']|['cs.CV', 'cs.AI', 'cs.GR', 'cs.LG', 'stat.ML']
2017-03-28T14:05:28Z|2017-03-21T09:57:59Z|http://arxiv.org/abs/1703.07116v1|http://arxiv.org/pdf/1703.07116v1|Interest-Driven Discovery of Local Process Models|interest driven discoveri local process model|Local Process Models (LPM) describe structured fragments of process behavior occurring in the context of less structured business processes. Traditional LPM discovery aims to generate a collection of process models that describe highly frequent behavior, but these models do not always provide useful answers for questions posed by process analysts aiming at business process improvement. We propose a framework for goal-driven LPM discovery, based on utility functions and constraints. We describe four scopes on which these utility functions and constrains can be defined, and show that utility functions and constraints on different scopes can be combined to form composite utility functions/constraints. Finally, we demonstrate the applicability of our approach by presenting several actionable business insights discovered with LPM discovery on two real life data sets.|local process model lpm describ structur fragment process behavior occur context less structur busi process tradit lpm discoveri aim generat collect process model describ high frequent behavior model alway provid use answer question pose process analyst aim busi process improv propos framework goal driven lpm discoveri base util function constraint describ four scope util function constrain defin show util function constraint differ scope combin form composit util function constraint final demonstr applic approach present sever action busi insight discov lpm discoveri two real life data set|['Niek Tax', 'Benjamin Dalmas', 'Natalia Sidorova', 'Wil M P van der Aalst', 'Sylvie Norre']|['cs.DB', 'cs.AI']
2017-03-28T14:05:28Z|2017-03-21T07:09:27Z|http://arxiv.org/abs/1703.07075v1|http://arxiv.org/pdf/1703.07075v1|Pseudorehearsal in value function approximation|pseudorehears valu function approxim|Catastrophic forgetting is of special importance in reinforcement learning, as the data distribution is generally non-stationary over time. We study and compare several pseudorehearsal approaches for Q-learning with function approximation in a pole balancing task. We have found that pseudorehearsal seems to assist learning even in such very simple problems, given proper initialization of the rehearsal parameters.|catastroph forget special import reinforc learn data distribut general non stationari time studi compar sever pseudorehears approach learn function approxim pole balanc task found pseudorehears seem assist learn even veri simpl problem given proper initi rehears paramet|['Vladimir Marochko', 'Leonard Johard', 'Manuel Mazzara']|['cs.AI']
2017-03-28T14:05:28Z|2017-03-21T04:56:14Z|http://arxiv.org/abs/1703.07055v1|http://arxiv.org/pdf/1703.07055v1|Investigation of Language Understanding Impact for Reinforcement   Learning Based Dialogue Systems|investig languag understand impact reinforc learn base dialogu system|Language understanding is a key component in a spoken dialogue system. In this paper, we investigate how the language understanding module influences the dialogue system performance by conducting a series of systematic experiments on a task-oriented neural dialogue system in a reinforcement learning based setting. The empirical study shows that among different types of language understanding errors, slot-level errors can have more impact on the overall performance of a dialogue system compared to intent-level errors. In addition, our experiments demonstrate that the reinforcement learning based dialogue system is able to learn when and what to confirm in order to achieve better performance and greater robustness.|languag understand key compon spoken dialogu system paper investig languag understand modul influenc dialogu system perform conduct seri systemat experi task orient neural dialogu system reinforc learn base set empir studi show among differ type languag understand error slot level error impact overal perform dialogu system compar intent level error addit experi demonstr reinforc learn base dialogu system abl learn confirm order achiev better perform greater robust|['Xiujun Li', 'Yun-Nung Chen', 'Lihong Li', 'Jianfeng Gao', 'Asli Celikyilmaz']|['cs.CL', 'cs.AI', 'cs.LG']
2017-03-28T14:05:28Z|2017-03-23T20:06:15Z|http://arxiv.org/abs/1703.07022v2|http://arxiv.org/pdf/1703.07022v2|Recurrent Topic-Transition GAN for Visual Paragraph Generation|recurr topic transit gan visual paragraph generat|A natural image usually conveys rich semantic content and can be viewed from different angles. Existing image description methods are largely restricted by small sets of biased visual paragraph annotations, and fail to cover rich underlying semantics. In this paper, we investigate a semi-supervised paragraph generative framework that is able to synthesize diverse and semantically coherent paragraph descriptions by reasoning over local semantic regions and exploiting linguistic knowledge. The proposed Recurrent Topic-Transition Generative Adversarial Network (RTT-GAN) builds an adversarial framework between a structured paragraph generator and multi-level paragraph discriminators. The paragraph generator generates sentences recurrently by incorporating region-based visual and language attention mechanisms at each step. The quality of generated paragraph sentences is assessed by multi-level adversarial discriminators from two aspects, namely, plausibility at sentence level and topic-transition coherence at paragraph level. The joint adversarial training of RTT-GAN drives the model to generate realistic paragraphs with smooth logical transition between sentence topics. Extensive quantitative experiments on image and video paragraph datasets demonstrate the effectiveness of our RTT-GAN in both supervised and semi-supervised settings. Qualitative results on telling diverse stories for an image also verify the interpretability of RTT-GAN.|natur imag usual convey rich semant content view differ angl exist imag descript method larg restrict small set bias visual paragraph annot fail cover rich semant paper investig semi supervis paragraph generat framework abl synthes divers semant coher paragraph descript reason local semant region exploit linguist knowledg propos recurr topic transit generat adversari network rtt gan build adversari framework structur paragraph generat multi level paragraph discrimin paragraph generat generat sentenc recurr incorpor region base visual languag attent mechan step qualiti generat paragraph sentenc assess multi level adversari discrimin two aspect name plausibl sentenc level topic transit coher paragraph level joint adversari train rtt gan drive model generat realist paragraph smooth logic transit sentenc topic extens quantit experi imag video paragraph dataset demonstr effect rtt gan supervis semi supervis set qualit result tell divers stori imag also verifi interpret rtt gan|['Xiaodan Liang', 'Zhiting Hu', 'Hao Zhang', 'Chuang Gan', 'Eric P. Xing']|['cs.CV', 'cs.AI', 'cs.LG']
2017-03-28T14:05:28Z|2017-03-20T19:32:40Z|http://arxiv.org/abs/1703.06939v1|http://arxiv.org/pdf/1703.06939v1|Distributed Constraint Problems for Utilitarian Agents with Privacy   Concerns, Recast as POMDPs|distribut constraint problem utilitarian agent privaci concern recast pomdp|Privacy has traditionally been a major motivation for distributed problem solving. Distributed Constraint Satisfaction Problem (DisCSP) as well as Distributed Constraint Optimization Problem (DCOP) are fundamental models used to solve various families of distributed problems. Even though several approaches have been proposed to quantify and preserve privacy in such problems, none of them is exempt from limitations. Here we approach the problem by assuming that computation is performed among utilitarian agents. We introduce a utilitarian approach where the utility of each state is estimated as the difference between the reward for reaching an agreement on assignments of shared variables and the cost of privacy loss. We investigate extensions to solvers where agents integrate the utility function to guide their search and decide which action to perform, defining thereby their policy. We show that these extended solvers succeed in significantly reducing privacy loss without significant degradation of the solution quality.|privaci tradit major motiv distribut problem solv distribut constraint satisfact problem discsp well distribut constraint optim problem dcop fundament model use solv various famili distribut problem even though sever approach propos quantifi preserv privaci problem none exempt limit approach problem assum comput perform among utilitarian agent introduc utilitarian approach util state estim differ reward reach agreement assign share variabl cost privaci loss investig extens solver agent integr util function guid search decid action perform defin therebi polici show extend solver succeed signific reduc privaci loss without signific degrad solut qualiti|['Julien Savaux', 'Julien Vion', 'Sylvain Piechowiak', 'René Mandiau', 'Toshihiro Matsui', 'Katsutoshi Hirayama', 'Makoto Yokoo', 'Shakre Elmane', 'Marius Silaghi']|['cs.AI']
2017-03-28T14:05:28Z|2017-03-20T19:17:14Z|http://arxiv.org/abs/1703.06931v1|http://arxiv.org/pdf/1703.06931v1|Learning Correspondence Structures for Person Re-identification|learn correspond structur person identif|This paper addresses the problem of handling spatial misalignments due to camera-view changes or human-pose variations in person re-identification. We first introduce a boosting-based approach to learn a correspondence structure which indicates the patch-wise matching probabilities between images from a target camera pair. The learned correspondence structure can not only capture the spatial correspondence pattern between cameras but also handle the viewpoint or human-pose variation in individual images. We further introduce a global constraint-based matching process. It integrates a global matching constraint over the learned correspondence structure to exclude cross-view misalignments during the image patch matching process, hence achieving a more reliable matching score between images. Finally, we also extend our approach by introducing a multi-structure scheme, which learns a set of local correspondence structures to capture the spatial correspondence sub-patterns between a camera pair, so as to handle the spatial misalignments between individual images in a more precise way. Experimental results on various datasets demonstrate the effectiveness of our approach.|paper address problem handl spatial misalign due camera view chang human pose variat person identif first introduc boost base approach learn correspond structur indic patch wise match probabl imag target camera pair learn correspond structur onli captur spatial correspond pattern camera also handl viewpoint human pose variat individu imag introduc global constraint base match process integr global match constraint learn correspond structur exclud cross view misalign dure imag patch match process henc achiev reliabl match score imag final also extend approach introduc multi structur scheme learn set local correspond structur captur spatial correspond sub pattern camera pair handl spatial misalign individu imag precis way experiment result various dataset demonstr effect approach|['Weiyao Lin', 'Yang Shen', 'Junchi Yan', 'Mingliang Xu', 'Jianxin Wu', 'Jingdong Wang', 'Ke Lu']|['cs.CV', 'cs.AI', 'cs.MM']
2017-03-28T14:05:32Z|2017-03-20T16:03:36Z|http://arxiv.org/abs/1703.06815v1|http://arxiv.org/pdf/1703.06815v1|Foundations for a Probabilistic Event Calculus|foundat probabilist event calculus|We present PEC, an Event Calculus (EC) style action language for reasoning about probabilistic causal and narrative information. It has an action language style syntax similar to that of the EC variant Modular-E. Its semantics is given in terms of possible worlds which constitute possible evolutions of the domain, and builds on that of EFEC, an epistemic extension of EC. We also describe an ASP implementation of PEC and show the sense in which this is sound and complete.|present pec event calculus ec style action languag reason probabilist causal narrat inform action languag style syntax similar ec variant modular semant given term possibl world constitut possibl evolut domain build efec epistem extens ec also describ asp implement pec show sens sound complet|"[""Fabio Aurelio D'Asaro"", 'Antonis Bikakis', 'Luke Dickens', 'Rob Miller']"|['cs.AI']
2017-03-28T14:05:32Z|2017-03-20T11:44:00Z|http://arxiv.org/abs/1703.06692v1|http://arxiv.org/pdf/1703.06692v1|QMDP-Net: Deep Learning for Planning under Partial Observability|qmdp net deep learn plan partial observ|This paper introduces QMDP-net, a neural network architecture for planning under partial observability. The QMDP-net combines the strengths of model-free learning and model-based planning. It is a recurrent policy network, but it represents a policy by connecting a model with a planning algorithm that solves the model, thus embedding the solution structure of planning in the network architecture. The QMDP-net is fully differentiable and allows end-to-end training. We train a QMDP-net over a set of different environments so that it can generalize over new ones. In preliminary experiments, QMDP-net showed strong performance on several robotic tasks in simulation. Interestingly, it also sometimes outperformed the QMDP algorithm, which generated the data for learning, because of QMDP-net's robustness resulting from end-to-end learning.|paper introduc qmdp net neural network architectur plan partial observ qmdp net combin strength model free learn model base plan recurr polici network repres polici connect model plan algorithm solv model thus embed solut structur plan network architectur qmdp net fulli differenti allow end end train train qmdp net set differ environ general new one preliminari experi qmdp net show strong perform sever robot task simul interest also sometim outperform qmdp algorithm generat data learn becaus qmdp net robust result end end learn|['Peter Karkus', 'David Hsu', 'Wee Sun Lee']|['cs.AI', 'cs.LG', 'cs.NE', 'stat.ML']
2017-03-28T14:05:32Z|2017-03-20T09:28:38Z|http://arxiv.org/abs/1703.06642v1|http://arxiv.org/pdf/1703.06642v1|Towards a Quantum World Wide Web|toward quantum world wide web|We elaborate a quantum model for corpora of written documents, like the pages forming the World Wide Web. To that end, we are guided by how physicists constructed quantum theory for microscopic entities, which unlike classical objects cannot be fully represented in our spatial theater. We suggest that a similar construction needs to be carried out by linguists and computational scientists, to capture the full meaning content of collections of documental entities. More precisely, we show how to associate a quantum-like 'entity of meaning' to a 'language entity formed by printed documents', considering the latter as the collection of traces that are left by the former, in specific results of search actions that we describe as measurements. In other words, we offer a perspective where a collection of documents, like the Web, is described as the space of manifestation of a more complex entity - the QWeb - which is the object of our modeling, drawing its inspiration from previous studies on operational-realistic approaches to quantum physics and quantum modeling of human cognition and decision-making. We emphasize that a consistent QWeb model needs to account for the observed correlations between words appearing in printed documents, e.g., co-occurrences, as the latter would depend on the 'meaning connections' existing between the concepts that are associated with these words. In that respect, we show that both 'context and interference (quantum) effects' are required to explain the probabilities calculated by counting the relative number of documents containing certain words and co-ocurrrences of words.|elabor quantum model corpora written document like page form world wide web end guid physicist construct quantum theori microscop entiti unlik classic object cannot fulli repres spatial theater suggest similar construct need carri linguist comput scientist captur full mean content collect document entiti precis show associ quantum like entiti mean languag entiti form print document consid latter collect trace left former specif result search action describ measur word offer perspect collect document like web describ space manifest complex entiti qweb object model draw inspir previous studi oper realist approach quantum physic quantum model human cognit decis make emphas consist qweb model need account observ correl word appear print document co occurr latter would depend mean connect exist concept associ word respect show context interfer quantum effect requir explain probabl calcul count relat number document contain certain word co ocurrr word|['Diederik Aerts', 'Jonito Aerts Arguelles', 'Lester Beltran', 'Lyneth Beltran', 'Isaac Distrito', 'Massimiliano Sassoli de Bianchi', 'Sandro Sozzo', 'Tomas Veloz']|['cs.AI', 'cs.CL', 'quant-ph']
2017-03-28T14:05:32Z|2017-03-20T04:47:14Z|http://arxiv.org/abs/1703.06597v1|http://arxiv.org/pdf/1703.06597v1|Artificial Intelligence and Economic Theories|artifici intellig econom theori|The advent of artificial intelligence has changed many disciplines such as engineering, social science and economics. Artificial intelligence is a computational technique which is inspired by natural intelligence such as the swarming of birds, the working of the brain and the pathfinding of the ants. These techniques have impact on economic theories. This book studies the impact of artificial intelligence on economic theories, a subject that has not been extensively studied. The theories that are considered are: demand and supply, asymmetrical information, pricing, rational choice, rational expectation, game theory, efficient market hypotheses, mechanism design, prospect, bounded rationality, portfolio theory, rational counterfactual and causality. The benefit of this book is that it evaluates existing theories of economics and update them based on the developments in artificial intelligence field.|advent artifici intellig chang mani disciplin engin social scienc econom artifici intellig comput techniqu inspir natur intellig swarm bird work brain pathfind ant techniqu impact econom theori book studi impact artifici intellig econom theori subject extens studi theori consid demand suppli asymmetr inform price ration choic ration expect game theori effici market hypothes mechan design prospect bound ration portfolio theori ration counterfactu causal benefit book evalu exist theori econom updat base develop artifici intellig field|['Tshilidzi Marwala', 'Evan Hurwitz']|['cs.AI']
2017-03-28T14:05:32Z|2017-03-21T17:41:23Z|http://arxiv.org/abs/1703.06585v2|http://arxiv.org/pdf/1703.06585v2|Learning Cooperative Visual Dialog Agents with Deep Reinforcement   Learning|learn cooper visual dialog agent deep reinforc learn|We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative 'image guessing' game between two agents -- Qbot and Abot -- who communicate in natural language dialog so that Qbot can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end -- from pixels to multi-agent multi-round dialog to game reward.   We demonstrate two experimental results.   First, as a 'sanity check' demonstration of pure RL (from scratch), we show results on a synthetic world, where the agents communicate in ungrounded vocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes (shape/color/style). Thus, we demonstrate the emergence of grounded language and communication among 'visual' dialog agents with no human supervision.   Second, we conduct large-scale real-image experiments on the VisDial dataset, where we pretrain with supervised dialog data and show that the RL 'fine-tuned' agents significantly outperform SL agents. Interestingly, the RL Qbot learns to ask questions that Abot is good at, ultimately resulting in more informative dialog and a better team.|introduc first goal driven train visual question answer dialog agent specif pose cooper imag guess game two agent qbot abot communic natur languag dialog qbot select unseen imag lineup imag use deep reinforc learn rl learn polici agent end end pixel multi agent multi round dialog game reward demonstr two experiment result first saniti check demonstr pure rl scratch show result synthet world agent communic unground vocabulari symbol pre specifi mean find two bot invent communic protocol start use certain symbol ask answer certain visual attribut shape color style thus demonstr emerg ground languag communic among visual dialog agent human supervis second conduct larg scale real imag experi visdial dataset pretrain supervis dialog data show rl fine tune agent signific outperform sl agent interest rl qbot learn ask question abot good ultim result inform dialog better team|['Abhishek Das', 'Satwik Kottur', 'José M. F. Moura', 'Stefan Lee', 'Dhruv Batra']|['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG']
2017-03-28T14:05:32Z|2017-03-20T02:29:53Z|http://arxiv.org/abs/1703.06565v1|http://arxiv.org/pdf/1703.06565v1|Evidence Updating for Stream-Processing in Big-Data: Robust Conditioning   in Soft and Hard Fusion Environments|evid updat stream process big data robust condit soft hard fusion environ|Conditioning is the primary method for belief revision in data fusion systems employing probabilistic inferencing. However, big-data environments, where soft (i.e., human or human-based) sources are commonly utilized in addition to hard (i.e., physics-based sensors, pose several challenges to traditional conditioning tasks primarily due to the numerous data/source imperfections that are characteristic of such data. The objective of this paper is to investigate the most natural extension of Bayes conditioning based evidence updates in the presence of such large-scale data uncertainties and source/sensor imperfections. By viewing the evidence updating process as a thought experiment, we devise an elegant strategy for robust evidence updating in the presence of extreme uncertainties characteristic of big-data environments. In particular, we look at the formulation of a belief theoretic evidence updating mechanism that is derived as a natural extension of Bayes conditional approach when the incoming evidence takes the form of a general belief function. Proposed method generalizes the belief theoretic Fagin-Halpern conditional notion, and provides a novel evidence updating strategy that is derived as a natural extension of Bayes conditional applied in a highly uncertain and complex fusion scenario that is characteristic of big-data environments. The presented extension differs fundamentally from the previously published work on Conditional Update Equation (CUE) as well as authors own work. An overview of this development is provided via illustrative examples. Furthermore, insights into parameter selection under various fusion contexts are also provided.|condit primari method belief revis data fusion system employ probabilist inferenc howev big data environ soft human human base sourc common util addit hard physic base sensor pose sever challeng tradit condit task primarili due numer data sourc imperfect characterist data object paper investig natur extens bay condit base evid updat presenc larg scale data uncertainti sourc sensor imperfect view evid updat process thought experi devis eleg strategi robust evid updat presenc extrem uncertainti characterist big data environ particular look formul belief theoret evid updat mechan deriv natur extens bay condit approach incom evid take form general belief function propos method general belief theoret fagin halpern condit notion provid novel evid updat strategi deriv natur extens bay condit appli high uncertain complex fusion scenario characterist big data environ present extens differ fundament previous publish work condit updat equat cue well author work overview develop provid via illustr exampl furthermor insight paramet select various fusion context also provid|['Thanuka Wickramarathne']|['cs.AI']
2017-03-28T14:05:32Z|2017-03-19T17:31:13Z|http://arxiv.org/abs/1703.06471v1|http://arxiv.org/pdf/1703.06471v1|Multi-Timescale, Gradient Descent, Temporal Difference Learning with   Linear Options|multi timescal gradient descent tempor differ learn linear option|Deliberating on large or continuous state spaces have been long standing challenges in reinforcement learning. Temporal Abstraction have somewhat made this possible, but efficiently planing using temporal abstraction still remains an issue. Moreover using spatial abstractions to learn policies for various situations at once while using temporal abstraction models is an open problem. We propose here an efficient algorithm which is convergent under linear function approximation while planning using temporally abstract actions. We show how this algorithm can be used along with randomly generated option models over multiple time scales to plan agents which need to act real time. Using these randomly generated option models over multiple time scales are shown to reduce number of decision epochs required to solve the given task, hence effectively reducing the time needed for deliberation.|deliber larg continu state space long stand challeng reinforc learn tempor abstract somewhat made possibl effici plane use tempor abstract still remain issu moreov use spatial abstract learn polici various situat onc use tempor abstract model open problem propos effici algorithm converg linear function approxim plan use tempor abstract action show algorithm use along random generat option model multipl time scale plan agent need act real time use random generat option model multipl time scale shown reduc number decis epoch requir solv given task henc effect reduc time need deliber|['Peeyush Kumar', 'Doina Precup']|['cs.AI']
2017-03-28T14:05:32Z|2017-03-19T15:21:32Z|http://arxiv.org/abs/1703.06452v1|http://arxiv.org/pdf/1703.06452v1|Deep Neural Networks for Semantic Segmentation of Multispectral Remote   Sensing Imagery|deep neural network semant segment multispectr remot sens imageri|A semantic segmentation algorithm must assign a label to every pixel in an image. Recently, semantic segmentation of RGB imagery has advanced significantly due to deep learning. Because creating datasets for semantic segmentation is laborious, these datasets tend to be significantly smaller than object recognition datasets. This makes it difficult to directly train a deep neural network for semantic segmentation, because it will be prone to overfitting. To cope with this, deep learning models typically use convolutional neural networks pre-trained on large-scale image classification datasets, which are then fine-tuned for semantic segmentation. For non-RGB imagery, this is currently not possible because large-scale labeled non-RGB datasets do not exist. In this paper, we developed two deep neural networks for semantic segmentation of multispectral remote sensing imagery. Prior to training on the target dataset, we initialize the networks with large amounts of synthetic multispectral imagery. We show that this significantly improves results on real-world remote sensing imagery, and we establish a new state-of-the-art result on the challenging Hamlin Beach State Park Dataset.|semant segment algorithm must assign label everi pixel imag recent semant segment rgb imageri advanc signific due deep learn becaus creat dataset semant segment labori dataset tend signific smaller object recognit dataset make difficult direct train deep neural network semant segment becaus prone overfit cope deep learn model typic use convolut neural network pre train larg scale imag classif dataset fine tune semant segment non rgb imageri current possibl becaus larg scale label non rgb dataset exist paper develop two deep neural network semant segment multispectr remot sens imageri prior train target dataset initi network larg amount synthet multispectr imageri show signific improv result real world remot sens imageri establish new state art result challeng hamlin beach state park dataset|['Ronald Kemker', 'Christopher Kanan']|['cs.CV', 'cs.AI']
2017-03-28T14:05:32Z|2017-03-18T21:25:29Z|http://arxiv.org/abs/1703.06354v1|http://arxiv.org/pdf/1703.06354v1|Goal Conflict in Designing an Autonomous Artificial System|goal conflict design autonom artifici system|Research on human self-regulation has shown that people hold many goals simultaneously and have complex self-regulation mechanisms to deal with this goal conflict. Artificial autonomous systems may also need to find ways to cope with conflicting goals. Indeed, the intricate interplay among different goals may be critical to the design as well as long-term safety and stability of artificial autonomous systems. I discuss some of the critical features of the human self-regulation system and how it might be applied to an artificial system. Furthermore, the implications of goal conflict for the reliability and stability of artificial autonomous systems and ensuring their alignment with human goals and ethics is examined.|research human self regul shown peopl hold mani goal simultan complex self regul mechan deal goal conflict artifici autonom system may also need find way cope conflict goal inde intric interplay among differ goal may critic design well long term safeti stabil artifici autonom system discuss critic featur human self regul system might appli artifici system furthermor implic goal conflict reliabl stabil artifici autonom system ensur align human goal ethic examin|['Mark Muraven']|['cs.AI']
2017-03-28T14:05:32Z|2017-03-21T08:11:19Z|http://arxiv.org/abs/1703.06321v2|http://arxiv.org/pdf/1703.06321v2|Solving the Goddard problem by an influence diagram|solv goddard problem influenc diagram|Influence diagrams are a decision-theoretic extension of probabilistic graphical models. In this paper we show how they can be used to solve the Goddard problem. We present results of numerical experiments with this problem and compare the solutions provided by influence diagrams with the optimal solution.|influenc diagram decis theoret extens probabilist graphic model paper show use solv goddard problem present result numer experi problem compar solut provid influenc diagram optim solut|['Jiří Vomlel', 'Václav Kratochvíl']|['cs.AI', '68T37', 'I.2']
2017-03-28T14:05:37Z|2017-03-18T10:52:53Z|http://arxiv.org/abs/1703.06283v1|http://arxiv.org/pdf/1703.06283v1|Recognition in-the-Tail: Training Detectors for Unusual Pedestrians with   Synthetic Imposters|recognit tail train detector unusu pedestrian synthet impost|"As autonomous vehicles become an every-day reality, high-accuracy pedestrian detection is of paramount practical importance. Pedestrian detection is a highly researched topic with mature methods, but most datasets focus on common scenes of people engaged in typical walking poses on sidewalks. But performance is most crucial for dangerous scenarios, such as children playing in the street or people using bicycles/skateboards in unexpected ways. Such ""in-the-tail"" data is notoriously hard to observe, making both training and testing difficult. To analyze this problem, we have collected a novel annotated dataset of dangerous scenarios called the Precarious Pedestrian dataset. Even given a dedicated collection effort, it is relatively small by contemporary standards (around 1000 images). To explore large-scale data-driven learning, we explore the use of synthetic data generated by a game engine. A significant challenge is selected the right ""priors"" or parameters for synthesis: we would like realistic data with realistic poses and object configurations. Inspired by Generative Adversarial Networks, we generate a massive amount of synthetic data and train a discriminative classifier to select a realistic subset, which we deem Synthetic Imposters. We demonstrate that this pipeline allows one to generate realistic training data by making use of rendering/animation engines. Interestingly, we also demonstrate that such data can be used to rank algorithms, suggesting that Synthetic Imposters can also be used for ""in-the-tail"" validation at test-time, a notoriously difficult challenge for real-world deployment."|autonom vehicl becom everi day realiti high accuraci pedestrian detect paramount practic import pedestrian detect high research topic matur method dataset focus common scene peopl engag typic walk pose sidewalk perform crucial danger scenario children play street peopl use bicycl skateboard unexpect way tail data notori hard observ make train test difficult analyz problem collect novel annot dataset danger scenario call precari pedestrian dataset even given dedic collect effort relat small contemporari standard around imag explor larg scale data driven learn explor use synthet data generat game engin signific challeng select right prior paramet synthesi would like realist data realist pose object configur inspir generat adversari network generat massiv amount synthet data train discrimin classifi select realist subset deem synthet impost demonstr pipelin allow one generat realist train data make use render anim engin interest also demonstr data use rank algorithm suggest synthet impost also use tail valid test time notori difficult challeng real world deploy|['Shiyu Huang', 'Deva Ramanan']|['cs.CV', 'cs.AI']
2017-03-28T14:05:37Z|2017-03-18T09:04:05Z|http://arxiv.org/abs/1703.06275v1|http://arxiv.org/pdf/1703.06275v1|Evolving Game Skill-Depth using General Video Game AI Agents|evolv game skill depth use general video game ai agent|Most games have, or can be generalised to have, a number of parameters that may be varied in order to provide instances of games that lead to very different player experiences. The space of possible parameter settings can be seen as a search space, and we can therefore use a Random Mutation Hill Climbing algorithm or other search methods to find the parameter settings that induce the best games. One of the hardest parts of this approach is defining a suitable fitness function. In this paper we explore the possibility of using one of a growing set of General Video Game AI agents to perform automatic play-testing. This enables a very general approach to game evaluation based on estimating the skill-depth of a game. Agent-based play-testing is computationally expensive, so we compare two simple but efficient optimisation algorithms: the Random Mutation Hill-Climber and the Multi-Armed Bandit Random Mutation Hill-Climber. For the test game we use a space-battle game in order to provide a suitable balance between simulation speed and potential skill-depth. Results show that both algorithms are able to rapidly evolve game versions with significant skill-depth, but that choosing a suitable resampling number is essential in order to combat the effects of noise.|game generalis number paramet may vari order provid instanc game lead veri differ player experi space possibl paramet set seen search space therefor use random mutat hill climb algorithm search method find paramet set induc best game one hardest part approach defin suitabl fit function paper explor possibl use one grow set general video game ai agent perform automat play test enabl veri general approach game evalu base estim skill depth game agent base play test comput expens compar two simpl effici optimis algorithm random mutat hill climber multi arm bandit random mutat hill climber test game use space battl game order provid suitabl balanc simul speed potenti skill depth result show algorithm abl rapid evolv game version signific skill depth choos suitabl resampl number essenti order combat effect nois|['Jialin Liu', 'Julian Togelius', 'Diego Perez-Liebana', 'Simon M. Lucas']|['cs.AI']
2017-03-28T14:05:37Z|2017-03-21T14:26:33Z|http://arxiv.org/abs/1703.06207v2|http://arxiv.org/pdf/1703.06207v2|Cooperating with Machines|cooper machin|Since Alan Turing envisioned Artificial Intelligence (AI) [1], a major driving force behind technical progress has been competition with human cognition. Historical milestones have been frequently associated with computers matching or outperforming humans in difficult cognitive tasks (e.g. face recognition [2], personality classification [3], driving cars [4], or playing video games [5]), or defeating humans in strategic zero-sum encounters (e.g. Chess [6], Checkers [7], Jeopardy! [8], Poker [9], or Go [10]). In contrast, less attention has been given to developing autonomous machines that establish mutually cooperative relationships with people who may not share the machine's preferences. A main challenge has been that human cooperation does not require sheer computational power, but rather relies on intuition [11], cultural norms [12], emotions and signals [13, 14, 15, 16], and pre-evolved dispositions toward cooperation [17], common-sense mechanisms that are difficult to encode in machines for arbitrary contexts. Here, we combine a state-of-the-art machine-learning algorithm with novel mechanisms for generating and acting on signals to produce a new learning algorithm that cooperates with people and other machines at levels that rival human cooperation in a variety of two-player repeated stochastic games. This is the first general-purpose algorithm that is capable, given a description of a previously unseen game environment, of learning to cooperate with people within short timescales in scenarios previously unanticipated by algorithm designers. This is achieved without complex opponent modeling or higher-order theories of mind, thus showing that flexible, fast, and general human-machine cooperation is computationally achievable using a non-trivial, but ultimately simple, set of algorithmic mechanisms.|sinc alan ture envis artifici intellig ai major drive forc behind technic progress competit human cognit histor mileston frequent associ comput match outperform human difficult cognit task face recognit person classif drive car play video game defeat human strateg zero sum encount chess checker jeopardi poker go contrast less attent given develop autonom machin establish mutual cooper relationship peopl may share machin prefer main challeng human cooper doe requir sheer comput power rather reli intuit cultur norm emot signal pre evolv disposit toward cooper common sens mechan difficult encod machin arbitrari context combin state art machin learn algorithm novel mechan generat act signal produc new learn algorithm cooper peopl machin level rival human cooper varieti two player repeat stochast game first general purpos algorithm capabl given descript previous unseen game environ learn cooper peopl within short timescal scenario previous unanticip algorithm design achiev without complex oppon model higher order theori mind thus show flexibl fast general human machin cooper comput achiev use non trivial ultim simpl set algorithm mechan|['Jacob W. Crandall', 'Mayada Oudah', 'Tennom', 'Fatimah Ishowo-Oloko', 'Sherief Abdallah', 'Jean-François Bonnefon', 'Manuel Cebrian', 'Azim Shariff', 'Michael A. Goodrich', 'Iyad Rahwan']|['cs.AI']
2017-03-28T14:05:37Z|2017-03-25T15:54:36Z|http://arxiv.org/abs/1703.06182v2|http://arxiv.org/pdf/1703.06182v2|Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under   Partial Observability|deep decentr multi task multi agent reinforc learn partial observ|Many real-world tasks involve multiple agents with partial observability and limited communication. Learning is challenging in these settings due to local viewpoints of agents, which perceive the world as non-stationary due to concurrently-exploring teammates. Approaches that learn specialized policies for individual tasks face major problems when applied to the real world: not only do agents have to learn and store a distinct policy for each task, but in practice the identity of the task is often non-observable, making these algorithms inapplicable. This paper formalizes and addresses the problem of multi-task multi-agent reinforcement learning under partial observability. We introduce a decentralized single-task learning approach that is robust to concurrent interactions of teammates, and present an approach for distilling single-task policies into a unified policy that performs well across multiple related tasks, without explicit provision of task identity.|mani real world task involv multipl agent partial observ limit communic learn challeng set due local viewpoint agent perceiv world non stationari due concurr explor teammat approach learn special polici individu task face major problem appli real world onli agent learn store distinct polici task practic ident task often non observ make algorithm inapplic paper formal address problem multi task multi agent reinforc learn partial observ introduc decentr singl task learn approach robust concurr interact teammat present approach distil singl task polici unifi polici perform well across multipl relat task without explicit provis task ident|['Shayegan Omidshafiei', 'Jason Pazis', 'Christopher Amato', 'Jonathan P. How', 'John Vian']|['cs.LG', 'cs.AI']
2017-03-28T14:05:37Z|2017-03-17T17:09:14Z|http://arxiv.org/abs/1703.06103v1|http://arxiv.org/pdf/1703.06103v1|Modeling Relational Data with Graph Convolutional Networks|model relat data graph convolut network|Knowledge bases play a crucial role in many applications, for example question answering and information retrieval. Despite the great effort invested in creating and maintaining them, even the largest representatives (e.g., Yago, DBPedia or Wikidata) are highly incomplete. We introduce relational graph convolutional networks (R-GCNs) and apply them to two standard knowledge base completion tasks: link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing attributes of entities). R-GCNs are a generalization of graph convolutional networks, a recent class of neural networks operating on graphs, and are developed specifically to deal with highly multi-relational data, characteristic of realistic knowledge bases. Our methods achieve competitive results on standard benchmarks for both tasks.|knowledg base play crucial role mani applic exampl question answer inform retriev despit great effort invest creat maintain even largest repres yago dbpedia wikidata high incomplet introduc relat graph convolut network gcns appli two standard knowledg base complet task link predict recoveri miss fact subject predic object tripl entiti classif recoveri miss attribut entiti gcns general graph convolut network recent class neural network oper graph develop specif deal high multi relat data characterist realist knowledg base method achiev competit result standard benchmark task|['Michael Schlichtkrull', 'Thomas N. Kipf', 'Peter Bloem', 'Rianne van den Berg', 'Ivan Titov', 'Max Welling']|['stat.ML', 'cs.AI', 'cs.DB', 'cs.LG']
2017-03-28T14:05:37Z|2017-03-17T15:00:03Z|http://arxiv.org/abs/1703.06045v1|http://arxiv.org/pdf/1703.06045v1|Approximation Complexity of Maximum A Posteriori Inference in   Sum-Product Networks|approxim complex maximum posteriori infer sum product network|We discuss the computational complexity of approximating maximum a posteriori inference in sum-product networks. We first show NP-hardness in three-level trees by a reduction from maximum independent set; this implies non-approximability within a sublinear factor. We show that this is a tight bound, as we can find an approximation within a linear factor in three-level networks. We then show that in four-level trees it is NP-hard to approximate the problem within a factor $2^{f(n)}$ for any sublinear function $f$ of the size of the input $n$. Again, this is bound is tight, as we prove that the usual max-product algorithm finds (in any network) approximations within factor $2^{c n}$ from some constant $c < 1$. Last, we present a simple algorithm, and show that it provably produces solutions at least as good as, and potentially much better than, the max-product algorithm.|discuss comput complex approxim maximum posteriori infer sum product network first show np hard three level tree reduct maximum independ set impli non approxim within sublinear factor show tight bound find approxim within linear factor three level network show four level tree np hard approxim problem within factor ani sublinear function size input bound tight prove usual max product algorithm find ani network approxim within factor constant last present simpl algorithm show provabl produc solut least good potenti much better max product algorithm|['Denis Deratani Mauá', 'Cassio P. de Campos']|['cs.AI', '68T37']
2017-03-28T14:05:37Z|2017-03-16T21:08:31Z|http://arxiv.org/abs/1703.05820v1|http://arxiv.org/pdf/1703.05820v1|Particle Value Functions|particl valu function|The policy gradients of the expected return objective can react slowly to rare rewards. Yet, in some cases agents may wish to emphasize the low or high returns regardless of their probability. Borrowing from the economics and control literature, we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example. This risk-sensitive value function is not always applicable to reinforcement learning problems, so we introduce the particle value function defined by a particle filter over the distributions of an agent's experience, which bounds the risk-sensitive one. We illustrate the benefit of the policy gradients of this objective in Cliffworld.|polici gradient expect return object react slowli rare reward yet case agent may wish emphas low high return regardless probabl borrow econom control literatur review risk sensit valu function aris exponenti util illustr effect exampl risk sensit valu function alway applic reinforc learn problem introduc particl valu function defin particl filter distribut agent experi bound risk sensit one illustr benefit polici gradient object cliffworld|['Chris J. Maddison', 'Dieterich Lawson', 'George Tucker', 'Nicolas Heess', 'Arnaud Doucet', 'Andriy Mnih', 'Yee Whye Teh']|['cs.LG', 'cs.AI']
2017-03-28T14:05:37Z|2017-03-16T13:36:41Z|http://arxiv.org/abs/1703.05614v1|http://arxiv.org/pdf/1703.05614v1|ParaGraphE: A Library for Parallel Knowledge Graph Embedding|paragraph librari parallel knowledg graph embed|Knowledge graph embedding aims at translating the knowledge graph into numerical representations by transforming the entities and relations into con- tinuous low-dimensional vectors. Recently, many methods [1, 5, 3, 2, 6] have been proposed to deal with this problem, but existing single-thread implemen- tations of them are time-consuming for large-scale knowledge graphs. Here, we design a unified parallel framework to parallelize these methods, which achieves a significant time reduction without in uencing the accuracy. We name our framework as ParaGraphE, which provides a library for parallel knowledge graph embedding. The source code can be downloaded from https: //github.com/LIBBLE/LIBBLE-MultiThread/tree/master/ParaGraphE.|knowledg graph embed aim translat knowledg graph numer represent transform entiti relat con tinuous low dimension vector recent mani method propos deal problem exist singl thread implemen tation time consum larg scale knowledg graph design unifi parallel framework parallel method achiev signific time reduct without uenc accuraci name framework paragraph provid librari parallel knowledg graph embed sourc code download https github com libbl libbl multithread tree master paragraph|['Xiao-Fan Niu', 'Wu-Jun Li']|['cs.AI']
2017-03-28T14:05:37Z|2017-03-16T13:07:54Z|http://arxiv.org/abs/1703.06109v1|http://arxiv.org/pdf/1703.06109v1|Generalised Reichenbachian Common Cause Systems|generalis reichenbachian common caus system|The principle of the common cause claims that if an improbable coincidence has occurred, there must exist a common cause. This is generally taken to mean that positive correlations between non-causally related events should disappear when conditioning on the action of some underlying common cause. The extended interpretation of the principle, by contrast, urges that common causes should be called for in order to explain positive deviations between the estimated correlation of two events and the expected value of their correlation. The aim of this paper is to provide the extended reading of the principle with a general probabilistic model, capturing the simultaneous action of a system of multiple common causes. To this end, two distinct models are elaborated, and the necessary and sufficient conditions for their existence are determined.|principl common caus claim improb coincid occur must exist common caus general taken mean posit correl non causal relat event disappear condit action common caus extend interpret principl contrast urg common caus call order explain posit deviat estim correl two event expect valu correl aim paper provid extend read principl general probabilist model captur simultan action system multipl common caus end two distinct model elabor necessari suffici condit exist determin|['Claudio Mazzola']|['stat.OT', 'cs.AI']
2017-03-28T14:05:37Z|2017-03-16T12:53:52Z|http://arxiv.org/abs/1703.06042v1|http://arxiv.org/pdf/1703.06042v1|A Visual Web Tool to Perform What-If Analysis of Optimization Approaches|visual web tool perform analysi optim approach|In Operation Research, practical evaluation is essential to validate the efficacy of optimization approaches. This paper promotes the usage of performance profiles as a standard practice to visualize and analyze experimental results. It introduces a Web tool to construct and export performance profiles as SVG or HTML files. In addition, the application relies on a methodology to estimate the benefit of hypothetical solver improvements. Therefore, the tool allows one to employ what-if analysis to screen possible research directions, and identify those having the best potential. The approach is showcased on two Operation Research technologies: Constraint Programming and Mixed Integer Linear Programming.|oper research practic evalu essenti valid efficaci optim approach paper promot usag perform profil standard practic visual analyz experiment result introduc web tool construct export perform profil svg html file addit applic reli methodolog estim benefit hypothet solver improv therefor tool allow one employ analysi screen possibl research direct identifi best potenti approach showcas two oper research technolog constraint program mix integ linear program|['Sascha Van Cauwelaert', 'Michele Lombardi', 'Pierre Schaus']|['cs.AI', 'cs.PF']
2017-03-28T14:05:41Z|2017-03-16T03:36:28Z|http://arxiv.org/abs/1703.05468v1|http://arxiv.org/abs/1703.05468v1|Database Learning: Toward a Database that Becomes Smarter Every Time|databas learn toward databas becom smarter everi time|In today's databases, previous query answers rarely benefit answering future queries. For the first time, to the best of our knowledge, we change this paradigm in an approximate query processing (AQP) context. We make the following observation: the answer to each query reveals some degree of knowledge about the answer to another query because their answers stem from the same underlying distribution that has produced the entire dataset. Exploiting and refining this knowledge should allow us to answer queries more analytically, rather than by reading enormous amounts of raw data. Also, processing more queries should continuously enhance our knowledge of the underlying distribution, and hence lead to increasingly faster response times for future queries.   We call this novel idea---learning from past query answers---Database Learning. We exploit the principle of maximum entropy to produce answers, which are in expectation guaranteed to be more accurate than existing sample-based approximations. Empowered by this idea, we build a query engine on top of Spark SQL, called Verdict. We conduct extensive experiments on real-world query traces from a large customer of a major database vendor. Our results demonstrate that database learning supports 73.7% of these queries, speeding them up by up to 23.0x for the same accuracy level compared to existing AQP systems.|today databas previous queri answer rare benefit answer futur queri first time best knowledg chang paradigm approxim queri process aqp context make follow observ answer queri reveal degre knowledg answer anoth queri becaus answer stem distribut produc entir dataset exploit refin knowledg allow us answer queri analyt rather read enorm amount raw data also process queri continu enhanc knowledg distribut henc lead increas faster respons time futur queri call novel idea learn past queri answer databas learn exploit principl maximum entropi produc answer expect guarante accur exist sampl base approxim empow idea build queri engin top spark sql call verdict conduct extens experi real world queri trace larg custom major databas vendor result demonstr databas learn support queri speed accuraci level compar exist aqp system|['Yongjoo Park', 'Ahmad Shahab Tajik', 'Michael Cafarella', 'Barzan Mozafari']|['cs.DB', 'cs.AI']
2017-03-28T14:05:41Z|2017-03-16T01:37:25Z|http://arxiv.org/abs/1703.05452v1|http://arxiv.org/pdf/1703.05452v1|Efficient Online Learning for Optimizing Value of Information: Theory   and Application to Interactive Troubleshooting|effici onlin learn optim valu inform theori applic interact troubleshoot|We consider the optimal value of information (VoI) problem, where the goal is to sequentially select a set of tests with a minimal cost, so that one can efficiently make the best decision based on the observed outcomes. Existing algorithms are either heuristics with no guarantees, or scale poorly (with exponential run time in terms of the number of available tests). Moreover, these methods assume a known distribution over the test outcomes, which is often not the case in practice. We propose an efficient sampling-based online learning framework to address the above issues. First, assuming the distribution over hypotheses is known, we propose a dynamic hypothesis enumeration strategy, which allows efficient information gathering with strong theoretical guarantees. We show that with sufficient amount of samples, one can identify a near-optimal decision with high probability. Second, when the parameters of the hypotheses distribution are unknown, we propose an algorithm which learns the parameters progressively via posterior sampling in an online fashion. We further establish a rigorous bound on the expected regret. We demonstrate the effectiveness of our approach on a real-world interactive troubleshooting application and show that one can efficiently make high-quality decisions with low cost.|consid optim valu inform voi problem goal sequenti select set test minim cost one effici make best decis base observ outcom exist algorithm either heurist guarante scale poor exponenti run time term number avail test moreov method assum known distribut test outcom often case practic propos effici sampl base onlin learn framework address abov issu first assum distribut hypothes known propos dynam hypothesi enumer strategi allow effici inform gather strong theoret guarante show suffici amount sampl one identifi near optim decis high probabl second paramet hypothes distribut unknown propos algorithm learn paramet progress via posterior sampl onlin fashion establish rigor bound expect regret demonstr effect approach real world interact troubleshoot applic show one effici make high qualiti decis low cost|['Yuxin Chen', 'Jean-Michel Renders', 'Morteza Haghir Chehreghani', 'Andreas Krause']|['cs.AI', 'cs.LG', 'stat.ML']
2017-03-28T14:05:41Z|2017-03-16T01:31:33Z|http://arxiv.org/abs/1703.05449v1|http://arxiv.org/pdf/1703.05449v1|Minimax Regret Bounds for Reinforcement Learning|minimax regret bound reinforc learn|"We consider the problem of efficient exploration in finite horizon MDPs.We show that an optimistic modification to model-based value iteration, can achieve a regret bound $\tilde{O}( \sqrt{HSAT} + H^2S^2A+H\sqrt{T})$ where $H$ is the time horizon, $S$ the number of states, $A$ the number of actions and $T$ the time elapsed. This result improves over the best previous known bound $\tilde{O}(HS \sqrt{AT})$ achieved by the UCRL2 algorithm.The key significance of our new results is that when $T\geq H^3S^3A$ and $SA\geq H$, it leads to a regret of $\tilde{O}(\sqrt{HSAT})$ that matches the established lower bounds of $\Omega(\sqrt{HSAT})$ up to a logarithmic factor. Our analysis contain two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in $S$), and we use ""exploration bonuses"" based on Bernstein's inequality, together with using a recursive -Bellman-type- Law of Total Variance (to improve scaling in $H$)."|consid problem effici explor finit horizon mdps show optimist modif model base valu iter achiev regret bound tild sqrt hsat sqrt time horizon number state number action time elaps result improv best previous known bound tild hs sqrt achiev ucrl algorithm key signific new result geq sa geq lead regret tild sqrt hsat match establish lower bound omega sqrt hsat logarithm factor analysi contain two key insight use care applic concentr inequ optim valu function whole rather transit probabl improv scale use explor bonus base bernstein inequ togeth use recurs bellman type law total varianc improv scale|['Mohammad Gheshlaghi Azar', 'Ian Osband', 'Rémi Munos']|['stat.ML', 'cs.AI', 'cs.LG']
2017-03-28T14:05:41Z|2017-03-16T01:14:36Z|http://arxiv.org/abs/1703.05446v1|http://arxiv.org/pdf/1703.05446v1|Look into Person: Self-supervised Structure-sensitive Learning and A New   Benchmark for Human Parsing|look person self supervis structur sensit learn new benchmark human pars|"Human parsing has recently attracted a lot of research interests due to its huge application potentials. However existing datasets have limited number of images and annotations, and lack the variety of human appearances and the coverage of challenging cases in unconstrained environment. In this paper, we introduce a new benchmark ""Look into Person (LIP)"" that makes a significant advance in terms of scalability, diversity and difficulty, a contribution that we feel is crucial for future developments in human-centric analysis. This comprehensive dataset contains over 50,000 elaborately annotated images with 19 semantic part labels, which are captured from a wider range of viewpoints, occlusions and background complexity. Given these rich annotations we perform detailed analyses of the leading human parsing approaches, gaining insights into the success and failures of these methods. Furthermore, in contrast to the existing efforts on improving the feature discriminative capability, we solve human parsing by exploring a novel self-supervised structure-sensitive learning approach, which imposes human pose structures into parsing results without resorting to extra supervision (i.e., no need for specifically labeling human joints in model training). Our self-supervised learning framework can be injected into any advanced neural networks to help incorporate rich high-level knowledge regarding human joints from a global perspective and improve the parsing results. Extensive evaluations on our LIP and the public PASCAL-Person-Part dataset demonstrate the superiority of our method."|human pars recent attract lot research interest due huge applic potenti howev exist dataset limit number imag annot lack varieti human appear coverag challeng case unconstrain environ paper introduc new benchmark look person lip make signific advanc term scalabl divers difficulti contribut feel crucial futur develop human centric analysi comprehens dataset contain elabor annot imag semant part label captur wider rang viewpoint occlus background complex given rich annot perform detail analys lead human pars approach gain insight success failur method furthermor contrast exist effort improv featur discrimin capabl solv human pars explor novel self supervis structur sensit learn approach impos human pose structur pars result without resort extra supervis need specif label human joint model train self supervis learn framework inject ani advanc neural network help incorpor rich high level knowledg regard human joint global perspect improv pars result extens evalu lip public pascal person part dataset demonstr superior method|['Ke Gong', 'Xiaodan Liang', 'Xiaohui Shen', 'Liang Lin']|['cs.CV', 'cs.AI', 'cs.LG']
2017-03-28T14:05:41Z|2017-03-16T01:06:07Z|http://arxiv.org/abs/1703.05320v1|http://arxiv.org/pdf/1703.05320v1|Legal Question Answering using Ranking SVM and Deep Convolutional Neural   Network|legal question answer use rank svm deep convolut neural network|This paper presents a study of employing Ranking SVM and Convolutional Neural Network for two missions: legal information retrieval and question answering in the Competition on Legal Information Extraction/Entailment. For the first task, our proposed model used a triple of features (LSI, Manhattan, Jaccard), and is based on paragraph level instead of article level as in previous studies. In fact, each single-paragraph article corresponds to a particular paragraph in a huge multiple-paragraph article. For the legal question answering task, additional statistical features from information retrieval task integrated into Convolutional Neural Network contribute to higher accuracy.|paper present studi employ rank svm convolut neural network two mission legal inform retriev question answer competit legal inform extract entail first task propos model use tripl featur lsi manhattan jaccard base paragraph level instead articl level previous studi fact singl paragraph articl correspond particular paragraph huge multipl paragraph articl legal question answer task addit statist featur inform retriev task integr convolut neural network contribut higher accuraci|['Phong-Khac Do', 'Huy-Tien Nguyen', 'Chien-Xuan Tran', 'Minh-Tien Nguyen', 'Minh-Le Nguyen']|['cs.CL', 'cs.AI', '14J30 (Primary)', 'H.3; H.3.3; I.2.7']
2017-03-28T14:05:41Z|2017-03-15T21:20:44Z|http://arxiv.org/abs/1703.05390v1|http://arxiv.org/pdf/1703.05390v1|Convolutional Recurrent Neural Networks for Small-Footprint Keyword   Spotting|convolut recurr neural network small footprint keyword spot|Keyword spotting (KWS) constitutes a major component of human-technology interfaces. Maximizing the detection accuracy at a low false alarm (FA) rate, while minimizing the footprint size, latency and complexity are the goals for KWS. Towards achieving them, we study Convolutional Recurrent Neural Networks (CRNNs). Inspired by large-scale state-of-the-art speech recognition systems, we combine the strengths of convolutional layers and recurrent layers to exploit local structure and long-range context. We analyze the effect of architecture parameters, and propose training strategies to improve performance. With only ~230k parameters, our CRNN model yields acceptably low latency, and achieves 97.71% accuracy at 0.5 FA/hour for 5 dB signal-to-noise ratio.|keyword spot kws constitut major compon human technolog interfac maxim detect accuraci low fals alarm fa rate minim footprint size latenc complex goal kws toward achiev studi convolut recurr neural network crnns inspir larg scale state art speech recognit system combin strength convolut layer recurr layer exploit local structur long rang context analyz effect architectur paramet propos train strategi improv perform onli paramet crnn model yield accept low latenc achiev accuraci fa hour db signal nois ratio|['Sercan O. Arik', 'Markus Kliegl', 'Rewon Child', 'Joel Hestness', 'Andrew Gibiansky', 'Chris Fougner', 'Ryan Prenger', 'Adam Coates']|['cs.CL', 'cs.AI', 'cs.LG']
2017-03-28T14:05:41Z|2017-03-15T20:23:45Z|http://arxiv.org/abs/1703.05376v1|http://arxiv.org/pdf/1703.05376v1|Concentration Bounds for Two Timescale Stochastic Approximation with   Applications to Reinforcement Learning|concentr bound two timescal stochast approxim applic reinforc learn|Two-timescale Stochastic Approximation (SA) algorithms are widely used in Reinforcement Learning (RL). In such methods, the iterates consist of two parts that are updated using different stepsizes. We develop the first convergence rate result for these algorithms; in particular, we provide a general methodology for analyzing two-timescale linear SA. We apply our methodology to two-timescale RL algorithms such as GTD(0), GTD2, and TDC.|two timescal stochast approxim sa algorithm wide use reinforc learn rl method iter consist two part updat use differ stepsiz develop first converg rate result algorithm particular provid general methodolog analyz two timescal linear sa appli methodolog two timescal rl algorithm gtd gtd tdc|['Gal Dalal', 'Balazs Szorenyi', 'Gugan Thoppe', 'Shie Mannor']|['cs.AI']
2017-03-28T14:05:41Z|2017-03-15T17:01:20Z|http://arxiv.org/abs/1703.05260v1|http://arxiv.org/pdf/1703.05260v1|InScript: Narrative texts annotated with script information|inscript narrat text annot script inform|This paper presents the InScript corpus (Narrative Texts Instantiating Script structure). InScript is a corpus of 1,000 stories centered around 10 different scenarios. Verbs and noun phrases are annotated with event and participant types, respectively. Additionally, the text is annotated with coreference information. The corpus shows rich lexical variation and will serve as a unique resource for the study of the role of script knowledge in natural language processing.|paper present inscript corpus narrat text instanti script structur inscript corpus stori center around differ scenario verb noun phrase annot event particip type respect addit text annot corefer inform corpus show rich lexic variat serv uniqu resourc studi role script knowledg natur languag process|['Ashutosh Modi', 'Tatjana Anikina', 'Simon Ostermann', 'Manfred Pinkal']|['cs.CL', 'cs.AI']
2017-03-28T14:05:41Z|2017-03-15T15:19:28Z|http://arxiv.org/abs/1703.05204v1|http://arxiv.org/pdf/1703.05204v1|On Inconsistency Indices and Inconsistency Axioms in Pairwise   Comparisons|inconsist indic inconsist axiom pairwis comparison|Pairwise comparisons are an important tool of modern (multiple criteria) decision making. Since human judgments are often inconsistent, many studies focused on the ways how to express and measure this inconsistency, and several inconsistency indices were proposed as an alternative to Saaty inconsistency index and inconsistency ratio for reciprocal pairwise comparisons matrices. This paper aims to: firstly, introduce a new measure of inconsistency of pairwise comparisons and to prove its basic properties; secondly, to postulate an additional axiom, an upper boundary axiom, to an existing set of axioms; and the last, but not least, the paper provides proofs of satisfaction of this additional axiom by selected inconsistency indices as well as it provides their numerical comparison.|pairwis comparison import tool modern multipl criteria decis make sinc human judgment often inconsist mani studi focus way express measur inconsist sever inconsist indic propos altern saati inconsist index inconsist ratio reciproc pairwis comparison matric paper aim first introduc new measur inconsist pairwis comparison prove basic properti second postul addit axiom upper boundari axiom exist set axiom last least paper provid proof satisfact addit axiom select inconsist indic well provid numer comparison|['Jiri Mazurek']|['cs.AI']
2017-03-28T14:05:41Z|2017-03-15T15:13:42Z|http://arxiv.org/abs/1703.05201v1|http://arxiv.org/pdf/1703.05201v1|Fuzzy Rankings: Properties and Applications|fuzzi rank properti applic|In practice, a ranking of objects with respect to given set of criteria is of considerable importance. However, due to lack of knowledge, information of time pressure, decision makers might not be able to provide a (crisp) ranking of objects from the top to the bottom. Instead, some objects might be ranked equally, or better than other objects only to some degree. In such cases, a generalization of crisp rankings to fuzzy rankings can be more useful. The aim of the article is to introduce the notion of a fuzzy ranking and to discuss its several properties, namely orderings, similarity and indecisiveness. The proposed approach can be used both for group decision making or multiple criteria decision making when uncertainty is involved.|practic rank object respect given set criteria consider import howev due lack knowledg inform time pressur decis maker might abl provid crisp rank object top bottom instead object might rank equal better object onli degre case general crisp rank fuzzi rank use aim articl introduc notion fuzzi rank discuss sever properti name order similar indecis propos approach use group decis make multipl criteria decis make uncertainti involv|['Jiří Mazurek']|['cs.AI']
2017-03-28T14:05:45Z|2017-03-15T07:57:51Z|http://arxiv.org/abs/1703.04990v1|http://arxiv.org/pdf/1703.04990v1|Neural Programming by Example|neural program exampl|Programming by Example (PBE) targets at automatically inferring a computer program for accomplishing a certain task from sample input and output. In this paper, we propose a deep neural networks (DNN) based PBE model called Neural Programming by Example (NPBE), which can learn from input-output strings and induce programs that solve the string manipulation problems. Our NPBE model has four neural network based components: a string encoder, an input-output analyzer, a program generator, and a symbol selector. We demonstrate the effectiveness of NPBE by training it end-to-end to solve some common string manipulation problems in spreadsheet systems. The results show that our model can induce string manipulation programs effectively. Our work is one step towards teaching DNN to generate computer programs.|program exampl pbe target automat infer comput program accomplish certain task sampl input output paper propos deep neural network dnn base pbe model call neural program exampl npbe learn input output string induc program solv string manipul problem npbe model four neural network base compon string encod input output analyz program generat symbol selector demonstr effect npbe train end end solv common string manipul problem spreadsheet system result show model induc string manipul program effect work one step toward teach dnn generat comput program|['Chengxun Shu', 'Hongyu Zhang']|['cs.AI', 'cs.NE', 'cs.SE']
2017-03-28T14:05:45Z|2017-03-15T05:43:48Z|http://arxiv.org/abs/1703.04940v1|http://arxiv.org/pdf/1703.04940v1|Resilience: A Criterion for Learning in the Presence of Arbitrary   Outliers|resili criterion learn presenc arbitrari outlier|We introduce a criterion, resilience, which allows properties of a dataset (such as its mean or best low rank approximation) to be robustly computed, even in the presence of a large fraction of arbitrary additional data. Resilience is a weaker condition than most other properties considered so far in the literature, and yet enables robust estimation in a broader variety of settings, including the previously unstudied problem of robust mean estimation in $\ell_p$-norms.|introduc criterion resili allow properti dataset mean best low rank approxim robust comput even presenc larg fraction arbitrari addit data resili weaker condit properti consid far literatur yet enabl robust estim broader varieti set includ previous unstudi problem robust mean estim ell norm|['Jacob Steinhardt', 'Moses Charikar', 'Gregory Valiant']|['cs.LG', 'cs.AI', 'cs.CC', 'cs.CR', 'stat.ML']
2017-03-28T14:05:45Z|2017-03-17T00:56:18Z|http://arxiv.org/abs/1703.04912v2|http://arxiv.org/pdf/1703.04912v2|Syntax-Preserving Belief Change Operators for Logic Programs|syntax preserv belief chang oper logic program|Recent methods have adapted the well-established AGM and belief base frameworks for belief change to cover belief revision in logic programs. In this study here, we present two new sets of belief change operators for logic programs. They focus on preserving the explicit relationships expressed in the rules of a program, a feature that is missing in purely semantic approaches that consider programs only in their entirety. In particular, operators of the latter class fail to satisfy preservation and support, two important properties for belief change in logic programs required to ensure intuitive results.   We address this shortcoming of existing approaches by introducing partial meet and ensconcement constructions for logic program belief change, which allow us to define syntax-preserving operators that satisfy preservation and support. Our work is novel in that our constructions not only preserve more information from a logic program during a change operation than existing ones, but they also facilitate natural definitions of contraction operators, the first in the field to the best of our knowledge.   In order to evaluate the rationality of our operators, we translate the revision and contraction postulates from the AGM and belief base frameworks to the logic programming setting. We show that our operators fully comply with the belief base framework and formally state the interdefinability between our operators. We further propose an algorithm that is based on modularising a logic program to reduce partial meet and ensconcement revisions or contractions to performing the operation only on the relevant modules of that program. Finally, we compare our approach to two state-of-the-art logic program revision methods and demonstrate that our operators address the shortcomings of one and generalise the other method.|recent method adapt well establish agm belief base framework belief chang cover belief revis logic program studi present two new set belief chang oper logic program focus preserv explicit relationship express rule program featur miss pure semant approach consid program onli entireti particular oper latter class fail satisfi preserv support two import properti belief chang logic program requir ensur intuit result address shortcom exist approach introduc partial meet ensconc construct logic program belief chang allow us defin syntax preserv oper satisfi preserv support work novel construct onli preserv inform logic program dure chang oper exist one also facilit natur definit contract oper first field best knowledg order evalu ration oper translat revis contract postul agm belief base framework logic program set show oper fulli compli belief base framework formal state interdefin oper propos algorithm base modularis logic program reduc partial meet ensconc revis contract perform oper onli relev modul program final compar approach two state art logic program revis method demonstr oper address shortcom one generalis method|['Sebastian Binnewies', 'Zhiqiang Zhuang', 'Kewen Wang', 'Bela Stantic']|['cs.AI', 'I.2.3; I.2.4; F.4.1']
2017-03-28T14:05:45Z|2017-03-15T03:30:13Z|http://arxiv.org/abs/1703.04908v1|http://arxiv.org/pdf/1703.04908v1|Emergence of Grounded Compositional Language in Multi-Agent Populations|emerg ground composit languag multi agent popul|By capturing statistical patterns in large corpora, machine learning has enabled significant advances in natural language processing, including in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply capturing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. Towards this end, we propose a multi-agent learning environment and learning methods that bring about emergence of a basic compositional language. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of non-verbal communication such as pointing and guiding when language communication is unavailable.|captur statist pattern larg corpora machin learn enabl signific advanc natur languag process includ machin translat question answer sentiment analysi howev agent intellig interact human simpli captur statist pattern insuffici paper investig ground composit languag emerg mean achiev goal multi agent popul toward end propos multi agent learn environ learn method bring emerg basic composit languag languag repres stream abstract discret symbol utter agent time nonetheless coher structur possess defin vocabulari syntax also observ emerg non verbal communic point guid languag communic unavail|['Igor Mordatch', 'Pieter Abbeel']|['cs.AI', 'cs.CL']
2017-03-28T14:05:45Z|2017-03-15T01:04:49Z|http://arxiv.org/abs/1703.04862v1|http://arxiv.org/pdf/1703.04862v1|Exploring the Combination Rules of D Numbers From a Perspective of   Conflict Redistribution|explor combin rule number perspect conflict redistribut|Dempster-Shafer theory of evidence is widely applied to uncertainty modelling and knowledge reasoning because of its advantages in dealing with uncertain information. But some conditions or requirements, such as exclusiveness hypothesis and completeness constraint, limit the development and application of that theory to a large extend. To overcome the shortcomings and enhance its capability of representing the uncertainty, a novel model, called D numbers, has been proposed recently. However, many key issues, for example how to implement the combination of D numbers, remain unsolved. In the paper, we have explored the combination of D Numbers from a perspective of conflict redistribution, and proposed two combination rules being suitable for different situations for the fusion of two D numbers. The proposed combination rules can reduce to the classical Dempster's rule in Dempster-Shafer theory under a certain conditions. Numerical examples and discussion about the proposed rules are also given in the paper.|dempster shafer theori evid wide appli uncertainti model knowledg reason becaus advantag deal uncertain inform condit requir exclus hypothesi complet constraint limit develop applic theori larg extend overcom shortcom enhanc capabl repres uncertainti novel model call number propos recent howev mani key issu exampl implement combin number remain unsolv paper explor combin number perspect conflict redistribut propos two combin rule suitabl differ situat fusion two number propos combin rule reduc classic dempster rule dempster shafer theori certain condit numer exampl discuss propos rule also given paper|['Xinyang Deng', 'Wen Jiang']|['cs.AI']
2017-03-28T14:05:45Z|2017-03-14T23:09:45Z|http://arxiv.org/abs/1703.04816v1|http://arxiv.org/pdf/1703.04816v1|FastQA: A Simple and Efficient Neural Architecture for Question   Answering|fastqa simpl effici neural architectur question answer|Recent development of large-scale question answering (QA) datasets triggered a substantial amount of research into end-to-end neural architectures for QA. Increasingly complex systems have been conceived without comparison to a simpler neural baseline system that would justify their complexity. In this work, we propose a simple heuristic that guided the development of FastQA, an efficient end-to-end neural model for question answering that is very competitive with existing models. We further demonstrate, that an extended version (FastQAExt) achieves state-of-the-art results on recent benchmark datasets, namely SQuAD, NewsQA and MsMARCO, outperforming most existing models. However, we show that increasing the complexity of FastQA to FastQAExt does not yield any systematic improvements. We argue that the same holds true for most existing systems that are similar to FastQAExt. A manual analysis reveals that our proposed heuristic explains most predictions of our model, which indicates that modeling a simple heuristic is enough to achieve strong performance on extractive QA datasets. The overall strong performance of FastQA puts results of existing, more complex models into perspective.|recent develop larg scale question answer qa dataset trigger substanti amount research end end neural architectur qa increas complex system conceiv without comparison simpler neural baselin system would justifi complex work propos simpl heurist guid develop fastqa effici end end neural model question answer veri competit exist model demonstr extend version fastqaext achiev state art result recent benchmark dataset name squad newsqa msmarco outperform exist model howev show increas complex fastqa fastqaext doe yield ani systemat improv argu hold true exist system similar fastqaext manual analysi reveal propos heurist explain predict model indic model simpl heurist enough achiev strong perform extract qa dataset overal strong perform fastqa put result exist complex model perspect|['Dirk Weissenborn', 'Georg Wiese', 'Laura Seiffe']|['cs.CL', 'cs.AI', 'cs.NE']
2017-03-28T14:05:45Z|2017-03-14T22:13:20Z|http://arxiv.org/abs/1703.04756v1|http://arxiv.org/pdf/1703.04756v1|Weighted Voting Via No-Regret Learning|weight vote via regret learn|Voting systems typically treat all voters equally. We argue that perhaps they should not: Voters who have supported good choices in the past should be given higher weight than voters who have supported bad ones. To develop a formal framework for desirable weighting schemes, we draw on no-regret learning. Specifically, given a voting rule, we wish to design a weighting scheme such that applying the voting rule, with voters weighted by the scheme, leads to choices that are almost as good as those endorsed by the best voter in hindsight. We derive possibility and impossibility results for the existence of such weighting schemes, depending on whether the voting rule and the weighting scheme are deterministic or randomized, as well as on the social choice axioms satisfied by the voting rule.|vote system typic treat voter equal argu perhap voter support good choic past given higher weight voter support bad one develop formal framework desir weight scheme draw regret learn specif given vote rule wish design weight scheme appli vote rule voter weight scheme lead choic almost good endors best voter hindsight deriv possibl imposs result exist weight scheme depend whether vote rule weight scheme determinist random well social choic axiom satisfi vote rule|['Nika Haghtalab', 'Ritesh Noothigattu', 'Ariel D. Procaccia']|['cs.GT', 'cs.AI', 'cs.LG', 'cs.MA']
2017-03-28T14:05:45Z|2017-03-17T08:12:10Z|http://arxiv.org/abs/1703.04741v2|http://arxiv.org/pdf/1703.04741v2|Towards Moral Autonomous Systems|toward moral autonom system|Both the ethics of autonomous systems and the problems of their technical implementation have by now been studied in some detail. Less attention has been given to the areas in which these two separate concerns meet. This paper, written by both philosophers and engineers of autonomous systems, addresses a number of issues in machine ethics that are located at precisely the intersection between ethics and engineering. We first discuss different approaches towards the conceptual design of autonomous systems and their implications on the ethics implementation in such systems. Then we examine problematic areas regarding the specification and verification of ethical behavior in autonomous systems, particularly with a view towards the requirements of future legislation. We discuss transparency and accountability issues that will be crucial for any future wide deployment of autonomous systems in society. Finally we consider the, often overlooked, possibility of intentional misuse of AI systems and the possible dangers arising out of deliberately unethical design, implementation, and use of autonomous robots.|ethic autonom system problem technic implement studi detail less attent given area two separ concern meet paper written philosoph engin autonom system address number issu machin ethic locat precis intersect ethic engin first discuss differ approach toward conceptu design autonom system implic ethic implement system examin problemat area regard specif verif ethic behavior autonom system particular view toward requir futur legisl discuss transpar account issu crucial ani futur wide deploy autonom system societi final consid often overlook possibl intent misus ai system possibl danger aris deliber uneth design implement use autonom robot|['Vicky Charisi', 'Louise Dennis', 'Michael Fisher', 'Robert Lieck', 'Andreas Matthias', 'Marija Slavkovik', 'Janina Sombetzki', 'Alan F. T. Winfield', 'Roman Yampolskiy']|['cs.AI']
2017-03-28T14:05:45Z|2017-03-14T21:07:01Z|http://arxiv.org/abs/1703.04730v1|http://arxiv.org/pdf/1703.04730v1|Understanding Black-box Predictions via Influence Functions|understand black box predict via influenc function|How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, identifying the points most responsible for a given prediction. Applying ideas from second-order optimization, we scale up influence functions to modern machine learning settings and show that they can be applied to high-dimensional black-box models, even in non-convex and non-differentiable settings. We give a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for many different purposes: to understand model behavior, debug models and detect dataset errors, and even identify and exploit vulnerabilities to adversarial training-set attacks.|explain predict black box model paper use influenc function classic techniqu robust statist trace model predict learn algorithm back train data identifi point respons given predict appli idea second order optim scale influenc function modern machin learn set show appli high dimension black box model even non convex non differenti set give simpl effici implement requir onli oracl access gradient hessian vector product linear model convolut neural network demonstr influenc function use mani differ purpos understand model behavior debug model detect dataset error even identifi exploit vulner adversari train set attack|['Pang Wei Koh', 'Percy Liang']|['stat.ML', 'cs.AI', 'cs.LG']
2017-03-28T14:05:45Z|2017-03-14T19:14:32Z|http://arxiv.org/abs/1703.04677v1|http://arxiv.org/pdf/1703.04677v1|A computational investigation of sources of variability in sentence   comprehension difficulty in aphasia|comput investig sourc variabl sentenc comprehens difficulti aphasia|We present a computational evaluation of three hypotheses about sources of deficit in sentence comprehension in aphasia: slowed processing, intermittent deficiency, and resource reduction. The ACT-R based Lewis & Vasishth 2005 model is used to implement these three proposals. Slowed processing is implemented as slowed default production-rule firing time; intermittent deficiency as increased random noise in activation of chunks in memory; and resource reduction as reduced goal activation. As data, we considered subject vs. object relatives presented in a self-paced listening modality to 56 individuals with aphasia (IWA) and 46 matched controls. The participants heard the sentences and carried out a picture verification task to decide on an interpretation of the sentence. These response accuracies are used to identify the best parameters (for each participant) that correspond to the three hypotheses mentioned above. We show that controls have more tightly clustered (less variable) parameter values than IWA; specifically, compared to controls, among IWA there are more individuals with low goal activations, high noise, and slow default action times. This suggests that (i) individual patients show differential amounts of deficit along the three dimensions of slowed processing, intermittent deficient, and resource reduction, (ii) overall, there is evidence for all three sources of deficit playing a role, and (iii) IWA have a more variable range of parameter values than controls. In sum, this study contributes a proof of concept of a quantitative implementation of, and evidence for, these three accounts of comprehension deficits in aphasia.|present comput evalu three hypothes sourc deficit sentenc comprehens aphasia slow process intermitt defici resourc reduct act base lewi vasishth model use implement three propos slow process implement slow default product rule fire time intermitt defici increas random nois activ chunk memori resourc reduct reduc goal activ data consid subject vs object relat present self pace listen modal individu aphasia iwa match control particip heard sentenc carri pictur verif task decid interpret sentenc respons accuraci use identifi best paramet particip correspond three hypothes mention abov show control tight cluster less variabl paramet valu iwa specif compar control among iwa individu low goal activ high nois slow default action time suggest individu patient show differenti amount deficit along three dimens slow process intermitt defici resourc reduct ii overal evid three sourc deficit play role iii iwa variabl rang paramet valu control sum studi contribut proof concept quantit implement evid three account comprehens deficit aphasia|['Paul Mätzig', 'Shravan Vasishth', 'Felix Engelmann', 'David Caplan']|['cs.CL', 'cs.AI']
2017-03-28T14:05:50Z|2017-03-14T17:15:42Z|http://arxiv.org/abs/1703.04587v1|http://arxiv.org/pdf/1703.04587v1|Minimizing Maximum Regret in Commitment Constrained Sequential Decision   Making|minim maximum regret commit constrain sequenti decis make|In cooperative multiagent planning, it can often be beneficial for an agent to make commitments about aspects of its behavior to others, allowing them in turn to plan their own behaviors without taking the agent's detailed behavior into account. Extending previous work in the Bayesian setting, we consider instead a worst-case setting in which the agent has a set of possible environments (MDPs) it could be in, and develop a commitment semantics that allows for probabilistic guarantees on the agent's behavior in any of the environments it could end up facing. Crucially, an agent receives observations (of reward and state transitions) that allow it to potentially eliminate possible environments and thus obtain higher utility by adapting its policy to the history of observations. We develop algorithms and provide theory and some preliminary empirical results showing that they ensure an agent meets its commitments with history-dependent policies while minimizing maximum regret over the possible environments.|cooper multiag plan often benefici agent make commit aspect behavior allow turn plan behavior without take agent detail behavior account extend previous work bayesian set consid instead worst case set agent set possibl environ mdps could develop commit semant allow probabilist guarante agent behavior ani environ could end face crucial agent receiv observ reward state transit allow potenti elimin possibl environ thus obtain higher util adapt polici histori observ develop algorithm provid theori preliminari empir result show ensur agent meet commit histori depend polici minim maximum regret possibl environ|['Qi Zhang', 'Satinder Singh', 'Edmund Durfee']|['cs.AI']
2017-03-28T14:05:50Z|2017-03-13T17:58:36Z|http://arxiv.org/abs/1703.04529v1|http://arxiv.org/pdf/1703.04529v1|Task-based End-to-end Model Learning|task base end end model learn|As machine learning techniques have become more ubiquitous, it has become common to see machine learning prediction algorithms operating within some larger process. However, the criteria by which we train machine learning algorithms often differ from the ultimate criteria on which we evaluate them. This paper proposes an end-to-end approach for learning probabilistic machine learning models within the context of stochastic programming, in a manner that directly captures the ultimate task-based objective for which they will be used. We then present two experimental evaluations of the proposed approach, one as applied to a generic inventory stock problem and the second to a real-world electrical grid scheduling task. In both cases, we show that the proposed approach can outperform both a traditional modeling approach and a purely black-box policy optimization approach.|machin learn techniqu becom ubiquit becom common see machin learn predict algorithm oper within larger process howev criteria train machin learn algorithm often differ ultim criteria evalu paper propos end end approach learn probabilist machin learn model within context stochast program manner direct captur ultim task base object use present two experiment evalu propos approach one appli generic inventori stock problem second real world electr grid schedul task case show propos approach outperform tradit model approach pure black box polici optim approach|['Priya L. Donti', 'Brandon Amos', 'J. Zico Kolter']|['cs.LG', 'cs.AI']
2017-03-28T14:05:50Z|2017-03-13T17:34:18Z|http://arxiv.org/abs/1703.04498v1|http://arxiv.org/pdf/1703.04498v1|High-Throughput and Language-Agnostic Entity Disambiguation and Linking   on User Generated Data|high throughput languag agnost entiti disambigu link user generat data|The Entity Disambiguation and Linking (EDL) task matches entity mentions in text to a unique Knowledge Base (KB) identifier such as a Wikipedia or Freebase id. It plays a critical role in the construction of a high quality information network, and can be further leveraged for a variety of information retrieval and NLP tasks such as text categorization and document tagging. EDL is a complex and challenging problem due to ambiguity of the mentions and real world text being multi-lingual. Moreover, EDL systems need to have high throughput and should be lightweight in order to scale to large datasets and run on off-the-shelf machines. More importantly, these systems need to be able to extract and disambiguate dense annotations from the data in order to enable an Information Retrieval or Extraction task running on the data to be more efficient and accurate. In order to address all these challenges, we present the Lithium EDL system and algorithm - a high-throughput, lightweight, language-agnostic EDL system that extracts and correctly disambiguates 75% more entities than state-of-the-art EDL systems and is significantly faster than them.|entiti disambigu link edl task match entiti mention text uniqu knowledg base kb identifi wikipedia freebas id play critic role construct high qualiti inform network leverag varieti inform retriev nlp task text categor document tag edl complex challeng problem due ambigu mention real world text multi lingual moreov edl system need high throughput lightweight order scale larg dataset run shelf machin import system need abl extract disambigu dens annot data order enabl inform retriev extract task run data effici accur order address challeng present lithium edl system algorithm high throughput lightweight languag agnost edl system extract correct disambigu entiti state art edl system signific faster|['Preeti Bhargava', 'Nemanja Spasojevic', 'Guoning Hu']|['cs.IR', 'cs.AI', 'cs.CL']
2017-03-28T14:05:50Z|2017-03-13T17:13:51Z|http://arxiv.org/abs/1703.04489v1|http://arxiv.org/pdf/1703.04489v1|Reinforcement Learning for Transition-Based Mention Detection|reinforc learn transit base mention detect|This paper describes an application of reinforcement learning to the mention detection task. We define a novel action-based formulation for the mention detection task, in which a model can flexibly revise past labeling decisions by grouping together tokens and assigning partial mention labels. We devise a method to create mention-level episodes and we train a model by rewarding correctly labeled complete mentions, irrespective of the inner structure created. The model yields results which are on par with a competitive supervised counterpart while being more flexible in terms of achieving targeted behavior through reward modeling and generating internal mention structure, especially on longer mentions.|paper describ applic reinforc learn mention detect task defin novel action base formul mention detect task model flexibl revis past label decis group togeth token assign partial mention label devis method creat mention level episod train model reward correct label complet mention irrespect inner structur creat model yield result par competit supervis counterpart flexibl term achiev target behavior reward model generat intern mention structur especi longer mention|['Georgiana Dinu', 'Wael Hamza', 'Radu Florian']|['cs.CL', 'cs.AI']
2017-03-28T14:05:50Z|2017-03-13T13:45:13Z|http://arxiv.org/abs/1703.04389v1|http://arxiv.org/pdf/1703.04389v1|Bayesian Optimization with Gradients|bayesian optim gradient|In recent years, Bayesian optimization has proven successful for global optimization of expensive-to-evaluate multimodal objective functions. However, unlike most optimization methods, Bayesian optimization typically does not use derivative information. In this paper we show how Bayesian optimization can exploit derivative information to decrease the number of objective function evaluations required for good performance. In particular, we develop a novel Bayesian optimization algorithm, the derivative-enabled knowledge-gradient (dKG), for which we show one-step Bayes-optimality, asymptotic consistency, and greater one-step value of information than is possible in the derivative-free setting. Our procedure accommodates noisy and incomplete derivative information, and comes in both sequential and batch forms. We show dKG provides state-of-the-art performance compared to a wide range of optimization procedures with and without gradients, on benchmarks including logistic regression, kernel learning, and k-nearest neighbors.|recent year bayesian optim proven success global optim expens evalu multimod object function howev unlik optim method bayesian optim typic doe use deriv inform paper show bayesian optim exploit deriv inform decreas number object function evalu requir good perform particular develop novel bayesian optim algorithm deriv enabl knowledg gradient dkg show one step bay optim asymptot consist greater one step valu inform possibl deriv free set procedur accommod noisi incomplet deriv inform come sequenti batch form show dkg provid state art perform compar wide rang optim procedur without gradient benchmark includ logist regress kernel learn nearest neighbor|['Jian Wu', 'Matthias Poloczek', 'Andrew Gordon Wilson', 'Peter I. Frazier']|['stat.ML', 'cs.AI', 'cs.LG', 'math.OC']
2017-03-28T14:05:50Z|2017-03-13T13:32:46Z|http://arxiv.org/abs/1703.04382v1|http://arxiv.org/pdf/1703.04382v1|Cost-Based Intuitionist Probabilities on Spaces of Graphs, Hypergraphs   and Theorems|cost base intuitionist probabl space graph hypergraph theorem|A novel partial order is defined on the space of digraphs or hypergraphs, based on assessing the cost of producing a graph via a sequence of elementary transformations. Leveraging work by Knuth and Skilling on the foundations of inference, and the structure of Heyting algebras on graph space, this partial order is used to construct an intuitionistic probability measure that applies to either digraphs or hypergraphs. As logical inference steps can be represented as transformations on hypergraphs representing logical statements, this also yields an intuitionistic probability measure on spaces of theorems. The central result is also extended to yield intuitionistic probabilities based on more general weighted rule systems defined over bicartesian closed categories.|novel partial order defin space digraph hypergraph base assess cost produc graph via sequenc elementari transform leverag work knuth skill foundat infer structur heyt algebra graph space partial order use construct intuitionist probabl measur appli either digraph hypergraph logic infer step repres transform hypergraph repres logic statement also yield intuitionist probabl measur space theorem central result also extend yield intuitionist probabl base general weight rule system defin bicartesian close categori|['Ben Goertzel']|['cs.AI']
2017-03-28T14:05:50Z|2017-03-13T13:06:49Z|http://arxiv.org/abs/1703.04368v1|http://arxiv.org/pdf/1703.04368v1|Symbol Grounding via Chaining of Morphisms|symbol ground via chain morphism|"A new model of symbol grounding is presented, in which the structures of natural language, logical semantics, perception and action are represented categorically, and symbol grounding is modeled via the composition of morphisms between the relevant categories. This model gives conceptual insight into the fundamentally systematic nature of symbol grounding, and also connects naturally to practical real-world AI systems in current research and commercial use. Specifically, it is argued that the structure of linguistic syntax can be modeled as a certain asymmetric monoidal category, as e.g. implicit in the link grammar formalism; the structure of spatiotemporal relationships and action plans can be modeled similarly using ""image grammars"" and ""action grammars""; and common-sense logical semantic structure can be modeled using dependently-typed lambda calculus with uncertain truth values. Given these formalisms, the grounding of linguistic descriptions in spatiotemporal perceptions and coordinated actions consists of following morphisms from language to logic through to spacetime and body (for comprehension), and vice versa (for generation). The mapping is indicated between the spatial relationships in the Region Connection Calculus and Allen Interval Algebra and corresponding entries in the link grammar syntax parsing dictionary. Further, the abstractions introduced here are shown to naturally model the structures and systems currently being deployed in the context of using the OpenCog cognitive architecture to control Hanson Robotics humanoid robots."|new model symbol ground present structur natur languag logic semant percept action repres categor symbol ground model via composit morphism relev categori model give conceptu insight fundament systemat natur symbol ground also connect natur practic real world ai system current research commerci use specif argu structur linguist syntax model certain asymmetr monoid categori implicit link grammar formal structur spatiotempor relationship action plan model similar use imag grammar action grammar common sens logic semant structur model use depend type lambda calculus uncertain truth valu given formal ground linguist descript spatiotempor percept coordin action consist follow morphism languag logic spacetim bodi comprehens vice versa generat map indic spatial relationship region connect calculus allen interv algebra correspond entri link grammar syntax pars dictionari abstract introduc shown natur model structur system current deploy context use opencog cognit architectur control hanson robot humanoid robot|['Ruiting Lian', 'Ben Goertzel', 'Linas Vepstas', 'David Hanson', 'Changle Zhou']|['cs.AI']
2017-03-28T14:05:50Z|2017-03-13T12:49:20Z|http://arxiv.org/abs/1703.04363v1|http://arxiv.org/pdf/1703.04363v1|Deep Value Networks Learn to Evaluate and Iteratively Refine Structured   Outputs|deep valu network learn evalu iter refin structur output|We approach structured output prediction by learning a deep value network (DVN) that evaluates different output structures for a given input. For example, when applied to image segmentation, the value network takes an image and a segmentation mask as inputs and predicts a scalar score evaluating the mask quality and its correspondence with the image. Once the value network is optimized, at inference, it finds output structures that maximize the score of the value net via gradient descent on continuous relaxations of structured outputs. Thus DVN takes advantage of the joint modeling of the inputs and outputs. Our framework applies to a wide range of structured output prediction problems. We conduct experiments on multi-label classification based on text data and on image segmentation problems. DVN outperforms several strong baselines and the state-of-the-art results on these benchmarks. In addition, on image segmentation, the proposed deep value network learns complex shape priors and effectively combines image information with the prior to obtain competitive segmentation results.|approach structur output predict learn deep valu network dvn evalu differ output structur given input exampl appli imag segment valu network take imag segment mask input predict scalar score evalu mask qualiti correspond imag onc valu network optim infer find output structur maxim score valu net via gradient descent continu relax structur output thus dvn take advantag joint model input output framework appli wide rang structur output predict problem conduct experi multi label classif base text data imag segment problem dvn outperform sever strong baselin state art result benchmark addit imag segment propos deep valu network learn complex shape prior effect combin imag inform prior obtain competit segment result|['Michael Gygli', 'Mohammad Norouzi', 'Anelia Angelova']|['cs.LG', 'cs.AI', 'cs.CV']
2017-03-28T14:05:50Z|2017-03-13T12:48:15Z|http://arxiv.org/abs/1703.04361v1|http://arxiv.org/pdf/1703.04361v1|Toward a Formal Model of Cognitive Synergy|toward formal model cognit synergi|"""Cognitive synergy"" refers to a dynamic in which multiple cognitive processes, cooperating to control the same cognitive system, assist each other in overcoming bottlenecks encountered during their internal processing. Cognitive synergy has been posited as a key feature of real-world general intelligence, and has been used explicitly in the design of the OpenCog cognitive architecture. Here category theory and related concepts are used to give a formalization of the cognitive synergy concept.   A series of formal models of intelligent agents is proposed, with increasing specificity and complexity: simple reinforcement learning agents; ""cognit"" agents with an abstract memory and processing model; hypergraph-based agents (in which ""cognit"" operations are carried out via hypergraphs); hypergraph agents with a rich language of nodes and hyperlinks (such as the OpenCog framework provides); ""PGMC"" agents whose rich hypergraphs are endowed with cognitive processes guided via Probabilistic Growth and Mining of Combinations; and finally variations of the PrimeAGI design, which is currently being built on top of OpenCog.   A notion of cognitive synergy is developed for cognitive processes acting within PGMC agents, based on developing a formal notion of ""stuckness,"" and defining synergy as a relationship between cognitive processes in which they can help each other out when they get stuck. It is proposed that cognitive processes relating to each other synergetically, associate in a certain way with functors that map into each other via natural transformations. Cognitive synergy is proposed to correspond to a certain inequality regarding the relative costs of different paths through certain commutation diagrams.   Applications of this notion of cognitive synergy to particular cognitive phenomena, and specific cognitive processes in the PrimeAGI design, are discussed."|cognit synergi refer dynam multipl cognit process cooper control cognit system assist overcom bottleneck encount dure intern process cognit synergi posit key featur real world general intellig use explicit design opencog cognit architectur categori theori relat concept use give formal cognit synergi concept seri formal model intellig agent propos increas specif complex simpl reinforc learn agent cognit agent abstract memori process model hypergraph base agent cognit oper carri via hypergraph hypergraph agent rich languag node hyperlink opencog framework provid pgmc agent whose rich hypergraph endow cognit process guid via probabilist growth mine combin final variat primeagi design current built top opencog notion cognit synergi develop cognit process act within pgmc agent base develop formal notion stuck defin synergi relationship cognit process help get stuck propos cognit process relat synerget associ certain way functor map via natur transform cognit synergi propos correspond certain inequ regard relat cost differ path certain commut diagram applic notion cognit synergi particular cognit phenomena specif cognit process primeagi design discuss|['Ben Goertzel']|['cs.AI']
2017-03-28T14:05:50Z|2017-03-13T03:29:23Z|http://arxiv.org/abs/1703.04232v1|http://arxiv.org/pdf/1703.04232v1|Numerical Integration and Dynamic Discretization in Heuristic Search   Planning over Hybrid Domains|numer integr dynam discret heurist search plan hybrid domain|In this paper we look into the problem of planning over hybrid domains, where change can be both discrete and instantaneous, or continuous over time. In addition, it is required that each state on the trajectory induced by the execution of plans complies with a given set of global constraints. We approach the computation of plans for such domains as the problem of searching over a deterministic state model. In this model, some of the successor states are obtained by solving numerically the so-called initial value problem over a set of ordinary differential equations (ODE) given by the current plan prefix. These equations hold over time intervals whose duration is determined dynamically, according to whether zero crossing events take place for a set of invariant conditions. The resulting planner, FS+, incorporates these features together with effective heuristic guidance. FS+ does not impose any of the syntactic restrictions on process effects often found on the existing literature on Hybrid Planning. A key concept of our approach is that a clear separation is struck between planning and simulation time steps. The former is the time allowed to observe the evolution of a given dynamical system before committing to a future course of action, whilst the later is part of the model of the environment. FS+ is shown to be a robust planner over a diverse set of hybrid domains, taken from the existing literature on hybrid planning and systems.|paper look problem plan hybrid domain chang discret instantan continu time addit requir state trajectori induc execut plan compli given set global constraint approach comput plan domain problem search determinist state model model successor state obtain solv numer call initi valu problem set ordinari differenti equat ode given current plan prefix equat hold time interv whose durat determin dynam accord whether zero cross event take place set invari condit result planner fs incorpor featur togeth effect heurist guidanc fs doe impos ani syntact restrict process effect often found exist literatur hybrid plan key concept approach clear separ struck plan simul time step former time allow observ evolut given dynam system befor commit futur cours action whilst later part model environ fs shown robust planner divers set hybrid domain taken exist literatur hybrid plan system|['Miquel Ramirez', 'Enrico Scala', 'Patrik Haslum', 'Sylvie Thiebaux']|['cs.AI', 'I.2.8; F.2.2; I.6; J.2']
2017-03-28T14:05:54Z|2017-03-13T01:49:27Z|http://arxiv.org/abs/1703.04221v1|http://arxiv.org/pdf/1703.04221v1|A Hierarchical Framework of Cloud Resource Allocation and Power   Management Using Deep Reinforcement Learning|hierarch framework cloud resourc alloc power manag use deep reinforc learn|Automatic decision-making approaches, such as reinforcement learning (RL), have been applied to (partially) solve the resource allocation problem adaptively in the cloud computing system. However, a complete cloud resource allocation framework exhibits high dimensions in state and action spaces, which prohibit the usefulness of traditional RL techniques. In addition, high power consumption has become one of the critical concerns in design and control of cloud computing systems, which degrades system reliability and increases cooling cost. An effective dynamic power management (DPM) policy should minimize power consumption while maintaining performance degradation within an acceptable level. Thus, a joint virtual machine (VM) resource allocation and power management framework is critical to the overall cloud computing system. Moreover, novel solution framework is necessary to address the even higher dimensions in state and action spaces. In this paper, we propose a novel hierarchical framework for solving the overall resource allocation and power management problem in cloud computing systems. The proposed hierarchical framework comprises a global tier for VM resource allocation to the servers and a local tier for distributed power management of local servers. The emerging deep reinforcement learning (DRL) technique, which can deal with complicated control problems with large state space, is adopted to solve the global tier problem. Furthermore, an autoencoder and a novel weight sharing structure are adopted to handle the high-dimensional state space and accelerate the convergence speed. On the other hand, the local tier of distributed server power managements comprises an LSTM based workload predictor and a model-free RL based power manager, operating in a distributed manner.|automat decis make approach reinforc learn rl appli partial solv resourc alloc problem adapt cloud comput system howev complet cloud resourc alloc framework exhibit high dimens state action space prohibit use tradit rl techniqu addit high power consumpt becom one critic concern design control cloud comput system degrad system reliabl increas cool cost effect dynam power manag dpm polici minim power consumpt maintain perform degrad within accept level thus joint virtual machin vm resourc alloc power manag framework critic overal cloud comput system moreov novel solut framework necessari address even higher dimens state action space paper propos novel hierarch framework solv overal resourc alloc power manag problem cloud comput system propos hierarch framework compris global tier vm resourc alloc server local tier distribut power manag local server emerg deep reinforc learn drl techniqu deal complic control problem larg state space adopt solv global tier problem furthermor autoencod novel weight share structur adopt handl high dimension state space acceler converg speed hand local tier distribut server power manag compris lstm base workload predictor model free rl base power manag oper distribut manner|['Ning Liu', 'Zhe Li', 'Zhiyuan Xu', 'Jielong Xu', 'Sheng Lin', 'Qinru Qiu', 'Jian Tang', 'Yanzhi Wang']|['cs.DC', 'cs.AI']
2017-03-28T14:05:54Z|2017-03-15T08:16:37Z|http://arxiv.org/abs/1703.04159v2|http://arxiv.org/pdf/1703.04159v2|Any-Angle Pathfinding for Multiple Agents Based on SIPP Algorithm|ani angl pathfind multipl agent base sipp algorithm|The problem of finding conflict-free trajectories for multiple agents of identical circular shape, operating in shared 2D workspace, is addressed in the paper and decoupled, e.g., prioritized, approach is used to solve this problem. Agents' workspace is tessellated into the square grid on which any-angle moves are allowed, e.g. each agent can move into an arbitrary direction as long as this move follows the straight line segment whose endpoints are tied to the distinct grid elements. A novel any-angle planner based on Safe Interval Path Planning (SIPP) algorithm is proposed to find trajectories for an agent moving amidst dynamic obstacles (other agents) on a grid. This algorithm is then used as part of a prioritized multi-agent planner AA-SIPP(m). On the theoretical, side we show that AA-SIPP(m) is complete under well-defined conditions. On the experimental side, in simulation tests with up to 200 agents involved, we show that our planner finds much better solutions in terms of cost (up to 20%) compared to the planners relying on cardinal moves only.|problem find conflict free trajectori multipl agent ident circular shape oper share workspac address paper decoupl priorit approach use solv problem agent workspac tessel squar grid ani angl move allow agent move arbitrari direct long move follow straight line segment whose endpoint tie distinct grid element novel ani angl planner base safe interv path plan sipp algorithm propos find trajectori agent move amidst dynam obstacl agent grid algorithm use part priorit multi agent planner aa sipp theoret side show aa sipp complet well defin condit experiment side simul test agent involv show planner find much better solut term cost compar planner reli cardin move onli|['Konstantin Yakovlev', 'Anton Andreychuk']|['cs.AI']
2017-03-28T14:05:54Z|2017-03-12T13:17:08Z|http://arxiv.org/abs/1703.04115v1|http://arxiv.org/pdf/1703.04115v1|BetaRun 2017 Team Description Paper: Variety, Complexity, and Learning|betarun team descript paper varieti complex learn|RoboCup offers a set of benchmark problems for Artificial Intelligence in form of official world championships since 1997. The most tactical advanced and richest in terms of behavioural complexity of these is the 2D Soccer Simulation League, a simulated robotic soccer competition. BetaRun is a new attempt combining both machine learning and manual programming approaches, with the ultimate goal to arrive at a team that is trained entirely from observing and playing games, and a successor of the World Champion team Gliders 2016.|robocup offer set benchmark problem artifici intellig form offici world championship sinc tactic advanc richest term behaviour complex soccer simul leagu simul robot soccer competit betarun new attempt combin machin learn manual program approach ultim goal arriv team train entir observ play game successor world champion team glider|['Olivia Michael', 'Oliver Obst']|['cs.AI']
2017-03-28T14:05:54Z|2017-03-25T03:21:57Z|http://arxiv.org/abs/1703.04071v2|http://arxiv.org/pdf/1703.04071v2|A Compact DNN: Approaching GoogLeNet-Level Accuracy of Classification   and Domain Adaptation|compact dnn approach googlenet level accuraci classif domain adapt|Recently, DNN model compression based on network architecture design, e.g., SqueezeNet, attracted a lot attention. No accuracy drop on image classification is observed on these extremely compact networks, compared to well-known models. An emerging question, however, is whether these model compression techniques hurt DNN's learning ability other than classifying images on a single dataset. Our preliminary experiment shows that these compression methods could degrade domain adaptation (DA) ability, though the classification performance is preserved. Therefore, we propose a new compact network architecture and unsupervised DA method in this paper. The DNN is built on a new basic module Conv-M which provides more diverse feature extractors without significantly increasing parameters. The unified framework of our DA method will simultaneously learn invariance across domains, reduce divergence of feature representations, and adapt label prediction. Our DNN has 4.1M parameters, which is only 6.7% of AlexNet or 59% of GoogLeNet. Experiments show that our DNN obtains GoogLeNet-level accuracy both on classification and DA, and our DA method slightly outperforms previous competitive ones. Put all together, our DA strategy based on our DNN achieves state-of-the-art on sixteen of total eighteen DA tasks on popular Office-31 and Office-Caltech datasets.|recent dnn model compress base network architectur design squeezenet attract lot attent accuraci drop imag classif observ extrem compact network compar well known model emerg question howev whether model compress techniqu hurt dnn learn abil classifi imag singl dataset preliminari experi show compress method could degrad domain adapt da abil though classif perform preserv therefor propos new compact network architectur unsupervis da method paper dnn built new basic modul conv provid divers featur extractor without signific increas paramet unifi framework da method simultan learn invari across domain reduc diverg featur represent adapt label predict dnn paramet onli alexnet googlenet experi show dnn obtain googlenet level accuraci classif da da method slight outperform previous competit one put togeth da strategi base dnn achiev state art sixteen total eighteen da task popular offic offic caltech dataset|['Chunpeng Wu', 'Wei Wen', 'Tariq Afzal', 'Yongmei Zhang', 'Yiran Chen', 'Hai Li']|['cs.CV', 'cs.AI', 'cs.NE']
2017-03-28T14:05:54Z|2017-03-11T20:24:06Z|http://arxiv.org/abs/1703.04565v1|http://arxiv.org/pdf/1703.04565v1|Fuzzy Model Tree For Early Effort Estimation|fuzzi model tree earli effort estim|Use Case Points (UCP) is a well-known method to estimate the project size, based on Use Case diagram, at early phases of software development. Although the Use Case diagram is widely accepted as a de-facto model for analyzing object oriented software requirements over the world, UCP method did not take sufficient amount of attention because, as yet, there is no consensus on how to produce software effort from UCP. This paper aims to study the potential of using Fuzzy Model Tree to derive effort estimates based on UCP size measure using a dataset collected for that purpose. The proposed approach has been validated against Treeboost model, Multiple Linear Regression and classical effort estimation based on the UCP model. The obtained results are promising and show better performance than those obtained by classical UCP, Multiple Linear Regression and slightly better than those obtained by Tree boost model.|use case point ucp well known method estim project size base use case diagram earli phase softwar develop although use case diagram wide accept de facto model analyz object orient softwar requir world ucp method take suffici amount attent becaus yet consensus produc softwar effort ucp paper aim studi potenti use fuzzi model tree deriv effort estim base ucp size measur use dataset collect purpos propos approach valid treeboost model multipl linear regress classic effort estim base ucp model obtain result promis show better perform obtain classic ucp multipl linear regress slight better obtain tree boost model|['Mohammad Azzeh', 'Ali Bou Nassif']|['cs.SE', 'cs.AI']
2017-03-28T14:05:54Z|2017-03-11T20:19:05Z|http://arxiv.org/abs/1703.04567v1|http://arxiv.org/pdf/1703.04567v1|Learning best K analogies from data distribution for case-based software   effort estimation|learn best analog data distribut case base softwar effort estim|Case-Based Reasoning (CBR) has been widely used to generate good software effort estimates. The predictive performance of CBR is a dataset dependent and subject to extremely large space of configuration possibilities. Regardless of the type of adaptation technique, deciding on the optimal number of similar cases to be used before applying CBR is a key challenge. In this paper we propose a new technique based on Bisecting k-medoids clustering algorithm to better understanding the structure of a dataset and discovering the the optimal cases for each individual project by excluding irrelevant cases. Results obtained showed that understanding of the data characteristic prior prediction stage can help in automatically finding the best number of cases for each test project. Performance figures of the proposed estimation method are better than those of other regular K-based CBR methods.|case base reason cbr wide use generat good softwar effort estim predict perform cbr dataset depend subject extrem larg space configur possibl regardless type adapt techniqu decid optim number similar case use befor appli cbr key challeng paper propos new techniqu base bisect medoid cluster algorithm better understand structur dataset discov optim case individu project exclud irrelev case result obtain show understand data characterist prior predict stage help automat find best number case test project perform figur propos estim method better regular base cbr method|['Mohammad Azzeh', 'Yousef Elsheikh']|['cs.SE', 'cs.AI']
2017-03-28T14:05:54Z|2017-03-11T09:08:48Z|http://arxiv.org/abs/1703.03933v1|http://arxiv.org/pdf/1703.03933v1|Micro-Objective Learning : Accelerating Deep Reinforcement Learning   through the Discovery of Continuous Subgoals|micro object learn acceler deep reinforc learn discoveri continu subgoal|Recently, reinforcement learning has been successfully applied to the logical game of Go, various Atari games, and even a 3D game, Labyrinth, though it continues to have problems in sparse reward settings. It is difficult to explore, but also difficult to exploit, a small number of successes when learning policy. To solve this issue, the subgoal and option framework have been proposed. However, discovering subgoals online is too expensive to be used to learn options in large state spaces. We propose Micro-objective learning (MOL) to solve this problem. The main idea is to estimate how important a state is while training and to give an additional reward proportional to its importance. We evaluated our algorithm in two Atari games: Montezuma's Revenge and Seaquest. With three experiments to each game, MOL significantly improved the baseline scores. Especially in Montezuma's Revenge, MOL achieved two times better results than the previous state-of-the-art model.|recent reinforc learn success appli logic game go various atari game even game labyrinth though continu problem spars reward set difficult explor also difficult exploit small number success learn polici solv issu subgoal option framework propos howev discov subgoal onlin expens use learn option larg state space propos micro object learn mol solv problem main idea estim import state train give addit reward proport import evalu algorithm two atari game montezuma reveng seaquest three experi game mol signific improv baselin score especi montezuma reveng mol achiev two time better result previous state art model|['Sungtae Lee', 'Sang-Woo Lee', 'Jinyoung Choi', 'Dong-Hyun Kwak', 'Byoung-Tak Zhang']|['cs.AI']
2017-03-28T14:05:54Z|2017-03-11T07:46:51Z|http://arxiv.org/abs/1703.03924v1|http://arxiv.org/pdf/1703.03924v1|Real-Time Machine Learning: The Missing Pieces|real time machin learn miss piec|Machine learning applications are increasingly deployed not only to serve predictions using static models, but also as tightly-integrated components of feedback loops involving dynamic, real-time decision making. These applications pose a new set of requirements, none of which are difficult to achieve in isolation, but the combination of which creates a challenge for existing distributed execution frameworks: computation with millisecond latency at high throughput, adaptive construction of arbitrary task graphs, and execution of heterogeneous kernels over diverse sets of resources. We assert that a new distributed execution framework is needed for such ML applications and propose a candidate approach with a proof-of-concept architecture that achieves a 63x performance improvement over a state-of-the-art execution framework for a representative application.|machin learn applic increas deploy onli serv predict use static model also tight integr compon feedback loop involv dynam real time decis make applic pose new set requir none difficult achiev isol combin creat challeng exist distribut execut framework comput millisecond latenc high throughput adapt construct arbitrari task graph execut heterogen kernel divers set resourc assert new distribut execut framework need ml applic propos candid approach proof concept architectur achiev perform improv state art execut framework repres applic|['Robert Nishihara', 'Philipp Moritz', 'Stephanie Wang', 'Alexey Tumanov', 'William Paul', 'Johann Schleier-Smith', 'Richard Liaw', 'Michael I. Jordan', 'Ion Stoica']|['cs.DC', 'cs.AI', 'cs.LG']
2017-03-28T14:05:54Z|2017-03-11T06:37:09Z|http://arxiv.org/abs/1703.03916v1|http://arxiv.org/pdf/1703.03916v1|Axioms in Model-based Planners|axiom model base planner|Axioms can be used to model derived predicates in domain- independent planning models. Formulating models which use axioms can sometimes result in problems with much smaller search spaces and shorter plans than the original model. Previous work on axiom-aware planners focused solely on state- space search planners. We propose axiom-aware planners based on answer set programming and integer programming. We evaluate them on PDDL domains with axioms and show that they can exploit additional expressivity of axioms.|axiom use model deriv predic domain independ plan model formul model use axiom sometim result problem much smaller search space shorter plan origin model previous work axiom awar planner focus sole state space search planner propos axiom awar planner base answer set program integ program evalu pddl domain axiom show exploit addit express axiom|['Shuwa Miura', 'Alex Fukunaga']|['cs.AI']
2017-03-28T14:05:54Z|2017-03-11T05:35:09Z|http://arxiv.org/abs/1703.03912v1|http://arxiv.org/pdf/1703.03912v1|The Curse of Correlation in Security Games and Principle of Max-Entropy|curs correl secur game principl max entropi|In this paper, we identify and study a fundamental, yet underexplored, phenomenon in security games, which we term the Curse of Correlation (CoC). Specifically, we observe that there is inevitable correlation among the protection status of different targets. Such correlation is a crucial concern, especially in spatio-temporal domains like conservation area patrolling, where attackers can monitor patrollers at certain areas and then infer their patrolling routes using such correlation. To mitigate this issue, we introduce the principle of max-entropy to security games, and focus on designing entropy-maximizing defending strategies for the spatio-temporal security game -- a major victim of CoC. We prove that the problem is #P-hard in general, but propose efficient algorithms in well-motivated special settings. Our experiments show significant advantages of the max-entropy algorithms against previous algorithms.|paper identifi studi fundament yet underexplor phenomenon secur game term curs correl coc specif observ inevit correl among protect status differ target correl crucial concern especi spatio tempor domain like conserv area patrol attack monitor patrol certain area infer patrol rout use correl mitig issu introduc principl max entropi secur game focus design entropi maxim defend strategi spatio tempor secur game major victim coc prove problem hard general propos effici algorithm well motiv special set experi show signific advantag max entropi algorithm previous algorithm|['Haifeng Xu', 'Milind Tambe', 'Shaddin Dughmi', 'Venil Loyd Noronha']|['cs.GT', 'cs.AI', 'cs.CR']
2017-04-07T11:26:31Z|2017-04-06T17:29:16Z|http://arxiv.org/abs/1704.01946v1|http://arxiv.org/pdf/1704.01946v1|From Data to City Indicators: A Knowledge Graph for Supporting Automatic   Generation of Dashboards|data citi indic knowledg graph support automat generat dashboard|In the context of Smart Cities, indicator definitions have been used to calculate values that enable the comparison among different cities. The calculation of an indicator values has challenges as the calculation may need to combine some aspects of quality while addressing different levels of abstraction. Knowledge graphs (KGs) have been used successfully to support flexible representation, which can support improved understanding and data analysis in similar settings. This paper presents an operational description for a city KG, an indicator ontology that support indicator discovery and data visualization and an application capable of performing metadata analysis to automatically build and display dashboards according to discovered indicators. We describe our implementation in an urban mobility setting.|context smart citi indic definit use calcul valu enabl comparison among differ citi calcul indic valu challeng calcul may need combin aspect qualiti address differ level abstract knowledg graph kgs use success support flexibl represent support improv understand data analysi similar set paper present oper descript citi kg indic ontolog support indic discoveri data visual applic capabl perform metadata analysi automat build display dashboard accord discov indic describ implement urban mobil set|['Henrique Santos', 'Victor Dantas', 'Vasco Furtado', 'Paulo Pinheiro', 'Deborah L. McGuinness']|['cs.AI', 'cs.CY', 'I.2.4']
2017-04-07T11:26:31Z|2017-04-06T17:25:39Z|http://arxiv.org/abs/1704.01944v1|http://arxiv.org/pdf/1704.01944v1|The quality of priority ratios estimation in relation to a selected   prioritization procedure and consistency measure for a Pairwise Comparison   Matrix|qualiti prioriti ratio estim relat select priorit procedur consist measur pairwis comparison matrix|An overview of current debates and contemporary research devoted to the modeling of decision making processes and their facilitation directs attention to the Analytic Hierarchy Process (AHP). At the core of the AHP are various prioritization procedures (PPs) and consistency measures (CMs) for a Pairwise Comparison Matrix (PCM) which, in a sense, reflects preferences of decision makers. Certainly, when judgments about these preferences are perfectly consistent (cardinally transitive), all PPs coincide and the quality of the priority ratios (PRs) estimation is exemplary. However, human judgments are very rarely consistent, thus the quality of PRs estimation may significantly vary. The scale of these variations depends on the applied PP and utilized CM for a PCM. This is why it is important to find out which PPs and which CMs for a PCM lead directly to an improvement of the PRs estimation accuracy. The main goal of this research is realized through the properly designed, coded and executed seminal and sophisticated simulation algorithms in Wolfram Mathematica 8.0. These research results convince that the embedded in the AHP and commonly applied, both genuine PP and CM for PCM may significantly deteriorate the quality of PRs estimation; however, solutions proposed in this paper can significantly improve the methodology.|overview current debat contemporari research devot model decis make process facilit direct attent analyt hierarchi process ahp core ahp various priorit procedur pps consist measur cms pairwis comparison matrix pcm sens reflect prefer decis maker certain judgment prefer perfect consist cardin transit pps coincid qualiti prioriti ratio prs estim exemplari howev human judgment veri rare consist thus qualiti prs estim may signific vari scale variat depend appli pp util cm pcm whi import find pps cms pcm lead direct improv prs estim accuraci main goal research realiz proper design code execut semin sophist simul algorithm wolfram mathematica research result convinc embed ahp common appli genuin pp cm pcm may signific deterior qualiti prs estim howev solut propos paper signific improv methodolog|['Paul Thaddeus Kazibudzki']|['cs.AI']
2017-04-07T11:26:31Z|2017-04-06T16:37:15Z|http://arxiv.org/abs/1704.01920v1|http://arxiv.org/pdf/1704.01920v1|Encoder Based Lifelong Learning|encod base lifelong learn|This paper introduces a new lifelong learning solution where a single model is trained for a sequence of tasks. The main challenge that vision systems face in this context is catastrophic forgetting: as they tend to adapt to the most recently seen task, they lose performance on the tasks that were learned previously. Our method aims at preserving the knowledge of the previous tasks while learning a new one by using autoencoders. For each task, an under-complete autoencoder is learned, capturing the features that are crucial for its achievement. When a new task is presented to the system, we prevent the reconstructions of the features with these autoencoders from changing, which has the effect of preserving the information on which the previous tasks are mainly relying. At the same time, the features are given space to adjust to the most recent environment as only their projection into a low dimension submanifold is controlled. The proposed system is evaluated on image classification tasks and shows a reduction of forgetting over the state-of-the-art|paper introduc new lifelong learn solut singl model train sequenc task main challeng vision system face context catastroph forget tend adapt recent seen task lose perform task learn previous method aim preserv knowledg previous task learn new one use autoencod task complet autoencod learn captur featur crucial achiev new task present system prevent reconstruct featur autoencod chang effect preserv inform previous task main reli time featur given space adjust recent environ onli project low dimens submanifold control propos system evalu imag classif task show reduct forget state art|['Amal Rannen Triki', 'Rahaf Aljundi', 'Mathew B. Blaschko', 'Tinne Tuytelaars']|['cs.CV', 'cs.AI', 'stat.ML']
2017-04-07T11:26:31Z|2017-04-06T15:44:29Z|http://arxiv.org/abs/1704.01897v1|http://arxiv.org/abs/1704.01897v1|Online Hashing|onlin hash|Although hash function learning algorithms have achieved great success in recent years, most existing hash models are off-line, which are not suitable for processing sequential or online data. To address this problem, this work proposes an online hash model to accommodate data coming in stream for online learning. Specifically, a new loss function is proposed to measure the similarity loss between a pair of data samples in hamming space. Then, a structured hash model is derived and optimized in a passive-aggressive way. Theoretical analysis on the upper bound of the cumulative loss for the proposed online hash model is provided. Furthermore, we extend our online hashing from a single-model to a multi-model online hashing that trains multiple models so as to retain diverse online hashing models in order to avoid biased update. The competitive efficiency and effectiveness of the proposed online hash models are verified through extensive experiments on several large-scale datasets as compared to related hashing methods.|although hash function learn algorithm achiev great success recent year exist hash model line suitabl process sequenti onlin data address problem work propos onlin hash model accommod data come stream onlin learn specif new loss function propos measur similar loss pair data sampl ham space structur hash model deriv optim passiv aggress way theoret analysi upper bound cumul loss propos onlin hash model provid furthermor extend onlin hash singl model multi model onlin hash train multipl model retain divers onlin hash model order avoid bias updat competit effici effect propos onlin hash model verifi extens experi sever larg scale dataset compar relat hash method|['Long-Kai Huang', 'Qiang Yang', 'Wei-Shi Zheng']|['cs.CV', 'cs.AI']
2017-04-07T11:26:31Z|2017-04-06T15:31:48Z|http://arxiv.org/abs/1704.01889v1|http://arxiv.org/pdf/1704.01889v1|Conformative Filtering for Implicit Feedback Data|conform filter implicit feedback data|Implicit feedback is the simplest form of user feedback that can be used for item recommendation. It is easy to collect and domain independent. However, there is a lack of negative examples. Existing works circumvent this problem by making various assumptions regarding the unconsumed items, which fail to hold when the user did not consume an item because she was unaware of it. In this paper we propose Conformative Filtering (CoF) as a novel method for addressing the lack of negative examples in implicit feedback. The motivation is that if there is a large group of users who share the same taste and none of them consumed an item, then it is highly likely that the item is irrelevant to this taste. We use Hierarchical Latent Tree Analysis (HLTA) to identify taste-based user groups, and make recommendations for a user based on her memberships in the groups. Experiments on real-world datasets from different domains show that CoF has superior performance compared to other baselines and more than 10% improvement in Recall@5 and Recall@10 is observed.|implicit feedback simplest form user feedback use item recommend easi collect domain independ howev lack negat exampl exist work circumv problem make various assumpt regard unconsum item fail hold user consum item becaus unawar paper propos conform filter cof novel method address lack negat exampl implicit feedback motiv larg group user share tast none consum item high like item irrelev tast use hierarch latent tree analysi hlta identifi tast base user group make recommend user base membership group experi real world dataset differ domain show cof superior perform compar baselin improv recal recal observ|['Farhan Khawar', 'Nevin L. Zhang', 'Jinxing Yu']|['cs.IR', 'cs.AI']
2017-04-07T11:26:31Z|2017-04-06T15:16:03Z|http://arxiv.org/abs/1704.01886v1|http://arxiv.org/pdf/1704.01886v1|Landmark Guided Probabilistic Roadmap Queries|landmark guid probabilist roadmap queri|A landmark based heuristic is investigated for reducing query phase run-time of the probabilistic roadmap (\PRM) motion planning method. The heuristic is generated by storing minimum spanning trees from a small number of vertices within the \PRM graph and using these trees to approximate the cost of a shortest path between any two vertices of the graph. The intermediate step of preprocessing the graph increases the time and memory requirements of the classical motion planning technique in exchange for speeding up individual queries making the method advantageous in multi-query applications. This paper investigates these trade-offs on \PRM graphs constructed in randomized environments as well as a practical manipulator simulation.We conclude that the method is preferable to Dijkstra's algorithm or the ${\rm A}^*$ algorithm with conventional heuristics in multi-query applications.|landmark base heurist investig reduc queri phase run time probabilist roadmap prm motion plan method heurist generat store minimum span tree small number vertic within prm graph use tree approxim cost shortest path ani two vertic graph intermedi step preprocess graph increas time memori requir classic motion plan techniqu exchang speed individu queri make method advantag multi queri applic paper investig trade prm graph construct random environ well practic manipul simul conclud method prefer dijkstra algorithm rm algorithm convent heurist multi queri applic|['Brian Paden', 'Yannik Nager', 'Emilio Frazzoli']|['cs.RO', 'cs.AI']
2017-04-07T11:26:31Z|2017-04-06T14:39:54Z|http://arxiv.org/abs/1704.01864v1|http://arxiv.org/pdf/1704.01864v1|Robust Causal Estimation in the Large-Sample Limit without Strict   Faithfulness|robust causal estim larg sampl limit without strict faith|Causal effect estimation from observational data is an important and much studied research topic. The instrumental variable (IV) and local causal discovery (LCD) patterns are canonical examples of settings where a closed-form expression exists for the causal effect of one variable on another, given the presence of a third variable. Both rely on faithfulness to infer that the latter only influences the target effect via the cause variable. In reality, it is likely that this assumption only holds approximately and that there will be at least some form of weak interaction. This brings about the paradoxical situation that, in the large-sample limit, no predictions are made, as detecting the weak edge invalidates the setting. We introduce an alternative approach by replacing strict faithfulness with a prior that reflects the existence of many 'weak' (irrelevant) and 'strong' interactions. We obtain a posterior distribution over the target causal effect estimator which shows that, in many cases, we can still make good estimates. We demonstrate the approach in an application on a simple linear-Gaussian setting, using the MultiNest sampling algorithm, and compare it with established techniques to show our method is robust even when strict faithfulness is violated.|causal effect estim observ data import much studi research topic instrument variabl iv local causal discoveri lcd pattern canon exampl set close form express exist causal effect one variabl anoth given presenc third variabl reli faith infer latter onli influenc target effect via caus variabl realiti like assumpt onli hold approxim least form weak interact bring paradox situat larg sampl limit predict made detect weak edg invalid set introduc altern approach replac strict faith prior reflect exist mani weak irrelev strong interact obtain posterior distribut target causal effect estim show mani case still make good estim demonstr approach applic simpl linear gaussian set use multinest sampl algorithm compar establish techniqu show method robust even strict faith violat|['Ioan Gabriel Bucur', 'Tom Claassen', 'Tom Heskes']|['stat.ML', 'cs.AI', 'stat.ME']
2017-04-07T11:26:31Z|2017-04-06T14:29:14Z|http://arxiv.org/abs/1704.01859v1|http://arxiv.org/pdf/1704.01859v1|Tackling Dynamic Vehicle Routing Problem with Time Windows by means of   Ant Colony System|tackl dynam vehicl rout problem time window mean ant coloni system|The Dynamic Vehicle Routing Problem with Time Windows (DVRPTW) is an extension of the well-known Vehicle Routing Problem (VRP), which takes into account the dynamic nature of the problem. This aspect requires the vehicle routes to be updated in an ongoing manner as new customer requests arrive in the system and must be incorporated into an evolving schedule during the working day. Besides the vehicle capacity constraint involved in the classical VRP, DVRPTW considers in addition time windows, which are able to better capture real-world situations. Despite this, so far, few studies have focused on tackling this problem of greater practical importance. To this end, this study devises for the resolution of DVRPTW, an ant colony optimization based algorithm, which resorts to a joint solution construction mechanism, able to construct in parallel the vehicle routes. This method is coupled with a local search procedure, aimed to further improve the solutions built by ants, and with an insertion heuristics, which tries to reduce the number of vehicles used to service the available customers. The experiments indicate that the proposed algorithm is competitive and effective, and on DVRPTW instances with a higher dynamicity level, it is able to yield better results compared to existing ant-based approaches.|dynam vehicl rout problem time window dvrptw extens well known vehicl rout problem vrp take account dynam natur problem aspect requir vehicl rout updat ongo manner new custom request arriv system must incorpor evolv schedul dure work day besid vehicl capac constraint involv classic vrp dvrptw consid addit time window abl better captur real world situat despit far studi focus tackl problem greater practic import end studi devis resolut dvrptw ant coloni optim base algorithm resort joint solut construct mechan abl construct parallel vehicl rout method coupl local search procedur aim improv solut built ant insert heurist tri reduc number vehicl use servic avail custom experi indic propos algorithm competit effect dvrptw instanc higher dynam level abl yield better result compar exist ant base approach|['Raluca Necula', 'Mihaela Breaban', 'Madalina Raschip']|['cs.NE', 'cs.AI']
2017-04-07T11:26:31Z|2017-04-06T14:22:56Z|http://arxiv.org/abs/1704.01855v1|http://arxiv.org/abs/1704.01855v1|A Service-Oriented Architecture for Assisting the Authoring of Semantic   Crowd Maps|servic orient architectur assist author semant crowd map|Although there are increasingly more initiatives for the generation of semantic knowledge based on user participation, there is still a shortage of platforms for regular users to create applications on which semantic data can be exploited and generated automatically. We propose an architecture, called Semantic Maps (SeMaps), for assisting the authoring and hosting of applications in which the maps combine the aggregation of a Geographic Information System and crowd-generated content (called here crowd maps). In these systems, the digital map works as a blackboard for accommodating stories told by people about events they want to share with others typically participating in their social networks. SeMaps offers an environment for the creation and maintenance of sites based on crowd maps with the possibility for the user to characterize semantically that which s/he intends to mark on the map. The designer of a crowd map, by informing a linguistic expression that designates what has to be marked on the maps, is guided in a process that aims to associate a concept from a common-sense base to this linguistic expression. Thus, the crowd maps start to have dominion over common-sense inferential relations that define the meaning of the marker, and are able to make inferences about the network of linked data. This makes it possible to generate maps that have the power to perform inferences and access external sources (such as DBpedia) that constitute information that is useful and appropriate to the context of the map. In this paper we describe the architecture of SeMaps and how it was applied in a crowd map authoring tool.|although increas initi generat semant knowledg base user particip still shortag platform regular user creat applic semant data exploit generat automat propos architectur call semant map semap assist author host applic map combin aggreg geograph inform system crowd generat content call crowd map system digit map work blackboard accommod stori told peopl event want share typic particip social network semap offer environ creation mainten site base crowd map possibl user character semant intend mark map design crowd map inform linguist express design mark map guid process aim associ concept common sens base linguist express thus crowd map start dominion common sens inferenti relat defin mean marker abl make infer network link data make possibl generat map power perform infer access extern sourc dbpedia constitut inform use appropri context map paper describ architectur semap appli crowd map author tool|['Henrique Santos', 'Vasco Furtado']|['cs.AI', 'cs.CY', 'I.2.4']
2017-04-07T11:26:31Z|2017-04-06T13:03:05Z|http://arxiv.org/abs/1704.01815v1|http://arxiv.org/pdf/1704.01815v1|Incremental Transductive Learning Approaches to Schistosomiasis Vector   Classification|increment transduct learn approach schistosomiasi vector classif|The key issues pertaining to collection of epidemic disease data for our analysis purposes are that it is a labour intensive, time consuming and expensive process resulting in availability of sparse sample data which we use to develop prediction models. To address this sparse data issue, we present novel Incremental Transductive methods to circumvent the data collection process by applying previously acquired data to provide consistent, confidence-based labelling alternatives to field survey research. We investigated various reasoning approaches for semisupervised machine learning including Bayesian models for labelling data. The results show that using the proposed methods, we can label instances of data with a class of vector density at a high level of confidence. By applying the Liberal and Strict Training Approaches, we provide a labelling and classification alternative to standalone algorithms. The methods in this paper are components in the process of reducing the proliferation of the Schistosomiasis disease and its effects.|key issu pertain collect epidem diseas data analysi purpos labour intens time consum expens process result avail spars sampl data use develop predict model address spars data issu present novel increment transduct method circumv data collect process appli previous acquir data provid consist confid base label altern field survey research investig various reason approach semisupervis machin learn includ bayesian model label data result show use propos method label instanc data class vector densiti high level confid appli liber strict train approach provid label classif altern standalon algorithm method paper compon process reduc prolifer schistosomiasi diseas effect|['Terence Fusco', 'Yaxin Bi', 'Haiying Wang', 'Fiona Browne']|['cs.AI']
2017-04-07T11:26:35Z|2017-04-06T12:36:45Z|http://arxiv.org/abs/1704.01806v1|http://arxiv.org/pdf/1704.01806v1|Human-Aware Sensor Network Ontology: Semantic Support for Empirical Data   Collection|human awar sensor network ontolog semant support empir data collect|Significant efforts have been made to understand and document knowledge related to scientific measurements. Many of those efforts resulted in one or more high-quality ontologies that describe some aspects of scientific measurements, but not in a comprehensive and coherently integrated manner. For instance, we note that many of these high-quality ontologies are not properly aligned, and more challenging, that they have different and often conflicting concepts and approaches for encoding knowledge about empirical measurements. As a result of this lack of an integrated view, it is often challenging for scientists to determine whether any two scientific measurements were taken in semantically compatible manners, thus making it difficult to decide whether measurements should be analyzed in combination or not. In this paper, we present the Human-Aware Sensor Network Ontology that is a comprehensive alignment and integration of a sensing infrastructure ontology and a provenance ontology. HASNetO has been under development for more than one year, and has been reviewed, shared and used by multiple scientific communities. The ontology has been in use to support the data management of a number of large-scale ecological monitoring activities (observations) and empirical experiments.|signific effort made understand document knowledg relat scientif measur mani effort result one high qualiti ontolog describ aspect scientif measur comprehens coher integr manner instanc note mani high qualiti ontolog proper align challeng differ often conflict concept approach encod knowledg empir measur result lack integr view often challeng scientist determin whether ani two scientif measur taken semant compat manner thus make difficult decid whether measur analyz combin paper present human awar sensor network ontolog comprehens align integr sens infrastructur ontolog proven ontolog hasneto develop one year review share use multipl scientif communiti ontolog use support data manag number larg scale ecolog monitor activ observ empir experi|['Paulo Pinheiro', 'Deborah L. McGuinness', 'Henrique Santos']|['cs.AI', 'cs.CY', 'I.2.4']
2017-04-07T11:26:35Z|2017-04-06T12:21:57Z|http://arxiv.org/abs/1704.01802v1|http://arxiv.org/pdf/1704.01802v1|Contextual Data Collection for Smart Cities|contextu data collect smart citi|As part of Smart Cities initiatives, national, regional and local governments all over the globe are under the mandate of being more open regarding how they share their data. Under this mandate, many of these governments are publishing data under the umbrella of open government data, which includes measurement data from city-wide sensor networks. Furthermore, many of these data are published in so-called data portals as documents that may be spreadsheets, comma-separated value (CSV) data files, or plain documents in PDF or Word documents. The sharing of these documents may be a convenient way for the data provider to convey and publish data but it is not the ideal way for data consumers to reuse the data. For example, the problems of reusing the data may range from difficulty opening a document that is provided in any format that is not plain text, to the actual problem of understanding the meaning of each piece of knowledge inside of the document. Our proposal tackles those challenges by identifying metadata that has been regarded to be relevant for measurement data and providing a schema for this metadata. We further leverage the Human-Aware Sensor Network Ontology (HASNetO) to build an architecture for data collected in urban environments. We discuss the use of HASNetO and the supporting infrastructure to manage both data and metadata in support of the City of Fortaleza, a large metropolitan area in Brazil.|part smart citi initi nation region local govern globe mandat open regard share data mandat mani govern publish data umbrella open govern data includ measur data citi wide sensor network furthermor mani data publish call data portal document may spreadsheet comma separ valu csv data file plain document pdf word document share document may conveni way data provid convey publish data ideal way data consum reus data exampl problem reus data may rang difficulti open document provid ani format plain text actual problem understand mean piec knowledg insid document propos tackl challeng identifi metadata regard relev measur data provid schema metadata leverag human awar sensor network ontolog hasneto build architectur data collect urban environ discuss use hasneto support infrastructur manag data metadata support citi fortaleza larg metropolitan area brazil|['Henrique Santos', 'Vasco Furtado', 'Paulo Pinheiro', 'Deborah L. McGuinness']|['cs.AI', 'cs.CY', 'I.2.4']
2017-04-07T11:26:35Z|2017-04-06T11:17:54Z|http://arxiv.org/abs/1704.01785v1|http://arxiv.org/pdf/1704.01785v1|Geometry of Policy Improvement|geometri polici improv|We investigate the geometry of optimal memoryless time independent decision making in relation to the amount of information that the acting agent has about the state of the system. We show that the expected long term reward, discounted or per time step, is maximized by policies that randomize among at most $k$ actions whenever at most $k$ world states are consistent with the agent's observation. Moreover, we show that the expected reward per time step can be studied in terms of the expected discounted reward. Our main tool is a geometric version of the policy improvement lemma, which identifies a polyhedral cone of policy changes in which the state value function increases for all states.|investig geometri optim memoryless time independ decis make relat amount inform act agent state system show expect long term reward discount per time step maxim polici random among action whenev world state consist agent observ moreov show expect reward per time step studi term expect discount reward main tool geometr version polici improv lemma identifi polyhedr cone polici chang state valu function increas state|['Guido Montufar', 'Johannes Rauh']|['cs.AI', 'math.OC', '68T05, 90C40', 'G.3; I.2.8']
2017-04-07T11:26:35Z|2017-04-06T09:44:08Z|http://arxiv.org/abs/1704.01759v1|http://arxiv.org/pdf/1704.01759v1|A Multi-view Context-aware Approach to Android Malware Detection and   Malicious Code Localization|multi view context awar approach android malwar detect malici code local|Existing Android malware detection approaches use a variety of features such as security sensitive APIs, system calls, control-flow structures and information flows in conjunction with Machine Learning classifiers to achieve accurate detection. Each of these feature sets provides a unique semantic perspective (or view) of apps' behaviours with inherent strengths and limitations. Meaning, some views are more amenable to detect certain attacks but may not be suitable to characterise several other attacks. Most of the existing malware detection approaches use only one (or a selected few) of the aforementioned feature sets which prevent them from detecting a vast majority of attacks. Addressing this limitation, we propose MKLDroid, a unified framework that systematically integrates multiple views of apps for performing comprehensive malware detection and malicious code localisation. The rationale is that, while a malware app can disguise itself in some views, disguising in every view while maintaining malicious intent will be much harder.   MKLDroid uses a graph kernel to capture structural and contextual information from apps' dependency graphs and identify malice code patterns in each view. Subsequently, it employs Multiple Kernel Learning (MKL) to find a weighted combination of the views which yields the best detection accuracy. Besides multi-view learning, MKLDroid's unique and salient trait is its ability to locate fine-grained malice code portions in dependency graphs (e.g., methods/classes). Through our large-scale experiments on several datasets (incl. wild apps), we demonstrate that MKLDroid outperforms three state-of-the-art techniques consistently, in terms of accuracy while maintaining comparable efficiency. In our malicious code localisation experiments on a dataset of repackaged malware, MKLDroid was able to identify all the malice classes with 94% average recall.|exist android malwar detect approach use varieti featur secur sensit api system call control flow structur inform flow conjunct machin learn classifi achiev accur detect featur set provid uniqu semant perspect view app behaviour inher strength limit mean view amen detect certain attack may suitabl characteris sever attack exist malwar detect approach use onli one select aforement featur set prevent detect vast major attack address limit propos mkldroid unifi framework systemat integr multipl view app perform comprehens malwar detect malici code localis rational malwar app disguis view disguis everi view maintain malici intent much harder mkldroid use graph kernel captur structur contextu inform app depend graph identifi malic code pattern view subsequ employ multipl kernel learn mkl find weight combin view yield best detect accuraci besid multi view learn mkldroid uniqu salient trait abil locat fine grain malic code portion depend graph method class larg scale experi sever dataset incl wild app demonstr mkldroid outperform three state art techniqu consist term accuraci maintain compar effici malici code localis experi dataset repackag malwar mkldroid abl identifi malic class averag recal|['Annamalai Narayanan', 'Mahinthan Chandramohan', 'Lihui Chen', 'Yang Liu']|['cs.CR', 'cs.AI', 'cs.SE']
2017-04-07T11:26:35Z|2017-04-06T08:08:38Z|http://arxiv.org/abs/1704.01742v1|http://arxiv.org/pdf/1704.01742v1|Transferrable Plausibility Model - A Probabilistic Interpretation of   Mathematical Theory of Evidence|transferr plausibl model probabilist interpret mathemat theori evid|This paper suggests a new interpretation of the Dempster-Shafer theory in terms of probabilistic interpretation of plausibility. A new rule of combination of independent evidence is shown and its preservation of interpretation is demonstrated.|paper suggest new interpret dempster shafer theori term probabilist interpret plausibl new rule combin independ evid shown preserv interpret demonstr|['Mieczysław Kłopotek']|['cs.AI']
2017-04-07T11:26:35Z|2017-04-05T19:44:23Z|http://arxiv.org/abs/1704.01631v1|http://arxiv.org/pdf/1704.01631v1|Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder   Based Speech Recognition|multitask learn low level auxiliari task encod decod base speech recognit|End-to-end training of deep learning-based models allows for implicit learning of intermediate representations based on the final task loss. However, the end-to-end approach ignores the useful domain knowledge encoded in explicit intermediate-level supervision. We hypothesize that using intermediate representations as auxiliary supervision at lower levels of deep networks may be a good way of combining the advantages of end-to-end training and more traditional pipeline approaches. We present experiments on conversational speech recognition where we use lower-level tasks, such as phoneme recognition, in a multitask training approach with an encoder-decoder model for direct character transcription. We compare multiple types of lower-level tasks and analyze the effects of the auxiliary tasks. Our results on the Switchboard corpus show that this approach improves recognition accuracy over a standard encoder-decoder model on the Eval2000 test set.|end end train deep learn base model allow implicit learn intermedi represent base final task loss howev end end approach ignor use domain knowledg encod explicit intermedi level supervis hypothes use intermedi represent auxiliari supervis lower level deep network may good way combin advantag end end train tradit pipelin approach present experi convers speech recognit use lower level task phonem recognit multitask train approach encod decod model direct charact transcript compar multipl type lower level task analyz effect auxiliari task result switchboard corpus show approach improv recognit accuraci standard encod decod model eval test set|['Shubham Toshniwal', 'Hao Tang', 'Liang Lu', 'Karen Livescu']|['cs.CL', 'cs.AI']
2017-04-07T11:26:35Z|2017-04-05T17:59:07Z|http://arxiv.org/abs/1704.01568v1|http://arxiv.org/pdf/1704.01568v1|Best Practices for Applying Deep Learning to Novel Applications|best practic appli deep learn novel applic|This report is targeted to groups who are subject matter experts in their application but deep learning novices. It contains practical advice for those interested in testing the use of deep neural networks on applications that are novel for deep learning. We suggest making your project more manageable by dividing it into phases. For each phase this report contains numerous recommendations and insights to assist novice practitioners.|report target group subject matter expert applic deep learn novic contain practic advic interest test use deep neural network applic novel deep learn suggest make project manag divid phase phase report contain numer recommend insight assist novic practition|['Leslie N. Smith']|['cs.SE', 'cs.AI', 'cs.NE']
2017-04-07T11:26:35Z|2017-04-05T16:54:20Z|http://arxiv.org/abs/1704.01523v1|http://arxiv.org/pdf/1704.01523v1|MIT at SemEval-2017 Task 10: Relation Extraction with Convolutional   Neural Networks|mit semev task relat extract convolut neural network|Over 50 million scholarly articles have been published: they constitute a unique repository of knowledge. In particular, one may infer from them relations between scientific concepts, such as synonyms and hyponyms. Artificial neural networks have been recently explored for relation extraction. In this work, we continue this line of work and present a system based on a convolutional neural network to extract relations. Our model ranked first in the SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific articles (subtask C).|million scholar articl publish constitut uniqu repositori knowledg particular one may infer relat scientif concept synonym hyponym artifici neural network recent explor relat extract work continu line work present system base convolut neural network extract relat model rank first semev task sciencei relat extract scientif articl subtask|['Ji Young Lee', 'Franck Dernoncourt', 'Peter Szolovits']|['cs.CL', 'cs.AI', 'cs.NE', 'stat.ML']
2017-04-07T11:26:35Z|2017-04-05T13:26:50Z|http://arxiv.org/abs/1704.01407v1|http://arxiv.org/pdf/1704.01407v1|Embodied Artificial Intelligence through Distributed Adaptive Control:   An Integrated Framework|embodi artifici intellig distribut adapt control integr framework|In this paper, we argue that the future of Artificial Intelligence research resides in two keywords: integration and embodiment. We support this claim by analyzing the recent advances of the field. Regarding integration, we note that the most impactful recent contributions have been made possible through the integration of recent Machine Learning methods (based in particular on Deep Learning and Recurrent Neural Networks) with more traditional ones (e.g. Monte-Carlo tree search, goal babbling exploration or addressable memory systems). Regarding embodiment, we note that the traditional benchmark tasks (e.g. visual classification or board games) are becoming obsolete as state-of-the-art learning algorithms approach or even surpass human performance in most of them, having recently encouraged the development of first-person 3D game platforms embedding realistic physics. Building upon this analysis, we first propose an embodied cognitive architecture integrating heterogenous sub-fields of Artificial Intelligence into a unified framework. We demonstrate the utility of our approach by showing how major contributions of the field can be expressed within the proposed framework. We then claim that benchmarking environments need to reproduce ecologically-valid conditions for bootstrapping the acquisition of increasingly complex cognitive skills through the concept of a cognitive arms race between embodied agents.|paper argu futur artifici intellig research resid two keyword integr embodi support claim analyz recent advanc field regard integr note impact recent contribut made possibl integr recent machin learn method base particular deep learn recurr neural network tradit one mont carlo tree search goal babbl explor address memori system regard embodi note tradit benchmark task visual classif board game becom obsolet state art learn algorithm approach even surpass human perform recent encourag develop first person game platform embed realist physic build upon analysi first propos embodi cognit architectur integr heterogen sub field artifici intellig unifi framework demonstr util approach show major contribut field express within propos framework claim benchmark environ need reproduc ecolog valid condit bootstrap acquisit increas complex cognit skill concept cognit arm race embodi agent|['Clément Moulin-Frier', 'Jordi-Ysard Puigbò', 'Xerxes D. Arsiwalla', 'Martì Sanchez-Fibla', 'Paul F. M. J. Verschure']|['cs.AI', 'cs.LG', 'cs.MA']
2017-04-07T11:26:35Z|2017-04-05T12:35:20Z|http://arxiv.org/abs/1704.01383v1|http://arxiv.org/pdf/1704.01383v1|Finite-Time Stabilization of Longitudinal Control for Autonomous   Vehicles via a Model-Free Approach|finit time stabil longitudin control autonom vehicl via model free approach|This communication presents a longitudinal model-free control approach for computing the wheel torque command to be applied on a vehicle. This setting enables us to overcome the problem of unknown vehicle parameters for generating a suitable control law. An important parameter in this control setting is made time-varying for ensuring finite-time stability. Several convincing computer simulations are displayed and discussed. Overshoots become therefore smaller. The driving comfort is increased and the robustness to time-delays is improved.|communic present longitudin model free control approach comput wheel torqu command appli vehicl set enabl us overcom problem unknown vehicl paramet generat suitabl control law import paramet control set made time vari ensur finit time stabil sever convinc comput simul display discuss overshoot becom therefor smaller drive comfort increas robust time delay improv|"['Philip Polack', ""Brigitte d'Andréa-Novel"", 'Michel Fliess', 'Arnaud de la Fortelle', 'Lghani Menhour']"|['cs.SY', 'cs.AI', 'math.OC']
2017-04-07T11:26:39Z|2017-04-05T06:34:22Z|http://arxiv.org/abs/1704.01279v1|http://arxiv.org/pdf/1704.01279v1|Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders|neural audio synthesi music note wavenet autoencod|Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.|generat model vision seen rapid progress due algorithm improv avail high qualiti imag dataset paper offer contribut area enabl similar progress audio model first detail power new wavenet style autoencod model condit autoregress decod tempor code learn raw audio waveform second introduc nsynth larg scale high qualiti dataset music note order magnitud larger compar public dataset use nsynth demonstr improv qualit quantit perform wavenet autoencod well tune spectral autoencod baselin final show model learn manifold embed allow morph instrument meaning interpol timbr creat new type sound realist express|['Jesse Engel', 'Cinjon Resnick', 'Adam Roberts', 'Sander Dieleman', 'Douglas Eck', 'Karen Simonyan', 'Mohammad Norouzi']|['cs.LG', 'cs.AI', 'cs.SD']
2017-04-07T11:26:39Z|2017-04-04T19:47:52Z|http://arxiv.org/abs/1704.01161v1|http://arxiv.org/pdf/1704.01161v1|Finite Sample Analysis for TD(0) with Linear Function Approximation|finit sampl analysi td linear function approxim|TD(0) is one of the most commonly used algorithms in reinforcement learning. Despite this, there is no existing finite sample analysis for TD(0) with function approximation, even for the linear case. Our work is the first to provide such a result. Works that managed to obtain concentration bounds for online Temporal Difference (TD) methods analyzed modified versions of them, carefully crafted for the analyses to hold. These modifications include projections and step-sizes dependent on unknown problem parameters. Our analysis obviates these artificial alterations by exploiting strong properties of TD(0) and tailor-made stochastic approximation tools.|td one common use algorithm reinforc learn despit exist finit sampl analysi td function approxim even linear case work first provid result work manag obtain concentr bound onlin tempor differ td method analyz modifi version care craft analys hold modif includ project step size depend unknown problem paramet analysi obviat artifici alter exploit strong properti td tailor made stochast approxim tool|['Gal Dalal', 'Balázs Szörényi', 'Gugan Thoppe', 'Shie Mannor']|['cs.AI']
2017-04-07T11:26:39Z|2017-04-04T16:18:07Z|http://arxiv.org/abs/1704.01087v1|http://arxiv.org/pdf/1704.01087v1|Probabilistic Search for Structured Data via Probabilistic Programming   and Nonparametric Bayes|probabilist search structur data via probabilist program nonparametr bay|Databases are widespread, yet extracting relevant data can be difficult. Without substantial domain knowledge, multivariate search queries often return sparse or uninformative results. This paper introduces an approach for searching structured data based on probabilistic programming and nonparametric Bayes. Users specify queries in a probabilistic language that combines standard SQL database search operators with an information theoretic ranking function called predictive relevance. Predictive relevance can be calculated by a fast sparse matrix algorithm based on posterior samples from CrossCat, a nonparametric Bayesian model for high-dimensional, heterogeneously-typed data tables. The result is a flexible search technique that applies to a broad class of information retrieval problems, which we integrate into BayesDB, a probabilistic programming platform for probabilistic data analysis. This paper demonstrates applications to databases of US colleges, global macroeconomic indicators of public health, and classic cars. We found that human evaluators often prefer the results from probabilistic search to results from a standard baseline.|databas widespread yet extract relev data difficult without substanti domain knowledg multivari search queri often return spars uninform result paper introduc approach search structur data base probabilist program nonparametr bay user specifi queri probabilist languag combin standard sql databas search oper inform theoret rank function call predict relev predict relev calcul fast spars matrix algorithm base posterior sampl crosscat nonparametr bayesian model high dimension heterogen type data tabl result flexibl search techniqu appli broad class inform retriev problem integr bayesdb probabilist program platform probabilist data analysi paper demonstr applic databas us colleg global macroeconom indic public health classic car found human evalu often prefer result probabilist search result standard baselin|['Feras Saad', 'Leonardo Casarsa', 'Vikash Mansinghka']|['cs.AI', 'cs.DB', 'cs.LG', 'stat.ML']
2017-04-07T11:26:39Z|2017-04-04T12:50:25Z|http://arxiv.org/abs/1704.01415v1|http://arxiv.org/pdf/1704.01415v1|Multi-Label Learning with Global and Local Label Correlation|multi label learn global local label correl|It is well-known that exploiting label correlations is important to multi-label learning. Existing approaches either assume that the label correlations are global and shared by all instances; or that the label correlations are local and shared only by a data subset. In fact, in the real-world applications, both cases may occur that some label correlations are globally applicable and some are shared only in a local group of instances. Moreover, it is also a usual case that only partial labels are observed, which makes the exploitation of the label correlations much more difficult. That is, it is hard to estimate the label correlations when many labels are absent. In this paper, we propose a new multi-label approach GLOCAL dealing with both the full-label and the missing-label cases, exploiting global and local label correlations simultaneously, through learning a latent label representation and optimizing label manifolds. The extensive experimental studies validate the effectiveness of our approach on both full-label and missing-label data.|well known exploit label correl import multi label learn exist approach either assum label correl global share instanc label correl local share onli data subset fact real world applic case may occur label correl global applic share onli local group instanc moreov also usual case onli partial label observ make exploit label correl much difficult hard estim label correl mani label absent paper propos new multi label approach glocal deal full label miss label case exploit global local label correl simultan learn latent label represent optim label manifold extens experiment studi valid effect approach full label miss label data|['Yue Zhu', 'James T. Kwok', 'Zhi-Hua Zhou']|['cs.LG', 'cs.AI']
2017-04-07T11:26:39Z|2017-04-04T12:04:03Z|http://arxiv.org/abs/1704.01399v1|http://arxiv.org/pdf/1704.01399v1|Geracao Automatica de Paineis de Controle para Analise de Mobilidade   Urbana Utilizando Redes Complexas|geracao automatica de painei de control para analis de mobilidad urbana utilizando rede complexa|In this paper we describe an automatic generator to support the data scientist to construct, in a user-friendly way, dashboards from data represented as networks. The generator called SBINet (Semantic for Business Intelligence from Networks) has a semantic layer that, through ontologies, describes the data that represents a network as well as the possible metrics to be calculated in the network. Thus, with SBINet, the stages of the dashboard constructing process that uses complex network metrics are facilitated and can be done by users who do not necessarily know about complex networks.|paper describ automat generat support data scientist construct user friend way dashboard data repres network generat call sbinet semant busi intellig network semant layer ontolog describ data repres network well possibl metric calcul network thus sbinet stage dashboard construct process use complex network metric facilit done user necessarili know complex network|['Victor Dantas', 'Henrique Santos', 'Carlos Caminha', 'Vasco Furtado']|['cs.AI', 'cs.SI']
2017-04-07T11:26:39Z|2017-04-04T11:29:02Z|http://arxiv.org/abs/1704.00961v1|http://arxiv.org/pdf/1704.00961v1|Adaptive Motion Gaming AI for Health Promotion|adapt motion game ai health promot|This paper presents a design of a non-player character (AI) for promoting balancedness in use of body segments when engaging in full-body motion gaming. In our experiment, we settle a battle between the proposed AI and a player by using FightingICE, a fighting game platform for AI development. A middleware called UKI is used to allow the player to control the game by using body motion instead of the keyboard and mouse. During gameplay, the proposed AI analyze health states of the player; it determines its next action by predicting how each candidate action, recommended by a Monte-Carlo tree search algorithm, will induce the player to move, and how the player's health tends to be affected. Our result demonstrates successful improvement in balancedness in use of body segments on 4 out of 5 subjects.|paper present design non player charact ai promot balanced use bodi segment engag full bodi motion game experi settl battl propos ai player use fightingic fight game platform ai develop middlewar call uki use allow player control game use bodi motion instead keyboard mous dure gameplay propos ai analyz health state player determin next action predict candid action recommend mont carlo tree search algorithm induc player move player health tend affect result demonstr success improv balanced use bodi segment subject|['Pujana Paliyawan', 'Takahiro Kusano', 'Yuto Nakagawa', 'Tomohiro Harada', 'Ruck Thawonmas']|['cs.AI', 'cs.CY', 'cs.HC', 'I.2.1; K.8.0']
2017-04-07T11:26:39Z|2017-04-04T08:27:21Z|http://arxiv.org/abs/1704.00917v1|http://arxiv.org/pdf/1704.00917v1|Deriving Probability Density Functions from Probabilistic Functional   Programs|deriv probabl densiti function probabilist function program|The probability density function of a probability distribution is a fundamental concept in probability theory and a key ingredient in various widely used machine learning methods. However, the necessary framework for compiling probabilistic functional programs to density functions has only recently been developed. In this work, we present a density compiler for a probabilistic language with failure and both discrete and continuous distributions, and provide a proof of its soundness. The compiler greatly reduces the development effort of domain experts, which we demonstrate by solving inference problems from various scientific applications, such as modelling the global carbon cycle, using a standard Markov chain Monte Carlo framework.|probabl densiti function probabl distribut fundament concept probabl theori key ingredi various wide use machin learn method howev necessari framework compil probabilist function program densiti function onli recent develop work present densiti compil probabilist languag failur discret continu distribut provid proof sound compil great reduc develop effort domain expert demonstr solv infer problem various scientif applic model global carbon cycl use standard markov chain mont carlo framework|['Sooraj Bhat', 'Johannes Borgström', 'Andrew D. Gordon', 'Claudio Russo']|['cs.PL', 'cs.AI', 'F.3.2; G.3; I.2.5']
2017-04-07T11:26:39Z|2017-04-04T02:28:59Z|http://arxiv.org/abs/1704.00853v1|http://arxiv.org/pdf/1704.00853v1|A History of Metaheuristics|histori metaheurist|This chapter describes the history of metaheuristics in five distinct periods, starting long before the first use of the term and ending a long time in the future.|chapter describ histori metaheurist five distinct period start long befor first use term end long time futur|['Kenneth Sorensen', 'Marc Sevaux', 'Fred Glover']|['cs.AI']
2017-04-07T11:26:39Z|2017-04-03T20:22:52Z|http://arxiv.org/abs/1704.00797v1|http://arxiv.org/pdf/1704.00797v1|On the idea of a new artificial intelligence based optimization   algorithm inspired from the nature of vortex|idea new artifici intellig base optim algorithm inspir natur vortex|In this paper, the idea of a new artificial intelligence based optimization algorithm, which is inspired from the nature of vortex, has been provided briefly. As also a bio-inspired computation algorithm, the idea is generally focused on a typical vortex flow / behavior in nature and inspires from some dynamics that are occurred in the sense of vortex nature. Briefly, the algorithm is also a swarm-oriented evolutional problem solution approach; because it includes many methods related to elimination of weak swarm members and trying to improve the solution process by supporting the solution space via new swarm members. In order have better idea about success of the algorithm; it has been tested via some benchmark functions. At this point, the obtained results show that the algorithm can be an alternative to the literature in terms of single-objective optimization solution ways. Vortex Optimization Algorithm (VOA) is the name suggestion by the authors; for this new idea of intelligent optimization approach.|paper idea new artifici intellig base optim algorithm inspir natur vortex provid briefli also bio inspir comput algorithm idea general focus typic vortex flow behavior natur inspir dynam occur sens vortex natur briefli algorithm also swarm orient evolut problem solut approach becaus includ mani method relat elimin weak swarm member tri improv solut process support solut space via new swarm member order better idea success algorithm test via benchmark function point obtain result show algorithm altern literatur term singl object optim solut way vortex optim algorithm voa name suggest author new idea intellig optim approach|['Utku Kose', 'Ahmet Arslan']|['cs.AI', 'math.OC']
2017-04-07T11:26:39Z|2017-04-03T20:17:32Z|http://arxiv.org/abs/1704.00795v1|http://arxiv.org/pdf/1704.00795v1|Design and development of a software system for swarm intelligence based   research studies|design develop softwar system swarm intellig base research studi|This paper introduce a software system including widely-used Swarm Intelligence algorithms or approaches to be used for the related scientific research studies associated with the subject area. The programmatic infrastructure of the system allows working on a fast, easy-to-use, interactive platform to perform Swarm Intelligence based studies in a more effective, efficient and accurate way. In this sense, the system employs all of the necessary controls for the algorithms and it ensures an interactive platform on which computer users can perform studies on a wide spectrum of solution approaches associated with simple and also more advanced problems.|paper introduc softwar system includ wide use swarm intellig algorithm approach use relat scientif research studi associ subject area programmat infrastructur system allow work fast easi use interact platform perform swarm intellig base studi effect effici accur way sens system employ necessari control algorithm ensur interact platform comput user perform studi wide spectrum solut approach associ simpl also advanc problem|['Utku Kose']|['cs.AI', 'cs.SE']
2017-04-07T11:26:43Z|2017-04-03T19:45:04Z|http://arxiv.org/abs/1704.00783v1|http://arxiv.org/pdf/1704.00783v1|Brief Notes on Hard Takeoff, Value Alignment, and Coherent Extrapolated   Volition|brief note hard takeoff valu align coher extrapol volit|I make some basic observations about hard takeoff, value alignment, and coherent extrapolated volition, concepts which have been central in analyses of superintelligent AI systems.|make basic observ hard takeoff valu align coher extrapol volit concept central analys superintellig ai system|['Gopal P. Sarma']|['cs.AI', 'cs.CY', 'cs.LG']
2017-04-07T11:26:43Z|2017-04-03T18:37:12Z|http://arxiv.org/abs/1704.00756v1|http://arxiv.org/pdf/1704.00756v1|Multi-Advisor Reinforcement Learning|multi advisor reinforc learn|This article deals with a novel branch of Separation of Concerns, called Multi-Advisor Reinforcement Learning (MAd-RL), where a single-agent RL problem is distributed to $n$ learners, called advisors. Each advisor tries to solve the problem with a different focus. Their advice is then communicated to an aggregator, which is in control of the system. For the local training, three off-policy bootstrapping methods are proposed and analysed: local-max bootstraps with the local greedy action, rand-policy bootstraps with respect to the random policy, and agg-policy bootstraps with respect to the aggregator's greedy policy. MAd-RL is positioned as a generalisation of Reinforcement Learning with Ensemble methods. An experiment is held on a simplified version of the Ms. Pac-Man Atari game. The results confirm the theoretical relative strengths and weaknesses of each method.|articl deal novel branch separ concern call multi advisor reinforc learn mad rl singl agent rl problem distribut learner call advisor advisor tri solv problem differ focus advic communic aggreg control system local train three polici bootstrap method propos analys local max bootstrap local greedi action rand polici bootstrap respect random polici agg polici bootstrap respect aggreg greedi polici mad rl posit generalis reinforc learn ensembl method experi held simplifi version ms pac man atari game result confirm theoret relat strength weak method|['Romain Laroche', 'Mehdi Fatemi', 'Joshua Romoff', 'Harm van Seijen']|['cs.LG', 'cs.AI', 'stat.ML']
2017-04-07T11:26:43Z|2017-04-03T17:58:07Z|http://arxiv.org/abs/1704.00717v1|http://arxiv.org/pdf/1704.00717v1|It Takes Two to Tango: Towards Theory of AI's Mind|take two tango toward theori ai mind|Theory of Mind is the ability to attribute mental states (beliefs, intents, knowledge, perspectives, etc.) to others and recognize that these mental states may differ from one's own. Theory of Mind is critical to effective communication and to teams demonstrating higher collective performance. To effectively leverage the progress in Artificial Intelligence (AI) to make our lives more productive, it is important for humans and AI to work well together in a team. Traditionally, there has been much emphasis on research to make AI more accurate, and (to a lesser extent) on having it better understand human intentions, tendencies, beliefs, and contexts. The latter involves making AI more human-like and having it develop a theory of our minds.   In this work, we argue that for human-AI teams to be effective, humans must also develop a theory of AI's mind - get to know its strengths, weaknesses, beliefs, and quirks. We instantiate these ideas within the domain of Visual Question Answering (VQA). We find that using just a few examples(50), lay people can be trained to better predict responses and oncoming failures of a complex VQA model. Surprisingly, we find that having access to the model's internal states - its confidence in its top-k predictions, explicit or implicit attention maps which highlight regions in the image (and words in the question) the model is looking at (and listening to) while answering a question about an image - do not help people better predict its behavior|theori mind abil attribut mental state belief intent knowledg perspect etc recogn mental state may differ one theori mind critic effect communic team demonstr higher collect perform effect leverag progress artifici intellig ai make live product import human ai work well togeth team tradit much emphasi research make ai accur lesser extent better understand human intent tendenc belief context latter involv make ai human like develop theori mind work argu human ai team effect human must also develop theori ai mind get know strength weak belief quirk instanti idea within domain visual question answer vqa find use exampl lay peopl train better predict respons oncom failur complex vqa model surpris find access model intern state confid top predict explicit implicit attent map highlight region imag word question model look listen answer question imag help peopl better predict behavior|['Arjun Chandrasekaran', 'Deshraj Yadav', 'Prithvijit Chattopadhyay', 'Viraj Prabhu', 'Devi Parikh']|['cs.CV', 'cs.AI', 'cs.CL']
2017-04-07T11:26:43Z|2017-04-03T15:25:47Z|http://arxiv.org/abs/1704.00637v1|http://arxiv.org/pdf/1704.00637v1|Semi-Supervised Generation with Cluster-aware Generative Models|semi supervis generat cluster awar generat model|Deep generative models trained with large amounts of unlabelled data have proven to be powerful within the domain of unsupervised learning. Many real life data sets contain a small amount of labelled data points, that are typically disregarded when training generative models. We propose the Cluster-aware Generative Model, that uses unlabelled information to infer a latent representation that models the natural clustering of the data, and additional labelled data points to refine this clustering. The generative performances of the model significantly improve when labelled information is exploited, obtaining a log-likelihood of -79.38 nats on permutation invariant MNIST, while also achieving competitive semi-supervised classification accuracies. The model can also be trained fully unsupervised, and still improve the log-likelihood performance with respect to related methods.|deep generat model train larg amount unlabel data proven power within domain unsupervis learn mani real life data set contain small amount label data point typic disregard train generat model propos cluster awar generat model use unlabel inform infer latent represent model natur cluster data addit label data point refin cluster generat perform model signific improv label inform exploit obtain log likelihood nat permut invari mnist also achiev competit semi supervis classif accuraci model also train fulli unsupervis still improv log likelihood perform respect relat method|['Lars Maaløe', 'Marco Fraccaro', 'Ole Winther']|['stat.ML', 'cs.AI', 'cs.LG']
2017-04-07T11:26:43Z|2017-04-03T14:29:40Z|http://arxiv.org/abs/1704.00616v1|http://arxiv.org/pdf/1704.00616v1|Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance   for Action Classification and Detection|chain multi stream network exploit pose motion appear action classif detect|General human action recognition requires understanding of various visual cues. In this paper, we propose a network architecture that computes and integrates the most important visual cues for action recognition: pose, motion, and the raw images. For the integration, we introduce a Markov chain model which adds cues successively. The resulting approach is efficient and applicable to action classification as well as to spatial and temporal action localization. The two contributions clearly improve the performance over respective baselines. The overall approach achieves state-of-the-art action classification performance on HMDB51, J-HMDB and NTU RGB+D datasets. Moreover, it yields state-of-the-art spatio-temporal action localization results on UCF101 and J-HMDB.|general human action recognit requir understand various visual cue paper propos network architectur comput integr import visual cue action recognit pose motion raw imag integr introduc markov chain model add cue success result approach effici applic action classif well spatial tempor action local two contribut clear improv perform respect baselin overal approach achiev state art action classif perform hmdb hmdb ntu rgb dataset moreov yield state art spatio tempor action local result ucf hmdb|['Mohammadreza Zolfaghari', 'Gabriel L. Oliveira', 'Nima Sedaghat', 'Thomas Brox']|['cs.CV', 'cs.AI', 'cs.HC', 'cs.MM', 'cs.NE']
2017-04-07T11:26:43Z|2017-04-03T10:25:22Z|http://arxiv.org/abs/1704.00514v1|http://arxiv.org/pdf/1704.00514v1|Multi-Task Learning of Keyphrase Boundary Classification|multi task learn keyphras boundari classif|Keyphrase boundary classification (KBC) is the task of detecting keyphrases in scientific articles and labelling them with respect to predefined types. Although important in practice, this task is so far underexplored, partly due to the lack of labelled data. To overcome this, we explore several auxiliary tasks, including semantic super-sense tagging and identification of multi-word expressions, and cast the task as a multi-task learning problem with deep recurrent neural networks. Our multi-task models perform significantly better than previous state of the art approaches on two scientific KBC datasets, particularly for long keyphrases.|keyphras boundari classif kbc task detect keyphras scientif articl label respect predefin type although import practic task far underexplor part due lack label data overcom explor sever auxiliari task includ semant super sens tag identif multi word express cast task multi task learn problem deep recurr neural network multi task model perform signific better previous state art approach two scientif kbc dataset particular long keyphras|['Isabelle Augenstein', 'Anders Søgaard']|['cs.CL', 'cs.AI', 'stat.ML']
2017-04-07T11:26:43Z|2017-04-02T16:22:31Z|http://arxiv.org/abs/1704.00325v1|http://arxiv.org/pdf/1704.00325v1|Structured Parallel Programming for Monte Carlo Tree Search|structur parallel program mont carlo tree search|In this paper, we present a new algorithm for parallel Monte Carlo tree search (MCTS). It is based on the pipeline pattern and allows flexible management of the control flow of the operations in parallel MCTS. The pipeline pattern provides for the first structured parallel programming approach to MCTS. Moreover, we propose a new lock-free tree data structure for parallel MCTS which removes synchronization overhead. The Pipeline Pattern for Parallel MCTS algorithm (called 3PMCTS), scales very well to higher numbers of cores when compared to the existing methods.|paper present new algorithm parallel mont carlo tree search mcts base pipelin pattern allow flexibl manag control flow oper parallel mcts pipelin pattern provid first structur parallel program approach mcts moreov propos new lock free tree data structur parallel mcts remov synchron overhead pipelin pattern parallel mcts algorithm call pmcts scale veri well higher number core compar exist method|['S. Ali Mirsoleimani', 'Aske Plaat', 'Jaap van den Herik', 'Jos Vermaseren']|['cs.AI']
2017-04-07T11:26:43Z|2017-04-02T10:45:29Z|http://arxiv.org/abs/1704.00725v1|http://arxiv.org/pdf/1704.00725v1|Reprogramming Matter, Life, and Purpose|reprogram matter life purpos|Reprogramming matter may sound far-fetched, but we have been doing it with increasing power and staggering efficiency for at least 60 years, and for centuries we have been paving the way toward the ultimate reprogrammed fate of the universe, the vessel of all programs. How will we be doing it in 60 years' time and how will it impact life and the purpose both of machines and of humans?|reprogram matter may sound far fetch increas power stagger effici least year centuri pave way toward ultim reprogram fate univers vessel program year time impact life purpos machin human|['Hector Zenil']|['cs.CY', 'cs.AI']
2017-04-07T11:26:43Z|2017-04-02T08:21:30Z|http://arxiv.org/abs/1704.00264v1|http://arxiv.org/abs/1704.00264v1|Potential Functions based Sampling Heuristic For Optimal Path Planning|potenti function base sampl heurist optim path plan|Rapidly-exploring Random Tree Star(RRT*) is a recently proposed extension of Rapidly-exploring Random Tree (RRT) algorithm that provides a collision-free, asymptotically optimal path regardless of obstacle's geometry in a given environment. However, one of the limitations in the RRT* algorithm is slow convergence to optimal path solution. As a result, it consumes high memory as well as time due to a large number of iterations utilised in achieving optimal path solution. To overcome these limitations, we propose the Potential Function Based-RRT* (P-RRT*) that incorporates the Artificial Potential Field Algorithm in RRT*. The proposed algorithm allows a considerable decrease in the number of iterations and thus leads to more efficient memory utilization and an accelerated convergence rate. In order to illustrate the usefulness of the proposed algorithm in terms of space execution and convergence rate, this paper presents rigorous simulation based comparisons between the proposed techniques and RRT* under different environmental conditions. Moreover, both algorithms are also tested and compared under non-holonomic differential constraints.|rapid explor random tree star rrt recent propos extens rapid explor random tree rrt algorithm provid collis free asymptot optim path regardless obstacl geometri given environ howev one limit rrt algorithm slow converg optim path solut result consum high memori well time due larg number iter utilis achiev optim path solut overcom limit propos potenti function base rrt rrt incorpor artifici potenti field algorithm rrt propos algorithm allow consider decreas number iter thus lead effici memori util acceler converg rate order illustr use propos algorithm term space execut converg rate paper present rigor simul base comparison propos techniqu rrt differ environment condit moreov algorithm also test compar non holonom differenti constraint|['Ahmed Hussain Qureshi', 'Yasar Ayaz']|['cs.RO', 'cs.AI']
2017-04-07T11:26:43Z|2017-04-02T08:01:30Z|http://arxiv.org/abs/1704.00260v1|http://arxiv.org/pdf/1704.00260v1|Aligned Image-Word Representations Improve Inductive Transfer Across   Vision-Language Tasks|align imag word represent improv induct transfer across vision languag task|A grand goal of computer vision is to build systems that learn visual representations over time that can be applied to many tasks. In this paper, we investigate a vision-language embedding as a core representation and show that it leads to better cross-task transfer than standard multi-task learning. In particular, the task of visual recognition is aligned to the task of visual question answering by forcing each to use the same word-region embeddings. We show this leads to greater inductive transfer from recognition to VQA than standard multitask learning. Visual recognition also improves, especially for categories that have relatively few recognition training labels but appear often in the VQA setting. Thus, our paper takes a small step towards creating more general vision systems by showing the benefit of interpretable, flexible, and trainable core representations.|grand goal comput vision build system learn visual represent time appli mani task paper investig vision languag embed core represent show lead better cross task transfer standard multi task learn particular task visual recognit align task visual question answer forc use word region embed show lead greater induct transfer recognit vqa standard multitask learn visual recognit also improv especi categori relat recognit train label appear often vqa set thus paper take small step toward creat general vision system show benefit interpret flexibl trainabl core represent|['Tanmay Gupta', 'Kevin Shih', 'Saurabh Singh', 'Derek Hoiem']|['cs.CV', 'cs.AI', 'cs.LG', 'cs.NE', 'stat.ML']
2017-04-07T11:26:47Z|2017-04-01T21:26:37Z|http://arxiv.org/abs/1704.01014v1|http://arxiv.org/abs/1704.01014v1|An Ontological Architecture for Orbital Debris Data|ontolog architectur orbit debri data|The orbital debris problem presents an opportunity for inter-agency and international cooperation toward the mutually beneficial goals of debris prevention, mitigation, remediation, and improved space situational awareness (SSA). Achieving these goals requires sharing orbital debris and other SSA data. Toward this, I present an ontological architecture for the orbital debris domain, taking steps in the creation of an orbital debris ontology (ODO). The purpose of this ontological system is to (I) represent general orbital debris and SSA domain knowledge, (II) structure, and standardize where needed, orbital data and terminology, and (III) foster semantic interoperability and data-sharing. In doing so I hope to (IV) contribute to solving the orbital debris problem, improving peaceful global SSA, and ensuring safe space travel for future generations.|orbit debri problem present opportun inter agenc intern cooper toward mutual benefici goal debri prevent mitig remedi improv space situat awar ssa achiev goal requir share orbit debri ssa data toward present ontolog architectur orbit debri domain take step creation orbit debri ontolog odo purpos ontolog system repres general orbit debri ssa domain knowledg ii structur standard need orbit data terminolog iii foster semant interoper data share hope iv contribut solv orbit debri problem improv peac global ssa ensur safe space travel futur generat|['Robert J. Rovetto']|['cs.AI', 'cs.DB']
2017-04-07T11:26:47Z|2017-04-01T19:29:21Z|http://arxiv.org/abs/1704.00217v1|http://arxiv.org/pdf/1704.00217v1|Adversarial Connective-exploiting Networks for Implicit Discourse   Relation Classification|adversari connect exploit network implicit discours relat classif|Implicit discourse relation classification is of great challenge due to the lack of connectives as strong linguistic cues, which motivates the use of annotated implicit connectives to improve the recognition. We propose a feature imitation framework in which an implicit relation network is driven to learn from another neural network with access to connectives, and thus encouraged to extract similarly salient features for accurate classification. We develop an adversarial model to enable an adaptive imitation scheme through competition between the implicit network and a rival feature discriminator. Our method effectively transfers discriminability of connectives to the implicit features, and achieves state-of-the-art performance on the PDTB benchmark.|implicit discours relat classif great challeng due lack connect strong linguist cue motiv use annot implicit connect improv recognit propos featur imit framework implicit relat network driven learn anoth neural network access connect thus encourag extract similar salient featur accur classif develop adversari model enabl adapt imit scheme competit implicit network rival featur discrimin method effect transfer discrimin connect implicit featur achiev state art perform pdtb benchmark|['Lianhui Qin', 'Zhisong Zhang', 'Hai Zhao', 'Zhiting Hu', 'Eric P. Xing']|['cs.CL', 'cs.AI', 'cs.LG', 'stat.ML']
2017-04-07T11:26:47Z|2017-04-01T03:50:53Z|http://arxiv.org/abs/1704.00115v1|http://arxiv.org/pdf/1704.00115v1|Ontological Multidimensional Data Models and Contextual Data Qality|ontolog multidimension data model contextu data qaliti|Data quality assessment and data cleaning are context-dependent activities. Motivated by this observation, we propose the Ontological Multidimensional Data Model (OMD model), which can be used to model and represent contexts as logic-based ontologies. The data under assessment is mapped into the context, for additional analysis, processing, and quality data extraction. The resulting contexts allow for the representation of dimensions, and multidimensional data quality assessment becomes possible. At the core of a multidimensional context we include a generalized multidimensional data model and a Datalog+/- ontology with provably good properties in terms of query answering. These main components are used to represent dimension hierarchies, dimensional constraints, dimensional rules, and define predicates for quality data specification. Query answering relies upon and triggers navigation through dimension hierarchies, and becomes the basic tool for the extraction of quality data. The OMD model is interesting per se, beyond applications to data quality. It allows for a logic-based, and computationally tractable representation of multidimensional data, extending previous multidimensional data models with additional expressive power and functionalities.|data qualiti assess data clean context depend activ motiv observ propos ontolog multidimension data model omd model use model repres context logic base ontolog data assess map context addit analysi process qualiti data extract result context allow represent dimens multidimension data qualiti assess becom possibl core multidimension context includ general multidimension data model datalog ontolog provabl good properti term queri answer main compon use repres dimens hierarchi dimension constraint dimension rule defin predic qualiti data specif queri answer reli upon trigger navig dimens hierarchi becom basic tool extract qualiti data omd model interest per se beyond applic data qualiti allow logic base comput tractabl represent multidimension data extend previous multidimension data model addit express power function|['Leopoldo Bertossi', 'Mostafa Milani']|['cs.DB', 'cs.AI']
2017-04-07T11:26:47Z|2017-03-31T18:55:48Z|http://arxiv.org/abs/1704.00023v1|http://arxiv.org/pdf/1704.00023v1|On the Reliable Detection of Concept Drift from Streaming Unlabeled Data|reliabl detect concept drift stream unlabel data|Classifiers deployed in the real world operate in a dynamic environment, where the data distribution can change over time. These changes, referred to as concept drift, can cause the predictive performance of the classifier to drop over time, thereby making it obsolete. To be of any real use, these classifiers need to detect drifts and be able to adapt to them, over time. Detecting drifts has traditionally been approached as a supervised task, with labeled data constantly being used for validating the learned model. Although effective in detecting drifts, these techniques are impractical, as labeling is a difficult, costly and time consuming activity. On the other hand, unsupervised change detection techniques are unreliable, as they produce a large number of false alarms. The inefficacy of the unsupervised techniques stems from the exclusion of the characteristics of the learned classifier, from the detection process. In this paper, we propose the Margin Density Drift Detection (MD3) algorithm, which tracks the number of samples in the uncertainty region of a classifier, as a metric to detect drift. The MD3 algorithm is a distribution independent, application independent, model independent, unsupervised and incremental algorithm for reliably detecting drifts from data streams. Experimental evaluation on 6 drift induced datasets and 4 additional datasets from the cybersecurity domain demonstrates that the MD3 approach can reliably detect drifts, with significantly fewer false alarms compared to unsupervised feature based drift detectors. The reduced false alarms enables the signaling of drifts only when they are most likely to affect classification performance. As such, the MD3 approach leads to a detection scheme which is credible, label efficient and general in its applicability.|classifi deploy real world oper dynam environ data distribut chang time chang refer concept drift caus predict perform classifi drop time therebi make obsolet ani real use classifi need detect drift abl adapt time detect drift tradit approach supervis task label data constant use valid learn model although effect detect drift techniqu impract label difficult cost time consum activ hand unsupervis chang detect techniqu unreli produc larg number fals alarm inefficaci unsupervis techniqu stem exclus characterist learn classifi detect process paper propos margin densiti drift detect md algorithm track number sampl uncertainti region classifi metric detect drift md algorithm distribut independ applic independ model independ unsupervis increment algorithm reliabl detect drift data stream experiment evalu drift induc dataset addit dataset cybersecur domain demonstr md approach reliabl detect drift signific fewer fals alarm compar unsupervis featur base drift detector reduc fals alarm enabl signal drift onli like affect classif perform md approach lead detect scheme credibl label effici general applic|['Tegjyot Singh Sethi', 'Mehmed Kantardzic']|['stat.ML', 'cs.AI', 'cs.LG']
2017-04-07T11:26:47Z|2017-03-31T17:45:53Z|http://arxiv.org/abs/1703.11000v1|http://arxiv.org/pdf/1703.11000v1|Learning Visual Servoing with Deep Features and Fitted Q-Iteration|learn visual servo deep featur fit iter|Visual servoing involves choosing actions that move a robot in response to observations from a camera, in order to reach a goal configuration in the world. Standard visual servoing approaches typically rely on manually designed features and analytical dynamics models, which limits their generalization capability and often requires extensive application-specific feature and model engineering. In this work, we study how learned visual features, learned predictive dynamics models, and reinforcement learning can be combined to learn visual servoing mechanisms. We focus on target following, with the goal of designing algorithms that can learn a visual servo using low amounts of data of the target in question, to enable quick adaptation to new targets. Our approach is based on servoing the camera in the space of learned visual features, rather than image pixels or manually-designed keypoints. We demonstrate that standard deep features, in our case taken from a model trained for object classification, can be used together with a bilinear predictive model to learn an effective visual servo that is robust to visual variation, changes in viewing angle and appearance, and occlusions. A key component of our approach is to use a sample-efficient fitted Q-iteration algorithm to learn which features are best suited for the task at hand. We show that we can learn an effective visual servo on a complex synthetic car following benchmark using just 20 training trajectory samples for reinforcement learning. We demonstrate substantial improvement over a conventional approach based on image pixels or hand-designed keypoints, and we show an improvement in sample-efficiency of more than two orders of magnitude over standard model-free deep reinforcement learning algorithms. Videos are available at \url{http://rll.berkeley.edu/visual_servoing}.|visual servo involv choos action move robot respons observ camera order reach goal configur world standard visual servo approach typic reli manual design featur analyt dynam model limit general capabl often requir extens applic specif featur model engin work studi learn visual featur learn predict dynam model reinforc learn combin learn visual servo mechan focus target follow goal design algorithm learn visual servo use low amount data target question enabl quick adapt new target approach base servo camera space learn visual featur rather imag pixel manual design keypoint demonstr standard deep featur case taken model train object classif use togeth bilinear predict model learn effect visual servo robust visual variat chang view angl appear occlus key compon approach use sampl effici fit iter algorithm learn featur best suit task hand show learn effect visual servo complex synthet car follow benchmark use train trajectori sampl reinforc learn demonstr substanti improv convent approach base imag pixel hand design keypoint show improv sampl effici two order magnitud standard model free deep reinforc learn algorithm video avail url http rll berkeley edu visual servo|['Alex X. Lee', 'Sergey Levine', 'Pieter Abbeel']|['cs.LG', 'cs.AI', 'cs.RO']
2017-04-07T11:26:47Z|2017-03-31T15:55:00Z|http://arxiv.org/abs/1703.10960v1|http://arxiv.org/pdf/1703.10960v1|Learning Discourse-level Diversity for Neural Dialog Models using   Conditional Variational Autoencoders|learn discours level divers neural dialog model use condit variat autoencod|While recent neural encoder-decoder models have shown great promise in modeling open-domain conversations, they often generate dull and generic responses. Unlike past work that has focused on diversifying the output of the decoder at word-level to alleviate this problem, we present a novel framework based on conditional variational autoencoders that captures the discourse-level diversity in the encoder. Our model uses latent variables to learn a distribution over potential conversational intents and generates diverse responses using only greedy decoders. We have further developed a novel variant that is integrated with linguistic prior knowledge for better performance. Finally, the training procedure is improved by introducing a bag-of-word loss. Our proposed models have been validated to generate significantly more diverse responses than baseline approaches and exhibit competence in discourse-level decision-making.|recent neural encod decod model shown great promis model open domain convers often generat dull generic respons unlik past work focus diversifi output decod word level allevi problem present novel framework base condit variat autoencod captur discours level divers encod model use latent variabl learn distribut potenti convers intent generat divers respons use onli greedi decod develop novel variant integr linguist prior knowledg better perform final train procedur improv introduc bag word loss propos model valid generat signific divers respons baselin approach exhibit compet discours level decis make|['Tiancheng Zhao', 'Ran Zhao', 'Maxine Eskenazi']|['cs.CL', 'cs.AI']
2017-04-07T11:26:47Z|2017-03-31T14:59:15Z|http://arxiv.org/abs/1703.10926v1|http://arxiv.org/abs/1703.10926v1|EMULATOR vs REAL PHONE: Android Malware Detection Using Machine Learning|emul vs real phone android malwar detect use machin learn|The Android operating system has become the most popular operating system for smartphones and tablets leading to a rapid rise in malware. Sophisticated Android malware employ detection avoidance techniques in order to hide their malicious activities from analysis tools. These include a wide range of anti-emulator techniques, where the malware programs attempt to hide their malicious activities by detecting the emulator. For this reason, countermeasures against antiemulation are becoming increasingly important in Android malware detection. Analysis and detection based on real devices can alleviate the problems of anti-emulation as well as improve the effectiveness of dynamic analysis. Hence, in this paper we present an investigation of machine learning based malware detection using dynamic analysis on real devices. A tool is implemented to automatically extract dynamic features from Android phones and through several experiments, a comparative analysis of emulator based vs. device based detection by means of several machine learning algorithms is undertaken. Our study shows that several features could be extracted more effectively from the on-device dynamic analysis compared to emulators. It was also found that approximately 24% more apps were successfully analysed on the phone. Furthermore, all of the studied machine learning based detection performed better when applied to features extracted from the on-device dynamic analysis.|android oper system becom popular oper system smartphon tablet lead rapid rise malwar sophist android malwar employ detect avoid techniqu order hide malici activ analysi tool includ wide rang anti emul techniqu malwar program attempt hide malici activ detect emul reason countermeasur antiemul becom increas import android malwar detect analysi detect base real devic allevi problem anti emul well improv effect dynam analysi henc paper present investig machin learn base malwar detect use dynam analysi real devic tool implement automat extract dynam featur android phone sever experi compar analysi emul base vs devic base detect mean sever machin learn algorithm undertaken studi show sever featur could extract effect devic dynam analysi compar emul also found approxim app success analys phone furthermor studi machin learn base detect perform better appli featur extract devic dynam analysi|['Mohammed K. Alzaylaee', 'Suleiman Y. Yerima', 'Sakir Sezer']|['cs.CR', 'cs.AI']
2017-04-07T11:26:47Z|2017-03-31T10:59:58Z|http://arxiv.org/abs/1703.10847v1|http://arxiv.org/pdf/1703.10847v1|MidiNet: A Convolutional Generative Adversarial Network for   Symbolic-domain Music Generation using 1D and 2D Conditions|midinet convolut generat adversari network symbol domain music generat use condit|In this paper, we present MidiNet, a deep convolutional neural network (CNN) based generative adversarial network (GAN) that is intended to provide a general, highly adaptive network structure for symbolic-domain music generation. The network takes random noise as input and generates a melody sequence one mea- sure (bar) after another. Moreover, it has a novel reflective CNN sub-model that allows us to guide the generation process by providing not only 1D but also 2D conditions. In our implementation, we used the intended chord of the current bar as a 1D condition to provide a harmonic context, and the melody generated for the preceding bar previously as a 2D condition to provide sequential information. The output of the network is a 16 by 128 matrix each time, representing the presence of each of the 128 MIDI notes in the generated melody sequence of that bar, with the smallest temporal unit being the sixteenth note. MidiNet can generate music of arbitrary number of bars, by concatenating these 16 by 128 matrices. The melody sequence can then be played back with a synthesizer. We provide example clips showing the effectiveness of MidiNet in generating harmonic music.|paper present midinet deep convolut neural network cnn base generat adversari network gan intend provid general high adapt network structur symbol domain music generat network take random nois input generat melodi sequenc one mea sure bar anoth moreov novel reflect cnn sub model allow us guid generat process provid onli also condit implement use intend chord current bar condit provid harmon context melodi generat preced bar previous condit provid sequenti inform output network matrix time repres presenc midi note generat melodi sequenc bar smallest tempor unit sixteenth note midinet generat music arbitrari number bar concaten matric melodi sequenc play back synthes provid exampl clip show effect midinet generat harmon music|['Li-Chia Yang', 'Szu-Yu Chou', 'Yi-Hsuan Yang']|['cs.SD', 'cs.AI']
2017-04-07T11:26:47Z|2017-03-30T19:51:03Z|http://arxiv.org/abs/1703.10651v1|http://arxiv.org/pdf/1703.10651v1|What-If Reasoning with Counterfactual Gaussian Processes|reason counterfactu gaussian process|"Answering ""What if?"" questions is important in many domains. For example, would a patient's disease progression slow down if I were to give them a dose of drug A? Ideally, we answer our question using an experiment, but this is not always possible (e.g., it may be unethical). As an alternative, we can use non-experimental data to learn models that make counterfactual predictions of what we would observe had we run an experiment. In this paper, we propose a model to make counterfactual predictions about how continuous-time trajectories (time series) respond to sequences of actions taken in continuous-time. We develop our model within the potential outcomes framework of Neyman and Rubin. One challenge is that the assumptions commonly made to learn potential outcome (counterfactual) models from observational data are not applicable in continuous-time as-is. We therefore propose a model using marked point processes and Gaussian processes, and develop alternative assumptions that allow us to learn counterfactual models from continuous-time observational data. We evaluate our approach on two tasks from health care: disease trajectory prediction and personalized treatment planning."|answer question import mani domain exampl would patient diseas progress slow give dose drug ideal answer question use experi alway possibl may uneth altern use non experiment data learn model make counterfactu predict would observ run experi paper propos model make counterfactu predict continu time trajectori time seri respond sequenc action taken continu time develop model within potenti outcom framework neyman rubin one challeng assumpt common made learn potenti outcom counterfactu model observ data applic continu time therefor propos model use mark point process gaussian process develop altern assumpt allow us learn counterfactu model continu time observ data evalu approach two task health care diseas trajectori predict person treatment plan|['Peter Schulam', 'Suchi Saria']|['stat.ML', 'cs.AI', 'cs.LG']
2017-04-07T11:26:47Z|2017-03-30T17:25:47Z|http://arxiv.org/abs/1703.10579v1|http://arxiv.org/pdf/1703.10579v1|Evaluating Complex Task through Crowdsourcing: Multiple Views Approach|evalu complex task crowdsourc multipl view approach|With the popularity of massive open online courses, grading through crowdsourcing has become a prevalent approach towards large scale classes. However, for getting grades for complex tasks, which require specific skills and efforts for grading, crowdsourcing encounters a restriction of insufficient knowledge of the workers from the crowd. Due to knowledge limitation of the crowd graders, grading based on partial perspectives becomes a big challenge for evaluating complex tasks through crowdsourcing. Especially for those tasks which not only need specific knowledge for grading, but also should be graded as a whole instead of being decomposed into smaller and simpler subtasks. We propose a framework for grading complex tasks via multiple views, which are different grading perspectives defined by experts for the task, to provide uniformity. Aggregation algorithm based on graders variances are used to combine the grades for each view. We also detect bias patterns of the graders, and debias them regarding each view of the task. Bias pattern determines how the behavior is biased among graders, which is detected by a statistical technique. The proposed approach is analyzed on a synthetic data set. We show that our model gives more accurate results compared to the grading approaches without different views and debiasing algorithm.|popular massiv open onlin cours grade crowdsourc becom preval approach toward larg scale class howev get grade complex task requir specif skill effort grade crowdsourc encount restrict insuffici knowledg worker crowd due knowledg limit crowd grader grade base partial perspect becom big challeng evalu complex task crowdsourc especi task onli need specif knowledg grade also grade whole instead decompos smaller simpler subtask propos framework grade complex task via multipl view differ grade perspect defin expert task provid uniform aggreg algorithm base grader varianc use combin grade view also detect bias pattern grader debia regard view task bias pattern determin behavior bias among grader detect statist techniqu propos approach analyz synthet data set show model give accur result compar grade approach without differ view debias algorithm|['Lingyu Lyu', 'Mehmed Kantardzic']|['cs.AI', 'cs.HC']
2017-04-07T11:26:51Z|2017-03-30T17:09:39Z|http://arxiv.org/abs/1703.10571v1|http://arxiv.org/pdf/1703.10571v1|Bootstrapping Labelled Dataset Construction for Cow Tracking and   Behavior Analysis|bootstrap label dataset construct cow track behavior analysi|This paper introduces a new approach to the long-term tracking of an object in a challenging environment. The object is a cow and the environment is an enclosure in a cowshed. Some of the key challenges in this domain are a cluttered background, low contrast and high similarity between moving objects which greatly reduces the efficiency of most existing approaches, including those based on background subtraction. Our approach is split into object localization, instance segmentation, learning and tracking stages. Our solution is compared to a range of semi-supervised object tracking algorithms and we show that the performance is strong and well suited to subsequent analysis. We present our solution as a first step towards broader tracking and behavior monitoring for cows in precision agriculture with the ultimate objective of early detection of lameness.|paper introduc new approach long term track object challeng environ object cow environ enclosur cowsh key challeng domain clutter background low contrast high similar move object great reduc effici exist approach includ base background subtract approach split object local instanc segment learn track stage solut compar rang semi supervis object track algorithm show perform strong well suit subsequ analysi present solut first step toward broader track behavior monitor cow precis agricultur ultim object earli detect lame|['Aram Ter-Sarkisov', 'Robert Ross', 'John Kelleher']|['cs.CV', 'cs.AI', 'cs.LG']
2017-04-07T11:26:51Z|2017-03-30T16:02:25Z|http://arxiv.org/abs/1703.10545v1|http://arxiv.org/pdf/1703.10545v1|FairJudge: Trustworthy User Prediction in Rating Platforms|fairjudg trustworthi user predict rate platform|Rating platforms enable large-scale collection of user opinion about items (products, other users, etc.). However, many untrustworthy users give fraudulent ratings for excessive monetary gains. In the paper, we present FairJudge, a system to identify such fraudulent users. We propose three metrics: (i) the fairness of a user that quantifies how trustworthy the user is in rating the products, (ii) the reliability of a rating that measures how reliable the rating is, and (iii) the goodness of a product that measures the quality of the product. Intuitively, a user is fair if it provides reliable ratings that are close to the goodness of the product. We formulate a mutually recursive definition of these metrics, and further address cold start problems and incorporate behavioral properties of users and products in the formulation. We propose an iterative algorithm, FairJudge, to predict the values of the three metrics. We prove that FairJudge is guaranteed to converge in a bounded number of iterations, with linear time complexity. By conducting five different experiments on five rating platforms, we show that FairJudge significantly outperforms nine existing algorithms in predicting fair and unfair users. We reported the 100 most unfair users in the Flipkart network to their review fraud investigators, and 80 users were correctly identified (80% accuracy). The FairJudge algorithm is already being deployed at Flipkart.|rate platform enabl larg scale collect user opinion item product user etc howev mani untrustworthi user give fraudul rate excess monetari gain paper present fairjudg system identifi fraudul user propos three metric fair user quantifi trustworthi user rate product ii reliabl rate measur reliabl rate iii good product measur qualiti product intuit user fair provid reliabl rate close good product formul mutual recurs definit metric address cold start problem incorpor behavior properti user product formul propos iter algorithm fairjudg predict valu three metric prove fairjudg guarante converg bound number iter linear time complex conduct five differ experi five rate platform show fairjudg signific outperform nine exist algorithm predict fair unfair user report unfair user flipkart network review fraud investig user correct identifi accuraci fairjudg algorithm alreadi deploy flipkart|['Srijan Kumar', 'Bryan Hooi', 'Disha Makhija', 'Mohit Kumar', 'Christos Faloutsos', 'V. S. Subrahamanian']|['cs.SI', 'cs.AI', 'stat.ML']
2017-04-07T11:26:51Z|2017-03-30T13:54:51Z|http://arxiv.org/abs/1703.10476v1|http://arxiv.org/pdf/1703.10476v1|Speaking the Same Language: Matching Machine to Human Captions by   Adversarial Training|speak languag match machin human caption adversari train|While strong progress has been made in image captioning over the last years, machine and human captions are still quite distinct. A closer look reveals that this is due to the deficiencies in the generated word distribution, vocabulary size, and strong bias in the generators towards frequent captions. Furthermore, humans -- rightfully so -- generate multiple, diverse captions, due to the inherent ambiguity in the captioning task which is not considered in today's systems.   To address these challenges, we change the training objective of the caption generator from reproducing groundtruth captions to generating a set of captions that is indistinguishable from human generated captions. Instead of handcrafting such a learning target, we employ adversarial training in combination with an approximate Gumbel sampler to implicitly match the generated distribution to the human one. While our method achieves comparable performance to the state-of-the-art in terms of the correctness of the captions, we generate a set of diverse captions, that are significantly less biased and match the word statistics better in several aspects.|strong progress made imag caption last year machin human caption still quit distinct closer look reveal due defici generat word distribut vocabulari size strong bias generat toward frequent caption furthermor human right generat multipl divers caption due inher ambigu caption task consid today system address challeng chang train object caption generat reproduc groundtruth caption generat set caption indistinguish human generat caption instead handcraft learn target employ adversari train combin approxim gumbel sampler implicit match generat distribut human one method achiev compar perform state art term correct caption generat set divers caption signific less bias match word statist better sever aspect|['Rakshith Shetty', 'Marcus Rohrbach', 'Lisa Anne Hendricks', 'Mario Fritz', 'Bernt Schiele']|['cs.CV', 'cs.AI', 'cs.CL']
2017-04-07T11:26:51Z|2017-03-30T12:06:15Z|http://arxiv.org/abs/1703.10429v1|http://arxiv.org/pdf/1703.10429v1|An Empirical Approach for Modeling Fuzzy Geographical Descriptors|empir approach model fuzzi geograph descriptor|We present a novel heuristic approach that defines fuzzy geographical descriptors using data gathered from a survey with human subjects. The participants were asked to provide graphical interpretations of the descriptors `north' and `south' for the Galician region (Spain). Based on these interpretations, our approach builds fuzzy descriptors that are able to compute membership degrees for geographical locations. We evaluated our approach in terms of efficiency and precision. The fuzzy descriptors are meant to be used as the cornerstones of a geographical referring expression generation algorithm that is able to linguistically characterize geographical locations and regions. This work is also part of a general research effort that intends to establish a methodology which reunites the empirical studies traditionally practiced in data-to-text and the use of fuzzy sets to model imprecision and vagueness in words and expressions for text generation purposes.|present novel heurist approach defin fuzzi geograph descriptor use data gather survey human subject particip ask provid graphic interpret descriptor north south galician region spain base interpret approach build fuzzi descriptor abl comput membership degre geograph locat evalu approach term effici precis fuzzi descriptor meant use cornerston geograph refer express generat algorithm abl linguist character geograph locat region work also part general research effort intend establish methodolog reunit empir studi tradit practic data text use fuzzi set model imprecis vagu word express text generat purpos|['Alejandro Ramos-Soto', 'Jose M. Alonso', 'Ehud Reiter', 'Kees van Deemter', 'Albert Gatt']|['cs.AI']
2017-04-07T11:26:51Z|2017-03-30T09:10:09Z|http://arxiv.org/abs/1703.10371v1|http://arxiv.org/pdf/1703.10371v1|Born to Learn: the Inspiration, Progress, and Future of Evolved Plastic   Artificial Neural Networks|born learn inspir progress futur evolv plastic artifici neural network|Biological neural networks are systems of extraordinary computational capabilities shaped by evolution, development, and lifetime learning. The interplay of these elements leads to the emergence of adaptive behavior and intelligence, but the complexity of the whole system of interactions is an obstacle to the understanding of the key factors at play. Inspired by such intricate natural phenomena, Evolved Plastic Artificial Neural Networks (EPANNs) use simulated evolution in-silico to breed plastic neural networks, artificial systems composed of sensors, outputs, and plastic components that change in response to sensory-output experiences in an environment. These systems may reveal key algorithmic ingredients of adaptation, autonomously discover novel adaptive algorithms, and lead to hypotheses on the emergence of biological adaptation. EPANNs have seen considerable progress over the last two decades. Current scientific and technological advances in artificial neural networks are now setting the conditions for radically new approaches and results. In particular, the limitations of hand-designed structures and algorithms currently used in most deep neural networks could be overcome by more flexible and innovative solutions. This paper brings together a variety of inspiring ideas that define the field of EPANNs. The main computational methods and results are reviewed. Finally, new opportunities and developments are presented.|biolog neural network system extraordinari comput capabl shape evolut develop lifetim learn interplay element lead emerg adapt behavior intellig complex whole system interact obstacl understand key factor play inspir intric natur phenomena evolv plastic artifici neural network epann use simul evolut silico breed plastic neural network artifici system compos sensor output plastic compon chang respons sensori output experi environ system may reveal key algorithm ingredi adapt autonom discov novel adapt algorithm lead hypothes emerg biolog adapt epann seen consider progress last two decad current scientif technolog advanc artifici neural network set condit radic new approach result particular limit hand design structur algorithm current use deep neural network could overcom flexibl innov solut paper bring togeth varieti inspir idea defin field epann main comput method result review final new opportun develop present|['Andrea Soltoggio', 'Kenneth O. Stanley', 'Sebastian Risi']|['cs.NE', 'cs.AI']
2017-04-07T11:26:51Z|2017-03-30T07:50:15Z|http://arxiv.org/abs/1703.10342v1|http://arxiv.org/pdf/1703.10342v1|Efficient Benchmarking of Algorithm Configuration Procedures via   Model-Based Surrogates|effici benchmark algorithm configur procedur via model base surrog|The optimization of algorithm (hyper-)parameters is crucial for achieving peak performance across a wide range of domains, ranging from deep neural networks to solvers for hard combinatorial problems. The resulting algorithm configuration (AC) problem has attracted much attention from the machine learning community. However, the proper evaluation of new AC procedures is hindered by two key hurdles. First, AC benchmarks are hard to set up. Second and even more significantly, they are computationally expensive: a single run of an AC procedure involves many costly runs of the target algorithm whose performance is to be optimized in a given AC benchmark scenario. One common workaround is to optimize cheap-to-evaluate artificial benchmark functions (e.g., Branin) instead of actual algorithms; however, these have different properties than realistic AC problems. Here, we propose an alternative benchmarking approach that is similarly cheap to evaluate but much closer to the original AC problem: replacing expensive benchmarks by surrogate benchmarks constructed from AC benchmarks. These surrogate benchmarks approximate the response surface corresponding to true target algorithm performance using a regression model, and the original and surrogate benchmark share the same (hyper-)parameter space. In our experiments, we construct and evaluate surrogate benchmarks for hyperparameter optimization as well as for AC problems that involve performance optimization of solvers for hard combinatorial problems, drawing training data from the runs of existing AC procedures. We show that our surrogate benchmarks capture overall important characteristics of the AC scenarios, such as high- and low-performing regions, from which they were derived, while being much easier to use and orders of magnitude cheaper to evaluate.|optim algorithm hyper paramet crucial achiev peak perform across wide rang domain rang deep neural network solver hard combinatori problem result algorithm configur ac problem attract much attent machin learn communiti howev proper evalu new ac procedur hinder two key hurdl first ac benchmark hard set second even signific comput expens singl run ac procedur involv mani cost run target algorithm whose perform optim given ac benchmark scenario one common workaround optim cheap evalu artifici benchmark function branin instead actual algorithm howev differ properti realist ac problem propos altern benchmark approach similar cheap evalu much closer origin ac problem replac expens benchmark surrog benchmark construct ac benchmark surrog benchmark approxim respons surfac correspond true target algorithm perform use regress model origin surrog benchmark share hyper paramet space experi construct evalu surrog benchmark hyperparamet optim well ac problem involv perform optim solver hard combinatori problem draw train data run exist ac procedur show surrog benchmark captur overal import characterist ac scenario high low perform region deriv much easier use order magnitud cheaper evalu|['Katharina Eggensperger', 'Marius Lindauer', 'Holger H. Hoos', 'Frank Hutter', 'Kevin Leyton-Brown']|['cs.AI', 'stat.ML']
2017-04-07T11:26:51Z|2017-03-30T05:20:18Z|http://arxiv.org/abs/1703.10316v1|http://arxiv.org/pdf/1703.10316v1|Efficient Parallel Translating Embedding For Knowledge Graphs|effici parallel translat embed knowledg graph|Knowledge graph embedding aims to embed entities and relations of knowledge graphs into low-dimensional vector spaces. Translating embedding methods regard relations as the translation from head entities to tail entities, which achieve the state-of-the-art results among knowledge graph embedding methods. However, a major limitation of these methods is the time consuming training process, which may take several days or even weeks for large knowledge graphs, and result in great difficulty in practical applications. In this paper, we propose an efficient parallel framework for translating embedding methods, called ParTrans-X, which enables the methods to be paralleled without locks by utilizing the distinguished structures of knowledge graphs. Experiments on two datasets with three typical translating embedding methods, i.e., TransE [3], TransH [17], and a more efficient variant TransE- AdaGrad [10] validate that ParTrans-X can speed up the training process by more than an order of magnitude.|knowledg graph embed aim emb entiti relat knowledg graph low dimension vector space translat embed method regard relat translat head entiti tail entiti achiev state art result among knowledg graph embed method howev major limit method time consum train process may take sever day even week larg knowledg graph result great difficulti practic applic paper propos effici parallel framework translat embed method call partran enabl method parallel without lock util distinguish structur knowledg graph experi two dataset three typic translat embed method trans transh effici variant trans adagrad valid partran speed train process order magnitud|['Denghui Zhang', 'Manling Li', 'Yantao Jia', 'Yuanzhuo Wang']|['cs.AI', 'I.2.4']
2017-04-07T11:26:51Z|2017-03-30T01:35:01Z|http://arxiv.org/abs/1703.10284v1|http://arxiv.org/pdf/1703.10284v1|Enter the Matrix: A Virtual World Approach to Safely Interruptable   Autonomous Systems|enter matrix virtual world approach safe interrupt autonom system|Robots and autonomous systems that operate around humans will likely always rely on kill switches that stop their execution and allow them to be remote-controlled for the safety of humans or to prevent damage to the system. It is theoretically possible for an autonomous system with sufficient sensor and effector capability and using reinforcement learning to learn that the kill switch deprives it of long-term reward and learn to act to disable the switch or otherwise prevent a human operator from using the switch. This is referred to as the big red button problem. We present a technique which prevents a reinforcement learning agent from learning to disable the big red button. Our technique interrupts the agent or robot by placing it in a virtual simulation where it continues to receive reward. We illustrate our technique in a simple grid world environment.|robot autonom system oper around human like alway reli kill switch stop execut allow remot control safeti human prevent damag system theoret possibl autonom system suffici sensor effector capabl use reinforc learn learn kill switch depriv long term reward learn act disabl switch otherwis prevent human oper use switch refer big red button problem present techniqu prevent reinforc learn agent learn disabl big red button techniqu interrupt agent robot place virtual simul continu receiv reward illustr techniqu simpl grid world environ|['Mark O. Riedl', 'Brent Harrison']|['cs.AI', 'cs.LG']
2017-04-07T11:26:51Z|2017-03-29T22:15:46Z|http://arxiv.org/abs/1703.10254v1|http://arxiv.org/pdf/1703.10254v1|Bandit-Based Model Selection for Deformable Object Manipulation|bandit base model select deform object manipul|"We present a novel approach to deformable object manipulation that does not rely on highly-accurate modeling. The key contribution of this paper is to formulate the task as a Multi-Armed Bandit problem, with each arm representing a model of the deformable object. To ""pull"" an arm and evaluate its utility, we use the arm's model to generate a velocity command for the gripper(s) holding the object and execute it. As the task proceeds and the object deforms, the utility of each model can change. Our framework estimates these changes and balances exploration of the model set with exploitation of high-utility models. We also propose an approach based on Kalman Filtering for Non-stationary Multi-armed Normal Bandits (KF-MANB) to leverage the coupling between models to learn more from each arm pull. We demonstrate that our method outperforms previous methods on synthetic trials, and performs competitively on several manipulation tasks in simulation."|present novel approach deform object manipul doe reli high accur model key contribut paper formul task multi arm bandit problem arm repres model deform object pull arm evalu util use arm model generat veloc command gripper hold object execut task proceed object deform util model chang framework estim chang balanc explor model set exploit high util model also propos approach base kalman filter non stationari multi arm normal bandit kf manb leverag coupl model learn arm pull demonstr method outperform previous method synthet trial perform competit sever manipul task simul|['Dale McConachie', 'Dmitry Berenson']|['cs.RO', 'cs.AI']
2017-04-07T11:26:51Z|2017-03-29T21:52:52Z|http://arxiv.org/abs/1703.10251v1|http://arxiv.org/pdf/1703.10251v1|Dialectical Rough Sets, Parthood and Figures of Opposition|dialect rough set parthood figur opposit|In one perspective, the central problem pursued in this research is that of the inverse problem in the context of general rough sets. The problem is about the existence of rough basis for given approximations in a context. Granular operator spaces were recently introduced by the present author as an optimal framework for anti-chain based algebraic semantics of general rough sets and the inverse problem. In the framework, various subtypes of crisp and non crisp objects are identifiable that may be missed in more restrictive formalism. This is also because in the latter cases the concept of complementation and negation are taken for granted. This opens the door for a general approach to dialectical rough sets building on previous work of the present author and figures of opposition. In this paper dialectical rough logics are developed from a semantic perspective, concept of dialectical predicates is formalized, connection with dialethias and glutty negation established, parthood analyzed and studied from the point of view of classical and dialectical figures of opposition. Potential semantics through dialectical counting based on these figures are proposed building on earlier work by the present author. Her methods become more geometrical and encompass parthood as a primary relation (as opposed to roughly equivalent objects) for algebraic semantics. Dialectical counting strategies over anti chains (a specific form of dialectical structure) for semantics are also proposed.|one perspect central problem pursu research invers problem context general rough set problem exist rough basi given approxim context granular oper space recent introduc present author optim framework anti chain base algebra semant general rough set invers problem framework various subtyp crisp non crisp object identifi may miss restrict formal also becaus latter case concept complement negat taken grant open door general approach dialect rough set build previous work present author figur opposit paper dialect rough logic develop semant perspect concept dialect predic formal connect dialethia glutti negat establish parthood analyz studi point view classic dialect figur opposit potenti semant dialect count base figur propos build earlier work present author method becom geometr encompass parthood primari relat oppos rough equival object algebra semant dialect count strategi anti chain specif form dialect structur semant also propos|['A. Mani']|['math.LO', 'cs.AI', 'cs.IT', 'cs.LO', 'math.IT', '03B60, 68T27, 03B52, 68T30']
2017-04-07T11:26:55Z|2017-03-29T20:02:39Z|http://arxiv.org/abs/1704.01006v1|http://arxiv.org/pdf/1704.01006v1|Ontology based Scene Creation for the Development of Automated Vehicles|ontolog base scene creation develop autom vehicl|The introduction of automated vehicles without permanent human supervision demands a functional system description including functional system boundaries and a comprehensive safety analysis. These inputs to the technical development can be identified and analyzed by a scenario-based approach. Experts are doing well to identify scenarios that are difficult to handle or unlikely to happen. To establish an economical test and release process, a large number of scenarios must be identified to enable an execution of test-cases in simulation environments. Expert knowledge modeled for computer aided processing may help for the purpose of providing a wide range of scenarios. This contribution reviews the use of ontologies as knowledge-based systems in the field of automated vehicles, and proposes a modeling concept for traffic scenes. Afterwards, a process to create scenes from the gathered knowledge is introduced and the utilization of ontologies in the given problem statement is evaluated using criteria from literature.|introduct autom vehicl without perman human supervis demand function system descript includ function system boundari comprehens safeti analysi input technic develop identifi analyz scenario base approach expert well identifi scenario difficult handl unlik happen establish econom test releas process larg number scenario must identifi enabl execut test case simul environ expert knowledg model comput aid process may help purpos provid wide rang scenario contribut review use ontolog knowledg base system field autom vehicl propos model concept traffic scene afterward process creat scene gather knowledg introduc util ontolog given problem statement evalu use criteria literatur|['Gerrit Bagschik', 'Till Menzel', 'Markus Maurer']|['cs.AI', 'cs.RO']
2017-04-07T11:26:55Z|2017-03-29T16:29:04Z|http://arxiv.org/abs/1703.10121v1|http://arxiv.org/pdf/1703.10121v1|The Top 10 Topics in Machine Learning Revisited: A Quantitative   Meta-Study|top topic machin learn revisit quantit meta studi|Which topics of machine learning are most commonly addressed in research? This question was initially answered in 2007 by doing a qualitative survey among distinguished researchers. In our study, we revisit this question from a quantitative perspective. Concretely, we collect 54K abstracts of papers published between 2007 and 2016 in leading machine learning journals and conferences. We then use machine learning in order to determine the top 10 topics in machine learning. We not only include models, but provide a holistic view across optimization, data, features, etc. This quantitative approach allows reducing the bias of surveys. It reveals new and up-to-date insights into what the 10 most prolific topics in machine learning research are. This allows researchers to identify popular topics as well as new and rising topics for their research.|topic machin learn common address research question initi answer qualit survey among distinguish research studi revisit question quantit perspect concret collect abstract paper publish lead machin learn journal confer use machin learn order determin top topic machin learn onli includ model provid holist view across optim data featur etc quantit approach allow reduc bias survey reveal new date insight prolif topic machin learn research allow research identifi popular topic well new rise topic research|['Patrick Glauner', 'Manxing Du', 'Victor Paraschiv', 'Andrey Boytsov', 'Isabel Lopez Andrade', 'Jorge Meira', 'Petko Valtchev', 'Radu State']|['cs.LG', 'cs.AI']
2017-04-07T11:26:55Z|2017-03-29T15:30:40Z|http://arxiv.org/abs/1703.10098v1|http://arxiv.org/pdf/1703.10098v1|Rational Choice and Artificial Intelligence|ration choic artifici intellig|The theory of rational choice assumes that when people make decisions they do so in order to maximize their utility. In order to achieve this goal they ought to use all the information available and consider all the choices available to choose an optimal choice. This paper investigates what happens when decisions are made by artificially intelligent machines in the market rather than human beings. Firstly, the expectations of the future are more consistent if they are made by an artificially intelligent machine and the decisions are more rational and thus marketplace becomes more rational.|theori ration choic assum peopl make decis order maxim util order achiev goal ought use inform avail consid choic avail choos optim choic paper investig happen decis made artifici intellig machin market rather human first expect futur consist made artifici intellig machin decis ration thus marketplac becom ration|['Tshilidzi Marwala']|['cs.AI']
2017-04-07T11:26:55Z|2017-03-29T15:20:01Z|http://arxiv.org/abs/1704.00045v1|http://arxiv.org/pdf/1704.00045v1|Comparison of ontology alignment algorithms across single matching task   via the McNemar test|comparison ontolog align algorithm across singl match task via mcnemar test|Ontology alignment is widely used to find the correspondences between different ontologies in diverse fields. After discovering the alignment by methods, several performance scores are available to evaluate them. The scores require the produced alignment by a method and the reference alignment containing the underlying actual correspondences of the given ontologies. The current trend in alignment evaluation is to put forward a new score and to compare various alignments by juxtaposing their performance scores. However, it is substantially provocative to select one performance score among others for comparison. On top of that, claiming if one method has a better performance than one another can not be substantiated by solely comparing the scores. In this paper, we propose the statistical procedures which enable us to theoretically favor one method over one another. The McNemar test is considered as a reliable and suitable means for comparing two ontology alignment methods over one matching task. The test applies to a 2 x 2 contingency table which can be constructed in two different ways based on the alignments, each of which has their own merits/pitfalls. The ways of the contingency table construction and various apposite statistics from the McNemar test are elaborated in minute detail. In the case of having more than two alignment methods for comparison, the family-wise error rate is expected to happen. Thus, the ways of preventing such an error are also discussed. A directed graph visualizes the outcome of the McNemar test in the presence of multiple alignment methods. From this graph, it is readily understood if one method is better than one another or if their differences are imperceptible. Our investigation on the methods participated in the anatomy track of OAEI 2016 demonstrates that AML and CroMatcher are the top two methods and DKP-AOM and Alin are the bottom two ones.|ontolog align wide use find correspond differ ontolog divers field discov align method sever perform score avail evalu score requir produc align method refer align contain actual correspond given ontolog current trend align evalu put forward new score compar various align juxtapos perform score howev substanti provoc select one perform score among comparison top claim one method better perform one anoth substanti sole compar score paper propos statist procedur enabl us theoret favor one method one anoth mcnemar test consid reliabl suitabl mean compar two ontolog align method one match task test appli conting tabl construct two differ way base align merit pitfal way conting tabl construct various apposit statist mcnemar test elabor minut detail case two align method comparison famili wise error rate expect happen thus way prevent error also discuss direct graph visual outcom mcnemar test presenc multipl align method graph readili understood one method better one anoth differ impercept investig method particip anatomi track oaei demonstr aml cromatch top two method dkp aom alin bottom two one|['Majid Mohammadi', 'Amir Ahooye Atashin', 'Wout Hofman', 'Yaohua Tan']|['cs.AI']
2017-04-07T11:26:55Z|2017-03-29T14:37:25Z|http://arxiv.org/abs/1703.10069v1|http://arxiv.org/pdf/1703.10069v1|Multiagent Bidirectionally-Coordinated Nets for Learning to Play   StarCraft Combat Games|multiag bidirect coordin net learn play starcraft combat game|Real-world artificial intelligence (AI) applications often require multiple agents to work in a collaborative effort. Efficient learning for intra-agent communication and coordination is an indispensable step towards general AI. In this paper, we take StarCraft combat game as the test scenario, where the task is to coordinate multiple agents as a team to defeat their enemies. To maintain a scalable yet effective communication protocol, we introduce a multiagent bidirectionally-coordinated network (BiCNet ['bIknet]) with a vectorised extension of actor-critic formulation. We show that BiCNet can handle different types of combats under diverse terrains with arbitrary numbers of AI agents for both sides. Our analysis demonstrates that without any supervisions such as human demonstrations or labelled data, BiCNet could learn various types of coordination strategies that is similar to these of experienced game players. Moreover, BiCNet is easily adaptable to the tasks with heterogeneous agents. In our experiments, we evaluate our approach against multiple baselines under different scenarios; it shows state-of-the-art performance, and possesses potential values for large-scale real-world applications.|real world artifici intellig ai applic often requir multipl agent work collabor effort effici learn intra agent communic coordin indispens step toward general ai paper take starcraft combat game test scenario task coordin multipl agent team defeat enemi maintain scalabl yet effect communic protocol introduc multiag bidirect coordin network bicnet biknet vectoris extens actor critic formul show bicnet handl differ type combat divers terrain arbitrari number ai agent side analysi demonstr without ani supervis human demonstr label data bicnet could learn various type coordin strategi similar experienc game player moreov bicnet easili adapt task heterogen agent experi evalu approach multipl baselin differ scenario show state art perform possess potenti valu larg scale real world applic|['Peng Peng', 'Quan Yuan', 'Ying Wen', 'Yaodong Yang', 'Zhenkun Tang', 'Haitao Long', 'Jun Wang']|['cs.AI', 'cs.LG']
2017-04-07T11:26:55Z|2017-03-29T10:31:04Z|http://arxiv.org/abs/1703.09962v1|http://arxiv.org/pdf/1703.09962v1|Spaceprint: a Mobility-based Fingerprinting Scheme for Public Spaces|spaceprint mobil base fingerprint scheme public space|In this paper, we address the problem of how automated situation-awareness can be achieved by learning real-world situations from ubiquitously generated mobility data. Without semantic input about the time and space where situations take place, this turns out to be a fundamental challenging problem. Uncertainties also introduce technical challenges when data is generated in irregular time intervals, being mixed with noise, and errors. Purely relying on temporal patterns observable in mobility data, in this paper, we propose Spaceprint, a fully automated algorithm for finding the repetitive pattern of similar situations in spaces. We evaluate this technique by showing how the latent variables describing the category, and the actual identity of a space can be discovered from the extracted situation patterns. Doing so, we use different real-world mobility datasets with data about the presence of mobile entities in a variety of spaces. We also evaluate the performance of this technique by showing its robustness against uncertainties.|paper address problem autom situat awar achiev learn real world situat ubiquit generat mobil data without semant input time space situat take place turn fundament challeng problem uncertainti also introduc technic challeng data generat irregular time interv mix nois error pure reli tempor pattern observ mobil data paper propos spaceprint fulli autom algorithm find repetit pattern similar situat space evalu techniqu show latent variabl describ categori actual ident space discov extract situat pattern use differ real world mobil dataset data presenc mobil entiti varieti space also evalu perform techniqu show robust uncertainti|['Mitra Baratchi', 'Geert Heijenk', 'Maarten van Steen']|['cs.AI']
2017-04-07T11:26:55Z|2017-03-29T07:53:43Z|http://arxiv.org/abs/1703.09923v1|http://arxiv.org/pdf/1703.09923v1|On Convergence Property of Implicit Self-paced Objective|converg properti implicit self pace object|Self-paced learning (SPL) is a new methodology that simulates the learning principle of humans/animals to start learning easier aspects of a learning task, and then gradually take more complex examples into training. This new-coming learning regime has been empirically substantiated to be effective in various computer vision and pattern recognition tasks. Recently, it has been proved that the SPL regime has a close relationship to a implicit self-paced objective function. While this implicit objective could provide helpful interpretations to the effectiveness, especially the robustness, insights under the SPL paradigms, there are still no theoretical results strictly proved to verify such relationship. To this issue, in this paper, we provide some convergence results on this implicit objective of SPL. Specifically, we prove that the learning process of SPL always converges to critical points of this implicit objective under some mild conditions. This result verifies the intrinsic relationship between SPL and this implicit objective, and makes the previous robustness analysis on SPL complete and theoretically rational.|self pace learn spl new methodolog simul learn principl human anim start learn easier aspect learn task gradual take complex exampl train new come learn regim empir substanti effect various comput vision pattern recognit task recent prove spl regim close relationship implicit self pace object function implicit object could provid help interpret effect especi robust insight spl paradigm still theoret result strict prove verifi relationship issu paper provid converg result implicit object spl specif prove learn process spl alway converg critic point implicit object mild condit result verifi intrins relationship spl implicit object make previous robust analysi spl complet theoret ration|['Zilu Ma', 'Shiqi Liu', 'Deyu Meng']|['cs.AI']
2017-04-07T11:26:55Z|2017-03-29T06:51:00Z|http://arxiv.org/abs/1703.09902v1|http://arxiv.org/pdf/1703.09902v1|Survey of the State of the Art in Natural Language Generation: Core   tasks, applications and evaluation|survey state art natur languag generat core task applic evalu|This paper surveys the current state of the art in Natural Language Generation (NLG), defined as the task of generating text or speech from non-linguistic input. A survey of NLG is timely in view of the changes that the field has undergone over the past decade or so, especially in relation to new (usually data-driven) methods, as well as new applications of NLG technology. This survey therefore aims to (a) give an up-to-date synthesis of research on the core tasks in NLG and the architectures adopted in which such tasks are organised; (b) highlight a number of relatively recent research topics that have arisen partly as a result of growing synergies between NLG and other areas of artificial intelligence; (c) draw attention to the challenges in NLG evaluation, relating them to similar challenges faced in other areas of Natural Language Processing, with an emphasis on different evaluation methods and the relationships between them.|paper survey current state art natur languag generat nlg defin task generat text speech non linguist input survey nlg time view chang field undergon past decad especi relat new usual data driven method well new applic nlg technolog survey therefor aim give date synthesi research core task nlg architectur adopt task organis highlight number relat recent research topic arisen part result grow synergi nlg area artifici intellig draw attent challeng nlg evalu relat similar challeng face area natur languag process emphasi differ evalu method relationship|['Albert Gatt', 'Emiel Krahmer']|['cs.CL', 'cs.AI', 'cs.NE', 'I.2.7; H.5']
2017-04-07T11:26:55Z|2017-03-29T05:58:21Z|http://arxiv.org/abs/1703.09891v1|http://arxiv.org/pdf/1703.09891v1|LabelBank: Revisiting Global Perspectives for Semantic Segmentation|labelbank revisit global perspect semant segment|Semantic segmentation requires a detailed labeling of image pixels by object category. Information derived from local image patches is necessary to describe the detailed shape of individual objects. However, this information is ambiguous and can result in noisy labels. Global inference of image content can instead capture the general semantic concepts present. We advocate that holistic inference of image concepts provides valuable information for detailed pixel labeling. We propose a generic framework to leverage holistic information in the form of a LabelBank for pixel-level segmentation.   We show the ability of our framework to improve semantic segmentation performance in a variety of settings. We learn models for extracting a holistic LabelBank from visual cues, attributes, and/or textual descriptions. We demonstrate improvements in semantic segmentation accuracy on standard datasets across a range of state-of-the-art segmentation architectures and holistic inference approaches.|semant segment requir detail label imag pixel object categori inform deriv local imag patch necessari describ detail shape individu object howev inform ambigu result noisi label global infer imag content instead captur general semant concept present advoc holist infer imag concept provid valuabl inform detail pixel label propos generic framework leverag holist inform form labelbank pixel level segment show abil framework improv semant segment perform varieti set learn model extract holist labelbank visual cue attribut textual descript demonstr improv semant segment accuraci standard dataset across rang state art segment architectur holist infer approach|['Hexiang Hu', 'Zhiwei Deng', 'Guang-Tong Zhou', 'Fei Sha', 'Greg Mori']|['cs.CV', 'cs.AI', 'cs.LG']
2017-04-07T11:26:55Z|2017-03-29T00:21:02Z|http://arxiv.org/abs/1703.09845v1|http://arxiv.org/pdf/1703.09845v1|Bringing Salary Transparency to the World: Computing Robust Compensation   Insights via LinkedIn Salary|bring salari transpar world comput robust compens insight via linkedin salari|The recently launched LinkedIn Salary product has been designed to realize the vision of helping the world's professionals optimize their earning potential through salary transparency. We describe the overall design and architecture of the salary modeling system underlying this product. We focus on the unique data mining challenges in designing and implementing the system, and describe the modeling components such as outlier detection and Bayesian hierarchical smoothing that help to compute and present robust compensation insights to users. We report on extensive evaluation with nearly one year of anonymized compensation data collected from over one million LinkedIn users, thereby demonstrating the efficacy of the statistical models. We also highlight the lessons learned through the deployment of our system at LinkedIn.|recent launch linkedin salari product design realiz vision help world profession optim earn potenti salari transpar describ overal design architectur salari model system product focus uniqu data mine challeng design implement system describ model compon outlier detect bayesian hierarch smooth help comput present robust compens insight user report extens evalu near one year anonym compens data collect one million linkedin user therebi demonstr efficaci statist model also highlight lesson learn deploy system linkedin|['Krishnaram Kenthapadi', 'Stuart Ambler', 'Liang Zhang', 'Deepak Agarwal']|['cs.SI', 'cs.AI', 'cs.IR']
2017-04-07T11:26:59Z|2017-04-03T01:35:06Z|http://arxiv.org/abs/1703.10970v2|http://arxiv.org/pdf/1703.10970v2|Diversity of preferences can increase collective welfare in sequential   exploration problems|divers prefer increas collect welfar sequenti explor problem|In search engines, online marketplaces and other human-computer interfaces large collectives of individuals sequentially interact with numerous alternatives of varying quality. In these contexts, trial and error (exploration) is crucial for uncovering novel high-quality items or solutions, but entails a high cost for individual users. Self-interested decision makers, are often better off imitating the choices of individuals who have already incurred the costs of exploration. Although imitation makes sense at the individual level, it deprives the group of additional information that could have been gleaned by individual explorers. In this paper we show that in such problems, preference diversity can function as a welfare enhancing mechanism. It leads to a consistent increase in the quality of the consumed alternatives that outweighs the increased cost of search for the users.|search engin onlin marketplac human comput interfac larg collect individu sequenti interact numer altern vari qualiti context trial error explor crucial uncov novel high qualiti item solut entail high cost individu user self interest decis maker often better imit choic individu alreadi incur cost explor although imit make sens individu level depriv group addit inform could glean individu explor paper show problem prefer divers function welfar enhanc mechan lead consist increas qualiti consum altern outweigh increas cost search user|['Pantelis P. Analytis', 'Hrvoje Stojic', 'Alexandros Gelastopoulos', 'Mehdi Moussaïd']|['cs.AI', 'cs.MA']
2017-04-07T11:26:59Z|2017-03-28T17:48:07Z|http://arxiv.org/abs/1703.09684v1|http://arxiv.org/pdf/1703.09684v1|An Analysis of Visual Question Answering Algorithms|analysi visual question answer algorithm|In visual question answering (VQA), an algorithm must answer text-based questions about images. While multiple datasets for VQA have been created since late 2014, they all have flaws in both their content and the way algorithms are evaluated on them. As a result, evaluation scores are inflated and predominantly determined by answering easier questions, making it difficult to compare different methods. In this paper, we analyze existing VQA algorithms using a new dataset. It contains over 1.6 million questions organized into 12 different categories. We also introduce questions that are meaningless for a given image to force a VQA system to reason about image content. We propose new evaluation schemes that compensate for over-represented question-types and make it easier to study the strengths and weaknesses of algorithms. We analyze the performance of both baseline and state-of-the-art VQA models, including multi-modal compact bilinear pooling (MCB), neural module networks, and recurrent answering units. Our experiments establish how attention helps certain categories more than others, determine which models work better than others, and explain how simple models (e.g. MLP) can surpass more complex models (MCB) by simply learning to answer large, easy question categories.|visual question answer vqa algorithm must answer text base question imag multipl dataset vqa creat sinc late flaw content way algorithm evalu result evalu score inflat predomin determin answer easier question make difficult compar differ method paper analyz exist vqa algorithm use new dataset contain million question organ differ categori also introduc question meaningless given imag forc vqa system reason imag content propos new evalu scheme compens repres question type make easier studi strength weak algorithm analyz perform baselin state art vqa model includ multi modal compact bilinear pool mcb neural modul network recurr answer unit experi establish attent help certain categori determin model work better explain simpl model mlp surpass complex model mcb simpli learn answer larg easi question categori|['Kushal Kafle', 'Christopher Kanan']|['cs.CV', 'cs.AI', 'cs.CL']
2017-04-07T11:26:59Z|2017-03-28T16:13:23Z|http://arxiv.org/abs/1703.09700v1|http://arxiv.org/pdf/1703.09700v1|Inverse Reinforcement Learning from Summary Data|invers reinforc learn summari data|Inverse reinforcement learning (IRL) aims to explain observed complex behavior by fitting reinforcement learning models to behavioral data. However, traditional IRL methods are only applicable when the observations are in the form of state-action paths. This is a problem in many real-world modelling settings, where only more limited observations are easily available. To address this issue, we extend the traditional IRL problem formulation. We call this new formulation the inverse reinforcement learning from summary data (IRL-SD) problem, where instead of state-action paths, only summaries of the paths are observed. We propose exact and approximate methods for both maximum likelihood and full posterior estimation for IRL-SD problems. Through case studies we compare these methods, demonstrating that the approximate methods can be used to solve moderate-sized IRL-SD problems in reasonable time.|invers reinforc learn irl aim explain observ complex behavior fit reinforc learn model behavior data howev tradit irl method onli applic observ form state action path problem mani real world model set onli limit observ easili avail address issu extend tradit irl problem formul call new formul invers reinforc learn summari data irl sd problem instead state action path onli summari path observ propos exact approxim method maximum likelihood full posterior estim irl sd problem case studi compar method demonstr approxim method use solv moder size irl sd problem reason time|['Antti Kangasrääsiö', 'Samuel Kaski']|['cs.LG', 'cs.AI', 'stat.ML']
2017-04-07T11:26:59Z|2017-03-28T15:00:57Z|http://arxiv.org/abs/1703.09620v1|http://arxiv.org/pdf/1703.09620v1|Universal Reasoning, Rational Argumentation and Human-Machine   Interaction|univers reason ration argument human machin interact|Classical higher-order logic, when utilized as a meta-logic in which various other (classical and non-classical) logics can be shallowly embedded, is well suited for realising a universal logic reasoning approach. Universal logic reasoning in turn, as envisioned already by Leibniz, may support the rigorous formalisation and deep logical analysis of rational arguments within machines. A respective universal logic reasoning framework is described and a range of exemplary applications are discussed. In the future, universal logic reasoning in combination with appropriate, controlled forms of rational argumentation may serve as a communication layer between humans and intelligent machines.|classic higher order logic util meta logic various classic non classic logic shallowli embed well suit realis univers logic reason approach univers logic reason turn envis alreadi leibniz may support rigor formalis deep logic analysi ration argument within machin respect univers logic reason framework describ rang exemplari applic discuss futur univers logic reason combin appropri control form ration argument may serv communic layer human intellig machin|['Christoph Benzmüller']|['cs.AI', '03Axx, 03Bxx, 03B15, 68T15', 'F.4; I.2.3; I.2.4']
2017-04-07T11:26:59Z|2017-03-28T12:08:46Z|http://arxiv.org/abs/1703.09527v1|http://arxiv.org/abs/1703.09527v1|Is This a Joke? Detecting Humor in Spanish Tweets|joke detect humor spanish tweet|While humor has been historically studied from a psychological, cognitive and linguistic standpoint, its study from a computational perspective is an area yet to be explored in Computational Linguistics. There exist some previous works, but a characterization of humor that allows its automatic recognition and generation is far from being specified. In this work we build a crowdsourced corpus of labeled tweets, annotated according to its humor value, letting the annotators subjectively decide which are humorous. A humor classifier for Spanish tweets is assembled based on supervised learning, reaching a precision of 84% and a recall of 69%.|humor histor studi psycholog cognit linguist standpoint studi comput perspect area yet explor comput linguist exist previous work character humor allow automat recognit generat far specifi work build crowdsourc corpus label tweet annot accord humor valu let annot subject decid humor humor classifi spanish tweet assembl base supervis learn reach precis recal|['Santiago Castro', 'Matías Cubero', 'Diego Garat', 'Guillermo Moncecchi']|['cs.CL', 'cs.AI']
2017-04-07T11:26:59Z|2017-03-28T11:40:44Z|http://arxiv.org/abs/1703.09513v1|http://arxiv.org/pdf/1703.09513v1|Mining Best Closed Itemsets for Projection-antimonotonic Constraints in   Polynomial Time|mine best close itemset project antimonoton constraint polynomi time|"The exponential explosion of the set of patterns is one of the main challenges in pattern mining. This challenge is approached by introducing a constraint for pattern selection. One of the first constraints proposed in pattern mining is support (frequency) of a pattern in a dataset. Frequency is an anti-monotonic function, i.e., given an infrequent pattern, all its superpatterns are not frequent. However, many other constraints for pattern selection are neither monotonic nor anti-monotonic, which makes it difficult to generate patterns satisfying these constraints.   In order to deal with nonmonotonic constraints we introduce the notion of ""projection antimonotonicity"" and SOFIA algorithm that allow generating best patterns for a class of nonmonotonic constraints. Cosine interest, robustness, stability of closed itemsets, and the associated delta-measure are among these constraints. SOFIA starts from light descriptions of transactions in dataset (a small set of items in the case of itemset description) and then iteratively adds more information to these descriptions (more items with indication of tidsets they describe)."|exponenti explos set pattern one main challeng pattern mine challeng approach introduc constraint pattern select one first constraint propos pattern mine support frequenc pattern dataset frequenc anti monoton function given infrequ pattern superpattern frequent howev mani constraint pattern select neither monoton anti monoton make difficult generat pattern satisfi constraint order deal nonmonoton constraint introduc notion project antimonoton sofia algorithm allow generat best pattern class nonmonoton constraint cosin interest robust stabil close itemset associ delta measur among constraint sofia start light descript transact dataset small set item case itemset descript iter add inform descript item indic tidset describ|['Aleksey Buzmakov', 'Sergei O. Kuznetsov', 'Amedeo Napoli']|['cs.AI']
2017-04-07T11:26:59Z|2017-03-28T03:24:33Z|http://arxiv.org/abs/1703.09387v1|http://arxiv.org/pdf/1703.09387v1|Adversarial Transformation Networks: Learning to Generate Adversarial   Examples|adversari transform network learn generat adversari exampl|Multiple different approaches of generating adversarial examples have been proposed to attack deep neural networks. These approaches involve either directly computing gradients with respect to the image pixels, or directly solving an optimization on the image pixels. In this work, we present a fundamentally new method for generating adversarial examples that is fast to execute and provides exceptional diversity of output. We efficiently train feed-forward neural networks in a self-supervised manner to generate adversarial examples against a target network or set of networks. We call such a network an Adversarial Transformation Network (ATN). ATNs are trained to generate adversarial examples that minimally modify the classifier's outputs given the original input, while constraining the new classification to match an adversarial target class. We present methods to train ATNs and analyze their effectiveness targeting a variety of MNIST classifiers as well as the latest state-of-the-art ImageNet classifier Inception ResNet v2.|multipl differ approach generat adversari exampl propos attack deep neural network approach involv either direct comput gradient respect imag pixel direct solv optim imag pixel work present fundament new method generat adversari exampl fast execut provid except divers output effici train feed forward neural network self supervis manner generat adversari exampl target network set network call network adversari transform network atn atn train generat adversari exampl minim modifi classifi output given origin input constrain new classif match adversari target class present method train atn analyz effect target varieti mnist classifi well latest state art imagenet classifi incept resnet|['Shumeet Baluja', 'Ian Fischer']|['cs.NE', 'cs.AI', 'cs.CV']
2017-04-07T11:26:59Z|2017-03-28T02:00:47Z|http://arxiv.org/abs/1703.09370v1|http://arxiv.org/pdf/1703.09370v1|Ensembles of Deep LSTM Learners for Activity Recognition using Wearables|ensembl deep lstm learner activ recognit use wearabl|Recently, deep learning (DL) methods have been introduced very successfully into human activity recognition (HAR) scenarios in ubiquitous and wearable computing. Especially the prospect of overcoming the need for manual feature design combined with superior classification capabilities render deep neural networks very attractive for real-life HAR application. Even though DL-based approaches now outperform the state-of-the-art in a number of recognitions tasks of the field, yet substantial challenges remain. Most prominently, issues with real-life datasets, typically including imbalanced datasets and problematic data quality, still limit the effectiveness of activity recognition using wearables. In this paper we tackle such challenges through Ensembles of deep Long Short Term Memory (LSTM) networks. We have developed modified training procedures for LSTM networks and combine sets of diverse LSTM learners into classifier collectives. We demonstrate, both formally and empirically, that Ensembles of deep LSTM learners outperform the individual LSTM networks. Through an extensive experimental evaluation on three standard benchmarks (Opportunity, PAMAP2, Skoda) we demonstrate the excellent recognition capabilities of our approach and its potential for real-life applications of human activity recognition.|recent deep learn dl method introduc veri success human activ recognit har scenario ubiquit wearabl comput especi prospect overcom need manual featur design combin superior classif capabl render deep neural network veri attract real life har applic even though dl base approach outperform state art number recognit task field yet substanti challeng remain promin issu real life dataset typic includ imbalanc dataset problemat data qualiti still limit effect activ recognit use wearabl paper tackl challeng ensembl deep long short term memori lstm network develop modifi train procedur lstm network combin set divers lstm learner classifi collect demonstr formal empir ensembl deep lstm learner outperform individu lstm network extens experiment evalu three standard benchmark opportun pamap skoda demonstr excel recognit capabl approach potenti real life applic human activ recognit|['Yu Guan', 'Thomas Ploetz']|['cs.LG', 'cs.AI', 'cs.CV']
2017-04-07T11:26:59Z|2017-03-28T01:51:34Z|http://arxiv.org/abs/1703.09368v1|http://arxiv.org/pdf/1703.09368v1|Learning and inference in knowledge-based probabilistic model for   medical diagnosis|learn infer knowledg base probabilist model medic diagnosi|Based on a weighted knowledge graph to represent first-order knowledge and combining it with a probabilistic model, we propose a methodology for the creation of a medical knowledge network (MKN) in medical diagnosis. When a set of symptoms is activated for a specific patient, we can generate a ground medical knowledge network composed of symptom nodes and potential disease nodes. By Incorporating a Boltzmann machine into the potential function of a Markov network, we investigated the joint probability distribution of the MKN. In order to deal with numerical symptoms, a multivariate inference model is presented that uses conditional probability. In addition, the weights for the knowledge graph were efficiently learned from manually annotated Chinese Electronic Medical Records (CEMRs). In our experiments, we found numerically that the optimum choice of the quality of disease node and the expression of symptom variable can improve the effectiveness of medical diagnosis. Our experimental results comparing a Markov logic network and the logistic regression algorithm on an actual CEMR database indicate that our method holds promise and that MKN can facilitate studies of intelligent diagnosis.|base weight knowledg graph repres first order knowledg combin probabilist model propos methodolog creation medic knowledg network mkn medic diagnosi set symptom activ specif patient generat ground medic knowledg network compos symptom node potenti diseas node incorpor boltzmann machin potenti function markov network investig joint probabl distribut mkn order deal numer symptom multivari infer model present use condit probabl addit weight knowledg graph effici learn manual annot chines electron medic record cemr experi found numer optimum choic qualiti diseas node express symptom variabl improv effect medic diagnosi experiment result compar markov logic network logist regress algorithm actual cemr databas indic method hold promis mkn facilit studi intellig diagnosi|['Jingchi Jiang', 'Chao Zhao', 'Yi Guan', 'Qiubin Yu']|['cs.AI']
2017-04-07T11:26:59Z|2017-03-27T21:05:15Z|http://arxiv.org/abs/1703.09310v1|http://arxiv.org/pdf/1703.09310v1|Adaptive Simulation-based Training of AI Decision-makers using Bayesian   Optimization|adapt simul base train ai decis maker use bayesian optim|This work studies how an AI-controlled dog-fighting agent with tunable decision-making parameters can learn to optimize performance against an intelligent adversary, as measured by a stochastic objective function evaluated on simulated combat engagements. Gaussian process Bayesian optimization (GPBO) techniques are developed to automatically learn global Gaussian Process (GP) surrogate models, which provide statistical performance predictions in both explored and unexplored areas of the parameter space. This allows a learning engine to sample full-combat simulations at parameter values that are most likely to optimize performance and also provide highly informative data points for improving future predictions. However, standard GPBO methods do not provide a reliable surrogate model for the highly volatile objective functions found in aerial combat, and thus do not reliably identify global maxima. These issues are addressed by novel Repeat Sampling (RS) and Hybrid Repeat/Multi-point Sampling (HRMS) techniques. Simulation studies show that HRMS improves the accuracy of GP surrogate models, allowing AI decision-makers to more accurately predict performance and efficiently tune parameters.|work studi ai control dog fight agent tunabl decis make paramet learn optim perform intellig adversari measur stochast object function evalu simul combat engag gaussian process bayesian optim gpbo techniqu develop automat learn global gaussian process gp surrog model provid statist perform predict explor unexplor area paramet space allow learn engin sampl full combat simul paramet valu like optim perform also provid high inform data point improv futur predict howev standard gpbo method provid reliabl surrog model high volatil object function found aerial combat thus reliabl identifi global maxima issu address novel repeat sampl rs hybrid repeat multi point sampl hrms techniqu simul studi show hrms improv accuraci gp surrog model allow ai decis maker accur predict perform effici tune paramet|['Brett W. Israelsen', 'Nisar Ahmed', 'Kenneth Center', 'Roderick Green', 'Winston Bennett Jr']|['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']
2017-04-07T11:27:05Z|2017-03-27T16:48:03Z|http://arxiv.org/abs/1703.09179v1|http://arxiv.org/pdf/1703.09179v1|Transfer learning for music classification and regression tasks|transfer learn music classif regress task|In this paper, we present a transfer learning approach for music classification and regression tasks. We propose to use a pretrained convnet feature, a concatenated feature vector using activations of feature maps of multiple layers in a trained convolutional network. We show that how this convnet feature can serve as a general-purpose music representation. In the experiment, a convnet is trained for music tagging and then transferred for many music-related classification and regression tasks as well as an audio-related classification task. In experiments, the convnet feature outperforms the baseline MFCC feature in all tasks and many reported approaches of aggregating MFCCs and low- and high-level music features.|paper present transfer learn approach music classif regress task propos use pretrain convnet featur concaten featur vector use activ featur map multipl layer train convolut network show convnet featur serv general purpos music represent experi convnet train music tag transfer mani music relat classif regress task well audio relat classif task experi convnet featur outperform baselin mfcc featur task mani report approach aggreg mfccs low high level music featur|['Keunwoo Choi', 'György Fazekas', 'Mark Sandler', 'Kyunghyun Cho']|['cs.CV', 'cs.AI', 'cs.MM', 'cs.SD']
2017-04-07T11:27:05Z|2017-03-27T06:19:38Z|http://arxiv.org/abs/1703.08944v1|http://arxiv.org/abs/1703.08944v1|Intelligent bidirectional rapidly-exploring random trees for optimal   motion planning in complex cluttered environments|intellig bidirect rapid explor random tree optim motion plan complex clutter environ|The sampling based motion planning algorithm known as Rapidly-exploring Random Trees (RRT) has gained the attention of many researchers due to their computational efficiency and effectiveness. Recently, a variant of RRT called RRT* has been proposed that ensures asymptotic optimality. Subsequently its bidirectional version has also been introduced in the literature known as Bidirectional-RRT* (B-RRT*). We introduce a new variant called Intelligent Bidirectional-RRT* (IB-RRT*) which is an improved variant of the optimal RRT* and bidirectional version of RRT* (B-RRT*) algorithms and is specially designed for complex cluttered environments. IB-RRT* utilizes the bidirectional trees approach and introduces intelligent sample insertion heuristic for fast convergence to the optimal path solution using uniform sampling heuristics. The proposed algorithm is evaluated theoretically and experimental results are presented that compares IB-RRT* with RRT* and B-RRT*. Moreover, experimental results demonstrate the superior efficiency of IB-RRT* in comparison with RRT* and B-RRT in complex cluttered environments.|sampl base motion plan algorithm known rapid explor random tree rrt gain attent mani research due comput effici effect recent variant rrt call rrt propos ensur asymptot optim subsequ bidirect version also introduc literatur known bidirect rrt rrt introduc new variant call intellig bidirect rrt ib rrt improv variant optim rrt bidirect version rrt rrt algorithm special design complex clutter environ ib rrt util bidirect tree approach introduc intellig sampl insert heurist fast converg optim path solut use uniform sampl heurist propos algorithm evalu theoret experiment result present compar ib rrt rrt rrt moreov experiment result demonstr superior effici ib rrt comparison rrt rrt complex clutter environ|['Ahmed Hussain Qureshi', 'Yasar Ayaz']|['cs.RO', 'cs.AI']
2017-04-07T11:27:05Z|2017-03-27T04:03:56Z|http://arxiv.org/abs/1703.08922v1|http://arxiv.org/pdf/1703.08922v1|On Automating the Doctrine of Double Effect|autom doctrin doubl effect|The doctrine of double effect ($\mathcal{DDE}$) is a long-studied ethical principle that governs when actions that have both positive and negative effects are to be allowed. The goal in this paper is to automate $\mathcal{DDE}$. We briefly present $\mathcal{DDE}$, and use a first-order modal logic, the deontic cognitive event calculus, as our framework to formalize the doctrine. We present formalizations of increasingly stronger versions of the principle, including what is known as the doctrine of triple effect. We then use our framework to simulate successfully scenarios that have been used to test for the presence of the principle in human subjects. Our framework can be used in two different modes: One can use it to build $\mathcal{DDE}$-compliant autonomous systems from scratch, or one can use it to verify that a given AI system is $\mathcal{DDE}$-compliant, by applying a $\mathcal{DDE}$ layer on an existing system or model. For the latter mode, the underlying AI system can be built using any architecture (planners, deep neural networks, bayesian networks, knowledge-representation systems, or a hybrid); as long as the system exposes a few parameters in its model, such verification is possible. The role of the $\mathcal{DDE}$ layer here is akin to a (dynamic or static) software verifier that examines existing software modules. Finally, we end by presenting initial work on how one can apply our $\mathcal{DDE}$ layer to the STRIPS-style planning model, and to a modified POMDP model.|doctrin doubl effect mathcal dde long studi ethic principl govern action posit negat effect allow goal paper autom mathcal dde briefli present mathcal dde use first order modal logic deontic cognit event calculus framework formal doctrin present formal increas stronger version principl includ known doctrin tripl effect use framework simul success scenario use test presenc principl human subject framework use two differ mode one use build mathcal dde compliant autonom system scratch one use verifi given ai system mathcal dde compliant appli mathcal dde layer exist system model latter mode ai system built use ani architectur planner deep neural network bayesian network knowledg represent system hybrid long system expos paramet model verif possibl role mathcal dde layer akin dynam static softwar verifi examin exist softwar modul final end present initi work one appli mathcal dde layer strip style plan model modifi pomdp model|['Naveen Sundar Govindarajulu', 'Selmer Bringsjord']|['cs.AI', 'cs.LO', 'cs.RO']
2017-04-07T11:27:05Z|2017-03-26T19:40:07Z|http://arxiv.org/abs/1703.09794v1|http://arxiv.org/pdf/1703.09794v1|Probabilistic Models for Computerized Adaptive Testing|probabilist model computer adapt test|In this paper we follow our previous research in the area of Computerized Adaptive Testing (CAT). We present three different methods for CAT. One of them, the item response theory, is a well established method, while the other two, Bayesian and neural networks, are new in the area of educational testing. In the first part of this paper, we present the concept of CAT and its advantages and disadvantages. We collected data from paper tests performed with grammar school students. We provide the summary of data used for our experiments in the second part. Next, we present three different model types for CAT. They are based on the item response theory, Bayesian networks, and neural networks. The general theory associated with each type is briefly explained and the utilization of these models for CAT is analyzed. Future research is outlined in the concluding part of the paper. It shows many interesting research paths that are important not only for CAT but also for other areas of artificial intelligence.|paper follow previous research area computer adapt test cat present three differ method cat one item respons theori well establish method two bayesian neural network new area educ test first part paper present concept cat advantag disadvantag collect data paper test perform grammar school student provid summari data use experi second part next present three differ model type cat base item respons theori bayesian network neural network general theori associ type briefli explain util model cat analyz futur research outlin conclud part paper show mani interest research path import onli cat also area artifici intellig|['Martin Plajner']|['cs.CY', 'cs.AI']
2017-04-07T11:27:05Z|2017-03-26T19:39:50Z|http://arxiv.org/abs/1703.08862v1|http://arxiv.org/pdf/1703.08862v1|Socially Aware Motion Planning with Deep Reinforcement Learning|social awar motion plan deep reinforc learn|For robotic vehicles to navigate safely and efficiently in pedestrian-rich environments, it is important to model subtle human behaviors and navigation rules. However, while instinctive to humans, socially compliant navigation is still difficult to quantify due to the stochasticity in people's behaviors. Existing works are mostly focused on using feature-matching techniques to describe and imitate human paths, but often do not generalize well since the feature values can vary from person to person, and even run to run. This work notes that while it is challenging to directly specify the details of what to do (precise mechanisms of human navigation), it is straightforward to specify what not to do (violations of social norms). Specifically, using deep reinforcement learning, this work develops a time-efficient navigation policy that respects common social norms. The proposed method is shown to enable fully autonomous navigation of a robotic vehicle moving at human walking speed in an environment with many pedestrians.|robot vehicl navig safe effici pedestrian rich environ import model subtl human behavior navig rule howev instinct human social compliant navig still difficult quantifi due stochast peopl behavior exist work focus use featur match techniqu describ imit human path often general well sinc featur valu vari person person even run run work note challeng direct specifi detail precis mechan human navig straightforward specifi violat social norm specif use deep reinforc learn work develop time effici navig polici respect common social norm propos method shown enabl fulli autonom navig robot vehicl move human walk speed environ mani pedestrian|['Yu Fan Chen', 'Michael Everett', 'Miao Liu', 'Jonathan P. How']|['cs.RO', 'cs.AI', 'cs.HC']
2017-04-07T11:27:05Z|2017-03-26T16:20:36Z|http://arxiv.org/abs/1703.08840v1|http://arxiv.org/pdf/1703.08840v1|Inferring The Latent Structure of Human Decision-Making from Raw Visual   Inputs|infer latent structur human decis make raw visual input|The goal of imitation learning is to match example expert behavior, without access to a reinforcement signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are not explicitly modeled. We introduce an extension to the Generative Adversarial Imitation Learning method that can infer the latent structure of human decision-making in an unsupervised way. Our method can not only imitate complex behaviors, but also learn interpretable and meaningful representations. We demonstrate that the approach is applicable to high-dimensional environments including raw visual inputs. In the highway driving domain, we show that a model learned from demonstrations is able to both produce different styles of human-like driving behaviors and accurately anticipate human actions. Our method surpasses various baselines in terms of performance and functionality.|goal imit learn match exampl expert behavior without access reinforc signal expert demonstr provid human howev often show signific variabl due latent factor explicit model introduc extens generat adversari imit learn method infer latent structur human decis make unsupervis way method onli imit complex behavior also learn interpret meaning represent demonstr approach applic high dimension environ includ raw visual input highway drive domain show model learn demonstr abl produc differ style human like drive behavior accur anticip human action method surpass various baselin term perform function|['Yunzhu Li', 'Jiaming Song', 'Stefano Ermon']|['cs.LG', 'cs.AI', 'cs.CV']
2017-04-07T11:27:05Z|2017-03-26T15:26:34Z|http://arxiv.org/abs/1703.08825v1|http://arxiv.org/pdf/1703.08825v1|Surrogate Model of Multi-Period Flexibility from a Home Energy   Management System|surrog model multi period flexibl home energi manag system|Near-future electric distribution grids operation will have to rely on demand-side flexibility, both by implementation of demand response strategies and by taking advantage of the intelligent management of increasingly common small-scale energy storage. Home energy management systems (HEMS) will play a crucial role on the flexibility provision to both system operators and market players like aggregators. Modeling multi-period flexibility from residential consumers (HEMS flexibility), such as battery storage and electric water heater, while complying with internal constraints (comfort levels, data privacy) and uncertainty is a complex task. This paper describes a computational method that is capable of efficiently define and learn the feasible flexibility set from controllable resources connected to a HEMS. An Evolutionary Particle Swarm Optimization (EPSO) algorithm is adopted and reshaped to derive a set of feasible temporal trajectories for the residential net-load, considering storage, flexible appliances, and predefined costumer preferences, as well as load and photovoltaic (PV) forecast uncertainty. A support vector data description (SVDD) algorithm is used to build models capable of classifying feasible and unfeasible HEMS operating trajectories upon request from an optimization/control algorithm operated by a DSO or market player.|near futur electr distribut grid oper reli demand side flexibl implement demand respons strategi take advantag intellig manag increas common small scale energi storag home energi manag system hem play crucial role flexibl provis system oper market player like aggreg model multi period flexibl residenti consum hem flexibl batteri storag electr water heater compli intern constraint comfort level data privaci uncertainti complex task paper describ comput method capabl effici defin learn feasibl flexibl set control resourc connect hem evolutionari particl swarm optim epso algorithm adopt reshap deriv set feasibl tempor trajectori residenti net load consid storag flexibl applianc predefin costum prefer well load photovolta pv forecast uncertainti support vector data descript svdd algorithm use build model capabl classifi feasibl unfeas hem oper trajectori upon request optim control algorithm oper dso market player|['Rui Pinto', 'Ricardo Bessa', 'Manuel Matos']|['cs.NE', 'cs.AI']
2017-04-07T11:27:05Z|2017-04-04T18:28:20Z|http://arxiv.org/abs/1703.08769v2|http://arxiv.org/pdf/1703.08769v2|Open Vocabulary Scene Parsing|open vocabulari scene pars|Recognizing arbitrary objects in the wild has been a challenging problem due to the limitations of existing classification models and datasets. In this paper, we propose a new task that aims at parsing scenes with a large and open vocabulary, and several evaluation metrics are explored for this problem. Our proposed approach to this problem is a joint image pixel and word concept embeddings framework, where word concepts are connected by semantic relations. We validate the open vocabulary prediction ability of our framework on ADE20K dataset which covers a wide variety of scenes and objects. We further explore the trained joint embedding space to show its interpretability.|recogn arbitrari object wild challeng problem due limit exist classif model dataset paper propos new task aim pars scene larg open vocabulari sever evalu metric explor problem propos approach problem joint imag pixel word concept embed framework word concept connect semant relat valid open vocabulari predict abil framework adek dataset cover wide varieti scene object explor train joint embed space show interpret|['Hang Zhao', 'Xavier Puig', 'Bolei Zhou', 'Sanja Fidler', 'Antonio Torralba']|['cs.CV', 'cs.AI']
2017-04-07T11:27:05Z|2017-03-26T03:47:54Z|http://arxiv.org/abs/1703.08762v1|http://arxiv.org/pdf/1703.08762v1|Team Formation for Scheduling Educational Material in Massive Online   Classes|team format schedul educ materi massiv onlin class|Whether teaching in a classroom or a Massive Online Open Course it is crucial to present the material in a way that benefits the audience as a whole. We identify two important tasks to solve towards this objective, 1 group students so that they can maximally benefit from peer interaction and 2 find an optimal schedule of the educational material for each group. Thus, in this paper, we solve the problem of team formation and content scheduling for education. Given a time frame d, a set of students S with their required need to learn different activities T and given k as the number of desired groups, we study the problem of finding k group of students. The goal is to teach students within time frame d such that their potential for learning is maximized and find the best schedule for each group. We show this problem to be NP-hard and develop a polynomial algorithm for it. We show our algorithm to be effective both on synthetic as well as a real data set. For our experiments, we use real data on students' grades in a Computer Science department. As part of our contribution, we release a semi-synthetic dataset that mimics the properties of the real data.|whether teach classroom massiv onlin open cours crucial present materi way benefit audienc whole identifi two import task solv toward object group student maxim benefit peer interact find optim schedul educ materi group thus paper solv problem team format content schedul educ given time frame set student requir need learn differ activ given number desir group studi problem find group student goal teach student within time frame potenti learn maxim find best schedul group show problem np hard develop polynomi algorithm show algorithm effect synthet well real data set experi use real data student grade comput scienc depart part contribut releas semi synthet dataset mimic properti real data|['Sanaz Bahargam', 'Dóra Erdos', 'Azer Bestavros', 'Evimaria Terzi']|['cs.AI']
2017-04-07T11:27:05Z|2017-03-25T15:37:09Z|http://arxiv.org/abs/1703.08705v1|http://arxiv.org/pdf/1703.08705v1|Comparing Rule-Based and Deep Learning Models for Patient Phenotyping|compar rule base deep learn model patient phenotyp|Objective: We investigate whether deep learning techniques for natural language processing (NLP) can be used efficiently for patient phenotyping. Patient phenotyping is a classification task for determining whether a patient has a medical condition, and is a crucial part of secondary analysis of healthcare data. We assess the performance of deep learning algorithms and compare them with classical NLP approaches.   Materials and Methods: We compare convolutional neural networks (CNNs), n-gram models, and approaches based on cTAKES that extract pre-defined medical concepts from clinical notes and use them to predict patient phenotypes. The performance is tested on 10 different phenotyping tasks using 1,610 discharge summaries extracted from the MIMIC-III database.   Results: CNNs outperform other phenotyping algorithms in all 10 tasks. The average F1-score of our model is 76 (PPV of 83, and sensitivity of 71) with our model having an F1-score up to 37 points higher than alternative approaches. We additionally assess the interpretability of our model by presenting a method that extracts the most salient phrases for a particular prediction.   Conclusion: We show that NLP methods based on deep learning improve the performance of patient phenotyping. Our CNN-based algorithm automatically learns the phrases associated with each patient phenotype. As such, it reduces the annotation complexity for clinical domain experts, who are normally required to develop task-specific annotation rules and identify relevant phrases. Our method performs well in terms of both performance and interpretability, which indicates that deep learning is an effective approach to patient phenotyping based on clinicians' notes.|object investig whether deep learn techniqu natur languag process nlp use effici patient phenotyp patient phenotyp classif task determin whether patient medic condit crucial part secondari analysi healthcar data assess perform deep learn algorithm compar classic nlp approach materi method compar convolut neural network cnns gram model approach base ctake extract pre defin medic concept clinic note use predict patient phenotyp perform test differ phenotyp task use discharg summari extract mimic iii databas result cnns outperform phenotyp algorithm task averag score model ppv sensit model score point higher altern approach addit assess interpret model present method extract salient phrase particular predict conclus show nlp method base deep learn improv perform patient phenotyp cnn base algorithm automat learn phrase associ patient phenotyp reduc annot complex clinic domain expert normal requir develop task specif annot rule identifi relev phrase method perform well term perform interpret indic deep learn effect approach patient phenotyp base clinician note|['Sebastian Gehrmann', 'Franck Dernoncourt', 'Yeran Li', 'Eric T. Carlson', 'Joy T. Wu', 'Jonathan Welt', 'John Foote Jr.', 'Edward T. Moseley', 'David W. Grant', 'Patrick D. Tyler', 'Leo Anthony Celi']|['cs.CL', 'cs.AI', 'cs.NE', 'stat.ML']
2017-04-07T11:27:10Z|2017-03-25T14:15:35Z|http://arxiv.org/abs/1704.01049v1|http://arxiv.org/pdf/1704.01049v1|A simulated annealing approach to optimal storing in a multi-level   warehouse|simul anneal approach optim store multi level warehous|We propose a simulated annealing algorithm specifically tailored to optimise total retrieval times in a multi-level warehouse under complex pre-batched picking constraints. Experiments on real data from a picker-to-parts order picking process in the warehouse of a European manufacturer show that optimal storage assignments do not necessarily display features presumed in heuristics, such as clustering of positively correlated items or ordering of items by picking frequency.   In an experiment run on more than 4000 batched orders with 1 to 150 items per batch, the storage assignment suggested by the algorithm produces a 21\% reduction in the total retrieval time with respect to a frequency-based storage assignment.|propos simul anneal algorithm specif tailor optimis total retriev time multi level warehous complex pre batch pick constraint experi real data picker part order pick process warehous european manufactur show optim storag assign necessarili display featur presum heurist cluster posit correl item order item pick frequenc experi run batch order item per batch storag assign suggest algorithm produc reduct total retriev time respect frequenc base storag assign|['Alexander Eckrot', 'Carina Geldhauser', 'Jan Jurczyk']|['cs.AI']
2017-04-07T11:27:10Z|2017-03-24T15:43:39Z|http://arxiv.org/abs/1703.08475v1|http://arxiv.org/pdf/1703.08475v1|Overcoming Catastrophic Forgetting by Incremental Moment Matching|overcom catastroph forget increment moment match|Catastrophic forgetting is a problem which refers to losing the information of the first task after training from the second task in continual learning of neural networks. To resolve this problem, we propose the incremental moment matching (IMM), which uses the Bayesian neural network framework. IMM assumes that the posterior distribution of parameters of neural networks is approximated with Gaussian distribution and incrementally matches the moment of the posteriors, which are trained for the first and second task, respectively. To make our Gaussian assumption reasonable, the IMM procedure utilizes various transfer learning techniques including weight transfer, L2-norm of old and new parameters, and a newly proposed variant of dropout using old parameters. We analyze our methods on the MNIST and CIFAR-10 datasets, and then evaluate them on a real-world life-log dataset collected using Google Glass. Experimental results show that IMM produces state-of-the-art performance in a variety of datasets.|catastroph forget problem refer lose inform first task train second task continu learn neural network resolv problem propos increment moment match imm use bayesian neural network framework imm assum posterior distribut paramet neural network approxim gaussian distribut increment match moment posterior train first second task respect make gaussian assumpt reason imm procedur util various transfer learn techniqu includ weight transfer norm old new paramet newli propos variant dropout use old paramet analyz method mnist cifar dataset evalu real world life log dataset collect use googl glass experiment result show imm produc state art perform varieti dataset|['Sang-Woo Lee', 'Jin-Hwa Kim', 'Jung-Woo Ha', 'Byoung-Tak Zhang']|['cs.LG', 'cs.AI']
2017-04-07T11:27:10Z|2017-03-24T14:40:31Z|http://arxiv.org/abs/1703.08428v1|http://arxiv.org/abs/1703.08428v1|Calendar.help: Designing a Workflow-Based Scheduling Agent with Humans   in the Loop|calendar help design workflow base schedul agent human loop|Although information workers may complain about meetings, they are an essential part of their work life. Consequently, busy people spend a significant amount of time scheduling meetings. We present Calendar.help, a system that provides fast, efficient scheduling through structured workflows. Users interact with the system via email, delegating their scheduling needs to the system as if it were a human personal assistant. Common scheduling scenarios are broken down using well-defined workflows and completed as a series of microtasks that are automated when possible and executed by a human otherwise. Unusual scenarios fall back to a trained human assistant who executes them as unstructured macrotasks. We describe the iterative approach we used to develop Calendar.help, and share the lessons learned from scheduling thousands of meetings during a year of real-world deployments. Our findings provide insight into how complex information tasks can be broken down into repeatable components that can be executed efficiently to improve productivity.|although inform worker may complain meet essenti part work life consequ busi peopl spend signific amount time schedul meet present calendar help system provid fast effici schedul structur workflow user interact system via email deleg schedul need system human person assist common schedul scenario broken use well defin workflow complet seri microtask autom possibl execut human otherwis unusu scenario fall back train human assist execut unstructur macrotask describ iter approach use develop calendar help share lesson learn schedul thousand meet dure year real world deploy find provid insight complex inform task broken repeat compon execut effici improv product|['Justin Cranshaw', 'Emad Elwany', 'Todd Newman', 'Rafal Kocielnik', 'Bowen Yu', 'Sandeep Soni', 'Jaime Teevan', 'Andrés Monroy-Hernández']|['cs.HC', 'cs.AI', 'cs.CL']
2017-04-07T11:27:10Z|2017-03-24T13:00:52Z|http://arxiv.org/abs/1703.08397v1|http://arxiv.org/pdf/1703.08397v1|Reasoning by Cases in Structured Argumentation|reason case structur argument|We extend the $ASPIC^+$ framework for structured argumentation so as to allow applications of the reasoning by cases inference scheme for defeasible arguments. Given an argument with conclusion `$A$ or $B$', an argument based on $A$ with conclusion $C$, and an argument based on $B$ with conclusion $C$, we allow the construction of an argument with conclusion $C$. We show how our framework leads to different results than other approaches in non-monotonic logic for dealing with disjunctive information, such as disjunctive default theory or approaches based on the OR-rule (which allows to derive a defeasible rule `If ($A$ or $B$) then $C$', given two defeasible rules `If $A$ then $C$' and `If $B$ then $C$'). We raise new questions regarding the subtleties of reasoning defeasibly with disjunctive information, and show that its formalization is more intricate than one would presume.|extend aspic framework structur argument allow applic reason case infer scheme defeas argument given argument conclus argument base conclus argument base conclus allow construct argument conclus show framework lead differ result approach non monoton logic deal disjunct inform disjunct default theori approach base rule allow deriv defeas rule given two defeas rule rais new question regard subtleti reason defeas disjunct inform show formal intric one would presum|['Mathieu Beirlaen', 'Jesse Heyninck', 'Christian Straßer']|['cs.AI', '68T27', 'I.2.3; I.2.4']
2017-04-07T11:27:10Z|2017-03-24T12:07:34Z|http://arxiv.org/abs/1703.08383v1|http://arxiv.org/pdf/1703.08383v1|Smart Augmentation - Learning an Optimal Data Augmentation Strategy|smart augment learn optim data augment strategi|A recurring problem faced when training neural networks is that there is typically not enough data to maximize the generalization capability of deep neural networks(DNN). There are many techniques to address this, including data augmentation, dropout, and transfer learning. In this paper, we introduce an additional method which we call Smart Augmentation and we show how to use it to increase the accuracy and reduce overfitting on a target network. Smart Augmentation works by creating a network that learns how to generate augmented data during the training process of a target network in a way that reduces that networks loss. This allows us to learn augmentations that minimize the error of that network.   Smart Augmentation has shown the potential to increase accuracy by demonstrably significant measures on all datasets tested. In addition, it has shown potential to achieve similar or improved performance levels with significantly smaller network sizes in a number of tested cases.|recur problem face train neural network typic enough data maxim general capabl deep neural network dnn mani techniqu address includ data augment dropout transfer learn paper introduc addit method call smart augment show use increas accuraci reduc overfit target network smart augment work creat network learn generat augment data dure train process target network way reduc network loss allow us learn augment minim error network smart augment shown potenti increas accuraci demonstr signific measur dataset test addit shown potenti achiev similar improv perform level signific smaller network size number test case|['Joseph Lemley', 'Shabab Bazrafkan', 'Peter Corcoran']|['cs.AI', 'cs.LG', 'stat.ML']
2017-04-07T11:27:10Z|2017-03-24T01:59:11Z|http://arxiv.org/abs/1703.08262v1|http://arxiv.org/pdf/1703.08262v1|Supervisor Synthesis of POMDP based on Automata Learning|supervisor synthesi pomdp base automata learn|As a general and thus popular model for autonomous systems, partially observable Markov decision process (POMDP) can capture uncertainties from different sources like sensing noises, actuation errors, and uncertain environments. However, its comprehensiveness makes the planning and control in POMDP difficult. Traditional POMDP planning problems target to find the optimal policy to maximize the expectation of accumulated rewards. But for safety critical applications, guarantees of system performance described by formal specifications are desired, which motivates us to consider formal methods to synthesize supervisor for POMDP. With system specifications given by Probabilistic Computation Tree Logic (PCTL), we propose a supervisory control framework with a type of deterministic finite automata (DFA), za-DFA, as the controller form. While the existing work mainly relies on optimization techniques to learn fixed-size finite state controllers (FSCs), we develop an $L^*$ learning based algorithm to determine both space and transitions of za-DFA. Membership queries and different oracles for conjectures are defined. The learning algorithm is sound and complete. An example is given in detailed steps to illustrate the supervisor synthesis algorithm.|general thus popular model autonom system partial observ markov decis process pomdp captur uncertainti differ sourc like sens nois actuat error uncertain environ howev comprehens make plan control pomdp difficult tradit pomdp plan problem target find optim polici maxim expect accumul reward safeti critic applic guarante system perform describ formal specif desir motiv us consid formal method synthes supervisor pomdp system specif given probabilist comput tree logic pctl propos supervisori control framework type determinist finit automata dfa za dfa control form exist work main reli optim techniqu learn fix size finit state control fscs develop learn base algorithm determin space transit za dfa membership queri differ oracl conjectur defin learn algorithm sound complet exampl given detail step illustr supervisor synthesi algorithm|['Xiaobin Zhang', 'Bo Wu', 'Hai Lin']|['cs.SY', 'cs.AI', 'cs.FL']
2017-04-07T11:27:10Z|2017-03-24T01:25:30Z|http://arxiv.org/abs/1703.09784v1|http://arxiv.org/pdf/1703.09784v1|Perception Driven Texture Generation|percept driven textur generat|This paper investigates a novel task of generating texture images from perceptual descriptions. Previous work on texture generation focused on either synthesis from examples or generation from procedural models. Generating textures from perceptual attributes have not been well studied yet. Meanwhile, perceptual attributes, such as directionality, regularity and roughness are important factors for human observers to describe a texture. In this paper, we propose a joint deep network model that combines adversarial training and perceptual feature regression for texture generation, while only random noise and user-defined perceptual attributes are required as input. In this model, a preliminary trained convolutional neural network is essentially integrated with the adversarial framework, which can drive the generated textures to possess given perceptual attributes. An important aspect of the proposed model is that, if we change one of the input perceptual features, the corresponding appearance of the generated textures will also be changed. We design several experiments to validate the effectiveness of the proposed method. The results show that the proposed method can produce high quality texture images with desired perceptual properties.|paper investig novel task generat textur imag perceptu descript previous work textur generat focus either synthesi exampl generat procedur model generat textur perceptu attribut well studi yet meanwhil perceptu attribut direct regular rough import factor human observ describ textur paper propos joint deep network model combin adversari train perceptu featur regress textur generat onli random nois user defin perceptu attribut requir input model preliminari train convolut neural network essenti integr adversari framework drive generat textur possess given perceptu attribut import aspect propos model chang one input perceptu featur correspond appear generat textur also chang design sever experi valid effect propos method result show propos method produc high qualiti textur imag desir perceptu properti|['Yanhai Gan', 'Huifang Chi', 'Ying Gao', 'Jun Liu', 'Guoqiang Zhong', 'Junyu Dong']|['cs.CV', 'cs.AI', 'cs.LG']
2017-04-07T11:27:10Z|2017-03-23T17:07:14Z|http://arxiv.org/abs/1703.08144v1|http://arxiv.org/pdf/1703.08144v1|Note Value Recognition for Rhythm Transcription Using a Markov Random   Field Model for Musical Scores and Performances of Piano Music|note valu recognit rhythm transcript use markov random field model music score perform piano music|This paper presents a statistical method for music transcription that can estimate score times of note onsets and offsets from polyphonic MIDI performance signals. Because performed note durations can deviate largely from score-indicated values, previous methods had the problem of not being able to accurately estimate offset score times (or note values) and thus could only output incomplete musical scores. Based on observations that the pitch context and onset score times are influential on the configuration of note values, we construct a context-tree model that provides prior distributions of note values using these features and combine it with a performance model in the framework of Markov random fields. Evaluation results showed that our method reduces the average error rate by around 40 percent compared to existing/simple methods. We also confirmed that, in our model, the score model plays a more important role than the performance model, and it automatically captures the voice structure by unsupervised learning.|paper present statist method music transcript estim score time note onset offset polyphon midi perform signal becaus perform note durat deviat larg score indic valu previous method problem abl accur estim offset score time note valu thus could onli output incomplet music score base observ pitch context onset score time influenti configur note valu construct context tree model provid prior distribut note valu use featur combin perform model framework markov random field evalu result show method reduc averag error rate around percent compar exist simpl method also confirm model score model play import role perform model automat captur voic structur unsupervis learn|['Eita Nakamura', 'Kazuyoshi Yoshii', 'Simon Dixon']|['cs.AI', 'cs.SD']
2017-04-07T11:27:10Z|2017-03-28T15:28:08Z|http://arxiv.org/abs/1703.08098v2|http://arxiv.org/pdf/1703.08098v2|An overview of embedding models of entities and relationships for   knowledge base completion|overview embed model entiti relationship knowledg base complet|Knowledge bases of real-world facts about entities and their relationships are useful resources for a variety of natural language processing tasks. However, because knowledge bases are typically incomplete, it is useful to be able to perform knowledge base completion, i.e., predict whether a relationship not in the knowledge base is likely to be true. This article presents an overview of embedding models of entities and relationships for knowledge base completion, with up-to-date experimental results on two standard evaluation tasks of link prediction (i.e. entity prediction) and triple classification.|knowledg base real world fact entiti relationship use resourc varieti natur languag process task howev becaus knowledg base typic incomplet use abl perform knowledg base complet predict whether relationship knowledg base like true articl present overview embed model entiti relationship knowledg base complet date experiment result two standard evalu task link predict entiti predict tripl classif|['Dat Quoc Nguyen']|['cs.CL', 'cs.AI', 'cs.IR']
2017-04-07T11:27:10Z|2017-03-23T12:32:10Z|http://arxiv.org/abs/1703.08041v1|http://arxiv.org/pdf/1703.08041v1|Resolving the Complexity of Some Fundamental Problems in Computational   Social Choice|resolv complex fundament problem comput social choic|This thesis is in the area called computational social choice which is an intersection area of algorithms and social choice theory.|thesi area call comput social choic intersect area algorithm social choic theori|['Palash Dey']|['cs.DS', 'cs.AI', 'cs.MA']
