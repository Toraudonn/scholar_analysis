,プログラム実行日時,論文更新日時,論文リンク,PDFリンク,元論文タイトル,論文タイトル,元サマリ,サマリ,著者,事前付与ジャンル,ニューラルネットワーク,自然言語処理,マーケティング,画像解析,音声解析,強化学習
11,2017-04-07T15:27:53Z,2017-04-04T15:56:55Z,http://arxiv.org/abs/1704.01079v1,http://arxiv.org/pdf/1704.01079v1,Homotopy Parametric Simplex Method for Sparse Learning,homotopi parametr simplex method spars learn,"High dimensional sparse learning has imposed a great computational challenge to large scale data analysis. In this paper, we are interested in a broad class of sparse learning approaches formulated as linear programs parametrized by a {\em regularization factor}, and solve them by the parametric simplex method (PSM). Our parametric simplex method offers significant advantages over other competing methods: (1) PSM naturally obtains the complete solution path for all values of the regularization parameter; (2) PSM provides a high precision dual certificate stopping criterion; (3) PSM yields sparse solutions through very few iterations, and the solution sparsity significantly reduces the computational cost per iteration. Particularly, we demonstrate the superiority of PSM over various sparse learning approaches, including Dantzig selector for sparse linear regression, LAD-Lasso for sparse robust linear regression, CLIME for sparse precision matrix estimation, sparse differential network estimation, and sparse Linear Programming Discriminant (LPD) analysis. We then provide sufficient conditions under which PSM always outputs sparse solutions such that its computational performance can be significantly boosted. Thorough numerical experiments are provided to demonstrate the outstanding performance of the PSM method.",high dimension spars learn impos great comput challeng larg scale data analysi paper interest broad class spars learn approach formul linear program parametr em regular factor solv parametr simplex method psm parametr simplex method offer signific advantag compet method psm natur obtain complet solut path valu regular paramet psm provid high precis dual certif stop criterion psm yield spars solut veri iter solut sparsiti signific reduc comput cost per iter particular demonstr superior psm various spars learn approach includ dantzig selector spars linear regress lad lasso spars robust linear regress clime spars precis matrix estim spars differenti network estim spars linear program discrimin lpd analysi provid suffici condit psm alway output spars solut comput perform signific boost thorough numer experi provid demonstr outstand perform psm method,"['Haotian Pang', 'Tuo Zhao', 'Robert Vanderbei', 'Han Liu']","['cs.LG', 'math.OC', 'stat.ML']",False,False,True,False,False,False
12,2017-04-07T15:28:01Z,2017-04-05T15:02:26Z,http://arxiv.org/abs/1704.01469v1,http://arxiv.org/pdf/1704.01469v1,Notes on Creating a Standardized Version of DVARS,note creat standard version dvar,By constructing a sampling distribution for DVARS we can create a standardized version of DVARS that should be more similar across scanners and datasets.,construct sampl distribut dvar creat standard version dvar similar across scanner dataset,['Thomas E. Nichols'],['stat.AP'],False,False,True,False,False,False
13,2017-04-07T15:28:01Z,2017-04-04T23:47:41Z,http://arxiv.org/abs/1704.01220v1,http://arxiv.org/pdf/1704.01220v1,Perceived Performance of Webpages In the Wild: Insights from Large-scale   Crowdsourcing of Above-the-Fold QoE,perceiv perform webpag wild insight larg scale crowdsourc abov fold qoe,"Clearly, no one likes webpages with poor quality of experience (QoE). Being perceived as slow or fast is a key element in the overall perceived QoE of web applications. While extensive effort has been put into optimizing web applications (both in industry and academia), not a lot of work exists in characterizing what aspects of webpage loading process truly influence human end-user's perception of the ""Speed"" of a page. In this paper we present ""SpeedPerception"", a large-scale web performance crowdsourcing framework focused on understanding the perceived loading performance of above-the-fold (ATF) webpage content. Our end goal is to create free open-source benchmarking datasets to advance the systematic analysis of how humans perceive webpage loading process. In Phase-1 of our ""SpeedPerception"" study using Internet Retailer Top 500 (IR 500) websites (https://github.com/pahammad/speedperception), we found that commonly used navigation metrics such as ""onLoad"" and ""Time To First Byte (TTFB)"" fail (less than 60% match) to represent majority human perception when comparing the speed of two webpages. We present a simple 3-variable-based machine learning model that explains the majority end-user choices better (with $87 \pm 2\%$ accuracy). In addition, our results suggest that the time needed by end-users to evaluate relative perceived speed of webpage is far less than the time of its ""visualComplete"" event.",clear one like webpag poor qualiti experi qoe perceiv slow fast key element overal perceiv qoe web applic extens effort put optim web applic industri academia lot work exist character aspect webpag load process truli influenc human end user percept speed page paper present speedpercept larg scale web perform crowdsourc framework focus understand perceiv load perform abov fold atf webpag content end goal creat free open sourc benchmark dataset advanc systemat analysi human perceiv webpag load process phase speedpercept studi use internet retail top ir websit https github com pahammad speedpercept found common use navig metric onload time first byte ttfb fail less match repres major human percept compar speed two webpag present simpl variabl base machin learn model explain major end user choic better pm accuraci addit result suggest time need end user evalu relat perceiv speed webpag far less time visualcomplet event,"['Qingzhu Gao', 'Prasenjit Dey', 'Parvez Ahammad']","['cs.NI', 'cs.HC', 'stat.AP']",False,False,True,False,False,False
14,2017-04-07T15:28:01Z,2017-04-04T22:34:49Z,http://arxiv.org/abs/1704.01207v1,http://arxiv.org/pdf/1704.01207v1,Bayesian Model Averaging for the X-Chromosome Inactivation Dilemma in   Genetic Association Study,bayesian model averag chromosom inactiv dilemma genet associ studi,"X-chromosome is often excluded from the so called `whole-genome' association studies due to its intrinsic difference between males and females. One particular analytical challenge is the unknown status of X-inactivation, where one of the two X-chromosome variants in females may be randomly selected to be silenced. In the absence of biological evidence in favour of one specific model, we consider a Bayesian model averaging framework that offers a principled way to account for the inherent model uncertainty, providing BMA-based posterior density intervals and Bayes factors. We examine the inferential properties of the proposed methods via extensive simulations and an association study of meconium ileus, an intestinal disease occurring in about twenty percent of Cystic Fibrosis patients. Compared with results previously reported assuming the presence of X-inactivation, we show that the proposed Bayesian methods provide more feature-rich quantities that are useful in practice.",chromosom often exclud call whole genom associ studi due intrins differ male femal one particular analyt challeng unknown status inactiv one two chromosom variant femal may random select silenc absenc biolog evid favour one specif model consid bayesian model averag framework offer principl way account inher model uncertainti provid bma base posterior densiti interv bay factor examin inferenti properti propos method via extens simul associ studi meconium ileus intestin diseas occur twenti percent cystic fibrosi patient compar result previous report assum presenc inactiv show propos bayesian method provid featur rich quantiti use practic,"['Bo Chen', 'Radu', 'V. Craiu', 'Lei Sun']",['stat.AP'],False,False,True,False,False,False
18,2017-04-07T15:28:32Z,2017-04-05T01:11:23Z,http://arxiv.org/abs/1704.01233v1,http://arxiv.org/pdf/1704.01233v1,Smoothing and filtering with a class of outer measures,smooth filter class outer measur,"Filtering and smoothing with a generalised representation of uncertainty is considered. Here, uncertainty is represented using a class of outer measures. It is shown how this representation of uncertainty can be propagated using outer-measure-type versions of Markov kernels and generalised Bayesian-like update equations. This leads to a system of generalised smoothing and filtering equations where integrals are replaced by supremums and probability density functions are replaced by positive functions with supremum equal to one. Interestingly, these equations retain most of the structure found in the classical Bayesian filtering framework. It is additionally shown that the Kalman filter recursion in terms of mean and variance can be recovered from weaker assumptions on the available information on the corresponding hidden Markov model.",filter smooth generalis represent uncertainti consid uncertainti repres use class outer measur shown represent uncertainti propag use outer measur type version markov kernel generalis bayesian like updat equat lead system generalis smooth filter equat integr replac supremum probabl densiti function replac posit function supremum equal one interest equat retain structur found classic bayesian filter framework addit shown kalman filter recurs term mean varianc recov weaker assumpt avail inform correspond hidden markov model,"['Jeremie Houssineau', 'Adrian N. Bishop']","['stat.ME', '60A10, 60J05, 62L12']",False,False,True,False,False,False
19,2017-04-07T15:28:32Z,2017-04-04T21:16:30Z,http://arxiv.org/abs/1704.01190v1,http://arxiv.org/pdf/1704.01190v1,Testing for network interference on experimentation platforms,test network interfer experiment platform,"Modern large IT companies have so called experimentation platforms, which are used carry out a large number of randomized experiments daily. On such platforms, the assumption of no interference among users is not tenable; that is, the fact that the outcome of a user does not depend on the treatment assigned to other users, often referred to as stable unit treatment value assumption (SUTVA). Here, we introduce an experimental design strategy for testing whether this assumption holds. The idea is to compare two different estimates of the total treatment effect obtained through two randomization strategies: complete randomization and cluster-based randomization. We propose a multilevel assignment strategy for obtaining these two estimates simultaneously, and we develop theoretical guarantees for rejecting the null hypothesis that SUTVA holds without the need to specify a model for the interference. We discuss how to apply the proposed design to large experimentation platforms. Finally, we illustrate this design strategy in a live experiment on the LinkedIn experimentation platform.",modern larg compani call experiment platform use carri larg number random experi daili platform assumpt interfer among user tenabl fact outcom user doe depend treatment assign user often refer stabl unit treatment valu assumpt sutva introduc experiment design strategi test whether assumpt hold idea compar two differ estim total treatment effect obtain two random strategi complet random cluster base random propos multilevel assign strategi obtain two estim simultan develop theoret guarante reject null hypothesi sutva hold without need specifi model interfer discuss appli propos design larg experiment platform final illustr design strategi live experi linkedin experiment platform,"['Jean Pouget-Abadie', 'Martin Saveski', 'Guillaume Saint-Jacques', 'Weitao Duan', 'Ya Xu', 'Souvik Ghosh', 'Edoardo Maria Airoldi']",['stat.ME'],False,False,True,False,False,False
22,2017-04-07T15:28:46Z,2017-04-05T14:11:05Z,http://arxiv.org/abs/1704.01437v1,http://arxiv.org/pdf/1704.01437v1,Time-frequency analysis of locally stationary Hawkes processes,time frequenc analysi local stationari hawk process,"Locally stationary Hawkes processes have been introduced in order to generalise classical Hawkes processes away from stationarity by allowing for a time-varying second-order structure. This class of self-exciting point processes has recently attracted a lot of interest in applications in the life sciences (seismology, genomics, neuro-science,...), but also in the modelling of high-frequency financial data. In this contribution we provide a fully developed nonparametric estimation theory of both local mean density and local Bartlett spectra of a locally stationary Hawkes process. In particular we apply our kernel estimation of the spectrum localised both in time and frequency to two data sets of transaction times revealing pertinent features in the data that had not been made visible by classical non-localised approaches based on models with constant fertility functions over time.",local stationari hawk process introduc order generalis classic hawk process away stationar allow time vari second order structur class self excit point process recent attract lot interest applic life scienc seismolog genom neuro scienc also model high frequenc financi data contribut provid fulli develop nonparametr estim theori local mean densiti local bartlett spectra local stationari hawk process particular appli kernel estim spectrum localis time frequenc two data set transact time reveal pertin featur data made visibl classic non localis approach base model constant fertil function time,"['François Roueff', 'Rainer Von Sachs']","['math.ST', 'stat.TH']",False,False,True,False,False,False
25,2017-04-07T15:28:46Z,2017-04-04T20:14:00Z,http://arxiv.org/abs/1704.01171v1,http://arxiv.org/pdf/1704.01171v1,Rethinking probabilistic prediction in the wake of the 2016 U.S.   presidential election,rethink probabilist predict wake presidenti elect,"To many statisticians and citizens, the outcome of the most recent U.S. presidential election represents a failure of data-driven methods on the grandest scale. This impression has led to much debate and discussion about how the election predictions went awry -- Were the polls inaccurate? Were the models wrong? Did we misinterpret the probabilities? -- and how they went right -- Perhaps the analyses were correct even though the predictions were wrong, that's just the nature of probabilistic forecasting. With this in mind, we analyze the election outcome with respect to a core set of effectiveness principles. Regardless of whether and how the election predictions were right or wrong, we argue that they were ineffective in conveying the extent to which the data was informative of the outcome and the level of uncertainty in making these assessments. Among other things, our analysis sheds light on the shortcomings of the classical interpretations of probability and its communication to consumers in the form of predictions. We present here an alternative approach, based on a notion of validity, which offers two immediate insights for predictive inference. First, the predictions are more conservative, arguably more realistic, and come with certain guarantees on the probability of an erroneous prediction. Second, our approach easily and naturally reflects the (possibly substantial) uncertainty about the model by outputting plausibilities instead of probabilities. Had these simple steps been taken by the popular prediction outlets, the election outcome may not have been so shocking.",mani statistician citizen outcom recent presidenti elect repres failur data driven method grandest scale impress led much debat discuss elect predict went awri poll inaccur model wrong misinterpret probabl went right perhap analys correct even though predict wrong natur probabilist forecast mind analyz elect outcom respect core set effect principl regardless whether elect predict right wrong argu ineffect convey extent data inform outcom level uncertainti make assess among thing analysi shed light shortcom classic interpret probabl communic consum form predict present altern approach base notion valid offer two immedi insight predict infer first predict conserv arguabl realist come certain guarante probabl erron predict second approach easili natur reflect possibl substanti uncertainti model output plausibl instead probabl simpl step taken popular predict outlet elect outcom may shock,"['Harry Crane', 'Ryan Martin']","['stat.OT', 'math.ST', 'stat.TH']",False,False,True,False,False,False
27,2017-04-07T15:29:00Z,2017-04-05T13:26:50Z,http://arxiv.org/abs/1704.01407v1,http://arxiv.org/pdf/1704.01407v1,Embodied Artificial Intelligence through Distributed Adaptive Control:   An Integrated Framework,embodi artifici intellig distribut adapt control integr framework,"In this paper, we argue that the future of Artificial Intelligence research resides in two keywords: integration and embodiment. We support this claim by analyzing the recent advances of the field. Regarding integration, we note that the most impactful recent contributions have been made possible through the integration of recent Machine Learning methods (based in particular on Deep Learning and Recurrent Neural Networks) with more traditional ones (e.g. Monte-Carlo tree search, goal babbling exploration or addressable memory systems). Regarding embodiment, we note that the traditional benchmark tasks (e.g. visual classification or board games) are becoming obsolete as state-of-the-art learning algorithms approach or even surpass human performance in most of them, having recently encouraged the development of first-person 3D game platforms embedding realistic physics. Building upon this analysis, we first propose an embodied cognitive architecture integrating heterogenous sub-fields of Artificial Intelligence into a unified framework. We demonstrate the utility of our approach by showing how major contributions of the field can be expressed within the proposed framework. We then claim that benchmarking environments need to reproduce ecologically-valid conditions for bootstrapping the acquisition of increasingly complex cognitive skills through the concept of a cognitive arms race between embodied agents.",paper argu futur artifici intellig research resid two keyword integr embodi support claim analyz recent advanc field regard integr note impact recent contribut made possibl integr recent machin learn method base particular deep learn recurr neural network tradit one mont carlo tree search goal babbl explor address memori system regard embodi note tradit benchmark task visual classif board game becom obsolet state art learn algorithm approach even surpass human perform recent encourag develop first person game platform embed realist physic build upon analysi first propos embodi cognit architectur integr heterogen sub field artifici intellig unifi framework demonstr util approach show major contribut field express within propos framework claim benchmark environ need reproduc ecolog valid condit bootstrap acquisit increas complex cognit skill concept cognit arm race embodi agent,"['Clément Moulin-Frier', 'Jordi-Ysard Puigbò', 'Xerxes D. Arsiwalla', 'Martì Sanchez-Fibla', 'Paul F. M. J. Verschure']","['cs.AI', 'cs.LG', 'cs.MA']",False,False,True,False,False,False
37,2017-04-07T15:29:23Z,2017-04-05T13:23:09Z,http://arxiv.org/abs/1704.01405v1,http://arxiv.org/pdf/1704.01405v1,Polynomial running times for polynomial-time oracle machines,polynomi run time polynomi time oracl machin,"This paper introduces a more restrictive notion of feasibility of functionals on Baire space than the established one from second-order complexity theory. Thereby making it possible to consider functions on the natural numbers as running times of oracle Turing machines and avoiding second-order polynomials, which are notoriously difficult to handle. Furthermore, all machines that witness this stronger kind of feasibility can be clocked and the different traditions of treating partial operators from computable analysis and second-order complexity theory are equated in a precise sense. The new notion is named ""strong polynomial-time computability"", and proven to be a strictly stronger requirement than polynomial-time computability. It is proven that within the framework for complexity of operators from analysis introduced by Kawamura and Cook the classes of strongly polynomial-time computable operators and polynomial-time computable operators coincide.",paper introduc restrict notion feasibl function bair space establish one second order complex theori therebi make possibl consid function natur number run time oracl ture machin avoid second order polynomi notori difficult handl furthermor machin wit stronger kind feasibl clock differ tradit treat partial oper comput analysi second order complex theori equat precis sens new notion name strong polynomi time comput proven strict stronger requir polynomi time comput proven within framework complex oper analysi introduc kawamura cook class strong polynomi time comput oper polynomi time comput oper coincid,"['Akitoshi Kawamura', 'Florian Steinberg']",['cs.CC'],False,False,True,False,False,False
38,2017-04-07T15:29:23Z,2017-04-04T16:57:18Z,http://arxiv.org/abs/1704.01104v1,http://arxiv.org/pdf/1704.01104v1,Communication Complexity of Correlated Equilibrium in Two-Player Games,communic complex correl equilibrium two player game,"We show a communication complexity lower bound for finding a correlated equilibrium of a two-player game. More precisely, we define a two-player $N \times N$ game called the 2-cycle game and show that the randomized communication complexity of finding a 1/poly($N$)-approximate correlated equilibrium of the 2-cycle game is $\Omega(N)$. For small approximation values, this answers an open question of Babichenko and Rubinstein (STOC 2017). Our lower bound is obtained via a direct reduction from the unique set disjointness problem.",show communic complex lower bound find correl equilibrium two player game precis defin two player time game call cycl game show random communic complex find poli approxim correl equilibrium cycl game omega small approxim valu answer open question babichenko rubinstein stoc lower bound obtain via direct reduct uniqu set disjoint problem,"['Anat Ganor', 'Karthik C. S.']","['cs.GT', 'cs.CC']",False,False,True,False,False,False
40,2017-04-07T15:29:46Z,2017-04-05T03:49:10Z,http://arxiv.org/abs/1704.01260v1,http://arxiv.org/pdf/1704.01260v1,Profit Maximization Auction and Data Management in Big Data Markets,profit maxim auction data manag big data market,"Big data services is any data-originated resource that is offered over the Internet. The performance of a big data service depends on the data bought from the data collectors. However, the problem of optimal pricing and data allocation in big data services is not well-studied. In this paper, we propose an auction-based big data market model. We first define the data cost and utility based on the impact of data size on the performance of big data analytics, e.g., machine learning algorithms. The big data services are considered as digital goods and uniquely characterized with ""unlimited supply"" compared to conventional goods which are limited. We therefore propose a Bayesian profit maximization auction which is truthful, rational, and computationally efficient. The optimal service price and data size are obtained by solving the profit maximization auction. Finally, experimental results of a real-world taxi trip dataset show that our big data market model and auction mechanism effectively solve the profit maximization problem of the service provider.",big data servic ani data origin resourc offer internet perform big data servic depend data bought data collector howev problem optim price data alloc big data servic well studi paper propos auction base big data market model first defin data cost util base impact data size perform big data analyt machin learn algorithm big data servic consid digit good uniqu character unlimit suppli compar convent good limit therefor propos bayesian profit maxim auction truth ration comput effici optim servic price data size obtain solv profit maxim auction final experiment result real world taxi trip dataset show big data market model auction mechan effect solv profit maxim problem servic provid,"['Yutao Jiao', 'Ping Wang', 'Dusit Niyato', 'Mohammad Abu Alsheikh', 'Shaohan Feng']",['cs.GT'],False,False,True,False,False,False
41,2017-04-07T15:29:46Z,2017-04-04T21:32:54Z,http://arxiv.org/abs/1704.01195v1,http://arxiv.org/pdf/1704.01195v1,Statistical Estimation with Strategic Data Sources in Competitive   Settings,statist estim strateg data sourc competit set,"In this paper, we introduce a preliminary model for interactions in the data market. Recent research has shown ways in which a data aggregator can design mechanisms for users to ensure the quality of data, even in situations where the users are effort-averse (i.e. prefer to submit lower-quality estimates) and the data aggregator cannot observe the effort exerted by the users (i.e. the contract suffers from the principal-agent problem). However, we have shown that these mechanisms often break down in more realistic models, where multiple data aggregators are in competition. Under minor assumptions on the properties of the statistical estimators in use by data aggregators, we show that there is either no Nash equilibrium, or there is an infinite number of Nash equilibrium. In the latter case, there is a fundamental ambiguity in who bears the burden of incentivizing different data sources. We are also able to calculate the price of anarchy, which measures how much social welfare is lost between the Nash equilibrium and the social optimum, i.e. between non-cooperative strategic play and cooperation.",paper introduc preliminari model interact data market recent research shown way data aggreg design mechan user ensur qualiti data even situat user effort avers prefer submit lower qualiti estim data aggreg cannot observ effort exert user contract suffer princip agent problem howev shown mechan often break realist model multipl data aggreg competit minor assumpt properti statist estim use data aggreg show either nash equilibrium infinit number nash equilibrium latter case fundament ambigu bear burden incentiv differ data sourc also abl calcul price anarchi measur much social welfar lost nash equilibrium social optimum non cooper strateg play cooper,"['Tyler Westenbroek', 'Roy Dong', 'Lillian J. Ratliff', 'S. Shankar Sastry']",['cs.GT'],False,False,True,False,False,False
42,2017-04-07T15:29:46Z,2017-04-04T16:57:18Z,http://arxiv.org/abs/1704.01104v1,http://arxiv.org/pdf/1704.01104v1,Communication Complexity of Correlated Equilibrium in Two-Player Games,communic complex correl equilibrium two player game,"We show a communication complexity lower bound for finding a correlated equilibrium of a two-player game. More precisely, we define a two-player $N \times N$ game called the 2-cycle game and show that the randomized communication complexity of finding a 1/poly($N$)-approximate correlated equilibrium of the 2-cycle game is $\Omega(N)$. For small approximation values, this answers an open question of Babichenko and Rubinstein (STOC 2017). Our lower bound is obtained via a direct reduction from the unique set disjointness problem.",show communic complex lower bound find correl equilibrium two player game precis defin two player time game call cycl game show random communic complex find poli approxim correl equilibrium cycl game omega small approxim valu answer open question babichenko rubinstein stoc lower bound obtain via direct reduct uniqu set disjoint problem,"['Anat Ganor', 'Karthik C. S.']","['cs.GT', 'cs.CC']",False,False,True,False,False,False
47,2017-04-07T15:30:05Z,2017-04-05T10:37:42Z,http://arxiv.org/abs/1704.01358v1,http://arxiv.org/pdf/1704.01358v1,Incremental Tube Construction for Human Action Detection,increment tube construct human action detect,"Current state-of-the-art action detection systems are tailored for offline batch-processing applications. However, for online applications like human-robot interaction, current systems fall short, either because they only detect one action per video, or because they assume that the entire video is available ahead of time. In this work, we introduce a real-time and online joint-labelling and association algorithm for action detection that can incrementally construct space-time action tubes on the most challenging action videos in which different action categories occur concurrently. In contrast to previous methods, we solve the detection-window association and action labelling problems jointly in a single pass. We demonstrate superior online association accuracy and speed (2.2ms per frame) as compared to the current state-of-the-art offline systems. We further demonstrate that the entire action detection pipeline can easily be made to work effectively in real-time using our action tube construction algorithm.",current state art action detect system tailor offlin batch process applic howev onlin applic like human robot interact current system fall short either becaus onli detect one action per video becaus assum entir video avail ahead time work introduc real time onlin joint label associ algorithm action detect increment construct space time action tube challeng action video differ action categori occur concurr contrast previous method solv detect window associ action label problem joint singl pass demonstr superior onlin associ accuraci speed ms per frame compar current state art offlin system demonstr entir action detect pipelin easili made work effect real time use action tube construct algorithm,"['Harkirat S. Behl', 'Michael Sapienza', 'Gurkirt Singh', 'Suman Saha', 'Fabio Cuzzolin', 'Philip H. S. Torr']",['cs.CV'],False,False,True,False,False,False
51,2017-04-07T15:30:05Z,2017-04-05T04:24:41Z,http://arxiv.org/abs/1704.01266v1,http://arxiv.org/pdf/1704.01266v1,Supporting Navigation of Outdoor Shopping Complexes for   Visually-impaired Users through Multi-modal Data Fusion,support navig outdoor shop complex visual impair user multi modal data fusion,"Outdoor shopping complexes (OSC) are extremely difficult for people with visual impairment to navigate. Existing GPS devices are mostly designed for roadside navigation and seldom transition well into an OSC-like setting. We report our study on the challenges faced by a blind person in navigating OSC through developing a new mobile application named iExplore. We first report an exploratory study aiming at deriving specific design principles for building this system by learning the unique challenges of the problem. Then we present a methodology that can be used to derive the necessary information for the development of iExplore, followed by experimental validation of the technology by a group of visually impaired users in a local outdoor shopping center. User feedback and other experiments suggest that iExplore, while at its very initial phase, has the potential of filling a practical gap in existing assistive technologies for the visually impaired.",outdoor shop complex osc extrem difficult peopl visual impair navig exist gps devic design roadsid navig seldom transit well osc like set report studi challeng face blind person navig osc develop new mobil applic name iexplor first report exploratori studi aim deriv specif design principl build system learn uniqu challeng problem present methodolog use deriv necessari inform develop iexplor follow experiment valid technolog group visual impair user local outdoor shop center user feedback experi suggest iexplor veri initi phase potenti fill practic gap exist assist technolog visual impair,"['Archana Paladugu', 'Parag S. Chandakkar', 'Peng Zhang', 'Baoxin Li']","['cs.CV', 'cs.CY', 'cs.HC']",False,False,True,False,False,False
54,2017-04-07T15:30:05Z,2017-04-05T03:38:08Z,http://arxiv.org/abs/1704.01256v1,http://arxiv.org/pdf/1704.01256v1,Improving Vision-based Self-positioning in Intelligent Transportation   Systems via Integrated Lane and Vehicle Detection,improv vision base self posit intellig transport system via integr lane vehicl detect,"Traffic congestion is a widespread problem. Dynamic traffic routing systems and congestion pricing are getting importance in recent research. Lane prediction and vehicle density estimation is an important component of such systems. We introduce a novel problem of vehicle self-positioning which involves predicting the number of lanes on the road and vehicle's position in those lanes using videos captured by a dashboard camera. We propose an integrated closed-loop approach where we use the presence of vehicles to aid the task of self-positioning and vice-versa. To incorporate multiple factors and high-level semantic knowledge into the solution, we formulate this problem as a Bayesian framework. In the framework, the number of lanes, the vehicle's position in those lanes and the presence of other vehicles are considered as parameters. We also propose a bounding box selection scheme to reduce the number of false detections and increase the computational efficiency. We show that the number of box proposals decreases by a factor of 6 using the selection approach. It also results in large reduction in the number of false detections. The entire approach is tested on real-world videos and is found to give acceptable results.",traffic congest widespread problem dynam traffic rout system congest price get import recent research lane predict vehicl densiti estim import compon system introduc novel problem vehicl self posit involv predict number lane road vehicl posit lane use video captur dashboard camera propos integr close loop approach use presenc vehicl aid task self posit vice versa incorpor multipl factor high level semant knowledg solut formul problem bayesian framework framework number lane vehicl posit lane presenc vehicl consid paramet also propos bound box select scheme reduc number fals detect increas comput effici show number box propos decreas factor use select approach also result larg reduct number fals detect entir approach test real world video found give accept result,"['Parag S. Chandakkar', 'Yilin Wang', 'Baoxin Li']",['cs.CV'],False,False,True,False,False,True
63,2017-04-07T15:30:06Z,2017-04-04T18:41:47Z,http://arxiv.org/abs/1704.01152v1,http://arxiv.org/pdf/1704.01152v1,Pose2Instance: Harnessing Keypoints for Person Instance Segmentation,poseinst har keypoint person instanc segment,"Human keypoints are a well-studied representation of people.We explore how to use keypoint models to improve instance-level person segmentation. The main idea is to harness the notion of a distance transform of oracle provided keypoints or estimated keypoint heatmaps as a prior for person instance segmentation task within a deep neural network. For training and evaluation, we consider all those images from COCO where both instance segmentation and human keypoints annotations are available. We first show how oracle keypoints can boost the performance of existing human segmentation model during inference without any training. Next, we propose a framework to directly learn a deep instance segmentation model conditioned on human pose. Experimental results show that at various Intersection Over Union (IOU) thresholds, in a constrained environment with oracle keypoints, the instance segmentation accuracy achieves 10% to 12% relative improvements over a strong baseline of oracle bounding boxes. In a more realistic environment, without the oracle keypoints, the proposed deep person instance segmentation model conditioned on human pose achieves 3.8% to 10.5% relative improvements comparing with its strongest baseline of a deep network trained only for segmentation.",human keypoint well studi represent peopl explor use keypoint model improv instanc level person segment main idea har notion distanc transform oracl provid keypoint estim keypoint heatmap prior person instanc segment task within deep neural network train evalu consid imag coco instanc segment human keypoint annot avail first show oracl keypoint boost perform exist human segment model dure infer without ani train next propos framework direct learn deep instanc segment model condit human pose experiment result show various intersect union iou threshold constrain environ oracl keypoint instanc segment accuraci achiev relat improv strong baselin oracl bound box realist environ without oracl keypoint propos deep person instanc segment model condit human pose achiev relat improv compar strongest baselin deep network train onli segment,"['Subarna Tripathi', 'Maxwell Collins', 'Matthew Brown', 'Serge Belongie']",['cs.CV'],False,False,True,False,False,False
69,2017-04-07T15:30:13Z,2017-04-05T08:45:22Z,http://arxiv.org/abs/1704.01311v1,http://arxiv.org/pdf/1704.01311v1,Optimal trade-offs for pattern matching with $k$ mismatches,optim trade pattern match mismatch,"Given a pattern of length $m$ and a text of length $n$, the goal in $k$-mismatch pattern matching is to compute, for every $m$-substring of the text, the exact Hamming distance to the pattern or report that it exceeds $k$. This can be solved in either $\widetilde{O}(n \sqrt{k})$ time as shown by Amir et al. [J. Algorithms 2004] or $\widetilde{O}((m + k^2) \cdot n/m)$ time due to a result of Clifford et al. [SODA 2016]. We provide a smooth time trade-off between these two bounds by designing an algorithm working in time $\widetilde{O}( (m + k \sqrt{m}) \cdot n/m)$. We complement this with a matching conditional lower bound, showing that a significantly faster combinatorial algorithm is not possible, unless the combinatorial matrix multiplication conjecture fails.",given pattern length text length goal mismatch pattern match comput everi substr text exact ham distanc pattern report exceed solv either widetild sqrt time shown amir et al algorithm widetild cdot time due result clifford et al soda provid smooth time trade two bound design algorithm work time widetild sqrt cdot complement match condit lower bound show signific faster combinatori algorithm possibl unless combinatori matrix multipl conjectur fail,"['Paweł Gawrychowski', 'Przemysław Uznański']",['cs.DS'],False,False,True,False,False,False
71,2017-04-07T15:30:13Z,2017-04-04T23:29:54Z,http://arxiv.org/abs/1704.01218v1,http://arxiv.org/pdf/1704.01218v1,Storing complex data sharing policies with the Min Mask Sketch,store complex data share polici min mask sketch,"More data is currently being collected and shared by software applications than ever before. In many cases, the user is asked if either all or none of their data can be shared. We hypothesize that in some cases, users would like to share data in more complex ways. In order to implement the sharing of data using more complicated privacy preferences, complex data sharing policies must be used. These complex sharing policies require more space to store than a simple ""all or nothing"" approach to data sharing. In this paper, we present a new probabilistic data structure, called the Min Mask Sketch, to efficiently store these complex data sharing policies. We describe an implementation for the Min Mask Sketch in PostgreSQL and analyze the practicality and feasibility of using a probabilistic data structure for storing complex data sharing policies.",data current collect share softwar applic ever befor mani case user ask either none data share hypothes case user would like share data complex way order implement share data use complic privaci prefer complex data share polici must use complex share polici requir space store simpl noth approach data share paper present new probabilist data structur call min mask sketch effici store complex data share polici describ implement min mask sketch postgresql analyz practic feasibl use probabilist data structur store complex data share polici,"['Stephen Smart', 'Christan Grant']","['cs.DS', 'cs.DB']",False,False,True,False,False,False
73,2017-04-07T15:30:14Z,2017-04-04T15:48:55Z,http://arxiv.org/abs/1704.01077v1,http://arxiv.org/pdf/1704.01077v1,Computing top-k Closeness Centrality Faster in Unweighted Graphs,comput top close central faster unweight graph,"Given a connected graph $G=(V,E)$, the closeness centrality of a vertex $v$ is defined as $\frac{n-1}{\sum_{w \in V} d(v,w)}$. This measure is widely used in the analysis of real-world complex networks, and the problem of selecting the $k$ most central vertices has been deeply analysed in the last decade. However, this problem is computationally not easy, especially for large networks: in the first part of the paper, we prove that it is not solvable in time $\O( E ^{2-\epsilon})$ on directed graphs, for any constant $\epsilon>0$, under reasonable complexity assumptions. Furthermore, we propose a new algorithm for selecting the $k$ most central nodes in a graph: we experimentally show that this algorithm improves significantly both the textbook algorithm, which is based on computing the distance between all pairs of vertices, and the state of the art. For example, we are able to compute the top $k$ nodes in few dozens of seconds in real-world networks with millions of nodes and edges. Finally, as a case study, we compute the $10$ most central actors in the IMDB collaboration network, where two actors are linked if they played together in a movie, and in the Wikipedia citation network, which contains a directed edge from a page $p$ to a page $q$ if $p$ contains a link to $q$.",given connect graph close central vertex defin frac sum measur wide use analysi real world complex network problem select central vertic deepli analys last decad howev problem comput easi especi larg network first part paper prove solvabl time epsilon direct graph ani constant epsilon reason complex assumpt furthermor propos new algorithm select central node graph experiment show algorithm improv signific textbook algorithm base comput distanc pair vertic state art exampl abl comput top node dozen second real world network million node edg final case studi comput central actor imdb collabor network two actor link play togeth movi wikipedia citat network contain direct edg page page contain link,"['Elisabetta Bergamini', 'Michele Borassi', 'Pierluigi Crescenzi', 'Andrea Marino', 'Henning Meyerhenke']",['cs.DS'],False,False,True,False,False,False
74,2017-04-07T15:30:25Z,2017-04-05T13:26:50Z,http://arxiv.org/abs/1704.01407v1,http://arxiv.org/pdf/1704.01407v1,Embodied Artificial Intelligence through Distributed Adaptive Control:   An Integrated Framework,embodi artifici intellig distribut adapt control integr framework,"In this paper, we argue that the future of Artificial Intelligence research resides in two keywords: integration and embodiment. We support this claim by analyzing the recent advances of the field. Regarding integration, we note that the most impactful recent contributions have been made possible through the integration of recent Machine Learning methods (based in particular on Deep Learning and Recurrent Neural Networks) with more traditional ones (e.g. Monte-Carlo tree search, goal babbling exploration or addressable memory systems). Regarding embodiment, we note that the traditional benchmark tasks (e.g. visual classification or board games) are becoming obsolete as state-of-the-art learning algorithms approach or even surpass human performance in most of them, having recently encouraged the development of first-person 3D game platforms embedding realistic physics. Building upon this analysis, we first propose an embodied cognitive architecture integrating heterogenous sub-fields of Artificial Intelligence into a unified framework. We demonstrate the utility of our approach by showing how major contributions of the field can be expressed within the proposed framework. We then claim that benchmarking environments need to reproduce ecologically-valid conditions for bootstrapping the acquisition of increasingly complex cognitive skills through the concept of a cognitive arms race between embodied agents.",paper argu futur artifici intellig research resid two keyword integr embodi support claim analyz recent advanc field regard integr note impact recent contribut made possibl integr recent machin learn method base particular deep learn recurr neural network tradit one mont carlo tree search goal babbl explor address memori system regard embodi note tradit benchmark task visual classif board game becom obsolet state art learn algorithm approach even surpass human perform recent encourag develop first person game platform embed realist physic build upon analysi first propos embodi cognit architectur integr heterogen sub field artifici intellig unifi framework demonstr util approach show major contribut field express within propos framework claim benchmark environ need reproduc ecolog valid condit bootstrap acquisit increas complex cognit skill concept cognit arm race embodi agent,"['Clément Moulin-Frier', 'Jordi-Ysard Puigbò', 'Xerxes D. Arsiwalla', 'Martì Sanchez-Fibla', 'Paul F. M. J. Verschure']","['cs.AI', 'cs.LG', 'cs.MA']",False,False,True,False,False,False
