,プログラム実行日時,論文更新日時,論文リンク,PDFリンク,元論文タイトル,論文タイトル,元サマリ,サマリ,著者,事前付与ジャンル,ニューラルネットワーク,自然言語処理,マーケティング,画像解析,音声解析,強化学習
6,2017-04-07T15:27:53Z,2017-04-05T06:39:51Z,http://arxiv.org/abs/1704.01280v1,http://arxiv.org/pdf/1704.01280v1,Revisiting the problem of audio-based hit song prediction using   convolutional neural networks,revisit problem audio base hit song predict use convolut neural network,"Being able to predict whether a song can be a hit has impor- tant applications in the music industry. Although it is true that the popularity of a song can be greatly affected by exter- nal factors such as social and commercial influences, to which degree audio features computed from musical signals (whom we regard as internal factors) can predict song popularity is an interesting research question on its own. Motivated by the recent success of deep learning techniques, we attempt to ex- tend previous work on hit song prediction by jointly learning the audio features and prediction models using deep learning. Specifically, we experiment with a convolutional neural net- work model that takes the primitive mel-spectrogram as the input for feature learning, a more advanced JYnet model that uses an external song dataset for supervised pre-training and auto-tagging, and the combination of these two models. We also consider the inception model to characterize audio infor- mation in different scales. Our experiments suggest that deep structures are indeed more accurate than shallow structures in predicting the popularity of either Chinese or Western Pop songs in Taiwan. We also use the tags predicted by JYnet to gain insights into the result of different models.",abl predict whether song hit impor tant applic music industri although true popular song great affect exter nal factor social commerci influenc degre audio featur comput music signal regard intern factor predict song popular interest research question motiv recent success deep learn techniqu attempt ex tend previous work hit song predict joint learn audio featur predict model use deep learn specif experi convolut neural net work model take primit mel spectrogram input featur learn advanc jynet model use extern song dataset supervis pre train auto tag combin two model also consid incept model character audio infor mation differ scale experi suggest deep structur inde accur shallow structur predict popular either chines western pop song taiwan also use tag predict jynet gain insight result differ model,"['Li-Chia Yang', 'Szu-Yu Chou', 'Jen-Yu Liu', 'Yi-Hsuan Yang', 'Yi-An Chen']","['cs.SD', 'cs.LG', 'stat.ML']",False,False,False,False,True,False
20,2017-04-07T15:28:32Z,2017-04-04T15:33:19Z,http://arxiv.org/abs/1704.01066v1,http://arxiv.org/pdf/1704.01066v1,Tests for qualitative features in the random coefficients model,test qualit featur random coeffici model,"The random coefficients model is an extension of the linear regression model which allows for additional heterogeneity in the population by modeling the regression coefficients as random variables. Given data from this model, the statistical challenge is to recover information about the joint density of the random coefficients which is a multivariate and ill-posed problem. Because of the curse of dimensionality and the ill-posedness, pointwise nonparametric estimation of the joint density is difficult and suffers from slow convergence rates. Larger features, such as an increase of the density along some direction or a well-accentuated mode can, however, be much easier detected from data by means of statistical tests. In this article, we follow this strategy and construct tests and confidence statements for qualitative features of the joint density, such as increases, decreases and modes. We propose a multiple testing approach based on aggregating single tests which are designed to extract shape information on fixed scales and directions. Using recent tools for Gaussian approximations of multivariate empirical processes, we derive expressions for the critical value. We apply our method to simulated and real data.",random coeffici model extens linear regress model allow addit heterogen popul model regress coeffici random variabl given data model statist challeng recov inform joint densiti random coeffici multivari ill pose problem becaus curs dimension ill posed pointwis nonparametr estim joint densiti difficult suffer slow converg rate larger featur increas densiti along direct well accentu mode howev much easier detect data mean statist test articl follow strategi construct test confid statement qualit featur joint densiti increas decreas mode propos multipl test approach base aggreg singl test design extract shape inform fix scale direct use recent tool gaussian approxim multivari empir process deriv express critic valu appli method simul real data,"['Fabian Dunker', 'Konstantin Eckle', 'Katharina Proksch', 'Johannes Schmidt-Hieber']","['stat.ME', '62G10, 62G15, 62G20']",False,False,False,False,True,False
33,2017-04-07T15:29:12Z,2017-04-05T13:38:01Z,http://arxiv.org/abs/1704.01419v1,http://arxiv.org/pdf/1704.01419v1,Linear Ensembles of Word Embedding Models,linear ensembl word embed model,"This paper explores linear methods for combining several word embedding models into an ensemble. We construct the combined models using an iterative method based on either ordinary least squares regression or the solution to the orthogonal Procrustes problem.   We evaluate the proposed approaches on Estonian---a morphologically complex language, for which the available corpora for training word embeddings are relatively small. We compare both combined models with each other and with the input word embedding models using synonym and analogy tests. The results show that while using the ordinary least squares regression performs poorly in our experiments, using orthogonal Procrustes to combine several word embedding models into an ensemble model leads to 7-10% relative improvements over the mean result of the initial models in synonym tests and 19-47% in analogy tests.",paper explor linear method combin sever word embed model ensembl construct combin model use iter method base either ordinari least squar regress solut orthogon procrust problem evalu propos approach estonian morpholog complex languag avail corpora train word embed relat small compar combin model input word embed model use synonym analog test result show use ordinari least squar regress perform poor experi use orthogon procrust combin sever word embed model ensembl model lead relat improv mean result initi model synonym test analog test,"['Avo Muromägi', 'Kairit Sirts', 'Sven Laur']",['cs.CL'],False,False,False,False,True,False
45,2017-04-07T15:30:04Z,2017-04-05T13:49:27Z,http://arxiv.org/abs/1704.01426v1,http://arxiv.org/pdf/1704.01426v1,The UMCD Dataset,umcd dataset,"In recent years, the technological improvements of low-cost small-scale Unmanned Aerial Vehicles (UAVs) are promoting an ever-increasing use of them in different tasks. In particular, the use of small-scale UAVs is useful in all these low-altitude tasks in which common UAVs cannot be adopted, such as recurrent comprehensive view of wide environments, frequent monitoring of military areas, real-time classification of static and moving entities (e.g., people, cars, etc.). These tasks can be supported by mosaicking and change detection algorithms achieved at low-altitude. Currently, public datasets for testing these algorithms are not available. This paper presents the UMCD dataset, the first collection of geo-referenced video sequences acquired at low-altitude for mosaicking and change detection purposes. Five reference scenarios are also reported.",recent year technolog improv low cost small scale unman aerial vehicl uav promot ever increas use differ task particular use small scale uav use low altitud task common uav cannot adopt recurr comprehens view wide environ frequent monitor militari area real time classif static move entiti peopl car etc task support mosaick chang detect algorithm achiev low altitud current public dataset test algorithm avail paper present umcd dataset first collect geo referenc video sequenc acquir low altitud mosaick chang detect purpos five refer scenario also report,"['Danilo Avola', 'Gian Luca Foresti', 'Niki Martinel', 'Daniele Pannone', 'Claudio Piciarelli']",['cs.CV'],False,False,False,False,True,False
64,2017-04-07T15:30:06Z,2017-04-04T18:14:02Z,http://arxiv.org/abs/1704.01137v1,http://arxiv.org/pdf/1704.01137v1,DyVEDeep: Dynamic Variable Effort Deep Neural Networks,dyvedeep dynam variabl effort deep neural network,"Deep Neural Networks (DNNs) have advanced the state-of-the-art in a variety of machine learning tasks and are deployed in increasing numbers of products and services. However, the computational requirements of training and evaluating large-scale DNNs are growing at a much faster pace than the capabilities of the underlying hardware platforms that they are executed upon. In this work, we propose Dynamic Variable Effort Deep Neural Networks (DyVEDeep) to reduce the computational requirements of DNNs during inference. Previous efforts propose specialized hardware implementations for DNNs, statically prune the network, or compress the weights. Complementary to these approaches, DyVEDeep is a dynamic approach that exploits the heterogeneity in the inputs to DNNs to improve their compute efficiency with comparable classification accuracy. DyVEDeep equips DNNs with dynamic effort mechanisms that, in the course of processing an input, identify how critical a group of computations are to classify the input. DyVEDeep dynamically focuses its compute effort only on the critical computa- tions, while skipping or approximating the rest. We propose 3 effort knobs that operate at different levels of granularity viz. neuron, feature and layer levels. We build DyVEDeep versions for 5 popular image recognition benchmarks - one for CIFAR-10 and four for ImageNet (AlexNet, OverFeat and VGG-16, weight-compressed AlexNet). Across all benchmarks, DyVEDeep achieves 2.1x-2.6x reduction in the number of scalar operations, which translates to 1.8x-2.3x performance improvement over a Caffe-based implementation, with < 0.5% loss in accuracy.",deep neural network dnns advanc state art varieti machin learn task deploy increas number product servic howev comput requir train evalu larg scale dnns grow much faster pace capabl hardwar platform execut upon work propos dynam variabl effort deep neural network dyvedeep reduc comput requir dnns dure infer previous effort propos special hardwar implement dnns static prune network compress weight complementari approach dyvedeep dynam approach exploit heterogen input dnns improv comput effici compar classif accuraci dyvedeep equip dnns dynam effort mechan cours process input identifi critic group comput classifi input dyvedeep dynam focus comput effort onli critic computa tion skip approxim rest propos effort knob oper differ level granular viz neuron featur layer level build dyvedeep version popular imag recognit benchmark one cifar four imagenet alexnet overfeat vgg weight compress alexnet across benchmark dyvedeep achiev reduct number scalar oper translat perform improv caff base implement loss accuraci,"['Sanjay Ganapathy', 'Swagath Venkataramani', 'Balaraman Ravindran', 'Anand Raghunathan']","['cs.NE', 'cs.CV', 'cs.LG']",False,False,False,False,True,False
75,2017-04-07T15:30:37Z,2017-04-05T06:39:51Z,http://arxiv.org/abs/1704.01280v1,http://arxiv.org/pdf/1704.01280v1,Revisiting the problem of audio-based hit song prediction using   convolutional neural networks,revisit problem audio base hit song predict use convolut neural network,"Being able to predict whether a song can be a hit has impor- tant applications in the music industry. Although it is true that the popularity of a song can be greatly affected by exter- nal factors such as social and commercial influences, to which degree audio features computed from musical signals (whom we regard as internal factors) can predict song popularity is an interesting research question on its own. Motivated by the recent success of deep learning techniques, we attempt to ex- tend previous work on hit song prediction by jointly learning the audio features and prediction models using deep learning. Specifically, we experiment with a convolutional neural net- work model that takes the primitive mel-spectrogram as the input for feature learning, a more advanced JYnet model that uses an external song dataset for supervised pre-training and auto-tagging, and the combination of these two models. We also consider the inception model to characterize audio infor- mation in different scales. Our experiments suggest that deep structures are indeed more accurate than shallow structures in predicting the popularity of either Chinese or Western Pop songs in Taiwan. We also use the tags predicted by JYnet to gain insights into the result of different models.",abl predict whether song hit impor tant applic music industri although true popular song great affect exter nal factor social commerci influenc degre audio featur comput music signal regard intern factor predict song popular interest research question motiv recent success deep learn techniqu attempt ex tend previous work hit song predict joint learn audio featur predict model use deep learn specif experi convolut neural net work model take primit mel spectrogram input featur learn advanc jynet model use extern song dataset supervis pre train auto tag combin two model also consid incept model character audio infor mation differ scale experi suggest deep structur inde accur shallow structur predict popular either chines western pop song taiwan also use tag predict jynet gain insight result differ model,"['Li-Chia Yang', 'Szu-Yu Chou', 'Jen-Yu Liu', 'Yi-Hsuan Yang', 'Yi-An Chen']","['cs.SD', 'cs.LG', 'stat.ML']",False,False,False,False,True,False
