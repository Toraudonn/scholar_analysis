カテゴリ,元サマリ,元論文タイトル,著者,論文リンク
ニューラルネットワーク,"Over 50 million scholarly articles have been published: they constitute a unique repository of knowledge. In particular, one may infer from them relations between scientific concepts, such as synonyms and hyponyms. Artificial neural networks have been recently explored for relation extraction. In this work, we continue this line of work and present a system based on a convolutional neural network to extract relations. Our model ranked first in the SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific articles (subtask C).",MIT at SemEval-2017 Task 10: Relation Extraction with Convolutional   Neural Networks,"['Ji Young Lee', 'Franck Dernoncourt', 'Peter Szolovits']",http://arxiv.org/abs/1704.01523v1
ニューラルネットワーク,"Over 50 million scholarly articles have been published: they constitute a unique repository of knowledge. In particular, one may infer from them relations between scientific concepts, such as synonyms and hyponyms. Artificial neural networks have been recently explored for relation extraction. In this work, we continue this line of work and present a system based on a convolutional neural network to extract relations. Our model ranked first in the SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific articles (subtask C).",MIT at SemEval-2017 Task 10: Relation Extraction with Convolutional   Neural Networks,"['Ji Young Lee', 'Franck Dernoncourt', 'Peter Szolovits']",http://arxiv.org/abs/1704.01523v1
ニューラルネットワーク,"Over 50 million scholarly articles have been published: they constitute a unique repository of knowledge. In particular, one may infer from them relations between scientific concepts, such as synonyms and hyponyms. Artificial neural networks have been recently explored for relation extraction. In this work, we continue this line of work and present a system based on a convolutional neural network to extract relations. Our model ranked first in the SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific articles (subtask C).",MIT at SemEval-2017 Task 10: Relation Extraction with Convolutional   Neural Networks,"['Ji Young Lee', 'Franck Dernoncourt', 'Peter Szolovits']",http://arxiv.org/abs/1704.01523v1
ニューラルネットワーク,"Large amount of image denoising literature focuses on single channel images and often experimentally validates the proposed methods on tens of images at most. In this paper, we investigate the interaction between denoising and classification on large scale dataset. Inspired by classification models, we propose a novel deep learning architecture for color (multichannel) image denoising and report on thousands of images from ImageNet dataset as well as commonly used imagery. We study the importance of (sufficient) training data, how semantic class information can be traded for improved denoising results. As a result, our method greatly improves PSNR performance by 0.34 - 0.51 dB on average over state-of-the art methods on large scale dataset. We conclude that it is beneficial to incorporate in classification models. On the other hand, we also study how noise affect classification performance. In the end, we come to a number of interesting conclusions, some being counter-intuitive.",On the Relation between Color Image Denoising and Classification,"['Jiqing Wu', 'Radu Timofte', 'Zhiwu Huang', 'Luc Van Gool']",http://arxiv.org/abs/1704.01372v1
ニューラルネットワーク,"Personalized and content-adaptive image enhancement can find many applications in the age of social media and mobile computing. This paper presents a relative-learning-based approach, which, unlike previous methods, does not require matching original and enhanced images for training. This allows the use of massive online photo collections to train a ranking model for improved enhancement. We first propose a multi-level ranking model, which is learned from only relatively-labeled inputs that are automatically crawled. Then we design a novel parameter sampling scheme under this model to generate the desired enhancement parameters for a new image. For evaluation, we first verify the effectiveness and the generalization abilities of our approach, using images that have been enhanced/labeled by experts. Then we carry out subjective tests, which show that users prefer images enhanced by our approach over other existing methods.",Relative Learning from Web Images for Content-adaptive Enhancement,"['Parag S. Chandakkar', 'Qiongjie Tian', 'Baoxin Li']",http://arxiv.org/abs/1704.01250v1
ニューラルネットワーク,"Research on automated image enhancement has gained momentum in recent years, partially due to the need for easy-to-use tools for enhancing pictures captured by ubiquitous cameras on mobile devices. Many of the existing leading methods employ machine-learning-based techniques, by which some enhancement parameters for a given image are found by relating the image to the training images with known enhancement parameters. While knowing the structure of the parameter space can facilitate search for the optimal solution, none of the existing methods has explicitly modeled and learned that structure. This paper presents an end-to-end, novel joint regression and ranking approach to model the interaction between desired enhancement parameters and images to be processed, employing a Gaussian process (GP). GP allows searching for ideal parameters using only the image features. The model naturally leads to a ranking technique for comparing images in the induced feature space. Comparative evaluation using the ground-truth based on the MIT-Adobe FiveK dataset plus subjective tests on an additional data-set were used to demonstrate the effectiveness of the proposed approach.",Joint Regression and Ranking for Image Enhancement,"['Parag S. Chandakkar', 'Baoxin Li']",http://arxiv.org/abs/1704.01235v1
ニューラルネットワーク,"Although deep neural networks (DNNs) have achieved great success in many computer vision tasks, recent studies have shown they are vulnerable to adversarial examples. Such examples, typically generated by adding small but purposeful distortions, can frequently fool DNN models. Previous studies to defend against adversarial examples mostly focused on refining the DNN models. They have either shown limited success or suffer from the expensive computation. We propose a new strategy, \emph{feature squeezing}, that can be used to harden DNN models by detecting adversarial examples. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many different feature vectors in the original space into a single sample. By comparing a DNN model's prediction on the original input with that on the squeezed input, feature squeezing detects adversarial examples with high accuracy and few false positives. This paper explores two instances of feature squeezing: reducing the color bit depth of each pixel and smoothing using a spatial filter. These strategies are straightforward, inexpensive, and complementary to defensive methods that operate on the underlying model, such as adversarial training.",Feature Squeezing: Detecting Adversarial Examples in Deep Neural   Networks,"['Weilin Xu', 'David Evans', 'Yanjun Qi']",http://arxiv.org/abs/1704.01155v1
自然言語処理,"Why do large neural network generalize so well on complex tasks such as image classification or speech recognition? What exactly is the role regularization for them? These are arguably among the most important open questions in machine learning today. In a recent and thought provoking paper [C. Zhang et al.] several authors performed a number of numerical experiments that hint at the need for novel theoretical concepts to account for this phenomenon. The paper stirred quit a lot of excitement among the machine learning community but at the same time it created some confusion as discussions on OpenReview.net testifies. The aim of this pedagogical paper is to make this debate accessible to a wider audience of data scientists without advanced theoretical knowledge in statistical learning. The focus here is on explicit mathematical definitions and on a discussion of relevant concepts, not on proofs for which we provide references.",On Generalization and Regularization in Deep Learning,['Pirmin Lemberger'],http://arxiv.org/abs/1704.01312v1
自然言語処理,"Why do large neural network generalize so well on complex tasks such as image classification or speech recognition? What exactly is the role regularization for them? These are arguably among the most important open questions in machine learning today. In a recent and thought provoking paper [C. Zhang et al.] several authors performed a number of numerical experiments that hint at the need for novel theoretical concepts to account for this phenomenon. The paper stirred quit a lot of excitement among the machine learning community but at the same time it created some confusion as discussions on OpenReview.net testifies. The aim of this pedagogical paper is to make this debate accessible to a wider audience of data scientists without advanced theoretical knowledge in statistical learning. The focus here is on explicit mathematical definitions and on a discussion of relevant concepts, not on proofs for which we provide references.",On Generalization and Regularization in Deep Learning,['Pirmin Lemberger'],http://arxiv.org/abs/1704.01312v1
自然言語処理,"Regarding the analysis of Web communication, social and complex networks the fast finding of most influential nodes in a network graph constitutes an important research problem. We use two indices of the influence of those nodes, namely, PageRank and a Max-linear model. We consider the PageRank %both as %Galton-Watson branching process and as an autoregressive process with a random number of random coefficients that depend on ranks of incoming nodes and their out-degrees and assume that the coefficients are independent and distributed with regularly varying tail and with the same tail index. Then it is proved that the tail index and the extremal index are the same for both PageRank and the Max-linear model and the values of these indices are found. The achievements are based on the study of random sequences of a random length and the comparison of the distribution of their maxima and linear combinations.",Extremes in Random Graphs Models of Complex Networks,['Natalia Markovich'],http://arxiv.org/abs/1704.01302v1
自然言語処理,"Recognizing arbitrary objects in the wild has been a challenging problem due to the limitations of existing classification models and datasets. In this paper, we propose a new task that aims at parsing scenes with a large and open vocabulary, and several evaluation metrics are explored for this problem. Our proposed approach to this problem is a joint image pixel and word concept embeddings framework, where word concepts are connected by semantic relations. We validate the open vocabulary prediction ability of our framework on ADE20K dataset which covers a wide variety of scenes and objects. We further explore the trained joint embedding space to show its interpretability.",Open Vocabulary Scene Parsing,"['Hang Zhao', 'Xavier Puig', 'Bolei Zhou', 'Sanja Fidler', 'Antonio Torralba']",http://arxiv.org/abs/1703.08769v2
自然言語処理,"We present our submitted systems for Semantic Textual Similarity (STS) Track 4 at SemEval-2017. Given a pair of Spanish-English sentences, each system must estimate their semantic similarity by a score between 0 and 5. In our submission, we use syntax-based, dictionary-based, context-based, and MT-based methods. We also combine these methods in unsupervised and supervised way. Our best run ranked 1st on track 4a with a correlation of 83.02% with human annotations.",CompiLIG at SemEval-2017 Task 1: Cross-Language Plagiarism Detection   Methods for Semantic Textual Similarity,"['Jeremy Ferrero', 'Frederic Agnes', 'Laurent Besacier', 'Didier Schwab']",http://arxiv.org/abs/1704.01346v1
自然言語処理,"Computational visual aesthetics has recently become an active research area. Existing state-of-art methods formulate this as a binary classification task where a given image is predicted to be beautiful or not. In many applications such as image retrieval and enhancement, it is more important to rank images based on their aesthetic quality instead of binary-categorizing them. Furthermore, in such applications, it may be possible that all images belong to the same category. Hence determining the aesthetic ranking of the images is more appropriate. To this end, we formulate a novel problem of ranking images with respect to their aesthetic quality. We construct a new dataset of image pairs with relative labels by carefully selecting images from the popular AVA dataset. Unlike in aesthetics classification, there is no single threshold which would determine the ranking order of the images across our entire dataset. We propose a deep neural network based approach that is trained on image pairs by incorporating principles from relative learning. Results show that such relative training procedure allows our network to rank the images with a higher accuracy than a state-of-art network trained on the same set of images using binary labels.",A Computational Approach to Relative Aesthetics,"['Parag S. Chandakkar', 'Vijetha Gattupalli', 'Baoxin Li']",http://arxiv.org/abs/1704.01248v1
マーケティング,By constructing a sampling distribution for DVARS we can create a standardized version of DVARS that should be more similar across scanners and datasets.,Notes on Creating a Standardized Version of DVARS,['Thomas E. Nichols'],http://arxiv.org/abs/1704.01469v1
マーケティング,"Clearly, no one likes webpages with poor quality of experience (QoE). Being perceived as slow or fast is a key element in the overall perceived QoE of web applications. While extensive effort has been put into optimizing web applications (both in industry and academia), not a lot of work exists in characterizing what aspects of webpage loading process truly influence human end-user's perception of the ""Speed"" of a page. In this paper we present ""SpeedPerception"", a large-scale web performance crowdsourcing framework focused on understanding the perceived loading performance of above-the-fold (ATF) webpage content. Our end goal is to create free open-source benchmarking datasets to advance the systematic analysis of how humans perceive webpage loading process. In Phase-1 of our ""SpeedPerception"" study using Internet Retailer Top 500 (IR 500) websites (https://github.com/pahammad/speedperception), we found that commonly used navigation metrics such as ""onLoad"" and ""Time To First Byte (TTFB)"" fail (less than 60% match) to represent majority human perception when comparing the speed of two webpages. We present a simple 3-variable-based machine learning model that explains the majority end-user choices better (with $87 \pm 2\%$ accuracy). In addition, our results suggest that the time needed by end-users to evaluate relative perceived speed of webpage is far less than the time of its ""visualComplete"" event.",Perceived Performance of Webpages In the Wild: Insights from Large-scale   Crowdsourcing of Above-the-Fold QoE,"['Qingzhu Gao', 'Prasenjit Dey', 'Parvez Ahammad']",http://arxiv.org/abs/1704.01220v1
マーケティング,"X-chromosome is often excluded from the so called `whole-genome' association studies due to its intrinsic difference between males and females. One particular analytical challenge is the unknown status of X-inactivation, where one of the two X-chromosome variants in females may be randomly selected to be silenced. In the absence of biological evidence in favour of one specific model, we consider a Bayesian model averaging framework that offers a principled way to account for the inherent model uncertainty, providing BMA-based posterior density intervals and Bayes factors. We examine the inferential properties of the proposed methods via extensive simulations and an association study of meconium ileus, an intestinal disease occurring in about twenty percent of Cystic Fibrosis patients. Compared with results previously reported assuming the presence of X-inactivation, we show that the proposed Bayesian methods provide more feature-rich quantities that are useful in practice.",Bayesian Model Averaging for the X-Chromosome Inactivation Dilemma in   Genetic Association Study,"['Bo Chen', 'Radu', 'V. Craiu', 'Lei Sun']",http://arxiv.org/abs/1704.01207v1
マーケティング,"Filtering and smoothing with a generalised representation of uncertainty is considered. Here, uncertainty is represented using a class of outer measures. It is shown how this representation of uncertainty can be propagated using outer-measure-type versions of Markov kernels and generalised Bayesian-like update equations. This leads to a system of generalised smoothing and filtering equations where integrals are replaced by supremums and probability density functions are replaced by positive functions with supremum equal to one. Interestingly, these equations retain most of the structure found in the classical Bayesian filtering framework. It is additionally shown that the Kalman filter recursion in terms of mean and variance can be recovered from weaker assumptions on the available information on the corresponding hidden Markov model.",Smoothing and filtering with a class of outer measures,"['Jeremie Houssineau', 'Adrian N. Bishop']",http://arxiv.org/abs/1704.01233v1
マーケティング,"Modern large IT companies have so called experimentation platforms, which are used carry out a large number of randomized experiments daily. On such platforms, the assumption of no interference among users is not tenable; that is, the fact that the outcome of a user does not depend on the treatment assigned to other users, often referred to as stable unit treatment value assumption (SUTVA). Here, we introduce an experimental design strategy for testing whether this assumption holds. The idea is to compare two different estimates of the total treatment effect obtained through two randomization strategies: complete randomization and cluster-based randomization. We propose a multilevel assignment strategy for obtaining these two estimates simultaneously, and we develop theoretical guarantees for rejecting the null hypothesis that SUTVA holds without the need to specify a model for the interference. We discuss how to apply the proposed design to large experimentation platforms. Finally, we illustrate this design strategy in a live experiment on the LinkedIn experimentation platform.",Testing for network interference on experimentation platforms,"['Jean Pouget-Abadie', 'Martin Saveski', 'Guillaume Saint-Jacques', 'Weitao Duan', 'Ya Xu', 'Souvik Ghosh', 'Edoardo Maria Airoldi']",http://arxiv.org/abs/1704.01190v1
マーケティング,"Locally stationary Hawkes processes have been introduced in order to generalise classical Hawkes processes away from stationarity by allowing for a time-varying second-order structure. This class of self-exciting point processes has recently attracted a lot of interest in applications in the life sciences (seismology, genomics, neuro-science,...), but also in the modelling of high-frequency financial data. In this contribution we provide a fully developed nonparametric estimation theory of both local mean density and local Bartlett spectra of a locally stationary Hawkes process. In particular we apply our kernel estimation of the spectrum localised both in time and frequency to two data sets of transaction times revealing pertinent features in the data that had not been made visible by classical non-localised approaches based on models with constant fertility functions over time.",Time-frequency analysis of locally stationary Hawkes processes,"['François Roueff', 'Rainer Von Sachs']",http://arxiv.org/abs/1704.01437v1
マーケティング,"To many statisticians and citizens, the outcome of the most recent U.S. presidential election represents a failure of data-driven methods on the grandest scale. This impression has led to much debate and discussion about how the election predictions went awry -- Were the polls inaccurate? Were the models wrong? Did we misinterpret the probabilities? -- and how they went right -- Perhaps the analyses were correct even though the predictions were wrong, that's just the nature of probabilistic forecasting. With this in mind, we analyze the election outcome with respect to a core set of effectiveness principles. Regardless of whether and how the election predictions were right or wrong, we argue that they were ineffective in conveying the extent to which the data was informative of the outcome and the level of uncertainty in making these assessments. Among other things, our analysis sheds light on the shortcomings of the classical interpretations of probability and its communication to consumers in the form of predictions. We present here an alternative approach, based on a notion of validity, which offers two immediate insights for predictive inference. First, the predictions are more conservative, arguably more realistic, and come with certain guarantees on the probability of an erroneous prediction. Second, our approach easily and naturally reflects the (possibly substantial) uncertainty about the model by outputting plausibilities instead of probabilities. Had these simple steps been taken by the popular prediction outlets, the election outcome may not have been so shocking.",Rethinking probabilistic prediction in the wake of the 2016 U.S.   presidential election,"['Harry Crane', 'Ryan Martin']",http://arxiv.org/abs/1704.01171v1
マーケティング,"In this paper, we argue that the future of Artificial Intelligence research resides in two keywords: integration and embodiment. We support this claim by analyzing the recent advances of the field. Regarding integration, we note that the most impactful recent contributions have been made possible through the integration of recent Machine Learning methods (based in particular on Deep Learning and Recurrent Neural Networks) with more traditional ones (e.g. Monte-Carlo tree search, goal babbling exploration or addressable memory systems). Regarding embodiment, we note that the traditional benchmark tasks (e.g. visual classification or board games) are becoming obsolete as state-of-the-art learning algorithms approach or even surpass human performance in most of them, having recently encouraged the development of first-person 3D game platforms embedding realistic physics. Building upon this analysis, we first propose an embodied cognitive architecture integrating heterogenous sub-fields of Artificial Intelligence into a unified framework. We demonstrate the utility of our approach by showing how major contributions of the field can be expressed within the proposed framework. We then claim that benchmarking environments need to reproduce ecologically-valid conditions for bootstrapping the acquisition of increasingly complex cognitive skills through the concept of a cognitive arms race between embodied agents.",Embodied Artificial Intelligence through Distributed Adaptive Control:   An Integrated Framework,"['Clément Moulin-Frier', 'Jordi-Ysard Puigbò', 'Xerxes D. Arsiwalla', 'Martì Sanchez-Fibla', 'Paul F. M. J. Verschure']",http://arxiv.org/abs/1704.01407v1
マーケティング,"We study how complexity classes above BQP, such as postBQP, ${\rm postBQP}_{\rm FP}$, and SBQP, change if we ""Merlinize"" them, i.e., if we allow an extra input quantum state (or classical bit string) given by Merlin as witness. Main results are the following three: First, the Merlinized version of postBQP is equal to PSPACE. Second, if the Merlinized postBQP is restricted in such a way that the postselection probability is equal to all witness states, then the class is equal to PP. Finally, the Merlinization does not change the class SBQP.",Merlinization of complexity classes above BQP,"['Tomoyuki Morimae', 'Harumichi Nishimura']",http://arxiv.org/abs/1704.01514v1
マーケティング,"This paper introduces a more restrictive notion of feasibility of functionals on Baire space than the established one from second-order complexity theory. Thereby making it possible to consider functions on the natural numbers as running times of oracle Turing machines and avoiding second-order polynomials, which are notoriously difficult to handle. Furthermore, all machines that witness this stronger kind of feasibility can be clocked and the different traditions of treating partial operators from computable analysis and second-order complexity theory are equated in a precise sense. The new notion is named ""strong polynomial-time computability"", and proven to be a strictly stronger requirement than polynomial-time computability. It is proven that within the framework for complexity of operators from analysis introduced by Kawamura and Cook the classes of strongly polynomial-time computable operators and polynomial-time computable operators coincide.",Polynomial running times for polynomial-time oracle machines,"['Akitoshi Kawamura', 'Florian Steinberg']",http://arxiv.org/abs/1704.01405v1
マーケティング,"Big data services is any data-originated resource that is offered over the Internet. The performance of a big data service depends on the data bought from the data collectors. However, the problem of optimal pricing and data allocation in big data services is not well-studied. In this paper, we propose an auction-based big data market model. We first define the data cost and utility based on the impact of data size on the performance of big data analytics, e.g., machine learning algorithms. The big data services are considered as digital goods and uniquely characterized with ""unlimited supply"" compared to conventional goods which are limited. We therefore propose a Bayesian profit maximization auction which is truthful, rational, and computationally efficient. The optimal service price and data size are obtained by solving the profit maximization auction. Finally, experimental results of a real-world taxi trip dataset show that our big data market model and auction mechanism effectively solve the profit maximization problem of the service provider.",Profit Maximization Auction and Data Management in Big Data Markets,"['Yutao Jiao', 'Ping Wang', 'Dusit Niyato', 'Mohammad Abu Alsheikh', 'Shaohan Feng']",http://arxiv.org/abs/1704.01260v1
マーケティング,"In this paper, we introduce a preliminary model for interactions in the data market. Recent research has shown ways in which a data aggregator can design mechanisms for users to ensure the quality of data, even in situations where the users are effort-averse (i.e. prefer to submit lower-quality estimates) and the data aggregator cannot observe the effort exerted by the users (i.e. the contract suffers from the principal-agent problem). However, we have shown that these mechanisms often break down in more realistic models, where multiple data aggregators are in competition. Under minor assumptions on the properties of the statistical estimators in use by data aggregators, we show that there is either no Nash equilibrium, or there is an infinite number of Nash equilibrium. In the latter case, there is a fundamental ambiguity in who bears the burden of incentivizing different data sources. We are also able to calculate the price of anarchy, which measures how much social welfare is lost between the Nash equilibrium and the social optimum, i.e. between non-cooperative strategic play and cooperation.",Statistical Estimation with Strategic Data Sources in Competitive   Settings,"['Tyler Westenbroek', 'Roy Dong', 'Lillian J. Ratliff', 'S. Shankar Sastry']",http://arxiv.org/abs/1704.01195v1
マーケティング,"Learning how to generate descriptions of images or videos received major interest both in the Computer Vision and Natural Language Processing communities. While a few works have proposed to learn a grounding during the generation process in an unsupervised way (via an attention mechanism), it remains unclear how good the quality of the grounding is and whether it benefits the description quality. In this work we propose a movie description model which learns to generate description and jointly ground (localize) the mentioned characters as well as do visual co-reference resolution between pairs of consecutive sentences/clips. We also propose to use weak localization supervision through character mentions provided in movie descriptions to learn the character grounding. At training time, we first learn how to localize characters by relating their visual appearance to mentions in the descriptions via a semi-supervised approach. We then provide this (noisy) supervision into our description model which greatly improves its performance. Our proposed description model improves over prior work w.r.t. generated description quality and additionally provides grounding and local co-reference resolution. We evaluate it on the MPII Movie Description dataset using automatic and human evaluation measures and using our newly collected grounding and co-reference data for characters.",Generating Descriptions with Grounded and Co-Referenced People,"['Anna Rohrbach', 'Marcus Rohrbach', 'Siyu Tang', 'Seong Joon Oh', 'Bernt Schiele']",http://arxiv.org/abs/1704.01518v1
マーケティング,"Current state-of-the-art action detection systems are tailored for offline batch-processing applications. However, for online applications like human-robot interaction, current systems fall short, either because they only detect one action per video, or because they assume that the entire video is available ahead of time. In this work, we introduce a real-time and online joint-labelling and association algorithm for action detection that can incrementally construct space-time action tubes on the most challenging action videos in which different action categories occur concurrently. In contrast to previous methods, we solve the detection-window association and action labelling problems jointly in a single pass. We demonstrate superior online association accuracy and speed (2.2ms per frame) as compared to the current state-of-the-art offline systems. We further demonstrate that the entire action detection pipeline can easily be made to work effectively in real-time using our action tube construction algorithm.",Incremental Tube Construction for Human Action Detection,"['Harkirat S. Behl', 'Michael Sapienza', 'Gurkirt Singh', 'Suman Saha', 'Fabio Cuzzolin', 'Philip H. S. Torr']",http://arxiv.org/abs/1704.01358v1
マーケティング,"Outdoor shopping complexes (OSC) are extremely difficult for people with visual impairment to navigate. Existing GPS devices are mostly designed for roadside navigation and seldom transition well into an OSC-like setting. We report our study on the challenges faced by a blind person in navigating OSC through developing a new mobile application named iExplore. We first report an exploratory study aiming at deriving specific design principles for building this system by learning the unique challenges of the problem. Then we present a methodology that can be used to derive the necessary information for the development of iExplore, followed by experimental validation of the technology by a group of visually impaired users in a local outdoor shopping center. User feedback and other experiments suggest that iExplore, while at its very initial phase, has the potential of filling a practical gap in existing assistive technologies for the visually impaired.",Supporting Navigation of Outdoor Shopping Complexes for   Visually-impaired Users through Multi-modal Data Fusion,"['Archana Paladugu', 'Parag S. Chandakkar', 'Peng Zhang', 'Baoxin Li']",http://arxiv.org/abs/1704.01266v1
マーケティング,"Traffic congestion is a widespread problem. Dynamic traffic routing systems and congestion pricing are getting importance in recent research. Lane prediction and vehicle density estimation is an important component of such systems. We introduce a novel problem of vehicle self-positioning which involves predicting the number of lanes on the road and vehicle's position in those lanes using videos captured by a dashboard camera. We propose an integrated closed-loop approach where we use the presence of vehicles to aid the task of self-positioning and vice-versa. To incorporate multiple factors and high-level semantic knowledge into the solution, we formulate this problem as a Bayesian framework. In the framework, the number of lanes, the vehicle's position in those lanes and the presence of other vehicles are considered as parameters. We also propose a bounding box selection scheme to reduce the number of false detections and increase the computational efficiency. We show that the number of box proposals decreases by a factor of 6 using the selection approach. It also results in large reduction in the number of false detections. The entire approach is tested on real-world videos and is found to give acceptable results.",Improving Vision-based Self-positioning in Intelligent Transportation   Systems via Integrated Lane and Vehicle Detection,"['Parag S. Chandakkar', 'Yilin Wang', 'Baoxin Li']",http://arxiv.org/abs/1704.01256v1
マーケティング,"Human keypoints are a well-studied representation of people.We explore how to use keypoint models to improve instance-level person segmentation. The main idea is to harness the notion of a distance transform of oracle provided keypoints or estimated keypoint heatmaps as a prior for person instance segmentation task within a deep neural network. For training and evaluation, we consider all those images from COCO where both instance segmentation and human keypoints annotations are available. We first show how oracle keypoints can boost the performance of existing human segmentation model during inference without any training. Next, we propose a framework to directly learn a deep instance segmentation model conditioned on human pose. Experimental results show that at various Intersection Over Union (IOU) thresholds, in a constrained environment with oracle keypoints, the instance segmentation accuracy achieves 10% to 12% relative improvements over a strong baseline of oracle bounding boxes. In a more realistic environment, without the oracle keypoints, the proposed deep person instance segmentation model conditioned on human pose achieves 3.8% to 10.5% relative improvements comparing with its strongest baseline of a deep network trained only for segmentation.",Pose2Instance: Harnessing Keypoints for Person Instance Segmentation,"['Subarna Tripathi', 'Maxwell Collins', 'Matthew Brown', 'Serge Belongie']",http://arxiv.org/abs/1704.01152v1
マーケティング,"Given a pattern of length $m$ and a text of length $n$, the goal in $k$-mismatch pattern matching is to compute, for every $m$-substring of the text, the exact Hamming distance to the pattern or report that it exceeds $k$. This can be solved in either $\widetilde{O}(n \sqrt{k})$ time as shown by Amir et al. [J. Algorithms 2004] or $\widetilde{O}((m + k^2) \cdot n/m)$ time due to a result of Clifford et al. [SODA 2016]. We provide a smooth time trade-off between these two bounds by designing an algorithm working in time $\widetilde{O}( (m + k \sqrt{m}) \cdot n/m)$. We complement this with a matching conditional lower bound, showing that a significantly faster combinatorial algorithm is not possible, unless the combinatorial matrix multiplication conjecture fails.",Optimal trade-offs for pattern matching with $k$ mismatches,"['Paweł Gawrychowski', 'Przemysław Uznański']",http://arxiv.org/abs/1704.01311v1
マーケティング,"More data is currently being collected and shared by software applications than ever before. In many cases, the user is asked if either all or none of their data can be shared. We hypothesize that in some cases, users would like to share data in more complex ways. In order to implement the sharing of data using more complicated privacy preferences, complex data sharing policies must be used. These complex sharing policies require more space to store than a simple ""all or nothing"" approach to data sharing. In this paper, we present a new probabilistic data structure, called the Min Mask Sketch, to efficiently store these complex data sharing policies. We describe an implementation for the Min Mask Sketch in PostgreSQL and analyze the practicality and feasibility of using a probabilistic data structure for storing complex data sharing policies.",Storing complex data sharing policies with the Min Mask Sketch,"['Stephen Smart', 'Christan Grant']",http://arxiv.org/abs/1704.01218v1
マーケティング,"In this paper, we argue that the future of Artificial Intelligence research resides in two keywords: integration and embodiment. We support this claim by analyzing the recent advances of the field. Regarding integration, we note that the most impactful recent contributions have been made possible through the integration of recent Machine Learning methods (based in particular on Deep Learning and Recurrent Neural Networks) with more traditional ones (e.g. Monte-Carlo tree search, goal babbling exploration or addressable memory systems). Regarding embodiment, we note that the traditional benchmark tasks (e.g. visual classification or board games) are becoming obsolete as state-of-the-art learning algorithms approach or even surpass human performance in most of them, having recently encouraged the development of first-person 3D game platforms embedding realistic physics. Building upon this analysis, we first propose an embodied cognitive architecture integrating heterogenous sub-fields of Artificial Intelligence into a unified framework. We demonstrate the utility of our approach by showing how major contributions of the field can be expressed within the proposed framework. We then claim that benchmarking environments need to reproduce ecologically-valid conditions for bootstrapping the acquisition of increasingly complex cognitive skills through the concept of a cognitive arms race between embodied agents.",Embodied Artificial Intelligence through Distributed Adaptive Control:   An Integrated Framework,"['Clément Moulin-Frier', 'Jordi-Ysard Puigbò', 'Xerxes D. Arsiwalla', 'Martì Sanchez-Fibla', 'Paul F. M. J. Verschure']",http://arxiv.org/abs/1704.01407v1
画像解析,"This paper focuses on a novel and challenging vision task, dense video captioning, which aims to automatically describe a video clip with multiple informative and diverse caption sentences. The proposed method is trained without explicit annotation of fine-grained sentence to video region-sequence correspondence, but is only based on weak video-level sentence annotations. It differs from existing video captioning systems in three technical aspects. First, we propose lexical fully convolutional neural networks (Lexical-FCN) with weakly supervised multi-instance multi-label learning to weakly link video regions with lexical labels. Second, we introduce a novel submodular maximization scheme to generate multiple informative and diverse region-sequences based on the Lexical-FCN outputs. A winner-takes-all scheme is adopted to weakly associate sentences to region-sequences in the training phase. Third, a sequence-to-sequence learning based language model is trained with the weakly supervised information obtained through the association process. We show that the proposed method can not only produce informative and diverse dense captions, but also outperform state-of-the-art single video captioning methods by a large margin.",Weakly Supervised Dense Video Captioning,"['Zhiqiang Shen', 'Jianguo Li', 'Zhou Su', 'Minjun Li', 'Yurong Chen', 'Yu-Gang Jiang', 'Xiangyang Xue']",http://arxiv.org/abs/1704.01502v1
画像解析,"Nonlocal image representation or group sparsity has attracted considerable interest in various low-level vision tasks and has led to several state-of-the-art image denoising techniques, such as BM3D, LSSC. In the past, convex optimization with sparsity-promoting convex regularization was usually regarded as a standard scheme for estimating sparse signals in noise. However, using convex regularization can not still obtain the correct sparsity solution under some practical problems including image inverse problems. In this paper we propose a non-convex weighted $\ell_p$ minimization based group sparse representation (GSR) framework for image denoising. To make the proposed scheme tractable and robust, the generalized soft-thresholding (GST) algorithm is adopted to solve the non-convex $\ell_p$ minimization problem. In addition, to improve the accuracy of the nonlocal similar patches selection, an adaptive patch search (APS) scheme is proposed. Experimental results have demonstrated that the proposed approach not only outperforms many state-of-the-art denoising methods such as BM3D and WNNM, but also results in a competitive speed.",Non-Convex Weighted Lp Minimization based Group Sparse Representation   Framework for Image Denoising,"['Qiong Wang', 'Xinggan Zhang', 'Yu Wu', 'Yechao Bai', 'Lan Tang', 'Zhiyuan Zha']",http://arxiv.org/abs/1704.01429v1
画像解析,"All people with diabetes have the risk of developing diabetic retinopathy (DR), a vision-threatening complication. Early detection and timely treatment can reduce the occurrence of blindness due to DR. Computer-aided diagnosis has the potential benefit of improving the accuracy and speed in DR detection. This study is concerned with automatic classification of images with microaneurysm (MA) and neovascularization (NV), two important DR clinical findings. Together with normal images, this presents a 3-class classification problem. We propose a modified color auto-correlogram feature (AutoCC) with low dimensionality that is spectrally tuned towards DR images. Recognizing the fact that the images with or without MA or NV are generally different only in small, localized regions, we propose to employ a multi-class, multiple-instance learning framework for performing the classification task using the proposed feature. Extensive experiments including comparison with a few state-of-art image classification approaches have been performed and the results suggest that the proposed approach is promising as it outperforms other methods by a large margin.",Classification of Diabetic Retinopathy Images Using Multi-Class   Multiple-Instance Learning Based on Color Correlogram Features,"['Ragav Venkatesan', 'Parag S. Chandakkar', 'Baoxin Li']",http://arxiv.org/abs/1704.01264v1
画像解析,"Social networking on mobile devices has become a commonplace of everyday life. In addition, photo capturing process has become trivial due to the advances in mobile imaging. Hence people capture a lot of photos everyday and they want them to be visually-attractive. This has given rise to automated, one-touch enhancement tools. However, the inability of those tools to provide personalized and content-adaptive enhancement has paved way for machine-learned methods to do the same. The existing typical machine-learned methods heuristically (e.g. kNN-search) predict the enhancement parameters for a new image by relating the image to a set of similar training images. These heuristic methods need constant interaction with the training images which makes the parameter prediction sub-optimal and computationally expensive at test time which is undesired. This paper presents a novel approach to predicting the enhancement parameters given a new image using only its features, without using any training images. We propose to model the interaction between the image features and its corresponding enhancement parameters using the matrix factorization (MF) principles. We also propose a way to integrate the image features in the MF formulation. We show that our approach outperforms heuristic approaches as well as recent approaches in MF and structured prediction on synthetic as well as real-world data of image enhancement.",A Structured Approach to Predicting Image Enhancement Parameters,"['Parag S. Chandakkar', 'Baoxin Li']",http://arxiv.org/abs/1704.01249v1
画像解析,"Research on automated image enhancement has gained momentum in recent years, partially due to the need for easy-to-use tools for enhancing pictures captured by ubiquitous cameras on mobile devices. Many of the existing leading methods employ machine-learning-based techniques, by which some enhancement parameters for a given image are found by relating the image to the training images with known enhancement parameters. While knowing the structure of the parameter space can facilitate search for the optimal solution, none of the existing methods has explicitly modeled and learned that structure. This paper presents an end-to-end, novel joint regression and ranking approach to model the interaction between desired enhancement parameters and images to be processed, employing a Gaussian process (GP). GP allows searching for ideal parameters using only the image features. The model naturally leads to a ranking technique for comparing images in the induced feature space. Comparative evaluation using the ground-truth based on the MIT-Adobe FiveK dataset plus subjective tests on an additional data-set were used to demonstrate the effectiveness of the proposed approach.",Joint Regression and Ranking for Image Enhancement,"['Parag S. Chandakkar', 'Baoxin Li']",http://arxiv.org/abs/1704.01235v1
画像解析,"We present a new deep learning architecture (called Kd-network) that is designed for 3D model recognition tasks and works with unstructured point clouds. The new architecture performs multiplicative transformations and share parameters of these transformations according to the subdivisions of the point clouds imposed onto them by Kd-trees. Unlike the currently dominant convolutional architectures that usually require rasterization on uniform two-dimensional or three-dimensional grids, Kd-networks do not rely on such grids in any way and therefore avoid poor scaling behaviour. In a series of experiments with popular shape recognition benchmarks, Kd-networks demonstrate competitive performance in a number of shape recognition tasks such as shape classification, shape retrieval and shape part segmentation.",Escape from Cells: Deep Kd-Networks for The Recognition of 3D Point   Cloud Models,"['Roman Klokov', 'Victor Lempitsky']",http://arxiv.org/abs/1704.01222v1
画像解析,"In this paper we address the problem of human action recognition from video sequences. Inspired by the exemplary results obtained via automatic feature learning and deep learning approaches in computer vision, we focus our attention towards learning salient spatial features via a convolutional neural network (CNN) and then map their temporal relationship with the aid of Long-Short-Term-Memory (LSTM) networks. Our contribution in this paper is a deep fusion framework that more effectively exploits spatial features from CNNs with temporal features from LSTM models. We also extensively evaluate their strengths and weaknesses. We find that by combining both the sets of features, the fully connected features effectively act as an attention mechanism to direct the LSTM to interesting parts of the convolutional feature sequence. The significance of our fusion method is its simplicity and effectiveness compared to other state-of-the-art methods. The evaluation results demonstrate that this hierarchical multi stream fusion method has higher performance compared to single stream mapping methods allowing it to achieve high accuracy outperforming current state-of-the-art methods in three widely used databases: UCF11, UCFSports, jHMDB.",Two Stream LSTM: A Deep Fusion Framework for Human Action Recognition,"['Harshala Gammulle', 'Simon Denman', 'Sridha Sridharan', 'Clinton Fookes']",http://arxiv.org/abs/1704.01194v1
画像解析,"Although deep neural networks (DNNs) have achieved great success in many computer vision tasks, recent studies have shown they are vulnerable to adversarial examples. Such examples, typically generated by adding small but purposeful distortions, can frequently fool DNN models. Previous studies to defend against adversarial examples mostly focused on refining the DNN models. They have either shown limited success or suffer from the expensive computation. We propose a new strategy, \emph{feature squeezing}, that can be used to harden DNN models by detecting adversarial examples. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many different feature vectors in the original space into a single sample. By comparing a DNN model's prediction on the original input with that on the squeezed input, feature squeezing detects adversarial examples with high accuracy and few false positives. This paper explores two instances of feature squeezing: reducing the color bit depth of each pixel and smoothing using a spatial filter. These strategies are straightforward, inexpensive, and complementary to defensive methods that operate on the underlying model, such as adversarial training.",Feature Squeezing: Detecting Adversarial Examples in Deep Neural   Networks,"['Weilin Xu', 'David Evans', 'Yanjun Qi']",http://arxiv.org/abs/1704.01155v1
画像解析,"We propose a vision-based method that localizes a ground vehicle using publicly available satellite imagery as the only prior knowledge of the environment. Our approach takes as input a sequence of ground-level images acquired by the vehicle as it navigates, and outputs an estimate of the vehicle's pose relative to a georeferenced satellite image. We overcome the significant viewpoint and appearance variations between the images through a neural multi-view model that learns location-discriminative embeddings in which ground-level images are matched with their corresponding satellite view of the scene. We use this learned function as an observation model in a filtering framework to maintain a distribution over the vehicle's pose. We evaluate our method on different benchmark datasets and demonstrate its ability localize ground-level images in environments novel relative to training, despite the challenges of significant viewpoint and appearance variations.",Satellite Image-based Localization via Learned Embeddings,"['Dong-Ki Kim', 'Matthew R. Walter']",http://arxiv.org/abs/1704.01133v1
音声解析,"Being able to predict whether a song can be a hit has impor- tant applications in the music industry. Although it is true that the popularity of a song can be greatly affected by exter- nal factors such as social and commercial influences, to which degree audio features computed from musical signals (whom we regard as internal factors) can predict song popularity is an interesting research question on its own. Motivated by the recent success of deep learning techniques, we attempt to ex- tend previous work on hit song prediction by jointly learning the audio features and prediction models using deep learning. Specifically, we experiment with a convolutional neural net- work model that takes the primitive mel-spectrogram as the input for feature learning, a more advanced JYnet model that uses an external song dataset for supervised pre-training and auto-tagging, and the combination of these two models. We also consider the inception model to characterize audio infor- mation in different scales. Our experiments suggest that deep structures are indeed more accurate than shallow structures in predicting the popularity of either Chinese or Western Pop songs in Taiwan. We also use the tags predicted by JYnet to gain insights into the result of different models.",Revisiting the problem of audio-based hit song prediction using   convolutional neural networks,"['Li-Chia Yang', 'Szu-Yu Chou', 'Jen-Yu Liu', 'Yi-Hsuan Yang', 'Yi-An Chen']",http://arxiv.org/abs/1704.01280v1
音声解析,"This paper explores linear methods for combining several word embedding models into an ensemble. We construct the combined models using an iterative method based on either ordinary least squares regression or the solution to the orthogonal Procrustes problem.   We evaluate the proposed approaches on Estonian---a morphologically complex language, for which the available corpora for training word embeddings are relatively small. We compare both combined models with each other and with the input word embedding models using synonym and analogy tests. The results show that while using the ordinary least squares regression performs poorly in our experiments, using orthogonal Procrustes to combine several word embedding models into an ensemble model leads to 7-10% relative improvements over the mean result of the initial models in synonym tests and 19-47% in analogy tests.",Linear Ensembles of Word Embedding Models,"['Avo Muromägi', 'Kairit Sirts', 'Sven Laur']",http://arxiv.org/abs/1704.01419v1
音声解析,"In recent years, the technological improvements of low-cost small-scale Unmanned Aerial Vehicles (UAVs) are promoting an ever-increasing use of them in different tasks. In particular, the use of small-scale UAVs is useful in all these low-altitude tasks in which common UAVs cannot be adopted, such as recurrent comprehensive view of wide environments, frequent monitoring of military areas, real-time classification of static and moving entities (e.g., people, cars, etc.). These tasks can be supported by mosaicking and change detection algorithms achieved at low-altitude. Currently, public datasets for testing these algorithms are not available. This paper presents the UMCD dataset, the first collection of geo-referenced video sequences acquired at low-altitude for mosaicking and change detection purposes. Five reference scenarios are also reported.",The UMCD Dataset,"['Danilo Avola', 'Gian Luca Foresti', 'Niki Martinel', 'Daniele Pannone', 'Claudio Piciarelli']",http://arxiv.org/abs/1704.01426v1
音声解析,"Deep Neural Networks (DNNs) have advanced the state-of-the-art in a variety of machine learning tasks and are deployed in increasing numbers of products and services. However, the computational requirements of training and evaluating large-scale DNNs are growing at a much faster pace than the capabilities of the underlying hardware platforms that they are executed upon. In this work, we propose Dynamic Variable Effort Deep Neural Networks (DyVEDeep) to reduce the computational requirements of DNNs during inference. Previous efforts propose specialized hardware implementations for DNNs, statically prune the network, or compress the weights. Complementary to these approaches, DyVEDeep is a dynamic approach that exploits the heterogeneity in the inputs to DNNs to improve their compute efficiency with comparable classification accuracy. DyVEDeep equips DNNs with dynamic effort mechanisms that, in the course of processing an input, identify how critical a group of computations are to classify the input. DyVEDeep dynamically focuses its compute effort only on the critical computa- tions, while skipping or approximating the rest. We propose 3 effort knobs that operate at different levels of granularity viz. neuron, feature and layer levels. We build DyVEDeep versions for 5 popular image recognition benchmarks - one for CIFAR-10 and four for ImageNet (AlexNet, OverFeat and VGG-16, weight-compressed AlexNet). Across all benchmarks, DyVEDeep achieves 2.1x-2.6x reduction in the number of scalar operations, which translates to 1.8x-2.3x performance improvement over a Caffe-based implementation, with < 0.5% loss in accuracy.",DyVEDeep: Dynamic Variable Effort Deep Neural Networks,"['Sanjay Ganapathy', 'Swagath Venkataramani', 'Balaraman Ravindran', 'Anand Raghunathan']",http://arxiv.org/abs/1704.01137v1
音声解析,"Being able to predict whether a song can be a hit has impor- tant applications in the music industry. Although it is true that the popularity of a song can be greatly affected by exter- nal factors such as social and commercial influences, to which degree audio features computed from musical signals (whom we regard as internal factors) can predict song popularity is an interesting research question on its own. Motivated by the recent success of deep learning techniques, we attempt to ex- tend previous work on hit song prediction by jointly learning the audio features and prediction models using deep learning. Specifically, we experiment with a convolutional neural net- work model that takes the primitive mel-spectrogram as the input for feature learning, a more advanced JYnet model that uses an external song dataset for supervised pre-training and auto-tagging, and the combination of these two models. We also consider the inception model to characterize audio infor- mation in different scales. Our experiments suggest that deep structures are indeed more accurate than shallow structures in predicting the popularity of either Chinese or Western Pop songs in Taiwan. We also use the tags predicted by JYnet to gain insights into the result of different models.",Revisiting the problem of audio-based hit song prediction using   convolutional neural networks,"['Li-Chia Yang', 'Szu-Yu Chou', 'Jen-Yu Liu', 'Yi-Hsuan Yang', 'Yi-An Chen']",http://arxiv.org/abs/1704.01280v1
強化学習,"We consider machine learning in a comparison-based setting where we are given a set of points in a metric space, but we have no access to the actual distances between the points. Instead, we can only ask an oracle whether the distance between two points $i$ and $j$ is smaller than the distance between the points $i$ and $k$. We are concerned with data structures and algorithms to find nearest neighbors based on such comparisons. We focus on a simple yet effective algorithm that recursively splits the space by first selecting two random pivot points and then assigning all other points to the closer of the two (comparison tree). We prove that if the metric space satisfies certain expansion conditions, then with high probability the height of the comparison tree is logarithmic in the number of points, leading to efficient search performance. We also provide an upper bound for the failure probability to return the true nearest neighbor. Experiments show that the comparison tree is competitive with algorithms that have access to the actual distance values, and needs less triplet comparisons than other competitors.",Comparison Based Nearest Neighbor Search,"['Siavash Haghiri', 'Debarghya Ghoshdastidar', 'Ulrike von Luxburg']",http://arxiv.org/abs/1704.01460v1
強化学習,"The log-determinant of a kernel matrix appears in a variety of machine learning problems, ranging from determinantal point processes and generalized Markov random fields, through to the training of Gaussian processes. Exact calculation of this term is often intractable when the size of the kernel matrix exceeds a few thousand. In the spirit of probabilistic numerics, we reinterpret the problem of computing the log-determinant as a Bayesian inference problem. In particular, we combine prior knowledge in the form of bounds from matrix theory and evidence derived from stochastic trace estimation to obtain probabilistic estimates for the log-determinant and its associated uncertainty within a given computational budget. Beyond its novelty and theoretic appeal, the performance of our proposal is competitive with state-of-the-art approaches to approximating the log-determinant, while also quantifying the uncertainty due to budget-constrained evidence.",Bayesian Inference of Log Determinants,"['Jack Fitzsimons', 'Kurt Cutajar', 'Michael Osborne', 'Stephen Roberts', 'Maurizio Filippone']",http://arxiv.org/abs/1704.01445v1
強化学習,"We study a model where one target variable Y is correlated with a vector X:=(X_1,...,X_d) of predictor variables being potential causes of Y. We describe a method that infers to what extent the statistical dependences between X and Y are due to the influence of X on Y and to what extent due to a hidden common cause (confounder) of X and Y. The method relies on concentration of measure results for large dimensions d and an independence assumption stating that, in the absence of confounding, the vector of regression coefficients describing the influence of each X on Y typically has `generic orientation' relative to the eigenspaces of the covariance matrix of X. For the special case of a scalar confounder we show that confounding typically spoils this generic orientation in a characteristic way that can be used to quantitatively estimate the amount of confounding.",Detecting confounding in multivariate linear models via spectral   analysis,"['Dominik Janzing', 'Bernhard Schoelkopf']",http://arxiv.org/abs/1704.01430v1
強化学習,"It has recently been shown that many of the existing quasi-Newton algorithms can be formulated as learning algorithms, capable of learning local models of the cost functions. Importantly, this understanding allows us to safely start assembling probabilistic Newton-type algorithms, applicable in situations where we only have access to noisy observations of the cost function and its derivatives. This is where our interest lies.   We make contributions to the use of the non-parametric and probabilistic Gaussian process models in solving these stochastic optimisation problems. Specifically, we present a new algorithm that unites these approximations together with recent probabilistic line search routines to deliver a probabilistic quasi-Newton approach.   We also show that the probabilistic optimisation algorithms deliver promising results on challenging nonlinear system identification problems where the very nature of the problem is such that we can only access the cost function and its derivative via noisy observations, since there are no closed-form expressions available.",On the construction of probabilistic Newton-type algorithms,"['Adrian G. Wills', 'Thomas B. Schön']",http://arxiv.org/abs/1704.01382v1
強化学習,"Sampling is a fundamental topic in graph signal processing, having found applications in estimation, clustering, and video compression. In contrast to traditional signal processing, the irregularity of the signal domain makes selecting a sampling set non-trivial and hard to analyze. Indeed, though conditions for graph signal interpolation from noiseless samples exist, they do not lead to a unique sampling set. Thus, the presence of noise makes sampling set selection a hard combinatorial problem. Although greedy sampling schemes have become ubiquitous in practice, they have no performance guarantee. This work takes a twofold approach to address this issue. First, universal performance bounds are derived for the interpolation of stochastic graph signals from noisy samples. In contrast to currently available bounds, they are not restricted to specific sampling schemes and hold for any sampling sets. Second, this paper provides near-optimal guarantees for greedy sampling by introducing the concept of approximate submodularity and updating the classical greedy bound. It then provides explicit bounds on the approximate supermodularity of the interpolation mean-square error showing that it can be optimized with worst-case guarantees using greedy search even though it is not supermodular. Simulations illustrate the derived bound for different graph models and show an application of graph signal sampling to reduce the complexity of kernel principal component analysis.",Greedy Sampling of Graph Signals,"['Luiz F. O. Chamon', 'Alejandro Ribeiro']",http://arxiv.org/abs/1704.01223v1
強化学習,"In modern probabilistic learning we often wish to perform automatic inference for Bayesian models. However, informative prior distributions can be costly and difficult to elicit, and, as a consequence, flat priors are often chosen with the hope that they are reasonably uninformative. Objective priors such as the Jeffreys and reference priors are generally preferable over flat priors but are not tractable to derive for many models of interest. We address this issue by proposing techniques for learning reference prior approximations: we select a parametric family and optimize a lower bound on the reference prior objective to find the member of the family that serves as a good approximation. Moreover, optimization can be made derivation-free via differentiable Monte Carlo expectations. We experimentally demonstrate the method's effectiveness by recovering Jeffreys priors and learning the Variational Autoencoder's reference prior.",Learning Approximately Objective Priors,"['Eric Nalisnick', 'Padhraic Smyth']",http://arxiv.org/abs/1704.01168v1
強化学習,"The log-determinant of a kernel matrix appears in a variety of machine learning problems, ranging from determinantal point processes and generalized Markov random fields, through to the training of Gaussian processes. Exact calculation of this term is often intractable when the size of the kernel matrix exceeds a few thousand. In the spirit of probabilistic numerics, we reinterpret the problem of computing the log-determinant as a Bayesian inference problem. In particular, we combine prior knowledge in the form of bounds from matrix theory and evidence derived from stochastic trace estimation to obtain probabilistic estimates for the log-determinant and its associated uncertainty within a given computational budget. Beyond its novelty and theoretic appeal, the performance of our proposal is competitive with state-of-the-art approaches to approximating the log-determinant, while also quantifying the uncertainty due to budget-constrained evidence.",Bayesian Inference of Log Determinants,"['Jack Fitzsimons', 'Kurt Cutajar', 'Michael Osborne', 'Stephen Roberts', 'Maurizio Filippone']",http://arxiv.org/abs/1704.01445v1
強化学習,"In modern probabilistic learning we often wish to perform automatic inference for Bayesian models. However, informative prior distributions can be costly and difficult to elicit, and, as a consequence, flat priors are often chosen with the hope that they are reasonably uninformative. Objective priors such as the Jeffreys and reference priors are generally preferable over flat priors but are not tractable to derive for many models of interest. We address this issue by proposing techniques for learning reference prior approximations: we select a parametric family and optimize a lower bound on the reference prior objective to find the member of the family that serves as a good approximation. Moreover, optimization can be made derivation-free via differentiable Monte Carlo expectations. We experimentally demonstrate the method's effectiveness by recovering Jeffreys priors and learning the Variational Autoencoder's reference prior.",Learning Approximately Objective Priors,"['Eric Nalisnick', 'Padhraic Smyth']",http://arxiv.org/abs/1704.01168v1
強化学習,"The iterated posterior linearization filter (IPLF) is an algorithm for Bayesian state estimation that performs the measurement update using iterative statistical regression. The main result behind IPLF is that the posterior approximation is more accurate when the statistical regression of measurement function is done in the posterior instead of the prior as is done in non-iterative Kalman filter extensions. In IPLF, each iteration in principle gives a better posterior estimate to obtain a better statistical regression and more accurate posterior estimate in the next iteration. However, IPLF may diverge. IPLF's fixed- points are not described as solutions to an optimization problem, which makes it challenging to improve its convergence properties. In this letter, we introduce a double-loop version of IPLF, where the inner loop computes the posterior mean using an optimization algorithm. Simulation results are presented to show that the proposed algorithm has better convergence than IPLF and its accuracy is similar to or better than other state-of-the-art algorithms.",Damped Posterior Linearization Filter,"['Matti Raitoharju', 'Lennart Svensson', 'Ángel F. García-Fernández', 'Robert Piché']",http://arxiv.org/abs/1704.01113v1
強化学習,This communication presents a longitudinal model-free control approach for computing the wheel torque command to be applied on a vehicle. This setting enables us to overcome the problem of unknown vehicle parameters for generating a suitable control law. An important parameter in this control setting is made time-varying for ensuring finite-time stability. Several convincing computer simulations are displayed and discussed. Overshoots become therefore smaller. The driving comfort is increased and the robustness to time-delays is improved.,Finite-Time Stabilization of Longitudinal Control for Autonomous   Vehicles via a Model-Free Approach,"['Philip Polack', ""Brigitte d'Andréa-Novel"", 'Michel Fliess', 'Arnaud de la Fortelle', 'Lghani Menhour']",http://arxiv.org/abs/1704.01383v1
強化学習,"TD(0) is one of the most commonly used algorithms in reinforcement learning. Despite this, there is no existing finite sample analysis for TD(0) with function approximation, even for the linear case. Our work is the first to provide such a result. Works that managed to obtain concentration bounds for online Temporal Difference (TD) methods analyzed modified versions of them, carefully crafted for the analyses to hold. These modifications include projections and step-sizes dependent on unknown problem parameters. Our analysis obviates these artificial alterations by exploiting strong properties of TD(0) and tailor-made stochastic approximation tools.",Finite Sample Analysis for TD(0) with Linear Function Approximation,"['Gal Dalal', 'Balázs Szörényi', 'Gugan Thoppe', 'Shie Mannor']",http://arxiv.org/abs/1704.01161v1
強化学習,"While the P vs NP problem is mainly being approach form the point of view of discrete mathematics, this paper proposes two reformulations into the field of abstract algebra and of continuous global optimization - which advanced tools might bring new perspectives and approaches to approach this question. The first one is equivalence of satisfaction of 3-SAT problem with the question of reaching zero of a nonnegative degree 4 multivariate polynomial (sum of squares), what could be tested from the perspective of algebra by using discriminant. Alternatively, in addition to search in the discrete set $\{0,1\}^n$ of boolean values, this minimization formulation allows to take the search inside the continuous $[0,1]^n$ hypercube, exploiting gradients based on the entire problem, which suggest local search direction. Unless exponential growth of uninteresting local minima, such gradient descent from multiple random initial points might be essentially faster than brute force, or be exploited in a physical realization like adiabatic quantum computer, what might endanger some current cryptography. The second discussed approach is using anti-commuting Grassmann numbers $\theta_i$, making $(A \cdot \textrm{diag}(\theta_i))^n$ nonzero only if $A$ has a Hamilton cycle. Hence, the P$\ne$NP assumption implies exponential growth of matrix representation of Grassmann numbers.",P?=NP as minimization of degree 4 polynomial or Grassmann number problem,['Jarek Duda'],http://arxiv.org/abs/1703.04456v2
強化学習,"We propose a novel deep layer cascade (LC) method to improve the accuracy and speed of semantic segmentation. Unlike the conventional model cascade (MC) that is composed of multiple independent models, LC treats a single deep model as a cascade of several sub-models. Earlier sub-models are trained to handle easy and confident regions, and they progressively feed-forward harder regions to the next sub-model for processing. Convolutions are only calculated on these regions to reduce computations. The proposed method possesses several advantages. First, LC classifies most of the easy regions in the shallow stage and makes deeper stage focuses on a few hard regions. Such an adaptive and 'difficulty-aware' learning improves segmentation performance. Second, LC accelerates both training and testing of deep network thanks to early decisions in the shallow stage. Third, in comparison to MC, LC is an end-to-end trainable framework, allowing joint learning of all sub-models. We evaluate our method on PASCAL VOC and Cityscapes datasets, achieving state-of-the-art performance and fast speed.",Not All Pixels Are Equal: Difficulty-aware Semantic Segmentation via   Deep Layer Cascade,"['Xiaoxiao Li', 'Ziwei Liu', 'Ping Luo', 'Chen Change Loy', 'Xiaoou Tang']",http://arxiv.org/abs/1704.01344v1
強化学習,"To solve deep metric learning problems and producing feature embeddings, current methodologies will commonly use a triplet model to minimise the relative distance between samples from the same class and maximise the relative distance between samples from different classes. Though successful, the training convergence of this triplet model can be compromised by the fact that the vast majority of the training samples will produce gradients with magnitudes that are close to zero. This issue has motivated the development of methods that explore the global structure of the embedding and other methods that explore hard negative/positive mining. The effectiveness of such mining methods is often associated with intractable computational requirements. In this paper, we propose a novel deep metric learning method that combines the triplet model and the global structure of the embedding space. We rely on a smart mining procedure that produces effective training samples for a low computational cost. In addition, we propose an adaptive controller that automatically adjusts the smart mining hyper-parameters and speeds up the convergence of the training process. We show empirically that our proposed method allows for fast and more accurate training of triplet ConvNets than other competing mining methods. Additionally, we show that our method achieves new state-of-the-art embedding results for CUB-200-2011 and Cars196 datasets.",Smart Mining for Deep Metric Learning,"['Vijay B G Kumar', 'Ben Harwood', 'Gustavo Carneiro', 'Ian Reid', 'Tom Drummond']",http://arxiv.org/abs/1704.01285v1
強化学習,"Traffic congestion is a widespread problem. Dynamic traffic routing systems and congestion pricing are getting importance in recent research. Lane prediction and vehicle density estimation is an important component of such systems. We introduce a novel problem of vehicle self-positioning which involves predicting the number of lanes on the road and vehicle's position in those lanes using videos captured by a dashboard camera. We propose an integrated closed-loop approach where we use the presence of vehicles to aid the task of self-positioning and vice-versa. To incorporate multiple factors and high-level semantic knowledge into the solution, we formulate this problem as a Bayesian framework. In the framework, the number of lanes, the vehicle's position in those lanes and the presence of other vehicles are considered as parameters. We also propose a bounding box selection scheme to reduce the number of false detections and increase the computational efficiency. We show that the number of box proposals decreases by a factor of 6 using the selection approach. It also results in large reduction in the number of false detections. The entire approach is tested on real-world videos and is found to give acceptable results.",Improving Vision-based Self-positioning in Intelligent Transportation   Systems via Integrated Lane and Vehicle Detection,"['Parag S. Chandakkar', 'Yilin Wang', 'Baoxin Li']",http://arxiv.org/abs/1704.01256v1
強化学習,"We consider machine learning in a comparison-based setting where we are given a set of points in a metric space, but we have no access to the actual distances between the points. Instead, we can only ask an oracle whether the distance between two points $i$ and $j$ is smaller than the distance between the points $i$ and $k$. We are concerned with data structures and algorithms to find nearest neighbors based on such comparisons. We focus on a simple yet effective algorithm that recursively splits the space by first selecting two random pivot points and then assigning all other points to the closer of the two (comparison tree). We prove that if the metric space satisfies certain expansion conditions, then with high probability the height of the comparison tree is logarithmic in the number of points, leading to efficient search performance. We also provide an upper bound for the failure probability to return the true nearest neighbor. Experiments show that the comparison tree is competitive with algorithms that have access to the actual distance values, and needs less triplet comparisons than other competitors.",Comparison Based Nearest Neighbor Search,"['Siavash Haghiri', 'Debarghya Ghoshdastidar', 'Ulrike von Luxburg']",http://arxiv.org/abs/1704.01460v1
