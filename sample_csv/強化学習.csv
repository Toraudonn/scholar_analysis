,プログラム実行日時,論文更新日時,論文リンク,PDFリンク,元論文タイトル,論文タイトル,元サマリ,サマリ,著者,事前付与ジャンル,ニューラルネットワーク,自然言語処理,マーケティング,画像解析,音声解析,強化学習
1,2017-04-07T15:27:52Z,2017-04-05T14:54:28Z,http://arxiv.org/abs/1704.01460v1,http://arxiv.org/pdf/1704.01460v1,Comparison Based Nearest Neighbor Search,comparison base nearest neighbor search,"We consider machine learning in a comparison-based setting where we are given a set of points in a metric space, but we have no access to the actual distances between the points. Instead, we can only ask an oracle whether the distance between two points $i$ and $j$ is smaller than the distance between the points $i$ and $k$. We are concerned with data structures and algorithms to find nearest neighbors based on such comparisons. We focus on a simple yet effective algorithm that recursively splits the space by first selecting two random pivot points and then assigning all other points to the closer of the two (comparison tree). We prove that if the metric space satisfies certain expansion conditions, then with high probability the height of the comparison tree is logarithmic in the number of points, leading to efficient search performance. We also provide an upper bound for the failure probability to return the true nearest neighbor. Experiments show that the comparison tree is competitive with algorithms that have access to the actual distance values, and needs less triplet comparisons than other competitors.",consid machin learn comparison base set given set point metric space access actual distanc point instead onli ask oracl whether distanc two point smaller distanc point concern data structur algorithm find nearest neighbor base comparison focus simpl yet effect algorithm recurs split space first select two random pivot point assign point closer two comparison tree prove metric space satisfi certain expans condit high probabl height comparison tree logarithm number point lead effici search perform also provid upper bound failur probabl return true nearest neighbor experi show comparison tree competit algorithm access actual distanc valu need less triplet comparison competitor,"['Siavash Haghiri', 'Debarghya Ghoshdastidar', 'Ulrike von Luxburg']","['stat.ML', 'cs.DS', 'cs.LG']",False,False,False,False,False,True
2,2017-04-07T15:27:52Z,2017-04-05T14:23:53Z,http://arxiv.org/abs/1704.01445v1,http://arxiv.org/pdf/1704.01445v1,Bayesian Inference of Log Determinants,bayesian infer log determin,"The log-determinant of a kernel matrix appears in a variety of machine learning problems, ranging from determinantal point processes and generalized Markov random fields, through to the training of Gaussian processes. Exact calculation of this term is often intractable when the size of the kernel matrix exceeds a few thousand. In the spirit of probabilistic numerics, we reinterpret the problem of computing the log-determinant as a Bayesian inference problem. In particular, we combine prior knowledge in the form of bounds from matrix theory and evidence derived from stochastic trace estimation to obtain probabilistic estimates for the log-determinant and its associated uncertainty within a given computational budget. Beyond its novelty and theoretic appeal, the performance of our proposal is competitive with state-of-the-art approaches to approximating the log-determinant, while also quantifying the uncertainty due to budget-constrained evidence.",log determin kernel matrix appear varieti machin learn problem rang determinant point process general markov random field train gaussian process exact calcul term often intract size kernel matrix exceed thousand spirit probabilist numer reinterpret problem comput log determin bayesian infer problem particular combin prior knowledg form bound matrix theori evid deriv stochast trace estim obtain probabilist estim log determin associ uncertainti within given comput budget beyond novelti theoret appeal perform propos competit state art approach approxim log determin also quantifi uncertainti due budget constrain evid,"['Jack Fitzsimons', 'Kurt Cutajar', 'Michael Osborne', 'Stephen Roberts', 'Maurizio Filippone']","['stat.ML', 'cs.NA', 'stat.CO']",False,False,False,False,False,True
3,2017-04-07T15:27:52Z,2017-04-05T13:54:29Z,http://arxiv.org/abs/1704.01430v1,http://arxiv.org/pdf/1704.01430v1,Detecting confounding in multivariate linear models via spectral   analysis,detect confound multivari linear model via spectral analysi,"We study a model where one target variable Y is correlated with a vector X:=(X_1,...,X_d) of predictor variables being potential causes of Y. We describe a method that infers to what extent the statistical dependences between X and Y are due to the influence of X on Y and to what extent due to a hidden common cause (confounder) of X and Y. The method relies on concentration of measure results for large dimensions d and an independence assumption stating that, in the absence of confounding, the vector of regression coefficients describing the influence of each X on Y typically has `generic orientation' relative to the eigenspaces of the covariance matrix of X. For the special case of a scalar confounder we show that confounding typically spoils this generic orientation in a characteristic way that can be used to quantitatively estimate the amount of confounding.",studi model one target variabl correl vector predictor variabl potenti caus describ method infer extent statist depend due influenc extent due hidden common caus confound method reli concentr measur result larg dimens independ assumpt state absenc confound vector regress coeffici describ influenc typic generic orient relat eigenspac covari matrix special case scalar confound show confound typic spoil generic orient characterist way use quantit estim amount confound,"['Dominik Janzing', 'Bernhard Schoelkopf']",['stat.ML'],False,False,False,False,False,True
4,2017-04-07T15:27:52Z,2017-04-05T12:34:20Z,http://arxiv.org/abs/1704.01382v1,http://arxiv.org/pdf/1704.01382v1,On the construction of probabilistic Newton-type algorithms,construct probabilist newton type algorithm,"It has recently been shown that many of the existing quasi-Newton algorithms can be formulated as learning algorithms, capable of learning local models of the cost functions. Importantly, this understanding allows us to safely start assembling probabilistic Newton-type algorithms, applicable in situations where we only have access to noisy observations of the cost function and its derivatives. This is where our interest lies.   We make contributions to the use of the non-parametric and probabilistic Gaussian process models in solving these stochastic optimisation problems. Specifically, we present a new algorithm that unites these approximations together with recent probabilistic line search routines to deliver a probabilistic quasi-Newton approach.   We also show that the probabilistic optimisation algorithms deliver promising results on challenging nonlinear system identification problems where the very nature of the problem is such that we can only access the cost function and its derivative via noisy observations, since there are no closed-form expressions available.",recent shown mani exist quasi newton algorithm formul learn algorithm capabl learn local model cost function import understand allow us safe start assembl probabilist newton type algorithm applic situat onli access noisi observ cost function deriv interest lie make contribut use non parametr probabilist gaussian process model solv stochast optimis problem specif present new algorithm unit approxim togeth recent probabilist line search routin deliv probabilist quasi newton approach also show probabilist optimis algorithm deliv promis result challeng nonlinear system identif problem veri natur problem onli access cost function deriv via noisi observ sinc close form express avail,"['Adrian G. Wills', 'Thomas B. Schön']",['stat.ML'],False,False,False,False,False,True
8,2017-04-07T15:27:53Z,2017-04-05T00:09:37Z,http://arxiv.org/abs/1704.01223v1,http://arxiv.org/pdf/1704.01223v1,Greedy Sampling of Graph Signals,greedi sampl graph signal,"Sampling is a fundamental topic in graph signal processing, having found applications in estimation, clustering, and video compression. In contrast to traditional signal processing, the irregularity of the signal domain makes selecting a sampling set non-trivial and hard to analyze. Indeed, though conditions for graph signal interpolation from noiseless samples exist, they do not lead to a unique sampling set. Thus, the presence of noise makes sampling set selection a hard combinatorial problem. Although greedy sampling schemes have become ubiquitous in practice, they have no performance guarantee. This work takes a twofold approach to address this issue. First, universal performance bounds are derived for the interpolation of stochastic graph signals from noisy samples. In contrast to currently available bounds, they are not restricted to specific sampling schemes and hold for any sampling sets. Second, this paper provides near-optimal guarantees for greedy sampling by introducing the concept of approximate submodularity and updating the classical greedy bound. It then provides explicit bounds on the approximate supermodularity of the interpolation mean-square error showing that it can be optimized with worst-case guarantees using greedy search even though it is not supermodular. Simulations illustrate the derived bound for different graph models and show an application of graph signal sampling to reduce the complexity of kernel principal component analysis.",sampl fundament topic graph signal process found applic estim cluster video compress contrast tradit signal process irregular signal domain make select sampl set non trivial hard analyz inde though condit graph signal interpol noiseless sampl exist lead uniqu sampl set thus presenc nois make sampl set select hard combinatori problem although greedi sampl scheme becom ubiquit practic perform guarante work take twofold approach address issu first univers perform bound deriv interpol stochast graph signal noisi sampl contrast current avail bound restrict specif sampl scheme hold ani sampl set second paper provid near optim guarante greedi sampl introduc concept approxim submodular updat classic greedi bound provid explicit bound approxim supermodular interpol mean squar error show optim worst case guarante use greedi search even though supermodular simul illustr deriv bound differ graph model show applic graph signal sampl reduc complex kernel princip compon analysi,"['Luiz F. O. Chamon', 'Alejandro Ribeiro']","['cs.IT', 'cs.SI', 'math.IT', 'stat.ML']",False,False,False,False,False,True
9,2017-04-07T15:27:53Z,2017-04-04T20:07:26Z,http://arxiv.org/abs/1704.01168v1,http://arxiv.org/pdf/1704.01168v1,Learning Approximately Objective Priors,learn approxim object prior,"In modern probabilistic learning we often wish to perform automatic inference for Bayesian models. However, informative prior distributions can be costly and difficult to elicit, and, as a consequence, flat priors are often chosen with the hope that they are reasonably uninformative. Objective priors such as the Jeffreys and reference priors are generally preferable over flat priors but are not tractable to derive for many models of interest. We address this issue by proposing techniques for learning reference prior approximations: we select a parametric family and optimize a lower bound on the reference prior objective to find the member of the family that serves as a good approximation. Moreover, optimization can be made derivation-free via differentiable Monte Carlo expectations. We experimentally demonstrate the method's effectiveness by recovering Jeffreys priors and learning the Variational Autoencoder's reference prior.",modern probabilist learn often wish perform automat infer bayesian model howev inform prior distribut cost difficult elicit consequ flat prior often chosen hope reason uninform object prior jeffrey refer prior general prefer flat prior tractabl deriv mani model interest address issu propos techniqu learn refer prior approxim select parametr famili optim lower bound refer prior object find member famili serv good approxim moreov optim made deriv free via differenti mont carlo expect experiment demonstr method effect recov jeffrey prior learn variat autoencod refer prior,"['Eric Nalisnick', 'Padhraic Smyth']","['stat.ML', 'stat.CO']",False,False,False,False,False,True
10,2017-04-07T15:27:53Z,2017-04-04T16:18:07Z,http://arxiv.org/abs/1704.01087v1,http://arxiv.org/pdf/1704.01087v1,Probabilistic Search for Structured Data via Probabilistic Programming   and Nonparametric Bayes,probabilist search structur data via probabilist program nonparametr bay,"Databases are widespread, yet extracting relevant data can be difficult. Without substantial domain knowledge, multivariate search queries often return sparse or uninformative results. This paper introduces an approach for searching structured data based on probabilistic programming and nonparametric Bayes. Users specify queries in a probabilistic language that combines standard SQL database search operators with an information theoretic ranking function called predictive relevance. Predictive relevance can be calculated by a fast sparse matrix algorithm based on posterior samples from CrossCat, a nonparametric Bayesian model for high-dimensional, heterogeneously-typed data tables. The result is a flexible search technique that applies to a broad class of information retrieval problems, which we integrate into BayesDB, a probabilistic programming platform for probabilistic data analysis. This paper demonstrates applications to databases of US colleges, global macroeconomic indicators of public health, and classic cars. We found that human evaluators often prefer the results from probabilistic search to results from a standard baseline.",databas widespread yet extract relev data difficult without substanti domain knowledg multivari search queri often return spars uninform result paper introduc approach search structur data base probabilist program nonparametr bay user specifi queri probabilist languag combin standard sql databas search oper inform theoret rank function call predict relev predict relev calcul fast spars matrix algorithm base posterior sampl crosscat nonparametr bayesian model high dimension heterogen type data tabl result flexibl search techniqu appli broad class inform retriev problem integr bayesdb probabilist program platform probabilist data analysi paper demonstr applic databas us colleg global macroeconom indic public health classic car found human evalu often prefer result probabilist search result standard baselin,"['Feras Saad', 'Leonardo Casarsa', 'Vikash Mansinghka']","['cs.AI', 'cs.DB', 'cs.LG', 'stat.ML']",False,False,False,False,False,True
15,2017-04-07T15:28:15Z,2017-04-05T14:23:53Z,http://arxiv.org/abs/1704.01445v1,http://arxiv.org/pdf/1704.01445v1,Bayesian Inference of Log Determinants,bayesian infer log determin,"The log-determinant of a kernel matrix appears in a variety of machine learning problems, ranging from determinantal point processes and generalized Markov random fields, through to the training of Gaussian processes. Exact calculation of this term is often intractable when the size of the kernel matrix exceeds a few thousand. In the spirit of probabilistic numerics, we reinterpret the problem of computing the log-determinant as a Bayesian inference problem. In particular, we combine prior knowledge in the form of bounds from matrix theory and evidence derived from stochastic trace estimation to obtain probabilistic estimates for the log-determinant and its associated uncertainty within a given computational budget. Beyond its novelty and theoretic appeal, the performance of our proposal is competitive with state-of-the-art approaches to approximating the log-determinant, while also quantifying the uncertainty due to budget-constrained evidence.",log determin kernel matrix appear varieti machin learn problem rang determinant point process general markov random field train gaussian process exact calcul term often intract size kernel matrix exceed thousand spirit probabilist numer reinterpret problem comput log determin bayesian infer problem particular combin prior knowledg form bound matrix theori evid deriv stochast trace estim obtain probabilist estim log determin associ uncertainti within given comput budget beyond novelti theoret appeal perform propos competit state art approach approxim log determin also quantifi uncertainti due budget constrain evid,"['Jack Fitzsimons', 'Kurt Cutajar', 'Michael Osborne', 'Stephen Roberts', 'Maurizio Filippone']","['stat.ML', 'cs.NA', 'stat.CO']",False,False,False,False,False,True
16,2017-04-07T15:28:15Z,2017-04-04T20:07:26Z,http://arxiv.org/abs/1704.01168v1,http://arxiv.org/pdf/1704.01168v1,Learning Approximately Objective Priors,learn approxim object prior,"In modern probabilistic learning we often wish to perform automatic inference for Bayesian models. However, informative prior distributions can be costly and difficult to elicit, and, as a consequence, flat priors are often chosen with the hope that they are reasonably uninformative. Objective priors such as the Jeffreys and reference priors are generally preferable over flat priors but are not tractable to derive for many models of interest. We address this issue by proposing techniques for learning reference prior approximations: we select a parametric family and optimize a lower bound on the reference prior objective to find the member of the family that serves as a good approximation. Moreover, optimization can be made derivation-free via differentiable Monte Carlo expectations. We experimentally demonstrate the method's effectiveness by recovering Jeffreys priors and learning the Variational Autoencoder's reference prior.",modern probabilist learn often wish perform automat infer bayesian model howev inform prior distribut cost difficult elicit consequ flat prior often chosen hope reason uninform object prior jeffrey refer prior general prefer flat prior tractabl deriv mani model interest address issu propos techniqu learn refer prior approxim select parametr famili optim lower bound refer prior object find member famili serv good approxim moreov optim made deriv free via differenti mont carlo expect experiment demonstr method effect recov jeffrey prior learn variat autoencod refer prior,"['Eric Nalisnick', 'Padhraic Smyth']","['stat.ML', 'stat.CO']",False,False,False,False,False,True
17,2017-04-07T15:28:15Z,2017-04-04T17:48:39Z,http://arxiv.org/abs/1704.01113v1,http://arxiv.org/pdf/1704.01113v1,Damped Posterior Linearization Filter,damp posterior linear filter,"The iterated posterior linearization filter (IPLF) is an algorithm for Bayesian state estimation that performs the measurement update using iterative statistical regression. The main result behind IPLF is that the posterior approximation is more accurate when the statistical regression of measurement function is done in the posterior instead of the prior as is done in non-iterative Kalman filter extensions. In IPLF, each iteration in principle gives a better posterior estimate to obtain a better statistical regression and more accurate posterior estimate in the next iteration. However, IPLF may diverge. IPLF's fixed- points are not described as solutions to an optimization problem, which makes it challenging to improve its convergence properties. In this letter, we introduce a double-loop version of IPLF, where the inner loop computes the posterior mean using an optimization algorithm. Simulation results are presented to show that the proposed algorithm has better convergence than IPLF and its accuracy is similar to or better than other state-of-the-art algorithms.",iter posterior linear filter iplf algorithm bayesian state estim perform measur updat use iter statist regress main result behind iplf posterior approxim accur statist regress measur function done posterior instead prior done non iter kalman filter extens iplf iter principl give better posterior estim obtain better statist regress accur posterior estim next iter howev iplf may diverg iplf fix point describ solut optim problem make challeng improv converg properti letter introduc doubl loop version iplf inner loop comput posterior mean use optim algorithm simul result present show propos algorithm better converg iplf accuraci similar better state art algorithm,"['Matti Raitoharju', 'Lennart Svensson', 'Ángel F. García-Fernández', 'Robert Piché']","['math.OC', 'stat.CO']",False,False,False,False,False,True
26,2017-04-07T15:28:46Z,2017-04-04T15:30:50Z,http://arxiv.org/abs/1704.01055v1,http://arxiv.org/pdf/1704.01055v1,One-step Local M-estimator for Integrated Jump-Diffusion Models,one step local estim integr jump diffus model,"In this paper, robust nonparametric estimators, instead of local linear estimators, are adapted for infinitesimal coefficients associated with integrated jump-diffusion models to avoid the impact of outliers on accuracy. Furthermore, consider the complexity of iteration of the solution for local M-estimator, we propose the one-step local M-estimators to release the computation burden. Under appropriate regularity conditions, we prove that one-step local M-estimators and the fully iterative M-estimators have the same performance in consistency and asymptotic normality. Through simulation, our method present advantages in bias reduction, robustness and reducing computation cost. In addition, the estimators are illustrated empirically through stock index under different sampling frequency.",paper robust nonparametr estim instead local linear estim adapt infinitesim coeffici associ integr jump diffus model avoid impact outlier accuraci furthermor consid complex iter solut local estim propos one step local estim releas comput burden appropri regular condit prove one step local estim fulli iter estim perform consist asymptot normal simul method present advantag bias reduct robust reduc comput cost addit estim illustr empir stock index differ sampl frequenc,"['Yuping Song', 'Hanchao Wang']","['math.ST', 'stat.TH']",False,False,False,False,False,True
28,2017-04-07T15:29:00Z,2017-04-05T12:35:20Z,http://arxiv.org/abs/1704.01383v1,http://arxiv.org/pdf/1704.01383v1,Finite-Time Stabilization of Longitudinal Control for Autonomous   Vehicles via a Model-Free Approach,finit time stabil longitudin control autonom vehicl via model free approach,This communication presents a longitudinal model-free control approach for computing the wheel torque command to be applied on a vehicle. This setting enables us to overcome the problem of unknown vehicle parameters for generating a suitable control law. An important parameter in this control setting is made time-varying for ensuring finite-time stability. Several convincing computer simulations are displayed and discussed. Overshoots become therefore smaller. The driving comfort is increased and the robustness to time-delays is improved.,communic present longitudin model free control approach comput wheel torqu command appli vehicl set enabl us overcom problem unknown vehicl paramet generat suitabl control law import paramet control set made time vari ensur finit time stabil sever convinc comput simul display discuss overshoot becom therefor smaller drive comfort increas robust time delay improv,"['Philip Polack', ""Brigitte d'Andréa-Novel"", 'Michel Fliess', 'Arnaud de la Fortelle', 'Lghani Menhour']","['cs.SY', 'cs.AI', 'math.OC']",False,False,False,False,False,True
30,2017-04-07T15:29:01Z,2017-04-04T19:47:52Z,http://arxiv.org/abs/1704.01161v1,http://arxiv.org/pdf/1704.01161v1,Finite Sample Analysis for TD(0) with Linear Function Approximation,finit sampl analysi td linear function approxim,"TD(0) is one of the most commonly used algorithms in reinforcement learning. Despite this, there is no existing finite sample analysis for TD(0) with function approximation, even for the linear case. Our work is the first to provide such a result. Works that managed to obtain concentration bounds for online Temporal Difference (TD) methods analyzed modified versions of them, carefully crafted for the analyses to hold. These modifications include projections and step-sizes dependent on unknown problem parameters. Our analysis obviates these artificial alterations by exploiting strong properties of TD(0) and tailor-made stochastic approximation tools.",td one common use algorithm reinforc learn despit exist finit sampl analysi td function approxim even linear case work first provid result work manag obtain concentr bound onlin tempor differ td method analyz modifi version care craft analys hold modif includ project step size depend unknown problem paramet analysi obviat artifici alter exploit strong properti td tailor made stochast approxim tool,"['Gal Dalal', 'Balázs Szörényi', 'Gugan Thoppe', 'Shie Mannor']",['cs.AI'],False,False,False,False,False,True
31,2017-04-07T15:29:01Z,2017-04-04T16:18:07Z,http://arxiv.org/abs/1704.01087v1,http://arxiv.org/pdf/1704.01087v1,Probabilistic Search for Structured Data via Probabilistic Programming   and Nonparametric Bayes,probabilist search structur data via probabilist program nonparametr bay,"Databases are widespread, yet extracting relevant data can be difficult. Without substantial domain knowledge, multivariate search queries often return sparse or uninformative results. This paper introduces an approach for searching structured data based on probabilistic programming and nonparametric Bayes. Users specify queries in a probabilistic language that combines standard SQL database search operators with an information theoretic ranking function called predictive relevance. Predictive relevance can be calculated by a fast sparse matrix algorithm based on posterior samples from CrossCat, a nonparametric Bayesian model for high-dimensional, heterogeneously-typed data tables. The result is a flexible search technique that applies to a broad class of information retrieval problems, which we integrate into BayesDB, a probabilistic programming platform for probabilistic data analysis. This paper demonstrates applications to databases of US colleges, global macroeconomic indicators of public health, and classic cars. We found that human evaluators often prefer the results from probabilistic search to results from a standard baseline.",databas widespread yet extract relev data difficult without substanti domain knowledg multivari search queri often return spars uninform result paper introduc approach search structur data base probabilist program nonparametr bay user specifi queri probabilist languag combin standard sql databas search oper inform theoret rank function call predict relev predict relev calcul fast spars matrix algorithm base posterior sampl crosscat nonparametr bayesian model high dimension heterogen type data tabl result flexibl search techniqu appli broad class inform retriev problem integr bayesdb probabilist program platform probabilist data analysi paper demonstr applic databas us colleg global macroeconom indic public health classic car found human evalu often prefer result probabilist search result standard baselin,"['Feras Saad', 'Leonardo Casarsa', 'Vikash Mansinghka']","['cs.AI', 'cs.DB', 'cs.LG', 'stat.ML']",False,False,False,False,False,True
39,2017-04-07T15:29:23Z,2017-04-04T16:45:29Z,http://arxiv.org/abs/1704.01101v1,http://arxiv.org/pdf/1704.01101v1,On Resource-bounded versions of the van Lambalgen theorem,resourc bound version van lambalgen theorem,"The van Lambalgen theorem is a surprising result in algorithmic information theory concerning the symmetry of relative randomness. It establishes that for any pair of infinite sequences $A$ and $B$, $B$ is Martin-L\""of random and $A$ is Martin-L\""of random relative to $B$ if and only if the interleaved sequence $A \uplus B$ is Martin-L\""of random. This implies that $A$ is relative random to $B$ if and only if $B$ is random relative to $A$ \cite{vanLambalgen}, \cite{Nies09}, \cite{HirschfeldtBook}. This paper studies the validity of this phenomenon for different notions of time-bounded relative randomness.   We prove the classical van Lambalgen theorem using martingales and Kolmogorov compressibility. We establish the failure of relative randomness in these settings, for both time-bounded martingales and time-bounded Kolmogorov complexity. We adapt our classical proofs when applicable to the time-bounded setting, and construct counterexamples when they fail. The mode of failure of the theorem may depend on the notion of time-bounded randomness.",van lambalgen theorem surpris result algorithm inform theori concern symmetri relat random establish ani pair infinit sequenc martin random martin random relat onli interleav sequenc uplus martin random impli relat random onli random relat cite vanlambalgen cite nie cite hirschfeldtbook paper studi valid phenomenon differ notion time bound relat random prove classic van lambalgen theorem use martingal kolmogorov compress establish failur relat random set time bound martingal time bound kolmogorov complex adapt classic proof applic time bound set construct counterexampl fail mode failur theorem may depend notion time bound random,"['Diptarka Chakraborty', 'Satyadev Nandakumar', 'Himanshu Shukla']",['cs.CC'],False,False,False,False,False,True
48,2017-04-07T15:30:05Z,2017-04-05T09:58:51Z,http://arxiv.org/abs/1704.01344v1,http://arxiv.org/pdf/1704.01344v1,Not All Pixels Are Equal: Difficulty-aware Semantic Segmentation via   Deep Layer Cascade,pixel equal difficulti awar semant segment via deep layer cascad,"We propose a novel deep layer cascade (LC) method to improve the accuracy and speed of semantic segmentation. Unlike the conventional model cascade (MC) that is composed of multiple independent models, LC treats a single deep model as a cascade of several sub-models. Earlier sub-models are trained to handle easy and confident regions, and they progressively feed-forward harder regions to the next sub-model for processing. Convolutions are only calculated on these regions to reduce computations. The proposed method possesses several advantages. First, LC classifies most of the easy regions in the shallow stage and makes deeper stage focuses on a few hard regions. Such an adaptive and 'difficulty-aware' learning improves segmentation performance. Second, LC accelerates both training and testing of deep network thanks to early decisions in the shallow stage. Third, in comparison to MC, LC is an end-to-end trainable framework, allowing joint learning of all sub-models. We evaluate our method on PASCAL VOC and Cityscapes datasets, achieving state-of-the-art performance and fast speed.",propos novel deep layer cascad lc method improv accuraci speed semant segment unlik convent model cascad mc compos multipl independ model lc treat singl deep model cascad sever sub model earlier sub model train handl easi confid region progress feed forward harder region next sub model process convolut onli calcul region reduc comput propos method possess sever advantag first lc classifi easi region shallow stage make deeper stage focus hard region adapt difficulti awar learn improv segment perform second lc acceler train test deep network thank earli decis shallow stage third comparison mc lc end end trainabl framework allow joint learn sub model evalu method pascal voc cityscap dataset achiev state art perform fast speed,"['Xiaoxiao Li', 'Ziwei Liu', 'Ping Luo', 'Chen Change Loy', 'Xiaoou Tang']","['cs.CV', 'cs.LG']",False,False,False,False,False,True
50,2017-04-07T15:30:05Z,2017-04-05T06:58:56Z,http://arxiv.org/abs/1704.01285v1,http://arxiv.org/pdf/1704.01285v1,Smart Mining for Deep Metric Learning,smart mine deep metric learn,"To solve deep metric learning problems and producing feature embeddings, current methodologies will commonly use a triplet model to minimise the relative distance between samples from the same class and maximise the relative distance between samples from different classes. Though successful, the training convergence of this triplet model can be compromised by the fact that the vast majority of the training samples will produce gradients with magnitudes that are close to zero. This issue has motivated the development of methods that explore the global structure of the embedding and other methods that explore hard negative/positive mining. The effectiveness of such mining methods is often associated with intractable computational requirements. In this paper, we propose a novel deep metric learning method that combines the triplet model and the global structure of the embedding space. We rely on a smart mining procedure that produces effective training samples for a low computational cost. In addition, we propose an adaptive controller that automatically adjusts the smart mining hyper-parameters and speeds up the convergence of the training process. We show empirically that our proposed method allows for fast and more accurate training of triplet ConvNets than other competing mining methods. Additionally, we show that our method achieves new state-of-the-art embedding results for CUB-200-2011 and Cars196 datasets.",solv deep metric learn problem produc featur embed current methodolog common use triplet model minimis relat distanc sampl class maximis relat distanc sampl differ class though success train converg triplet model compromis fact vast major train sampl produc gradient magnitud close zero issu motiv develop method explor global structur embed method explor hard negat posit mine effect mine method often associ intract comput requir paper propos novel deep metric learn method combin triplet model global structur embed space reli smart mine procedur produc effect train sampl low comput cost addit propos adapt control automat adjust smart mine hyper paramet speed converg train process show empir propos method allow fast accur train triplet convnet compet mine method addit show method achiev new state art embed result cub car dataset,"['Vijay B G Kumar', 'Ben Harwood', 'Gustavo Carneiro', 'Ian Reid', 'Tom Drummond']",['cs.CV'],False,False,False,False,False,True
54,2017-04-07T15:30:05Z,2017-04-05T03:38:08Z,http://arxiv.org/abs/1704.01256v1,http://arxiv.org/pdf/1704.01256v1,Improving Vision-based Self-positioning in Intelligent Transportation   Systems via Integrated Lane and Vehicle Detection,improv vision base self posit intellig transport system via integr lane vehicl detect,"Traffic congestion is a widespread problem. Dynamic traffic routing systems and congestion pricing are getting importance in recent research. Lane prediction and vehicle density estimation is an important component of such systems. We introduce a novel problem of vehicle self-positioning which involves predicting the number of lanes on the road and vehicle's position in those lanes using videos captured by a dashboard camera. We propose an integrated closed-loop approach where we use the presence of vehicles to aid the task of self-positioning and vice-versa. To incorporate multiple factors and high-level semantic knowledge into the solution, we formulate this problem as a Bayesian framework. In the framework, the number of lanes, the vehicle's position in those lanes and the presence of other vehicles are considered as parameters. We also propose a bounding box selection scheme to reduce the number of false detections and increase the computational efficiency. We show that the number of box proposals decreases by a factor of 6 using the selection approach. It also results in large reduction in the number of false detections. The entire approach is tested on real-world videos and is found to give acceptable results.",traffic congest widespread problem dynam traffic rout system congest price get import recent research lane predict vehicl densiti estim import compon system introduc novel problem vehicl self posit involv predict number lane road vehicl posit lane use video captur dashboard camera propos integr close loop approach use presenc vehicl aid task self posit vice versa incorpor multipl factor high level semant knowledg solut formul problem bayesian framework framework number lane vehicl posit lane presenc vehicl consid paramet also propos bound box select scheme reduc number fals detect increas comput effici show number box propos decreas factor use select approach also result larg reduct number fals detect entir approach test real world video found give accept result,"['Parag S. Chandakkar', 'Yilin Wang', 'Baoxin Li']",['cs.CV'],False,False,True,False,False,True
66,2017-04-07T15:30:06Z,2017-04-04T16:15:54Z,http://arxiv.org/abs/1704.01085v1,http://arxiv.org/pdf/1704.01085v1,Deep Depth From Focus,deep depth focus,"Depth from Focus (DFF) is one of the classical ill-posed inverse problems in computer vision. Most approaches recover the depth at each pixel based on the focal setting which exhibits maximal sharpness. Yet, it is not obvious how to reliably estimate the sharpness level, particularly in low-textured areas. In this paper, we propose 'Deep Depth From Focus (DDFF)' as the first end-to-end learning approach to this problem. Towards this goal, we create a novel real-scene indoor benchmark composed of 4D light-field images obtained from a plenoptic camera and ground truth depth obtained from a registered RGB-D sensor. Compared to existing benchmarks our dataset is 30 times larger, enabling the use of machine learning for this inverse problem. We compare our results with state-of-the-art DFF methods and we also analyze the effect of several key deep architectural components. These experiments show that DDFFNet achieves state-of-the-art performance in all scenes, reducing depth error by more than 70% wrt classic DFF methods.",depth focus dff one classic ill pose invers problem comput vision approach recov depth pixel base focal set exhibit maxim sharp yet obvious reliabl estim sharp level particular low textur area paper propos deep depth focus ddff first end end learn approach problem toward goal creat novel real scene indoor benchmark compos light field imag obtain plenopt camera ground truth depth obtain regist rgb sensor compar exist benchmark dataset time larger enabl use machin learn invers problem compar result state art dff method also analyz effect sever key deep architectur compon experi show ddffnet achiev state art perform scene reduc depth error wrt classic dff method,"['Caner Hazirbas', 'Laura Leal-Taixé', 'Daniel Cremers']",['cs.CV'],False,False,False,False,False,True
68,2017-04-07T15:30:13Z,2017-04-05T14:54:28Z,http://arxiv.org/abs/1704.01460v1,http://arxiv.org/pdf/1704.01460v1,Comparison Based Nearest Neighbor Search,comparison base nearest neighbor search,"We consider machine learning in a comparison-based setting where we are given a set of points in a metric space, but we have no access to the actual distances between the points. Instead, we can only ask an oracle whether the distance between two points $i$ and $j$ is smaller than the distance between the points $i$ and $k$. We are concerned with data structures and algorithms to find nearest neighbors based on such comparisons. We focus on a simple yet effective algorithm that recursively splits the space by first selecting two random pivot points and then assigning all other points to the closer of the two (comparison tree). We prove that if the metric space satisfies certain expansion conditions, then with high probability the height of the comparison tree is logarithmic in the number of points, leading to efficient search performance. We also provide an upper bound for the failure probability to return the true nearest neighbor. Experiments show that the comparison tree is competitive with algorithms that have access to the actual distance values, and needs less triplet comparisons than other competitors.",consid machin learn comparison base set given set point metric space access actual distanc point instead onli ask oracl whether distanc two point smaller distanc point concern data structur algorithm find nearest neighbor base comparison focus simpl yet effect algorithm recurs split space first select two random pivot point assign point closer two comparison tree prove metric space satisfi certain expans condit high probabl height comparison tree logarithm number point lead effici search perform also provid upper bound failur probabl return true nearest neighbor experi show comparison tree competit algorithm access actual distanc valu need less triplet comparison competitor,"['Siavash Haghiri', 'Debarghya Ghoshdastidar', 'Ulrike von Luxburg']","['stat.ML', 'cs.DS', 'cs.LG']",False,False,False,False,False,True
