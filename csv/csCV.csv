2017-02-28T17:22:36Z,2017-02-27T18:54:41Z,http://arxiv.org/abs/1702.08434v1,http://arxiv.org/pdf/1702.08434v1,Skin Lesion Classification Using Hybrid Deep Neural Networks,"Skin cancer is one of the major types of cancers and its incidence has been
increasing over the past decades. Skin lesions can arise from various
dermatologic disorders and can be classified to various types according to
their texture structure color and other morphological features. The accuracy
of diagnosis of skin lesions specifically the discrimination of benign and
malignant lesions is paramount to ensure appropriate patient treatment.
Machine learning-based classification approaches are among popular automatic
methods for skin lesion classification. While there are many existing methods
convolutional neural networks (CNN) have shown to be superior over other
classical machine learning methods for object detection and classification
tasks. In this work a fully automatic computerized method is proposed which
employs well established pre-trained convolutional neural networks and
ensembles learning to classify skin lesions. We trained the networks using 2000
skin lesion images available from the ISIC 2017 challenge which has three main
categories and includes 374 melanoma 254 seborrheic keratosis and 1372 benign
nevi images. The trained classifier was then tested on 150 unlabeled images.
The results evaluated by the challenge organizer and based on the area under
the receiver operating characteristic curve (AUC) were 84.8% and 93.6% for
Melanoma and seborrheic keratosis binary classification problem respectively.
  The proposed method achieved competitive results to experienced
dermatologist. Further improvement and optimization of the proposed method with
a larger training dataset could lead to a more precise reliable and robust
method for skin lesion classification.",Amirreza Mahbod|Rupert Ecker|Isabella Ellinger,cs.CV
2017-02-28T17:22:36Z,2017-02-27T18:28:58Z,http://arxiv.org/abs/1702.08423v1,http://arxiv.org/pdf/1702.08423v1,Age Progression/Regression by Conditional Adversarial Autoencoder,"""If I provide you a face image of mine (without telling you the actual age
when I took the picture) and a large amount of face images that I crawled
(containing labeled faces of different ages but not necessarily paired) can
you show me what I would look like when I am 80 or what I was like when I was
5?"" The answer is probably a ""No."" Most existing face aging works attempt to
learn the transformation between age groups and thus would require the paired
samples as well as the labeled query image. In this paper we look at the
problem from a generative modeling perspective such that no paired samples is
required. In addition given an unlabeled image the generative model can
directly produce the image with desired age attribute. We propose a conditional
adversarial autoencoder (CAAE) that learns a face manifold traversing on which
smooth age progression and regression can be realized simultaneously. In CAAE
the face is first mapped to a latent vector through a convolutional encoder
and then the vector is projected to the face manifold conditional on age
through a deconvolutional generator. The latent vector preserves personalized
face features (i.e. personality) and the age condition controls progression
vs. regression. Two adversarial networks are imposed on the encoder and
generator respectively forcing to generate more photo-realistic faces.
Experimental results demonstrate the appealing performance and flexibility of
the proposed framework by comparing with the state-of-the-art and ground truth.",Zhifei Zhang|Yang Song|Hairong Qi,cs.CV
2017-02-28T17:22:36Z,2017-02-27T17:48:17Z,http://arxiv.org/abs/1702.08400v1,http://arxiv.org/pdf/1702.08400v1,Asymmetric Tri-training for Unsupervised Domain Adaptation,"Deep-layered models trained on a large number of labeled samples boost the
accuracy of many tasks. It is important to apply such models to different
domains because collecting many labeled samples in various domains is
expensive. In unsupervised domain adaptation one needs to train a classifier
that works well on a target domain when provided with labeled source samples
and unlabeled target samples. Although many methods aim to match the
distributions of source and target samples simply matching the distribution
cannot ensure accuracy on the target domain. To learn discriminative
representations for the target domain we assume that artificially labeling
target samples can result in a good representation. Tri-training leverages
three classifiers equally to give pseudo-labels to unlabeled samples but the
method does not assume labeling samples generated from a different domain.In
this paper we propose an asymmetric tri-training method for unsupervised
domain adaptation where we assign pseudo-labels to unlabeled samples and train
neural networks as if they are true labels. In our work we use three networks
asymmetrically. By asymmetric we mean that two networks are used to label
unlabeled target samples and one network is trained by the samples to obtain
target-discriminative representations. We evaluate our method on digit
recognition and sentiment analysis datasets. Our proposed method achieves
state-of-the-art performance on the benchmark digit recognition datasets of
domain adaptation.",Kuniaki Saito|Yoshitaka Ushiku|Tatsuya Harada,cs.CV|cs.AI
2017-02-28T17:22:36Z,2017-02-27T17:06:20Z,http://arxiv.org/abs/1702.08379v1,http://arxiv.org/pdf/1702.08379v1,Revealing Hidden Potentials of q-Space Imaging in Breast Cancer,"Mammography screening for early detection of breast lesions currently suffers
from high amounts of false positive findings which result in unnecessary
invasive biopsies. Diffusion-weighted MR images (DWI) can help to reduce many
of these false-positive findings prior to biopsy. Current approaches estimate
tissue properties by means of quantitative parameters taken from generative
biophysical models fit to the q-space encoded signal under certain assumptions
regarding noise and spatial homogeneity. This process is prone to fitting
instability and partial information loss due to model simplicity. We reveal
previously unexplored potentials of the signal by integrating all data
processing components into a convolutional neural network (CNN) architecture
that is designed to propagate clinical target information down to the raw input
images. This approach enables simultaneous and target-specific optimization of
image normalization signal exploitation global representation learning and
classification. Using a multicentric data set of 222 patients we demonstrate
that our approach significantly improves clinical decision making with respect
to the current state of the art.",Paul Jaeger|Sebastian Bickelhaupt|Frederik Bernd Laun|Wolfgang Lederer|Daniel Heidi|Tristan Anselm Kuder|Daniel Paech|David Bonekamp|Alexander Radbruch|Stefan Delorme|Heinz-Peter Schlemmer|Franziska Steudle|Klaus H. Maier-Hein,cs.CV
2017-02-28T17:22:36Z,2017-02-27T15:50:35Z,http://arxiv.org/abs/1702.08336v1,http://arxiv.org/pdf/1702.08336v1,Multi-Label Segmentation via Residual-Driven Adaptive Regularization,"We present a variational multi-label segmentation algorithm based on a robust
Huber loss for both the data and the regularizer minimized within a convex
optimization framework. We introduce a novel constraint on the common areas to
bias the solution towards mutually exclusive regions. We also propose a
regularization scheme that is adapted to the spatial statistics of the residual
at each iteration resulting in a varying degree of regularization being
applied as the algorithm proceeds: the effect of the regularizer is strongest
at initialization and wanes as the solution increasingly fits the data. This
minimizes the bias induced by the regularizer at convergence. We design an
efficient convex optimization algorithm based on the alternating direction
method of multipliers using the equivalent relation between the Huber function
and the proximal operator of the one-norm. We empirically validate our proposed
algorithm on synthetic and real images and offer an information-theoretic
derivation of the cost-function that highlights the modeling choices made.",Byung-Woo Hong|Ja-Keoung Koo|Stefano Soatto,cs.CV
2017-02-28T17:22:36Z,2017-02-27T15:16:47Z,http://arxiv.org/abs/1702.08319v1,http://arxiv.org/pdf/1702.08319v1,Visual Translation Embedding Network for Visual Relation Detection,"Visual relations such as ""person ride bike"" and ""bike next to car"" offer a
comprehensive scene understanding of an image and have already shown their
great utility in connecting computer vision and natural language. However due
to the challenging combinatorial complexity of modeling
subject-predicate-object relation triplets very little work has been done to
localize and predict visual relations. Inspired by the recent advances in
relational representation learning of knowledge bases and convolutional object
detection networks we propose a Visual Translation Embedding network (VTransE)
for visual relation detection. VTransE places objects in a low-dimensional
relation space where a relation can be modeled as a simple vector translation
i.e. subject + predicate $\approx$ object. We propose a novel feature
extraction layer that enables object-relation knowledge transfer in a
fully-convolutional fashion that supports training and inference in a single
forward/backward pass. To the best of our knowledge VTransE is the first
end-to-end relation detection network. We demonstrate the effectiveness of
VTransE over other state-of-the-art methods on two large-scale datasets: Visual
Relationship and Visual Genome. Note that even though VTransE is a purely
visual model it is still competitive to the Lu's multi-modal model with
language priors.",Hanwang Zhang|Zawlin Kyaw|Shih-Fu Chang|Tat-Seng Chua,cs.CV|I.4
2017-02-28T17:22:36Z,2017-02-27T15:16:23Z,http://arxiv.org/abs/1702.08318v1,http://arxiv.org/pdf/1702.08318v1,"Efficient Privacy Preserving Viola-Jones Type Object Detection via
  Random Base Image Representation","A cloud server spent a lot of time energy and money to train a Viola-Jones
type object detector with high accuracy. Clients can upload their photos to the
cloud server to find objects. However the client does not want the leakage of
the content of his/her photos. In the meanwhile the cloud server is also
reluctant to leak any parameters of the trained object detectors. 10 years ago
Avidan & Butman introduced Blind Vision which is a method for securely
evaluating a Viola-Jones type object detector. Blind Vision uses standard
cryptographic tools and is painfully slow to compute taking a couple of hours
to scan a single image. The purpose of this work is to explore an efficient
method that can speed up the process. We propose the Random Base Image (RBI)
Representation. The original image is divided into random base images. Only the
base images are submitted randomly to the cloud server. Thus the content of
the image can not be leaked. In the meanwhile a random vector and the secure
Millionaire protocol are leveraged to protect the parameters of the trained
object detector. The RBI makes the integral-image enable again for the great
acceleration. The experimental results reveal that our method can retain the
detection accuracy of that of the plain vision algorithm and is significantly
faster than the traditional blind vision with only a very low probability of
the information leakage theoretically.",Xin Jin|Peng Yuan|Xiaodong Li|Chenggen Song|Shiming Ge|Geng Zhao|Yingya Chen,cs.CV
2017-02-28T17:22:36Z,2017-02-27T13:23:35Z,http://arxiv.org/abs/1702.08272v1,http://arxiv.org/pdf/1702.08272v1,A Dataset for Developing and Benchmarking Active Vision,"We present a new public dataset with a focus on simulating robotic vision
tasks in everyday indoor environments using real imagery. The dataset includes
20000+ RGB-D images and 50000+ 2D bounding boxes of object instances densely
captured in 9 unique scenes. We train a fast object category detector for
instance detection on our data. Using the dataset we show that although
increasingly accurate and fast the state of the art for object detection is
still severely impacted by object scale occlusion and viewing direction all
of which matter for robotics applications. We next validate the dataset for
simulating active vision and use the dataset to develop and evaluate a
deep-network-based system for next best move prediction for object
classification using reinforcement learning. Our dataset is available for
download at cs.unc.edu/~ammirato/active_vision_dataset_website/.",Phil Ammirato|Patrick Poirson|Eunbyung Park|Jana Kosecka|Alexander C. Berg,cs.CV
2017-02-28T17:22:36Z,2017-02-27T12:54:54Z,http://arxiv.org/abs/1702.08259v1,http://arxiv.org/pdf/1702.08259v1,"Fast and Accurate Inference with Adaptive Ensemble Prediction in Image
  Classification with Deep Neural Networks","Ensembling multiple predictions is a widely used technique to improve the
accuracy of various machine learning tasks. In image classification tasks for
example averaging the predictions for multiple patches extracted from the
input image significantly improves accuracy. Using multiple networks trained
independently to make predictions improves accuracy further. One obvious
drawback of the ensembling technique is its higher execution cost during
inference. If we average 100 predictions the execution cost will be 100 times
as high as the cost without the ensemble. This higher cost limits the
real-world use of ensembling even though using it is almost the norm to win
image classification competitions. In this paper we describe a new technique
called adaptive ensemble prediction which achieves the benefits of ensembling
with much smaller additional execution costs. Our observation behind this
technique is that many easy-to-predict inputs do not require ensembling. Hence
we calculate the confidence level of the prediction for each input on the basis
of the probability of the predicted label i.e. the outputs from the softmax
during the ensembling computation. If the prediction for an input reaches a
high enough probability on the basis of the confidence level we stop
ensembling for this input to avoid wasting computation power. We evaluated the
adaptive ensembling by using various datasets and showed that it reduces the
computation time significantly while achieving similar accuracy to the naive
ensembling.",Hiroshi Inoue,cs.LG|cs.CV|stat.ML
2017-02-28T17:22:36Z,2017-02-27T11:10:54Z,http://arxiv.org/abs/1702.08231v1,http://arxiv.org/pdf/1702.08231v1,Low-Precision Batch-Normalized Activations,"Artificial neural networks can be trained with relatively low-precision
floating-point and fixed-point arithmetic using between one and 16 bits.
Previous works have focused on relatively wide-but-shallow feed-forward
networks. We introduce a quantization scheme that is compatible with training
very deep neural networks. Quantizing the network activations in the middle of
each batch-normalization module can greatly reduce the amount of memory and
computational power needed with little loss in accuracy.",Benjamin Graham,cs.NE|cs.CV
2017-02-28T17:22:40Z,2017-02-27T10:02:40Z,http://arxiv.org/abs/1702.08212v1,http://arxiv.org/pdf/1702.08212v1,"Anticipating many futures: Online human motion prediction and synthesis
  for human-robot collaboration","Fluent and safe interactions of humans and robots require both partners to
anticipate the others' actions. A common approach to human intention inference
is to model specific trajectories towards known goals with supervised
classifiers. However these approaches do not take possible future movements
into account nor do they make use of kinematic cues such as legible and
predictable motion. The bottleneck of these methods is the lack of an accurate
model of general human motion. In this work we present a conditional
variational autoencoder that is trained to predict a window of future human
motion given a window of past frames. Using skeletal data obtained from RGB
depth images we show how this unsupervised approach can be used for online
motion prediction for up to 1660 ms. Additionally we demonstrate online target
prediction within the first 300-500 ms after motion onset without the use of
target specific training data. The advantage of our probabilistic approach is
the possibility to draw samples of possible future motions. Finally we
investigate how movements and kinematic cues are represented on the learned low
dimensional manifold.",Judith Bütepage|Hedvig Kjellström|Danica Kragic,cs.RO|cs.CV|cs.HC
2017-02-28T17:22:40Z,2017-02-27T08:53:31Z,http://arxiv.org/abs/1702.08192v1,http://arxiv.org/abs/1702.08192v1,DeepNAT: Deep Convolutional Neural Network for Segmenting Neuroanatomy,"We introduce DeepNAT a 3D Deep convolutional neural network for the
automatic segmentation of NeuroAnaTomy in T1-weighted magnetic resonance
images. DeepNAT is an end-to-end learning-based approach to brain segmentation
that jointly learns an abstract feature representation and a multi-class
classification. We propose a 3D patch-based approach where we do not only
predict the center voxel of the patch but also neighbors which is formulated
as multi-task learning. To address a class imbalance problem we arrange two
networks hierarchically where the first one separates foreground from
background and the second one identifies 25 brain structures on the
foreground. Since patches lack spatial context we augment them with
coordinates. To this end we introduce a novel intrinsic parameterization of
the brain volume formed by eigenfunctions of the Laplace-Beltrami operator. As
network architecture we use three convolutional layers with pooling batch
normalization and non-linearities followed by fully connected layers with
dropout. The final segmentation is inferred from the probabilistic output of
the network with a 3D fully connected conditional random field which ensures
label agreement between close voxels. The roughly 2.7 million parameters in the
network are learned with stochastic gradient descent. Our results show that
DeepNAT compares favorably to state-of-the-art methods. Finally the purely
learning-based method may have a high potential for the adaptation to young
old or diseased brains by fine-tuning the pre-trained network with a small
training sample on the target application where the availability of larger
datasets with manual annotations may boost the overall segmentation accuracy in
the future.",Christian Wachinger|Martin Reuter|Tassilo Klein,cs.CV|cs.AI|cs.LG
2017-02-28T17:22:40Z,2017-02-27T06:40:22Z,http://arxiv.org/abs/1702.08160v1,http://arxiv.org/pdf/1702.08160v1,"HashBox: Hash Hierarchical Segmentation exploiting Bounding Box Object
  Detection","We propose a novel approach to address the Simultaneous Detection and
Segmentation problem. Using hierarchical structures we use an efficient and
accurate procedure that exploits the hierarchy feature information using
Locality Sensitive Hashing. We build on recent work that utilizes convolutional
neural networks to detect bounding boxes in an image (Faster R-CNN) and then
use the top similar hierarchical region that best fits each bounding box after
hashing we call this approach HashBox. We then refine our final segmentation
results by automatic hierarchy pruning. HashBox introduces a train-free
alternative to Hypercolumns. We conduct extensive experiments on Pascal VOC
2012 segmentation dataset showing that HashBox gives competitive
state-of-the-art object segmentations.",Joachim Curto|Irene Zarza|Alexander J. Smola|Luc Van Gool,cs.CV
2017-02-28T17:22:40Z,2017-02-27T06:04:52Z,http://arxiv.org/abs/1702.08155v1,http://arxiv.org/pdf/1702.08155v1,"Multi-scale Image Fusion Between Pre-operative Clinical CT and X-ray
  Microtomography of Lung Pathology","Computational anatomy allows the quantitative analysis of organs in medical
images. However most analysis is constrained to the millimeter scale because
of the limited resolution of clinical computed tomography (CT). X-ray
microtomography ($\mu$CT) on the other hand allows imaging of ex-vivo tissues
at a resolution of tens of microns. In this work we use clinical CT to image
lung cancer patients before partial pneumonectomy (resection of pathological
lung tissue). The resected specimen is prepared for $\mu$CT imaging at a voxel
resolution of 50 $\mu$m (0.05 mm). This high-resolution image of the lung
cancer tissue allows further insides into understanding of tumor growth and
categorization. For making full use of this additional information image
fusion (registration) needs to be performed in order to re-align the $\mu$CT
image with clinical CT. We developed a multi-scale non-rigid registration
approach. After manual initialization using a few landmark points and rigid
alignment several levels of non-rigid registration between down-sampled (in
the case of $\mu$CT) and up-sampled (in the case of clinical CT)
representations of the image are performed. Any non-lung tissue is ignored
during the computation of the similarity measure used to guide the registration
during optimization. We are able to recover the volume differences introduced
by the resection and preparation of the lung specimen. The average ($\pm$ std.
dev.) minimum surface distance between $\mu$CT and clinical CT at the resected
lung surface is reduced from 3.3 $\pm$ 2.9 (range: [0.1 15.9]) to 2.3 mm $\pm$
2.8 (range: [0.0 15.3]) mm. The alignment of clinical CT with $\mu$CT will
allow further registration with even finer resolutions of $\mu$CT (up to 10
$\mu$m resolution) and ultimately with histopathological microscopy images for
further macro to micro image fusion that can aid medical image analysis.",Holger R. Roth|Kai Nagara|Hirohisa Oda|Masahiro Oda|Tomoshi Sugiyama|Shota Nakamura|Kensaku Mori,cs.CV
2017-02-28T17:22:40Z,2017-02-27T00:38:33Z,http://arxiv.org/abs/1702.08115v1,http://arxiv.org/pdf/1702.08115v1,"Bioplausible multiscale filtering in retino-cortical processing as a
  mechanism in perceptual grouping","Why does our visual system fail to reconstruct reality when we look at
certain patterns? Where do Geometrical illusions start to emerge in the visual
pathway? Should computational models of vision have the same visual ability to
detect illusions as we do? This study addresses these questions by focusing on
a specific underlying neural mechanism involved in our visual experiences that
affects our final perception. Among many types of visual illusion
'Geometrical' and in particular 'Tilt' illusions are rather important being
characterized by misperception of geometric patterns involving lines and tiles
in combination with contrasting orientation size or position. Over the last
decade many new neurophysiological experiments have led to new insights as to
how when and where retinal processing takes place and the encoding nature of
the retinal representation that is sent to the cortex for further processing.
Based on these neurobiological discoveries we provide computer simulation
evidence to suggest that visual Geometrical illusions are explained in part by
the interaction of multiscale visual processing performed in the retina. The
output of our retinal stage model is presented for several types of Tilt
illusion in which the final tilt percept arises from multiple scale processing
of Differences of Gaussian and the perceptual interaction of foreground and
background elements. Our results suggest that this multilevel filtering
explanation which is a simplified simulation for Retinal Ganglion Cell's
responses to these patterns is indeed the underlying mechanism connecting
low-level filtering to mid- and high-level explanations such as 'anchoring
theory' and 'perceptual grouping'.",Nasim Nematzadeh|David M. W. Powers|Trent W. Lewis,cs.CV
2017-02-28T17:22:40Z,2017-02-26T23:52:00Z,http://arxiv.org/abs/1702.08112v1,http://arxiv.org/pdf/1702.08112v1,3D Scanning System for Automatic High-Resolution Plant Phenotyping,"Thin leaves fine stems self-occlusion non-rigid and slowly changing
structures make plants difficult for three-dimensional (3D) scanning and
reconstruction -- two critical steps in automated visual phenotyping. Many
current solutions such as laser scanning structured light and multiview
stereo can struggle to acquire usable 3D models because of limitations in
scanning resolution and calibration accuracy. In response we have developed a
fast low-cost 3D scanning platform to image plants on a rotating stage with
two tilting DSLR cameras centred on the plant. This uses new methods of camera
calibration and background removal to achieve high-accuracy 3D reconstruction.
We assessed the system's accuracy using a 3D visual hull reconstruction
algorithm applied on 2 plastic models of dicotyledonous plants 2 sorghum
plants and 2 wheat plants across different sets of tilt angles. Scan times
ranged from 3 minutes (to capture 72 images using 2 tilt angles) to 30 minutes
(to capture 360 images using 10 tilt angles). The leaf lengths widths areas
and perimeters of the plastic models were measured manually and compared to
measurements from the scanning system: results were within 3-4% of each other.
The 3D reconstructions obtained with the scanning system show excellent
geometric agreement with all six plant specimens even plants with thin leaves
and fine stems.",Chuong V Nguyen|Jurgen Fripp|David R Lovell|Robert Furbank|Peter Kuffner|Helen Daily|Xavier Sirault,cs.CV
2017-02-28T17:22:40Z,2017-02-26T10:08:49Z,http://arxiv.org/abs/1702.08014v1,http://arxiv.org/pdf/1702.08014v1,Adversarial Networks for the Detection of Aggressive Prostate Cancer,"Semantic segmentation constitutes an integral part of medical image analyses
for which breakthroughs in the field of deep learning were of high relevance.
The large number of trainable parameters of deep neural networks however
renders them inherently data hungry a characteristic that heavily challenges
the medical imaging community. Though interestingly with the de facto standard
training of fully convolutional networks (FCNs) for semantic segmentation being
agnostic towards the `structure' of the predicted label maps valuable
complementary information about the global quality of the segmentation lies
idle. In order to tap into this potential we propose utilizing an adversarial
network which discriminates between expert and generated annotations in order
to train FCNs for semantic segmentation. Because the adversary constitutes a
learned parametrization of what makes a good segmentation at a global level we
hypothesize that the method holds particular advantages for segmentation tasks
on complex structured small datasets. This holds true in our experiments: We
learn to segment aggressive prostate cancer utilizing MRI images of 152
patients and show that the proposed scheme is superior over the de facto
standard in terms of the detection sensitivity and the dice-score for
aggressive prostate cancer. The achieved relative gains are shown to be
particularly pronounced in the small dataset limit.",Simon Kohl|David Bonekamp|Heinz-Peter Schlemmer|Kaneschka Yaqubi|Markus Hohenfellner|Boris Hadaschik|Jan-Philipp Radtke|Klaus Maier-Hein,cs.CV
2017-02-28T17:22:40Z,2017-02-26T09:30:08Z,http://arxiv.org/abs/1702.08009v1,http://arxiv.org/pdf/1702.08009v1,"Analyzing Modular CNN Architectures for Joint Depth Prediction and
  Semantic Segmentation","This paper addresses the task of designing a modular neural network
architecture that jointly solves different tasks. As an example we use the
tasks of depth estimation and semantic segmentation given a single RGB image.
The main focus of this work is to analyze the cross-modality influence between
depth and semantic prediction maps on their joint refinement. While most
previous works solely focus on measuring improvements in accuracy we propose a
way to quantify the cross-modality influence. We show that there is a
relationship between final accuracy and cross-modality influence although not
a simple linear one. Hence a larger cross-modality influence does not
necessarily translate into an improved accuracy. We find that a beneficial
balance between the cross-modality influences can be achieved by network
architecture and conjecture that this relationship can be utilized to
understand different network design choices. Towards this end we propose a
Convolutional Neural Network (CNN) architecture that fuses the state of the
state-of-the-art results for depth estimation and semantic labeling. By
balancing the cross-modality influences between depth and semantic prediction
we achieve improved results for both tasks using the NYU-Depth v2 benchmark.",Omid Hosseini Jafari|Oliver Groth|Alexander Kirillov|Michael Ying Yang|Carsten Rother,cs.CV|cs.RO
2017-02-28T17:22:40Z,2017-02-26T09:10:45Z,http://arxiv.org/abs/1702.08007v1,http://arxiv.org/pdf/1702.08007v1,Bayesian Nonparametric Unmixing of Hyperspectral Images,"Hyperspectral imaging is an important tool in remote sensing allowing for
accurate analysis of vast areas. Due to a low spatial resolution a pixel of a
hyperspectral image rarely represents a single material but rather a mixture
of different spectra. HSU aims at estimating the pure spectra present in the
scene of interest referred to as endmembers and their fractions in each
pixel referred to as abundances. Today many HSU algorithms have been
proposed based either on a geometrical or statistical model. While most
methods assume that the number of endmembers present in the scene is known
there is only little work about estimating this number from the observed data.
In this work we propose a Bayesian nonparametric framework that jointly
estimates the number of endmembers the endmembers itself and their
abundances by making use of the Indian Buffet Process as a prior for the
endmembers. Simulation results and experiments on real data demonstrate the
effectiveness of the proposed algorithm yielding results comparable with
state-of-the-art methods while being able to reliably infer the number of
endmembers. In scenarios with strong noise where other algorithms provide only
poor results the proposed approach tends to overestimate the number of
endmembers slightly. The additional endmembers however often simply represent
noisy replicas of present endmembers and could easily be merged in a
post-processing step.",Jürgen Hahn|Abdelhak M. Zoubir,cs.CV
2017-02-28T17:22:40Z,2017-02-26T08:34:26Z,http://arxiv.org/abs/1702.08001v1,http://arxiv.org/pdf/1702.08001v1,Bayesian Nonparametric Feature and Policy Learning for Decision-Making,"Learning from demonstrations has gained increasing interest in the recent
past enabling an agent to learn how to make decisions by observing an
experienced teacher. While many approaches have been proposed to solve this
problem there is only little work that focuses on reasoning about the observed
behavior. We assume that in many practical problems an agent makes its
decision based on latent features indicating a certain action. Therefore we
propose a generative model for the states and actions. Inference reveals the
number of features the features and the policies allowing us to learn and to
analyze the underlying structure of the observed behavior. Further our
approach enables prediction of actions for new states. Simulations are used to
assess the performance of the algorithm based upon this model. Moreover the
problem of learning a driver's behavior is investigated demonstrating the
performance of the proposed model in a real-world scenario.",Jürgen Hahn|Abdelhak M. Zoubir,cs.LG|cs.CV
2017-02-28T17:22:43Z,2017-02-26T04:23:36Z,http://arxiv.org/abs/1702.07985v1,http://arxiv.org/pdf/1702.07985v1,"A multi-task convolutional neural network for mega-city analysis using
  very high resolution satellite imagery and geospatial data","Mega-city analysis with very high resolution (VHR) satellite images has been
drawing increasing interest in the fields of city planning and social
investigation. It is known that accurate land-use urban density and
population distribution information is the key to mega-city monitoring and
environmental studies. Therefore how to generate land-use urban density and
population distribution maps at a fine scale using VHR satellite images has
become a hot topic. Previous studies have focused solely on individual tasks
with elaborate hand-crafted features and have ignored the relationship between
different tasks. In this study we aim to propose a universal framework which
can: 1) automatically learn the internal feature representation from the raw
image data; and 2) simultaneously produce fine-scale land-use urban density
and population distribution maps. For the first target a deep convolutional
neural network (CNN) is applied to learn the hierarchical feature
representation from the raw image data. For the second target a novel
CNN-based universal framework is proposed to process the VHR satellite images
and generate the land-use urban density and population distribution maps. To
the best of our knowledge this is the first CNN-based mega-city analysis
method which can process a VHR remote sensing image with such a large data
volume. A VHR satellite image (1.2 m spatial resolution) of the center of Wuhan
covering an area of 2606 km2 was used to evaluate the proposed method. The
experimental results confirm that the proposed method can achieve a promising
accuracy for land-use urban density and population distribution maps.",Fan Zhang|Bo Du|Liangpei Zhang,cs.CV
2017-02-28T17:22:43Z,2017-02-26T02:13:20Z,http://arxiv.org/abs/1702.07975v1,http://arxiv.org/pdf/1702.07975v1,"Building Fast and Compact Convolutional Neural Networks for Offline
  Handwritten Chinese Character Recognition","Like other problems in computer vision offline handwritten Chinese character
recognition (HCCR) has achieved impressive results using convolutional neural
network (CNN)-based methods. However larger and deeper networks are needed to
deliver state-of-the-art results in this domain. Such networks intuitively
appear to incur high computational cost and require the storage of a large
number of parameters which renders them unfeasible for deployment in portable
devices. To solve this problem we propose a Global Supervised Low-rank
Expansion (GSLRE) method and an Adaptive Drop-weight (ADW) technique to solve
the problems of speed and storage capacity. We design a nine-layer CNN for HCCR
consisting of 3755 classes and devise an algorithm that can reduce the
networks computational cost by nine times and compress the network to 1/18 of
the original size of the baseline model with only a 0.21% drop in accuracy. In
tests the proposed algorithm surpassed the best single-network performance
reported thus far in the literature while requiring only 2.3 MB for storage.
Furthermore when integrated with our effective forward implementation the
recognition of an offline character image took only 9.7 ms on a CPU. Compared
with the state-of-the-art CNN model for HCCR our approach is approximately 30
times faster yet 10 times more cost efficient.",Xuefeng Xiao|Lianwen Jin|Yafeng Yang|Weixin Yang|Jun Sun|Tianhai Chang,cs.CV
2017-02-28T17:22:43Z,2017-02-26T01:56:38Z,http://arxiv.org/abs/1702.07971v1,http://arxiv.org/pdf/1702.07971v1,"Seeing What Is Not There: Learning Context to Determine Where Objects
  Are Missing","Most of computer vision focuses on what is in an image. We propose to train a
standalone object-centric context representation to perform the opposite task:
seeing what is not there. Given an image our context model can predict where
objects should exist even when no object instances are present. Combined with
object detection results we can perform a novel vision task: finding where
objects are missing in an image. Our model is based on a convolutional neural
network structure. With a specially designed training strategy the model
learns to ignore objects and focus on context only. It is fully convolutional
thus highly efficient. Experiments show the effectiveness of the proposed
approach in one important accessibility task: finding city street regions where
curb ramps are missing which could help millions of people with mobility
disabilities.",Jin Sun|David W. Jacobs,cs.CV
2017-02-28T17:22:43Z,2017-02-26T00:56:25Z,http://arxiv.org/abs/1702.07963v1,http://arxiv.org/pdf/1702.07963v1,"Spatially Aware Melanoma Segmentation Using Hybrid Deep Learning
  Techniques","In this paper we proposed using a hybrid method that utilises deep
convolutional and recurrent neural networks for accurate delineation of skin
lesion of images supplied with ISBI 2017 lesion segmentation challenge. The
proposed method was trained using 1800 images and tested on 150 images from
ISBI 2017 challenge.",M. Attia|M. Hossny|S. Nahavandi|A. Yazdabadi,cs.CV
2017-02-28T17:22:43Z,2017-02-26T00:17:42Z,http://arxiv.org/abs/1702.07959v1,http://arxiv.org/pdf/1702.07959v1,"Supervised Learning of Labeled Pointcloud Differences via Cover-Tree
  Entropy Reduction","We introduce a new algorithm called CDER for supervised machine learning
that merges the multi-scale geometric properties of Cover Trees with the
information-theoretic properties of entropy. CDER applies to a training set of
labeled pointclouds embedded in a common Euclidean space. If typical
pointclouds corresponding to distinct labels tend to differ at any scale in any
sub-region CDER can identify these differences in (typically) linear time
creating a set of distributional coordinates which act as a feature extraction
mechanism for supervised learning. We describe theoretical properties and
implementation details of CDER and illustrate its benefits on several
synthetic examples.",Abraham Smith|Paul Bendich|John Harer|Jay Hineman,cs.LG|cs.CV|stat.ML
2017-02-28T17:22:43Z,2017-02-25T19:59:39Z,http://arxiv.org/abs/1702.07942v1,http://arxiv.org/abs/1702.07942v1,BARCHAN: Blob Alignment for Robust CHromatographic ANalysis,"Comprehensive Two dimensional gas chromatography (GCxGC) plays a central role
into the elucidation of complex samples. The automation of the identification
of peak areas is of prime interest to obtain a fast and repeatable analysis of
chromatograms. To determine the concentration of compounds or pseudo-compounds
templates of blobs are defined and superimposed on a reference chromatogram.
The templates then need to be modified when different chromatograms are
recorded. In this study we present a chromatogram and template alignment
method based on peak registration called BARCHAN. Peaks are identified using a
robust mathematical morphology tool. The alignment is performed by a
probabilistic estimation of a rigid transformation along the first dimension
and a non-rigid transformation in the second dimension taking into account
noise outliers and missing peaks in a fully automated way. Resulting aligned
chromatograms and masks are presented on two datasets. The proposed algorithm
proves to be fast and reliable. It significantly reduces the time to results
for GCxGC analysis.",Camille Couprie|Laurent Duval|Maxime Moreaud|Sophie Hénon|Mélinda Tebib|Vincent Souchon,cs.CV|physics.data-an
2017-02-28T17:22:43Z,2017-02-25T18:15:51Z,http://arxiv.org/abs/1702.07935v1,http://arxiv.org/pdf/1702.07935v1,"Image Stitching by Line-guided Local Warping with Global Similarity
  Constraint","Low-textured image stitching remains a challenging problem. It is difficult
to achieve good alignment and is easy to break image structures due to the
insufficient and unreliable point correspondences. Besides for the viewpoint
variations between multiple images the stitched images suffer from projective
distortions. To this end this paper presents a line-guided local warping with
global similarity constraint for image stitching. A two-stage alignment scheme
is adopted for good alignment. More precisely the line correspondence is
employed as alignment constraint to guide the accurate estimation of projective
warp then line feature constraints are integrated into mesh-based warping
framework to refine the alignment while preserving image structures. To
mitigate projectve distortions in non-overlapping regions we combine global
similarity constraint with the projective warps via a weight strategy so that
the final warp slowly changes from projective to similarity across the image.
This is also integrated into local multiple homographies model for better
parallax handling. Our method is evaluated on a series of images and compared
with several other methods. Experiments demonstrate that the proposed method
provides convincing stitching performance and outperforms other
state-of-the-art methods.",Tianzhu Xiang|Gui-Song Xia|Xiang Bai|Liangpei Zhang,cs.CV
2017-02-28T17:22:43Z,2017-02-25T15:48:44Z,http://arxiv.org/abs/1702.07908v1,http://arxiv.org/abs/1702.07908v1,"CHAOS: A Parallelization Scheme for Training Convolutional Neural
  Networks on Intel Xeon Phi","Deep learning is an important component of big-data analytic tools and
intelligent applications such as self-driving cars computer vision speech
recognition or precision medicine. However the training process is
computationally intensive and often requires a large amount of time if
performed sequentially. Modern parallel computing systems provide the
capability to reduce the required training time of deep neural networks. In
this paper we present our parallelization scheme for training convolutional
neural networks (CNN) named Controlled Hogwild with Arbitrary Order of
Synchronization (CHAOS). Major features of CHAOS include the support for thread
and vector parallelism non-instant updates of weight parameters during
back-propagation without a significant delay and implicit synchronization in
arbitrary order. CHAOS is tailored for parallel computing systems that are
accelerated with the Intel Xeon Phi. We evaluate our parallelization approach
empirically using measurement techniques and performance modeling for various
numbers of threads and CNN architectures. Experimental results for the MNIST
dataset of handwritten digits using the total number of threads on the Xeon Phi
show speedups of up to 103x compared to the execution on one thread of the Xeon
Phi 14x compared to the sequential execution on Intel Xeon E5 and 58x
compared to the sequential execution on Intel Core i5.",Andre Viebke|Suejb Memeti|Sabri Pllana|Ajith Abraham,cs.DC|cs.CV|cs.LG
2017-02-28T17:22:43Z,2017-02-25T14:50:43Z,http://arxiv.org/abs/1702.07898v1,http://arxiv.org/pdf/1702.07898v1,Learning Deep NBNN Representations for Robust Place Categorization,"This paper presents an approach for semantic place categorization using data
obtained from RGB cameras. Previous studies on visual place recognition and
classification have shown that by considering features derived from
pre-trained Convolutional Neural Networks (CNNs) in combination with part-based
classification models high recognition accuracy can be achieved even in
presence of occlusions and severe viewpoint changes. Inspired by these works
we propose to exploit local deep representations representing images as set of
regions applying a Na\""{i}ve Bayes Nearest Neighbor (NBNN) model for image
classification. As opposed to previous methods where CNNs are merely used as
feature extractors our approach seamlessly integrates the NBNN model into a
fully-convolutional neural network. Experimental results show that the proposed
algorithm outperforms previous methods based on pre-trained CNN models and
that when employed in challenging robot place recognition tasks it is robust
to occlusions environmental and sensor changes.",Massimiliano Mancini|Samuel Rota Bulò|Elisa Ricci|Barbara Caputo,cs.RO|cs.CV
2017-02-28T17:22:43Z,2017-02-25T12:50:35Z,http://arxiv.org/abs/1702.07884v1,http://arxiv.org/pdf/1702.07884v1,"An EM Based Probabilistic Two-Dimensional CCA with Application to Face
  Recognition","Recently two-dimensional canonical correlation analysis (2DCCA) has been
successfully applied for image feature extraction. The method instead of
concatenating the columns of the images to the one-dimensional vectors
directly works with two-dimensional image matrices. Although 2DCCA works well
in different recognition tasks it lacks a probabilistic interpretation. In
this paper we present a probabilistic framework for 2DCCA called probabilistic
2DCCA (P2DCCA) and an iterative EM based algorithm for optimizing the
parameters. Experimental results on synthetic and real data demonstrate
superior performance in loading factor estimation for P2DCCA compared to 2DCCA.
For real data three subsets of AR face database and also the UMIST face
database confirm the robustness of the proposed algorithm in face recognition
tasks with different illumination conditions facial expressions poses and
occlusions.",Mehran Safayani|Seyed Hashem Ahmadi|Homayun Afrabandpey|Abdolreza Mirzaei,cs.CV|cs.LG|stat.ML
2017-02-28T17:22:47Z,2017-02-25T07:04:25Z,http://arxiv.org/abs/1702.07841v1,http://arxiv.org/pdf/1702.07841v1,"Transfer Learning for Domain Adaptation in MRI: Application in Brain
  Lesion Segmentation","Magnetic Resonance Imaging (MRI) is widely used in routine clinical diagnosis
and treatment. However variations in MRI acquisition protocols result in
different appearances of normal and diseased tissue in the images.
Convolutional neural networks (CNNs) which have shown to be successful in many
medical image analysis tasks are typically sensitive to the variations in
imaging protocols. Therefore in many cases networks trained on data acquired
with one MRI protocol do not perform satisfactorily on data acquired with
different protocols. This limits the use of models trained with large annotated
legacy datasets on a new dataset with a different domain which is often a
recurring situation in clinical settings. In this study we aim to answer the
following central questions regarding domain adaptation in medical image
analysis: Given a fitted legacy model 1) How much data from the new domain is
required for a decent adaptation of the original network?; and 2) What portion
of the pre-trained model parameters should be retrained given a certain number
of the new domain training samples? To address these questions we conducted
extensive experiments in white matter hyperintensity segmentation task. We
trained a CNN on legacy MR images of brain and evaluated the performance of the
domain-adapted network on the same task with images from a different domain. We
then compared the performance of the model to the surrogate scenarios where
either the same trained network is used or a new network is trained from
scratch on the new dataset.The domain-adapted network tuned only by two
training examples achieved a Dice score of 0.63 substantially outperforming a
similar network trained on the same set of examples from scratch.",Mohsen Ghafoorian|Alireza Mehrtash|Tina Kapur|Nico Karssemeijer|Elena Marchiori|Mehran Pesteie|Charles R. G. Guttmann|Frank-Erik de Leeuw|Clare M. Tempany|Bram van Ginneken|Andriy Fedorov|Purang Abolmaesumi|Bram Platel|William M. Wells III,cs.CV
2017-02-28T17:22:47Z,2017-02-25T06:04:42Z,http://arxiv.org/abs/1702.07836v1,http://arxiv.org/pdf/1702.07836v1,Synthesizing Training Data for Object Detection in Indoor Scenes,"Detection of objects in cluttered indoor environments is one of the key
enabling functionalities for service robots. The best performing object
detection approaches in computer vision exploit deep Convolutional Neural
Networks (CNN) to simultaneously detect and categorize the objects of interest
in cluttered scenes. Training of such models typically requires large amounts
of annotated training data which is time consuming and costly to obtain. In
this work we explore the ability of using synthetically generated composite
images for training state of the art object detectors. We superimpose 2D images
of textured object models into images of real environments at variety of
locations and scales. Our experiments evaluate different superimposition
strategies ranging from purely image-based blending all the way to depth and
semantics informed positioning of the object models to real scenes. We
demonstrate the effectiveness of these object detector training strategies on
publicly available datasets of GMU-Kitchens and Washington RGB-D Scenes v2 and
show how object detectors can be trained with limited amounts of annotated real
scenes with objects present. This charts new opportunities for training
detectors for new objects by exploiting existing object model repositories in
either a purely automatic fashion or with only a very small number of
human-annotated examples.",Georgios Georgakis|Arsalan Mousavian|Alexander C. Berg|Jana Kosecka,cs.CV|cs.RO
2017-02-28T17:22:47Z,2017-02-25T00:22:51Z,http://arxiv.org/abs/1702.07811v1,http://arxiv.org/pdf/1702.07811v1,Adaptive Neural Networks for Fast Test-Time Prediction,"We present an approach to adaptively utilize deep neural networks in order to
reduce the evaluation time on new examples without loss of classification
performance. Rather than attempting to redesign or approximate existing
networks we propose two schemes that adaptively utilize networks. First we
pose an adaptive network evaluation scheme where we learn a system to
adaptively choose the components of a deep network to be evaluated for each
example. By allowing examples correctly classified using early layers of the
system to exit we avoid the computational time associated with full evaluation
of the network. Building upon this approach we then learn a network selection
system that adaptively selects the network to be evaluated for each example. We
exploit the fact that many examples can be correctly classified using
relatively efficient networks and that complex computationally costly networks
are only necessary for a small fraction of examples. By avoiding evaluation of
these complex networks for a large fraction of examples computational time can
be dramatically reduced. Empirically these approaches yield dramatic
reductions in computational cost with up to a 2.8x speedup on state-of-the-art
networks from the ImageNet image recognition challenge with minimal (less than
1%) loss of accuracy.",Tolga Bolukbasi|Joseph Wang|Ofer Dekel|Venkatesh Saligrama,cs.LG|cs.CV|cs.NE|stat.ML
2017-02-28T17:22:47Z,2017-02-24T21:30:31Z,http://arxiv.org/abs/1702.07772v1,http://arxiv.org/pdf/1702.07772v1,"Video and Accelerometer-Based Motion Analysis for Automated Surgical
  Skills Assessment","Purpose: Basic surgical skills of suturing and knot tying are an essential
part of medical training. Having an automated system for surgical skills
assessment could help save experts time and improve training efficiency. There
have been some recent attempts at automated surgical skills assessment using
either video analysis or acceleration data. In this paper we present a novel
approach for automated assessment of OSATS based surgical skills and provide an
analysis of different features on multi-modal data (video and accelerometer
data). Methods: We conduct the largest study to the best of our knowledge for
basic surgical skills assessment on a dataset that contained video and
accelerometer data for suturing and knot-tying tasks. We introduce ""entropy
based"" features - Approximate Entropy (ApEn) and Cross-Approximate Entropy
(XApEn) which quantify the amount of predictability and regularity of
fluctuations in time-series data. The proposed features are compared to
existing methods of Sequential Motion Texture (SMT) Discrete Cosine Transform
(DCT) and Discrete Fourier Transform (DFT) for surgical skills assessment.
Results: We report average performance of different features across all
applicable OSATS criteria for suturing and knot tying tasks. Our analysis shows
that the proposed entropy based features out-perform previous state-of-the-art
methods using video data. For accelerometer data our method performs better
for suturing only. We also show that fusion of video and acceleration features
can improve overall performance with the proposed entropy features achieving
highest accuracy. Conclusions: Automated surgical skills assessment can be
achieved with high accuracy using the proposed entropy features. Such a system
can significantly improve the efficiency of surgical training in medical
schools and teaching hospitals.",Aneeq Zia|Yachna Sharma|Vinay Bettadapura|Eric L. Sarin|Irfan Essa,cs.CV
2017-02-28T17:22:47Z,2017-02-24T20:56:26Z,http://arxiv.org/abs/1702.07759v1,http://arxiv.org/pdf/1702.07759v1,Unifying local and non-local signal processing with graph CNNs,"This paper deals with the unification of local and non-local signal
processing on graphs within a single convolutional neural network (CNN)
framework. Building upon recent works on graph CNNs we propose to use
convolutional layers that take as inputs two variables a signal and a graph
allowing the network to adapt to changes in the graph structure. This also
allows us to learn through training the optimal mixing of locality and
non-locality in cases where the graph is built on the input signal itself. We
demonstrate the versatility and the effectiveness of our framework on several
types of signals (greyscale and color images color palettes and speech
signals) and on several applications (style transfer color transfer and
denoising).",Gilles Puy|Srdan Kitic|Patrick Pérez,cs.CV
2017-02-28T17:22:47Z,2017-02-24T17:36:46Z,http://arxiv.org/abs/1702.07679v1,http://arxiv.org/pdf/1702.07679v1,A recommender system to restore images with impulse noise,"We build a collaborative filtering recommender system to restore images with
impulse noise for which the noisy pixels have been previously identified. We
define this recommender system in terms of a new color image representation
using three matrices that depend on the noise-free pixels of the image to
restore and two parameters: $k$ the number of features; and $\lambda$ the
regularization factor. We perform experiments on a well known image database to
test our algorithm and we provide image quality statistics for the results
obtained. We discuss the roles of bias and variance in the performance of our
algorithm as determined by the values of $k$ and $\lambda$ and provide
guidance on how to choose the values of these parameters. Finally we discuss
the possibility of using our collaborative filtering recommender system to
perform image inpainting and super-resolution.",Alfredo Nava-Tudela,cs.CV|stat.ML
2017-02-28T17:22:47Z,2017-02-24T17:09:22Z,http://arxiv.org/abs/1702.07664v1,http://arxiv.org/pdf/1702.07664v1,How ConvNets model Non-linear Transformations,"In this paper we theoretically address three fundamental problems involving
deep convolutional networks regarding invariance depth and hierarchy. We
introduce the paradigm of Transformation Networks (TN) which are a direct
generalization of Convolutional Networks (ConvNets). Theoretically we show
that TNs (and thereby ConvNets) are can be invariant to non-linear
transformations of the input despite pooling over mere local translations. Our
analysis provides clear insights into the increase in invariance with depth in
these networks. Deeper networks are able to model much richer classes of
transformations. We also find that a hierarchical architecture allows the
network to generate invariance much more efficiently than a non-hierarchical
network. Our results provide useful insight into these three fundamental
problems in deep learning using ConvNets.",Dipan K. Pal|Marios Savvides,cs.CV|cs.LG
2017-02-28T17:22:47Z,2017-02-24T15:43:10Z,http://arxiv.org/abs/1702.07630v1,http://arxiv.org/pdf/1702.07630v1,"Inertia-Constrained Pixel-by-Pixel Nonnegative Matrix Factorisation: a
  Hyperspectral Unmixing Method Dealing with Intra-class Variability","Blind source separation is a common processing tool to analyse the
constitution of pixels of hyperspectral images. Such methods usually suppose
that pure pixel spectra (endmembers) are the same in all the image for each
class of materials. In the framework of remote sensing such an assumption is
no more valid in the presence of intra-class variabilities due to illumination
conditions weathering slight variations of the pure materials etc... In this
paper we first describe the results of investigations highlighting intra-class
variability measured in real images. Considering these results a new
formulation of the linear mixing model is presented leading to two new methods.
Unconstrained Pixel-by-pixel NMF (UP-NMF) is a new blind source separation
method based on the assumption of a linear mixing model which can deal with
intra-class variability. To overcome UP-NMF limitations an extended method is
proposed named Inertia-constrained Pixel-by-pixel NMF (IP-NMF). For each
sensed spectrum these extended versions of NMF extract a corresponding set of
source spectra. A constraint is set to limit the spreading of each source's
estimates in IP-NMF. The methods are tested on a semi-synthetic data set built
with spectra extracted from a real hyperspectral image and then numerically
mixed. We thus demonstrate the interest of our methods for realistic source
variabilities. Finally IP-NMF is tested on a real data set and it is shown to
yield better performance than state of the art methods.",Charlotte Revel|Yannick Deville|Véronique Achard|Xavier Briottet,stat.ME|cs.CV|physics.data-an|stat.ML
2017-02-28T17:22:47Z,2017-02-24T15:01:22Z,http://arxiv.org/abs/1702.07619v1,http://arxiv.org/pdf/1702.07619v1,Fast and robust curve skeletonization for real-world elongated objects,"We consider the problem of extracting curve skeletons of three-dimensional
elongated objects given a noisy surface which has applications in agricultural
contexts such as extracting the branching structure of plants. We describe an
efficient and robust method based on breadth-first search that can determine
curve skeletons in these contexts. Our approach is capable of automatically
detecting junction points as well as spurious segments and loops. All of that
is accomplished with only one user-adjustable parameter. The run time of our
method ranges from hundreds of milliseconds to less than four seconds on large
challenging datasets which makes it appropriate for situations where real-time
decision making is needed. Experiments on synthetic models as well as on data
from real world objects some of which were collected in challenging field
conditions show that our approach compares favorably to classical thinning
algorithms as well as to recent contributions to the field.",Amy Tabb|Henry Medeiros,cs.CV|cs.GR
2017-02-28T17:22:47Z,2017-02-24T14:46:55Z,http://arxiv.org/abs/1702.07611v1,http://arxiv.org/pdf/1702.07611v1,Automatic segmentation in dynamic outdoor environments,"Segmentation in dynamic outdoor environments can be difficult when the
illumination levels and other aspects of the scene cannot be controlled. In
this paper we describe a method that uses superpixels to determine low texture
regions of the image that correspond to the background material and then show
how this information can be integrated with the color distribution of the image
to compute optimal segmentation parameters for traditional binary segmentation
as well as to produce silhouette probability maps. We show results of this
algorithm in the context of an application for tree modeling.",Amy Tabb|Henry Medeiros,cs.CV
2017-02-28T17:22:51Z,2017-02-24T14:29:35Z,http://arxiv.org/abs/1702.07600v1,http://arxiv.org/pdf/1702.07600v1,"How hard is it to cross the room? -- Training (Recurrent) Neural
  Networks to steer a UAV","This work explores the feasibility of steering a drone with a (recurrent)
neural network based on input from a forward looking camera in the context of
a high-level navigation task. We set up a generic framework for training a
network to perform navigation tasks based on imitation learning. It can be
applied to both aerial and land vehicles. As a proof of concept we apply it to
a UAV (Unmanned Aerial Vehicle) in a simulated environment learning to cross a
room containing a number of obstacles. So far only feedforward neural networks
(FNNs) have been used to train UAV control. To cope with more complex tasks we
propose the use of recurrent neural networks (RNN) instead and successfully
train an LSTM (Long-Short Term Memory) network for controlling UAVs. Vision
based control is a sequential prediction problem known for its highly
correlated input data. The correlation makes training a network hard
especially an RNN. To overcome this issue we investigate an alternative
sampling method during training namely window-wise truncated backpropagation
through time (WW-TBPTT). Further end-to-end training requires a lot of data
which often is not available. Therefore we compare the performance of
retraining only the Fully Connected (FC) and LSTM control layers with networks
which are trained end-to-end. Performing the relatively simple task of crossing
a room already reveals important guidelines and good practices for training
neural control networks. Different visualizations help to explain the behavior
learned.",Klaas Kelchtermans|Tinne Tuytelaars,cs.CV
2017-02-28T17:22:51Z,2017-02-24T09:26:15Z,http://arxiv.org/abs/1702.07508v1,http://arxiv.org/abs/1702.07508v1,"Toward high-performance online HCCR: a CNN approach with DropDistortion
  path signature and spatial stochastic max-pooling","This paper presents an investigation of several techniques that increase the
accuracy of online handwritten Chinese character recognition (HCCR). We propose
a new training strategy named DropDistortion to train a deep convolutional
neural network (DCNN) with distorted samples. DropDistortion gradually lowers
the degree of character distortion during training which allows the DCNN to
better generalize. Path signature is used to extract effective features for
online characters. Further improvement is achieved by employing spatial
stochastic max-pooling as a method of feature map distortion and model
averaging. Experiments were carried out on three publicly available datasets
namely CASIA-OLHWDB 1.0 CASIA-OLHWDB 1.1 and the ICDAR2013 online HCCR
competition dataset. The proposed techniques yield state-of-the-art recognition
accuracies of 97.67% 97.30% and 97.99% respectively.",Songxuan Lai|Lianwen Jin|Weixin Yang,cs.CV
2017-02-28T17:22:51Z,2017-02-24T08:30:43Z,http://arxiv.org/abs/1702.07492v1,http://arxiv.org/pdf/1702.07492v1,"Robot gains Social Intelligence through Multimodal Deep Reinforcement
  Learning","For robots to coexist with humans in a social world like ours it is crucial
that they possess human-like social interaction skills. Programming a robot to
possess such skills is a challenging task. In this paper we propose a
Multimodal Deep Q-Network (MDQN) to enable a robot to learn human-like
interaction skills through a trial and error method. This paper aims to develop
a robot that gathers data during its interaction with a human and learns human
interaction behaviour from the high-dimensional sensory information using
end-to-end reinforcement learning. This paper demonstrates that the robot was
able to learn basic interaction skills successfully after 14 days of
interacting with people.",Ahmed Hussain Qureshi|Yutaka Nakamura|Yuichiro Yoshikawa|Hiroshi Ishiguro,cs.RO|cs.AI|cs.CV|stat.ML
2017-02-28T17:22:51Z,2017-02-24T07:51:29Z,http://arxiv.org/abs/1702.07486v1,http://arxiv.org/pdf/1702.07486v1,"Deep representation learning for human motion prediction and
  classification","Generative models of 3D human motion are often restricted to a small number
of activities and can therefore not generalize well to novel movements or
applications. In this work we propose a deep learning framework for human
motion capture data that learns a generic representation from a large corpus of
motion capture data and generalizes well to new unseen motions. Using an
encoding-decoding network that learns to predict future 3D poses from the most
recent past we extract a feature representation of human motion. Most work on
deep learning for sequence prediction focuses on video and speech. Since
skeletal data has a different structure we present and evaluate different
network architectures that make different assumptions about time dependencies
and limb correlations. To quantify the learned features we use the output of
different layers for action classification and visualize the receptive fields
of the network units. Our method outperforms the recent state of the art in
skeletal motion prediction even though these use action specific training data.
Our results show that deep feedforward networks trained from a generic mocap
database can successfully be used for feature extraction from human motion
data and that this representation can be used as a foundation for
classification and prediction.",Judith Bütepage|Michael Black|Danica Kragic|Hedvig Kjellström,cs.CV
2017-02-28T17:22:51Z,2017-02-24T07:34:31Z,http://arxiv.org/abs/1702.07482v1,http://arxiv.org/abs/1702.07482v1,Speckle Reduction with Trained Nonlinear Diffusion Filtering,"Speckle reduction is a prerequisite for many image processing tasks in
synthetic aperture radar (SAR) images as well as all coherent images. In
recent years predominant state-of-the-art approaches for despeckling are
usually based on nonlocal methods which mainly concentrate on achieving utmost
image restoration quality with relatively low computational efficiency.
Therefore in this study we aim to propose an efficient despeckling model with
both high computational efficiency and high recovery quality. To this end we
exploit a newly-developed trainable nonlinear reaction diffusion(TNRD)
framework which has proven a simple and effective model for various image
restoration problems. {In the original TNRD applications the diffusion network
is usually derived based on the direct gradient descent scheme. However this
approach will encounter some problem for the task of multiplicative noise
reduction exploited in this study. To solve this problem we employed a new
architecture derived from the proximal gradient descent method.} {Taking into
account the speckle noise statistics the diffusion process for the despeckling
task is derived. We then retrain all the model parameters in the presence of
speckle noise. Finally optimized nonlinear diffusion filtering models are
obtained which are specialized for despeckling with various noise levels.
Experimental results substantiate that the trained filtering models provide
comparable or even better results than state-of-the-art nonlocal approaches.
Meanwhile our proposed model merely contains convolution of linear filters
with an image which offers high level parallelism on GPUs. As a consequence
for images of size $512 \times 512$ our GPU implementation takes less than 0.1
seconds to produce state-of-the-art despeckling performance.}",Wensen Feng|Yunjin Chen,cs.CV
2017-02-28T17:22:51Z,2017-02-24T06:37:06Z,http://arxiv.org/abs/1702.07475v1,http://arxiv.org/pdf/1702.07475v1,"Sequence-based Multimodal Apprenticeship Learning For Robot Perception
  and Decision Making","Apprenticeship learning has recently attracted a wide attention due to its
capability of allowing robots to learn physical tasks directly from
demonstrations provided by human experts. Most previous techniques assumed that
the state space is known a priori or employed simple state representations that
usually suffer from perceptual aliasing. Different from previous research we
propose a novel approach named Sequence-based Multimodal Apprenticeship
Learning (SMAL) which is capable to simultaneously fusing temporal information
and multimodal data and to integrate robot perception with decision making. To
evaluate the SMAL approach experiments are performed using both simulations
and real-world robots in the challenging search and rescue scenarios. The
empirical study has validated that our SMAL approach can effectively learn
plans for robots to make decisions using sequence of multimodal observations.
Experimental results have also showed that SMAL outperforms the baseline
methods using individual images.",Fei Han|Xue Yang|Yu Zhang|Hao Zhang,cs.RO|cs.AI|cs.CV
2017-02-28T17:22:51Z,2017-02-24T06:35:10Z,http://arxiv.org/abs/1702.07474v1,http://arxiv.org/pdf/1702.07474v1,"Simultaneous Feature and Body-Part Learning for Real-Time Robot
  Awareness of Human Behaviors","Robot awareness of human actions is an essential research problem in robotics
with many important real-world applications including human-robot
collaboration and teaming. Over the past few years depth sensors have become a
standard device widely used by intelligent robots for 3D perception which can
also offer human skeletal data in 3D space. Several methods based on skeletal
data were designed to enable robot awareness of human actions with satisfactory
accuracy. However previous methods treated all body parts and features equally
important without the capability to identify discriminative body parts and
features. In this paper we propose a novel simultaneous Feature And Body-part
Learning (FABL) approach that simultaneously identifies discriminative body
parts and features and efficiently integrates all available information
together to enable real-time robot awareness of human behaviors. We formulate
FABL as a regression-like optimization problem with structured
sparsity-inducing norms to model interrelationships of body parts and features.
We also develop an optimization algorithm to solve the formulated problem
which possesses a theoretical guarantee to find the optimal solution. To
evaluate FABL three experiments were performed using public benchmark
datasets including the MSR Action3D and CAD-60 datasets as well as a Baxter
robot in practical assistive living applications. Experimental results show
that our FABL approach obtains a high recognition accuracy with a processing
speed of the order-of-magnitude of 10e4 Hz which makes FABL a promising method
to enable real-time robot awareness of human behaviors in practical robotics
applications.",Fei Han|Xue Yang|Christopher Reardon|Yu Zhang|Hao Zhang,cs.CV|cs.RO
2017-02-28T17:22:51Z,2017-02-24T06:12:55Z,http://arxiv.org/abs/1702.07472v1,http://arxiv.org/pdf/1702.07472v1,Learning Non-local Image Diffusion for Image Denoising,"Image diffusion plays a fundamental role for the task of image denoising.
Recently proposed trainable nonlinear reaction diffusion (TNRD) model defines a
simple but very effective framework for image denoising. However as the TNRD
model is a local model the diffusion behavior of which is purely controlled by
information of local patches it is prone to create artifacts in the homogenous
regions and over-smooth highly textured regions especially in the case of
strong noise levels. Meanwhile it is widely known that the non-local
self-similarity (NSS) prior stands as an effective image prior for image
denoising which has been widely exploited in many non-local methods. In this
work we are highly motivated to embed the NSS prior into the TNRD model to
tackle its weaknesses. In order to preserve the expected property that
end-to-end training is available we exploit the NSS prior by a set of
non-local filters and derive our proposed trainable non-local reaction
diffusion (TNLRD) model for image denoising. Together with the local filters
and influence functions the non-local filters are learned by employing
loss-specific training. The experimental results show that the trained TNLRD
model produces visually plausible recovered images with more textures and less
artifacts compared to its local versions. Moreover the trained TNLRD model
can achieve strongly competitive performance to recent state-of-the-art image
denoising methods in terms of peak signal-to-noise ratio (PSNR) and structural
similarity index (SSIM).",Peng Qiao|Yong Dou|Wensen Feng|Yunjin Chen,cs.CV
2017-02-28T17:22:51Z,2017-02-24T02:37:15Z,http://arxiv.org/abs/1702.07451v1,http://arxiv.org/pdf/1702.07451v1,Viewpoint Adaptation for Rigid Object Detection,"An object detector performs suboptimally when applied to image data taken
from a viewpoint different from the one with which it was trained. In this
paper we present a viewpoint adaptation algorithm that allows a trained
single-view object detector to be adapted to a new distinct viewpoint. We
first illustrate how a feature space transformation can be inferred from a
known homography between the source and target viewpoints. Second we show that
a variety of trained classifiers can be modified to behave as if that
transformation were applied to each testing instance. The proposed algorithm is
evaluated on a person detection task using images from the PETS 2007 and CAVIAR
datasets as well as from a new synthetic multi-view person detection dataset.
It yields substantial performance improvements when adapting single-view person
detectors to new viewpoints and simultaneously reduces computational
complexity. This work has the potential to improve detection performance for
cameras viewing objects from arbitrary viewpoints while simplifying data
collection and feature extraction.",Patrick Wang|Kenneth Morton|Peter Torrione|Leslie Collins,cs.CV
2017-02-28T17:22:51Z,2017-02-24T01:10:53Z,http://arxiv.org/abs/1702.07432v1,http://arxiv.org/pdf/1702.07432v1,Multi-Context Attention for Human Pose Estimation,"In this paper we propose to incorporate convolutional neural networks with a
multi-context attention mechanism into an end-to-end framework for human pose
estimation. We adopt stacked hourglass networks to generate attention maps from
features at multiple resolutions with various semantics. The Conditional Random
Field (CRF) is utilized to model the correlations among neighboring regions in
the attention map. We further combine the holistic attention model which
focuses on the global consistency of the full human body and the body part
attention model which focuses on the detailed description for different body
parts. Hence our model has the ability to focus on different granularity from
local salient regions to global semantic-consistent spaces. Additionally we
design novel Hourglass Residual Units (HRUs) to increase the receptive field of
the network. These units are extensions of residual units with a side branch
incorporating filters with larger receptive fields hence features with various
scales are learned and combined within the HRUs. The effectiveness of the
proposed multi-context attention mechanism and the hourglass residual units is
evaluated on two widely used human pose estimation benchmarks. Our approach
outperforms all existing methods on both benchmarks over all the body parts.",Xiao Chu|Wei Yang|Wanli Ouyang|Cheng Ma|Alan L. Yuille|Xiaogang Wang,cs.CV
2017-02-28T17:22:55Z,2017-02-23T23:39:21Z,http://arxiv.org/abs/1702.07424v1,http://arxiv.org/pdf/1702.07424v1,Building Usage Profiles Using Deep Neural Nets,"To improve software quality one needs to build test scenarios resembling the
usage of a software product in the field. This task is rendered challenging
when a product's customer base is large and diverse. In this scenario existing
profiling approaches such as operational profiling are difficult to apply. In
this work we consider publicly available video tutorials of a product to
profile usage. Our goal is to construct an automatic approach to extract
information about user actions from instructional videos. To achieve this goal
we use a Deep Convolutional Neural Network (DCNN) to recognize user actions.
Our pilot study shows that a DCNN trained to recognize user actions in video
can classify five different actions in a collection of 236 publicly available
Microsoft Word tutorial videos (published on YouTube). In our empirical
evaluation we report a mean average precision of 94.42% across all actions.
This study demonstrates the efficacy of DCNN-based methods for extracting
software usage information from videos. Moreover this approach may aid in
other software engineering activities that require information about customer
usage of a product.",Domenic Curro|Konstantinos G. Derpanis|Andriy V. Miranskyy,cs.SE|cs.CV
2017-02-28T17:22:55Z,2017-02-23T21:09:46Z,http://arxiv.org/abs/1702.07343v1,http://arxiv.org/pdf/1702.07343v1,Improving high-pass fusion method using wavelets,"In an appropriate image fusion method spatial information of the
panchromatic image is injected into the multispectral images such that the
spectral information is not distorted. The high-pass modulation method is a
successful method in image fusion. However the main drawback of this method is
that this technique uses the boxcar filter to extract the high frequency
information of the panchromatic image. Using the boxcar filter introduces the
ringing effect into the fused image. To cope with this problem we use the
wavelet transform instead of boxcar filters. Then the results of the proposed
method and those of other methods such as Brovey IHS and PCA ones are
compared. Experiments show the superiority of the proposed method in terms of
correlation coefficient and mutual information.",Hamid Reza Shahdoosti,cs.CV
2017-02-28T17:22:55Z,2017-02-23T21:06:51Z,http://arxiv.org/abs/1702.07392v1,http://arxiv.org/pdf/1702.07392v1,"WaterGAN: Unsupervised Generative Network to Enable Real-time Color
  Correction of Monocular Underwater Images","This paper reports on WaterGAN a generative adversarial network (GAN) for
generating realistic underwater images from in-air image and depth pairings in
an unsupervised pipeline used for color correction of monocular underwater
images. Cameras onboard autonomous and remotely operated vehicles can capture
high resolution images to map the seafloor however underwater image formation
is subject to the complex process of light propagation through the water
column. The raw images retrieved are characteristically different than images
taken in air due to effects such as absorption and scattering which cause
attenuation of light at different rates for different wavelengths. While this
physical process is well described theoretically the model depends on many
parameters intrinsic to the water column as well as the objects in the scene.
These factors make recovery of these parameters difficult without simplifying
assumptions or field calibration hence restoration of underwater images is a
non-trivial problem. Deep learning has demonstrated great success in modeling
complex nonlinear systems but requires a large amount of training data which
is difficult to compile in deep sea environments. Using WaterGAN we generate a
large training dataset of paired imagery both raw underwater and true color
in-air as well as depth data. This data serves as input to a novel end-to-end
network for color correction of monocular underwater images. Due to the
depth-dependent water column effects inherent to underwater environments we
show that our end-to-end network implicitly learns a coarse depth estimate of
the underwater scene from monocular underwater images. Our proposed pipeline is
validated with testing on real data collected from both a pure water tank and
from underwater surveys in field testing. Source code is made publicly
available with sample datasets and pretrained models.",Jie Li|Katherine A. Skinner|Ryan M. Eustice|Matthew Johnson-Roberson,cs.CV|cs.RO
2017-02-28T17:22:55Z,2017-02-23T20:56:09Z,http://arxiv.org/abs/1702.07389v1,http://arxiv.org/pdf/1702.07389v1,Continuous-Time Visual-Inertial Trajectory Estimation with Event Cameras,"In contrast to traditional cameras which output images at a fixed rate
event cameras have independent pixels that output asynchronous pixel-level
brightness changes with microsecond resolution. In this paper we leverage a
continuous-time framework to perform trajectory estimation by fusing visual
data from a moving event camera with inertial data from an IMU. This framework
allows direct integration of the asynchronous events with micro-second accuracy
and the inertial measurements at high frequency. The pose trajectory is
approximated by a smooth curve in the space of rigid-body motions using cubic
splines. This formulation significantly reduces the number of variables in
trajectory estimation problems. We evaluate our method on real data from
several scenes and compare the results against ground truth from a
motion-capture system. We show superior performance of the proposed technique
compared to non-batch event-based algorithms. We also show that both the map
orientation and scale can be recovered accurately by fusing events and inertial
data. To the best of our knowledge this is the first work on visual-inertial
fusion with event cameras using a continuous-time framework.",Elias Mueggler|Guillermo Gallego|Henri Rebecq|Davide Scaramuzza,cs.RO|cs.CV
2017-02-28T17:22:55Z,2017-02-23T20:48:13Z,http://arxiv.org/abs/1702.07386v1,http://arxiv.org/pdf/1702.07386v1,Toward Streaming Synapse Detection with Compositional ConvNets,"Connectomics is an emerging field in neuroscience that aims to reconstruct
the 3-dimensional morphology of neurons from electron microscopy (EM) images.
Recent studies have successfully demonstrated the use of convolutional neural
networks (ConvNets) for segmenting cell membranes to individuate neurons.
However there has been comparatively little success in high-throughput
identification of the intercellular synaptic connections required for deriving
connectivity graphs.
  In this study we take a compositional approach to segmenting synapses
modeling them explicitly as an intercellular cleft co-located with an
asymmetric vesicle density along a cell membrane. Instead of requiring a deep
network to learn all natural combinations of this compositionality we train
lighter networks to model the simpler marginal distributions of membranes
clefts and vesicles from just 100 electron microscopy samples. These feature
maps are then combined with simple rules-based heuristics derived from prior
biological knowledge.
  Our approach to synapse detection is both more accurate than previous
state-of-the-art (7% higher recall and 5% higher F1-score) and yields a 20-fold
speed-up compared to the previous fastest implementations. We demonstrate by
reconstructing the first complete directed connectome from the largest
available anisotropic microscopy dataset (245 GB) of mouse somatosensory cortex
(S1) in just 9.7 hours on a single shared-memory CPU system. We believe that
this work marks an important step toward the goal of a microscope-pace
streaming connectomics pipeline.",Shibani Santurkar|David Budden|Alexander Matveev|Heather Berlin|Hayk Saribekyan|Yaron Meirovitch|Nir Shavit,cs.CV
2017-02-28T17:22:55Z,2017-02-23T19:34:25Z,http://arxiv.org/abs/1702.07371v1,http://arxiv.org/pdf/1702.07371v1,"Feasibility of Principal Component Analysis in hand gesture recognition
  system","Nowadays actions are increasingly being handled in electronic ways instead
of physical interaction. From earlier times biometrics is used in the
authentication of a person. It recognizes a person by using a human trait
associated with it like eyes (by calculating the distance between the eyes) and
using hand gestures fingerprint detection face detection etc. Advantages of
using these traits for identification are that they uniquely identify a person
and cannot be forgotten or lost. These are unique features of a human being
which are being used widely to make the human life simpler. Hand gesture
recognition system is a powerful tool that supports efficient interaction
between the user and the computer. The main moto of hand gesture recognition
research is to create a system which can recognise specific hand gestures and
use them to convey useful information for device control. This paper presents
an experimental study over the feasibility of principal component analysis in
hand gesture recognition system. PCA is a powerful tool for analyzing data. The
primary goal of PCA is dimensionality reduction. Frames are extracted from the
Sheffield KInect Gesture (SKIG) dataset. The implementation is done by creating
a training set and then training the recognizer. It uses Eigen space by
processing the eigenvalues and eigenvectors of the images in training set.
Euclidean distance with the threshold value is used as similarity metric to
recognize the gestures. The experimental results show that PCA is feasible to
be used for hand gesture recognition system.",Tanu Srivastava|Raj Shree Singh|Sunil Kumar|Pavan Chakraborty,cs.CV
2017-02-28T17:22:55Z,2017-02-23T18:43:30Z,http://arxiv.org/abs/1702.07333v1,http://arxiv.org/pdf/1702.07333v1,"k-Means Clustering and Ensemble of Regressions: An Algorithm for the
  ISIC 2017 Skin Lesion Segmentation Challenge","This abstract briefly describes a segmentation algorithm developed for the
ISIC 2017 Skin Lesion Detection Competition hosted at [ref]. The objective of
the competition is to perform a segmentation (in the form of a binary mask
image) of skin lesions in dermoscopic images as close as possible to a
segmentation performed by trained clinicians which is taken as ground truth.
This project only takes part in the segmentation phase of the challenge. The
other phases of the competition (feature extraction and lesion identification)
are not considered.
  The proposed algorithm consists of 4 steps: (1) lesion image preprocessing
(2) image segmentation using k-means clustering of pixel colors (3)
calculation of a set of features describing the properties of each segmented
region and (4) calculation of a final score for each region representing the
likelihood of corresponding to a suitable lesion segmentation. The scores in
step (4) are obtained by averaging the results of 2 different regression models
using the scores of each region as input. Before using the algorithm these
regression models must be trained using the training set of images and ground
truth masks provided by the Competition. Steps 2 to 4 are repeated with an
increasing number of clusters (and therefore the image is segmented into more
regions) until there is no further improvement of the calculated scores.",David Alvarez|Monica Iglesias,cs.CV
2017-02-28T17:22:55Z,2017-02-23T12:28:18Z,http://arxiv.org/abs/1702.07191v1,http://arxiv.org/pdf/1702.07191v1,"ViP-CNN: A Visual Phrase Reasoning Convolutional Neural Network for
  Visual Relationship Detection","As the intermediate level task connecting image captioning and object
detection visual relationship detection started to catch researchers'
attention because of its descriptive power and clear structure. It localizes
the objects and captures their interactions with a subject-predicate-object
triplet e.g. person-ride-horse. In this paper the visual relationship is
considered as a phrase with three components. So we formulate the visual
relationship detection as three inter-connected recognition problems and
propose a Visual Phrase reasoning Convolutional Neural Network (ViP-CNN) to
address them simultaneously. In ViP-CNN we present a Visual Phrase Reasoning
Structure (VPRS) to set up the connection among the relationship components and
help the model consider the three problems jointly. Corresponding non-maximum
suppression method and model training strategy are also proposed. Experimental
results show that our ViP-CNN outperforms the state-of-art method both in speed
and accuracy. We further pretrain our model on our cleansed Visual Genome
Relationship dataset which is found to perform better than the pretraining on
the ImageNet for this task.",Yikang Li|Wanli Ouyang|Xiaogang Wang,cs.CV
2017-02-28T17:22:55Z,2017-02-23T12:11:42Z,http://arxiv.org/abs/1702.07189v1,http://arxiv.org/pdf/1702.07189v1,"Analyzing Learned Convnet Features with Dirichlet Process Gaussian
  Mixture Models","Convolutional Neural Networks (Convnets) have achieved good results in a
range of computer vision tasks the recent years. Though given a lot of
attention visualizing the learned representations to interpret Convnets still
remains a challenging task. The high dimensionality of internal representations
and the high abstractions of deep layers are the main challenges when
visualizing Convnet functionality. We present in this paper a technique based
on clustering internal Convnet representations with a Dirichlet Process
Gaussian Mixture Model for visualization of learned representations in
Convnets. Our method copes with the high dimensionality of a Convnet by
clustering representations across all nodes of each layer. We will discuss how
this application is useful when considering transfer learning i.e.\
transferring a model trained on one dataset to solve a task on a different one.",David Malmgren-Hansen|Allan Aasbjerg Nielsen|Rasmus Engholm,cs.CV
2017-02-28T17:22:55Z,2017-02-23T01:23:45Z,http://arxiv.org/abs/1702.07059v1,http://arxiv.org/pdf/1702.07059v1,Robust and fully automated segmentation of mandible from CT scans,"Mandible bone segmentation from computed tomography (CT) scans is challenging
due to mandible's structural irregularities complex shape patterns and lack
of contrast in joints. Furthermore connections of teeth to mandible and
mandible to remaining parts of the skull make it extremely difficult to
identify mandible boundary automatically. This study addresses these challenges
by proposing a novel framework where we define the segmentation as two
complementary tasks: recognition and delineation. For recognition we use
random forest regression to localize mandible in 3D. For delineation we
propose to use 3D gradient-based fuzzy connectedness (FC) image segmentation
algorithm operating on the recognized mandible sub-volume. Despite heavy CT
artifacts and dental fillings consisting half of the CT image data in our
experiments we have achieved highly accurate detection and delineation
results. Specifically detection accuracy more than 96% (measured by union of
intersection (UoI)) the delineation accuracy of 91% (measured by dice
similarity coefficient) and less than 1 mm in shape mismatch (Hausdorff
Distance) were found.",Neslisah Torosdagli|Denise K. Liberton|Payal Verma|Murat Sincan Janice Lee|Sumanta Pattanaik|Ulas Bagci,cs.CV
2017-02-28T17:22:59Z,2017-02-23T00:40:29Z,http://arxiv.org/abs/1702.07054v1,http://arxiv.org/pdf/1702.07054v1,"Learning Chained Deep Features and Classifiers for Cascade in Object
  Detection","Cascade is a widely used approach that rejects obvious negative samples at
early stages for learning better classifier and faster inference. This paper
presents chained cascade network (CC-Net). In this CC-Net the cascaded
classifier at a stage is aided by the classification scores in previous stages.
Feature chaining is further proposed so that the feature learning for the
current cascade stage uses the features in previous stages as the prior
information. The chained ConvNet features and classifiers of multiple stages
are jointly learned in an end-to-end network. In this way features and
classifiers at latter stages handle more difficult samples with the help of
features and classifiers in previous stages. It yields consistent boost in
detection performance on benchmarks like PASCAL VOC 2007 and ImageNet. Combined
with better region proposal CC-Net leads to state-of-the-art result of 81.1%
mAP on PASCAL VOC 2007.",Wanli Ouyang|Ku Wang|Xin Zhu|Xiaogang Wang,cs.CV
2017-02-28T17:22:59Z,2017-02-22T22:17:13Z,http://arxiv.org/abs/1702.07025v1,http://arxiv.org/pdf/1702.07025v1,"Increasing Deep Learning Melanoma Classification by Classical And Expert
  Knowledge Based Image Transforms","Skin cancer is a major public health problem as is the most common type of
cancer and represents more than half of cancer diagnoses worldwide. Early
detection influences the outcome of the disease and motivates our work. We
obtain the state of the art results for the ISBI 2016 Melanoma Classification
Challenge (named Skin Lesion Analysis towards Melanoma Detection) facing the
peculiarities of dealing with such a small unbalanced biological database.
For that we explore committees of Convolutional Neural Networks trained over
the ISBI challenge training dataset artificially augmented by both classical
image processing transforms and image warping guided by specialist knowledge
about the lesion axis and improve the final classifier invariance to common
melanoma variations.",Cristina Nader Vasconcelos|Bárbara Nader Vasconcelos,cs.CV
2017-02-28T17:22:59Z,2017-02-22T21:50:55Z,http://arxiv.org/abs/1702.07019v1,http://arxiv.org/pdf/1702.07019v1,CT Image Denoising with Perceptive Deep Neural Networks,"Increasing use of CT in modern medical practice has raised concerns over
associated radiation dose. Reduction of radiation dose associated with CT can
increase noise and artifacts which can adversely affect diagnostic confidence.
Denoising of low-dose CT images on the other hand can help improve diagnostic
confidence which however is a challenging problem due to its ill-posed nature
since one noisy image patch may correspond to many different output patches. In
the past decade machine learning based approaches have made quite impressive
progress in this direction. However most of those methods including the
recently popularized deep learning techniques aim for minimizing
mean-squared-error (MSE) between a denoised CT image and the ground truth
which results in losing important structural details due to over-smoothing
although the PSNR based performance measure looks great. In this work we
introduce a new perceptual similarity measure as the objective function for a
deep convolutional neural network to facilitate CT image denoising. Instead of
directly computing MSE for pixel-to-pixel intensity loss we compare the
perceptual features of a denoised output against those of the ground truth in a
feature space. Therefore our proposed method is capable of not only reducing
the image noise levels but also keeping the critical structural information at
the same time. Promising results have been obtained in our experiments with a
large number of CT images.",Qingsong Yang|Pingkun Yan|Mannudeep K. Kalra|Ge Wang,cs.CV
2017-02-28T17:22:59Z,2017-02-22T21:03:49Z,http://arxiv.org/abs/1702.07006v1,http://arxiv.org/pdf/1702.07006v1,Synthesising Dynamic Textures using Convolutional Neural Networks,"Here we present a parametric model for dynamic textures. The model is based
on spatiotemporal summary statistics computed from the feature representations
of a Convolutional Neural Network (CNN) trained on object recognition. We
demonstrate how the model can be used to synthesise new samples of dynamic
textures and to predict motion in simple movies.",Christina M. Funke|Leon A. Gatys|Alexander S. Ecker|Matthias Bethge,cs.CV
2017-02-28T17:22:59Z,2017-02-22T18:15:42Z,http://arxiv.org/abs/1702.06925v1,http://arxiv.org/abs/1702.06925v1,Transferring Face Verification Nets To Pain and Expression Regression,"Limited annotated data is available for the research of estimating facial
expression intensities which makes the training of deep networks for automated
expression assessment very challenging. Fortunately fine-tuning from a
data-extensive pre-trained domain such as face verification can alleviate the
problem. In this paper we propose a transferred network that fine-tunes a
state-of-the-art face verification network using expression-intensity labeled
data with a regression layer. In this way the expression regression task can
benefit from the rich feature representations trained on a huge amount of data
for face verification. The proposed transferred deep regressor is applied in
estimating the intensity of facial action units (2017 EmotionNet Challenge) and
in particular pain intensity estimation (UNBS-McMaster Shoulder-Pain dataset).
It wins the second place in the challenge and achieves the state-of-the-art
performance on Shoulder-Pain dataset. Particularly for Shoulder-Pain with the
imbalance issue of different pain levels a new weighted evaluation metric is
proposed.",Feng Wang|Xiang Xiang|Chang Liu|Trac D. Tran|Austin Reiter|Gregory D. Hager|Harry Quon|Jian Cheng|Alan L. Yuille,cs.CV|cs.AI|cs.LG|cs.MM
2017-02-28T17:22:59Z,2017-02-22T16:45:48Z,http://arxiv.org/abs/1702.06890v1,http://arxiv.org/pdf/1702.06890v1,"Learning Deep Features via Congenerous Cosine Loss for Person
  Recognition","Person recognition aims at recognizing the same identity across time and
space with complicated scenes and similar appearance. In this paper we propose
a novel method to address this task by training a network to obtain robust and
representative features. A key observation is that traditional cross entropy
loss only enforces the inter-class variation among samples and ignores to
narrow down the similarity within each category. We propose a congenerous
cosine loss to enlarge the inter-class distinction as well as alleviate the
inner-class variance. Such a design is achieved by minimizing the cosine
distance between sample and its cluster centroid in a cooperative way. Our
method differs from previous work in person recognition that we do not conduct
a second training on the test subset and thus maintain a good generalization
ability. The identity of a person is determined by measuring the similarity
from several body regions in the reference set. Experimental results show that
the proposed approach achieves better classification accuracy against previous
state-of-the-arts.",Yu Liu|Hongyang Li|Xiaogang Wang,cs.CV|cs.LG|stat.ML
2017-02-28T17:22:59Z,2017-02-22T13:56:54Z,http://arxiv.org/abs/1702.06799v1,http://arxiv.org/pdf/1702.06799v1,Boosted Multiple Kernel Learning for First-Person Activity Recognition,"Activity recognition from first-person (ego-centric) videos has recently
gained attention due to the increasing ubiquity of the wearable cameras. There
has been a surge of efforts adapting existing feature descriptors and designing
new descriptors for the first-person videos. An effective activity recognition
system requires selection and use of complementary features and appropriate
kernels for each feature. In this study we propose a data-driven framework for
first-person activity recognition which effectively selects and combines
features and their respective kernels during the training. Our experimental
results show that use of Multiple Kernel Learning (MKL) and Boosted MKL in
first-person activity recognition problem exhibits improved results in
comparison to the state-of-the-art. In addition these techniques enable the
expansion of the framework with new features in an efficient and convenient
way.",Fatih Ozkan|Mehmet Ali Arabaci|Elif Surer|Alptekin Temizel,cs.CV
2017-02-28T17:22:59Z,2017-02-22T12:08:09Z,http://arxiv.org/abs/1702.06767v1,http://arxiv.org/pdf/1702.06767v1,MomentsNet: a simple learning-free method for binary image recognition,"In this paper we propose a new simple and learning-free deep learning
network named MomentsNet whose convolution layer nonlinear processing layer
and pooling layer are constructed by Moments kernels binary hashing and
block-wise histogram respectively. Twelve typical moments (including
geometrical moment Zernike moment Tchebichef moment etc.) are used to
construct the MomentsNet whose recognition performance for binary image is
studied. The results reveal that MomentsNet has better recognition performance
than its corresponding moments in almost all cases and ZernikeNet achieves the
best recognition performance among MomentsNet constructed by twelve moments.
ZernikeNet also shows better recognition performance on binary image database
than that of PCANet which is a learning-based deep learning network.",Jiasong Wu|Shijie Qiu|Youyong Kong|Yang Chen|Lotfi Senhadji|Huazhong Shu,cs.CV
2017-02-28T17:22:59Z,2017-02-22T09:32:09Z,http://arxiv.org/abs/1702.06722v1,http://arxiv.org/pdf/1702.06722v1,"3D Reconstruction of Temples in the Special Region of Yogyakarta By
  Using Close-Range Photogrammetry","Object reconstruction is one of the main problems in cultural heritage
preservation. This problem is due to lack of data in documentation. Thus in
this research we presented a method of 3D reconstruction using close-range
photogrammetry. We collected 1319 photos from five temples in Yogyakarta. Using
A-KAZE algorithm keypoints of each image were obtained. Then we employed LIOP
to create feature descriptor from it. After performing feature matching L1RA
was utilized to create sparse point clouds. In order to generate the geometry
shape MVS was used. Finally FSSR and Large Scale Texturing were employed to
deal with the surface and texture of the object. The quality of the
reconstructed 3D model was measured by comparing the 3D images of the model
with the original photos utilizing SSIM. The results showed that in terms of
quality our method was on par with other commercial method such as
PhotoModeler and PhotoScan.",Adityo Priyandito Utomo|Canggih Puspo Wibowo,cs.CV
2017-02-28T17:22:59Z,2017-02-22T08:19:38Z,http://arxiv.org/abs/1702.06700v1,http://arxiv.org/pdf/1702.06700v1,"Task-driven Visual Saliency and Attention-based Visual Question
  Answering","Visual question answering (VQA) has witnessed great progress since May 2015
as a classic problem unifying visual and textual data into a system. Many
enlightening VQA works explore deep into the image and question encodings and
fusing methods of which attention is the most effective and infusive
mechanism. Current attention based methods focus on adequate fusion of visual
and textual features but lack the attention to where people focus to ask
questions about the image. Traditional attention based methods attach a single
value to the feature at each spatial location which losses many useful
information. To remedy these problems we propose a general method to perform
saliency-like pre-selection on overlapped region features by the interrelation
of bidirectional LSTM (BiLSTM) and use a novel element-wise multiplication
based attention method to capture more competent correlation information
between visual and textual features. We conduct experiments on the large-scale
COCO-VQA dataset and analyze the effectiveness of our model demonstrated by
strong empirical results.",Yuetan Lin|Zhangyang Pang|Donghui Wang|Yueting Zhuang,cs.CV|cs.AI|cs.CL|cs.NE
2017-02-28T17:23:02Z,2017-02-22T06:20:13Z,http://arxiv.org/abs/1702.06683v1,http://arxiv.org/pdf/1702.06683v1,"Using Deep Learning and Google Street View to Estimate the Demographic
  Makeup of the US","The United States spends more than $1B each year on the American Community
Survey (ACS) a labor-intensive door-to-door study that measures statistics
relating to race gender education occupation unemployment and other
demographic factors. Although a comprehensive source of data the lag between
demographic changes and their appearance in the ACS can exceed half a decade.
As digital imagery becomes ubiquitous and machine vision techniques improve
automated data analysis may provide a cheaper and faster alternative. Here we
present a method that determines socioeconomic trends from 50 million images of
street scenes gathered in 200 American cities by Google Street View cars.
Using deep learning-based computer vision techniques we determined the make
model and year of all motor vehicles encountered in particular neighborhoods.
Data from this census of motor vehicles which enumerated 22M automobiles in
total (8% of all automobiles in the US) was used to accurately estimate
income race education and voting patterns with single-precinct resolution.
(The average US precinct contains approximately 1000 people.) The resulting
associations are surprisingly simple and powerful. For instance if the number
of sedans encountered during a 15-minute drive through a city is higher than
the number of pickup trucks the city is likely to vote for a Democrat during
the next Presidential election (88% chance); otherwise it is likely to vote
Republican (82%). Our results suggest that automated systems for monitoring
demographic trends may effectively complement labor-intensive approaches with
the potential to detect trends with fine spatial resolution in close to real
time.",Timnit Gebru|Jonathan Krause|Yilun Wang|Duyun Chen|Jia Deng|Erez Lieberman Aiden|Li Fei-Fei,cs.CV
2017-02-28T17:23:02Z,2017-02-22T04:34:31Z,http://arxiv.org/abs/1702.06674v1,http://arxiv.org/pdf/1702.06674v1,Unsupervised Diverse Colorization via Generative Adversarial Networks,"Colorization of grayscale images has been a hot topic in computer vision.
Previous research mainly focuses on producing a colored image to match the
original one. However since many colors share the same gray value an input
grayscale image could be diversely colored while maintaining its reality. In
this paper we design a novel solution for unsupervised diverse colorization.
Specifically we leverage conditional generative adversarial networks to model
the distribution of real-world item colors in which we develop a fully
convolutional generator with multi-layer noise to enhance diversity with
multi-layer condition concatenation to maintain reality and with stride 1 to
keep spatial information. With such a novel network architecture the model
yields highly competitive performance on the open LSUN bedroom dataset. The
Turing test of 80 humans further indicates our generated color schemes are
highly convincible.",Yun Cao|Zhiming Zhou|Weinan Zhang|Yong Yu,cs.CV|cs.AI
2017-02-28T17:23:02Z,2017-02-21T23:20:54Z,http://arxiv.org/abs/1702.06619v1,http://arxiv.org/pdf/1702.06619v1,Lensless Photography with only an image sensor,"Photography usually requires optics in conjunction with a recording device
(an image sensor). Eliminating the optics could lead to new form factors for
cameras. Here we report a simple demonstration of imaging using a bare CMOS
sensor that utilizes computation. The technique relies on the space variant
point-spread functions resulting from the interaction of a point source in the
field of view with the image sensor. These space-variant point-spread functions
are combined with a reconstruction algorithm in order to image simple objects
displayed on a discrete LED array as well as on an LCD screen. We extended the
approach to video imaging at the native frame rate of the sensor. Finally we
performed experiments to analyze the parametric impact of the object distance.
Improving the sensor designs and reconstruction algorithms can lead to useful
cameras without optics.",Ganghun Kim|Kyle Isaacson|Racheal Palmer|Rajesh Menon,cs.CV|physics.optics
2017-02-28T17:23:02Z,2017-02-21T18:49:26Z,http://arxiv.org/abs/1702.06521v1,http://arxiv.org/pdf/1702.06521v1,VidLoc: 6-DoF Video-Clip Relocalization,"Machine learning techniques namely convolutional neural networks (CNN) and
regression forests have recently shown great promise in performing 6-DoF
localization of monocular images. However in most cases image-sequences
rather only single images are readily available. To this extent none of the
proposed learning-based approaches exploit the valuable constraint of temporal
smoothness often leading to situations where the per-frame error is larger
than the camera motion. In this paper we propose a recurrent model for
performing 6-DoF localization of video-clips. We find that even by considering
only short sequences (20 frames) the pose estimates are smoothed and the
localization error can be drastically reduced. Finally we consider means of
obtaining probabilistic pose estimates from our model. We evaluate our method
on openly-available real-world autonomous driving and indoor localization
datasets.",Ronald Clark|Sen Wang|Andrew Markham|Niki Trigoni|Hongkai Wen,cs.CV
2017-02-28T17:23:02Z,2017-02-21T18:20:30Z,http://arxiv.org/abs/1702.06506v1,http://arxiv.org/pdf/1702.06506v1,"PixelNet: Representation of the pixels by the pixels and for the
  pixels","We explore design principles for general pixel-level prediction problems
from low-level edge detection to mid-level surface normal estimation to
high-level semantic segmentation. Convolutional predictors such as the
fully-convolutional network (FCN) have achieved remarkable success by
exploiting the spatial redundancy of neighboring pixels through convolutional
processing. Though computationally efficient we point out that such approaches
are not statistically efficient during learning precisely because spatial
redundancy limits the information learned from neighboring pixels. We
demonstrate that stratified sampling of pixels allows one to (1) add diversity
during batch updates speeding up learning; (2) explore complex nonlinear
predictors improving accuracy; and (3) efficiently train state-of-the-art
models tabula rasa (i.e. ""from scratch"") for diverse pixel-labeling tasks. Our
single architecture produces state-of-the-art results for semantic segmentation
on PASCAL-Context dataset surface normal estimation on NYUDv2 depth dataset
and edge detection on BSDS.",Aayush Bansal|Xinlei Chen|Bryan Russell|Abhinav Gupta|Deva Ramanan,cs.CV|cs.LG|cs.RO
2017-02-28T17:23:02Z,2017-02-21T16:12:18Z,http://arxiv.org/abs/1702.06461v1,http://arxiv.org/pdf/1702.06461v1,Crowd Sourcing Image Segmentation with iaSTAPLE,"We propose a novel label fusion technique as well as a crowdsourcing protocol
to efficiently obtain accurate epithelial cell segmentations from non-expert
crowd workers. Our label fusion technique simultaneously estimates the true
segmentation the performance levels of individual crowd workers and an image
segmentation model in the form of a pairwise Markov random field. We term our
approach image-aware STAPLE (iaSTAPLE) since our image segmentation model
seamlessly integrates into the well-known and widely used STAPLE approach. In
an evaluation on a light microscopy dataset containing more than 5000 membrane
labeled epithelial cells of a fly wing we show that iaSTAPLE outperforms
STAPLE in terms of segmentation accuracy as well as in terms of the accuracy of
estimated crowd worker performance levels and is able to correctly segment 99%
of all cells when compared to expert segmentations. These results show that
iaSTAPLE is a highly useful tool for crowd sourcing image segmentation.",Dmitrij Schlesinger|Florian Jug|Gene Myers|Carsten Rother|Dagmar Kainmüller,cs.CV
2017-02-28T17:23:02Z,2017-02-21T16:01:28Z,http://arxiv.org/abs/1702.06456v1,http://arxiv.org/pdf/1702.06456v1,"Online Representation Learning with Multi-layer Hebbian Networks for
  Image Classification Tasks","Unsupervised learning allows algorithms to adapt to different data thanks to
the autonomous discovery of discriminating features during the training. When
these algorithms are reducible to cost-function minimisation better
interpretations of their learning dynamics are possible. Recently new
Hebbian-like plasticity bio-inspired local and unsupervised learning rules
for neural networks have been shown to minimise a cost-function while
performing online sparse representation learning. However it is unclear to
what degree such rules are effective to learn features from images. To
investigate this point this study introduces a novel multi-layer Hebbian
network trained by a rule derived from a non-negative classical
multidimensional scaling cost-function. The performance is compared to that of
other fully unsupervised learning algorithms.",Yanis Bahroun|Andrea Soltoggio,cs.NE|cs.CV|I.5.1
2017-02-28T17:23:02Z,2017-02-21T15:50:10Z,http://arxiv.org/abs/1702.06451v1,http://arxiv.org/pdf/1702.06451v1,"Traffic Surveillance Camera Calibration by 3D Model Bounding Box
  Alignment for Accurate Vehicle Speed Measurement","In this paper we focus on fully automatic traffic surveillance camera
calibration which we use for speed measurement of passing vehicles. We improve
over a recent state-of-the-art camera calibration method for traffic
surveillance based on two detected vanishing points. More importantly we
propose a novel automatic scene scale inference based on matching bounding
boxes of rendered 3D models of vehicles with detected bounding boxes in the
image. The proposed method can be used from an arbitrary viewpoint and it has
no constraints on camera placement. We evaluate our method on recent
comprehensive dataset for speed measurement BrnoCompSpeed. Experiments show
that our automatic camera calibration by detected two vanishing points method
reduces the error by 50% compared to the previous state-of-the-art method. We
also show that our scene scale inference method is much more precise (mean
speed measurement error 1.10km/h) outperforming both state of the art automatic
calibration method (error reduction by 86% -- mean error 7.98km/h) and manual
calibration (error reduction by 19% -- mean error 1.35km/h). We also present
qualitative results of automatic camera calibration method on video sequences
obtained from real surveillance cameras on various places and under different
lighting conditions (night dawn day).",Jakub Sochor|Roman Juránek|Adam Herout,cs.CV
2017-02-28T17:23:02Z,2017-02-21T15:34:20Z,http://arxiv.org/abs/1702.06441v1,http://arxiv.org/pdf/1702.06441v1,"BrnoCompSpeed: Review of Traffic Camera Calibration and Comprehensive
  Dataset for Monocular Speed Measurement","In this paper we focus on traffic camera calibration and visual speed
measurement from a single monocular camera which is an important task of
visual traffic surveillance. Existing methods addressing this problem are hard
to compare due to lack of a common dataset with reliable ground truth.
Therefore it is not clear how the methods compare in various aspects and what
are the factors affecting their performance. We captured a new dataset of 18
full-HD videos each around one hour long captured at 6 different locations.
Vehicles in videos (20865 instances in total) are annotated with precise speed
measurements from optical gates using LIDAR and verified with several reference
GPS tracks. We provide the videos and metadata (calibration lengths of
features in image annotations etc.) for future comparison and evaluation.
Camera calibration is the most crucial part of the speed measurement;
therefore we provide a review of the methods and analyze a recently published
method for fully automatic camera calibration and vehicle speed measurement and
report the results on this dataset in detail.",Jakub Sochor|Roman Juránek|Jakub Špaňhel|Lukáš Maršík|Adam Široký|Adam Herout|Pavel Zemčík,cs.CV
2017-02-28T17:23:02Z,2017-02-21T14:41:15Z,http://arxiv.org/abs/1702.06408v1,http://arxiv.org/pdf/1702.06408v1,"A Discriminative Event Based Model for Alzheimer's Disease Progression
  Modeling","The event-based model (EBM) for data-driven disease progression modeling
estimates the sequence in which biomarkers for a disease become abnormal. This
helps in understanding the dynamics of disease progression and facilitates
early diagnosis by staging patients on a disease progression timeline. Existing
EBM methods are all generative in nature. In this work we propose a novel
discriminative approach to EBM which is shown to be more accurate as well as
computationally more efficient than existing state-of-the art EBM methods. The
method first estimates for each subject an approximate ordering of events by
ranking the posterior probabilities of individual biomarkers being abnormal.
Subsequently the central ordering over all subjects is estimated by fitting a
generalized Mallows model to these approximate subject-specific orderings based
on a novel probabilistic Kendall's Tau distance. To evaluate the accuracy we
performed extensive experiments on synthetic data simulating the progression of
Alzheimer's disease. Subsequently the method was applied to the Alzheimer's
Disease Neuroimaging Initiative (ADNI) data to estimate the central event
ordering in the dataset. The experiments benchmark the accuracy of the new
model under various conditions and compare it with existing state-of-the-art
EBM methods. The results indicate that discriminative EBM could be a simple and
elegant approach to disease progression modeling.",Vikram Venkatraghavan|Esther Bron|Wiro Niessen|Stefan Klein,cs.CV|q-bio.QM
2017-02-28T17:23:06Z,2017-02-21T13:55:47Z,http://arxiv.org/abs/1702.06383v1,http://arxiv.org/pdf/1702.06383v1,Deep Geometric Retrieval,"Comparing images in order to recommend items from an image-inventory is a
subject of continued interest. Added with the scalability of deep-learning
architectures the once `manual' job of hand-crafting features have been largely
alleviated and images can be compared according to features generated from a
deep convolutional neural network. In this paper we compare distance metrics
(and divergences) to rank features generated from a neural network for
content-based image retrieval. Specifically after modelling individual images
using approximations of mixture models or sparse covariance estimators we
resort to their information-theoretic and Riemann geometric comparisons. We
show that using approximations of mixture models enable us to to compute a
distance measure based on the Wasserstein metric that requires less effort than
computationally intensive optimal transport plans; finally an affine invariant
metric is used to compare the optimal transport metric to its Riemann geometric
counterpart -- we conclude that although expensive retrieval metric based on
Wasserstein geometry are more suitable than information theoretic comparison of
images. In short we combine GPU scalability in learning deep feature vectors
with computationally efficient metrics that we foresee being utilized in a
commercial setting.",Y Qian|E Vazquez|B Sengupta,cs.IR|cs.CV
2017-02-28T17:23:06Z,2017-02-21T13:33:37Z,http://arxiv.org/abs/1702.06376v1,http://arxiv.org/pdf/1702.06376v1,Mimicking Ensemble Learning with Deep Branched Networks,"This paper proposes a branched residual network for image classification. It
is known that high-level features of deep neural network are more
representative than lower-level features. By sharing the low-level features
the network can allocate more memory to high-level features. The upper layers
of our proposed network are branched so that it mimics the ensemble learning.
By mimicking ensemble learning with single network we have achieved better
performance on ImageNet classification task.",Byungju Kim|Youngsoo Kim|Yeakang Lee|Junmo Kim,cs.CV
2017-02-28T17:23:06Z,2017-02-21T12:38:11Z,http://arxiv.org/abs/1702.06355v1,http://arxiv.org/pdf/1702.06355v1,Object Detection in Videos with Tubelet Proposal Networks,"Object detection in videos has drawn increasing attention recently with the
introduction of the large-scale ImageNet VID dataset. Different from object
detection in static images temporal information in videos provides vital
information for object detection. To fully utilize temporal information
state-of-the-art methods are therefore based on spatiotemporal tubelets which
are essentially sequences of associated bounding boxes across time. However
the existing methods have major limitations in generating tubelets in terms of
quality and efficiency. Motion-based methods are able to obtain dense tubelets
but the lengths are generally only several frames which is not optimal to
incorporate long-term temporal information. Appearance-based methods usually
involving generic object tracking could generate long tubelets but are
usually computational expensive. In this work we propose a framework for
object detection in videos which consists of a novel tubelet proposal network
to efficiently generate spatiotemporal proposals and a Long Short-term Memory
(LSTM) network that incorporates temporal information from tubelet proposals
for achieving high object detection accuracy in videos. The experiments on the
large-scale ImageNet VID dataset demonstrate the effectiveness of the proposed
framework for object detection in videos.",Kai Kang|Hongsheng Li|Tong Xiao|Wanli Ouyang|Junjie Yan|Xihui Liu|Xiaogang Wang,cs.CV
2017-02-28T17:23:06Z,2017-02-21T11:23:48Z,http://arxiv.org/abs/1702.06332v1,http://arxiv.org/pdf/1702.06332v1,Just DIAL: DomaIn Alignment Layers for Unsupervised Domain Adaptation,"The empirical fact that classifiers trained on given data collections
perform poorly when tested on data acquired in different settings is
theoretically explained in domain adaptation through a shift among
distributions of the source and target domains. Alleviating the domain shift
problem especially in the challenging setting where no labeled data are
available for the target domain is paramount for having visual recognition
systems working in the wild. As the problem stems from a shift among
distributions intuitively one should try to align them. In the literature
this has resulted in a stream of works attempting to align the feature
representations learned from the source and target domains. Here we take a
different route. Rather than introducing regularization terms aiming to promote
the alignment of the two representations we act at the distribution level
through the introduction of \emph{DomaIn Alignment Layers} (\DIAL) able to
match the observed source and target data distributions to a reference one.
Thorough experiments on three different public benchmarks we confirm the power
of our approach.",Fabio Maria Carlucci|Lorenzo Porzi|Barbara Caputo|Elisa Ricci|Samuel Rota Bulò,cs.CV
2017-02-28T17:23:06Z,2017-02-21T10:36:28Z,http://arxiv.org/abs/1702.06318v1,http://arxiv.org/pdf/1702.06318v1,"Is Saki #delicious? The Food Perception Gap on Instagram and Its
  Relation to Health","Food is an integral part of our life and what and how much we eat crucially
affects our health. Our food choices largely depend on how we perceive certain
characteristics of food such as whether it is healthy delicious or if it
qualifies as a salad. But these perceptions differ from person to person and
one person's ""single lettuce leaf"" might be another person's ""side salad"".
Studying how food is perceived in relation to what it actually is typically
involves a laboratory setup. Here we propose to use recent advances in image
recognition to tackle this problem. Concretely we use data for 1.9 million
images from Instagram from the US to look at systematic differences in how a
machine would objectively label an image compared to how a human subjectively
does. We show that this difference which we call the ""perception gap"" relates
to a number of health outcomes observed at the county level. To the best of our
knowledge this is the first time that image recognition is being used to study
the ""misalignment"" of how people describe food images vs. what they actually
depict.",Ferda Ofli|Yusuf Aytar|Ingmar Weber|Raggi al Hammouri|Antonio Torralba,cs.CY|cs.CV|cs.SI
2017-02-28T17:23:06Z,2017-02-21T08:58:28Z,http://arxiv.org/abs/1702.06294v1,http://arxiv.org/pdf/1702.06294v1,"Learning Compact Appearance Representation for Video-based Person
  Re-Identification","This paper presents a novel approach for video-based person re-identification
using multiple Convolutional Neural Networks (CNNs). Unlike previous work we
intend to extract a compact yet discriminative appearance representation from
several frames rather than the whole sequence. Specifically given a video the
representative frames are selected based on the walking profile of consecutive
frames. A multiple CNN architecture incorporated with feature pooling is
proposed to learn and compile the features of the selected representative
frames into a compact description about the pedestrian for identification.
Experiments are conducted on benchmark datasets to demonstrate the superiority
of the proposed method over existing person re-identification approaches.",Wei Zhang|Shengnan Hu|Kan Liu,cs.CV
2017-02-28T17:23:06Z,2017-02-21T08:15:51Z,http://arxiv.org/abs/1702.06291v1,http://arxiv.org/pdf/1702.06291v1,Visual Tracking by Reinforced Decision Making,"One of the major challenges of model-free visual tracking problem has been
the difficulty originating from the unpredictable and drastic changes in the
appearance of objects we target to track. Existing methods tackle this problem
by updating the appearance model on-line in order to adapt to the changes in
the appearance. Despite the success of these methods however inaccurate and
erroneous updates of the appearance model result in a tracker drift. In this
paper we introduce a novel visual tracking algorithm based on a template
selection strategy constructed by deep reinforcement learning methods. The
tracking algorithm utilizes this strategy to choose the best template for
tracking a given frame. The template selection strategy is self-learned by
utilizing a simple policy gradient method on numerous training episodes
randomly generated from a tracking benchmark dataset. Our proposed
reinforcement learning framework is generally applicable to other confidence
map based tracking algorithms. The experiment shows that our tracking algorithm
effectively decides the best template for visual tracking.",Janghoon Choi|Junseok Kwon|Kyoung Mu Lee,cs.CV
2017-02-28T17:23:06Z,2017-02-21T06:57:37Z,http://arxiv.org/abs/1702.06850v1,http://arxiv.org/pdf/1702.06850v1,Scene Recognition by Combining Local and Global Image Descriptors,"Object recognition is an important problem in computer vision having diverse
applications. In this work we construct an end-to-end scene recognition
pipeline consisting of feature extraction encoding pooling and
classification. Our approach simultaneously utilize global feature descriptors
as well as local feature descriptors from images to form a hybrid feature
descriptor corresponding to each image. We utilize DAISY features associated
with key points within images as our local feature descriptor and histogram of
oriented gradients (HOG) corresponding to an entire image as a global
descriptor. We make use of a bag-of-visual-words encoding and apply Mini- Batch
K-Means algorithm to reduce the complexity of our feature encoding scheme. A
2-level pooling procedure is used to combine DAISY and HOG features
corresponding to each image. Finally we experiment with a multi-class SVM
classifier with several kernels in a cross-validation setting and tabulate
our results on the fifteen scene categories dataset. The average accuracy of
our model was 76.4% in the case of a 40%-60% random split of images into
training and testing datasets respectively. The primary objective of this work
is to clearly outline the practical implementation of a basic
screne-recognition pipeline having a reasonable accuracy in python using
open-source libraries. A full implementation of the proposed model is available
in our github repository.",Jobin Wilson|Muhammad Arif,cs.CV|cs.LG
2017-02-28T17:23:06Z,2017-02-21T06:35:01Z,http://arxiv.org/abs/1702.06277v1,http://arxiv.org/pdf/1702.06277v1,"Projection based advanced motion model for cubic mapping for 360-degree
  video","This paper proposes a novel advanced motion model to handle the irregular
motion for the cubic map projection of 360-degree video. Since the irregular
motion is mainly caused by the projection from the sphere to the cube map we
first try to project the pixels in both the current picture and reference
picture from unfolding cube back to the sphere. Then through utilizing the
characteristic that most of the motions in the sphere are uniform we can
derive the relationship between the motion vectors of various pixels in the
unfold cube. The proposed advanced motion model is implemented in the High
Efficiency Video Coding reference software. Experimental results demonstrate
that quite obvious performance improvement can be achieved for the sequences
with obvious motions.",Li Li|Zhu Li|Madhukar Budagavi|Houqiang Li,cs.MM|cs.CV
2017-02-28T17:23:06Z,2017-02-22T01:57:04Z,http://arxiv.org/abs/1702.06264v2,http://arxiv.org/pdf/1702.06264v2,Weighted Motion Averaging for the Registration of Multi-View Range Scans,"Multi-view registration is a fundamental but challenging problem in 3D
reconstruction and robot vision. Although the original motion averaging
algorithm has been introduced as an effective means to solve the multi-view
registration problem it does not consider the reliability and accuracy of each
relative motion. Accordingly this paper proposes a novel motion averaging
algorithm for multi-view registration. Firstly it utilizes the pair-wise
registration algorithm to estimate the relative motion and overlapping
percentage of each scan pair with a certain degree of overlap. With the
overlapping percentage available it views the overlapping percentage as the
corresponding weight of each scan pair and proposes the weight motion averaging
algorithm which can pay more attention to reliable and accurate relative
motions. By treating each relative motion distinctively more accurate
registration can be achieved by applying the weighted motion averaging to
multi-view range scans. Experimental results demonstrate the superiority of our
proposed approach compared with the state-of-the-art methods in terms of
accuracy robustness and efficiency.",Rui Guo|Jihua Zhu|Yaochen Li|Dapeng Chen|Zhongyu Li|Yongqin Zhang,cs.CV
2017-02-28T17:23:10Z,2017-02-21T04:10:31Z,http://arxiv.org/abs/1702.06257v1,http://arxiv.org/pdf/1702.06257v1,The Power of Sparsity in Convolutional Neural Networks,"Deep convolutional networks are well-known for their high computational and
memory demands. Given limited resources how does one design a network that
balances its size training time and prediction accuracy? A surprisingly
effective approach to trade accuracy for size and speed is to simply reduce the
number of channels in each convolutional layer by a fixed fraction and retrain
the network. In many cases this leads to significantly smaller networks with
only minimal changes to accuracy. In this paper we take a step further by
empirically examining a strategy for deactivating connections between filters
in convolutional layers in a way that allows us to harvest savings both in
run-time and memory for many network architectures. More specifically we
generalize 2D convolution to use a channel-wise sparse connection structure and
show that this leads to significantly better results than the baseline approach
for large networks including VGG and Inception V3.",Soravit Changpinyo|Mark Sandler|Andrey Zhmoginov,cs.CV
2017-02-28T17:23:10Z,2017-02-21T01:02:56Z,http://arxiv.org/abs/1702.06228v1,http://arxiv.org/pdf/1702.06228v1,"Learning to Generate Posters of Scientific Papers by Probabilistic
  Graphical Models","Researchers often summarize their work in the form of scientific posters.
Posters provide a coherent and efficient way to convey core ideas expressed in
scientific papers. Generating a good scientific poster however is a complex
and time consuming cognitive task since such posters need to be readable
informative and visually aesthetic. In this paper for the first time we
study the challenging problem of learning to generate posters from scientific
papers. To this end a data-driven framework that utilizes graphical models
is proposed. Specifically given content to display the key elements of a good
poster including attributes of each panel and arrangements of graphical
elements are learned and inferred from data. During the inference stage an MAP
inference framework is employed to incorporate some design principles. In order
to bridge the gap between panel attributes and the composition within each
panel we also propose a recursive page splitting algorithm to generate the
panel layout for a poster. To learn and validate our model we collect and
release a new benchmark dataset called NJU-Fudan Paper-Poster dataset which
consists of scientific papers and corresponding posters with exhaustively
labelled panels and attributes. Qualitative and quantitative results indicate
the effectiveness of our approach.",Yu-ting Qiang|Yanwei Fu|Xiao Yu|Yanwen Guo|Zhi-Hua Zhou|Leonid Sigal,cs.CV|cs.GR|cs.HC|cs.MM
2017-02-28T17:23:10Z,2017-02-21T00:12:32Z,http://arxiv.org/abs/1702.06813v1,http://arxiv.org/pdf/1702.06813v1,"RenderMap: Exploiting the Link Between Perception and Rendering for
  Dense Mapping","We introduce an approach for the real-time (2Hz) creation of a dense map and
alignment of a moving robotic agent within that map by rendering using a
Graphics Processing Unit (GPU). This is done by recasting the scan alignment
part of the dense mapping process as a rendering task. Alignment errors are
computed from rendering the scene comparing with range data from the sensors
and minimized by an optimizer. The proposed approach takes advantage of the
advances in rendering techniques for computer graphics and GPU hardware to
accelerate the algorithm. Moreover it allows one to exploit information not
used in classic dense mapping algorithms such as Iterative Closest Point (ICP)
by rendering interfaces between the free space occupied space and the unknown.
The proposed approach leverages directly the rendering capabilities of the GPU
in contrast to other GPU-based approaches that deploy the GPU as a general
purpose parallel computation platform.
  We argue that the proposed concept is a general consequence of treating
perception problems as inverse problems of rendering. Many perception problems
can be recast into a form where much of the computation is replaced by render
operations. This is not only efficient since rendering is fast but also
simpler to implement and will naturally benefit from future advancements in GPU
speed and rendering techniques. Furthermore this general concept can go beyond
addressing perception problems and can be used for other problem domains such
as path planning.",Julian Ryde|Xuchu|Ding,cs.CV|cs.RO
2017-02-28T17:23:10Z,2017-02-20T23:44:54Z,http://arxiv.org/abs/1702.06212v1,http://arxiv.org/pdf/1702.06212v1,"Efficient Dense Labeling of Human Activity Sequences from Wearables
  using Fully Convolutional Networks","Recognizing human activities in a sequence is a challenging area of research
in ubiquitous computing. Most approaches use a fixed size sliding window over
consecutive samples to extract features---either handcrafted or learned
features---and predict a single label for all samples in the window. Two key
problems emanate from this approach: i) the samples in one window may not
always share the same label. Consequently using one label for all samples
within a window inevitably lead to loss of information; ii) the testing phase
is constrained by the window size selected during training while the best
window size is difficult to tune in practice. We propose an efficient algorithm
that can predict the label of each sample which we call dense labeling in a
sequence of human activities of arbitrary length using a fully convolutional
network. In particular our approach overcomes the problems posed by the
sliding window step. Additionally our algorithm learns both the features and
classifier automatically. We release a new daily activity dataset based on a
wearable sensor with hospitalized patients. We conduct extensive experiments
and demonstrate that our proposed approach is able to outperform the
state-of-the-arts in terms of classification and label misalignment measures on
three challenging datasets: Opportunity Hand Gesture and our new dataset.",Rui Yao|Guosheng Lin|Qinfeng Shi|Damith Ranasinghe,cs.CV|cs.HC
2017-02-28T17:23:10Z,2017-02-20T19:22:21Z,http://arxiv.org/abs/1702.06151v1,http://arxiv.org/pdf/1702.06151v1,Developing a comprehensive framework for multimodal feature extraction,"Feature extraction is a critical component of many applied data science
workflows. In recent years rapid advances in artificial intelligence and
machine learning have led to an explosion of feature extraction tools and
services that allow data scientists to cheaply and effectively annotate their
data along a vast array of dimensions---ranging from detecting faces in images
to analyzing the sentiment expressed in coherent text. Unfortunately the
proliferation of powerful feature extraction services has been mirrored by a
corresponding expansion in the number of distinct interfaces to feature
extraction services. In a world where nearly every new service has its own API
documentation and/or client library data scientists who need to combine
diverse features obtained from multiple sources are often forced to write and
maintain ever more elaborate feature extraction pipelines. To address this
challenge we introduce a new open-source framework for comprehensive
multimodal feature extraction. Pliers is an open-source Python package that
supports standardized annotation of diverse data types (video images audio
and text) and is expressly with both ease-of-use and extensibility in mind.
Users can apply a wide range of pre-existing feature extraction tools to their
data in just a few lines of Python code and can also easily add their own
custom extractors by writing modular classes. A graph-based API enables rapid
development of complex feature extraction pipelines that output results in a
single standardized format. We describe the package's architecture detail its
major advantages over previous feature extraction toolboxes and use a sample
application to a large functional MRI dataset to illustrate how pliers can
significantly reduce the time and effort required to construct sophisticated
feature extraction workflows while increasing code clarity and maintainability.",Quinten McNamara|Alejandro de la Vega|Tal Yarkoni,cs.CV|cs.IR|cs.LG|cs.MM
2017-02-28T17:23:10Z,2017-02-20T18:04:31Z,http://arxiv.org/abs/1702.06086v1,http://arxiv.org/pdf/1702.06086v1,Label Distribution Learning Forests,"Label distribution learning (LDL) is a general learning framework which
assigns a distribution over a set of labels to an instance rather than a single
label or multiple labels. Current LDL methods have either restricted
assumptions on the expression form of the label distribution or limitations in
representation learning. This paper presents label distribution learning
forests (LDLFs) - a novel label distribution learning algorithm based on
differentiable decision trees which have several advantages: 1) Decision trees
have the potential to model any general form of label distributions by the
mixture of leaf node predictions. 2) The learning of differentiable decision
trees can be combined with representation learning e.g. to learn deep
features in an end-to-end manner. We define a distribution-based loss function
for forests enabling all the trees to be learned jointly and show that an
update function for leaf node predictions which guarantees a strict decrease
of the loss function can be derived by variational bounding. The effectiveness
of the proposed LDLFs is verified on two LDL problems including age estimation
and crowd opinion prediction on movies showing significant improvements to the
state-of-the-art LDL methods.",Wei Shen|Kai Zhao|Yilu Guo|Alan Yuille,cs.LG|cs.CV
2017-02-28T17:23:10Z,2017-02-20T17:59:38Z,http://arxiv.org/abs/1702.06085v1,http://arxiv.org/pdf/1702.06085v1,Synthesis versus analysis in patch-based image priors,"In global models/priors (for example using wavelet frames) there is a well
known analysis vs synthesis dichotomy in the way signal/image priors are
formulated. In patch-based image models/priors this dichotomy is also present
in the choice of how each patch is modeled. This paper shows that there is
another analysis vs synthesis dichotomy in terms of how the whole image is
related to the patches and that all existing patch-based formulations that
provide a global image prior belong to the analysis category. We then propose a
synthesis formulation where the image is explicitly modeled as being
synthesized by additively combining a collection of independent patches. We
formally establish that these analysis and synthesis formulations are not
equivalent in general and that both formulations are compatible with analysis
and synthesis formulations at the patch level. Finally we present an instance
of the alternating direction method of multipliers (ADMM) that can be used to
perform image denoising under the proposed synthesis formulation showing its
computational feasibility. Rather than showing the superiority of the synthesis
or analysis formulations the contributions of this paper is to establish the
existence of both alternatives thus closing the corresponding gap in the field
of patch-based image processing.",Mario A. T. Figueiredo,cs.CV|94A08|I.4.0
2017-02-28T17:23:10Z,2017-02-20T15:00:13Z,http://arxiv.org/abs/1702.05993v1,http://arxiv.org/pdf/1702.05993v1,An Extended Framework for Marginalized Domain Adaptation,"We propose an extended framework for marginalized domain adaptation aimed at
addressing unsupervised supervised and semi-supervised scenarios. We argue
that the denoising principle should be extended to explicitly promote
domain-invariant features as well as help the classification task. Therefore we
propose to jointly learn the data auto-encoders and the target classifiers.
First in order to make the denoised features domain-invariant we propose a
domain regularization that may be either a domain prediction loss or a maximum
mean discrepancy between the source and target data. The noise marginalization
in this case is reduced to solving the linear matrix system $AX=B$ which has a
closed-form solution. Second in order to help the classification we include a
class regularization term. Adding this component reduces the learning problem
to solving a Sylvester linear matrix equation $AX+BX=C$ for which an efficient
iterative procedure exists as well. We did an extensive study to assess how
these regularization terms improve the baseline performance in the three domain
adaptation scenarios and present experimental results on two image and one text
benchmark datasets conventionally used for validating domain adaptation
methods. We report our findings and comparison with state-of-the-art methods.",Gabriela Csurka|Boris Chidlovski|Stephane Clinchant|Sophia Michel,cs.CV|cs.LG
2017-02-28T17:23:10Z,2017-02-23T15:02:59Z,http://arxiv.org/abs/1702.05970v2,http://arxiv.org/pdf/1702.05970v2,"Automatic Liver and Tumor Segmentation of CT and MRI Volumes using
  Cascaded Fully Convolutional Neural Networks","Automatic segmentation of the liver and hepatic lesions is an important step
towards deriving quantitative biomarkers for accurate clinical diagnosis and
computer-aided decision support systems. This paper presents a method to
automatically segment liver and lesions in CT and MRI abdomen images using
cascaded fully convolutional neural networks (CFCNs) enabling the segmentation
of a large-scale medical trial or quantitative image analysis. We train and
cascade two FCNs for a combined segmentation of the liver and its lesions. In
the first step we train a FCN to segment the liver as ROI input for a second
FCN. The second FCN solely segments lesions within the predicted liver ROIs of
step 1. CFCN models were trained on an abdominal CT dataset comprising 100
hepatic tumor volumes. Validations on further datasets show that CFCN-based
semantic liver and lesion segmentation achieves Dice scores over 94% for liver
with computation times below 100s per volume. We further experimentally
demonstrate the robustness of the proposed method on an 38 MRI liver tumor
volumes and the public 3DIRCAD dataset.",Patrick Ferdinand Christ|Florian Ettlinger|Felix Grün|Mohamed Ezzeldin A. Elshaera|Jana Lipkova|Sebastian Schlecht|Freba Ahmaddy|Sunil Tatavarty|Marc Bickel|Patrick Bilic|Markus Rempfler|Felix Hofmann|Melvin D Anastasi|Seyed-Ahmad Ahmadi|Georgios Kaissis|Julian Holch|Wieland Sommer|Rickmer Braren|Volker Heinemann|Bjoern Menze,cs.CV|cs.AI
2017-02-28T17:23:10Z,2017-02-20T13:21:20Z,http://arxiv.org/abs/1702.05958v1,http://arxiv.org/pdf/1702.05958v1,Reflection Separation Using Guided Annotation,"Photographs taken through a glass surface often contain an approximately
linear superposition of reflected and transmitted layers. Decomposing an image
into these layers is generally an ill-posed task and the use of an additional
image prior and user provided cues is presently necessary in order to obtain
good results. Current annotation approaches rely on a strong sparsity
assumption. For images with significant texture this assumption does not
typically hold thus rendering the annotation process unviable.
  In this paper we show that using a Gaussian Mixture Model patch prior the
correct local decomposition can almost always be found as one of 100 likely
modes of the posterior. Thus the user need only choose one of these modes in a
sparse set of patches and the decomposition may then be completed
automatically. We demonstrate the performance of our method using synthesized
and real reflection images.",Ofer Springer|Yair Weiss,cs.CV
