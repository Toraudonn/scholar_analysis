2017-02-28T17:23:14Z,2017-02-27T18:23:28Z,http://arxiv.org/abs/1702.08415v1,http://arxiv.org/pdf/1702.08415v1,An SDP-Based Algorithm for Linear-Sized Spectral Sparsification,"For any undirected and weighted graph $G=(VEw)$ with $n$ vertices and $m$
edges we call a sparse subgraph $H$ of $G$ with proper reweighting of the
edges a $(1+\varepsilon)$-spectral sparsifier if \[
(1-\varepsilon)x^{\intercal}L_Gx\leq x^{\intercal} L_{H} x\leq (1+\varepsilon)
x^{\intercal} L_Gx \] holds for any $x\in\mathbb{R}^n$ where $L_G$ and $L_{H}$
are the respective Laplacian matrices of $G$ and $H$. Noticing that $\Omega(m)$
time is needed for any algorithm to construct a spectral sparsifier and a
spectral sparsifier of $G$ requires $\Omega(n)$ edges a natural question is to
investigate for any constant $\varepsilon$ if a $(1+\varepsilon)$-spectral
sparsifier of $G$ with $O(n)$ edges can be constructed in $\tilde{O}(m)$ time
where the $\tilde{O}$ notation suppresses polylogarithmic factors. All previous
constructions on spectral sparsification require either super-linear number of
edges or $m^{1+\Omega(1)}$ time.
  In this work we answer this question affirmatively by presenting an algorithm
that for any undirected graph $G$ and $\varepsilon>0$ outputs a
$(1+\varepsilon)$-spectral sparsifier of $G$ with $O(n/\varepsilon^2)$ edges in
$\tilde{O}(m/\varepsilon^{O(1)})$ time. Our algorithm is based on three novel
techniques: (1) a new potential function which is much easier to compute yet
has similar guarantees as the potential functions used in previous references;
(2) an efficient reduction from a two-sided spectral sparsifier to a one-sided
spectral sparsifier; (3) constructing a one-sided spectral sparsifier by a
semi-definite program.",Yin Tat Lee|He Sun,cs.DS|cs.LG
2017-02-28T17:23:14Z,2017-02-27T14:25:36Z,http://arxiv.org/abs/1702.08299v1,http://arxiv.org/pdf/1702.08299v1,Independent Set Size Approximation in Graph Streams,"We study the problem of estimating the size of independent sets in a graph
$G$ defined by a stream of edges. Our approach relies on the Caro-Wei bound
which expresses the desired quantity in terms of a sum over nodes of the
reciprocal of their degrees denoted by $\beta(G)$. Our results show that
$\beta(G)$ can be approximated accurately based on a provided lower bound on
$\beta$. Stronger results are possible when the edges are promised to arrive
grouped by an incident node. In this setting we obtain a value that is at most
a logarithmic factor below the true value of $\beta$ and no more than the true
independent set size. To justify the form of this bound we also show an
$\Omega(n/\beta)$ lower bound on any algorithm that approximates $\beta$ up to
a constant factor.",Graham Cormode|Jacques Dark|Christian Konrad,cs.DS
2017-02-28T17:23:14Z,2017-02-27T12:03:01Z,http://arxiv.org/abs/1702.08248v1,http://arxiv.org/pdf/1702.08248v1,Scalable and Distributed Clustering via Lightweight Coresets,"Coresets are compact representations of data sets such that models trained on
a coreset are provably competitive with models trained on the full data set. As
such they have been successfully used to scale up clustering models to massive
data sets. While existing approaches generally only allow for multiplicative
approximation errors we propose a novel notion of coresets called lightweight
coresets that allows for both multiplicative and additive errors. We provide a
single algorithm to construct light-weight coresets for k-Means clustering
Bregman clustering and maximum likelihood estimation of Gaussian mixture
models. The algorithm is substantially faster than existing constructions
embarrassingly parallel and resulting coresets are smaller. In an extensive
experimental evaluation we demonstrate that the proposed method outperforms
existing coreset constructions.",Olivier Bachem|Mario Lucic|Andreas Krause,stat.ML|cs.DC|cs.DS|cs.LG|stat.CO
2017-02-28T17:23:14Z,2017-02-27T11:55:20Z,http://arxiv.org/abs/1702.08247v1,http://arxiv.org/pdf/1702.08247v1,"On the Expected Value of the Determinant of Random Sum of Rank-One
  Matrices","We present a simple yet useful result about the expected value of the
determinant of random sum of rank-one matrices. Computing such expectations in
general may involve a sum over exponentially many terms. Nevertheless we show
that an interesting and useful class of such expectations that arise in e.g.
D-optimal estimation and random graphs can be computed efficiently via
computing a single determinant.",Kasra Khosoussi,cs.DS|math.PR
2017-02-28T17:23:14Z,2017-02-27T09:52:17Z,http://arxiv.org/abs/1702.08207v1,http://arxiv.org/pdf/1702.08207v1,Approximation Strategies for Generalized Binary Search in Weighted Trees,"We consider the following generalization of the binary search problem. A
search strategy is required to locate an unknown target node $t$ in a given
tree $T$. Upon querying a node $v$ of the tree the strategy receives as a
reply an indication of the connected component of $T\setminus\{v\}$ containing
the target $t$. The cost of querying each node is given by a known non-negative
weight function and the considered objective is to minimize the total query
cost for a worst-case choice of the target. Designing an optimal strategy for a
weighted tree search instance is known to be strongly NP-hard in contrast to
the unweighted variant of the problem which can be solved optimally in linear
time. Here we show that weighted tree search admits a quasi-polynomial time
approximation scheme: for any $0 \textless{} \varepsilon \textless{} 1$ there
exists a $(1+\varepsilon)$-approximation strategy with a computation time of
$n^{O(\log n / \varepsilon^2)}$. Thus the problem is not APX-hard unless $NP
\subseteq DTIME(n^{O(\log n)})$. By applying a generic reduction we obtain as
a corollary that the studied problem admits a polynomial-time $O(\sqrt{\log
n})$-approximation. This improves previous $\hat O(\log n)$-approximation
approaches where the $\hat O$-notation disregards $O(\mathrm{poly}\log\log
n)$-factors.",Dariusz Dereniowski|Adrian Kosowski|Przemyslaw Uznanski|Mengchuan Zou,cs.DS
2017-02-28T17:23:14Z,2017-02-26T00:51:38Z,http://arxiv.org/abs/1702.07961v1,http://arxiv.org/pdf/1702.07961v1,An Efficient Multiway Mergesort for GPU Architectures,"Sorting is a primitive operation that is a building block for countless
algorithms. As such it is important to design sorting algorithms that approach
peak performance on a range of hardware architectures. Graphics Processing
Units (GPUs) are particularly attractive architectures as they provides massive
parallelism and computing power. However the intricacies of their compute and
memory hierarchies make designing GPU-efficient algorithms challenging. In this
work we present GPU Multiway Mergesort (MMS) a new GPU-efficient multiway
mergesort algorithm. MMS employs a new partitioning technique that exposes the
parallelism needed by modern GPU architectures. To the best of our knowledge
MMS is the first sorting algorithm for the GPU that is asymptotically optimal
in terms of global memory accesses and that is completely free of shared memory
bank conflicts. We realize an initial implementation of MMS evaluate its
performance on three modern GPU architectures and compare it to competitive
implementations available in state-of-the- art GPU libraries. Despite these
implementations being highly optimized MMS compares favorably achieving
performance improvements for most random inputs. Furthermore unlike MMS
state-of-the-art algorithms are susceptible to bank conflicts. We find that for
certain inputs that cause these algorithms to incur large numbers of bank
conflicts MMS can achieve a 33.7% performance improvement over its fastest
competitor. Overall even though its current implementation is not fully
optimized due to its efficient use of the memory hierarchy MMS outperforms
the fastest comparison-based sorting implementations available to date.",Henri Casanova|John Iacono|Ben Karsin|Nodari Sitchinava|Volker Weichert,cs.DS
2017-02-28T17:23:14Z,2017-02-25T04:13:22Z,http://arxiv.org/abs/1702.07832v1,http://arxiv.org/pdf/1702.07832v1,Constructing Adjacency Arrays from Incidence Arrays,"Graph construction a fundamental operation in a data processing pipeline is
typically done by multiplying the incidence array representations of a graph
$\mathbf{E}_\mathrm{in}$ and $\mathbf{E}_\mathrm{out}$ to produce an adjacency
array of the graph $\mathbf{A}$ that can be processed with a variety of
algorithms. This paper provides the mathematical criteria to determine if the
product $\mathbf{A} = \mathbf{E}^{\sf T}_\mathrm{out}\mathbf{E}_\mathrm{in}$
will have the required structure of the adjacency array of the graph. The
values in the resulting adjacency array are determined by the corresponding
addition $\oplus$ and multiplication $\otimes$ operations used to perform the
array multiplication. Illustrations of the various results possible from
different $\oplus$ and $\otimes$ operations are provided using a small
collection of popular music metadata.",Hayden Jananthan|Karia Dibert|Jeremy Kepner,cs.DS|cs.DM|math.CO
2017-02-28T17:23:14Z,2017-02-25T01:24:03Z,http://arxiv.org/abs/1702.07815v1,http://arxiv.org/pdf/1702.07815v1,"Subquadratic Algorithms for the Diameter and the Sum of Pairwise
  Distances in Planar Graphs","We show how to compute for $n$-vertex planar graphs in $O(n^{11/6}{\rm
polylog}(n))$ expected time the diameter and the sum of the pairwise distances.
The algorithms work for directed graphs with real weights and no negative
cycles. In $O(n^{15/8}{\rm polylog}(n))$ expected time we can also compute the
number of pairs of vertices at distance smaller than a given threshold. These
are the first algorithms for these problems using time $O(n^c)$ for some
constant $c<2$ even when restricted to undirected unweighted planar graphs.",Sergio Cabello,cs.DS
2017-02-28T17:23:14Z,2017-02-24T18:59:08Z,http://arxiv.org/abs/1702.07709v1,http://arxiv.org/pdf/1702.07709v1,Computationally Efficient Robust Estimation of Sparse Functionals,"Many conventional statistical procedures are extremely sensitive to seemingly
minor deviations from modeling assumptions. This problem is exacerbated in
modern high-dimensional settings where the problem dimension can grow with and
possibly exceed the sample size. We consider the problem of robust estimation
of sparse functionals and provide a computationally and statistically
efficient algorithm in the high-dimensional setting. Our theory identifies a
unified set of deterministic conditions under which our algorithm guarantees
accurate recovery. By further establishing that these deterministic conditions
hold with high-probability for a wide range of statistical models our theory
applies to many problems of considerable interest including sparse mean and
covariance estimation; sparse linear regression; and sparse generalized linear
models.",Simon S. Du|Sivaraman Balakrishnan|Aarti Singh,stat.ML|cs.DS|cs.LG
2017-02-28T17:23:14Z,2017-02-24T18:30:03Z,http://arxiv.org/abs/1702.07696v1,http://arxiv.org/pdf/1702.07696v1,An Efficient Data Structure for Dynamic Two-Dimensional Reconfiguration,"In the presence of dynamic insertions and deletions into a partially
reconfigurable FPGA fragmentation is unavoidable. This poses the challenge of
developing efficient approaches to dynamic defragmentation and reallocation.
One key aspect is to develop efficient algorithms and data structures that
exploit the two-dimensional geometry of a chip instead of just one. We propose
a new method for this task based on the fractal structure of a quadtree which
allows dynamic segmentation of the chip area along with dynamically adjusting
the necessary communication infrastructure. We describe a number of algorithmic
aspects and present different solutions. We also provide a number of basic
simulations that indicate that the theoretical worst-case bound may be
pessimistic.",Sándor P. Fekete|Jan-Marc Reinhardt|Christian Scheffer,cs.DS|F.2.2
2017-02-28T17:23:18Z,2017-02-24T17:18:02Z,http://arxiv.org/abs/1702.07669v1,http://arxiv.org/pdf/1702.07669v1,On problems equivalent to (min+)-convolution,"In the recent years significant progress has been made in explaining
apparent hardness of improving over naive solutions for many fundamental
polynomially solvable problems. This came in the form of conditional lower
bounds - reductions to one of problems assumed to be hard. These include 3SUM
All-Pairs Shortest Paths SAT and Orthogonal Vectors and others.
  In the (min+)-convolution problem the goal is to compute a sequence
$(c[i])^{n-1}_{i=0}$ where $c[k] = \min_{i=0\ldotsk} \{a[i]+b[k-i]\}$ given
sequences $(a[i])^{n-1}_{i=0}$ and $(b[i])_{i=0}^{n-1}$. This can easily be
done in $O(n^2)$ time but no $O(n^{2-\varepsilon})$ algorithm is known for
$\varepsilon > 0$. In this paper we undertake a systematic study of the
(min+)-convolution problem as a hardness assumption.
  As the first step we establish equivalence of this problem to a group of
other problems including variants of the classic knapsack problem and problems
related to subadditive sequences. The (min+)-convolution has been used as a
building block in algorithms for many problems notably problems in
stringology. It has also already appeared as an ad hoc hardness assumption. We
investigate some of these connections and provide new reductions and other
results.",Marek Cygan|Marcin Mucha|Karol Węgrzycki|Michał Włodarczyk,cs.DS|cs.CC|F.1.3; F.2
2017-02-28T17:23:18Z,2017-02-24T17:09:22Z,http://arxiv.org/abs/1702.07665v1,http://arxiv.org/pdf/1702.07665v1,Truthful Mechanisms for Delivery with Mobile Agents,"We study the game-theoretic task of selecting mobile agents to deliver
multiple items on a network. An instance is given by $m$ messages (physical
objects) which have to be transported between specified source-target pairs in
a weighted undirected graph and $k$ mobile heterogeneous agents each being
able to transport one message at a time. Following a recent model by
[B\""artschi et al. 2016] each agent $i$ consumes energy proportional to the
distance it travels in the graph where the different rates of energy
consumption are given by weight factors $w_i$. We are interested in optimizing
or approximating the total energy consumption over all selected agents.
  Unlike previous research we assume the weights to be private values known
only to the respective agents. We present three different mechanisms which
select route and pay the agents in a truthful way that guarantees voluntary
participation of the agents while approximating the optimum energy consumption
by a constant factor. To this end we analyze a previous structural result and
an approximation algorithm given by [B\""artschi et al. 2017]. Finally we show
that for some instances in the case of a single message ($m=1$) the sum of the
payments can be bounded in terms of the optimum as well.",Andreas Bärtschi|Daniel Graf|Paolo Penna,cs.GT|cs.DS
2017-02-28T17:23:18Z,2017-02-24T13:43:20Z,http://arxiv.org/abs/1702.07578v1,http://arxiv.org/pdf/1702.07578v1,Fast and Simple Parallel Wavelet Tree and Matrix Construction,"The wavelet tree (Grossi et al. [SODA 2003]) and wavelet matrix (Claude et
al. [Inf. Syst. 47:15--32 2015]) are compact indices for texts over an
alphabet $[0\sigma)$ that support rank select and access queries in $O(\lg
\sigma)$ time. We first present new practical sequential and parallel
algorithms for wavelet matrix construction. Their unifying characteristics is
that they construct the wavelet matrix bottom-up i.e. they compute the last
level first. We also show that this bottom-up construction can easily be
adapted to wavelet trees. In practice our best sequential algorithm is up to
twice as fast as the currently fastest sequential construction algorithm
(serialWT) simultaneously saving a factor of 2 in space. On 4 cores our best
parallel algorithm is at least twice as fast as the currently fastest parallel
algorithm (recWT) while also using less space. This scales up to 32 cores
where we are about equally fast as recWT but still use only about 75% of the
space. An additional theoretical result shows how to adapt any wavelet tree
construction algorithm to the wavelet matrix in the same (asymptotic) time
using only little extra space.",Johannes Fischer|Florian Kurpicz,cs.DS
2017-02-28T17:23:18Z,2017-02-24T13:41:29Z,http://arxiv.org/abs/1702.07577v1,http://arxiv.org/pdf/1702.07577v1,Compression with the tudocomp Framework,"We present a framework facilitating the implementation and comparison of text
compression algorithms. We evaluate its features by a case study on two novel
compression algorithms based on the Lempel-Ziv compression schemes that perform
well on highly repetitive texts.",Patrick Dinklage|Johannes Fischer|Dominik Köppl|Marvin Löbel|Kunihiko Sadakane,cs.DS
2017-02-28T17:23:18Z,2017-02-24T03:57:57Z,http://arxiv.org/abs/1702.07458v1,http://arxiv.org/pdf/1702.07458v1,Small-space encoding LCE data structure with constant-time queries,"The \emph{longest common extension} (\emph{LCE}) problem is to preprocess a
given string $w$ of length $n$ so that the length of the longest common prefix
between suffixes of $w$ that start at any two given positions is answered
quickly. In this paper we present a data structure of $O(z \tau^2 +
\frac{n}{\tau})$ words of space which answers LCE queries in $O(1)$ time and
can be built in $O(n \log \sigma)$ time where $1 \leq \tau \leq \sqrt{n}$ is a
parameter $z$ is the size of the Lempel-Ziv 77 factorization of $w$ and
$\sigma$ is the alphabet size. This is an \emph{encoding} data structure i.e.
it does not access the input string $w$ when answering queries and thus $w$ can
be deleted after preprocessing. On top of this main result we obtain further
results using (variants of) our LCE data structure which include the
following:
  - For highly repetitive strings where the $z\tau^2$ term is dominated by
$\frac{n}{\tau}$ we obtain a \emph{constant-time and sub-linear space} LCE
query data structure.
  - Even when the input string is not well compressible via Lempel-Ziv 77
factorization we still can obtain a \emph{constant-time and sub-linear space}
LCE data structure for suitable $\tau$ and for $\sigma \leq 2^{o(\log n)}$.
  - The time-space trade-off lower bounds for the LCE problem by Bille et al.
[J. Discrete Algorithms 25:42-50 2014] and by Kosolobov [CoRR
abs/1611.02891 2016] can be ""surpassed"" in some cases with our LCE data
structure.",Yuka Tanimura|Takaaki Nishimoto|Hideo Bannai|Shunsuke Inenaga|Masayuki Takeda,cs.DS
2017-02-28T17:23:18Z,2017-02-24T01:28:11Z,http://arxiv.org/abs/1702.07435v1,http://arxiv.org/pdf/1702.07435v1,Capacitated Center Problems with Two-Sided Bounds and Outliers,"In recent years the capacitated center problems have attracted a lot of
research interest. Given a set of vertices $V$ we want to find a subset of
vertices $S$ called centers such that the maximum cluster radius is
minimized. Moreover each center in $S$ should satisfy some capacity
constraint which could be an upper or lower bound on the number of vertices it
can serve. Capacitated $k$-center problems with one-sided bounds (upper or
lower) have been well studied in previous work and a constant factor
approximation was obtained.
  We are the first to study the capacitated center problem with both capacity
lower and upper bounds (with or without outliers). We assume each vertex has a
uniform lower bound and a non-uniform upper bound. For the case of opening
exactly $k$ centers we note that a generalization of a recent LP approach can
achieve constant factor approximation algorithms for our problems. Our main
contribution is a simple combinatorial algorithm for the case where there is no
cardinality constraint on the number of open centers. Our combinatorial
algorithm is simpler and achieves better constant approximation factor compared
to the LP approach.",Hu Ding|Lunjia Hu|Lingxiao Huang|Jian Li,cs.DS
2017-02-28T17:23:18Z,2017-02-23T21:49:52Z,http://arxiv.org/abs/1702.07403v1,http://arxiv.org/pdf/1702.07403v1,Making Asynchronous Distributed Computations Robust to Noise,"We consider the problem of making distributed computations robust to noise
in particular to worst-case (adversarial) corruptions of messages. We give a
general distributed interactive coding scheme which simulates any asynchronous
distributed protocol while tolerating an optimal corruption of a $\Theta(1/n)$
fraction of all messages while incurring a moderate blowup of $O(n\log^2 n)$ in
the communication complexity.
  Our result is the first fully distributed interactive coding scheme in which
the topology of the communication network is not known in advance. Prior work
required either a coordinating node to be connected to all other nodes in the
network or assumed a synchronous network in which all nodes already know the
complete topology of the network.",Keren Censor-Hillel|Ran Gelles|Bernhard Haeupler,cs.DS|cs.DC
2017-02-28T17:23:18Z,2017-02-23T16:59:25Z,http://arxiv.org/abs/1702.07292v1,http://arxiv.org/pdf/1702.07292v1,Network Construction with Ordered Constraints,"In this paper we study the problem of constructing a network by observing
ordered connectivity constraints which we define herein. These ordered
constraints are made to capture realistic properties of real-world problems
that are not reflected in previous more general models. We give hardness of
approximation results and nearly-matching upper bounds for the offline problem
and we study the online problem in both general graphs and restricted
sub-classes. In the online problem for general graphs we give exponentially
better upper bounds than exist for algorithms for general connectivity
problems. For the restricted classes of stars and paths we are able to find
algorithms with optimal competitive ratios the latter of which involve
analysis using a potential function defined over pq-trees.",Yi Huang|Mano Vikash Janardhanan|Lev Reyzin,cs.DS
2017-02-28T17:23:18Z,2017-02-23T10:57:56Z,http://arxiv.org/abs/1702.07172v1,http://arxiv.org/pdf/1702.07172v1,Tight Bounds for Online Coloring of Basic Graph Classes,"We resolve a number of long-standing open problems in online graph coloring.
More specifically we develop tight lower bounds on the performance of online
algorithms for fundamental graph classes. An important contribution is that our
bounds also hold for randomized online algorithms for which hardly any results
were known. Technically we construct lower bounds for chordal graphs. The
constructions then allow us to derive results on the performance of randomized
online algorithms for the following further graph classes: trees planar
bipartite inductive bounded-treewidth and disk graphs. It shows that the best
competitive ratio of both deterministic and randomized online algorithms is
$\Theta(\log n)$ where $n$ is the number of vertices of a graph. Furthermore
we prove that this guarantee cannot be improved if an online algorithm has a
lookahead of size $O(n/\log n)$ or access to a reordering buffer of size
$n^{1-\epsilon}$ for any $0<\epsilon\leq 1$. A consequence of our results is
that for all of the above mentioned graph classes except bipartite graphs the
natural $\textit{First Fit}$ coloring algorithm achieves an optimal
performance up to constant factors among deterministic and randomized online
algorithms.",Susanne Albers|Sebastian Schraink,cs.DS|cs.DM
2017-02-28T17:23:18Z,2017-02-23T08:35:45Z,http://arxiv.org/abs/1702.07134v1,http://arxiv.org/pdf/1702.07134v1,Diverse Weighted Bipartite b-Matching,"Bipartite matching where agents on one side of a market are matched to
agents or items on the other is a classical problem in computer science and
economics with widespread application in healthcare education advertising
and general resource allocation. A practitioner's goal is typically to maximize
a matching market's economic efficiency possibly subject to some fairness
requirements that promote equal access to resources. A natural balancing act
exists between fairness and efficiency in matching markets and has been the
subject of much research.
  In this paper we study a complementary goal---balancing diversity and
efficiency---in a generalization of bipartite matching where agents on one side
of the market can be matched to sets of agents on the other. Adapting a
classical definition of the diversity of a set we propose a quadratic
programming-based approach to solving a supermodular minimization problem that
balances diversity and total weight of the solution. We also provide a scalable
greedy algorithm with theoretical performance bounds. We then define the price
of diversity a measure of the efficiency loss due to enforcing diversity and
give a worst-case theoretical bound. Finally we demonstrate the efficacy of
our methods on three real-world datasets and show that the price of diversity
is not bad in practice.",Faez Ahmed|John P. Dickerson|Mark Fuge,cs.DS
2017-02-28T17:23:22Z,2017-02-22T22:43:45Z,http://arxiv.org/abs/1702.07032v1,http://arxiv.org/pdf/1702.07032v1,On the Complexity of Bundle-Pricing and Simple Mechanisms,"We show that the problem of finding an optimal bundle-pricing for a single
additive buyer is #P-hard even when the distributions have support size 2 for
each item and the optimal solution is guaranteed to be a simple one: the seller
picks a price for the grand bundle and a price for each individual item; the
buyer can purchase either the grand bundle at the given price or any bundle of
items at their total individual prices. We refer to this simple and natural
family of pricing schemes as discounted item-pricings. In addition to the
hardness result we show that when the distributions are i.i.d. with support
size 2 a discounted item-pricing can achieve the optimal revenue obtainable by
lottery-pricings and it can be found in polynomial time.",Xi Chen|George Matikas|Dimitris Paparas|Mihalis Yannakakis,cs.GT|cs.CC|cs.DS
2017-02-28T17:23:22Z,2017-02-22T20:59:22Z,http://arxiv.org/abs/1702.07002v1,http://arxiv.org/pdf/1702.07002v1,"Breaking the Bonds of Submodularity: Empirical Estimation of
  Approximation Ratios for Monotone Non-Submodular Greedy Maximization","While greedy algorithms have long been observed to perform well on a wide
variety of problems up to now approximation ratios have only been known for
their application to problems having submodular objective functions $f$. Since
many practical problems have non-submodular $f$ there is a critical need to
devise new techniques to bound the performance of greedy algorithms in the case
of non-submodularity.
  Our primary contribution is the introduction of a novel technique for
estimating the approximation ratio of the greedy algorithm for maximization of
monotone non-decreasing functions based on the curvature of $f$ without relying
on the submodularity constraint. We show that this technique reduces to the
classical $(1 - 1/e)$ ratio for submodular functions. Furthermore we develop
an extension of this ratio to the adaptive greedy algorithm which allows
applications to non-submodular stochastic maximization problems. This notably
extends support to applications modeling incomplete data with uncertainty.
Finally we use this new technique to derive a $(1 - 1/\sqrt{e})$ ratio for a
popular problem Robust Influence Maximization which is non-submodular and
$1/2$ for Adaptive Max-Crawling which is adaptive non-submodular.",J. David Smith|My T. Thai,cs.DS|cs.DM|F.2.2; G.2
2017-02-28T17:23:22Z,2017-02-27T05:09:46Z,http://arxiv.org/abs/1702.06969v2,http://arxiv.org/pdf/1702.06969v2,Approximating Unique Games Using Low Diameter Graph Decomposition,"We design approximation algorithms for Unique Games when the constraint graph
admits good low diameter graph decomposition. For the ${\sf Max2Lin}_k$ problem
in $K_r$-minor free graphs when there is an assignment satisfying
$1-\varepsilon$ fraction of constraints we present an algorithm that produces
an assignment satisfying $1-O(r\varepsilon)$ fraction of constraints with the
approximation ratio independent of the alphabet size. A corollary is an
improved approximation algorithm for the ${\sf MaxCut}$ problem for $K_r$-minor
free graphs. For general Unique Games in $K_r$-minor free graphs we provide
another algorithm that produces an assignment satisfying $1-O(r
\sqrt{\varepsilon})$ fraction of constraints.
  Our approach is to round a linear programming relaxation to find a minimum
subset of edges that intersects all the inconsistent cycles. We show that it is
possible to apply the low diameter graph decomposition technique on the
constraint graph directly rather than to work on the label extended graph as
in previous algorithms for Unique Games. The same approach applies when the
constraint graph is of genus $g$ and we get similar results with $r$ replaced
by $\log g$ in the ${\sf Max2Lin}_k$ problem and by $\sqrt{\log g}$ in the
general problem. The former result generalizes the result of Gupta-Talwar for
Unique Games in the ${\sf Max2Lin}_k$ case and the latter result generalizes
the result of Trevisan for general Unique Games.",Vedat Levi Alev|Lap Chi Lau,cs.DS
2017-02-28T17:23:22Z,2017-02-22T15:03:08Z,http://arxiv.org/abs/1702.06829v1,http://arxiv.org/pdf/1702.06829v1,A Simple Convex Layers Algorithm,"Given a set of $n$ points $P$ in the plane the first layer $L_1$ of $P$ is
formed by the points that appear on $P$'s convex hull. In general a point
belongs to layer $L_i$ if it lies on the convex hull of the set $P \setminus
\bigcup_{j<i}\{L_j\}$. The \emph{convex layers problem} is to compute the
convex layers $L_i$. Existing algorithms for this problem either do not achieve
the optimal $\mathcal{O}\left(n\log n\right)$ runtime and linear space or are
overly complex and difficult to apply in practice. We propose a new algorithm
that is both optimal and simple. The simplicity is achieved by independently
computing four sets of monotone convex chains in $\mathcal{O}\left(n\log
n\right)$ time and linear space. These are then merged in
$\mathcal{O}\left(n\log n\right)$ time.",Raimi A. Rufai|Dana S. Richard,cs.CG|cs.DS|68W99|I.3.5
2017-02-28T17:23:22Z,2017-02-22T09:35:30Z,http://arxiv.org/abs/1702.06723v1,http://arxiv.org/pdf/1702.06723v1,Compact linear programs for 2SAT,"For each integer $n$ we present an explicit formulation of a compact linear
program with $O(n^3)$ variables and constraints which determines the
satisfiability of any 2SAT formula with $n$ boolean variables by a single
linear optimization. This contrasts with the fact that the natural polytope for
this problem formed from the convex hull of all satisfiable formulas and their
satisfying assignments has superpolynomial extension complexity. Our
formulation is based on multicommodity flows. We also discuss connections of
these results to the stable matching problem.",David Avis|Hans Raj Tiwary,math.OC|cs.DM|cs.DS
2017-02-28T17:23:22Z,2017-02-21T23:02:59Z,http://arxiv.org/abs/1702.06614v1,http://arxiv.org/pdf/1702.06614v1,Double Threshold Digraphs,"A semiorder is a model of preference relations where each element $x$ is
associated with a utility value $\alpha(x)$ and there is a threshold $t$ such
that $y$ is preferred to $x$ iff $\alpha(y) > \alpha(x)+t$. These are motivated
by the notion that there is some uncertainty in the utility values we assign an
object or that a subject may be unable to distinguish a preference between
objects whose values are close. However they fail to model the well-known
phenomenon that preferences are not always transitive. Also if we are
uncertain of the utility values it is not logical that preference is
determined absolutely by a comparison of them with an exact threshold. We
propose a new model in which there are two thresholds $t_1$ and $t_2$; if the
difference $\alpha(y) - \alpha(x)$ less than $t_1$ then $y$ is not preferred
to $x$; if the difference is greater than $t_2$ then $y$ is preferred to $x$;
if it is between $t_1$ and $t_2$ then then $y$ may or may not be preferred to
$x$. We call such a relation a double-threshold semiorder and the
corresponding directed graph $G = (VE)$ a double threshold digraph. Every
directed acyclic graph is a double threshold graph; bounds on $t_2/t_1$ give a
nested hierarchy of subclasses of the directed acyclic graphs. In this paper we
characterize the subclasses in terms of forbidden subgraphs and give
algorithms for finding an assignment of of utility values that explains the
relation in terms of a given $(t_1t_2)$ or else produces a forbidden subgraph
and finding the minimum value $\lambda$ of $t_2/t_1$ that is satisfiable for a
given directed acyclic graph. We show that $\lambda$ gives a measure of the
complexity of a directed acyclic graph with respect to several optimization
problems that are NP-hard on arbitrary directed acyclic graphs.",Peter Hamburger|Ross M. McConnell|Attila Pór|Jeremy P. Spinrad,cs.DS
2017-02-28T17:23:22Z,2017-02-21T19:02:49Z,http://arxiv.org/abs/1702.06548v1,http://arxiv.org/pdf/1702.06548v1,Parameterized Aspects of Triangle Enumeration,"Listing all triangles in an undirected graph is a fundamental graph primitive
with numerous applications. It is trivially solvable in time cubic in the
number of vertices. It has seen a significant body of work contributing to both
theoretical aspects (e.g. lower and upper bounds on running time adaption to
new computational models) as well practical aspects (e.g. algorithms tuned for
large graphs). Motivated by the fact that the worst-case running time is cubic
we perform a systematic parameterized complexity study mostly of triangle
enumeration providing both positive results (new enumerative kernelizations
""subcubic"" parameterized solving algorithms) as well as negative results
(uselessness (in terms of possibility of ""faster"" parameterized algorithms) of
certain parameters such as domination or chromatic number).",Matthias Bentert|Till Fluschnik|André Nichterlein|Rolf Niedermeier,cs.DS|cs.DM
2017-02-28T17:23:22Z,2017-02-21T18:13:40Z,http://arxiv.org/abs/1702.06503v1,http://arxiv.org/pdf/1702.06503v1,When can Graph Hyperbolicity be computed in Linear Time?,"Hyperbolicity measures in terms of (distance) metrics how close a given
graph is to being a tree. Due to its relevance in modeling real-world networks
hyperbolicity has seen intensive research over the last years. Unfortunately
the best known algorithms for computing the hyperbolicity number of a graph
(the smaller the more tree-like) have running time $O(n^4)$ where $n$ is the
number of graph vertices. Exploiting the framework of parameterized complexity
analysis we explore possibilities for ""linear-time FPT"" algorithms to compute
hyperbolicity. For instance we show that hyperbolicity can be computed in time
$O(2^{O(k)} + n +m)$ ($m$ being the number of graph edges) while at the same
time unless the SETH fails there is no $2^{o(k)}n^2$-time algorithm.",Till Fluschnik|Christian Komusiewicz|George B. Mertzios|André Nichterlein|Rolf Niedermeier|Nimrod Talmon,"cs.CC|cs.DS|05C12, 68R10, 68Q25, 68Q17|F.2.2; G.2.2"
2017-02-28T17:23:22Z,2017-02-21T13:15:27Z,http://arxiv.org/abs/1702.06370v1,http://arxiv.org/pdf/1702.06370v1,Answering Conjunctive Queries under Updates,"We consider the task of enumerating and counting answers to $k$-ary
conjunctive queries against relational databases that may be updated by
inserting or deleting tuples. We exhibit a new notion of q-hierarchical
conjunctive queries and show that these can be maintained efficiently in the
following sense. During a linear time preprocessing phase we can build a data
structure that enables constant delay enumeration of the query results; and
when the database is updated we can update the data structure and restart the
enumeration phase within constant time. For the special case of self-join free
conjunctive queries we obtain a dichotomy: if a query is not q-hierarchical
then query enumeration with sublinear$^\ast$ delay and sublinear update time
(and arbitrary preprocessing time) is impossible.
  For answering Boolean conjunctive queries and for the more general problem of
counting the number of solutions of k-ary queries we obtain complete
dichotomies: if the query's homomorphic core is q-hierarchical then size of
the the query result can be computed in linear time and maintained with
constant update time. Otherwise the size of the query result cannot be
maintained with sublinear update time. All our lower bounds rely on the
OMv-conjecture a conjecture on the hardness of online matrix-vector
multiplication that has recently emerged in the field of fine-grained
complexity to characterise the hardness of dynamic problems. The lower bound
for the counting problem additionally relies on the orthogonal vectors
conjecture which in turn is implied by the strong exponential time hypothesis.
  $^\ast)$ By sublinear we mean $O(n^{1-\varepsilon})$ for some
$\varepsilon>0$ where $n$ is the size of the active domain of the current
database.",Christoph Berkholz|Jens Keppeler|Nicole Schweikardt,cs.DB|cs.DS|cs.LO
2017-02-28T17:23:22Z,2017-02-21T13:08:06Z,http://arxiv.org/abs/1702.06365v1,http://arxiv.org/abs/1702.06365v1,"Automatic implementation of material laws: Jacobian calculation in a
  finite element code with TAPENADE","In an effort to increase the versatility of finite element codes we explore
the possibility of automatically creating the Jacobian matrix necessary for the
gradient-based solution of nonlinear systems of equations. Particularly we aim
to assess the feasibility of employing the automatic differentiation tool
TAPENADE for this purpose on a large Fortran codebase that is the result of
many years of continuous development. As a starting point we will describe the
special structure of finite element codes and the implications that this code
design carries for an efficient calculation of the Jacobian matrix. We will
also propose a first approach towards improving the efficiency of such a
method. Finally we will present a functioning method for the automatic
implementation of the Jacobian calculation in a finite element software but
will also point out important shortcomings that will have to be addressed in
the future.",Florian Zwicke|Philipp Knechtges|Marek Behr|Stefanie Elgeti,"cs.NA|cs.DS|physics.flu-dyn|68W99, 49M15, 76M10"
2017-02-28T17:23:26Z,2017-02-21T13:06:59Z,http://arxiv.org/abs/1702.06364v1,http://arxiv.org/pdf/1702.06364v1,Linear-Time Tree Containment in Phylogenetic Networks,"We consider the NP-hard Tree Containment problem that has important
applications in phylogenetics. The problem asks if a given leaf-labeled network
contains a subdivision of a given leaf-labeled tree. We develop a fast
algorithm for the case that the input network is indeed a tree in which
multiple leaves might share a label. By combining this algorithm with a
generalization of a previously known decomposition scheme we improve the
running time on reticulation visible networks and nearly stable networks to
linear time. While these are special classes of networks they rank among the
most general of the previously considered classes.",Mathias Weller,cs.CC|cs.DS
2017-02-28T17:23:26Z,2017-02-21T09:06:04Z,http://arxiv.org/abs/1702.06298v1,http://arxiv.org/pdf/1702.06298v1,Computing Influence of a Product through Uncertain Reverse Skyline,"Understanding the influence of a product is crucially important for making
informed business decisions. This paper introduces a new type of skyline
queries called uncertain reverse skyline for measuring the influence of a
probabilistic product in uncertain data settings. More specifically given a
dataset of probabilistic products P and a set of customers C an uncertain
reverse skyline of a probabilistic product q retrieves all customers c in C
which include q as one of their preferred products. We present efficient
pruning ideas and techniques for processing the uncertain reverse skyline query
of a probabilistic product using R-Tree data index. We also present an
efficient parallel approach to compute the uncertain reverse skyline and
influence score of a probabilistic product. Our approach significantly
outperforms the baseline approach derived from the existing literature. The
efficiency of our approach is demonstrated by conducting extensive experiments
with both real and synthetic datasets.",Md. Saiful Islam|Wenny Rahayu|Chengfei Liu|Tarique Anwar|Bela Stantic,cs.DB|cs.DC|cs.DS
2017-02-28T17:23:26Z,2017-02-21T04:06:15Z,http://arxiv.org/abs/1702.06256v1,http://arxiv.org/pdf/1702.06256v1,"A $(1.4 + ε)$-approximation algorithm for the $2$-{\sc Max-Duo}
  problem","The {\em maximum duo-preservation string mapping} ({\sc Max-Duo}) problem is
the complement of the well studied {\em minimum common string partition} ({\sc
MCSP}) problem both of which have applications in many fields including text
compression and bioinformatics. $k$-{\sc Max-Duo} is the restricted version of
{\sc Max-Duo} where every letter of the alphabet occurs at most $k$ times in
each of the strings which is readily reduced into the well known {\em maximum
independent set} ({\sc MIS}) problem on a graph of maximum degree $\Delta \le
6(k-1)$. In particular $2$-{\sc Max-Duo} can then be approximated arbitrarily
close to $1.8$ using the state-of-the-art approximation algorithm for the {\sc
MIS} problem. In this paper we present a vertex-degree reduction technique
and based on which we show that $2$-{\sc Max-Duo} can be approximated
arbitrarily close to $1.4$.",Yao Xu|Yong Chen|Taibo Luo|Guohui Lin,cs.DS
2017-02-28T17:23:26Z,2017-02-23T02:48:22Z,http://arxiv.org/abs/1702.06237v2,http://arxiv.org/pdf/1702.06237v2,Exact tensor completion with sum-of-squares,"We obtain the first polynomial-time algorithm for exact tensor completion
that improves over the bound implied by reduction to matrix completion. The
algorithm recovers an unknown 3-tensor with $r$ incoherent orthogonal
components in $\mathbb R^n$ from $r\cdot \tilde O(n^{1.5})$ randomly observed
entries of the tensor. This bound improves over the previous best one of
$r\cdot \tilde O(n^{2})$ by reduction to exact matrix completion. Our bound
also matches the best known results for the easier problem of approximate
tensor completion (Barak & Moitra 2015).
  Our algorithm and analysis extends seminal results for exact matrix
completion (Candes & Recht 2009) to the tensor setting via the sum-of-squares
method. The main technical challenge is to show that a small number of randomly
chosen monomials are enough to construct a degree-3 polynomial with precisely
planted orthogonal global optima over the sphere and that this fact can be
certified within the sum-of-squares proof system.",Aaron Potechin|David Steurer,cs.LG|cs.CC|cs.DS|cs.IT|math.IT|stat.ML
2017-02-28T17:23:26Z,2017-02-20T18:50:26Z,http://arxiv.org/abs/1702.06110v1,http://arxiv.org/pdf/1702.06110v1,Density Independent Algorithms for Sparsifying $k$-Step Random Walks,"We give faster algorithms for producing sparse approximations of the
transition matrices of $k$-step random walks on undirected weighted graphs.
These transition matrices also form graphs and arise as intermediate objects
in a variety of graph algorithms. Our improvements are based on a better
understanding of processes that sample such walks as well as tighter bounds on
key weights underlying these sampling processes. On a graph with $n$ vertices
and $m$ edges our algorithm produces a graph with about $n\log{n}$ edges that
approximates the $k$-step random walk graph in about $m + n \log^4{n}$ time. In
order to obtain this runtime bound we also revisit ""density independent""
algorithms for sparsifying graphs whose runtime overhead is expressed only in
terms of the number of vertices.",Gorav Jindal|Pavel Kolev|Richard Peng|Saurabh Sawlani,cs.DS
2017-02-28T17:23:26Z,2017-02-20T18:36:25Z,http://arxiv.org/abs/1702.06099v1,http://arxiv.org/pdf/1702.06099v1,Preemptive Online Partitioning of Sequences,"Online algorithms process their inputs piece by piece taking irrevocable
decisions for each data item. This model is too restrictive for most
partitioning problems since data that is yet to arrive may render it
impossible to extend partial partitionings to the entire data set reasonably
well.
  In this work we show that preemption might be a potential remedy. We
consider the problem of partitioning online sequences where $p-1$ separators
need to be inserted into a sequence of integers that arrives online so as to
create $p$ contiguous partitions of similar weight. While without preemption no
algorithm with non-trivial competitive ratio is possible if preemption is
allowed i.e. inserted partition separators may be removed but not reinserted
again then we show that constant competitive algorithms can be obtained. Our
contributions include:
  We first give a simple deterministic $2$-competitive preemptive algorithm for
arbitrary $p$ and arbitrary sequences. Our main contribution is the design of a
highly non-trivial partitioning scheme which under some natural conditions
and $p$ being a power of two allows us to improve the competitiveness to
$1.68$. We also show that the competitiveness of deterministic (randomized)
algorithms is at least $\frac{4}{3}$ (resp. $\frac{6}{5}$).
  For $p=2$ the problem corresponds to the interesting special case of
preemptively guessing the center of a weighted request sequence. While
deterministic algorithms fail here we provide a randomized $1.345$-competitive
algorithm for all-ones sequences and prove that this is optimal. For weighted
sequences we give a $1.628$-competitive algorithm and a lower bound of $1.5$.",Christian Konrad|Tigran Tonoyan,cs.DS
2017-02-28T17:23:26Z,2017-02-20T18:33:11Z,http://arxiv.org/abs/1702.06095v1,http://arxiv.org/pdf/1702.06095v1,"An optimal XP algorithm for Hamiltonian cycle on graphs of bounded
  clique-width","For MSO$_2$-expressible problems like Edge Dominating Set or Hamiltonian
Cycle it was open for a long time whether there is an algorithm which given a
clique-width $k$-expression of an $n$-vertex graph runs in time $f(k) \cdot
n^{\mathcal{O}(1)}$ for some function $f$. Recently Fomin et al. (\emph{SIAM.
J. Computing} 2014) presented several lower bounds; for instance there are no
$f(k)\cdot n^{o(k)}$-time algorithms for Edge Dominating Set and for
Hamiltonian Cycle unless the Exponential Time Hypothesis (ETH) fails. They also
provided an algorithm running in time $n^{\mathcal{O}(k)}$ for Edge Dominating
Set but left open whether Hamiltonian Cycle can be solved in time
$n^{\mathcal{O}(k)}$.
  In this paper we prove that Hamiltonian Cycle can be solved in time
$n^{\mathcal{O}(k)}$. This improves the naive algorithm that runs in time
$n^{\mathcal{O}(k^2)}$ by Espelage et al. (WG 2001). We present a general
technique of representative sets using two-edge colored multigraphs on $k$
vertices. The essential idea behind is that for a two-edge colored multigraph
the existence of an Eulerian trail that uses edges with different colors
alternatively can be determined by two information that are the number of
colored edges incident with each vertex and the number of connected components
containing an edge. This allows to avoid storing all possible graphs on $k$
vertices with at most $n$ edges which gives the $n^{\mathcal{O}(k^2)}$ running
time.",Benjamin Bergougnoux|Mamadou Moustapha Kanté|O-joung Kwon,cs.DS|math.CO
2017-02-28T17:23:26Z,2017-02-22T10:44:45Z,http://arxiv.org/abs/1702.06087v2,http://arxiv.org/abs/1702.06087v2,Identifying high betweenness centrality nodes in large social networks,"This paper proposes an alternative way to identify nodes with high
betweenness centrality. It introduces a new metric k-path centrality and a
randomized algorithm for estimating it and shows empirically that nodes with
high k-path centrality have high node betweenness centrality. The randomized
algorithm runs in time $O(\kappa^{3}n^{2-2\alpha}\log n)$ and outputs for each
vertex v an estimate of its k-path centrality up to additive error of $\pm
n^{1/2+ \alpha}$ with probability $1-1/n^2$. Experimental evaluations on real
and synthetic social networks show improved accuracy in detecting high
betweenness centrality nodes and significantly reduced execution time when
compared with existing randomized algorithms.",Nicolas Kourtellis|Tharaka Alahakoon|Ramanuja Simha|Adriana Iamnitchi|Rahul Tripathi,cs.DS|cs.SI
2017-02-28T17:23:26Z,2017-02-20T14:51:14Z,http://arxiv.org/abs/1702.06121v1,http://arxiv.org/pdf/1702.06121v1,"Reconstructing binary matrices under window constraints from their row
  and column sums","The present paper deals with the discrete inverse problem of reconstructing
binary matrices from their row and column sums under additional constraints on
the number and pattern of entries in specified minors. While the classical
consistency and reconstruction problems for two directions in discrete
tomography can be solved in polynomial time it turns out that these window
constraints cause various unexpected complexity jumps back and forth from
polynomial-time solvability to $\mathbb{N}\mathbb{P}$-hardness.",Andreas Alpers|Peter Gritzmann,"cs.DS|math.CO|68R05, 68Q25, 49N45, 94A08"
2017-02-28T17:23:26Z,2017-02-20T12:42:07Z,http://arxiv.org/abs/1702.05951v1,http://arxiv.org/pdf/1702.05951v1,Refined Vertex Sparsifiers of Planar Graphs,"We study the following version of cut sparsification. Given a large
edge-weighted network $G$ with $k$ terminal vertices compress it into a small
network $H$ with the same terminals such that the minimum cut in $H$ between
every bipartition of the terminals approximates the corresponding one in $G$
within factor $q\geq 1$ called the quality.
  We provide two new insights about the structure of cut sparsifiers and then
apply them to obtain improved cut sparsifiers (and data structures) for planar
graphs. Our first main contribution identifies a subset of the minimum terminal
cuts that generates all the other ones. Restricting attention to these cuts
which we call elementary is effective in reducing the number of requirements
from the sparsifier $H$. Our applications of this result are new cut
sparsifiers of quality $q=1$ (also called mimicking networks): (1) for planar
networks $G$ we design a cut sparsifier of size $O(k2^{2k})$ slightly
improving the known bound from [Krauthgamer and Rika SODA 2013]; and (2) for
planar networks $G$ whose terminals are incident to at most $\gamma=\gamma(G)$
faces we design a cut sparsifier of size $O(\gamma^5 2^{2\gamma} k^4)$ which
was previously unknown.
  Our second main contribution is a duality between minor cut sparsification
and minor distance sparsification (i.e. when the sparsifier $H$ is required to
be a minor of $G$) applicable to planar networks $G$ with all their terminals
on the same face. This duality connects problems that were previously studied
separately implying new results new proofs of known results and equivalences
between open gaps. All our cut sparsifiers are minors of the input network $G$.",Robert Krauthgamer|Inbal Rika,cs.DS
2017-02-28T17:23:29Z,2017-02-20T11:16:35Z,http://arxiv.org/abs/1702.05932v1,http://arxiv.org/pdf/1702.05932v1,Robust and adaptive search,"Binary search finds a given element in a sorted array with an optimal number
of $\log n$ queries. However binary search fails even when the array is only
slightly disordered or access to its elements is subject to errors. We study
the worst-case query complexity of search algorithms that are robust to
imprecise queries and that adapt to perturbations of the order of the elements.
We give (almost) tight results for various parameters that quantify query
errors and that measure array disorder. In particular we exhibit settings
where query complexities of $\log n + ck$ $(1+\varepsilon)\log n + ck$ and
$\sqrt{cnk}+o(nk)$ are best-possible for parameter value $k$ any
$\varepsilon>0$ and constant $c$.",Yann Disser|Stefan Kratsch,cs.DS
2017-02-28T17:23:29Z,2017-02-20T08:09:09Z,http://arxiv.org/abs/1702.05888v1,http://arxiv.org/pdf/1702.05888v1,Memory Efficient Max Flow for Multi-label Submodular MRFs,"Multi-label submodular Markov Random Fields (MRFs) have been shown to be
solvable using max-flow based on an encoding of the labels proposed by
Ishikawa in which each variable $X_i$ is represented by $\ell$ nodes (where
$\ell$ is the number of labels) arranged in a column. However this method in
general requires $2\\ell^2$ edges for each pair of neighbouring variables.
This makes it inapplicable to realistic problems with many variables and
labels due to excessive memory requirement. In this paper we introduce a
variant of the max-flow algorithm that requires much less storage.
Consequently our algorithm makes it possible to optimally solve multi-label
submodular problems involving large numbers of variables and labels on a
standard computer.",Thalaiyasingam Ajanthan|Richard Hartley|Mathieu Salzmann,cs.DS|cs.CV|G.2.2; F.2.2; I.4.0
2017-02-28T17:23:29Z,2017-02-20T05:22:55Z,http://arxiv.org/abs/1702.05860v1,http://arxiv.org/pdf/1702.05860v1,Robust Sparse Estimation Tasks in High Dimensions,"In this paper we initiate the study of whether or not sparse estimation tasks
can be performed efficiently in high dimensions in the robust setting where an
$\eps$-fraction of samples are corrupted adversarially. We study the natural
robust version of two classical sparse estimation problems namely sparse mean
estimation and sparse PCA in the spiked covariance model. For both of these
problems we provide the first efficient algorithms that provide non-trivial
error guarantees in the presence of noise using only a number of samples which
is similar to the number required for these problems without noise. In
particular our sample complexities are sublinear in the ambient dimension $d$.
Our work also suggests evidence for new computational-vs-statistical gaps for
these problems (similar to those for sparse PCA without noise) which only arise
in the presence of noise.",Jerry Li,cs.LG|cs.DS
2017-02-28T17:23:29Z,2017-02-19T22:04:58Z,http://arxiv.org/abs/1702.05805v1,http://arxiv.org/pdf/1702.05805v1,Conditional Lower Bounds for All-Pairs Max-Flow,"We provide evidence that computing the maximum flow value between every pair
of nodes in a directed graph on $n$ nodes $m$ edgesand capacities in the
range $[1..n]$ which we call the All-Pairs Max-Flow problem cannot be solved
in time that is faster significantly (i.e. by a polynomial factor) than $O(n^2
m)$. Since a single maximum $st$-flow in such graphs can be solved in time
$\tilde{O}(m\sqrt{n})$ [Lee and Sidford FOCS 2014]we conclude that the
all-pairs version might require time equivalent to $\tilde\Omega(n^{3/2})$
computations of maximum $st$-flowwhich strongly separates the directed case
from the undirected one. Moreover if maximum $st$-flow can be solved in time
$\tilde{O}(m)$then the runtime of $\tilde\Omega(n^2)$ computations is needed.
The latter settles a conjecture of Lacki Nussbaum Sankowski and Wulf-Nilsen
[FOCS 2012] negatively.
  Specifically we show that in sparse graphs $G=(VEw)$ if one can compute
the maximum $st$-flow from every $s$ in an input set of sources $S\subseteq V$
to every $t$ in an input set of sinks $T\subseteq V$ in time $O((|S| |T|
m)^{1-\epsilon})$for some $|S|$ $|T|$ and a constant $\epsilon>0$then
MAX-CNF-SAT with $n'$ variables and $m'$ clauses can be solved in time
${m'}^{O(1)}2^{(1-\delta)n'}$ for a constant $\delta(\epsilon)>0$a problem for
which not even $2^{n'}/poly(n')$ algorithms are known. Such runtime for
MAX-CNF-SAT would in particular refute the Strong Exponential Time Hypothesis
(SETH). Hence we improve the lower bound of Abboud Vassilevska-Williams and
Yu [STOC 2015] who showed that for every fixed $\epsilon>0$ and
$|S|=|T|=O(\sqrt{n})$ if the above problem can be solved in time
$O(n^{3/2-\epsilon})$ then some incomparable conjecture is false. Furthermore
a larger lower bound than ours implies strictly super-linear time for maximum
$st$-flow problem which would be an amazing breakthrough.",Robert Krauthgamer|Ohad Trabelsi,cs.DS
2017-02-28T17:23:29Z,2017-02-19T16:09:28Z,http://arxiv.org/abs/1702.05763v1,http://arxiv.org/pdf/1702.05763v1,Canonical Representations for Circular-Arc Graphs Using Flip Sets,"We show how to find canonical representations for circular-arc (CA) graphs by
computing certain subsets of vertices called flip sets. A flip set enables one
to convert a CA graph into an interval matrix in a reversible way. Since
canonical representations for interval matrices can be computed in logspace
this essentially means that the problem of finding canonical representations
for CA graphs is logspace-reducible to computing 'canonical' flip sets. By
applying this reduction we reveal that the canonical representation problem for
a broad class of CA graphs reduces to the representation problem. We call this
class uniform CA graphs. As a consequences canonical representations for
uniform CA graphs can be obtained in polynomial time and the isomorphism
problem for CA graphs is reducible to that of non-uniform CA graphs. Our main
result is a logspace reduction from the canonical representation problem for CA
graphs to the canonical representation problem for vertex-colored restricted CA
matrices. Restricted CA matrices can be seen as slight generalization of
non-uniform CA graphs. The class of restricted CA matrices has a very
particular and easy to understand yet non-trivial structure which makes it
suitable for combinatorial analysis. As a byproduct we obtain the result that
canonical representations for CA graphs without induced 4-cycles can be
computed in logspace.",Maurice Chandoo,cs.DS|G.2.2
2017-02-28T17:23:29Z,2017-02-19T15:48:11Z,http://arxiv.org/abs/1702.05760v1,http://arxiv.org/pdf/1702.05760v1,Hypercube LSH for approximate near neighbors,"A celebrated technique for finding near neighbors for the angular distance
involves using a set of \textit{random} hyperplanes to partition the space into
hash regions [Charikar STOC 2002]. Experiments later showed that using a set
of \textit{orthogonal} hyperplanes thereby partitioning the space into the
Voronoi regions induced by a hypercube leads to even better results [Terasawa
and Tanaka WADS 2007]. However no theoretical explanation for this
improvement was ever given and it remained unclear how the resulting hypercube
hash method scales in high dimensions.
  In this work we provide explicit asymptotics for the collision probabilities
when using hypercubes to partition the space. For instance two near-orthogonal
vectors are expected to collide with probability $(\frac{1}{\pi})^{d + o(d)}$
in dimension $d$ compared to $(\frac{1}{2})^d$ when using random hyperplanes.
Vectors at angle $\frac{\pi}{3}$ collide with probability
$(\frac{\sqrt{3}}{\pi})^{d + o(d)}$ compared to $(\frac{2}{3})^d$ for random
hyperplanes and near-parallel vectors collide with similar asymptotic
probabilities in both cases.
  For $c$-approximate nearest neighbor searching this translates to a decrease
in the exponent $\rho$ of locality-sensitive hashing (LSH) methods of a factor
up to $\log_2(\pi) \approx 1.652$ compared to hyperplane LSH. For $c = 2$ we
obtain $\rho \approx 0.302 + o(1)$ for hypercube LSH improving upon the $\rho
\approx 0.377$ for hyperplane LSH. We further describe how to use hypercube LSH
in practice and we consider an example application in the area of lattice
algorithms.",Thijs Laarhoven,cs.DS|cs.CC|cs.CG|cs.CR
2017-02-28T17:23:29Z,2017-02-19T07:19:52Z,http://arxiv.org/abs/1702.05710v1,http://arxiv.org/pdf/1702.05710v1,"Polynomial Time Efficient Construction Heuristics for Vertex Separation
  Minimization Problem","Vertex Separation Minimization Problem (VSMP) consists of finding a layout of
a graph G = (VE) which minimizes the maximum vertex cut or separation of a
layout. It is an NP-complete problem in general for which metaheuristic
techniques can be applied to find near optimal solution. VSMP has applications
in VLSI design graph drawing and computer language compiler design. VSMP is
polynomially solvable for grids trees permutation graphs and cographs.
Construction heuristics play a very important role in the metaheuristic
techniques as they are responsible for generating initial solutions which lead
to fast convergence. In this paper we have proposed three construction
heuristics H1 H2 and H3 and performed experiments on Grids Small graphs
Trees and Harwell Boeing graphs totaling 248 instances of graphs. Experiments
reveal that H1 H2 and H3 are able to achieve best results for 88.71% 43.5%
and 37.1% of the total instances respectively while the best construction
heuristic in the literature achieves the best solution for 39.9% of the total
instances. We have also compared the results with the state-of-the-art
metaheuristic GVNS and observed that the proposed construction heuristics
improves the results for some of the input instances. It was found that GVNS
obtained best results for 82.9% instances of all input instances and the
heuristic H1 obtained best results for 82.3% of all input instances.",Pallavi Jain|Gur Saran|Kamal Srivastava,cs.DS|cs.AI
2017-02-28T17:23:29Z,2017-02-19T00:03:09Z,http://arxiv.org/abs/1702.05678v1,http://arxiv.org/pdf/1702.05678v1,An Adaptivity Hierarchy Theorem for Property Testing,"Adaptivity is known to play a crucial role in property testing. In
particular there exist properties for which there is an exponential gap
between the power of \emph{adaptive} testing algorithms wherein each query may
be determined by the answers received to prior queries and their
\emph{non-adaptive} counterparts in which all queries are independent of
answers obtained from previous queries.
  In this work we investigate the role of adaptivity in property testing at a
finer level. We first quantify the degree of adaptivity of a testing algorithm
by considering the number of ""rounds of adaptivity"" it uses. More accurately
we say that a tester is $k$-(round) adaptive if it makes queries in $k+1$
rounds where the queries in the $i$'th round may depend on the answers
obtained in the previous $i-1$ rounds. Then we ask the following question:
  Does the power of testing algorithms smoothly grow with the number of rounds
of adaptivity?
  We provide a positive answer to the foregoing question by proving an
adaptivity hierarchy theorem for property testing. Specifically our main
result shows that for every $n\in \mathbb{N}$ and $0 \le k \le n^{0.99}$ there
exists a property $\mathcal{P}_{nk}$ of functions for which (1) there exists a
$k$-adaptive tester for $\mathcal{P}_{nk}$ with query complexity
$\tilde{O}(k)$ yet (2) any $(k-1)$-adaptive tester for $\mathcal{P}_{nk}$
must make $\Omega(n)$ queries. In addition we show that such a qualitative
adaptivity hierarchy can be witnessed for testing natural properties of graphs.",Clement Canonne|Tom Gur,cs.DS|cs.LG
2017-02-28T17:23:29Z,2017-02-18T15:47:39Z,http://arxiv.org/abs/1702.05626v1,http://arxiv.org/pdf/1702.05626v1,Embeddings of Schatten Norms with Applications to Data Streams,"Given an $n \times d$ matrix $A$ its Schatten-$p$ norm $p \geq 1$ is
defined as $\|A\|_p = \left (\sum_{i=1}^{\textrm{rank}(A)}\sigma_i(A)^p \right
)^{1/p}$ where $\sigma_i(A)$ is the $i$-th largest singular value of $A$.
These norms have been studied in functional analysis in the context of
non-commutative $\ell_p$-spaces and recently in data stream and linear
sketching models of computation. Basic questions on the relations between these
norms such as their embeddability are still open. Specifically given a set
of matrices $A^1 \ldots A^{\operatorname{poly}(nd)} \in \mathbb{R}^{n \times
d}$ suppose we want to construct a linear map $L$ such that $L(A^i) \in
\mathbb{R}^{n' \times d'}$ for each $i$ where $n' \leq n$ and $d' \leq d$ and
further $\|A^i\|_p \leq \|L(A^i)\|_q \leq D_{pq} \|A^i\|_p$ for a given
approximation factor $D_{pq}$ and real number $q \geq 1$. Then how large do
$n'$ and $d'$ need to be as a function of $D_{pq}$?
  We nearly resolve this question for every $p q \geq 1$ for the case where
$L(A^i)$ can be expressed as $R \cdot A^i \cdot S$ where $R$ and $S$ are
arbitrary matrices that are allowed to depend on $A^1 \ldots A^t$ that is
$L(A^i)$ can be implemented by left and right matrix multiplication. Namely
for every $p q \geq 1$ we provide nearly matching upper and lower bounds on
the size of $n'$ and $d'$ as a function of $D_{pq}$. Importantly our upper
bounds are {\it oblivious} meaning that $R$ and $S$ do not depend on the
$A^i$ while our lower bounds hold even if $R$ and $S$ depend on the $A^i$. As
an application of our upper bounds we answer a recent open question of Blasiok
et al. about space-approximation trade-offs for the Schatten $1$-norm showing
in a data stream it is possible to estimate the Schatten-$1$ norm up to a
factor of $D \geq 1$ using $\tilde{O}(\min(nd)^2/D^4)$ space.",Yi Li|David P. Woodruff,cs.DS
2017-02-28T17:23:29Z,2017-02-18T10:47:20Z,http://arxiv.org/abs/1702.05597v1,http://arxiv.org/pdf/1702.05597v1,One-Pass Error Bounded Trajectory Simplification,"Nowadays various sensors are collecting storing and transmitting tremendous
trajectory data and it is known that raw trajectory data seriously wastes the
storage network band and computing resource. Line simplification (LS)
algorithms are an effective approach to attacking this issue by compressing
data points in a trajectory to a set of continuous line segments and are
commonly used in practice. However existing LS algorithms are not sufficient
for the needs of sensors in mobile devices. In this study we first develop a
one-pass error bounded trajectory simplification algorithm (OPERB) which scans
each data point in a trajectory once and only once. We then propose an
aggressive one-pass error bounded trajectory simplification algorithm
(OPERB-A) which allows interpolating new data points into a trajectory under
certain conditions. Finally we experimentally verify that our approaches
(OPERB and OPERB-A) are both efficient and effective using four real-life
trajectory datasets.",Xuelian Lin|Shuai Ma|Han Zhang|Tianyu Wo|Jinpeng Huai,cs.DB|cs.DS
2017-02-28T17:23:33Z,2017-02-18T09:46:32Z,http://arxiv.org/abs/1702.05589v1,http://arxiv.org/pdf/1702.05589v1,A Circuit-Based Approach to Efficient Enumeration,"We study the problem of enumerating the satisfying valuations of a circuit
while bounding the delay i.e. the time needed to compute each successive
valuation. We focus on the class of structured d-DNNF circuits originally
introduced in knowledge compilation a sub-area of artificial intelligence. We
propose an algorithm for these circuits that enumerates valuations with linear
preprocessing and delay linear in the Hamming weight of each valuation.
Moreover valuations of constant Hamming weight can be enumerated with linear
preprocessing and constant delay.
  Our results yield a framework for efficient enumeration that applies to all
problems whose solutions can be compiled to structured d-DNNFs. In particular
we use it to recapture classical results in database theory for factorized
database representations and for MSO evaluation. This gives an independent
proof of constant-delay enumeration for MSO formulae with first-order free
variables on bounded-treewidth structures.",Antoine Amarilli|Pierre Bourhis|Louis Jachiet|Stefan Mengel,cs.DS|cs.LO
2017-02-28T17:23:33Z,2017-02-18T04:49:13Z,http://arxiv.org/abs/1702.05570v1,http://arxiv.org/pdf/1702.05570v1,"Multi-way sparsest cut problem on trees with a control on the number of
  parts and outliers","Given a graph the sparsest cut problem asks for a subset of vertices whose
edge expansion (the normalized cut given by the subset) is minimized. In this
paper we study a generalization of this problem seeking for $ k $ disjoint
subsets of vertices (clusters) whose all edge expansions are small and
furthermore the number of vertices remained in the exterior of the subsets
(outliers) is also small. We prove that although this problem is $ NP-$hard for
trees it can be solved in polynomial time for all weighted trees provided
that we restrict the search space to subsets which induce connected subgraphs.
The proposed algorithm is based on dynamic programming and runs in the worst
case in $ O(k^2 n^3) $ when $ n $ is the number of vertices and $ k $ is the
number of clusters. It also runs in linear time when the number of clusters and
the number of outliers is bounded by a constant.",Ramin Javadi|Saleh Ashkboos,"cs.DS|05C85, 68Q25, 68R10"
2017-02-28T17:23:33Z,2017-02-17T17:48:41Z,http://arxiv.org/abs/1702.05456v1,http://arxiv.org/pdf/1702.05456v1,LCL problems on grids,"LCLs or locally checkable labelling problems (e.g. maximal independent set
maximal matching and vertex colouring) in the LOCAL model of computation are
very well-understood in cycles (toroidal 1-dimensional grids): every problem
has a complexity of $O(1)$ $\Theta(\log^* n)$ or $\Theta(n)$ and the design
of optimal algorithms can be fully automated.
  This work develops the complexity theory of LCL problems for toroidal
2-dimensional grids. The complexity classes are the same as in the
1-dimensional case: $O(1)$ $\Theta(\log^* n)$ and $\Theta(n)$. However given
an LCL problem it is undecidable whether its complexity is $\Theta(\log^* n)$
or $\Theta(n)$ in 2-dimensional grids.
  Nevertheless if we correctly guess that the complexity of a problem is
$\Theta(\log^* n)$ we can completely automate the design of optimal
algorithms. For any problem we can find an algorithm that is of a normal form
$A' \circ S_k$ where $A'$ is a finite function $S_k$ is an algorithm for
finding a maximal independent set in $k$th power of the grid and $k$ is a
constant.
  With the help of this technique we study several concrete \lcl{} problems
also in more general settings. For example for all $d \ge 2$ we prove that:
  - $d$-dimensional grids can be $k$-vertex coloured in time $O(\log^* n)$ iff
$k \ge 4$
  - $d$-dimensional grids can be $k$-edge coloured in time $O(\log^* n)$ iff $k
\ge 2d+1$.
  The proof that $3$-colouring of $2$-dimensional grids requires $\Theta(n)$
time introduces a new topological proof technique which can also be applied to
e.g. orientation problems.",Sebastian Brandt|Juho Hirvonen|Janne H. Korhonen|Tuomo Lempiäinen|Patric R. J. Östergård|Christopher Purcell|Joel Rybicki|Jukka Suomela|Przemysław Uznański,cs.DC|cs.CC|cs.DS
2017-02-28T17:23:33Z,2017-02-17T17:18:58Z,http://arxiv.org/abs/1702.05446v1,http://arxiv.org/pdf/1702.05446v1,Network Flow Based Post Processing for Sales Diversity,"Collaborative filtering is a broad and powerful framework for building
recommendation systems that has seen widespread adoption. Over the past decade
the propensity of such systems for favoring popular products and thus creating
echo chambers have been observed. This has given rise to an active area of
research that seeks to diversify recommendations generated by such algorithms.
  We address the problem of increasing diversity in recommendation systems that
are based on collaborative filtering that use past ratings to predicting a
rating quality for potential recommendations. Following our earlier work we
formulate recommendation system design as a subgraph selection problem from a
candidate super-graph of potential recommendations where both diversity and
rating quality are explicitly optimized: (1) On the modeling side we define a
new flexible notion of diversity that allows a system designer to prescribe the
number of recommendations each item should receive and smoothly penalizes
deviations from this distribution. (2) On the algorithmic side we show that
minimum-cost network flow methods yield fast algorithms in theory and practice
for designing recommendation subgraphs that optimize this notion of diversity.
(3) On the empirical side we show the effectiveness of our new model and
method to increase diversity while maintaining high rating quality in standard
rating data sets from Netflix and MovieLens.",Arda Antikacioglu|R Ravi,cs.IR|cs.DS
2017-02-28T17:23:33Z,2017-02-27T18:04:49Z,http://arxiv.org/abs/1702.05425v2,http://arxiv.org/pdf/1702.05425v2,Exact clustering in linear time,"The time complexity of data clustering has been viewed as fundamentally
quadratic slowing with the number of data items as each item is compared for
similarity to preceding items. Clustering of large data sets has been
infeasible without resorting to probabilistic methods or to capping the number
of clusters. Here we introduce MIMOSA a novel class of algorithms which
achieve linear time computational complexity on clustering tasks. MIMOSA
algorithms mark and match partial-signature keys in a hash table to obtain
exact error-free cluster retrieval. Benchmark measurements on clustering a
data set of 10000000 news articles by news topic found that a MIMOSA
implementation finished more than four orders of magnitude faster than a
standard centroid implementation.",Jonathan A. Marshall|Lawrence C. Rafsky,cs.DS|cs.DB|H.3.3; I.5.3; F.2.2; E.1
2017-02-28T17:23:33Z,2017-02-17T16:01:50Z,http://arxiv.org/abs/1702.05413v1,http://arxiv.org/pdf/1702.05413v1,3D Cell Nuclei Segmentation with Balanced Graph Partitioning,"Cell nuclei segmentation is one of the most important tasks in the analysis
of biomedical images. With ever-growing sizes and amounts of three-dimensional
images to be processed there is a need for better and faster segmentation
methods. Graph-based image segmentation has seen a rise in popularity in recent
years but is seen as very costly with regard to computational demand. We
propose a new segmentation algorithm which overcomes these limitations. Our
method uses recursive balanced graph partitioning to segment foreground
components of a fast and efficient binarization. We construct a model for the
cell nuclei to guide the partitioning process. Our algorithm is compared to
other state-of-the-art segmentation algorithms in an experimental evaluation on
two sets of realistically simulated inputs. Our method is faster has similar
or better quality and an acceptable memory overhead.",Julian Arz|Peter Sanders|Johannes Stegmaier|Ralf Mikut,cs.CV|cs.DS
2017-02-28T17:23:33Z,2017-02-17T14:34:08Z,http://arxiv.org/abs/1702.05358v1,http://arxiv.org/pdf/1702.05358v1,Computational topology of graphs on surfaces,"Computational topology is an area that revisits topological problems from an
algorithmic point of view and develops topological tools for improved
algorithms. We survey results in computational topology that are concerned with
graphs drawn on surfaces. Typical questions include representing surfaces and
graphs embedded on them computationally deciding whether a graph embeds on a
surface solving computational problems related to homotopy optimizing curves
and graphs on surfaces and solving standard graph algorithm problems more
efficiently in the case of surface-embedded graphs.",Éric Colin de Verdière,"cs.CG|cs.DM|cs.DS|math.AT|math.CO|68U05, 05C10, 57M15, 68R10|F.2.2; G.2.2; I.3.5"
2017-02-28T17:23:33Z,2017-02-17T10:00:58Z,http://arxiv.org/abs/1702.05284v1,http://arxiv.org/pdf/1702.05284v1,Improving the betweenness centrality of a node by adding links,"Betweenness is a well-known centrality measure that ranks the nodes according
to their participation in the shortest paths of a network. In several
scenarios having a high betweenness can have a positive impact on the node
itself. Hence in this paper we consider the problem of determining how much a
vertex can increase its centrality by creating a limited amount of new edges
incident to it. In particular we study the problem of maximizing the
betweenness score of a given node -- Maximum Betweenness Improvement (MBI) --
and that of maximizing the ranking of a given node -- Maximum Ranking
Improvement (MRI). We show that MBI cannot be approximated in polynomial-time
within a factor $(1-\frac{1}{2e})$ and that MRI does not admit any
polynomial-time constant factor approximation algorithm both unless $P=NP$. We
then propose a simple greedy approximation algorithm for MBI with an almost
tight approximation ratio and we test its performance on several real-world
networks. We experimentally show that our algorithm highly increases both the
betweenness score and the ranking of a given node ant that it outperforms
several competitive baselines. To speed up the computation of our greedy
algorithm we also propose a new dynamic algorithm for updating the betweenness
of one node after an edge insertion which might be of independent interest.
Using the dynamic algorithm we are now able to compute an approximation of MBI
on networks with up to $10^5$ edges in most cases in a matter of seconds or a
few minutes.",Elisabetta Bergamini|Pierluigi Crescenzi|Gianlorenzo D'Angelo|Henning Meyerhenke|Lorenzo Severini|Yllka Velaj,cs.DS|cs.SI
2017-02-28T17:23:33Z,2017-02-17T04:11:08Z,http://arxiv.org/abs/1702.05218v1,http://arxiv.org/pdf/1702.05218v1,"Complete Submodularity Characterization in the Comparative Independent
  Cascade Model","We study the propagation of comparative ideas in social network. A full
characterization for submodularity in the comparative independent cascade
(Com-IC) model of two-idea cascade is given for competing ideas and
complementary ideas respectively. We further introduce One-Shot model where
agents show less patience toward ideas and show that in One-Shot model only
the stronger idea spreads with submodularity.",Wei Chen|Hanrui Zhang,cs.SI|cs.DS
2017-02-28T17:23:33Z,2017-02-17T04:06:45Z,http://arxiv.org/abs/1702.05217v1,http://arxiv.org/pdf/1702.05217v1,A Fully Polynomial Time Approximation Scheme for Packing While Traveling,"Understanding the interactions between different combinatorial optimisation
problems in real-world applications is a challenging task. Recently the
traveling thief problem (TTP) as a combination of the classical traveling
salesperson problem and the knapsack problem has been introduced to study
these interactions in a systematic way. We investigate the underlying
non-linear packing while traveling (PWT) problem of the TTP where items have to
be selected along a fixed route. We give an exact dynamic programming approach
for this problem and a fully polynomial time approximation scheme (FPTAS) when
maximising the benefit that can be gained over the baseline travel cost. Our
experimental investigations show that our new approaches outperform current
state-of-the-art approaches on a wide range of benchmark instances.",Frank Neumann|Sergey Polyakovskiy|Martin Skutella|Leen Stougie|Junhua Wu,cs.DS|90C27|G.2.1; F.2.2
2017-02-28T17:23:37Z,2017-02-16T17:02:05Z,http://arxiv.org/abs/1702.05051v1,http://arxiv.org/pdf/1702.05051v1,Succinct progress measures for solving parity games,"Calude et al. have given the first algorithm for solving parity games in
quasi-polynomial time where previously the best algorithms were mildly
subexponential. We combine the succinct counting technique of Calude et al.\
with the small progress measure technique of Jurdzi\'nski. Our contribution is
two-fold: we provide an alternative exposition of the ideas behind the
groundbreaking quasi-polynomial algorithm of Calude et al. and we use it to
reduce the space required from quasi-polynomial to nearly linear.",Marcin Jurdziński|Ranko Lazić,cs.DS|cs.LO
2017-02-28T17:23:37Z,2017-02-16T06:37:42Z,http://arxiv.org/abs/1702.04871v1,http://arxiv.org/pdf/1702.04871v1,Online Constrained Forest and Prize-Collecting Network Design,"In this paper we study a very general type of online network design problem
and generalize two different previous algorithms one for an online network
design problem due to Berman and Coulston [4] and one for (offline) general
network design problems due to Goemans and Williamson [9]; we give an O(log
k)-competitive algorithm where k is the number of nodes that must be
connected. We also consider a further generalization of the problem that allows
us to pay penalties in exchange for violating connectivity constraints; we give
an online O(log k)-competitive algorithm for this case as well.",Jiawei Qian|Seeun William Umboh|David P. Williamson,cs.DS
2017-02-28T17:23:37Z,2017-02-16T06:07:25Z,http://arxiv.org/abs/1702.04866v1,http://arxiv.org/pdf/1702.04866v1,"Proust: A Design Space for Highly-Concurrent Transactional Data
  Structures","Most STM systems are poorly equipped to support libraries of concurrent data
structures. One reason is that they typically detect conflicts by tracking
transactions' read sets and write sets an approach that often leads to false
conflicts. A second is that existing data structures and libraries often need
to be rewritten from scratch to support transactional conflict detection and
rollback. This paper introduces Proust a framework for the design and
implementation of transactional data structures. Proust is designed to maximize
re-use of existing well-engineered by providing transactional ""wrappers"" to
make existing thread-safe concurrent data structures transactional. Proustian
objects are also integrated with an underling STM system allowing them to take
advantage of well-engineered STM conflict detection mechanisms. Proust
generalizes and unifies prior approaches such as boosting and predication.",Thomas D. Dickerson|Paul Gazzillo|Eric Koskinen|Maurice Herlihy,cs.DC|cs.DS
2017-02-28T17:23:37Z,2017-02-18T20:40:11Z,http://arxiv.org/abs/1702.04786v2,http://arxiv.org/pdf/1702.04786v2,Finding All Useless Arcs in Directed Planar Graphs,"Maximum flow is a fundamental problem in Combinatorial Optimization that has
numerous applications in both theory and practice. In this paper we study the
flow network simplification problem which asks to remove all the useless arcs
from the graph. To be precise an arc is useless if it does not participate in
any simple s t-path. Weihe [FOCS'94 JCSS'97] showed that if there exists an
$O(n \log n)$-time algorithm for simplifying a flow network then a maximum s
t-flow in directed planar graphs can be computed in $O(n \log n)$-time.
However there was no known algorithm that could determine all the useless arcs
in $O(n \log n)$-time. Although an $O(n \log n)$-time algorithm for computing
maximum flow on planar directed graphs without simplifying a flow network has
been later discovered by Borradaile and Klein [SODA'06 J.ACM'09] it remains
open whether a directed planar flow network can be simplified in $O(n \log
n)$-time.
  Here we present an algorithm that determines all the useless arcs in $O(n
\log n)$-time thus completing the framework of Weihe. Our algorithm improves
upon the previous best running time of $\tilde{O}(n^2)$ for removing all the
useless arcs by Misiolek and Chen [COCOON'05 IPL'06] and by Biedl Brejova and
Vinar [MFCS'00]. Our main algorithm requires the planar embedding to contain no
clockwise cycle and every vertex except source and sink has degree three. The
bottleneck of our algorithm is the $O(n \log n)$ time algorithm for
preprocessing the planar embedding while all the other parts runs in linear
time.",Jittat Fakcharoenphol|Bundit Laekhanukit|Pattara Sukprasert,cs.DS
2017-02-28T17:23:37Z,2017-02-15T15:09:15Z,http://arxiv.org/abs/1702.04645v1,http://arxiv.org/pdf/1702.04645v1,A parallel implementation of the Synchronised Louvain method,"Community detection in networks is a very actual and important field of
research with applications in many areas. But given that the amount of
processed data increases more and more existing algorithms need to be adapted
for very large graphs. The objective of this project was to parallelise the
Synchronised Louvain Method a community detection algorithm developed by
Arnaud Browet in order to improve its performances in terms of computation
time and thus be able to faster detect communities in very large graphs. To
reach this goal we used the API OpenMP to parallelise the algorithm and then
carried out performance tests. We studied the computation time and speedup of
the parallelised algorithm and were able to bring out some qualitative trends.
We obtained a great speedup compared with the theoretical prediction of Amdahl
law. To conclude using the parallel implementation of the algorithm of Browet
on large graphs seems to give good results both in terms of computation time
and speedup. Further tests should be carried out in order to obtain more
quantitative results.",Benjamin Chiêm|Andine Havelange|Paul Van Dooren,cs.DS|cs.DC|cs.SI
2017-02-28T17:23:37Z,2017-02-15T10:14:41Z,http://arxiv.org/abs/1702.04536v1,http://arxiv.org/abs/1702.04536v1,"A $(2+ε)$-Approximation for Maximum Weight Matching in the
  Semi-Streaming Model","We present a simple deterministic single-pass $(2+\epsilon)$-approximation
algorithm for the maximum weight matching problem in the semi-streaming model.
This improves upon the currently best known approximation ratio of
$(3.5+\epsilon)$.
  Our algorithm uses $O(n\log^2 n)$ space for constant values of $\epsilon$. It
relies on a variation of the local-ratio theorem which may be of use for other
algorithms in the semi-streaming model as well.",Ami Paz|Gregory Schwartzman,cs.DS
2017-02-28T17:23:37Z,2017-02-14T20:17:00Z,http://arxiv.org/abs/1702.04376v1,http://arxiv.org/pdf/1702.04376v1,Querying languages over sliding windows,"We study the space complexity of querying languages over data streams in the
sliding window model. The algorithm has to answer at any point of time whether
the content of the sliding window belongs to a fixed regular language. For
regular languages a trichotomy is shown: For every regular language the
optimal space requirement is asymptotically either constant logarithmic or
linear in the size of the sliding window. Moreover given a DFA for the
language one can check in nondeterministic logspace (and hence polynomial
time) which of the three cases holds. For context-free languages the
trichotomy no longer holds: For every $c \geq 1$ there exists a context-free
language $L_c$ for which the optimal space required to query $L_c$ in the
sliding window model is asymptotically $n^{1/c}$. Finally it is shown that all
results also hold for randomized (Monte-Carlo) streaming algorithms.",Moses Ganardi|Danny Hucke|Markus Lohrey,"cs.FL|cs.DS|68Q45, 68W32, 68W20"
2017-02-28T17:23:37Z,2017-02-14T18:21:28Z,http://arxiv.org/abs/1702.04322v1,http://arxiv.org/pdf/1702.04322v1,"Parameterized Algorithms for Recognizing Monopolar and 2-Subcolorable
  Graphs","A graph $G$ is a $(\Pi_A\Pi_B)$-graph if $V(G)$ can be bipartitioned into
$A$ and $B$ such that $G[A]$ satisfies property $\Pi_A$ and $G[B]$ satisfies
property $\Pi_B$. The $(\Pi_{A}\Pi_{B})$-Recognition problem is to recognize
whether a given graph is a $(\Pi_A\Pi_B)$-graph. There are many
$(\Pi_{A}\Pi_{B})$-Recognition problems including the recognition problems
for bipartite split and unipolar graphs. We present efficient algorithms for
many cases of $(\Pi_A\Pi_B)$-Recognition based on a technique which we dub
inductive recognition. In particular we give fixed-parameter algorithms for
two NP-hard $(\Pi_{A}\Pi_{B})$-Recognition problems Monopolar Recognition and
2-Subcoloring. We complement our algorithmic results with several hardness
results for $(\Pi_{A}\Pi_{B})$-Recognition.",Iyad Kanj|Christian Komusiewicz|Manuel Sorge|Erik Jan van Leeuwen,cs.CC|cs.DS
2017-02-28T17:23:37Z,2017-02-14T17:33:47Z,http://arxiv.org/abs/1702.04307v1,http://arxiv.org/pdf/1702.04307v1,Approximating the Held-Karp Bound for Metric TSP in Nearly-Linear Time,"We give a nearly linear-time randomized approximation scheme for the
Held-Karp bound [Held and Karp 1970] for Metric-TSP. Formally give an
undirected edge-weighted graph $G = (VE)$ on $m$ edges and $\epsilon > 0$ the
algorithm outputs in $O(m \operatorname{polylog}(m)/\epsilon^2)$ time with
high probability a $(1 + \epsilon)$-approximation to the Held-Karp bound on
the Metric-TSP instance induced by the shortest path metric on $G$. We
substantially improve upon the $O(m^2 \log^2(m)/ \epsilon^2)$ run time achieved
previously by Garg and Khandekar. The algorithm builds on our recent framework
for accelerating multiplicative weight update based methods for implicit
fractional packing problems.",Chandra Chekuri|Kent Quanrud,cs.DS
2017-02-28T17:23:37Z,2017-02-14T17:29:03Z,http://arxiv.org/abs/1702.04304v1,http://arxiv.org/pdf/1702.04304v1,Solving Orienteering with Category Constraints Using Prioritized Search,"We develop an approach for solving rooted orienteering problems with category
constraints as found in tourist trip planning and logistics. It is based on
expanding partial solutions in a systematic way prioritizing promising ones
which reduces the search space we have to traverse during the search. The
category constraints help in reducing the space we have to explore even
further. We implement an algorithm that computes the optimal solution and also
illustrate how our approach can be turned into an approximation algorithm
yielding much faster run times and guaranteeing lower bounds on the quality of
the solution found. We demonstrate the effectiveness of our algorithms by
comparing them to the state-of-the-art approach and an optimal algorithm based
on dynamic programming showing that our technique clearly outperforms these
methods.",Paolo Bolzoni|Sven Helmer,cs.DS
2017-02-28T17:23:40Z,2017-02-14T14:00:01Z,http://arxiv.org/abs/1702.04211v1,http://arxiv.org/pdf/1702.04211v1,"Dynamic programming algorithms efficient solution of the LP-relaxation
  and approximation schemes for the Penalized Knapsack Problem","We consider the 0-1 Penalized Knapsack Problem (PKP). Each item has a profit
a weight and a penalty and the goal is to maximize the sum of the profits minus
the greatest penalty value of the items included in a solution. We propose an
exact approach relying on a procedure which narrows the relevant range of
penalties on the identification of a core problem and on dynamic programming.
The proposed approach turns out to be very effective in solving hard instances
of PKP and compares favorably both to commercial solver CPLEX 12.5 applied to
the ILP formulation of the problem and to the best available exact algorithm in
the literature. Then we present a general inapproximability result and
investigate several relevant special cases which permit fully polynomial time
approximation schemes (FPTASs).",Federico Della Croce|Ulrich Pferschy|Rosario Scatamacchia,cs.DS
2017-02-28T17:23:40Z,2017-02-14T12:05:58Z,http://arxiv.org/abs/1702.04170v1,http://arxiv.org/pdf/1702.04170v1,Optimal Longest Paths by Dynamic Programming,"We propose an optimal algorithm for solving the longest path problem in
undirected weighted graphs. By using graph partitioning and dynamic
programming we obtain an algorithm that is significantly faster than other
state-of-the-art methods. This enables us to solve instances that have
previously been unsolved.",Tomas Balyo|Kai Fieger|Christian Schulz,cs.DS
2017-02-28T17:23:40Z,2017-02-14T11:40:21Z,http://arxiv.org/abs/1702.04164v1,http://arxiv.org/pdf/1702.04164v1,Better Process Mapping and Sparse Quadratic Assignment,"Communication and topology aware process mapping is a powerful approach to
reduce communication time in parallel applications with known communication
patterns on large distributed memory systems. We address the problem as a
quadratic assignment problem (QAP) and present algorithms to construct initial
mappings of processes to processors as well as fast local search algorithms to
further improve the mappings. By exploiting assumptions that typically hold for
applications and modern supercomputer systems such as sparse communication
patterns and hierarchically organized communication systems we arrive at
significantly more powerful algorithms for these special QAPs. Our multilevel
construction algorithms employ recently developed perfectly balanced graph
partitioning techniques and excessively exploit the given communication system
hierarchy. We present improvements to a local search algorithm of Brandfass et
al. and decrease the running time by reducing the time needed to perform swaps
in the assignment as well as by carefully constraining local search
neighborhoods. Experiments indicate that our algorithms not only dramatically
speed up local search but due to the multilevel approach also find much better
solutions~in~practice.",Christian Schulz|Jesper Larsson Träff,cs.DC|cs.DS
2017-02-28T17:23:40Z,2017-02-14T06:15:28Z,http://arxiv.org/abs/1702.04088v1,http://arxiv.org/pdf/1702.04088v1,"Solving Tree Containment Problem for Reticulation-visible Networks with
  Optimal Running Time","Tree containment problem is a fundamental problem in phylogenetic study as
it is used to verify a network model. It asks whether a given network contain a
subtree that resembles a binary tree. The problem is NP-complete in general
even in the class of binary network. Recently it was proven to be solvable in
cubic time and later in quadratic time for the class of general reticulation
visible networks. In this paper we further improve the time complexity into
linear time.",Andreas Gunawan,q-bio.PE|cs.DS
2017-02-28T17:23:40Z,2017-02-13T21:19:32Z,http://arxiv.org/abs/1702.03989v1,http://arxiv.org/pdf/1702.03989v1,"The submodular secretary problem under a cardinality constraint and with
  limited resources","We study the submodular secretary problem subject to a cardinality
constraint in long-running scenarios or under resource constraints. In these
scenarios the resources consumed by the algorithm should not grow with the
input size and the online selection algorithm should be an anytime algorithm.
We propose a $0.1933$-competitive anytime algorithm which performs only a
single evaluation of the marginal contribution for each observed item and
requires a memory of order only $k$ (up to logarithmic factors) where $k$ is
the cardinality constraint. The best competitive ratio for this problem known
so far under these constraints is $\frac{e-1}{e^2+e} \approx 0.1700$ (Feldman
et al. 2011). Our algorithm is based on the observation that information
collected during times in which no good items were selected can be used to
improve the subsequent probability of selection success. The improvement is
obtained by using an adaptive selection strategy which is a solution to a
stand-alone online selection problem. We develop general tools for analyzing
this algorithmic framework which we believe will be useful also for other
online selection problems.",Tom Hess|Sivan Sabato,cs.DS
2017-02-28T17:23:40Z,2017-02-13T19:35:15Z,http://arxiv.org/abs/1702.03959v1,http://arxiv.org/pdf/1702.03959v1,How large is your graph?,"We consider the problem of estimating the graph size where one is given only
local access to the graph. We formally define a query model in which one starts
with a \emph{seed} node and is allowed to make queries about neighbours of
nodes that have already been seen. In the case of undirected graphs an
estimator of Katzir et al. (2014) based on a sample from the stationary
distribution $\pi$ uses $O\left(\frac{1}{\|\pi\|_2} + \text{davg}\right)$
queries we prove that this is tight. In addition we establish this as a lower
bound even when the algorithm is allowed to crawl the graph arbitrarily the
results of Katzir et al. give an upper bound that is worse by a multiplicative
factor $t_\text{mix} \cdot \log(n)$. The picture becomes significantly
different in the case of directed graphs. We show that without strong
assumptions on the graph structure the number of nodes cannot be predicted to
within a constant multiplicative factor without using a number of queries that
are at least linear in the number of nodes in particular rapid mixing and
small diameter properties that most real-world networks exhibit do not
suffice. The question of interest is whether any algorithm can beat
breadth-first search. We introduce a new parameter generalising the
well-studied conductance such that if a suitable bound on it exists and is
known to the algorithm the number of queries required is sublinear in the
number of edges we show that this is tight.",Varun Kanade|Frederik Mallmann-Trenn|Victor Verdugo,cs.DS
2017-02-28T17:23:40Z,2017-02-13T13:41:58Z,http://arxiv.org/abs/1702.03773v1,http://arxiv.org/pdf/1702.03773v1,"Overlapping Community Detection by Local Decentralised Vertex-centred
  Process","This paper focuses on the identification of overlapping communities allowing
nodes to simultaneously belong to several communities in a decentralised way.
To that aim it proposes LOCNeSs an algorithm specially designed to run in a
decentralised environment and to limit propagation two essential
characteristics to be applied in mobile networks. It is based on the
exploitation of the preferential attachment mechanism in networks. Experimental
results show that LOCNeSs is stable and achieves good overlapping vertex
identification.",Maël Canu|Marie-Jeanne Lesot|Adrien Revault d'Allonnes,cs.SI|cs.DM|cs.DS
2017-02-28T17:23:40Z,2017-02-13T07:18:13Z,http://arxiv.org/abs/1702.03657v1,http://arxiv.org/pdf/1702.03657v1,Trie Compression for GPU Accelerated Multi-Pattern Matching,"Graphics Processing Units allow for running massively parallel applications
offloading the CPU from computationally intensive resources however GPUs have
a limited amount of memory. In this paper a trie compression algorithm for
massively parallel pattern matching is presented demonstrating 85% less space
requirements than the original highly efficient parallel failure-less
aho-corasick whilst demonstrating over 22 Gbps throughput. The algorithm
presented takes advantage of compressed row storage matrices as well as shared
and texture memory on the GPU.",Xavier Bellekens|Amar Seeam|Christos Tachtatzis|Robert Atkinson,cs.DS|cs.DC
2017-02-28T17:23:40Z,2017-02-13T06:17:16Z,http://arxiv.org/abs/1702.03644v1,http://arxiv.org/pdf/1702.03644v1,Coresets for Kernel Regression,"Kernel regression is an essential and ubiquitous tool for non-parametric data
analysis particularly popular among time series and spatial data. However the
central operation which is performed many times evaluating a kernel on the
data set takes linear time. This is impractical for modern large data sets.
  In this paper we describe coresets for kernel regression: compressed data
sets which can be used as proxy for the original data and have provably bounded
worst case error. The size of the coresets are independent of the raw number of
data points rather they only depend on the error guarantee and in some cases
the size of domain and amount of smoothing. We evaluate our methods on very
large time series and spatial data and demonstrate that they incur negligible
error can be constructed extremely efficiently and allow for great
computational gains.",Yan Zheng|Jeff M. Phillips,cs.LG|cs.DS
2017-02-28T17:23:40Z,2017-02-13T01:31:46Z,http://arxiv.org/abs/1702.03605v1,http://arxiv.org/pdf/1702.03605v1,Nearly Instance Optimal Sample Complexity Bounds for Top-k Arm Selection,"In the Best-$k$-Arm problem we are given $n$ stochastic bandit arms each
associated with an unknown reward distribution. We are required to identify the
$k$ arms with the largest means by taking as few samples as possible. In this
paper we make progress towards a complete characterization of the
instance-wise sample complexity bounds for the Best-$k$-Arm problem. On the
lower bound side we obtain a novel complexity term to measure the sample
complexity that every Best-$k$-Arm instance requires. This is derived by an
interesting and nontrivial reduction from the Best-$1$-Arm problem. We also
provide an elimination-based algorithm that matches the instance-wise lower
bound within doubly-logarithmic factors. The sample complexity of our algorithm
strictly dominates the state-of-the-art for Best-$k$-Arm (module constant
factors).",Lijie Chen|Jian Li|Mingda Qiao,cs.LG|cs.DS|stat.ML
2017-02-28T17:23:44Z,2017-02-12T16:07:32Z,http://arxiv.org/abs/1702.03536v1,http://arxiv.org/pdf/1702.03536v1,A new lower bound for the on-line coloring of intervals with bandwidth,"The on-line interval coloring and its variants are important combinatorial
problems with many applications in network multiplexing resource allocation
and job scheduling. In this paper we present a new lower bound of $4.1626$ for
the competitive ratio for the on-line coloring of intervals with bandwidth
which improves the best known lower bound of $\frac{24}{7}$. For the on-line
coloring of unit intervals with bandwidth we improve the lower bound of $1.831$
to $2$.",Patryk Mikos,math.CO|cs.DS
2017-02-28T17:23:44Z,2017-02-11T02:50:27Z,http://arxiv.org/abs/1702.03375v1,http://arxiv.org/pdf/1702.03375v1,Derandomized Balanced Allocation,"In this paper we study the number of random bits in the explicit
constructions of hash functions and their maximum loads in the $d$-choice
schemes when allocate sequentially $n$ balls into $n$ bins. We consider the
\emph{Uniform-Greedy} scheme \cite{ABKU} which provides $d$ independent bins
for each ball and places the ball into the bin with the least load and its
non-uniform variant --- the \emph{Always-Go-Left} scheme introduced by
V\""ocking~\cite{Vocking}. We construct a hash function based on the previous
work of Celis et al.~\cite{CRSW} and show the following results.
  1. This hash function with $O(\log n \log \log n)$ random bits has a maximum
load of $\frac{\log \log n}{\log d} + O(1)$ with high probability in the
\emph{Uniform-Greedy} scheme which matches the maximum load of a perfectly
random hash function \cite{ABKUVocking}.
  2. This hash function with $O(\log n \log \log n)$ random bits has a maximum
load of $\frac{\log \log n}{d \log \phi_d} + O(1)$ with high probability in the
\emph{Always-Go-Left} scheme for a constant $\phi_d>1.61$ which matches the
maximum load of a perfectly random hash function \cite{Vocking}.
  Previously the best known hash function that guarantees the same maximum
loads as a perfectly random hash function in these two schemes was $O(\log
n)$-wise independent functions \cite{Vocking} which needs $\Theta(\log^2 n)$
random bits.",Xue Chen,cs.DS
2017-02-28T17:23:44Z,2017-02-10T17:27:35Z,http://arxiv.org/abs/1702.03259v1,http://arxiv.org/pdf/1702.03259v1,Fast and Compact Exact Distance Oracle for Planar Graphs,"For a given a graph a distance oracle is a data structure that answers
distance queries between pairs of vertices. We introduce an $O(n^{5/3})$-space
distance oracle which answers exact distance queries in $O(\log n)$ time for
$n$-vertex planar edge-weighted digraphs. All previous distance oracles for
planar graphs with truly subquadratic space (i.e. space $O(n^{2 - \epsilon})$
for some constant $\epsilon > 0$) either required query time polynomial in $n$
or could only answer approximate distance queries.",Vincent Cohen-Addad|Søren Dahlgaard|Christian Wulff-Nilsen,cs.DS
2017-02-28T17:23:44Z,2017-02-16T13:13:02Z,http://arxiv.org/abs/1702.03154v2,http://arxiv.org/pdf/1702.03154v2,Fast and scalable minimal perfect hashing for massive key sets,"Minimal perfect hash functions provide space-efficient and collision-free
hashing on static sets. Existing algorithms and implementations that build such
functions have practical limitations on the number of input elements they can
process due to high construction time RAM or external memory usage. We
revisit a simple algorithm and show that it is highly competitive with the
state of the art especially in terms of construction time and memory usage. We
provide a parallel C++ implementation called BBhash. It is capable of creating
a minimal perfect hash function of $10^{10}$ elements in less than 7 minutes
using 8 threads and 5 GB of memory and the resulting function uses 3.7
bits/element. To the best of our knowledge this is also the first
implementation that has been successfully tested on an input of cardinality
$10^{12}$. Source code: https://github.com/rizkg/BBHash",Antoine Limasset|Guillaume Rizk|Rayan Chikhi|Pierre Peterlongo,cs.DS
2017-02-28T17:23:44Z,2017-02-10T12:43:46Z,http://arxiv.org/abs/1702.03152v1,http://arxiv.org/pdf/1702.03152v1,A Variation of Levin Search for All Well-Defined Problems,"In 1973 L.A. Levin published an algorithm that solves any inversion problem
$\pi$ as quickly as the fastest algorithm $p^*$ computing a solution for $\pi$
in time bounded by $2^{l(p^*)}.t^*$ where $l(p^*)$ is the length of the binary
encoding of $p^*$ and $t^*$ is the runtime of $p^*$ plus the time to verify
its correctness. In 2002 M. Hutter published an algorithm that solves any
well-defined problem $\pi$ as quickly as the fastest algorithm $p^*$ computing
a solution for $\pi$ in time bounded by $5.t_{p}(x)+d_p.time_{t_{p}}(x)+c_p$
where $d_p=40.2^{l(p)+l(t_{p})}$ and $c_p=40.2^{l(f)+1}.O(l(f)^2)$ where
$l(f)$ is the length of the binary encoding of a proof $f$ that produces a pair
$(pt_p)$ where $t_p(x)$ is a provable time bound on the runtime of the
fastest program $p$ provably equivalent to $p^*$. In this paper we rewrite
Levin Search using the ideas of Hutter so that we have a new simple algorithm
that solves any well-defined problem $\pi$ as quickly as the fastest algorithm
$p^*$ computing a solution for $\pi$ in time bounded by $O(l(f)^2).t_p(x)$.",Fouad B. Chedid,cs.CC|cs.DS
2017-02-28T17:23:44Z,2017-02-25T06:15:31Z,http://arxiv.org/abs/1702.03106v2,http://arxiv.org/pdf/1702.03106v2,A Las Vegas approximation algorithm for metric $1$-median selection,"Given an $n$-point metric space consider the problem of finding a point with
the minimum sum of distances to all points. We show that this problem has a
randomized algorithm that {\em always} outputs a $(2+\epsilon)$-approximate
solution in an expected $O(n/\epsilon^2)$ time for each constant $\epsilon>0$.
Inheriting Indyk's algorithm our algorithm outputs a
$(1+\epsilon)$-approximate $1$-median in $O(n/\epsilon^2)$ time with
probability $\Omega(1)$.",Ching-Lueh Chang,cs.DS
2017-02-28T17:23:44Z,2017-02-09T20:11:49Z,http://arxiv.org/abs/1702.02970v1,http://arxiv.org/pdf/1702.02970v1,The Price of Selection in Differential Privacy,"In the differentially private top-$k$ selection problem we are given a
dataset $X \in \{\pm 1\}^{n \times d}$ in which each row belongs to an
individual and each column corresponds to some binary attribute and our goal
is to find a set of $k \ll d$ columns whose means are approximately as large as
possible. Differential privacy requires that our choice of these $k$ columns
does not depend too much on any on individual's dataset. This problem can be
solved using the well known exponential mechanism and composition properties of
differential privacy. In the high-accuracy regime where we require the error
of the selection procedure to be to be smaller than the so-called sampling
error $\alpha \approx \sqrt{\ln(d)/n}$ this procedure succeeds given a dataset
of size $n \gtrsim k \ln(d)$.
  We prove a matching lower bound showing that a dataset of size $n \gtrsim k
\ln(d)$ is necessary for private top-$k$ selection in this high-accuracy
regime. Our lower bound is the first to show that selecting the $k$ largest
columns requires more data than simply estimating the value of those $k$
columns which can be done using a dataset of size just $n \gtrsim k$.",Mitali Bafna|Jonathan Ullman,cs.DS|cs.CR
2017-02-28T17:23:44Z,2017-02-09T18:46:54Z,http://arxiv.org/abs/1702.02937v1,http://arxiv.org/pdf/1702.02937v1,"A Generalization of Permanent Inequalities and Applications in Counting
  and Optimization","A polynomial $p\in\mathbb{R}[z_1\dotsz_n]$ is real stable if it has no
roots in the upper-half complex plane. Gurvits's permanent inequality gives a
lower bound on the coefficient of the $z_1z_2\dots z_n$ monomial of a real
stable polynomial $p$ with nonnegative coefficients. This fundamental
inequality has been used to attack several counting and optimization problems.
  Here we study a more general question: Given a stable multilinear polynomial
$p$ with nonnegative coefficients and a set of monomials $S$ we show that if
the polynomial obtained by summing up all monomials in $S$ is real stable then
we can lowerbound the sum of coefficients of monomials of $p$ that are in $S$.
We also prove generalizations of this theorem to (real stable) polynomials that
are not multilinear. We use our theorem to give a new proof of Schrijver's
inequality on the number of perfect matchings of a regular bipartite graph
generalize a recent result of Nikolov and Singh and give deterministic
polynomial time approximation algorithms for several counting problems.",Nima Anari|Shayan Oveis Gharan,cs.DS|cs.DM|cs.IT|math.CO|math.IT|math.PR
2017-02-28T17:23:44Z,2017-02-09T16:51:34Z,http://arxiv.org/abs/1702.02891v1,http://arxiv.org/pdf/1702.02891v1,Sparse Approximation by Semidefinite Programming,"The problem of sparse approximation and the closely related compressed
sensing have received tremendous attention in the past decade. Primarily
studied from the viewpoint of applied harmonic analysis and signal processing
there have been two dominant algorithmic approaches to this problem: Greedy
methods called the matching pursuit (MP) and the linear programming based
approaches called the basis pursuit (BP). The aim of the current paper is to
bring a fresh perspective to sparse approximation by treating it as a
combinatorial optimization problem and providing an algorithm based on the
powerful optimization technique semidefinite programming (SDP). In particular
we show that there is a randomized algorithm based on a semidefinite relaxation
of the problem with performance guarantees depending on the coherence and the
restricted isometry constant of the dictionary used. We then show a
derandomization of the algorithm based on the method of conditional
probabilities.",Ali Çivril,cs.IT|cs.DS|math.IT
2017-02-28T17:23:44Z,2017-02-09T14:32:14Z,http://arxiv.org/abs/1702.02848v1,http://arxiv.org/pdf/1702.02848v1,Distributed Domination on Graph Classes of Bounded Expansion,"We provide a new constant factor approximation algorithm for the (connected)
distance-$r$ dominating set problem on graph classes of bounded expansion.
Classes of bounded expansion include many familiar classes of sparse graphs
such as planar graphs and graphs with excluded (topological) minors and
notably these classes form the most general subgraph closed classes of graphs
for which a sequential constant factor approximation algorithm for the
distance-$r$ dominating set problem is currently known. Our algorithm can be
implemented in the $\mathcal{CONGEST}_{\mathrm{BC}}$ model of distributed
computing and uses $\mathcal{O}(r^2 \log n)$ communication rounds.
  Our techniques which may be of independent interest are based on a
distributed computation of sparse neighborhood covers of small radius on
bounded expansion classes. We show how to compute an $r$-neighborhood cover of
radius $2r$ and overlap $f(r)$ on every class of bounded expansion in
$\mathcal{O}(r^2\log n)$ communication rounds.
  Finally we show how to use the greater power of the $\mathcal{LOCAL}$ model
to turn any distance-$r$ dominating set into a constantly larger connected
distance-$r$ dominating set in $3r+1$ rounds on any class of bounded expansion.
Combining this algorithm e.g. with the constant factor approximation
algorithm for dominating sets on planar graphs of Lenzen et al. gives a
constant factor approximation algorithm for connected dominating sets on planar
graphs in a constant number of rounds in the $\mathcal{LOCAL}$ model where the
approximation ratio is only $6$ times larger than that of Lenzen et al.'s
algorithm.",Saeed Akhoondian Amiri|Patrice Ossona de Mendez|Roman Rabinovich|Sebastian Siebertz,cs.DC|cs.DS
2017-02-28T17:23:48Z,2017-02-09T03:51:01Z,http://arxiv.org/abs/1702.02693v1,http://arxiv.org/pdf/1702.02693v1,Dichotomy for Real Holant$^c$ Problems,"Holant problems capture a class of Sum-of-Product computations such as
counting matchings. It is inspired by holographic algorithms and is equivalent
to tensor networks with counting CSP being a special case. A classification
for Holant problems is more difficult to prove not only because it implies a
classification for counting CSP but also due to the deeper reason that there
exist more intricate polynomial time tractable problems in the broader
framework.
  We discover a new family of constraint functions $\mathscr{L}$ which define
polynomial time computable counting problems. These do not appear in counting
CSP and no newly discovered tractable constraints can be symmetric. It has a
delicate support structure related to error-correcting codes. Local holographic
transformations is fundamental in its tractability. We prove a complexity
dichotomy theorem for all Holant problems defined by any real valued constraint
function set on Boolean variables and contains two 0-1 pinning functions.
Previously dichotomy for the same framework was only known for symmetric
constraint functions. he set $\mathscr{L}$ supplies the last piece of
tractability. We also prove a dichotomy for a variant of counting CSP as a
technical component toward this Holant dichotomy.",Jin-Yi Cai|Pinyan Lu|Mingji Xia,cs.CC|cs.DS
2017-02-28T17:23:48Z,2017-02-18T16:22:01Z,http://arxiv.org/abs/1702.02559v2,http://arxiv.org/pdf/1702.02559v2,Maximum Matching in Two Three and a Few More Passes Over Graph Streams,"We consider the maximum matching problem in the semi-streaming model
formalized by Feigenbaum Kannan McGregor Suri and Zhang that is inspired by
giant graphs of today. As our main result we give a two-pass $(1/2 +
1/16)$-approximation algorithm for triangle-free graphs and a two-pass $(1/2 +
1/32)$-approximation algorithm for general graphs; these improve the
approximation ratios of $1/2 + 1/52$ for bipartite graphs and $1/2 + 1/140$ for
general graphs by Konrad Magniez and Mathieu. In three passes we achieve
approximation ratios of $1/2 + 1/10$ for triangle-free graphs and $1/2 +
1/19.753$ for general graphs. We also give a multi-pass algorithm where we
bound the number of passes precisely---we give a $(2/3
-\varepsilon)$-approximation algorithm that uses $2/(3\varepsilon)$ passes for
triangle-free graphs and $4/(3\varepsilon)$ passes for general graphs. Our
algorithms are simple and combinatorial use $O(n \log n)$ space and have
$O(1)$ update time per edge.
  For general graphs our multi-pass algorithm improves the best known
deterministic algorithms in terms of the number of passes:
  --Ahn and Guha give a $(2/3 - \varepsilon)$-approximation algorithm that uses
$O(\log(1/\varepsilon)/\varepsilon^2)$ passes whereas our $(2/3 -
\varepsilon)$-approximation algorithm uses $4/(3\varepsilon)$ passes;
  --they also give a $(1-\varepsilon)$-approximation algorithm that uses
$O(\log n \cdot$ poly$(1/\varepsilon))$ passes where $n$ is the number of
vertices of the input graph; although our algorithm is $(2/3 -
\varepsilon)$-approximation our number of passes do not depend on $n$.
  Earlier multi-pass algorithms either have a large constant inside big-$O$
notation for the number of passes or the constant cannot be determined due to
the involved analysis so our multi-pass algorithm should use much fewer passes
for approximation ratios bounded slightly below $2/3$.",Sagar Kale|Sumedh Tirodkar|Sundar Vishwanathan,cs.DS
2017-02-28T17:23:48Z,2017-02-16T20:52:49Z,http://arxiv.org/abs/1702.02547v2,http://arxiv.org/pdf/1702.02547v2,Parallel algorithms for the Lopsided Lovász Local Lemma,"The Lov\'{a}sz Local Lemma (LLL) shows that if a set of collection of ""bad""
events $\mathcal B$ in a probability space are not too likely and not too
interdependent then there is a positive probability that no bad-events in
$\mathcal B$ occur. Moser & Tardos (2010) gave sequential and parallel
algorithms which transformed most applications of the variable-assignment LLL
into efficient (parallel and sequential) algorithms.
  There has been limited success in developing parallel algorithms for more
generalized forms of the LLL. Harris (2016) developed RNC algorithms for the
variable-assignment Lopsided Lov\'{a}sz Local Lemma (LLLL) and Harris &
Srinivasan (2014) developed an algorithm for the permutation LLL. Kolmogorov
(2016) developed a framework which partially parallelizes LLL settings such as
random matchings of $K_n$ although key algorithm components remained missing.
  We give new parallel algorithms for these forms of the LLLL. These algorithms
cover probability spaces for which no previous RNC algorithm was known
including matchings and hamiltonian cycles of $K_n$. They cover the
variable-assignment LLLL and permutation LLL as well and there they are are
simpler faster and more general than the algorithms of Harris and Harris &
Srinivasan.
  We also provide a new algebraic framework for applications of the LLLL to
permutations providing a unified treatment of matchings of $K_n$ hamiltonian
cycles of $K_n$ and random permutations.
  A key building block for our parallel algorithm is a new primitive for
parallel computing which we refer to as the exicographically-first
maximal-independent-set of a directed graph. We give an efficient algorithm for
constructing this and show that it is precisely what is needed for our LLLL
algorithms. This generalizes an algorithm given by Blelloch Fineman Shun
(2012) for undirected graphs.",David G. Harris,cs.DS|cs.DM|math.CO
2017-02-28T17:23:48Z,2017-02-08T15:14:05Z,http://arxiv.org/abs/1702.02460v1,http://arxiv.org/pdf/1702.02460v1,"Deterministic Backbone Creation in an SINR Network without Knowledge of
  Location","For a given network a backbone is an overlay network consisting of a
connected dominating set with additional accessibility properties. Once a
backbone is created for a network it can be utilized for fast communication
amongst the nodes of the network.
  The Signal-to-Interference-plus-Noise-Ratio (SINR) model has become the
standard for modeling communication among devices in wireless networks. For
this model the community has pondered what the most realistic solutions for
communication problems in wireless networks would look like. Such solutions
would have the characteristic that they would make the least number of
assumptions about the availability of information about the participating
nodes. Solving problems when nothing at all is known about the network and
having nodes just start participating would be ideal. However this is quite
challenging and most likely not feasible. The pragmatic approach is then to
make meaningful assumptions about the available information and present
efficient solutions based on this information.
  We present a solution for creation of backbone in the SINR model when nodes
do not have access to their physical coordinates or the coordinates of other
nodes in the network. This restriction models the deployment of nodes in
various situations for sensing hurricanes cyclones and so on where only
information about nodes prior to their deployment may be known but not their
actual locations post deployment. We assume that nodes have access to knowledge
of their label the labels of nodes within their neighborhood the range from
which labels are taken $[N]$ and the total number of participating nodes $n$.
We also assume that nodes wake up spontaneously. We present an efficient
deterministic protocol to create a backbone with a round complexity of
$O(\Delta \lg^2 N)$.",Dariusz R. Kowalski|William K. Moses Jr.|Shailesh Vaya,cs.DC|cs.DS|F.2.2; G.2.2
2017-02-28T17:23:48Z,2017-02-08T14:56:25Z,http://arxiv.org/abs/1702.02455v1,http://arxiv.org/pdf/1702.02455v1,"Deterministic Protocols in the SINR Model without Knowledge of
  Coordinates","Much work has been developed for studying the classical broadcasting problem
in the SINR (Signal-to-Interference-plus-Noise-Ratio) model for wireless device
transmission. The setting typically studied is when all radio nodes transmit a
signal of the same strength. This work studies the challenging problem of
devising a distributed algorithm for multi-broadcasting assuming a subset of
nodes are initially awake for the SINR model when each device only has access
to knowledge about the total number of nodes in the network $n$ the range from
which each node's label is taken $[1\dotsN]$ and the label of the device
itself. Specifically we assume no knowledge of the physical coordinates of
devices and also no knowledge of neighborhood of each node.
  We present a deterministic protocol for the multi-broadcast problem in $O(n
\lg^2 N \lg n)$ rounds for this setting assuming that only a subset of all
$n$ nodes are initially awake. A lower bound of $\Omega(n \lg N)$ rounds is
known for this but even that assumes knowledge of physical coordinates. All
previous results in the literature are either randomized or intensively use the
physical coordinates of the nodes and/or knowledge of the labels of nodes'
neighbors.
  In addition to the above result we present algorithms to achieve
multi-broadcast in $O(n \lg^2 N)$ rounds and create a backbone in $O(n \lg^2
N)$ rounds assuming that all nodes are initially awake. For a given backbone
messages can be exchanged between every pair of connected nodes in the backbone
in $O(\lg N)$ rounds and between any node and its leader in the backbone in
$O(\Delta \lg N)$ rounds.",William K. Moses Jr.|Shailesh Vaya,cs.DC|cs.DS|F.2.2; G.2.2
2017-02-28T17:23:48Z,2017-02-08T12:46:12Z,http://arxiv.org/abs/1702.02405v1,http://arxiv.org/pdf/1702.02405v1,"A family of approximation algorithms for the maximum duo-preservation
  string mapping problem","In the Maximum Duo-Preservation String Mapping Problem we are given two
strings and wish to map the letters of the former to the letters of the latter
as to maximise the number of duos. A duo is a pair of consecutive letters that
is mapped to a pair of consecutive letters in the same order. This is
complementary to the well-studied Minimum Common String Partition Problem
where the goal is to partition the former string into blocks that can be
permuted and concatenated to obtain the latter string.
  Maximum Duo-Preservation String Mapping Problem is APX-hard. After a series
of improvements Brubach [WABI 2016] showed a polynomial-time
$3.25$-approximation algorithm. Our main contribution is that for any
$\epsilon>0$ there exists a polynomial-time $(2+\epsilon)$-approximation
algorithm. Similarly to a previous solution by Boria et al. [CPM 2016] our
algorithm uses the local search technique. However this is used only after a
certain preliminary greedy procedure which gives us more structure and makes a
more general local search possible. We complement this with a specialised
version of the algorithm that achieves $2.67$-approximation in quadratic time.",Bartłomiej Dudek|Paweł Gawrychowski|Piotr Ostropolski-Nalewaja,cs.DS
2017-02-28T17:23:48Z,2017-02-08T08:28:46Z,http://arxiv.org/abs/1702.02321v1,http://arxiv.org/pdf/1702.02321v1,Position Heaps for Parameterized Strings,"We propose a new indexing structure for parameterized strings called
parameterized position heap. Parameterized position heap is applicable for
parameterized pattern matching problem where the pattern matches a substring
of the text if there exists a bijective mapping from the symbols of the pattern
to the symbols of the substring.We proposed an online construction algorithm of
parameterized position heap and show that our algorithm runs in linear time
with respect to the text size. We also proposed a parameterized pattern
matching algorithm using parameterized position heap that runs in linear time
respect to with the pattern size.",Diptarama|Takashi Katsura|Yuhei Otomo|Kazuyuki Narisawa|Ayumi Shinohara,cs.DS|F.2.2
2017-02-28T17:23:48Z,2017-02-08T04:28:05Z,http://arxiv.org/abs/1702.02279v1,http://arxiv.org/pdf/1702.02279v1,Decoding from Pooled Data: Phase Transitions of Message Passing,"We consider the problem of decoding a discrete signal of categorical
variables from the observation of several histograms of pooled subsets of it.
We present an Approximate Message Passing (AMP) algorithm for recovering the
signal in the random dense setting where each observed histogram involves a
random subset of entries of size proportional to n. We characterize the
performance of the algorithm in the asymptotic regime where the number of
observations $m$ tends to infinity proportionally to n by deriving the
corresponding State Evolution (SE) equations and studying their dynamics. We
initiate the analysis of the multi-dimensional SE dynamics by proving their
convergence to a fixed point along with some further properties of the
iterates. The analysis reveals sharp phase transition phenomena where the
behavior of AMP changes from exact recovery to weak correlation with the signal
as m/n crosses a threshold. We derive formulae for the threshold in some
special cases and show that they accurately match experimental behavior.",Ahmed El Alaoui|Aaditya Ramdas|Florent krzakala|Lenka Zdeborova|Michael I. Jordan,cs.IT|cs.DS|math.IT
2017-02-28T17:23:48Z,2017-02-08T03:52:40Z,http://arxiv.org/abs/1702.02267v1,http://arxiv.org/pdf/1702.02267v1,Matrix Completion from $O(n)$ Samples in Linear Time,"We consider the problem of reconstructing a rank-$k$ $n \times n$ matrix $M$
from a sampling of its entries. Under a certain incoherence assumption on $M$
and for the case when both the rank and the condition number of $M$ are
bounded it was shown in \cite{CandesRecht2009 CandesTao2010 keshavan2010
Recht2011 Jain2012 Hardt2014} that $M$ can be recovered exactly or
approximately (depending on some trade-off between accuracy and computational
complexity) using $O(n \ \text{poly}(\log n))$ samples in super-linear time
$O(n^{a} \ \text{poly}(\log n))$ for some constant $a \geq 1$.
  In this paper we propose a new matrix completion algorithm using a novel
sampling scheme based on a union of independent sparse random regular bipartite
graphs. We show that under the same conditions w.h.p. our algorithm recovers an
$\epsilon$-approximation of $M$ in terms of the Frobenius norm using $O(n
\log^2(1/\epsilon))$ samples and in linear time $O(n \log^2(1/\epsilon))$. This
provides the best known bound on the sample complexity and computational cost
for reconstructing an unknown low-rank matrix.
  The novelty of our algorithm is a new step of thresholding singular values
and rescaling singular vectors in the application of the ""vanilla"" alternating
minimization algorithm. The structure of sparse random regular graphs is used
heavily for controlling the impact of these regularization steps.",David Gamarnik|Quan Li|Hongyi Zhang,stat.ML|cs.DS|cs.LG|math.OC
2017-02-28T17:23:48Z,2017-02-07T18:46:02Z,http://arxiv.org/abs/1702.02133v1,http://arxiv.org/pdf/1702.02133v1,Towards A Unified View of Linear Structure on Graph Classes,"Graph searching a mechanism to traverse a graph in a spe- cific manner is a
powerful tool that has led to a number of elegant algo- rithms. This paper
focuses on lexicographic breadth first search (LexBFS) (a variant of BFS)
introduced in [8] to recognize chordal graphs. Since then there has been a
number of elegant LexBFS based often multi-sweep algorithms. These are
algorithms that compute a sequence of LexBFS orderings {\sigma}1  . . . 
{\sigma}k  and use {\sigma}i to break ties when computing {\sigma}i+1 . The
tie-breaking rule is often referred to as a + rule and the graph search as
LexBFS+ . We write LexBFS+ (G {\sigma}i ) = {\sigma}i+1 . In this paper we
prove that LexBFS converges for a number of graph families including interval
graphs cobipartite graphs trees and domino-free cocomparability graphs. In
particular we prove a fixed point theorem for LexBFS that shows the existence
of two orderings {\sigma} and {\pi} where LexBFS+ (G {\sigma}) = {\pi} and
LexBFS+ (G {\pi}) = {\sigma}. A consequence of this result in particular is
the simplest algorithm to compute a transitive orientation for these graph
classes as well as providing a unified view of the linear structure seen on
various graph families. In addition to this algorithmic consequence we provide
fixed point theorems for proper interval interval cobipartite and
domino-free cocomparability graphs as well as trees. These are the first
non-trivial results of this kind.",Pierre Charbit|Michel Habib|Lalla Mouatadid|Reza Naserasr,cs.DS
