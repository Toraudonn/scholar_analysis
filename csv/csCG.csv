2017-02-28T17:21:19Z,2017-02-27T17:07:31Z,http://arxiv.org/abs/1702.08380v1,http://arxiv.org/pdf/1702.08380v1,Exploring Increasing-Chord Paths and Trees,"A straight-line drawing $\Gamma$ of a graph $G=(VE)$ is a drawing of $G$ in
the Euclidean plane where every vertex in $G$ is mapped to a distinct point
and every edge in $G$ is mapped to a straight line segment between their
endpoints. A path $P$ in $\Gamma$ is called increasing-chord if for every four
points (not necessarily vertices) $abcd$ on $P$ in this order the Euclidean
distance between $bc$ is at most the Euclidean distance between $ad$. A
spanning tree $T$ rooted at some vertex $r$ in $\Gamma$ is called
increasing-chord if $T$ contains an increasing-chord path from $r$ to every
vertex in $T$. In this paper we prove that given a vertex $r$ in a
straight-line drawing $\Gamma$ it is NP-complete to determine whether $\Gamma$
contains an increasing-chord spanning tree rooted at $r$. We conjecture that
finding an increasing-chord path between a pair of vertices in $\Gamma$ which
is an intriguing open problem posed by Alamdari et al. is also NP-complete
and show a (non-polynomial) reduction from the 3-SAT problem.",Yeganeh Bahoo|Stephane Durocher|Sahar Mehrpour|Debajyoti Mondal,cs.CG
2017-02-28T17:21:19Z,2017-02-25T13:55:53Z,http://arxiv.org/abs/1702.07893v1,http://arxiv.org/pdf/1702.07893v1,The Persistent Homotopy Type Distance,"We introduce the persistent homotopy type distance dHT to compare real valued
functions defined on possibly different homotopy equivalent topological spaces.
The underlying idea in the definition of dHT is to measure the minimal shift
that is necessary to apply to one of the two functions in order that the
sublevel sets of the two functions become homotopically equivalent. This
distance is interesting in connection with persistent homology. Indeed our
main result states that dHT still provides an upper bound for the bottleneck
distance between the persistence diagrams of the intervening functions.
Moreover because homotopy equivalences are weaker than homeomorphisms this
implies a lifting of the standard stability results provided by the L-infty
distance and the natural pseudo-distance dNP. From a different standpoint we
prove that dHT extends the L-infty distance and dNP in two ways. First we show
that appropriately restricting the category of objects to which dHT applies
it can be made to coincide with the other two distances. Finally we show that
dHT has an interpretation in terms of interleavings that naturally places it in
the family of distances used in persistence theory.",Patrizio Frosini|Claudia Landi|Facundo Memoli,cs.CG|math.AT
2017-02-28T17:21:19Z,2017-02-24T14:06:31Z,http://arxiv.org/abs/1702.07589v1,http://arxiv.org/pdf/1702.07589v1,Generalization of Schnyder woods to orientable surfaces and applications,"Schnyder woods are particularly elegant combinatorial structures with
numerous applications concerning planar triangulations and more generally
3-connected planar maps. We propose a simple generalization of Schnyder woods
from the plane to maps on orientable surfaces of any genus with a special
emphasis on the toroidal case. We provide a natural partition of the set of
Schnyder woods of a given map into distributive lattices depending on the
surface homology. In the toroidal case we show the existence of particular
Schnyder woods with some global properties that are useful for optimal encoding
or graph drawing purpose.",Benjamin Lévêque,cs.DM|cs.CG|math.CO
2017-02-28T17:21:19Z,2017-02-24T12:25:43Z,http://arxiv.org/abs/1702.07555v1,http://arxiv.org/pdf/1702.07555v1,A generalization of crossing families,"For a set of points in the plane a \emph{crossing family} is a set of line
segments each joining two of the points such that any two line segments
cross. We investigate the following generalization of crossing families: a
\emph{spoke set} is a set of lines drawn through a point set such that each
unbounded region of the induced line arrangement contains at least one point of
the point set. We show that every point set has a spoke set of size
$\sqrt{\frac{n}{8}}$. We also characterize the matchings obtained by selecting
exactly one point in each unbounded region and connecting every such point to
the point in the antipodal unbounded region.",Patrick Schnider,cs.CG
2017-02-28T17:21:19Z,2017-02-23T21:32:10Z,http://arxiv.org/abs/1702.07399v1,http://arxiv.org/pdf/1702.07399v1,"An Optimal Algorithm for Computing the Spherical Depth of Points in the
  Plane","For a distribution function $F$ on $\mathbb{R}^d$ and a point $q\in
\mathbb{R}^d$ the \emph{spherical depth} $\SphD(q;F)$ is defined to be the
probability that a point $q$ is contained inside a random closed hyper-ball
obtained from a pair of points from $F$. The spherical depth $\SphD(q;S)$ is
also defined for an arbitrary data set $S\subseteq \mathbb{R}^d$ and $q\in
\mathbb{R}^d$. This definition is based on counting all of the closed
hyper-balls obtained from pairs of points in $S$ that contain $q$. The
significant advantage of using the spherical depth in multivariate data
analysis is related to its complexity of computation. Unlike most other data
depths the time complexity of the spherical depth grows linearly rather than
exponentially in the dimension $d$. The straightforward algorithm for computing
the spherical depth in dimension $d$ takes $O(dn^2)$. The main result of this
paper is an optimal algorithm that we present for computing the bivariate
spherical depth. The algorithm takes $O(n \log n)$ time. By reducing the
problem of \textit{Element Uniqueness} we prove that computing the spherical
depth requires $\Omega(n \log n)$ time. Some geometric properties of spherical
depth are also investigated in this paper. These properties indicate that
\emph{simplicial depth} ($\SD$) (Liu 1990) is linearly bounded by spherical
depth (in particular $\SphD\geq \frac{2}{3}SD$). To illustrate this
relationship between the spherical depth and the simplicial depth some
experimental results are provided. The obtained experimental bound ($\SphD\geq
2\SD$) indicates that perhaps a stronger theoretical bound can be achieved.",David Bremner|Rasoul Shahsavarifar,cs.CG
2017-02-28T17:21:19Z,2017-02-22T15:03:08Z,http://arxiv.org/abs/1702.06829v1,http://arxiv.org/pdf/1702.06829v1,A Simple Convex Layers Algorithm,"Given a set of $n$ points $P$ in the plane the first layer $L_1$ of $P$ is
formed by the points that appear on $P$'s convex hull. In general a point
belongs to layer $L_i$ if it lies on the convex hull of the set $P \setminus
\bigcup_{j<i}\{L_j\}$. The \emph{convex layers problem} is to compute the
convex layers $L_i$. Existing algorithms for this problem either do not achieve
the optimal $\mathcal{O}\left(n\log n\right)$ runtime and linear space or are
overly complex and difficult to apply in practice. We propose a new algorithm
that is both optimal and simple. The simplicity is achieved by independently
computing four sets of monotone convex chains in $\mathcal{O}\left(n\log
n\right)$ time and linear space. These are then merged in
$\mathcal{O}\left(n\log n\right)$ time.",Raimi A. Rufai|Dana S. Richard,cs.CG|cs.DS|68W99|I.3.5
2017-02-28T17:21:19Z,2017-02-20T20:14:46Z,http://arxiv.org/abs/1702.06163v1,http://arxiv.org/pdf/1702.06163v1,1-Fan-Bundle-Planar Drawings of Graphs,"Edge bundling is an important concept heavily used for graph visualization
purposes. To enable the comparison with other established nearly-planarity
models in graph drawing we formulate a new edge-bundling model which is
inspired by the recently introduced fan-planar graphs. In particular we
restrict the bundling to the endsegments of the edges. As in 1-planarity we
call our model 1-fan-bundle-planarity as we allow at most one crossing per
bundle.
  For the two variants where we allow either one or more naturally both
endsegments of each edge to be part of bundles we present edge density results
and consider various recognition questions not only for general graphs but
also for the outer and 2-layer variants. We conclude with a series of
challenging questions.",Patrizio Angelini|Michael A. Bekos|Michael Kaufmann|Philipp Kindermann|Thomas Schneck,cs.CG|math.CO
2017-02-28T17:21:19Z,2017-02-20T08:56:40Z,http://arxiv.org/abs/1702.05900v1,http://arxiv.org/pdf/1702.05900v1,$δ$-Greedy $t$-spanner,"We introduce a new geometric spanner $\delta$-Greedy whose construction is
based on a generalization of the known Path-Greedy and Gap-Greedy spanners. The
$\delta$-Greedy spanner combines the most desirable properties of geometric
spanners both in theory and in practice. More specifically it has the same
theoretical and practical properties as the Path-Greedy spanner: a natural
definition small degree linear number of edges low weight and strong
$(1+\varepsilon)$-spanner for every $\varepsilon>0$. The $\delta$-Greedy
algorithm is an improvement over the Path-Greedy algorithm with respect to the
number of shortest path queries and hence with respect to its construction
time. We show how to construct such a spanner for a set of $n$ points in the
plane in $O(n^2 \log n)$ time.
  The $\delta$-Greedy spanner has an additional parameter $\delta$ which
indicates how close it is to the Path-Greedy spanner on the account of the
number of shortest path queries. For $\delta = t$ the output spanner is
identical to the Path-Greedy spanner while the number of shortest path queries
is in practice linear.
  Finally we show that for a set of $n$ points placed independently at random
in a unit square the expected construction time of the $\delta$-Greedy
algorithm is $O(n \log n)$. Our analysis indicates that the $\delta$-Greedy
spanner gives the best results among the known spanners of expected $O(n \log
n)$ time for random point sets. Moreover the analysis implies that by setting
$\delta = t$ the $\delta$-Greedy algorithm provides a spanner identical to the
Path-Greedy spanner in expected $O(n \log n)$ time.",Gali Bar-On|Paz Carmi,cs.CG
2017-02-28T17:21:19Z,2017-02-19T15:48:11Z,http://arxiv.org/abs/1702.05760v1,http://arxiv.org/pdf/1702.05760v1,Hypercube LSH for approximate near neighbors,"A celebrated technique for finding near neighbors for the angular distance
involves using a set of \textit{random} hyperplanes to partition the space into
hash regions [Charikar STOC 2002]. Experiments later showed that using a set
of \textit{orthogonal} hyperplanes thereby partitioning the space into the
Voronoi regions induced by a hypercube leads to even better results [Terasawa
and Tanaka WADS 2007]. However no theoretical explanation for this
improvement was ever given and it remained unclear how the resulting hypercube
hash method scales in high dimensions.
  In this work we provide explicit asymptotics for the collision probabilities
when using hypercubes to partition the space. For instance two near-orthogonal
vectors are expected to collide with probability $(\frac{1}{\pi})^{d + o(d)}$
in dimension $d$ compared to $(\frac{1}{2})^d$ when using random hyperplanes.
Vectors at angle $\frac{\pi}{3}$ collide with probability
$(\frac{\sqrt{3}}{\pi})^{d + o(d)}$ compared to $(\frac{2}{3})^d$ for random
hyperplanes and near-parallel vectors collide with similar asymptotic
probabilities in both cases.
  For $c$-approximate nearest neighbor searching this translates to a decrease
in the exponent $\rho$ of locality-sensitive hashing (LSH) methods of a factor
up to $\log_2(\pi) \approx 1.652$ compared to hyperplane LSH. For $c = 2$ we
obtain $\rho \approx 0.302 + o(1)$ for hypercube LSH improving upon the $\rho
\approx 0.377$ for hyperplane LSH. We further describe how to use hypercube LSH
in practice and we consider an example application in the area of lattice
algorithms.",Thijs Laarhoven,cs.DS|cs.CC|cs.CG|cs.CR
2017-02-28T17:21:19Z,2017-02-18T17:02:58Z,http://arxiv.org/abs/1702.05633v1,http://arxiv.org/pdf/1702.05633v1,"Approximation Algorithms for Independence and Domination on B$_1$-VPG
  and B$_1$-EPG Graphs","A graph $G$ is called B$_k$-VPG (resp. B$_k$-EPG) for some constant $k\geq
0$ if it has a string representation on a grid such that each vertex is an
orthogonal path with at most $k$ bends and two vertices are adjacent in $G$ if
and only if the corresponding strings intersect (resp. the corresponding
strings share at least one grid edge). If two adjacent strings of a B$_k$-VPG
graph intersect exactly once then the graph is called a one-string B$_k$-VPG
graph.
  In this paper we study the Maximum Independent Set and Minimum Dominating
Set problems on B$_1$-VPG and B$_1$-EPG graphs. We first give a simple $O(\log
n)$-approximation algorithm for the Maximum Independent Set problem on
B$_1$-VPG graphs improving the previous $O((\log n)^2)$-approximation
algorithm of Lahiri et al. (COCOA 2015). Then we consider the Minimum
Dominating Set problem. We give an $O(1)$-approximation algorithm for this
problem on one-string B$_1$-VPG graphs providing the first constant-factor
approximation algorithm for this problem. Moreover we show that the Minimum
Dominating Set problem is APX-hard on B$_1$-EPG graphs ruling out the
possibility of a PTAS unless P=NP. Finally we give constant-factor
approximation algorithms for this problem on two non-trivial subclasses of
B$_1$-EPG graphs. To our knowledge these are the first results for the Minimum
Dominating Set problem on B$_1$-EPG graphs partially answering a question
posed by Epstein et al. (WADS 2013).",Saeed Mehrabi,cs.CG
2017-02-28T17:21:23Z,2017-02-17T16:07:53Z,http://arxiv.org/abs/1702.06188v1,http://arxiv.org/pdf/1702.06188v1,"Forest understory trees revealed using sufficiently dense airborne laser
  scanning point clouds","Airborne laser scanning (lidar) point clouds can be process to extract
tree-level information over large forested landscapes. Existing procedures
typically detect more than 90% of overstory trees yet they barely detect 60%
of understory trees because of reduced number of lidar points penetrating the
top canopy layer. Although understory trees provide limited financial value
they offer habitat for numerous wildlife species and are important for stand
development. Here we model tree identification accuracy according to point
cloud density by decomposing lidar point cloud into overstory and multiple
understory canopy layers estimating the fraction of points representing the
different layers and inspecting tree identification accuracy as a function of
point density. We show at a density of about 170 pt/m2 understory tree
identification accuracy likely plateaus which we regard as the required point
density for reasonable identification of understory trees. Given the
advancements of lidar sensor technology point clouds can feasibly reach the
required density to enable effective identification of individual understory
trees ultimately making remote quantification of forest resources more
accurate. The layer decomposition methodology can also be adopted for other
similar remote sensing or advanced imaging applications such as geological
subsurface modelling or biomedical tissue analysis.",Hamid Hamraz|Marco A. Contreras|Jun Zhang,cs.CV|cs.CG
2017-02-28T17:21:23Z,2017-02-17T14:34:08Z,http://arxiv.org/abs/1702.05358v1,http://arxiv.org/pdf/1702.05358v1,Computational topology of graphs on surfaces,"Computational topology is an area that revisits topological problems from an
algorithmic point of view and develops topological tools for improved
algorithms. We survey results in computational topology that are concerned with
graphs drawn on surfaces. Typical questions include representing surfaces and
graphs embedded on them computationally deciding whether a graph embeds on a
surface solving computational problems related to homotopy optimizing curves
and graphs on surfaces and solving standard graph algorithm problems more
efficiently in the case of surface-embedded graphs.",Éric Colin de Verdière,"cs.CG|cs.DM|cs.DS|math.AT|math.CO|68U05, 05C10, 57M15, 68R10|F.2.2; G.2.2; I.3.5"
2017-02-28T17:21:23Z,2017-02-17T09:19:02Z,http://arxiv.org/abs/1702.05265v1,http://arxiv.org/pdf/1702.05265v1,T-Shape Visibility Representations of 1-Planar Graphs,"A shape visibility representation displays a graph so that each vertex is
represented by an orthogonal polygon of a particular shape and for each edge
there is a horizontal or vertical line of sight between the polygons assigned
to its endvertices. Special shapes are rectangles L T E and H-shapes and
caterpillars. A flat rectangle is a horizontal bar of height $\epsilon>0$. A
graph is 1-planar if there is a drawing in the plane such that each edge is
crossed at most once and is IC-planar if in addition no two crossing edges
share a vertex.
  We show that every IC-planar graph has a flat rectangle visibility
representation and that every 1-planar graph has a T-shape visibility
representation. The representations use quadratic area and can be computed in
linear time from a given embedding.",Franz J. Brandenburg,cs.CG|68R10|G.2.2
2017-02-28T17:21:23Z,2017-02-15T14:59:10Z,http://arxiv.org/abs/1702.04641v1,http://arxiv.org/pdf/1702.04641v1,"Filling missing data in point clouds by merging structured and
  unstructured point clouds","Point clouds arising from structured data mainly as a result of CT scans
provides special properties on the distribution of points and the distances
between those. Yet often the amount of data provided can not compare to
unstructured point clouds i.e. data that arises from 3D light scans or laser
scans. This article hereby proposes an approach to extend structured data and
enhance the quality by inserting selected points from an unstructured point
cloud. The resulting point cloud still has a partial structure that is called
""half-structure"". In this way missing data that can not be optimally recovered
through other surface reconstruction methods can be completed.",Franziska Lippoldt|Hartmut Schwandt,cs.CG|cs.CV|cs.DM|53A05|F.2.2; G.2.1; I.3.5
2017-02-28T17:21:23Z,2017-02-14T15:20:23Z,http://arxiv.org/abs/1702.04259v1,http://arxiv.org/pdf/1702.04259v1,On the metastable Mabillard-Wagner conjecture,"The purpose of this note is to attract attention to the following conjecture
(metastable $r$-fold Whitney trick) by clarifying its status as not having a
complete proof in the sense described in the paper.
  Assume that $D=D_1\sqcup\ldots\sqcup D_r$ is disjoint union of $r$ disks of
dimension $s$ $f:D\to B^d$ a proper PL map such that $f\partial
D_1\cap\ldots\cap f\partial D_r=\emptyset$ $rd\ge (r+1)s+3$ and $d\ge s+3$. If
the map $$f^r:\partial(D_1\times\ldots\times D_r)\to
(B^d)^r-\{(xx\ldotsx)\in(B^d)^r\ |\ x\in B^d\}$$ extends to
$D_1\times\ldots\times D_r$ then there is a PL map $\overline f:D\to B^d$ such
that $$\overline f=f \quad\text{on}\quad D_r\cup\partial D\quad\text{and}\quad
\overline fD_1\cap\ldots\cap \overline fD_r=\emptyset.$$",A. Skopenkov,"math.GT|cs.CG|57Q35, 57R65, 52B99"
2017-02-28T17:21:23Z,2017-02-13T08:54:44Z,http://arxiv.org/abs/1702.03676v1,http://arxiv.org/pdf/1702.03676v1,Epsilon-approximations and epsilon-nets,"The use of random samples to approximate properties of geometric
configurations has been an influential idea for both combinatorial and
algorithmic purposes. This chapter considers two related
notions---$\epsilon$-approximations and $\epsilon$-nets---that capture the most
important quantitative properties that one would expect from a random sample
with respect to an underlying geometric configuration.",Nabil H. Mustafa|Kasturi Varadarajan,cs.CG|math.CO|math.PR
2017-02-28T17:21:23Z,2017-02-11T01:20:50Z,http://arxiv.org/abs/1702.03364v1,http://arxiv.org/pdf/1702.03364v1,Techniques in Lattice Basis Reduction,"The credit on {\it reduction theory} goes back to the work of Lagrange
Gauss Hermite Korkin Zolotarev and Minkowski. Modern reduction theory is
voluminous and includes the work of A. Lenstra H. Lenstra and L. Lovasz who
created the well known LLL algorithm and many other researchers such as L.
Babai and C. P. Schnorr who created significant new variants of basis reduction
algorithms. In this paper we propose and investigate the efficacy of new
optimization techniques to be used along with LLL algorithm. The techniques we
have proposed are: i) {\it hill climbing (HC)} ii) {\it lattice diffusion-sub
lattice fusion (LDSF)} and iii) {\it multistage hybrid LDSF-HC}. The first
technique relies on the sensitivity of LLL to permutations of the input basis
$B$ and optimization ideas over the symmetric group $S_m$ viewed as a metric
space. The second technique relies on partitioning the lattice into
sublattices performing basis reduction in the partition sublattice blocks
fusing the sublattices and repeating. We also point out places where parallel
computation can reduce run-times achieving almost linear speedup. The
multistage hybrid technique relies on the lattice diffusion and sublattice
fusion and hill climbing algorithms.",Bal K. Khadka|Spyros M. Magliveras,cs.CG
2017-02-28T17:21:23Z,2017-02-10T17:54:59Z,http://arxiv.org/abs/1702.03266v1,http://arxiv.org/pdf/1702.03266v1,Two Optimization Problems for Unit Disks,"We present an implementation of a recent algorithm to compute shortest-path
trees in unit disk graphs in $O(n\log n)$ worst-case time where $n$ is the
number of disks.
  In the minimum-separation problem we are given $n$ unit disks and two points
$s$ and $t$ not contained in any of the disks and we want to compute the
minimum number of disks one needs to retain so that any curve connecting $s$ to
$t$ intersects some of the retained disks. We present a new algorithm solving
this problem in $O(n^2\log^3 n)$ worst-case time and its implementation.",Sergio Cabello|Lazar Milinković,cs.CG
2017-02-28T17:21:23Z,2017-02-10T14:37:00Z,http://arxiv.org/abs/1702.03187v1,http://arxiv.org/pdf/1702.03187v1,On vertices and facets of combinatorial 2-level polytopes,"2-level polytopes naturally appear in several areas of pure and applied
mathematics including combinatorial optimization polyhedral combinatorics
communication complexity and statistics. In this paper we present a
polyhedral study of 2-level polytopes arising in combinatorial settings. For
all the known (to the best of our knowledge) such polytopes P we show that
v(P).f(P) is upper bounded by d2^(d+1). Here v(P) (resp. f(P)) is the number of
vertices (resp. facets) of P and d is its dimension. Whether this holds for
all 2-level polytopes was asked in [Bohn et al. ESA 2015] where experimental
results showed it true up to dimension 6. The key to most of our proofs is an
understanding of the combinatorial structures underlying those polytopes. This
leads to a number of results that we believe to be of independent interest: a
trade-off formula for the number of cliques and stable sets in a graph; a
description of the facets of the base polytope of the 2-sum of matroids; a
linear-size description of the base polytope of matroids that are 2-level in
terms of cuts of an associated tree. We also give a self-contained proof of the
characterization of the last class a result first obtained by Grande and
Sanyal.",Manuel Aprile|Alfonso Cevallos|Yuri Faenza,math.CO|cs.CG
2017-02-28T17:21:23Z,2017-02-09T14:04:20Z,http://arxiv.org/abs/1702.02838v1,http://arxiv.org/pdf/1702.02838v1,"The DTM-signature for a geometric comparison of metric-measure spaces
  from samples","In this paper we introduce the notion of DTM-signature a measure on R +
that can be associated to any metric-measure space. This signature is based on
the distance to a measure (DTM) introduced by Chazal Cohen-Steiner and
M\'erigot. It leads to a pseudo-metric between metric-measure spaces
upper-bounded by the Gromov-Wasserstein distance. Under some geometric
assumptions we derive lower bounds for this pseudo-metric. Given two
N-samples we also build an asymptotic statistical test based on the
DTM-signature to reject the hypothesis of equality of the two underlying
metric-measure spaces up to a measure-preserving isometry. We give strong
theoretical justifications for this test and propose an algorithm for its
implementation.",Claire Brécheteau,cs.CG|math.PR|math.ST|stat.TH
2017-02-28T17:21:27Z,2017-02-07T01:11:38Z,http://arxiv.org/abs/1702.01836v1,http://arxiv.org/pdf/1702.01836v1,Linear Time Approximation Schemes for Geometric Maximum Coverage,"We study approximation algorithms for the following geometric version of the
maximum coverage problem: Let $\mathcal{P}$ be a set of $n$ weighted points in
the plane. Let $D$ represent a planar object such as a rectangle or a disk.
We want to place $m$ copies of $D$ such that the sum of the weights of the
points in $\mathcal{P}$ covered by these copies is maximized. For any fixed
$\varepsilon>0$ we present efficient approximation schemes that can find a
$(1-\varepsilon)$-approximation to the optimal solution. In particular for
$m=1$ and for the special case where $D$ is a rectangle our algorithm runs in
time $O(n\log (\frac{1}{\varepsilon}))$ improving on the previous result. For
$m>1$ and the rectangular case our algorithm runs in
$O(\frac{n}{\varepsilon}\log (\frac{1}{\varepsilon})+\frac{m}{\varepsilon}\log
m +m(\frac{1}{\varepsilon})^{O(\min(\sqrt{m}\frac{1}{\varepsilon}))})$ time.
For a more general class of shapes (including disks polygons with $O(1)$
edges) our algorithm runs in
$O(n(\frac{1}{\varepsilon})^{O(1)}+\frac{m}{\epsilon}\log m +
m(\frac{1}{\varepsilon})^{O(\min(m\frac{1}{\varepsilon^2}))})$ time.",Kai Jin|Jian Li|Haitao Wang|Bowei Zhang|Ningye Zhang,cs.CG|F.2.2
2017-02-28T17:21:27Z,2017-02-06T21:31:57Z,http://arxiv.org/abs/1702.01799v1,http://arxiv.org/pdf/1702.01799v1,Radial Contour Labeling with Straight Leaders,"The usefulness of technical drawings as well as scientific illustrations such
as medical drawings of human anatomy essentially depends on the placement of
labels that describe all relevant parts of the figure. In order to not spoil or
clutter the figure with text the labels are often placed around the figure and
are associated by thin connecting lines to their features respectively. This
labeling technique is known as external label placement.
  In this paper we introduce a flexible and general approach for external label
placement assuming a contour of the figure prescribing the possible positions
of the labels. While much research on external label placement aims for fast
labeling procedures for interactive systems we focus on highest-quality
illustrations. Based on interviews with domain experts and a semi-automatic
analysis of 202 handmade anatomical drawings we identify a set of 18 layout
quality criteria naturally not all of equal importance. We design a new
geometric label placement algorithm that is based only on the most important
criteria. Yet other criteria can flexibly be included in the algorithm either
as hard constraints not to be violated or as soft constraints whose violation
is penalized by a general cost function. We formally prove that our approach
yields labelings that satisfy all hard constraints and have minimum overall
cost. Introducing several speedup techniques we further demonstrate how to
deploy our approach in practice. In an experimental evaluation on real-world
anatomical drawings we show that the resulting labelings are of high quality
and can be produced in adequate time.",Benjamin Niedermann|Martin Nöllenburg|Ignaz Rutter,cs.CG
2017-02-28T17:21:27Z,2017-02-06T17:38:26Z,http://arxiv.org/abs/1702.01719v1,http://arxiv.org/pdf/1702.01719v1,A 2-Approximation for the Height of Maximal Outerplanar Graph Drawings,"In this paper we study planar drawings of maximal outerplanar graphs with
the objective of achieving small height. A recent paper gave an algorithm for
such drawings that is within a factor of 4 of the optimum height. In this
paper we substantially improve the approximation factor to become 2. The main
ingredient is to define a new parameter of outerplanar graphs (the so-called
umbrella depth obtained by recursively splitting the graph into graphs called
umbrellas). We argue that the height of any poly-line drawing must be at least
the umbrella depth and then devise an algorithm that achieves height at most
twice the umbrella depth.",Therese Biedl|Philippe Demontigny,cs.DS|cs.CG
2017-02-28T17:21:27Z,2017-02-06T08:01:16Z,http://arxiv.org/abs/1702.01524v1,http://arxiv.org/pdf/1702.01524v1,"Edge N-Level Sparse Visibility Graphs: Fast Optimal Any-Angle
  Pathfinding Using Hierarchical Taut Paths","In the Any-Angle Pathfinding problem the goal is to find the shortest path
between a pair of vertices on a uniform square grid that is not constrained to
any fixed number of possible directions over the grid. Visibility Graphs are a
known optimal algorithm for solving the problem with the use of pre-processing.
However Visibility Graphs are known to perform poorly in terms of running
time especially on large complex maps. In this paper we introduce two
improvements over the Visibility Graph Algorithm to compute optimal paths.
Sparse Visibility Graphs (SVGs) are constructed by pruning unnecessary edges
from the original Visibility Graph. Edge N-Level Sparse Visibility Graphs
(ENLSVGs) is a hierarchical SVG built by iteratively pruning non-taut paths. We
also introduce Line-of-Sight Scans a faster algorithm for building Visibility
Graphs over a grid. SVGs run much faster than Visibility Graphs by reducing the
average vertex degree. ENLSVGs a hierarchical algorithm improves this
further especially on larger maps. On large maps with the use of
pre-processing these algorithms are orders of magnitude faster than existing
algorithms like Visibility Graphs and Theta*.",Shunhao Oh|Hon Wai Leong,cs.CG
2017-02-28T17:21:27Z,2017-02-09T01:46:20Z,http://arxiv.org/abs/1702.01446v2,http://arxiv.org/pdf/1702.01446v2,Efficient Algorithms for k-Regret Minimizing Sets,"A regret minimizing set Q is a small size representation of a much larger
database P so that user queries executed on Q return answers whose scores are
not much worse than those on the full dataset. In particular a k-regret
minimizing set has the property that the regret ratio between the score of the
top-1 item in Q and the score of the top-k item in P is minimized where the
score of an item is the inner product of the item's attributes with a user's
weight (preference) vector. The problem is challenging because we want to find
a single representative set Q whose regret ratio is small with respect to all
possible user weight vectors.
  We show that k-regret minimization is NP-Complete for all dimensions d >= 3.
This settles an open problem from Chester et al. [VLDB 2014] and resolves the
complexity status of the problem for all d: the problem is known to have
polynomial-time solution for d <= 2. In addition we propose two new
approximation schemes for regret minimization both with provable guarantees
one based on coresets and another based on hitting sets. We also carry out
extensive experimental evaluation and show that our schemes compute
regret-minimizing sets comparable in size to the greedy algorithm proposed in
[VLDB 14] but our schemes are significantly faster and scalable to large data
sets.",Pankaj K. Agarwal|Nirman Kumar|Stavros Sintos|Subhash Suri,cs.DS|cs.CG|cs.DB
2017-02-28T17:21:27Z,2017-02-04T11:51:50Z,http://arxiv.org/abs/1702.01277v1,http://arxiv.org/pdf/1702.01277v1,Geometric Biplane Graphs II: Graph Augmentation,"We study biplane graphs drawn on a finite point set $S$ in the plane in
general position. This is the family of geometric graphs whose vertex set is
$S$ and which can be decomposed into two plane graphs. We show that every
sufficiently large point set admits a 5-connected biplane graph and that there
are arbitrarily large point sets that do not admit any 6-connected biplane
graph. Furthermore we show that every plane graph (other than a wheel or a
fan) can be augmented into a 4-connected biplane graph. However there are
arbitrarily large plane graphs that cannot be augmented to a 5-connected
biplane graph by adding pairwise noncrossing edges.",Alfredo García|Ferran Hurtado|Matias Korman|Inês Matos|Maria Saumell|Rodrigo I. Silveira|Javier Tejel|Csaba D. Tóth,cs.CG
2017-02-28T17:21:27Z,2017-02-04T11:51:44Z,http://arxiv.org/abs/1702.01275v1,http://arxiv.org/pdf/1702.01275v1,Geometric Biplane Graphs I: Maximal Graphs,"We study biplane graphs drawn on a finite planar point set $S$ in general
position. This is the family of geometric graphs whose vertex set is $S$ and
can be decomposed into two plane graphs. We show that two maximal biplane
graphs---in the sense that no edge can be added while staying biplane---may
differ in the number of edges and we provide an efficient algorithm for adding
edges to a biplane graph to make it maximal. We also study extremal properties
of maximal biplane graphs such as the maximum number of edges and the largest
maximum connectivity over $n$-element point sets.",Alfredo García|Ferran Hurtado|Matias Korman|Inês Matos|Maria Saumell|Rodrigo I. Silveira|Javier Tejel|Csaba D. Tóth,cs.CG
2017-02-28T17:21:27Z,2017-02-02T22:20:25Z,http://arxiv.org/abs/1702.00849v1,http://arxiv.org/pdf/1702.00849v1,"On the union complexity of families of axis-parallel rectangles with a
  low packing number","Let R be a family of n axis-parallel rectangles with packing number p-1
meaning that among any p of the rectangles there are two with a non-empty
intersection. We show that the union complexity of R is at most O(n+p^2) and
that the (<=k)-level complexity of R is at most O(kn+k^2p^2). Both upper bounds
are tight.",Chaya Keller|Shakhar Smorodinsky,"math.CO|cs.CG|52C45, 52C15"
2017-02-28T17:21:27Z,2017-02-01T16:55:41Z,http://arxiv.org/abs/1702.00353v1,http://arxiv.org/pdf/1702.00353v1,"The non-cooperative tile assembly model is not intrinsically universal
  or capable of bounded Turing machine simulation","The field of algorithmic self-assembly is concerned with the computational
and expressive power of nanoscale self-assembling molecular systems. In the
well-studied cooperative or temperature 2 abstract tile assembly model it is
known that there is a tile set to simulate any Turing machine and an
intrinsically universal tile set that simulates the shapes and dynamics of any
instance of the model up to spatial rescaling. It has been an open question as
to whether the seemingly simpler noncooperative or temperature 1 model is
capable of such behaviour. Here we show that this is not the case by showing
that there is no tile set in the noncooperative model that is intrinsically
universal nor one capable of time-bounded Turing machine simulation within a
bounded region of the plane.
  Although the noncooperative model intuitively seems to lack the complexity
and power of the cooperative model it has been exceedingly hard to prove this.
One reason is that there have been few tools to analyse the structure of
complicated paths in the plane. This paper provides a number of such tools. A
second reason is that almost every obvious and small generalisation to the
model (e.g. allowing error 3D non-square tiles signals/wires on tiles tiles
that repel each other parallel synchronous growth) endows it with great
computational and sometimes simulation power. Our main results show that all
of these generalisations provably increase computational and/or simulation
power. Our results hold for both deterministic and nondeterministic
noncooperative systems. Our first main result stands in stark contrast with the
fact that for both the cooperative tile assembly model and for 3D
noncooperative tile assembly there are respective intrinsically universal
tilesets. Our second main result gives a new technique (reduction to
simulation) for proving negative results about computation in tile assembly.",Pierre-Étienne Meunier|Damien Woods,cs.CC|cs.CG|cs.DS
2017-02-28T17:21:27Z,2017-02-01T06:45:40Z,http://arxiv.org/abs/1702.00146v1,http://arxiv.org/pdf/1702.00146v1,Untangling Planar Curves,"Any generic closed curve in the plane can be transformed into a simple closed
curve by a finite sequence of local transformations called homotopy moves. We
prove that simplifying a planar closed curve with $n$ self-crossings requires
$\Theta(n^{3/2})$ homotopy moves in the worst case. Our algorithm improves the
best previous upper bound $O(n^2)$ which is already implicit in the classical
work of Steinitz; the matching lower bound follows from the construction of
closed curves with large defect a topological invariant of generic closed
curves introduced by Aicardi and Arnold. Our lower bound also implies that
$\Omega(n^{3/2})$ facial electrical transformations are required to reduce any
plane graph with treewidth $\Omega(\sqrt{n})$ to a single vertex matching
known upper bounds for rectangular and cylindrical grid graphs. More generally
we prove that transforming one immersion of $k$ circles with at most $n$
self-crossings into another requires $\Theta(n^{3/2} + nk + k^2)$ homotopy
moves in the worst case. Finally we prove that transforming one
noncontractible closed curve to another on any orientable surface requires
$\Omega(n^2)$ homotopy moves in the worst case; this lower bound is tight if
the curve is homotopic to a simple closed curve.",Hsien-Chih Chang|Jeff Erickson,cs.CG|math.GT
2017-02-28T17:21:30Z,2017-01-29T19:55:27Z,http://arxiv.org/abs/1701.08423v1,http://arxiv.org/pdf/1701.08423v1,One Size Fits All : Effectiveness of Local Search on Structured Data,"In this paper we analyze the performance of a simple and standard Local
Search algorithm for clustering on well behaved data. Since the seminal paper
by Ostrovsky Rabani Schulman and Swamy [FOCS 2006] much progress has been
made to characterize real-world instances. We distinguish the three main
definitions -- Distribution Stability (Awasthi Blum Sheffet FOCS 2010) --
Spectral Separability (Kumar Kannan FOCS 2010) -- Perturbation Resilience
(Bilu Linial ICS 2010) We show that Local Search performs well on the
instances with the aforementioned stability properties. Specifically for the
$k$-means and $k$-median objective we show that Local Search exactly recovers
the optimal clustering if the dataset is $3+\varepsilon$-perturbation
resilient and is a PTAS for distribution stability and spectral separability.
This implies the first PTAS for instances satisfying the spectral separability
condition. For the distribution stability condition we also go beyond previous
work by showing that the clustering output by the algorithm and the optimal
clustering are very similar. This is a significant step toward understanding
the success of Local Search heuristics in clustering applications and supports
the legitimacy of the stability conditions: They characterize some of the
structure of real-world instances that make Local Search a popular heuristic.",Vincent Cohen-Addad|Chris Schwiegelshohn,cs.DS|cs.CG|cs.LG
2017-02-28T17:21:30Z,2017-01-19T18:01:10Z,http://arxiv.org/abs/1701.05532v1,http://arxiv.org/pdf/1701.05532v1,Tighter Bounds for the Discrepancy of Boxes and Polytopes,"Combinatorial discrepancy is a complexity measure of a collection of sets
which quantifies how well the sets in the collection can be simultaneously
balanced. More precisely we are given an n-point set $P$ and a collection
$\mathcal{F} = \{F_1 ... F_m\}$ of subsets of $P$ and our goal is color $P$
with two colors red and blue so that the largest absolute difference between
the number of red elements and the number of blue elements (i.e. the
discrepancy) in any $F_i$ is minimized. Combinatorial discrepancy has many
applications in mathematics and computer science including constructions of
uniformly distributed point sets and lower bounds for data structures and
private data analysis algorithms.
  We investigate the combinatorial discrepancy of geometrically defined set
systems in which $P$ is an n-point set in $d$-dimensional space and
$\mathcal{F}$ is the collection of subsets of $P$ induced by dilations and
translations of a fixed convex polytope $B$. Such set systems include sets
induced by axis-aligned boxes whose discrepancy is the subject of the well
known Tusnady problem. We prove new discrepancy upper bounds for such set
systems by extending the approach based on factorization norms previously used
by the author and Matousek. We improve the best known upper bound for the
Tusnady problem by a logarithmic factor using a result of Banaszczyk on signed
series of vectors. We extend this improvement to any arbitrary convex polytope
B by using a decomposition due to Matousek.",Aleksandar Nikolov,math.CO|cs.CG
2017-02-28T17:21:30Z,2017-01-19T16:24:27Z,http://arxiv.org/abs/1701.05500v1,http://arxiv.org/pdf/1701.05500v1,The number of realizations of a Laman graph,"Laman graphs model planar frameworks that are rigid for a general choice of
distances between the vertices. There are finitely many ways up to isometries
to realize a Laman graph in the plane. Such realizations can be seen as
solutions of systems of quadratic equations prescribing the distances between
pairs of points. Using ideas from algebraic and tropical geometry we provide a
recursion formula for the number of complex solutions of such systems.",Jose Capco|Matteo Gallet|Georg Grasegger|Christoph Koutschan|Niels Lubbes|Josef Schicho,"math.AG|cs.CG|cs.SC|math.CO|14T05, 14N99, 52C25, 05C99"
2017-02-28T17:21:30Z,2017-01-19T15:33:50Z,http://arxiv.org/abs/1701.05475v1,http://arxiv.org/pdf/1701.05475v1,Irrational Guards are Sometimes Needed,"In this paper we study the art gallery problem which is one of the
fundamental problems in computational geometry. The objective is to place a
minimum number of guards inside a simple polygon such that the guards together
can see the whole polygon. We say that a guard at position $x$ sees a point $y$
if the line segment $xy$ is fully contained in the polygon.
  Despite an extensive study of the art gallery problem it remained an open
question whether there are polygons given by integer coordinates that require
guard positions with irrational coordinates in any optimal solution. We give a
positive answer to this question by constructing a monotone polygon with
integer coordinates that can be guarded by three guards only when we allow to
place the guards at points with irrational coordinates. Otherwise four guards
are needed. By extending this example we show that for every $n$ there is
polygon which can be guarded by $3n$ guards with irrational coordinates but
need $4n$ guards if the coordinates have to be rational. Subsequently we show
that there are rectilinear polygons given by integer coordinates that require
guards with irrational coordinates in any optimal solution.",Mikkel Abrahamsen|Anna Adamaszek|Tillmann Miltzow,cs.CG|cs.DM|math.CO
2017-02-28T17:21:30Z,2017-01-19T03:57:28Z,http://arxiv.org/abs/1701.05290v1,http://arxiv.org/pdf/1701.05290v1,"Range-efficient consistent sampling and locality-sensitive hashing for
  polygons","Locality-sensitive hashing (LSH) is a fundamental technique for similarity
search and similarity estimation in high-dimensional spaces. The basic idea is
that similar objects should produce hash collisions with probability
significantly larger than objects with low similarity. We consider LSH for
objects that can be represented as point sets in either one or two dimensions.
To make the point sets finite size we consider the subset of points on a grid.
Directly applying LSH (e.g. min-wise hashing) to these point sets would require
time proportional to the number of points. We seek to achieve time that is much
lower than direct approaches.
  Technically we introduce new primitives for range-efficient consistent
sampling (of independent interest) and show how to turn such samples into LSH
values. Another application of our technique is a data structure for quickly
estimating the size of the intersection or union of a set of preprocessed
polygons. Curiously our consistent sampling method uses transformation to a
geometric problem.",Joachim Gudmundsson|Rasmus Pagh,cs.CG|68U05|F.2.2
2017-02-28T17:21:30Z,2017-01-19T03:20:51Z,http://arxiv.org/abs/1701.05286v1,http://arxiv.org/pdf/1701.05286v1,Algorithms For Longest Chains In Pseudo- Transitive Graphs,"A directed acyclic graph G = (V E) is pseudo-transitive with respect to a
given subset of edges E1 if for any edge ab in E1 and any edge bc in E we
have ac in E. We give algorithms for computing longest chains and demonstrate
geometric applications that unify and improves some important past results.
(For specific applications see the introduction.)",Farhad Shahrokhi,cs.CG|math.CO
2017-02-28T17:21:30Z,2017-01-18T16:46:08Z,http://arxiv.org/abs/1701.05141v1,http://arxiv.org/pdf/1701.05141v1,"The Explicit Corridor Map: A Medial Axis-Based Navigation Mesh for
  Multi-Layered Environments","Path planning for walking characters in complicated virtual environments is a
fundamental task in simulations and games. In this paper we present an
improved definition of the Explicit Corridor Map (ECM) a navigation mesh that
allows efficient path planning and crowd simulation for disk-shaped characters
of any radius. The ECM is a medial axis (MA) annotated with nearest-obstacle
information. For a planar environment with $n$ obstacle vertices the ECM has
size $O(n)$ and can be computed in $O(n \log n)$ time.
  We also introduce multi-layered environments (MLEs) in which multiple planar
layers are connected by line segment connections. Typical real-world examples
are multi-storey buildings train stations and sports stadiums. We define the
MA and the ECM for multi-layered environments based on projected distances on
the ground plane. For an MLE with $n$ obstacle points and $k$ connections the
MA has size $O(n)$. We present an improved algorithm that constructs the MA and
ECM in $O(n \log n \log k)$ time.
  Our implementations show that the ECM can be computed efficiently for large
2D and multi-layered environments and that it can be used to compute paths
within milliseconds. This enables simulations of large virtual crowds of
heterogeneous characters in real-time.",Wouter van Toll|Atlas F. Cook IV|Marc J. van Kreveld|Roland Geraerts,cs.CG|cs.DS
2017-02-28T17:21:30Z,2017-01-13T15:08:46Z,http://arxiv.org/abs/1701.03693v1,http://arxiv.org/pdf/1701.03693v1,Multivariate Analysis for Computing Maxima in High Dimensions,"We study the problem of computing the \textsc{Maxima} of a set of $n$
$d$-dimensional points. For dimensions 2 and 3 there are algorithms to solve
the problem with order-oblivious instance-optimal running time. However in
higher dimensions there is still room for improvements. We present an algorithm
sensitive to the structural entropy of the input set which improves the
running time for large classes of instances on the best solution for
\textsc{Maxima} to date for $d \ge 4$.",Jérémy Barbay|Javiel Rojas,cs.CG|cs.DS|F.2.2
2017-02-28T17:21:30Z,2017-01-12T16:01:50Z,http://arxiv.org/abs/1701.03388v1,http://arxiv.org/pdf/1701.03388v1,"Dynamic and Kinetic Conflict-Free Coloring of Intervals with Respect to
  Points","We introduce the dynamic conflict-free coloring problem for a set $S$ of
intervals in $\mathbb{R}^1$ with respect to points where the goal is to
maintain a conflict-free coloring for $S$ under insertions and deletions. We
investigate trade-offs between the number of colors used and the number of
intervals that are recolored upon insertion or deletion of an interval. Our
results include:
  - a lower bound on the number of recolorings as a function of the number of
colors which implies that with $O(1)$ recolorings per update the worst-case
number of colors is $\Omega(\log n/\log\log n)$ and that any strategy using
$O(1/\varepsilon)$ colors needs $\Omega(\varepsilon n^{\varepsilon})$
recolorings;
  - a coloring strategy that uses $O(\log n)$ colors at the cost of $O(\log n)$
recolorings and another strategy that uses $O(1/\varepsilon)$ colors at the
cost of $O(n^{\varepsilon}/\varepsilon)$ recolorings;
  - stronger upper and lower bounds for special cases.
  We also consider the kinetic setting where the intervals move continuously
(but there are no insertions or deletions); here we show how to maintain a
coloring with only four colors at the cost of three recolorings per event and
show this is tight.",Mark de Berg|Tim Leijsen|André van Renssen|Marcel Roeloffzen|Aleksandar Markovic|Gerhard Woeginger,cs.CG
2017-02-28T17:21:31Z,2017-01-12T04:51:15Z,http://arxiv.org/abs/1701.03230v1,http://arxiv.org/pdf/1701.03230v1,Surface Reconstruction with Data-driven Exemplar Priors,"In this paper we propose a framework to reconstruct 3D models from raw
scanned points by learning the prior knowledge of a specific class of objects.
Unlike previous work that heuristically specifies particular regularities and
defines parametric models our shape priors are learned directly from existing
3D models under a framework based on affinity propagation. Given a database of
3D models within the same class of objects we build a comprehensive library of
3D local shape priors. We then formulate the problem to select
as-few-as-possible priors from the library referred to as exemplar priors.
These priors are sufficient to represent the 3D shapes of the whole class of
objects from where they are generated. By manipulating these priors we are
able to reconstruct geometrically faithful models with the same class of
objects from raw point clouds. Our framework can be easily generalized to
reconstruct various categories of 3D objects that have more geometrically or
topologically complex structures. Comprehensive experiments exhibit the power
of our exemplar priors for gracefully solving several problems in 3D shape
reconstruction such as preserving sharp features recovering fine details and
so on.",Oussama Remil|Qian Xie|Xingyu Xie|Kai Xu|Jun Wang,cs.CG|cs.GR
2017-02-28T17:21:34Z,2017-01-11T04:04:43Z,http://arxiv.org/abs/1701.02843v1,http://arxiv.org/pdf/1701.02843v1,"Solve Partial Differential Equations on Manifold From Incomplete
  Inter-Point Distance","Solutions of partial differential equations (PDEs) on manifolds have provided
important applications in different fields in science and engineering. Existing
methods are majorly based on discretization of manifolds as implicit functions
triangle meshes or point clouds where the manifold structure is approximated
by either zero level set of an implicit function or a set of points. In many
applications manifolds might be only provided as an inter-point distance
matrix with possible missing values. This paper discusses a framework to
discretize PDEs on manifolds represented as incomplete distance information.
Without conducting a time-consuming global coordinates reconstruction we
propose a more efficient strategy by discretizing differential operators only
based on point-wisely local reconstruction. Our local reconstruction model is
based on the recent advances of low-rank matrix completion theory where only a
very small random portion of distance information is required. This method
enables us to conduct analyses of incomplete distance data using solutions of
special designed PDEs such as the Laplace-Beltrami (LB) eigen-system. As an
application we demonstrate a new way of manifold reconstruction from an
incomplete distance by stitching patches using the spectrum of the LB operator.
Intensive numerical experiments demonstrate the effectiveness of the proposed
methods.",Rongjie Lai|Jia Li,"math.NA|cs.CG|65D18, 65D25, 65N25"
2017-02-28T17:21:34Z,2017-01-09T16:17:10Z,http://arxiv.org/abs/1701.02229v1,http://arxiv.org/pdf/1701.02229v1,Searching edges in the overlap of two plane graphs,"Consider a pair of plane straight-line graphs whose edges are colored red
and blue respectively and let n be the total complexity of both graphs. We
present a O(n log n)-time O(n)-space technique to preprocess such pair of
graphs that enables efficient searches among the red-blue intersections along
edges of one of the graphs. Our technique has a number of applications to
geometric problems. This includes: (1) a solution to the batched red-blue
search problem [Dehne et al. 2006] in O(n log n) queries to the oracle; (2) an
algorithm to compute the maximum vertical distance between a pair of 3D
polyhedral terrains one of which is convex in O(n log n) time where n is the
total complexity of both terrains; (3) an algorithm to construct the Hausdorff
Voronoi diagram of a family of point clusters in the plane in O((n+m) log^3 n)
time and O(n+m) space where n is the total number of points in all clusters
and m is the number of crossings between all clusters; (4) an algorithm to
construct the farthest-color Voronoi diagram of the corners of n axis-aligned
rectangles in O(n log^2 n) time; (5) an algorithm to solve the stabbing circle
problem for n parallel line segments in the plane in optimal O(n log n) time.
All these results are new or improve on the best known algorithms.",John Iacono|Elena Khramtcova|Stefan Langerman,cs.CG
2017-02-28T17:21:34Z,2017-01-09T15:18:28Z,http://arxiv.org/abs/1701.02208v1,http://arxiv.org/pdf/1701.02208v1,Barcodes of Towers and a Streaming Algorithm for Persistent Homology,"A tower is a sequence of simplicial complexes connected by simplicial maps.
We show how to compute a filtration a sequence of nested simplicial complexes
with the same persistent barcode as the tower. Our approach is based on the
coning strategy by Dey et al. (SoCG 2014). We show that a variant of this
approach yields a filtration that is asymptotically only marginally larger than
the tower and can be efficiently computed by a streaming algorithm both in
theory and in practice. Furthermore we show that our approach can be combined
with a streaming algorithm to compute the barcode of the tower via matrix
reduction. The space complexity of the algorithm does not depend on the length
of the tower but the maximal size of any subcomplex within the tower.",Michael Kerber|Hannah Schreiber,math.AT|cs.CG
2017-02-28T17:21:34Z,2017-02-22T20:40:08Z,http://arxiv.org/abs/1701.02200v2,http://arxiv.org/pdf/1701.02200v2,Bounding a global red-blue proportion using local conditions,"We study the following local-to-global phenomenon: Let $B$ and $R$ be two
finite sets of (blue and red) points in the Euclidean plane $\mathbb{R}^2$.
Suppose that in each ""neighborhood"" of a red point the number of blue points
is at least as large as the number of red points. We show that in this case the
total number of blue points is at least one fifth of the total number of red
points. We also show that this bound is optimal and we generalize the result to
arbitrary dimension and arbitrary norm using results from Minkowski
arrangements.",Márton Naszódi|Leonardo Martínez Sandoval|Shakhar Smorodinsky,cs.CG
2017-02-28T17:21:34Z,2017-01-05T12:00:37Z,http://arxiv.org/abs/1701.06430v1,http://arxiv.org/pdf/1701.06430v1,An Upper Bound of the Minimal Dispersion via Delta Covers,"For a point set of $n$ elements in the $d$-dimensional unit cube and a class
of test sets we are interested in the largest volume of a test set which does
not contain any point. For all natural numbers $n$ $d$ and under the
assumption of a $delta$-cover with cardinality $\vert \Gamma_\delta \vert$ we
prove that there is a point set such that the largest volume of such a test
set without any point is bounded by $\frac{\log \vert \Gamma_\delta \vert}{n} +
\delta$. For axis-parallel boxes on the unit cube this leads to a volume of at
most $\frac{4d}{n}\log(\frac{9n}{d})$ and on the torus to $\frac{4d}{n}\log
(2n)$.",Daniel Rudolf,"cs.CG|math.NA|52B55, 68Q25"
2017-02-28T17:21:34Z,2017-01-04T07:44:19Z,http://arxiv.org/abs/1701.00921v1,http://arxiv.org/pdf/1701.00921v1,Towards Faithful Graph Visualizations,"Readability criteria have been addressed as a measurement of the quality of
graph visualizations. In this paper we argue that readability criteria are
necessary but not sufficient. We propose a new kind of criteria namely
faithfulness to evaluate the quality of graph layouts. We introduce a general
model for quantify faithfulness and contrast it with the well established
readability criteria. We show examples of common visualization techniques such
as multidimensional scaling edge bundling and several other visualization
metaphors for the study of faithfulness.",Quan Hoang Nguyen|Peter Eades|Seok-Hee Hong,cs.CG
2017-02-28T17:21:34Z,2017-01-03T12:41:18Z,http://arxiv.org/abs/1701.00679v1,http://arxiv.org/pdf/1701.00679v1,"Removing Depth-Order Cycles Among Triangles: An Efficient Algorithm
  Generating Triangular Fragments","We present an algorithm that cuts any collection of n disjoint triangles in
R^3 into O(n^{7/4} polylog n) triangular fragments such that all cycles in the
depth-order relation are eliminated. The running time of our algorithm is
O(n^{3.69}). We also prove a refined bound which depends on the number K of
intersections between the projections of the triangle edges onto the xy-plane.
More precisely we show that O(n^{1+\eps} + n^{1/4} K^{3/4} polylog n)
fragments suffice to eliminate all cycles. This result extends to xy-monotone
surface patches bounded by a constant number of bounded-degree algebraic arcs
in general position.",Mark de Berg,cs.CG|F.2.2
2017-02-28T17:21:34Z,2017-01-02T16:45:18Z,http://arxiv.org/abs/1701.00441v1,http://arxiv.org/pdf/1701.00441v1,Collecting a Swarm in a Grid Environment Using Shared Global Inputs,"This paper investigates efficient techniques to collect and concentrate an
under-actuated particle swarm despite obstacles. Concentrating a swarm of
particles is of critical importance in health-care for targeted drug delivery
where micro-scale particles must be steered to a goal location. Individual
particles must be small in order to navigate through micro-vasculature but
decreasing size brings new challenges. Individual particles are too small to
contain on-board power or computation and are instead controlled by a global
input such as an applied fluidic flow or electric field.
  To make progress this paper considers a swarm of robots initialized in a
grid world in which each position is either free-space or obstacle. This paper
provides algorithms that collect all the robots to one position and compares
these algorithms on the basis of efficiency and implementation time.",Arun V. Mahadev|Dominik Krupke|Jan-Marc Reinhardt|Sándor P. Fekete|Aaron T. Becker,cs.RO|cs.CG|I.2.11; F.2.2
2017-02-28T17:21:34Z,2017-01-01T04:49:47Z,http://arxiv.org/abs/1701.00198v1,http://arxiv.org/abs/1701.00198v1,"A robust approach for tree segmentation in deciduous forests using
  small-footprint airborne LiDAR data","This paper presents a non-parametric approach for segmenting trees from
airborne LiDAR data in deciduous forests. Based on the LiDAR point cloud the
approach collects crown information such as steepness and height on-the-fly to
delineate crown boundaries and most importantly does not require a priori
assumptions of crown shape and size. The approach segments trees iteratively
starting from the tallest within a given area to the smallest until all trees
have been segmented. To evaluate its performance the approach was applied to
the University of Kentucky Robinson Forest a deciduous closed-canopy forest
with complex terrain and vegetation conditions. The approach identified 94% of
dominant and co-dominant trees with a false detection rate of 13%. About 62% of
intermediate overtopped and dead trees were also detected with a false
detection rate of 15%. The overall segmentation accuracy was 77%. Correlations
of the segmentation scores of the proposed approach with local terrain and
stand metrics was not significant which is likely an indication of the
robustness of the approach as results are not sensitive to the differences in
terrain and stand structures.",Hamid Hamraz|Marco A. Contreras|Jun Zhang,cs.CV|cs.CE|cs.CG
2017-02-28T17:21:34Z,2016-12-31T21:53:09Z,http://arxiv.org/abs/1701.00169v1,http://arxiv.org/pdf/1701.00169v1,"Tree segmentation in multi-story stands within small-footprint airborne
  LiDAR data","Airborne LiDAR point cloud of a forest contains three dimensional data from
which vertical stand structure (including information about under-story trees)
can be derived. This paper presents a segmentation approach for multi-story
stands that strips the point cloud to its canopy layers identifies individual
tree segments within each layer using a DSM-based tree identification method as
a building block and combines the segments of immediate layers in order to fix
potential over-segmentation of tree crowns across the layers. We introduce
local layering that analyzes the vertical distributions of LiDAR points within
their local neighborhoods in order to locally determine the height thresholds
for layering the canopy. Unlike the previous work that stripped stiff layers
within constrained areas the local layering method strips flexible (in
thickness and elevation) and narrower canopy layers within unconstrained areas.
Statistical analyses showed that layering in general strongly improves
identifying (specifically under-story) trees for the cost of moderately
increasing over-segmentation rate of the identified trees. Combining tree
segments across the immediate layers did not seem to improve tree
identification accuracy remarkably suggesting that layers separated canopy
layers rather precisely.",Hamid Hamraz|Marco A. Contreras|Jun Zhang,cs.CV|cs.CE|cs.CG
2017-02-28T17:21:38Z,2016-12-31T17:05:53Z,http://arxiv.org/abs/1701.00146v1,http://arxiv.org/pdf/1701.00146v1,Even $1 \times n$ Edge-Matching and Jigsaw Puzzles are Really Hard,"We prove the computational intractability of rotating and placing $n$ square
tiles into a $1 \times n$ array such that adjacent tiles are compatible--either
equal edge colors as in edge-matching puzzles or matching tab/pocket shapes
as in jigsaw puzzles. Beyond basic NP-hardness we prove that it is NP-hard
even to approximately maximize the number of placed tiles (allowing blanks)
while satisfying the compatibility constraint between nonblank tiles within a
factor of 0.9999999851. (On the other hand there is an easy $1 \over
2$-approximation.) This is the first (correct) proof of inapproximability for
edge-matching and jigsaw puzzles. Along the way we prove NP-hardness of
distinguishing for a directed graph on $n$ nodes between having a Hamiltonian
path (length $n-1$) and having at most $0.999999284 (n-1)$ edges that form a
vertex-disjoint union of paths. We use this gap hardness and gap-preserving
reductions to establish similar gap hardness for $1 \times n$ jigsaw and
edge-matching puzzles.",Jeffrey Bosboom|Erik D. Demaine|Martin L. Demaine|Adam Hesterberg|Pasin Manurangsi|Anak Yodpinyanee,cs.CC|cs.CG
2017-02-28T17:21:38Z,2016-12-30T09:33:07Z,http://arxiv.org/abs/1612.09434v1,http://arxiv.org/pdf/1612.09434v1,Data driven estimation of Laplace-Beltrami operator,"Approximations of Laplace-Beltrami operators on manifolds through graph
Lapla-cians have become popular tools in data analysis and machine learning.
These discretized operators usually depend on bandwidth parameters whose tuning
remains a theoretical and practical problem. In this paper we address this
problem for the unnormalized graph Laplacian by establishing an oracle
inequality that opens the door to a well-founded data-driven procedure for the
bandwidth selection. Our approach relies on recent results by Lacour and
Massart [LM15] on the so-called Lepski's method.",Frédéric Chazal|Ilaria Giulini|Bertrand Michel,cs.CG|cs.LG|math.ST|stat.TH
2017-02-28T17:21:38Z,2017-01-02T14:15:21Z,http://arxiv.org/abs/1612.09277v2,http://arxiv.org/pdf/1612.09277v2,On Planar Greedy Drawings of 3-Connected Planar Graphs,"A graph drawing is $\textit{greedy}$ if for every ordered pair of vertices
$(xy)$ there is a path from $x$ to $y$ such that the Euclidean distance to
$y$ decreases monotonically at every vertex of the path. Greedy drawings
support a simple geometric routing scheme in which any node that has to send a
packet to a destination ""greedily"" forwards the packet to any neighbor that is
closer to the destination than itself according to the Euclidean distance in
the drawing. In a greedy drawing such a neighbor always exists and hence this
routing scheme is guaranteed to succeed.
  In 2004 Papadimitriou and Ratajczak stated two conjectures related to greedy
drawings. The $\textit{greedy embedding conjecture}$ states that every
$3$-connected planar graph admits a greedy drawing. The $\textit{convex greedy
embedding conjecture}$ asserts that every $3$-connected planar graph admits a
planar greedy drawing in which the faces are delimited by convex polygons. In
2008 the greedy embedding conjecture was settled in the positive by Leighton
and Moitra.
  In this paper we prove that every $3$-connected planar graph admits a
$\textit{planar}$ greedy drawing. Apart from being a strengthening of Leighton
and Moitra's result this theorem constitutes a natural intermediate step
towards a proof of the convex greedy embedding conjecture.",Giordano Da Lozzo|Anthony D'Angelo|Fabrizio Frati,cs.CG
2017-02-28T17:21:38Z,2016-12-24T09:00:37Z,http://arxiv.org/abs/1612.08153v1,http://arxiv.org/pdf/1612.08153v1,"EgoReID: Cross-view Self-Identification and Human Re-identification in
  Egocentric and Surveillance Videos","Human identification remains to be one of the challenging tasks in computer
vision community due to drastic changes in visual features across different
viewpoints lighting conditions occlusion etc. Most of the literature has
been focused on exploring human re-identification across viewpoints that are
not too drastically different in nature. Cameras usually capture oblique or
side views of humans leaving room for a lot of geometric and visual reasoning.
Given the recent popularity of egocentric and top-view vision
re-identification across these two drastically different views can now be
explored. Having an egocentric and a top view video our goal is to identify
the cameraman in the content of the top-view video and also re-identify the
people visible in the egocentric video by matching them to the identities
present in the top-view video. We propose a CRF-based method to address the two
problems. Our experimental results demonstrates the efficiency of the proposed
approach over a variety of video recorded from two views.",Shervin Ardeshir|Sandesh Sharma|Ali Broji,cs.CV|cs.CG
2017-02-28T17:21:38Z,2016-12-22T00:55:29Z,http://arxiv.org/abs/1612.07405v1,http://arxiv.org/pdf/1612.07405v1,Practical linear-space Approximate Near Neighbors in high dimension,"The $c$-approximate Near Neighbor problem in high dimensional spaces has been
mainly addressed by Locality Sensitive Hashing (LSH) which offers polynomial
dependence on the dimension query time sublinear in the size of the dataset
and subquadratic space requirement. For practical applications linear space is
typically imperative. Most previous work in the linear space regime focuses on
the case that $c$ exceeds $1$ by a constant term. In a recently accepted paper
optimal bounds have been achieved for any $c>1$ \cite{ALRW17}.
  Towards practicality we present a new and simple data structure using linear
space and sublinear query time for any $c>1$ including $c\to 1^+$. Given an LSH
family of functions for some metric space we randomly project points to the
Hamming cube of dimension $\log n$ where $n$ is the number of input points.
The projected space contains strings which serve as keys for buckets containing
the input points. The query algorithm simply projects the query point then
examines points which are assigned to the same or nearby vertices on the
Hamming cube. We analyze in detail the query time for some standard LSH
families.
  To illustrate our claim of practicality we offer an open-source
implementation in {\tt C++} and report on several experiments in dimension up
to 1000 and $n$ up to $10^6$. Our algorithm is one to two orders of magnitude
faster than brute force search. Experiments confirm the sublinear dependence on
$n$ and the linear dependence on the dimension. We have compared against
state-of-the-art LSH-based library {\tt FALCONN}: our search is somewhat
slower but memory usage and preprocessing time are significantly smaller.",Georgia Avarikioti|Ioannis Z. Emiris|Ioannis Psarros|Georgios Samaras,cs.CG
2017-02-28T17:21:38Z,2016-12-21T18:58:17Z,http://arxiv.org/abs/1612.07276v1,http://arxiv.org/pdf/1612.07276v1,Splitting $B_2$-VPG graphs into outer-string and co-comparability graphs,"In this paper we show that any $B_2$-VPG graph (i.e. an intersection graph
of orthogonal curves with at most 2 bends) can be decomposed into $O(\log n)$
outerstring graphs or $O(\log^3 n)$ permutation graphs. This leads to better
approximation algorithms for hereditary graph problems such as independent
set clique and clique cover on $B_2$-VPG graphs.",Martin Derka|Therese Biedl,cs.CG|cs.DS
2017-02-28T17:21:38Z,2017-02-27T10:00:30Z,http://arxiv.org/abs/1701.00541v2,http://arxiv.org/pdf/1701.00541v2,"Packing Unequal Circles into a Square Container by Partitioning Narrow
  Action Spaces and Circle Items","We address the NP-hard problem of finding a non-overlapping dense packing
pattern for n Unequal Circle items in a two-dimensional Square Container
(PUC-SC) such that the size of the container is minimized. Based on our
previous work on an Action Space based Global Optimization (ASGO) that
approximates each circle item as a square item to efficiently find the large
unoccupied spaces we propose an optimization algorithm based on the
Partitioned Action Space and Partitioned Circle Items (PAS-PCI). The PAS is to
partition the narrow action space on the long side to find two equal action
spaces to fully utilize the unoccupied spaces. The PCI is to partition the
circle items into four groups based on size for the basin hopping strategy.
Experiments on two sets of benchmark instances show the effectiveness of the
proposed method. In comparison with our previous algorithm ASGO on the 68
tested instances that ASGO published PAS-PCI not only gains smaller containers
in 64 instances and matches the other 4 but also runs faster in most instances.
In comparison with the best record of the Packomania website on a total of 98
instances PAS-PCI finds smaller containers on 82 and matches the other 16.
Note that we updated 19 records for (47-48 51-54 57 61-72) that had been
kept unchanged since 2013.",Kun He|Mohammed Dosh|Shenghao Zou,cs.CG|cs.DS|52C26
2017-02-28T17:21:38Z,2017-01-03T14:51:30Z,http://arxiv.org/abs/1612.06954v2,http://arxiv.org/pdf/1612.06954v2,Colored stochastic dominance problems,"In this paper we study the dominance relation under a stochastic setting.
Let $\mathcal{S}$ be a set of $n$ colored stochastic points in $\mathbb{R}^d$
each of which is associated with an existence probability. We investigate the
problem of computing the probability that a realization of $\mathcal{S}$
contains inter-color dominances which we call the \textit{colored stochastic
dominance} (CSD) problem. We propose the first algorithm to solve the CSD
problem for $d=2$ in $O(n^2 \log^2 n)$ time. On the other hand we prove that
for $d \geq 3$ even the CSD problem with a restricted color pattern is
#P-hard. In addition even if the existence probabilities are restricted to be
$\frac{1}{2}$ the problem remains #P-hard for $d \geq 7$. A simple FPRAS is
then provided to approximate the desired probability in any dimension. We also
study a variant of the CSD problem in which the dominance relation is
considered with respect to not only the standard basis but any orthogonal basis
of $\mathbb{R}^d$. Specifically this variant which we call the {\em
free-basis colored stochastic dominance} (FBCSD) problem considers the
probability that a realization of $\mathcal{S}$ contains inter-color dominances
with respect to any orthogonal basis of $\mathbb{R}^d$. We show that the CSD
problem is polynomial-time reducible to the FBCSD problem in the same
dimension which proves the #P-hardness of the latter for $d \geq 3$.
Conversely we reduce the FBCSD problem in $\mathbb{R}^2$ to the CSD problem in
$\mathbb{R}^2$ by which an $O(n^4 \log^2 n)$ time algorithm for the former is
obtained.",Jie Xue|Yuan Li,cs.CG|F.2.2
2017-02-28T17:21:38Z,2016-12-15T15:03:57Z,http://arxiv.org/abs/1612.05101v1,http://arxiv.org/pdf/1612.05101v1,Open problem on risk-aware planning in the plane,"We consider the problem of planning a collision-free path of a robot in the
presence of risk zones. The robot is allowed to travel in these zones but is
penalized in a super-linear fashion for consecutive accumulative time spent
there. We recently suggested a natural cost function that balances path length
and risk-exposure time. When no risk zones exists our problem resorts to
computing minimal-length paths which is known to be computationally hard in the
number of dimensions. It is well known that in two-dimensions computing
minimal-length paths can be done efficiently. Thus a natural question we pose
is ""Is our problem computationally hard or not?"" If the problem is hard we
wish to find an approximation algorithm to compute a near-optimal path. If not
then a polynomial-time algorithm should be found.",Oren Salzman|Siddhartha Srinivasa,cs.CG
2017-02-28T17:21:38Z,2016-12-15T00:17:43Z,http://arxiv.org/abs/1612.04890v1,http://arxiv.org/pdf/1612.04890v1,"Stochastic closest-pair problem and most-likely nearest-neighbor search
  in tree spaces","Let $T$ be a tree space (or tree network) represented by a weighted tree with
$t$ vertices and $S$ be a set of $n$ stochastic points in $T$ each of which
has a fixed location with an independent existence probability. We investigate
two fundamental problems under such a stochastic setting the closest-pair
problem and the nearest-neighbor search. For the former we study the
computation of the $\ell$-threshold probability and the expectation of the
closest-pair distance of a realization of $S$. We propose the first algorithm
to compute the $\ell$-threshold probability in $O(t+n\log n+ \min\{tnn^2\})$
time for any given threshold $\ell$ which immediately results in an
$O(t+\min\{tn^3n^4\})$-time algorithm for computing the expected closest-pair
distance. Based on this we further show that one can compute a
$(1+\varepsilon)$-approximation for the expected closest-pair distance in
$O(t+\varepsilon^{-1}\min\{tn^2n^3\})$ time by arguing that the expected
closest-pair distance can be approximated via $O(\varepsilon^{-1}n)$ threshold
probability queries. For the latter we study the $k$ most-likely
nearest-neighbor search ($k$-LNN) via a notion called $k$ most-likely Voronoi
Diagram ($k$-LVD). We show that the size of the $k$-LVD $\varPsi_T^S$ of $S$ on
$T$ is bounded by $O(kn)$ if the existence probabilities of the points in $S$
are constant-far from 0. Furthermore we establish an $O(kn)$ average-case
upper bound for the size of $\varPsi_T^S$ by regarding the existence
probabilities as i.i.d. random variables drawn from some fixed distribution.
Our results imply the existence of an LVD data structure which answers $k$-LNN
queries in $O(\log n+k)$ time using average-case $O(t+k^2n)$ space and
worst-case $O(t+kn^2)$ space if the existence probabilities are constant-far
from 0. Finally we also give an $O(t+ n^2\log n+n^2k)$-time algorithm to
construct the LVD data structure.",Jie Xue|Yuan Li,cs.CG|F.2.2
2017-02-28T17:21:42Z,2016-12-14T19:33:00Z,http://arxiv.org/abs/1612.04780v1,http://arxiv.org/pdf/1612.04780v1,Minimum Weight Connectivity Augmentation for Planar Straight-Line Graphs,"We consider edge insertion and deletion operations that increase the
connectivity of a given planar straight-line graph (PSLG) while minimizing the
total edge length of the output. We show that every connected PSLG $G=(VE)$ in
general position can be augmented to a 2-connected PSLG $(VE\cup E^+)$ by
adding new edges of total Euclidean length $\|E^+\|\leq 2\|E\|$ and this bound
is the best possible. An optimal edge set $E^+$ can be computed in $O(|V|^4)$
time; however the problem becomes NP-hard when $G$ is disconnected. Further
there is a sequence of edge insertions and deletions that transforms a
connected PSLG $G=(VE)$ into a planar straight-line cycle $G'=(VE')$ such
that $\|E'\|\leq 2\|{\rm MST}(V)\|$ and the graph remains connected with edge
length below $\|E\|+\|{\rm MST}(V)\|$ at all stages. These bounds are the best
possible.",Hugo A. Akitaya|Rajasekhar Inkulu|Torrie L. Nichols|Diane L. Souvaine|Csaba D. Tóth|Charles R. Winston,"cs.CG|05C40, 05C85, 68R10|I.3.5"
2017-02-28T17:21:42Z,2016-12-14T19:06:05Z,http://arxiv.org/abs/1612.04774v1,http://arxiv.org/pdf/1612.04774v1,"Beam Search for Learning a Deep Convolutional Neural Network of 3D
  Shapes","This paper addresses 3D shape recognition. Recent work typically represents a
3D shape as a set of binary variables corresponding to 3D voxels of a uniform
3D grid centered on the shape and resorts to deep convolutional neural
networks(CNNs) for modeling these binary variables. Robust learning of such
CNNs is currently limited by the small datasets of 3D shapes available an
order of magnitude smaller than other common datasets in computer vision.
Related work typically deals with the small training datasets using a number of
ad hoc hand-tuning strategies. To address this issue we formulate CNN
learning as a beam search aimed at identifying an optimal CNN architecture
namely the number of layers nodes and their connectivity in the network as
well as estimating parameters of such an optimal CNN. Each state of the beam
search corresponds to a candidate CNN. Two types of actions are defined to add
new convolutional filters or new convolutional layers to a parent CNN and thus
transition to children states. The utility function of each action is
efficiently computed by transferring parameter values of the parent CNN to its
children thereby enabling an efficient beam search. Our experimental
evaluation on the 3D ModelNet dataset demonstrates that our model pursuit using
the beam search yields a CNN with superior performance on 3D shape
classification than the state of the art.",Xu Xu|Sinisa Todorovic,cs.CV|cs.CG
2017-02-28T17:21:42Z,2016-12-14T10:52:01Z,http://arxiv.org/abs/1612.04571v1,http://arxiv.org/pdf/1612.04571v1,A Refined Analysis of LSH for Well-dispersed Data Points,"Near neighbor problems are fundamental in algorithms for high-dimensional
Euclidean spaces. While classical approaches suffer from the curse of
dimensionality locality sensitive hashing (LSH) can effectively solve
a-approximate r-near neighbor problem and has been proven to be optimal in the
worst case. However for real-world data sets LSH can naturally benefit from
well-dispersed data and low doubling dimension leading to significantly
improved performance. In this paper we address this issue and propose a
refined analyses for running time of approximating near neighbors queries via
LSH. We characterize dispersion of data using N_b the number of b*r-near pairs
among the data points. Combined with optimal data-oblivious LSH scheme we get
a new query time bound depending on N_b and doubling dimension. For many
natural scenarios where points are well-dispersed or lying in a
low-doubling-dimension space our result leads to sharper performance than
existing worst-case analysis. This paper not only present first rigorous proof
on how LSHs make use of the structure of data points but also provide
important insights into parameter setting in the practice of LSH beyond worst
case. Besides the techniques in our analysis involve a generalized version of
sphere packing problem which might be of some independent interest.",Wenlong Mou|Liwei Wang,cs.DS|cs.CG
2017-02-28T17:21:42Z,2016-12-13T18:12:44Z,http://arxiv.org/abs/1612.04304v1,http://arxiv.org/pdf/1612.04304v1,Towards a Constructive Version of Banaszczyk's Vector Balancing Theorem,"An important theorem of Banaszczyk (Random Structures & Algorithms `98)
states that for any sequence of vectors of $\ell_2$ norm at most $1/5$ and any
convex body $K$ of Gaussian measure $1/2$ in $\mathbb{R}^n$ there exists a
signed combination of these vectors which lands inside $K$. A major open
problem is to devise a constructive version of Banaszczyk's vector balancing
theorem i.e. to find an efficient algorithm which constructs the signed
combination.
  We make progress towards this goal along several fronts. As our first
contribution we show an equivalence between Banaszczyk's theorem and the
existence of $O(1)$-subgaussian distributions over signed combinations. For the
case of symmetric convex bodies our equivalence implies the existence of a
universal signing algorithm (i.e. independent of the body) which simply
samples from the subgaussian sign distribution and checks to see if the
associated combination lands inside the body. For asymmetric convex bodies we
provide a novel recentering procedure which allows us to reduce to the case
where the body is symmetric.
  As our second main contribution we show that the above framework can be
efficiently implemented when the vectors have length $O(1/\sqrt{\log n})$
recovering Banaszczyk's results under this stronger assumption. More precisely
we use random walk techniques to produce the required $O(1)$-subgaussian
signing distributions when the vectors have length $O(1/\sqrt{\log n})$ and
use a stochastic gradient ascent method to implement the recentering procedure
for asymmetric bodies.",Daniel Dadush|Shashwat Garg|Shachar Lovett|Aleksandar Nikolov,cs.DS|cs.CG|math.FA|math.PR
2017-02-28T17:21:42Z,2016-12-12T19:12:47Z,http://arxiv.org/abs/1612.03854v1,http://arxiv.org/pdf/1612.03854v1,Crossing Number for Graphs With Bounded Pathwidth,"The crossing number is the smallest number of pairwise edge-crossings when
drawing a graph into the plane. There are only very few graph classes for which
the exact crossing number is known or for which there at least exist constant
approximation ratios. Furthermore up to now general crossing number
computations have never been successfully tackled using bounded width of graph
decompositions like treewidth or pathwidth.
  In this paper we for the first time show that crossing number is tractable
(even in linear time) for maximal graphs of bounded pathwidth~3. The technique
also shows that the crossing number and the rectilinear (a.k.a. straight-line)
crossing number are identical for this graph class and that we require only an
$O(n)\times O(n)$-grid to achieve such a drawing.
  Our techniques can further be extended to devise a 2-approximation for
general graphs with pathwidth 3 and a $4w^3$-approximation for maximal graphs
of pathwidth $w$. This is a constant approximation for bounded pathwidth
graphs.",Therese Biedl|Markus Chimani|Martin Derka|Petra Mutzel,cs.CG
2017-02-28T17:21:42Z,2016-12-12T12:10:02Z,http://arxiv.org/abs/1612.03638v1,http://arxiv.org/pdf/1612.03638v1,Intersection Graphs of Rays and Grounded Segments,"We consider several classes of intersection graphs of line segments in the
plane and prove new equality and separation results between those classes. In
particular we show that: (1) intersection graphs of grounded segments and
intersection graphs of downward rays form the same graph class (2) not every
intersection graph of rays is an intersection graph of downward rays and (3)
not every intersection graph of rays is an outer segment graph. The first
result answers an open problem posed by Cabello and Jej\v{c}i\v{c}. The third
result confirms a conjecture by Cabello. We thereby completely elucidate the
remaining open questions on the containment relations between these classes of
segment graphs. We further characterize the complexity of the recognition
problems for the classes of outer segment grounded segment and ray
intersection graphs. We prove that these recognition problems are complete for
the existential theory of the reals. This holds even if a 1-string realization
is given as additional input.",Jean Cardinal|Stefan Felsner|Tillmann Miltzow|Casey Tompkins|Birgit Vogtenhuber,cs.DM|cs.CC|cs.CG|math.CO
2017-02-28T17:21:42Z,2016-12-09T13:00:53Z,http://arxiv.org/abs/1612.04861v1,http://arxiv.org/pdf/1612.04861v1,Some Counterexamples for Compatible Triangulations,"We consider the conjecture by Aichholzer Aurenhammer Hurtado and Krasser
that any two points sets with the same cardinality and the same size convex
hull can be triangulated in the ""same"" way more precisely via \emph{compatible
triangulations}. We show counterexamples to various strengthened versions of
this conjecture.",Cody Barnson|Dawn Chandler|Qiao Chen|Christina Chung|Andrew Coccimiglio|Sean La|Lily Li|Aïna Linn|Anna Lubiw|Clare Lyle|Shikha Mahajan|Gregory Mierzwinski|Simon Pratt|Yoon Su|Yoo|Hongbo Zhang|Kevin Zhang,cs.CG|cs.DM|math.CO|68R10|F.2.2
2017-02-28T17:21:42Z,2016-12-09T10:31:04Z,http://arxiv.org/abs/1612.02967v1,http://arxiv.org/pdf/1612.02967v1,"Dune-CurvilinearGrid: Parallel Dune Grid Manager for Unstructured
  Tetrahedral Curvilinear Meshes","We introduce the dune-curvilineargrid module. The module provides the
self-contained parallel grid manager as well as the underlying elementary
curvilinear geometry module dune-curvilineargeometry. This work is motivated by
the need for reliable and scalable electromagnetic design of nanooptical
devices. Curvilinear geometries improve both the accuracy of modeling smooth
material boundaries and the h/p-convergence rate of PDE solutions reducing
the necessary computational effort. dune-curvilineargrid provides a large
spectrum of features for scalable parallel implementations of Finite Element
and Boundary Integral methods over curvilinear tetrahedral geometries
including symbolic polynomial mappings and operations recursive integration
sparse and dense grid communication parallel timing and memory footprint
diagnostics utilities. It is written in templated C++ using MPI for
parallelization and ParMETIS for grid partitioning and is provided as a module
for the DUNE interface. The dune-curvilineargrid grid manager is continuously
developed and improved and so is this documentation. For the most recent
version of the documentation as well as the source code please refer to the
provided repositories and our website.",Aleksejs Fomins|Benedikt Oswald,"cs.CG|51P05, 78-04|F.2.2; G.1.8; I.3.5"
2017-02-28T17:21:42Z,2016-12-09T04:10:03Z,http://arxiv.org/abs/1612.02905v1,http://arxiv.org/pdf/1612.02905v1,An obstruction to Delaunay triangulations in Riemannian manifolds,"Delaunay has shown that the Delaunay complex of a finite set of points $P$ of
Euclidean space $\mathbb{R}^m$ triangulates the convex hull of $P$ provided
that $P$ satisfies a mild genericity property. Voronoi diagrams and Delaunay
complexes can be defined for arbitrary Riemannian manifolds. However
Delaunay's genericity assumption no longer guarantees that the Delaunay complex
will yield a triangulation; stronger assumptions on $P$ are required. A natural
one is to assume that $P$ is sufficiently dense. Although results in this
direction have been claimed we show that sample density alone is insufficient
to ensure that the Delaunay complex triangulates a manifold of dimension
greater than 2.",Jean-Daniel Boissonnat|Ramsay Dyer|Arijit Ghosh|Nikolay Martynchuk,"cs.CG|math.DG|math.GT|57R05 (Primary), 54B15, 53B20 (Secondary)"
2017-02-28T17:21:42Z,2016-12-12T17:12:07Z,http://arxiv.org/abs/1612.02861v2,http://arxiv.org/pdf/1612.02861v2,"Multi-Scale Projective Coordinates via Persistent Cohomology of Sparse
  Filtrations","We present in this paper a framework for mapping data onto real and complex
projective spaces. The resulting projective coordinates provide a multi-scale
representation of the data and capture low dimensional underlying topological
features. An initial map is obtained in two steps: First the persistent
cohomology of a sparse filtration is used to compute systems of transition
functions for (real and complex) line bundles over neighborhoods of the data.
Next the transition functions are used to produce explicit classifying maps
for the induced bundles. A framework for dimensionality reduction in projective
space (Principal Projective Components) is also developed aimed at decreasing
the target dimension of the original map. Several examples are provided as well
as theorems addressing choices in the construction.",Jose A. Perea,math.AT|cs.CG
2017-02-28T17:21:46Z,2016-12-08T01:57:47Z,http://arxiv.org/abs/1612.02509v1,http://arxiv.org/pdf/1612.02509v1,Geodesics using Waves: Computing Distances using Wave Propagation,"In this paper we present a new method for computing approximate geodesic
distances. We introduce the wave method for approximating geodesic distances
from a point on a manifold mesh. Our method involves the solution of two linear
systems of equations. One system of equations is solved repeatedly to propagate
the wave on the entire mesh and one system is solved once after wave
propagation is complete in order to compute the approximate geodesic distances
up to an additive constant. However these systems need to be pre-factored only
once and can be solved efficiently at each iteration. All of our tests
required approximately between 300 and 400 iterations which were completed in
a few seconds. Therefore this method can approximate geodesic distances
quickly and the approximation is highly accurate.",Ayushi Sinha|Michael Kazhdan,cs.CG|cs.GR|I.3.5
2017-02-28T17:21:46Z,2016-12-07T23:24:33Z,http://arxiv.org/abs/1612.02483v1,http://arxiv.org/pdf/1612.02483v1,High Dimensional Consistent Digital Segments,"We consider the problem of digitalizing Euclidean line segments from
$\mathbb{R}^d$ to $\mathbb{Z}^d$. Christ {\em et al.} (DCG 2012) showed how to
construct a set of {\em consistent digital segment} (CDS) for $d=2$: a
collection of segments connecting any two points in $\mathbb{Z}^2$ that
satisfies the natural extension of the Euclidean axioms to $\mathbb{Z}^d$. In
this paper we study the construction of CDSs in higher dimensions.
  We show that any total order can be used to create a set of {\em consistent
digital rays} CDR in $\mathbb{Z}^d$ (a set of rays emanating from a fixed point
$p$ that satisfies the extension of the Euclidean axioms). We fully
characterize for which total orders the construction holds and study their
Hausdorff distance which in particular positively answers the question posed
by Christ {\em et al.}.",Man-Kwun Chiu|Matias Korman,cs.CG|I.3.5; I.4.1
2017-02-28T17:21:46Z,2016-12-07T20:44:21Z,http://arxiv.org/abs/1612.02412v1,http://arxiv.org/pdf/1612.02412v1,Shortcuts for the Circle,"Let $C$ be the unit circle in $\mathbb{R}^2$. We can view $C$ as a plane
graph whose vertices are all the points on $C$ and the distance between any
two points on $C$ is the length of the smaller arc between them. We consider a
graph augmentation problem on $C$ where we want to place $k\geq 1$
\emph{shortcuts} on $C$ such that the diameter of the resulting graph is
minimized.
  We analyze for each $k$ with $1\leq k\leq 7$ what the optimal set of
shortcuts is. Interestingly the minimum diameter one can obtain is not a
strictly decreasing function of~$k$. For example with seven shortcuts one
cannot obtain a smaller diameter than with six shortcuts. Finally we prove
that the optimal diameter is $2 + \Theta(1/k^{\frac{2}{3}})$ for any~$k$.",Sang Won Bae|Mark de Berg|Otfried Cheong|Joachim Gudmundsson|Christos Levcopoulos,math.MG|cs.CG
2017-02-28T17:21:46Z,2016-12-07T14:26:36Z,http://arxiv.org/abs/1612.02261v1,http://arxiv.org/pdf/1612.02261v1,Sparse Geometric Representation Through Local Shape Probing,"Shape analysis is very often performed by segmenting the shape into smooth
surface parts that can be further classified using a set of predefined
primitives such as planes cylinders or spheres. Hence the shape is generally
assumed to be manifold and smooth or to be an assembly of primitive parts. In
this paper we propose an approach which does not make any assumption on the
shape properties but rather learns its characteristics through a statistical
analysis of local shape variations. Armed solely with a local probing operator
we are able to perform a non local analysis of the shape yielding a shape
dictionary which encodes its structures. Our method relies on a novel
description of shape variations called Local Probing Field (LPF) which
describes how a generic pattern is transformed onto the shape. By carefully
optimizing the position and orientation of these descriptors we are able to
capture shape similarities and gather them into a geometrically relevant
dictionary over which the shape decomposes sparsely. Furthermore this analysis
also reveals the local dimensions of the shape. Our shape representation has
several potential applications; here we demonstrate its efficiency for shape
resampling and point set denoising.",Julie Digne|Sébastien Valette|Raphaëlle Chaine,cs.CG|cs.GR
2017-02-28T17:21:46Z,2016-12-07T09:12:00Z,http://arxiv.org/abs/1612.02158v1,http://arxiv.org/pdf/1612.02158v1,Proper Coloring of Geometric Hypergraphs,"We study whether for a given planar family F there is an m such that any
finite set of points can be 3-colored such that any member of F that contains
at least m points contains two points with different colors. We conjecture that
if F is a family of pseudo-disks then m=3 is sufficient. We prove that when F
is the family of all homothetic copies of a given convex polygon then such an
m exists. We also study the problem in higher dimensions.",Balázs Keszegh|Dömötör Pálvölgyi,math.CO|cs.CG
2017-02-28T17:21:46Z,2016-12-07T08:41:46Z,http://arxiv.org/abs/1612.02149v1,http://arxiv.org/pdf/1612.02149v1,Covering many points with a small-area box,"Let $P$ be a set of $n$ points in the plane. We show how to find for a given
integer $k>0$ the smallest-area axis-parallel rectangle that covers $k$ points
of $P$ in $O(nk^2 \log n+ n\log^2 n)$ time. We also consider the problem of
given a value $\alpha>0$ covering as many points of $P$ as possible with an
axis-parallel rectangle of area at most $\alpha$. For this problem we give a
randomized $(1-\varepsilon)$-approximation that works in near-linear time: in
$O((n/\varepsilon^4)\log^3 n \log (1/\varepsilon))$ time we find an
axis-parallel rectangle of area at most $\alpha$ that covers at least
$(1-\varepsilon)\kappa^*$ points where $\kappa^*$ is the maximum possible
number of points that could be covered.",Mark de Berg|Sergio Cabello|Otfried Cheong|David Eppstein|Christian Knauer,cs.CG
2017-02-28T17:21:46Z,2017-01-23T22:00:34Z,http://arxiv.org/abs/1612.01944v2,http://arxiv.org/pdf/1612.01944v2,"Porous Structure Design in Tissue Engineering Using Anisotropic Radial
  Basis Function","Development of additive manufacturing in last decade greatly improves tissue
engineering. During the manufacturing of porous scaffold simplified but
functionally equivalent models are getting focused for practically reasons.
Scaffolds can be classified into regular porous scaffolds and irregular porous
scaffolds. Several methodologies are developed to design these scaffolds. A
novel method is proposed in this paper using anisotropic radial basis function
(ARBF) interpolation. This is method uses geometric models such as volumetric
meshes as input and proves to be flexible because geometric models are able to
capture the characteristics of complex tissues easily. Moreover this method is
straightforward and easy to implement.",Ke Liu|Ye Guo|Zeyun Yu,cs.GR|cs.CG
2017-02-28T17:21:46Z,2016-12-06T08:07:36Z,http://arxiv.org/abs/1612.01696v1,http://arxiv.org/pdf/1612.01696v1,Optimal Approximate Polytope Membership,"In the polytope membership problem a convex polytope $K$ in $R^d$ is given
and the objective is to preprocess $K$ into a data structure so that given a
query point $q \in R^d$ it is possible to determine efficiently whether $q \in
K$. We consider this problem in an approximate setting and assume that $d$ is a
constant. Given an approximation parameter $\varepsilon > 0$ the query can be
answered either way if the distance from $q$ to $K$'s boundary is at most
$\varepsilon$ times $K$'s diameter. Previous solutions to the problem were on
the form of a space-time trade-off where logarithmic query time demands
$O(1/\varepsilon^{d-1})$ storage whereas storage $O(1/\varepsilon^{(d-1)/2})$
admits roughly $O(1/\varepsilon^{(d-1)/8})$ query time. In this paper we
present a data structure that achieves logarithmic query time with storage of
only $O(1/\varepsilon^{(d-1)/2})$ which matches the worst-case lower bound on
the complexity of any $\varepsilon$-approximating polytope. Our data structure
is based on a new technique a hierarchy of ellipsoids defined as
approximations to Macbeath regions.
  As an application we obtain major improvements to approximate Euclidean
nearest neighbor searching. Notably the storage needed to answer
$\varepsilon$-approximate nearest neighbor queries for a set of $n$ points in
$O(\log \frac{n}{\varepsilon})$ time is reduced to $O(n/\varepsilon^{d/2})$.
This halves the exponent in the $\varepsilon$-dependency of the existing space
bound of roughly $O(n/\varepsilon^d)$ which has stood for 15 years (Har-Peled
2001).",Sunil Arya|Guilherme D. da Fonseca|David M. Mount,cs.CG
2017-02-28T17:21:46Z,2016-12-09T20:40:47Z,http://arxiv.org/abs/1612.01507v2,http://arxiv.org/pdf/1612.01507v2,"Eldan's Stochastic Localization and the KLS Hyperplane Conjecture: An
  Improved Lower Bound for Expansion","We show that the KLS constant for $n$-dimensional isotropic logconcave
measures is $O(n^{1/4})$ improving on the current best bound of
$O(n^{1/3}\sqrt{\log n})$$.$ As corollaries we obtain the same improved bound
on the thin-shell estimate Poincare constant and exponential concentration
constant and an alternative proof of this bound for the isotropic constant; it
also follows that the ball walk for sampling from an isotropic logconcave
density in ${\bf R}^{n}$ converges in $O^{*}(n^{2.5})$ steps from a warm start.",Yin Tat Lee|Santosh S. Vempala,math.FA|cs.CG|cs.DS|math.MG|math.PR
2017-02-28T17:21:46Z,2016-12-05T15:30:57Z,http://arxiv.org/abs/1701.01322v1,http://arxiv.org/pdf/1701.01322v1,A Hybrid Energy Sharing Framework for Green Cellular Networks,"Cellular operators are increasingly turning towards renewable energy (RE) as
an alternative to using traditional electricity in order to reduce operational
expenditure and carbon footprint. Due to the randomness in both RE generation
and mobile traffic at each base station (BS) a surplus or shortfall of energy
may occur at any given time. To increase energy self-reliance and minimize the
network's energy cost the operator needs to efficiently exploit the RE
generated across all BSs. In this paper a hybrid energy sharing framework for
cellular network is proposed where a combination of physical power lines and
energy trading with other BSs using smart grid is used. Algorithms for physical
power lines deployment between BSs based on average and complete statistics of
the net RE available are developed. Afterwards an energy management framework
is formulated to optimally determine the quantities of electricity and RE to be
procured and exchanged among BSs respectively while considering battery
capacities and real-time energy pricing. Three cases are investigated where RE
generation is unknown perfectly known and partially known ahead of time.
Results investigate the time varying energy management of BSs and demonstrate
considerable reduction in average energy cost thanks to the hybrid energy
sharing scheme.",Muhammad Junaid Farooq|Hakim Ghazzai|Abdullah Kadri|Hesham ElSawy|Mohamed-Slim Alouini,cs.NI|cs.CG|cs.IT|math.IT
2017-02-28T17:21:49Z,2016-12-05T14:29:01Z,http://arxiv.org/abs/1612.01370v1,http://arxiv.org/pdf/1612.01370v1,"Minimizing the Continuous Diameter when Augmenting a Tree with a
  Shortcut","We augment a tree $T$ with a shortcut $pq$ to minimize the largest distance
between any two points along the resulting augmented tree $T+pq$. We study this
problem in a continuous and geometric setting where $T$ is a geometric tree in
the Euclidean plane where a shortcut is a line segment connecting any two
points along the edges of $T$ and we consider all points on $T+pq$ (i.e.
vertices and points along edges) when determining the largest distance along
$T+pq$. We refer to the largest distance between any two points along edges as
the continuous diameter to distinguish it from the discrete diameter i.e. the
largest distance between any two vertices.
  We establish that a single shortcut is sufficient to reduce the continuous
diameter of a geometric tree $T$ if and only if the intersection of all
diametral paths of $T$ is neither a line segment nor a single point. We
determine an optimal shortcut for a geometric tree with $n$ straight-line edges
in $O(n \log n)$ time. Apart from the running time our results extend to
geometric trees whose edges are rectifiable curves. The algorithm for trees
generalizes our algorithm for paths.",Jean-Lou De Carufel|Carsten Grimm|Stefan Schirra|Michiel Smid,cs.CG|F.2.2; G.2.2; I.3.5
2017-02-28T17:21:49Z,2016-12-05T13:02:30Z,http://arxiv.org/abs/1612.01335v1,http://arxiv.org/pdf/1612.01335v1,"Randomized Incremental Construction for the Hausdorff Voronoi Diagram of
  point clusters","This paper applies the randomized incremental construction (RIC) framework to
computing the Hausdorff Voronoi diagram of a family of k clusters of points in
the plane. The total number of points is n. The diagram is a generalization of
Voronoi diagrams based on the Hausdorff distance function. The combinatorial
complexity of the Hausdorff Voronoi diagram is O(n+m) where m is the total
number of crossings between pairs of clusters. For non-crossing clusters (m=0)
our algorithm works in expected O(n log n + k log n log k) time and
deterministic
  O(n) space. For arbitrary clusters (m=O(n^2)) the algorithm runs in expected
O((m+n log k) log n) time and O(m +n log k) space. When clusters cross
bisectors are disconnected curves resulting in disconnected Voronoi regions
that challenge the incremental construction. This paper applies the RIC
paradigm to a Voronoi diagram with disconnected regions and disconnected
bisectors for the first time.",Elena Khramtcova|Evanthia Papadopoulou,cs.CG
2017-02-28T17:21:49Z,2016-12-04T23:36:09Z,http://arxiv.org/abs/1612.03735v1,http://arxiv.org/pdf/1612.03735v1,A Note on Testing Intersection of Convex Sets in Sublinear Time,"We present a simple sublinear time algorithm for testing the following
geometric property. Let $P_1 ... P_n$ be $n$ convex sets in $\mathbb{R}^d$
($n \gg d$) such as polytopes balls etc. We assume that the complexity of
each set depends only on $d$ (and not on the number of sets $n$). We test the
property that there exists a common point in all sets i.e. that their
intersection is nonempty. Our goal is to distinguish between the case where the
intersection is nonempty and the case where even after removing many of the
sets the intersection is empty. In particular our algorithm returns PASS if
all of the $n$ sets intersect and returns FAIL with probability at least
$1-\epsilon$ if no point belongs to $\frac{\alpha}{d+1} n$ sets for any given
$0 < \alpha \epsilon < 1$.",Israela Solomon,cs.DS|cs.CG
2017-02-28T17:21:49Z,2016-12-03T01:20:25Z,http://arxiv.org/abs/1612.00908v1,http://arxiv.org/pdf/1612.00908v1,Cutting lemma and Zarankiewicz's problem in distal structures,"We establish a cutting lemma for definable families of sets in distal
structures as well as the optimality of the distal cell decomposition for
definable families of sets on the plane in $o$-minimal expansions of fields.
Using it we generalize the results in [J. Fox J. Pach A. Sheffer A. Suk
and J. Zahl. ""A semi-algebraic version of Zarankiewicz's problem"" Preprint
arXiv:1407.5705 (2014)] on the semialgebraic planar Zarankiewicz problem to
arbitrary $o$-minimal structures in particular obtaining an $o$-minimal
generalization of the Szemer\'edi-Trotter theorem.",Artem Chernikov|David Galvin|Sergei Starchenko,"math.LO|cs.CG|math.CO|03C45, 03C64, 05C35, 05D40"
2017-02-28T17:21:49Z,2016-12-01T16:45:30Z,http://arxiv.org/abs/1612.00343v1,http://arxiv.org/abs/1612.00343v1,Global Minimum for a Finsler Elastica Minimal Path Approach,"In this paper we propose a novel curvature-penalized minimal path model via
an orientation-lifted Finsler metric and the Euler elastica curve. The original
minimal path model computes the globally minimal geodesic by solving an Eikonal
partial differential equation (PDE). Essentially this first-order model is
unable to penalize curvature which is related to the path rigidity property in
the classical active contour models. To solve this problem we present an
Eikonal PDE-based Finsler elastica minimal path approach to address the
curvature-penalized geodesic energy minimization problem. We were successful at
adding the curvature penalization to the classical geodesic energy. The basic
idea of this work is to interpret the Euler elastica bending energy via a novel
Finsler elastica metric that embeds a curvature penalty. This metric is
non-Riemannian anisotropic and asymmetric and is defined over an
orientation-lifted space by adding to the image domain the orientation as an
extra space dimension. Based on this orientation lifting the proposed minimal
path model can benefit from both the curvature and orientation of the paths.
Thanks to the fast marching method the global minimum of the
curvature-penalized geodesic energy can be computed efficiently. We introduce
two anisotropic image data-driven speed functions that are computed by
steerable filters. Based on these orientation-dependent speed functions we can
apply the proposed Finsler elastica minimal path model to the applications of
closed contour detection perceptual grouping and tubular structure extraction.
Numerical experiments on both synthetic and real images show that these
applications of the proposed model indeed obtain promising results.",Da Chen|Jean-Marie Mirebeau|Laurent D. Cohen,cs.CG|cs.CV
2017-02-28T17:21:49Z,2016-11-30T14:27:53Z,http://arxiv.org/abs/1612.03195v1,http://arxiv.org/pdf/1612.03195v1,"No equations no parameters no variables: data and the reconstruction
  of normal forms by learning informed observation geometries","The discovery of physical laws consistent with empirical observations lies at
the heart of (applied) science and engineering. These laws typically take the
form of nonlinear differential equations depending on parameters dynamical
systems theory provides through the appropriate normal forms an ""intrinsic""
prototypical characterization of the types of dynamical regimes accessible to a
given model. Using an implementation of data-informed geometry learning we
directly reconstruct the relevant ""normal forms"": a quantitative mapping from
empirical observations to prototypical realizations of the underlying dynamics.
Interestingly the state variables and the parameters of these realizations are
inferred from the empirical observations without prior knowledge or
understanding they parametrize the dynamics {\em intrinsically} without
explicit reference to fundamental physical quantities.",Or Yair|Ronen Talmon|Ronald R. Coifman|Ioannis G. Kevrekidis,nlin.PS|cs.CG
2017-02-28T17:21:49Z,2016-11-30T09:30:41Z,http://arxiv.org/abs/1611.10059v1,http://arxiv.org/pdf/1611.10059v1,"An Efficient Algorithm for Vertex Enumeration of Two-Dimensional
  Projection of Polytopes","An efficient algorithm to enumerate the vertices of a two-dimensional (2D)
projection of a polytope is presented in this paper. The proposed algorithm
uses the support function of the polytope to be projected and enumerated for
vertices. The complexity of our algorithm is linear in the number of vertices
of the projected polytope and we show empirically that the performance is
significantly better in comparison to some known efficient algorithms of
projection and enumeration.",Amit Gurung|Rajarshi Ray,cs.CG
2017-02-28T17:21:49Z,2016-11-29T05:03:39Z,http://arxiv.org/abs/1611.09485v1,http://arxiv.org/pdf/1611.09485v1,Dispersing Points on Intervals,"We consider a problem of dispersing points on disjoint intervals on a line.
Given n pairwise disjoint intervals sorted on a line we want to find a point
in each interval such that the minimum pairwise distance of these points is
maximized. Based on a greedy strategy we present a linear time algorithm for
the problem. Further we also solve in linear time the cycle version of the
problem where the intervals are given on a cycle.",Shimin Li|Haitao Wang,cs.CG
2017-02-28T17:21:49Z,2016-11-28T21:29:07Z,http://arxiv.org/abs/1611.09392v1,http://arxiv.org/pdf/1611.09392v1,Generating Holistic 3D Scene Abstractions for Text-based Image Retrieval,"Spatial relationships between objects provide important information for
text-based image retrieval. As users are more likely to describe a scene from a
real world perspective using 3D spatial relationships rather than 2D
relationships that assume a particular viewing direction one of the main
challenges is to infer the 3D structure that bridges images with users' text
descriptions. However direct inference of 3D structure from images requires
learning from large scale annotated data. Since interactions between objects
can be reduced to a limited set of atomic spatial relations in 3D we study the
possibility of inferring 3D structure from a text description rather than an
image applying physical relation models to synthesize holistic 3D abstract
object layouts satisfying the spatial constraints present in a textual
description. We present a generic framework for retrieving images from a
textual description of a scene by matching images with these generated abstract
object layouts. Images are ranked by matching object detection outputs
(bounding boxes) to 2D layout candidates (also represented by bounding boxes)
which are obtained by projecting the 3D scenes with sampled camera directions.
We validate our approach using public indoor scene datasets and show that our
method outperforms an object occurrence based and a learned 2D pairwise
relation based baselines.",Ang Li|Jin Sun|Joe Yue-Hei Ng|Ruichi Yu|Vlad I. Morariu|Larry S. Davis,cs.CV|cs.CG|cs.IR
2017-02-28T17:21:49Z,2016-12-02T23:24:30Z,http://arxiv.org/abs/1611.08996v2,http://arxiv.org/pdf/1611.08996v2,"Locally-orthogonal unstructured grid-generation for general circulation
  modelling on the sphere","An algorithm for the generation of non-uniform locally-orthogonal staggered
unstructured grids on spheroidal geometries is described. This technique is
designed to generate high-quality staggered Voronoi/Delaunay dual meshes
appropriate for general circulation modelling on the sphere including
applications to atmospheric simulation ocean-modelling and numerical weather
predication. Using a recently developed Frontal-Delaunay refinement technique
a method for the construction of guaranteed-quality unstructured spheroidal
Delaunay triangulations is introduced. A locally-orthogonal polygonal grid
derived from the associated Voronoi diagram is computed as the staggered dual.
The initial staggered Voronoi/Delaunay tessellation is iteratively improved
through hill-climbing optimisation techniques. It is shown that this approach
typically produces grids with very high element quality and smooth grading
characteristics while imposing relatively low computational expense. Initial
results are presented for a selection of uniform and non-uniform spheroidal
grids appropriate for high-resolution multi-scale general circulation
modelling. The use of user-defined mesh spacing functions to generate smoothly
graded non-uniform grids for multi-resolution type studies is discussed in
detail.",Darren Engwirda,physics.ao-ph|cs.CG|physics.comp-ph|physics.flu-dyn
2017-02-28T17:21:53Z,2016-11-29T19:13:15Z,http://arxiv.org/abs/1611.08757v2,http://arxiv.org/pdf/1611.08757v2,Number Balancing is as hard as Minkowski's Theorem and Shortest Vector,"The number balancing (NBP) problem is the following: given real numbers
$a_1\ldotsa_n \in [01]$ find two disjoint subsets $I_1I_2 \subseteq [n]$
so that the difference $|\sum_{i \in I_1}a_i - \sum_{i \in I_2}a_i|$ of their
sums is minimized. An application of the pigeonhole principle shows that there
is always a solution where the difference is at most $O(\frac{\sqrt{n}}{2^n})$.
Finding the minimum however is NP-hard. In polynomial timethe differencing
algorithm by Karmarkar and Karp from 1982 can produce a solution with
difference at most $n^{-\Theta(\log n)}$ but no further improvement has been
made since then.
  In this paper we show a relationship between NBP and Minkowski's Theorem.
First we show that an approximate oracle for Minkowski's Theorem gives an
approximate NBP oracle. Perhaps more surprisingly we show that an approximate
NBP oracle gives an approximate Minkowski oracle. In particular we prove that
any polynomial time algorithm that guarantees a solution of difference at most
$2^{\sqrt{n}} / 2^{n}$ would give a polynomial approximation for Minkowski as
well as a polynomial factor approximation algorithm for the Shortest Vector
Problem.",Rebecca Hoberg|Harishchandra Ramadas|Thomas Rothvoss|Xin Yang,cs.DM|cs.CC|cs.CG|cs.DS
2017-02-28T17:21:53Z,2016-11-26T22:27:17Z,http://arxiv.org/abs/1611.08752v1,http://arxiv.org/pdf/1611.08752v1,"Deterministic Discrepancy Minimization via the Multiplicative Weight
  Update Method","A well-known theorem of Spencer shows that any set system with $n$ sets over
$n$ elements admits a coloring of discrepancy $O(\sqrt{n})$. While the original
proof was non-constructive recent progress brought polynomial time algorithms
by Bansal Lovett and Meka and Rothvoss. All those algorithms are randomized
even though Bansal's algorithm admitted a complicated derandomization.
  We propose an elegant deterministic polynomial time algorithm that is
inspired by Lovett-Meka as well as the Multiplicative Weight Update method. The
algorithm iteratively updates a fractional coloring while controlling the
exponential weights that are assigned to the set constraints.
  A conjecture by Meka suggests that Spencer's bound can be generalized to
symmetric matrices. We prove that $n \times n$ matrices that are block diagonal
with block size $q$ admit a coloring of discrepancy $O(\sqrt{n} \cdot
\sqrt{\log(q)})$.
  Bansal Dadush and Garg recently gave a randomized algorithm to find a vector
$x$ with entries in $\lbrace{-11\rbrace}$ with $\|Ax\|_{\infty} \leq
O(\sqrt{\log n})$ in polynomial time where $A$ is any matrix whose columns
have length at most 1. We show that our method can be used to deterministically
obtain such a vector.",Avi Levy|Harishchandra Ramadas|Thomas Rothvoss,cs.DM|cs.CG|cs.DS|math.CO
2017-02-28T17:21:53Z,2016-11-25T01:19:55Z,http://arxiv.org/abs/1612.01400v1,http://arxiv.org/pdf/1612.01400v1,A Distance Function for Comparing Straight-Edge Geometric Figures,"This paper defines a distance function that measures the dissimilarity
between planar geometric figures formed with straight lines. This function can
in turn be used in partial matching of different geometric figures. For a given
pair of geometric figures that are graphically isomorphic one function
measures the angular dissimilarity and another function measures the edge
length disproportionality. The distance function is then defined as the convex
sum of these two functions. The novelty of the presented function is that it
satisfies all properties of a distance function and the computation of the same
is done by projecting appropriate features to a cartesian plane. To compute the
deviation from the angular similarity property the Euclidean distance between
the given angular pairs and the corresponding points on the $y=x$ line is
measured. Further while computing the deviation from the edge length
proportionality property the best fit line for the set of edge lengths which
passes through the origin is found and the Euclidean distance between the
given edge length pairs and the corresponding point on a $y=mx$ line is
calculated. Iterative Proportional Fitting Procedure (IPFP) is used to find
this best fit line. We demonstrate the behavior of the defined function for
some sample pairs of figures.",Apoorva Honnegowda Roopa|Shrisha Rao,"cs.CV|cs.CG|65D10 (primary), 51K05 (secondary)"
2017-02-28T17:21:53Z,2016-11-24T21:38:59Z,http://arxiv.org/abs/1611.08315v1,http://arxiv.org/pdf/1611.08315v1,Universal Guard Problems,"We provide a spectrum of results for the Universal Guard Problem in which
one is to obtain a small set of points (""guards"") that are ""universal"" in their
ability to guard any of a set of possible polygonal domains in the plane. We
give upper and lower bounds on the number of universal guards that are always
sufficient to guard all polygons having a given set of n vertices or to guard
all polygons in a given set of k polygons on an n-point vertex set. Our upper
bound proofs include algorithms to construct universal guard sets of the
respective cardinalities.",Sándor P. Fekete|Qian Li|Joseph S. B. Mitchell|Christian Scheffer,cs.CG|F.2.2
2017-02-28T17:21:53Z,2016-11-23T14:21:02Z,http://arxiv.org/abs/1611.07808v1,http://arxiv.org/pdf/1611.07808v1,Hardness of Liar's Domination on Unit Disk Graphs,"A unit disk graph is the intersection graph of a set of unit diameter disks
in the plane. In this paper we consider liar's domination problem on unit disk
graphs a variant of dominating set problem. We call this problem as {\it
Euclidean liar's domination problem}. In the Euclidean liar's domination
problem a set ${\cal P}=\{p_1p_2\ldotsp_n\}$ of $n$ points (disk centers)
are given in the Euclidean plane. For $p \in {\cal P}$ $N[p]$ is a subset of
${\cal P}$ such that for any $q \in N[p]$ the Euclidean distance between $p$
and $q$ is less than or equal to 1 i.e. the corresponding unit diameter disks
intersect. The objective of the Euclidean liar's domination problem is to find
a subset $D\; (\subseteq {\cal P})$ of minimum size having the following
properties : (i) $|N[p_i] \cap D| \geq 2$ for $1 \leq i \leq n$ and (ii)
$|(N[p_i] \cup N[p_j]) \cap D| \geq 3$ for $i\neq j 1\leq ij \leq n$. This
article aims to prove the Euclidean liar's domination problem is NP-complete.",Ramesh K Jallu|Gautam K Das,cs.CG
2017-02-28T17:21:53Z,2017-02-01T05:13:55Z,http://arxiv.org/abs/1611.07369v2,http://arxiv.org/pdf/1611.07369v2,Geometry of 3D Environments and Sum of Squares Polynomials,"Motivated by applications in robotics and computer vision we study problems
related to spatial reasoning of a 3D environment using sublevel sets of
polynomials. These include: tightly containing a cloud of points (e.g.
representing an obstacle) with convex or nearly-convex basic semialgebraic
sets computation of Euclidean distances between two such sets separation of
two convex basic semalgebraic sets that overlap and tight containment of the
union of several basic semialgebraic sets with a single convex one. We use
algebraic techniques from sum of squares optimization that reduce all these
tasks to semidefinite programs of small size and present numerical experiments
in realistic scenarios.",Amir Ali Ahmadi|Georgina Hall|Ameesh Makadia|Vikas Sindhwani,math.OC|cs.CG|cs.CV|cs.GR
2017-02-28T17:21:53Z,2016-12-22T15:16:29Z,http://arxiv.org/abs/1611.07362v2,http://arxiv.org/pdf/1611.07362v2,An o-minimal Szemerédi-Trotter theorem,"We prove an analog of the Szemer\'edi-Trotter theorem in the plane for
definable curves and points in any o-minimal structure over an arbitrary real
closed field $\mathrm{R}$. One new ingredient in the proof is an extension of
the well known crossing number inequality for graphs to the case of embeddings
in any o-minimal structure over an arbitrary real closed field.",Saugata Basu|Orit E. Raz,"math.LO|cs.CG|math.CO|03C64, 05D40"
2017-02-28T17:21:53Z,2016-11-23T01:38:44Z,http://arxiv.org/abs/1611.07073v2,http://arxiv.org/pdf/1611.07073v2,Squarability of rectangle arrangements,"We study when an arrangement of axis-aligned rectangles can be transformed
into an arrangement of axis-aligned squares in $\mathbb{R}^2$ while preserving
its structure. We found a counterexample to the conjecture of J. Klawitter M.
N\""ollenburg and T. Ueckerdt whether all arrangements without crossing and
side-piercing can be squared. Our counterexample also works in a more general
case when we only need to preserve the intersection graph and we forbid
side-piercing between squares. We also show counterexamples for transforming
box arrangements into combinatorially equivalent hypercube arrangements.
Finally we introduce a linear program deciding whether an arrangement of
rectangles can be squared in a more restrictive version where the order of all
sides is preserved.",Matěj Konečný|Stanislav Kučera|Michal Opler|Jakub Sosnovec|Štěpán Šimsa|Martin Töpfer,cs.CG
2017-02-28T17:21:53Z,2017-01-09T18:43:44Z,http://arxiv.org/abs/1611.06915v2,http://arxiv.org/pdf/1611.06915v2,Space-Efficient Hidden Surface Removal,"We propose a space-efficient algorithm for hidden surface removal that
combines one of the fastest previous algorithms for that problem with
techniques based on bit manipulation. Such techniques had been successfully
used in other settings for example to reduce working space for several graph
algorithms. However bit manipulation is not usually employed in geometric
algorithms because the standard model of computation (the real RAM) does not
support it. For this reason we first revisit our model of computation to have
a reasonable theoretical framework. Under this framework we show how the use of
a bit representation for the union of triangles in combination with
rank-select data structures allows us to implicitly compute the union of $n$
triangles with roughly $O(1)$ bits per union boundary vertex. This results in
an algorithm that uses at most as much space as the previous one and depending
on the input can give a reduction of up to a factor $\Theta(\log n)$ while
maintaining the running time.",Frank Kammer|Maarten Löffler|Rodrigo I. Silveira,cs.CG|F.2.2
2017-02-28T17:21:53Z,2016-11-21T12:57:43Z,http://arxiv.org/abs/1611.06768v1,http://arxiv.org/pdf/1611.06768v1,Symmetries of Canal Surfaces and Dupin Cyclides,"We develop a characterization for the existence of symmetries of canal
surfaces defined by a rational spine curve and rational radius function. This
characterization leads to a method for constructing rational canal surfaces
with prescribed symmetries and it inspires an algorithm for computing the
symmetries of such canal surfaces. For Dupin cyclides in canonical form we
apply the characterization to derive an intrinsic description of their
symmetries and symmetry groups which gives rise to a method for computing the
symmetries of a Dupin cyclide not necessarily in canonical form.",Juan Gerardo Alcázar|Heidi E. I. Dahl|Georg Muntingh,"math.AG|cs.CG|cs.SC|14Q10, 68W30"
