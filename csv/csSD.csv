2017-02-28T17:24:32Z,2017-02-25T03:11:04Z,http://arxiv.org/abs/1702.07825v1,http://arxiv.org/pdf/1702.07825v1,Deep Voice: Real-time Neural Text-to-Speech,"We present Deep Voice a production-quality text-to-speech system constructed
entirely from deep neural networks. Deep Voice lays the groundwork for truly
end-to-end neural speech synthesis. The system comprises five major building
blocks: a segmentation model for locating phoneme boundaries a
grapheme-to-phoneme conversion model a phoneme duration prediction model a
fundamental frequency prediction model and an audio synthesis model. For the
segmentation model we propose a novel way of performing phoneme boundary
detection with deep neural networks using connectionist temporal classification
(CTC) loss. For the audio synthesis model we implement a variant of WaveNet
that requires fewer parameters and trains faster than the original. By using a
neural network for each component our system is simpler and more flexible than
traditional text-to-speech systems where each component requires laborious
feature engineering and extensive domain expertise. Finally we show that
inference with our system can be performed faster than real time and describe
optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x
speedups over existing implementations.",Sercan O. Arik|Mike Chrzanowski|Adam Coates|Gregory Diamos|Andrew Gibiansky|Yongguo Kang|Xian Li|John Miller|Jonathan Raiman|Shubho Sengupta|Mohammad Shoeybi,cs.CL|cs.LG|cs.NE|cs.SD
2017-02-28T17:24:32Z,2017-02-24T22:27:29Z,http://arxiv.org/abs/1702.07787v1,http://arxiv.org/pdf/1702.07787v1,"Convolutional Gated Recurrent Neural Network Incorporating Spatial
  Features for Audio Tagging","Environmental audio tagging is a newly proposed task to predict the presence
or absence of a specific audio event in a chunk. Deep neural network (DNN)
based methods have been successfully adopted for predicting the audio tags in
the domestic audio scene. In this paper we propose to use a convolutional
neural network (CNN) to extract robust features from mel-filter banks (MFBs)
spectrograms or even raw waveforms for audio tagging. Gated recurrent unit
(GRU) based recurrent neural networks (RNNs) are then cascaded to model the
long-term temporal structure of the audio signal. To complement the input
information an auxiliary CNN is designed to learn on the spatial features of
stereo recordings. We evaluate our proposed methods on Task 4 (audio tagging)
of the Detection and Classification of Acoustic Scenes and Events 2016 (DCASE
2016) challenge. Compared with our recent DNN-based method the proposed
structure can reduce the equal error rate (EER) from 0.13 to 0.11 on the
development set. The spatial features can further reduce the EER to 0.10. The
performance of the end-to-end learning on raw waveforms is also comparable.
Finally on the evaluation set we get the state-of-the-art performance with
0.12 EER while the performance of the best existing system is 0.15 EER.",Yong Xu|Qiuqiang Kong|Qiang Huang|Wenwu Wang|Mark D. Plumbley,cs.SD|cs.LG|cs.NE
2017-02-28T17:24:32Z,2017-02-24T17:23:01Z,http://arxiv.org/abs/1702.07713v1,http://arxiv.org/pdf/1702.07713v1,"Multichannel Linear Prediction for Blind Reverberant Audio Source
  Separation","A class of methods based on multichannel linear prediction (MCLP) can achieve
effective blind dereverberation of a source when the source is observed with a
microphone array. We propose an inventive use of MCLP as a pre-processing step
for blind source separation with a microphone array. We show theoretically
that under certain assumptions such pre-processing reduces the original blind
reverberant source separation problem to a non-reverberant one which in turn
can be effectively tackled using existing methods. We demonstrate our claims
using real recordings obtained with an eight-microphone circular array in
reverberant environments.",İlker Bayram|Savaşkan Bulek,cs.SD|cs.CE
2017-02-28T17:24:32Z,2017-02-23T02:31:03Z,http://arxiv.org/abs/1702.07071v1,http://arxiv.org/pdf/1702.07071v1,"Pronunciation recognition of English phonemes /\textipa{@}/ /æ/
  /\textipa{A}:/ and /\textipa{2}/ using Formants and Mel Frequency Cepstral
  Coefficients","The Vocal Joystick Vowel Corpus by Washington University was used to study
monophthongs pronounced by native English speakers. The objective of this study
was to quantitatively measure the extent at which speech recognition methods
can distinguish between similar sounding vowels. In particular the phonemes
/\textipa{@}/ /{\ae}/ /\textipa{A}:/ and /\textipa{2}/ were analysed. 748
sound files from the corpus were used and subjected to Linear Predictive Coding
(LPC) to compute their formants and to Mel Frequency Cepstral Coefficients
(MFCC) algorithm to compute the cepstral coefficients. A Decision Tree
Classifier was used to build a predictive model that learnt the patterns of the
two first formants measured in the data set as well as the patterns of the 13
cepstral coefficients. An accuracy of 70\% was achieved using formants for the
mentioned phonemes. For the MFCC analysis an accuracy of 52 \% was achieved and
an accuracy of 71\% when /\textipa{@}/ was ignored. The results obtained show
that the studied algorithms are far from mimicking the ability of
distinguishing subtle differences in sounds like human hearing does.",Keith Y. Patarroyo|Vladimir Vargas-Calderón,cs.CL|cs.SD
2017-02-28T17:24:32Z,2017-02-22T09:36:48Z,http://arxiv.org/abs/1702.06724v1,http://arxiv.org/pdf/1702.06724v1,"A new cosine series antialiasing function and its application to
  aliasing-free glottal source models for speech and singing synthesis","We formulated and implemented a procedure to generate aliasing-free
excitation source signals based on the Fujisaki- Ljungqvist model. It uses a
new antialiasing filter in the contin- uous time domain followed by an IIR
digital filter for response equalization. We introduced a general designing
procedure of cosine series to design the new antialiasing function. We also
applied this new procedure to revise out previous implementa- tion of the
antialiased Fant-Liljencrants model. Combination of these signals and a lattice
implementation of time varying vocal tract model provides a reliable and
flexible basis to test F0 ex- tractors and source aperiodicity analysis
methods. MATLAB implementation of these antialiased excitation source models
are available as part of our open source tools for speech science",Hideki Kawahara|Ken-Ichi Sakakibara|Hideki Banno|Masanori Morise|Tomoki Toda|Toshio Irino,cs.SD
2017-02-28T17:24:32Z,2017-02-21T07:37:59Z,http://arxiv.org/abs/1702.06286v1,http://arxiv.org/pdf/1702.06286v1,"Convolutional Recurrent Neural Networks for Polyphonic Sound Event
  Detection","Sound events often occur in unstructured environments where they exhibit wide
variations in their frequency content and temporal structure. Convolutional
neural networks (CNN) are able to extract higher level features that are
invariant to local spectral and temporal variations. Recurrent neural networks
(RNNs) are powerful in learning the longer term temporal context in the audio
signals. CNNs and RNNs as classifiers have recently shown improved performances
over established methods in various sound recognition tasks. We combine these
two approaches in a Convolutional Recurrent Neural Network (CRNN) and apply it
on a polyphonic sound event detection task. We compare the performance of the
proposed CRNN method with CNN RNN and other established methods and observe
a considerable improvement for four different datasets consisting of everyday
sound events.",Emre Çakır|Giambattista Parascandolo|Toni Heittola|Heikki Huttunen|Tuomas Virtanen,cs.LG|cs.SD
2017-02-28T17:24:32Z,2017-02-13T14:44:17Z,http://arxiv.org/abs/1702.03791v1,http://arxiv.org/pdf/1702.03791v1,DNN Filter Bank Cepstral Coefficients for Spoofing Detection,"With the development of speech synthesis techniques automatic speaker
verification systems face the serious challenge of spoofing attack. In order to
improve the reliability of speaker verification systems we develop a new
filter bank based cepstral feature deep neural network filter bank cepstral
coefficients (DNN-FBCC) to distinguish between natural and spoofed speech. The
deep neural network filter bank is automatically generated by training a filter
bank neural network (FBNN) using natural and synthetic speech. By adding
restrictions on the training rules the learned weight matrix of FBNN is
band-limited and sorted by frequency similar to the normal filter bank. Unlike
the manually designed filter bank the learned filter bank has different filter
shapes in different channels which can capture the differences between natural
and synthetic speech more effectively. The experimental results on the ASVspoof
{2015} database show that the Gaussian mixture model maximum-likelihood
(GMM-ML) classifier trained by the new feature performs better than the
state-of-the-art linear frequency cepstral coefficients (LFCC) based
classifier especially on detecting unknown attacks.",Hong Yu|Zheng-Hua Tan|Zhanyu Ma|Jun Guo,cs.SD|cs.CR|cs.LG
2017-02-28T17:24:32Z,2017-02-08T04:59:00Z,http://arxiv.org/abs/1702.02289v1,http://arxiv.org/pdf/1702.02289v1,"Neural Network Based Speaker Classification and Verification Systems
  with Enhanced Features","This work presents a novel framework based on feed-forward neural network for
text-independent speaker classification and verification two related systems
of speaker recognition. With optimized features and model training it achieves
100% classification rate in classification and less than 6% Equal Error Rate
(ERR) using merely about 1 second and 5 seconds of data respectively. Features
with stricter Voice Active Detection (VAD) than the regular one for speech
recognition ensure extracting stronger voiced portion for speaker recognition
speaker-level mean and variance normalization helps to eliminate the
discrepancy between samples from the same speaker. Both are proven to improve
the system performance. In building the neural network speaker classifier the
network structure parameters are optimized with grid search and dynamically
reduced regularization parameters are used to avoid training terminated in
local minimum. It enables the training goes further with lower cost. In speaker
verification performance is improved with prediction score normalization
which rewards the speaker identity indices with distinct peaks and penalizes
the weak ones with high scores but more competitors and speaker-specific
thresholding which significantly reduces ERR in the ROC curve. TIMIT corpus
with 8K sampling rate is used here. First 200 male speakers are used to train
and test the classification performance. The testing files of them are used as
in-domain registered speakers while data from the remaining 126 male speakers
are used as out-of-domain speakers i.e. imposters in speaker verification.",Zhenhao Ge|Ananth N. Iyer|Srinath Cheluvaraja|Ram Sundaram|Aravind Ganapathiraju,cs.SD
2017-02-28T17:24:32Z,2017-02-08T04:37:40Z,http://arxiv.org/abs/1702.02285v1,http://arxiv.org/pdf/1702.02285v1,"Speaker Change Detection Using Features through A Neural Network Speaker
  Classifier","The mechanism proposed here is for real-time speaker change detection in
conversations which firstly trains a neural network text-independent speaker
classifier using in-domain speaker data. Through the network features of
conversational speech from out-of-domain speakers are then converted into
likelihood vectors i.e. similarity scores comparing to the in-domain speakers.
These transformed features demonstrate very distinctive patterns which
facilitates differentiating speakers and enable speaker change detection with
some straight-forward distance metrics. The speaker classifier and the speaker
change detector are trained/tested using speech of the first 200 (in-domain)
and the remaining 126 (out-of-domain) male speakers in TIMIT respectively. For
the speaker classification 100% accuracy at a 200 speaker size is achieved on
any testing file given the speech duration is at least 0.97 seconds. For the
speaker change detection using speaker classification outputs performance
based on 0.5 1 and 2 seconds of inspection intervals were evaluated in terms
of error rate and F1 score using synthesized data by concatenating speech from
various speakers. It captures close to 97% of the changes by comparing the
current second of speech with the previous second which is very competitive
among literature using other methods.",Zhenhao Ge|Ananth N. Iyer|Srinath Cheluvaraja|Aravind Ganapathiraju,cs.SD
2017-02-28T17:24:32Z,2017-02-07T18:41:31Z,http://arxiv.org/abs/1702.02130v1,http://arxiv.org/pdf/1702.02130v1,"On the Importance of Temporal Context in Proximity Kernels: A Vocal
  Separation Case Study","Musical source separation methods exploit source-specific spectral
characteristics to facilitate the decomposition process. Kernel Additive
Modelling (KAM) models a source applying robust statistics to time-frequency
bins as specified by a source-specific kernel a function defining similarity
between bins. Kernels in existing approaches are typically defined using
metrics between single time frames. In the presence of noise and other sound
sources information from a single-frame however turns out to be unreliable
and often incorrect frames are selected as similar. In this paper we
incorporate a temporal context into the kernel to provide additional
information stabilizing the similarity search. Evaluated in the context of
vocal separation our simple extension led to a considerable improvement in
separation quality compared to previous kernels.",Delia Fano Yela|Sebastian Ewert|Derry FitzGerald|Mark Sandler,cs.SD|H.5.5
2017-02-28T17:24:36Z,2017-02-07T13:19:08Z,http://arxiv.org/abs/1702.01999v1,http://arxiv.org/pdf/1702.01999v1,"Identification of Voice Utterance with Aging Factor Using the Method of
  MFCC Multichannel","This research was conducted to develop a method to identify voice utterance.
For voice utterance that encounters change caused by aging factor with the
interval of 10 to 25 years. The change of voice utterance influenced by aging
factor might be extracted by MFCC (Mel Frequency Cepstrum Coefficient).
However the level of the compatibility of the feature may be dropped down to
55%. While the ones which do not encounter it may reach 95%. To improve the
compatibility of the changing voice feature influenced by aging factor then
the method of the more specific feature extraction is developed: which is by
separating the voice into several channels suggested as MFCC multichannel
consisting of multichannel 5 filterbank (M5FB) multichannel 2 filterbank
(M2FB) and multichannel 1 filterbank (M1FB). The result of the test shows that
for model M5FB and M2FB have the highest score in the level of compatibility
with 85% and 82% with 25 years interval. While model M5FB gets the highest
score of 86% for 10 years time interval.",Roy Rudolf Huizen|Jazi Eko Istiyanto|Agfianto Eko Putra,cs.SD
2017-02-28T17:24:36Z,2017-02-06T03:37:28Z,http://arxiv.org/abs/1702.00956v2,http://arxiv.org/pdf/1702.00956v2,"KU-ISPL Speaker Recognition Systems under Language mismatch condition
  for NIST 2016 Speaker Recognition Evaluation","Korea University Intelligent Signal Processing Lab. (KU-ISPL) developed
speaker recognition system for SRE16 fixed training condition. Data for
evaluation trials are collected from outside North America spoken in Tagalog
and Cantonese while training data only is spoken English. Thus main issue for
SRE16 is compensating the discrepancy between different languages. As
development dataset which is spoken in Cebuano and Mandarin we could prepare
the evaluation trials through preliminary experiments to compensate the
language mismatched condition. Our team developed 4 different approaches to
extract i-vectors and applied state-of-the-art techniques as backend. To
compensate language mismatch we investigated and endeavored unique method such
as unsupervised language clustering inter language variability compensation
and gender/language dependent score normalization.",Suwon Shon|Hanseok Ko,cs.SD|cs.CL
2017-02-28T17:24:36Z,2017-02-01T09:44:44Z,http://arxiv.org/abs/1702.00178v1,http://arxiv.org/pdf/1702.00178v1,"On the Futility of Learning Complex Frame-Level Language Models for
  Chord Recognition","Chord recognition systems use temporal models to post-process frame-wise
chord preditions from acoustic models. Traditionally first-order models such
as Hidden Markov Models were used for this task with recent works suggesting
to apply Recurrent Neural Networks instead. Due to their ability to learn
longer-term dependencies these models are supposed to learn and to apply
musical knowledge instead of just smoothing the output of the acoustic model.
In this paper we argue that learning complex temporal models at the level of
audio frames is futile on principle and that non-Markovian models do not
perform better than their first-order counterparts. We support our argument
through three experiments on the McGill Billboard dataset. The first two show
1) that when learning complex temporal models at the frame level improvements
in chord sequence modelling are marginal; and 2) that these improvements do not
translate when applied within a full chord recognition system. The third still
rather preliminary experiment gives first indications that the use of complex
sequential models for chord prediction at higher temporal levels might be more
promising.",Filip Korzeniowski|Gerhard Widmer,cs.SD|cs.LG
2017-02-28T17:24:36Z,2017-01-31T19:21:41Z,http://arxiv.org/abs/1702.00025v1,http://arxiv.org/pdf/1702.00025v1,"An Experimental Analysis of the Entanglement Problem in
  Neural-Network-based Music Transcription Systems","Several recent polyphonic music transcription systems have utilized deep
neural networks to achieve state of the art results on various benchmark
datasets pushing the envelope on framewise and note-level performance
measures. Unfortunately we can observe a sort of glass ceiling effect. To
investigate this effect we provide a detailed analysis of the particular kinds
of errors that state of the art deep neural transcription systems make when
trained and tested on a piano transcription task. We are ultimately forced to
draw a rather disheartening conclusion: the networks seem to learn combinations
of notes and have a hard time generalizing to unseen combinations of notes.
Furthermore we speculate on various means to alleviate this situation.",Rainer Kelz|Gerhard Widmer,cs.SD
2017-02-28T17:24:36Z,2017-01-29T01:25:57Z,http://arxiv.org/abs/1701.08343v1,http://arxiv.org/pdf/1701.08343v1,"Rhythm Transcription of Polyphonic Piano Music Based on Merged-Output
  HMM for Multiple Voices","In a recent conference paper we have reported a rhythm transcription method
based on a merged-output hidden Markov model (HMM) that explicitly describes
the multiple-voice structure of polyphonic music. This model solves a major
problem of conventional methods that could not properly describe the nature of
multiple voices as in polyrhythmic scores or in the phenomenon of loose
synchrony between voices. In this paper we present a complete description of
the proposed model and develop an inference technique which is valid for any
merged-output HMMs for which output probabilities depend on past events. We
also examine the influence of the architecture and parameters of the method in
terms of accuracies of rhythm transcription and voice separation and perform
comparative evaluations with six other algorithms. Using MIDI recordings of
classical piano pieces we found that the proposed model outperformed other
methods by more than 12 points in the accuracy for polyrhythmic performances
and performed almost as good as the best one for non-polyrhythmic performances.
This reveals the state-of-the-art methods of rhythm transcription for the first
time in the literature. Publicly available source codes are also provided for
future comparisons.",Eita Nakamura|Kazuyoshi Yoshii|Shigeki Sagayama,cs.AI|cs.SD
2017-02-28T17:24:36Z,2017-01-27T12:38:47Z,http://arxiv.org/abs/1701.08156v1,http://arxiv.org/pdf/1701.08156v1,A Comprehensive Survey on Bengali Phoneme Recognition,"Hidden Markov model based various phoneme recognition methods for Bengali
language is reviewed. Automatic phoneme recognition for Bengali language using
multilayer neural network is reviewed. Usefulness of multilayer neural network
over single layer neural network is discussed. Bangla phonetic feature table
construction and enhancement for Bengali speech recognition is also discussed.
Comparison among these methods is discussed.",Sadia Tasnim Swarna|Shamim Ehsan|Md. Saiful Islam|Marium E Jannat,cs.SD|cs.CL
2017-02-28T17:24:36Z,2017-01-27T19:21:10Z,http://arxiv.org/abs/1701.07138v3,http://arxiv.org/pdf/1701.07138v3,Learning Mid-Level Auditory Codes from Natural Sound Statistics,"Interaction with the world requires an organism to transform sensory signals
into representations in which behaviorally meaningful properties of the
environment are made explicit. These representations are derived through
cascades of neuronal processing stages in which neurons at each stage recode
the output of preceding stages. Explanations of sensory coding may thus involve
understanding how low-level patterns are combined into more complex structures.
Although models exist in the visual domain to explain how mid-level features
such as junctions and curves might be derived from oriented filters in early
visual cortex little is known about analogous grouping principles for
mid-level auditory representations. We propose a hierarchical generative model
of natural sounds that learns combinations of spectrotemporal features from
natural stimulus statistics. In the first layer the model forms a sparse
convolutional code of spectrograms using a dictionary of learned
spectrotemporal kernels. To generalize from specific kernel activation
patterns the second layer encodes patterns of time-varying magnitude of
multiple first layer coefficients. Because second-layer features are sensitive
to combinations of spectrotemporal features the representation they support
encodes more complex acoustic patterns than the first layer. When trained on
corpora of speech and environmental sounds some second-layer units learned to
group spectrotemporal features that occur together in natural sounds. Others
instantiate opponency between dissimilar sets of spectrotemporal features. Such
groupings might be instantiated by neurons in the auditory cortex providing a
hypothesis for mid-level neuronal computation.",Wiktor Młynarski|Josh H. McDermott,q-bio.NC|cs.SD
2017-02-28T17:24:36Z,2017-01-23T11:18:06Z,http://arxiv.org/abs/1702.02092v1,http://arxiv.org/pdf/1702.02092v1,Characterisation of speech diversity using self-organising maps,"We report investigations into speaker classification of larger quantities of
unlabelled speech data using small sets of manually phonemically annotated
speech. The Kohonen speech typewriter is a semi-supervised method comprised of
self-organising maps (SOMs) that achieves low phoneme error rates. A SOM is a
2D array of cells that learn vector representations of the data based on
neighbourhoods. In this paper we report a method to evaluate pronunciation
using multilevel SOMs with /hVd/ single syllable utterances for the study of
vowels for Australian pronunciation.",Tom A. F. Anderson|David M. W. Powers,cs.CL|cs.NE|cs.SD
2017-02-28T17:24:36Z,2017-01-24T16:25:15Z,http://arxiv.org/abs/1701.06078v2,http://arxiv.org/pdf/1701.06078v2,"Lyrics-to-Audio Alignment by Unsupervised Discovery of Repetitive
  Patterns in Vowel Acoustics","Most of the previous approaches to lyrics-to-audio alignment used a
pre-developed automatic speech recognition (ASR) system that innately suffered
from several difficulties to adapt the speech model to individual singers. A
significant aspect missing in previous works is the self-learnability of
repetitive vowel patterns in the singing voice where the vowel part used is
more consistent than the consonant part. Based on this our system first learns
a discriminative subspace of vowel sequences based on weighted symmetric
non-negative matrix factorization (WS-NMF) by taking the self-similarity of a
standard acoustic feature as an input. Then we make use of canonical time
warping (CTW) derived from a recent computer vision technique to find an
optimal spatiotemporal transformation between the text and the acoustic
sequences. Experiments with Korean and English data sets showed that deploying
this method after a pre-developed unsupervised singing source separation
achieved more promising results than other state-of-the-art unsupervised
approaches and an existing ASR-based system.",Sungkyun Chang|Kyogu Lee,cs.SD|cs.LG
2017-02-28T17:24:36Z,2017-01-20T12:48:02Z,http://arxiv.org/abs/1701.05779v1,http://arxiv.org/pdf/1701.05779v1,"Empirical Study of Drone Sound Detection in Real-Life Environment with
  Deep Neural Networks","This work aims to investigate the use of deep neural network to detect
commercial hobby drones in real-life environments by analyzing their sound
data. The purpose of work is to contribute to a system for detecting drones
used for malicious purposes such as for terrorism. Specifically we present a
method capable of detecting the presence of commercial hobby drones as a binary
classification problem based on sound event detection. We recorded the sound
produced by a few popular commercial hobby drones and then augmented this data
with diverse environmental sound data to remedy the scarcity of drone sound
data in diverse environments. We investigated the effectiveness of
state-of-the-art event sound classification methods i.e. a Gaussian Mixture
Model (GMM) Convolutional Neural Network (CNN) and Recurrent Neural Network
(RNN) for drone sound detection. Our empirical results which were obtained
with a testing dataset collected on an urban street confirmed the
effectiveness of these models for operating in a real environment. In summary
our RNN models showed the best detection performance with an F-Score of 0.8009
with 240 ms of input audio with a short processing time indicating their
applicability to real-time detection systems.",Sungho Jeon|Jong-Woo Shin|Young-Jun Lee|Woong-Hee Kim|YoungHyoun Kwon|Hae-Yong Yang,cs.SD|cs.LG
2017-02-28T17:24:39Z,2017-01-12T09:26:22Z,http://arxiv.org/abs/1701.03274v1,http://arxiv.org/pdf/1701.03274v1,"Investigating the role of musical genre in human perception of music
  stretching resistance","To stretch a music piece to a given length is a common demand in people's
daily lives e.g. in audio-video synchronization and animation production.
However it is not always guaranteed that the stretched music piece is
acceptable for general audience since music stretching suffers from people's
perceptual artefacts. Over-stretching a music piece will make it uncomfortable
for human psychoacoustic hearing. The research on music stretching resistance
attempts to estimate the maximum stretchability of music pieces to further
avoid over-stretch. It has been observed that musical genres can significantly
improve the accuracy of automatic estimation of music stretching resistance
but how musical genres are related to music stretching resistance has never
been explained or studied in detail in the literature. In this paper the
characteristics of music stretching resistance are compared across different
musical genres. It is found that music stretching resistance has strong
intra-genre cohesiveness and inter-genre discrepancies in the experiments.
Moreover the ambiguity and the symmetry of music stretching resistance are
also observed in the experimental analysis. These findings lead to a new
measurement on the similarity between different musical genres based on their
music stretching resistance. In addition the analysis of variance (ANOVA) also
supports the findings in this paper by verifying the significance of musical
genre in shaping music stretching resistance.",Jun Chen|Chaokun Wang,cs.MM|cs.SD
2017-02-28T17:24:39Z,2017-01-12T01:02:22Z,http://arxiv.org/abs/1701.03198v1,http://arxiv.org/pdf/1701.03198v1,"Unsupervised Latent Behavior Manifold Learning from Acoustic Features:
  audio2behavior","Behavioral annotation using signal processing and machine learning is highly
dependent on training data and manual annotations of behavioral labels.
Previous studies have shown that speech information encodes significant
behavioral information and be used in a variety of automated behavior
recognition tasks. However extracting behavior information from speech is
still a difficult task due to the sparseness of training data coupled with the
complex high-dimensionality of speech and the complex and multiple
information streams it encodes. In this work we exploit the slow varying
properties of human behavior. We hypothesize that nearby segments of speech
share the same behavioral context and hence share a similar underlying
representation in a latent space. Specifically we propose a Deep Neural
Network (DNN) model to connect behavioral context and derive the behavioral
manifold in an unsupervised manner. We evaluate the proposed manifold in the
couples therapy domain and also provide examples from publicly available data
(e.g. stand-up comedy). We further investigate training within the couples'
therapy domain and from movie data. The results are extremely encouraging and
promise improved behavioral quantification in an unsupervised manner and
warrants further investigation in a range of applications.",Haoqi Li|Brian Baucom|Panayiotis Georgiou,cs.LG|cs.SD
2017-02-28T17:24:39Z,2017-01-10T20:03:37Z,http://arxiv.org/abs/1701.03360v1,http://arxiv.org/pdf/1701.03360v1,"Residual LSTM: Design of a Deep Recurrent Architecture for Distant
  Speech Recognition","In this paper a novel architecture for a deep recurrent neural network
residual LSTM is introduced. A plain LSTM has an internal memory cell that can
learn long term dependencies of sequential data. It also provides a temporal
shortcut path to avoid vanishing or exploding gradients in the temporal domain.
The proposed residual LSTM architecture provides an additional spatial shortcut
path from lower layers for efficient training of deep networks with multiple
LSTM layers. Compared with the previous work highway LSTM residual LSTM
reuses the output projection matrix and the output gate of LSTM to control the
spatial information flow instead of additional gate networks which effectively
reduces more than 10% of network parameters. An experiment for distant speech
recognition on the AMI SDM corpus indicates that the performance of plain and
highway LSTM networks degrades with increasing network depth. For example
10-layer plain and highway LSTM networks showed 13.7% and 6.2% increase in WER
over 3-layer baselines respectively. On the contrary 10-layer residual LSTM
networks provided the lowest WER 41.0% which corresponds to 3.3% and 2.8% WER
reduction over 3-layer plain and highway LSTM networks respectively. Training
with both the IHM and SDM corpora the residual LSTM architecture provided
larger gain from increasing depth: a 10-layer residual LSTM showed 3.0% WER
reduction over the corresponding 5-layer one.",Jaeyoung Kim|Mostafa El-Khamy|Jungwon Lee,cs.LG|cs.AI|cs.SD
2017-02-28T17:24:39Z,2017-01-09T15:10:38Z,http://arxiv.org/abs/1701.03834v1,http://arxiv.org/pdf/1701.03834v1,On Higher Order Positive Differential Energy Operator,"The higher order differential energy operator (DEO) denoted via
$\Upsilon_k(x)$ is an extension to the second order famous Teager-Kaiser
operator. The DEO helps measuring the higher order gauge of energy of a signal
which is useful for AM-FM demodulation. However the energy criterion defined
by the DEO is not compliant with the presumption of positivity of energy. In
this paper we introduce a higher order operator called Positive Differential
Energy Operator (PDEO). This operator which can be obtained using alternative
recursive relations resolves the energy sign problem. The simulations
demonstrate that the proposed operator can outperform DEOs in terms of Average
Signal to Error Ratio (ASER) in AM/FM demodulation.",Amirhossein Javaheri|Mohammad Bagher Shamsollahi,cs.SD
2017-02-28T17:24:39Z,2017-01-04T04:07:11Z,http://arxiv.org/abs/1701.00599v2,http://arxiv.org/pdf/1701.00599v2,AENet: Learning Deep Audio Features for Video Analysis,"We propose a new deep network for audio event recognition called AENet. In
contrast to speech sounds coming from audio events may be produced by a wide
variety of sources. Furthermore distinguishing them often requires analyzing
an extended time period due to the lack of clear sub-word units that are
present in speech. In order to incorporate this long-time frequency structure
of audio events we introduce a convolutional neural network (CNN) operating on
a large temporal input. In contrast to previous works this allows us to train
an audio event detection system end-to-end. The combination of our network
architecture and a novel data augmentation outperforms previous methods for
audio event detection by 16%. Furthermore we perform transfer learning and
show that our model learnt generic audio features similar to the way CNNs
learn generic features on vision tasks. In video analysis combining visual
features and traditional audio features such as MFCC typically only leads to
marginal improvements. Instead combining visual features with our AENet
features which can be computed efficiently on a GPU leads to significant
performance improvements on action recognition and video highlight detection.
In video highlight detection our audio features improve the performance by
more than 8% over visual features alone.",Naoya Takahashi|Michael Gygli|Luc Van Gool,cs.MM|cs.CV|cs.SD
2017-02-28T17:24:39Z,2017-01-09T17:35:17Z,http://arxiv.org/abs/1701.00495v2,http://arxiv.org/pdf/1701.00495v2,Vid2speech: Speech Reconstruction from Silent Video,"Speechreading is a notoriously difficult task for humans to perform. In this
paper we present an end-to-end model based on a convolutional neural network
(CNN) for generating an intelligible acoustic speech signal from silent video
frames of a speaking person. The proposed CNN generates sound features for each
frame based on its neighboring frames. Waveforms are then synthesized from the
learned speech features to produce intelligible speech. We show that by
leveraging the automatic feature learning capabilities of a CNN we can obtain
state-of-the-art word intelligibility on the GRID dataset and show promising
results for learning out-of-vocabulary (OOV) words.",Ariel Ephrat|Shmuel Peleg,cs.CV|cs.SD
2017-02-28T17:24:39Z,2016-12-30T08:46:05Z,http://arxiv.org/abs/1612.09150v2,http://arxiv.org/pdf/1612.09150v2,"Phase-incorporating Speech Enhancement Based on Complex-valued Gaussian
  Process Latent Variable Model","Traditional speech enhancement techniques modify the magnitude of a speech in
time-frequency domain and use the phase of a noisy speech to resynthesize a
time domain speech. This work proposes a complex-valued Gaussian process latent
variable model (CGPLVM) to enhance directly the complex-valued noisy spectrum
modifying not only the magnitude but also the phase. The main idea that
underlies the developed method is the modeling of short-time Fourier transform
(STFT) coefficients across the time frames of a speech as a proper complex
Gaussian process (GP) with noise added. The proposed method is based on
projecting the spectrum into a low-dimensional subspace. The likelihood
criterion is used to optimize the hyperparameters of the model. Experiments
were carried out on the CHTTL database which contains the digits zero to nine
in Mandarin. Several standard measures are used to demonstrate that the
proposed method outperforms baseline methods.",Sih-Huei Chen|Yuan-Shan Lee|Jia-Ching Wang,cs.SD
2017-02-28T17:24:39Z,2016-12-30T09:26:26Z,http://arxiv.org/abs/1612.09089v2,http://arxiv.org/pdf/1612.09089v2,What Makes Audio Event Detection Harder than Classification?,"There is a common observation that audio event classification is easier to
deal with than detection. So far this observation has been accepted as a fact
and we lack a careful analysis. In this paper we reason the rationale behind
this fact and more importantly leverage them to benefit the audio event
detection task. We present an improved detection pipeline in which a
verification step is appended to augment a detection system. This step employs
a high-quality event classifier to postprocess the benign event hypotheses
outputted by the detection system and reject false alarms. To demonstrate the
effectiveness of the proposed pipeline we implement and pair up different
event detectors based on the most common detection schemes and various event
classifiers ranging from the standard bag-of-words model to the
state-of-the-art bank-of-regressors one. Experimental results on the ITC-Irst
dataset show significant improvements to detection performance. More
importantly these improvements are consistent for all detector-classifier
combinations.",Huy Phan|Philipp Koch|Marco Maass|Radoslaw Mazur|Ian McLoughlin|Alfred Mertins,cs.SD
2017-02-28T17:24:39Z,2016-12-27T20:27:24Z,http://arxiv.org/abs/1612.08727v1,http://arxiv.org/pdf/1612.08727v1,"Creating A Musical Performance Dataset for Multimodal Music Analysis:
  Challenges Insights and Applications","We introduce a dataset for facilitating audio-visual analysis of musical
performances. The dataset comprises a number of simple multi-instrument musical
pieces assembled from coordinated but separately recorded performances of
individual tracks. For each piece we provide the musical score in MIDI format
the audio recordings of the individual tracks the audio and video recording of
the assembled mixture and ground-truth annotation files including frame-level
and note-level transcriptions. We anticipate that the dataset will be useful
for developing and evaluating multi-modal techniques for music source
separation transcription score following and performance analysis. We
describe our methodology for the creation of this dataset particularly
highlighting our approaches for addressing the challenges involved in
maintaining synchronization and naturalness. We briefly discuss the research
questions that can be investigated with this dataset.",Bochen Li|Xinzhao Liu|Karthik Dinesh|Zhiyao Duan|Gaurav Sharma,cs.MM|cs.SD
2017-02-28T17:24:39Z,2017-02-11T20:04:46Z,http://arxiv.org/abs/1612.07837v2,http://arxiv.org/pdf/1612.07837v2,SampleRNN: An Unconditional End-to-End Neural Audio Generation Model,"In this paper we propose a novel model for unconditional audio generation
based on generating one audio sample at a time. We show that our model which
profits from combining memory-less modules namely autoregressive multilayer
perceptrons and stateful recurrent neural networks in a hierarchical structure
is able to capture underlying sources of variations in the temporal sequences
over very long time spans on three datasets of different nature. Human
evaluation on the generated samples indicate that our model is preferred over
competing models. We also show how each component of the model contributes to
the exhibited performance.",Soroush Mehri|Kundan Kumar|Ishaan Gulrajani|Rithesh Kumar|Shubham Jain|Jose Sotelo|Aaron Courville|Yoshua Bengio,cs.SD|cs.AI
2017-02-28T17:24:43Z,2016-12-22T10:14:59Z,http://arxiv.org/abs/1612.07523v1,http://arxiv.org/pdf/1612.07523v1,Robustness of Voice Conversion Techniques Under Mismatched Conditions,"Most of the existing studies on voice conversion (VC) are conducted in
acoustically matched conditions between source and target signal. However the
robustness of VC methods in presence of mismatch remains unknown. In this
paper we report a comparative analysis of different VC techniques under
mismatched conditions. The extensive experiments with five different VC
techniques on CMU ARCTIC corpus suggest that performance of VC methods
substantially degrades in noisy conditions. We have found that bilinear
frequency warping with amplitude scaling (BLFWAS) outperforms other methods in
most of the noisy conditions. We further explore the suitability of different
speech enhancement techniques for robust conversion. The objective evaluation
results indicate that spectral subtraction and log minimum mean square error
(logMMSE) based speech enhancement techniques can be used to improve the
performance in specific noisy conditions.",Monisankha Pal|Dipjyoti Paul|Md Sahidullah|Goutam Saha,cs.SD|cs.LG|stat.ML
2017-02-28T17:24:43Z,2016-12-21T00:02:08Z,http://arxiv.org/abs/1612.07608v1,http://arxiv.org/pdf/1612.07608v1,"EchoWear: Smartwatch Technology for Voice and Speech Treatments of
  Patients with Parkinson's Disease","About 90 percent of people with Parkinson's disease (PD) experience decreased
functional communication due to the presence of voice and speech disorders
associated with dysarthria that can be characterized by monotony of pitch (or
fundamental frequency) reduced loudness irregular rate of speech imprecise
consonants and changes in voice quality. Speech-language pathologists (SLPs)
work with patients with PD to improve speech intelligibility using various
intensive in-clinic speech treatments. SLPs also prescribe home exercises to
enhance generalization of speech strategies outside of the treatment room. Even
though speech therapies are found to be highly effective in improving vocal
loudness and speech quality patients with PD find it difficult to follow the
prescribed exercise regimes outside the clinic and to continue exercises once
the treatment is completed. SLPs need techniques to monitor compliance and
accuracy of their patients exercises at home and in ecologically valid
communication situations. We have designed EchoWear a smartwatch-based system
to remotely monitor speech and voice exercises as prescribed by SLPs. We
conducted a study of 6 individuals; three with PD and three healthy controls.
To assess the performance of EchoWear technology compared with high quality
audio equipment obtained in a speech laboratory. Our preliminary analysis shows
promising outcomes for using EchoWear in speech therapies for people with PD.
  Keywords: Dysarthria; knowledge-based speech processing; Parkinson's disease;
smartwatch; speech therapy; wearable system.",Harishchandra Dubey|Jon C. Goldberg|Mohammadreza Abtahi|Leslie Mahler|Kunal Mankodiya,cs.CY|cs.SD
2017-02-28T17:24:43Z,2016-12-20T13:04:33Z,http://arxiv.org/abs/1612.06642v1,http://arxiv.org/pdf/1612.06642v1,Efficient Target Activity Detection based on Recurrent Neural Networks,"This paper addresses the problem of Target Activity Detection (TAD) for
binaural listening devices. TAD denotes the problem of robustly detecting the
activity of a target speaker in a harsh acoustic environment which comprises
interfering speakers and noise (cocktail party scenario). In previous work it
has been shown that employing a Feed-forward Neural Network (FNN) for detecting
the target speaker activity is a promising approach to combine the advantage of
different TAD features (used as network inputs). In this contribution we
exploit a larger context window for TAD and compare the performance of FNNs and
Recurrent Neural Networks (RNNs) with an explicit focus on small network
topologies as desirable for embedded acoustic signal processing systems. More
specifically the investigations include a comparison between three different
types of RNNs namely plain RNNs Long Short-Term Memories and Gated Recurrent
Units. The results indicate that all versions of RNNs outperform FNNs for the
task of TAD.",Daniel Gerber|Stefan Meier|Walter Kellermann,cs.SD
2017-02-28T17:24:43Z,2016-12-19T12:19:25Z,http://arxiv.org/abs/1612.06151v1,http://arxiv.org/pdf/1612.06151v1,"HRTF-based two-dimensional robust least-squares frequency-invariant
  beamformer design for robot audition","In this work we propose a two-dimensional Head-Related Transfer Function
(HRTF)-based robust beamformer design for robot audition which allows for
explicit control of the beamformer response for the entire three-dimensional
sound field surrounding a humanoid robot. We evaluate the proposed method by
means of both signal-independent and signal-dependent measures in a robot
audition scenario. Our results confirm the effectiveness of the proposed
two-dimensional HRTF-based beamformer design compared to our previously
published one-dimensional HRTF-based beamformer design which was carried out
for a fixed elevation angle only.",Hendrik Barfuss|Michael Buerger|Jasper Podschus|Walter Kellermann,cs.SD
2017-02-28T17:24:43Z,2016-12-16T14:40:43Z,http://arxiv.org/abs/1612.05489v1,http://arxiv.org/pdf/1612.05489v1,"On-bird Sound Recordings: Automatic Acoustic Recognition of Activities
  and Contexts","We introduce a novel approach to studying animal behaviour and the context in
which it occurs through the use of microphone backpacks carried on the backs
of individual free-flying birds. These sensors are increasingly used by animal
behaviour researchers to study individual vocalisations of freely behaving
animals even in the field. However such devices may record more than an
animals vocal behaviour and have the potential to be used for investigating
specific activities (movement) and context (background) within which
vocalisations occur. To facilitate this approach we investigate the automatic
annotation of such recordings through two different sound scene analysis
paradigms: a scene-classification method using feature learning and an
event-detection method using probabilistic latent component analysis (PLCA). We
analyse recordings made with Eurasian jackdaws (Corvus monedula) in both
captive and field settings. Results are comparable with the state of the art in
sound scene analysis; we find that the current recognition quality level
enables scalable automatic annotation of audio logger data given partial
annotation but also find that individual differences between animals and/or
their backpacks limit the generalisation from one individual to another. we
consider the interrelation of 'scenes' and 'events' in this particular task
and issues of temporal resolution.",Dan Stowell|Emmanouil Benetos|Lisa F. Gill,cs.SD
2017-02-28T17:24:43Z,2016-12-16T11:05:52Z,http://arxiv.org/abs/1612.05432v1,http://arxiv.org/pdf/1612.05432v1,Basis-Function Modeling of Loudness Variations in Ensemble Performance,"This paper describes a computational model of loudness variations in
expressive ensemble performance. The model predicts and explains the continuous
variation of loudness as a function of information extracted automatically from
the written score. Although such models have been proposed for expressive
performance in solo instruments this is (to the best of our knowledge) the
first attempt to define a model for expressive performance in ensembles. To
that end we extend an existing model that was designed to model expressive
piano performances and describe the additional steps necessary for the model
to deal with scores of arbitrary instrumentation including orchestral scores.
We test both linear and non-linear variants of the extended model n a data set
of audio recordings of symphonic music in a leave-one-out setting. The
experiments reveal that the most successful model variant is a recurrent
non-linear model. Even if the accuracy of the predicted loudness varies from
one recording to another in several cases the model explains well over 50% of
the variance in loudness.",Thassilo Gadermaier|Maarten Grachten|Carlos Eduardo Cancino Chacón,cs.SD
2017-02-28T17:24:43Z,2016-12-16T05:09:14Z,http://arxiv.org/abs/1612.05369v1,http://arxiv.org/pdf/1612.05369v1,Neural networks based EEG-Speech Models,"In this paper we describe three neural network (NN) based EEG-Speech (NES)
models that map the unspoken EEG signals to the corresponding phonemes. Instead
of using conventional feature extraction techniques the proposed NES models
rely on graphic learning to project both EEG and speech signals into deep
representation feature spaces. This NN based linear projection helps to realize
multimodal data fusion (i.e. EEG and acoustic signals). It is convenient to
construct the mapping between unspoken EEG signals and phonemes. Specifically
among three NES models two augmented models (i.e. IANES-B and IANES-G)
include spoken EEG signals as either bias or gate information to strengthen the
feature learning and translation of unspoken EEG signals. A combined
unsupervised and supervised training is implemented stepwise to learn the
mapping for all three NES models. To enhance the computational performance
three way factored NN training technique is applied to IANES-G model. Unlike
many existing methods our augmented NES models incorporate spoken-EEG signals
that can efficiently suppress the artifacts in unspoken-EEG signals.
Experimental results reveal that all three proposed NES models outperform the
baseline SVM method whereas IANES-G demonstrates the best performance on
speech recovery and classification task comparatively.",Pengfei Sun|Jun Qin,cs.SD|cs.LG
2017-02-28T17:24:43Z,2016-12-15T19:43:54Z,http://arxiv.org/abs/1612.05156v1,http://arxiv.org/pdf/1612.05156v1,A Phase Vocoder based on Nonstationary Gabor Frames,"We propose a new algorithm for time stretching music signals based on the
theory of nonstationary Gabor frames. The algorithm extends the techniques of
the classical phase vocoder by incorporating adaptive time-frequency
representations and adaptive phase locking. Applying a preliminary onset
detection algorithm the obtained time-frequency representation implies good
time resolution for the onsets and good frequency resolution for the sinusoidal
components.
  The phase estimates are done only at peak channels using quadratic
interpolation and the remaining phases are then locked to the values of the
peaks in an adaptive manner. In contrast to previous attempts we let the number
of frequency channels vary over time in order to obtain a low redundancy of the
corresponding transform. We show that with a redundancy comparable to that of
the phase vocoder we can greatly reduce artefacts such as phasiness and
transient smearing. The algorithm is tested on both synthetic and real world
signals and compared with state of the art algorithms in a reproducible manner.",Emil Solsbæk Ottosen|Monika Dörfler,cs.SD
2017-02-28T17:24:43Z,2016-12-15T17:59:05Z,http://arxiv.org/abs/1612.05168v1,http://arxiv.org/pdf/1612.05168v1,LIA system description for NIST SRE 2016,"This paper describes the LIA speaker recognition system developed for the
Speaker Recognition Evaluation (SRE) campaign. Eight sub-systems are developed
all based on a state-of-the-art approach: i-vector/PLDA which represents the
mainstream technique in text-independent speaker recognition. These sub-systems
differ: on the acoustic feature extraction front-end (MFCC PLP) at the
i-vector extraction stage (UBM DNN or two-feats posteriors) and finally on the
data-shifting (IDVC mean-shifting). The submitted system is a fusion at the
score-level of these eight sub-systems.",Mickael Rouvier|Pierre-Michel Bousquet|Moez Ajili|Waad Ben Kheder|Driss Matrouf|Jean-François Bonastre,cs.SD
2017-02-28T17:24:43Z,2016-12-15T17:32:11Z,http://arxiv.org/abs/1612.05153v1,http://arxiv.org/pdf/1612.05153v1,On the Potential of Simple Framewise Approaches to Piano Transcription,"In an attempt at exploring the limitations of simple approaches to the task
of piano transcription (as usually defined in MIR) we conduct an in-depth
analysis of neural network-based framewise transcription. We systematically
compare different popular input representations for transcription systems to
determine the ones most suitable for use with neural networks. Exploiting
recent advances in training techniques and new regularizers and taking into
account hyper-parameter tuning we show that it is possible by simple
bottom-up frame-wise processing to obtain a piano transcriber that outperforms
the current published state of the art on the publicly available MAPS dataset
-- without any complex post-processing steps. Thus we propose this simple
approach as a new baseline for this dataset for future transcription research
to build on and improve.",Rainer Kelz|Matthias Dorfer|Filip Korzeniowski|Sebastian Böck|Andreas Arzt|Gerhard Widmer,cs.SD|cs.LG
2017-02-28T17:24:47Z,2016-12-15T14:32:20Z,http://arxiv.org/abs/1612.05082v1,http://arxiv.org/abs/1612.05082v1,A Fully Convolutional Deep Auditory Model for Musical Chord Recognition,"Chord recognition systems depend on robust feature extraction pipelines.
While these pipelines are traditionally hand-crafted recent advances in
end-to-end machine learning have begun to inspire researchers to explore
data-driven methods for such tasks. In this paper we present a chord
recognition system that uses a fully convolutional deep auditory model for
feature extraction. The extracted features are processed by a Conditional
Random Field that decodes the final chord sequence. Both processing stages are
trained automatically and do not require expert knowledge for optimising
parameters. We show that the learned auditory system extracts musically
interpretable features and that the proposed chord recognition system achieves
results on par or better than state-of-the-art algorithms.",Filip Korzeniowski|Gerhard Widmer,cs.LG|cs.SD
2017-02-28T17:24:47Z,2016-12-15T14:16:56Z,http://arxiv.org/abs/1612.05076v1,http://arxiv.org/pdf/1612.05076v1,Live Score Following on Sheet Music Images,"In this demo we show a novel approach to score following. Instead of relying
on some symbolic representation we are using a multi-modal convolutional
neural network to match the incoming audio stream directly to sheet music
images. This approach is in an early stage and should be seen as proof of
concept. Nonetheless the audience will have the opportunity to test our
implementation themselves via 3 simple piano pieces.",Matthias Dorfer|Andreas Arzt|Sebastian Böck|Amaury Durand|Gerhard Widmer,cs.SD
2017-02-28T17:24:47Z,2016-12-15T14:07:51Z,http://arxiv.org/abs/1612.05070v1,http://arxiv.org/pdf/1612.05070v1,Towards End-to-End Audio-Sheet-Music Retrieval,"This paper demonstrates the feasibility of learning to retrieve short
snippets of sheet music (images) when given a short query excerpt of music
(audio) -- and vice versa -- without any symbolic representation of music or
scores. This would be highly useful in many content-based musical retrieval
scenarios. Our approach is based on Deep Canonical Correlation Analysis (DCCA)
and learns correlated latent spaces allowing for cross-modality retrieval in
both directions. Initial experiments with relatively simple monophonic music
show promising results.",Matthias Dorfer|Andreas Arzt|Gerhard Widmer,cs.SD|cs.IR|cs.LG
2017-02-28T17:24:47Z,2016-12-15T14:01:50Z,http://arxiv.org/abs/1612.05065v1,http://arxiv.org/pdf/1612.05065v1,Feature Learning for Chord Recognition: The Deep Chroma Extractor,"We explore frame-level audio feature learning for chord recognition using
artificial neural networks. We present the argument that chroma vectors
potentially hold enough information to model harmonic content of audio for
chord recognition but that standard chroma extractors compute too noisy
features. This leads us to propose a learned chroma feature extractor based on
artificial neural networks. It is trained to compute chroma features that
encode harmonic information important for chord recognition while being robust
to irrelevant interferences. We achieve this by feeding the network an audio
spectrum with context instead of a single frame as input. This way the network
can learn to selectively compensate noise and resolve harmonic ambiguities.
  We compare the resulting features to hand-crafted ones by using a simple
linear frame-wise classifier for chord recognition on various data sets. The
results show that the learned feature extractor produces superior chroma
vectors for chord recognition.",Filip Korzeniowski|Gerhard Widmer,cs.SD|cs.LG
2017-02-28T17:24:47Z,2016-12-15T05:06:40Z,http://arxiv.org/abs/1612.04928v1,http://arxiv.org/pdf/1612.04928v1,Music Generation with Deep Learning,"The use of deep learning to solve problems in literary arts has been a recent
trend that has gained a lot of attention and automated generation of music has
been an active area. This project deals with the generation of music using raw
audio files in the frequency domain relying on various LSTM architectures.
Fully connected and convolutional layers are used along with LSTM's to capture
rich features in the frequency domain and increase the quality of music
generated. The work is focused on unconstrained music generation and uses no
information about musical structure(notes or chords) to aid learning.The music
generated from various architectures are compared using blind fold tests. Using
the raw audio to train models is the direction to tapping the enormous amount
of mp3 files that exist over the internet without requiring the manual effort
to make structured MIDI files. Moreover not all audio files can be represented
with MIDI files making the study of these models an interesting prospect to the
future of such models.",Vasanth Kalingeri|Srikanth Grandhe,cs.SD
2017-02-28T17:24:47Z,2016-12-15T04:22:39Z,http://arxiv.org/abs/1612.04919v1,http://arxiv.org/pdf/1612.04919v1,"Combination of Linear Prediction and Phase Decomposition for Glottal
  Source Analysis on Voiced Speech","Some glottal analysis approaches based upon linear prediction or complex
cepstrum approaches have been proved to be effective to estimate glottal source
from real speech utterances. We propose a new approach employing both an
all-pole odd-order linear prediction to provide a coarse estimation and phase
decomposition based causality/anti-causality separation to generate further
refinements. The obtained measures show that this method improved performance
in terms of reducing source-filter separation in estimation of glottal flow
pulses (GFP). No glottal model fitting is required by this method thus it has
wide and flexible adaptation to retain fidelity of speakers's vocal features
with computationally affordable resource. The method is evaluated on real
speech utterances to validate it.",Yiqiao Chen|John N. Gowdy,cs.SD
2017-02-28T17:24:47Z,2016-12-14T17:40:02Z,http://arxiv.org/abs/1612.04744v1,http://arxiv.org/pdf/1612.04744v1,Incorporating Language Level Information into Acoustic Models,"This paper proposed a class of novel Deep Recurrent Neural Networks which can
incorporate language-level information into acoustic models. For simplicity we
named these networks Recurrent Deep Language Networks (RDLNs). Multiple
variants of RDLNs were considered including two kinds of context information
two methods to process the context and two methods to incorporate the
language-level information. RDLNs provided possible methods to fine-tune the
whole Automatic Speech Recognition (ASR) system in the acoustic modeling
process.",Peidong Wang|Deliang Wang,cs.CL|cs.LG|cs.SD
2017-02-28T17:24:47Z,2016-12-15T01:48:52Z,http://arxiv.org/abs/1612.04742v2,http://arxiv.org/pdf/1612.04742v2,"Imposing higher-level Structure in Polyphonic Music Generation using
  Convolutional Restricted Boltzmann Machines and Constraints","We introduce a method for imposing higher-level structure on generated
polyphonic music. A Convolutional Restricted Boltzmann Machine (C-RBM) as a
generative model is combined with gradient descent constraint optimization to
provide further control over the generation process. Among other things this
allows for the use of a ""template"" piece from which some structural properties
can be extracted and transferred as constraints to newly generated material.
The sampling process is guided with Simulated Annealing in order to avoid local
optima and find solutions that both satisfy the constraints and are
relatively stable with respect to the C-RBM. Results show that with this
approach it is possible to control the higher level self-similarity structure
the meter as well as tonal properties of the resulting musical piece while
preserving its local musical coherence.",Stefan Lattner|Maarten Grachten|Gerhard Widmer,cs.SD|cs.AI|cs.NE
2017-02-28T17:24:47Z,2016-12-14T15:40:44Z,http://arxiv.org/abs/1612.06287v1,http://arxiv.org/pdf/1612.06287v1,VAST : The Virtual Acoustic Space Traveler Dataset,"This paper introduces a new paradigm for sound source lo-calization referred
to as virtual acoustic space traveling (VAST) and presents a first dataset
designed for this purpose. Existing sound source localization methods are
either based on an approximate physical model (physics-driven) or on a
specific-purpose calibration set (data-driven). With VAST the idea is to learn
a mapping from audio features to desired audio properties using a massive
dataset of simulated room impulse responses. This virtual dataset is designed
to be maximally representative of the potential audio scenes that the
considered system may be evolving in while remaining reasonably compact. We
show that virtually-learned mappings on this dataset generalize to real data
overcoming some intrinsic limitations of traditional binaural sound
localization methods based on time differences of arrival.",Clément Gaultier|Saurabh Kataria|Antoine Deleforge,cs.SD|cs.LG
2017-02-28T17:24:47Z,2016-12-14T15:07:51Z,http://arxiv.org/abs/1612.04675v1,http://arxiv.org/pdf/1612.04675v1,Recurrent Deep Stacking Networks for Speech Recognition,"This paper presented our work on applying Recurrent Deep Stacking Networks
(RDSNs) to Robust Automatic Speech Recognition (ASR) tasks. In the paper we
also proposed a more efficient yet comparable substitute to RDSN Bi- Pass
Stacking Network (BPSN). The main idea of these two models is to add
phoneme-level information into acoustic models transforming an acoustic model
to the combination of an acoustic model and a phoneme-level N-gram model.
Experiments showed that RDSN and BPsn can substantially improve the
performances over conventional DNNs.",Peidong Wang|Zhongqiu Wang|Deliang Wang,cs.CL|cs.SD
2017-02-28T17:24:51Z,2017-01-19T15:33:53Z,http://arxiv.org/abs/1612.04056v2,http://arxiv.org/pdf/1612.04056v2,Joint Bayesian Gaussian discriminant analysis for speaker verification,"State-of-the-art i-vector based speaker verification relies on variants of
Probabilistic Linear Discriminant Analysis (PLDA) for discriminant analysis. We
are mainly motivated by the recent work of the joint Bayesian (JB) method
which is originally proposed for discriminant analysis in face verification. We
apply JB to speaker verification and make three contributions beyond the
original JB. 1) In contrast to the EM iterations with approximated statistics
in the original JB the EM iterations with exact statistics are employed and
give better performance. 2) We propose to do simultaneous diagonalization (SD)
of the within-class and between-class covariance matrices to achieve efficient
testing which has broader application scope than the SVD-based efficient
testing method in the original JB. 3) We scrutinize similarities and
differences between various Gaussian PLDAs and JB complementing the previous
analysis of comparing JB only with Prince-Elder PLDA. Extensive experiments are
conducted on NIST SRE10 core condition 5 empirically validating the
superiority of JB with faster convergence rate and 9-13% EER reduction compared
with state-of-the-art PLDA.",Yiyan Wang|Haotian Xu|Zhijian Ou,cs.SD|cs.LG
2017-02-28T17:24:51Z,2016-12-16T14:25:44Z,http://arxiv.org/abs/1612.04028v2,http://arxiv.org/abs/1612.04028v2,Adaptive DCTNet for Audio Signal Classification,"In this paper we investigate DCTNet for audio signal classification. Its
output feature is related to Cohen's class of time-frequency distributions. We
introduce the use of adaptive DCTNet (A-DCTNet) for audio signals feature
extraction. The A-DCTNet applies the idea of constant-Q transform with its
center frequencies of filterbanks geometrically spaced. The A-DCTNet is
adaptive to different acoustic scales and it can better capture low frequency
acoustic information that is sensitive to human audio perception than features
such as Mel-frequency spectral coefficients (MFSC). We use features extracted
by the A-DCTNet as input for classifiers. Experimental results show that the
A-DCTNet and Recurrent Neural Networks (RNN) achieve state-of-the-art
performance in bird song classification rate and improve artist identification
accuracy in music data. They demonstrate A-DCTNet's applicability to signal
processing problems.",Yin Xian|Yunchen Pu|Zhe Gan|Liang Lu|Andrew Thompson,cs.SD
2017-02-28T17:24:51Z,2016-12-12T17:06:19Z,http://arxiv.org/abs/1612.03789v1,http://arxiv.org/pdf/1612.03789v1,"A Unit Selection Methodology for Music Generation Using Deep Neural
  Networks","Several methods exist for a computer to generate music based on data
including Markov chains recurrent neural networks recombinancy and grammars.
We explore the use of unit selection and concatenation as a means of generating
music using a procedure based on ranking where we consider a unit to be a
variable length number of measures of music. We first examine whether a unit
selection method that is restricted to a finite size unit library can be
sufficient for encompassing a wide spectrum of music. We do this by developing
a deep autoencoder that encodes a musical input and reconstructs the input by
selecting from the library. We then describe a generative model that combines a
deep structured semantic model (DSSM) with an LSTM to predict the next unit
where units consist of four two and one measures of music. We evaluate the
generative model using objective metrics including mean rank and accuracy and
with a subjective listening test in which expert musicians are asked to
complete a forced-choiced ranking task. We compare our model to a note-level
generative baseline that consists of a stacked LSTM trained to predict forward
by one note.",Mason Bretan|Gil Weinberg|Larry Heck,cs.SD|cs.AI|cs.LG
2017-02-28T17:24:51Z,2016-12-12T00:13:35Z,http://arxiv.org/abs/1612.03505v1,http://arxiv.org/pdf/1612.03505v1,"Convolutional Neural Networks for Passive Monitoring of a Shallow Water
  Environment using a Single Sensor","A cost effective approach to remote monitoring of protected areas such as
marine reserves and restricted naval waters is to use passive sonar to detect
classify localize and track marine vessel activity (including small boats and
autonomous underwater vehicles). Cepstral analysis of underwater acoustic data
enables the time delay between the direct path arrival and the first multipath
arrival to be measured which in turn enables estimation of the instantaneous
range of the source (a small boat). However this conventional method is
limited to ranges where the Lloyd's mirror effect (interference pattern formed
between the direct and first multipath arrivals) is discernible. This paper
proposes the use of convolutional neural networks (CNNs) for the joint
detection and ranging of broadband acoustic noise sources such as marine
vessels in conjunction with a data augmentation approach for improving network
performance in varied signal-to-noise ratio (SNR) situations. Performance is
compared with a conventional passive sonar ranging method for monitoring marine
vessel activity using real data from a single hydrophone mounted above the sea
floor. It is shown that CNNs operating on cepstrum data are able to detect the
presence and estimate the range of transiting vessels at greater distances than
the conventional method.",Eric L. Ferguson|Rishi Ramakrishnan|Stefan B. Williams|Craig T. Jin,cs.SD
2017-02-28T17:24:51Z,2016-12-28T14:47:27Z,http://arxiv.org/abs/1612.02350v2,http://arxiv.org/pdf/1612.02350v2,"An Information-theoretic Approach to Machine-oriented Music
  Summarization","Applying generic media-agnostic summarization to music allows for higher
efficiency in automatic processing storage and communication of datasets
while also alleviating copyright issues. This process has already been proven
useful in the context of music genre classification. In this paper we
generalize conclusions from previous work by evaluating the impact of generic
summarization in music from a probabilistic perspective and agnostic relative
to certain tasks. We estimate Gaussian distributions for original and
summarized songs and compute their relative entropy to measure how much
information is lost in the summarization process. Based on this observation we
further propose a simple yet expressive summarization method that objectively
outperforms previous methods and is better suited to avoid copyright issues. We
present results suggesting that relative entropy is a good predictor of
summarization performance in the context of tasks relying on a bag-of-features
model.",Francisco Raposo|David Martins de Matos|Ricardo Ribeiro,cs.IR|cs.LG|cs.SD|H.5.5
2017-02-28T17:24:51Z,2016-12-13T11:14:30Z,http://arxiv.org/abs/1612.02198v2,http://arxiv.org/pdf/1612.02198v2,Towards computer-assisted understanding of dynamics in symphonic music,"Many people enjoy classical symphonic music. Its diverse instrumentation
makes for a rich listening experience. This diversity adds to the conductor's
expressive freedom to shape the sound according to their imagination. As a
result the same piece may sound quite differently from one conductor to
another. Differences in interpretation may be noticeable subjectively to
listeners but they are sometimes hard to pinpoint presumably because of the
acoustic complexity of the sound. We describe a computational model that
interprets dynamics---expressive loudness variations in performances---in terms
of the musical score highlighting differences between performances of the same
piece. We demonstrate experimentally that the model has predictive power and
give examples of conductor ideosyncrasies found by using the model as an
explanatory tool. Although the present model is still in active development it
may pave the road for a consumer-oriented companion to interactive classical
music understanding.",Maarten Grachten|Carlos Eduardo Cancino-Chacón|Thassilo Gadermaier|Gerhard Widmer,cs.SD|cs.MM
2017-02-28T17:24:51Z,2016-12-06T18:37:30Z,http://arxiv.org/abs/1612.01943v1,http://arxiv.org/pdf/1612.01943v1,"Segmental Convolutional Neural Networks for Detection of Cardiac
  Abnormality With Noisy Heart Sound Recordings","Heart diseases constitute a global health burden and the problem is
exacerbated by the error-prone nature of listening to and interpreting heart
sounds. This motivates the development of automated classification to screen
for abnormal heart sounds. Existing machine learning-based systems achieve
accurate classification of heart sound recordings but rely on expert features
that have not been thoroughly evaluated on noisy recordings. Here we propose a
segmental convolutional neural network architecture that achieves automatic
feature learning from noisy heart sound recordings. Our experiments show that
our best model trained on noisy recording segments acquired with an existing
hidden semi-markov model-based approach attains a classification accuracy of
87.5% on the 2016 PhysioNet/CinC Challenge dataset compared to the 84.6%
accuracy of the state-of-the-art statistical classifier trained and evaluated
on the same dataset. Our results indicate the potential of using neural
network-based methods to increase the accuracy of automated classification of
heart sound recordings for improved screening of heart diseases.",Yuhao Zhang|Sandeep Ayyar|Long-Huei Chen|Ethan J. Li,cs.SD|cs.LG|stat.ML
2017-02-28T17:24:51Z,2016-12-06T14:58:59Z,http://arxiv.org/abs/1612.01840v1,http://arxiv.org/pdf/1612.01840v1,FMA: A Dataset For Music Analysis,"We present a new music dataset that can be used for several music analysis
tasks. Our major goal is to go beyond the existing limitations of available
music datasets which are either the small size of datasets with raw audio
tracks the availability and legality of the music data or the lack of
meta-data for artists analysis or song ratings for recommender systems.
Existing datasets such as GTZAN TagATune and Million Song suffer from the
previous limitations. It is however essential to establish such benchmark
datasets to advance the field of music analysis like the ImageNet dataset
which made possible the large success of deep learning techniques in computer
vision. In this paper we introduce the Free Music Archive (FMA) which contains
77643 songs and 68 genres spanning 26.9 days of song listening and meta-data
including artist name song title music genre and track counts. For research
purposes we define two additional datasets from the original one: a small
genre-balanced dataset of 4000 song data and 10 genres compassing 33.3 hours
of raw audio and a medium genre-unbalanced dataset of 14511 data and 20 genres
offering 5.1 days of track listening both datasets come with meta-data and
Echonest audio features. For all datasets we provide a train-test splitting
for future algorithms' comparisons.",Kirell Benzi|Michaël Defferrard|Pierre Vandergheynst|Xavier Bresson,cs.SD|cs.IR
2017-02-28T17:24:51Z,2017-01-31T17:33:34Z,http://arxiv.org/abs/1612.01860v4,http://arxiv.org/pdf/1612.01860v4,"An algorithm to assign musical prime commas to every prime number and
  construct a universal and compact free Just Intonation musical notation","Musical frequencies in Just Intonation are comprised of rational numbers. The
structure of rational numbers is determined by prime factorisations. Just
Intonation frequencies can be split into two components. The larger component
uses only integer powers of the first two primes 2 and 3. The smaller
component decomposes into a series of microtonal adjustments one for each
prime number 5 and above present in the original frequency. The larger 3-limit
component can be notated using scientific pitch notation modified to use
Pythagorean tuning. The microtonal adjustments can be notated using rational
commas which are built up from prime commas. This gives a notation system for
the whole of free-JI called Rational Comma Notation. RCN is compact since all
microtonal adjustments can be represented by a single notational unit based on
a rational number. RCN has different versions depending on the choice of
algorithm to assign a prime comma to each prime number. Two existing algorithms
SAG and KG are found in the literature. A novel algorithm DR is developed based
on discussion of mathematical and musical criteria for algorithm design.
Results for DR are presented for primes below 1400. Some observations are made
about these results and their applications including shorthand notation and
pitch class lattices. Results for DR are compared with those for SAG and KG.
Translation is possible between any two free-JI notations and any two versions
of RCN since they all represent the same underlying set of rational numbers.",David Ryan,cs.SD
2017-02-28T17:24:51Z,2016-12-04T03:36:51Z,http://arxiv.org/abs/1612.01058v1,http://arxiv.org/pdf/1612.01058v1,Algorithmic Songwriting with ALYSIA,"This paper introduces ALYSIA: Automated LYrical SongwrIting Application.
ALYSIA is based on a machine learning model using Random Forests and we
discuss its success at pitch and rhythm prediction. Next we show how ALYSIA
was used to create original pop songs that were subsequently recorded and
produced. Finally we discuss our vision for the future of Automated
Songwriting for both co-creative and autonomous systems.",Margareta Ackerman|David Loker,cs.AI|cs.LG|cs.MM|cs.SD
2017-02-28T17:24:55Z,2016-12-03T19:17:29Z,http://arxiv.org/abs/1612.01010v1,http://arxiv.org/pdf/1612.01010v1,DeepBach: a Steerable Model for Bach chorales generation,"The composition of polyphonic chorale music in the style of J.S Bach has
represented a major challenge in automatic music composition over the last
decades. The art of Bach chorales composition involves combining four-part
harmony with characteristic rhythmic patterns and typical melodic movements to
produce musical phrases which begin evolve and end (cadences) in a harmonious
way. To our knowledge no model so far was able to solve all these problems
simultaneously using an agnostic machine-learning approach. This paper
introduces DeepBach a statistical model aimed at modeling polyphonic music and
specifically four parts hymn-like pieces. We claim that after being trained
on the chorale harmonizations by Johann Sebastian Bach our model is capable of
generating highly convincing chorales in the style of Bach. We evaluate how
indistinguishable our generated chorales are from existing Bach chorales with a
listening test. The results corroborate our claim. A key strength of DeepBach
is that it is agnostic and flexible. Users can constrain the generation by
imposing some notes rhythms or cadences in the generated score. This allows
users to reharmonize user-defined melodies. DeepBach's generation is fast
making it usable for interactive music composition applications. Several
generation examples are provided and discussed from a musical point of view.",Gaëtan Hadjeres|François Pachet,cs.AI|cs.SD
2017-02-28T17:24:55Z,2016-12-02T22:02:04Z,http://arxiv.org/abs/1612.00876v1,http://arxiv.org/pdf/1612.00876v1,FRIDA: FRI-Based DOA Estimation for Arbitrary Array Layouts,"In this paper we present FRIDA---an algorithm for estimating directions of
arrival of multiple wideband sound sources. FRIDA combines multi-band
information coherently and achieves state-of-the-art resolution at extremely
low signal-to-noise ratios. It works for arbitrary array layouts but unlike
the various steered response power and subspace methods it does not require a
grid search. FRIDA leverages recent advances in sampling signals with a finite
rate of innovation. It is based on the insight that for any array layout the
entries of the spatial covariance matrix can be linearly transformed into a
uniformly sampled sum of sinusoids.",Hanjie Pan|Robin Scheibler|Eric Bezzam|Ivan Dokmanic|Martin Vetterli,cs.SD
2017-02-28T17:24:55Z,2016-12-01T08:31:23Z,http://arxiv.org/abs/1612.00172v1,http://arxiv.org/pdf/1612.00172v1,"A Non Linear Approach towards Automated Emotion Analysis in Hindustani
  Music","In North Indian Classical Music raga forms the basic structure over which
individual improvisations is performed by an artist based on his/her
creativity. The Alap is the opening section of a typical Hindustani Music (HM)
performance where the raga is introduced and the paths of its development are
revealed using all the notes used in that particular raga and allowed
transitions between them with proper distribution over time. In India
corresponding to each raga several emotional flavors are listed namely erotic
love pathetic devotional comic horrific repugnant heroic fantastic
furious peaceful. The detection of emotional cues from Hindustani Classical
music is a demanding task due to the inherent ambiguity present in the
different ragas which makes it difficult to identify any particular emotion
from a certain raga. In this study we took the help of a high resolution
mathematical microscope (MFDFA or Multifractal Detrended Fluctuation Analysis)
to procure information about the inherent complexities and time series
fluctuations that constitute an acoustic signal. With the help of this
technique 3 min alap portion of six conventional ragas of Hindustani classical
music namely Darbari Kanada Yaman Mian ki Malhar Durga Jay Jayanti and
Hamswadhani played in three different musical instruments were analyzed. The
results are discussed in detail.",Shankha Sanyal|Archi Banerjee|Tarit Guhathakurata|Ranjan Sengupta|Dipak Ghosh,cs.SD|nlin.CD
2017-02-28T17:24:55Z,2016-12-01T08:25:45Z,http://arxiv.org/abs/1612.00171v1,http://arxiv.org/pdf/1612.00171v1,"A Non Linear Multifractal Study to Illustrate the Evolution of Tagore
  Songs Over a Century","The works of Rabindranath Tagore have been sung by various artistes over
generations spanning over almost 100 years. there are few songs which were
popular in the early years and have been able to retain their popularity over
the years while some others have faded away. In this study we look to find cues
for the singing style of these songs which have kept them alive for all these
years. For this we took 3 min clip of four Tagore songs which have been sung by
five generation of artistes over 100 years and analyze them with the help of
latest nonlinear techniques Multifractal Detrended Fluctuation Analysis
(MFDFA). The multifractal spectral width is a manifestation of the inherent
complexity of the signal and may prove to be an important parameter to identify
the singing style of particular generation of singers and how this style varies
over different generations. The results are discussed in detail.",Shankha Sanyal|Archi Banerjee|Tarit Guhathakurata|Ranjan Sengupta|Dipak Ghosh,cs.SD|nlin.CD
2017-02-28T17:24:55Z,2016-11-29T20:26:00Z,http://arxiv.org/abs/1611.09827v1,http://arxiv.org/pdf/1611.09827v1,Learning Features of Music from Scratch,"We introduce a new large-scale music dataset MusicNet to serve as a source
of supervision and evaluation of machine learning methods for music research.
MusicNet consists of hundreds of freely-licensed classical music recordings by
10 composers written for 11 instruments together with instrument/note
annotations resulting in over 1 million temporal labels on 34 hours of chamber
music performances under various studio and microphone conditions.
  We define a multi-label classification task to predict notes in musical
recordings along with an evaluation protocol. We benchmark several machine
learning architectures for this task: i) learning from ""hand-crafted""
spectrogram features; ii) end-to-end learning with a neural net; iii)
end-to-end learning with a convolutional neural net. We show that several
end-to-end learning proposals outperform approaches based on learning from
hand-crafted audio features.",John Thickstun|Zaid Harchaoui|Sham Kakade,stat.ML|cs.LG|cs.SD
2017-02-28T17:24:55Z,2016-11-29T17:19:45Z,http://arxiv.org/abs/1611.09733v1,http://arxiv.org/abs/1611.09733v1,Getting Closer to the Essence of Music: The Con Espressione Manifesto,"This text offers a personal and very subjective view on the current situation
of Music Information Research (MIR). Motivated by the desire to build systems
with a somewhat deeper understanding of music than the ones we currently have
I try to sketch a number of challenges for the next decade of MIR research
grouped around six simple truths about music that are probably generally agreed
on but often ignored in everyday research.",Gerhard Widmer,cs.SD
2017-02-28T17:24:55Z,2016-11-29T08:46:26Z,http://arxiv.org/abs/1611.09526v1,http://arxiv.org/pdf/1611.09526v1,Learning Filter Banks Using Deep Learning For Acoustic Signals,"Designing appropriate features for acoustic event recognition tasks is an
active field of research. Expressive features should both improve the
performance of the tasks and also be interpret-able. Currently heuristically
designed features based on the domain knowledge requires tremendous effort in
hand-crafting while features extracted through deep network are difficult for
human to interpret. In this work we explore the experience guided learning
method for designing acoustic features. This is a novel hybrid approach
combining both domain knowledge and purely data driven feature designing. Based
on the procedure of log Mel-filter banks we design a filter bank learning
layer. We concatenate this layer with a convolutional neural network (CNN)
model. After training the network the weight of the filter bank learning layer
is extracted to facilitate the design of acoustic features. We smooth the
trained weight of the learning layer and re-initialize it in filter bank
learning layer as audio feature extractor. For the environmental sound
recognition task based on the Urban- sound8K dataset the experience guided
learning leads to a 2% accuracy improvement compared with the fixed feature
extractors (the log Mel-filter bank). The shape of the new filter banks are
visualized and explained to prove the effectiveness of the feature design
process.",Shuhui Qu|Juncheng Li|Wei Dai|Samarjit Das,cs.SD|cs.AI
2017-02-28T17:24:55Z,2016-11-29T08:33:48Z,http://arxiv.org/abs/1611.09524v1,http://arxiv.org/pdf/1611.09524v1,"Understanding Audio Pattern Using Convolutional Neural Network From Raw
  Waveforms","One key step in audio signal processing is to transform the raw signal into
representations that are efficient for encoding the original information.
Traditionally people transform the audio into spectral representations as a
function of frequency amplitude and phase transformation. In this work we
take a purely data-driven approach to understand the temporal dynamics of audio
at the raw signal level. We maximize the information extracted from the raw
signal through a deep convolutional neural network (CNN) model. Our CNN model
is trained on the urbansound8k dataset. We discover that salient audio patterns
embedded in the raw waveforms can be efficiently extracted through a
combination of nonlinear filters learned by the CNN model.",Shuhui Qu|Juncheng Li|Wei Dai|Samarjit Das,cs.SD
2017-02-28T17:24:55Z,2016-11-29T04:16:44Z,http://arxiv.org/abs/1611.09482v1,http://arxiv.org/pdf/1611.09482v1,Fast Wavenet Generation Algorithm,"This paper presents an efficient implementation of the Wavenet generation
process called Fast Wavenet. Compared to a naive implementation that has
complexity O(2^L) (L denotes the number of layers in the network) our proposed
approach removes redundant convolution operations by caching previous
calculations thereby reducing the complexity to O(L) time. Timing experiments
show significant advantages of our fast implementation over a naive one. While
this method is presented for Wavenet the same scheme can be applied anytime
one wants to perform autoregressive generation or online prediction using a
model with dilated convolution layers. The code for our method is publicly
available.",Tom Le Paine|Pooya Khorrami|Shiyu Chang|Yang Zhang|Prajit Ramachandran|Mark A. Hasegawa-Johnson|Thomas S. Huang,cs.SD|cs.DS|cs.LG
2017-02-28T17:24:55Z,2016-11-27T22:47:23Z,http://arxiv.org/abs/1611.08930v1,http://arxiv.org/pdf/1611.08930v1,Deep attractor network for single-microphone speaker separation,"Despite the overwhelming success of deep learning in various speech
processing tasks the problem of separating simultaneous speakers in a mixture
remains challenging. Two major difficulties in such systems are the arbitrary
source permutation and unknown number of sources in the mixture. We propose a
novel deep learning framework for single channel speech separation by creating
attractor points in high dimensional embedding space of the acoustic signals
which pull together the time-frequency bins corresponding to each source.
Attractor points in this study are created by finding the centroids of the
sources in the embedding space which are subsequently used to determine the
similarity of each bin in the mixture to each source. The network is then
trained to minimize the reconstruction error of each source by optimizing the
embeddings. The proposed model is different from prior works in that it
implements an end-to-end training and it does not depend on the number of
sources in the mixture. Two strategies are explored in the test time K-means
and fixed attractor points where the latter requires no post-processing and
can be implemented in real-time. We evaluated our system on Wall Street Journal
dataset and show 5.49\% improvement over the previous state-of-the-art methods.",Zhuo Chen|Yi Luo|Nima Mesgarani,cs.SD|cs.LG
2017-02-28T17:24:59Z,2016-11-27T22:20:51Z,http://arxiv.org/abs/1612.01928v1,http://arxiv.org/pdf/1612.01928v1,Invariant Representations for Noisy Speech Recognition,"Modern automatic speech recognition (ASR) systems need to be robust under
acoustic variability arising from environmental speaker channel and
recording conditions. Ensuring such robustness to variability is a challenge in
modern day neural network-based ASR systems especially when all types of
variability are not seen during training. We attempt to address this problem by
encouraging the neural network acoustic model to learn invariant feature
representations. We use ideas from recent research on image generation using
Generative Adversarial Networks and domain adaptation ideas extending
adversarial gradient-based training. A recent work from Ganin et al. proposes
to use adversarial training for image domain adaptation by using an
intermediate representation from the main target classification network to
deteriorate the domain classifier performance through a separate neural
network. Our work focuses on investigating neural architectures which produce
representations invariant to noise conditions for ASR. We evaluate the proposed
architecture on the Aurora-4 task a popular benchmark for noise robust ASR. We
show that our method generalizes better than the standard multi-condition
training especially when only a few noise categories are seen during training.",Dmitriy Serdyuk|Kartik Audhkhasi|Philémon Brakel|Bhuvana Ramabhadran|Samuel Thomas|Yoshua Bengio,cs.CL|cs.CV|cs.LG|cs.SD|stat.ML
2017-02-28T17:24:59Z,2016-11-27T20:29:53Z,http://arxiv.org/abs/1611.08905v1,http://arxiv.org/pdf/1611.08905v1,"SISO and SIMO Accompaniment Cancellation for Live Solo Recordings Based
  on Short-Time ERB-Band Wiener Filtering and Spectral Subtraction","Research in collaborative music learning is subject to unresolved problems
demanding new technological solutions. One such problem poses the suppression
of the accompaniment in a live recording of a performance during practice
which can be for the purposes of self-assessment or further machine-aided
analysis. Being able to separate a solo from the accompaniment allows to create
learning agents that may act as personal tutors and help the apprentice improve
his or her technique. First we start from the classical adaptive noise
cancelling approach and adjust it to the problem at hand. In a second step we
compare some adaptive and Wiener filtering approaches and assess their
performances on the task. Our findings underpin that adaptive filtering is
inapt of dealing with music signals and that Wiener filtering in the short-time
Fourier transform domain is a much more effective approach. In addition it is
very cheap if carried out in the frequency bands of auditory filters. A
double-output extension based on maximal-ratio combining is also proposed.",Stanislaw Gorlow|Mathieu Ramona|François Pachet,cs.SD
2017-02-28T17:24:59Z,2017-01-22T22:28:47Z,http://arxiv.org/abs/1611.08749v2,http://arxiv.org/pdf/1611.08749v2,"Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on
  Animal calls and Speech","The scattering framework offers an optimal hierarchical convolutional
decomposition according to its kernels. Convolutional Neural Net (CNN) can be
seen as an optimal kernel decomposition nevertheless it requires large amount
of training data to learn its kernels. We propose a trade-off between these two
approaches: a Chirplet kernel as an efficient Q constant bioacoustic
representation to pretrain CNN. First we motivate Chirplet bioinspired auditory
representation. Second we give the first algorithm (and code) of a Fast
Chirplet Transform (FCT). Third we demonstrate the computation efficiency of
FCT on large environmental data base: months of Orca recordings and 1000 Birds
species from the LifeClef challenge. Fourth we validate FCT on the vowels
subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN
when it pretrains low level layers: it reduces training duration by -28\% for
birds classification and by -26% for vowels classification. Scores are also
enhanced by FCT pretraining with a relative gain of +7.8% of Mean Average
Precision on birds and +2.3\% of vowel accuracy against raw audio CNN. We
conclude on perspectives on tonotopic FCT deep machine listening and
inter-species bioacoustic transfer learning to generalise the representation of
animal communication systems.",Herve Glotin|Julien Ricard|Randall Balestriero,cs.SD
2017-02-28T17:24:59Z,2016-11-22T15:18:31Z,http://arxiv.org/abs/1611.07351v1,http://arxiv.org/pdf/1611.07351v1,MOMOS-MT: Mobile Monophonic System for Music Transcription,"Music holds a significant cultural role in social identity and in the
encouragement of socialization. Technology by the destruction of physical and
cultural distance has lead to many changes in musical themes and the complete
loss of forms. Yet it also allows for the preservation and distribution of
music from societies without a history of written sheet music. This paper
presents early work on a tool for musicians and ethnomusicologists to
transcribe sheet music from monophonic voiced pieces for preservation and
distribution. Using FFT the system detects the pitch frequencies also other
methods detect note durations tempo time signatures and generates sheet
music. The final system is able to be used in mobile platforms allowing the
user to take recordings and produce sheet music in situ to a performance.",Munir Makhmutov|Joseph Alexander Brown|Manuel Mazzara|Leonard Johard,cs.SD|cs.MM
2017-02-28T17:24:59Z,2016-11-21T20:08:51Z,http://arxiv.org/abs/1611.06986v1,http://arxiv.org/pdf/1611.06986v1,Robust end-to-end deep audiovisual speech recognition,"Speech is one of the most effective ways of communication among humans. Even
though audio is the most common way of transmitting speech very important
information can be found in other modalities such as vision. Vision is
particularly useful when the acoustic signal is corrupted. Multi-modal speech
recognition however has not yet found wide-spread use mostly because the
temporal alignment and fusion of the different information sources is
challenging.
  This paper presents an end-to-end audiovisual speech recognizer (AVSR) based
on recurrent neural networks (RNN) with a connectionist temporal classification
(CTC) loss function. CTC creates sparse ""peaky"" output activations and we
analyze the differences in the alignments of output targets (phonemes or
visemes) between audio-only video-only and audio-visual feature
representations. We present the first such experiments on the large vocabulary
IBM ViaVoice database which outperform previously published approaches on
phone accuracy in clean and noisy conditions.",Ramon Sanabria|Florian Metze|Fernando De La Torre,cs.CL|cs.LG|cs.SD
2017-02-28T17:24:59Z,2016-11-20T12:11:58Z,http://arxiv.org/abs/1611.06505v1,http://arxiv.org/pdf/1611.06505v1,"Decision-Based Transcription of Jazz Guitar Solos Using a Harmonic
  Bident Analysis Filter Bank and Spectral Distribution Weighting","Jazz guitar solos are improvised melody lines played on one instrument on top
of a chordal accompaniment (comping). As the improvisation happens
spontaneously a reference score is non-existent only a lead sheet. There are
situations however when one would like to have the original melody lines in
the form of notated music see the Real Book. The motivation is either for the
purpose of practice and imitation or for musical analysis. In this work an
automatic transcriber for jazz guitar solos is developed. It resorts to a very
intuitive representation of tonal music signals: the pitchgram. No
instrument-specific modeling is involved so the transcriber should be
applicable to other pitched instruments as well. Neither is there the need to
learn any note profiles prior to or during the transcription. Essentially the
proposed transcriber is a decision tree thus a classifier with a depth of 3.
It has a (very) low computational complexity and can be run on-line. The
decision rules can be refined or extended with no or little musical education.
The transcriber's performance is evaluated on a set of ten jazz solo excerpts
and compared with a state-of-the-art transcription system for the guitar plus
PYIN. We achieve an improvement of 34% w.r.t. the reference system and 19%
w.r.t. PYIN in terms of the F-measure. Another measure of accuracy the error
score attests that the number of erroneous pitch detections is reduced by more
than 50% w.r.t. the reference system and by 45% w.r.t. PYIN.",Stanislaw Gorlow|Mathieu Ramona|François Pachet,cs.SD
2017-02-28T17:24:59Z,2016-11-18T22:33:05Z,http://arxiv.org/abs/1611.06265v1,http://arxiv.org/pdf/1611.06265v1,"Deep Clustering and Conventional Networks for Music Separation: Stronger
  Together","Deep clustering is the first method to handle general audio separation
scenarios with multiple sources of the same type and an arbitrary number of
sources performing impressively in speaker-independent speech separation
tasks. However little is known about its effectiveness in other challenging
situations such as music source separation. Contrary to conventional networks
that directly estimate the source signals deep clustering generates an
embedding for each time-frequency bin and separates sources by clustering the
bins in the embedding space. We show that deep clustering outperforms
conventional networks on a singing voice separation task in both matched and
mismatched conditions even though conventional networks have the advantage of
end-to-end training for best signal approximation presumably because its more
flexible objective engenders better regularization. Since the strengths of deep
clustering and conventional network architectures appear complementary we
explore combining them in a single hybrid network trained via an approach akin
to multi-task learning. Remarkably the combination significantly outperforms
either of its components.",Yi Luo|Zhuo Chen|John R. Hershey|Jonathan Le Roux|Nima Mesgarani,stat.ML|cs.LG|cs.SD
2017-02-28T17:24:59Z,2016-12-07T20:51:36Z,http://arxiv.org/abs/1611.05416v2,http://arxiv.org/pdf/1611.05416v2,"Composing Music with Grammar Argumented Neural Networks and Note-Level
  Encoding","Creating aesthetically pleasing pieces of art including music has been a
long-term goal for artificial intelligence research. Despite recent successes
of long-short term memory (LSTM) recurrent neural networks (RNNs) in sequential
learning LSTM neural networks have not by themselves been able to generate
natural-sounding music conforming to music theory. To transcend this
inadequacy we put forward a novel method for music composition that combines
the LSTM with Grammars motivated by music theory. The main tenets of music
theory are encoded as grammar argumented (GA) filters on the training data
such that the machine can be trained to generate music inheriting the
naturalness of human-composed pieces from the original dataset while adhering
to the rules of music theory. Unlike previous approaches pitches and durations
are encoded as one semantic entity which we refer to as note-level encoding.
This allows easy implementation of music theory grammars as well as closer
emulation of the thinking pattern of a musician. Although the GA rules are
applied to the training data and never directly to the LSTM music generation
our machine still composes music that possess high incidences of diatonic scale
notes small pitch intervals and chords in deference to music theory.",Zheng Sun|Jiaqi Liu|Zewang Zhang|Jingwen Chen|Zhao Huo|Ching Hua Lee|Xiao Zhang,cs.LG|cs.AI|cs.SD
2017-02-28T17:24:59Z,2016-11-16T08:15:00Z,http://arxiv.org/abs/1611.05182v1,http://arxiv.org/pdf/1611.05182v1,Detecting tala Computationally in Polyphonic Context - A Novel Approach,"In North-Indian-Music-System(NIMS)tabla is mostly used as percussive
accompaniment for vocal-music in polyphonic-compositions. The human auditory
system uses perceptual grouping of musical-elements and easily filters the
tabla component thereby decoding prominent rhythmic features like tala tempo
from a polyphonic composition. For Western music lots of work have been
reported for automated drum analysis of polyphonic composition. However
attempts at computational analysis of tala by separating the tabla-signal from
mixed signal in NIMS have not been successful. Tabla is played with two
components - right and left. The right-hand component has frequency overlap
with voice and other instruments. So tala analysis of polyphonic-composition
by accurately separating the tabla-signal from the mixture is a baffling task
therefore an area of challenge. In this work we propose a novel technique for
successfully detecting tala using left-tabla signal producing meaningful
results because the left-tabla normally doesn't have frequency overlap with
voice and other instruments. North-Indian-rhythm follows complex cyclic
pattern against linear approach of Western-rhythm. We have exploited this
cyclic property along with stressed and non-stressed methods of playing
tabla-strokes to extract a characteristic pattern from the left-tabla strokes
which after matching with the grammar of tala-system determines the tala and
tempo of the composition. A large number of
polyphonic(vocal+tabla+other-instruments) compositions has been analyzed with
the methodology and the result clearly reveals the effectiveness of proposed
techniques.",Susmita Bhaduri|Dipak Ghosh,cs.SD|68T10|I.5.2
2017-02-28T17:24:59Z,2016-11-15T17:23:17Z,http://arxiv.org/abs/1611.04947v1,http://arxiv.org/pdf/1611.04947v1,"Detection of north atlantic right whale upcalls using local binary
  patterns in a two-stage strategy","In this paper we investigate the effectiveness of two-stage classification
strategies in detecting north Atlantic right whale upcalls. Time-frequency
measurements of data from passive acoustic monitoring devices are evaluated as
images. Vocalization spectrograms are preprocessed for noise reduction and tone
removal. First stage of the algorithm eliminates non-upcalls by an energy
detection algorithm. In the second stage two sets of features are extracted
from the remaining signals using contour-based and texture based methods. The
former is based on extraction of time-frequency features from upcall contours
and the latter employs a Local Binary Pattern operator to extract
distinguishing texture features of the upcalls. Subsequently evaluation phase
is carried out by using several classifiers to assess the effectiveness of both
the contour-based and texture-based features for upcall detection. Experimental
results with the data set provided by the Cornell University Bioacoustics
Research Program reveal that classifiers show accuracy improvements of 3% to 4%
when using LBP features over time-frequency features. Classifiers such as the
Linear Discriminant Analysis Support Vector Machine and TreeBagger achieve
high upcall detection rates with LBP features.",Mahdi Esfahanian|Hanqi Zhuang|Nurgun Erdol|Edmund Gerstein,cs.SD
2017-02-28T17:25:03Z,2017-02-18T07:18:32Z,http://arxiv.org/abs/1611.04871v3,http://arxiv.org/pdf/1611.04871v3,"Audio Event and Scene Recognition: A Unified Approach using Strongly and
  Weakly Labeled Data","In this paper we propose a novel learning framework called Supervised and
Weakly Supervised Learning where the goal is to learn simultaneously from
weakly and strongly labeled data. Strongly labeled data can be simply
understood as fully supervised data where all labeled instances are available.
In weakly supervised learning only data is weakly labeled which prevents one
from directly applying supervised learning methods. Our proposed framework is
motivated by the fact that a small amount of strongly labeled data can give
considerable improvement over only weakly supervised learning. The primary
problem domain focus of this paper is acoustic event and scene detection in
audio recordings. We first propose a naive formulation for leveraging labeled
data in both forms. We then propose a more general framework for Supervised and
Weakly Supervised Learning (SWSL). Based on this general framework we propose
a graph based approach for SWSL. Our main method is based on manifold
regularization on graphs in which we show that the unified learning can be
formulated as a constraint optimization problem which can be solved by
iterative concave-convex procedure (CCCP). Our experiments show that our
proposed framework can address several concerns of audio content analysis using
weakly labeled data.",Anurag Kumar|Bhiksha Raj,cs.LG|cs.CV|cs.SD
2017-02-28T17:25:03Z,2016-11-10T22:11:16Z,http://arxiv.org/abs/1611.03533v1,http://arxiv.org/pdf/1611.03533v1,Landmark-based consonant voicing detection on multilingual corpora,"This paper tests the hypothesis that distinctive feature classifiers anchored
at phonetic landmarks can be transferred cross-lingually without loss of
accuracy. Three consonant voicing classifiers were developed: (1) manually
selected acoustic features anchored at a phonetic landmark (2) MFCCs (either
averaged across the segment or anchored at the landmark) and(3) acoustic
features computed using a convolutional neural network (CNN). All detectors are
trained on English data (TIMIT)and tested on English Turkish and Spanish
(performance measured using F1 and accuracy). Experiments demonstrate that
manual features outperform all MFCC classifiers while CNNfeatures outperform
both. MFCC-based classifiers suffer an F1reduction of 16% absolute when
generalized from English to other languages. Manual features suffer only a 5%
F1 reductionand CNN features actually perform better in Turkish and Span-ish
than in the training language demonstrating that features capable of
representing long-term spectral dynamics (CNN and landmark-based features) are
able to generalize cross-lingually with little or no loss of accuracy",Xiang Kong|Xuesong Yang|Mark Hasegawa-Johnson|Jeung-Yoon Choi|Stefanie Shattuck-Hufnagel,cs.CL|cs.SD
2017-02-28T17:25:03Z,2016-11-10T20:35:47Z,http://arxiv.org/abs/1611.03477v1,http://arxiv.org/pdf/1611.03477v1,Song From PI: A Musically Plausible Network for Pop Music Generation,"We present a novel framework for generating pop music. Our model is a
hierarchical Recurrent Neural Network where the layers and the structure of
the hierarchy encode our prior knowledge about how pop music is composed. In
particular the bottom layers generate the melody while the higher levels
produce the drums and chords. We conduct several human studies that show strong
preference of our generated music over that produced by the recent method by
Google. We additionally show two applications of our framework: neural dancing
and karaoke as well as neural story singing.",Hang Chu|Raquel Urtasun|Sanja Fidler,cs.SD|cs.AI
2017-02-28T17:25:03Z,2016-11-10T03:51:42Z,http://arxiv.org/abs/1611.03178v1,http://arxiv.org/pdf/1611.03178v1,Noise reduction combining microphone and piezoelectric device,"It is often required to extract the sound of an objective instrument played
in concert with other instruments. Microphone array is one of the effective
ways to enhance a sound from a specific direction. However it is not effective
in an echoic room such as concert hall. The pickup microphone attached on the
specific musical instrument is often employed to obtain the sound exclusively
from other instrumental sounds. The obtained timbre differ from the one we hear
at the usual listening position. The purpose of this paper is to propose a new
method of sound separation that utilizes the piezoelectric device attached on
the body of the instrument. The signal from the attached device has a different
spectrum from the sound heard by the audience but has the same frequency
components as the instrumental sound. Our idea is to use the device signal as a
modifier of the sound focusing filter applied to the microphone sound at the
listening position. The proposed method firstly estimates the frequency
components of the signal from the piezoelectric device. The frequency
characteristics for filtering the microphone sound are changed so that it pass
the estimated frequency components. Thus we can extractthe target sound without
distortion. The proposed method is a sort of dynamic sparseness approach. It
was found that SNR is improved by 8.7dB through the experiments.",Naoya Takahashi|Mitsuharu Matsumoto|Shuji Hashimoto,cs.SD
2017-02-28T17:25:03Z,2016-11-09T09:54:45Z,http://arxiv.org/abs/1611.03081v1,http://arxiv.org/pdf/1611.03081v1,"VR 'SPACE OPERA': Mimetic Spectralism in an Immersive Starlight
  Audification System","This paper describes a system designed as part of an interactive VR opera
which immerses a real-time composer and an audience (via a network) in the
historical location of Gobeklitepe in southern Turkey during an imaginary
scenario set in the Pre-Pottery Neolithic period (8500-5500 BCE) viewed by
some to be the earliest example of a temple or observatory. In this scene
music is generated where the harmonic material is determined based on
observations of light variation from pulsating stars that would have
theoretically been overhead on the 1st of October 8000 BC at 23:00 and animal
calls based on the reliefs in the temple. Based on the observations of the
stars V465 Per HD 217860 16 Lac BG CVn and KIC 6382916 frequency
collections were derived and applied to the generation of musical sound and
notation sequences within a custom VR environment using a novel method
incorporating spectralist techniques. Parameters controlling this 'resynthesis'
can be manipulated by the performer using a Leap Motion controller and Oculus
Rift HMD yielding both sonic and visual results in the environment. The final
opera is to be viewed via Google Cardboard and delivered over the Internet.
This entire process aims to pose questions about real-time composition through
time distortion and invoke a sense of wonder and meaningfulness through a
ritualistic experience.",Benedict Carey|Burak Ulas,cs.SD|physics.pop-ph
2017-02-28T17:25:03Z,2016-11-08T09:50:30Z,http://arxiv.org/abs/1611.02695v1,http://arxiv.org/pdf/1611.02695v1,"Automatic recognition of child speech for robotic applications in noisy
  environments","Automatic speech recognition (ASR) allows a natural and intuitive interface
for robotic educational applications for children. However there are a number
of challenges to overcome to allow such an interface to operate robustly in
realistic settings including the intrinsic difficulties of recognising child
speech and high levels of background noise often present in classrooms. As part
of the EU EASEL project we have provided several contributions to address these
challenges implementing our own ASR module for use in robotics applications.
We used the latest deep neural network algorithms which provide a leap in
performance over the traditional GMM approach and apply data augmentation
methods to improve robustness to noise and speaker variation. We provide a
close integration between the ASR module and the rest of the dialogue system
allowing the ASR to receive in real-time the language models relevant to the
current section of the dialogue greatly improving the accuracy. We integrated
our ASR module into an interactive multimodal system using a small humanoid
robot to help children learn about exercise and energy. The system was
installed at a public museum event as part of a research study where 320
children (aged 3 to 14) interacted with the robot with our ASR achieving 90%
accuracy for fluent and near-fluent speech.",Samuel Fernando|Roger K. Moore|David Cameron|Emily C. Collins|Abigail Millings|Amanda J. Sharkey|Tony J. Prescott,cs.CL|cs.SD
2017-02-28T17:25:03Z,2016-11-06T14:00:14Z,http://arxiv.org/abs/1611.01783v1,http://arxiv.org/pdf/1611.01783v1,Domain Adaptation For Formant Estimation Using Deep Learning,"In this paper we present a domain adaptation technique for formant estimation
using a deep network. We first train a deep learning network on a small read
speech dataset. We then freeze the parameters of the trained network and use
several different datasets to train an adaptation layer that makes the obtained
network universal in the sense that it works well for a variety of speakers and
speech domains with very different characteristics. We evaluated our adapted
network on three datasets each of which has different speaker characteristics
and speech styles. The performance of our method compares favorably with
alternative methods for formant estimation.",Yehoshua Dissen|Joseph Keshet|Jacob Goldberger|Cynthia Clopper,cs.CL|cs.SD
2017-02-28T17:25:03Z,2016-11-03T20:08:40Z,http://arxiv.org/abs/1611.01172v1,http://arxiv.org/pdf/1611.01172v1,"Multiple-Speaker Localization Based on Direct-Path Features and
  Likelihood Maximization with Spatial Sparsity Regularization","This paper addresses the problem of multiple-speaker localization in noisy
and reverberant environments using binaural recordings of an acoustic scene. A
Gaussian mixture model (GMM) is adopted whose components correspond to all the
possible candidate source locations defined on a grid. After optimizing the
GMM-based objective function given an observed set of binaural features both
the number of sources and their locations are estimated by selecting the GMM
components with the largest priors. This is achieved by enforcing a sparse
solution thus favoring a small number of speakers with respect to the large
number of initial candidate source locations. An entropy-based penalty term is
added to the likelihood thus imposing sparsity over the set of GMM priors. In
addition the direct-path relative transfer function (DP-RTF) is used to build
robust binaural features. The DP-RTF recently proposed for single-source
localization was shown to be robust to reverberations since it encodes
inter-channel information corresponding to the direct-path of sound
propagation. In this paper we extend the DP-RTF estimation to the case of
multiple sources. In the short-time Fourier transform domain a consistency
test is proposed to check whether a set of consecutive frames is associated to
the same source or not. Reliable DP-RTF features are selected from the frames
that pass the consistency test to be used for source localization. Experiments
carried out using both simulation data and real data gathered with a robotic
head confirm the efficiency of the proposed multi-source localization method.",Xiaofei Li|Laurent Girin|Sharon Gannot|Radu Horaud,cs.SD
2017-02-28T17:25:03Z,2016-11-03T12:08:25Z,http://arxiv.org/abs/1611.00966v1,http://arxiv.org/pdf/1611.00966v1,Frame Theory for Signal Processing in Psychoacoustics,"This review chapter aims to strengthen the link between frame theory and
signal processing tasks in psychoacoustics. On the one side the basic concepts
of frame theory are presented and some proofs are provided to explain those
concepts in some detail. The goal is to reveal to hearing scientists how this
mathematical theory could be relevant for their research. In particular we
focus on frame theory in a filter bank approach which is probably the most
relevant view-point for audio signal processing. On the other side basic
psychoacoustic concepts are presented to stimulate mathematicians to apply
their knowledge in this field.",Peter Balazs|Nicki Holighaus|Thibaud Necciari|Diana Stoeva,cs.SD|math.FA
2017-02-28T17:25:03Z,2016-11-02T09:24:10Z,http://arxiv.org/abs/1611.00514v1,http://arxiv.org/pdf/1611.00514v1,The Intelligent Voice 2016 Speaker Recognition System,"This paper presents the Intelligent Voice (IV) system submitted to the NIST
2016 Speaker Recognition Evaluation (SRE). The primary emphasis of SRE this
year was on developing speaker recognition technology which is robust for novel
languages that are much more heterogeneous than those used in the current
state-of-the-art using significantly less training data that does not contain
meta-data from those languages. The system is based on the state-of-the-art
i-vector/PLDA which is developed on the fixed training condition and the
results are reported on the protocol defined on the development set of the
challenge.",Abbas Khosravani|Cornelius Glackin|Nazim Dugan|Gérard Chollet|Nigel Cannings,cs.SD|cs.CL|stat.ML
2017-02-28T17:25:06Z,2017-01-27T06:01:20Z,http://arxiv.org/abs/1611.00326v2,http://arxiv.org/pdf/1611.00326v2,"Enhanced Factored Three-Way Restricted Boltzmann Machines for Speech
  Detection","In this letter we propose enhanced factored three way restricted Boltzmann
machines (EFTW-RBMs) for speech detection. The proposed model incorporates
conditional feature learning by multiplying the dynamical state of the third
unit which allows a modulation over the visible-hidden node pairs. Instead of
stacking previous frames of speech as the third unit in a recursive manner the
correlation related weighting coefficients are assigned to the contextual
neighboring frames. Specifically a threshold function is designed to capture
the long-term features and blend the globally stored speech structure. A
factored low rank approximation is introduced to reduce the parameters of the
three-dimensional interaction tensor on which non-negative constraint is
imposed to address the sparsity characteristic. The validations through the
area-under-ROC-curve (AUC) and signal distortion ratio (SDR) show that our
approach outperforms several existing 1D and 2D (i.e. time and time-frequency
domain) speech detection algorithms in various noisy environments.",Pengfei Sun|Jun Qin,cs.SD|cs.LG|stat.ML
2017-02-28T17:25:06Z,2016-10-27T20:23:39Z,http://arxiv.org/abs/1610.09001v1,http://arxiv.org/pdf/1610.09001v1,SoundNet: Learning Sound Representations from Unlabeled Video,"We learn rich natural sound representations by capitalizing on large amounts
of unlabeled sound data collected in the wild. We leverage the natural
synchronization between vision and sound to learn an acoustic representation
using two-million unlabeled videos. Unlabeled video has the advantage that it
can be economically acquired at massive scales yet contains useful signals
about natural sound. We propose a student-teacher training procedure which
transfers discriminative visual knowledge from well established visual
recognition models into the sound modality using unlabeled video as a bridge.
Our sound representation yields significant performance improvements over the
state-of-the-art results on standard benchmarks for acoustic scene/object
classification. Visualizations suggest some high-level semantics automatically
emerge in the sound network even though it is trained without ground truth
labels.",Yusuf Aytar|Carl Vondrick|Antonio Torralba,cs.CV|cs.LG|cs.SD
2017-02-28T17:25:06Z,2016-10-27T18:58:06Z,http://arxiv.org/abs/1610.08927v1,http://arxiv.org/pdf/1610.08927v1,Voice Conversion using Convolutional Neural Networks,"The human auditory system is able to distinguish the vocal source of
thousands of speakers yet not much is known about what features the auditory
system uses to do this. Fourier Transforms are capable of capturing the pitch
and harmonic structure of the speaker but this alone proves insufficient at
identifying speakers uniquely. The remaining structure often referred to as
timbre is critical to identifying speakers but we understood little about it.
In this paper we use recent advances in neural networks in order to manipulate
the voice of one speaker into another by transforming not only the pitch of the
speaker but the timbre. We review generative models built with neural networks
as well as architectures for creating neural networks that learn analogies. Our
preliminary results converting voices from one speaker to another are
encouraging.",Shariq Mobin|Joan Bruna,stat.ML|cs.SD
2017-02-28T17:25:06Z,2016-10-26T04:50:35Z,http://arxiv.org/abs/1610.08166v1,http://arxiv.org/pdf/1610.08166v1,Automatic measurement of vowel duration via structured prediction,"A key barrier to making phonetic studies scalable and replicable is the need
to rely on subjective manual annotation. To help meet this challenge a
machine learning algorithm was developed for automatic measurement of a widely
used phonetic measure: vowel duration. Manually-annotated data were used to
train a model that takes as input an arbitrary length segment of the acoustic
signal containing a single vowel that is preceded and followed by consonants
and outputs the duration of the vowel. The model is based on the structured
prediction framework. The input signal and a hypothesized set of a vowel's
onset and offset are mapped to an abstract vector space by a set of acoustic
feature functions. The learning algorithm is trained in this space to minimize
the difference in expectations between predicted and manually-measured vowel
durations. The trained model can then automatically estimate vowel durations
without phonetic or orthographic transcription. Results comparing the model to
three sets of manually annotated data suggest it out-performed the current gold
standard for duration measurement an HMM-based forced aligner (which requires
orthographic or phonetic transcription as an input).",Yossi Adi|Joseph Keshet|Emily Cibelli|Erin Gustafson|Cynthia Clopper|Matthew Goldrick,stat.ML|cs.LG|cs.SD
2017-02-28T17:25:06Z,2016-10-19T20:56:05Z,http://arxiv.org/abs/1610.06214v1,http://arxiv.org/pdf/1610.06214v1,A model of infant speech perception and learning,"Infant speech perception and learning is modeled using Echo State Network
classification and Reinforcement Learning. Ambient speech for the modeled
infant learner is created using the speech synthesizer Vocaltractlab. An
auditory system is trained to recognize vowel sounds from a series of speakers
of different anatomies in Vocaltractlab. Having formed perceptual targets the
infant uses Reinforcement Learning to imitate his ambient speech. A possible
way of bridging the problem of speaker normalisation is proposed using direct
imitation but also including a caregiver who listens to the infants sounds and
imitates those that sound vowel-like.",Philip Zurbuchen,cs.SD
2017-02-28T17:25:06Z,2016-10-19T10:16:46Z,http://arxiv.org/abs/1610.05948v1,http://arxiv.org/pdf/1610.05948v1,A Bayesian Approach to Estimation of Speaker Normalization Parameters,"In this work a Bayesian approach to speaker normalization is proposed to
compensate for the degradation in performance of a speaker independent speech
recognition system. The speaker normalization method proposed herein uses the
technique of vocal tract length normalization (VTLN). The VTLN parameters are
estimated using a novel Bayesian approach which utilizes the Gibbs sampler a
special type of Markov Chain Monte Carlo method. Additionally the
hyperparameters are estimated using maximum likelihood approach. This model is
used assuming that human vocal tract can be modeled as a tube of uniform cross
section. It captures the variation in length of the vocal tract of different
speakers more effectively than the linear model used in literature. The work
has also investigated different methods like minimization of Mean Square Error
(MSE) and Mean Absolute Error (MAE) for the estimation of VTLN parameters. Both
single pass and two pass approaches are then used to build a VTLN based speech
recognizer. Experimental results on recognition of vowels and Hindi phrases
from a medium vocabulary indicate that the Bayesian method improves the
performance by a considerable margin.",Dhananjay Ram|Debasis Kundu|Rajesh M. Hegde,cs.SD|cs.CL|stat.AP
2017-02-28T17:25:06Z,2016-10-19T10:06:14Z,http://arxiv.org/abs/1610.05945v1,http://arxiv.org/pdf/1610.05945v1,"A multi-task learning model for malware classification with useful file
  access pattern from API call sequence","Based on API call sequences semantic-aware and machine learning (ML) based
malware classifiers can be built for malware detection or classification.
Previous works concentrate on crafting and extracting various features from
malware binaries disassembled binaries or API calls via static or dynamic
analysis and resorting to ML to build classifiers. However they tend to
involve too much feature engineering and fail to provide interpretability. We
solve these two problems with the recent advances in deep learning: 1)
RNN-based autoencoders (RNN-AEs) can automatically learn low-dimensional
representation of a malware from its raw API call sequence. 2) Multiple
decoders can be trained under different supervisions to give more information
other than the class or family label of a malware. Inspired by the works of
document classification and automatic sentence summarization each API call
sequence can be regarded as a sentence. In this paper we make the first
attempt to build a multi-task malware learning model based on API call
sequences. The model consists of two decoders one for malware classification
and one for $\emph{file access pattern}$ (FAP) generation given the API call
sequence of a malware. We base our model on the general seq2seq framework.
Experiments show that our model can give competitive classification results as
well as insightful FAP information.",Xin Wang|Siu Ming Yiu,cs.SD|cs.CR|cs.LG
2017-02-28T17:25:06Z,2017-01-05T13:34:14Z,http://arxiv.org/abs/1610.05653v2,http://arxiv.org/abs/1610.05653v2,"Acoustic Reflector Localization: Novel Image Source Reversion and Direct
  Localization Methods","Acoustic reflector localization is an important issue in audio signal
processing with direct applications in spatial audio scene reconstruction
and source separation. Several methods have recently been proposed to estimate
the 3D positions of acoustic reflectors given room impulse responses (RIRs). In
this article we categorize these methods as ""image-source reversion"" which
localizes the image source before finding the reflector position and ""direct
localization"" which localizes the reflector without intermediate steps. We
present five new contributions. First an onset detector called the clustered
dynamic programming projected phase-slope algorithm is proposed to
automatically extract the time of arrival for early reflections within the RIRs
of a compact microphone array. Second we propose an image-source reversion
method that uses the RIRs from a single loudspeaker. It is constructed by
combining an image source locator (the image source direction and range (ISDAR)
algorithm) and a reflector locator (using the loudspeaker-image bisection
(LIB) algorithm). Third two variants of it exploiting multiple loudspeakers
are proposed. Fourth we present a direct localization method the ellipsoid
tangent sample consensus (ETSAC) exploiting ellipsoid properties to localize
the reflector. Finally systematic experiments on simulated and measured RIRs
are presented comparing the proposed methods with the state-of-the-art. ETSAC
generates errors lower than the alternative methods compared through our
datasets. Nevertheless the ISDAR-LIB combination performs well and has a run
time 200 times faster than ETSAC.",Luca Remaggi|Philip J. B. Jackson|Philip Coleman|Wenwu Wang,cs.SD
2017-02-28T17:25:06Z,2016-10-17T03:36:42Z,http://arxiv.org/abs/1610.04965v1,http://arxiv.org/pdf/1610.04965v1,"Improving Short Utterance PLDA Speaker Verification using SUV Modelling
  and Utterance Partitioning Approach","This paper analyses the short utterance probabilistic linear discriminant
analysis (PLDA) speaker verification with utterance partitioning and short
utterance variance (SUV) modelling approaches. Experimental studies have found
that instead of using single long-utterance as enrolment data if long enrolled
utterance is partitioned into multiple short utterances and average of short
utterance i-vectors is used as enrolled data that improves the Gaussian PLDA
(GPLDA) speaker verification. This is because short utterance i-vectors have
speaker session and utterance variations and utterance-partitioning approach
compensates the utterance variation. Subsequently SUV-PLDA is also studied
with utterance partitioning approach and utterance partitioning-based
SUV-GPLDA system shows relative improvement of 9% and 16% in EER for NIST 2008
and NIST 2010 truncated 10sec-10sec evaluation condition as utterance
partitioning approach compensates the utterance variation and SUV modelling
approach compensates the mismatch between full-length development data and
short-length evaluation data.",Ahilan Kanagasundaram|David Dean|Sridha Sridharan|Clinton Fookes,cs.SD
2017-02-28T17:25:06Z,2016-10-16T22:27:33Z,http://arxiv.org/abs/1610.04922v1,http://arxiv.org/abs/1610.04922v1,Making Mainstream Synthesizers with Csound,"For more than the past twenty years Csound has been one of the leaders in
the world of the computer music research implementing innovative synthesis
methods and making them available beyond the academic environments from which
they often arise and into the hands of musicians and sound designers
throughout the world. In its present state Csound offers an efficient
environment for sound experimentation allowing the user to work with almost
any known sound synthesis or signal processing method through its vast
collection of ready-made opcodes. But despite all this potential the shared
resource of Csound instruments still lacks quality reproductions of well-known
synthesizers; even with its ability to generate commercial standard user
interfaces and with the possibility to compile Csound instruments in such as
fashion so that they can be used with no knowledge of Csound code. To fill this
gap the authors have implemented two commercial-style synthesizers as VST
plug-ins using the Csound front-end ""Cabbage"". This paper describes their
architecture and some of the Csound specific challenges involved in the
development of fully featured synthesizers.",Gleb G. Rogozinsky|Eugene Cherny|Ivan Osipenko,cs.SD|H.5.5
