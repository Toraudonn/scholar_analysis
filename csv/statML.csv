2017-02-28T17:16:09Z,2017-02-27T18:58:44Z,http://arxiv.org/abs/1702.08435v1,http://arxiv.org/pdf/1702.08435v1,"Statistical Anomaly Detection via Composite Hypothesis Testing for
  Markov Models","Under Markovian assumptions we leverage a Central Limit Theorem (CLT) related
to the test statistic in the composite hypothesis Hoeffding test so as to
derive a new estimator for the threshold needed by the test. We first show the
advantages of our estimator over an existing estimator by conducting extensive
numerical experiments. We then apply the Hoeffding test with our threshold
estimator to detecting anomalies in both communication and transportation
networks. The former application seeks to enhance cyber security and the latter
aims at building smarter transportation systems in cities.",Jing Zhang|Ioannis Ch. Paschalidis,cs.SY|math.OC|stat.ML
2017-02-28T17:16:09Z,2017-02-27T18:51:41Z,http://arxiv.org/abs/1702.08431v1,http://arxiv.org/pdf/1702.08431v1,Boundary-Seeking Generative Adversarial Networks,"We introduce a novel approach to training generative adversarial networks
where we train a generator to match a target distribution that converges to the
data distribution at the limit of a perfect discriminator. This objective can
be interpreted as training a generator to produce samples that lie on the
decision boundary of a current discriminator in training at each update and we
call a GAN trained using this algorithm a boundary-seeking GAN (BS-GAN). This
approach can be used to train a generator with discrete output when the
generator outputs a parametric conditional distribution. We demonstrate the
effectiveness of the proposed algorithm with discrete image data. In contrary
to the proposed algorithm we observe that the recently proposed Gumbel-Softmax
technique for re-parametrizing the discrete variables does not work for
training a GAN with discrete data. Finally we notice that the proposed
boundary-seeking algorithm works even with continuous variables and
demonstrate its effectiveness with two widely used image data sets SVHN and
CelebA.",R Devon Hjelm|Athul Paul Jacob|Tong Che|Kyunghyun Cho|Yoshua Bengio,stat.ML|cs.LG
2017-02-28T17:16:09Z,2017-02-27T18:26:45Z,http://arxiv.org/abs/1702.08420v1,http://arxiv.org/pdf/1702.08420v1,Embarrassingly parallel inference for Gaussian processes,"Training Gaussian process (GP) based models typically involves an O(N^3)
computational bottleneck. Popular methods for overcoming the matrix inversion
problem include sparse approximations of the covariance matrix through inducing
variables or through dimensionality reduction via ""local experts"". However
these type of models cannot account for both long and short range correlations
in the GP functions. Furthermore these methods are often ill-suited for cases
where the input data are not uniformly distributed. We present an
embarrassingly parallel method that takes advantage of the computational ease
of inverting block diagonal matrices while maintaining much of the
expressivity of a full covariance matrix. By using importance sampling to
average over different realizations of low-rank approximations of the GP model
we ensure our algorithm is both asymptotically unbiased and embarrassingly
parallel.",Michael M. Zhang|Sinead A. Williamson,stat.ML
2017-02-28T17:16:09Z,2017-02-27T17:48:46Z,http://arxiv.org/abs/1702.08402v1,http://arxiv.org/pdf/1702.08402v1,Latent Correlation Gaussian Processes,"We introduce a novel kernel that models input-dependent couplings across
multiple latent processes. The pairwise kernel measures covariance both along
inputs and across different latent signals in a mutually-dependent fashion. The
latent correlation Gaussian process (LCGP) model combines these non-stationary
latent components into multiple outputs by an input-dependent mixing matrix.
Probit classification and support for multiple observation sets are derived by
Variational Bayesian inference. Results on several datasets indicate that the
LCGP model can recover the correlations between latent signals while
simultaneously achieving state-of-the-art performance. We highlight the latent
covariances with an EEG classification dataset where latent brain processes and
their couplings simultaneously emerge from the model.",Sami Remes|Markus Heinonen|Samuel Kaski,stat.ML
2017-02-28T17:16:09Z,2017-02-27T17:46:30Z,http://arxiv.org/abs/1702.08398v1,http://arxiv.org/pdf/1702.08398v1,McGan: Mean and Covariance Feature Matching GAN,"We introduce new families of Integral Probability Metrics (IPM) for training
Generative Adversarial Networks (GAN). Our IPMs are based on matching
statistics of distributions embedded in a finite dimensional feature space.
Mean and covariance feature matching IPMs allow for stable training of GANs
which we will call McGan. McGan minimizes a meaningful loss between
distributions.",Youssef Mroueh|Tom Sercu|Vaibhava Goel,cs.LG|stat.ML
2017-02-28T17:16:09Z,2017-02-27T17:43:34Z,http://arxiv.org/abs/1702.08396v1,http://arxiv.org/pdf/1702.08396v1,Learning Hierarchical Features from Generative Models,"Deep neural networks have been shown to be very successful at learning
feature hierarchies in supervised learning tasks. Generative models on the
other hand have benefited less from hierarchical models with multiple layers
of latent variables. In this paper we prove that certain classes of
hierarchical latent variable models do not take advantage of the hierarchical
structure when trained with existing variational methods and provide some
limitations on the kind of features existing models can learn. Finally we
propose an alternative flat architecture that learns meaningful and
disentangled features on natural images.",Shengjia Zhao|Jiaming Song|Stefano Ermon,cs.LG|stat.ML
2017-02-28T17:16:09Z,2017-02-27T17:22:29Z,http://arxiv.org/abs/1702.08389v1,http://arxiv.org/pdf/1702.08389v1,Equivariance Through Parameter-Sharing,"We propose to study equivariance in deep neural networks through parameter
symmetries. In particular given a group G that acts discretely on the input
and output of a standard neural network layer $\phi_W$ we show that
equivariance of $\phi_W$ is linked to the symmetry group of network parameters
W. We then propose a sparse parameter-sharing scheme to induce the desirable
symmetry on W. Under some conditions on the action of G our procedure for
tying the parameters achieves G-equivariance and guarantee sensitivity to all
other permutation groups outside G. We demonstrate the relation of our approach
to recently-proposed ""structured"" neural layers such as group-convolution and
graph-convolution which leads to new insights and improvement of these
operations.",Siamak Ravanbakhsh|Jeff Schneider|Barnabas Poczos,stat.ML|cs.NE
2017-02-28T17:16:09Z,2017-02-27T16:31:48Z,http://arxiv.org/abs/1702.08359v1,http://arxiv.org/pdf/1702.08359v1,Dynamic Word Embeddings via Skip-Gram Filtering,"We present a probabilistic language model for time-stamped text data which
tracks the semantic evolution of individual words over time. The model
represents words and contexts by latent trajectories in an embedding space. At
each moment in time the embedding vectors are inferred from a probabilistic
version of word2vec [Mikolov 2013]. These embedding vectors are connected in
time through a latent diffusion process. We describe two scalable variational
inference algorithms---skip-gram smoothing and skip-gram filtering---that allow
us to train the model jointly over all times; thus learning on all data while
simultaneously allowing word and context vectors to drift. Experimental results
on three different corpora demonstrate that our dynamic model infers word
embedding trajectories that are more interpretable and lead to higher
predictive likelihoods than competing methods that are based on static models
trained separately on time slices.",Robert Bamler|Stephan Mandt,stat.ML|cs.LG
2017-02-28T17:16:09Z,2017-02-27T16:01:46Z,http://arxiv.org/abs/1702.08343v1,http://arxiv.org/pdf/1702.08343v1,Approximate Inference with Amortised MCMC,"We propose a novel approximate inference algorithm that approximates a target
distribution by amortising the dynamics of a user-selected MCMC sampler. The
idea is to initialise MCMC using samples from an approximation network apply
the MCMC operator to improve these samples and finally use the samples to
update the approximation network thereby improving its quality. This provides a
new generic framework for approximate inference allowing us to deploy highly
complex or implicitly defined approximation families with intractable
densities including approximations produced by warping a source of randomness
through a deep neural network. Experiments consider image modelling with deep
generative models as a challenging test for the method. Deep models trained
using amortised MCMC are shown to generate realistic looking samples as well as
producing diverse imputations for images with regions of missing pixels.",Yingzhen Li|Richard E. Turner|Qiang Liu,stat.ML|cs.LG
2017-02-28T17:16:09Z,2017-02-27T15:17:04Z,http://arxiv.org/abs/1702.08320v1,http://arxiv.org/pdf/1702.08320v1,"An Efficient Pseudo-likelihood Method for Sparse Binary Pairwise Markov
  Network Estimation","The pseudo-likelihood method is one of the most popular algorithms for
learning sparse binary pairwise Markov networks. In this paper we formulate
the $L_1$ regularized pseudo-likelihood problem as a sparse multiple logistic
regression problem. In this way many insights and optimization procedures for
sparse logistic regression can be applied to the learning of discrete Markov
networks. Specifically we use the coordinate descent algorithm for generalized
linear models with convex penalties combined with strong screening rules to
solve the pseudo-likelihood problem with $L_1$ regularization. Our method
offers a substantial speedup without losing any accuracy. Furthermore the
proposed method is more stable than the node-wise logistic regression approach
on unbalanced high-dimensional data when penalized by small regularization
parameters. Thorough numerical experiments on simulated data and real world
data demonstrate the advantages of the proposed method.",Sinong Geng|Zhaobin Kuang|David Page,stat.ML
2017-02-28T17:16:12Z,2017-02-27T12:54:54Z,http://arxiv.org/abs/1702.08259v1,http://arxiv.org/pdf/1702.08259v1,"Fast and Accurate Inference with Adaptive Ensemble Prediction in Image
  Classification with Deep Neural Networks","Ensembling multiple predictions is a widely used technique to improve the
accuracy of various machine learning tasks. In image classification tasks for
example averaging the predictions for multiple patches extracted from the
input image significantly improves accuracy. Using multiple networks trained
independently to make predictions improves accuracy further. One obvious
drawback of the ensembling technique is its higher execution cost during
inference. If we average 100 predictions the execution cost will be 100 times
as high as the cost without the ensemble. This higher cost limits the
real-world use of ensembling even though using it is almost the norm to win
image classification competitions. In this paper we describe a new technique
called adaptive ensemble prediction which achieves the benefits of ensembling
with much smaller additional execution costs. Our observation behind this
technique is that many easy-to-predict inputs do not require ensembling. Hence
we calculate the confidence level of the prediction for each input on the basis
of the probability of the predicted label i.e. the outputs from the softmax
during the ensembling computation. If the prediction for an input reaches a
high enough probability on the basis of the confidence level we stop
ensembling for this input to avoid wasting computation power. We evaluated the
adaptive ensembling by using various datasets and showed that it reduces the
computation time significantly while achieving similar accuracy to the naive
ensembling.",Hiroshi Inoue,cs.LG|cs.CV|stat.ML
2017-02-28T17:16:12Z,2017-02-27T12:03:41Z,http://arxiv.org/abs/1702.08249v1,http://arxiv.org/pdf/1702.08249v1,Uniform Deviation Bounds for Unbounded Loss Functions like k-Means,"Uniform deviation bounds limit the difference between a model's expected loss
and its loss on an empirical sample uniformly for all models in a learning
problem. As such they are a critical component to empirical risk minimization.
In this paper we provide a novel framework to obtain uniform deviation bounds
for loss functions which are *unbounded*. In our main application this allows
us to obtain bounds for $k$-Means clustering under weak assumptions on the
underlying distribution. If the fourth moment is bounded we prove a rate of
$\mathcal{O}\left(m^{-\frac12}\right)$ compared to the previously known
$\mathcal{O}\left(m^{-\frac14}\right)$ rate. Furthermore we show that the rate
also depends on the kurtosis - the normalized fourth moment which measures the
""tailedness"" of a distribution. We further provide improved rates under
progressively stronger assumptions namely bounded higher moments
subgaussianity and bounded support.",Olivier Bachem|Mario Lucic|S. Hamed Hassani|Andreas Krause,stat.ML|cs.LG
2017-02-28T17:16:12Z,2017-02-27T12:03:01Z,http://arxiv.org/abs/1702.08248v1,http://arxiv.org/pdf/1702.08248v1,Scalable and Distributed Clustering via Lightweight Coresets,"Coresets are compact representations of data sets such that models trained on
a coreset are provably competitive with models trained on the full data set. As
such they have been successfully used to scale up clustering models to massive
data sets. While existing approaches generally only allow for multiplicative
approximation errors we propose a novel notion of coresets called lightweight
coresets that allows for both multiplicative and additive errors. We provide a
single algorithm to construct light-weight coresets for k-Means clustering
Bregman clustering and maximum likelihood estimation of Gaussian mixture
models. The algorithm is substantially faster than existing constructions
embarrassingly parallel and resulting coresets are smaller. In an extensive
experimental evaluation we demonstrate that the proposed method outperforms
existing coreset constructions.",Olivier Bachem|Mario Lucic|Andreas Krause,stat.ML|cs.DC|cs.DS|cs.LG|stat.CO
2017-02-28T17:16:12Z,2017-02-27T11:26:33Z,http://arxiv.org/abs/1702.08239v1,http://arxiv.org/pdf/1702.08239v1,"Bayesian inference on random simple graphs with power law degree
  distributions","We present a model for random simple graphs with a degree distribution that
obeys a power law (i.e. is heavy-tailed). To attain this behavior the edge
probabilities in the graph are constructed from Bertoin-Fujita-Roynette-Yor
(BFRY) random variables which have been recently utilized in Bayesian
statistics for the construction of power law models in several applications.
Our construction readily extends to capture the structure of latent factors
similarly to stochastic blockmodels while maintaining its power law degree
distribution. The BFRY random variables are well approximated by gamma random
variables in a variational Bayesian inference routine which we apply to
several network datasets for which power law degree distributions are a natural
assumption. By learning the parameters of the BFRY distribution via
probabilistic inference we are able to automatically select the appropriate
power law behavior from the data. In order to further scale our inference
procedure we adopt stochastic gradient ascent routines where the gradients are
computed on minibatches (i.e. subsets) of the edges in the graph.",Juho Lee|Creighton Heaukulani|Zoubin Ghahramani|Lancelot F. James|Seungjin Choi,stat.ML
2017-02-28T17:16:12Z,2017-02-27T11:16:54Z,http://arxiv.org/abs/1702.08235v1,http://arxiv.org/pdf/1702.08235v1,Variational Inference using Implicit Distributions,"Generative adversarial networks (GANs) have given us a great tool to fit
implicit generative models to data. Implicit distributions are ones we can
sample from easily and take derivatives of samples with respect to model
parameters. These models are highly expressive and we argue they can prove just
as useful for variational inference (VI) as they are for generative modelling.
Several papers have proposed GAN-like algorithms for inference however
connections to the theory of VI are not always well understood. This paper
provides a unifying review of existing algorithms establishing connections
between variational autoencoders adversarially learned inference operator VI
GAN-based image reconstruction and more. Secondly the paper provides a
framework for building new algorithms: depending on the way the variational
bound is expressed we introduce prior-contrastive and joint-contrastive
methods and show practical inference algorithms based on either density ratio
estimation or denoising.",Ferenc Huszár,stat.ML|cs.LG
2017-02-28T17:16:12Z,2017-02-27T10:01:36Z,http://arxiv.org/abs/1702.08211v1,http://arxiv.org/pdf/1702.08211v1,"Online Nonparametric Learning Chaining and the Role of Partial
  Feedback","We investigate contextual online learning with nonparametric (Lipschitz)
comparison classes under different assumptions on losses and feedback
information. For full information feedback and Lipschitz losses we
characterize the minimax regret up to log factors by proving an upper bound
matching a previously known lower bound. In a partial feedback model motivated
by second-price auctions we prove upper bounds for Lipschitz and
semi-Lipschitz losses that improve on the known bounds for standard bandit
feedback. Our analysis combines novel results for contextual second-price
auctions with a novel algorithmic approach based on chaining. When the context
space is Euclidean our chaining approach is efficient and delivers an even
better regret bound.",Nicolò Cesa-Bianchi|Pierre Gaillard|Claudio Gentile|Sébastien Gerchinovitz,stat.ML|cs.LG|math.ST|stat.TH
2017-02-28T17:16:12Z,2017-02-27T08:33:26Z,http://arxiv.org/abs/1702.08185v1,http://arxiv.org/pdf/1702.08185v1,An update on statistical boosting in biomedicine,"Statistical boosting algorithms have triggered a lot of research during the
last decade. They combine a powerful machine-learning approach with classical
statistical modelling offering various practical advantages like automated
variable selection and implicit regularization of effect estimates. They are
extremely flexible as the underlying base-learners (regression functions
defining the type of effect for the explanatory variables) can be combined with
any kind of loss function (target function to be optimized defining the type
of regression setting). In this review article we highlight the most recent
methodological developments on statistical boosting regarding variable
selection functional regression and advanced time-to-event modelling.
Additionally we provide a short overview on relevant applications of
statistical boosting in biomedicine.",Andreas Mayr|Benjamin Hofner|Elisabeth Waldmann|Tobias Hepp|Olaf Gefeller|Matthias Schmid,stat.AP|stat.CO|stat.ML
2017-02-28T17:16:12Z,2017-02-27T04:30:06Z,http://arxiv.org/abs/1702.08142v1,http://arxiv.org/pdf/1702.08142v1,Tensor Balancing on Statistical Manifold,"We solve tensor balancing rescaling an Nth order nonnegative tensor by
multiplying (N - 1)th order N tensors so that every fiber sums to one. This
generalizes a fundamental process of matrix balancing used to compare matrices
in a wide range of applications from biology to economics. We present an
efficient balancing algorithm with quadratic convergence using Newton's method
and show in numerical experiments that the proposed algorithm is several orders
of magnitude faster than existing ones. To theoretically prove the correctness
of the algorithm we model tensors as probability distributions in a
statistical manifold and realize tensor balancing as projection onto a
submanifold. The key to our algorithm is that the gradient of the manifold
used as a Jacobian matrix in Newton's method can be analytically obtained
using the M\""obius inversion formula the essential of combinatorial
mathematics. Our model is not limited to tensor balancing but has a wide
applicability as it includes various statistical and machine learning models
such as weighted DAGs and Boltzmann machines.",Mahito Sugiyama|Hiroyuki Nakahara|Koji Tsuda,stat.ME|cs.IT|cs.NA|math.IT|stat.ML
2017-02-28T17:16:12Z,2017-02-27T03:51:46Z,http://arxiv.org/abs/1702.08134v1,http://arxiv.org/pdf/1702.08134v1,"Online Multiview Representation Learning: Dropping Convexity for Better
  Efficiency","Multiview representation learning is very popular for latent factor analysis.
It naturally arises in many data analysis machine learning and information
retrieval applications to model dependent structures between a pair of data
matrices. For computational convenience existing approaches usually formulate
the multiview representation learning as convex optimization problems where
global optima can be obtained by certain algorithms in polynomial time.
However many evidences have corroborated that heuristic nonconvex approaches
also have good empirical computational performance and convergence to the
global optima although there is a lack of theoretical justification. Such a
gap between theory and practice motivates us to study a nonconvex formulation
for multiview representation learning which can be efficiently solved by two
stochastic gradient descent (SGD) methods. Theoretically by analyzing the
dynamics of the algorithms based on diffusion processes we establish global
rates of convergence to the global optima with high probability. Numerical
experiments are provided to support our theory.",Zhehui Chen|Forest L. Yang|Chris J. Li|Tuo Zhao,cs.LG|math.OC|stat.ML
2017-02-28T17:16:12Z,2017-02-26T10:38:39Z,http://arxiv.org/abs/1702.08019v1,http://arxiv.org/pdf/1702.08019v1,"Support vector machine and its bias correction in high-dimension
  low-sample-size settings","In this paper we consider asymptotic properties of the support vector
machine (SVM) in high-dimension low-sample-size (HDLSS) settings. We show that
the hard-margin linear SVM holds a consistency property in which
misclassification rates tend to zero as the dimension goes to infinity under
certain severe conditions. We show that the SVM is very biased in HDLSS
settings and its performance is affected by the bias directly. In order to
overcome such difficulties we propose a bias-corrected SVM (BC-SVM). We show
that the BC-SVM gives preferable performances in HDLSS settings. We also
discuss the SVMs in multiclass HDLSS settings. Finally we check the
performance of the classifiers in actual data analyses.",Yugo Nakayama|Kazuyoshi Yata|Makoto Aoshima,"stat.ML|cs.LG|62H30, 62G20"
2017-02-28T17:16:16Z,2017-02-26T08:24:11Z,http://arxiv.org/abs/1702.08000v1,http://arxiv.org/pdf/1702.08000v1,"Kiefer Wolfowitz Algorithm is Asymptotically Optimal for a Class of
  Non-Stationary Bandit Problems","We consider the problem of designing an allocation rule or an ""online
learning algorithm"" for a class of bandit problems in which the set of control
actions available at each time $t$ is a convex compact subset of
$\mathbb{R}^d$. Upon choosing an action $x$ at time $t$ the algorithm obtains
a noisy value of the unknown and time-varying function $f_t$ evaluated at $x$.
The ""regret"" of an algorithm is the gap between its expected reward and the
reward earned by a strategy which has the knowledge of the function $f_t$ at
each time $t$ and hence chooses the action $x_t$ that maximizes $f_t$.
  For this non-stationary bandit problem set-up we propose two variants of the
Kiefer Wolfowitz (KW) algorithm i) KW with fixed step-size $\beta$ and ii) KW
with sliding window of length $L$. We show that if the number of times that the
function $f_t$ varies during time $t$ is $o(T)$ and if the learning rates of
the proposed algorithms are chosen ""optimally"" then the regret of the proposed
algorithms is $o(T)$.",Rahul Singh|Taposh Banerjee,stat.ML|cs.LG
2017-02-28T17:16:16Z,2017-02-26T02:14:05Z,http://arxiv.org/abs/1702.07976v1,http://arxiv.org/pdf/1702.07976v1,"Ratio Utility and Cost Analysis for Privacy Preserving Subspace
  Projection","With a rapidly increasing number of devices connected to the internet big
data has been applied to various domains of human life. Nevertheless it has
also opened new venues for breaching users' privacy. Hence it is highly
required to develop techniques that enable data owners to privatize their data
while keeping it useful for intended applications. Existing methods however
do not offer enough flexibility for controlling the utility-privacy trade-off
and may incur unfavorable results when privacy requirements are high. To tackle
these drawbacks we propose a compressive-privacy based method namely RUCA
(Ratio Utility and Cost Analysis) which can not only maximize performance for
a privacy-insensitive classification task but also minimize the ability of any
classifier to infer private information from the data. Experimental results on
Census and Human Activity Recognition data sets demonstrate that RUCA
significantly outperforms existing privacy preserving data projection
techniques for a wide range of privacy pricings.",Mert Al|Shibiao Wan|Sun-Yuan Kung,stat.ML|cs.LG
2017-02-28T17:16:16Z,2017-02-26T01:12:20Z,http://arxiv.org/abs/1702.07966v1,http://arxiv.org/pdf/1702.07966v1,Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs,"Deep learning models are often successfully trained using gradient descent
despite the worst case hardness of the underlying non-convex optimization
problem. The key question is then under what conditions can one prove that
optimization will succeed. Here we provide a strong result of this kind. We
consider a neural net with one hidden layer and a convolutional structure with
no overlap and a ReLU activation function. For this architecture we show that
learning is NP-complete in the general case but that when the input
distribution is Gaussian gradient descent converges to the global optimum in
polynomial time. To the best of our knowledge this is the first global
optimality guarantee of gradient descent on a convolutional neural network with
ReLU activations.",Alon Brutzkus|Amir Globerson,cs.LG|math.OC|stat.ML
2017-02-28T17:16:16Z,2017-02-26T00:17:42Z,http://arxiv.org/abs/1702.07959v1,http://arxiv.org/pdf/1702.07959v1,"Supervised Learning of Labeled Pointcloud Differences via Cover-Tree
  Entropy Reduction","We introduce a new algorithm called CDER for supervised machine learning
that merges the multi-scale geometric properties of Cover Trees with the
information-theoretic properties of entropy. CDER applies to a training set of
labeled pointclouds embedded in a common Euclidean space. If typical
pointclouds corresponding to distinct labels tend to differ at any scale in any
sub-region CDER can identify these differences in (typically) linear time
creating a set of distributional coordinates which act as a feature extraction
mechanism for supervised learning. We describe theoretical properties and
implementation details of CDER and illustrate its benefits on several
synthetic examples.",Abraham Smith|Paul Bendich|John Harer|Jay Hineman,cs.LG|cs.CV|stat.ML
2017-02-28T17:16:16Z,2017-02-25T23:15:55Z,http://arxiv.org/abs/1702.07958v1,http://arxiv.org/pdf/1702.07958v1,"Efficient Online Bandit Multiclass Learning with $\tilde{O}(\sqrt{T})$
  Regret","We present an efficient second-order algorithm with
$\tilde{O}(\frac{1}{\eta}\sqrt{T})$ regret for the bandit online multiclass
problem. The regret bound holds simultaneously with respect to a family of loss
functions parameterized by $\eta$ for a range of $\eta$ restricted by the norm
of the competitor. The family of loss functions ranges from hinge loss
($\eta=0$) to squared hinge loss ($\eta=1$). This provides a solution to the
open problem of (J. Abernethy and A. Rakhlin. An efficient bandit algorithm for
$\sqrt{T}$-regret in online multiclass prediction? In COLT 2009). We test our
algorithm experimentally showing that it also performs favorably against
earlier algorithms.",Alina Beygelzimer|Francesco Orabona|Chicheng Zhang,cs.LG|stat.ML
2017-02-28T17:16:16Z,2017-02-25T22:45:20Z,http://arxiv.org/abs/1702.07956v1,http://arxiv.org/pdf/1702.07956v1,Generative Adversarial Active Learning,"We propose a new active learning approach using Generative Adversarial
Networks (GAN). Different from regular active learning we adaptively
synthesize training instances for querying to increase learning speed. Our
approach outperforms random generation using GAN alone in active learning
experiments. We demonstrate the effectiveness of the proposed algorithm in
various datasets when compared to other algorithms. To the best our knowledge
this is the first active learning work using GAN.",Jia-Jie Zhu|Jose Bento,cs.LG|stat.ML
2017-02-28T17:16:16Z,2017-02-25T20:15:55Z,http://arxiv.org/abs/1702.07944v1,http://arxiv.org/pdf/1702.07944v1,Stochastic Variance Reduction Methods for Policy Evaluation,"Policy evaluation is a crucial step in many reinforcement-learning
procedures which estimates a value function that predicts states' long-term
value under a given policy. In this paper we focus on policy evaluation with
linear function approximation over a fixed dataset. We first transform the
empirical policy evaluation problem into a (quadratic) convex-concave saddle
point problem and then present a primal-dual batch gradient method as well as
two stochastic variance reduction methods for solving the problem. These
algorithms scale linearly in both sample size and feature dimension. Moreover
they achieve linear convergence even when the saddle-point problem has only
strong concavity in the dual variables but no strong convexity in the primal
variables. Numerical experiments on benchmark problems demonstrate the
effectiveness of our methods.",Simon S. Du|Jianshu Chen|Lihong Li|Lin Xiao|Dengyong Zhou,cs.LG|cs.AI|cs.SY|math.OC|stat.ML
2017-02-28T17:16:16Z,2017-02-25T18:00:57Z,http://arxiv.org/abs/1702.07933v1,http://arxiv.org/pdf/1702.07933v1,Efficient Learning of Graded Membership Models,"We present an efficient algorithm for learning graded membership models when
the number of variables $p$ is much larger than the number of hidden components
$k$. This algorithm reduces the computational complexity of state-of-the-art
tensor methods which require decomposing an $O\left(p^3\right)$ tensor to
factorizing $O\left(p/k\right)$ sub-tensors each of size $O\left(k^3\right)$.
In addition we address the issue of negative entries in the empirical method
of moments based estimators. We provide sufficient conditions under which our
approach has provable guarantees. Our approach obtains competitive empirical
results on both simulated and real data.",Zilong Tan|Sayan Mukherjee,cs.LG|stat.ML
2017-02-28T17:16:16Z,2017-02-25T12:50:35Z,http://arxiv.org/abs/1702.07884v1,http://arxiv.org/pdf/1702.07884v1,"An EM Based Probabilistic Two-Dimensional CCA with Application to Face
  Recognition","Recently two-dimensional canonical correlation analysis (2DCCA) has been
successfully applied for image feature extraction. The method instead of
concatenating the columns of the images to the one-dimensional vectors
directly works with two-dimensional image matrices. Although 2DCCA works well
in different recognition tasks it lacks a probabilistic interpretation. In
this paper we present a probabilistic framework for 2DCCA called probabilistic
2DCCA (P2DCCA) and an iterative EM based algorithm for optimizing the
parameters. Experimental results on synthetic and real data demonstrate
superior performance in loading factor estimation for P2DCCA compared to 2DCCA.
For real data three subsets of AR face database and also the UMIST face
database confirm the robustness of the proposed algorithm in face recognition
tasks with different illumination conditions facial expressions poses and
occlusions.",Mehran Safayani|Seyed Hashem Ahmadi|Homayun Afrabandpey|Abdolreza Mirzaei,cs.CV|cs.LG|stat.ML
2017-02-28T17:16:16Z,2017-02-25T05:11:25Z,http://arxiv.org/abs/1702.07834v1,http://arxiv.org/pdf/1702.07834v1,Efficient coordinate-wise leading eigenvector computation,"We develop and analyze efficient ""coordinate-wise"" methods for finding the
leading eigenvector where each step involves only a vector-vector product. We
establish global convergence with overall runtime guarantees that are at least
as good as Lanczos's method and dominate it for slowly decaying spectrum. Our
methods are based on combining a shift-and-invert approach with coordinate-wise
algorithms for linear regression.",Jialei Wang|Weiran Wang|Dan Garber|Nathan Srebro,cs.NA|cs.LG|stat.ML
2017-02-28T17:16:20Z,2017-02-25T00:22:51Z,http://arxiv.org/abs/1702.07811v1,http://arxiv.org/pdf/1702.07811v1,Adaptive Neural Networks for Fast Test-Time Prediction,"We present an approach to adaptively utilize deep neural networks in order to
reduce the evaluation time on new examples without loss of classification
performance. Rather than attempting to redesign or approximate existing
networks we propose two schemes that adaptively utilize networks. First we
pose an adaptive network evaluation scheme where we learn a system to
adaptively choose the components of a deep network to be evaluated for each
example. By allowing examples correctly classified using early layers of the
system to exit we avoid the computational time associated with full evaluation
of the network. Building upon this approach we then learn a network selection
system that adaptively selects the network to be evaluated for each example. We
exploit the fact that many examples can be correctly classified using
relatively efficient networks and that complex computationally costly networks
are only necessary for a small fraction of examples. By avoiding evaluation of
these complex networks for a large fraction of examples computational time can
be dramatically reduced. Empirically these approaches yield dramatic
reductions in computational cost with up to a 2.8x speedup on state-of-the-art
networks from the ImageNet image recognition challenge with minimal (less than
1%) loss of accuracy.",Tolga Bolukbasi|Joseph Wang|Ofer Dekel|Venkatesh Saligrama,cs.LG|cs.CV|cs.NE|stat.ML
2017-02-28T17:16:20Z,2017-02-24T23:43:06Z,http://arxiv.org/abs/1702.07803v1,http://arxiv.org/pdf/1702.07803v1,Nonparanormal Information Estimation,"We study the problem of using i.i.d. samples from an unknown multivariate
probability distribution $p$ to estimate the mutual information of $p$. This
problem has recently received attention in two settings: (1) where $p$ is
assumed to be Gaussian and (2) where $p$ is assumed only to lie in a large
nonparametric smoothness class. Estimators proposed for the Gaussian case
converge in high dimensions when the Gaussian assumption holds but are
brittle failing dramatically when $p$ is not Gaussian. Estimators proposed for
the nonparametric case fail to converge with realistic sample sizes except in
very low dimensions. As a result there is a lack of robust mutual information
estimators for many realistic data. To address this we propose estimators for
mutual information when $p$ is assumed to be a nonparanormal (a.k.a. Gaussian
copula) model a semiparametric compromise between Gaussian and nonparametric
extremes. Using theoretical bounds and experiments we show these estimators
strike a practical balance between robustness and scaling with dimensionality.",Shashank Singh|Barnabás Pøczos,math.ST|cs.IT|math.IT|stat.ML|stat.TH
2017-02-28T17:16:20Z,2017-02-24T23:30:08Z,http://arxiv.org/abs/1702.07800v1,http://arxiv.org/pdf/1702.07800v1,On the Origin of Deep Learning,"This paper is a review of the evolutionary history of deep learning models.
It covers from the genesis of neural networks when associationism modeling of
the brain is studied to the models that dominate the last decade of research
in deep learning like convolutional neural networks deep belief networks and
recurrent neural networks and extends to popular recent models like
variational autoencoder and generative adversarial nets. In addition to a
review of these models this paper primarily focuses on the precedents of the
models above examining how the initial ideas are assembled to construct the
early models and how these preliminary models are developed into their current
forms. Many of these evolutionary paths last more than half a century and have
a diversity of directions. For example CNN is built on prior knowledge of
biological vision system; DBN is evolved from a trade-off of modeling power and
computation complexity of graphical models and many nowadays models are neural
counterparts of ancient linear models. This paper reviews these evolutionary
paths and offers a concise thought flow of how these models are developed and
aims to provide a thorough background for deep learning. More importantly
along with the path this paper summarizes the gist behind these milestones and
proposes many directions to guide the future research of deep learning.",Haohan Wang|Bhiksha Raj|Eric P. Xing,cs.LG|cs.NE|stat.ML
2017-02-28T17:16:20Z,2017-02-24T23:20:03Z,http://arxiv.org/abs/1702.07798v1,http://arxiv.org/pdf/1702.07798v1,Rank-to-engage: New Listwise Approaches to Maximize Engagement,"For many internet businesses presenting a given list of items in an order
that maximizes a certain metric of interest (e.g. click-through-rate average
engagement time etc.) is crucial. We approach the aforementioned task from a
learning-to-rank perspective which reveals a new problem setup. In traditional
learning-to-rank literature it is implicitly assumed that during the training
data generation one has access to the \emph{best or desired} order for the
given list of items. In this work we consider a problem setup where we do not
observe the desired ranking. We present two novel solutions: the first solution
is an extension of already existing listwise learning-to-rank
technique--Listwise maximum likelihood estimation (ListMLE)--while the second
one is a generic machine learning based framework that tackles the problem in
its entire generality. We discuss several challenges associated with this
generic framework and propose a simple \emph{item-payoff} and
\emph{positional-gain} model that addresses these challenges. We provide
training algorithms inference procedures and demonstrate the effectiveness of
the two approaches over traditional ListMLE on synthetic as well as on
real-life setting of ranking news articles for increased dwell time.",Swayambhoo Jain|Akshay Soni|Nikolay Laptev|Yashar Mehdad,stat.ML|cs.LG
2017-02-28T17:16:20Z,2017-02-24T22:30:29Z,http://arxiv.org/abs/1702.07790v1,http://arxiv.org/pdf/1702.07790v1,Activation Ensembles for Deep Neural Networks,"Many activation functions have been proposed in the past but selecting an
adequate one requires trial and error. We propose a new methodology of
designing activation functions within a neural network at each layer. We call
this technique an ""activation ensemble"" because it allows the use of multiple
activation functions at each layer. This is done by introducing additional
variables $\alpha$ at each activation layer of a network to allow for
multiple activation functions to be active at each neuron. By design
activations with larger $\alpha$ values at a neuron is equivalent to having the
largest magnitude. Hence those higher magnitude activations are ""chosen"" by
the network. We implement the activation ensembles on a variety of datasets
using an array of Feed Forward and Convolutional Neural Networks. By using the
activation ensemble we achieve superior results compared to traditional
techniques. In addition because of the flexibility of this methodology we
more deeply explore activation functions and the features that they capture.",Mark Harmon|Diego Klabjan,stat.ML|cs.LG
2017-02-28T17:16:20Z,2017-02-24T22:00:41Z,http://arxiv.org/abs/1702.07780v1,http://arxiv.org/pdf/1702.07780v1,Changing Model Behavior at Test-Time Using Reinforcement Learning,"Machine learning models are often used at test-time subject to constraints
and trade-offs not present at training-time. For example a computer vision
model operating on an embedded device may need to perform real-time inference
or a translation model operating on a cell phone may wish to bound its average
compute time in order to be power-efficient. In this work we describe a
mixture-of-experts model and show how to change its test-time resource-usage on
a per-input basis using reinforcement learning. We test our method on a small
MNIST-based example.",Augustus Odena|Dieterich Lawson|Christopher Olah,stat.ML|cs.LG
2017-02-28T17:16:20Z,2017-02-24T18:59:08Z,http://arxiv.org/abs/1702.07709v1,http://arxiv.org/pdf/1702.07709v1,Computationally Efficient Robust Estimation of Sparse Functionals,"Many conventional statistical procedures are extremely sensitive to seemingly
minor deviations from modeling assumptions. This problem is exacerbated in
modern high-dimensional settings where the problem dimension can grow with and
possibly exceed the sample size. We consider the problem of robust estimation
of sparse functionals and provide a computationally and statistically
efficient algorithm in the high-dimensional setting. Our theory identifies a
unified set of deterministic conditions under which our algorithm guarantees
accurate recovery. By further establishing that these deterministic conditions
hold with high-probability for a wide range of statistical models our theory
applies to many problems of considerable interest including sparse mean and
covariance estimation; sparse linear regression; and sparse generalized linear
models.",Simon S. Du|Sivaraman Balakrishnan|Aarti Singh,stat.ML|cs.DS|cs.LG
2017-02-28T17:16:20Z,2017-02-24T18:28:18Z,http://arxiv.org/abs/1702.07694v1,http://arxiv.org/pdf/1702.07694v1,"Bayes-Optimal Entropy Pursuit for Active Choice-Based Preference
  Learning","We analyze the problem of learning a single user's preferences in an active
learning setting sequentially and adaptively querying the user over a finite
time horizon. Learning is conducted via choice-based queries where the user
selects her preferred option among a small subset of offered alternatives.
These queries have been shown to be a robust and efficient way to learn an
individual's preferences. We take a parametric approach and model the user's
preferences through a linear classifier using a Bayesian prior to encode our
current knowledge of this classifier. The rate at which we learn depends on the
alternatives offered at every time epoch. Under certain noise assumptions we
show that the Bayes-optimal policy for maximally reducing entropy of the
posterior distribution of this linear classifier is a greedy policy and that
this policy achieves a linear lower bound when alternatives can be constructed
from the continuum. Further we analyze a different metric called
misclassification error proving that the performance of the optimal policy
that minimizes misclassification error is bounded below by a linear function of
differential entropy. Lastly we numerically compare the greedy entropy
reduction policy with a knowledge gradient policy under a number of scenarios
examining their performance under both differential entropy and
misclassification error.",Stephen N. Pallone|Peter I. Frazier|Shane G. Henderson,stat.ML|cs.IT|cs.LG|math.IT
2017-02-28T17:16:20Z,2017-02-24T17:40:28Z,http://arxiv.org/abs/1702.07680v1,http://arxiv.org/pdf/1702.07680v1,Consistent Alignment of Word Embedding Models,"Word embedding models offer continuous vector representations that can
capture rich contextual semantics based on their word co-occurrence patterns.
While these word vectors can provide very effective features used in many NLP
tasks such as clustering similar words and inferring learning relationships
many challenges and open research questions remain. In this paper we propose a
solution that aligns variations of the same model (or different models) in a
joint low-dimensional latent space leveraging carefully generated synthetic
data points. This generative process is inspired by the observation that a
variety of linguistic relationships is captured by simple linear operations in
embedded space. We demonstrate that our approach can lead to substantial
improvements in recovering embeddings of local neighborhoods.",Cem Safak Sahin|Rajmonda S. Caceres|Brandon Oselio|William M. Campbell,cs.CL|cs.IR|stat.ML
2017-02-28T17:16:20Z,2017-02-24T17:36:46Z,http://arxiv.org/abs/1702.07679v1,http://arxiv.org/pdf/1702.07679v1,A recommender system to restore images with impulse noise,"We build a collaborative filtering recommender system to restore images with
impulse noise for which the noisy pixels have been previously identified. We
define this recommender system in terms of a new color image representation
using three matrices that depend on the noise-free pixels of the image to
restore and two parameters: $k$ the number of features; and $\lambda$ the
regularization factor. We perform experiments on a well known image database to
test our algorithm and we provide image quality statistics for the results
obtained. We discuss the roles of bias and variance in the performance of our
algorithm as determined by the values of $k$ and $\lambda$ and provide
guidance on how to choose the values of these parameters. Finally we discuss
the possibility of using our collaborative filtering recommender system to
perform image inpainting and super-resolution.",Alfredo Nava-Tudela,cs.CV|stat.ML
2017-02-28T17:16:24Z,2017-02-24T16:32:57Z,http://arxiv.org/abs/1702.07652v1,http://arxiv.org/pdf/1702.07652v1,"Control of Gene Regulatory Networks with Noisy Measurements and
  Uncertain Inputs","This paper is concerned with the problem of stochastic control of gene
regulatory networks (GRNs) observed indirectly through noisy measurements and
with uncertainty in the intervention inputs. The partial observability of the
gene states and uncertainty in the intervention process are accounted for by
modeling GRNs using the partially-observed Boolean dynamical system (POBDS)
signal model with noisy gene expression measurements. Obtaining the optimal
infinite-horizon control strategy for this problem is not attainable in
general and we apply reinforcement learning and Gaussian process techniques to
find a near-optimal solution. The POBDS is first transformed to a
directly-observed Markov Decision Process in a continuous belief space and the
Gaussian process is used for modeling the cost function over the belief and
intervention spaces. Reinforcement learning then is used to learn the cost
function from the available gene expression data. In addition we employ
sparsification which enables the control of large partially-observed GRNs. The
performance of the resulting algorithm is studied through a comprehensive set
of numerical experiments using synthetic gene expression data generated from a
melanoma gene regulatory network.",Mahdi Imani|Ulisses Braga-Neto,q-bio.MN|cs.LG|stat.ML
2017-02-28T17:16:24Z,2017-02-24T15:43:10Z,http://arxiv.org/abs/1702.07630v1,http://arxiv.org/pdf/1702.07630v1,"Inertia-Constrained Pixel-by-Pixel Nonnegative Matrix Factorisation: a
  Hyperspectral Unmixing Method Dealing with Intra-class Variability","Blind source separation is a common processing tool to analyse the
constitution of pixels of hyperspectral images. Such methods usually suppose
that pure pixel spectra (endmembers) are the same in all the image for each
class of materials. In the framework of remote sensing such an assumption is
no more valid in the presence of intra-class variabilities due to illumination
conditions weathering slight variations of the pure materials etc... In this
paper we first describe the results of investigations highlighting intra-class
variability measured in real images. Considering these results a new
formulation of the linear mixing model is presented leading to two new methods.
Unconstrained Pixel-by-pixel NMF (UP-NMF) is a new blind source separation
method based on the assumption of a linear mixing model which can deal with
intra-class variability. To overcome UP-NMF limitations an extended method is
proposed named Inertia-constrained Pixel-by-pixel NMF (IP-NMF). For each
sensed spectrum these extended versions of NMF extract a corresponding set of
source spectra. A constraint is set to limit the spreading of each source's
estimates in IP-NMF. The methods are tested on a semi-synthetic data set built
with spectra extracted from a real hyperspectral image and then numerically
mixed. We thus demonstrate the interest of our methods for realistic source
variabilities. Finally IP-NMF is tested on a real data set and it is shown to
yield better performance than state of the art methods.",Charlotte Revel|Yannick Deville|Véronique Achard|Xavier Briottet,stat.ME|cs.CV|physics.data-an|stat.ML
2017-02-28T17:16:24Z,2017-02-24T14:42:06Z,http://arxiv.org/abs/1702.07608v1,http://arxiv.org/pdf/1702.07608v1,"Microwave breast cancer detection using Empirical Mode Decomposition
  features","Microwave-based breast cancer detection has been proposed as a complementary
approach to compensate for some drawbacks of existing breast cancer detection
techniques. Among the existing microwave breast cancer detection methods
machine learning-type algorithms have recently become more popular. These focus
on detecting the existence of breast tumours rather than performing imaging to
identify the exact tumour position. A key step of the machine learning
approaches is feature extraction. One of the most widely used feature
extraction method is principle component analysis (PCA). However it can be
sensitive to signal misalignment. This paper presents an empirical mode
decomposition (EMD)-based feature extraction method which is more robust to
the misalignment. Experimental results involving clinical data sets combined
with numerically simulated tumour responses show that combined features from
EMD and PCA improve the detection performance with an ensemble selection-based
classifier.",Hongchao Song|Yunpeng Li|Mark Coates|Aidong Men,stat.ML
2017-02-28T17:16:24Z,2017-02-27T11:09:08Z,http://arxiv.org/abs/1702.07552v2,http://arxiv.org/pdf/1702.07552v2,Learning Rates for Kernel-Based Expectile Regression,"Conditional expectiles are becoming an increasingly important tool in finance
as well as in other areas of applications. We analyse a support vector machine
type approach for estimating conditional expectiles and establish learning
rates that are minimax optimal modulo a logarithmic factor if Gaussian RBF
kernels are used and the desired expectile is smooth in a Besov sense. As a
special case our learning rates improve the best known rates for kernel-based
least squares regression in this scenario. Key ingredients of our statistical
analysis are a general calibration inequality for the asymmetric least squares
loss a corresponding variance bound as well as an improved entropy number
bound for Gaussian RBF kernels.",Muhammad Farooq|Ingo Steinwart,stat.ML|cs.LG
2017-02-28T17:16:24Z,2017-02-24T11:55:13Z,http://arxiv.org/abs/1702.07549v1,http://arxiv.org/pdf/1702.07549v1,"The Stochastic complexity of spin models: how simple are simple spin
  models?","Models can be simple for different reasons. In science models are simple
when they depend on few variables they are meant to yield predictions that can
be falsified. Parameters are often mutually constrained within scales. They
spot symmetries and conservation laws specific of the phenomenon under study.
Models in statistical learning are simple because they are easy to evaluate
train and/or to infer. They are simple to interpret in terms of low order
dependencies (e.g. graphical models). Their aim is not to uncover fundamental
laws but to ""generalise well"" i.e. to describe well yet unseen data. We show
that for spin models with interactions of arbitrary order the information
theoretic notion of complexity (or simplicity) provided by Minimum Description
Length (MDL) conforms with the scientific notion of simplicity rather than with
that in machine learning. Simple models in MDL have statistical dependencies
concentrated in groups of few variables they afford predictions on
independencies that are easy to falsify. On the contrary pairwise interacting
models which are often used in statistical learning appear to be rather
complex.",Alberto Beretta|Claudia Battistin|Clélia de Mulatier|Iacopo Mastromatteo|Matteo Marsili,cond-mat.dis-nn|stat.ML
2017-02-28T17:16:24Z,2017-02-24T08:30:43Z,http://arxiv.org/abs/1702.07492v1,http://arxiv.org/pdf/1702.07492v1,"Robot gains Social Intelligence through Multimodal Deep Reinforcement
  Learning","For robots to coexist with humans in a social world like ours it is crucial
that they possess human-like social interaction skills. Programming a robot to
possess such skills is a challenging task. In this paper we propose a
Multimodal Deep Q-Network (MDQN) to enable a robot to learn human-like
interaction skills through a trial and error method. This paper aims to develop
a robot that gathers data during its interaction with a human and learns human
interaction behaviour from the high-dimensional sensory information using
end-to-end reinforcement learning. This paper demonstrates that the robot was
able to learn basic interaction skills successfully after 14 days of
interacting with people.",Ahmed Hussain Qureshi|Yutaka Nakamura|Yuichiro Yoshikawa|Hiroshi Ishiguro,cs.RO|cs.AI|cs.CV|stat.ML
2017-02-28T17:16:24Z,2017-02-24T05:00:37Z,http://arxiv.org/abs/1702.07464v1,http://arxiv.org/pdf/1702.07464v1,"Deep Models Under the GAN: Information Leakage from Collaborative Deep
  Learning","In recent years a branch of machine learning called Deep Learning has become
incredibly popular thanks to the ability of a new class of algorithms to model
and interpret a large quantity of data in a similar way to humans. Properly
training deep learning models involves collecting a vast amount of users'
private data including habits geographical positions interests and much
more. Another major issue is that it is possible to extract from trained models
useful information about the training set and this hinders collaboration among
distrustful participants or parties that deal with sensitive information. To
tackle this problem collaborative deep learning models have recently been
proposed where parties share only a subset of the parameters in the attempt to
keep their respective training sets private. Parameters can also be obfuscated
via differential privacy to make information extraction even more challenging
as shown by Shokri and Shmatikov at CCS'15. Unfortunately we show that any
privacy-preserving collaborative deep learning is susceptible to a powerful
attack that we devise in this paper. In particular we show that a distributed
or decentralized deep learning approach is fundamentally broken and does not
protect the training sets of honest participants. The attack we developed
exploits the real-time nature of the learning process that allows the adversary
to train a Generative Adversarial Network (GAN) that generates valid samples of
the targeted training set that was meant to be private. Interestingly we show
that differential privacy applied to shared parameters of the model as
suggested at CCS'15 and CCS'16 is utterly futile. In our generative model
attack all techniques adopted to scramble or obfuscate shared parameters in
collaborative deep learning are rendered ineffective with no possibility of a
remedy under the threat model considered.",Briland Hitaj|Giuseppe Ateniese|Fernando Perez-Cruz,cs.CR|cs.LG|stat.ML
2017-02-28T17:16:24Z,2017-02-27T01:16:23Z,http://arxiv.org/abs/1702.07463v2,http://arxiv.org/pdf/1702.07463v2,Sequence Modeling via Segmentations,"Segmental structure is a common pattern in many types of sequences such as
phrases in human languages. In this paper we present a probabilistic model for
sequences via their segmentations. The probability of a segmented sequence is
calculated as the product of the probabilities of all its segments where each
segment is modeled using existing tools such as recurrent neural networks.
Since the segmentation of a sequence is usually unknown in advance we sum over
all valid segmentations to obtain the final probability for the sequence. An
efficient dynamic programming algorithm is developed for forward and backward
computations without resorting to any approximation. We demonstrate our
approach on text segmentation and speech recognition tasks. In addition to
quantitative results we also show that our approach can discover meaningful
segments in their respective application contexts.",Chong Wang|Yining Wang|Po-Sen Huang|Abdelrahman Mohamed|Dengyong Zhou|Li Deng,stat.ML|cs.LG
2017-02-28T17:16:24Z,2017-02-24T04:52:30Z,http://arxiv.org/abs/1702.07462v1,http://arxiv.org/pdf/1702.07462v1,Hidden Community Detection in Social Networks,"We introduce a new paradigm that is important for community detection in the
realm of network analysis. Networks contain a set of strong dominant
communities which interfere with the detection of weak natural community
structure. When most of the members of the weak communities also belong to
stronger communities they are extremely hard to be uncovered. We call the weak
communities the hidden community structure.
  We present a novel approach called HICODE (HIdden COmmunity DEtection) that
identifies the hidden community structure as well as the dominant community
structure. By weakening the strength of the dominant structure one can uncover
the hidden structure beneath. Likewise by reducing the strength of the hidden
structure one can more accurately identify the dominant structure. In this
way HICODE tackles both tasks simultaneously.
  Extensive experiments on real-world networks demonstrate that HICODE
outperforms several state-of-the-art community detection methods in uncovering
both the dominant and the hidden structure. In the Facebook university social
networks we find multiple non-redundant sets of communities that are strongly
associated with residential hall year of registration or career position of
the faculties or students while the state-of-the-art algorithms mainly locate
the dominant ground truth category. In the Due to the difficulty of labeling
all ground truth communities in real-world datasets HICODE provides a
promising approach to pinpoint the existing latent communities and uncover
communities for which there is no ground truth. Finding this unknown structure
is an extremely important community detection problem.",Kun He|Yingru Li|Sucheta Soundarajan|John E. Hopcroft,cs.SI|physics.soc-ph|stat.ML
2017-02-28T17:16:24Z,2017-02-23T21:58:27Z,http://arxiv.org/abs/1702.07405v1,http://arxiv.org/pdf/1702.07405v1,"GapTV: Accurate and Interpretable Low-Dimensional Regression and
  Classification","We consider the problem of estimating a regression function in the common
situation where the number of features is small where interpretability of the
model is a high priority and where simple linear or additive models fail to
provide adequate performance. To address this problem we present GapTV an
approach that is conceptually related both to CART and to the more recent CRISP
algorithm a state-of-the-art alternative method for interpretable nonlinear
regression. GapTV divides the feature space into blocks of constant value and
fits the value of all blocks jointly via a convex optimization routine. Our
method is fully data-adaptive in that it incorporates highly robust routines
for tuning all hyperparameters automatically. We compare our approach against
CART and CRISP and demonstrate that GapTV finds a much better trade-off between
accuracy and interpretability.",Wesley Tansey|James G. Scott,stat.ML
2017-02-28T17:16:28Z,2017-02-23T21:37:06Z,http://arxiv.org/abs/1702.07400v1,http://arxiv.org/pdf/1702.07400v1,Horseshoe Regularization for Feature Subset Selection,"Feature subset selection arises in many high-dimensional applications in
machine learning and statistics such as compressed sensing and genomics. The
$\ell_0$ penalty is ideal for this task the caveat being it requires the
NP-hard combinatorial evaluation of all models. A recent area of considerable
interest is to develop efficient algorithms to fit models with a non-convex
$\ell_\gamma$ penalty for $\gamma\in (01)$ which results in sparser models
than the convex $\ell_1$ or lasso penalty but is harder to fit. We propose an
alternative termed the horseshoe regularization penalty for feature subset
selection and demonstrate its theoretical and computational advantages. The
distinguishing feature from existing non-convex optimization approaches is a
full probabilistic representation of the penalty as the negative of the
logarithm of a suitable prior which in turn enables an efficient
expectation-maximization algorithm for optimization and MCMC for uncertainty
quantification. In synthetic and real data the resulting algorithm provides
better statistical performance and the computation requires a fraction of time
of state of the art non-convex solvers.",Anindya Bhadra|Jyotishka Datta|Nicholas G. Polson|Brandon Willard,stat.ML|stat.CO
2017-02-28T17:16:28Z,2017-02-23T21:29:13Z,http://arxiv.org/abs/1702.07398v1,http://arxiv.org/pdf/1702.07398v1,"Deep Nonparametric Estimation of Discrete Conditional Distributions via
  Smoothed Dyadic Partitioning","We present an approach to deep estimation of discrete conditional probability
distributions. Such models have several applications including generative
modeling of audio image and video data. Our approach combines two main
techniques: dyadic partitioning and graph-based smoothing of the discrete
space. By recursively decomposing each dimension into a series of binary splits
and smoothing over the resulting distribution using graph-based trend
filtering we impose a strict structure to the model and achieve much higher
sample efficiency. We demonstrate the advantages of our model through a series
of benchmarks on both synthetic and real-world datasets in some cases reducing
the error by nearly half in comparison to other popular methods in the
literature. All of our models are implemented in Tensorflow and publicly
available $\href{https://github.com/tansey/sdp}{\text{at this url}}$.",Wesley Tansey|Karl Pichotta|James G. Scott,stat.ML
2017-02-28T17:16:28Z,2017-02-23T19:09:19Z,http://arxiv.org/abs/1702.07367v1,http://arxiv.org/pdf/1702.07367v1,"Stochastic Newton and Quasi-Newton Methods for Large Linear
  Least-squares Problems","We describe stochastic Newton and stochastic quasi-Newton approaches to
efficiently solve large linear least-squares problems where the very large data
sets present a significant computational burden (e.g. the size may exceed
computer memory or data are collected in real-time). In our proposed framework
stochasticity is introduced in two different frameworks as a means to overcome
these computational limitations and probability distributions that can exploit
structure and/or sparsity are considered. Theoretical results on consistency of
the approximations for both the stochastic Newton and the stochastic
quasi-Newton methods are provided. The results show in particular that
stochastic Newton iterates in contrast to stochastic quasi-Newton iterates
may not converge to the desired least-squares solution. Numerical examples
including an example from extreme learning machines demonstrate the potential
applications of these methods.",Julianne Chung|Matthias Chung|J. Tanner Slagel|Luis Tenorio,math.NA|cs.NA|stat.ML
2017-02-28T17:16:28Z,2017-02-23T19:02:32Z,http://arxiv.org/abs/1702.07360v1,http://arxiv.org/pdf/1702.07360v1,Neural Decision Trees,"In this paper we propose a synergistic melting of neural networks and
decision trees into a deep hashing neural network (HNN) having a modeling
capability exponential with respect to its number of neurons. We first derive a
soft decision tree named neural decision tree allowing the optimization of
arbitrary decision function at each split node. We then rewrite this soft space
partitioning as a new kind of neural network layer namely the hashing layer
(HL) which can be seen as a generalization of the known soft-max layer. This
HL can easily replace the standard last layer of ANN in any known network
topology and thus can be used after a convolutional or recurrent neural network
for example. We present the modeling capacity of this deep hashing function on
small datasets where one can reach at least equally good results as standard
neural networks by diminishing the number of output neurons. Finally we show
that for the case where the number of output neurons is large the neural
network can mitigate the absence of linear decision boundaries by learning for
each difficult class a collection of not necessarily connected sub-regions of
the space leading to more flexible decision surfaces. Finally the HNN can be
seen as a deep locality sensitive hashing function which can be trained in a
supervised or unsupervised setting as we will demonstrate for classification
and regression problems.",Randall Balestriero,stat.ML|cs.LG
2017-02-28T17:16:28Z,2017-02-23T18:52:31Z,http://arxiv.org/abs/1702.07339v1,http://arxiv.org/pdf/1702.07339v1,A Converse to Banach's Fixed Point Theorem and its CLS Completeness,"Banach's fixed point theorem for contraction maps has been widely used to
analyze the convergence of iterative methods in non-convex problems. It is a
common experience however that iterative maps fail to be globally contracting
under the natural metric in their domain making the applicability of Banach's
theorem limited. We explore how generally we can apply Banach's fixed point
theorem to establish the convergence of iterative methods when pairing it with
carefully designed metrics.
  Our first result is a strong converse of Banach's theorem showing that it is
a universal analysis tool for establishing uniqueness of fixed points and for
bounding the convergence rate of iterative maps to a unique fixed point. In
other words we show that whenever an iterative map globally converges to a
unique fixed point there exists a metric under which the iterative map is
contracting and which can be used to bound the number of iterations until
convergence. We illustrate our approach in the widely used power method
providing a new way of bounding its convergence rate through contraction
arguments.
  We next consider the computational complexity of Banach's fixed point
theorem. Making the proof of our converse theorem constructive we show that
computing a fixed point whose existence is guaranteed by Banach's fixed point
theorem is CLS-complete. We thus provide the first natural complete problem for
the class CLS which was defined in [Daskalakis-Papadimitriou 2011] to capture
the complexity of problems such as P-matrix LCP computing KKT-points and
finding mixed Nash equilibria in congestion and network coordination games.",Constantinos Daskalakis|Christos Tzamos|Manolis Zampetakis,cs.CC|cs.LG|math.GN|stat.ML
2017-02-28T17:16:28Z,2017-02-23T18:09:14Z,http://arxiv.org/abs/1702.07319v1,http://arxiv.org/pdf/1702.07319v1,"Learning to Draw Dynamic Agent Goals with Generative Adversarial
  Networks","We address the problem of designing artificial agents capable of reproducing
human behavior in a competitive game involving dynamic control. Given data
consisting of multiple realizations of inputs generated by pairs of interacting
players we model each agent's actions as governed by a time-varying latent
goal state coupled to a control model. These goals in turn are described as
stochastic processes evolving according to player-specific value functions
depending on the current state of the game. We model these value functions
using generative adversarial networks (GANs) and show that our GAN-based
approach succeeds in producing sample gameplay that captures the rich dynamics
of human agents. The latent goal dynamics inferred and generated by our model
has applications to fields like neuroscience and animal behavior where the
underlying value functions themselves are of theoretical interest.",Shariq Iqbal|John Pearson,q-bio.NC|cs.LG|stat.ML
2017-02-28T17:16:28Z,2017-02-23T17:46:39Z,http://arxiv.org/abs/1702.07306v1,http://arxiv.org/pdf/1702.07306v1,Causal Discovery Using Proxy Variables,"Discovering causal relations is fundamental to reasoning and intelligence. In
particular observational causal discovery algorithms estimate the cause-effect
relation between two random entities $X$ and $Y$ given $n$ samples from
$P(XY)$.
  In this paper we develop a framework to estimate the cause-effect relation
between two static entities $x$ and $y$: for instance an art masterpiece $x$
and its fraudulent copy $y$. To this end we introduce the notion of proxy
variables which allow the construction of a pair of random entities $(AB)$
from the pair of static entities $(xy)$. Then estimating the cause-effect
relation between $A$ and $B$ using an observational causal discovery algorithm
leads to an estimation of the cause-effect relation between $x$ and $y$. For
example our framework detects the causal relation between unprocessed
photographs and their modifications and orders in time a set of shuffled
frames from a video.
  As our main case study we introduce a human-elicited dataset of 10000 pairs
of casually-linked pairs of words from natural language. Our methods discover
75% of these causal relations. Finally we discuss the role of proxy variables
in machine learning as a general tool to incorporate static knowledge into
prediction tasks.",Mateo Rojas-Carulla|Marco Baroni|David Lopez-Paz,stat.ML|cs.LG
2017-02-28T17:16:28Z,2017-02-23T17:46:22Z,http://arxiv.org/abs/1702.07305v1,http://arxiv.org/pdf/1702.07305v1,Online Multiclass Boosting,"Recent work has extended the theoretical analysis of boosting algorithms to
multiclass problems and online settings. However the multiclass extension is
in the batch setting and the online extensions only consider binary
classification. To the best of our knowledge there exists no framework to
analyze online boosting algorithms for multiclass classification. We fill this
gap in the literature by defining and justifying a weak learning condition
for online multiclass boosting. We also provide an algorithm called online
multiclass boost-by-majority to optimally combine weak learners in our setting.",Young Hun Jung|Ambuj Tewari,stat.ML|cs.LG
2017-02-28T17:16:28Z,2017-02-24T11:09:19Z,http://arxiv.org/abs/1702.07274v2,http://arxiv.org/pdf/1702.07274v2,Rotting Bandits,"The Multi-Armed Bandits (MAB) framework highlights the tension between
acquiring new knowledge (Exploration) and leveraging available knowledge
(Exploitation). In the classical MAB problem a decision maker must choose an
arm at each time step upon which she receives a reward. The decision maker's
objective is to maximize her cumulative expected reward over the time horizon.
The MAB problem has been studied extensively specifically under the assumption
of the arms' rewards distributions being stationary or quasi-stationary over
time. We consider a variant of the MAB framework which we termed
\textit{Rotting Bandits} where each arm's expected reward decays as a function
of the number of times it has been pulled. We are motivated by many real-world
scenarios such as online advertising content recommendation crowdsourcing
and more. We present algorithms accompanied by simulations and derive
theoretical guarantees.",Nir Levine|Koby Crammer|Shie Mannor,stat.ML|cs.LG
2017-02-28T17:16:28Z,2017-02-23T15:10:15Z,http://arxiv.org/abs/1702.07254v1,http://arxiv.org/pdf/1702.07254v1,Sobolev Norm Learning Rates for Regularized Least-Squares Algorithm,"Learning rates for regularized least-squares algorithms are in most cases
expressed with respect to the excess risk or equivalently the $L_2$-norm. For
some applications however guarantees with respect to stronger norms such as
the $L_\infty$-norm are desirable. We address this problem by establishing
learning rates for a continuous scale of norms between the $L_2$- and the RKHS
norm. As a byproduct we derive $L_\infty$-norm learning rates and in the case
of Sobolev RKHSs we actually obtain Sobolev norm learning rates which may also
imply $L_\infty$-norm rates for some derivatives. In all cases we do not need
to assume the target function to be contained in the used RKHS. Finally we
show that in many cases the derived rates are minimax optimal.",Simon Fischer|Ingo Steinwart,stat.ML
2017-02-28T17:16:31Z,2017-02-23T13:49:57Z,http://arxiv.org/abs/1702.07211v1,http://arxiv.org/pdf/1702.07211v1,A minimax and asymptotically optimal algorithm for stochastic bandits,"We propose the kl-UCB ++ algorithm for regret minimization in stochastic
bandit models with exponential families of distributions. We prove that it is
simultaneously asymptotically optimal (in the sense of Lai and Robbins' lower
bound) and minimax optimal. This is the first algorithm proved to enjoy these
two properties at the same time. This work thus merges two different lines of
research with simple proofs involving no complexity overhead.",Pierre Ménard|Aurélien Garivier,stat.ML|cs.LG|math.ST|stat.TH
2017-02-28T17:16:31Z,2017-02-23T12:19:31Z,http://arxiv.org/abs/1702.07190v1,http://arxiv.org/pdf/1702.07190v1,"Spectral Clustering using PCKID - A Probabilistic Cluster Kernel for
  Incomplete Data","In this paper we propose PCKID a novel robust kernel function for
spectral clustering specifically designed to handle incomplete data. By
combining posterior distributions of Gaussian Mixture Models for incomplete
data on different scales we are able to learn a kernel for incomplete data
that does not depend on any critical hyperparameters unlike the commonly used
RBF kernel. To evaluate our method we perform experiments on two real
datasets. PCKID outperforms the baseline methods for all fractions of missing
values and in some cases outperforms the baseline methods with up to 25
percentage points.",Sigurd Løkse|Filippo Maria Bianchi|Arnt-Børre Salberg|Robert Jenssen,stat.ML
2017-02-28T17:16:31Z,2017-02-23T12:00:10Z,http://arxiv.org/abs/1702.07186v1,http://arxiv.org/pdf/1702.07186v1,Stability of Topic Modeling via Matrix Factorization,"Topic models can provide us with an insight into the underlying latent
structure of a large corpus of documents. A range of methods have been proposed
in the literature including probabilistic topic models and techniques based on
matrix factorization. However in both cases standard implementations rely on
stochastic elements in their initialization phase which can potentially lead
to different results being generated on the same corpus when using the same
parameter values. This corresponds to the concept of ""instability"" which has
previously been studied in the context of $k$-means clustering. In many
applications of topic modeling this problem of instability is not considered
and topic models are treated as being definitive even though the results may
change considerably if the initialization process is altered. In this paper we
demonstrate the inherent instability of popular topic modeling approaches
using a number of new measures to assess stability. To address this issue in
the context of matrix factorization for topic modeling we propose the use of
ensemble learning strategies. Based on experiments performed on annotated text
corpora we show that a K-Fold ensemble strategy combining both ensembles and
structured initialization can significantly reduce instability while
simultaneously yielding more accurate topic models.",Mark Belford|Brian Mac Namee|Derek Greene,cs.IR|cs.CL|cs.LG|stat.ML
2017-02-28T17:16:31Z,2017-02-23T07:59:22Z,http://arxiv.org/abs/1702.07125v1,http://arxiv.org/pdf/1702.07125v1,Automatic Representation for Lifetime Value Recommender Systems,"Many modern commercial sites employ recommender systems to propose relevant
content to users. While most systems are focused on maximizing the immediate
gain (clicks purchases or ratings) a better notion of success would be the
lifetime value (LTV) of the user-system interaction. The LTV approach considers
the future implications of the item recommendation and seeks to maximize the
cumulative gain over time. The Reinforcement Learning (RL) framework is the
standard formulation for optimizing cumulative successes over time. However RL
is rarely used in practice due to its associated representation optimization
and validation techniques which can be complex. In this paper we propose a new
architecture for combining RL with recommendation systems which obviates the
need for hand-tuned features thus automating the state-space representation
construction process. We analyze the practical difficulties in this formulation
and test our solutions on batch off-line real-world recommendation data.",Assaf Hallak|Yishay Mansour|Elad Yom-Tov,stat.ML|cs.LG
2017-02-28T17:16:31Z,2017-02-23T07:44:43Z,http://arxiv.org/abs/1702.07121v1,http://arxiv.org/pdf/1702.07121v1,Consistent On-Line Off-Policy Evaluation,"The problem of on-line off-policy evaluation (OPE) has been actively studied
in the last decade due to its importance both as a stand-alone problem and as a
module in a policy improvement scheme. However most Temporal Difference (TD)
based solutions ignore the discrepancy between the stationary distribution of
the behavior and target policies and its effect on the convergence limit when
function approximation is applied. In this paper we propose the Consistent
Off-Policy Temporal Difference (COP-TD($\lambda$ $\beta$)) algorithm that
addresses this issue and reduces this bias at some computational expense. We
show that COP-TD($\lambda$ $\beta$) can be designed to converge to the same
value that would have been obtained by using on-policy TD($\lambda$) with the
target policy. Subsequently the proposed scheme leads to a related and
promising heuristic we call log-COP-TD($\lambda$ $\beta$). Both algorithms
have favorable empirical results to the current state of the art on-line OPE
algorithms. Finally our formulation sheds some new light on the recently
proposed Emphatic TD learning.",Assaf Hallak|Shie Mannor,stat.ML|cs.LG
2017-02-28T17:16:31Z,2017-02-23T03:34:07Z,http://arxiv.org/abs/1702.07083v1,http://arxiv.org/pdf/1702.07083v1,Scalable Inference for Nested Chinese Restaurant Process Topic Models,"Nested Chinese Restaurant Process (nCRP) topic models are powerful
nonparametric Bayesian methods to extract a topic hierarchy from a given text
corpus where the hierarchical structure is automatically determined by the
data. Hierarchical Latent Dirichlet Allocation (hLDA) is a popular instance of
nCRP topic models. However hLDA has only been evaluated at small scale
because the existing collapsed Gibbs sampling and instantiated weight
variational inference algorithms either are not scalable or sacrifice inference
quality with mean-field assumptions. Moreover an efficient distributed
implementation of the data structures such as dynamically growing count
matrices and trees is challenging.
  In this paper we propose a novel partially collapsed Gibbs sampling (PCGS)
algorithm which combines the advantages of collapsed and instantiated weight
algorithms to achieve good scalability as well as high model quality. An
initialization strategy is presented to further improve the model quality.
Finally we propose an efficient distributed implementation of PCGS through
vectorization pre-processing and a careful design of the concurrent data
structures and communication strategy.
  Empirical studies show that our algorithm is 111 times more efficient than
the previous open-source implementation for hLDA with comparable or even
better model quality. Our distributed implementation can extract 1722 topics
from a 131-million-document corpus with 28 billion tokens which is 4-5 orders
of magnitude larger than the previous largest corpus with 50 machines in 7
hours.",Jianfei Chen|Jun Zhu|Jie Lu|Shixia Liu,stat.ML|cs.DC|cs.IR|cs.LG
2017-02-28T17:16:31Z,2017-02-23T02:08:51Z,http://arxiv.org/abs/1702.07066v1,http://arxiv.org/pdf/1702.07066v1,"A Unified Parallel Algorithm for Regularized Group PLS Scalable to Big
  Data","Partial Least Squares (PLS) methods have been heavily exploited to analyse
the association between two blocs of data. These powerful approaches can be
applied to data sets where the number of variables is greater than the number
of observations and in presence of high collinearity between variables.
Different sparse versions of PLS have been developed to integrate multiple data
sets while simultaneously selecting the contributing variables. Sparse
modelling is a key factor in obtaining better estimators and identifying
associations between multiple data sets. The cornerstone of the sparsity
version of PLS methods is the link between the SVD of a matrix (constructed
from deflated versions of the original matrices of data) and least squares
minimisation in linear regression. We present here an accurate description of
the most popular PLS methods alongside their mathematical proofs. A unified
algorithm is proposed to perform all four types of PLS including their
regularised versions. Various approaches to decrease the computation time are
offered and we show how the whole procedure can be scalable to big data sets.",Pierre Lafaye de Micheaux|Benoit Liquet|Matthew Sutton,stat.ML
2017-02-28T17:16:31Z,2017-02-22T21:54:12Z,http://arxiv.org/abs/1702.07021v1,http://arxiv.org/pdf/1702.07021v1,One Size Fits Many: Column Bundle for Multi-X Learning,"Much recent machine learning research has been directed towards leveraging
shared statistics among labels instances and data views commonly referred to
as multi-label multi-instance and multi-view learning. The underlying premises
are that there exist correlations among input parts and among output targets
and the predictive performance would increase when the correlations are
incorporated. In this paper we propose Column Bundle (CLB) a novel deep
neural network for capturing the shared statistics in data. CLB is generic that
the same architecture can be applied for various types of shared statistics by
changing only input and output handling. CLB is capable of scaling to thousands
of input parts and output labels by avoiding explicit modeling of pairwise
relations. We evaluate CLB on different types of data: (a) multi-label (b)
multi-view (c) multi-view/multi-label and (d) multi-instance. CLB demonstrates
a comparable and competitive performance in all datasets against
state-of-the-art methods designed specifically for each type.",Trang Pham|Truyen Tran|Svetha Venkatesh,stat.ML|cs.LG
2017-02-28T17:16:31Z,2017-02-22T21:41:05Z,http://arxiv.org/abs/1702.07013v1,http://arxiv.org/pdf/1702.07013v1,Learning Hawkes Processes from Short Doubly-Censored Event Sequences,"Many real-world applications require robust algorithms to learn point process
models based on a type of incomplete data --- the so-called short
doubly-censored (SDC) event sequences. In this paper we study this critical
problem of quantitative asynchronous event sequence analysis under the
framework of Hawkes processes by leveraging the general idea of data synthesis.
In particular given SDC event sequences observed in a variety of time
intervals we propose a sampling-stitching data synthesis method --- sampling
predecessor and successor for each SDC event sequence from potential candidates
and stitching them together to synthesize long training sequences. The
rationality and the feasibility of our method are discussed in terms of
arguments based on likelihood. Experiments on both synthetic and real-world
data demonstrate that the proposed data synthesis method improves learning
results indeed for both time-invariant and time-varying Hawkes processes.",Hongteng Xu|Dixin Luo|Hongyuan Zha,cs.LG|math.PR|stat.ML
2017-02-28T17:16:31Z,2017-02-22T19:36:25Z,http://arxiv.org/abs/1702.06980v1,http://arxiv.org/pdf/1702.06980v1,On Polynomial Time Methods for Exact Low Rank Tensor Completion,"In this paper we investigate the sample size requirement for exact recovery
of a high order tensor of low rank from a subset of its entries. We show that a
gradient descent algorithm with initial value obtained from a spectral method
can in particular reconstruct a ${d\times d\times d}$ tensor of multilinear
ranks $(rrr)$ with high probability from as few as
$O(r^{7/2}d^{3/2}\log^{7/2}d+r^7d\log^6d)$ entries. In the case when the ranks
$r=O(1)$ our sample size requirement matches those for nuclear norm
minimization (Yuan and Zhang 2016a) or alternating least squares assuming
orthogonal decomposability (Jain and Oh 2014). Unlike these earlier
approaches however our method is efficient to compute easy to implement and
does not impose extra structures on the tensor. Numerical results are presented
to further demonstrate the merits of the proposed approach.",Dong Xia|Ming Yuan,stat.ML|cs.IT|cs.LG|math.IT
2017-02-28T17:16:35Z,2017-02-22T19:27:38Z,http://arxiv.org/abs/1702.06976v1,http://arxiv.org/pdf/1702.06976v1,Heavy-Tailed Analogues of the Covariance Matrix for ICA,"Independent Component Analysis (ICA) is the problem of learning a square
matrix $A$ given samples of $X=AS$ where $S$ is a random vector with
independent coordinates. Most existing algorithms are provably efficient only
when each $S_i$ has finite and moderately valued fourth moment. However there
are practical applications where this assumption need not be true such as
speech and finance. Algorithms have been proposed for heavy-tailed ICA but
they are not practical using random walks and the full power of the ellipsoid
algorithm multiple times. The main contributions of this paper are:
  (1) A practical algorithm for heavy-tailed ICA that we call HTICA. We provide
theoretical guarantees and show that it outperforms other algorithms in some
heavy-tailed regimes both on real and synthetic data. Like the current
state-of-the-art the new algorithm is based on the centroid body (a first
moment analogue of the covariance matrix). Unlike the state-of-the-art our
algorithm is practically efficient. To achieve this we use explicit analytic
representations of the centroid body which bypasses the use of the ellipsoid
method and random walks.
  (2) We study how heavy tails affect different ICA algorithms including
HTICA. Somewhat surprisingly we show that some algorithms that use the
covariance matrix or higher moments can successfully solve a range of ICA
instances with infinite second moment. We study this theoretically and
experimentally with both synthetic and real-world heavy-tailed data.",Joseph Anderson|Navin Goyal|Anupama Nandi|Luis Rademacher,cs.LG|stat.ML
2017-02-28T17:16:35Z,2017-02-22T19:22:55Z,http://arxiv.org/abs/1702.06972v1,http://arxiv.org/pdf/1702.06972v1,Approximations of the Restless Bandit Problem,"The multi-armed restless bandit problem is studied in the case where the
pay-offs are not necessarily independent over time nor across the arms. Even
though this version of the problem provides a more realistic model for most
real-world applications it cannot be optimally solved in practice since it is
known to be PSPACE-hard. The objective of this paper is to characterize special
sub-classes of the problem where good approximate solutions can be found using
tractable approaches. Specifically it is shown that in the case where the
joint distribution over the arms is $\varphi$-mixing and under some conditions
on the $\varphi$-mixing coefficients a modified version of UCB can prove
optimal. On the other hand it is shown that when the pay-off distributions are
strongly dependent simple switching strategies may be devised which leverage
the strong inter-dependencies. To this end an example is provided using
Gaussian Processes. The techniques developed in this paper apply more
generally to the problem of online sampling under dependence.",Steffen Grunewalder|Azadeh Khaleghi,math.ST|cs.LG|math.PR|stat.ML|stat.TH
2017-02-28T17:16:35Z,2017-02-22T18:58:25Z,http://arxiv.org/abs/1702.06943v1,http://arxiv.org/pdf/1702.06943v1,"When Lempel-Ziv-Welch Meets Machine Learning: A Case Study of
  Accelerating Machine Learning using Coding","In this paper we study the use of coding techniques to accelerate machine
learning (ML). Coding techniques such as prefix codes have been extensively
studied and used to accelerate low-level data processing primitives such as
scans in a relational database system. However there is little work on how to
exploit them to accelerate ML algorithms. In fact applying coding techniques
for faster ML faces a unique challenge: one needs to consider both how the
codes fit into the optimization algorithm used to train a model and the
interplay between the model sstructure and the coding scheme. Surprisingly and
intriguingly our study demonstrates that a slight variant of the classical
Lempel-Ziv-Welch (LZW) coding scheme is a good fit for several popular ML
algorithms resulting in substantial runtime savings. Comprehensive experiments
on several real-world datasets show that our LZW-based ML algorithms exhibit
speedups of up to 31x compared to a popular and state-of-the-art ML library
with no changes to ML accuracy even though the implementations of our LZW
variants are not heavily tuned. Thus our study reveals a new avenue for
accelerating ML algorithms using coding techniques and we hope this opens up a
new direction for more research.",Fengan Li|Lingjiao Chen|Arun Kumar|Jeffrey F. Naughton|Jignesh M. Patel|Xi Wu,cs.LG|cs.DB|stat.ML
2017-02-28T17:16:35Z,2017-02-22T18:06:13Z,http://arxiv.org/abs/1702.06921v1,http://arxiv.org/pdf/1702.06921v1,Distributed Representation of Subgraphs,"Network embeddings have become very popular in learning effective feature
representations of networks. Motivated by the recent successes of embeddings in
natural language processing researchers have tried to find network embeddings
in order to exploit machine learning algorithms for mining tasks like node
classification and edge prediction. However most of the work focuses on
finding distributed representations of nodes which are inherently ill-suited
to tasks such as community detection which are intuitively dependent on
subgraphs.
  Here we propose sub2vec an unsupervised scalable algorithm to learn feature
representations of arbitrary subgraphs. We provide means to characterize
similarties between subgraphs and provide theoretical analysis of sub2vec and
demonstrate that it preserves the so-called local proximity. We also highlight
the usability of sub2vec by leveraging it for network mining tasks like
community detection. We show that sub2vec gets significant gains over
state-of-the-art methods and node-embedding methods. In particular sub2vec
offers an approach to generate a richer vocabulary of features of subgraphs to
support representation and reasoning.",Bijaya Adhikari|Yao Zhang|Naren Ramakrishnan|B. Aditya Prakash,cs.SI|cs.LG|stat.ML
2017-02-28T17:16:35Z,2017-02-22T17:55:09Z,http://arxiv.org/abs/1702.06917v1,http://arxiv.org/pdf/1702.06917v1,Bandit Optimization with Upper-Confidence Frank-Wolfe,"We consider the problem of bandit optimization inspired by stochastic
optimization and online learning problems with bandit feedback. In this
problem the objective is to minimize a global loss function of all the
actions not necessarily a cumulative loss. This framework allows us to study a
very general class of problems with applications in statistics machine
learning and other fields. To solve this problem we introduce the
Upper-Confidence Frank-Wolfe algorithm inspired by techniques for bandits and
convex optimization. We show upper bounds on the optimization error of this
algorithm over various classes of functions and discuss the optimality of
these results.",Quentin Berthet|Vianney Perchet,cs.LG|math.OC|stat.ML
2017-02-28T17:16:35Z,2017-02-22T17:17:26Z,http://arxiv.org/abs/1702.06899v1,http://arxiv.org/pdf/1702.06899v1,liquidSVM: A Fast and Versatile SVM package,"liquidSVM is a package written in C++ that provides SVM-type solvers for
various classification and regression tasks. Because of a fully integrated
hyper-parameter selection very carefully implemented solvers multi-threading
and GPU support and several built-in data decomposition strategies it provides
unprecedented speed for small training sizes as well as for data sets of tens
of millions of samples. Besides the C++ API and a command line interface
bindings to R MATLAB Java Python and Spark are available. We present a
brief description of the package and report experimental comparisons to other
SVM packages.",Ingo Steinwart|Philipp Thomann,stat.ML|cs.LG
2017-02-28T17:16:35Z,2017-02-22T16:45:48Z,http://arxiv.org/abs/1702.06890v1,http://arxiv.org/pdf/1702.06890v1,"Learning Deep Features via Congenerous Cosine Loss for Person
  Recognition","Person recognition aims at recognizing the same identity across time and
space with complicated scenes and similar appearance. In this paper we propose
a novel method to address this task by training a network to obtain robust and
representative features. A key observation is that traditional cross entropy
loss only enforces the inter-class variation among samples and ignores to
narrow down the similarity within each category. We propose a congenerous
cosine loss to enlarge the inter-class distinction as well as alleviate the
inner-class variance. Such a design is achieved by minimizing the cosine
distance between sample and its cluster centroid in a cooperative way. Our
method differs from previous work in person recognition that we do not conduct
a second training on the test subset and thus maintain a good generalization
ability. The identity of a person is determined by measuring the similarity
from several body regions in the reference set. Experimental results show that
the proposed approach achieves better classification accuracy against previous
state-of-the-arts.",Yu Liu|Hongyang Li|Xiaogang Wang,cs.CV|cs.LG|stat.ML
2017-02-28T17:16:35Z,2017-02-22T16:28:11Z,http://arxiv.org/abs/1702.06879v1,http://arxiv.org/pdf/1702.06879v1,Knowledge Graph Completion via Complex Tensor Factorization,"In statistical relational learning knowledge graph completion deals with
automatically understanding the structure of large knowledge graphs---labeled
directed graphs---and predicting missing relationships---labeled edges.
State-of-the-art embedding models propose different trade-offs between modeling
expressiveness and time and space complexity. We reconcile both expressiveness
and complexity through the use of complex-valued embeddings and explore the
link between such complex-valued embeddings and unitary diagonalization. We
corroborate our approach theoretically and show that all real square
matrices---thus all possible relation/adjacency matrices---are the real part of
some unitarily diagonalizable matrix. This results opens the door to a lot of
other applications of square matrices factorization. Our approach based on
complex embeddings is arguably simple as it only involves a Hermitian dot
product the complex counterpart of the standard dot product between real
vectors whereas other methods resort to more and more complicated composition
functions to increase their expressiveness. The proposed complex embeddings are
scalable to large data sets as it remains linear in both space and time while
consistently outperforming alternative approaches on standard link prediction
benchmarks.",Théo Trouillon|Christopher R. Dance|Johannes Welbl|Sebastian Riedel|Éric Gaussier|Guillaume Bouchard,cs.AI|cs.LG|math.SP|stat.ML
2017-02-28T17:16:35Z,2017-02-22T15:43:15Z,http://arxiv.org/abs/1702.06861v1,http://arxiv.org/pdf/1702.06861v1,"On the Power of Truncated SVD for General High-rank Matrix Estimation
  Problems","We show that given an estimate $\widehat{A}$ that is close to a general
high-rank positive semi-definite (PSD) matrix $A$ in spectral norm (i.e.
$\|\widehat{A}-A\|_2 \leq \delta$) the simple truncated SVD of $\widehat{A}$
produces a multiplicative approximation of $A$ in Frobenius norm. This
observation leads to many interesting results on general high-rank matrix
estimation problems which we briefly summarize below ($A$ is an $n\times n$
high-rank PSD matrix and $A_k$ is the best rank-$k$ approximation of $A$):
  (1) High-rank matrix completion: By observing
$\Omega(\frac{n\max\{\epsilon^{-4}k^2\}\mu_0^2\|A\|_F^2\log
n}{\sigma_{k+1}(A)^2})$ elements of $A$ where $\sigma_{k+1}\left(A\right)$ is
the $\left(k+1\right)$-th singular value of $A$ and $\mu_0$ is the incoherence
the truncated SVD on a zero-filled matrix satisfies $\|\widehat{A}_k-A\|_F \leq
(1+O(\epsilon))\|A-A_k\|_F$ with high probability.
  (2)High-rank matrix de-noising: Let $\widehat{A}=A+E$ where $E$ is a Gaussian
random noise matrix with zero mean and $\nu^2/n$ variance on each entry. Then
the truncated SVD of $\widehat{A}$ satisfies $\|\widehat{A}_k-A\|_F \leq
(1+O(\sqrt{\nu/\sigma_{k+1}(A)}))\|A-A_k\|_F + O(\sqrt{k}\nu)$.
  (3) Low-rank Estimation of high-dimensional covariance: Given $N$
i.i.d.~samples $X_1\cdotsX_N\sim\mathcal N_n(0A)$ can we estimate $A$ with
a relative-error Frobenius norm bound? We show that if $N =
\Omega\left(n\max\{\epsilon^{-4}k^2\}\gamma_k(A)^2\log N\right)$ for
$\gamma_k(A)=\sigma_1(A)/\sigma_{k+1}(A)$ then $\|\widehat{A}_k-A\|_F \leq
(1+O(\epsilon))\|A-A_k\|_F$ with high probability where
$\widehat{A}=\frac{1}{N}\sum_{i=1}^N{X_iX_i^\top}$ is the sample covariance.",Simon S. Du|Yining Wang|Aarti Singh,stat.ML|cs.LG|math.NA
2017-02-28T17:16:35Z,2017-02-22T15:23:10Z,http://arxiv.org/abs/1702.06838v1,http://arxiv.org/pdf/1702.06838v1,"Sketchy Decisions: Convex Low-Rank Matrix Optimization with Optimal
  Storage","This paper concerns a fundamental class of convex matrix optimization
problems. It presents the first algorithm that uses optimal storage and
provably computes a low-rank approximation of a solution. In particular when
all solutions have low rank the algorithm converges to a solution. This
algorithm SketchyCGM modifies a standard convex optimization scheme the
conditional gradient method to store only a small randomized sketch of the
matrix variable. After the optimization terminates the algorithm extracts a
low-rank approximation of the solution from the sketch. In contrast to
nonconvex heuristics the guarantees for SketchyCGM do not rely on statistical
models for the problem data. Numerical work demonstrates the benefits of
SketchyCGM over heuristics.",Alp Yurtsever|Madeleine Udell|Joel A. Tropp|Volkan Cevher,math.OC|stat.ML
2017-02-28T17:16:39Z,2017-02-22T15:11:25Z,http://arxiv.org/abs/1702.06832v1,http://arxiv.org/pdf/1702.06832v1,Adversarial examples for generative models,"We explore methods of producing adversarial examples on deep generative
models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning
architectures are known to be vulnerable to adversarial examples but previous
work has focused on the application of adversarial examples to classification
tasks. Deep generative models have recently become popular due to their ability
to model input data distributions and generate realistic examples from those
distributions. We present three classes of attacks on the VAE and VAE-GAN
architectures and demonstrate them against networks trained on MNIST SVHN and
CelebA. Our first attack leverages classification-based adversaries by
attaching a classifier to the trained encoder of the target generative model
which can then be used to indirectly manipulate the latent representation. Our
second attack directly uses the VAE loss function to generate a target
reconstruction image from the adversarial example. Our third attack moves
beyond relying on classification or the standard loss for the gradient and
directly optimizes against differences in source and target latent
representations. We also motivate why an attacker might be interested in
deploying such techniques against a target generative network.",Jernej Kos|Ian Fischer|Dawn Song,stat.ML|cs.LG
2017-02-28T17:16:39Z,2017-02-22T14:51:48Z,http://arxiv.org/abs/1702.06819v1,http://arxiv.org/pdf/1702.06819v1,SIGNet: Scalable Embeddings for Signed Networks,"Recent successes in word embedding and document embedding have motivated
researchers to explore similar representations for networks and to use such
representations for tasks such as edge prediction node label prediction and
community detection. Existing methods are largely focused on finding
distributed representations for unsigned networks and are unable to discover
embeddings that respect polarities inherent in edges. We propose SIGNet a fast
scalable embedding method suitable for signed networks. Our proposed objective
function aims to carefully model the social structure implicit in signed
networks by reinforcing the principles of social balance theory. Our method
builds upon the traditional word2vec family of embedding approaches but we
propose a new targeted node sampling strategy to maintain structural balance in
higher-order neighborhoods. We demonstrate the superiority of SIGNet over
state-of-the-art methods proposed for both signed and unsigned networks on
several real world datasets from different domains. In particular SIGNet
offers an approach to generate a richer vocabulary of features of signed
networks to support representation and reasoning.",Mohammad Raihanul Islam|B. Aditya Prakash|Naren Ramakrishnan,stat.ML|cs.LG|cs.SI
2017-02-28T17:16:39Z,2017-02-22T14:45:02Z,http://arxiv.org/abs/1702.06818v1,http://arxiv.org/pdf/1702.06818v1,Stochastic Approximation for Canonical Correlation Analysis,"We study canonical correlation analysis (CCA) as a stochastic optimization
problem. We show that regularized CCA is efficiently PAC-learnable. We give
stochastic approximation (SA) algorithms that are instances of stochastic
mirror descent which achieve $\epsilon$-suboptimality in the population
objective in time $\operatorname{poly}(\frac{1}{\epsilon}\frac{1}{\delta}d)$
with probability $1-\delta$ where $d$ is the input dimensionality.",Raman Arora|Teodor V. Marinov|Poorya Mianjy,cs.LG|stat.ML
2017-02-28T17:16:39Z,2017-02-22T11:37:49Z,http://arxiv.org/abs/1702.06760v1,http://arxiv.org/pdf/1702.06760v1,Memory Matching Networks for Genomic Sequence Classification,"When analyzing the genome researchers have discovered that proteins bind to
DNA based on certain patterns of the DNA sequence known as ""motifs"". However
it is difficult to manually construct motifs due to their complexity. Recently
externally learned memory models have proven to be effective methods for
reasoning over inputs and supporting sets. In this work we present memory
matching networks (MMN) for classifying DNA sequences as protein binding sites.
Our model learns a memory bank of encoded motifs which are dynamic memory
modules and then matches a new test sequence to each of the motifs to classify
the sequence as a binding or nonbinding site.",Jack Lanchantin|Ritambhara Singh|Yanjun Qi,cs.LG|q-bio.GN|stat.ML
2017-02-28T17:16:39Z,2017-02-22T04:50:47Z,http://arxiv.org/abs/1702.06676v1,http://arxiv.org/pdf/1702.06676v1,Counterfactual Control for Free from Generative Models,"We introduce a method by which a generative model learning the joint
distribution between actions and future states can be used to automatically
infer a control scheme for any desired reward function which may be altered on
the fly without retraining the model. In this method the problem of action
selection is reduced to one of gradient descent on the latent space of the
generative model with the model itself providing the means of evaluating
outcomes and finding the gradient much like how the reward network in Deep
Q-Networks (DQN) provides gradient information for the action generator. Unlike
DQN or Actor-Critic which are conditional models for a specific reward using
a generative model of the full joint distribution permits the reward to be
changed on the fly. In addition the generated futures can be inspected to gain
insight in to what the network 'thinks' will happen and to what went wrong
when the outcomes deviate from prediction.",Nicholas Guttenberg|Yen Yu|Ryota Kanai,cs.LG|stat.ML|68T05
2017-02-28T17:16:39Z,2017-02-22T03:10:41Z,http://arxiv.org/abs/1702.06661v1,http://arxiv.org/pdf/1702.06661v1,"Social Learning and Diffusion of Pervasive Goods: An Empirical Study of
  an African App Store","In this study the authors develop a structural model that combines a macro
diffusion model with a micro choice model to control for the effect of social
influence on the mobile app choices of customers over app stores. Social
influence refers to the density of adopters within the proximity of other
customers. Using a large data set from an African app store and Bayesian
estimation methods the authors quantify the effect of social influence and
investigate the impact of ignoring this process in estimating customer choices.
The findings show that customer choices in the app store are explained better
by offline than online density of adopters and that ignoring social influence
in estimations results in biased estimates. Furthermore the findings show that
the mobile app adoption process is similar to adoption of music CDs among all
other classic economy goods. A counterfactual analysis shows that the app store
can increase its revenue by 13.6% through a viral marketing policy (e.g. a
sharing with friends and family button).",Meisam Hejazi Nia|Brian T. Ratchford|Norris Bruce,stat.ML|cs.SI|stat.AP
2017-02-28T17:16:39Z,2017-02-21T22:05:13Z,http://arxiv.org/abs/1702.06602v1,http://arxiv.org/pdf/1702.06602v1,Exemplar-Centered Supervised Shallow Parametric Data Embedding,"Metric learning methods for dimensionality reduction in combination with
k-Nearest Neighbors (kNN) have been extensively deployed in many
classification data embedding and information retrieval applications.
However most of these approaches involve pairwise training data comparisons
and thus have quadratic computational complexity with respect to the size of
training set preventing them from scaling to fairly big datasets. Moreover
during testing comparing test data against all the training data points is
also expensive in terms of both computational cost and resources required.
Furthermore previous metrics are either too constrained or too expressive to
be well learned. To effectively solve these issues we present an
exemplar-centered supervised shallow parametric data embedding model using a
Maximally Collapsing Metric Learning (MCML) objective. Our strategy learns a
shallow high-order parametric embedding function and compares training/test
data only with learned or precomputed exemplars resulting in a cost function
with linear computational complexity for both training and testing. We also
empirically demonstrate using several benchmark datasets that for
classification in two-dimensional embedding space our approach not only gains
speedup of kNN by hundreds of times but also outperforms state-of-the-art
supervised embedding approaches.",Martin Renqiang Min|Hongyu Guo|Dongjin Song,cs.LG|stat.ML
2017-02-28T17:16:39Z,2017-02-21T18:56:42Z,http://arxiv.org/abs/1702.06525v1,http://arxiv.org/pdf/1702.06525v1,A Nonconvex Free Lunch for Low-Rank plus Sparse Matrix Recovery,"We study the problem of low-rank plus sparse matrix recovery. We propose a
generic and efficient nonconvex optimization algorithm based on projected
gradient descent and double thresholding operator with much lower
computational complexity. Compared with existing convex-relaxation based
methods the proposed algorithm recovers the low-rank plus sparse matrices for
free without incurring any additional statistical cost. It not only enables
exact recovery of the unknown low-rank and sparse matrices in the noiseless
setting and achieves minimax optimal statistical error rate in the noisy case
but also matches the best-known robustness guarantee (i.e. tolerance for
sparse corruption). At the core of our theory is a novel structural Lipschitz
gradient condition for low-rank plus sparse matrices which is essential for
proving the linear convergence rate of our algorithm and we believe is of
independent interest to prove fast rates for general superposition-structured
models. We demonstrate the superiority of our generic algorithm both
theoretically and experimentally through three concrete applications: robust
matrix sensing robust PCA and one-bit matrix decomposition.",Xiao Zhang|Lingxiao Wang|Quanquan Gu,stat.ML
2017-02-28T17:16:39Z,2017-02-21T18:39:54Z,http://arxiv.org/abs/1702.06516v1,http://arxiv.org/pdf/1702.06516v1,"A data-driven basis for direct estimation of functionals of
  distributions","A number of fundamental quantities in statistical signal processing and
information theory can be expressed as integral functions of two probability
density functions. Such quantities are called density functionals as they map
density functions onto the real line. For example information divergence
functions measure the dissimilarity between two probability density functions
and are particularly useful in a number of applications. Typically estimating
these quantities requires complete knowledge of the underlying distribution
followed by multi-dimensional integration. Existing methods make parametric
assumptions about the data distribution or use non-parametric density
estimation followed by high-dimensional integration. In this paper we propose
a new alternative. We introduce the concept of ""data-driven"" basis functions -
functions of distributions whose value we can estimate given only samples from
the underlying distributions without requiring distribution fitting or direct
integration. We derive a new data-driven complete basis that is similar to the
deterministic Bernstein polynomial basis and develop two methods for performing
basis expansions of functionals of two distributions. We also show that the new
basis set allows us to approximate functions of distributions as closely as
desired. Finally we evaluate the methodology by developing data driven
estimators for the Kullback-Leibler divergences and the Hellinger distance and
by constructing tight data-driven bounds on the Bayes Error Rate.",Alan Wisler|Visar Berisha|Andreas Spanias|Alfred O. Hero,cs.IT|math.IT|stat.ML
2017-02-28T17:16:39Z,2017-02-21T16:04:10Z,http://arxiv.org/abs/1702.06457v1,http://arxiv.org/pdf/1702.06457v1,"A Unified Optimization View on Generalized Matching Pursuit and
  Frank-Wolfe","Two of the most fundamental prototypes of greedy optimization are the
matching pursuit and Frank-Wolfe algorithms. In this paper we take a unified
view on both classes of methods leading to the first explicit convergence
rates of matching pursuit methods in an optimization sense for general sets of
atoms. We derive sublinear ($1/t$) convergence for both classes on general
smooth objectives and linear convergence on strongly convex objectives as
well as a clear correspondence of algorithm variants. Our presented algorithms
and rates are affine invariant and do not need any incoherence or sparsity
assumptions.",Francesco Locatello|Rajiv Khanna|Michael Tschannen|Martin Jaggi,cs.LG|stat.ML
2017-02-28T17:16:43Z,2017-02-21T15:19:29Z,http://arxiv.org/abs/1702.06435v1,http://arxiv.org/pdf/1702.06435v1,"Phase Transitions of Spectral Initialization for High-Dimensional
  Nonconvex Estimation","We study a spectral initialization method that serves as a key ingredient in
recent work on using efficient iterative algorithms for estimating signals in
nonconvex settings. Unlike previous analysis in the literature which is
restricted to the phase retrieval setting and which provides only performance
bounds we consider arbitrary generalized linear sensing models and present a
precise asymptotic characterization of the performance of the spectral method
in the high-dimensional regime. Our analysis reveals a phase transition
phenomenon that depends on the sampling ratio. When the ratio is below a
minimum threshold the estimates given by the spectral method are no better
than a random guess drawn uniformly from the hypersphere; above a maximum
threshold however the estimates become increasingly aligned with the target
signal. The computational complexity of the spectral method is also markedly
different in the two phases. Worked examples and numerical results are provided
to illustrate and verify the analytical predictions. In particular simulations
show that our asymptotic formulas provide accurate predictions even at moderate
signal dimensions.",Yue M. Lu|Gen Li,cs.IT|math.IT|stat.ML
2017-02-28T17:16:43Z,2017-02-21T15:05:36Z,http://arxiv.org/abs/1702.06429v1,http://arxiv.org/pdf/1702.06429v1,"Stochastic Composite Least-Squares Regression with convergence rate
  O(1/n)","We consider the minimization of composite objective functions composed of the
expectation of quadratic functions and an arbitrary convex function. We study
the stochastic dual averaging algorithm with a constant step-size showing that
it leads to a convergence rate of O(1/n) without strong convexity assumptions.
This thus extends earlier results on least-squares regression with the
Euclidean geometry to (a) all convex regularizers and constraints and (b) all
geome-tries represented by a Bregman divergence. This is achieved by a new
proof technique that relates stochastic and deterministic recursions.",Nicolas Flammarion|Francis Bach,math.OC|stat.ML
2017-02-28T17:16:43Z,2017-02-21T13:59:23Z,http://arxiv.org/abs/1702.06385v1,http://arxiv.org/pdf/1702.06385v1,"Causal Inference on Multivariate Mixed-Type Data by Minimum Description
  Length","Given data over the joint distribution of two univariate or multivariate
random variables $X$ and $Y$ of mixed or single type data we consider the
problem of inferring the most likely causal direction between $X$ and $Y$. We
take an information theoretic approach from which it follows that first
describing the data over cause and then that of effect given cause is shorter
than the reverse direction.
  For practical inference we propose a score for causal models for mixed type
data based on the Minimum Description Length (MDL) principle. In particular we
model dependencies between $X$ and $Y$ using classification and regression
trees. Inferring the optimal model is NP-hard and hence we propose Crack a
fast greedy algorithm to infer the most likely causal direction directly from
the data.
  Empirical evaluation on synthetic benchmark and real world data shows that
Crack reliably and with high accuracy infers the correct causal direction on
both univariate and multivariate cause--effect pairs over both single and mixed
type data.",Alexander Marx|Jilles Vreeken,stat.ML|cs.LG
2017-02-28T17:16:43Z,2017-02-21T12:37:35Z,http://arxiv.org/abs/1702.06354v1,http://arxiv.org/pdf/1702.06354v1,"Interpreting Outliers: Localized Logistic Regression for Density Ratio
  Estimation","We propose an inlier-based outlier detection method capable of both
identifying the outliers and explaining why they are outliers by identifying
the outlier-specific features. Specifically we employ an inlier-based outlier
detection criterion which uses the ratio of inlier and test probability
densities as a measure of plausibility of being an outlier. For estimating the
density ratio function we propose a localized logistic regression algorithm.
Thanks to the locality of the model variable selection can be
outlier-specific and will help interpret why points are outliers in a
high-dimensional space. Through synthetic experiments we show that the
proposed algorithm can successfully detect the important features for outliers.
Moreover we show that the proposed algorithm tends to outperform existing
algorithms in benchmark datasets.",Makoto Yamada|Song Liu|Samuel Kaski,stat.ML|cs.LG
2017-02-28T17:16:43Z,2017-02-21T11:56:33Z,http://arxiv.org/abs/1702.06341v1,http://arxiv.org/pdf/1702.06341v1,"Fast rates for online learning in Linearly Solvable Markov Decision
  Processes","We study the problem of online learning in a class of Markov decision
processes known as linearly solvable MDPs. In the stationary version of this
problem a learner interacts with its environment by directly controlling the
state transitions attempting to balance a fixed state-dependent cost and a
certain smooth cost penalizing extreme control inputs. In the current paper we
consider an online setting where the state costs may change arbitrarily between
consecutive rounds and the learner only observes the costs at the end of each
respective round. We are interested in constructing algorithms for the learner
that guarantee small regret against the best stationary control policy chosen
in full knowledge of the cost sequence. Our main result is showing that the
smoothness of the control cost enables the simple algorithm of following the
leader to achieve a regret of order $\log^2 T$ after $T$ rounds vastly
improving on the best known regret bound of order $T^{3/4}$ for this setting.",Gergely Neu|Vicenç Gómez,cs.LG|math.OC|stat.ML
2017-02-28T17:16:43Z,2017-02-27T17:38:58Z,http://arxiv.org/abs/1702.06295v3,http://arxiv.org/pdf/1702.06295v3,Convolution Aware Initialization,"Initialization of parameters in deep neural networks has been shown to have a
big impact on the performance of the networks (Mishkin & Matas 2015). The
initialization scheme devised by He et al allowed convolution activations to
carry a constrained mean which allowed deep networks to be trained effectively
(He et al. 2015a). Orthogonal initializations and more generally orthogonal
matrices in standard recurrent networks have been proved to eradicate the
vanishing and exploding gradient problem (Pascanu et al. 2012). Majority of
current initialization schemes do not take fully into account the intrinsic
structure of the convolution operator. Using the duality of the Fourier
transform and the convolution operator Convolution Aware Initialization builds
orthogonal filters in the Fourier space and using the inverse Fourier
transform represents them in the standard space. With Convolution Aware
Initialization we noticed not only higher accuracy and lower loss but faster
convergence. We achieve new state of the art on the CIFAR10 dataset and
achieve close to state of the art on various other tasks.",Armen Aghajanyan,cs.LG|stat.ML
2017-02-28T17:16:43Z,2017-02-21T07:03:11Z,http://arxiv.org/abs/1702.06280v1,http://arxiv.org/pdf/1702.06280v1,On the (Statistical) Detection of Adversarial Examples,"Machine Learning (ML) models are applied in a variety of tasks such as
network intrusion detection or malware classification. Yet these models are
vulnerable to a class of malicious inputs known as adversarial examples. These
are slightly perturbed inputs that are classified incorrectly by the ML model.
The mitigation of these adversarial inputs remains an open problem.
  As a step towards a model-agnostic defense against adversarial examples we
show that they are not drawn from the same distribution than the original data
and can thus be detected using statistical tests. As the number of malicious
points included in samples presented to the test diminishes its detection
confidence decreases. Hence we introduce a complimentary approach to identify
specific inputs that are adversarial among sets of inputs flagged by the
statistical test. Specifically we augment our ML model with an additional
output in which the model is trained to classify all adversarial inputs.
  We evaluate our approach on multiple adversarial example crafting methods
(including the fast gradient sign and Jacobian-based saliency map methods) with
several datasets. The statistical test flags sample sets containing adversarial
inputs with confidence above 80%. Furthermore our augmented model either
detects adversarial examples with high accuracy (>80%) or increases the
adversary's cost---the perturbation added---by more than 150%. In this way we
show that statistical properties of adversarial examples are essential to their
detection.",Kathrin Grosse|Praveen Manoharan|Nicolas Papernot|Michael Backes|Patrick McDaniel,cs.CR|cs.LG|stat.ML
2017-02-28T17:16:43Z,2017-02-21T06:42:28Z,http://arxiv.org/abs/1702.06278v1,http://arxiv.org/pdf/1702.06278v1,Column normalization of a random measurement matrix,"In this note we answer a question of G. Lecu\'{e} by showing that column
normalization of a random matrix with iid entries need not lead to good sparse
recovery properties even if the generating random variable has a reasonable
moment growth. Specifically for every $2 \leq p \leq c_1\log d$ we construct a
random vector $X \in R^d$ with iid mean-zero variance $1$ coordinates that
satisfies $\sup_{t \in S^{d-1}} \|<Xt>\|_{L_q} \leq c_2\sqrt{q}$ for every
$2\leq q \leq p$.
  We show that if $m \leq c_3\sqrt{p}d^{1/p}$ and $\tilde{\Gamma}:R^d \to R^m$
is the column-normalized matrix generated by $m$ independent copies of $X$
then with probability at least $1-2\exp(-c_4m)$ $\tilde{\Gamma}$ does not
satisfy the exact reconstruction property of order $2$.",Shahar Mendelson,stat.ML
2017-02-28T17:16:43Z,2017-02-21T04:23:41Z,http://arxiv.org/abs/1702.06533v1,http://arxiv.org/pdf/1702.06533v1,Stochastic Canonical Correlation Analysis,"We tightly analyze the sample complexity of CCA provide a learning algorithm
that achieves optimal statistical performance in time linear in the required
number of samples (up to log factors) as well as a streaming algorithm with
similar guarantees.",Chao Gao|Dan Garber|Nathan Srebro|Jialei Wang|Weiran Wang,cs.LG|stat.ML
2017-02-28T17:16:43Z,2017-02-21T02:23:41Z,http://arxiv.org/abs/1702.06240v1,http://arxiv.org/pdf/1702.06240v1,Best Linear Predictor with Missing Response: Locally Robust Approach,"This paper provides asymptotic theory for Inverse Probability Weighing (IPW)
and Locally Robust Estimator (LRE) of Best Linear Predictor where the response
missing at random (MAR) but not completely at random (MCAR). We relax previous
assumptions in the literature about the first-step nonparametric components
requiring only their mean square convergence. This relaxation allows to use a
wider class of machine leaning methods for the first-step such as lasso. For a
generic first-step IPW incurs a first-order bias unless the model it
approximates is truly linear in the predictors. In contrast LRE remains
first-order unbiased provided one can estimate the conditional expectation of
the response with sufficient accuracy. An additional novelty is allowing the
dimension of Best Linear Predictor to grow with sample size. These relaxations
are important for estimation of best linear predictor of teacher-specific and
hospital-specific effects with large number of individuals.",Victor Chernozhukov|Vira Semenova,stat.ME|stat.ML
