2017-02-28T17:16:47Z,2017-02-27T13:03:33Z,http://arxiv.org/abs/1702.08262v1,http://arxiv.org/pdf/1702.08262v1,"Sequential Discrete Kalman Filter for Real-Time State Estimation in
  Power Distribution Systems: Theory and Implementation","This paper demonstrates the feasibility of implementing Real-Time State
Estimators (RTSEs) for Active Distribution Networks (ADNs) in
Field-Programmable Gate Arrays (FPGAs) by presenting an operational prototype.
The prototype is based on a Linear State Estimator (LSE) that uses
synchrophasor measurements from Phasor Measurement Units (PMUs). The underlying
algorithm is the Sequential Discrete Kalman Filter (SDKF) an equivalent
formulation of the Discrete Kalman Filter (DKF) for the case of uncorrelated
measurement noise. In this regard this work formally proves the equivalence
the SDKF and the DKF and highlights the suitability of the SDKF for an FPGA
implementation by means of a computational complexity analysis. The developed
prototype is validated using a case study adapted from the IEEE 34-node
distribution test feeder.",Andreas Martin Kettner|Mario Paolone,stat.AP
2017-02-28T17:16:47Z,2017-02-27T08:33:26Z,http://arxiv.org/abs/1702.08185v1,http://arxiv.org/pdf/1702.08185v1,An update on statistical boosting in biomedicine,"Statistical boosting algorithms have triggered a lot of research during the
last decade. They combine a powerful machine-learning approach with classical
statistical modelling offering various practical advantages like automated
variable selection and implicit regularization of effect estimates. They are
extremely flexible as the underlying base-learners (regression functions
defining the type of effect for the explanatory variables) can be combined with
any kind of loss function (target function to be optimized defining the type
of regression setting). In this review article we highlight the most recent
methodological developments on statistical boosting regarding variable
selection functional regression and advanced time-to-event modelling.
Additionally we provide a short overview on relevant applications of
statistical boosting in biomedicine.",Andreas Mayr|Benjamin Hofner|Elisabeth Waldmann|Tobias Hepp|Olaf Gefeller|Matthias Schmid,stat.AP|stat.CO|stat.ML
2017-02-28T17:16:47Z,2017-02-27T04:17:36Z,http://arxiv.org/abs/1702.08140v1,http://arxiv.org/pdf/1702.08140v1,"A mixture model approach to infer land-use influence on point referenced
  water quality","The assessment of water quality across space and time is of considerable
interest for both agricultural and public health reasons. The standard method
to assess the water quality of a catchment or a group of catchments usually
involves collecting point measurements of water quality and other additional
information such as the date and time of measurements rainfall amounts the
land-use and soil-type of the catchment and the elevation. Some of this
auxiliary information will be point data measured at the exact location
whereas other such as land-use will be areal data often in a compositional
format. Two problems arise if analysts try to incorporate this information into
a statistical model in order to predict (for example) the influence of land-use
on water quality. First is the spatial change of support problem that arises
when using areal data to predict outcomes at point locations. Secondly the
physical process driving water quality is not compositional rather it is the
observation process that provides compositional data. In this paper we present
an approach that accounts for these two issues by using a latent variable to
identify the land-use that most likely influences water quality. This latent
variable is used in a spatial mixture model to help estimate the influence of
land-use on water quality. We demonstrate the potential of this approach with
data from a water quality research study in the Mount Lofty range in South
Australia.",Adrien Ickowicz|Jessica H. Ford|Keith R. Hayes,stat.AP|stat.CO
2017-02-28T17:16:47Z,2017-02-26T21:23:33Z,http://arxiv.org/abs/1702.08088v1,http://arxiv.org/pdf/1702.08088v1,"Selection of training populations (and other subset selection problems)
  with an accelerated genetic algorithm (STPGA: An R-package for selection of
  training populations with a genetic algorithm)","Optimal subset selection is an important task that has numerous algorithms
designed for it and has many application areas. STPGA contains a special
genetic algorithm supplemented with a tabu memory property (that keeps track of
previously tried solutions and their fitness for a number of iterations) and
with a regression of the fitness of the solutions on their coding that is used
to form the ideal estimated solution (look ahead property) to search for
solutions of generic optimal subset selection problems. I have initially
developed the programs for the specific problem of selecting training
populations for genomic prediction or association problems therefore I give
discussion of the theory behind optimal design of experiments to explain the
default optimization criteria in STPGA and illustrate the use of the programs
in this endeavor. Nevertheless I have picked a few other areas of application:
supervised and unsupervised variable selection based on kernel alignment
supervised variable selection with design criteria influential observation
identification for regression solving mixed integer quadratic optimization
problems balancing gains and inbreeding in a breeding population. Some of
these illustrations pertain new statistical approaches.",Deniz Akdemir,stat.ME|cs.LG|q-bio.GN|q-bio.QM|stat.AP
2017-02-28T17:16:47Z,2017-02-26T03:10:41Z,http://arxiv.org/abs/1702.07981v1,http://arxiv.org/pdf/1702.07981v1,"BayCount: A Bayesian Decomposition Method for Inferring Tumor
  Heterogeneity using RNA-Seq Counts","Tumor is heterogeneous - a tumor sample usually consists of a set of
subclones with distinct transcriptional profiles and potentially different
degrees of aggressiveness and responses to drugs. Understanding tumor
heterogeneity is therefore critical to precise cancer prognosis and treatment.
In this paper we introduce BayCount a Bayesian decomposition method to infer
tumor heterogeneity with highly over-dispersed RNA sequencing count data. Using
negative binomial factor analysis BayCount takes into account both the
between-sample and gene-specific random effects on raw counts of sequencing
reads mapped to each gene. For posterior inference we develop an efficient
compound Poisson based blocked Gibbs sampler. Through extensive simulation
studies and analysis of The Cancer Genome Atlas lung cancer and kidney cancer
RNA sequencing count data we show that BayCount is able to accurately estimate
the number of subclones the proportions of these subclones in each tumor
sample and the gene expression profiles in each subclone. Our method
represents the first effort in characterizing tumor heterogeneity using RNA
sequencing count data that simultaneously removes the need of normalizing the
counts achieves statistical robustness and obtains biologically and
clinically meaningful insights.",Fangzheng Xie|Mingyuan Zhou|Yanxun Xu,stat.AP
2017-02-28T17:16:47Z,2017-02-25T15:56:33Z,http://arxiv.org/abs/1702.07909v1,http://arxiv.org/pdf/1702.07909v1,Analysis of Urban Vibrancy and Safety in Philadelphia,"Statistical analyses of urban environments have been recently improved
through publicly available high resolution data and mapping technologies that
have adopted across industries. These technologies allow us to create metrics
to empirically investigate urban design principles of the past half-century.
Philadelphia is an interesting case study for this work with its rapid urban
development and population increase in the last decade. We focus on features of
what urban planners call {\it vibrancy}: measures of positive healthy activity
or energy in an area. Historically vibrancy has been very challenging to
measure empirically. We explore the association between safety (violent and
non-violent crime) and features of local neighborhood vibrancy such as
population economic measures and land use zoning. Despite rhetoric about the
negative effects of population density in the 1960s and 70s we find very
little association between crime and population density. Measures based on land
use zoning are not an adequate description of local vibrancy and so we
construct a database and set of measures of business activity in each
neighborhood. We employ several matching analyses within census block groups to
explore the relationship between neighborhood vibrancy and safety at a higher
resolution. We find that neighborhoods with more vacancy have higher crime but
within neighborhoods crimes tend not to be located near vacant properties. We
also find that more crimes occur near business locations but businesses that
are active (open) for longer periods are associated with fewer crimes.",Colman Humphrey|Shane T. Jensen|Dylan Small|Rachel Thurston,stat.AP
2017-02-28T17:16:47Z,2017-02-25T10:26:59Z,http://arxiv.org/abs/1702.07869v1,http://arxiv.org/pdf/1702.07869v1,Signal Denoising Using the Minimum-Probability-of-Error Criterion,"We address the problem of signal denoising via transform-domain shrinkage
based on a novel $\textit{risk}$ criterion called the minimum probability of
error (MPE) which measures the probability that the estimated parameter lies
outside an $\epsilon$-neighborhood of the actual value. However the MPE
similar to the mean-squared error (MSE) depends on the ground-truth parameter
and has to be estimated from the noisy observations. We consider linear
shrinkage-based denoising functions wherein the optimum shrinkage parameter is
obtained by minimizing an estimate of the MPE. When the probability of error is
integrated over $\epsilon$ it leads to the expected $\ell_1$ distortion. The
proposed MPE and $\ell_1$ distortion formulations are applicable to various
noise distributions by invoking a Gaussian mixture model approximation. Within
the realm of MPE we also develop an extension of the transform-domain
shrinkage by grouping transform coefficients resulting in $\textit{subband
shrinkage}$. The denoising performance obtained within the proposed framework
is shown to be better than that obtained using the minimum MSE-based approaches
formulated within $\textbf{$\textit {Stein's unbiased risk estimation}$}$
(SURE) framework especially in the low measurement signal-to-noise ratio (SNR)
regime. Performance comparison with three state-of-the-art denoising
algorithms carried out on electrocardiogram signals and two test signals taken
from the $\textit{Wavelab}$ toolbox exhibits that the MPE framework results in
consistent SNR gains for input SNRs below $5$ dB.",Jishnu Sadasivan|Subhadip Mukherjee|Chandra Sekhar Seelamantula,stat.AP
2017-02-28T17:16:47Z,2017-02-24T05:16:23Z,http://arxiv.org/abs/1702.07465v1,http://arxiv.org/pdf/1702.07465v1,PairClone: A Bayesian Subclone Caller Based on Mutation Pairs,"Tumor cell populations can be thought of as being composed of homogeneous
cell subpopulations with each subpopulation being characterized by overlapping
sets of single nucleotide variants (SNVs). Such subpopulations are known as
subclones and are an important target for precision medicine. Reconstructing
such subclones from next-generation sequencing (NGS) data is one of the major
challenges in precision medicine. We present PairClone as a new tool to
implement this reconstruction. The main idea of PairClone is to model short
reads mapped to pairs of proximal SNVs. In contrast most existing methods use
only marginal reads for unpaired SNVs. Using Bayesian nonparametric models we
estimate posterior probabilities of the number genotypes and population
frequencies of subclones in one or more tumor sample. We use the categorical
Indian buffet process (cIBP) as a prior probability model for subclones that
are represented as vectors of categorical matrices that record the
corresponding sets of mutation pairs. Performance of PairClone is assessed
using simulated and real datasets. An open source software package can be
obtained at http://www.compgenome.org/pairclone.",Tianjian Zhou|Peter Mueller|Subhajit Sengupta|Yuan Ji,stat.AP
2017-02-28T17:16:47Z,2017-02-23T23:25:32Z,http://arxiv.org/abs/1702.07422v1,http://arxiv.org/pdf/1702.07422v1,"sourceR: Classification and Source Attribution of Infectious Agents
  among Heterogeneous Populations","Zoonotic diseases are a major cause of morbidity and productivity losses in
both humans and animal populations. Identifying the source of food-borne
zoonoses (e.g. an animal reservoir or food product) is crucial for the
identification and prioritisation of food safety interventions. For many
zoonotic diseases it is difficult to attribute human cases to sources of
infection because there is little epidemiological information on the cases.
However microbial strain typing allows zoonotic pathogens to be categorised
and the relative frequencies of the strain types among the sources and in human
cases allows inference on the likely source of each infection. We introduce
sourceR an R package for quantitative source attribution aimed at food-borne
diseases. It implements a fully joint Bayesian model using strain-typed
surveillance data from both human cases and source samples capable of
identifying important sources of infection. The model measures the force of
infection from each source allowing for varying survivability pathogenicity
and virulence of pathogen strains and varying abilities of the sources to act
as vehicles of infection. A Bayesian non-parametric (Dirichlet process)
approach is used to cluster pathogen strain types by epidemiological behaviour
avoiding model overfitting and allowing detection of strain types associated
with potentially high 'virulence'.
  sourceR is demonstrated using Campylobacter jejuni isolate data collected in
New Zealand between 2005 and 2008. It enables straightforward attribution of
cases of zoonotic infection to putative sources of infection by epidemiologists
and public health decision makers. As sourceR develops we intend it to become
an important and flexible resource for food-borne disease attribution studies.",Poppy Miller|Jonathan Marshall|Nigel French|Chris Jewell,stat.AP
2017-02-28T17:16:47Z,2017-02-23T18:24:58Z,http://arxiv.org/abs/1702.07326v1,http://arxiv.org/pdf/1702.07326v1,"Time-Series Adaptive Estimation of Vaccination Uptake Using Web Search
  Queries","Estimating vaccination uptake is an integral part of ensuring public health.
It was recently shown that vaccination uptake can be estimated automatically
from web data instead of slowly collected clinical records or population
surveys. All prior work in this area assumes that features of vaccination
uptake collected from the web are temporally regular. We present the first ever
method to remove this assumption from vaccination uptake estimation: our method
dynamically adapts to temporal fluctuations in time series web data used to
estimate vaccination uptake. We show our method to outperform the state of the
art compared to competitive baselines that use not only web data but also
curated clinical data. This performance improvement is more pronounced for
vaccines whose uptake has been irregular due to negative media attention (HPV-1
and HPV-2) problems in vaccine supply (DiTeKiPol) and targeted at children of
12 years old (whose vaccination is more irregular compared to younger
children).",Niels Dalum Hansen|Kåre Mølbak|Ingemar J. Cox|Christina Lioma,cs.IR|q-bio.QM|stat.AP
2017-02-28T17:16:51Z,2017-02-22T21:21:54Z,http://arxiv.org/abs/1702.07009v1,http://arxiv.org/pdf/1702.07009v1,"The Impact of Confounder Selection in Propensity Scores for Rare Events
  Data - with Applications to Birth Defects","Our work was motivated by a recent study on birth defects of infants born to
pregnant women exposed to a certain medication for treating chronic diseases.
Outcomes such as birth defects are rare events in the general population which
often translate to very small numbers of events in the unexposed group. As drug
safety studies in pregnancy are typically observational in nature we control
for confounding in this rare events setting using propensity scores (PS). Using
our empirical data we noticed that the estimated odds ratio for birth defects
due to exposure varied drastically depending on the specific approach used. The
commonly used approaches with PS are matching stratification inverse
probability weighting (IPW) and regression adjustment. The extremely rare
events setting renders the matching or stratification infeasible. In addition
the PS itself may be formed via different approaches to select confounders from
a relatively long list of potential confounders. We carried out simulation
experiments to compare different combinations of approaches: IPW or regression
adjustment with 1) including all potential confounders without selection 2)
selection based on univariate association between the candidate variable and
the outcome 3) selection based on change in effects (CIE). The simulation
showed that IPW without selection leads to extremely large variances in the
estimated odds ratio which help to explain the empirical data analysis results
that we had observed. The simulation also showed that IPW with selection based
on univariate association with the outcome is preferred over IPW with CIE.
Regression adjustment has small variances of the estimated odds ratio
regardless of the selection methods used.",Ronghui Xu|Jue Hou|Christina D. Chambers,stat.AP
2017-02-28T17:16:51Z,2017-02-22T21:09:19Z,http://arxiv.org/abs/1702.07007v1,http://arxiv.org/pdf/1702.07007v1,Detecting causal associations in large nonlinear time series datasets,"Detecting causal associations in time series datasets is a key challenge for
novel insights into complex dynamical systems such as the Earth system or the
human brain. Interactions in high-dimensional dynamical systems often involve
time-delays nonlinearity and strong autocorrelations. These present major
challenges for causal discovery techniques such as Granger causality leading to
low detection power biases and unreliable hypothesis tests. Here we introduce
a reliable and fast method that outperforms current approaches in detection
power and scales up to high-dimensional datasets. It overcomes detection
biases especially when strong autocorrelations are present and allows ranking
associations in large-scale analyses by their causal strength. We provide
mathematical proofs evaluate our method in extensive numerical experiments
and illustrate its capabilities in a large-scale analysis of the global
surface-pressure system where we unravel spurious associations and find several
potentially causal links that are difficult to detect with standard methods.
The broadly applicable method promises to discover novel causal insights also
in many other fields of science.",Jakob Runge|Dino Sejdinovic|Seth Flaxman,stat.ME|physics.ao-ph|stat.AP
2017-02-28T17:16:51Z,2017-02-22T20:18:36Z,http://arxiv.org/abs/1702.06993v1,http://arxiv.org/pdf/1702.06993v1,Generalized Pareto Processes and Liquidity,"Motivated by the modeling of liquidity risk in fund management in a dynamic
setting we propose and investigate a class of time series models with
generalized Pareto marginals: the autoregressive generalized Pareto process
(ARGP) a modified ARGP (MARGP) and a thresholded ARGP (TARGP). These models
are able to capture key data features apparent in fund liquidity data and
reflect the underlying phenomena via easily interpreted low-dimensional model
parameters. We establish stationarity and ergodicity provide a link to the
class of shot-noise processes and determine the associated interarrival
distributions for exceedances. Moreover we provide estimators for all relevant
model parameters and establish consistency and asymptotic normality for all
estimators (except the threshold parameter which as usual must be dealt with
separately). Finally we illustrate our approach using real-world fund
redemption data and we discuss the goodness-of-fit of the estimated models.",Sascha Desmettre|Johan de Kock|Peter Ruckdeschel|Frank Thomas Seifried,"stat.AP|60G70, 62P05"
2017-02-28T17:16:51Z,2017-02-22T03:10:41Z,http://arxiv.org/abs/1702.06661v1,http://arxiv.org/pdf/1702.06661v1,"Social Learning and Diffusion of Pervasive Goods: An Empirical Study of
  an African App Store","In this study the authors develop a structural model that combines a macro
diffusion model with a micro choice model to control for the effect of social
influence on the mobile app choices of customers over app stores. Social
influence refers to the density of adopters within the proximity of other
customers. Using a large data set from an African app store and Bayesian
estimation methods the authors quantify the effect of social influence and
investigate the impact of ignoring this process in estimating customer choices.
The findings show that customer choices in the app store are explained better
by offline than online density of adopters and that ignoring social influence
in estimations results in biased estimates. Furthermore the findings show that
the mobile app adoption process is similar to adoption of music CDs among all
other classic economy goods. A counterfactual analysis shows that the app store
can increase its revenue by 13.6% through a viral marketing policy (e.g. a
sharing with friends and family button).",Meisam Hejazi Nia|Brian T. Ratchford|Norris Bruce,stat.ML|cs.SI|stat.AP
2017-02-28T17:16:51Z,2017-02-22T02:21:02Z,http://arxiv.org/abs/1702.06650v1,http://arxiv.org/pdf/1702.06650v1,"Reducing the uncertainty in the forest volume-to-biomass relationship
  built from limited field plots","The method of biomass estimation based on a volume-to-biomass relationship
has been applied in estimating forest biomass conventionally through the mean
volume (m3 ha-1). However few studies have been reported concerning the
verification of the volume-biomass equations regressed using field data. The
possible bias may result from the volume measurements and extrapolations from
sample plots to stands or a unit area. This paper addresses (i) how to verify
the volume-biomass equations and (ii) how to reduce the bias while building
these equations. This paper presents an applicable method for verifying the
field data using reasonable wood densities restricting the error in field data
processing based on limited field plots and achieving a better understanding
of the uncertainty in building those equations. The verified and improved
volume-biomass equations are more reliable and will help to estimate forest
carbon sequestration and carbon balance at any large scale.",Caixia Liu|Xiaolu Zhou|Xiangdong Lei|Huabing Huang|Changhui Peng|Xiaoyi Wang|Jianfeng Sun|Carl Zhou,stat.AP|q-bio.QM
2017-02-28T17:16:51Z,2017-02-21T18:32:22Z,http://arxiv.org/abs/1702.06512v1,http://arxiv.org/pdf/1702.06512v1,Semiparametric panel data models using neural networks,"This paper presents an estimator for semiparametric models that uses a
feed-forward neural network to fit the nonparametric component. Unlike many
methodologies from the machine learning literature this approach is suitable
for longitudinal/panel data. It provides unbiased estimation of the parametric
component of the model with associated confidence intervals that have
near-nominal coverage rates. It is further shown that this model and estimator
nests a nonparametric heterogeneous treatment effects model and estimator
which can consistently estimate individualized treatment effects conditional on
covariates. Simulations demonstrate (1) efficiency (2) that parametric
estimates are unbiased and (3) coverage properties of estimated intervals. An
application section demonstrates the method by predicting county-level corn
yield using daily weather data from the period 1981-2015 along with parametric
time trends representing technological change. The method is shown to
out-perform linear methods such as OLS and ridge/lasso as well as random
forest. The procedures described in this paper are implemented in the R package
panelNNET.",Andrew Crane-Droesch,stat.AP
2017-02-28T17:16:51Z,2017-02-19T10:08:16Z,http://arxiv.org/abs/1702.05732v1,http://arxiv.org/pdf/1702.05732v1,Low-dose cryo electron ptychography via non-convex Bayesian optimization,"Electron ptychography has seen a recent surge of interest for phase sensitive
imaging at atomic or near-atomic resolution. However applications are so far
mainly limited to radiation-hard samples because the required doses are too
high for imaging biological samples at high resolution. We propose the use of
non-convex Bayesian optimization to overcome this problem and reduce the dose
required for successful reconstruction by two orders of magnitude compared to
previous experiments. We suggest to use this method for imaging single
biological macromolecules at cryogenic temperatures and demonstrate 2D
single-particle reconstructions from simulated data with a resolution of 7.9
\AA$\$ at a dose of 20 $e^- / \AA^2$. When averaging over only 15 low-dose
datasets a resolution of 4 \AA$\$ is possible for large macromolecular
complexes. With its independence from microscope transfer function direct
recovery of phase contrast and better scaling of signal-to-noise ratio
cryo-electron ptychography may become a promising alternative to Zernike
phase-contrast microscopy.",Philipp Michael Pelz|Wen Xuan Qiu|Robert Bücker|Günther Kassier|R. J. Dwayne Miller,physics.comp-ph|math.OC|physics.data-an|stat.AP
2017-02-28T17:16:51Z,2017-02-19T04:08:18Z,http://arxiv.org/abs/1702.05698v1,http://arxiv.org/pdf/1702.05698v1,Online Robust Principal Component Analysis with Change Point Detection,"Robust PCA methods are typically batch algorithms which requires loading all
observations into memory before processing. This makes them inefficient to
process big data. In this paper we develop an efficient online robust
principal component methods namely online moving window robust principal
component analysis (OMWRPCA). Unlike existing algorithms OMWRPCA can
successfully track not only slowly changing subspace but also abruptly changed
subspace. By embedding hypothesis testing into the algorithm OMWRPCA can
detect change points of the underlying subspaces. Extensive simulation studies
demonstrate the superior performance of OMWRPCA comparing with other
state-of-art approach. We also apply the algorithm for real-time background
subtraction of surveillance video.",Wei Xiao|Xiaolin Huang|Jorge Silva|Saba Emrani|Arin Chaudhuri,cs.LG|cs.CV|stat.AP|stat.CO|stat.ML
2017-02-28T17:16:51Z,2017-02-18T21:57:40Z,http://arxiv.org/abs/1702.05662v1,http://arxiv.org/pdf/1702.05662v1,"Spatial modeling of shot conversion in soccer to single out goalscoring
  ability","Goals are results of pin-point shots and it is a pivotal decision in soccer
when how and where to shoot. The main contribution of this study is two-fold.
At first after showing that there exists high spatial correlation in the data
of shots across games we introduce a spatial process in the error structure to
model the probability of conversion from a shot depending on positional and
situational covariates. The model is developed using a full Bayesian framework.
Secondly based on the proposed model we define two new measures that can
appropriately quantify the impact of an individual in soccer by evaluating the
positioning senses and shooting abilities of the players. As a practical
application the method is implemented on Major League Soccer data from 2016/17
season.",Soudeep Deb|Debangan Dey,stat.AP
2017-02-28T17:16:51Z,2017-02-17T18:06:27Z,http://arxiv.org/abs/1702.05462v1,http://arxiv.org/pdf/1702.05462v1,Objective Bayesian Analysis for Change Point Problems,"In this paper we present an objective approach to change point analysis. In
particular we look at the problem from two perspectives. The first focuses on
the definition of an objective prior when the number of change points is known
a priori. The second contribution aims to estimate the number of change points
by using an objective approach recently introduced in the literature based on
losses. The latter considers change point estimation as a model selection
exercise. We show the performance of the proposed approach on simulated data
and on real data sets.",Laurentiu Hinoveanu|Fabrizio Leisen|Cristiano Villa,stat.ME|math.ST|stat.AP|stat.CO|stat.ML|stat.TH
2017-02-28T17:16:55Z,2017-02-17T12:13:24Z,http://arxiv.org/abs/1702.06913v1,http://arxiv.org/pdf/1702.06913v1,Structural Change in (Economic) Time Series,"Methods for detecting structural changes or change points in time series
data are widely used in many fields of science and engineering. This chapter
sketches some basic methods for the analysis of structural changes in time
series data. The exposition is confined to retrospective methods for univariate
time series. Several recent methods for dating structural changes are compared
using a time series of oil prices spanning more than 60 years. The methods
broadly agree for the first part of the series up to the mid-1980s for which
changes are associated with major historical events but provide somewhat
different solutions thereafter reflecting a gradual increase in oil prices
that is not well described by a step function. As a further illustration 1990s
data on the volatility of the Hang Seng stock market index are reanalyzed.",Christian Kleiber,q-fin.ST|physics.data-an|stat.AP|62P20
2017-02-28T17:16:55Z,2017-02-17T12:12:57Z,http://arxiv.org/abs/1702.05982v1,http://arxiv.org/pdf/1702.05982v1,"Wages of wins: could an amateur make money from match outcome
  predictions?","Evaluating the accuracies of models for match outcome predictions is nice and
well but in the end the real proof is in the money to be made by betting. To
evaluate the question whether the models developed by us could be used easily
to make money via sports betting we evaluate three cases: NCAAB post-season
NBA season and NFL season and find that it is possible yet not without its
pitfalls. In particular we illustrate that high accuracy does not
automatically equal high pay-out by looking at the type of match-ups that are
predicted correctly by different models.",Albrecht Zimmermann,stat.AP|cs.LG
2017-02-28T17:16:55Z,2017-02-17T08:22:52Z,http://arxiv.org/abs/1702.05254v1,http://arxiv.org/pdf/1702.05254v1,"A Biased Look at Phase Locking: Brief Critical Review and Proposed
  Remedy","A number of popular measures of dependence between pairs of band-limited
signals rely on analytic phase. A common misconception is that the dependence
revealed by these measures must be specific to the spectral range of the
filtered input signals. Implicitly or explicitly obtaining analytic phase
involves normalizing the signal by its own envelope which is a nonlinear
operation that introduces broad spectral leakage. We review how this generates
bias and complicates the interpretation of commonly used measures of phase
locking. A specific example of this effect may create spurious phase locking as
a consequence of nonzero circular mean in the phase of input signals which can
be viewed as spectral leakage to 0 Hz. Corrections for this problem which
recenter or uniformize the distribution of phase may fail when the amplitudes
of the compared signals are correlated. To address the more general problem of
spectral bias a novel measure of phase locking is proposed the
amplitude-weighted phase locking value (awPLV). This measure is closely related
to coherence but it removes ambiguities of interpretation that detract from
the latter.",Christopher K. Kovach,q-bio.NC|stat.AP|94A12
2017-02-28T17:16:55Z,2017-02-16T04:08:56Z,http://arxiv.org/abs/1702.04854v1,http://arxiv.org/pdf/1702.04854v1,"Two-stage Plant Species Recognition by Combining Local K-NN and Weighted
  Sparse Representation","In classical sparse representation based classification and weighted SRC
algorithms the test samples are sparely represented by all training samples.
They emphasize the sparsity of the coding coefficients but without considering
the local structure of the input data. To overcome the shortcoming aiming at
the difficult problem of plant leaf recognition on the large-scale database a
two-stage local similarity based classification learning method is proposed by
combining local mean-based classification method and local WSRC. In the first
stage LMC is applied to coarsely classifying the test sample. nearest
neighbors of the test sample as a neighbor subset is selected from each
training class then the local geometric center of each class is calculated. S
candidate neighbor subsets of the test sample are determined with the first
smallest distances between the test sample and each local geometric center. In
the second stage LWSRC is proposed to approximately represent the test sample
through a linear weighted sum of all samples of the candidate neighbor subsets.
The rationale of the proposed method is as follows: the first stage aims to
eliminate the training samples that are far from the test sample and assume
that these samples have no effects on the ultimate classification decision
then select the candidate neighbor subsets of the test sample. Thus the
classification problem becomes simple with fewer subsets; the second stage pays
more attention to those training samples of the candidate neighbor subsets in
weighted representing the test sample. This is helpful to accurately represent
the test sample. Experimental results on the leaf image database demonstrate
that the proposed method not only has a high accuracy and low time cost but
also can be clearly interpreted.",Shanwen Zhang|Harry Wang|Wenzhun Huang,stat.AP
2017-02-28T17:16:55Z,2017-02-16T03:46:19Z,http://arxiv.org/abs/1702.04851v1,http://arxiv.org/pdf/1702.04851v1,"A Comparison of Parametric and Permutation Tests for Regression Analysis
  of Randomized Experiments","Hypothesis tests based on linear models are widely accepted by organizations
that regulate clinical trials. These tests are derived using strong assumptions
about the data-generating process so that the resulting inference can be based
on parametric distributions. Because these methods are well understood and
robust they are sometimes applied to data that depart from assumptions such
as ordinal integer scores. Permutation tests are a nonparametric alternative
that require minimal assumptions which are often guaranteed by the
randomization that was conducted. We compare analysis of covariance (ANCOVA) a
special case of linear regression that incorporates stratification to several
permutation tests based on linear models that control for pretreatment
covariates. In simulations using a variety of data-generating processes some
of which violate the parametric assumptions the permutation tests maintain
power comparable to ANCOVA. We illustrate the use of these permutation tests
alongside ANCOVA with data from a clinical trial comparing the effectiveness of
two treatments for gastroesophageal reflux disease. Given the considerable
costs and scientific importance of clinical trials one may want to include an
additional nonparametric method such as a linear model permutation test as a
robustness check on the statistical inference for the main study endpoints.",Kellie Ottoboni|Fraser Lewis|Luigi Salmaso,stat.AP
2017-02-28T17:16:55Z,2017-02-16T03:16:14Z,http://arxiv.org/abs/1702.04846v1,http://arxiv.org/pdf/1702.04846v1,FMRI Clustering and False Positive Rates,"Recently Eklund et al. (2016) analyzed clustering methods in standard FMRI
packages: AFNI (which we maintain) FSL and SPM [1]. They claimed: 1) false
positive rates (FPRs) in traditional approaches are greatly inflated
questioning the validity of ""countless published fMRI studies""; 2)
nonparametric methods produce valid but slightly conservative FPRs; 3) a
common flawed assumption is that the spatial autocorrelation function (ACF) of
FMRI noise is Gaussian-shaped; and 4) a 15-year-old bug in AFNI's 3dClustSim
significantly contributed to producing ""particularly high"" FPRs compared to
other software. We repeated simulations from [1] (Beijing-Zang data [2] see
[3]) and comment on each point briefly.",Robert W. Cox|Gang Chen|Daniel R. Glen|Richard C. Reynolds|Paul A. Taylor,q-bio.QM|stat.AP
2017-02-28T17:16:55Z,2017-02-16T03:12:50Z,http://arxiv.org/abs/1702.04845v1,http://arxiv.org/pdf/1702.04845v1,FMRI Clustering in AFNI: False Positive Rates Redux,"Recent reports of inflated false positive rates (FPRs) in FMRI group analysis
tools by Eklund et al. (2016) have become a large topic within (and outside)
neuroimaging. They concluded that: existing parametric methods for determining
statistically significant clusters had greatly inflated FPRs (""up to 70%""
mainly due to the faulty assumption that the noise spatial autocorrelation
function is Gaussian- shaped and stationary) calling into question potentially
""countless"" previous results; in contrast nonparametric methods such as their
approach accurately reflected nominal 5% FPRs. They also stated that AFNI
showed ""particularly high"" FPRs compared to other software largely due to a
bug in 3dClustSim. We comment on these points using their own results and
figures and by repeating some of their simulations. Briefly while parametric
methods show some FPR inflation in those tests (and assumptions of
Gaussian-shaped spatial smoothness also appear to be generally incorrect)
their emphasis on reporting the single worst result from thousands of
simulation cases greatly exaggerated the scale of the problem. Importantly FPR
statistics depend on ""task"" paradigm and voxelwise p-value threshold; as such
we show how results of their study provide useful suggestions for FMRI study
design and analysis rather than simply a catastrophic downgrading of the
field's earlier results. Regarding AFNI (which we maintain) 3dClustSim's
bug-effect was greatly overstated - their own results show that AFNI results
were not ""particularly"" worse than others. We describe further updates in AFNI
for characterizing spatial smoothness more appropriately (greatly reducing
FPRs though some remain >5%); additionally we outline two newly implemented
permutation/randomization-based approaches producing FPRs clustered much more
tightly about 5% for voxelwise p<=0.01.",Robert W. Cox|Gang Chen|Daniel R. Glen|Richard C. Reynolds|Paul A. Taylor,q-bio.QM|stat.AP
2017-02-28T17:16:55Z,2017-02-15T22:50:27Z,http://arxiv.org/abs/1702.04808v1,http://arxiv.org/pdf/1702.04808v1,"A Model for Paired-Multinomial Data and Its Application to Analysis of
  Data on a Taxonomic Tree","In human microbiome studies sequencing reads data are often summarized as
counts of bacterial taxa at various taxonomic levels specified by a taxonomic
tree. This paper considers the problem of analyzing two repeated measurements
of microbiome data from the same subjects. Such data are often collected to
assess the change of microbial composition after certain treatment or the
difference in microbial compositions across body sites. Existing models for
such count data are limited in modeling the covariance structure of the counts
and in handling paired multinomial count data. A new probability distribution
is proposed for paired-multinomial count data which allows flexible covariance
structure and can be used to model repeatedly measured multivariate count data.
Based on this distribution a test statistic is developed for testing the
difference in compositions based on paired multinomial count data. The proposed
test can be applied to the count data observed on a taxonomic tree in order to
test difference in microbiome compositions and to identify the subtrees with
different subcompositions. Simulation results indicate that proposed test has
correct type 1 errors and increased power compared to some commonly used
methods. An analysis of an upper respiratory tract microbiome data set is used
to illustrate the proposed methods.",Pixu Shi|Hongzhe Li,stat.AP
2017-02-28T17:16:55Z,2017-02-16T21:06:31Z,http://arxiv.org/abs/1702.04690v2,http://arxiv.org/pdf/1702.04690v2,Simple rules for complex decisions,"From doctors diagnosing patients to judges setting bail experts often base
their decisions on experience and intuition rather than on statistical models.
While understandable relying on intuition over models has often been found to
result in inferior outcomes. Here we present a new method
select-regress-and-round for constructing simple rules that perform well for
complex decisions. These rules take the form of a weighted checklist can be
applied mentally and nonetheless rival the performance of modern machine
learning algorithms. Our method for creating these rules is itself simple and
can be carried out by practitioners with basic statistics knowledge. We
demonstrate this technique with a detailed case study of judicial decisions to
release or detain defendants while they await trial. In this application as in
many policy settings the effects of proposed decision rules cannot be directly
observed from historical data: if a rule recommends releasing a defendant that
the judge in reality detained we do not observe what would have happened under
the proposed action. We address this key counterfactual estimation problem by
drawing on tools from causal inference. We find that simple rules significantly
outperform judges and are on par with decisions derived from random forests
trained on all available features. Generalizing to 22 varied decision-making
domains we find this basic result replicates. We conclude with an analytical
framework that helps explain why these simple decision rules perform as well as
they do.",Jongbin Jung|Connor Concannon|Ravi Shroff|Sharad Goel|Daniel G. Goldstein,stat.AP|stat.ML
2017-02-28T17:16:55Z,2017-02-15T11:26:02Z,http://arxiv.org/abs/1702.04552v1,http://arxiv.org/pdf/1702.04552v1,A new class of robust two-sample Wald-type tests,"Parametric hypothesis testing associated with two independent samples arises
frequently in several applications in biology medical sciences epidemiology
reliability and many more. In this paper we propose robust Wald-type tests for
testing such two sample problems using the minimum density power divergence
estimators of the underlying parameters. In particular we consider the simple
two-sample hypothesis concerning the full parametric homogeneity of the samples
as well as the general two-sample (composite) hypotheses involving nuisance
parameters also. The asymptotic and theoretical robustness properties of the
proposed Wald-type tests have been developed for both the simple and general
composite hypotheses. Some particular cases of testing against one-sided
alternatives are discussed with specific attention to testing the effectiveness
of a treatment in clinical trials. Performances of the proposed tests have also
been illustrated numerically through appropriate real data examples.",Abhik Ghosh|Nirian Martin|Ayanendranath Basu|Leandro Pardo,stat.ME|stat.AP
2017-02-28T17:17:01Z,2017-02-15T03:02:20Z,http://arxiv.org/abs/1702.04450v1,http://arxiv.org/pdf/1702.04450v1,"Applying Spatial Bootstrap and Bayesian Update in uncertainty assessment
  at oil reservoir appraisal stages","Geostatistical modeling of the reservoir intrinsic properties starts only
with sparse data available. These estimates will depend largely on the number
of wells and their location. The drilling costs are so high that they do not
allow new wells to be placed for uncertainty assessment. Besides that
difficulty usual geostatistical models do not account for the uncertainty of
conceptual models which should be considered.
  Spatial bootstrap is applied to assess the estimate reliability when
resampling from original field is not an option. Considering different
realities (conceptual models) and different scenarios (estimates) spatial
bootstrapping applied with Bayesian update allows uncertainty assessment of the
initial estimate and of the conceptual model.
  In this work an approach is suggested to integrate both these techniques
resulting in a method to assess which models are more appropriate for a given
scenario.",Júlio Caineta,stat.AP
2017-02-28T17:17:01Z,2017-02-14T16:35:31Z,http://arxiv.org/abs/1702.04281v1,http://arxiv.org/pdf/1702.04281v1,"Fitting Markovian binary trees using global and individual demographic
  data","We consider a class of branching processes called Markovian binary trees in
which the individuals lifetime and reproduction epochs are modeled using a
transient Markovian arrival process (TMAP). We estimate the parameters of the
TMAP based on population data containing information on age-specific fertility
and mortality rates. Depending on the degree of detail of the available data a
weighted non-linear regression method or a maximum likelihood method is
applied. We discuss the optimal choice of the number of phases in the TMAP and
we provide confidence intervals for the model outputs. The results are
illustrated using real data on human and bird populations.",Sophie Hautphenne|Melanie Massaro|Katharine Turner,stat.AP|q-bio.PE
2017-02-28T17:17:01Z,2017-02-14T13:27:12Z,http://arxiv.org/abs/1702.04197v1,http://arxiv.org/pdf/1702.04197v1,Dissimilar Symmetric Word Pairs in the Human Genome,"In this work we explore the dissimilarity between symmetric word pairs by
comparing the inter-word distance distribution of a word to that of its
reversed complement. We propose a new measure of dissimilarity between such
distributions. Since symmetric pairs with different patterns could point to
evolutionary features we search for the pairs with the most dissimilar
behaviour. We focus our study on the complete human genome and its
repeat-masked version.",Ana Helena Tavares|Jakob Raymaekers|Peter J. Rousseeuw|Raquel M. Silva|Carlos A. C. Bastos|Armando Pinho|Paula Brito|Vera Afreixo,stat.AP|q-bio.GN
2017-02-28T17:17:01Z,2017-02-14T08:26:33Z,http://arxiv.org/abs/1702.04108v1,http://arxiv.org/pdf/1702.04108v1,"Structure-Based Subspace Method for Multi-Channel Blind System
  Identification","In this work a novel subspace-based method for blind identification of
multichannel finite impulse response (FIR) systems is presented. Here we
exploit directly the impeded Toeplitz channel structure in the signal linear
model to build a quadratic form whose minimization leads to the desired channel
estimation up to a scalar factor. This method can be extended to estimate any
predefined linear structure e.g. Hankel that is usually encountered in linear
systems. Simulation findings are provided to highlight the appealing advantages
of the new structure-based subspace (SSS) method over the standard subspace
(SS) method in certain adverse identification scenarii.",Qadri Mayyala|Karim Abed-Meraim|Azzedine Zerguine,stat.AP|cs.IT|math.IT
2017-02-28T17:17:01Z,2017-02-14T02:56:58Z,http://arxiv.org/abs/1702.04052v1,http://arxiv.org/pdf/1702.04052v1,When does poor governance presage biosecurity risk?,"Border inspection and the challenge of deciding which of the tens of
millions of consignments that arrive should be inspected is a perennial
problem for regulatory authorities. The objective of these inspections is to
minimise the risk of contraband entering the country. As an example for
regulatory authorities in charge of biosecurity material consignments of goods
are classified before arrival according to their economic tariff number
(Department of Immigration and Border Protection 2016). This classification
perhaps along with other information is used as a screening step to determine
whether further biosecurity intervention such as inspection is necessary.
Other information associated with consignments includes details such as the
country of origin supplier and importer for example.
  The choice of which consignments to inspect has typically been informed by
historical records of intercepted material. Fortunately for regulators
interception is a rare event however this sparsity undermines the utility of
historical records for deciding which containers to inspect.
  In this paper we report on an analysis that uses more detailed information to
inform inspection. Using quarantine biosecurity as a case study we create
statistical profiles using generalised linear mixed models and compare
different model specifications with historical information alone demonstrating
the utility of a statistical modelling approach. We also demonstrate some
graphical model summaries that provide managers with insight into pathway
governance.",Stephen E Lane|Tony Arthur|Christina Aston|Sam Zhao|Andrew P Robinson,stat.AP
2017-02-28T17:17:01Z,2017-02-14T01:41:21Z,http://arxiv.org/abs/1702.04044v1,http://arxiv.org/pdf/1702.04044v1,"Statistical profiling to predict the biosecurity risk presented by
  non-compliant international passengers","Biosecurity risk material (BRM) presents a clear and significant threat to
national and international environmental and economic assets. Intercepting BRM
carried by non-compliant international passengers is a key priority of border
biosecurity services. Global travel rates are constantly increasing which
complicates this important responsibility and necessitates judicious
intervention. Selection of passengers for intervention is generally performed
manually and the quality of the selection depends on the experience and
judgement of the officer making the selection. In this article we report on a
case study to assess the predictive ability of statistical profiling methods
that predict non-compliance with biosecurity regulations using data obtained
from regulatory documents as inputs. We then evaluate the performance arising
from using risk predictions to select higher risk passengers for screening. We
find that both prediction performance and screening higher risk passengers from
regulatory documents are superior to manual and random screening and recommend
that authorities further investigate statistical profiling for efficient
intervention of biosecurity risk material on incoming passengers.",Stephen E Lane|Richard Gao|Matthew Chisholm|Andrew P Robinson,stat.AP
2017-02-28T17:17:01Z,2017-02-13T13:19:07Z,http://arxiv.org/abs/1702.03762v1,http://arxiv.org/pdf/1702.03762v1,An integrate-and-fire model to generate spike trains with long memory,"We consider a new model of individual neuron of Integrate-and-Fire (IF) type
with fractional noise. The correlations of its spike trains are studied and
proved to have long memory unlike classical IF models. To measure correctly
long-range dependence it is often necessary to know if the data are
stationary. Thus a methodology to evaluate stationarity of the interspike
intervals (ISIs) is presented and applied to various IF models. It appears that
the spike trains of our fractional model have the long-range dependence
property while those from classical Markovian models do not. However they may
seem to have it because of non-stationarities.",Alexandre Richard|Patricio Orio|Etienne Tanré,q-bio.NC|stat.AP
2017-02-28T17:17:01Z,2017-02-13T02:48:16Z,http://arxiv.org/abs/1702.03613v1,http://arxiv.org/pdf/1702.03613v1,"A Multi-model Combination Approach for Probabilistic Wind Power
  Forecasting","Short-term probabilistic wind power forecasting can provide critical
quantified uncertainty information of wind generation for power system
operation and control. As the complicated characteristics of wind power
prediction error it would be difficult to develop a universal forecasting
model dominating over other alternative models. Therefore a novel multi-model
combination (MMC) approach for short-term probabilistic wind generation
forecasting is proposed in this paper to exploit the advantages of different
forecasting models. The proposed approach can combine different forecasting
models those provide different kinds of probability density functions to
improve the probabilistic forecast accuracy. Three probabilistic forecasting
models based on the sparse Bayesian learning kernel density estimation and
beta distribution fitting are used to form the combined model. The parameters
of the MMC model are solved based on Bayesian framework. Numerical tests
illustrate the effectiveness of the proposed MMC approach.",You Lin|Ming Yang|Can Wan|Jianhui Wang|Yonghua Song,cs.LG|stat.AP
2017-02-28T17:17:01Z,2017-02-11T10:52:36Z,http://arxiv.org/abs/1702.03409v1,http://arxiv.org/pdf/1702.03409v1,Disruptive Behavior Disorder (DBD) Rating Scale for Georgian Population,"In the presented study Parent/Teacher Disruptive Behavior Disorder (DBD)
rating scale based on the Diagnostic and Statistical Manual of Mental Disorders
(DSM-IV-TR [APA 2000]) which was developed by Pelham and his colleagues
(Pelham et al. 1992) was translated and adopted for assessment of childhood
behavioral abnormalities especially ADHD ODD and CD in Georgian children and
adolescents. The DBD rating scale was translated into Georgian language using
back translation technique by English language philologists and checked and
corrected by qualified psychologists and psychiatrist of Georgia. Children and
adolescents in the age range of 6 to 16 years (N 290; Mean Age 10.50 SD=2.88)
including 153 males (Mean Age 10.42 SD= 2.62) and 141 females (Mean Age 10.60
SD=3.14) were recruited from different public schools of Tbilisi and the
Neurology Department of the Pediatric Clinic of the Tbilisi State Medical
University. Participants objectively were assessed via interviewing
parents/teachers and qualified psychologists in three different settings
including school home and clinic. In terms of DBD total scores revealed
statistically significant differences between healthy controls (M=27.71
SD=17.26) and children and adolescents with ADHD (M=61.51 SD= 22.79).
Statistically significant differences were found for inattentive subtype
between control (M=8.68 SD=5.68) and ADHD (M=18.15 SD=6.57) groups. In
general it was shown that children and adolescents with ADHD had high score on
DBD in comparison to typically developed persons. In the study also was
determined gender wise prevalence in children and adolescents with ADHD ODD
and CD. The research revealed prevalence of males in comparison with females in
all investigated categories.",Vera Bzhalava|Ketevan Inasaridze,q-bio.NC|stat.AP
2017-02-28T17:17:01Z,2017-02-10T17:08:43Z,http://arxiv.org/abs/1702.03252v1,http://arxiv.org/pdf/1702.03252v1,Markov Models for Health Economic Evaluation: The R Package heemod,"Health economic evaluation studies are widely used in public health to assess
health strategies in terms of their cost-effectiveness and inform public
policies. We developed an R package for Markov models implementing most of the
modelling and reporting features described in reference textbooks and
guidelines: deterministic and probabilistic sensitivity analysis heterogeneity
analysis time dependency on state-time and model-time (semi-Markov and
non-homogeneous Markov models) etc. In this paper we illustrate the features
of heemod by building and analysing an example Markov model. We then explain
the design and the underlying implementation of the package.",Antoine Filipović-Pierucci|Kevin Zarca|Isabelle Durand-Zaleski,stat.AP
2017-02-28T17:17:05Z,2017-02-09T20:10:32Z,http://arxiv.org/abs/1702.02966v1,http://arxiv.org/pdf/1702.02966v1,A monitoring and diagnostic approach for stochastic textured surfaces,"We develop a supervised-learning-based approach for monitoring and diagnosing
texture-related defects in manufactured products characterized by stochastic
textured surfaces that satisfy the locality and stationarity properties of
Markov random fields. Examples of stochastic textured surface data include
images of woven textiles; image or surface metrology data for machined cast
or formed metal parts; microscopy images of material microstructure samples;
etc. To characterize the complex spatial statistical dependencies of in-control
samples of the stochastic textured surface we use rather generic supervised
learning methods which provide an implicit characterization of the joint
distribution of the surface texture. We propose two spatial moving statistics
which are computed from residual errors of the fitted supervised learning
model for monitoring and diagnosing local aberrations in the general spatial
statistical behavior of newly manufactured stochastic textured surface samples
in a statistical process control context. We illustrate the approach using
images of textile fabric samples and simulated 2-D stochastic processes for
which the algorithm successfully detects local defects of various natures.
Additional discussions and results are available in the supplementary
materials.",Anh Tuan Bui|Daniel W. Apley,stat.AP
2017-02-28T17:17:05Z,2017-02-09T17:57:14Z,http://arxiv.org/abs/1702.03862v1,http://arxiv.org/pdf/1702.03862v1,Bayesian Networks Analysis of Malocclusion Data,"In this paper we use Bayesian networks to determine and visualise the
interactions among various Class III malocclusion maxillofacial features during
growth and treatment. We start from a sample of 143 patients characterised
through a series of a maximum of 21 different craniofacial features. We
estimate a network model from these data and we test its consistency by
verifying some commonly accepted hypotheses on the evolution of these
disharmonies by means of Bayesian statistics. We show that untreated subjects
develop different Class III craniofacial growth patterns as compared to
patients submitted to orthodontic treatment with rapid maxillary expantion and
facemask therapy. Among treated patients the CoA segment (the maxillary length)
and the ANB angle (the antero-posterior relation of the maxilla to the
mandible) seem to be the skeletal subspaces that receive the main effect of the
treatment.",Marco Scutari|Pietro Auconi|Guido Caldarelli|Lorenzo Franchi,stat.AP
2017-02-28T17:17:05Z,2017-02-08T03:56:00Z,http://arxiv.org/abs/1702.02268v1,http://arxiv.org/pdf/1702.02268v1,"Efficient Modelling & Forecasting with range based volatility models and
  application","This paper considers an alternative method for fitting CARR models using
combined estimating functions (CEF) by showing its usefulness in applications
in economics and quantitative finance. The associated information matrix for
corresponding new estimates is derived to calculate the standard errors. A
simulation study is carried out to demonstrate its superiority relative to
other two competitors: linear estimating functions (LEF) and the maximum
likelihood (ML). Results show that CEF estimates are more efficient than LEF
and ML estimates when the error distribution is mis-specified. Taking a real
data set from financial economics we illustrate the usefulness and
applicability of the CEF method in practice and report reliable forecast values
to minimize the risk in the decision making process.",Kok-Haur Ng|Shelton Peiris|Jennifer So-kuen-Chan|David Allen|Kooi-Huat Ng,stat.AP|math.ST|stat.TH
2017-02-28T17:17:05Z,2017-02-08T03:32:19Z,http://arxiv.org/abs/1702.02264v1,http://arxiv.org/pdf/1702.02264v1,"Precision Therapeutic Biomarker Identification with Application to the
  Cancer Genome Project","Cancer cell lines have frequently been used to link drug sensitivity and
resistance with genomic profiles. To capture genomic complexity in cancer the
Cancer Genome Project (CGP) (Garnett et al. 2012) screened 639 human tumor
cell lines with 130 drugs ranging from known chemotherapeutic agents to
experimental compounds. Questions of interest include: i) can cancer-specific
therapeutic biomarkers be detected ii) can drug resistance patterns be
identified along with predictive strategies to circumvent resistance using
alternate drugs iii) can biomarkers of drug synergies be predicted ? To tackle
these questions following statistical challenges still exist: i)biomarkers
cluster among the cell lines; ii) clusters can overlap (e.g. a cell line may
belong to multiple clusters); iii) drugs should be modeled jointly. We
introduce a multivariate regression model with a latent overlapping cluster
indicator variable to address above issues. A generalized finite mixture of
multivariate regression (FMMR) model in connection with the new model and a new
EM algorithm for fitting are proposed. Re-analysis of the dataset sheds new
light on the therapeutic inter-relationships between cancers as well existing
and novel drug behaviors for the treatment and management of cancer.",Hongmei Liu|J. Sunil Rao,"stat.AP|62-07, 62J07"
2017-02-28T17:17:05Z,2017-02-07T19:00:00Z,http://arxiv.org/abs/1702.02149v1,http://arxiv.org/abs/1702.02149v1,"An intermediate-mass black hole in the centre of the globular cluster 47
  Tucanae","Intermediate mass black holes play a critical role in understanding the
evolutionary connection between stellar mass and super-massive black holes.
However to date the existence of these species of black holes remains
ambiguous and their formation process is therefore unknown. It has been long
suspected that black holes with masses $10^{2}-10^{4}M_{\odot}$ should form and
reside in dense stellar systems. Therefore dedicated observational campaigns
have targeted globular cluster for many decades searching for signatures of
these elusive objects. All candidates found in these targeted searches appear
radio dim and do not have the X-ray to radio flux ratio predicted by the
fundamental plane for accreting black holes. Based on the lack of an
electromagnetic counterpart upper limits of $2060 M_{\odot}$ and $470
M_{\odot}$ have been placed on the mass of a putative black hole in 47 Tucanae
(NGC 104) from radio and X-ray observations respectively. Here we show there is
evidence for a central black hole in 47 Tuc with a mass of M$_{\bullet}\sim2200
M_{\odot}$$_{-800}^{+1500}$ when the dynamical state of the globular cluster is
probed with pulsars. The existence of an intermediate mass black hole in the
centre of one of the densest clusters with no detectable electromagnetic
counterpart suggests that the black hole is not accreting at a sufficient rate
and therefore contrary to expectations is gas starved. This intermediate mass
black hole might be a member of electromagnetically invisible population of
black holes that are the elusive seeds leading to the formation of supermassive
black holes in galaxies.",Bülent Kızıltan|Holger Baumgardt|Abraham Loeb,astro-ph.GA|astro-ph.IM|astro-ph.SR|stat.AP
2017-02-28T17:17:05Z,2017-02-07T14:06:58Z,http://arxiv.org/abs/1702.02025v1,http://arxiv.org/pdf/1702.02025v1,"Efficient fetal-maternal ECG signal separation from two channel maternal
  abdominal ECG via diffusion-based channel selection","There is a need for affordable widely deployable maternal-fetal ECG monitors
to improve maternal and fetal health during pregnancy and delivery. Based on
the diffusion-based channel selection here we present the mathematical
formalism and clinical validation of an algorithm capable of accurate
separation of maternal and fetal ECG from a two channel signal acquired over
maternal abdomen.",Ruilin Li|Martin G. Frasch|Hau-tieng Wu,physics.med-ph|physics.data-an|stat.AP|stat.ML
2017-02-28T17:17:05Z,2017-02-07T13:12:51Z,http://arxiv.org/abs/1702.01995v1,http://arxiv.org/pdf/1702.01995v1,Statistics-Based Compression of Global Wind Fields,"Wind has the potential to make a significant contribution to future energy
resources; however the task of locating the sources of this renewable energy
on a global scale with climate models along with the associated uncertainty
is hampered by the storage challenges associated with the extremely large
amounts of computer output. Various data compression techniques can be used to
mitigate this problem but traditional algorithms deliver relatively small
compression rates by focusing on individual simulations. We propose a
statistical model that aims at reproducing the data-generating mechanism of an
ensemble of runs by providing a stochastic approximation of global annual wind
data and compressing all the scientific information in the estimated
statistical parameters. We introduce an evolutionary spectrum approach with
spatially varying parameters based on large-scale geographical descriptors such
as altitude to better account for different regimes across the Earth's
orography. We consider a multi-step conditional likelihood approach to estimate
the parameters that explicitly accounts for nonstationary features while also
balancing memory storage and distributed computation and we apply the proposed
model to more than 18 million points on yearly global wind speed. The proposed
model achieves compression rates that are orders of magnitude higher than those
achieved by traditional algorithms on yearly-averaged variables and once the
statistical model is fitted decompressed runs can be almost instantaneously
generated to better assess wind speed uncertainty due to internal variability.",Jaehong Jeong|Stefano Castruccio|Paola Crippa|Marc G. Genton,stat.AP
2017-02-28T17:17:05Z,2017-02-07T12:45:25Z,http://arxiv.org/abs/1702.01987v1,http://arxiv.org/pdf/1702.01987v1,Importance sampling with transformed weights,"The importance sampling (IS) method lies at the core of many Monte
Carlo-based techniques. IS allows the approximation of a target probability
distribution by drawing samples from a proposal (or importance) distribution
different from the target and computing importance weights (IWs) that account
for the discrepancy between these two distributions. The main drawback of IS
schemes is the degeneracy of the IWs which significantly reduces the
efficiency of the method. It has been recently proposed to use transformed IWs
(TIWs) to alleviate the degeneracy problem in the context of Population Monte
Carlo which is an iterative version of IS. However the effectiveness of this
technique for standard IS is yet to be investigated. In this letter we
numerically assess the performance of IS when using TIWs and show that the
method can attain robustness to weight degeneracy thanks to a bias/variance
trade-off.",Manuel A. Vázquez|Joaquín Míguez,stat.AP
2017-02-28T17:17:05Z,2017-02-07T06:14:55Z,http://arxiv.org/abs/1702.01890v1,http://arxiv.org/pdf/1702.01890v1,"Graphical Models and Belief Propagation-hierarchy for Optimal
  Physics-Constrained Network Flows","In this manuscript we review new ideas and first results on application of
the Graphical Models approach originated from Statistical Physics Information
Theory Computer Science and Machine Learning to optimization problems of
network flow type with additional constraints related to the physics of the
flow. We illustrate the general concepts on a number of enabling examples from
power system and natural gas transmission (continental scale) and distribution
(district scale) systems.",Michael Chertkov|Sidhant Misra|Marc Vuffray|Dvijotham Krishnamurty|Pascal Van Hentenryck,cs.SY|stat.AP
2017-02-28T17:17:05Z,2017-02-07T02:52:52Z,http://arxiv.org/abs/1702.01858v1,http://arxiv.org/pdf/1702.01858v1,2D Sinusoidal Parameter Estimation with Offset Term,"We consider the parameter estimation of a 2D sinusoid. Although sinusoidal
parameter estimation has been extensively studied our model differs from those
examined in the available literature by the inclusion of an offset term. We
derive both the maximum likelihood estimation (MLE) solution and the Cramer-Rao
lower bound (CRLB) on the variance of the model's estimators.",A. Pasha Hosseinbor|Renat Zhdanov,stat.AP
2017-02-28T17:17:09Z,2017-02-07T01:16:03Z,http://arxiv.org/abs/1702.01838v1,http://arxiv.org/pdf/1702.01838v1,"Meta Analytic Data Integration for Phenotype Prediction: Application to
  Chronic Fatigue Syndrome","Predictive modeling plays key role in providing accurate prognosis and
enables us to take a step closer to personalized treatment. We identified two
potential sources of human induced biases that can lead to disparate
conclusions. We illustrate through a complex phenotype that robust results can
still be drawn after accounting for such biases.
  Often predictive models build based in high dimensional data suffers from the
drawback of lack of interpretability. To achieve interpretability in the form
of description of the organism level phenomena in term of molecular or cellular
level activities functional and pathway information is often augmented.
Functional information can greatly facilitate the interpretation of the results
of the predictive model.
  However an important aspect of (vertical) data augmentation is routinely
ignored that is there could be several stages of analysis where such
information could be meaningfully integrated. There is no know criteria to
enable us to assess the effect of such augmentation. A novel aspect of the
proposed work is in exploring possibilities of stages of analysis where
functional information may be incorporated and in assessing the extent to which
the ultimate conclusions would differ depending on level of amalgamation.
  To boost our confidence on the key findings a first level of meta-analysis is
done by exploring different levels of data augmentation. This is followed by
comparison of predictive models across different definitions of the same
phenotype developed by different groups which is also an extended form of
meta-analysis.
  We have used real life data on a complex phenotype to illustrate the above.
The data pertains to Chronic Fatigue Syndrome (CFS) and another novel aspect of
the current work is in modeling the underlying continuous symptom measurements
for CFS which is the first for this disease to our knowledge.",Madhuchhanda Bhattacharjee,stat.AP
2017-02-28T17:17:09Z,2017-02-07T00:32:25Z,http://arxiv.org/abs/1702.01830v1,http://arxiv.org/pdf/1702.01830v1,Incoherence of Partial-Component Sampling in multidimensional NMR,"In NMR spectroscopy undersampling in the indirect dimensions causes
reconstruction artifacts whose size can be bounded using the so-called {\it
coherence}. In experiments with multiple indirect dimensions new undersampling
approaches were recently proposed: random phase detection (RPD)
\cite{Maciejewski11} and its generalization partial component sampling (PCS)
\cite{Schuyler13}. The new approaches are fully aware of the fact that
high-dimensional experiments generate hypercomplex-valued free induction
decays; they randomly acquire only certain low-dimensional components of each
high-dimensional hypercomplex entry. We provide a classification of various
hypercomplex-aware undersampling schemes and define a hypercomplex-aware
coherence appropriate for such undersampling schemes; we then use it to
quantify undersampling artifacts of RPD and various PCS schemes.",Hatef Monajemi|David L. Donoho|Jeffrey C. Hoch|Adam D. Schuyler,stat.AP
2017-02-28T17:17:09Z,2017-02-06T21:10:43Z,http://arxiv.org/abs/1702.01793v1,http://arxiv.org/pdf/1702.01793v1,Multiuser Communication Based on the DFT Eigenstructure,"The eigenstructure of the discrete Fourier transform (DFT) is examined and
new systematic procedures to generate eigenvectors of the unitary DFT are
proposed. DFT eigenvectors are suggested as user signatures for data
communication over the real adder channel (RAC). The proposed multiuser
communication system over the 2-user RAC is detailed.",R. M. Campello de Souza|H. M. de Oliveira|R. J. Cintra,"cs.IT|math.IT|stat.AP|94A05, 94A11, 94A40"
2017-02-28T17:17:09Z,2017-02-06T11:45:39Z,http://arxiv.org/abs/1702.01576v1,http://arxiv.org/pdf/1702.01576v1,"Quickest Localization of Anomalies in Power Grids: A Stochastic
  Graphical Framework","Agile localization of anomalous events plays a pivotal role in enhancing the
overall reliability of the grid and avoiding cascading failures. This is
especially of paramount significance in the large-scale grids due to their
geographical expansions and the large volume of data generated. This paper
proposes a stochastic graphical framework by leveraging which it aims to
localize the anomalies with the minimum amount of data. This framework
capitalizes on the strong correlation structures observed among the
measurements collected from different buses. The proposed approach at its
core collects the measurements sequentially and progressively updates its
decision about the location of the anomaly. The process resumes until the
location of the anomaly can be identified with desired reliability. We provide
a general theory for the quickest anomaly localization and also investigate its
application for quickest line outage localization. Simulations in the IEEE
118-bus model are provided to establish the gains of the proposed approach.",Javad Heydari|Ali Tajer,stat.AP|cs.IT|math.IT
2017-02-28T17:17:09Z,2017-02-03T23:55:52Z,http://arxiv.org/abs/1702.01206v1,http://arxiv.org/pdf/1702.01206v1,"Adaptation of the visibility graph algorithm to find the time lag
  between hydrogeological time series","Estimating the time lag between two hydrogeologic time series (e.g.
precipitation and water levels in an aquifer) is of significance for a
hydrogeologist-modeler. In this paper we present a method to quantify such
lags by adapting the visibility graph algorithm which converts time series
into a mathematical graph. We present simulation results to assess the
performance of the method. We also illustrate the utility of our approach using
a real world hydrogeologic dataset.",Rahul John|Majnu John,stat.AP
2017-02-28T17:17:09Z,2017-02-10T21:47:39Z,http://arxiv.org/abs/1702.01201v2,http://arxiv.org/pdf/1702.01201v2,Statistical details of the default priors in the Bambi library,"This is a companion paper to Yarkoni and Westfall (2017) which describes the
Python package Bambi for estimating Bayesian generalized linear mixed models
using a simple interface. Here I give the statistical details underlying the
default weakly informative priors used in all models when the user does not
specify the priors. Our approach is to first deduce what the variances of the
slopes would be if we were instead to have defined the priors on the partial
correlation scale and then to set independent Normal priors on the slopes with
variances equal to these implied variances. Our approach is similar in spirit
to that of Zellner's g-prior (Zellner 1986) in that it involves a multivariate
normal prior on the regression slopes with a tuning parameter to control the
width or informativeness of the priors irrespective of the scales of the data
and predictors. The primary differences are that here the tuning parameter is
directly interpretable as the standard deviation of the distribution of
plausible partial correlations and that this tuning parameter can have
different values for different coefficients. The default priors for the
intercepts and random effects are ultimately based on the prior slope
variances.",Jacob Westfall,stat.AP
2017-02-28T17:17:09Z,2017-02-03T22:18:08Z,http://arxiv.org/abs/1702.01191v1,http://arxiv.org/pdf/1702.01191v1,Radiologic Image-based Statistical Shape Analysis of Brain Tumors,"We propose a curve-based Riemannian-geometric approach for general
shape-based statistical analyses of tumors obtained from radiologic images. A
key component of the framework is a suitable metric that (1) enables
comparisons of tumor shapes (2) provides tools for computing descriptive
statistics and implementing principal component analysis on the space of tumor
shapes and (3) allows for a rich class of continuous deformations of a tumor
shape. The utility of the framework is illustrated through specific statistical
tasks on a dataset of radiologic images of patients diagnosed with glioblastoma
multiforme a malignant brain tumor with poor prognosis. In particular our
analysis discovers two patient clusters with very different survival subtype
and genomic characteristics. Furthermore it is demonstrated that adding tumor
shape information into survival models containing clinical and genomic
variables results in a significant increase in predictive power.",Karthik Bharath|Sebastian Kurtek|Arvind Rao|Veerabhadran Baladandayuthapani,stat.AP
2017-02-28T17:17:09Z,2017-02-03T22:00:37Z,http://arxiv.org/abs/1702.01183v1,http://arxiv.org/abs/1702.01183v1,A Geometric Approach to Visualization of Variability in Functional Data,"We propose a new method for the construction and visualization of
boxplot-type displays for functional data. We use a recent functional data
analysis framework based on a representation of functions called square-root
slope functions to decompose observed variation in functional data into three
main components: amplitude phase and vertical translation. We then construct
separate displays for each component using the geometry and metric of each
representation space based on a novel definition of the median the two
quartiles and extreme observations. The outlyingness of functional data is a
very complex concept. Thus we propose to identify outliers based on any of the
three main components after decomposition. We provide a variety of
visualization tools for the proposed boxplot-type displays including surface
plots. We evaluate the proposed method using extensive simulations and then
focus our attention on three real data applications including exploratory data
analysis of sea surface temperature functions electrocardiogram functions and
growth curves.",Weiyi Xie|Sebastian Kurtek|Karthik Bharath|Ying Sun,stat.AP
2017-02-28T17:17:09Z,2017-02-03T19:00:03Z,http://arxiv.org/abs/1702.01119v1,http://arxiv.org/abs/1702.01119v1,"Anyone Can Become a Troll: Causes of Trolling Behavior in Online
  Discussions","In online communities antisocial behavior such as trolling disrupts
constructive discussion. While prior work suggests that trolling behavior is
confined to a vocal and antisocial minority we demonstrate that ordinary
people can engage in such behavior as well. We propose two primary trigger
mechanisms: the individual's mood and the surrounding context of a discussion
(e.g. exposure to prior trolling behavior). Through an experiment simulating
an online discussion we find that both negative mood and seeing troll posts by
others significantly increases the probability of a user trolling and together
double this probability. To support and extend these results we study how
these same mechanisms play out in the wild via a data-driven longitudinal
analysis of a large online news discussion community. This analysis reveals
temporal mood effects and explores long range patterns of repeated exposure to
trolling. A predictive model of trolling behavior shows that mood and
discussion context together can explain trolling behavior better than an
individual's history of trolling. These results combine to suggest that
ordinary people can under the right circumstances behave like trolls.",Justin Cheng|Michael Bernstein|Cristian Danescu-Niculescu-Mizil|Jure Leskovec,cs.SI|cs.CY|cs.HC|stat.AP|H.2.8; J.4
2017-02-28T17:17:09Z,2017-02-02T20:08:42Z,http://arxiv.org/abs/1702.00817v1,http://arxiv.org/abs/1702.00817v1,DCT-like Transform for Image Compression Requires 14 Additions Only,"A low-complexity 8-point orthogonal approximate DCT is introduced. The
proposed transform requires no multiplications or bit-shift operations. The
derived fast algorithm requires only 14 additions less than any existing DCT
approximation. Moreover in several image compression scenarios the proposed
transform could outperform the well-known signed DCT as well as
state-of-the-art algorithms.",F. M. Bayer|R. J. Cintra,cs.MM|cs.DS|stat.AP|stat.CO
2017-02-28T17:17:13Z,2017-02-02T16:05:01Z,http://arxiv.org/abs/1702.00728v1,http://arxiv.org/pdf/1702.00728v1,"Evaluation of time series models under non-stationarity with application
  to the comparison of regional climate models","Different disciplines pursue the aim to develop models which characterize
certain phenomena as accurately as possible. Climatology is a prime example
where the temporal evolution of the climate is modeled. In order to compare and
improve different models methodology for a fair model evaluation is
indispensable. As models and forecasts of a phenomenon are usually associated
with uncertainty proper scoring rules which are tools that account for this
kind of uncertainty are an adequate choice for model evaluation. However
under the presence of non-stationarity such a model evaluation becomes
challenging as the characteristics of the phenomenon of interest change. We
provide methodology for model evaluation in the context of non-stationary time
series. Our methodology assumes stationarity of the time series in shorter
moving time windows. These moving windows which are selected based on a
changepoint analysis are used to characterize the uncertainty of the
phenomenon/model for the corresponding time instances. This leads to the
concept of moving scores allowing for a temporal assessment of the model
performance. The merits of the proposed methodology are illustrated in a
simulation and a case study.",T. M. Erhardt|C. Czado|T. L. Thorarinsdottir,stat.ME|stat.AP
2017-02-28T17:17:13Z,2017-02-02T08:53:59Z,http://arxiv.org/abs/1702.00584v1,http://arxiv.org/pdf/1702.00584v1,Ultra Reliable Short Message Relaying with Wireless Power Transfer,"We consider a dual-hop wireless network where an energy constrained relay
node first harvests energy through the received radio-frequency signal from the
source and then uses the harvested energy to forward the source's information
to the destination node. The throughput and delay metrics are investigated for
a decode-and-forward relaying mechanism at finite blocklength regime and
delay-limited transmission mode. We consider ultra-reliable communication
scenarios under discussion for the next fifth-generation of wireless systems
with error and latency constraints. The impact on these metrics of the
blocklength information bits and relay position is investigated.",Onel L. Alcaraz López|Hirley Alves|Richard Demo Souza|Evelio Martín García Fernández,cs.IT|math.IT|stat.AP
2017-02-28T17:17:13Z,2017-02-02T07:48:58Z,http://arxiv.org/abs/1702.00564v1,http://arxiv.org/pdf/1702.00564v1,"Modelling dependency completion in sentence comprehension as a Bayesian
  hierarchical mixture process: A case study involving Chinese relative clauses","In sentence comprehension it is widely assumed (Gibson 2000 Lewis &
Vasishth 2005) that the distance between linguistic co-dependents affects the
latency of dependency resolution: the longer the distance the longer the
retrieval time (the distance-based account). An alternative theory of
dependency resolution difficulty is the direct-access model (McElree et al.
2003); this model assumes that retrieval times are a mixture of two
distributions: one distribution represents successful retrieval and the other
represents an initial failure to retrieve the correct dependent followed by a
reanalysis that leads to successful retrieval. The time needed for a successful
retrieval is independent of the dependency distance (cf. the distance-based
account) but reanalyses cost extra time and the proportion of failures
increases with increasing dependency distance. We implemented a series of
increasingly complex hierarchical Bayesian models to compare the distance-based
account and the direct-access model; the latter was implemented as a
hierarchical finite mixture model with heterogeneous variances for the two
mixture distributions. We evaluated the models using two published data-sets on
Chinese relative clauses which have been used to argue in favour of the
distance account but this account has found little support in subsequent work
(e.g. J\""ager et al. 2015). The hierarchical finite mixture model i.e. an
implementation of direct-access is shown to provide a superior account of the
data than the distance account.",Shravan Vasishth|Nicolas Chopin|Robin Ryder|Bruno Nicenboim,stat.AP|cs.CL|stat.ME|stat.ML
2017-02-28T17:17:13Z,2017-02-02T07:14:21Z,http://arxiv.org/abs/1702.00556v1,http://arxiv.org/pdf/1702.00556v1,"The illusion of power: How the statistical significance filter leads to
  overconfident expectations of replicability","We show that publishing results using the statistical significance
filter---publishing only when the p-value is less than 0.05---leads to a
vicious cycle of overoptimistic expectation of the replicability of results.
First we show through a simple derivation that when true statistical power is
relatively low computing power based on statistically significant results will
lead to overestimates of power. Then we present a case study using 10
experimental comparisons drawn from a recently published meta-analysis in
psycholinguistics (J\""ager et al. 2017). We show that the statistically
significant results yield an illusion of replicability i.e. an illusion that
power is high. This illusion holds even if the researcher doesn't conduct any
formal power analysis but just uses statistical significance to informally
assess robustness of results.",Shravan Vasishth|Andrew Gelman,stat.ME|math.ST|stat.AP|stat.TH
2017-02-28T17:17:13Z,2017-02-01T23:38:57Z,http://arxiv.org/abs/1702.00501v1,http://arxiv.org/pdf/1702.00501v1,Adaptive gPCA: A method for structured dimensionality reduction,"When working with large biological data sets exploratory analysis is an
important first step for understanding the latent structure and for generating
hypotheses to be tested in subsequent analyses. However when the number of
variables is large compared to the number of samples standard methods such as
principal components analysis give results which are unstable and difficult to
interpret.
  To mitigate these problems we have developed a method which allows the
analyst to incorporate side information about the relationships between the
variables in a way that encourages similar variables to have similar loadings
on the principal axes. This leads to a low-dimensional representation of the
samples which both describes the latent structure and which has axes which are
interpretable in terms of groups of closely related variables.
  The method is derived by putting a prior encoding the relationships between
the variables on the data and following through the analysis on the posterior
distributions of the samples. We show that our method does well at
reconstructing true latent structure in simulated data and we also demonstrate
the method on a dataset investigating the effects of antibiotics on the
composition of bacteria in the human gut.",Julia Fukuyama,stat.ME|stat.AP
2017-02-28T17:17:13Z,2017-02-01T19:44:14Z,http://arxiv.org/abs/1702.00434v1,http://arxiv.org/pdf/1702.00434v1,"Applying Nearest Neighbor Gaussian Processes to Massive Spatial Data
  Sets: Forest Canopy Height Prediction Across Tanana Valley Alaska","Light detection and ranging (LiDAR) data provide critical information on the
three-dimensional structure of forests. However collecting wall-to-wall LiDAR
data at regional and global scales is cost prohibitive. As a result studies
employing LiDAR data from airborne platforms typically collect data via strip
sampling; leaving large swaths of the forest domain unmeasured by the
instrument. Frameworks to accommodate incomplete coverage information from
LiDAR instruments are essential to advance our understanding of forest
structure and begin effectively monitoring forest resource dynamics over time.
Here we define and assess several spatial regression models capable of
delivering complete coverage forest canopy height prediction maps with
associated uncertainty estimates using sparsely sampled LiDAR data. Despite the
sparsity of the LiDAR data considered the number of observations is large
e.g. n=5x10^6. Computational hurdles associated with developing the desired
data products is overcome by using highly scalable hierarchical Nearest
Neighbor Gaussian Process (NNGP) models. We outline new Markov chain Monte
Carlo (MCMC) algorithms that provide improved convergence and run time over
existing algorithms. We also propose a MCMC free hybrid implementation of NNGP.
We assess the computational and inferential benefits of these alternate NNGP
specifications using simulated data sets and LiDAR data collected over the US
Forest Service Tanana Inventory Unit (TIU) in a remote portion of Interior
Alaska. The resulting data product is the first statistically robust map of
forest canopy for the TIU.",Andrew O. Finley|Abhirup Datta|Bruce C. Cook|Douglas C. Morton|Hans E. Andersen|Sudipto Banerjee,stat.CO|stat.AP
2017-02-28T17:17:13Z,2017-02-01T15:01:06Z,http://arxiv.org/abs/1702.00298v1,http://arxiv.org/pdf/1702.00298v1,"Cascading Failures in Interdependent Systems: Impact of Degree
  Variability and Dependence","We study cascading failures in a system comprising interdependent
networks/systems in which nodes rely on other nodes both in the same system
and in other systems to perform their function. The (inter-)dependence among
nodes is modeled using a dependence graph where the degree vector of a node
determines the number of other nodes it can potentially cause to fail in each
system through aforementioned dependency. In particular we examine the impact
of the variability and dependence properties of node degrees on the probability
of cascading failures. We show that larger variability in node degrees hampers
widespread failures in the system starting with random failures. Similarly
positive correlations in node degrees make it harder to set off an epidemic of
failures thereby rendering the system more robust against random failures.",Richard J. La,stat.AP|cs.SI|physics.soc-ph
2017-02-28T17:17:13Z,2017-02-01T14:01:57Z,http://arxiv.org/abs/1702.00261v1,http://arxiv.org/pdf/1702.00261v1,"Phenomenological forecasting of disease incidence using heteroskedastic
  Gaussian processes: a dengue case study","In 2015 the US federal government sponsored a dengue forecasting competition
using historical case data from Iquitos Peru and San Juan Puerto Rico.
Competitors were evaluated on several aspects of out-of-sample forecasts
including the targets of peak week peak incidence during that week and total
season incidence across each of several seasons. Our team was one of the top
performers of that competition outperforming all other teams in multiple
targets/locals. In this paper we report on our methodology a large component
of which surprisingly ignores the known biology of epidemics at large---in
particular relationships between dengue transmission and environmental
factors---and instead relies on flexible nonparametric nonlinear Gaussian
process (GP) regression fits that ""memorize"" the trajectories of past seasons
and then ""match"" the dynamics of the unfolding season to past ones in
real-time. Our phenomenological approach has advantages in situations where
disease dynamics are less well understood e.g. at sites with shorter
histories of disease (such as Iquitos) or where measurements and forecasts of
ancillary covariates like precipitation are unavailable and/or where the
strength of association with cases are as yet unknown. In particular we show
that the GP approach generally outperforms a more classical generalized linear
(autoregressive) model (GLM) that we developed to utilize abundant covariate
information. We illustrate variations of our method(s) on the two benchmark
locales alongside a full summary of results submitted by other contest
competitors.",Leah R. Johnson|Robert B. Gramacy|Jeremy Cohen|Erin Mordecai|Courtney Murdock|Jason Rohr|Sadie J. Ryan|Anna M. Stewart-Ibarra|Daniel Weikel,stat.AP|q-bio.QM|stat.ME
2017-02-28T17:17:13Z,2017-02-02T03:12:48Z,http://arxiv.org/abs/1702.00111v2,http://arxiv.org/pdf/1702.00111v2,"FAST Adaptive Smoothing and Thresholding for Improved Activation
  Detection in Low-Signal fMRI","Functional Magnetic Resonance Imaging is a noninvasive tool used to study
brain function. Detecting activation is challenged by many factors and even
more so in low-signal scenarios that arise in the performance of high-level
cognitive tasks. We provide a fully automated and fast adaptive smoothing and
thresholding (FAST) algorithm that uses smoothing and extreme value theory on
correlated statistical parametric maps for thresholding. Performance on
simulation experiments spanning a range of low-signal settings is very
encouraging. The methodology also performs well in a study to identify the
cerebral regions that perceive only-auditory-reliable and only-visual-reliable
speech stimuli as well as those that perceive one but not the other.",Israel Almodóvar-Rivera|Ranjan Maitra,"stat.ME|math.ST|stat.AP|stat.TH|62P10, 62P30, 62E20, 62H10, 62H35"
2017-02-28T17:17:13Z,2017-02-01T01:00:14Z,http://arxiv.org/abs/1702.00099v1,http://arxiv.org/abs/1702.00099v1,"A Statistical Framework for Improved Automatic Flaw Detection in
  Nondestructive Evaluation Images","Nondestructive evaluation (NDE) techniques are widely used to detect flaws in
critical components of systems like aircraft engines nuclear power plants and
oil pipelines in order to prevent catastrophic events. Many modern NDE systems
generate image data. In some applications an experienced inspector performs the
tedious task of visually examining every image to provide accurate conclusions
about the existence of flaws. This approach is labor-intensive and can cause
misses due to operator ennui. Automated evaluation methods seek to eliminate
human-factors variability and improve throughput. Simple methods based on peak
amplitude in an image are sometimes employed and a trained-operator-controlled
refinement that uses a dynamic threshold based on signal-to-noise ratio (SNR)
has also been implemented. We develop an automated and optimized detection
procedure that mimics these operations. The primary goal of our methodology is
to reduce the number of images requiring expert visual evaluation by filtering
out images that are overwhelmingly definitive on the existence or absence of a
flaw. We use an appropriate model for the observed values of the SNR-detection
criterion to estimate the probability of detection. Our methodology outperforms
current methods in terms of its ability to detect flaws.",Ye Tian|Ranjan Maitra|William Q. Meeker|Stephen D. Holland,"stat.ME|stat.AP|62P30,"
2017-02-28T17:17:17Z,2017-01-31T17:27:21Z,http://arxiv.org/abs/1701.09143v1,http://arxiv.org/pdf/1701.09143v1,Effective Calibration Transfer via Möbius and Affine Transformations,"A novel technique for calibration transfer called the Modified Four Point
Interpolant (MFPI) method is introduced for near infrared spectra. The method
is founded on physical intuition and utilizes a series of quasiconformal maps
in the frequency domain to transfer spectra from a slave instrument to a master
instrument's approximated space. Comparisons between direct standardization
(DS) piecewise direct standardization (PDS) and MFPI for two publicly
available datasets are detailed herein. The results suggest that MFPI can
outperform DS and PDS with respect to root mean squared errors of transfer and
prediction. Combinations of MFPI with DS/PDS are also shown to reduce
predictive errors after transfer.",Casey Kneale|Karl S. Booksh,stat.AP|stat.ME
2017-02-28T17:17:17Z,2017-01-31T05:43:39Z,http://arxiv.org/abs/1701.08923v1,http://arxiv.org/pdf/1701.08923v1,"One-step Estimation of Networked Population Size with Anonymity Using
  Respondent-Driven Capture-Recapture and Hashing","Estimates of population size for hidden and hard-to-reach individuals are of
particular interest to health officials when health problems are concentrated
in such populations. Efforts to derive these estimates are often frustrated by
a range of factors including social stigma or an association with illegal
activities that ordinarily preclude conventional survey strategies. This paper
builds on and extends prior work that proposed a method to meet these
challenges. Here we describe a rigorous formalization of a one-step
network-based population estimation procedure that can be employed under
conditions of anonymity. The estimation procedure is designed to be implemented
alongside currently accepted strategies for research with hidden populations.
Simulation experiments are described that test the efficacy of the method
across a range of implementation conditions and hidden population sizes. The
results of these experiments show that reliable population estimates can be
derived for hidden networked population as large as 12500 and perhaps larger
for one family of random graphs. As such the method shows potential for
cost-effective implementation health and disease surveillance officials
concerned with hidden populations. Limitations and future work are discussed in
the concluding section.",Bilal Khan|Hsuan-Wei Lee|Kirk Dombrowski,cs.SI|cs.DM|stat.AP|05C82 (Primary) 68R10 (Secondary)|G.2.2; J.4
2017-02-28T17:17:17Z,2017-01-31T21:48:50Z,http://arxiv.org/abs/1701.08588v2,http://arxiv.org/pdf/1701.08588v2,"Estimating the risk associated with transportation technology using
  multifidelity simulation","This paper provides a quantitative method for estimating the risk associated
with candidate transportation technology before it is developed and deployed.
The proposed solution extends previous methods that rely exclusively on
low-fidelity human-in-the-loop experimental data or high-fidelity traffic
data by adopting a multifidelity approach that leverages data from both low-
and high-fidelity sources. The multifidelity method overcomes limitations
inherent to existing approaches by allowing a model to be trained
inexpensively while still assuring that its predictions generalize to the
real-world. This allows for candidate technologies to be evaluated at the stage
of conception and enables a mechanism for only the safest and most effective
technology to be developed and released.",Erik J. Schlicht|Nichole L. Morris,stat.AP|stat.ML
2017-02-28T17:17:17Z,2017-01-29T11:42:03Z,http://arxiv.org/abs/1701.08365v1,http://arxiv.org/pdf/1701.08365v1,"Testing the Zonal Stationarity of Spatial Point Processes: Applied to
  prostate tissues and trees locations","We consider the problem of testing the stationarity and isotropy of a spatial
point pattern based on the concept of local spectra. Using a logarithmic
transformation the mechanism of the proposed test is approximately identical
to a simple two factor analysis of variance procedure when the variance of
residuals is known. This procedure is also used for testing the stationarity in
neighborhood of a particular point of the window of observation. The same idea
is used in post-hoc tests to cluster the point pattern into stationary and
nonstationary sub-windows. The performance of the proposed method is examined
via a simulation study and applied in a practical data.",Azam Saadatjouy|Ali R. Taheriyoun|Mohammad Q. Vahidi-Asl,"stat.AP|60G55, 62F15"
2017-02-28T17:17:17Z,2017-01-28T18:51:10Z,http://arxiv.org/abs/1701.08312v1,http://arxiv.org/pdf/1701.08312v1,ClipAudit: A Simple Risk-Limiting Post-Election Audit,"We propose a simple risk-limiting audit for elections ClipAudit. To
determine whether candidate A (the reported winner) actually beat candidate B
in a plurality election ClipAudit draws ballots at random without
replacement until either all cast ballots have been drawn or until \[ a - b
\ge \beta \sqrt{a+b}
  \] where $a$ is the number of ballots in the sample for the reported winner
A and $b$ is the number of ballots in the sample for opponent B and where
$\beta$ is a constant determined a priori as a function of the number $n$ of
ballots cast and the risk-limit $\alpha$. ClipAudit doesn't depend on the
unofficial margin (as does Bravo). We show how to extend ClipAudit to contests
with multiple winners or losers or to multiple contests.",Ronald L. Rivest,cs.CR|stat.AP
2017-02-28T17:17:17Z,2017-01-28T16:31:25Z,http://arxiv.org/abs/1701.08299v1,http://arxiv.org/pdf/1701.08299v1,"Computing the aggregate loss distribution based on numerical inversion
  of the compound empirical characteristic function of frequency and severity","A non-parametric method for evaluation of the aggregate loss distribution
(ALD) by combining and numerically inverting the empirical characteristic
functions (CFs) is presented and illustrated. This approach to evaluate ALD is
based on purely non-parametric considerations i.e. based on the empirical CFs
of frequency and severity of the claims in the actuarial risk applications.
This approach can be however naturally generalized to a more complex
semi-parametric modeling approach e.g. by incorporating the generalized
Pareto distribution fit of the severity distribution heavy tails and/or by
considering the weighted mixture of the parametric CFs (used to model the
expert knowledge) and the empirical CFs (used to incorporate the knowledge
based on the historical data - internal and/or external). Here we present a
simple and yet efficient method and algorithms for numerical inversion of the
CF suitable for evaluation of the ALDs and the associated measures of interest
important for applications as e.g. the value at risk (VaR). The presented
approach is based on combination of the Gil-Pelaez inversion formulae for
deriving the probability distribution (PDF and CDF) from the compound
(empirical) CF and the trapezoidal rule used for numerical integration. The
applicability of the suggested approach is illustrated by analysis of a well
know insurance dataset the Danish fire loss data.",Viktor Witkovsky|Gejza Wimmer|Tomas Duby,"stat.CO|q-fin.RM|stat.AP|91B30, 62G32"
2017-02-28T17:17:17Z,2017-02-18T18:48:36Z,http://arxiv.org/abs/1701.08230v3,http://arxiv.org/pdf/1701.08230v3,Algorithmic decision making and the cost of fairness,"Algorithms are now regularly used to decide whether defendants awaiting trial
are too dangerous to be released back into the community. In some cases black
defendants are substantially more likely than white defendants to be
incorrectly classified as high risk. To mitigate such disparities several
techniques recently have been proposed to achieve algorithmic fairness. Here we
reformulate algorithmic fairness as constrained optimization: the objective is
to maximize public safety while satisfying formal fairness constraints designed
to reduce racial disparities. We show that for several past definitions of
fairness the optimal algorithms that result require detaining defendants above
race-specific risk thresholds. We further show that the optimal unconstrained
algorithm requires applying a single uniform threshold to all defendants. The
unconstrained algorithm thus maximizes public safety while also satisfying one
important understanding of equality: that all individuals are held to the same
standard irrespective of race. Because the optimal constrained and
unconstrained algorithms generally differ there is tension between improving
public safety and satisfying prevailing notions of algorithmic fairness. By
examining data from Broward County Florida we show that this trade-off can be
large in practice. We focus on algorithms for pretrial release decisions but
the principles we discuss apply to other domains and also to human decision
makers carrying out structured decision rules.",Sam Corbett-Davies|Emma Pierson|Avi Feller|Sharad Goel|Aziz Huq,cs.CY|stat.AP
2017-02-28T17:17:17Z,2017-02-09T14:45:06Z,http://arxiv.org/abs/1701.08142v2,http://arxiv.org/pdf/1701.08142v2,Modelling Ranking Data with the Wallenius Distribution,"Ranking datasets is useful when statements on the order of observations are
more important than the magnitude of their differences and little is known
about the underlying distribution of the data. The Wallenius distribution is a
generalisation of the Hypergeometric distribution where weights are assigned to
balls of different colours. This naturally defines a model for ranking
categories which can be used for classification purposes. In this paper we
adopt an approximate Bayesian computational (ABC) approach since in general
the resulting likelihood is not analytically available. We illustrate the
performance of the estimation procedure on simulated datasets. Finally we use
the new model for analysing two datasets about movies ratings and Italian
academic statisticians' journals preferences. The latter is a novel dataset
collected by the authors.",Clara Grazian|Fabrizio Leisen|Brunero Liseo,stat.ME|stat.AP|stat.CO|stat.ML
2017-02-28T17:17:17Z,2017-01-27T16:37:03Z,http://arxiv.org/abs/1701.08107v1,http://arxiv.org/pdf/1701.08107v1,Deconvolution and Restoration of Optical Endomicroscopy Images,"Optical endomicroscopy (OEM) is an emerging technology platform with
preclinical and clinical imaging utility. Pulmonary OEM via multicore fibres
has the potential to provide in vivo in situ molecular signatures of disease
such as infection and inflammation. However enhancing the quality of data
acquired by this technique for better visualization and subsequent analysis
remains a challenging problem. Cross coupling between fiber cores is one of the
main reasons of poor detection performance (i.e. inflammation bacteria
etc.). In this work we address the problem of deconvolution and restoration of
OEM data. We propose and compare four methods three are based on the
alternating direction method of multipliers (ADMM) and one is based on Markov
chain Monte Carlo (MCMC) methods. Results on both synthetic and real datasets
illustrate the effectiveness of the proposed methods.",Ahmed Karam Eldaly|Yoann Altmann|Antonios Perperidis|Nikola Krstajic|Tushar Choudhary|Kevin Dhaliwal|Stephen McLaughlin,cs.CV|stat.AP
2017-02-28T17:17:17Z,2017-01-27T14:01:53Z,http://arxiv.org/abs/1701.08055v1,http://arxiv.org/pdf/1701.08055v1,"Modelling Competitive Sports: Bradley-Terry-Élő Models for
  Supervised and On-Line Learning of Paired Competition Outcomes","Prediction and modelling of competitive sports outcomes has received much
recent attention especially from the Bayesian statistics and machine learning
communities. In the real world setting of outcome prediction the seminal
\'{E}l\H{o} update still remains after more than 50 years a valuable baseline
which is difficult to improve upon though in its original form it is a
heuristic and not a proper statistical ""model"". Mathematically the \'{E}l\H{o}
rating system is very closely related to the Bradley-Terry models which are
usually used in an explanatory fashion rather than in a predictive supervised
or on-line learning setting.
  Exploiting this close link between these two model classes and some newly
observed similarities we propose a new supervised learning framework with
close similarities to logistic regression low-rank matrix completion and
neural networks. Building on it we formulate a class of structured log-odds
models unifying the desirable properties found in the above: supervised
probabilistic prediction of scores and wins/draws/losses batch/epoch and
on-line learning as well as the possibility to incorporate features in the
prediction without having to sacrifice simplicity parsimony of the
Bradley-Terry models or computational efficiency of \'{E}l\H{o}'s original
approach.
  We validate the structured log-odds modelling approach in synthetic
experiments and English Premier League outcomes where the added expressivity
yields the best predictions reported in the state-of-art close to the quality
of contemporary betting odds.",Franz J. Király|Zhaozhi Qian,stat.ML|cs.LG|stat.AP|stat.ME
2017-02-28T17:17:21Z,2017-01-27T13:07:49Z,http://arxiv.org/abs/1701.08043v1,http://arxiv.org/pdf/1701.08043v1,"A stage-structured Bayesian hierarchical model for salmon lice
  populations at individual salmon farms - Estimated from multiple farm data
  sets","Salmon farming has become a prosperous international industry over the last
decades. Along with growth in the production farmed salmon however an
increasing threat by pathogens has emerged. Of special concern is the
propagation and spread of the salmon louse Lepeophtheirus salmonis. In order
to gain insight into this parasites population dynamics in large scale salmon
farming system we present a fully mechanistic stage-structured population
model for the salmon louse also allowing for complexities involved in the
hierarchical structure of full scale salmon farming. The model estimates
parameters controlling a wide range of processes including temperature
dependent demographic rates fish size and abundance effects on louse
transmission rates effects sizes of various salmon louse control measures and
distance based between farm transmission rates. Model parameters were estimated
from data including 32 salmon farms except the last production months for five
farms which were used to evaluate model predictions. We used a Bayesian
estimation approach combining the prior distributions and the data likelihood
into a joint posterior distribution for all model parameters. The model
generated expected values that fitted the observed infection levels of the
chalimus adult female and other mobile stages of salmon lice reasonably well.
Predictions for the time periods not used for fitting the model were also
consistent with the observational data. We argue that the present model for the
population dynamics of the salmon louse in aquaculture farm systems may
contribute to resolve the complexity of processes that drive that drive this
host-parasite relationship and hence may improve strategies to control the
parasite in this production system.",Magne Aldrin|Ragnar Bang Huseby|Audun Stien|Randi Nygaard Grøntvedt|Hildegunn Viljugrein|Peder Andreas Jansen,q-bio.PE|q-bio.QM|stat.AP
2017-02-28T17:17:21Z,2017-01-27T12:47:39Z,http://arxiv.org/abs/1702.02089v1,http://arxiv.org/pdf/1702.02089v1,"A Statistical Model for Ideal Team Selection for A National Cricket
  Squad","Cricket is a game played between two teams which consists of eleven players
each. Nowadays cricket game is becoming more and more popular in Bangladesh and
other South Asian Countries. Before a match people are very enthusiastic about
team squads and ""Which players are playing today?"" ""How well will MR. X
perform today?"" are the million dollar questions before a big match. This
article will propose a method using statistical data analysis for recommending
a national team squad. Recent match scorecards for domestic and international
matches played by a specific team in recent years are used to recommend the
ideal squad. Impact point or rating points of all players in different
conditions are calculated and the best ones from different categories are
chosen to form optimal line-ups. To evaluate the efficiency of impact point
system it will be tested with real time match data to see how much accuracy it
gives.",Sadia Tasnim Swarna|Shamim Ehsan|Md. Saiful Islam,stat.AP
2017-02-28T17:17:21Z,2017-01-27T00:22:42Z,http://arxiv.org/abs/1701.07910v1,http://arxiv.org/pdf/1701.07910v1,An Application of Envelope Methodology and Aster Models,"Precise estimation of expected Darwinian fitness is a central component of
life history analysis. Our methods provide precise estimation by incorporating
general envelope methodology into the aster modeling framework. This level of
precision provided from out methods allow researchers to draw stronger
conclusions about the driving forces of Darwinian fitness from their life
history analyses than they could with the aster model alone. The aster model
serves as a defensible statistical model for distributions of Darwinian
fitness. Envelope methodology reduces asymptotic variability by establishing a
link between unknown parameters of interest and the asymptotic covariance
matrices of their estimators. It is known both theoretically and in
applications that incorporation of envelope methodology reduces asymptotic
variability. A novel envelope estimator is developed and used to obtain
variance reduction. Our methods are illustrated on a simulated dataset and a
real dataset of \emph{Mimulus guttatus} flowers. Variance reduction is obtained
in both analyses. Our examples are fully reproducible in an accompanying
technical report.",Daniel J. Eck|Charles J. Geyer|R. Dennis Cook,stat.AP|stat.ME
2017-02-28T17:17:21Z,2017-02-23T18:53:25Z,http://arxiv.org/abs/1701.07899v2,http://arxiv.org/pdf/1701.07899v2,"Nonlinear network-based quantitative trait prediction from
  transcriptomic data","Quantitatively predicting phenotype variables by the expression changes in a
set of candidate genes is of great interest in molecular biology but it is also
a challenging task for several reasons. First the collected biological
observations might be heterogeneous and correspond to different biological
mechanisms. Secondly the gene expression variables used to predict the
phenotype are potentially highly correlated since genes interact though unknown
regulatory networks. In this paper we present a novel approach designed to
predict quantitative trait from transcriptomic data taking into account the
heterogeneity in biological samples and the hidden gene regulatory networks
underlying different biological mechanisms. The proposed model performs well on
prediction but it is also fully parametric which facilitates the downstream
biological interpretation. The model provides clusters of individuals based on
the relation between gene expression data and the phenotype and also leads to
infer a gene regulatory network specific for each cluster of individuals. We
perform numerical simulations to demonstrate that our model is competitive with
other prediction models and we demonstrate the predictive performance and the
interpretability of our model to predict alcohol sensitivity from
transcriptomic data on real data from Drosophila Melanogaster Genetic Reference
Panel (DGRP).",Emilie Devijver|Mélina Gallopin|Emeline Perthame,stat.AP|stat.ME|stat.ML
2017-02-28T17:17:21Z,2017-02-04T09:45:36Z,http://arxiv.org/abs/1701.07638v2,http://arxiv.org/pdf/1701.07638v2,"The impact of stochastic lead times on the bullwhip effect under
  correlated demand and moving average forecasts","We quantify the bullwhip effect (which measures how the variance in
replenishment orders is amplified as the orders move up the supply chain) when
random demands and random lead times are estimated using the industrially
popular moving average forecasting method. We assume that the lead times
constitute a sequence of independent identically distributed random variables
and correlated demands are described by a first order autoregressive process.
We obtain an expression that reveals the impact of demand and lead time
forecasting on the bullwhip effect. We draw a number of conclusions on the
behavior of the bullwhip effect with respect to the demand auto-correlation and
the number of past lead times and demands used in the forecasts. Furthermore we
find the maxima and minima of the bullwhip measure as a function of the demand
auto-correlation.",Zbigniew Michna|Stephen M. Disney|Peter Nielsen,stat.AP
2017-02-28T17:17:21Z,2017-01-26T02:49:06Z,http://arxiv.org/abs/1701.07555v1,http://arxiv.org/pdf/1701.07555v1,"Robust analysis of second-leg home advantage in UEFA football through
  better nonparametric confidence intervals for binary regression functions","In international football (soccer) two-legged knockout ties with each team
playing at home in one leg and the final outcome decided on aggregate are
common. Many players managers and followers seem to believe in the `second-leg
home advantage' i.e. that it is beneficial to play at home on the second leg.
A more complex effect than the usual and well-established home advantage it is
harder to identify and previous statistical studies did not prove conclusive
about its actuality. Yet given the amount of money handled in international
football competitions nowadays the question of existence or otherwise of this
effect is of real import. As opposed to previous research this paper addresses
it from a purely nonparametric perspective and brings a very objective answer
not based on any particular model specification which could orientate the
analysis in one or the other direction. Along the way the paper reviews the
well-known shortcomings of the Wald confidence interval for a proportion
suggests new nonparametric confidence intervals for conditional probability
functions revisits the problem of the bias when building confidence intervals
in nonparametric regression and provides a novel bootstrap-based solution to
it. Finally the new intervals are used in a careful analysis of game outcome
data for the UEFA Champions and Europa leagues from 2009/10 to 2014/15. A
slight `second-leg home advantage' is evidenced.",Gery Geenens|Thomas Cuddihy,stat.ME|stat.AP
2017-02-28T17:17:21Z,2017-01-25T21:43:03Z,http://arxiv.org/abs/1701.07496v1,http://arxiv.org/pdf/1701.07496v1,Phylogenetic Factor Analysis,"Phylogenetic comparative methods explore the relationships between
quantitative traits adjusting for shared evolutionary history. This adjustment
often occurs through a Brownian diffusion process along the branches of the
phylogeny that generates model residuals or the traits themselves. For
high-dimensional traits inferring all pair-wise correlations within the
multivariate diffusion is limiting. To circumvent this problem we propose
phylogenetic factor analysis (PFA) that assumes a small unknown number of
independent evolutionary factors arise along the phylogeny and these factors
generate clusters of dependent traits. Set in a Bayesian framework PFA
provides measures of uncertainty on the factor number and groupings combines
both continuous and discrete traits integrates over missing measurements and
incorporates phylogenetic uncertainty with the help of molecular sequences. We
develop Gibbs samplers based on dynamic programming to estimate the PFA
posterior distribution over three-fold faster than for multivariate diffusion
and a further order-of-magnitude more efficiently in the presence of latent
traits. We further propose a novel marginal likelihood estimator for previously
impractical models with discrete data and find that PFA also provides a better
fit than multivariate diffusion in evolutionary questions in columbine flower
development placental reproduction transitions and triggerfish fin
morphometry.",Max R. Tolkoff|Michael L. Alfaro|Guy Baele|Philippe Lemey|Marc A. Suchard,stat.ME|stat.AP|stat.CO
2017-02-28T17:17:21Z,2017-01-25T20:47:40Z,http://arxiv.org/abs/1701.07483v1,http://arxiv.org/pdf/1701.07483v1,A Model-based Projection Technique for Segmenting Customers,"We consider the problem of segmenting a large population of customers into
non-overlapping groups with similar preferences using diverse preference
observations such as purchases ratings clicks etc. over subsets of items. We
focus on the setting where the universe of items is large (ranging from
thousands to millions) and unstructured (lacking well-defined attributes) and
each customer provides observations for only a few items. These data
characteristics limit the applicability of existing techniques in marketing and
machine learning. To overcome these limitations we propose a model-based
projection technique which transforms the diverse set of observations into a
more comparable scale and deals with missing data by projecting the transformed
data onto a low-dimensional space. We then cluster the projected data to obtain
the customer segments. Theoretically we derive precise necessary and
sufficient conditions that guarantee asymptotic recovery of the true customer
segments. Empirically we demonstrate the speed and performance of our method
in two real-world case studies: (a) 84% improvement in the accuracy of new
movie recommendations on the MovieLens data set and (b) 6% improvement in the
performance of similar item recommendations algorithm on an offline dataset at
eBay. We show that our method outperforms standard latent-class and
demographic-based techniques.",Srikanth Jagabathula|Lakshminarayanan Subramanian|Ashwin Venkataraman,stat.ME|cs.LG|stat.AP|stat.ML
2017-02-28T17:17:21Z,2017-01-25T17:47:32Z,http://arxiv.org/abs/1701.07402v1,http://arxiv.org/pdf/1701.07402v1,"Smallest eigenvalue density for regular or fixed-trace complex
  Wishart-Laguerre ensemble and entanglement in coupled kicked tops","The statistical behavior of the smallest eigenvalue has important
implications for systems which can be modeled using a Wishart-Laguerre
ensemble the regular one or the fixed trace one. For example the density of
the smallest eigenvalue of the Wishart-Laguerre ensemble plays a crucial role
in characterizing multiple channel telecommunication systems. Similarly in the
quantum entanglement problem the smallest eigenvalue of the fixed trace
ensemble carries information regarding the nature of entanglement.
  For the case of real Wishart-Laguerre matrices there exists an elegant
recurrence scheme suggested by Edelman to directly obtain the exact expression
for the smallest eigenvalue density. For the complex Wishart-Laguerre matrices
the existing results for the smallest eigenvalue density comprise determinants
which become difficult to handle when the determinants involve large-size
matrices. In this work we derive a recurrence scheme for the complex case
which is analogous to that of Edelman's for the real case. This is used to
obtain exact results for the smallest eigenvalue density for both the regular
and the fixed trace complex Wishart-Laguerre ensembles. We validate our
analytical results using Monte Carlo simulations. We also study scaled
Wishart-Laguerre ensemble and investigate its efficacy in approximating the
fixed-trace ensemble. Eventually we apply our result for the fixed-trace
ensemble to investigate the behavior of the smallest eigenvalue in the
paradigmatic system of coupled kicked tops.",Santosh Kumar|Bharath Sambasivam|Shashank Anand,"math-ph|cond-mat.stat-mech|math.MP|math.ST|stat.AP|stat.TH|15B52, 15A18, 65Q30, 81P40, 81Q50"
2017-02-28T17:17:21Z,2017-01-25T14:07:52Z,http://arxiv.org/abs/1701.07316v1,http://arxiv.org/pdf/1701.07316v1,"Prediction of the margin of victory only from team rankings for regular
  season games in NCAA men's basketball","The main objective of this paper is to investigate the extent to which the
margin of victory can be predicted solely by the rankings of the opposing teams
in NCAA Division I men's basketball games. Several past studies have modeled
this relationship for the games played during the March Madness tournament and
this work aims at verifying if the models advocated in these papers still
perform well for regular season games. Indeed most previous articles have
shown that a simple quadratic regression model provides fairly accurate
predictions of the margin of victory when team rankings only range from 1 to
16. Does that still hold true when team rankings can go as high as 351? Do the
model assumptions hold? Can we find semi- or non-parametric methods that yield
even better results (i.e. predicted margins of victory that more closely
resemble actual results)? The analyses presented in this paper suggest that the
answer is ""yes"" on all three counts!",David Beaudoin|Thierry Duchesne,stat.AP
2017-02-28T17:17:24Z,2017-01-25T08:41:12Z,http://arxiv.org/abs/1701.07203v1,http://arxiv.org/pdf/1701.07203v1,Estimation of Vertex Degrees in a Sampled Network,"The need to produce accurate estimates of vertex degree in a large network
based on observation of a subnetwork arises in a number of practical settings.
We study a formalized version of this problem wherein the goal is given a
randomly sampled subnetwork from a large parent network to estimate the actual
degree of the sampled nodes. Depending on the sampling scheme trivial method
of moments estimators (MMEs) can be used. However the MME is not expected in
general to use all relevant network information. In this study we propose a
handful of novel estimators derived from a risk-theoretic perspective which
make more sophisticated use of the information in the sampled network.
Theoretical assessment of the new estimators characterizes under what
conditions they can offer improvement over the MME while numerical comparisons
show that when such improvement obtains it can be substantial. Illustration is
provided on a human trafficking network.",Apratim Ganguly|Eric Kolaczyk,stat.AP
2017-02-28T17:17:24Z,2017-01-25T04:01:35Z,http://arxiv.org/abs/1701.07152v1,http://arxiv.org/pdf/1701.07152v1,Time Series Copulas for Heteroskedastic Data,"We propose parametric copulas that capture serial dependence in stationary
heteroskedastic time series. We develop our copula for first order Markov
series and extend it to higher orders and multivariate series. We derive the
copula of a volatility proxy based on which we propose new measures of
volatility dependence including co-movement and spillover in multivariate
series. In general these depend upon the marginal distributions of the series.
Using exchange rate returns we show that the resulting copula models can
capture their marginal distributions more accurately than univariate and
multivariate GARCH models and produce more accurate value at risk forecasts.",Rubén Loaiza-Maya|Michael S. Smith|Worapree Maneesoonthorn,stat.AP|q-fin.ST
2017-02-28T17:17:24Z,2017-01-24T18:56:45Z,http://arxiv.org/abs/1701.07011v1,http://arxiv.org/pdf/1701.07011v1,Comparable variable selection with Lasso,"P-values are being computed for increasingly complicated statistics but
lacking evaluations on their quality. Meanwhile accurate p-values enable
significance comparison across batches of hypothesis tests and consequently
unified false discover rate (FDR) control. This article discusses two related
questions in this setting. First we propose statistical tests to evaluate the
quality of p-value and the cross-batch comparability of any other statistic.
Second we propose a lasso based variable selection statistic based on when
the predictor variable first becomes active and compute its p-value to achieve
unified FDR control across multiple selections. In the end we apply our tests
on covTest selectiveInference and our statistic based on real and null
datasets for network inference in normal and high-dimensional settings. Results
demonstrate higher p-value quality from our statistic and reveal p-value errors
from others hidden before. We implement our statistic as lassopv in R.",Lingfei Wang|Tom Michoel,stat.ME|q-bio.MN|q-bio.QM|stat.AP
2017-02-28T17:17:24Z,2017-01-24T16:47:47Z,http://arxiv.org/abs/1701.06976v1,http://arxiv.org/pdf/1701.06976v1,"A unified framework for fitting Bayesian semiparametric models to
  arbitrarily censored spatial survival data","A comprehensive unified approach to modeling arbitrarily censored spatial
survival data is presented for the three most commonly-used semiparametric
models: proportional hazards proportional odds and accelerated failure time.
Unlike many other approaches all manner of censored survival times are
simultaneously accommodated including uncensored interval censored
current-status left and right censored and mixtures of these. Left-truncated
data are also accommodated leading to models for time-dependent covariates.
Both georeferenced (location observed exactly) and areally observed (location
known up to a geographic unit such as a county) spatial locations are handled;
formal variable selection makes model selection especially easy. Model fit is
assessed with conditional Cox-Snell residual plots and model choice is carried
out via LPML and DIC. Baseline survival is modeled with a novel transformed
Bernstein polynomial prior. All models are fit via a new function which calls
efficient compiled C++ in the R package \texttt{spBayesSurv}. The methodology
is broadly illustrated with simulations and real data applications. An
important finding is that proportional odds and accelerated failure time models
often fit significantly better than the commonly-used proportional hazards
model. Supplementary materials are available.",Haiming Zhou|Timothy Hanson,stat.AP|stat.ME
2017-02-28T17:17:24Z,2017-01-24T13:24:54Z,http://arxiv.org/abs/1701.07021v1,http://arxiv.org/abs/1701.07021v1,Short Term Power Demand Prediction Using Stochastic Gradient Boosting,"Power prediction demand is vital in power system and delivery engineering
fields. By efficiently predicting the power demand we can forecast the total
energy to be consumed in a certain city or district. Thus exact resources
required to produce the demand power can be allocated. In this paper a
Stochastic Gradient Boosting (aka Treeboost) model is used to predict the short
term power demand for the Emirate of Sharjah in the United Arab Emirates (UAE).
Results show that the proposed model gives promising results in comparison to
the model used by Sharjah Electricity and Water Authority (SEWA).",Ali Bou Nassif,stat.AP
2017-02-28T17:17:24Z,2017-01-24T07:37:01Z,http://arxiv.org/abs/1701.06754v1,http://arxiv.org/pdf/1701.06754v1,"Estimating Time-Varying Effective Connectivity in High-Dimensional fMRI
  Data Using Regime-Switching Factor Models","Recent studies on analyzing dynamic brain connectivity rely on sliding-window
analysis or time-varying coefficient models which are unable to capture both
smooth and abrupt changes simultaneously. Emerging evidence suggests
state-related changes in brain connectivity where dependence structure
alternates between a finite number of latent states or regimes. Another
challenge is inference of full-brain networks with large number of nodes. We
employ a Markov-switching dynamic factor model in which the state-driven
time-varying connectivity regimes of high-dimensional fMRI data are
characterized by lower-dimensional common latent factors following a
regime-switching process. It enables a reliable data-adaptive estimation of
change-points of connectivity regimes and the massive dependencies associated
with each regime. We consider the switching VAR to quantity the dynamic
effective connectivity. We propose a three-step estimation procedure: (1)
extracting the factors using principal component analysis (PCA) and (2)
identifying dynamic connectivity states using the factor-based switching vector
autoregressive (VAR) models in a state-space formulation using Kalman filter
and expectation-maximization (EM) algorithm and (3) constructing the
high-dimensional connectivity metrics for each state based on subspace
estimates. Simulation results show that our proposed estimator outperforms the
K-means clustering of time-windowed coefficients providing more accurate
estimation of regime dynamics and connectivity metrics in high-dimensional
settings. Applications to analyzing resting-state fMRI data identify dynamic
changes in brain states during rest and reveal distinct directed connectivity
patterns and modular organization in resting-state networks across different
states.",Chee-Ming Ting|Hernando Ombao|S. Balqis Samdin|Sh-Hussain Salleh,stat.AP
2017-02-28T17:17:24Z,2017-02-18T04:14:38Z,http://arxiv.org/abs/1701.06720v2,http://arxiv.org/abs/1701.06720v2,"Bayesian inference with Monte Carlo approximation: Measuring regional
  differentiation in ceramic and glass vessel assemblages in Republican Italy
  ca. 200 BCE - 20 CE","Methods of measuring differentiation in archaeological assemblages have long
been based on attribute-level analyses of assemblages. This paper considers a
method of comparing assemblages as probability distributions via the Hellinger
distance as calculated through a Dirichlet-categorical model of inference
using Monte Carlo methods of approximation. This method has application within
practice-theory traditions of archaeology an approach which seeks to measure
and associate different factors that comprise the habitus of society. It is
implemented here focusing on the question of regional food consumption habits
in Republican Italy in the last two centuries BCE toward informing a
perspective on mass social change.",Stephen A. Collins-Elliott,stat.AP
2017-02-28T17:17:24Z,2017-01-23T20:26:42Z,http://arxiv.org/abs/1701.06619v1,http://arxiv.org/pdf/1701.06619v1,Bayesian Inference in the Presence of Intractable Normalizing Functions,"Models with intractable normalizing functions arise frequently in statistics.
Common examples of such models include exponential random graph models for
social networks and Markov point processes for ecology and disease modeling.
Inference for these models is complicated because the normalizing functions of
their probability distributions include the parameters of interest. In Bayesian
analysis they result in so-called doubly intractable posterior distributions
which pose significant computational challenges. Several Monte Carlo methods
have emerged in recent years to address Bayesian inference for such models. We
provide a framework for understanding the algorithms and elucidate connections
among them. Through multiple simulated and real data examples we compare and
contrast the computational and statistical efficiency of these algorithms and
discuss their theoretical bases. Our study provides practical recommendations
for practitioners along with directions for future research for MCMC
methodologists.",Jaewoo Park|Murali Haran,stat.CO|stat.AP
2017-02-28T17:17:24Z,2017-01-23T15:19:48Z,http://arxiv.org/abs/1701.06445v1,http://arxiv.org/pdf/1701.06445v1,"Contrast Agent Quantification by Using Spatial Information in Dynamic
  Contrast Enhanced MRI","The purpose of this study is to investigate a method using simulations to
improve contrast agent quantification in Dynamic Contrast Enhanced MRI.
Bayesian hierarchical models (BHMs) are applied to smaller images
($10\times10\times10$) such that spatial information can be incorporated. Then
exploratory analysis is done for larger images ($64\times64\times64$) by using
maximum a posteriori (MAP).
  For smaller images: the estimators of proposed BHMs show improvements in
terms of the root mean squared error compared to the estimators in existing
method for a noise level equivalent of a 12-channel head coil at 3T. Moreover
Leroux model outperforms Besag models. For larger images: MAP estimators also
show improvements by assigning Leroux prior.",Jianfeng Wang|Anders Garpebring|Patrik Brynolfsson|Xijia Liu|Jun Yu,stat.AP
2017-02-28T17:17:24Z,2017-01-22T02:52:27Z,http://arxiv.org/abs/1701.08145v1,http://arxiv.org/pdf/1701.08145v1,"Synthesizing Correlations with Computational Likelihood Approach:
  Vitamin C Data","It is known that the primary source of dietary vitamin C is fruit and
vegetables and the plasma level of vitamin C has been considered a good
surrogate biomarker of vitamin C intake by fruit and vegetable consumption. To
combine the information about association between vitamin C intake and the
plasma level of vitamin C numerical approximation methods for likelihood
function of correlation coefficient are studied. The least squares approach is
used to estimate a log-likelihood function by a function from a space of
B-splines having desirable mathematical properties. The likelihood interval
from the Highest Likelihood Regions (HLR) is used for further inference. This
approach can be easily extended to the realm of meta-analysis involving sample
correlations from different studies by use of an approximated combined
likelihood function. The sample correlations between vitamin C intake and serum
level of vitamin C from many studies are used to illustrate application of this
approach.",Myung Soon Song,stat.AP
