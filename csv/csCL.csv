2017-02-28T17:20:02Z,2017-02-27T17:19:03Z,http://arxiv.org/abs/1702.08388v1,http://arxiv.org/pdf/1702.08388v1,Stance Classification of Social Media Users in Independence Movements,"Social media and data mining are increasingly being used to analyse political
and societal issues. Characterisation of users into socio-demographic groups is
crucial to improve these analyses. Here we undertake the classification of
social media users as supporting or opposing ongoing independence movements in
their territories. Independence movements occur in territories whose citizens
have conflicting national identities; users with opposing national identities
will then support or oppose the sense of being part of an independent nation
that differs from the officially recognised country. We describe a methodology
that relies on users' self-reported location to build datasets for three
territories -- Catalonia the Basque Country and Scotland -- and we test
language-independent classifiers using four types of features. We show the
effectiveness of the approach to build large annotated datasets and the
ability to achieve accurate language-independent classification performances
ranging from 85% to 97% for the three territories under study.",Arkaitz Zubiaga|Bo Wang|Maria Liakata|Rob Procter,cs.CL|cs.SI
2017-02-28T17:20:02Z,2017-02-27T14:37:21Z,http://arxiv.org/abs/1702.08303v1,http://arxiv.org/pdf/1702.08303v1,"Identifying beneficial task relations for multi-task learning in deep
  neural networks","Multi-task learning (MTL) in deep neural networks for NLP has recently
received increasing interest due to some compelling benefits including its
potential to efficiently regularize models and to reduce the need for labeled
data. While it has brought significant improvements in a number of NLP tasks
mixed results have been reported and little is known about the conditions
under which MTL leads to gains in NLP. This paper sheds light on the specific
task relations that can lead to gains from MTL models over single-task setups.",Joachim Bingel|Anders Søgaard,cs.CL|I.2.7
2017-02-28T17:20:02Z,2017-02-27T10:09:46Z,http://arxiv.org/abs/1702.08217v1,http://arxiv.org/pdf/1702.08217v1,A case study on English-Malayalam Machine Translation,"In this paper we present our work on a case study on Statistical Machine
Translation (SMT) and Rule based machine translation (RBMT) for translation
from English to Malayalam and Malayalam to English. One of the motivations of
our study is to make a three way performance comparison such as a) SMT and
RBMT b) English to Malayalam SMT and Malayalam to English SMT c) English to
Malayalam RBMT and Malayalam to English RBMT. We describe the development of
English to Malayalam and Malayalam to English baseline phrase based SMT system
and the evaluation of its performance compared against the RBMT system. Based
on our study the observations are: a) SMT systems outperform RBMT systems b)
In the case of SMT English - Malayalam systems perform better than that of
Malayalam - English systems c) In the case RBMT Malayalam to English systems
are performing better than English to Malayalam systems. Based on our
evaluations and detailed error analysis we describe the requirements of
incorporating morphological processing into the SMT to improve the accuracy of
translation.",Sreelekha S|Pushpak Bhattacharyya,cs.CL
2017-02-28T17:20:02Z,2017-02-27T04:16:01Z,http://arxiv.org/abs/1702.08139v1,http://arxiv.org/pdf/1702.08139v1,"Improved Variational Autoencoders for Text Modeling using Dilated
  Convolutions","Recent work on generative modeling of text has found that variational
auto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM
language models (Bowman et al. 2015). This negative result is so far poorly
understood but has been attributed to the propensity of LSTM decoders to
ignore conditioning information from the encoder. In this paper we experiment
with a new type of decoder for VAE: a dilated CNN. By changing the decoder's
dilation architecture we control the effective context from previously
generated words. In experiments we find that there is a trade off between the
contextual capacity of the decoder and the amount of encoding information used.
We show that with the right decoder VAE can outperform LSTM language models.
We demonstrate perplexity gains on two datasets representing the first
positive experimental result on the use VAE for generative modeling of text.
Further we conduct an in-depth investigation of the use of VAE (with our new
decoding architecture) for semi-supervised and unsupervised labeling tasks
demonstrating gains over several strong baselines.",Zichao Yang|Zhiting Hu|Ruslan Salakhutdinov|Taylor Berg-Kirkpatrick,cs.NE|cs.CL|cs.LG
2017-02-28T17:20:02Z,2017-02-26T11:22:41Z,http://arxiv.org/abs/1702.08021v1,http://arxiv.org/pdf/1702.08021v1,"Friends and Enemies of Clinton and Trump: Using Context for Detecting
  Stance in Political Tweets","Stance detection the task of identifying the speaker's opinion towards a
particular target has attracted the attention of researchers. This paper
describes a novel approach for detecting stance in Twitter. We define a set of
features in order to consider the context surrounding a target of interest with
the final aim of training a model for predicting the stance towards the
mentioned targets. In particular we are interested in investigating political
debates in social media. For this reason we evaluated our approach focusing on
two targets of the SemEval-2016 Task6 on Detecting stance in tweets which are
related to the political campaign for the 2016 U.S. presidential elections:
Hillary Clinton vs. Donald Trump. For the sake of comparison with the state of
the art we evaluated our model against the dataset released in the
SemEval-2016 Task 6 shared task competition. Our results outperform the best
ones obtained by participating teams and show that information about enemies
and friends of politicians help in detecting stance towards them.",Mirko Lai|Delia Irazú Hernández Farías|Viviana Patti|Paolo Rosso,cs.CL
2017-02-28T17:20:02Z,2017-02-26T08:07:26Z,http://arxiv.org/abs/1702.07998v1,http://arxiv.org/pdf/1702.07998v1,Detecting (Un)Important Content for Single-Document News Summarization,"We present a robust approach for detecting intrinsic sentence importance in
news by training on two corpora of document-summary pairs. When used for
single-document summarization our approach combined with the ""beginning of
document"" heuristic outperforms a state-of-the-art summarizer and the
beginning-of-article baseline in both automatic and manual evaluations. These
results represent an important advance because in the absence of cross-document
repetition single document summarizers for news have not been able to
consistently outperform the strong beginning-of-article baseline.",Yinfei Yang|Forrest Sheng Bao|Ani Nenkova,cs.CL
2017-02-28T17:20:02Z,2017-02-26T03:19:13Z,http://arxiv.org/abs/1702.07983v1,http://arxiv.org/pdf/1702.07983v1,Maximum-Likelihood Augmented Discrete Generative Adversarial Networks,"Despite the successes in capturing continuous distributions the application
of generative adversarial networks (GANs) to discrete settings like natural
language tasks is rather restricted. The fundamental reason is the difficulty
of back-propagation through discrete random variables combined with the
inherent instability of the GAN training objective. To address these problems
we propose Maximum-Likelihood Augmented Discrete Generative Adversarial
Networks. Instead of directly optimizing the GAN objective we derive a novel
and low-variance objective using the discriminator's output that follows
corresponds to the log-likelihood. Compared with the original the new
objective is proved to be consistent in theory and beneficial in practice. The
experimental results on various discrete datasets demonstrate the effectiveness
of the proposed approach.",Tong Che|Yanran Li|Ruixiang Zhang|R Devon Hjelm|Wenjie Li|Yangqiu Song|Yoshua Bengio,cs.AI|cs.CL|cs.LG
2017-02-28T17:20:02Z,2017-02-25T05:16:15Z,http://arxiv.org/abs/1702.07835v1,http://arxiv.org/pdf/1702.07835v1,Critical Survey of the Freely Available Arabic Corpora,"The availability of corpora is a major factor in building natural language
processing applications. However the costs of acquiring corpora can prevent
some researchers from going further in their endeavours. The ease of access to
freely available corpora is urgent needed in the NLP research community
especially for language such as Arabic. Currently there is not easy was to
access to a comprehensive and updated list of freely available Arabic corpora.
We present in this paper the results of a recent survey conducted to identify
the list of the freely available Arabic corpora and language resources. Our
preliminary results showed an initial list of 66 sources. We presents our
findings in the various categories studied and we provided the direct links to
get the data when possible.",Wajdi Zaghouani,cs.CL
2017-02-28T17:20:02Z,2017-02-25T03:20:49Z,http://arxiv.org/abs/1702.07826v1,http://arxiv.org/pdf/1702.07826v1,"Rationalization: A Neural Machine Translation Approach to Generating
  Natural Language Explanations","We introduce AI rationalization an approach for generating explanations of
autonomous system behavior as if a human had done the behavior. We describe a
rationalization technique that uses neural machine translation to translate
internal state-action representations of the autonomous agent into natural
language. We evaluate our technique in the Frogger game environment. The
natural language is collected from human players thinking out loud as they play
the game. We motivate the use of rationalization as an approach to explanation
generation show the results of experiments on the accuracy of our
rationalization technique and describe future research agenda.",Brent Harrison|Upol Ehsan|Mark O. Riedl,cs.AI|cs.CL|cs.HC|cs.LG
2017-02-28T17:20:02Z,2017-02-25T03:11:04Z,http://arxiv.org/abs/1702.07825v1,http://arxiv.org/pdf/1702.07825v1,Deep Voice: Real-time Neural Text-to-Speech,"We present Deep Voice a production-quality text-to-speech system constructed
entirely from deep neural networks. Deep Voice lays the groundwork for truly
end-to-end neural speech synthesis. The system comprises five major building
blocks: a segmentation model for locating phoneme boundaries a
grapheme-to-phoneme conversion model a phoneme duration prediction model a
fundamental frequency prediction model and an audio synthesis model. For the
segmentation model we propose a novel way of performing phoneme boundary
detection with deep neural networks using connectionist temporal classification
(CTC) loss. For the audio synthesis model we implement a variant of WaveNet
that requires fewer parameters and trains faster than the original. By using a
neural network for each component our system is simpler and more flexible than
traditional text-to-speech systems where each component requires laborious
feature engineering and extensive domain expertise. Finally we show that
inference with our system can be performed faster than real time and describe
optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x
speedups over existing implementations.",Sercan O. Arik|Mike Chrzanowski|Adam Coates|Gregory Diamos|Andrew Gibiansky|Yongguo Kang|Xian Li|John Miller|Jonathan Raiman|Shubho Sengupta|Mohammad Shoeybi,cs.CL|cs.LG|cs.NE|cs.SD
2017-02-28T17:20:05Z,2017-02-24T22:49:13Z,http://arxiv.org/abs/1702.07793v1,http://arxiv.org/pdf/1702.07793v1,Residual Convolutional CTC Networks for Automatic Speech Recognition,"Deep learning approaches have been widely used in Automatic Speech
Recognition (ASR) and they have achieved a significant accuracy improvement.
Especially Convolutional Neural Networks (CNNs) have been revisited in ASR
recently. However most CNNs used in existing work have less than 10 layers
which may not be deep enough to capture all human speech signal information. In
this paper we propose a novel deep and wide CNN architecture denoted as
RCNN-CTC which has residual connections and Connectionist Temporal
Classification (CTC) loss function. RCNN-CTC is an end-to-end system which can
exploit temporal and spectral structures of speech signals simultaneously.
Furthermore we introduce a CTC-based system combination which is different
from the conventional frame-wise senone-based one. The basic subsystems adopted
in the combination are different types and thus mutually complementary to each
other. Experimental results show that our proposed single system RCNN-CTC can
achieve the lowest word error rate (WER) on WSJ and Tencent Chat data sets
compared to several widely used neural network systems in ASR. In addition the
proposed system combination can offer a further error reduction on these two
data sets resulting in relative WER reductions of $14.91\%$ and $6.52\%$ on
WSJ dev93 and Tencent Chat data sets respectively.",Yisen Wang|Xuejiao Deng|Songbai Pu|Zhiheng Huang,cs.CL
2017-02-28T17:20:05Z,2017-02-24T19:00:01Z,http://arxiv.org/abs/1702.07717v1,http://arxiv.org/pdf/1702.07717v1,"When confidence and competence collide: Effects on online
  decision-making discussions","Group discussions are a way for individuals to exchange ideas and arguments
in order to reach better decisions than they could on their own. One of the
premises of productive discussions is that better solutions will prevail and
that the idea selection process is mediated by the (relative) competence of the
individuals involved. However since people may not know their actual
competence on a new task their behavior is influenced by their self-estimated
competence --- that is their confidence --- which can be misaligned with their
actual competence.
  Our goal in this work is to understand the effects of confidence-competence
misalignment on the dynamics and outcomes of discussions. To this end we
design a large-scale natural setting in the form of an online team-based
geography game that allows us to disentangle confidence from competence and
thus separate their effects.
  We find that in task-oriented discussions the more-confident individuals
have a larger impact on the group's decisions even when these individuals are
at the same level of competence as their teammates. Furthermore this
unjustified role of confidence in the decision-making process often leads teams
to under-perform. We explore this phenomenon by investigating the effects of
confidence on conversational dynamics.",Liye Fu|Lillian Lee|Cristian Danescu-Niculescu-Mizil,cs.CL|cs.CY|cs.HC|cs.SI|physics.soc-ph
2017-02-28T17:20:05Z,2017-02-24T17:40:28Z,http://arxiv.org/abs/1702.07680v1,http://arxiv.org/pdf/1702.07680v1,Consistent Alignment of Word Embedding Models,"Word embedding models offer continuous vector representations that can
capture rich contextual semantics based on their word co-occurrence patterns.
While these word vectors can provide very effective features used in many NLP
tasks such as clustering similar words and inferring learning relationships
many challenges and open research questions remain. In this paper we propose a
solution that aligns variations of the same model (or different models) in a
joint low-dimensional latent space leveraging carefully generated synthetic
data points. This generative process is inspired by the observation that a
variety of linguistic relationships is captured by simple linear operations in
embedded space. We demonstrate that our approach can lead to substantial
improvements in recovering embeddings of local neighborhoods.",Cem Safak Sahin|Rajmonda S. Caceres|Brandon Oselio|William M. Campbell,cs.CL|cs.IR|stat.ML
2017-02-28T17:20:05Z,2017-02-24T09:26:10Z,http://arxiv.org/abs/1702.07507v1,http://arxiv.org/pdf/1702.07507v1,Use Generalized Representations But Do Not Forget Surface Features,"Only a year ago all state-of-the-art coreference resolvers were using an
extensive amount of surface features. Recently there was a paradigm shift
towards using word embeddings and deep neural networks where the use of
surface features is very limited. In this paper we show that a simple SVM
model with surface features outperforms more complex neural models for
detecting anaphoric mentions. Our analysis suggests that using generalized
representations and surface features have different strength that should be
both taken into account for improving coreference resolution.",Nafise Sadat Moosavi|Michael Strube,cs.CL
2017-02-28T17:20:05Z,2017-02-24T08:35:10Z,http://arxiv.org/abs/1702.07495v1,http://arxiv.org/pdf/1702.07495v1,Dirichlet-vMF Mixture Model,"This document is about the multi-document Von-Mises-Fisher mixture model with
a Dirichlet prior referred to as VMFMix. VMFMix is analogous to Latent
Dirichlet Allocation (LDA) in that they can capture the co-occurrence patterns
acorss multiple documents. The difference is that in VMFMix the topic-word
distribution is defined on a continuous n-dimensional hypersphere. Hence VMFMix
is used to derive topic embeddings i.e. representative vectors from multiple
sets of embedding vectors. An efficient Variational Expectation-Maximization
inference algorithm is derived. The performance of VMFMix on two document
classification tasks is reported with some preliminary analysis.",Shaohua Li,cs.CL
2017-02-28T17:20:05Z,2017-02-23T18:19:35Z,http://arxiv.org/abs/1702.07324v1,http://arxiv.org/pdf/1702.07324v1,"Inherent Biases of Recurrent Neural Networks for Phonological
  Assimilation and Dissimilation","A recurrent neural network model of phonological pattern learning is
proposed. The model is a relatively simple neural network with one recurrent
layer and displays biases in learning that mimic observed biases in human
learning. Single-feature patterns are learned faster than two-feature patterns
and vowel or consonant-only patterns are learned faster than patterns involving
vowels and consonants mimicking the results of laboratory learning
experiments. In non-recurrent models capturing these biases requires the use
of alpha features or some other representation of repeated features but with a
recurrent neural network these elaborations are not necessary.",Amanda Doucette,cs.CL
2017-02-28T17:20:05Z,2017-02-24T12:59:19Z,http://arxiv.org/abs/1702.07285v2,http://arxiv.org/pdf/1702.07285v2,Are Emojis Predictable?,"Emojis are ideograms which are naturally combined with plain text to visually
complement or condense the meaning of a message. Despite being widely used in
social media their underlying semantics have received little attention from a
Natural Language Processing standpoint. In this paper we investigate the
relation between words and emojis studying the novel task of predicting which
emojis are evoked by text-based tweet messages. We train several models based
on Long Short-Term Memory networks (LSTMs) in this task. Our experimental
results show that our neural model outperforms two baselines as well as humans
solving the same task suggesting that computational models are able to better
capture the underlying semantics of emojis.",Francesco Barbieri|Miguel Ballesteros|Horacio Saggion,cs.CL
2017-02-28T17:20:05Z,2017-02-23T13:13:53Z,http://arxiv.org/abs/1702.07203v1,http://arxiv.org/pdf/1702.07203v1,"Utilizing Lexical Similarity for pivot translation involving
  resource-poor related languages","We investigate the use of pivot languages for phrase-based statistical
machine translation (PB-SMT) between related languages with limited parallel
corpora. We show that subword-level pivot translation via a related pivot
language is: (i) highly competitive with the best direct translation model and
(ii) better than a pivot model which uses an unrelated pivot language but has
at its disposal large parallel corpora to build the source-pivot (S-P) and
pivot-target (P-T) translation models. In contrast pivot models trained at
word and morpheme level are far inferior to their direct counterparts. We also
show that using multiple related pivot languages can outperform a direct
translation model. Thus the use of subwords as translation units coupled with
the use of multiple related pivot languages can compensate for the lack of a
direct parallel corpus. Subword units make pivot models competitive by (i)
utilizing lexical similarity to improve the underlying S-P and P-T translation
models and (ii) reducing loss of translation candidates during pivoting.",Anoop Kunchukuttan|Maulik Shah|Pradyot Prakash|Pushpak Bhattacharyya,cs.CL
2017-02-28T17:20:05Z,2017-02-23T12:00:10Z,http://arxiv.org/abs/1702.07186v1,http://arxiv.org/pdf/1702.07186v1,Stability of Topic Modeling via Matrix Factorization,"Topic models can provide us with an insight into the underlying latent
structure of a large corpus of documents. A range of methods have been proposed
in the literature including probabilistic topic models and techniques based on
matrix factorization. However in both cases standard implementations rely on
stochastic elements in their initialization phase which can potentially lead
to different results being generated on the same corpus when using the same
parameter values. This corresponds to the concept of ""instability"" which has
previously been studied in the context of $k$-means clustering. In many
applications of topic modeling this problem of instability is not considered
and topic models are treated as being definitive even though the results may
change considerably if the initialization process is altered. In this paper we
demonstrate the inherent instability of popular topic modeling approaches
using a number of new measures to assess stability. To address this issue in
the context of matrix factorization for topic modeling we propose the use of
ensemble learning strategies. Based on experiments performed on annotated text
corpora we show that a K-Fold ensemble strategy combining both ensembles and
structured initialization can significantly reduce instability while
simultaneously yielding more accurate topic models.",Mark Belford|Brian Mac Namee|Derek Greene,cs.IR|cs.CL|cs.LG|stat.ML
2017-02-28T17:20:05Z,2017-02-23T07:16:03Z,http://arxiv.org/abs/1702.07117v1,http://arxiv.org/pdf/1702.07117v1,"LTSG: Latent Topical Skip-Gram for Mutually Learning Topic Model and
  Vector Representations","Topic models have been widely used in discovering latent topics which are
shared across documents in text mining. Vector representations word embeddings
and topic embeddings map words and topics into a low-dimensional and dense
real-value vector space which have obtained high performance in NLP tasks.
However most of the existing models assume the result trained by one of them
are perfect correct and used as prior knowledge for improving the other model.
Some other models use the information trained from external large corpus to
help improving smaller corpus. In this paper we aim to build such an algorithm
framework that makes topic models and vector representations mutually improve
each other within the same corpus. An EM-style algorithm framework is employed
to iteratively optimize both topic model and vector representations.
Experimental results show that our model outperforms state-of-art methods on
various NLP tasks.",Jarvan Law|Hankz Hankui Zhuo|Junhua He|Erhu Rong,cs.CL
2017-02-28T17:20:09Z,2017-02-23T04:27:49Z,http://arxiv.org/abs/1702.07092v1,http://arxiv.org/pdf/1702.07092v1,A Neural Attention Model for Categorizing Patient Safety Events,"Medical errors are leading causes of death in the US and as such prevention
of these errors is paramount to promoting health care. Patient Safety Event
reports are narratives describing potential adverse events to the patients and
are important in identifying and preventing medical errors. We present a neural
network architecture for identifying the type of safety events which is the
first step in understanding these narratives. Our proposed model is based on a
soft neural attention model to improve the effectiveness of encoding long
sequences. Empirical results on two large-scale real-world datasets of patient
safety reports demonstrate the effectiveness of our method with significant
improvements over existing methods.",Arman Cohan|Allan Fong|Nazli Goharian|Raj Ratwani,cs.CL|cs.IR
2017-02-28T17:20:09Z,2017-02-23T02:31:03Z,http://arxiv.org/abs/1702.07071v1,http://arxiv.org/pdf/1702.07071v1,"Pronunciation recognition of English phonemes /\textipa{@}/ /æ/
  /\textipa{A}:/ and /\textipa{2}/ using Formants and Mel Frequency Cepstral
  Coefficients","The Vocal Joystick Vowel Corpus by Washington University was used to study
monophthongs pronounced by native English speakers. The objective of this study
was to quantitatively measure the extent at which speech recognition methods
can distinguish between similar sounding vowels. In particular the phonemes
/\textipa{@}/ /{\ae}/ /\textipa{A}:/ and /\textipa{2}/ were analysed. 748
sound files from the corpus were used and subjected to Linear Predictive Coding
(LPC) to compute their formants and to Mel Frequency Cepstral Coefficients
(MFCC) algorithm to compute the cepstral coefficients. A Decision Tree
Classifier was used to build a predictive model that learnt the patterns of the
two first formants measured in the data set as well as the patterns of the 13
cepstral coefficients. An accuracy of 70\% was achieved using formants for the
mentioned phonemes. For the MFCC analysis an accuracy of 52 \% was achieved and
an accuracy of 71\% when /\textipa{@}/ was ignored. The results obtained show
that the studied algorithms are far from mimicking the ability of
distinguishing subtle differences in sounds like human hearing does.",Keith Y. Patarroyo|Vladimir Vargas-Calderón,cs.CL|cs.SD
2017-02-28T17:20:09Z,2017-02-22T23:39:03Z,http://arxiv.org/abs/1702.07046v1,http://arxiv.org/pdf/1702.07046v1,Feature Generation for Robust Semantic Role Labeling,"Hand-engineered feature sets are a well understood method for creating robust
NLP models but they require a lot of expertise and effort to create. In this
work we describe how to automatically generate rich feature sets from simple
units called featlets requiring less engineering. Using information gain to
guide the generation process we train models which rival the state of the art
on two standard Semantic Role Labeling datasets with almost no task or
linguistic insight.",Travis Wolfe|Mark Dredze|Benjamin Van Durme,cs.CL
2017-02-28T17:20:09Z,2017-02-22T21:44:02Z,http://arxiv.org/abs/1702.07015v1,http://arxiv.org/pdf/1702.07015v1,Unsupervised Learning of Morphological Forests,"This paper focuses on unsupervised modeling of morphological families
collectively comprising a forest over the language vocabulary. This formulation
enables us to capture edgewise properties reflecting single-step morphological
derivations along with global distributional properties of the entire forest.
These global properties constrain the size of the affix set and encourage
formation of tight morphological families. The resulting objective is solved
using Integer Linear Programming (ILP) paired with contrastive estimation. We
train the model by alternating between optimizing the local log-linear model
and the global ILP objective. We evaluate our system on three tasks: root
detection clustering of morphological families and segmentation. Our
experiments demonstrate that our model yields consistent gains in all three
tasks compared with the best published results.",Jiaming Luo|Karthik Narasimhan|Regina Barzilay,cs.CL
2017-02-28T17:20:09Z,2017-02-22T16:50:25Z,http://arxiv.org/abs/1702.06891v1,http://arxiv.org/pdf/1702.06891v1,EVE: Explainable Vector Based Embedding Technique Using Wikipedia,"We present an unsupervised explainable word embedding technique called EVE
which is built upon the structure of Wikipedia. The proposed model defines the
dimensions of a semantic vector representing a word using human-readable
labels thereby it readily interpretable. Specifically each vector is
constructed using the Wikipedia category graph structure together with the
Wikipedia article link structure. To test the effectiveness of the proposed
word embedding model we consider its usefulness in three fundamental tasks: 1)
intruder detection - to evaluate its ability to identify a non-coherent vector
from a list of coherent vectors 2) ability to cluster - to evaluate its
tendency to group related vectors together while keeping unrelated vectors in
separate clusters and 3) sorting relevant items first - to evaluate its
ability to rank vectors (items) relevant to the query in the top order of the
result. For each task we also propose a strategy to generate a task-specific
human-interpretable explanation from the model. These demonstrate the overall
effectiveness of the explainable embeddings generated by EVE. Finally we
compare EVE with the Word2Vec FastText and GloVe embedding techniques across
the three tasks and report improvements over the state-of-the-art.",M. Atif Qureshi|Derek Greene,cs.CL
2017-02-28T17:20:09Z,2017-02-22T16:14:12Z,http://arxiv.org/abs/1702.06875v1,http://arxiv.org/abs/1702.06875v1,Triaging Content Severity in Online Mental Health Forums,"Mental health forums are online communities where people express their issues
and seek help from moderators and other users. In such forums there are often
posts with severe content indicating that the user is in acute distress and
there is a risk of attempted self-harm. Moderators need to respond to these
severe posts in a timely manner to prevent potential self-harm. However the
large volume of daily posted content makes it difficult for the moderators to
locate and respond to these critical posts. We present a framework for triaging
user content into four severity categories which are defined based on
indications of self-harm ideation. Our models are based on a feature-rich
classification framework which includes lexical psycholinguistic contextual
and topic modeling features. Our approaches improve the state of the art in
triaging the content severity in mental health forums by large margins (up to
17% improvement over the F-1 scores). Using the proposed model we analyze the
mental state of users and we show that overall long-term users of the forum
demonstrate a decreased severity of risk over time. Our analysis on the
interaction of the moderators with the users further indicates that without an
automatic way to identify critical content it is indeed challenging for the
moderators to provide timely response to the users in need.",Arman Cohan|Sydney Young|Andrew Yates|Nazli Goharian,cs.CL|cs.IR|cs.SI
2017-02-28T17:20:09Z,2017-02-22T13:49:18Z,http://arxiv.org/abs/1702.06794v1,http://arxiv.org/pdf/1702.06794v1,"Tackling Error Propagation through Reinforcement Learning: A Case of
  Greedy Dependency Parsing","Error propagation is a common problem in NLP. Reinforcement learning explores
erroneous states during training and can therefore be more robust when mistakes
are made early in a process. In this paper we apply reinforcement learning to
greedy dependency parsing which is known to suffer from error propagation.
Reinforcement learning improves accuracy of both labeled and unlabeled
dependencies of the Stanford Neural Dependency Parser a high performance
greedy parser while maintaining its efficiency. We investigate the portion of
errors which are the result of error propagation and confirm that reinforcement
learning reduces the occurrence of error propagation.",Minh Le|Antske Fokkens,cs.CL
2017-02-28T17:20:09Z,2017-02-22T12:42:06Z,http://arxiv.org/abs/1702.06777v1,http://arxiv.org/pdf/1702.06777v1,Dialectometric analysis of language variation in Twitter,"In the last few years microblogging platforms such as Twitter have given
rise to a deluge of textual data that can be used for the analysis of informal
communication between millions of individuals. In this work we propose an
information-theoretic approach to geographic language variation using a corpus
based on Twitter. We test our models with tens of concepts and their associated
keywords detected in Spanish tweets geolocated in Spain. We employ
dialectometric measures (cosine similarity and Jensen-Shannon divergence) to
quantify the linguistic distance on the lexical level between cells created in
a uniform grid over the map. This can be done for a single concept or in the
general case taking into account an average of the considered variants. The
latter permits an analysis of the dialects that naturally emerge from the data.
Interestingly our results reveal the existence of two dialect macrovarieties.
The first group includes a region-specific speech spoken in small towns and
rural areas whereas the second cluster encompasses cities that tend to use a
more uniform variety. Since the results obtained with the two different metrics
qualitatively agree our work suggests that social media corpora can be
efficiently used for dialectometric analyses.",Gonzalo Donoso|David Sanchez,cs.CL|cs.IR|cs.SI|physics.soc-ph
2017-02-28T17:20:09Z,2017-02-22T10:34:47Z,http://arxiv.org/abs/1702.06740v1,http://arxiv.org/pdf/1702.06740v1,A Progressive Learning Approach to Chinese SRL Using Heterogeneous Data,"Previous studies on Chinese semantic role labeling (SRL) have concentrated on
single semantically annotated corpus. But the training data of single corpus is
often limited. Meanwhile there usually exists other semantically annotated
corpora for Chinese SRL scattered across different annotation frameworks. Data
sparsity remains a bottleneck. This situation calls for larger training
datasets or effective approaches which can take advantage of highly
heterogeneous data. In these papers we focus mainly on the latter that is to
improve Chinese SRL by using heterogeneous corpora together. We propose a novel
progressive learning model which augments the Progressive Neural Network with
Gated Recurrent Adapters. The model can accommodate heterogeneous inputs and
effectively transfer knowledge between them. We also release a new corpus
Chinese SemBank for Chinese SRL. Experiments on CPB 1.0 show that ours model
outperforms state-of-the-art methods.",Qiaolin Xia|Baobao Chang|Zhifang Sui,cs.CL
2017-02-28T17:20:09Z,2017-02-22T10:10:44Z,http://arxiv.org/abs/1702.06733v1,http://arxiv.org/pdf/1702.06733v1,Improving a Strong Neural Parser with Conjunction-Specific Features,"While dependency parsers reach very high overall accuracy some dependency
relations are much harder than others. In particular dependency parsers
perform poorly in coordination construction (i.e. correctly attaching the
""conj"" relation). We extend a state-of-the-art dependency parser with
conjunction-specific features focusing on the similarity between the conjuncts
head words. Training the extended parser yields an improvement in ""conj""
attachment as well as in overall dependency parsing accuracy on the Stanford
dependency conversion of the Penn TreeBank.",Jessica Ficler|Yoav Goldberg,cs.CL
2017-02-28T17:20:13Z,2017-02-22T08:59:37Z,http://arxiv.org/abs/1702.06709v1,http://arxiv.org/pdf/1702.06709v1,"Fine-Grained Entity Type Classification by Jointly Learning
  Representations and Label Embeddings","Fine-grained entity type classification (FETC) is the task of classifying an
entity mention to a broad set of types. Distant supervision paradigm is
extensively used to generate training data for this task. However generated
training data assigns same set of labels to every mention of an entity without
considering its local context. Existing FETC systems have two major drawbacks:
assuming training data to be noise free and use of hand crafted features. Our
work overcomes both drawbacks. We propose a neural network model that jointly
learns entity mentions and their context representation to eliminate use of
hand crafted features. Our model treats training data as noisy and uses
non-parametric variant of hinge loss function. Experiments show that the
proposed model outperforms previous state-of-the-art methods on two publicly
available datasets namely FIGER (GOLD) and BBN with an average relative
improvement of 2.69% in micro-F1 score. Knowledge learnt by our model on one
dataset can be transferred to other datasets while using same model or other
FETC systems. These approaches of transferring knowledge further improve the
performance of respective models.",Abhishek|Ashish Anand|Amit Awekar,cs.CL
2017-02-28T17:20:13Z,2017-02-22T08:32:47Z,http://arxiv.org/abs/1702.06703v1,http://arxiv.org/pdf/1702.06703v1,Data Distillation for Controlling Specificity in Dialogue Generation,"People speak at different levels of specificity in different situations.
Depending on their knowledge interlocutors mood etc.} A conversational agent
should have this ability and know when to be specific and when to be general.
We propose an approach that gives a neural network--based conversational agent
this ability. Our approach involves alternating between \emph{data
distillation} and model training : removing training examples that are closest
to the responses most commonly produced by the model trained from the last
round and then retrain the model on the remaining dataset. Dialogue generation
models trained with different degrees of data distillation manifest different
levels of specificity.
  We then train a reinforcement learning system for selecting among this pool
of generation models to choose the best level of specificity for a given
input. Compared to the original generative model trained without distillation
the proposed system is capable of generating more interesting and
higher-quality responses in addition to appropriately adjusting specificity
depending on the context.
  Our research constitutes a specific case of a broader approach involving
training multiple subsystems from a single dataset distinguished by differences
in a specific property one wishes to model. We show that from such a set of
subsystems one can use reinforcement learning to build a system that tailors
its output to different input contexts at test time.",Jiwei Li|Will Monroe|Dan Jurafsky,cs.CL
2017-02-28T17:20:13Z,2017-02-22T08:19:38Z,http://arxiv.org/abs/1702.06700v1,http://arxiv.org/pdf/1702.06700v1,"Task-driven Visual Saliency and Attention-based Visual Question
  Answering","Visual question answering (VQA) has witnessed great progress since May 2015
as a classic problem unifying visual and textual data into a system. Many
enlightening VQA works explore deep into the image and question encodings and
fusing methods of which attention is the most effective and infusive
mechanism. Current attention based methods focus on adequate fusion of visual
and textual features but lack the attention to where people focus to ask
questions about the image. Traditional attention based methods attach a single
value to the feature at each spatial location which losses many useful
information. To remedy these problems we propose a general method to perform
saliency-like pre-selection on overlapped region features by the interrelation
of bidirectional LSTM (BiLSTM) and use a novel element-wise multiplication
based attention method to capture more competent correlation information
between visual and textual features. We conduct experiments on the large-scale
COCO-VQA dataset and analyze the effectiveness of our model demonstrated by
strong empirical results.",Yuetan Lin|Zhangyang Pang|Donghui Wang|Yueting Zhuang,cs.CV|cs.AI|cs.CL|cs.NE
2017-02-28T17:20:13Z,2017-02-22T07:41:08Z,http://arxiv.org/abs/1702.06696v1,http://arxiv.org/pdf/1702.06696v1,One Representation per Word - Does it make Sense for Composition?,"In this paper we investigate whether an a priori disambiguation of word
senses is strictly necessary or whether the meaning of a word in context can be
disambiguated through composition alone. We evaluate the performance of
off-the-shelf single-vector and multi-sense vector models on a benchmark phrase
similarity task and a novel task for word-sense discrimination. We find that
single-sense vector models perform as well or better than multi-sense vector
models despite arguably less clean elementary representations. Our findings
furthermore show that simple composition functions such as pointwise addition
are able to recover sense specific information from a single-sense vector model
remarkably well.",Thomas Kober|Julie Weeds|John Wilkie|Jeremy Reffin|David Weir,cs.CL
2017-02-28T17:20:13Z,2017-02-22T04:54:43Z,http://arxiv.org/abs/1702.06677v1,http://arxiv.org/pdf/1702.06677v1,Discussion quality diffuses in the digital public square,"Studies of online social influence have demonstrated that friends have
important effects on many types of behavior in a wide variety of settings.
However we know much less about how influence works among relative strangers
in digital public squares despite important conversations happening in such
spaces. We present the results of a study on large public Facebook pages where
we randomly used two different methods--most recent and social feedback--to
order comments on posts. We find that the social feedback condition results in
higher quality viewed comments and response comments. After measuring the
average quality of comments written by users before the study we find that
social feedback has a positive effect on response quality for both low and high
quality commenters. We draw on a theoretical framework of social norms to
explain this empirical result. In order to examine the influence mechanism
further we measure the similarity between comments viewed and written during
the study finding that similarity increases for the highest quality
contributors under the social feedback condition. This suggests that in
addition to norms some individuals may respond with increased relevance to
high-quality comments.",George Berry|Sean J. Taylor,cs.CY|cs.CL|cs.SI
2017-02-28T17:20:13Z,2017-02-22T04:50:23Z,http://arxiv.org/abs/1702.06675v1,http://arxiv.org/pdf/1702.06675v1,Context-Aware Prediction of Derivational Word-forms,"Derivational morphology is a fundamental and complex characteristic of
language. In this paper we propose the new task of predicting the derivational
form of a given base-form lemma that is appropriate for a given context. We
present an encoder--decoder style neural network to produce a derived form
character-by-character based on its corresponding character-level
representation of the base form and the context. We demonstrate that our model
is able to generate valid context-sensitive derivations from known base forms
but is less accurate under a lexicon agnostic setting.",Ekaterina Vylomova|Ryan Cotterell|Timothy Baldwin|Trevor Cohn,cs.CL
2017-02-28T17:20:13Z,2017-02-22T04:30:09Z,http://arxiv.org/abs/1702.06672v1,http://arxiv.org/pdf/1702.06672v1,Calculating Probabilities Simplifies Word Learning,"Children can use the statistical regularities of their environment to learn
word meanings a mechanism known as cross-situational learning. We take a
computational approach to investigate how the information present during each
observation in a cross-situational framework can affect the overall acquisition
of word meanings. We do so by formulating various in-the-moment learning
mechanisms that are sensitive to different statistics of the environment such
as counts and conditional probabilities. Each mechanism introduces a unique
source of competition or mutual exclusivity bias to the model; the mechanism
that maximally uses the model's knowledge of word meanings performs the best.
Moreover the gap between this mechanism and others is amplified in more
challenging learning scenarios such as learning from few examples.",Aida Nematzadeh|Barend Beekhuizen|Shanshan Huang|Suzanne Stevenson,cs.CL
2017-02-28T17:20:13Z,2017-02-22T03:14:36Z,http://arxiv.org/abs/1702.06663v1,http://arxiv.org/pdf/1702.06663v1,"Guided Deep List: Automating the Generation of Epidemiological Line
  Lists from Open Sources","Real-time monitoring and responses to emerging public health threats rely on
the availability of timely surveillance data. During the early stages of an
epidemic the ready availability of line lists with detailed tabular
information about laboratory-confirmed cases can assist epidemiologists in
making reliable inferences and forecasts. Such inferences are crucial to
understand the epidemiology of a specific disease early enough to stop or
control the outbreak. However construction of such line lists requires
considerable human supervision and therefore difficult to generate in
real-time. In this paper we motivate Guided Deep List the first tool for
building automated line lists (in near real-time) from open source reports of
emerging disease outbreaks. Specifically we focus on deriving epidemiological
characteristics of an emerging disease and the affected population from reports
of illness. Guided Deep List uses distributed vector representations (ala
word2vec) to discover a set of indicators for each line list feature. This
discovery of indicators is followed by the use of dependency parsing based
techniques for final extraction in tabular form. We evaluate the performance of
Guided Deep List against a human annotated line list provided by HealthMap
corresponding to MERS outbreaks in Saudi Arabia. We demonstrate that Guided
Deep List extracts line list features with increased accuracy compared to a
baseline method. We further show how these automatically extracted line list
features can be used for making epidemiological inferences such as inferring
demographics and symptoms-to-hospitalization period of affected individuals.",Saurav Ghosh|Prithwish Chakraborty|Bryan L. Lewis|Maimuna S. Majumder|Emily Cohn|John S. Brownstein|Madhav V. Marathe|Naren Ramakrishnan,cs.CL|cs.IR
2017-02-28T17:20:13Z,2017-02-21T21:36:51Z,http://arxiv.org/abs/1702.06594v1,http://arxiv.org/pdf/1702.06594v1,On the Complexity of CCG Parsing,"We study the parsing complexity of Combinatory Categorial Grammar (CCG) in
the formalism of Vijay-Shanker and Weir (1994). As our main result we prove
that any parsing algorithm for this formalism will necessarily take exponential
time when the size of the grammar and not only the length of the input
sentence is included in the analysis. This result sets the formalism of
Vijay-Shanker and Weir (1994) apart from weakly equivalent formalisms such as
Tree-Adjoining Grammar (TAG) for which parsing can be performed in time
polynomial in the combined size of grammar and input sentence. Our proof
highlights important differences between the formalism of Vijay-Shanker and
Weir (1994) and contemporary incarnations of CCG.",Marco Kuhlmann|Giorgio Satta|Peter Jonsson,cs.CL
2017-02-28T17:20:13Z,2017-02-21T21:24:26Z,http://arxiv.org/abs/1702.06589v1,http://arxiv.org/pdf/1702.06589v1,"Neural Multi-Step Reasoning for Question Answering on Semi-Structured
  Tables","Advances in natural language processing tasks have gained momentum in recent
years due to the increasingly popular neural network methods. In this paper we
explore deep learning techniques for answering multi-step reasoning questions
that operate on semi-structured tables. Challenges here arise from the level of
logical compositionality expressed by questions as well as the domain
openness. Our approach is weakly supervised trained on question-answer-table
triples without requiring intermediate strong supervision. It performs two
phases: first machine understandable logical forms (programs) are generated
from natural language questions following the work of [Pasupat and Liang
2015]. Second paraphrases of logical forms and questions are embedded in a
jointly learned vector space using word and character convolutional neural
networks. A neural scoring function is further used to rank and retrieve the
most probable logical form (interpretation) of a question. Our best single
model achieves 34.8% accuracy on the WikiTableQuestions dataset while the best
ensemble of our models pushes the state-of-the-art score on this task to 38.7%
thus slightly surpassing both the engineered feature scoring baseline as well
as the Neural Programmer model of [Neelakantan et al. 2016].",Till Haug|Octavian-Eugen Ganea|Paulina Grnarova,cs.CL
2017-02-28T17:20:17Z,2017-02-21T18:24:52Z,http://arxiv.org/abs/1702.06510v1,http://arxiv.org/pdf/1702.06510v1,"Algorithmes de classification et d'optimisation: participation du
  LIA/ADOC á DEFT'14","This year the DEFT campaign (D\'efi Fouilles de Textes) incorporates a task
which aims at identifying the session in which articles of previous TALN
conferences were presented. We describe the three statistical systems developed
at LIA/ADOC for this task. A fusion of these systems enables us to obtain
interesting results (micro-precision score of 0.76 measured on the test corpus)",Luis Adrián Cabrera-Diego|Stéphane Huet|Bassam Jabaian|Alejandro Molina|Juan-Manuel Torres-Moreno|Marc El-Bèze|Barthélémy Durette,cs.IR|cs.CL
2017-02-28T17:20:17Z,2017-02-21T17:14:56Z,http://arxiv.org/abs/1702.06478v1,http://arxiv.org/pdf/1702.06478v1,Systèmes du LIA à DEFT'13,"The 2013 D\'efi de Fouille de Textes (DEFT) campaign is interested in two
types of language analysis tasks the document classification and the
information extraction in the specialized domain of cuisine recipes. We present
the systems that the LIA has used in DEFT 2013. Our systems show interesting
results even though the complexity of the proposed tasks.",Xavier Bost|Ilaria Brunetti|Luis Adrián Cabrera-Diego|Jean-Valère Cossu|Andréa Linhares|Mohamed Morchid|Juan-Manuel Torres-Moreno|Marc El-Bèze|Richard Dufour,cs.CL|cs.IR
2017-02-28T17:20:17Z,2017-02-21T16:26:54Z,http://arxiv.org/abs/1702.06467v1,http://arxiv.org/pdf/1702.06467v1,"Efficient Social Network Multilingual Classification using Character
  POS n-grams and Dynamic Normalization","In this paper we describe a dynamic normalization process applied to social
network multilingual documents (Facebook and Twitter) to improve the
performance of the Author profiling task for short texts. After the
normalization process $n$-grams of characters and n-grams of POS tags are
obtained to extract all the possible stylistic information encoded in the
documents (emoticons character flooding capital letters references to other
users hyperlinks hashtags etc.). Experiments with SVM showed up to 90% of
performance.",Carlos-Emiliano González-Gallardo|Juan-Manuel Torres-Moreno|Azucena Montes Rendón|Gerardo Sierra,cs.IR|cs.CL|cs.SI
2017-02-28T17:20:17Z,2017-02-21T13:39:35Z,http://arxiv.org/abs/1702.06378v1,http://arxiv.org/pdf/1702.06378v1,Multi-task Learning with CTC and Segmental CRF for Speech Recognition,"Segmental conditional random fields (SCRFs) and connectionist temporal
classification (CTC) are two sequence labeling objectives used for end-to-end
training of speech recognition models. Both models define the transcription
probability by marginalizing decisions about latent segmentation alternatives
to derive a sequence probability: the former uses a globally normalized joint
model of segment labels and durations and the latter classifies each frame as
either an output symbol or a ""continuation"" of the previous label. In this
paper we train a recognition model by optimizing an interpolation between the
SCRF and CTC losses where the same recurrent neural network (RNN) encoder used
for feature extraction for both outputs. We find that this multi-task objective
improves recognition accuracy when decoding with either the SCRF or CTC models.
Additionally we show that CTC can also be used to pretrain the RNN encoder
which improves the convergence rate when learning the joint model.",Liang Lu|Lingpeng Kong|Chris Dyer|Noah A. Smith,cs.CL
2017-02-28T17:20:17Z,2017-02-21T11:34:14Z,http://arxiv.org/abs/1702.06336v1,http://arxiv.org/pdf/1702.06336v1,Hybrid Dialog State Tracker with ASR Features,"This paper presents a hybrid dialog state tracker enhanced by trainable
Spoken Language Understanding (SLU) for slot-filling dialog systems. Our
architecture is inspired by previously proposed neural-network-based
belief-tracking systems. In addition we extended some parts of our modular
architecture with differentiable rules to allow end-to-end training. We
hypothesize that these rules allow our tracker to generalize better than pure
machine-learning based systems. For evaluation we used the Dialog State
Tracking Challenge (DSTC) 2 dataset - a popular belief tracking testbed with
dialogs from restaurant information system. To our knowledge our hybrid
tracker sets a new state-of-the-art result in three out of four categories
within the DSTC2.",Miroslav Vodolán|Rudolf Kadlec|Jan Kleindienst,cs.CL
2017-02-28T17:20:17Z,2017-02-21T02:18:38Z,http://arxiv.org/abs/1702.06239v1,http://arxiv.org/pdf/1702.06239v1,Reinforcement Learning Based Argument Component Detection,"Argument component detection (ACD) is an important sub-task in argumentation
mining. ACD aims at detecting and classifying different argument components in
natural language texts. Historical annotations (HAs) are important features the
human annotators consider when they manually perform the ACD task. However HAs
are largely ignored by existing automatic ACD techniques. Reinforcement
learning (RL) has proven to be an effective method for using HAs in some
natural language processing tasks. In this work we propose a RL-based ACD
technique and evaluate its performance on two well-annotated corpora. Results
suggest that in terms of classification accuracy HAs-augmented RL outperforms
plain RL by at most 17.85% and outperforms the state-of-the-art supervised
learning algorithm by at most 11.94%.",Yang Gao|Hao Wang|Chen Zhang|Wei Wang,cs.CL
2017-02-28T17:20:17Z,2017-02-21T01:30:59Z,http://arxiv.org/abs/1702.06235v1,http://arxiv.org/pdf/1702.06235v1,Learning to generate one-sentence biographies from Wikidata,"We investigate the generation of one-sentence Wikipedia biographies from
facts derived from Wikidata slot-value pairs. We train a recurrent neural
network sequence-to-sequence model with attention to select facts and generate
textual summaries. Our model incorporates a novel secondary objective that
helps ensure it generates sentences that contain the input facts. The model
achieves a BLEU score of 41 improving significantly upon the vanilla
sequence-to-sequence model and scoring roughly twice that of a simple template
baseline. Human preference evaluation suggests the model is nearly as good as
the Wikipedia reference. Manual analysis explores content selection suggesting
the model can trade the ability to infer knowledge against the risk of
hallucinating incorrect information.",Andrew Chisholm|Will Radford|Ben Hachey,cs.CL
2017-02-28T17:20:17Z,2017-02-20T23:48:39Z,http://arxiv.org/abs/1702.06216v1,http://arxiv.org/pdf/1702.06216v1,Filtering Tweets for Social Unrest,"Since the events of the Arab Spring there has been increased interest in
using social media to anticipate social unrest. While efforts have been made
toward automated unrest prediction we focus on filtering the vast volume of
tweets to identify tweets relevant to unrest which can be provided to
downstream users for further analysis. We train a supervised classifier that is
able to label Arabic language tweets as relevant to unrest with high
reliability. We examine the relationship between training data size and
performance and investigate ways to optimize the model building process while
minimizing cost. We also explore how confidence thresholds can be set to
achieve desired levels of performance.",Alan Mishler|Kevin Wonus|Wendy Chambers|Michael Bloodgood,cs.CL|cs.IR|cs.LG|stat.ML|H.3.3; I.2.6; I.2.7; I.5.4
2017-02-28T17:20:17Z,2017-02-20T19:00:06Z,http://arxiv.org/abs/1702.06135v1,http://arxiv.org/pdf/1702.06135v1,"Enabling Multi-Source Neural Machine Translation By Concatenating Source
  Sentences In Multiple Languages","In this paper we propose a novel and elegant solution to ""Multi-Source
Neural Machine Translation"" (MSNMT) which only relies on preprocessing a N-way
multilingual corpus without modifying the Neural Machine Translation (NMT)
architecture or training procedure. We simply concatenate the source sentences
to form a single long multi-source input sentence while keeping the target side
sentence as it is and train an NMT system using this augmented corpus. We
evaluate our method in a low resource general domain setting and show its
effectiveness (+2 BLEU using 2 source languages and +6 BLEU using 5 source
languages) along with some insights on how the NMT system leverages
multilingual information in such a scenario by visualizing attention.",Raj Dabre|Fabien Cromieres|Sadao Kurohashi,cs.CL
2017-02-28T17:20:17Z,2017-02-20T15:53:56Z,http://arxiv.org/abs/1702.06027v1,http://arxiv.org/pdf/1702.06027v1,Parent Oriented Teacher Selection Causes Language Diversity,"An evolutionary model for emergence of diversity in language is developed. We
investigated the effects of two real life observations namely people prefer
people that they communicate with and people interact with people that are
physically close to each other. Clearly these groups are relatively small
compared to the entire population. We restrict selection of the teachers from
such small groups called imitation sets around parents. Then child learns
language from a teacher selected within the imitation set of her parent. As a
result there are subcommunities with their own languages developed. Within
subcommunity comprehension is found to be high. The number of languages is
related to relative size of imitation set by a power law.",Ibrahim Cimentepe|Haluk O. Bingol,cs.CL
2017-02-28T17:20:21Z,2017-02-20T13:36:23Z,http://arxiv.org/abs/1702.05962v1,http://arxiv.org/pdf/1702.05962v1,Latent Variable Dialogue Models and their Diversity,"We present a dialogue generation model that directly captures the variability
in possible responses to a given input which reduces the `boring output' issue
of deterministic dialogue models. Experiments show that our model generates
more diverse outputs than baseline models and also generates more consistently
acceptable output than sampling from a deterministic encoder-decoder model.",Kris Cao|Stephen Clark,cs.CL
2017-02-28T17:20:21Z,2017-02-20T00:23:06Z,http://arxiv.org/abs/1702.05821v1,http://arxiv.org/pdf/1702.05821v1,Post-edit Analysis of Collective Biography Generation,"Text generation is increasingly common but often requires manual post-editing
where high precision is critical to end users. However manual editing is
expensive so we want to ensure this effort is focused on high-value tasks. And
we want to maintain stylistic consistency a particular challenge in crowd
settings. We present a case study analysing human post-editing in the context
of a template-based biography generation system. An edit flow visualisation
combined with manual characterisation of edits helps identify and prioritise
work for improving end-to-end efficiency and accuracy.",Bo Han|Will Radford|Anaïs Cadilhac|Art Harol|Andrew Chisholm|Ben Hachey,cs.CL
2017-02-28T17:20:21Z,2017-02-19T20:37:40Z,http://arxiv.org/abs/1702.05793v1,http://arxiv.org/pdf/1702.05793v1,"Harmonic Grammar Optimality Theory and Syntax Learnability: An
  Empirical Exploration of Czech Word Order","This work presents a systematic theoretical and empirical comparison of the
major algorithms that have been proposed for learning Harmonic and Optimality
Theory grammars (HG and OT respectively). By comparing learning algorithms we
are also able to compare the closely related OT and HG frameworks themselves.
Experimental results show that the additional expressivity of the HG framework
over OT affords performance gains in the task of predicting the surface word
order of Czech sentences. We compare the perceptron with the classic Gradual
Learning Algorithm (GLA) which learns OT grammars as well as the popular
Maximum Entropy model. In addition to showing that the perceptron is
theoretically appealing our work shows that the performance of the HG model it
learns approaches that of the upper bound in prediction accuracy on a held out
test set and that it is capable of accurately modeling observed variation.",Ann Irvine|Mark Dredze,cs.CL
2017-02-28T17:20:21Z,2017-02-18T18:10:04Z,http://arxiv.org/abs/1702.05638v1,http://arxiv.org/pdf/1702.05638v1,A Stylometric Inquiry into Hyperpartisan and Fake News,"This paper reports on a writing style analysis of hyperpartisan (i.e.
extremely one-sided) news in connection to fake news. It presents a large
corpus of 1627 articles that were manually fact-checked by professional
journalists from BuzzFeed. The articles originated from 9 well-known political
publishers 3 each from the mainstream the hyperpartisan left-wing and the
hyperpartisan right-wing. In sum the corpus contains 299 fake news 97% of
which originated from hyperpartisan publishers.
  We propose and demonstrate a new way of assessing style similarity between
text categories via Unmasking---a meta-learning approach originally devised for
authorship verification--- revealing that the style of left-wing and
right-wing news have a lot more in common than any of the two have with the
mainstream. Furthermore we show that hyperpartisan news can be discriminated
well by its style from the mainstream (F1=0.78) as can be satire from both
(F1=0.81). Unsurprisingly style-based fake news detection does not live up to
scratch (F1=0.46). Nevertheless the former results are important to implement
pre-screening for fake news detectors.",Martin Potthast|Johannes Kiesel|Kevin Reinartz|Janek Bevendorff|Benno Stein,cs.CL
2017-02-28T17:20:21Z,2017-02-18T15:29:01Z,http://arxiv.org/abs/1702.05624v1,http://arxiv.org/pdf/1702.05624v1,"Reproducing and learning new algebraic operations on word embeddings
  using genetic programming","Word-vector representations associate a high dimensional real-vector to every
word from a corpus. Recently neural-network based methods have been proposed
for learning this representation from large corpora. This type of
word-to-vector embedding is able to keep in the learned vector space some of
the syntactic and semantic relationships present in the original word corpus.
This in turn serves to address different types of language classification
tasks by doing algebraic operations defined on the vectors. The general
practice is to assume that the semantic relationships between the words can be
inferred by the application of a-priori specified algebraic operations. Our
general goal in this paper is to show that it is possible to learn methods for
word composition in semantic spaces. Instead of expressing the compositional
method as an algebraic operation we will encode it as a program which can be
linear nonlinear or involve more intricate expressions. More remarkably this
program will be evolved from a set of initial random programs by means of
genetic programming (GP). We show that our method is able to reproduce the same
behavior as human-designed algebraic operators. Using a word analogy task as
benchmark we also show that GP-generated programs are able to obtain accuracy
values above those produced by the commonly used human-designed rule for
algebraic manipulation of word vectors. Finally we show the robustness of our
approach by executing the evolved programs on the word2vec GoogleNews vectors
learned over 3 billion running words and assessing their accuracy in the same
word analogy task.",Roberto Santana,cs.CL
2017-02-28T17:20:21Z,2017-02-17T22:10:28Z,http://arxiv.org/abs/1702.05531v1,http://arxiv.org/pdf/1702.05531v1,Analysis and Optimization of fastText Linear Text Classifier,"The paper [1] shows that simple linear classifier can compete with complex
deep learning algorithms in text classification applications. Combining bag of
words (BoW) and linear classification techniques fastText [1] attains same or
only slightly lower accuracy than deep learning algorithms [2-9] that are
orders of magnitude slower. We proved formally that fastText can be transformed
into a simpler equivalent classifier which unlike fastText does not have any
hidden layer. We also proved that the necessary and sufficient dimensionality
of the word vector embedding space is exactly the number of document classes.
These results help constructing more optimal linear text classifiers with
guaranteed maximum classification capabilities. The results are proven exactly
by pure formal algebraic methods without attracting any empirical data.",Vladimir Zolotov|David Kung,cs.CL
2017-02-28T17:20:21Z,2017-02-17T20:26:50Z,http://arxiv.org/abs/1702.05512v1,http://arxiv.org/pdf/1702.05512v1,soc2seq: Social Embedding meets Conversation Model,"While liking or upvoting a post on a mobile app is easy to do replying with
a written note is much more difficult due to both the cognitive load of coming
up with a meaningful response as well as the mechanics of entering the text.
Here we present a novel textual reply generation model that goes beyond the
current auto-reply and predictive text entry models by taking into account the
content preferences of the user the idiosyncrasies of their conversational
style and even the structure of their social graph. Specifically we have
developed two types of models for personalized user interactions: a
content-based conversation model which makes use of location together with
user information and a social-graph-based conversation model which combines
content-based conversation models with social graphs.",Parminder Bhatia|Marsal Gavalda|Arash Einolghozati,cs.SI|cs.CL
2017-02-28T17:20:21Z,2017-02-17T15:39:21Z,http://arxiv.org/abs/1702.05398v1,http://arxiv.org/pdf/1702.05398v1,"Experiment Segmentation in Scientific Discourse as Clause-level
  Structured Prediction using Recurrent Neural Networks","We propose a deep learning model for identifying structure within experiment
narratives in scientific literature. We take a sequence labeling approach to
this problem and label clauses within experiment narratives to identify the
different parts of the experiment. Our dataset consists of paragraphs taken
from open access PubMed papers labeled with rhetorical information as a result
of our pilot annotation. Our model is a Recurrent Neural Network (RNN) with
Long Short-Term Memory (LSTM) cells that labels clauses. The clause
representations are computed by combining word representations using a novel
attention mechanism that involves a separate RNN. We compare this model against
LSTMs where the input layer has simple or no attention and a feature rich CRF
model. Furthermore we describe how our work could be useful for information
extraction from scientific literature.",Pradeep Dasigi|Gully A. P. C. Burns|Eduard Hovy|Anita de Waard,cs.CL
2017-02-28T17:20:21Z,2017-02-17T09:26:10Z,http://arxiv.org/abs/1702.05270v1,http://arxiv.org/pdf/1702.05270v1,"Be Precise or Fuzzy: Learning the Meaning of Cardinals and Quantifiers
  from Vision","People can refer to quantities in a visual scene by using either exact
cardinals (e.g. one two three) or natural language quantifiers (e.g. few
most all). In humans these two processes underlie fairly different cognitive
and neural mechanisms. Inspired by this evidence the present study proposes
two models for learning the objective meaning of cardinals and quantifiers from
visual scenes containing multiple objects. We show that a model capitalizing on
a 'fuzzy' measure of similarity is effective for learning quantifiers whereas
the learning of exact cardinals is better accomplished when information about
number is provided.",Sandro Pezzelle|Marco Marelli|Raffaella Bernardi,cs.CL|cs.AI|cs.CV
2017-02-28T17:20:21Z,2017-02-16T17:09:12Z,http://arxiv.org/abs/1702.05053v1,http://arxiv.org/pdf/1702.05053v1,Addressing the Data Sparsity Issue in Neural AMR Parsing,"Neural attention models have achieved great success in different NLP tasks.
How- ever they have not fulfilled their promise on the AMR parsing task due to
the data sparsity issue. In this paper we de- scribe a sequence-to-sequence
model for AMR parsing and present different ways to tackle the data sparsity
problem. We show that our methods achieve significant improvement over a
baseline neural atten- tion model and our results are also compet- itive
against state-of-the-art systems that do not use extra linguistic resources.",Xiaochang Peng|Chuan Wang|Daniel Gildea|Nianwen Xue,cs.CL
2017-02-28T17:20:25Z,2017-02-16T12:10:18Z,http://arxiv.org/abs/1702.04938v1,http://arxiv.org/pdf/1702.04938v1,Fast and unsupervised methods for multilingual cognate clustering,"In this paper we explore the use of unsupervised methods for detecting
cognates in multilingual word lists. We use online EM to train sound segment
similarity weights for computing similarity between two words. We tested our
online systems on geographically spread sixteen different language groups of
the world and show that the Online PMI system (Pointwise Mutual Information)
outperforms a HMM based system and two linguistically motivated systems:
LexStat and ALINE. Our results suggest that a PMI system trained in an online
fashion can be used by historical linguists for fast and accurate
identification of cognates in not so well-studied language families.",Taraka Rama|Johannes Wahle|Pavel Sofroniev|Gerhard Jäger,cs.CL
2017-02-28T17:20:25Z,2017-02-15T23:04:09Z,http://arxiv.org/abs/1702.04811v1,http://arxiv.org/pdf/1702.04811v1,An Analysis of Machine Learning Intelligence,"Deep neural networks (DNNs) have set state of the art results in many machine
learning and NLP tasks. However we do not have a strong understanding of what
DNN models learn. In this paper we examine learning in DNNs through analysis
of their outputs. We compare DNN performance directly to a human population
and use characteristics of individual data points such as difficulty to see how
well models perform on easy and hard examples. We investigate how training size
and the incorporation of noise affect a DNN's ability to generalize and learn.
Our experiments show that unlike traditional machine learning models (e.g.
Naive Bayes Decision Trees) DNNs exhibit human-like learning properties. As
they are trained with more data they are more able to distinguish between easy
and difficult items and performance on easy items improves at a higher rate
than difficult items. We find that different DNN models exhibit different
strengths in learning and are robust to noise in training data.",John P. Lalor|Hao Wu|Tsendsuren Munkhdalai|Hong Yu,cs.CL
2017-02-28T17:20:25Z,2017-02-15T20:56:30Z,http://arxiv.org/abs/1702.04770v1,http://arxiv.org/pdf/1702.04770v1,Training Language Models Using Target-Propagation,"While Truncated Back-Propagation through Time (BPTT) is the most popular
approach to training Recurrent Neural Networks (RNNs) it suffers from being
inherently sequential (making parallelization difficult) and from truncating
gradient flow between distant time-steps. We investigate whether Target
Propagation (TPROP) style approaches can address these shortcomings.
Unfortunately extensive experiments suggest that TPROP generally underperforms
BPTT and we end with an analysis of this phenomenon and suggestions for
future work.",Sam Wiseman|Sumit Chopra|Marc'Aurelio Ranzato|Arthur Szlam|Ruoyu Sun|Soumith Chintala|Nicolas Vasilache,cs.CL|cs.LG|cs.NE
2017-02-28T17:20:25Z,2017-02-15T09:45:23Z,http://arxiv.org/abs/1702.04521v1,http://arxiv.org/pdf/1702.04521v1,Frustratingly Short Attention Spans in Neural Language Modeling,"Neural language models predict the next token using a latent representation
of the immediate token history. Recently various methods for augmenting neural
language models with an attention mechanism over a differentiable memory have
been proposed. For predicting the next token these models query information
from a memory of the recent history which can facilitate learning mid- and
long-range dependencies. However conventional attention mechanisms used in
memory-augmented neural language models produce a single output vector per time
step. This vector is used both for predicting the next token as well as for the
key and value of a differentiable memory of a token history. In this paper we
propose a neural language model with a key-value attention mechanism that
outputs separate representations for the key and value of a differentiable
memory as well as for encoding the next-word distribution. This model
outperforms existing memory-augmented neural language models on two corpora.
Yet we found that our method mainly utilizes a memory of the five most recent
output representations. This led to the unexpected main finding that a much
simpler model based only on the concatenation of recent output representations
from previous time steps is on par with more sophisticated memory-augmented
neural language models.",Michał Daniluk|Tim Rocktäschel|Johannes Welbl|Sebastian Riedel,cs.CL|cs.AI|cs.LG|cs.NE
2017-02-28T17:20:25Z,2017-02-15T09:08:21Z,http://arxiv.org/abs/1702.04510v1,http://arxiv.org/pdf/1702.04510v1,"A Dependency-Based Neural Reordering Model for Statistical Machine
  Translation","In machine translation (MT) that involves translating between two languages
with significant differences in word order determining the correct word order
of translated words is a major challenge. The dependency parse tree of a source
sentence can help to determine the correct word order of the translated words.
In this paper we present a novel reordering approach utilizing a neural
network and dependency-based embeddings to predict whether the translations of
two source words linked by a dependency relation should remain in the same
order or should be swapped in the translated sentence. Experiments on
Chinese-to-English translation show that our approach yields a statistically
significant improvement of 0.57 BLEU point on benchmark NIST test sets
compared to our prior state-of-the-art statistical MT system that uses sparse
dependency-based reordering features.",Christian Hadiwinoto|Hwee Tou Ng,cs.CL
2017-02-28T17:20:25Z,2017-02-16T06:16:09Z,http://arxiv.org/abs/1702.04488v2,http://arxiv.org/pdf/1702.04488v2,"Transfer Learning for Low-Resource Chinese Word Segmentation with a
  Novel Neural Network","Recent works have been shown effective in using neural networks for Chinese
word segmentation. However these models rely on large-scale data and are less
effective for low-resource datasets because of insufficient training data.
Thus we propose a transfer learning method to improve low-resource word
segmentation by leveraging high-resource corpora. First we train a teacher
model on high-resource corpora and then use the learned knowledge to initialize
a student model. Second a weighted data similarity method is proposed to train
the student model on low-resource data with the help of high-resource corpora.
Finally given that insufficient data puts forward higher requirements for
feature extraction we propose a novel neural network which improves feature
learning. Experiment results show that our work significantly improves the
performance on low-resource datasets: 2.3% and 1.5% F-score on PKU and CTB
datasets. Furthermore this paper achieves state-of-the-art results: 96.1% and
96.2% F-score on PKU and CTB datasets. Besides we explore an asynchronous
parallel method on neural word segmentation to speed up training. The parallel
method accelerates training substantially and is almost five times faster than
a serial mode.",Jingjing Xu|Xu Sun,cs.CL
2017-02-28T17:20:25Z,2017-02-15T03:35:03Z,http://arxiv.org/abs/1702.04457v1,http://arxiv.org/pdf/1702.04457v1,Automated Phrase Mining from Massive Text Corpora,"As one of the fundamental tasks in text analysis phrase mining aims at
extracting quality phrases from a text corpus. Phrase mining is important in
various tasks including automatic term recognition document indexing
keyphrase extraction and topic modeling. Most existing methods rely on
complex trained linguistic analyzers and thus likely have unsatisfactory
performance on text corpora of new domains and genres without extra but
expensive adaption. Recently a few data-driven methods have been developed
successfully for extraction of phrases from massive domain-specific text.
However none of the state-of-the-art models is fully automated because they
require human experts for designing rules or labeling phrases.
  In this paper we propose a novel framework for automated phrase mining
AutoPhrase which can achieve high performance with minimal human effort. Two
new techniques have been developed: (1) by leveraging knowledge bases a robust
positive-only distant training method can avoid extra human labeling effort;
and (2) when the part-of-speech (POS) tagger is available a POS-guided phrasal
segmentation model can better understand the syntactic information for the
particular language and further enhance the performance by considering the
context. Note that AutoPhrase can support any language as long as a general
knowledge base (e.g. Wikipedia) in that language are available while
benefiting from but not requiring a POS tagger. Compared to the
state-of-the-art methods the new method has shown significant improvements on
effectiveness on five real-world datasets in different domains and languages.",Jingbo Shang|Jialu Liu|Meng Jiang|Xiang Ren|Clare R Voss|Jiawei Han,cs.CL
2017-02-28T17:20:25Z,2017-02-14T20:06:15Z,http://arxiv.org/abs/1702.04372v1,http://arxiv.org/pdf/1702.04372v1,"A case study on using speech-to-translation alignments for language
  documentation","For many low-resource or endangered languages spoken language resources are
more likely to be annotated with translations than with transcriptions. Recent
work exploits such annotations to produce speech-to-translation alignments
without access to any text transcriptions. We investigate whether providing
such information can aid in producing better (mismatched) crowdsourced
transcriptions which in turn could be valuable for training speech recognition
systems and show that they can indeed be beneficial through a small-scale case
study as a proof-of-concept. We also present a simple phonetically aware string
averaging technique that produces transcriptions of higher quality.",Antonios Anastasopoulos|David Chiang,cs.CL
2017-02-28T17:20:25Z,2017-02-14T18:46:47Z,http://arxiv.org/abs/1702.04333v1,http://arxiv.org/abs/1702.04333v1,"On the Relevance of Auditory-Based Gabor Features for Deep Learning in
  Automatic Speech Recognition","Previous studies support the idea of merging auditory-based Gabor features
with deep learning architectures to achieve robust automatic speech
recognition however the cause behind the gain of such combination is still
unknown. We believe these representations provide the deep learning decoder
with more discriminable cues. Our aim with this paper is to validate this
hypothesis by performing experiments with three different recognition tasks
(Aurora 4 CHiME 2 and CHiME 3) and assess the discriminability of the
information encoded by Gabor filterbank features. Additionally to identify the
contribution of low medium and high temporal modulation frequencies subsets of
the Gabor filterbank were used as features (dubbed LTM MTM and HTM
respectively). With temporal modulation frequencies between 16 and 25 Hz HTM
consistently outperformed the remaining ones in every condition highlighting
the robustness of these representations against channel distortions low
signal-to-noise ratios and acoustically challenging real-life scenarios with
relative improvements from 11 to 56% against a Mel-filterbank-DNN baseline. To
explain the results a measure of similarity between phoneme classes from DNN
activations is proposed and linked to their acoustic properties. We find this
measure to be consistent with the observed error rates and highlight specific
differences on phoneme level to pinpoint the benefit of the proposed features.",Angel Mario Castro Martinez|Sri Harish Mallidi|Bernd T. Meyer,cs.CL
2017-02-28T17:20:25Z,2017-02-14T03:47:34Z,http://arxiv.org/abs/1702.04066v1,http://arxiv.org/pdf/1702.04066v1,JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction,"We present a new parallel corpus JHU FLuency-Extended GUG corpus (JFLEG) for
developing and evaluating grammatical error correction (GEC). Unlike other
corpora it represents a broad range of language proficiency levels and uses
holistic fluency edits to not only correct grammatical errors but also make the
original text more native sounding. We describe the types of corrections made
and benchmark four leading GEC systems on this corpus identifying specific
areas in which they do well and how they can improve. JFLEG fulfills the need
for a new gold standard to properly assess the current state of GEC.",Courtney Napoles|Keisuke Sakaguchi|Joel Tetreault,cs.CL
2017-02-28T17:20:29Z,2017-02-13T19:52:02Z,http://arxiv.org/abs/1702.03964v1,http://arxiv.org/pdf/1702.03964v1,"The Parallel Meaning Bank: Towards a Multilingual Corpus of Translations
  Annotated with Compositional Meaning Representations","The Parallel Meaning Bank is a corpus of translations annotated with shared
formal meaning representations comprising over 11 million words divided over
four languages (English German Italian and Dutch). Our approach is based on
cross-lingual projection: automatically produced (and manually corrected)
semantic annotations for English sentences are mapped onto their word-aligned
translations assuming that the translations are meaning-preserving. The
semantic annotation consists of five main steps: (i) segmentation of the text
in sentences and lexical items; (ii) syntactic parsing with Combinatory
Categorial Grammar; (iii) universal semantic tagging; (iv) symbolization; and
(v) compositional semantic analysis based on Discourse Representation Theory.
These steps are performed using statistical models trained in a semi-supervised
manner. The employed annotation models are all language-neutral. Our first
results are promising.",Lasha Abzianidze|Johannes Bjerva|Kilian Evang|Hessel Haagsma|Rik van Noord|Pierre Ludmann|Duc-Duy Nguyen|Johan Bos,cs.CL
2017-02-28T17:20:29Z,2017-02-13T16:31:06Z,http://arxiv.org/abs/1702.03859v1,http://arxiv.org/pdf/1702.03859v1,"Offline bilingual word vectors orthogonal transformations and the
  inverted softmax","Usually bilingual word vectors are trained ""online"". Mikolov et al. showed
they can also be found ""offline"" whereby two pre-trained embeddings are
aligned with a linear transformation using dictionaries compiled from expert
knowledge. In this work we prove that the linear transformation between two
spaces should be orthogonal. This transformation can be obtained using the
singular value decomposition. We introduce a novel ""inverted softmax"" for
identifying translation pairs with which we improve the precision @1 of
Mikolov's original mapping from 34% to 43% when translating a test set
composed of both common and rare English words into Italian. Orthogonal
transformations are more robust to noise enabling us to learn the
transformation without expert bilingual signal by constructing a
""pseudo-dictionary"" from the identical character strings which appear in both
languages achieving 40% precision on the same test set. Finally we extend our
method to retrieve the true translations of English sentences from a corpus of
200k Italian sentences with a precision @1 of 68%.",Samuel L. Smith|David H. P. Turban|Steven Hamblin|Nils Y. Hammerla,cs.CL|cs.AI|cs.IR
2017-02-28T17:20:29Z,2017-02-13T16:30:23Z,http://arxiv.org/abs/1702.03856v1,http://arxiv.org/pdf/1702.03856v1,Towards speech-to-text translation without speech recognition,"We explore the problem of translating speech to text in low-resource
scenarios where neither automatic speech recognition (ASR) nor machine
translation (MT) are available but we have training data in the form of audio
paired with text translations. We present the first system for this problem
applied to a realistic multi-speaker dataset the CALLHOME Spanish-English
speech translation corpus. Our approach uses unsupervised term discovery (UTD)
to cluster repeated patterns in the audio creating a pseudotext which we pair
with translations to create a parallel text and train a simple bag-of-words MT
model. We identify the challenges faced by the system finding that the
difficulty of cross-speaker UTD results in low recall but that our system is
still able to correctly translate some content words in test data.",Sameer Bansal|Herman Kamper|Adam Lopez|Sharon Goldwater,cs.CL
2017-02-28T17:20:29Z,2017-02-13T15:26:27Z,http://arxiv.org/abs/1702.03814v1,http://arxiv.org/pdf/1702.03814v1,Bilateral Multi-Perspective Matching for Natural Language Sentences,"Natural language sentence matching is a fundamental technology for a variety
of tasks. Previous approaches either match sentences from a single direction or
only apply single granular (word-by-word or sentence-by-sentence) matching. In
this work we propose a bilateral multi-perspective matching (BiMPM) model
under the ""matching-aggregation"" framework. Given two sentences $P$ and $Q$
our model first encodes them with a BiLSTM encoder. Next we match the two
encoded sentences in two directions $P \rightarrow Q$ and $P \leftarrow Q$. In
each matching direction each time step of one sentence is matched against all
time-steps of the other sentence from multiple perspectives. Then another
BiLSTM layer is utilized to aggregate the matching results into a fix-length
matching vector. Finally based on the matching vector the decision is made
through a fully connected layer. We evaluate our model on three tasks:
paraphrase identification natural language inference and answer sentence
selection. Experimental results on standard benchmark datasets show that our
model achieves the state-of-the-art performance on all tasks.",Zhiguo Wang|Wael Hamza|Radu Florian,cs.AI|cs.CL
2017-02-28T17:20:29Z,2017-02-13T10:24:55Z,http://arxiv.org/abs/1702.03706v1,http://arxiv.org/pdf/1702.03706v1,"Multitask Learning with Deep Neural Networks for Community Question
  Answering","In this paper we developed a deep neural network (DNN) that learns to solve
simultaneously the three tasks of the cQA challenge proposed by the
SemEval-2016 Task 3 i.e. question-comment similarity question-question
similarity and new question-comment similarity. The latter is the main task
which can exploit the previous two for achieving better results. Our DNN is
trained jointly on all the three cQA tasks and learns to encode questions and
comments into a single vector representation shared across the multiple tasks.
The results on the official challenge test set show that our approach produces
higher accuracy and faster convergence rates than the individual neural
networks. Additionally our method which does not use any manual feature
engineering approaches the state of the art established with methods that make
heavy use of it.",Daniele Bonadiman|Antonio Uva|Alessandro Moschitti,cs.CL
2017-02-28T17:20:29Z,2017-02-13T07:08:28Z,http://arxiv.org/abs/1702.03654v1,http://arxiv.org/pdf/1702.03654v1,A Morphology-aware Network for Morphological Disambiguation,"Agglutinative languages such as Turkish Finnish and Hungarian require
morphological disambiguation before further processing due to the complex
morphology of words. A morphological disambiguator is used to select the
correct morphological analysis of a word. Morphological disambiguation is
important because it generally is one of the first steps of natural language
processing and its performance affects subsequent analyses. In this paper we
propose a system that uses deep learning techniques for morphological
disambiguation. Many of the state-of-the-art results in computer vision speech
recognition and natural language processing have been obtained through deep
learning models. However applying deep learning techniques to morphologically
rich languages is not well studied. In this work while we focus on Turkish
morphological disambiguation we also present results for French and German in
order to show that the proposed architecture achieves high accuracy with no
language-specific feature engineering or additional resource. In the
experiments we achieve 84.12 88.35 and 93.78 morphological disambiguation
accuracy among the ambiguous words for Turkish German and French respectively.",Eray Yildiz|Caglar Tirkaz|H. Bahadir Sahin|Mustafa Tolga Eren|Ozan Sonmez,cs.CL
2017-02-28T17:20:29Z,2017-02-12T13:19:03Z,http://arxiv.org/abs/1702.03525v1,http://arxiv.org/pdf/1702.03525v1,Learning to Parse and Translate Improves Neural Machine Translation,"There has been relatively little attention to incorporating linguistic prior
to neural machine translation. Much of the previous work was further
constrained to considering linguistic prior on the source side. In this paper
we propose a hybrid model called NMT+RG that learns to parse and translate by
combining the recurrent neural network grammar into the attention-based neural
machine translation. Our approach encourages the neural machine translation
model to incorporate linguistic prior during training and lets it translate on
its own afterward. Extensive experiments with four language pairs show the
effectiveness of the proposed NMT+RG.",Akiko Eriguchi|Yoshimasa Tsuruoka|Kyunghyun Cho,cs.CL
2017-02-28T17:20:29Z,2017-02-12T00:23:04Z,http://arxiv.org/abs/1702.03470v1,http://arxiv.org/pdf/1702.03470v1,Vector Embedding of Wikipedia Concepts and Entities,"Using deep learning for different machine learning tasks such as image
classification and word embedding has recently gained many attentions. Its
appealing performance reported across specific Natural Language Processing
(NLP) tasks in comparison with other approaches is the reason for its
popularity. Word embedding is the task of mapping words or phrases to a low
dimensional numerical vector. In this paper we use deep learning to embed
Wikipedia Concepts and Entities. The English version of Wikipedia contains more
than five million pages which suggest its capability to cover many English
Entities Phrases and Concepts. Each Wikipedia page is considered as a
concept. Some concepts correspond to entities such as a person's name an
organization or a place. Contrary to word embedding Wikipedia Concepts
Embedding is not ambiguous so there are different vectors for concepts with
similar surface form but different mentions. We proposed several approaches and
evaluated their performance based on Concept Analogy and Concept Similarity
tasks. The results show that proposed approaches have the performance
comparable and in some cases even higher than the state-of-the-art methods.",Ehsan Sherkat|Evangelos Milios,cs.CL
2017-02-28T17:20:29Z,2017-02-11T09:50:40Z,http://arxiv.org/abs/1702.03402v1,http://arxiv.org/pdf/1702.03402v1,Parallel Long Short-Term Memory for Multi-stream Classification,"Recently machine learning methods have provided a broad spectrum of original
and efficient algorithms based on Deep Neural Networks (DNN) to automatically
predict an outcome with respect to a sequence of inputs. Recurrent hidden cells
allow these DNN-based models to manage long-term dependencies such as Recurrent
Neural Networks (RNN) and Long Short-Term Memory (LSTM). Nevertheless these
RNNs process a single input stream in one (LSTM) or two (Bidirectional LSTM)
directions. But most of the information available nowadays is from multistreams
or multimedia documents and require RNNs to process these information
synchronously during the training. This paper presents an original LSTM-based
architecture named Parallel LSTM (PLSTM) that carries out multiple parallel
synchronized input sequences in order to predict a common output. The proposed
PLSTM method could be used for parallel sequence classification purposes. The
PLSTM approach is evaluated on an automatic telecast genre sequences
classification task and compared with different state-of-the-art architectures.
Results show that the proposed PLSTM method outperforms the baseline n-gram
models as well as the state-of-the-art LSTM approach.",Mohamed Bouaziz|Mohamed Morchid|Richard Dufour|Georges Linarès|Renato De Mori,cs.LG|cs.CL
2017-02-28T17:20:29Z,2017-02-11T02:21:42Z,http://arxiv.org/abs/1702.04615v1,http://arxiv.org/pdf/1702.04615v1,"Automated Identification of Drug-Drug Interactions in Pediatric
  Congestive Heart Failure Patients","Congestive Heart Failure or CHF is a serious medical condition that can
result in fluid buildup in the body as a result of a weak heart. When the heart
can't pump enough blood to efficiently deliver nutrients and oxygen to the
body kidney function may be impaired resulting in fluid retention. CHF
patients require a broad drug regimen to maintain the delicate system balance
particularly between their heart and kidneys. These drugs include ACE
inhibitors and Beta Blockers to control blood pressure anticoagulants to
prevent blood clots and diuretics to reduce fluid overload. Many of these
drugs may interact and potential effects of these interactions must be weighed
against their benefits. For this project we consider a set of 44 drugs
identified as specifically relevant for treating CHF by pediatric cardiologists
at Lucile Packard Children's Hospital. This list was generated as part of our
current work at the LPCH Heart Center. The goal of this project is to identify
and evaluate potentially harmful drug-drug interactions (DDIs) within pediatric
patients with Congestive Heart Failure. This identification will be done
autonomously so that it may continuously update by evaluating newly published
literature.",Daniel Miller,cs.CL
2017-02-28T17:20:33Z,2017-02-10T22:44:59Z,http://arxiv.org/abs/1702.03342v1,http://arxiv.org/pdf/1702.03342v1,Learning Concept Embeddings for Efficient Bag-of-Concepts Densification,"Explicit concept space models have proven efficacy for text representation in
many natural language and text mining applications. The idea is to embed
textual structures into a semantic space of concepts which captures the main
topics of these structures. That so called bag-of-concepts representation
suffers from data sparsity causing low similarity scores between similar texts
due to low concept overlap. In this paper we propose two neural embedding
models in order to learn continuous concept vectors. Once learned we propose
an efficient vector aggregation method to generate fully dense bag-of-concepts
representations. Empirical results on a benchmark dataset for measuring entity
semantic relatedness show superior performance over other concept embedding
models. In addition by utilizing our efficient aggregation method we
demonstrate the effectiveness of the densified vector representation over the
typical sparse representations for dataless classification where we can achieve
at least same or better accuracy with much less dimensions.",Walid Shalaby|Wlodek Zadrozny,cs.CL
2017-02-28T17:20:33Z,2017-02-10T19:16:40Z,http://arxiv.org/abs/1702.03305v1,http://arxiv.org/pdf/1702.03305v1,Universal Dependencies to Logical Forms with Negation Scope,"Many language technology applications would benefit from the ability to
represent negation and its scope on top of widely-used linguistic resources. In
this paper we investigate the possibility of obtaining a first-order logic
representation with negation scope marked using Universal Dependencies. To do
so we enhance UDepLambda a framework that converts dependency graphs to
logical forms. The resulting UDepLambda$\lnot$ is able to handle phenomena
related to scope by means of an higher-order type theory relevant not only to
negation but also to universal quantification and other complex semantic
phenomena. The initial conversion we did for English is promising in that one
can represent the scope of negation also in the presence of more complex
phenomena such as universal quantifiers.",Federico Fancellu|Siva Reddy|Adam Lopez|Bonnie Webber,cs.CL|03B65
2017-02-28T17:20:33Z,2017-02-10T18:24:13Z,http://arxiv.org/abs/1702.03274v1,http://arxiv.org/pdf/1702.03274v1,"Hybrid Code Networks: practical and efficient end-to-end dialog control
  with supervised and reinforcement learning","End-to-end learning of recurrent neural networks (RNNs) is an attractive
solution for dialog systems; however current techniques are data-intensive and
require thousands of dialogs to learn simple behaviors. We introduce Hybrid
Code Networks (HCNs) which combine an RNN with domain-specific knowledge
encoded as software and system action templates. Compared to existing
end-to-end approaches HCNs considerably reduce the amount of training data
required while retaining the key benefit of inferring a latent representation
of dialog state. In addition HCNs can be optimized with supervised learning
reinforcement learning or a mixture of both. HCNs attain state-of-the-art
performance on the bAbI dialog dataset and outperform two commercially
deployed customer-facing dialog systems.",Jason D. Williams|Kavosh Asadi|Geoffrey Zweig,cs.AI|cs.CL
2017-02-28T17:20:33Z,2017-02-10T14:59:14Z,http://arxiv.org/abs/1702.03197v1,http://arxiv.org/pdf/1702.03197v1,Arabic Language Sentiment Analysis on Health Services,"The social media network phenomenon leads to a massive amount of valuable
data that is available online and easy to access. Many users share images
videos comments reviews news and opinions on different social networks
sites with Twitter being one of the most popular ones. Data collected from
Twitter is highly unstructured and extracting useful information from tweets
is a challenging task. Twitter has a huge number of Arabic users who mostly
post and write their tweets using the Arabic language. While there has been a
lot of research on sentiment analysis in English the amount of researches and
datasets in Arabic language is limited. This paper introduces an Arabic
language dataset which is about opinions on health services and has been
collected from Twitter. The paper will first detail the process of collecting
the data from Twitter and also the process of filtering pre-processing and
annotating the Arabic text in order to build a big sentiment analysis dataset
in Arabic. Several Machine Learning algorithms (Naive Bayes Support Vector
Machine and Logistic Regression) alongside Deep and Convolutional Neural
Networks were utilized in our experiments of sentiment analysis on our health
dataset.",Abdulaziz M. Alayba|Vasile Palade|Matthew England|Rahat Iqbal,cs.CL|cs.NE|cs.SI|I.2.7; I.2.6
2017-02-28T17:20:33Z,2017-02-15T16:31:42Z,http://arxiv.org/abs/1702.03196v2,http://arxiv.org/pdf/1702.03196v2,Universal Semantic Parsing,"Universal Dependencies (UD) provides a cross-linguistically uniform syntactic
representation with the aim of advancing multilingual applications of parsing
and natural language understanding. Reddy et al. (2016) recently developed a
semantic interface for (English) Stanford Dependencies based on the lambda
calculus. In this work we introduce UDepLambda a similar semantic interface
for UD which allows mapping natural language to logical forms in an almost
language-independent framework. We evaluate our approach on semantic parsing
for the task of question answering against Freebase. To facilitate multilingual
evaluation we provide German and Spanish translations of the WebQuestions and
GraphQuestions datasets. Results show that UDepLambda outperforms strong
baselines across languages and datasets. For English it achieves the strongest
result to date on GraphQuestions with competitive results on WebQuestions.",Siva Reddy|Oscar Täckström|Slav Petrov|Mark Steedman|Mirella Lapata,cs.CL
2017-02-28T17:20:33Z,2017-02-10T10:31:57Z,http://arxiv.org/abs/1702.03121v1,http://arxiv.org/pdf/1702.03121v1,"Modeling Semantic Expectation: Using Script Knowledge for Referent
  Prediction","Recent research in psycholinguistics has provided increasing evidence that
humans predict upcoming content. Prediction also affects perception and might
be a key to robustness in human language processing. In this paper we
investigate the factors that affect human prediction by building a
computational model that can predict upcoming discourse referents based on
linguistic knowledge alone vs. linguistic knowledge jointly with common-sense
knowledge in the form of scripts. We find that script knowledge significantly
improves model estimates of human predictions. In a second study we test the
highly controversial hypothesis that predictability influences referring
expression type but do not find evidence for such an effect.",Ashutosh Modi|Ivan Titov|Vera Demberg|Asad Sayeed|Manfred Pinkal,cs.CL|cs.AI|stat.ML
2017-02-28T17:20:33Z,2017-02-10T07:22:08Z,http://arxiv.org/abs/1702.03082v1,http://arxiv.org/pdf/1702.03082v1,UsingWord Embedding for Cross-Language Plagiarism Detection,"This paper proposes to use distributed representation of words (word
embeddings) in cross-language textual similarity detection. The main
contributions of this paper are the following: (a) we introduce new
cross-language similarity detection methods based on distributed representation
of words; (b) we combine the different methods proposed to verify their
complementarity and finally obtain an overall F1 score of 89.15% for
English-French similarity detection at chunk level (88.5% at sentence level) on
a very challenging corpus.",J. Ferrero|F. Agnes|L. Besacier|D. Schwab,cs.CL
2017-02-28T17:20:33Z,2017-02-10T01:27:00Z,http://arxiv.org/abs/1702.03033v1,http://arxiv.org/pdf/1702.03033v1,Local System Voting Feature for Machine Translation System Combination,"In this paper we enhance the traditional confusion network system
combination approach with an additional model trained by a neural network. This
work is motivated by the fact that the commonly used binary system voting
models only assign each input system a global weight which is responsible for
the global impact of each input system on all translations. This prevents
individual systems with low system weights from having influence on the system
combination output although in some situations this could be helpful. Further
words which have only been seen by one or few systems rarely have a chance of
being present in the combined output. We train a local system voting model by a
neural network which is based on the words themselves and the combinatorial
occurrences of the different system outputs. This gives system combination the
option to prefer other systems at different word positions even for the same
sentence.",Markus Freitag|Jan-Thorsten Peter|Stephan Peitz|Minwei Feng|Hermann Ney,cs.CL
2017-02-28T17:20:33Z,2017-02-09T08:03:16Z,http://arxiv.org/abs/1702.02737v1,http://arxiv.org/pdf/1702.02737v1,"Mining User/Movie Preferred Features Based on Reviews for Video
  Recommendation System","In this work we present an approach for mining user preferences and
recommendation based on reviews. There have been various studies worked on
recommendation problem. However most of the studies beyond one aspect user
generated- content such as user ratings user feedback and so on to state user
preferences. There is a prob- lem in one aspect mining is lacking for stating
user preferences. As a demonstration in collaborative filter recommendation
we try to figure out the preference trend of crowded users then use that trend
to predict current user preference. Therefore there is a gap between real user
preferences and the trend of the crowded people. Additionally user preferences
can be addressed from mining user reviews since user often comment about
various aspects of products. To solve this problem we mainly focus on mining
product aspects and user aspects inside user reviews to directly state user
preferences. We also take into account Social Network Analysis for cold-start
item problem. With cold-start user problem collaborative filter algorithm is
employed in our work. The framework is general enough to be applied to
different recommendation domains. Theoretically our method would achieve a
significant enhancement.",Xuan-Son Vu|Seong-Bae Park,cs.IR|cs.CL
2017-02-28T17:20:33Z,2017-02-09T08:00:09Z,http://arxiv.org/abs/1702.02736v1,http://arxiv.org/pdf/1702.02736v1,"Challenges in Providing Automatic Affective Feedback in Instant
  Messaging Applications","Instant messaging is one of the major channels of computer mediated
communication. However humans are known to be very limited in understanding
others' emotions via text-based communication. Aiming on introducing emotion
sensing technologies to instant messaging we developed EmotionPush a system
that automatically detects the emotions of the messages end-users received on
Facebook Messenger and provides colored cues on their smartphones accordingly.
We conducted a deployment study with 20 participants during a time span of two
weeks. In this paper we revealed five challenges along with examples that we
observed in our study based on both user's feedback and chat logs including
(i)the continuum of emotions (ii)multi-user conversations (iii)different
dynamics between different users (iv)misclassification of emotions and
(v)unconventional content. We believe this discussion will benefit the future
exploration of affective computing for instant messaging and also shed light
on research of conversational emotion sensing.",Chieh-Yang Huang|Ting-Hao|Huang|Lun-Wei Ku,cs.CL|cs.HC|H.5.2; H.5.3; I.2.7
2017-02-28T17:20:36Z,2017-02-08T22:24:14Z,http://arxiv.org/abs/1702.02640v1,http://arxiv.org/pdf/1702.02640v1,Character-level Deep Conflation for Business Data Analytics,"Connecting different text attributes associated with the same entity
(conflation) is important in business data analytics since it could help merge
two different tables in a database to provide a more comprehensive profile of
an entity. However the conflation task is challenging because two text strings
that describe the same entity could be quite different from each other for
reasons such as misspelling. It is therefore critical to develop a conflation
model that is able to truly understand the semantic meaning of the strings and
match them at the semantic level. To this end we develop a character-level
deep conflation model that encodes the input text strings from character level
into finite dimension feature vectors which are then used to compute the
cosine similarity between the text strings. The model is trained in an
end-to-end manner using back propagation and stochastic gradient descent to
maximize the likelihood of the correct association. Specifically we propose
two variants of the deep conflation model based on long-short-term memory
(LSTM) recurrent neural network (RNN) and convolutional neural network (CNN)
respectively. Both models perform well on a real-world business analytics
dataset and significantly outperform the baseline bag-of-character (BoC) model.",Zhe Gan|P. D. Singh|Ameet Joshi|Xiaodong He|Jianshu Chen|Jianfeng Gao|Li Deng,cs.CL|cs.LG
2017-02-28T17:20:36Z,2017-02-08T19:10:53Z,http://arxiv.org/abs/1702.02584v1,http://arxiv.org/pdf/1702.02584v1,Convolutional Neural Network for Humor Recognition,"For the purpose of automatically evaluating speakers' humor usage we build a
presentation corpus containing humorous utterances based on TED talks. Compared
to previous data resources supporting humor recognition research ours has
several advantages including (a) both positive and negative instances coming
from a homogeneous data set (b) containing a large number of speakers and (c)
being open. Focusing on using lexical cues for humor recognition we
systematically compare a newly emerging text classification method based on
Convolutional Neural Networks (CNNs) with a well-established conventional
method using linguistic knowledge. The CNN method shows its advantages on both
higher recognition accuracies and being able to learn essential features
automatically.",Lei Chen|Chong MIn Lee,cs.CL
2017-02-28T17:20:36Z,2017-02-24T22:20:25Z,http://arxiv.org/abs/1702.02540v2,http://arxiv.org/pdf/1702.02540v2,Automatic Rule Extraction from Long Short Term Memory Networks,"Although deep learning models have proven effective at solving problems in
natural language processing the mechanism by which they come to their
conclusions is often unclear. As a result these models are generally treated
as black boxes yielding no insight of the underlying learned patterns. In this
paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new
approach for tracking the importance of a given input to the LSTM for a given
output. By identifying consistently important patterns of words we are able to
distill state of the art LSTMs on sentiment analysis and question answering
into a set of representative phrases. This representation is then
quantitatively validated by using the extracted phrases to construct a simple
rule-based classifier which approximates the output of the LSTM.",W. James Murdoch|Arthur Szlam,cs.CL|cs.AI|cs.NE|stat.ML
2017-02-28T17:20:36Z,2017-02-08T17:30:51Z,http://arxiv.org/abs/1702.02535v1,http://arxiv.org/pdf/1702.02535v1,"Exploiting Domain Knowledge via Grouped Weight Sharing with Application
  to Text Categorization","A fundamental advantage of neural models for NLP is their ability to learn
representations from scratch. However in practice this often means ignoring
existing external linguistic resources e.g. WordNet or domain specific
ontologies such as the Unified Medical Language System (UMLS). We propose a
general novel method for exploiting such resources via weight sharing. Prior
work on weight sharing in neural networks has considered it largely as a means
of model compression. In contrast we treat weight sharing as a flexible
mechanism for incorporating prior knowledge into neural models. We show that
this approach consistently yields improved performance on classification tasks
compared to baseline strategies that do not exploit weight sharing.",Ye Zhang|Matthew Lease|Byron C. Wallace,cs.CL
2017-02-28T17:20:36Z,2017-02-08T13:56:16Z,http://arxiv.org/abs/1702.02429v1,http://arxiv.org/pdf/1702.02429v1,Trainable Greedy Decoding for Neural Machine Translation,"Recent research in neural machine translation has largely focused on two
aspects; neural network architectures and end-to-end learning algorithms. The
problem of decoding however has received relatively little attention from the
research community. In this paper we solely focus on the problem of decoding
given a trained neural machine translation model. Instead of trying to build a
new decoding algorithm for any specific decoding objective we propose the idea
of trainable decoding algorithm in which we train a decoding algorithm to find
a translation that maximizes an arbitrary decoding objective. More
specifically we design an actor that observes and manipulates the hidden state
of the neural machine translation decoder and propose to train it using a
variant of deterministic policy gradient. We extensively evaluate the proposed
algorithm using four language pairs and two decoding objectives and show that
we can indeed train a trainable greedy decoder that generates a better
translation (in terms of a target decoding objective) with minimal
computational overhead.",Jiatao Gu|Kyunghyun Cho|Victor O. K. Li,cs.CL|cs.LG
2017-02-28T17:20:36Z,2017-02-08T13:49:59Z,http://arxiv.org/abs/1702.02426v1,http://arxiv.org/pdf/1702.02426v1,Data Selection Strategies for Multi-Domain Sentiment Analysis,"Domain adaptation is important in sentiment analysis as sentiment-indicating
words vary between domains. Recently multi-domain adaptation has become more
pervasive but existing approaches train on all available source domains
including dissimilar ones. However the selection of appropriate training data
is as important as the choice of algorithm. We undertake -- to our knowledge
for the first time -- an extensive study of domain similarity metrics in the
context of sentiment analysis and propose novel representations metrics and a
new scope for data selection. We evaluate the proposed methods on two
large-scale multi-domain adaptation settings on tweets and reviews and
demonstrate that they consistently outperform strong random and balanced
baselines while our proposed selection strategy outperforms instance-level
selection and yields the best score on a large reviews corpus.",Sebastian Ruder|Parsa Ghaffari|John G. Breslin,cs.CL|cs.LG
2017-02-28T17:20:36Z,2017-02-08T12:11:41Z,http://arxiv.org/abs/1702.02390v1,http://arxiv.org/pdf/1702.02390v1,A Hybrid Convolutional Variational Autoencoder for Text Generation,"In this paper we explore the effect of architectural choices on learning a
Variational Autoencoder (VAE) for text generation. In contrast to the
previously introduced VAE model for text where both the encoder and decoder are
RNNs we propose a novel hybrid architecture that blends fully feed-forward
convolutional and deconvolutional components with a recurrent language model.
Our architecture exhibits several attractive properties such as faster run time
and convergence ability to better handle long sequences and more importantly
it helps to avoid some of the major difficulties posed by training VAE models
on textual data.",Stanislau Semeniuta|Aliaksei Severyn|Erhardt Barth,cs.CL
2017-02-28T17:20:36Z,2017-02-08T10:58:02Z,http://arxiv.org/abs/1702.02367v1,http://arxiv.org/pdf/1702.02367v1,Iterative Multi-document Neural Attention for Multiple Answer Prediction,"People have information needs of varying complexity which can be solved by
an intelligent agent able to answer questions formulated in a proper way
eventually considering user context and preferences. In a scenario in which the
user profile can be considered as a question intelligent agents able to answer
questions can be used to find the most relevant answers for a given user. In
this work we propose a novel model based on Artificial Neural Networks to
answer questions with multiple answers by exploiting multiple facts retrieved
from a knowledge base. The model is evaluated on the factoid Question Answering
and top-n recommendation tasks of the bAbI Movie Dialog dataset. After
assessing the performance of the model on both tasks we try to define the
long-term goal of a conversational recommender system able to interact using
natural language and to support users in their information seeking processes in
a personalized way.",Claudio Greco|Alessandro Suglia|Pierpaolo Basile|Gaetano Rossiello|Giovanni Semeraro,cs.CL
2017-02-28T17:20:36Z,2017-02-09T08:35:12Z,http://arxiv.org/abs/1702.02363v2,http://arxiv.org/pdf/1702.02363v2,"Automatically Annotated Turkish Corpus for Named Entity Recognition and
  Text Categorization using Large-Scale Gazetteers","Turkish Wikipedia Named-Entity Recognition and Text Categorization (TWNERTC)
dataset is a collection of automatically categorized and annotated sentences
obtained from Wikipedia. We constructed large-scale gazetteers by using a graph
crawler algorithm to extract relevant entity and domain information from a
semantic knowledge base Freebase. The constructed gazetteers contains
approximately 300K entities with thousands of fine-grained entity types under
77 different domains. Since automated processes are prone to ambiguity we also
introduce two new content specific noise reduction methodologies. Moreover we
map fine-grained entity types to the equivalent four coarse-grained types:
person loc org misc. Eventually we construct six different dataset versions
and evaluate the quality of annotations by comparing ground truths from human
annotators. We make these datasets publicly available to support studies on
Turkish named-entity recognition (NER) and text categorization (TC).",H. Bahadir Sahin|Caglar Tirkaz|Eray Yildiz|Mustafa Tolga Eren|Ozan Sonmez,cs.CL
2017-02-28T17:20:36Z,2017-02-08T04:54:09Z,http://arxiv.org/abs/1702.02287v1,http://arxiv.org/pdf/1702.02287v1,"Name Entity Disambiguation in Anonymized Graphs using Link Analysis: A
  Network Embedding based Solution","In real-world our DNA is unique but many people share same names. This
phenomenon often causes erroneous aggregation of documents of multiple persons
who are namesake of one another. Such mistakes deteriorate the performance of
document retrieval web search and more seriously cause improper attribution
of credit or blame in digital forensic. To resolve this issue the name entity
disambiguation task is designed which aims to partition the documents
associated with a name reference such that each partition contains documents
pertaining to a unique real-life person. Existing solutions to this task
substantially rely on feature engineering such as biographical feature
extraction or construction of auxiliary features from Wikipedia. However for
many scenarios such features may be costly to obtain or unavailable due to the
risk of privacy violation. In this work we propose a novel name disambiguation
method. Our proposed method is non-intrusive of privacy because instead of
using attributes pertaining to a real-life person our method leverages only
relational data in the form of anonymized graphs. In the aspect of
methodological novelty the proposed method uses a representation learning
strategy to embed each document in a low dimensional vector space where name
disambiguation can be solved by a hierarchical agglomerative clustering
algorithm. Our experimental results demonstrate that the proposed method is
significantly better than the existing name entity disambiguation methods
working in a similar setting.",Baichuan Zhang|Mohammad Al Hasan,cs.SI|cs.CL|cs.IR
