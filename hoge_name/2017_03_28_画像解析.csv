,プログラム実行日時,論文更新日時,論文リンク,PDFリンク,元論文タイトル,論文タイトル,元サマリ,サマリ,著者,事前付与ジャンル,ニューラルネットワーク,自然言語処理,マーケティング,画像解析,音声解析,強化学習
12,2017-03-28T14:05:20Z,2017-03-24T12:07:34Z,http://arxiv.org/abs/1703.08383v1,http://arxiv.org/pdf/1703.08383v1,Smart Augmentation - Learning an Optimal Data Augmentation Strategy,smart augment learn optim data augment strategi,"A recurring problem faced when training neural networks is that there is typically not enough data to maximize the generalization capability of deep neural networks(DNN). There are many techniques to address this, including data augmentation, dropout, and transfer learning. In this paper, we introduce an additional method which we call Smart Augmentation and we show how to use it to increase the accuracy and reduce overfitting on a target network. Smart Augmentation works by creating a network that learns how to generate augmented data during the training process of a target network in a way that reduces that networks loss. This allows us to learn augmentations that minimize the error of that network.   Smart Augmentation has shown the potential to increase accuracy by demonstrably significant measures on all datasets tested. In addition, it has shown potential to achieve similar or improved performance levels with significantly smaller network sizes in a number of tested cases.",recur problem face train neural network typic enough data maxim general capabl deep neural network dnn mani techniqu address includ data augment dropout transfer learn paper introduc addit method call smart augment show use increas accuraci reduc overfit target network smart augment work creat network learn generat augment data dure train process target network way reduc network loss allow us learn augment minim error network smart augment shown potenti increas accuraci demonstr signific measur dataset test addit shown potenti achiev similar improv perform level signific smaller network size number test case,"['Joseph Lemley', 'Shabab Bazrafkan', 'Peter Corcoran']","['cs.AI', 'cs.LG', 'stat.ML']",False,False,False,True,False,False
33,2017-03-28T14:05:28Z,2017-03-22T17:08:40Z,http://arxiv.org/abs/1703.07255v2,http://arxiv.org/pdf/1703.07255v2,ZM-Net: Real-time Zero-shot Image Manipulation Network,zm net real time zero shot imag manipul network,"Many problems in image processing and computer vision (e.g. colorization, style transfer) can be posed as 'manipulating' an input image into a corresponding output image given a user-specified guiding signal. A holy-grail solution towards generic image manipulation should be able to efficiently alter an input image with any personalized signals (even signals unseen during training), such as diverse paintings and arbitrary descriptive attributes. However, existing methods are either inefficient to simultaneously process multiple signals (let alone generalize to unseen signals), or unable to handle signals from other modalities. In this paper, we make the first attempt to address the zero-shot image manipulation task. We cast this problem as manipulating an input image according to a parametric model whose key parameters can be conditionally generated from any guiding signal (even unseen ones). To this end, we propose the Zero-shot Manipulation Net (ZM-Net), a fully-differentiable architecture that jointly optimizes an image-transformation network (TNet) and a parameter network (PNet). The PNet learns to generate key transformation parameters for the TNet given any guiding signal while the TNet performs fast zero-shot image manipulation according to both signal-dependent parameters from the PNet and signal-invariant parameters from the TNet itself. Extensive experiments show that our ZM-Net can perform high-quality image manipulation conditioned on different forms of guiding signals (e.g. style images and attributes) in real-time (tens of milliseconds per image) even for unseen signals. Moreover, a large-scale style dataset with over 20,000 style images is also constructed to promote further research.",mani problem imag process comput vision color style transfer pose manipul input imag correspond output imag given user specifi guid signal holi grail solut toward generic imag manipul abl effici alter input imag ani person signal even signal unseen dure train divers paint arbitrari descript attribut howev exist method either ineffici simultan process multipl signal let alon general unseen signal unabl handl signal modal paper make first attempt address zero shot imag manipul task cast problem manipul input imag accord parametr model whose key paramet condit generat ani guid signal even unseen one end propos zero shot manipul net zm net fulli differenti architectur joint optim imag transform network tnet paramet network pnet pnet learn generat key transform paramet tnet given ani guid signal tnet perform fast zero shot imag manipul accord signal depend paramet pnet signal invari paramet tnet extens experi show zm net perform high qualiti imag manipul condit differ form guid signal style imag attribut real time ten millisecond per imag even unseen signal moreov larg scale style dataset style imag also construct promot research,"['Hao Wang', 'Xiaodan Liang', 'Hao Zhang', 'Dit-Yan Yeung', 'Eric P. Xing']","['cs.CV', 'cs.AI', 'cs.GR', 'cs.LG', 'stat.ML']",False,False,False,True,False,False
37,2017-03-28T14:05:28Z,2017-03-23T20:06:15Z,http://arxiv.org/abs/1703.07022v2,http://arxiv.org/pdf/1703.07022v2,Recurrent Topic-Transition GAN for Visual Paragraph Generation,recurr topic transit gan visual paragraph generat,"A natural image usually conveys rich semantic content and can be viewed from different angles. Existing image description methods are largely restricted by small sets of biased visual paragraph annotations, and fail to cover rich underlying semantics. In this paper, we investigate a semi-supervised paragraph generative framework that is able to synthesize diverse and semantically coherent paragraph descriptions by reasoning over local semantic regions and exploiting linguistic knowledge. The proposed Recurrent Topic-Transition Generative Adversarial Network (RTT-GAN) builds an adversarial framework between a structured paragraph generator and multi-level paragraph discriminators. The paragraph generator generates sentences recurrently by incorporating region-based visual and language attention mechanisms at each step. The quality of generated paragraph sentences is assessed by multi-level adversarial discriminators from two aspects, namely, plausibility at sentence level and topic-transition coherence at paragraph level. The joint adversarial training of RTT-GAN drives the model to generate realistic paragraphs with smooth logical transition between sentence topics. Extensive quantitative experiments on image and video paragraph datasets demonstrate the effectiveness of our RTT-GAN in both supervised and semi-supervised settings. Qualitative results on telling diverse stories for an image also verify the interpretability of RTT-GAN.",natur imag usual convey rich semant content view differ angl exist imag descript method larg restrict small set bias visual paragraph annot fail cover rich semant paper investig semi supervis paragraph generat framework abl synthes divers semant coher paragraph descript reason local semant region exploit linguist knowledg propos recurr topic transit generat adversari network rtt gan build adversari framework structur paragraph generat multi level paragraph discrimin paragraph generat generat sentenc recurr incorpor region base visual languag attent mechan step qualiti generat paragraph sentenc assess multi level adversari discrimin two aspect name plausibl sentenc level topic transit coher paragraph level joint adversari train rtt gan drive model generat realist paragraph smooth logic transit sentenc topic extens quantit experi imag video paragraph dataset demonstr effect rtt gan supervis semi supervis set qualit result tell divers stori imag also verifi interpret rtt gan,"['Xiaodan Liang', 'Zhiting Hu', 'Hao Zhang', 'Chuang Gan', 'Eric P. Xing']","['cs.CV', 'cs.AI', 'cs.LG']",False,False,False,True,False,False
39,2017-03-28T14:05:28Z,2017-03-20T19:17:14Z,http://arxiv.org/abs/1703.06931v1,http://arxiv.org/pdf/1703.06931v1,Learning Correspondence Structures for Person Re-identification,learn correspond structur person identif,"This paper addresses the problem of handling spatial misalignments due to camera-view changes or human-pose variations in person re-identification. We first introduce a boosting-based approach to learn a correspondence structure which indicates the patch-wise matching probabilities between images from a target camera pair. The learned correspondence structure can not only capture the spatial correspondence pattern between cameras but also handle the viewpoint or human-pose variation in individual images. We further introduce a global constraint-based matching process. It integrates a global matching constraint over the learned correspondence structure to exclude cross-view misalignments during the image patch matching process, hence achieving a more reliable matching score between images. Finally, we also extend our approach by introducing a multi-structure scheme, which learns a set of local correspondence structures to capture the spatial correspondence sub-patterns between a camera pair, so as to handle the spatial misalignments between individual images in a more precise way. Experimental results on various datasets demonstrate the effectiveness of our approach.",paper address problem handl spatial misalign due camera view chang human pose variat person identif first introduc boost base approach learn correspond structur indic patch wise match probabl imag target camera pair learn correspond structur onli captur spatial correspond pattern camera also handl viewpoint human pose variat individu imag introduc global constraint base match process integr global match constraint learn correspond structur exclud cross view misalign dure imag patch match process henc achiev reliabl match score imag final also extend approach introduc multi structur scheme learn set local correspond structur captur spatial correspond sub pattern camera pair handl spatial misalign individu imag precis way experiment result various dataset demonstr effect approach,"['Weiyao Lin', 'Yang Shen', 'Junchi Yan', 'Mingliang Xu', 'Jianxin Wu', 'Jingdong Wang', 'Ke Lu']","['cs.CV', 'cs.AI', 'cs.MM']",False,False,False,True,False,False
47,2017-03-28T14:05:32Z,2017-03-19T15:21:32Z,http://arxiv.org/abs/1703.06452v1,http://arxiv.org/pdf/1703.06452v1,Deep Neural Networks for Semantic Segmentation of Multispectral Remote   Sensing Imagery,deep neural network semant segment multispectr remot sens imageri,"A semantic segmentation algorithm must assign a label to every pixel in an image. Recently, semantic segmentation of RGB imagery has advanced significantly due to deep learning. Because creating datasets for semantic segmentation is laborious, these datasets tend to be significantly smaller than object recognition datasets. This makes it difficult to directly train a deep neural network for semantic segmentation, because it will be prone to overfitting. To cope with this, deep learning models typically use convolutional neural networks pre-trained on large-scale image classification datasets, which are then fine-tuned for semantic segmentation. For non-RGB imagery, this is currently not possible because large-scale labeled non-RGB datasets do not exist. In this paper, we developed two deep neural networks for semantic segmentation of multispectral remote sensing imagery. Prior to training on the target dataset, we initialize the networks with large amounts of synthetic multispectral imagery. We show that this significantly improves results on real-world remote sensing imagery, and we establish a new state-of-the-art result on the challenging Hamlin Beach State Park Dataset.",semant segment algorithm must assign label everi pixel imag recent semant segment rgb imageri advanc signific due deep learn becaus creat dataset semant segment labori dataset tend signific smaller object recognit dataset make difficult direct train deep neural network semant segment becaus prone overfit cope deep learn model typic use convolut neural network pre train larg scale imag classif dataset fine tune semant segment non rgb imageri current possibl becaus larg scale label non rgb dataset exist paper develop two deep neural network semant segment multispectr remot sens imageri prior train target dataset initi network larg amount synthet multispectr imageri show signific improv result real world remot sens imageri establish new state art result challeng hamlin beach state park dataset,"['Ronald Kemker', 'Christopher Kanan']","['cs.CV', 'cs.AI']",False,False,False,True,False,False
87,2017-03-28T14:05:50Z,2017-03-13T12:49:20Z,http://arxiv.org/abs/1703.04363v1,http://arxiv.org/pdf/1703.04363v1,Deep Value Networks Learn to Evaluate and Iteratively Refine Structured   Outputs,deep valu network learn evalu iter refin structur output,"We approach structured output prediction by learning a deep value network (DVN) that evaluates different output structures for a given input. For example, when applied to image segmentation, the value network takes an image and a segmentation mask as inputs and predicts a scalar score evaluating the mask quality and its correspondence with the image. Once the value network is optimized, at inference, it finds output structures that maximize the score of the value net via gradient descent on continuous relaxations of structured outputs. Thus DVN takes advantage of the joint modeling of the inputs and outputs. Our framework applies to a wide range of structured output prediction problems. We conduct experiments on multi-label classification based on text data and on image segmentation problems. DVN outperforms several strong baselines and the state-of-the-art results on these benchmarks. In addition, on image segmentation, the proposed deep value network learns complex shape priors and effectively combines image information with the prior to obtain competitive segmentation results.",approach structur output predict learn deep valu network dvn evalu differ output structur given input exampl appli imag segment valu network take imag segment mask input predict scalar score evalu mask qualiti correspond imag onc valu network optim infer find output structur maxim score valu net via gradient descent continu relax structur output thus dvn take advantag joint model input output framework appli wide rang structur output predict problem conduct experi multi label classif base text data imag segment problem dvn outperform sever strong baselin state art result benchmark addit imag segment propos deep valu network learn complex shape prior effect combin imag inform prior obtain competit segment result,"['Michael Gygli', 'Mohammad Norouzi', 'Anelia Angelova']","['cs.LG', 'cs.AI', 'cs.CV']",False,False,False,True,False,False
218,2017-03-28T14:07:20Z,2017-03-12T07:21:50Z,http://arxiv.org/abs/1703.04079v1,http://arxiv.org/pdf/1703.04079v1,SurfNet: Generating 3D shape surfaces using deep residual networks,surfnet generat shape surfac use deep residu network,"3D shape models are naturally parameterized using vertices and faces, \ie, composed of polygons forming a surface. However, current 3D learning paradigms for predictive and generative tasks using convolutional neural networks focus on a voxelized representation of the object. Lifting convolution operators from the traditional 2D to 3D results in high computational overhead with little additional benefit as most of the geometry information is contained on the surface boundary. Here we study the problem of directly generating the 3D shape surface of rigid and non-rigid shapes using deep convolutional neural networks. We develop a procedure to create consistent `geometry images' representing the shape surface of a category of 3D objects. We then use this consistent representation for category-specific shape surface generation from a parametric representation or an image by developing novel extensions of deep residual networks for the task of geometry image generation. Our experiments indicate that our network learns a meaningful representation of shape surfaces allowing it to interpolate between shape orientations and poses, invent new shape surfaces and reconstruct 3D shape surfaces from previously unseen images.",shape model natur parameter use vertic face ie compos polygon form surfac howev current learn paradigm predict generat task use convolut neural network focus voxel represent object lift convolut oper tradit result high comput overhead littl addit benefit geometri inform contain surfac boundari studi problem direct generat shape surfac rigid non rigid shape use deep convolut neural network develop procedur creat consist geometri imag repres shape surfac categori object use consist represent categori specif shape surfac generat parametr represent imag develop novel extens deep residu network task geometri imag generat experi indic network learn meaning represent shape surfac allow interpol shape orient pose invent new shape surfac reconstruct shape surfac previous unseen imag,"['Ayan Sinha', 'Asim Unmesh', 'Qixing Huang', 'Karthik Ramani']","['cs.CV', 'cs.CG']",False,False,False,True,False,False
250,2017-03-28T14:07:37Z,2017-02-17T14:34:08Z,http://arxiv.org/abs/1702.05358v1,http://arxiv.org/pdf/1702.05358v1,Computational topology of graphs on surfaces,comput topolog graph surfac,"Computational topology is an area that revisits topological problems from an algorithmic point of view, and develops topological tools for improved algorithms. We survey results in computational topology that are concerned with graphs drawn on surfaces. Typical questions include representing surfaces and graphs embedded on them computationally, deciding whether a graph embeds on a surface, solving computational problems related to homotopy, optimizing curves and graphs on surfaces, and solving standard graph algorithm problems more efficiently in the case of surface-embedded graphs.",comput topolog area revisit topolog problem algorithm point view develop topolog tool improv algorithm survey result comput topolog concern graph drawn surfac typic question includ repres surfac graph embed comput decid whether graph emb surfac solv comput problem relat homotopi optim curv graph surfac solv standard graph algorithm problem effici case surfac embed graph,['Éric Colin de Verdière'],"['cs.CG', 'cs.DM', 'cs.DS', 'math.AT', 'math.CO', '68U05, 05C10, 57M15, 68R10', 'F.2.2; G.2.2; I.3.5']",False,False,False,True,False,True
345,2017-03-28T14:06:16Z,2017-03-20T00:28:07Z,http://arxiv.org/abs/1703.05908v2,http://arxiv.org/pdf/1703.05908v2,Learning Robust Visual-Semantic Embeddings,learn robust visual semant embed,"Many of the existing methods for learning joint embedding of images and text use only supervised information from paired images and its textual attributes. Taking advantage of the recent success of unsupervised learning in deep neural networks, we propose an end-to-end learning framework that is able to extract more robust multi-modal representations across domains. The proposed method combines representation learning models (i.e., auto-encoders) together with cross-domain learning criteria (i.e., Maximum Mean Discrepancy loss) to learn joint embeddings for semantic and visual features. A novel technique of unsupervised-data adaptation inference is introduced to construct more comprehensive embeddings for both labeled and unlabeled data. We evaluate our method on Animals with Attributes and Caltech-UCSD Birds 200-2011 dataset with a wide range of applications, including zero and few-shot image recognition and retrieval, from inductive to transductive settings. Empirically, we show that our framework improves over the current state of the art on many of the considered tasks.",mani exist method learn joint embed imag text use onli supervis inform pair imag textual attribut take advantag recent success unsupervis learn deep neural network propos end end learn framework abl extract robust multi modal represent across domain propos method combin represent learn model auto encod togeth cross domain learn criteria maximum mean discrep loss learn joint embed semant visual featur novel techniqu unsupervis data adapt infer introduc construct comprehens embed label unlabel data evalu method anim attribut caltech ucsd bird dataset wide rang applic includ zero shot imag recognit retriev induct transduct set empir show framework improv current state art mani consid task,"['Yao-Hung Hubert Tsai', 'Liang-Kang Huang', 'Ruslan Salakhutdinov']","['cs.CV', 'cs.CL', 'cs.LG']",False,False,False,True,False,False
402,2017-03-28T14:08:43Z,2017-03-27T17:37:33Z,http://arxiv.org/abs/1703.09200v1,http://arxiv.org/pdf/1703.09200v1,Deep Poincare Map For Robust Medical Image Segmentation,deep poincar map robust medic imag segment,"Precise segmentation is a prerequisite for an accurate quantification of the imaged objects. It is a very challenging task in many medical imaging applications due to relatively poor image quality and data scarcity. In this work, we present an innovative segmentation paradigm, named Deep Poincare Map (DPM), by coupling the dynamical system theory with a novel deep learning based approach. Firstly, we model the image segmentation process as a dynamical system, in which limit cycle models the boundary of the region of interest (ROI). Secondly, instead of segmenting the ROI directly, convolutional neural network is employed to predict the vector field of the dynamical system. Finally, the boundary of the ROI is identified using the Poincare map and the flow integration. We demonstrate that our segmentation model can be built using a very limited number of train- ing data. By cross-validation, we can achieve a mean Dice score of 94% compared to the manual delineation (ground truth) of the left ventricle ROI defined by clinical experts on a cardiac MRI dataset. Compared with other state-of-the-art methods, we can conclude that the proposed DPM method is adaptive, accurate and robust. It is straightforward to apply this method for other medical imaging applications.",precis segment prerequisit accur quantif imag object veri challeng task mani medic imag applic due relat poor imag qualiti data scarciti work present innov segment paradigm name deep poincar map dpm coupl dynam system theori novel deep learn base approach first model imag segment process dynam system limit cycl model boundari region interest roi second instead segment roi direct convolut neural network employ predict vector field dynam system final boundari roi identifi use poincar map flow integr demonstr segment model built use veri limit number train ing data cross valid achiev mean dice score compar manual delin ground truth left ventricl roi defin clinic expert cardiac mri dataset compar state art method conclud propos dpm method adapt accur robust straightforward appli method medic imag applic,"['Yuanhan Mo', 'Fangde Liu', 'Jingqing Zhang', 'Guang Yang', 'Taigang He', 'Yike Guo']",['cs.CV'],False,False,False,True,False,False
407,2017-03-28T14:08:43Z,2017-03-27T15:57:27Z,http://arxiv.org/abs/1703.09157v1,http://arxiv.org/pdf/1703.09157v1,Reweighted Infrared Patch-Tensor Model With Both Non-Local and Local   Priors for Single-Frame Small Target Detection,reweight infrar patch tensor model non local local prior singl frame small target detect,"Many state-of-the-art methods have been proposed for infrared small target detection. They work well on the images with homogeneous backgrounds and high-contrast targets. However, when facing highly heterogeneous backgrounds, they would not perform very well, mainly due to: 1) the existence of strong edges and other interfering components, 2) not utilizing the priors fully. Inspired by this, we propose a novel method to exploit both local and non-local priors simultaneously. Firstly, we employ a new infrared patch-tensor (IPT) model to represent the image and preserve its spatial correlations. Exploiting the target sparse prior and background non-local self-correlation prior, the target-background separation is modeled as a robust low-rank tensor recovery problem. Moreover, with the help of the structure tensor and reweighted idea, we design an entry-wise local-structure-adaptive and sparsity enhancing weight to replace the globally constant weighting parameter. The decomposition could be achieved via the element-wise reweighted higher-order robust principal component analysis with an additional convergence condition according to the practical situation of target detection. Extensive experiments demonstrate that our model outperforms the other state-of-the-arts, in particular for the images with very dim targets and heavy clutters.",mani state art method propos infrar small target detect work well imag homogen background high contrast target howev face high heterogen background would perform veri well main due exist strong edg interf compon util prior fulli inspir propos novel method exploit local non local prior simultan first employ new infrar patch tensor ipt model repres imag preserv spatial correl exploit target spars prior background non local self correl prior target background separ model robust low rank tensor recoveri problem moreov help structur tensor reweight idea design entri wise local structur adapt sparsiti enhanc weight replac global constant weight paramet decomposit could achiev via element wise reweight higher order robust princip compon analysi addit converg condit accord practic situat target detect extens experi demonstr model outperform state art particular imag veri dim target heavi clutter,"['Yimian Dai', 'Yiquan Wu']",['cs.CV'],False,False,False,True,False,False
408,2017-03-28T14:08:43Z,2017-03-27T15:31:00Z,http://arxiv.org/abs/1703.09145v1,http://arxiv.org/pdf/1703.09145v1,"Multi-Path Region-Based Convolutional Neural Network for Accurate   Detection of Unconstrained ""Hard Faces""",multi path region base convolut neural network accur detect unconstrain hard face,"Large-scale variations still pose a challenge in unconstrained face detection. To the best of our knowledge, no current face detection algorithm can detect a face as large as 800 x 800 pixels while simultaneously detecting another one as small as 8 x 8 pixels within a single image with equally high accuracy. We propose a two-stage cascaded face detection framework, Multi-Path Region-based Convolutional Neural Network (MP-RCNN), that seamlessly combines a deep neural network with a classic learning strategy, to tackle this challenge. The first stage is a Multi-Path Region Proposal Network (MP-RPN) that proposes faces at three different scales. It simultaneously utilizes three parallel outputs of the convolutional feature maps to predict multi-scale candidate face regions. The ""atrous"" convolution trick (convolution with up-sampled filters) and a newly proposed sampling layer for ""hard"" examples are embedded in MP-RPN to further boost its performance. The second stage is a Boosted Forests classifier, which utilizes deep facial features pooled from inside the candidate face regions as well as deep contextual features pooled from a larger region surrounding the candidate face regions. This step is included to further remove hard negative samples. Experiments show that this approach achieves state-of-the-art face detection performance on the WIDER FACE dataset ""hard"" partition, outperforming the former best result by 9.6% for the Average Precision.",larg scale variat still pose challeng unconstrain face detect best knowledg current face detect algorithm detect face larg pixel simultan detect anoth one small pixel within singl imag equal high accuraci propos two stage cascad face detect framework multi path region base convolut neural network mp rcnn seamless combin deep neural network classic learn strategi tackl challeng first stage multi path region propos network mp rpn propos face three differ scale simultan util three parallel output convolut featur map predict multi scale candid face region atrous convolut trick convolut sampl filter newli propos sampl layer hard exampl embed mp rpn boost perform second stage boost forest classifi util deep facial featur pool insid candid face region well deep contextu featur pool larger region surround candid face region step includ remov hard negat sampl experi show approach achiev state art face detect perform wider face dataset hard partit outperform former best result averag precis,"['Yuguang Liu', 'Martin D. Levine']",['cs.CV'],False,False,False,True,False,False
413,2017-03-28T14:08:48Z,2017-03-27T09:51:55Z,http://arxiv.org/abs/1703.08987v1,http://arxiv.org/pdf/1703.08987v1,Simultaneous Perception and Path Generation Using Fully Convolutional   Neural Networks,simultan percept path generat use fulli convolut neural network,"In this work, a novel learning-based approach has been developed to generate driving paths by integrating LIDAR point clouds, GPS-IMU information, and Google driving directions. The system is based on a fully convolutional neural network that jointly learns to carry out perception and path generation from real-world driving sequences and that is trained using automatically generated training examples. Several combinations of input data were tested in order to assess the performance gain provided by specific information modalities. The fully convolutional neural network trained using all the available sensors together with driving directions achieved the best MaxF score of 88.13% when considering a region of interest of 60x60 meters. By considering a smaller region of interest, the agreement between predicted paths and ground-truth increased to 92.60%. The positive results obtained in this work indicate that the proposed system may help fill the gap between low-level scene parsing and behavior-reflex approaches by generating outputs that are close to vehicle control and at the same time human-interpretable.",work novel learn base approach develop generat drive path integr lidar point cloud gps imu inform googl drive direct system base fulli convolut neural network joint learn carri percept path generat real world drive sequenc train use automat generat train exampl sever combin input data test order assess perform gain provid specif inform modal fulli convolut neural network train use avail sensor togeth drive direct achiev best maxf score consid region interest meter consid smaller region interest agreement predict path ground truth increas posit result obtain work indic propos system may help fill gap low level scene pars behavior reflex approach generat output close vehicl control time human interpret,"['Luca Caltagirone', 'Mauro Bellone', 'Lennart Svensson', 'Mattias Wahde']",['cs.CV'],False,False,False,True,False,False
417,2017-03-28T14:08:48Z,2017-03-27T03:46:58Z,http://arxiv.org/abs/1703.08917v1,http://arxiv.org/pdf/1703.08917v1,A Visual Measure of Changes to Weighted Self-Organizing Map Patterns,visual measur chang weight self organ map pattern,"Estimating output changes by input changes is the main task in causal analysis. In previous work, input and output Self-Organizing Maps (SOMs) were associated for causal analysis of multivariate and nonlinear data. Based on the association, a weight distribution of the output conditional on a given input was obtained over the output map space. Such a weighted SOM pattern of the output changes when the input changes. In order to analyze the change, it is important to measure the difference of the patterns. Many methods have been proposed for the dissimilarity measure of patterns. However, it remains a major challenge when attempting to measure how the patterns change. In this paper, we propose a visualization approach that simplifies the comparison of the difference in terms of the pattern property. Using this approach, the change can be analyzed by integrating colors and star glyph shapes representing the property dissimilarity. Ecological data is used to demonstrate the usefulness of our approach and the experimental results show that our approach provides the change information effectively.",estim output chang input chang main task causal analysi previous work input output self organ map som associ causal analysi multivari nonlinear data base associ weight distribut output condit given input obtain output map space weight som pattern output chang input chang order analyz chang import measur differ pattern mani method propos dissimilar measur pattern howev remain major challeng attempt measur pattern chang paper propos visual approach simplifi comparison differ term pattern properti use approach chang analyz integr color star glyph shape repres properti dissimilar ecolog data use demonstr use approach experiment result show approach provid chang inform effect,"['Younjin Chung', 'Joachim Gudmundsson', 'Masahiro Takatsuka']",['cs.CV'],False,False,False,True,False,True
420,2017-03-28T14:08:52Z,2017-03-27T01:36:38Z,http://arxiv.org/abs/1703.08893v1,http://arxiv.org/pdf/1703.08893v1,Transductive Zero-Shot Learning with a Self-training dictionary approach,transduct zero shot learn self train dictionari approach,"As an important and challenging problem in computer vision, zero-shot learning (ZSL) aims at automatically recognizing the instances from unseen object classes without training data. To address this problem, ZSL is usually carried out in the following two aspects: 1) capturing the domain distribution connections between seen classes data and unseen classes data; and 2) modeling the semantic interactions between the image feature space and the label embedding space. Motivated by these observations, we propose a bidirectional mapping based semantic relationship modeling scheme that seeks for crossmodal knowledge transfer by simultaneously projecting the image features and label embeddings into a common latent space. Namely, we have a bidirectional connection relationship that takes place from the image feature space to the latent space as well as from the label embedding space to the latent space. To deal with the domain shift problem, we further present a transductive learning approach that formulates the class prediction problem in an iterative refining process, where the object classification capacity is progressively reinforced through bootstrapping-based model updating over highly reliable instances. Experimental results on three benchmark datasets (AwA, CUB and SUN) demonstrate the effectiveness of the proposed approach against the state-of-the-art approaches.",import challeng problem comput vision zero shot learn zsl aim automat recogn instanc unseen object class without train data address problem zsl usual carri follow two aspect captur domain distribut connect seen class data unseen class data model semant interact imag featur space label embed space motiv observ propos bidirect map base semant relationship model scheme seek crossmod knowledg transfer simultan project imag featur label embed common latent space name bidirect connect relationship take place imag featur space latent space well label embed space latent space deal domain shift problem present transduct learn approach formul class predict problem iter refin process object classif capac progress reinforc bootstrap base model updat high reliabl instanc experiment result three benchmark dataset awa cub sun demonstr effect propos approach state art approach,"['Yunlong Yu', 'Zhong Ji', 'Xi Li', 'Jichang Guo', 'Zhongfei Zhang', 'Haibin Ling', 'Fei Wu']",['cs.CV'],False,False,False,True,False,True
421,2017-03-28T14:08:52Z,2017-03-26T20:28:02Z,http://arxiv.org/abs/1703.08866v1,http://arxiv.org/pdf/1703.08866v1,Multi-View Deep Learning for Consistent Semantic Mapping with RGB-D   Cameras,multi view deep learn consist semant map rgb camera,"Visual scene understanding is an important capability that enables robots to purposefully act in their environment. In this paper, we propose a novel approach to object-class segmentation from multiple RGB-D views using deep learning. We train a deep neural network to predict object-class semantics that is consistent from several view points in a semi-supervised way. At test time, the semantics predictions of our network can be fused more consistently in semantic keyframe maps than predictions of a network trained on individual views. We base our network architecture on a recent single-view deep learning approach to RGB and depth fusion for semantic object-class segmentation and enhance it with multi-scale loss minimization. We obtain the camera trajectory using RGB-D SLAM and warp the predictions of RGB-D images into ground-truth annotated frames in order to enforce multi-view consistency during training. At test time, predictions from multiple views are fused into keyframes. We propose and analyze several methods for enforcing multi-view consistency during training and testing. We evaluate the benefit of multi-view consistency training and demonstrate that pooling of deep features and fusion over multiple views outperforms single-view baselines on the NYUDv2 benchmark for semantic segmentation. Our end-to-end trained network achieves state-of-the-art performance on the NYUDv2 dataset in single-view segmentation as well as multi-view semantic fusion.",visual scene understand import capabl enabl robot purpos act environ paper propos novel approach object class segment multipl rgb view use deep learn train deep neural network predict object class semant consist sever view point semi supervis way test time semant predict network fuse consist semant keyfram map predict network train individu view base network architectur recent singl view deep learn approach rgb depth fusion semant object class segment enhanc multi scale loss minim obtain camera trajectori use rgb slam warp predict rgb imag ground truth annot frame order enforc multi view consist dure train test time predict multipl view fuse keyfram propos analyz sever method enforc multi view consist dure train test evalu benefit multi view consist train demonstr pool deep featur fusion multipl view outperform singl view baselin nyudv benchmark semant segment end end train network achiev state art perform nyudv dataset singl view segment well multi view semant fusion,"['Lingni Ma', 'Jörg Stückler', 'Christian Kerl', 'Daniel Cremers']",['cs.CV'],False,False,False,True,False,False
423,2017-03-28T14:08:52Z,2017-03-26T16:18:48Z,http://arxiv.org/abs/1703.08837v1,http://arxiv.org/abs/1703.08837v1,Person Re-Identification by Camera Correlation Aware Feature   Augmentation,person identif camera correl awar featur augment,"The challenge of person re-identification (re-id) is to match individual images of the same person captured by different non-overlapping camera views against significant and unknown cross-view feature distortion. While a large number of distance metric/subspace learning models have been developed for re-id, the cross-view transformations they learned are view-generic and thus potentially less effective in quantifying the feature distortion inherent to each camera view. Learning view-specific feature transformations for re-id (i.e., view-specific re-id), an under-studied approach, becomes an alternative resort for this problem. In this work, we formulate a novel view-specific person re-identification framework from the feature augmentation point of view, called Camera coRrelation Aware Feature augmenTation (CRAFT). Specifically, CRAFT performs cross-view adaptation by automatically measuring camera correlation from cross-view visual data distribution and adaptively conducting feature augmentation to transform the original features into a new adaptive space. Through our augmentation framework, view-generic learning algorithms can be readily generalized to learn and optimize view-specific sub-models whilst simultaneously modelling view-generic discrimination information. Therefore, our framework not only inherits the strength of view-generic model learning but also provides an effective way to take into account view specific characteristics. Our CRAFT framework can be extended to jointly learn view-specific feature transformations for person re-id across a large network with more than two cameras, a largely under-investigated but realistic re-id setting. Additionally, we present a domain-generic deep person appearance representation which is designed particularly to be towards view invariant for facilitating cross-view adaptation by CRAFT.",challeng person identif id match individu imag person captur differ non overlap camera view signific unknown cross view featur distort larg number distanc metric subspac learn model develop id cross view transform learn view generic thus potenti less effect quantifi featur distort inher camera view learn view specif featur transform id view specif id studi approach becom altern resort problem work formul novel view specif person identif framework featur augment point view call camera correl awar featur augment craft specif craft perform cross view adapt automat measur camera correl cross view visual data distribut adapt conduct featur augment transform origin featur new adapt space augment framework view generic learn algorithm readili general learn optim view specif sub model whilst simultan model view generic discrimin inform therefor framework onli inherit strength view generic model learn also provid effect way take account view specif characterist craft framework extend joint learn view specif featur transform person id across larg network two camera larg investig realist id set addit present domain generic deep person appear represent design particular toward view invari facilit cross view adapt craft,"['Ying-Cong Chen', 'Xiatian Zhu', 'Wei-Shi Zheng', 'Jian-Huang Lai']",['cs.CV'],False,False,False,True,False,False
424,2017-03-28T14:08:52Z,2017-03-26T16:17:55Z,http://arxiv.org/abs/1703.08836v1,http://arxiv.org/pdf/1703.08836v1,Learned multi-patch similarity,learn multi patch similar,"Estimating a depth map from multiple views of a scene is a fundamental task in computer vision. As soon as more than two viewpoints are available, one faces the very basic question how to measure similarity across >2 image patches. Surprisingly, no direct solution exists, instead it is common to fall back to more or less robust averaging of two-view similarities. Encouraged by the success of machine learning, and in particular convolutional neural networks, we propose to learn a matching function which directly maps multiple image patches to a scalar similarity score. Experiments on several multi-view datasets demonstrate that this approach has advantages over methods based on pairwise patch similarity.",estim depth map multipl view scene fundament task comput vision soon two viewpoint avail one face veri basic question measur similar across imag patch surpris direct solut exist instead common fall back less robust averag two view similar encourag success machin learn particular convolut neural network propos learn match function direct map multipl imag patch scalar similar score experi sever multi view dataset demonstr approach advantag method base pairwis patch similar,"['Wilfried Hartmann', 'Silvano Galliani', 'Michal Havlena', 'Konrad Schindler', 'Luc Van Gool']","['cs.CV', 'cs.LG']",False,False,False,True,False,False
427,2017-03-28T14:08:52Z,2017-03-26T05:48:38Z,http://arxiv.org/abs/1703.08770v1,http://arxiv.org/pdf/1703.08770v1,SCAN: Structure Correcting Adversarial Network for Chest X-rays Organ   Segmentation,scan structur correct adversari network chest ray organ segment,"Chest X-ray (CXR) is one of the most commonly prescribed medical imaging procedures, often with over 2-10x more scans than other imaging modalities such as MRI, CT scan, and PET scans. These voluminous CXR scans place significant workloads on radiologists and medical practitioners. Organ segmentation is a crucial step to obtain effective computer-aided detection on CXR. In this work, we propose Structure Correcting Adversarial Network (SCAN) to segment lung fields and the heart in CXR images. SCAN incorporates a critic network to impose on the convolutional segmentation network the structural regularities emerging from human physiology. During training, the critic network learns to discriminate between the ground truth organ annotations from the masks synthesized by the segmentation network. Through this adversarial process the critic network learns the higher order structures and guides the segmentation model to achieve realistic segmentation outcomes. Extensive experiments show that our method produces highly accurate and natural segmentation. Using only very limited training data available, our model reaches human-level performance without relying on any existing trained model or dataset. Our method also generalizes well to CXR images from a different patient population and disease profiles, surpassing the current state-of-the-art.",chest ray cxr one common prescrib medic imag procedur often scan imag modal mri ct scan pet scan volumin cxr scan place signific workload radiologist medic practition organ segment crucial step obtain effect comput aid detect cxr work propos structur correct adversari network scan segment lung field heart cxr imag scan incorpor critic network impos convolut segment network structur regular emerg human physiolog dure train critic network learn discrimin ground truth organ annot mask synthes segment network adversari process critic network learn higher order structur guid segment model achiev realist segment outcom extens experi show method produc high accur natur segment use onli veri limit train data avail model reach human level perform without reli ani exist train model dataset method also general well cxr imag differ patient popul diseas profil surpass current state art,"['Wei Dai', 'Joseph Doyle', 'Xiaodan Liang', 'Hao Zhang', 'Nanqing Dong', 'Yuan Li', 'Eric P. Xing']",['cs.CV'],False,False,False,True,False,False
431,2017-03-28T14:08:56Z,2017-03-25T16:49:03Z,http://arxiv.org/abs/1703.08710v1,http://arxiv.org/pdf/1703.08710v1,Count-ception: Counting by Fully Convolutional Redundant Counting,count ception count fulli convolut redund count,"Counting objects in digital images is a process that should be replaced by machines. This tedious task is time consuming and prone to errors due to fatigue of human annotators. The goal is to have a system that takes as input an image and returns a count of the objects inside and justification for the prediction in the form of object localization. We repose a problem, originally posed by Lempitsky and Zisserman, to instead predict a count map which contains redundant counts based on the receptive field of a smaller regression network. The regression network predicts a count of the objects that exist inside this frame. By processing the image in a fully convolutional way each pixel is going to be accounted for some number of times, the number of windows which include it, which is the size of each window, (i.e., 32x32 = 1024). To recover the true count take the average over the redundant predictions. Our contribution is redundant counting instead of predicting a density map in order to average over errors. We also propose a novel deep neural network architecture adapted from the Inception family of networks called the Count-ception network. Together our approach results in a 20% gain over the state of the art method by Xie, Noble, and Zisserman in 2016.",count object digit imag process replac machin tedious task time consum prone error due fatigu human annot goal system take input imag return count object insid justif predict form object local repos problem origin pose lempitski zisserman instead predict count map contain redund count base recept field smaller regress network regress network predict count object exist insid frame process imag fulli convolut way pixel go account number time number window includ size window recov true count take averag redund predict contribut redund count instead predict densiti map order averag error also propos novel deep neural network architectur adapt incept famili network call count ception network togeth approach result gain state art method xie nobl zisserman,"['Joseph Paul Cohen', 'Henry Z. Lo', 'Yoshua Bengio']","['cs.CV', 'cs.LG', 'stat.ML']",False,False,False,True,False,False
433,2017-03-28T14:08:56Z,2017-03-25T06:18:38Z,http://arxiv.org/abs/1703.08653v1,http://arxiv.org/pdf/1703.08653v1,Bayesian Optimization for Refining Object Proposals,bayesian optim refin object propos,"We develop a general-purpose algorithm using a Bayesian optimization framework for the efficient refinement of object proposals. While recent research has achieved substantial progress for object localization and related objectives in computer vision, current state-of-the-art object localization procedures are nevertheless encumbered by inefficiency and inaccuracy. We present a novel, computationally efficient method for refining inaccurate bounding-box proposals for a target object using Bayesian optimization. Offline, image features from a convolutional neural network are used to train a model to predict the offset distance of an object proposal from a target object. Online, this model is used in a Bayesian active search to improve inaccurate object proposals. In experiments, we compare our approach to a state-of-the-art bounding-box regression method for localization refinement of pedestrian object proposals. Our method exhibits a substantial improvement for the task of localization refinement over this baseline regression method.",develop general purpos algorithm use bayesian optim framework effici refin object propos recent research achiev substanti progress object local relat object comput vision current state art object local procedur nevertheless encumb ineffici inaccuraci present novel comput effici method refin inaccur bound box propos target object use bayesian optim offlin imag featur convolut neural network use train model predict offset distanc object propos target object onlin model use bayesian activ search improv inaccur object propos experi compar approach state art bound box regress method local refin pedestrian object propos method exhibit substanti improv task local refin baselin regress method,"['Anthony D. Rhodes', 'Jordan Witte', 'Melanie Mitchell', 'Bruno Jedynak']",['cs.CV'],True,False,False,True,False,False
435,2017-03-28T14:08:56Z,2017-03-24T23:50:52Z,http://arxiv.org/abs/1703.08628v1,http://arxiv.org/pdf/1703.08628v1,AMAT: Medial Axis Transform for Natural Images,amat medial axi transform natur imag,"The medial axis transform (MAT) is a powerful shape abstraction that has been successfully used in shape editing, matching and retrieval. Despite its long history, the MAT has not found widespread use in tasks involving natural images, due to the lack of a generalization that accommodates color and texture. In this paper we introduce Appearance-MAT (AMAT), by framing the MAT of natural images as a weighted geometric set cover problem. We make the following contributions: i) we extend previous medial point detection methods for color images, by associating each medial point with a local scale; ii) inspired by the invertibility property of the binary MAT, we also associate each medial point with a local encoding that allows us to invert the AMAT, reconstructing the input image; iii) we describe a clustering scheme that takes advantage of the additional scale and appearance information to group individual points into medial branches, providing a shape decomposition of the underlying image regions. In our experiments, we show state-of-the-art performance in medial point detection on Berkeley Medial AXes (BMAX500), a new dataset of medial axes based on the established BSDS500 database. We also measure the quality of reconstructed images from the same dataset, obtained by inverting their computed AMAT. Our approach delivers significantly better reconstruction quality with respect to three baselines, using just 10% of the image pixels. Our code is available at https://github.com/tsogkas/amat.",medial axi transform mat power shape abstract success use shape edit match retriev despit long histori mat found widespread use task involv natur imag due lack general accommod color textur paper introduc appear mat amat frame mat natur imag weight geometr set cover problem make follow contribut extend previous medial point detect method color imag associ medial point local scale ii inspir invert properti binari mat also associ medial point local encod allow us invert amat reconstruct input imag iii describ cluster scheme take advantag addit scale appear inform group individu point medial branch provid shape decomposit imag region experi show state art perform medial point detect berkeley medial axe bmax new dataset medial axe base establish bsds databas also measur qualiti reconstruct imag dataset obtain invert comput amat approach deliv signific better reconstruct qualiti respect three baselin use imag pixel code avail https github com tsogka amat,"['Stavros Tsogkas', 'Sven Dickinson']",['cs.CV'],False,False,False,True,False,False
437,2017-03-28T14:08:56Z,2017-03-24T21:26:16Z,http://arxiv.org/abs/1703.08603v1,http://arxiv.org/pdf/1703.08603v1,Adversarial Examples for Semantic Segmentation and Object Detection,adversari exampl semant segment object detect,"It has been well demonstrated that adversarial examples, i.e., natural images with visually imperceptible perturbations added, generally exist for deep networks to fail on image classification. In this paper, we extend adversarial examples to semantic segmentation and object detection which are much more difficult. Our observation is that both segmentation and detection are based on classifying multiple targets on an image (e.g., the basic target is a pixel or a receptive field in segmentation, and an object proposal in detection), which inspires us to optimize a loss function over a set of pixels/proposals for generating adversarial perturbations. Based on this idea, we propose a novel algorithm named Dense Adversary Generation (DAG), which generates a large family of adversarial examples, and applies to a wide range of state-of-the-art deep networks for segmentation and detection. We also find that the adversarial perturbations can be transferred across networks with different training data, based on different architectures, and even for different recognition tasks. In particular, the transferability across networks with the same architecture is more significant than in other cases. Besides, summing up heterogeneous perturbations often leads to better transfer performance, which provides an effective method of black-box adversarial attack.",well demonstr adversari exampl natur imag visual impercept perturb ad general exist deep network fail imag classif paper extend adversari exampl semant segment object detect much difficult observ segment detect base classifi multipl target imag basic target pixel recept field segment object propos detect inspir us optim loss function set pixel propos generat adversari perturb base idea propos novel algorithm name dens adversari generat dag generat larg famili adversari exampl appli wide rang state art deep network segment detect also find adversari perturb transfer across network differ train data base differ architectur even differ recognit task particular transfer across network architectur signific case besid sum heterogen perturb often lead better transfer perform provid effect method black box adversari attack,"['Cihang Xie', 'Jianyu Wang', 'Zhishuai Zhang', 'Yuyin Zhou', 'Lingxi Xie', 'Alan Yuille']",['cs.CV'],False,False,False,True,False,False
440,2017-03-28T14:09:00Z,2017-03-24T16:41:19Z,http://arxiv.org/abs/1703.08497v1,http://arxiv.org/pdf/1703.08497v1,Local Deep Neural Networks for Age and Gender Classification,local deep neural network age gender classif,"Local deep neural networks have been recently introduced for gender recognition. Although, they achieve very good performance they are very computationally expensive to train. In this work, we introduce a simplified version of local deep neural networks which significantly reduces the training time. Instead of using hundreds of patches per image, as suggested by the original method, we propose to use 9 overlapping patches per image which cover the entire face region. This results in a much reduced training time, since just 9 patches are extracted per image instead of hundreds, at the expense of a slightly reduced performance. We tested the proposed modified local deep neural networks approach on the LFW and Adience databases for the task of gender and age classification. For both tasks and both databases the performance is up to 1% lower compared to the original version of the algorithm. We have also investigated which patches are more discriminative for age and gender classification. It turns out that the mouth and eyes regions are useful for age classification, whereas just the eye region is useful for gender classification.",local deep neural network recent introduc gender recognit although achiev veri good perform veri comput expens train work introduc simplifi version local deep neural network signific reduc train time instead use hundr patch per imag suggest origin method propos use overlap patch per imag cover entir face region result much reduc train time sinc patch extract per imag instead hundr expens slight reduc perform test propos modifi local deep neural network approach lfw adienc databas task gender age classif task databas perform lower compar origin version algorithm also investig patch discrimin age gender classif turn mouth eye region use age classif wherea eye region use gender classif,"['Zukang Liao', 'Stavros Petridis', 'Maja Pantic']",['cs.CV'],False,False,False,True,False,False
442,2017-03-28T14:09:00Z,2017-03-24T16:27:57Z,http://arxiv.org/abs/1703.08492v1,http://arxiv.org/pdf/1703.08492v1,Content-Based Image Retrieval Based on Late Fusion of Binary and Local   Descriptors,content base imag retriev base late fusion binari local descriptor,"One of the challenges in Content-Based Image Retrieval (CBIR) is to reduce the semantic gaps between low-level features and high-level semantic concepts. In CBIR, the images are represented in the feature space and the performance of CBIR depends on the type of selected feature representation. Late fusion also known as visual words integration is applied to enhance the performance of image retrieval. The recent advances in image retrieval diverted the focus of research towards the use of binary descriptors as they are reported computationally efficient. In this paper, we aim to investigate the late fusion of Fast Retina Keypoint (FREAK) and Scale Invariant Feature Transform (SIFT). The late fusion of binary and local descriptor is selected because among binary descriptors, FREAK has shown good results in classification-based problems while SIFT is robust to translation, scaling, rotation and small distortions. The late fusion of FREAK and SIFT integrates the performance of both feature descriptors for an effective image retrieval. Experimental results and comparisons show that the proposed late fusion enhances the performances of image retrieval.",one challeng content base imag retriev cbir reduc semant gap low level featur high level semant concept cbir imag repres featur space perform cbir depend type select featur represent late fusion also known visual word integr appli enhanc perform imag retriev recent advanc imag retriev divert focus research toward use binari descriptor report comput effici paper aim investig late fusion fast retina keypoint freak scale invari featur transform sift late fusion binari local descriptor select becaus among binari descriptor freak shown good result classif base problem sift robust translat scale rotat small distort late fusion freak sift integr perform featur descriptor effect imag retriev experiment result comparison show propos late fusion enhanc perform imag retriev,"['Nouman Ali', 'Danish Ali Mazhar', 'Zeshan Iqbal', 'Rehan Ashraf', 'Jawad Ahmed', 'Farrukh Zeeshan Khan']",['cs.CV'],False,True,False,True,False,False
443,2017-03-28T14:09:00Z,2017-03-24T15:41:01Z,http://arxiv.org/abs/1703.08472v1,http://arxiv.org/pdf/1703.08472v1,Medical Image Retrieval using Deep Convolutional Neural Network,medic imag retriev use deep convolut neural network,"With a widespread use of digital imaging data in hospitals, the size of medical image repositories is increasing rapidly. This causes difficulty in managing and querying these large databases leading to the need of content based medical image retrieval (CBMIR) systems. A major challenge in CBMIR systems is the semantic gap that exists between the low level visual information captured by imaging devices and high level semantic information perceived by human. The efficacy of such systems is more crucial in terms of feature representations that can characterize the high-level information completely. In this paper, we propose a framework of deep learning for CBMIR system by using deep Convolutional Neural Network (CNN) that is trained for classification of medical images. An intermodal dataset that contains twenty four classes and five modalities is used to train the network. The learned features and the classification results are used to retrieve medical images. For retrieval, best results are achieved when class based predictions are used. An average classification accuracy of 99.77% and a mean average precision of 0.69 is achieved for retrieval task. The proposed method is best suited to retrieve multimodal medical images for different body organs.",widespread use digit imag data hospit size medic imag repositori increas rapid caus difficulti manag queri larg databas lead need content base medic imag retriev cbmir system major challeng cbmir system semant gap exist low level visual inform captur imag devic high level semant inform perceiv human efficaci system crucial term featur represent character high level inform complet paper propos framework deep learn cbmir system use deep convolut neural network cnn train classif medic imag intermod dataset contain twenti four class five modal use train network learn featur classif result use retriev medic imag retriev best result achiev class base predict use averag classif accuraci mean averag precis achiev retriev task propos method best suit retriev multimod medic imag differ bodi organ,"['Adnan Qayyum', 'Syed Muhammad Anwar', 'Muhammad Awais', 'Muhammad Majid']",['cs.CV'],False,False,False,True,False,False
444,2017-03-28T14:09:00Z,2017-03-27T07:23:05Z,http://arxiv.org/abs/1703.08448v2,http://arxiv.org/pdf/1703.08448v2,Object Region Mining with Adversarial Erasing: A Simple Classification   to Semantic Segmentation Approach,object region mine adversari eras simpl classif semant segment approach,"We investigate a principle way to progressively mine discriminative object regions using classification networks to address the weakly-supervised semantic segmentation problems. Classification networks are only responsive to small and sparse discriminative regions from the object of interest, which deviates from the requirement of the segmentation task that needs to localize dense, interior and integral regions for pixel-wise inference. To mitigate this gap, we propose a new adversarial erasing approach for localizing and expanding object regions progressively. Starting with a single small object region, our proposed approach drives the classification network to sequentially discover new and complement object regions by erasing the current mined regions in an adversarial manner. These localized regions eventually constitute a dense and complete object region for learning semantic segmentation. To further enhance the quality of the discovered regions by adversarial erasing, an online prohibitive segmentation learning approach is developed to collaborate with adversarial erasing by providing auxiliary segmentation supervision modulated by the more reliable classification scores. Despite its apparent simplicity, the proposed approach achieves 55.0% and 55.7% mean Intersection-over-Union (mIoU) scores on PASCAL VOC 2012 val and test sets, which are the new state-of-the-arts.",investig principl way progress mine discrimin object region use classif network address weak supervis semant segment problem classif network onli respons small spars discrimin region object interest deviat requir segment task need local dens interior integr region pixel wise infer mitig gap propos new adversari eras approach local expand object region progress start singl small object region propos approach drive classif network sequenti discov new complement object region eras current mine region adversari manner local region eventu constitut dens complet object region learn semant segment enhanc qualiti discov region adversari eras onlin prohibit segment learn approach develop collabor adversari eras provid auxiliari segment supervis modul reliabl classif score despit appar simplic propos approach achiev mean intersect union miou score pascal voc val test set new state art,"['Yunchao Wei', 'Jiashi Feng', 'Xiaodan Liang', 'Ming-Ming Cheng', 'Yao Zhao', 'Shuicheng Yan']",['cs.CV'],False,False,False,True,False,False
446,2017-03-28T14:09:00Z,2017-03-24T11:58:14Z,http://arxiv.org/abs/1703.08378v1,http://arxiv.org/pdf/1703.08378v1,Feature Fusion using Extended Jaccard Graph and Stochastic Gradient   Descent for Robot,featur fusion use extend jaccard graph stochast gradient descent robot,"Robot vision is a fundamental device for human-robot interaction and robot complex tasks. In this paper, we use Kinect and propose a feature graph fusion (FGF) for robot recognition. Our feature fusion utilizes RGB and depth information to construct fused feature from Kinect. FGF involves multi-Jaccard similarity to compute a robust graph and utilize word embedding method to enhance the recognition results. We also collect DUT RGB-D face dataset and a benchmark datset to evaluate the effectiveness and efficiency of our method. The experimental results illustrate FGF is robust and effective to face and object datasets in robot applications.",robot vision fundament devic human robot interact robot complex task paper use kinect propos featur graph fusion fgf robot recognit featur fusion util rgb depth inform construct fuse featur kinect fgf involv multi jaccard similar comput robust graph util word embed method enhanc recognit result also collect dut rgb face dataset benchmark datset evalu effect effici method experiment result illustr fgf robust effect face object dataset robot applic,"['Shenglan Liu', 'Muxin Sun', 'Wei Wang', 'Feilong Wang']","['cs.CV', 'cs.LG', 'cs.RO']",False,False,False,True,False,False
450,2017-03-28T14:09:05Z,2017-03-24T05:54:11Z,http://arxiv.org/abs/1703.08289v1,http://arxiv.org/pdf/1703.08289v1,Deep Direct Regression for Multi-Oriented Scene Text Detection,deep direct regress multi orient scene text detect,"In this paper, we first provide a new perspective to divide existing high performance object detection methods into direct and indirect regressions. Direct regression performs boundary regression by predicting the offsets from a given point, while indirect regression predicts the offsets from some bounding box proposals. Then we analyze the drawbacks of the indirect regression, which the recent state-of-the-art detection structures like Faster-RCNN and SSD follows, for multi-oriented scene text detection, and point out the potential superiority of direct regression. To verify this point of view, we propose a deep direct regression based method for multi-oriented scene text detection. Our detection framework is simple and effective with a fully convolutional network and one-step post processing. The fully convolutional network is optimized in an end-to-end way and has bi-task outputs where one is pixel-wise classification between text and non-text, and the other is direct regression to determine the vertex coordinates of quadrilateral text boundaries. The proposed method is particularly beneficial for localizing incidental scene texts. On the ICDAR2015 Incidental Scene Text benchmark, our method achieves the F1-measure of 81%, which is a new state-of-the-art and significantly outperforms previous approaches. On other standard datasets with focused scene texts, our method also reaches the state-of-the-art performance.",paper first provid new perspect divid exist high perform object detect method direct indirect regress direct regress perform boundari regress predict offset given point indirect regress predict offset bound box propos analyz drawback indirect regress recent state art detect structur like faster rcnn ssd follow multi orient scene text detect point potenti superior direct regress verifi point view propos deep direct regress base method multi orient scene text detect detect framework simpl effect fulli convolut network one step post process fulli convolut network optim end end way bi task output one pixel wise classif text non text direct regress determin vertex coordin quadrilater text boundari propos method particular benefici local incident scene text icdar incident scene text benchmark method achiev measur new state art signific outperform previous approach standard dataset focus scene text method also reach state art perform,"['Wenhao He', 'Xu-Yao Zhang', 'Fei Yin', 'Cheng-Lin Liu']",['cs.CV'],True,False,False,True,False,False
460,2017-03-28T14:09:09Z,2017-03-23T12:19:09Z,http://arxiv.org/abs/1703.08033v1,http://arxiv.org/pdf/1703.08033v1,Generative Adversarial Residual Pairwise Networks for One Shot Learning,generat adversari residu pairwis network one shot learn,"Deep neural networks achieve unprecedented performance levels over many tasks and scale well with large quantities of data, but performance in the low-data regime and tasks like one shot learning still lags behind. While recent work suggests many hypotheses from better optimization to more complicated network structures, in this work we hypothesize that having a learnable and more expressive similarity objective is an essential missing component. Towards overcoming that, we propose a network design inspired by deep residual networks that allows the efficient computation of this more expressive pairwise similarity objective. Further, we argue that regularization is key in learning with small amounts of data, and propose an additional generator network based on the Generative Adversarial Networks where the discriminator is our residual pairwise network. This provides a strong regularizer by leveraging the generated data samples. The proposed model can generate plausible variations of exemplars over unseen classes and outperforms strong discriminative baselines for few shot classification tasks. Notably, our residual pairwise network design outperforms previous state-of-theart on the challenging mini-Imagenet dataset for one shot learning by getting over 55% accuracy for the 5-way classification task over unseen classes.",deep neural network achiev unpreced perform level mani task scale well larg quantiti data perform low data regim task like one shot learn still lag behind recent work suggest mani hypothes better optim complic network structur work hypothes learnabl express similar object essenti miss compon toward overcom propos network design inspir deep residu network allow effici comput express pairwis similar object argu regular key learn small amount data propos addit generat network base generat adversari network discrimin residu pairwis network provid strong regular leverag generat data sampl propos model generat plausibl variat exemplar unseen class outperform strong discrimin baselin shot classif task notabl residu pairwis network design outperform previous state theart challeng mini imagenet dataset one shot learn get accuraci way classif task unseen class,"['Akshay Mehrotra', 'Ambedkar Dukkipati']","['cs.CV', 'cs.NE']",False,False,False,True,False,False
464,2017-03-28T14:09:09Z,2017-03-23T11:02:42Z,http://arxiv.org/abs/1703.08001v1,http://arxiv.org/pdf/1703.08001v1,Nonlinear Spectral Image Fusion,nonlinear spectral imag fusion,"In this paper we demonstrate that the framework of nonlinear spectral decompositions based on total variation (TV) regularization is very well suited for image fusion as well as more general image manipulation tasks. The well-localized and edge-preserving spectral TV decomposition allows to select frequencies of a certain image to transfer particular features, such as wrinkles in a face, from one image to another. We illustrate the effectiveness of the proposed approach in several numerical experiments, including a comparison to the competing techniques of Poisson image editing, linear osmosis, wavelet fusion and Laplacian pyramid fusion. We conclude that the proposed spectral TV image decomposition framework is a valuable tool for semi- and fully-automatic image editing and fusion.",paper demonstr framework nonlinear spectral decomposit base total variat tv regular veri well suit imag fusion well general imag manipul task well local edg preserv spectral tv decomposit allow select frequenc certain imag transfer particular featur wrinkl face one imag anoth illustr effect propos approach sever numer experi includ comparison compet techniqu poisson imag edit linear osmosi wavelet fusion laplacian pyramid fusion conclud propos spectral tv imag decomposit framework valuabl tool semi fulli automat imag edit fusion,"['Martin Benning', 'Michael Möller', 'Raz Z. Nossek', 'Martin Burger', 'Daniel Cremers', 'Guy Gilboa', 'Carola-Bibiane Schönlieb']","['cs.CV', 'math.NA', '35P30, 62H35, 65M70, 94A08', 'G.1.3; G.1.6; G.1.8; I.4.0; I.4.5']",False,False,False,True,False,False
465,2017-03-28T14:09:09Z,2017-03-23T11:01:27Z,http://arxiv.org/abs/1703.08000v1,http://arxiv.org/pdf/1703.08000v1,Weakly Supervised Object Localization Using Things and Stuff Transfer,weak supervis object local use thing stuff transfer,"We propose to help weakly supervised object localization for classes where location annotations are not available, by transferring things and stuff knowledge from a source set with available annotations. The source and target classes might share similar appearance (e.g. bear fur is similar to cat fur) or appear against similar background (e.g. horse and sheep appear against grass). To exploit this, we acquire three types of knowledge from the source set: a segmentation model trained on both thing and stuff classes; similarity relations between target and source classes; and co-occurrence relations between thing and stuff classes in the source. The segmentation model is used to generate thing and stuff segmentation maps on a target image, while the class similarity and co-occurrence knowledge help refining them. We then incorporate these maps as new cues into a multiple instance learning framework (MIL), propagating the transferred knowledge from the pixel level to the object proposal level. In extensive experiments, we conduct our transfer from the PASCAL Context dataset (source) to the ILSVRC, COCO and PASCAL VOC 2007 datasets (targets). We evaluate our transfer across widely different thing classes, including some that are not similar in appearance, but appear against similar background. The results demonstrate significant improvement over standard MIL, and we outperform the state-of-the-art in the transfer setting.",propos help weak supervis object local class locat annot avail transfer thing stuff knowledg sourc set avail annot sourc target class might share similar appear bear fur similar cat fur appear similar background hors sheep appear grass exploit acquir three type knowledg sourc set segment model train thing stuff class similar relat target sourc class co occurr relat thing stuff class sourc segment model use generat thing stuff segment map target imag class similar co occurr knowledg help refin incorpor map new cue multipl instanc learn framework mil propag transfer knowledg pixel level object propos level extens experi conduct transfer pascal context dataset sourc ilsvrc coco pascal voc dataset target evalu transfer across wide differ thing class includ similar appear appear similar background result demonstr signific improv standard mil outperform state art transfer set,"['Miaojing Shi', 'Holger Caesar', 'Vittorio Ferrari']",['cs.CV'],False,False,False,True,False,False
466,2017-03-28T14:09:09Z,2017-03-23T09:49:37Z,http://arxiv.org/abs/1703.07980v1,http://arxiv.org/pdf/1703.07980v1,Discriminatively Boosted Image Clustering with Fully Convolutional   Auto-Encoders,discrimin boost imag cluster fulli convolut auto encod,"Traditional image clustering methods take a two-step approach, feature learning and clustering, sequentially. However, recent research results demonstrated that combining the separated phases in a unified framework and training them jointly can achieve a better performance. In this paper, we first introduce fully convolutional auto-encoders for image feature learning and then propose a unified clustering framework to learn image representations and cluster centers jointly based on a fully convolutional auto-encoder and soft $k$-means scores. At initial stages of the learning procedure, the representations extracted from the auto-encoder may not be very discriminative for latter clustering. We address this issue by adopting a boosted discriminative distribution, where high score assignments are highlighted and low score ones are de-emphasized. With the gradually boosted discrimination, clustering assignment scores are discriminated and cluster purities are enlarged. Experiments on several vision benchmark datasets show that our methods can achieve a state-of-the-art performance.",tradit imag cluster method take two step approach featur learn cluster sequenti howev recent research result demonstr combin separ phase unifi framework train joint achiev better perform paper first introduc fulli convolut auto encod imag featur learn propos unifi cluster framework learn imag represent cluster center joint base fulli convolut auto encod soft mean score initi stage learn procedur represent extract auto encod may veri discrimin latter cluster address issu adopt boost discrimin distribut high score assign highlight low score one de emphas gradual boost discrimin cluster assign score discrimin cluster puriti enlarg experi sever vision benchmark dataset show method achiev state art perform,"['Fengfu Li', 'Hong Qiao', 'Bo Zhang', 'Xuanyang Xi']","['cs.CV', 'cs.LG']",False,False,False,True,False,False
467,2017-03-28T14:09:09Z,2017-03-23T09:06:13Z,http://arxiv.org/abs/1703.07971v1,http://arxiv.org/pdf/1703.07971v1,Image-based Localization using Hourglass Networks,imag base local use hourglass network,"In this paper, we propose an encoder-decoder convolutional neural network (CNN) architecture for estimating camera pose (orientation and location) from a single RGB-image. The architecture has a hourglass shape consisting of a chain of convolution and up-convolution layers followed by a regression part. The up-convolution layers are introduced to preserve the fine-grained information of the input image. Following the common practice, we train our model in end-to-end manner utilizing transfer learning from large scale classification data. The experiments demonstrate the performance of the approach on data exhibiting different lighting conditions, reflections, and motion blur. The results indicate a clear improvement over the previous state-of-the-art even when compared to methods that utilize sequence of test frames instead of a single frame.",paper propos encod decod convolut neural network cnn architectur estim camera pose orient locat singl rgb imag architectur hourglass shape consist chain convolut convolut layer follow regress part convolut layer introduc preserv fine grain inform input imag follow common practic train model end end manner util transfer learn larg scale classif data experi demonstr perform approach data exhibit differ light condit reflect motion blur result indic clear improv previous state art even compar method util sequenc test frame instead singl frame,"['Iaroslav Melekhov', 'Juha Ylioinas', 'Juho Kannala', 'Esa Rahtu']",['cs.CV'],True,False,False,True,False,False
469,2017-03-28T14:09:09Z,2017-03-23T05:22:22Z,http://arxiv.org/abs/1703.07939v1,http://arxiv.org/pdf/1703.07939v1,Recurrent Multimodal Interaction for Referring Image Segmentation,recurr multimod interact refer imag segment,"In this paper we are interested in the problem of image segmentation given natural language descriptions, i.e. referring expressions. Existing works tackle this problem by first modeling images and sentences independently and then segment images by combining these two types of representations. We argue that learning word-to-image interaction is more native in the sense of jointly modeling two modalities for the image segmentation task, and we propose convolutional multimodal LSTM to encode the sequential interactions between individual words, visual information, and spatial information. We show that our proposed model outperforms the baseline model on benchmark datasets. In addition, we analyze the intermediate output of the proposed multimodal LSTM approach and empirically explains how this approach enforces a more effective word-to-image interaction.",paper interest problem imag segment given natur languag descript refer express exist work tackl problem first model imag sentenc independ segment imag combin two type represent argu learn word imag interact nativ sens joint model two modal imag segment task propos convolut multimod lstm encod sequenti interact individu word visual inform spatial inform show propos model outperform baselin model benchmark dataset addit analyz intermedi output propos multimod lstm approach empir explain approach enforc effect word imag interact,"['Chenxi Liu', 'Zhe Lin', 'Xiaohui Shen', 'Jimei Yang', 'Xin Lu', 'Alan Yuille']",['cs.CV'],False,False,False,True,False,False
473,2017-03-28T14:09:13Z,2017-03-23T02:50:32Z,http://arxiv.org/abs/1703.07910v1,http://arxiv.org/pdf/1703.07910v1,Bidirectional-Convolutional LSTM Based Spectral-Spatial Feature Learning   for Hyperspectral Image Classification,bidirect convolut lstm base spectral spatial featur learn hyperspectr imag classif,"This paper proposes a novel deep learning framework named bidirectional-convolutional long short term memory (Bi-CLSTM) network to automatically learn the spectral-spatial feature from hyperspectral images (HSIs). In the network, the issue of spectral feature extraction is considered as a sequence learning problem, and a recurrent connection operator across the spectral domain is used to address it. Meanwhile, inspired from the widely used convolutional neural network (CNN), a convolution operator across the spatial domain is incorporated into the network to extract the spatial feature. Besides, to sufficiently capture the spectral information, a bidirectional recurrent connection is proposed. In the classification phase, the learned features are concatenated into a vector and fed to a softmax classifier via a fully-connected operator. To validate the effectiveness of the proposed Bi-CLSTM framework, we compare it with several state-of-the-art methods, including the CNN framework, on three widely used HSIs. The obtained results show that Bi-CLSTM can improve the classification performance as compared to other methods.",paper propos novel deep learn framework name bidirect convolut long short term memori bi clstm network automat learn spectral spatial featur hyperspectr imag hsis network issu spectral featur extract consid sequenc learn problem recurr connect oper across spectral domain use address meanwhil inspir wide use convolut neural network cnn convolut oper across spatial domain incorpor network extract spatial featur besid suffici captur spectral inform bidirect recurr connect propos classif phase learn featur concaten vector fed softmax classifi via fulli connect oper valid effect propos bi clstm framework compar sever state art method includ cnn framework three wide use hsis obtain result show bi clstm improv classif perform compar method,"['Qingshan Liu', 'Feng Zhou', 'Renlong Hang', 'Xiaotong Yuan']",['cs.CV'],False,False,False,True,False,False
474,2017-03-28T14:09:13Z,2017-03-23T01:51:14Z,http://arxiv.org/abs/1703.08173v1,http://arxiv.org/pdf/1703.08173v1,Single Image Super-resolution with a Parameter Economic Residual-like   Convolutional Neural Network,singl imag super resolut paramet econom residu like convolut neural network,"Recent years have witnessed great success of convolutional neural network (CNN) for various problems both in low and high level visions. Especially noteworthy is the residual network which was originally proposed to handle high-level vision problems and enjoys several merits. This paper aims to extend the merits of residual network, such as skip connection induced fast training, for a typical low-level vision problem, i.e., single image super-resolution. In general, the two main challenges of existing deep CNN for supper-resolution lie in the gradient exploding/vanishing problem and large amount of parameters or computational cost as CNN goes deeper. Correspondingly, the skip connections or identity mapping shortcuts are utilized to avoid gradient exploding/vanishing problem. To tackle with the second problem, a parameter economic CNN architecture which has carefully designed width, depth and skip connections was proposed. Different residual-like architectures for image superresolution has also been compared. Experimental results have demonstrated that the proposed CNN model can not only achieve state-of-the-art PSNR and SSIM results for single image super-resolution but also produce visually pleasant results. This paper has extended the mmm 2017 paper with more experiments and explanations.",recent year wit great success convolut neural network cnn various problem low high level vision especi noteworthi residu network origin propos handl high level vision problem enjoy sever merit paper aim extend merit residu network skip connect induc fast train typic low level vision problem singl imag super resolut general two main challeng exist deep cnn supper resolut lie gradient explod vanish problem larg amount paramet comput cost cnn goe deeper correspond skip connect ident map shortcut util avoid gradient explod vanish problem tackl second problem paramet econom cnn architectur care design width depth skip connect propos differ residu like architectur imag superresolut also compar experiment result demonstr propos cnn model onli achiev state art psnr ssim result singl imag super resolut also produc visual pleasant result paper extend mmm paper experi explan,"['Yudong Liang', 'Ze Yang', 'Kai Zhang', 'Yihui He', 'Jinjun Wang', 'Nanning Zheng']",['cs.CV'],False,False,False,True,False,False
476,2017-03-28T14:09:13Z,2017-03-22T20:00:15Z,http://arxiv.org/abs/1703.07834v1,http://arxiv.org/pdf/1703.07834v1,Large Pose 3D Face Reconstruction from a Single Image via Direct   Volumetric CNN Regression,larg pose face reconstruct singl imag via direct volumetr cnn regress,"3D face reconstruction is a fundamental Computer Vision problem of extraordinary difficulty. Current systems often assume the availability of multiple facial images (sometimes from the same subject) as input, and must address a number of methodological challenges such as establishing dense correspondences across large facial poses, expressions, and non-uniform illumination. In general these methods require complex and inefficient pipelines for model building and fitting. In this work, we propose to address many of these limitations by training a Convolutional Neural Network (CNN) on an appropriate dataset consisting of 2D images and 3D facial models or scans. Our CNN works with just a single 2D facial image, does not require accurate alignment nor establishes dense correspondence between images, works for arbitrary facial poses and expressions, and can be used to reconstruct the whole 3D facial geometry (including the non-visible parts of the face) bypassing the construction (during training) and fitting (during testing) of a 3D Morphable Model. We achieve this via a simple CNN architecture that performs direct regression of a volumetric representation of the 3D facial geometry from a single 2D image. We also demonstrate how the related task of facial landmark localization can be incorporated into the proposed framework and help improve reconstruction quality, especially for the cases of large poses and facial expressions. Testing code will be made available online, along with pre-trained models http://aaronsplace.co.uk/papers/jackson2017recon.",face reconstruct fundament comput vision problem extraordinari difficulti current system often assum avail multipl facial imag sometim subject input must address number methodolog challeng establish dens correspond across larg facial pose express non uniform illumin general method requir complex ineffici pipelin model build fit work propos address mani limit train convolut neural network cnn appropri dataset consist imag facial model scan cnn work singl facial imag doe requir accur align establish dens correspond imag work arbitrari facial pose express use reconstruct whole facial geometri includ non visibl part face bypass construct dure train fit dure test morphabl model achiev via simpl cnn architectur perform direct regress volumetr represent facial geometri singl imag also demonstr relat task facial landmark local incorpor propos framework help improv reconstruct qualiti especi case larg pose facial express test code made avail onlin along pre train model http aaronsplac co uk paper jacksonrecon,"['Aaron S. Jackson', 'Adrian Bulat', 'Vasileios Argyriou', 'Georgios Tzimiropoulos']",['cs.CV'],False,False,False,True,False,False
477,2017-03-28T14:09:13Z,2017-03-22T18:51:51Z,http://arxiv.org/abs/1703.07815v1,http://arxiv.org/pdf/1703.07815v1,Cross-View Image Matching for Geo-localization in Urban Environments,cross view imag match geo local urban environ,"In this paper, we address the problem of cross-view image geo-localization. Specifically, we aim to estimate the GPS location of a query street view image by finding the matching images in a reference database of geo-tagged bird's eye view images, or vice versa. To this end, we present a new framework for cross-view image geo-localization by taking advantage of the tremendous success of deep convolutional neural networks (CNNs) in image classification and object detection. First, we employ the Faster R-CNN to detect buildings in the query and reference images. Next, for each building in the query image, we retrieve the $k$ nearest neighbors from the reference buildings using a Siamese network trained on both positive matching image pairs and negative pairs. To find the correct NN for each query building, we develop an efficient multiple nearest neighbors matching method based on dominant sets. We evaluate the proposed framework on a new dataset that consists of pairs of street view and bird's eye view images. Experimental results show that the proposed method achieves better geo-localization accuracy than other approaches and is able to generalize to images at unseen locations.",paper address problem cross view imag geo local specif aim estim gps locat queri street view imag find match imag refer databas geo tag bird eye view imag vice versa end present new framework cross view imag geo local take advantag tremend success deep convolut neural network cnns imag classif object detect first employ faster cnn detect build queri refer imag next build queri imag retriev nearest neighbor refer build use siames network train posit match imag pair negat pair find correct nn queri build develop effici multipl nearest neighbor match method base domin set evalu propos framework new dataset consist pair street view bird eye view imag experiment result show propos method achiev better geo local accuraci approach abl general imag unseen locat,"['Yicong Tian', 'Chen Chen', 'Mubarak Shah']",['cs.CV'],False,False,False,True,False,False
486,2017-03-28T14:09:17Z,2017-03-22T09:03:25Z,http://arxiv.org/abs/1703.07570v1,http://arxiv.org/pdf/1703.07570v1,Deep MANTA: A Coarse-to-fine Many-Task Network for joint 2D and 3D   vehicle analysis from monocular image,deep manta coars fine mani task network joint vehicl analysi monocular imag,"In this paper, we present a novel approach, called Deep MANTA (Deep Many-Tasks), for many-task vehicle analysis from a given image. A robust convolutional network is introduced for simultaneous vehicle detection, part localization, visibility characterization and 3D dimension estimation. Its architecture is based on a new coarse-to-fine object proposal that boosts the vehicle detection. Moreover, the Deep MANTA network is able to localize vehicle parts even if these parts are not visible. In the inference, the network's outputs are used by a real time robust pose estimation algorithm for fine orientation estimation and 3D vehicle localization. We show in experiments that our method outperforms monocular state-of-the-art approaches on vehicle detection, orientation and 3D location tasks on the very challenging KITTI benchmark.",paper present novel approach call deep manta deep mani task mani task vehicl analysi given imag robust convolut network introduc simultan vehicl detect part local visibl character dimens estim architectur base new coars fine object propos boost vehicl detect moreov deep manta network abl local vehicl part even part visibl infer network output use real time robust pose estim algorithm fine orient estim vehicl local show experi method outperform monocular state art approach vehicl detect orient locat task veri challeng kitti benchmark,"['Florian Chabot', 'Mohamed Chaouch', 'Jaonary Rabarisoa', 'Céline Teulière', 'Thierry Chateau']",['cs.CV'],False,False,False,True,False,False
487,2017-03-28T14:09:17Z,2017-03-23T13:56:01Z,http://arxiv.org/abs/1703.07523v2,http://arxiv.org/pdf/1703.07523v2,Deeply-Supervised CNN for Prostate Segmentation,deepli supervis cnn prostat segment,"Prostate segmentation from Magnetic Resonance (MR) images plays an important role in image guided interven- tion. However, the lack of clear boundary specifically at the apex and base, and huge variation of shape and texture between the images from different patients make the task very challenging. To overcome these problems, in this paper, we propose a deeply supervised convolutional neural network (CNN) utilizing the convolutional information to accurately segment the prostate from MR images. The proposed model can effectively detect the prostate region with additional deeply supervised layers compared with other approaches. Since some information will be abandoned after convolution, it is necessary to pass the features extracted from early stages to later stages. The experimental results show that significant segmentation accuracy improvement has been achieved by our proposed method compared to other reported approaches.",prostat segment magnet reson mr imag play import role imag guid interven tion howev lack clear boundari specif apex base huge variat shape textur imag differ patient make task veri challeng overcom problem paper propos deepli supervis convolut neural network cnn util convolut inform accur segment prostat mr imag propos model effect detect prostat region addit deepli supervis layer compar approach sinc inform abandon convolut necessari pass featur extract earli stage later stage experiment result show signific segment accuraci improv achiev propos method compar report approach,"['Qikui Zhu', 'Bo Du', 'Baris Turkbey', 'Peter L . Choyke', 'Pingkun Yan']",['cs.CV'],True,False,False,True,False,False
488,2017-03-28T14:09:17Z,2017-03-22T04:40:51Z,http://arxiv.org/abs/1703.07519v1,http://arxiv.org/pdf/1703.07519v1,Joint Intermodal and Intramodal Label Transfers for Extremely Rare or   Unseen Classes,joint intermod intramod label transfer extrem rare unseen class,"In this paper, we present a label transfer model from texts to images for image classification tasks. The problem of image classification is often much more challenging than text classification. On one hand, labeled text data is more widely available than the labeled images for classification tasks. On the other hand, text data tends to have natural semantic interpretability, and they are often more directly related to class labels. On the contrary, the image features are not directly related to concepts inherent in class labels. One of our goals in this paper is to develop a model for revealing the functional relationships between text and image features as to directly transfer intermodal and intramodal labels to annotate the images. This is implemented by learning a transfer function as a bridge to propagate the labels between two multimodal spaces. However, the intermodal label transfers could be undermined by blindly transferring the labels of noisy texts to annotate images. To mitigate this problem, we present an intramodal label transfer process, which complements the intermodal label transfer by transferring the image labels instead when relevant text is absent from the source corpus. In addition, we generalize the inter-modal label transfer to zero-shot learning scenario where there are only text examples available to label unseen classes of images without any positive image examples. We evaluate our algorithm on an image classification task and show the effectiveness with respect to the other compared algorithms.",paper present label transfer model text imag imag classif task problem imag classif often much challeng text classif one hand label text data wide avail label imag classif task hand text data tend natur semant interpret often direct relat class label contrari imag featur direct relat concept inher class label one goal paper develop model reveal function relationship text imag featur direct transfer intermod intramod label annot imag implement learn transfer function bridg propag label two multimod space howev intermod label transfer could undermin blind transfer label noisi text annot imag mitig problem present intramod label transfer process complement intermod label transfer transfer imag label instead relev text absent sourc corpus addit general inter modal label transfer zero shot learn scenario onli text exampl avail label unseen class imag without ani posit imag exampl evalu algorithm imag classif task show effect respect compar algorithm,"['Guo-Jun Qi', 'Wei Liu', 'Charu Aggarwal', 'Thomas Huang']",['cs.CV'],False,False,False,True,False,False
492,2017-03-28T14:09:21Z,2017-03-23T21:37:40Z,http://arxiv.org/abs/1703.07478v2,http://arxiv.org/pdf/1703.07478v2,Spatially-Varying Blur Detection Based on Multiscale Fused and Sorted   Transform Coefficients of Gradient Magnitudes,spatial vari blur detect base multiscal fuse sort transform coeffici gradient magnitud,"The detection of spatially-varying blur without having any information about the blur type is a challenging task. In this paper, we propose a novel effective approach to address the blur detection problem from a single image without requiring any knowledge about the blur type, level, or camera settings. Our approach computes blur detection maps based on a novel High-frequency multiscale Fusion and Sort Transform (HiFST) of gradient magnitudes. The evaluations of the proposed approach on a diverse set of blurry images with different blur types, levels, and contents demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods qualitatively and quantitatively.",detect spatial vari blur without ani inform blur type challeng task paper propos novel effect approach address blur detect problem singl imag without requir ani knowledg blur type level camera set approach comput blur detect map base novel high frequenc multiscal fusion sort transform hifst gradient magnitud evalu propos approach divers set blurri imag differ blur type level content demonstr propos algorithm perform favor state art method qualit quantit,"['S. Alireza Golestaneh', 'Lina J. Karam']",['cs.CV'],False,False,False,True,False,False
498,2017-03-28T14:09:21Z,2017-03-21T17:41:46Z,http://arxiv.org/abs/1703.07334v1,http://arxiv.org/pdf/1703.07334v1,Pop-up SLAM: Semantic Monocular Plane SLAM for Low-texture Environments,pop slam semant monocular plane slam low textur environ,"Existing simultaneous localization and mapping (SLAM) algorithms are not robust in challenging low-texture environments because there are only few salient features. The resulting sparse or semi-dense map also conveys little information for motion planning. Though some work utilize plane or scene layout for dense map regularization, they require decent state estimation from other sources. In this paper, we propose real-time monocular plane SLAM to demonstrate that scene understanding could improve both state estimation and dense mapping especially in low-texture environments. The plane measurements come from a pop-up 3D plane model applied to each single image. We also combine planes with point based SLAM to improve robustness. On a public TUM dataset, our algorithm generates a dense semantic 3D model with pixel depth error of 6.2 cm while existing SLAM algorithms fail. On a 60 m long dataset with loops, our method creates a much better 3D model with state estimation error of 0.67%.",exist simultan local map slam algorithm robust challeng low textur environ becaus onli salient featur result spars semi dens map also convey littl inform motion plan though work util plane scene layout dens map regular requir decent state estim sourc paper propos real time monocular plane slam demonstr scene understand could improv state estim dens map especi low textur environ plane measur come pop plane model appli singl imag also combin plane point base slam improv robust public tum dataset algorithm generat dens semant model pixel depth error cm exist slam algorithm fail long dataset loop method creat much better model state estim error,"['Shichao Yang', 'Yu Song', 'Michael Kaess', 'Sebastian Scherer']","['cs.CV', 'cs.RO']",False,False,False,True,False,False
499,2017-03-28T14:09:21Z,2017-03-21T17:37:36Z,http://arxiv.org/abs/1703.07332v1,http://arxiv.org/pdf/1703.07332v1,"How far are we from solving the 2D & 3D Face Alignment problem? (and a   dataset of 230,000 3D facial landmarks)",far solv face align problem dataset facial landmark,"This paper investigates how far a very deep neural network is from attaining close to saturating performance on existing 2D and 3D face alignment datasets. To this end, we make the following three contributions: (a) we construct, for the first time, a very strong baseline by combining a state-of-the-art architecture for landmark localization with a state-of-the-art residual block, train it on a very large yet synthetically expanded 2D facial landmark dataset and finally evaluate it on all other 2D facial landmark datasets. (b) We create a guided by 2D landmarks network which converts 2D landmark annotations to 3D and unifies all existing datasets, leading to the creation of LS3D-W, the largest and most challenging 3D facial landmark dataset to date (~230,000 images). (c) Following that, we train a neural network for 3D face alignment and evaluate it on the newly introduced LS3D-W. (d) We further look into the effect of all ""traditional"" factors affecting face alignment performance like large pose, initialization and resolution, and introduce a ""new"" one, namely the size of the network. (e) We show that both 2D and 3D face alignment networks achieve performance of remarkable accuracy which is probably close to saturating the datasets used. Demo code and pre-trained models can be downloaded from http://www.cs.nott.ac.uk/~psxab5/face-alignment/",paper investig far veri deep neural network attain close satur perform exist face align dataset end make follow three contribut construct first time veri strong baselin combin state art architectur landmark local state art residu block train veri larg yet synthet expand facial landmark dataset final evalu facial landmark dataset creat guid landmark network convert landmark annot unifi exist dataset lead creation lsd largest challeng facial landmark dataset date imag follow train neural network face align evalu newli introduc lsd look effect tradit factor affect face align perform like larg pose initi resolut introduc new one name size network show face align network achiev perform remark accuraci probabl close satur dataset use demo code pre train model download http www cs nott ac uk psxab face align,"['Adrian Bulat', 'Georgios Tzimiropoulos']",['cs.CV'],False,False,False,True,False,False
868,2017-03-28T14:11:17Z,2016-12-15T14:16:56Z,http://arxiv.org/abs/1612.05076v1,http://arxiv.org/pdf/1612.05076v1,Live Score Following on Sheet Music Images,live score follow sheet music imag,"In this demo we show a novel approach to score following. Instead of relying on some symbolic representation, we are using a multi-modal convolutional neural network to match the incoming audio stream directly to sheet music images. This approach is in an early stage and should be seen as proof of concept. Nonetheless, the audience will have the opportunity to test our implementation themselves via 3 simple piano pieces.",demo show novel approach score follow instead reli symbol represent use multi modal convolut neural network match incom audio stream direct sheet music imag approach earli stage seen proof concept nonetheless audienc opportun test implement themselv via simpl piano piec,"['Matthias Dorfer', 'Andreas Arzt', 'Sebastian Böck', 'Amaury Durand', 'Gerhard Widmer']",['cs.SD'],False,False,False,True,False,False
920,2017-03-28T14:02:35Z,2017-03-20T19:45:39Z,http://arxiv.org/abs/1703.06946v1,http://arxiv.org/pdf/1703.06946v1,SCALPEL: Extracting Neurons from Calcium Imaging Data,scalpel extract neuron calcium imag data,"In the past few years, new technologies in the field of neuroscience have made it possible to simultaneously image activity in large populations of neurons at cellular resolution in behaving animals. In mid-2016, a huge repository of this so-called ""calcium imaging"" data was made publicly-available. The availability of this large-scale data resource opens the door to a host of scientific questions, for which new statistical methods must be developed.   In this paper, we consider the first step in the analysis of calcium imaging data: namely, identifying the neurons in a calcium imaging video. We propose a dictionary learning approach for this task. First, we perform image segmentation to develop a dictionary containing a huge number of candidate neurons. Next, we refine the dictionary using clustering. Finally, we apply the dictionary in order to select neurons and estimate their corresponding activity over time, using a sparse group lasso optimization problem. We apply our proposal to three calcium imaging data sets.   Our proposed approach is implemented in the R package scalpel, which is available on CRAN.",past year new technolog field neurosci made possibl simultan imag activ larg popul neuron cellular resolut behav anim mid huge repositori call calcium imag data made public avail avail larg scale data resourc open door host scientif question new statist method must develop paper consid first step analysi calcium imag data name identifi neuron calcium imag video propos dictionari learn approach task first perform imag segment develop dictionari contain huge number candid neuron next refin dictionari use cluster final appli dictionari order select neuron estim correspond activ time use spars group lasso optim problem appli propos three calcium imag data set propos approach implement packag scalpel avail cran,"['Ashley Petersen', 'Noah Simon', 'Daniela Witten']","['stat.AP', 'q-bio.NC']",False,False,False,True,False,False
937,2017-03-28T14:02:39Z,2017-03-15T17:03:56Z,http://arxiv.org/abs/1703.05264v1,http://arxiv.org/pdf/1703.05264v1,Smooth Image-on-Scalar Regression for Brain Mapping,smooth imag scalar regress brain map,"Brain mapping is an increasingly important tool in neurology and psychiatry researches for the realization of data-driven personalized medicine in the big data era, which learns the statistical links between brain images and subject level features. Taking images as responses, the task raises a lot of challenges due to the high dimensionality of the image with relatively small number of samples, as well as the noisiness of measurements in medical images.   In this paper we propose a novel method {\it Smooth Image-on-scalar Regression} (SIR) for recovering the true association between an image outcome and scalar predictors. The estimator is achieved by minimizing a mean squared error with a total variation (TV) regularization term on the predicted mean image across all subjects. It denoises the images from all subjects and at the same time returns the coefficient maps estimation. We propose an algorithm to solve this optimization problem, which is efficient when combined with recent advances in graph fused lasso solvers. The statistical consistency of the estimator is shown via an oracle inequality.   Simulation results demonstrate that the proposed method outperforms existing methods with separate denoising and regression steps. Especially, SIR shows an evident advantage in recovering signals in small regions. We apply SIR on Alzheimer's Disease Neuroimaging Initiative data and produce interpretable brain maps of the PET image to patient-level features include age, gender, genotype and disease groups.",brain map increas import tool neurolog psychiatri research realize data driven person medicin big data era learn statist link brain imag subject level featur take imag respons task rais lot challeng due high dimension imag relat small number sampl well noisi measur medic imag paper propos novel method smooth imag scalar regress sir recov true associ imag outcom scalar predictor estim achiev minim mean squar error total variat tv regular term predict mean imag across subject denois imag subject time return coeffici map estim propos algorithm solv optim problem effici combin recent advanc graph fuse lasso solver statist consist estim shown via oracl inequ simul result demonstr propos method outperform exist method separ denois regress step especi sir show evid advantag recov signal small region appli sir alzheim diseas neuroimag initi data produc interpret brain map pet imag patient level featur includ age gender genotyp diseas group,"['Ying Liu', 'Bowei Yan']","['stat.ME', 'stat.AP']",False,False,False,True,False,False
949,2017-03-28T14:02:44Z,2017-03-10T22:46:09Z,http://arxiv.org/abs/1703.03862v1,http://arxiv.org/pdf/1703.03862v1,Joint Embedding of Graphs,joint embed graph,"Feature extraction and dimension reduction for networks is critical in a wide variety of domains. Efficiently and accurately learning features for multiple graphs has important applications in statistical inference on graphs. We propose a method to jointly embed multiple undirected graphs. Given a set of graphs, the joint embedding method identifies a linear subspace spanned by rank one symmetric matrices and projects adjacency matrices of graphs into this subspace. The projection coefficients can be treated as features of the graphs. We also propose a random graph model which generalizes classical random graph model and can be used to model multiple graphs. We show through theory and numerical experiments that under the model, the joint embedding method produces estimates of parameters with small errors. Via simulation experiments, we demonstrate that the joint embedding method produces features which lead to state of the art performance in classifying graphs. Applying the joint embedding method to human brain graphs, we find it extract interpretable features that can be used to predict individual composite creativity index.",featur extract dimens reduct network critic wide varieti domain effici accur learn featur multipl graph import applic statist infer graph propos method joint emb multipl undirect graph given set graph joint embed method identifi linear subspac span rank one symmetr matric project adjac matric graph subspac project coeffici treat featur graph also propos random graph model general classic random graph model use model multipl graph show theori numer experi model joint embed method produc estim paramet small error via simul experi demonstr joint embed method produc featur lead state art perform classifi graph appli joint embed method human brain graph find extract interpret featur use predict individu composit creativ index,"['Shangsi Wang', 'Joshua T. Vogelstein', 'Carey E. Priebe']","['stat.AP', 'cs.LG', 'stat.ML']",False,False,False,True,False,False
998,2017-03-28T14:03:04Z,2017-02-19T10:08:16Z,http://arxiv.org/abs/1702.05732v1,http://arxiv.org/pdf/1702.05732v1,Low-dose cryo electron ptychography via non-convex Bayesian optimization,low dose cryo electron ptychographi via non convex bayesian optim,"Electron ptychography has seen a recent surge of interest for phase sensitive imaging at atomic or near-atomic resolution. However, applications are so far mainly limited to radiation-hard samples because the required doses are too high for imaging biological samples at high resolution. We propose the use of non-convex, Bayesian optimization to overcome this problem and reduce the dose required for successful reconstruction by two orders of magnitude compared to previous experiments. We suggest to use this method for imaging single biological macromolecules at cryogenic temperatures and demonstrate 2D single-particle reconstructions from simulated data with a resolution of 7.9 \AA$\,$ at a dose of 20 $e^- / \AA^2$. When averaging over only 15 low-dose datasets, a resolution of 4 \AA$\,$ is possible for large macromolecular complexes. With its independence from microscope transfer function, direct recovery of phase contrast and better scaling of signal-to-noise ratio, cryo-electron ptychography may become a promising alternative to Zernike phase-contrast microscopy.",electron ptychographi seen recent surg interest phase sensit imag atom near atom resolut howev applic far main limit radiat hard sampl becaus requir dose high imag biolog sampl high resolut propos use non convex bayesian optim overcom problem reduc dose requir success reconstruct two order magnitud compar previous experi suggest use method imag singl biolog macromolecul cryogen temperatur demonstr singl particl reconstruct simul data resolut aa dose aa averag onli low dose dataset resolut aa possibl larg macromolecular complex independ microscop transfer function direct recoveri phase contrast better scale signal nois ratio cryo electron ptychographi may becom promis altern zernik phase contrast microscopi,"['Philipp Michael Pelz', 'Wen Xuan Qiu', 'Robert Bücker', 'Günther Kassier', 'R. J. Dwayne Miller']","['physics.comp-ph', 'math.OC', 'physics.data-an', 'stat.AP']",False,False,False,True,False,False
1073,2017-03-28T14:03:38Z,2017-02-17T18:06:27Z,http://arxiv.org/abs/1702.05462v1,http://arxiv.org/pdf/1702.05462v1,Objective Bayesian Analysis for Change Point Problems,object bayesian analysi chang point problem,"In this paper we present an objective approach to change point analysis. In particular, we look at the problem from two perspectives. The first focuses on the definition of an objective prior when the number of change points is known a priori. The second contribution aims to estimate the number of change points by using an objective approach, recently introduced in the literature, based on losses. The latter considers change point estimation as a model selection exercise. We show the performance of the proposed approach on simulated data and on real data sets.",paper present object approach chang point analysi particular look problem two perspect first focus definit object prior number chang point known priori second contribut aim estim number chang point use object approach recent introduc literatur base loss latter consid chang point estim model select exercis show perform propos approach simul data real data set,"['Laurentiu Hinoveanu', 'Fabrizio Leisen', 'Cristiano Villa']","['stat.ME', 'math.ST', 'stat.AP', 'stat.CO', 'stat.ML', 'stat.TH']",False,False,False,True,False,False
1150,2017-03-28T14:04:12Z,2017-03-15T17:03:56Z,http://arxiv.org/abs/1703.05264v1,http://arxiv.org/pdf/1703.05264v1,Smooth Image-on-Scalar Regression for Brain Mapping,smooth imag scalar regress brain map,"Brain mapping is an increasingly important tool in neurology and psychiatry researches for the realization of data-driven personalized medicine in the big data era, which learns the statistical links between brain images and subject level features. Taking images as responses, the task raises a lot of challenges due to the high dimensionality of the image with relatively small number of samples, as well as the noisiness of measurements in medical images.   In this paper we propose a novel method {\it Smooth Image-on-scalar Regression} (SIR) for recovering the true association between an image outcome and scalar predictors. The estimator is achieved by minimizing a mean squared error with a total variation (TV) regularization term on the predicted mean image across all subjects. It denoises the images from all subjects and at the same time returns the coefficient maps estimation. We propose an algorithm to solve this optimization problem, which is efficient when combined with recent advances in graph fused lasso solvers. The statistical consistency of the estimator is shown via an oracle inequality.   Simulation results demonstrate that the proposed method outperforms existing methods with separate denoising and regression steps. Especially, SIR shows an evident advantage in recovering signals in small regions. We apply SIR on Alzheimer's Disease Neuroimaging Initiative data and produce interpretable brain maps of the PET image to patient-level features include age, gender, genotype and disease groups.",brain map increas import tool neurolog psychiatri research realize data driven person medicin big data era learn statist link brain imag subject level featur take imag respons task rais lot challeng due high dimension imag relat small number sampl well noisi measur medic imag paper propos novel method smooth imag scalar regress sir recov true associ imag outcom scalar predictor estim achiev minim mean squar error total variat tv regular term predict mean imag across subject denois imag subject time return coeffici map estim propos algorithm solv optim problem effici combin recent advanc graph fuse lasso solver statist consist estim shown via oracl inequ simul result demonstr propos method outperform exist method separ denois regress step especi sir show evid advantag recov signal small region appli sir alzheim diseas neuroimag initi data produc interpret brain map pet imag patient level featur includ age gender genotyp diseas group,"['Ying Liu', 'Bowei Yan']","['stat.ME', 'stat.AP']",False,False,False,True,False,False
1201,2017-03-28T14:01:45Z,2017-03-27T17:45:07Z,http://arxiv.org/abs/1703.09202v1,http://arxiv.org/pdf/1703.09202v1,Biologically inspired protection of deep networks from adversarial   attacks,biolog inspir protect deep network adversari attack,"Inspired by biophysical principles underlying nonlinear dendritic computation in neural circuits, we develop a scheme to train deep neural networks to make them robust to adversarial attacks. Our scheme generates highly nonlinear, saturated neural networks that achieve state of the art performance on gradient based adversarial examples on MNIST, despite never being exposed to adversarially chosen examples during training. Moreover, these networks exhibit unprecedented robustness to targeted, iterative schemes for generating adversarial examples, including second-order methods. We further identify principles governing how these networks achieve their robustness, drawing on methods from information geometry. We find these networks progressively create highly flat and compressed internal representations that are sensitive to very few input dimensions, while still solving the task. Moreover, they employ highly kurtotic weight distributions, also found in the brain, and we demonstrate how such kurtosis can protect even linear classifiers from adversarial attack.",inspir biophys principl nonlinear dendrit comput neural circuit develop scheme train deep neural network make robust adversari attack scheme generat high nonlinear satur neural network achiev state art perform gradient base adversari exampl mnist despit never expos adversari chosen exampl dure train moreov network exhibit unpreced robust target iter scheme generat adversari exampl includ second order method identifi principl govern network achiev robust draw method inform geometri find network progress creat high flat compress intern represent sensit veri input dimens still solv task moreov employ high kurtot weight distribut also found brain demonstr kurtosi protect even linear classifi adversari attack,"['Aran Nayebi', 'Surya Ganguli']","['stat.ML', 'cs.LG', 'q-bio.NC']",False,False,False,True,False,False
1203,2017-03-28T14:01:45Z,2017-03-27T16:16:35Z,http://arxiv.org/abs/1703.09165v1,http://arxiv.org/pdf/1703.09165v1,PWLS-ULTRA: An Efficient Clustering and Learning-Based Approach for   Low-Dose 3D CT Image Reconstruction,pwls ultra effici cluster learn base approach low dose ct imag reconstruct,"The development of computed tomography (CT) image reconstruction methods that significantly reduce patient radiation exposure while maintaining high image quality is an important area of research in low-dose CT (LDCT) imaging. We propose a new penalized weighted least squares (PWLS) reconstruction method that exploits regularization based on an efficient Union of Learned TRAnsforms (PWLS-ULTRA). The union of square transforms is pre-learned from numerous image patches extracted from a dataset of CT images or volumes. The proposed PWLS-based cost function is optimized by alternating between an image update step, and a sparse coding and clustering step. The CT image update step is accelerated by a relaxed linearized augmented Lagrangian method with ordered-subsets that reduces the number of forward and backward projections. Simulations with 2D and 3D axial CT scans of the XCAT phantom and 3D helical chest scans show that for low-dose levels, the proposed method significantly improves the quality of reconstructed images compared to PWLS reconstruction with a nonadaptive edge-preserving regularizer (PWLS-EP). PWLS with regularization based on a union of learned transforms leads to better image reconstructions than using a single learned square transform or a learned overcomplete synthesis dictionary. We also incorporate patch-based weights in PWLS-ULTRA that enhance image quality and help improve image resolution uniformity.",develop comput tomographi ct imag reconstruct method signific reduc patient radiat exposur maintain high imag qualiti import area research low dose ct ldct imag propos new penal weight least squar pwls reconstruct method exploit regular base effici union learn transform pwls ultra union squar transform pre learn numer imag patch extract dataset ct imag volum propos pwls base cost function optim altern imag updat step spars code cluster step ct imag updat step acceler relax linear augment lagrangian method order subset reduc number forward backward project simul axial ct scan xcat phantom helic chest scan show low dose level propos method signific improv qualiti reconstruct imag compar pwls reconstruct nonadapt edg preserv regular pwls ep pwls regular base union learn transform lead better imag reconstruct use singl learn squar transform learn overcomplet synthesi dictionari also incorpor patch base weight pwls ultra enhanc imag qualiti help improv imag resolut uniform,"['Xuehang Zheng', 'Saiprasad Ravishankar', 'Yong Long', 'Jeffrey A. Fessler']",['stat.ML'],False,False,False,True,False,False
1213,2017-03-28T14:01:49Z,2017-03-25T16:49:03Z,http://arxiv.org/abs/1703.08710v1,http://arxiv.org/pdf/1703.08710v1,Count-ception: Counting by Fully Convolutional Redundant Counting,count ception count fulli convolut redund count,"Counting objects in digital images is a process that should be replaced by machines. This tedious task is time consuming and prone to errors due to fatigue of human annotators. The goal is to have a system that takes as input an image and returns a count of the objects inside and justification for the prediction in the form of object localization. We repose a problem, originally posed by Lempitsky and Zisserman, to instead predict a count map which contains redundant counts based on the receptive field of a smaller regression network. The regression network predicts a count of the objects that exist inside this frame. By processing the image in a fully convolutional way each pixel is going to be accounted for some number of times, the number of windows which include it, which is the size of each window, (i.e., 32x32 = 1024). To recover the true count take the average over the redundant predictions. Our contribution is redundant counting instead of predicting a density map in order to average over errors. We also propose a novel deep neural network architecture adapted from the Inception family of networks called the Count-ception network. Together our approach results in a 20% gain over the state of the art method by Xie, Noble, and Zisserman in 2016.",count object digit imag process replac machin tedious task time consum prone error due fatigu human annot goal system take input imag return count object insid justif predict form object local repos problem origin pose lempitski zisserman instead predict count map contain redund count base recept field smaller regress network regress network predict count object exist insid frame process imag fulli convolut way pixel go account number time number window includ size window recov true count take averag redund predict contribut redund count instead predict densiti map order averag error also propos novel deep neural network architectur adapt incept famili network call count ception network togeth approach result gain state art method xie nobl zisserman,"['Joseph Paul Cohen', 'Henry Z. Lo', 'Yoshua Bengio']","['cs.CV', 'cs.LG', 'stat.ML']",False,False,False,True,False,False
1220,2017-03-28T14:01:53Z,2017-03-24T12:07:34Z,http://arxiv.org/abs/1703.08383v1,http://arxiv.org/pdf/1703.08383v1,Smart Augmentation - Learning an Optimal Data Augmentation Strategy,smart augment learn optim data augment strategi,"A recurring problem faced when training neural networks is that there is typically not enough data to maximize the generalization capability of deep neural networks(DNN). There are many techniques to address this, including data augmentation, dropout, and transfer learning. In this paper, we introduce an additional method which we call Smart Augmentation and we show how to use it to increase the accuracy and reduce overfitting on a target network. Smart Augmentation works by creating a network that learns how to generate augmented data during the training process of a target network in a way that reduces that networks loss. This allows us to learn augmentations that minimize the error of that network.   Smart Augmentation has shown the potential to increase accuracy by demonstrably significant measures on all datasets tested. In addition, it has shown potential to achieve similar or improved performance levels with significantly smaller network sizes in a number of tested cases.",recur problem face train neural network typic enough data maxim general capabl deep neural network dnn mani techniqu address includ data augment dropout transfer learn paper introduc addit method call smart augment show use increas accuraci reduc overfit target network smart augment work creat network learn generat augment data dure train process target network way reduc network loss allow us learn augment minim error network smart augment shown potenti increas accuraci demonstr signific measur dataset test addit shown potenti achiev similar improv perform level signific smaller network size number test case,"['Joseph Lemley', 'Shabab Bazrafkan', 'Peter Corcoran']","['cs.AI', 'cs.LG', 'stat.ML']",False,False,False,True,False,False
1252,2017-03-28T14:02:06Z,2017-03-22T17:08:40Z,http://arxiv.org/abs/1703.07255v2,http://arxiv.org/pdf/1703.07255v2,ZM-Net: Real-time Zero-shot Image Manipulation Network,zm net real time zero shot imag manipul network,"Many problems in image processing and computer vision (e.g. colorization, style transfer) can be posed as 'manipulating' an input image into a corresponding output image given a user-specified guiding signal. A holy-grail solution towards generic image manipulation should be able to efficiently alter an input image with any personalized signals (even signals unseen during training), such as diverse paintings and arbitrary descriptive attributes. However, existing methods are either inefficient to simultaneously process multiple signals (let alone generalize to unseen signals), or unable to handle signals from other modalities. In this paper, we make the first attempt to address the zero-shot image manipulation task. We cast this problem as manipulating an input image according to a parametric model whose key parameters can be conditionally generated from any guiding signal (even unseen ones). To this end, we propose the Zero-shot Manipulation Net (ZM-Net), a fully-differentiable architecture that jointly optimizes an image-transformation network (TNet) and a parameter network (PNet). The PNet learns to generate key transformation parameters for the TNet given any guiding signal while the TNet performs fast zero-shot image manipulation according to both signal-dependent parameters from the PNet and signal-invariant parameters from the TNet itself. Extensive experiments show that our ZM-Net can perform high-quality image manipulation conditioned on different forms of guiding signals (e.g. style images and attributes) in real-time (tens of milliseconds per image) even for unseen signals. Moreover, a large-scale style dataset with over 20,000 style images is also constructed to promote further research.",mani problem imag process comput vision color style transfer pose manipul input imag correspond output imag given user specifi guid signal holi grail solut toward generic imag manipul abl effici alter input imag ani person signal even signal unseen dure train divers paint arbitrari descript attribut howev exist method either ineffici simultan process multipl signal let alon general unseen signal unabl handl signal modal paper make first attempt address zero shot imag manipul task cast problem manipul input imag accord parametr model whose key paramet condit generat ani guid signal even unseen one end propos zero shot manipul net zm net fulli differenti architectur joint optim imag transform network tnet paramet network pnet pnet learn generat key transform paramet tnet given ani guid signal tnet perform fast zero shot imag manipul accord signal depend paramet pnet signal invari paramet tnet extens experi show zm net perform high qualiti imag manipul condit differ form guid signal style imag attribut real time ten millisecond per imag even unseen signal moreov larg scale style dataset style imag also construct promot research,"['Hao Wang', 'Xiaodan Liang', 'Hao Zhang', 'Dit-Yan Yeung', 'Eric P. Xing']","['cs.CV', 'cs.AI', 'cs.GR', 'cs.LG', 'stat.ML']",False,False,False,True,False,False
1255,2017-03-28T14:02:06Z,2017-03-21T10:34:59Z,http://arxiv.org/abs/1703.07131v1,http://arxiv.org/pdf/1703.07131v1,Knowledge distillation using unlabeled mismatched images,knowledg distil use unlabel mismatch imag,"Current approaches for Knowledge Distillation (KD) either directly use training data or sample from the training data distribution. In this paper, we demonstrate effectiveness of 'mismatched' unlabeled stimulus to perform KD for image classification networks. For illustration, we consider scenarios where this is a complete absence of training data, or mismatched stimulus has to be used for augmenting a small amount of training data. We demonstrate that stimulus complexity is a key factor for distillation's good performance. Our examples include use of various datasets for stimulating MNIST and CIFAR teachers.",current approach knowledg distil kd either direct use train data sampl train data distribut paper demonstr effect mismatch unlabel stimulus perform kd imag classif network illustr consid scenario complet absenc train data mismatch stimulus use augment small amount train data demonstr stimulus complex key factor distil good perform exampl includ use various dataset stimul mnist cifar teacher,"['Mandar Kulkarni', 'Kalpesh Patil', 'Shirish Karande']","['cs.CV', 'cs.LG', 'stat.ML']",False,False,False,True,False,False
1257,2017-03-28T14:02:06Z,2017-03-21T04:11:13Z,http://arxiv.org/abs/1703.07047v1,http://arxiv.org/pdf/1703.07047v1,High-Resolution Breast Cancer Screening with Multi-View Deep   Convolutional Neural Networks,high resolut breast cancer screen multi view deep convolut neural network,"Recent advances in deep learning for object recognition in natural images has prompted a surge of interest in applying a similar set of techniques to medical images. Most of the initial attempts largely focused on replacing the input to such a deep convolutional neural network from a natural image to a medical image. This, however, does not take into consideration the fundamental differences between these two types of data. More specifically, detection or recognition of an anomaly in medical images depends significantly on fine details, unlike object recognition in natural images where coarser, more global structures matter more. This difference makes it inadequate to use the existing deep convolutional neural networks architectures, which were developed for natural images, because they rely on heavily downsampling an image to a much lower resolution to reduce the memory requirements. This hides details necessary to make accurate predictions for medical images. Furthermore, a single exam in medical imaging often comes with a set of different views which must be seamlessly fused in order to reach a correct conclusion. In our work, we propose to use a multi-view deep convolutional neural network that handles a set of more than one high-resolution medical image. We evaluate this network on large-scale mammography-based breast cancer screening (BI-RADS prediction) using 103 thousand images. We focus on investigating the impact of training set sizes and image sizes on the prediction accuracy. Our results highlight that performance clearly increases with the size of training set, and that the best performance can only be achieved using the images in the original resolution. This suggests the future direction of medical imaging research using deep neural networks is to utilize as much data as possible with the least amount of potentially harmful preprocessing.",recent advanc deep learn object recognit natur imag prompt surg interest appli similar set techniqu medic imag initi attempt larg focus replac input deep convolut neural network natur imag medic imag howev doe take consider fundament differ two type data specif detect recognit anomali medic imag depend signific fine detail unlik object recognit natur imag coarser global structur matter differ make inadequ use exist deep convolut neural network architectur develop natur imag becaus reli heavili downsampl imag much lower resolut reduc memori requir hide detail necessari make accur predict medic imag furthermor singl exam medic imag often come set differ view must seamless fuse order reach correct conclus work propos use multi view deep convolut neural network handl set one high resolut medic imag evalu network larg scale mammographi base breast cancer screen bi rad predict use thousand imag focus investig impact train set size imag size predict accuraci result highlight perform clear increas size train set best perform onli achiev use imag origin resolut suggest futur direct medic imag research use deep neural network util much data possibl least amount potenti harm preprocess,"['Krzysztof J. Geras', 'Stacey Wolfson', 'S. Gene Kim', 'Linda Moy', 'Kyunghyun Cho']","['cs.CV', 'cs.LG', 'stat.ML']",False,False,False,True,False,False
1259,2017-03-28T14:02:06Z,2017-03-21T02:04:30Z,http://arxiv.org/abs/1703.07026v1,http://arxiv.org/pdf/1703.07026v1,Cross-modal Deep Metric Learning with Multi-task Regularization,cross modal deep metric learn multi task regular,"DNN-based cross-modal retrieval has become a research hotspot, by which users can search results across various modalities like image and text. However, existing methods mainly focus on the pairwise correlation and reconstruction error of labeled data. They ignore the semantically similar and dissimilar constraints between different modalities, and cannot take advantage of unlabeled data. This paper proposes Cross-modal Deep Metric Learning with Multi-task Regularization (CDMLMR), which integrates quadruplet ranking loss and semi-supervised contrastive loss for modeling cross-modal semantic similarity in a unified multi-task learning architecture. The quadruplet ranking loss can model the semantically similar and dissimilar constraints to preserve cross-modal relative similarity ranking information. The semi-supervised contrastive loss is able to maximize the semantic similarity on both labeled and unlabeled data. Compared to the existing methods, CDMLMR exploits not only the similarity ranking information but also unlabeled cross-modal data, and thus boosts cross-modal retrieval accuracy.",dnn base cross modal retriev becom research hotspot user search result across various modal like imag text howev exist method main focus pairwis correl reconstruct error label data ignor semant similar dissimilar constraint differ modal cannot take advantag unlabel data paper propos cross modal deep metric learn multi task regular cdmlmr integr quadruplet rank loss semi supervis contrast loss model cross modal semant similar unifi multi task learn architectur quadruplet rank loss model semant similar dissimilar constraint preserv cross modal relat similar rank inform semi supervis contrast loss abl maxim semant similar label unlabel data compar exist method cdmlmr exploit onli similar rank inform also unlabel cross modal data thus boost cross modal retriev accuraci,"['Xin Huang', 'Yuxin Peng']","['cs.LG', 'cs.CV', 'stat.ML']",False,False,False,True,False,False
1264,2017-03-28T14:02:09Z,2017-03-20T17:21:19Z,http://arxiv.org/abs/1703.06857v1,http://arxiv.org/pdf/1703.06857v1,Deep Neural Networks Do Not Recognize Negative Images,deep neural network recogn negat imag,"Deep Neural Networks (DNNs) have achieved remarkable performance on a variety of pattern-recognition tasks, particularly visual classification problems, where new algorithms reported to achieve or even surpass the human performance. In this paper, we test the state-of-the-art DNNs with negative images and show that the accuracy drops to the level of random classification. This leads us to the conjecture that the DNNs, which are merely trained on raw data, do not recognize the semantics of the objects, but rather memorize the inputs. We suggest that negative images can be thought as ""semantic adversarial examples"", which we define as transformed inputs that semantically represent the same objects, but the model does not classify them correctly.",deep neural network dnns achiev remark perform varieti pattern recognit task particular visual classif problem new algorithm report achiev even surpass human perform paper test state art dnns negat imag show accuraci drop level random classif lead us conjectur dnns mere train raw data recogn semant object rather memor input suggest negat imag thought semant adversari exampl defin transform input semant repres object model doe classifi correct,"['Hossein Hosseini', 'Radha Poovendran']","['cs.CV', 'cs.LG', 'stat.ML']",False,False,False,True,False,False
