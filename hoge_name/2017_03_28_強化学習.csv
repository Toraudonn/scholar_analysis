,プログラム実行日時,論文更新日時,論文リンク,PDFリンク,元論文タイトル,論文タイトル,元サマリ,サマリ,著者,事前付与ジャンル,ニューラルネットワーク,自然言語処理,マーケティング,画像解析,音声解析,強化学習
1,2017-03-28T14:05:16Z,2017-03-27T06:19:38Z,http://arxiv.org/abs/1703.08944v1,http://arxiv.org/abs/1703.08944v1,Intelligent bidirectional rapidly-exploring random trees for optimal   motion planning in complex cluttered environments,intellig bidirect rapid explor random tree optim motion plan complex clutter environ,"The sampling based motion planning algorithm known as Rapidly-exploring Random Trees (RRT) has gained the attention of many researchers due to their computational efficiency and effectiveness. Recently, a variant of RRT called RRT* has been proposed that ensures asymptotic optimality. Subsequently its bidirectional version has also been introduced in the literature known as Bidirectional-RRT* (B-RRT*). We introduce a new variant called Intelligent Bidirectional-RRT* (IB-RRT*) which is an improved variant of the optimal RRT* and bidirectional version of RRT* (B-RRT*) algorithms and is specially designed for complex cluttered environments. IB-RRT* utilizes the bidirectional trees approach and introduces intelligent sample insertion heuristic for fast convergence to the optimal path solution using uniform sampling heuristics. The proposed algorithm is evaluated theoretically and experimental results are presented that compares IB-RRT* with RRT* and B-RRT*. Moreover, experimental results demonstrate the superior efficiency of IB-RRT* in comparison with RRT* and B-RRT in complex cluttered environments.",sampl base motion plan algorithm known rapid explor random tree rrt gain attent mani research due comput effici effect recent variant rrt call rrt propos ensur asymptot optim subsequ bidirect version also introduc literatur known bidirect rrt rrt introduc new variant call intellig bidirect rrt ib rrt improv variant optim rrt bidirect version rrt rrt algorithm special design complex clutter environ ib rrt util bidirect tree approach introduc intellig sampl insert heurist fast converg optim path solut use uniform sampl heurist propos algorithm evalu theoret experiment result present compar ib rrt rrt rrt moreov experiment result demonstr superior effici ib rrt comparison rrt rrt complex clutter environ,"['Ahmed Hussain Qureshi', 'Yasar Ayaz']","['cs.RO', 'cs.AI']",False,False,False,False,False,True
2,2017-03-28T14:05:16Z,2017-03-27T04:03:56Z,http://arxiv.org/abs/1703.08922v1,http://arxiv.org/pdf/1703.08922v1,On Automating the Doctrine of Double Effect,autom doctrin doubl effect,"The doctrine of double effect ($\mathcal{DDE}$) is a long-studied ethical principle that governs when actions that have both positive and negative effects are to be allowed. The goal in this paper is to automate $\mathcal{DDE}$. We briefly present $\mathcal{DDE}$, and use a first-order modal logic, the deontic cognitive event calculus, as our framework to formalize the doctrine. We present formalizations of increasingly stronger versions of the principle, including what is known as the doctrine of triple effect. We then use our framework to simulate successfully scenarios that have been used to test for the presence of the principle in human subjects. Our framework can be used in two different modes: One can use it to build $\mathcal{DDE}$-compliant autonomous systems from scratch, or one can use it to verify that a given AI system is $\mathcal{DDE}$-compliant, by applying a $\mathcal{DDE}$ layer on an existing system or model. For the latter mode, the underlying AI system can be built using any architecture (planners, deep neural networks, bayesian networks, knowledge-representation systems, or a hybrid); as long as the system exposes a few parameters in its model, such verification is possible. The role of the $\mathcal{DDE}$ layer here is akin to a (dynamic or static) software verifier that examines existing software modules. Finally, we end by presenting initial work on how one can apply our $\mathcal{DDE}$ layer to the STRIPS-style planning model, and to a modified POMDP model.",doctrin doubl effect mathcal dde long studi ethic principl govern action posit negat effect allow goal paper autom mathcal dde briefli present mathcal dde use first order modal logic deontic cognit event calculus framework formal doctrin present formal increas stronger version principl includ known doctrin tripl effect use framework simul success scenario use test presenc principl human subject framework use two differ mode one use build mathcal dde compliant autonom system scratch one use verifi given ai system mathcal dde compliant appli mathcal dde layer exist system model latter mode ai system built use ani architectur planner deep neural network bayesian network knowledg represent system hybrid long system expos paramet model verif possibl role mathcal dde layer akin dynam static softwar verifi examin exist softwar modul final end present initi work one appli mathcal dde layer strip style plan model modifi pomdp model,"['Naveen Sundar Govindarajulu', 'Selmer Bringsjord']","['cs.AI', 'cs.LO', 'cs.RO']",False,False,True,False,False,True
3,2017-03-28T14:05:16Z,2017-03-26T19:39:50Z,http://arxiv.org/abs/1703.08862v1,http://arxiv.org/pdf/1703.08862v1,Socially Aware Motion Planning with Deep Reinforcement Learning,social awar motion plan deep reinforc learn,"For robotic vehicles to navigate safely and efficiently in pedestrian-rich environments, it is important to model subtle human behaviors and navigation rules. However, while instinctive to humans, socially compliant navigation is still difficult to quantify due to the stochasticity in people's behaviors. Existing works are mostly focused on using feature-matching techniques to describe and imitate human paths, but often do not generalize well since the feature values can vary from person to person, and even run to run. This work notes that while it is challenging to directly specify the details of what to do (precise mechanisms of human navigation), it is straightforward to specify what not to do (violations of social norms). Specifically, using deep reinforcement learning, this work develops a time-efficient navigation policy that respects common social norms. The proposed method is shown to enable fully autonomous navigation of a robotic vehicle moving at human walking speed in an environment with many pedestrians.",robot vehicl navig safe effici pedestrian rich environ import model subtl human behavior navig rule howev instinct human social compliant navig still difficult quantifi due stochast peopl behavior exist work focus use featur match techniqu describ imit human path often general well sinc featur valu vari person person even run run work note challeng direct specifi detail precis mechan human navig straightforward specifi violat social norm specif use deep reinforc learn work develop time effici navig polici respect common social norm propos method shown enabl fulli autonom navig robot vehicl move human walk speed environ mani pedestrian,"['Yu Fan Chen', 'Michael Everett', 'Miao Liu', 'Jonathan P. How']","['cs.RO', 'cs.AI', 'cs.HC']",False,False,False,False,False,True
4,2017-03-28T14:05:16Z,2017-03-26T16:20:36Z,http://arxiv.org/abs/1703.08840v1,http://arxiv.org/pdf/1703.08840v1,Inferring The Latent Structure of Human Decision-Making from Raw Visual   Inputs,infer latent structur human decis make raw visual input,"The goal of imitation learning is to match example expert behavior, without access to a reinforcement signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are not explicitly modeled. We introduce an extension to the Generative Adversarial Imitation Learning method that can infer the latent structure of human decision-making in an unsupervised way. Our method can not only imitate complex behaviors, but also learn interpretable and meaningful representations. We demonstrate that the approach is applicable to high-dimensional environments including raw visual inputs. In the highway driving domain, we show that a model learned from demonstrations is able to both produce different styles of human-like driving behaviors and accurately anticipate human actions. Our method surpasses various baselines in terms of performance and functionality.",goal imit learn match exampl expert behavior without access reinforc signal expert demonstr provid human howev often show signific variabl due latent factor explicit model introduc extens generat adversari imit learn method infer latent structur human decis make unsupervis way method onli imit complex behavior also learn interpret meaning represent demonstr approach applic high dimension environ includ raw visual input highway drive domain show model learn demonstr abl produc differ style human like drive behavior accur anticip human action method surpass various baselin term perform function,"['Yunzhu Li', 'Jiaming Song', 'Stefano Ermon']","['cs.LG', 'cs.AI', 'cs.CV']",False,False,False,False,False,True
7,2017-03-28T14:05:16Z,2017-03-26T03:47:54Z,http://arxiv.org/abs/1703.08762v1,http://arxiv.org/pdf/1703.08762v1,Team Formation for Scheduling Educational Material in Massive Online   Classes,team format schedul educ materi massiv onlin class,"Whether teaching in a classroom or a Massive Online Open Course it is crucial to present the material in a way that benefits the audience as a whole. We identify two important tasks to solve towards this objective, 1 group students so that they can maximally benefit from peer interaction and 2 find an optimal schedule of the educational material for each group. Thus, in this paper, we solve the problem of team formation and content scheduling for education. Given a time frame d, a set of students S with their required need to learn different activities T and given k as the number of desired groups, we study the problem of finding k group of students. The goal is to teach students within time frame d such that their potential for learning is maximized and find the best schedule for each group. We show this problem to be NP-hard and develop a polynomial algorithm for it. We show our algorithm to be effective both on synthetic as well as a real data set. For our experiments, we use real data on students' grades in a Computer Science department. As part of our contribution, we release a semi-synthetic dataset that mimics the properties of the real data.",whether teach classroom massiv onlin open cours crucial present materi way benefit audienc whole identifi two import task solv toward object group student maxim benefit peer interact find optim schedul educ materi group thus paper solv problem team format content schedul educ given time frame set student requir need learn differ activ given number desir group studi problem find group student goal teach student within time frame potenti learn maxim find best schedul group show problem np hard develop polynomi algorithm show algorithm effect synthet well real data set experi use real data student grade comput scienc depart part contribut releas semi synthet dataset mimic properti real data,"['Sanaz Bahargam', 'Dóra Erdos', 'Azer Bestavros', 'Evimaria Terzi']",['cs.AI'],False,False,True,False,False,True
11,2017-03-28T14:05:20Z,2017-03-24T13:00:52Z,http://arxiv.org/abs/1703.08397v1,http://arxiv.org/pdf/1703.08397v1,Reasoning by Cases in Structured Argumentation,reason case structur argument,"We extend the $ASPIC^+$ framework for structured argumentation so as to allow applications of the reasoning by cases inference scheme for defeasible arguments. Given an argument with conclusion `$A$ or $B$', an argument based on $A$ with conclusion $C$, and an argument based on $B$ with conclusion $C$, we allow the construction of an argument with conclusion $C$. We show how our framework leads to different results than other approaches in non-monotonic logic for dealing with disjunctive information, such as disjunctive default theory or approaches based on the OR-rule (which allows to derive a defeasible rule `If ($A$ or $B$) then $C$', given two defeasible rules `If $A$ then $C$' and `If $B$ then $C$'). We raise new questions regarding the subtleties of reasoning defeasibly with disjunctive information, and show that its formalization is more intricate than one would presume.",extend aspic framework structur argument allow applic reason case infer scheme defeas argument given argument conclus argument base conclus argument base conclus allow construct argument conclus show framework lead differ result approach non monoton logic deal disjunct inform disjunct default theori approach base rule allow deriv defeas rule given two defeas rule rais new question regard subtleti reason defeas disjunct inform show formal intric one would presum,"['Mathieu Beirlaen', 'Jesse Heyninck', 'Christian Straßer']","['cs.AI', '68T27', 'I.2.3; I.2.4']",False,False,False,False,False,True
13,2017-03-28T14:05:20Z,2017-03-24T01:59:11Z,http://arxiv.org/abs/1703.08262v1,http://arxiv.org/pdf/1703.08262v1,Supervisor Synthesis of POMDP based on Automata Learning,supervisor synthesi pomdp base automata learn,"As a general and thus popular model for autonomous systems, partially observable Markov decision process (POMDP) can capture uncertainties from different sources like sensing noises, actuation errors, and uncertain environments. However, its comprehensiveness makes the planning and control in POMDP difficult. Traditional POMDP planning problems target to find the optimal policy to maximize the expectation of accumulated rewards. But for safety critical applications, guarantees of system performance described by formal specifications are desired, which motivates us to consider formal methods to synthesize supervisor for POMDP. With system specifications given by Probabilistic Computation Tree Logic (PCTL), we propose a supervisory control framework with a type of deterministic finite automata (DFA), za-DFA, as the controller form. While the existing work mainly relies on optimization techniques to learn fixed-size finite state controllers (FSCs), we develop an $L^*$ learning based algorithm to determine both space and transitions of za-DFA. Membership queries and different oracles for conjectures are defined. The learning algorithm is sound and complete. An example is given in detailed steps to illustrate the supervisor synthesis algorithm.",general thus popular model autonom system partial observ markov decis process pomdp captur uncertainti differ sourc like sens nois actuat error uncertain environ howev comprehens make plan control pomdp difficult tradit pomdp plan problem target find optim polici maxim expect accumul reward safeti critic applic guarante system perform describ formal specif desir motiv us consid formal method synthes supervisor pomdp system specif given probabilist comput tree logic pctl propos supervisori control framework type determinist finit automata dfa za dfa control form exist work main reli optim techniqu learn fix size finit state control fscs develop learn base algorithm determin space transit za dfa membership queri differ oracl conjectur defin learn algorithm sound complet exampl given detail step illustr supervisor synthesi algorithm,"['Xiaobin Zhang', 'Bo Wu', 'Hai Lin']","['cs.SY', 'cs.AI', 'cs.FL']",False,False,False,False,False,True
14,2017-03-28T14:05:20Z,2017-03-23T17:07:14Z,http://arxiv.org/abs/1703.08144v1,http://arxiv.org/pdf/1703.08144v1,Note Value Recognition for Rhythm Transcription Using a Markov Random   Field Model for Musical Scores and Performances of Piano Music,note valu recognit rhythm transcript use markov random field model music score perform piano music,"This paper presents a statistical method for music transcription that can estimate score times of note onsets and offsets from polyphonic MIDI performance signals. Because performed note durations can deviate largely from score-indicated values, previous methods had the problem of not being able to accurately estimate offset score times (or note values) and thus could only output incomplete musical scores. Based on observations that the pitch context and onset score times are influential on the configuration of note values, we construct a context-tree model that provides prior distributions of note values using these features and combine it with a performance model in the framework of Markov random fields. Evaluation results showed that our method reduces the average error rate by around 40 percent compared to existing/simple methods. We also confirmed that, in our model, the score model plays a more important role than the performance model, and it automatically captures the voice structure by unsupervised learning.",paper present statist method music transcript estim score time note onset offset polyphon midi perform signal becaus perform note durat deviat larg score indic valu previous method problem abl accur estim offset score time note valu thus could onli output incomplet music score base observ pitch context onset score time influenti configur note valu construct context tree model provid prior distribut note valu use featur combin perform model framework markov random field evalu result show method reduc averag error rate around percent compar exist simpl method also confirm model score model play import role perform model automat captur voic structur unsupervis learn,"['Eita Nakamura', 'Kazuyoshi Yoshii', 'Simon Dixon']","['cs.AI', 'cs.SD']",False,False,False,False,False,True
17,2017-03-28T14:05:20Z,2017-03-23T10:44:18Z,http://arxiv.org/abs/1703.07994v1,http://arxiv.org/pdf/1703.07994v1,Containment for Rule-Based Ontology-Mediated Queries,contain rule base ontolog mediat queri,"Many efforts have been dedicated to identifying restrictions on ontologies expressed as tuple-generating dependencies (tgds), a.k.a. existential rules, that lead to the decidability for the problem of answering ontology-mediated queries (OMQs). This has given rise to three families of formalisms: guarded, non-recursive, and sticky sets of tgds. In this work, we study the containment problem for OMQs expressed in such formalisms, which is a key ingredient for solving static analysis tasks associated with them. Our main contribution is the development of specially tailored techniques for OMQ containment under the classes of tgds stated above. This enables us to obtain sharp complexity bounds for the problems at hand. We also apply our techniques to pinpoint the complexity of problems associated with two emerging applications of OMQ containment: distribution over components and UCQ rewritability of OMQs.",mani effort dedic identifi restrict ontolog express tupl generat depend tgds existenti rule lead decid problem answer ontolog mediat queri omq given rise three famili formal guard non recurs sticki set tgds work studi contain problem omq express formal key ingredi solv static analysi task associ main contribut develop special tailor techniqu omq contain class tgds state abov enabl us obtain sharp complex bound problem hand also appli techniqu pinpoint complex problem associ two emerg applic omq contain distribut compon ucq rewrit omq,"['Pablo Barcelo', 'Gerald Berger', 'Andreas Pieris']","['cs.DB', 'cs.AI', 'cs.LO']",False,False,False,False,False,True
18,2017-03-28T14:05:20Z,2017-03-23T07:13:28Z,http://arxiv.org/abs/1703.07948v1,http://arxiv.org/pdf/1703.07948v1,Fast Stochastic Variance Reduced Gradient Method with Momentum   Acceleration for Machine Learning,fast stochast varianc reduc gradient method momentum acceler machin learn,"Recently, research on accelerated stochastic gradient descent methods (e.g., SVRG) has made exciting progress (e.g., linear convergence for strongly convex problems). However, the best-known methods (e.g., Katyusha) requires at least two auxiliary variables and two momentum parameters. In this paper, we propose a fast stochastic variance reduction gradient (FSVRG) method, in which we design a novel update rule with the Nesterov's momentum and incorporate the technique of growing epoch size. FSVRG has only one auxiliary variable and one momentum weight, and thus it is much simpler and has much lower per-iteration complexity. We prove that FSVRG achieves linear convergence for strongly convex problems and the optimal $\mathcal{O}(1/T^2)$ convergence rate for non-strongly convex problems, where $T$ is the number of outer-iterations. We also extend FSVRG to directly solve the problems with non-smooth component functions, such as SVM. Finally, we empirically study the performance of FSVRG for solving various machine learning problems such as logistic regression, ridge regression, Lasso and SVM. Our results show that FSVRG outperforms the state-of-the-art stochastic methods, including Katyusha.",recent research acceler stochast gradient descent method svrg made excit progress linear converg strong convex problem howev best known method katyusha requir least two auxiliari variabl two momentum paramet paper propos fast stochast varianc reduct gradient fsvrg method design novel updat rule nesterov momentum incorpor techniqu grow epoch size fsvrg onli one auxiliari variabl one momentum weight thus much simpler much lower per iter complex prove fsvrg achiev linear converg strong convex problem optim mathcal converg rate non strong convex problem number outer iter also extend fsvrg direct solv problem non smooth compon function svm final empir studi perform fsvrg solv various machin learn problem logist regress ridg regress lasso svm result show fsvrg outperform state art stochast method includ katyusha,"['Fanhua Shang', 'Yuanyuan Liu', 'James Cheng', 'Jiacheng Zhuo']","['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']",False,False,False,False,False,True
19,2017-03-28T14:05:20Z,2017-03-23T05:23:34Z,http://arxiv.org/abs/1703.07940v1,http://arxiv.org/pdf/1703.07940v1,Unsupervised Basis Function Adaptation for Reinforcement Learning,unsupervis basi function adapt reinforc learn,"When using reinforcement learning (RL) algorithms to evaluate a policy it is common, given a large state space, to introduce some form of approximation architecture for the value function (VF). The exact form of this architecture can have a significant effect on the accuracy of the VF estimate, however, and determining a suitable approximation architecture can often be a highly complex task. Consequently there is a large amount of interest in the potential for allowing RL algorithms to adaptively generate (i.e. to learn) approximation architectures.   We investigate a method of adapting approximation architectures which uses feedback regarding the frequency with which an agent has visited certain states to guide which areas of the state space to approximate with greater detail. We introduce an algorithm based upon this idea which adapts a state aggregation approximation architecture on-line.   Assuming $S$ states, we demonstrate theoretically that - provided the following relatively non-restrictive assumptions are satisfied: (a) the number of cells $X$ in the state aggregation architecture is of order $\sqrt{S}\ln{S}\log_2{S}$ or greater, (b) the policy and transition function are close to deterministic, and (c) the prior for the transition function is uniformly distributed - our algorithm can guarantee, assuming we use an appropriate scoring function to measure VF error, error which is arbitrarily close to zero as $S$ becomes large. It is able to do this despite having only $O(X\log_2{S})$ space complexity (and negligible time complexity). We conclude by generating a set of empirical results which support the theoretical results.",use reinforc learn rl algorithm evalu polici common given larg state space introduc form approxim architectur valu function vf exact form architectur signific effect accuraci vf estim howev determin suitabl approxim architectur often high complex task consequ larg amount interest potenti allow rl algorithm adapt generat learn approxim architectur investig method adapt approxim architectur use feedback regard frequenc agent visit certain state guid area state space approxim greater detail introduc algorithm base upon idea adapt state aggreg approxim architectur line assum state demonstr theoret provid follow relat non restrict assumpt satisfi number cell state aggreg architectur order sqrt ln log greater polici transit function close determinist prior transit function uniform distribut algorithm guarante assum use appropri score function measur vf error error arbitrarili close zero becom larg abl despit onli log space complex neglig time complex conclud generat set empir result support theoret result,"['Edward Barker', 'Charl Ras']","['cs.LG', 'cs.AI', 'stat.ML']",False,False,False,False,False,True
20,2017-03-28T14:05:24Z,2017-03-23T04:26:46Z,http://arxiv.org/abs/1703.07929v1,http://arxiv.org/pdf/1703.07929v1,Diversification-Based Learning in Computing and Optimization,diversif base learn comput optim,"Diversification-Based Learning (DBL) derives from a collection of principles and methods introduced in the field of metaheuristics that have broad applications in computing and optimization. We show that the DBL framework goes significantly beyond that of the more recent Opposition-based learning (OBL) framework introduced in Tizhoosh (2005), which has become the focus of numerous research initiatives in machine learning and metaheuristic optimization. We unify and extend earlier proposals in metaheuristic search (Glover, 1997, Glover and Laguna, 1997) to give a collection of approaches that are more flexible and comprehensive than OBL for creating intensification and diversification strategies in metaheuristic search. We also describe potential applications of DBL to various subfields of machine learning and optimization.",diversif base learn dbl deriv collect principl method introduc field metaheurist broad applic comput optim show dbl framework goe signific beyond recent opposit base learn obl framework introduc tizhoosh becom focus numer research initi machin learn metaheurist optim unifi extend earlier propos metaheurist search glover glover laguna give collect approach flexibl comprehens obl creat intensif diversif strategi metaheurist search also describ potenti applic dbl various subfield machin learn optim,"['Fred Glover', 'Jin-Kao Hao']",['cs.AI'],False,False,True,False,False,True
22,2017-03-28T14:05:24Z,2017-03-22T19:08:48Z,http://arxiv.org/abs/1703.07822v1,http://arxiv.org/pdf/1703.07822v1,Information-theoretic Model Identification and Policy Search using   Physics Engines with Application to Robotic Manipulation,inform theoret model identif polici search use physic engin applic robot manipul,"We consider the problem of a robot learning the mechanical properties of objects through physical interaction with the object, and introduce a practical, data-efficient approach for identifying the motion models of these objects. The proposed method utilizes a physics engine, where the robot seeks to identify the inertial and friction parameters of the object by simulating its motion under different values of the parameters and identifying those that result in a simulation which matches the observed real motions. The problem is solved in a Bayesian optimization framework. The same framework is used for both identifying the model of an object online and searching for a policy that would minimize a given cost function according to the identified model. Experimental results both in simulation and using a real robot indicate that the proposed method outperforms state-of-the-art model-free reinforcement learning approaches.",consid problem robot learn mechan properti object physic interact object introduc practic data effici approach identifi motion model object propos method util physic engin robot seek identifi inerti friction paramet object simul motion differ valu paramet identifi result simul match observ real motion problem solv bayesian optim framework framework use identifi model object onlin search polici would minim given cost function accord identifi model experiment result simul use real robot indic propos method outperform state art model free reinforc learn approach,"['Shaojun Zhu', 'Andrew Kimmel', 'Abdeslam Boularias']","['cs.RO', 'cs.AI', 'cs.LG']",False,False,False,False,False,True
23,2017-03-28T14:05:24Z,2017-03-22T18:20:07Z,http://arxiv.org/abs/1703.07805v1,http://arxiv.org/abs/1703.07805v1,Supervised Typing of Big Graphs using Semantic Embeddings,supervis type big graph use semant embed,"We propose a supervised algorithm for generating type embeddings in the same semantic vector space as a given set of entity embeddings. The algorithm is agnostic to the derivation of the underlying entity embeddings. It does not require any manual feature engineering, generalizes well to hundreds of types and achieves near-linear scaling on Big Graphs containing many millions of triples and instances by virtue of an incremental execution. We demonstrate the utility of the embeddings on a type recommendation task, outperforming a non-parametric feature-agnostic baseline while achieving 15x speedup and near-constant memory usage on a full partition of DBpedia. Using state-of-the-art visualization, we illustrate the agreement of our extensionally derived DBpedia type embeddings with the manually curated domain ontology. Finally, we use the embeddings to probabilistically cluster about 4 million DBpedia instances into 415 types in the DBpedia ontology.",propos supervis algorithm generat type embed semant vector space given set entiti embed algorithm agnost deriv entiti embed doe requir ani manual featur engin general well hundr type achiev near linear scale big graph contain mani million tripl instanc virtu increment execut demonstr util embed type recommend task outperform non parametr featur agnost baselin achiev speedup near constant memori usag full partit dbpedia use state art visual illustr agreement extension deriv dbpedia type embed manual curat domain ontolog final use embed probabilist cluster million dbpedia instanc type dbpedia ontolog,"['Mayank Kejriwal', 'Pedro Szekely']","['cs.CL', 'cs.AI']",False,False,False,False,False,True
24,2017-03-28T14:05:24Z,2017-03-22T17:27:57Z,http://arxiv.org/abs/1703.07758v1,http://arxiv.org/pdf/1703.07758v1,S-Concave Distributions: Towards Broader Distributions for   Noise-Tolerant and Sample-Efficient Learning Algorithms,concav distribut toward broader distribut nois toler sampl effici learn algorithm,"We provide new results concerning noise-tolerant and sample-efficient learning algorithms under $s$-concave distributions over $\mathbb{R}^n$ for $-\frac{1}{2n+3}\le s\le 0$. The new class of $s$-concave distributions is a broad and natural generalization of log-concavity, and includes many important additional distributions, e.g., the Pareto distribution and $t$-distribution. This class has been studied in the context of efficient sampling, integration, and optimization, but much remains unknown concerning the geometry of this class of distributions and their applications in the context of learning.   The challenge is that unlike the commonly used distributions in learning (uniform or more generally log-concave distributions), this broader class is not closed under the marginalization operator and many such distributions are fat-tailed. In this work, we introduce new convex geometry tools to study the properties of s-concave distributions and use these properties to provide bounds on quantities of interest to learning including the probability of disagreement between two halfspaces, disagreement outside a band, and disagreement coefficient. We use these results to significantly generalize prior results for margin-based active learning, disagreement-based active learning, and passively learning of intersections of halfspaces.   Our analysis of geometric properties of s-concave distributions might be of independent interest to optimization more broadly.",provid new result concern nois toler sampl effici learn algorithm concav distribut mathbb frac le le new class concav distribut broad natur general log concav includ mani import addit distribut pareto distribut distribut class studi context effici sampl integr optim much remain unknown concern geometri class distribut applic context learn challeng unlik common use distribut learn uniform general log concav distribut broader class close margin oper mani distribut fat tail work introduc new convex geometri tool studi properti concav distribut use properti provid bound quantiti interest learn includ probabl disagr two halfspac disagr outsid band disagr coeffici use result signific general prior result margin base activ learn disagr base activ learn passiv learn intersect halfspac analysi geometr properti concav distribut might independ interest optim broad,"['Maria-Florina Balcan', 'Hongyang Zhang']","['stat.ML', 'cs.AI', 'cs.LG']",False,False,False,False,False,True
26,2017-03-28T14:05:24Z,2017-03-22T15:34:23Z,http://arxiv.org/abs/1703.07710v1,http://arxiv.org/pdf/1703.07710v1,UBEV - A More Practical Algorithm for Episodic RL with Near-Optimal PAC   and Regret Guarantees,ubev practic algorithm episod rl near optim pac regret guarante,"We present UBEV, a simple and efficient reinforcement learning algorithm for fixed-horizon episodic Markov decision processes. The main contribution is a proof that UBEV enjoys a sample-complexity bound that holds for all accuracy levels simultaneously with high probability, and matches the lower bound except for logarithmic terms and one factor of the horizon. A consequence of the fact that our sample-complexity bound holds for all accuracy levels is that the new algorithm achieves a sub-linear regret of O(sqrt(SAT)), which is the first time the dependence on the size of the state space has provably appeared inside the square root. A brief empirical evaluation shows that UBEV is practically superior to existing algorithms with known sample-complexity guarantees.",present ubev simpl effici reinforc learn algorithm fix horizon episod markov decis process main contribut proof ubev enjoy sampl complex bound hold accuraci level simultan high probabl match lower bound except logarithm term one factor horizon consequ fact sampl complex bound hold accuraci level new algorithm achiev sub linear regret sqrt sat first time depend size state space provabl appear insid squar root brief empir evalu show ubev practic superior exist algorithm known sampl complex guarante,"['Christoph Dann', 'Tor Lattimore', 'Emma Brunskill']","['cs.LG', 'cs.AI', 'stat.ML']",False,False,False,False,False,True
27,2017-03-28T14:05:24Z,2017-03-22T11:53:53Z,http://arxiv.org/abs/1703.07608v1,http://arxiv.org/pdf/1703.07608v1,Deep Exploration via Randomized Value Functions,deep explor via random valu function,We study the use of randomized value functions to guide deep exploration in reinforcement learning. This offers an elegant means for synthesizing statistically and computationally efficient exploration with common practical approaches to value function learning. We present several reinforcement learning algorithms that leverage randomized value functions and demonstrate their efficacy through computational studies. We also prove a regret bound that establishes statistical efficiency with a tabular representation.,studi use random valu function guid deep explor reinforc learn offer eleg mean synthes statist comput effici explor common practic approach valu function learn present sever reinforc learn algorithm leverag random valu function demonstr efficaci comput studi also prove regret bound establish statist effici tabular represent,"['Ian Osband', 'Daniel Russo', 'Zheng Wen', 'Benjamin Van Roy']","['stat.ML', 'cs.AI', 'cs.LG']",False,False,False,False,False,True
29,2017-03-28T14:05:24Z,2017-03-21T19:12:35Z,http://arxiv.org/abs/1703.07394v1,http://arxiv.org/pdf/1703.07394v1,Deep Learning for Explicitly Modeling Optimization Landscapes,deep learn explicit model optim landscap,"In all but the most trivial optimization problems, the structure of the solutions exhibit complex interdependencies between the input parameters. Decades of research with stochastic search techniques has shown the benefit of explicitly modeling the interactions between sets of parameters and the overall quality of the solutions discovered. We demonstrate a novel method, based on learning deep networks, to model the global landscapes of optimization problems. To represent the search space concisely and accurately, the deep networks must encode information about the underlying parameter interactions and their contributions to the quality of the solution. Once the networks are trained, the networks are probed to reveal parameter combinations with high expected performance with respect to the optimization task. These estimates are used to initialize fast, randomized, local search algorithms, which in turn expose more information about the search space that is subsequently used to refine the models. We demonstrate the technique on multiple optimization problems that have arisen in a variety of real-world domains, including: packing, graphics, job scheduling, layout and compression. The problems include combinatoric search spaces, discontinuous and highly non-linear spaces, and span binary, higher-cardinality discrete, as well as continuous parameters. Strengths, limitations, and extensions of the approach are extensively discussed and demonstrated.",trivial optim problem structur solut exhibit complex interdepend input paramet decad research stochast search techniqu shown benefit explicit model interact set paramet overal qualiti solut discov demonstr novel method base learn deep network model global landscap optim problem repres search space concis accur deep network must encod inform paramet interact contribut qualiti solut onc network train network probe reveal paramet combin high expect perform respect optim task estim use initi fast random local search algorithm turn expos inform search space subsequ use refin model demonstr techniqu multipl optim problem arisen varieti real world domain includ pack graphic job schedul layout compress problem includ combinator search space discontinu high non linear space span binari higher cardin discret well continu paramet strength limit extens approach extens discuss demonstr,['Shumeet Baluja'],"['cs.NE', 'cs.AI', 'cs.LG']",False,False,False,False,False,True
32,2017-03-28T14:05:28Z,2017-03-22T00:24:03Z,http://arxiv.org/abs/1703.07326v2,http://arxiv.org/pdf/1703.07326v2,One-Shot Imitation Learning,one shot imit learn,"Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning.   Specifically, we consider the setting where there is a very large set of tasks, and each task has many instantiations. For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states. At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks. A neural net is trained that takes as input one demonstration and the current state (which initially is the initial state of the other demonstration of the pair), and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. At test time, a demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. The use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks.   Videos available at https://bit.ly/one-shot-imitation .",imit learn common appli solv differ task isol usual requir either care featur engin signific number sampl far desir ideal robot abl learn veri demonstr ani given task instant general new situat task without requir task specif engin paper propos meta learn framework achiev capabl call one shot imit learn specif consid set veri larg set task task mani instanti exampl task could stack block tabl singl tower anoth task could place block tabl two block tower etc case differ instanc task would consist differ set block differ initi state train time algorithm present pair demonstr subset task neural net train take input one demonstr current state initi initi state demonstr pair output action goal result sequenc state action match close possibl second demonstr test time demonstr singl instanc new task present neural net expect perform well new instanc new task use soft attent allow model general condit task unseen train data anticip train model much greater varieti task set obtain general system turn ani demonstr robust polici accomplish overwhelm varieti task video avail https bit ly one shot imit,"['Yan Duan', 'Marcin Andrychowicz', 'Bradly C. Stadie', 'Jonathan Ho', 'Jonas Schneider', 'Ilya Sutskever', 'Pieter Abbeel', 'Wojciech Zaremba']","['cs.AI', 'cs.LG', 'cs.NE', 'cs.RO']",False,False,False,False,False,True
34,2017-03-28T14:05:28Z,2017-03-21T09:57:59Z,http://arxiv.org/abs/1703.07116v1,http://arxiv.org/pdf/1703.07116v1,Interest-Driven Discovery of Local Process Models,interest driven discoveri local process model,"Local Process Models (LPM) describe structured fragments of process behavior occurring in the context of less structured business processes. Traditional LPM discovery aims to generate a collection of process models that describe highly frequent behavior, but these models do not always provide useful answers for questions posed by process analysts aiming at business process improvement. We propose a framework for goal-driven LPM discovery, based on utility functions and constraints. We describe four scopes on which these utility functions and constrains can be defined, and show that utility functions and constraints on different scopes can be combined to form composite utility functions/constraints. Finally, we demonstrate the applicability of our approach by presenting several actionable business insights discovered with LPM discovery on two real life data sets.",local process model lpm describ structur fragment process behavior occur context less structur busi process tradit lpm discoveri aim generat collect process model describ high frequent behavior model alway provid use answer question pose process analyst aim busi process improv propos framework goal driven lpm discoveri base util function constraint describ four scope util function constrain defin show util function constraint differ scope combin form composit util function constraint final demonstr applic approach present sever action busi insight discov lpm discoveri two real life data set,"['Niek Tax', 'Benjamin Dalmas', 'Natalia Sidorova', 'Wil M P van der Aalst', 'Sylvie Norre']","['cs.DB', 'cs.AI']",False,False,False,False,False,True
35,2017-03-28T14:05:28Z,2017-03-21T07:09:27Z,http://arxiv.org/abs/1703.07075v1,http://arxiv.org/pdf/1703.07075v1,Pseudorehearsal in value function approximation,pseudorehears valu function approxim,"Catastrophic forgetting is of special importance in reinforcement learning, as the data distribution is generally non-stationary over time. We study and compare several pseudorehearsal approaches for Q-learning with function approximation in a pole balancing task. We have found that pseudorehearsal seems to assist learning even in such very simple problems, given proper initialization of the rehearsal parameters.",catastroph forget special import reinforc learn data distribut general non stationari time studi compar sever pseudorehears approach learn function approxim pole balanc task found pseudorehears seem assist learn even veri simpl problem given proper initi rehears paramet,"['Vladimir Marochko', 'Leonard Johard', 'Manuel Mazzara']",['cs.AI'],False,False,False,False,False,True
36,2017-03-28T14:05:28Z,2017-03-21T04:56:14Z,http://arxiv.org/abs/1703.07055v1,http://arxiv.org/pdf/1703.07055v1,Investigation of Language Understanding Impact for Reinforcement   Learning Based Dialogue Systems,investig languag understand impact reinforc learn base dialogu system,"Language understanding is a key component in a spoken dialogue system. In this paper, we investigate how the language understanding module influences the dialogue system performance by conducting a series of systematic experiments on a task-oriented neural dialogue system in a reinforcement learning based setting. The empirical study shows that among different types of language understanding errors, slot-level errors can have more impact on the overall performance of a dialogue system compared to intent-level errors. In addition, our experiments demonstrate that the reinforcement learning based dialogue system is able to learn when and what to confirm in order to achieve better performance and greater robustness.",languag understand key compon spoken dialogu system paper investig languag understand modul influenc dialogu system perform conduct seri systemat experi task orient neural dialogu system reinforc learn base set empir studi show among differ type languag understand error slot level error impact overal perform dialogu system compar intent level error addit experi demonstr reinforc learn base dialogu system abl learn confirm order achiev better perform greater robust,"['Xiujun Li', 'Yun-Nung Chen', 'Lihong Li', 'Jianfeng Gao', 'Asli Celikyilmaz']","['cs.CL', 'cs.AI', 'cs.LG']",False,False,False,False,False,True
38,2017-03-28T14:05:28Z,2017-03-20T19:32:40Z,http://arxiv.org/abs/1703.06939v1,http://arxiv.org/pdf/1703.06939v1,"Distributed Constraint Problems for Utilitarian Agents with Privacy   Concerns, Recast as POMDPs",distribut constraint problem utilitarian agent privaci concern recast pomdp,"Privacy has traditionally been a major motivation for distributed problem solving. Distributed Constraint Satisfaction Problem (DisCSP) as well as Distributed Constraint Optimization Problem (DCOP) are fundamental models used to solve various families of distributed problems. Even though several approaches have been proposed to quantify and preserve privacy in such problems, none of them is exempt from limitations. Here we approach the problem by assuming that computation is performed among utilitarian agents. We introduce a utilitarian approach where the utility of each state is estimated as the difference between the reward for reaching an agreement on assignments of shared variables and the cost of privacy loss. We investigate extensions to solvers where agents integrate the utility function to guide their search and decide which action to perform, defining thereby their policy. We show that these extended solvers succeed in significantly reducing privacy loss without significant degradation of the solution quality.",privaci tradit major motiv distribut problem solv distribut constraint satisfact problem discsp well distribut constraint optim problem dcop fundament model use solv various famili distribut problem even though sever approach propos quantifi preserv privaci problem none exempt limit approach problem assum comput perform among utilitarian agent introduc utilitarian approach util state estim differ reward reach agreement assign share variabl cost privaci loss investig extens solver agent integr util function guid search decid action perform defin therebi polici show extend solver succeed signific reduc privaci loss without signific degrad solut qualiti,"['Julien Savaux', 'Julien Vion', 'Sylvain Piechowiak', 'René Mandiau', 'Toshihiro Matsui', 'Katsutoshi Hirayama', 'Makoto Yokoo', 'Shakre Elmane', 'Marius Silaghi']",['cs.AI'],False,False,False,False,False,True
44,2017-03-28T14:05:32Z,2017-03-21T17:41:23Z,http://arxiv.org/abs/1703.06585v2,http://arxiv.org/pdf/1703.06585v2,Learning Cooperative Visual Dialog Agents with Deep Reinforcement   Learning,learn cooper visual dialog agent deep reinforc learn,"We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative 'image guessing' game between two agents -- Qbot and Abot -- who communicate in natural language dialog so that Qbot can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end -- from pixels to multi-agent multi-round dialog to game reward.   We demonstrate two experimental results.   First, as a 'sanity check' demonstration of pure RL (from scratch), we show results on a synthetic world, where the agents communicate in ungrounded vocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes (shape/color/style). Thus, we demonstrate the emergence of grounded language and communication among 'visual' dialog agents with no human supervision.   Second, we conduct large-scale real-image experiments on the VisDial dataset, where we pretrain with supervised dialog data and show that the RL 'fine-tuned' agents significantly outperform SL agents. Interestingly, the RL Qbot learns to ask questions that Abot is good at, ultimately resulting in more informative dialog and a better team.",introduc first goal driven train visual question answer dialog agent specif pose cooper imag guess game two agent qbot abot communic natur languag dialog qbot select unseen imag lineup imag use deep reinforc learn rl learn polici agent end end pixel multi agent multi round dialog game reward demonstr two experiment result first saniti check demonstr pure rl scratch show result synthet world agent communic unground vocabulari symbol pre specifi mean find two bot invent communic protocol start use certain symbol ask answer certain visual attribut shape color style thus demonstr emerg ground languag communic among visual dialog agent human supervis second conduct larg scale real imag experi visdial dataset pretrain supervis dialog data show rl fine tune agent signific outperform sl agent interest rl qbot learn ask question abot good ultim result inform dialog better team,"['Abhishek Das', 'Satwik Kottur', 'José M. F. Moura', 'Stefan Lee', 'Dhruv Batra']","['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG']",False,False,False,False,False,True
46,2017-03-28T14:05:32Z,2017-03-19T17:31:13Z,http://arxiv.org/abs/1703.06471v1,http://arxiv.org/pdf/1703.06471v1,"Multi-Timescale, Gradient Descent, Temporal Difference Learning with   Linear Options",multi timescal gradient descent tempor differ learn linear option,"Deliberating on large or continuous state spaces have been long standing challenges in reinforcement learning. Temporal Abstraction have somewhat made this possible, but efficiently planing using temporal abstraction still remains an issue. Moreover using spatial abstractions to learn policies for various situations at once while using temporal abstraction models is an open problem. We propose here an efficient algorithm which is convergent under linear function approximation while planning using temporally abstract actions. We show how this algorithm can be used along with randomly generated option models over multiple time scales to plan agents which need to act real time. Using these randomly generated option models over multiple time scales are shown to reduce number of decision epochs required to solve the given task, hence effectively reducing the time needed for deliberation.",deliber larg continu state space long stand challeng reinforc learn tempor abstract somewhat made possibl effici plane use tempor abstract still remain issu moreov use spatial abstract learn polici various situat onc use tempor abstract model open problem propos effici algorithm converg linear function approxim plan use tempor abstract action show algorithm use along random generat option model multipl time scale plan agent need act real time use random generat option model multipl time scale shown reduc number decis epoch requir solv given task henc effect reduc time need deliber,"['Peeyush Kumar', 'Doina Precup']",['cs.AI'],False,False,False,False,False,True
48,2017-03-28T14:05:32Z,2017-03-18T21:25:29Z,http://arxiv.org/abs/1703.06354v1,http://arxiv.org/pdf/1703.06354v1,Goal Conflict in Designing an Autonomous Artificial System,goal conflict design autonom artifici system,"Research on human self-regulation has shown that people hold many goals simultaneously and have complex self-regulation mechanisms to deal with this goal conflict. Artificial autonomous systems may also need to find ways to cope with conflicting goals. Indeed, the intricate interplay among different goals may be critical to the design as well as long-term safety and stability of artificial autonomous systems. I discuss some of the critical features of the human self-regulation system and how it might be applied to an artificial system. Furthermore, the implications of goal conflict for the reliability and stability of artificial autonomous systems and ensuring their alignment with human goals and ethics is examined.",research human self regul shown peopl hold mani goal simultan complex self regul mechan deal goal conflict artifici autonom system may also need find way cope conflict goal inde intric interplay among differ goal may critic design well long term safeti stabil artifici autonom system discuss critic featur human self regul system might appli artifici system furthermor implic goal conflict reliabl stabil artifici autonom system ensur align human goal ethic examin,['Mark Muraven'],['cs.AI'],False,False,True,False,False,True
49,2017-03-28T14:05:32Z,2017-03-21T08:11:19Z,http://arxiv.org/abs/1703.06321v2,http://arxiv.org/pdf/1703.06321v2,Solving the Goddard problem by an influence diagram,solv goddard problem influenc diagram,Influence diagrams are a decision-theoretic extension of probabilistic graphical models. In this paper we show how they can be used to solve the Goddard problem. We present results of numerical experiments with this problem and compare the solutions provided by influence diagrams with the optimal solution.,influenc diagram decis theoret extens probabilist graphic model paper show use solv goddard problem present result numer experi problem compar solut provid influenc diagram optim solut,"['Jiří Vomlel', 'Václav Kratochvíl']","['cs.AI', '68T37', 'I.2']",False,False,False,False,False,True
51,2017-03-28T14:05:37Z,2017-03-18T09:04:05Z,http://arxiv.org/abs/1703.06275v1,http://arxiv.org/pdf/1703.06275v1,Evolving Game Skill-Depth using General Video Game AI Agents,evolv game skill depth use general video game ai agent,"Most games have, or can be generalised to have, a number of parameters that may be varied in order to provide instances of games that lead to very different player experiences. The space of possible parameter settings can be seen as a search space, and we can therefore use a Random Mutation Hill Climbing algorithm or other search methods to find the parameter settings that induce the best games. One of the hardest parts of this approach is defining a suitable fitness function. In this paper we explore the possibility of using one of a growing set of General Video Game AI agents to perform automatic play-testing. This enables a very general approach to game evaluation based on estimating the skill-depth of a game. Agent-based play-testing is computationally expensive, so we compare two simple but efficient optimisation algorithms: the Random Mutation Hill-Climber and the Multi-Armed Bandit Random Mutation Hill-Climber. For the test game we use a space-battle game in order to provide a suitable balance between simulation speed and potential skill-depth. Results show that both algorithms are able to rapidly evolve game versions with significant skill-depth, but that choosing a suitable resampling number is essential in order to combat the effects of noise.",game generalis number paramet may vari order provid instanc game lead veri differ player experi space possibl paramet set seen search space therefor use random mutat hill climb algorithm search method find paramet set induc best game one hardest part approach defin suitabl fit function paper explor possibl use one grow set general video game ai agent perform automat play test enabl veri general approach game evalu base estim skill depth game agent base play test comput expens compar two simpl effici optimis algorithm random mutat hill climber multi arm bandit random mutat hill climber test game use space battl game order provid suitabl balanc simul speed potenti skill depth result show algorithm abl rapid evolv game version signific skill depth choos suitabl resampl number essenti order combat effect nois,"['Jialin Liu', 'Julian Togelius', 'Diego Perez-Liebana', 'Simon M. Lucas']",['cs.AI'],False,False,False,False,False,True
52,2017-03-28T14:05:37Z,2017-03-21T14:26:33Z,http://arxiv.org/abs/1703.06207v2,http://arxiv.org/pdf/1703.06207v2,Cooperating with Machines,cooper machin,"Since Alan Turing envisioned Artificial Intelligence (AI) [1], a major driving force behind technical progress has been competition with human cognition. Historical milestones have been frequently associated with computers matching or outperforming humans in difficult cognitive tasks (e.g. face recognition [2], personality classification [3], driving cars [4], or playing video games [5]), or defeating humans in strategic zero-sum encounters (e.g. Chess [6], Checkers [7], Jeopardy! [8], Poker [9], or Go [10]). In contrast, less attention has been given to developing autonomous machines that establish mutually cooperative relationships with people who may not share the machine's preferences. A main challenge has been that human cooperation does not require sheer computational power, but rather relies on intuition [11], cultural norms [12], emotions and signals [13, 14, 15, 16], and pre-evolved dispositions toward cooperation [17], common-sense mechanisms that are difficult to encode in machines for arbitrary contexts. Here, we combine a state-of-the-art machine-learning algorithm with novel mechanisms for generating and acting on signals to produce a new learning algorithm that cooperates with people and other machines at levels that rival human cooperation in a variety of two-player repeated stochastic games. This is the first general-purpose algorithm that is capable, given a description of a previously unseen game environment, of learning to cooperate with people within short timescales in scenarios previously unanticipated by algorithm designers. This is achieved without complex opponent modeling or higher-order theories of mind, thus showing that flexible, fast, and general human-machine cooperation is computationally achievable using a non-trivial, but ultimately simple, set of algorithmic mechanisms.",sinc alan ture envis artifici intellig ai major drive forc behind technic progress competit human cognit histor mileston frequent associ comput match outperform human difficult cognit task face recognit person classif drive car play video game defeat human strateg zero sum encount chess checker jeopardi poker go contrast less attent given develop autonom machin establish mutual cooper relationship peopl may share machin prefer main challeng human cooper doe requir sheer comput power rather reli intuit cultur norm emot signal pre evolv disposit toward cooper common sens mechan difficult encod machin arbitrari context combin state art machin learn algorithm novel mechan generat act signal produc new learn algorithm cooper peopl machin level rival human cooper varieti two player repeat stochast game first general purpos algorithm capabl given descript previous unseen game environ learn cooper peopl within short timescal scenario previous unanticip algorithm design achiev without complex oppon model higher order theori mind thus show flexibl fast general human machin cooper comput achiev use non trivial ultim simpl set algorithm mechan,"['Jacob W. Crandall', 'Mayada Oudah', 'Tennom', 'Fatimah Ishowo-Oloko', 'Sherief Abdallah', 'Jean-François Bonnefon', 'Manuel Cebrian', 'Azim Shariff', 'Michael A. Goodrich', 'Iyad Rahwan']",['cs.AI'],False,False,False,False,False,True
53,2017-03-28T14:05:37Z,2017-03-25T15:54:36Z,http://arxiv.org/abs/1703.06182v2,http://arxiv.org/pdf/1703.06182v2,Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under   Partial Observability,deep decentr multi task multi agent reinforc learn partial observ,"Many real-world tasks involve multiple agents with partial observability and limited communication. Learning is challenging in these settings due to local viewpoints of agents, which perceive the world as non-stationary due to concurrently-exploring teammates. Approaches that learn specialized policies for individual tasks face major problems when applied to the real world: not only do agents have to learn and store a distinct policy for each task, but in practice the identity of the task is often non-observable, making these algorithms inapplicable. This paper formalizes and addresses the problem of multi-task multi-agent reinforcement learning under partial observability. We introduce a decentralized single-task learning approach that is robust to concurrent interactions of teammates, and present an approach for distilling single-task policies into a unified policy that performs well across multiple related tasks, without explicit provision of task identity.",mani real world task involv multipl agent partial observ limit communic learn challeng set due local viewpoint agent perceiv world non stationari due concurr explor teammat approach learn special polici individu task face major problem appli real world onli agent learn store distinct polici task practic ident task often non observ make algorithm inapplic paper formal address problem multi task multi agent reinforc learn partial observ introduc decentr singl task learn approach robust concurr interact teammat present approach distil singl task polici unifi polici perform well across multipl relat task without explicit provis task ident,"['Shayegan Omidshafiei', 'Jason Pazis', 'Christopher Amato', 'Jonathan P. How', 'John Vian']","['cs.LG', 'cs.AI']",False,False,False,False,False,True
55,2017-03-28T14:05:37Z,2017-03-17T15:00:03Z,http://arxiv.org/abs/1703.06045v1,http://arxiv.org/pdf/1703.06045v1,Approximation Complexity of Maximum A Posteriori Inference in   Sum-Product Networks,approxim complex maximum posteriori infer sum product network,"We discuss the computational complexity of approximating maximum a posteriori inference in sum-product networks. We first show NP-hardness in three-level trees by a reduction from maximum independent set; this implies non-approximability within a sublinear factor. We show that this is a tight bound, as we can find an approximation within a linear factor in three-level networks. We then show that in four-level trees it is NP-hard to approximate the problem within a factor $2^{f(n)}$ for any sublinear function $f$ of the size of the input $n$. Again, this is bound is tight, as we prove that the usual max-product algorithm finds (in any network) approximations within factor $2^{c n}$ from some constant $c < 1$. Last, we present a simple algorithm, and show that it provably produces solutions at least as good as, and potentially much better than, the max-product algorithm.",discuss comput complex approxim maximum posteriori infer sum product network first show np hard three level tree reduct maximum independ set impli non approxim within sublinear factor show tight bound find approxim within linear factor three level network show four level tree np hard approxim problem within factor ani sublinear function size input bound tight prove usual max product algorithm find ani network approxim within factor constant last present simpl algorithm show provabl produc solut least good potenti much better max product algorithm,"['Denis Deratani Mauá', 'Cassio P. de Campos']","['cs.AI', '68T37']",False,False,False,False,False,True
56,2017-03-28T14:05:37Z,2017-03-16T21:08:31Z,http://arxiv.org/abs/1703.05820v1,http://arxiv.org/pdf/1703.05820v1,Particle Value Functions,particl valu function,"The policy gradients of the expected return objective can react slowly to rare rewards. Yet, in some cases agents may wish to emphasize the low or high returns regardless of their probability. Borrowing from the economics and control literature, we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example. This risk-sensitive value function is not always applicable to reinforcement learning problems, so we introduce the particle value function defined by a particle filter over the distributions of an agent's experience, which bounds the risk-sensitive one. We illustrate the benefit of the policy gradients of this objective in Cliffworld.",polici gradient expect return object react slowli rare reward yet case agent may wish emphas low high return regardless probabl borrow econom control literatur review risk sensit valu function aris exponenti util illustr effect exampl risk sensit valu function alway applic reinforc learn problem introduc particl valu function defin particl filter distribut agent experi bound risk sensit one illustr benefit polici gradient object cliffworld,"['Chris J. Maddison', 'Dieterich Lawson', 'George Tucker', 'Nicolas Heess', 'Arnaud Doucet', 'Andriy Mnih', 'Yee Whye Teh']","['cs.LG', 'cs.AI']",False,False,False,False,False,True
59,2017-03-28T14:05:37Z,2017-03-16T12:53:52Z,http://arxiv.org/abs/1703.06042v1,http://arxiv.org/pdf/1703.06042v1,A Visual Web Tool to Perform What-If Analysis of Optimization Approaches,visual web tool perform analysi optim approach,"In Operation Research, practical evaluation is essential to validate the efficacy of optimization approaches. This paper promotes the usage of performance profiles as a standard practice to visualize and analyze experimental results. It introduces a Web tool to construct and export performance profiles as SVG or HTML files. In addition, the application relies on a methodology to estimate the benefit of hypothetical solver improvements. Therefore, the tool allows one to employ what-if analysis to screen possible research directions, and identify those having the best potential. The approach is showcased on two Operation Research technologies: Constraint Programming and Mixed Integer Linear Programming.",oper research practic evalu essenti valid efficaci optim approach paper promot usag perform profil standard practic visual analyz experiment result introduc web tool construct export perform profil svg html file addit applic reli methodolog estim benefit hypothet solver improv therefor tool allow one employ analysi screen possibl research direct identifi best potenti approach showcas two oper research technolog constraint program mix integ linear program,"['Sascha Van Cauwelaert', 'Michele Lombardi', 'Pierre Schaus']","['cs.AI', 'cs.PF']",False,False,False,False,False,True
60,2017-03-28T14:05:41Z,2017-03-16T03:36:28Z,http://arxiv.org/abs/1703.05468v1,http://arxiv.org/abs/1703.05468v1,Database Learning: Toward a Database that Becomes Smarter Every Time,databas learn toward databas becom smarter everi time,"In today's databases, previous query answers rarely benefit answering future queries. For the first time, to the best of our knowledge, we change this paradigm in an approximate query processing (AQP) context. We make the following observation: the answer to each query reveals some degree of knowledge about the answer to another query because their answers stem from the same underlying distribution that has produced the entire dataset. Exploiting and refining this knowledge should allow us to answer queries more analytically, rather than by reading enormous amounts of raw data. Also, processing more queries should continuously enhance our knowledge of the underlying distribution, and hence lead to increasingly faster response times for future queries.   We call this novel idea---learning from past query answers---Database Learning. We exploit the principle of maximum entropy to produce answers, which are in expectation guaranteed to be more accurate than existing sample-based approximations. Empowered by this idea, we build a query engine on top of Spark SQL, called Verdict. We conduct extensive experiments on real-world query traces from a large customer of a major database vendor. Our results demonstrate that database learning supports 73.7% of these queries, speeding them up by up to 23.0x for the same accuracy level compared to existing AQP systems.",today databas previous queri answer rare benefit answer futur queri first time best knowledg chang paradigm approxim queri process aqp context make follow observ answer queri reveal degre knowledg answer anoth queri becaus answer stem distribut produc entir dataset exploit refin knowledg allow us answer queri analyt rather read enorm amount raw data also process queri continu enhanc knowledg distribut henc lead increas faster respons time futur queri call novel idea learn past queri answer databas learn exploit principl maximum entropi produc answer expect guarante accur exist sampl base approxim empow idea build queri engin top spark sql call verdict conduct extens experi real world queri trace larg custom major databas vendor result demonstr databas learn support queri speed accuraci level compar exist aqp system,"['Yongjoo Park', 'Ahmad Shahab Tajik', 'Michael Cafarella', 'Barzan Mozafari']","['cs.DB', 'cs.AI']",False,False,True,False,False,True
61,2017-03-28T14:05:41Z,2017-03-16T01:37:25Z,http://arxiv.org/abs/1703.05452v1,http://arxiv.org/pdf/1703.05452v1,Efficient Online Learning for Optimizing Value of Information: Theory   and Application to Interactive Troubleshooting,effici onlin learn optim valu inform theori applic interact troubleshoot,"We consider the optimal value of information (VoI) problem, where the goal is to sequentially select a set of tests with a minimal cost, so that one can efficiently make the best decision based on the observed outcomes. Existing algorithms are either heuristics with no guarantees, or scale poorly (with exponential run time in terms of the number of available tests). Moreover, these methods assume a known distribution over the test outcomes, which is often not the case in practice. We propose an efficient sampling-based online learning framework to address the above issues. First, assuming the distribution over hypotheses is known, we propose a dynamic hypothesis enumeration strategy, which allows efficient information gathering with strong theoretical guarantees. We show that with sufficient amount of samples, one can identify a near-optimal decision with high probability. Second, when the parameters of the hypotheses distribution are unknown, we propose an algorithm which learns the parameters progressively via posterior sampling in an online fashion. We further establish a rigorous bound on the expected regret. We demonstrate the effectiveness of our approach on a real-world interactive troubleshooting application and show that one can efficiently make high-quality decisions with low cost.",consid optim valu inform voi problem goal sequenti select set test minim cost one effici make best decis base observ outcom exist algorithm either heurist guarante scale poor exponenti run time term number avail test moreov method assum known distribut test outcom often case practic propos effici sampl base onlin learn framework address abov issu first assum distribut hypothes known propos dynam hypothesi enumer strategi allow effici inform gather strong theoret guarante show suffici amount sampl one identifi near optim decis high probabl second paramet hypothes distribut unknown propos algorithm learn paramet progress via posterior sampl onlin fashion establish rigor bound expect regret demonstr effect approach real world interact troubleshoot applic show one effici make high qualiti decis low cost,"['Yuxin Chen', 'Jean-Michel Renders', 'Morteza Haghir Chehreghani', 'Andreas Krause']","['cs.AI', 'cs.LG', 'stat.ML']",False,False,False,False,False,True
62,2017-03-28T14:05:41Z,2017-03-16T01:31:33Z,http://arxiv.org/abs/1703.05449v1,http://arxiv.org/pdf/1703.05449v1,Minimax Regret Bounds for Reinforcement Learning,minimax regret bound reinforc learn,"We consider the problem of efficient exploration in finite horizon MDPs.We show that an optimistic modification to model-based value iteration, can achieve a regret bound $\tilde{O}( \sqrt{HSAT} + H^2S^2A+H\sqrt{T})$ where $H$ is the time horizon, $S$ the number of states, $A$ the number of actions and $T$ the time elapsed. This result improves over the best previous known bound $\tilde{O}(HS \sqrt{AT})$ achieved by the UCRL2 algorithm.The key significance of our new results is that when $T\geq H^3S^3A$ and $SA\geq H$, it leads to a regret of $\tilde{O}(\sqrt{HSAT})$ that matches the established lower bounds of $\Omega(\sqrt{HSAT})$ up to a logarithmic factor. Our analysis contain two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in $S$), and we use ""exploration bonuses"" based on Bernstein's inequality, together with using a recursive -Bellman-type- Law of Total Variance (to improve scaling in $H$).",consid problem effici explor finit horizon mdps show optimist modif model base valu iter achiev regret bound tild sqrt hsat sqrt time horizon number state number action time elaps result improv best previous known bound tild hs sqrt achiev ucrl algorithm key signific new result geq sa geq lead regret tild sqrt hsat match establish lower bound omega sqrt hsat logarithm factor analysi contain two key insight use care applic concentr inequ optim valu function whole rather transit probabl improv scale use explor bonus base bernstein inequ togeth use recurs bellman type law total varianc improv scale,"['Mohammad Gheshlaghi Azar', 'Ian Osband', 'Rémi Munos']","['stat.ML', 'cs.AI', 'cs.LG']",False,False,False,False,False,True
63,2017-03-28T14:05:41Z,2017-03-16T01:14:36Z,http://arxiv.org/abs/1703.05446v1,http://arxiv.org/pdf/1703.05446v1,Look into Person: Self-supervised Structure-sensitive Learning and A New   Benchmark for Human Parsing,look person self supervis structur sensit learn new benchmark human pars,"Human parsing has recently attracted a lot of research interests due to its huge application potentials. However existing datasets have limited number of images and annotations, and lack the variety of human appearances and the coverage of challenging cases in unconstrained environment. In this paper, we introduce a new benchmark ""Look into Person (LIP)"" that makes a significant advance in terms of scalability, diversity and difficulty, a contribution that we feel is crucial for future developments in human-centric analysis. This comprehensive dataset contains over 50,000 elaborately annotated images with 19 semantic part labels, which are captured from a wider range of viewpoints, occlusions and background complexity. Given these rich annotations we perform detailed analyses of the leading human parsing approaches, gaining insights into the success and failures of these methods. Furthermore, in contrast to the existing efforts on improving the feature discriminative capability, we solve human parsing by exploring a novel self-supervised structure-sensitive learning approach, which imposes human pose structures into parsing results without resorting to extra supervision (i.e., no need for specifically labeling human joints in model training). Our self-supervised learning framework can be injected into any advanced neural networks to help incorporate rich high-level knowledge regarding human joints from a global perspective and improve the parsing results. Extensive evaluations on our LIP and the public PASCAL-Person-Part dataset demonstrate the superiority of our method.",human pars recent attract lot research interest due huge applic potenti howev exist dataset limit number imag annot lack varieti human appear coverag challeng case unconstrain environ paper introduc new benchmark look person lip make signific advanc term scalabl divers difficulti contribut feel crucial futur develop human centric analysi comprehens dataset contain elabor annot imag semant part label captur wider rang viewpoint occlus background complex given rich annot perform detail analys lead human pars approach gain insight success failur method furthermor contrast exist effort improv featur discrimin capabl solv human pars explor novel self supervis structur sensit learn approach impos human pose structur pars result without resort extra supervis need specif label human joint model train self supervis learn framework inject ani advanc neural network help incorpor rich high level knowledg regard human joint global perspect improv pars result extens evalu lip public pascal person part dataset demonstr superior method,"['Ke Gong', 'Xiaodan Liang', 'Xiaohui Shen', 'Liang Lin']","['cs.CV', 'cs.AI', 'cs.LG']",False,False,False,False,False,True
66,2017-03-28T14:05:41Z,2017-03-15T20:23:45Z,http://arxiv.org/abs/1703.05376v1,http://arxiv.org/pdf/1703.05376v1,Concentration Bounds for Two Timescale Stochastic Approximation with   Applications to Reinforcement Learning,concentr bound two timescal stochast approxim applic reinforc learn,"Two-timescale Stochastic Approximation (SA) algorithms are widely used in Reinforcement Learning (RL). In such methods, the iterates consist of two parts that are updated using different stepsizes. We develop the first convergence rate result for these algorithms; in particular, we provide a general methodology for analyzing two-timescale linear SA. We apply our methodology to two-timescale RL algorithms such as GTD(0), GTD2, and TDC.",two timescal stochast approxim sa algorithm wide use reinforc learn rl method iter consist two part updat use differ stepsiz develop first converg rate result algorithm particular provid general methodolog analyz two timescal linear sa appli methodolog two timescal rl algorithm gtd gtd tdc,"['Gal Dalal', 'Balazs Szorenyi', 'Gugan Thoppe', 'Shie Mannor']",['cs.AI'],False,False,False,False,False,True
68,2017-03-28T14:05:41Z,2017-03-15T15:19:28Z,http://arxiv.org/abs/1703.05204v1,http://arxiv.org/pdf/1703.05204v1,On Inconsistency Indices and Inconsistency Axioms in Pairwise   Comparisons,inconsist indic inconsist axiom pairwis comparison,"Pairwise comparisons are an important tool of modern (multiple criteria) decision making. Since human judgments are often inconsistent, many studies focused on the ways how to express and measure this inconsistency, and several inconsistency indices were proposed as an alternative to Saaty inconsistency index and inconsistency ratio for reciprocal pairwise comparisons matrices. This paper aims to: firstly, introduce a new measure of inconsistency of pairwise comparisons and to prove its basic properties; secondly, to postulate an additional axiom, an upper boundary axiom, to an existing set of axioms; and the last, but not least, the paper provides proofs of satisfaction of this additional axiom by selected inconsistency indices as well as it provides their numerical comparison.",pairwis comparison import tool modern multipl criteria decis make sinc human judgment often inconsist mani studi focus way express measur inconsist sever inconsist indic propos altern saati inconsist index inconsist ratio reciproc pairwis comparison matric paper aim first introduc new measur inconsist pairwis comparison prove basic properti second postul addit axiom upper boundari axiom exist set axiom last least paper provid proof satisfact addit axiom select inconsist indic well provid numer comparison,['Jiri Mazurek'],['cs.AI'],False,False,True,False,False,True
69,2017-03-28T14:05:41Z,2017-03-15T15:13:42Z,http://arxiv.org/abs/1703.05201v1,http://arxiv.org/pdf/1703.05201v1,Fuzzy Rankings: Properties and Applications,fuzzi rank properti applic,"In practice, a ranking of objects with respect to given set of criteria is of considerable importance. However, due to lack of knowledge, information of time pressure, decision makers might not be able to provide a (crisp) ranking of objects from the top to the bottom. Instead, some objects might be ranked equally, or better than other objects only to some degree. In such cases, a generalization of crisp rankings to fuzzy rankings can be more useful. The aim of the article is to introduce the notion of a fuzzy ranking and to discuss its several properties, namely orderings, similarity and indecisiveness. The proposed approach can be used both for group decision making or multiple criteria decision making when uncertainty is involved.",practic rank object respect given set criteria consider import howev due lack knowledg inform time pressur decis maker might abl provid crisp rank object top bottom instead object might rank equal better object onli degre case general crisp rank fuzzi rank use aim articl introduc notion fuzzi rank discuss sever properti name order similar indecis propos approach use group decis make multipl criteria decis make uncertainti involv,['Jiří Mazurek'],['cs.AI'],False,False,False,False,False,True
71,2017-03-28T14:05:45Z,2017-03-15T05:43:48Z,http://arxiv.org/abs/1703.04940v1,http://arxiv.org/pdf/1703.04940v1,Resilience: A Criterion for Learning in the Presence of Arbitrary   Outliers,resili criterion learn presenc arbitrari outlier,"We introduce a criterion, resilience, which allows properties of a dataset (such as its mean or best low rank approximation) to be robustly computed, even in the presence of a large fraction of arbitrary additional data. Resilience is a weaker condition than most other properties considered so far in the literature, and yet enables robust estimation in a broader variety of settings, including the previously unstudied problem of robust mean estimation in $\ell_p$-norms.",introduc criterion resili allow properti dataset mean best low rank approxim robust comput even presenc larg fraction arbitrari addit data resili weaker condit properti consid far literatur yet enabl robust estim broader varieti set includ previous unstudi problem robust mean estim ell norm,"['Jacob Steinhardt', 'Moses Charikar', 'Gregory Valiant']","['cs.LG', 'cs.AI', 'cs.CC', 'cs.CR', 'stat.ML']",False,False,False,False,False,True
72,2017-03-28T14:05:45Z,2017-03-17T00:56:18Z,http://arxiv.org/abs/1703.04912v2,http://arxiv.org/pdf/1703.04912v2,Syntax-Preserving Belief Change Operators for Logic Programs,syntax preserv belief chang oper logic program,"Recent methods have adapted the well-established AGM and belief base frameworks for belief change to cover belief revision in logic programs. In this study here, we present two new sets of belief change operators for logic programs. They focus on preserving the explicit relationships expressed in the rules of a program, a feature that is missing in purely semantic approaches that consider programs only in their entirety. In particular, operators of the latter class fail to satisfy preservation and support, two important properties for belief change in logic programs required to ensure intuitive results.   We address this shortcoming of existing approaches by introducing partial meet and ensconcement constructions for logic program belief change, which allow us to define syntax-preserving operators that satisfy preservation and support. Our work is novel in that our constructions not only preserve more information from a logic program during a change operation than existing ones, but they also facilitate natural definitions of contraction operators, the first in the field to the best of our knowledge.   In order to evaluate the rationality of our operators, we translate the revision and contraction postulates from the AGM and belief base frameworks to the logic programming setting. We show that our operators fully comply with the belief base framework and formally state the interdefinability between our operators. We further propose an algorithm that is based on modularising a logic program to reduce partial meet and ensconcement revisions or contractions to performing the operation only on the relevant modules of that program. Finally, we compare our approach to two state-of-the-art logic program revision methods and demonstrate that our operators address the shortcomings of one and generalise the other method.",recent method adapt well establish agm belief base framework belief chang cover belief revis logic program studi present two new set belief chang oper logic program focus preserv explicit relationship express rule program featur miss pure semant approach consid program onli entireti particular oper latter class fail satisfi preserv support two import properti belief chang logic program requir ensur intuit result address shortcom exist approach introduc partial meet ensconc construct logic program belief chang allow us defin syntax preserv oper satisfi preserv support work novel construct onli preserv inform logic program dure chang oper exist one also facilit natur definit contract oper first field best knowledg order evalu ration oper translat revis contract postul agm belief base framework logic program set show oper fulli compli belief base framework formal state interdefin oper propos algorithm base modularis logic program reduc partial meet ensconc revis contract perform oper onli relev modul program final compar approach two state art logic program revis method demonstr oper address shortcom one generalis method,"['Sebastian Binnewies', 'Zhiqiang Zhuang', 'Kewen Wang', 'Bela Stantic']","['cs.AI', 'I.2.3; I.2.4; F.4.1']",False,False,False,False,False,True
74,2017-03-28T14:05:45Z,2017-03-15T01:04:49Z,http://arxiv.org/abs/1703.04862v1,http://arxiv.org/pdf/1703.04862v1,Exploring the Combination Rules of D Numbers From a Perspective of   Conflict Redistribution,explor combin rule number perspect conflict redistribut,"Dempster-Shafer theory of evidence is widely applied to uncertainty modelling and knowledge reasoning because of its advantages in dealing with uncertain information. But some conditions or requirements, such as exclusiveness hypothesis and completeness constraint, limit the development and application of that theory to a large extend. To overcome the shortcomings and enhance its capability of representing the uncertainty, a novel model, called D numbers, has been proposed recently. However, many key issues, for example how to implement the combination of D numbers, remain unsolved. In the paper, we have explored the combination of D Numbers from a perspective of conflict redistribution, and proposed two combination rules being suitable for different situations for the fusion of two D numbers. The proposed combination rules can reduce to the classical Dempster's rule in Dempster-Shafer theory under a certain conditions. Numerical examples and discussion about the proposed rules are also given in the paper.",dempster shafer theori evid wide appli uncertainti model knowledg reason becaus advantag deal uncertain inform condit requir exclus hypothesi complet constraint limit develop applic theori larg extend overcom shortcom enhanc capabl repres uncertainti novel model call number propos recent howev mani key issu exampl implement combin number remain unsolv paper explor combin number perspect conflict redistribut propos two combin rule suitabl differ situat fusion two number propos combin rule reduc classic dempster rule dempster shafer theori certain condit numer exampl discuss propos rule also given paper,"['Xinyang Deng', 'Wen Jiang']",['cs.AI'],False,False,True,False,False,True
76,2017-03-28T14:05:45Z,2017-03-14T22:13:20Z,http://arxiv.org/abs/1703.04756v1,http://arxiv.org/pdf/1703.04756v1,Weighted Voting Via No-Regret Learning,weight vote via regret learn,"Voting systems typically treat all voters equally. We argue that perhaps they should not: Voters who have supported good choices in the past should be given higher weight than voters who have supported bad ones. To develop a formal framework for desirable weighting schemes, we draw on no-regret learning. Specifically, given a voting rule, we wish to design a weighting scheme such that applying the voting rule, with voters weighted by the scheme, leads to choices that are almost as good as those endorsed by the best voter in hindsight. We derive possibility and impossibility results for the existence of such weighting schemes, depending on whether the voting rule and the weighting scheme are deterministic or randomized, as well as on the social choice axioms satisfied by the voting rule.",vote system typic treat voter equal argu perhap voter support good choic past given higher weight voter support bad one develop formal framework desir weight scheme draw regret learn specif given vote rule wish design weight scheme appli vote rule voter weight scheme lead choic almost good endors best voter hindsight deriv possibl imposs result exist weight scheme depend whether vote rule weight scheme determinist random well social choic axiom satisfi vote rule,"['Nika Haghtalab', 'Ritesh Noothigattu', 'Ariel D. Procaccia']","['cs.GT', 'cs.AI', 'cs.LG', 'cs.MA']",False,False,False,False,False,True
77,2017-03-28T14:05:45Z,2017-03-17T08:12:10Z,http://arxiv.org/abs/1703.04741v2,http://arxiv.org/pdf/1703.04741v2,Towards Moral Autonomous Systems,toward moral autonom system,"Both the ethics of autonomous systems and the problems of their technical implementation have by now been studied in some detail. Less attention has been given to the areas in which these two separate concerns meet. This paper, written by both philosophers and engineers of autonomous systems, addresses a number of issues in machine ethics that are located at precisely the intersection between ethics and engineering. We first discuss different approaches towards the conceptual design of autonomous systems and their implications on the ethics implementation in such systems. Then we examine problematic areas regarding the specification and verification of ethical behavior in autonomous systems, particularly with a view towards the requirements of future legislation. We discuss transparency and accountability issues that will be crucial for any future wide deployment of autonomous systems in society. Finally we consider the, often overlooked, possibility of intentional misuse of AI systems and the possible dangers arising out of deliberately unethical design, implementation, and use of autonomous robots.",ethic autonom system problem technic implement studi detail less attent given area two separ concern meet paper written philosoph engin autonom system address number issu machin ethic locat precis intersect ethic engin first discuss differ approach toward conceptu design autonom system implic ethic implement system examin problemat area regard specif verif ethic behavior autonom system particular view toward requir futur legisl discuss transpar account issu crucial ani futur wide deploy autonom system societi final consid often overlook possibl intent misus ai system possibl danger aris deliber uneth design implement use autonom robot,"['Vicky Charisi', 'Louise Dennis', 'Michael Fisher', 'Robert Lieck', 'Andreas Matthias', 'Marija Slavkovik', 'Janina Sombetzki', 'Alan F. T. Winfield', 'Roman Yampolskiy']",['cs.AI'],False,False,False,False,False,True
78,2017-03-28T14:05:45Z,2017-03-14T21:07:01Z,http://arxiv.org/abs/1703.04730v1,http://arxiv.org/pdf/1703.04730v1,Understanding Black-box Predictions via Influence Functions,understand black box predict via influenc function,"How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, identifying the points most responsible for a given prediction. Applying ideas from second-order optimization, we scale up influence functions to modern machine learning settings and show that they can be applied to high-dimensional black-box models, even in non-convex and non-differentiable settings. We give a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for many different purposes: to understand model behavior, debug models and detect dataset errors, and even identify and exploit vulnerabilities to adversarial training-set attacks.",explain predict black box model paper use influenc function classic techniqu robust statist trace model predict learn algorithm back train data identifi point respons given predict appli idea second order optim scale influenc function modern machin learn set show appli high dimension black box model even non convex non differenti set give simpl effici implement requir onli oracl access gradient hessian vector product linear model convolut neural network demonstr influenc function use mani differ purpos understand model behavior debug model detect dataset error even identifi exploit vulner adversari train set attack,"['Pang Wei Koh', 'Percy Liang']","['stat.ML', 'cs.AI', 'cs.LG']",False,False,False,False,False,True
79,2017-03-28T14:05:45Z,2017-03-14T19:14:32Z,http://arxiv.org/abs/1703.04677v1,http://arxiv.org/pdf/1703.04677v1,A computational investigation of sources of variability in sentence   comprehension difficulty in aphasia,comput investig sourc variabl sentenc comprehens difficulti aphasia,"We present a computational evaluation of three hypotheses about sources of deficit in sentence comprehension in aphasia: slowed processing, intermittent deficiency, and resource reduction. The ACT-R based Lewis & Vasishth 2005 model is used to implement these three proposals. Slowed processing is implemented as slowed default production-rule firing time; intermittent deficiency as increased random noise in activation of chunks in memory; and resource reduction as reduced goal activation. As data, we considered subject vs. object relatives presented in a self-paced listening modality to 56 individuals with aphasia (IWA) and 46 matched controls. The participants heard the sentences and carried out a picture verification task to decide on an interpretation of the sentence. These response accuracies are used to identify the best parameters (for each participant) that correspond to the three hypotheses mentioned above. We show that controls have more tightly clustered (less variable) parameter values than IWA; specifically, compared to controls, among IWA there are more individuals with low goal activations, high noise, and slow default action times. This suggests that (i) individual patients show differential amounts of deficit along the three dimensions of slowed processing, intermittent deficient, and resource reduction, (ii) overall, there is evidence for all three sources of deficit playing a role, and (iii) IWA have a more variable range of parameter values than controls. In sum, this study contributes a proof of concept of a quantitative implementation of, and evidence for, these three accounts of comprehension deficits in aphasia.",present comput evalu three hypothes sourc deficit sentenc comprehens aphasia slow process intermitt defici resourc reduct act base lewi vasishth model use implement three propos slow process implement slow default product rule fire time intermitt defici increas random nois activ chunk memori resourc reduct reduc goal activ data consid subject vs object relat present self pace listen modal individu aphasia iwa match control particip heard sentenc carri pictur verif task decid interpret sentenc respons accuraci use identifi best paramet particip correspond three hypothes mention abov show control tight cluster less variabl paramet valu iwa specif compar control among iwa individu low goal activ high nois slow default action time suggest individu patient show differenti amount deficit along three dimens slow process intermitt defici resourc reduct ii overal evid three sourc deficit play role iii iwa variabl rang paramet valu control sum studi contribut proof concept quantit implement evid three account comprehens deficit aphasia,"['Paul Mätzig', 'Shravan Vasishth', 'Felix Engelmann', 'David Caplan']","['cs.CL', 'cs.AI']",False,False,True,False,False,True
80,2017-03-28T14:05:50Z,2017-03-14T17:15:42Z,http://arxiv.org/abs/1703.04587v1,http://arxiv.org/pdf/1703.04587v1,Minimizing Maximum Regret in Commitment Constrained Sequential Decision   Making,minim maximum regret commit constrain sequenti decis make,"In cooperative multiagent planning, it can often be beneficial for an agent to make commitments about aspects of its behavior to others, allowing them in turn to plan their own behaviors without taking the agent's detailed behavior into account. Extending previous work in the Bayesian setting, we consider instead a worst-case setting in which the agent has a set of possible environments (MDPs) it could be in, and develop a commitment semantics that allows for probabilistic guarantees on the agent's behavior in any of the environments it could end up facing. Crucially, an agent receives observations (of reward and state transitions) that allow it to potentially eliminate possible environments and thus obtain higher utility by adapting its policy to the history of observations. We develop algorithms and provide theory and some preliminary empirical results showing that they ensure an agent meets its commitments with history-dependent policies while minimizing maximum regret over the possible environments.",cooper multiag plan often benefici agent make commit aspect behavior allow turn plan behavior without take agent detail behavior account extend previous work bayesian set consid instead worst case set agent set possibl environ mdps could develop commit semant allow probabilist guarante agent behavior ani environ could end face crucial agent receiv observ reward state transit allow potenti elimin possibl environ thus obtain higher util adapt polici histori observ develop algorithm provid theori preliminari empir result show ensur agent meet commit histori depend polici minim maximum regret possibl environ,"['Qi Zhang', 'Satinder Singh', 'Edmund Durfee']",['cs.AI'],False,False,False,False,False,True
81,2017-03-28T14:05:50Z,2017-03-13T17:58:36Z,http://arxiv.org/abs/1703.04529v1,http://arxiv.org/pdf/1703.04529v1,Task-based End-to-end Model Learning,task base end end model learn,"As machine learning techniques have become more ubiquitous, it has become common to see machine learning prediction algorithms operating within some larger process. However, the criteria by which we train machine learning algorithms often differ from the ultimate criteria on which we evaluate them. This paper proposes an end-to-end approach for learning probabilistic machine learning models within the context of stochastic programming, in a manner that directly captures the ultimate task-based objective for which they will be used. We then present two experimental evaluations of the proposed approach, one as applied to a generic inventory stock problem and the second to a real-world electrical grid scheduling task. In both cases, we show that the proposed approach can outperform both a traditional modeling approach and a purely black-box policy optimization approach.",machin learn techniqu becom ubiquit becom common see machin learn predict algorithm oper within larger process howev criteria train machin learn algorithm often differ ultim criteria evalu paper propos end end approach learn probabilist machin learn model within context stochast program manner direct captur ultim task base object use present two experiment evalu propos approach one appli generic inventori stock problem second real world electr grid schedul task case show propos approach outperform tradit model approach pure black box polici optim approach,"['Priya L. Donti', 'Brandon Amos', 'J. Zico Kolter']","['cs.LG', 'cs.AI']",False,False,False,False,False,True
83,2017-03-28T14:05:50Z,2017-03-13T17:13:51Z,http://arxiv.org/abs/1703.04489v1,http://arxiv.org/pdf/1703.04489v1,Reinforcement Learning for Transition-Based Mention Detection,reinforc learn transit base mention detect,"This paper describes an application of reinforcement learning to the mention detection task. We define a novel action-based formulation for the mention detection task, in which a model can flexibly revise past labeling decisions by grouping together tokens and assigning partial mention labels. We devise a method to create mention-level episodes and we train a model by rewarding correctly labeled complete mentions, irrespective of the inner structure created. The model yields results which are on par with a competitive supervised counterpart while being more flexible in terms of achieving targeted behavior through reward modeling and generating internal mention structure, especially on longer mentions.",paper describ applic reinforc learn mention detect task defin novel action base formul mention detect task model flexibl revis past label decis group togeth token assign partial mention label devis method creat mention level episod train model reward correct label complet mention irrespect inner structur creat model yield result par competit supervis counterpart flexibl term achiev target behavior reward model generat intern mention structur especi longer mention,"['Georgiana Dinu', 'Wael Hamza', 'Radu Florian']","['cs.CL', 'cs.AI']",False,False,False,False,False,True
84,2017-03-28T14:05:50Z,2017-03-13T13:45:13Z,http://arxiv.org/abs/1703.04389v1,http://arxiv.org/pdf/1703.04389v1,Bayesian Optimization with Gradients,bayesian optim gradient,"In recent years, Bayesian optimization has proven successful for global optimization of expensive-to-evaluate multimodal objective functions. However, unlike most optimization methods, Bayesian optimization typically does not use derivative information. In this paper we show how Bayesian optimization can exploit derivative information to decrease the number of objective function evaluations required for good performance. In particular, we develop a novel Bayesian optimization algorithm, the derivative-enabled knowledge-gradient (dKG), for which we show one-step Bayes-optimality, asymptotic consistency, and greater one-step value of information than is possible in the derivative-free setting. Our procedure accommodates noisy and incomplete derivative information, and comes in both sequential and batch forms. We show dKG provides state-of-the-art performance compared to a wide range of optimization procedures with and without gradients, on benchmarks including logistic regression, kernel learning, and k-nearest neighbors.",recent year bayesian optim proven success global optim expens evalu multimod object function howev unlik optim method bayesian optim typic doe use deriv inform paper show bayesian optim exploit deriv inform decreas number object function evalu requir good perform particular develop novel bayesian optim algorithm deriv enabl knowledg gradient dkg show one step bay optim asymptot consist greater one step valu inform possibl deriv free set procedur accommod noisi incomplet deriv inform come sequenti batch form show dkg provid state art perform compar wide rang optim procedur without gradient benchmark includ logist regress kernel learn nearest neighbor,"['Jian Wu', 'Matthias Poloczek', 'Andrew Gordon Wilson', 'Peter I. Frazier']","['stat.ML', 'cs.AI', 'cs.LG', 'math.OC']",False,False,True,False,False,True
85,2017-03-28T14:05:50Z,2017-03-13T13:32:46Z,http://arxiv.org/abs/1703.04382v1,http://arxiv.org/pdf/1703.04382v1,"Cost-Based Intuitionist Probabilities on Spaces of Graphs, Hypergraphs   and Theorems",cost base intuitionist probabl space graph hypergraph theorem,"A novel partial order is defined on the space of digraphs or hypergraphs, based on assessing the cost of producing a graph via a sequence of elementary transformations. Leveraging work by Knuth and Skilling on the foundations of inference, and the structure of Heyting algebras on graph space, this partial order is used to construct an intuitionistic probability measure that applies to either digraphs or hypergraphs. As logical inference steps can be represented as transformations on hypergraphs representing logical statements, this also yields an intuitionistic probability measure on spaces of theorems. The central result is also extended to yield intuitionistic probabilities based on more general weighted rule systems defined over bicartesian closed categories.",novel partial order defin space digraph hypergraph base assess cost produc graph via sequenc elementari transform leverag work knuth skill foundat infer structur heyt algebra graph space partial order use construct intuitionist probabl measur appli either digraph hypergraph logic infer step repres transform hypergraph repres logic statement also yield intuitionist probabl measur space theorem central result also extend yield intuitionist probabl base general weight rule system defin bicartesian close categori,['Ben Goertzel'],['cs.AI'],False,False,False,False,False,True
86,2017-03-28T14:05:50Z,2017-03-13T13:06:49Z,http://arxiv.org/abs/1703.04368v1,http://arxiv.org/pdf/1703.04368v1,Symbol Grounding via Chaining of Morphisms,symbol ground via chain morphism,"A new model of symbol grounding is presented, in which the structures of natural language, logical semantics, perception and action are represented categorically, and symbol grounding is modeled via the composition of morphisms between the relevant categories. This model gives conceptual insight into the fundamentally systematic nature of symbol grounding, and also connects naturally to practical real-world AI systems in current research and commercial use. Specifically, it is argued that the structure of linguistic syntax can be modeled as a certain asymmetric monoidal category, as e.g. implicit in the link grammar formalism; the structure of spatiotemporal relationships and action plans can be modeled similarly using ""image grammars"" and ""action grammars""; and common-sense logical semantic structure can be modeled using dependently-typed lambda calculus with uncertain truth values. Given these formalisms, the grounding of linguistic descriptions in spatiotemporal perceptions and coordinated actions consists of following morphisms from language to logic through to spacetime and body (for comprehension), and vice versa (for generation). The mapping is indicated between the spatial relationships in the Region Connection Calculus and Allen Interval Algebra and corresponding entries in the link grammar syntax parsing dictionary. Further, the abstractions introduced here are shown to naturally model the structures and systems currently being deployed in the context of using the OpenCog cognitive architecture to control Hanson Robotics humanoid robots.",new model symbol ground present structur natur languag logic semant percept action repres categor symbol ground model via composit morphism relev categori model give conceptu insight fundament systemat natur symbol ground also connect natur practic real world ai system current research commerci use specif argu structur linguist syntax model certain asymmetr monoid categori implicit link grammar formal structur spatiotempor relationship action plan model similar use imag grammar action grammar common sens logic semant structur model use depend type lambda calculus uncertain truth valu given formal ground linguist descript spatiotempor percept coordin action consist follow morphism languag logic spacetim bodi comprehens vice versa generat map indic spatial relationship region connect calculus allen interv algebra correspond entri link grammar syntax pars dictionari abstract introduc shown natur model structur system current deploy context use opencog cognit architectur control hanson robot humanoid robot,"['Ruiting Lian', 'Ben Goertzel', 'Linas Vepstas', 'David Hanson', 'Changle Zhou']",['cs.AI'],False,False,False,False,False,True
88,2017-03-28T14:05:50Z,2017-03-13T12:48:15Z,http://arxiv.org/abs/1703.04361v1,http://arxiv.org/pdf/1703.04361v1,Toward a Formal Model of Cognitive Synergy,toward formal model cognit synergi,"""Cognitive synergy"" refers to a dynamic in which multiple cognitive processes, cooperating to control the same cognitive system, assist each other in overcoming bottlenecks encountered during their internal processing. Cognitive synergy has been posited as a key feature of real-world general intelligence, and has been used explicitly in the design of the OpenCog cognitive architecture. Here category theory and related concepts are used to give a formalization of the cognitive synergy concept.   A series of formal models of intelligent agents is proposed, with increasing specificity and complexity: simple reinforcement learning agents; ""cognit"" agents with an abstract memory and processing model; hypergraph-based agents (in which ""cognit"" operations are carried out via hypergraphs); hypergraph agents with a rich language of nodes and hyperlinks (such as the OpenCog framework provides); ""PGMC"" agents whose rich hypergraphs are endowed with cognitive processes guided via Probabilistic Growth and Mining of Combinations; and finally variations of the PrimeAGI design, which is currently being built on top of OpenCog.   A notion of cognitive synergy is developed for cognitive processes acting within PGMC agents, based on developing a formal notion of ""stuckness,"" and defining synergy as a relationship between cognitive processes in which they can help each other out when they get stuck. It is proposed that cognitive processes relating to each other synergetically, associate in a certain way with functors that map into each other via natural transformations. Cognitive synergy is proposed to correspond to a certain inequality regarding the relative costs of different paths through certain commutation diagrams.   Applications of this notion of cognitive synergy to particular cognitive phenomena, and specific cognitive processes in the PrimeAGI design, are discussed.",cognit synergi refer dynam multipl cognit process cooper control cognit system assist overcom bottleneck encount dure intern process cognit synergi posit key featur real world general intellig use explicit design opencog cognit architectur categori theori relat concept use give formal cognit synergi concept seri formal model intellig agent propos increas specif complex simpl reinforc learn agent cognit agent abstract memori process model hypergraph base agent cognit oper carri via hypergraph hypergraph agent rich languag node hyperlink opencog framework provid pgmc agent whose rich hypergraph endow cognit process guid via probabilist growth mine combin final variat primeagi design current built top opencog notion cognit synergi develop cognit process act within pgmc agent base develop formal notion stuck defin synergi relationship cognit process help get stuck propos cognit process relat synerget associ certain way functor map via natur transform cognit synergi propos correspond certain inequ regard relat cost differ path certain commut diagram applic notion cognit synergi particular cognit phenomena specif cognit process primeagi design discuss,['Ben Goertzel'],['cs.AI'],False,False,False,False,False,True
90,2017-03-28T14:05:54Z,2017-03-13T01:49:27Z,http://arxiv.org/abs/1703.04221v1,http://arxiv.org/pdf/1703.04221v1,A Hierarchical Framework of Cloud Resource Allocation and Power   Management Using Deep Reinforcement Learning,hierarch framework cloud resourc alloc power manag use deep reinforc learn,"Automatic decision-making approaches, such as reinforcement learning (RL), have been applied to (partially) solve the resource allocation problem adaptively in the cloud computing system. However, a complete cloud resource allocation framework exhibits high dimensions in state and action spaces, which prohibit the usefulness of traditional RL techniques. In addition, high power consumption has become one of the critical concerns in design and control of cloud computing systems, which degrades system reliability and increases cooling cost. An effective dynamic power management (DPM) policy should minimize power consumption while maintaining performance degradation within an acceptable level. Thus, a joint virtual machine (VM) resource allocation and power management framework is critical to the overall cloud computing system. Moreover, novel solution framework is necessary to address the even higher dimensions in state and action spaces. In this paper, we propose a novel hierarchical framework for solving the overall resource allocation and power management problem in cloud computing systems. The proposed hierarchical framework comprises a global tier for VM resource allocation to the servers and a local tier for distributed power management of local servers. The emerging deep reinforcement learning (DRL) technique, which can deal with complicated control problems with large state space, is adopted to solve the global tier problem. Furthermore, an autoencoder and a novel weight sharing structure are adopted to handle the high-dimensional state space and accelerate the convergence speed. On the other hand, the local tier of distributed server power managements comprises an LSTM based workload predictor and a model-free RL based power manager, operating in a distributed manner.",automat decis make approach reinforc learn rl appli partial solv resourc alloc problem adapt cloud comput system howev complet cloud resourc alloc framework exhibit high dimens state action space prohibit use tradit rl techniqu addit high power consumpt becom one critic concern design control cloud comput system degrad system reliabl increas cool cost effect dynam power manag dpm polici minim power consumpt maintain perform degrad within accept level thus joint virtual machin vm resourc alloc power manag framework critic overal cloud comput system moreov novel solut framework necessari address even higher dimens state action space paper propos novel hierarch framework solv overal resourc alloc power manag problem cloud comput system propos hierarch framework compris global tier vm resourc alloc server local tier distribut power manag local server emerg deep reinforc learn drl techniqu deal complic control problem larg state space adopt solv global tier problem furthermor autoencod novel weight share structur adopt handl high dimension state space acceler converg speed hand local tier distribut server power manag compris lstm base workload predictor model free rl base power manag oper distribut manner,"['Ning Liu', 'Zhe Li', 'Zhiyuan Xu', 'Jielong Xu', 'Sheng Lin', 'Qinru Qiu', 'Jian Tang', 'Yanzhi Wang']","['cs.DC', 'cs.AI']",False,False,False,False,False,True
91,2017-03-28T14:05:54Z,2017-03-15T08:16:37Z,http://arxiv.org/abs/1703.04159v2,http://arxiv.org/pdf/1703.04159v2,Any-Angle Pathfinding for Multiple Agents Based on SIPP Algorithm,ani angl pathfind multipl agent base sipp algorithm,"The problem of finding conflict-free trajectories for multiple agents of identical circular shape, operating in shared 2D workspace, is addressed in the paper and decoupled, e.g., prioritized, approach is used to solve this problem. Agents' workspace is tessellated into the square grid on which any-angle moves are allowed, e.g. each agent can move into an arbitrary direction as long as this move follows the straight line segment whose endpoints are tied to the distinct grid elements. A novel any-angle planner based on Safe Interval Path Planning (SIPP) algorithm is proposed to find trajectories for an agent moving amidst dynamic obstacles (other agents) on a grid. This algorithm is then used as part of a prioritized multi-agent planner AA-SIPP(m). On the theoretical, side we show that AA-SIPP(m) is complete under well-defined conditions. On the experimental side, in simulation tests with up to 200 agents involved, we show that our planner finds much better solutions in terms of cost (up to 20%) compared to the planners relying on cardinal moves only.",problem find conflict free trajectori multipl agent ident circular shape oper share workspac address paper decoupl priorit approach use solv problem agent workspac tessel squar grid ani angl move allow agent move arbitrari direct long move follow straight line segment whose endpoint tie distinct grid element novel ani angl planner base safe interv path plan sipp algorithm propos find trajectori agent move amidst dynam obstacl agent grid algorithm use part priorit multi agent planner aa sipp theoret side show aa sipp complet well defin condit experiment side simul test agent involv show planner find much better solut term cost compar planner reli cardin move onli,"['Konstantin Yakovlev', 'Anton Andreychuk']",['cs.AI'],False,False,False,False,False,True
92,2017-03-28T14:05:54Z,2017-03-12T13:17:08Z,http://arxiv.org/abs/1703.04115v1,http://arxiv.org/pdf/1703.04115v1,"BetaRun 2017 Team Description Paper: Variety, Complexity, and Learning",betarun team descript paper varieti complex learn,"RoboCup offers a set of benchmark problems for Artificial Intelligence in form of official world championships since 1997. The most tactical advanced and richest in terms of behavioural complexity of these is the 2D Soccer Simulation League, a simulated robotic soccer competition. BetaRun is a new attempt combining both machine learning and manual programming approaches, with the ultimate goal to arrive at a team that is trained entirely from observing and playing games, and a successor of the World Champion team Gliders 2016.",robocup offer set benchmark problem artifici intellig form offici world championship sinc tactic advanc richest term behaviour complex soccer simul leagu simul robot soccer competit betarun new attempt combin machin learn manual program approach ultim goal arriv team train entir observ play game successor world champion team glider,"['Olivia Michael', 'Oliver Obst']",['cs.AI'],False,False,True,False,False,True
94,2017-03-28T14:05:54Z,2017-03-11T20:24:06Z,http://arxiv.org/abs/1703.04565v1,http://arxiv.org/pdf/1703.04565v1,Fuzzy Model Tree For Early Effort Estimation,fuzzi model tree earli effort estim,"Use Case Points (UCP) is a well-known method to estimate the project size, based on Use Case diagram, at early phases of software development. Although the Use Case diagram is widely accepted as a de-facto model for analyzing object oriented software requirements over the world, UCP method did not take sufficient amount of attention because, as yet, there is no consensus on how to produce software effort from UCP. This paper aims to study the potential of using Fuzzy Model Tree to derive effort estimates based on UCP size measure using a dataset collected for that purpose. The proposed approach has been validated against Treeboost model, Multiple Linear Regression and classical effort estimation based on the UCP model. The obtained results are promising and show better performance than those obtained by classical UCP, Multiple Linear Regression and slightly better than those obtained by Tree boost model.",use case point ucp well known method estim project size base use case diagram earli phase softwar develop although use case diagram wide accept de facto model analyz object orient softwar requir world ucp method take suffici amount attent becaus yet consensus produc softwar effort ucp paper aim studi potenti use fuzzi model tree deriv effort estim base ucp size measur use dataset collect purpos propos approach valid treeboost model multipl linear regress classic effort estim base ucp model obtain result promis show better perform obtain classic ucp multipl linear regress slight better obtain tree boost model,"['Mohammad Azzeh', 'Ali Bou Nassif']","['cs.SE', 'cs.AI']",False,False,False,False,False,True
96,2017-03-28T14:05:54Z,2017-03-11T09:08:48Z,http://arxiv.org/abs/1703.03933v1,http://arxiv.org/pdf/1703.03933v1,Micro-Objective Learning : Accelerating Deep Reinforcement Learning   through the Discovery of Continuous Subgoals,micro object learn acceler deep reinforc learn discoveri continu subgoal,"Recently, reinforcement learning has been successfully applied to the logical game of Go, various Atari games, and even a 3D game, Labyrinth, though it continues to have problems in sparse reward settings. It is difficult to explore, but also difficult to exploit, a small number of successes when learning policy. To solve this issue, the subgoal and option framework have been proposed. However, discovering subgoals online is too expensive to be used to learn options in large state spaces. We propose Micro-objective learning (MOL) to solve this problem. The main idea is to estimate how important a state is while training and to give an additional reward proportional to its importance. We evaluated our algorithm in two Atari games: Montezuma's Revenge and Seaquest. With three experiments to each game, MOL significantly improved the baseline scores. Especially in Montezuma's Revenge, MOL achieved two times better results than the previous state-of-the-art model.",recent reinforc learn success appli logic game go various atari game even game labyrinth though continu problem spars reward set difficult explor also difficult exploit small number success learn polici solv issu subgoal option framework propos howev discov subgoal onlin expens use learn option larg state space propos micro object learn mol solv problem main idea estim import state train give addit reward proport import evalu algorithm two atari game montezuma reveng seaquest three experi game mol signific improv baselin score especi montezuma reveng mol achiev two time better result previous state art model,"['Sungtae Lee', 'Sang-Woo Lee', 'Jinyoung Choi', 'Dong-Hyun Kwak', 'Byoung-Tak Zhang']",['cs.AI'],False,False,False,False,False,True
98,2017-03-28T14:05:54Z,2017-03-11T06:37:09Z,http://arxiv.org/abs/1703.03916v1,http://arxiv.org/pdf/1703.03916v1,Axioms in Model-based Planners,axiom model base planner,Axioms can be used to model derived predicates in domain- independent planning models. Formulating models which use axioms can sometimes result in problems with much smaller search spaces and shorter plans than the original model. Previous work on axiom-aware planners focused solely on state- space search planners. We propose axiom-aware planners based on answer set programming and integer programming. We evaluate them on PDDL domains with axioms and show that they can exploit additional expressivity of axioms.,axiom use model deriv predic domain independ plan model formul model use axiom sometim result problem much smaller search space shorter plan origin model previous work axiom awar planner focus sole state space search planner propos axiom awar planner base answer set program integ program evalu pddl domain axiom show exploit addit express axiom,"['Shuwa Miura', 'Alex Fukunaga']",['cs.AI'],False,False,False,False,False,True
99,2017-03-28T14:05:54Z,2017-03-11T05:35:09Z,http://arxiv.org/abs/1703.03912v1,http://arxiv.org/pdf/1703.03912v1,The Curse of Correlation in Security Games and Principle of Max-Entropy,curs correl secur game principl max entropi,"In this paper, we identify and study a fundamental, yet underexplored, phenomenon in security games, which we term the Curse of Correlation (CoC). Specifically, we observe that there is inevitable correlation among the protection status of different targets. Such correlation is a crucial concern, especially in spatio-temporal domains like conservation area patrolling, where attackers can monitor patrollers at certain areas and then infer their patrolling routes using such correlation. To mitigate this issue, we introduce the principle of max-entropy to security games, and focus on designing entropy-maximizing defending strategies for the spatio-temporal security game -- a major victim of CoC. We prove that the problem is #P-hard in general, but propose efficient algorithms in well-motivated special settings. Our experiments show significant advantages of the max-entropy algorithms against previous algorithms.",paper identifi studi fundament yet underexplor phenomenon secur game term curs correl coc specif observ inevit correl among protect status differ target correl crucial concern especi spatio tempor domain like conserv area patrol attack monitor patrol certain area infer patrol rout use correl mitig issu introduc principl max entropi secur game focus design entropi maxim defend strategi spatio tempor secur game major victim coc prove problem hard general propos effici algorithm well motiv special set experi show signific advantag max entropi algorithm previous algorithm,"['Haifeng Xu', 'Milind Tambe', 'Shaddin Dughmi', 'Venil Loyd Noronha']","['cs.GT', 'cs.AI', 'cs.CR']",False,False,True,False,False,True
100,2017-03-28T14:06:40Z,2017-03-25T22:50:20Z,http://arxiv.org/abs/1703.08746v1,http://arxiv.org/pdf/1703.08746v1,Proof Verification Can Be Hard!,proof verif hard,"The generally accepted wisdom in computational circles is that pure proof verification is a solved problem and that the computationally hard elements and fertile areas of study lie in proof discovery. This wisdom presumably does hold for conventional proof systems such as first-order logic with a standard proof calculus such as natural deduction or resolution. But this folk belief breaks down when we consider more user-friendly/powerful inference rules. One such rule is the restricted {\omega}-rule, which is not even semi-decidable when added to a standard proof calculus of a nice theory. While presumably not a novel result, we feel that the hardness of proof verification is under-appreciated in most communities that deal with proofs. A proof-sketch follows.",general accept wisdom comput circl pure proof verif solv problem comput hard element fertil area studi lie proof discoveri wisdom presum doe hold convent proof system first order logic standard proof calculus natur deduct resolut folk belief break consid user friend power infer rule one rule restrict omega rule even semi decid ad standard proof calculus nice theori presum novel result feel hard proof verif appreci communiti deal proof proof sketch follow,"['Naveen Sundar Govindarajulu', 'Selmer Bringsjord']","['cs.LO', 'cs.CC']",False,False,True,False,False,True
101,2017-03-28T14:06:40Z,2017-03-23T16:50:03Z,http://arxiv.org/abs/1703.08139v1,http://arxiv.org/pdf/1703.08139v1,"Optimal lower bounds for universal relation, samplers, and finding   duplicates",optim lower bound univers relat sampler find duplic,"In the communication problem $\mathbf{UR}$ (universal relation) [KRW95], Alice and Bob respectively receive $x$ and $y$ in $\{0,1\}^n$ with the promise that $x\neq y$. The last player to receive a message must output an index $i$ such that $x_i\neq y_i$. We prove that the randomized one-way communication complexity of this problem in the public coin model is exactly $\Theta(\min\{n, \log(1/\delta)\log^2(\frac{n}{\log(1/\delta)})\})$ bits for failure probability $\delta$. Our lower bound holds even if promised $\mathop{support}(y)\subset \mathop{support}(x)$. As a corollary, we obtain optimal lower bounds for $\ell_p$-sampling in strict turnstile streams for $0\le p < 2$, as well as for the problem of finding duplicates in a stream. Our lower bounds do not need to use large weights, and hold even if it is promised that $x\in\{0,1\}^n$ at all points in the stream.   Our lower bound demonstrates that any algorithm $\mathcal{A}$ solving sampling problems in turnstile streams in low memory can be used to encode subsets of $[n]$ of certain sizes into a number of bits below the information theoretic minimum. Our encoder makes adaptive queries to $\mathcal{A}$ throughout its execution, but done carefully so as to not violate correctness. This is accomplished by injecting random noise into the encoder's interactions with $\mathcal{A}$, which is loosely motivated by techniques in differential privacy. Our correctness analysis involves understanding the ability of $\mathcal{A}$ to correctly answer adaptive queries which have positive but bounded mutual information with $\mathcal{A}$'s internal randomness, and may be of independent interest in the newly emerging area of adaptive data analysis with a theoretical computer science lens.",communic problem mathbf ur univers relat krw alic bob respect receiv promis neq last player receiv messag must output index neq prove random one way communic complex problem public coin model exact theta min log delta log frac log delta bit failur probabl delta lower bound hold even promis mathop support subset mathop support corollari obtain optim lower bound ell sampl strict turnstil stream le well problem find duplic stream lower bound need use larg weight hold even promis point stream lower bound demonstr ani algorithm mathcal solv sampl problem turnstil stream low memori use encod subset certain size number bit inform theoret minimum encod make adapt queri mathcal throughout execut done care violat correct accomplish inject random nois encod interact mathcal loos motiv techniqu differenti privaci correct analysi involv understand abil mathcal correct answer adapt queri posit bound mutual inform mathcal intern random may independ interest newli emerg area adapt data analysi theoret comput scienc len,"['Jelani Nelson', 'Jakub Pachocki', 'Zhengyu Wang']","['cs.CC', 'cs.DS']",False,False,False,False,False,True
102,2017-03-28T14:06:40Z,2017-03-23T00:00:41Z,http://arxiv.org/abs/1703.07891v1,http://arxiv.org/pdf/1703.07891v1,Width Hierarchies for Quantum and Classical Ordered Binary Decision   Diagrams with Repeated Test,width hierarchi quantum classic order binari decis diagram repeat test,"We consider quantum, nondterministic and probabilistic versions of known computational model Ordered Read-$k$-times Branching Programs or Ordered Binary Decision Diagrams with repeated test ($k$-QOBDD, $k$-NOBDD and $k$-POBDD). We show width hierarchy for complexity classes of Boolean function computed by these models and discuss relation between different variants of $k$-OBDD.",consid quantum nondterminist probabilist version known comput model order read time branch program order binari decis diagram repeat test qobdd nobdd pobdd show width hierarchi complex class boolean function comput model discuss relat differ variant obdd,"['Kamil Khadiev', 'Rishat Ibrahimov']","['cs.CC', 'quant-ph']",False,False,False,False,False,True
103,2017-03-28T14:06:40Z,2017-03-22T19:56:27Z,http://arxiv.org/abs/1703.07833v1,http://arxiv.org/pdf/1703.07833v1,"Information complexity of the AND function in the two-Party, and   multiparty settings",inform complex function two parti multiparti set,"In a recent breakthrough paper [M. Braverman, A. Garg, D. Pankratov, and O. Weinstein, From information to exact communication, STOC'13] Braverman et al. developed a local characterization for the zero-error information complexity in the two party model, and used it to compute the exact internal and external information complexity of the 2-bit AND function, which was then applied to determine the exact asymptotic of randomized communication complexity of the set disjointness problem.   In this article, we extend their results on AND function to the multi-party number-in-hand model by proving that the generalization of their protocol has optimal internal and external information cost for certain distributions. Our proof has new components, and in particular it fixes some minor gaps in the proof of Braverman et al.",recent breakthrough paper braverman garg pankratov weinstein inform exact communic stoc braverman et al develop local character zero error inform complex two parti model use comput exact intern extern inform complex bit function appli determin exact asymptot random communic complex set disjoint problem articl extend result function multi parti number hand model prove general protocol optim intern extern inform cost certain distribut proof new compon particular fix minor gap proof braverman et al,"['Yuval Filmus', 'Hamed Hatami', 'Yaqiao Li', 'Suzin You']",['cs.CC'],False,False,True,False,False,True
104,2017-03-28T14:06:40Z,2017-03-23T07:30:19Z,http://arxiv.org/abs/1703.07768v2,http://arxiv.org/pdf/1703.07768v2,Quantum Communication-Query Tradeoffs,quantum communic queri tradeoff,"For any function $f: X \times Y \to Z$, we prove that $Q^{*\text{cc}}(f) \cdot Q^{\text{OIP}}(f) \cdot (\log Q^{\text{OIP}}(f) + \log  Z ) \geq \Omega(\log  X )$. Here, $Q^{*\text{cc}}(f)$ denotes the bounded-error communication complexity of $f$ using an entanglement-assisted two-way qubit channel, and $Q^{\text{OIP}}(f)$ denotes the number of quantum queries needed to determine $x$ with high probability given oracle access to the function $f_x(y) \stackrel{\text{def}}{=} f(x, y)$. We show that this tradeoff is close to the best possible. We also give a generalization of this tradeoff for distributional query complexity.   As an application, we prove an optimal $\Omega(\log q)$ lower bound on the $Q^{*\text{cc}}$ complexity of determining whether $x + y$ is a perfect square, where Alice holds $x \in \mathbf{F}_q$, Bob holds $y \in \mathbf{F}_q$, and $\mathbf{F}_q$ is a finite field of odd characteristic. As another application, we give a new, simpler proof that searching an ordered size-$N$ database requires $\Omega(\log N / \log \log N)$ quantum queries. (It was already known that $\Theta(\log N)$ queries are required.)",ani function time prove text cc cdot text oip cdot log text oip log geq omega log text cc denot bound error communic complex use entangl assist two way qubit channel text oip denot number quantum queri need determin high probabl given oracl access function stackrel text def show tradeoff close best possibl also give general tradeoff distribut queri complex applic prove optim omega log lower bound text cc complex determin whether perfect squar alic hold mathbf bob hold mathbf mathbf finit field odd characterist anoth applic give new simpler proof search order size databas requir omega log log log quantum queri alreadi known theta log queri requir,['William M. Hoza'],"['cs.CC', 'quant-ph']",False,False,False,False,False,True
105,2017-03-28T14:06:40Z,2017-03-22T14:14:35Z,http://arxiv.org/abs/1703.07666v1,http://arxiv.org/pdf/1703.07666v1,Query-to-Communication Lifting for BPP,queri communic lift bpp,"For any $n$-bit boolean function $f$, we show that the randomized communication complexity of the composed function $f\circ g^n$, where $g$ is an index gadget, is characterized by the randomized decision tree complexity of $f$. In particular, this means that many query complexity separations involving randomized models (e.g., classical vs. quantum) automatically imply analogous separations in communication complexity.",ani bit boolean function show random communic complex compos function circ index gadget character random decis tree complex particular mean mani queri complex separ involv random model classic vs quantum automat impli analog separ communic complex,"['Mika Göös', 'Toniann Pitassi', 'Thomas Watson']",['cs.CC'],False,False,False,False,False,True
106,2017-03-28T14:06:40Z,2017-03-23T20:55:10Z,http://arxiv.org/abs/1703.07657v2,http://arxiv.org/pdf/1703.07657v2,"A Counterexample to the ""Majority is Least Stable"" Conjecture",counterexampl major least stabl conjectur,"We exhibit a linear threshold function in 5 variables with strictly smaller noise stability (for small values of the correlation parameter) than the majority function on 5 variables, thereby providing a counterexample to the ""Majority is Least Stable"" Conjecture of Benjamini, Kalai, and Schramm.",exhibit linear threshold function variabl strict smaller nois stabil small valu correl paramet major function variabl therebi provid counterexampl major least stabl conjectur benjamini kalai schramm,['Vishesh Jain'],"['cs.CC', 'math.PR']",False,False,False,False,False,True
107,2017-03-28T14:06:40Z,2017-03-22T04:45:51Z,http://arxiv.org/abs/1703.07521v1,http://arxiv.org/pdf/1703.07521v1,Lifting randomized query complexity to randomized communication   complexity,lift random queri complex random communic complex,"We show that for any (partial) query function $f:\{0,1\}^n\rightarrow \{0,1\}$, the randomized communication complexity of $f$ composed with $\mathrm{Index}^n_m$ (with $m= \mathrm{poly}(n)$) is at least the randomized query complexity of $f$ times $\log n$. Here $\mathrm{Index}_m : [m] \times \{0,1\}^m \rightarrow \{0,1\}$ is defined as $\mathrm{Index}_m(x,y)= y_x$ (the $x$th bit of $y$).   Our proof follows on the lines of Raz and Mckenzie [RM99] (and its generalization due to [GPW15]), who showed a lifting theorem for deterministic query complexity to deterministic communication complexity. Our proof deviates from theirs in an important fashion that we consider partitions of rectangles into many sub-rectangles, as opposed to a particular sub-rectangle with desirable properties, as considered by Raz and McKenzie. As a consequence of our main result, some known separations between quantum and classical communication complexities follow from analogous separations in the query world.",show ani partial queri function rightarrow random communic complex compos mathrm index mathrm poli least random queri complex time log mathrm index time rightarrow defin mathrm index th bit proof follow line raz mckenzi rm general due gpw show lift theorem determinist queri complex determinist communic complex proof deviat import fashion consid partit rectangl mani sub rectangl oppos particular sub rectangl desir properti consid raz mckenzi consequ main result known separ quantum classic communic complex follow analog separ queri world,"['Anurag Anshu', 'Naresh B. Goud', 'Rahul Jain', 'Srijita Kundu', 'Priyanka Mukhopadhyay']","['cs.CC', 'quant-ph']",False,False,False,False,False,True
108,2017-03-28T14:06:40Z,2017-03-21T19:48:23Z,http://arxiv.org/abs/1703.07406v1,http://arxiv.org/pdf/1703.07406v1,Subset sum problem in polycyclic groups,subset sum problem polycycl group,We consider a group-theoretic analogue of the classic subset sum problem. It is known that every virtually nilpotent group has polynomial time decidable subset sum problem. In this paper we use subgroup distortion to show that every polycyclic non-virtually-nilpotent group has NP-complete subset sum problem.,consid group theoret analogu classic subset sum problem known everi virtual nilpot group polynomi time decid subset sum problem paper use subgroup distort show everi polycycl non virtual nilpot group np complet subset sum problem,"['Andrey Nikolaev', 'Alexander Ushakov']","['math.GR', 'cs.CC', 'math.CO', '03D15, 20F65, 20F10, 20F16']",False,False,False,False,False,True
111,2017-03-28T14:06:44Z,2017-03-19T11:24:26Z,http://arxiv.org/abs/1703.06423v1,http://arxiv.org/pdf/1703.06423v1,The Hardness of Embedding Grids and Walls,hard embed grid wall,"The dichotomy conjecture for the parameterized embedding problem states that the problem of deciding whether a given graph $G$ from some class $K$ of ""pattern graphs"" can be embedded into a given graph $H$ (that is, is isomorphic to a subgraph of $H$) is fixed-parameter tractable if $K$ is a class of graphs of bounded tree width and $W[1]$-complete otherwise.   Towards this conjecture, we prove that the embedding problem is $W[1]$-complete if $K$ is the class of all grids or the class of all walls.",dichotomi conjectur parameter embed problem state problem decid whether given graph class pattern graph embed given graph isomorph subgraph fix paramet tractabl class graph bound tree width complet otherwis toward conjectur prove embed problem complet class grid class wall,"['Yijia Chen', 'Martin Grohe', 'Bingkai Lin']",['cs.CC'],False,False,False,False,False,True
113,2017-03-28T14:06:44Z,2017-03-17T15:08:17Z,http://arxiv.org/abs/1703.06048v1,http://arxiv.org/pdf/1703.06048v1,An FPTAS for the Knapsack Problem with Parametric Weights,fptas knapsack problem parametr weight,"In this paper, we investigate the parametric weight knapsack problem, in which the item weights are affine functions of the form $w_i(\lambda) = a_i + \lambda \cdot b_i$ for $i \in \{1,\ldots,n\}$ depending on a real-valued parameter $\lambda$. The aim is to provide a solution for all values of the parameter. It is well-known that any exact algorithm for the problem may need to output an exponential number of knapsack solutions. We present the first fully polynomial-time approximation scheme (FPTAS) for the problem that, for any desired precision $\varepsilon \in (0,1)$, computes $(1-\varepsilon)$-approximate solutions for all values of the parameter. Our FPTAS is based on two different approaches and achieves a running time of $\mathcal{O}(n^3/\varepsilon^2 \cdot \min\{ \log^2 P, n^2 \} \cdot \min\{\log M, n \log (n/\varepsilon) / \log(n \log (n/\varepsilon) )\})$ where $P$ is an upper bound on the optimal profit and $M := \max\{W, n \cdot \max\{a_i,b_i: i \in \{1,\ldots,n\}\}\}$ for a knapsack with capacity $W$.",paper investig parametr weight knapsack problem item weight affin function form lambda lambda cdot ldot depend real valu paramet lambda aim provid solut valu paramet well known ani exact algorithm problem may need output exponenti number knapsack solut present first fulli polynomi time approxim scheme fptas problem ani desir precis varepsilon comput varepsilon approxim solut valu paramet fptas base two differ approach achiev run time mathcal varepsilon cdot min log cdot min log log varepsilon log log varepsilon upper bound optim profit max cdot max ldot knapsack capac,"['Michael Holzhauser', 'Sven O. Krumke']","['cs.DS', 'cs.CC', 'math.OC']",False,False,False,False,False,True
114,2017-03-28T14:06:44Z,2017-03-17T03:58:22Z,http://arxiv.org/abs/1703.05881v1,http://arxiv.org/pdf/1703.05881v1,Complexity of Correspondence Homomorphisms,complex correspond homomorph,"Correspondence homomorphisms are both a generalization of standard homomorphisms and a generalization of correspondence colourings. For a fixed target graph $H$, the problem is to decide whether an input graph $G$, with each edge labeled by a pair of permutations of $V(H)$, admits a homomorphism to $H$ 'corresponding' to the labels, in a sense explained below.   We classify the complexity of this problem as a function of the fixed graph $H$. It turns out that there is dichotomy -- each of the problems is polynomial-time solvable or NP-complete. While most graphs $H$ yield NP-complete problems, there are interesting cases of graphs $H$ for which we solve the problem by Gaussian elimination.   We also classify the complexity of the analogous correspondence list homomorphism problems.   In this note we only include the proofs for the case $H$ is reflexive.",correspond homomorph general standard homomorph general correspond colour fix target graph problem decid whether input graph edg label pair permut admit homomorph correspond label sens explain classifi complex problem function fix graph turn dichotomi problem polynomi time solvabl np complet graph yield np complet problem interest case graph solv problem gaussian elimin also classifi complex analog correspond list homomorph problem note onli includ proof case reflex,"['Tomas Feder', 'Pavol Hell']","['cs.DM', 'cs.CC', 'math.CO']",False,False,False,False,False,True
115,2017-03-28T14:06:44Z,2017-03-16T18:18:51Z,http://arxiv.org/abs/1703.05784v1,http://arxiv.org/pdf/1703.05784v1,A Nearly Optimal Lower Bound on the Approximate Degree of AC$^0$,near optim lower bound approxim degre ac,"The approximate degree of a Boolean function $f \colon \{-1, 1\}^n \rightarrow \{-1, 1\}$ is the least degree of a real polynomial that approximates $f$ pointwise to error at most $1/3$. We introduce a generic method for increasing the approximate degree of a given function, while preserving its computability by constant-depth circuits.   Specifically, we show how to transform any Boolean function $f$ with approximate degree $d$ into a function $F$ on $O(n \cdot \operatorname{polylog}(n))$ variables with approximate degree at least $D = \Omega(n^{1/3} \cdot d^{2/3})$. In particular, if $d= n^{1-\Omega(1)}$, then $D$ is polynomially larger than $d$. Moreover, if $f$ is computed by a polynomial-size Boolean circuit of constant depth, then so is $F$.   By recursively applying our transformation, for any constant $\delta > 0$ we exhibit an AC$^0$ function of approximate degree $\Omega(n^{1-\delta})$. This improves over the best previous lower bound of $\Omega(n^{2/3})$ due to Aaronson and Shi (J. ACM 2004), and nearly matches the trivial upper bound of $n$ that holds for any function. Our lower bounds also apply to (quasipolynomial-size) DNFs of polylogarithmic width.   We describe several applications of these results. We give:   * For any constant $\delta > 0$, an $\Omega(n^{1-\delta})$ lower bound on the quantum communication complexity of a function in AC$^0$.   * A Boolean function $f$ with approximate degree at least $C(f)^{2-o(1)}$, where $C(f)$ is the certificate complexity of $f$. This separation is optimal up to the $o(1)$ term in the exponent.   * Improved secret sharing schemes with reconstruction procedures in AC$^0$.",approxim degre boolean function colon rightarrow least degre real polynomi approxim pointwis error introduc generic method increas approxim degre given function preserv comput constant depth circuit specif show transform ani boolean function approxim degre function cdot operatornam polylog variabl approxim degre least omega cdot particular omega polynomi larger moreov comput polynomi size boolean circuit constant depth recurs appli transform ani constant delta exhibit ac function approxim degre omega delta improv best previous lower bound omega due aaronson shi acm near match trivial upper bound hold ani function lower bound also appli quasipolynomi size dnfs polylogarithm width describ sever applic result give ani constant delta omega delta lower bound quantum communic complex function ac boolean function approxim degre least certif complex separ optim term expon improv secret share scheme reconstruct procedur ac,"['Mark Bun', 'Justin Thaler']",['cs.CC'],False,False,False,False,False,True
116,2017-03-28T14:06:44Z,2017-03-15T18:00:05Z,http://arxiv.org/abs/1703.05332v1,http://arxiv.org/pdf/1703.05332v1,Complexity of sampling as an order parameter,complex sampl order paramet,"We consider the classical complexity of approximately simulating time evolution under spatially local quadratic bosonic Hamiltonians for time $t$. We obtain upper and lower bounds on the scaling of $t$ with the number of bosons, $n$, for which simulation, cast as a sampling problem, is classically efficient and provably hard, respectively. We view these results in the light of classifying phases of physical systems based on parameters in the Hamiltonian and conjecture a link to dynamical phase transitions. In doing so, we combine ideas from mathematical physics and computational complexity to gain insight into the behavior of condensed matter systems.",consid classic complex approxim simul time evolut spatial local quadrat boson hamiltonian time obtain upper lower bound scale number boson simul cast sampl problem classic effici provabl hard respect view result light classifi phase physic system base paramet hamiltonian conjectur link dynam phase transit combin idea mathemat physic comput complex gain insight behavior condens matter system,"['Abhinav Deshpande', 'Bill Fefferman', 'Michael Foss-Feig', 'Alexey V. Gorshkov']","['quant-ph', 'cond-mat.quant-gas', 'cs.CC']",False,False,True,False,False,True
117,2017-03-28T14:06:44Z,2017-03-15T14:21:40Z,http://arxiv.org/abs/1703.05170v1,http://arxiv.org/pdf/1703.05170v1,Busy beavers and Kolmogorov complexity,busi beaver kolmogorov complex,"The idea to find the ""maximal number that can be named"" can be traced back to Archimedes (see his Psammit). From the viewpoint of computation theory the natural question is ""which number can be described by at most n bits""? This question led to the definition of the so-called ""busy beaver"" numbers (introduced by T. Rado). In this note we consider different versions of the busy beaver-like notions defined in terms of Kolmogorov complexity. We show that these versions differ depending on the version of complexity used (plain, prefix, or a priori complexities) and find out how these notions are related, providing matching lower and upper bounds.",idea find maxim number name trace back archimed see psammit viewpoint comput theori natur question number describ bit question led definit call busi beaver number introduc rado note consid differ version busi beaver like notion defin term kolmogorov complex show version differ depend version complex use plain prefix priori complex find notion relat provid match lower upper bound,['Mikhail Andreev'],['cs.CC'],False,False,False,False,False,True
118,2017-03-28T14:06:44Z,2017-03-15T13:51:23Z,http://arxiv.org/abs/1703.05156v1,http://arxiv.org/pdf/1703.05156v1,Complexity Dichotomies for the Minimum F-Overlay Problem,complex dichotomi minimum overlay problem,"For a (possibly infinite) fixed family of graphs F, we say that a graph G overlays F on a hypergraph H if V(H) is equal to V(G) and the subgraph of G induced by every hyperedge of H contains some member of F as a spanning subgraph.While it is easy to see that the complete graph on  V(H)  overlays F on a hypergraph H whenever the problem admits a solution, the Minimum F-Overlay problem asks for such a graph with the minimum number of edges.This problem allows to generalize some natural problems which may arise in practice. For instance, if the family F contains all connected graphs, then Minimum F-Overlay corresponds to the Minimum Connectivity Inference problem (also known as Subset Interconnection Design problem) introduced for the low-resolution reconstruction of macro-molecular assembly in structural biology, or for the design of networks.Our main contribution is a strong dichotomy result regarding the polynomial vs. NP-hard status with respect to the considered family F. Roughly speaking, we show that the easy cases one can think of (e.g. when edgeless graphs of the right sizes are in F, or if F contains only cliques) are the only families giving rise to a polynomial problem: all others are NP-complete.We then investigate the parameterized complexity of the problem and give similar sufficient conditions on F that give rise to W[1]-hard, W[2]-hard or FPT problems when the parameter is the size of the solution.This yields an FPT/W[1]-hard dichotomy for a relaxed problem, where every hyperedge of H must contain some member of F as a (non necessarily spanning) subgraph.",possibl infinit fix famili graph say graph overlay hypergraph equal subgraph induc everi hyperedg contain member span subgraph easi see complet graph overlay hypergraph whenev problem admit solut minimum overlay problem ask graph minimum number edg problem allow general natur problem may aris practic instanc famili contain connect graph minimum overlay correspond minimum connect infer problem also known subset interconnect design problem introduc low resolut reconstruct macro molecular assembl structur biolog design network main contribut strong dichotomi result regard polynomi vs np hard status respect consid famili rough speak show easi case one think edgeless graph right size contain onli cliqu onli famili give rise polynomi problem np complet investig parameter complex problem give similar suffici condit give rise hard hard fpt problem paramet size solut yield fpt hard dichotomi relax problem everi hyperedg must contain member non necessarili span subgraph,"['Nathann Cohen', 'Frédéric Havet', 'Dorian Mazauric', 'Ignasi Sau', 'Rémi Watrigant']","['cs.DS', 'cs.CC']",False,False,False,False,False,True
119,2017-03-28T14:06:44Z,2017-03-15T08:54:12Z,http://arxiv.org/abs/1703.05015v1,http://arxiv.org/pdf/1703.05015v1,Lower Bound and Hierarchies for Quantum Ordered Read-$k$-times Branching   Programs,lower bound hierarchi quantum order read time branch program,"We consider quantum version of known computational model Ordered Read-$k$-times Branching Programs or Ordered Binary Decision Diagrams with repeated test ($k$-QOBDD). We get lower bound for quantum $k$-OBDD for $k=o(\sqrt{n})$. This lower bound gives connection between characteristics of model and number of subfunctions for function.   Additionally, we prove the hierarchies for sublinear width bounded error quantum $k$-OBDDs using our lower bounds for $ k=o(\sqrt{n})$. Also we prove hierarchy for polynomial size bounded error quantum $k$-OBDDs constant $k$, and it differs from situation with unbounded error where known that increasing of $k$ does not gives any advantages.   Finally, we discuss relations between different classical and quantum models of $k$-OBDD.",consid quantum version known comput model order read time branch program order binari decis diagram repeat test qobdd get lower bound quantum obdd sqrt lower bound give connect characterist model number subfunct function addit prove hierarchi sublinear width bound error quantum obdd use lower bound sqrt also prove hierarchi polynomi size bound error quantum obdd constant differ situat unbound error known increas doe give ani advantag final discuss relat differ classic quantum model obdd,"['Farid Ablayev', 'Kamil Khadiev', 'Aliya Khadieva']","['cs.CC', 'quant-ph']",False,False,False,False,False,True
120,2017-03-28T14:06:48Z,2017-03-15T05:43:48Z,http://arxiv.org/abs/1703.04940v1,http://arxiv.org/pdf/1703.04940v1,Resilience: A Criterion for Learning in the Presence of Arbitrary   Outliers,resili criterion learn presenc arbitrari outlier,"We introduce a criterion, resilience, which allows properties of a dataset (such as its mean or best low rank approximation) to be robustly computed, even in the presence of a large fraction of arbitrary additional data. Resilience is a weaker condition than most other properties considered so far in the literature, and yet enables robust estimation in a broader variety of settings, including the previously unstudied problem of robust mean estimation in $\ell_p$-norms.",introduc criterion resili allow properti dataset mean best low rank approxim robust comput even presenc larg fraction arbitrari addit data resili weaker condit properti consid far literatur yet enabl robust estim broader varieti set includ previous unstudi problem robust mean estim ell norm,"['Jacob Steinhardt', 'Moses Charikar', 'Gregory Valiant']","['cs.LG', 'cs.AI', 'cs.CC', 'cs.CR', 'stat.ML']",False,False,False,False,False,True
121,2017-03-28T14:06:48Z,2017-03-14T17:22:19Z,http://arxiv.org/abs/1703.04598v1,http://arxiv.org/pdf/1703.04598v1,Verification in Staged Tile Self-Assembly,verif stage tile self assembl,"We prove the unique assembly and unique shape verification problems, benchmark measures of self-assembly model power, are $\mathrm{coNP}^{\mathrm{NP}}$-hard and contained in $\mathrm{PSPACE}$ (and in $\mathrm{\Pi}^\mathrm{P}_{2s}$ for staged systems with $s$ stages). En route, we prove that unique shape verification problem in the 2HAM is $\mathrm{coNP}^{\mathrm{NP}}$-complete.",prove uniqu assembl uniqu shape verif problem benchmark measur self assembl model power mathrm conp mathrm np hard contain mathrm pspace mathrm pi mathrm stage system stage en rout prove uniqu shape verif problem ham mathrm conp mathrm np complet,"['Robert Schweller', 'Andrew Winslow', 'Tim Wylie']",['cs.CC'],False,False,False,False,False,True
122,2017-03-28T14:06:48Z,2017-03-13T15:47:16Z,http://arxiv.org/abs/1703.04456v1,http://arxiv.org/pdf/1703.04456v1,"P=?NP as minimization of degree 4 polynomial, or Grassmann number   problem",np minim degre polynomi grassmann number problem,"While the P vs NP problem is mainly being attacked form the point of view of discrete mathematics, this paper propses two reformulations into the field of abstract algebra and of continuous global optimization - which advanced tools might bring new perspectives and approaches to attack this problem. The first one is equivalence of satisfying the 3-SAT problem with the question of reaching zero of a nonnegative degree 4 multivariate polynomial. This continuous search between boolean 0 and 1 values could be attacked using methods of global optimization, suggesting exponential growth of the number of local minima, what might be also a crucial issue for example for adiabatic quantum computers. The second discussed approach is using anti-commuting Grassmann numbers $\theta_i$, making $(A \cdot \textrm{diag}(\theta_i))^n$ nonzero only if $A$ has a Hamilton cycle. Hence, the P$\ne$NP assumption implies exponential growth of matrix representation of Grassmann numbers.",vs np problem main attack form point view discret mathemat paper props two reformul field abstract algebra continu global optim advanc tool might bring new perspect approach attack problem first one equival satisfi sat problem question reach zero nonneg degre multivari polynomi continu search boolean valu could attack use method global optim suggest exponenti growth number local minima might also crucial issu exampl adiabat quantum comput second discuss approach use anti commut grassmann number theta make cdot textrm diag theta nonzero onli hamilton cycl henc ne np assumpt impli exponenti growth matrix represent grassmann number,['Jarek Duda'],['cs.CC'],False,False,False,False,False,True
123,2017-03-28T14:06:48Z,2017-03-13T09:26:05Z,http://arxiv.org/abs/1703.04300v1,http://arxiv.org/pdf/1703.04300v1,A Note on the Inapproximability of Induced Disjoint Paths,note inapproxim induc disjoint path,"We study the inapproximability of the induced disjoint paths problem on an arbitrary $n$-node $m$-edge undirected graph, which is to connect the maximum number of the $k$ source-sink pairs given in the graph via induced disjoint paths. It is known that the problem is NP-hard to approximate within $m^{{1\over 2}-\varepsilon}$ for a general $k$ and any $\varepsilon>0$. In this paper, we prove that the problem is NP-hard to approximate within $n^{1-\varepsilon}$ for a general $k$ and any $\varepsilon>0$ by giving a simple reduction from the independent set problem.",studi inapproxim induc disjoint path problem arbitrari node edg undirect graph connect maximum number sourc sink pair given graph via induc disjoint path known problem np hard approxim within varepsilon general ani varepsilon paper prove problem np hard approxim within varepsilon general ani varepsilon give simpl reduct independ set problem,"['Gaoxiu Dong', 'Weidong Chen']",['cs.CC'],False,False,False,False,False,True
125,2017-03-28T14:06:48Z,2017-03-12T17:11:49Z,http://arxiv.org/abs/1703.04143v1,http://arxiv.org/pdf/1703.04143v1,Bernoulli Factories and Black-Box Reductions in Mechanism Design,bernoulli factori black box reduct mechan design,"We provide a polynomial time reduction from Bayesian incentive compatible mechanism design to Bayesian algorithm design for welfare maximization problems. Unlike prior results, our reduction achieves exact incentive compatibility for problems with multi-dimensional and continuous type spaces. The key technical barrier preventing exact incentive compatibility in prior black-box reductions is that repairing violations of incentive constraints requires understanding the distribution of the mechanism's output. Reductions that instead estimate the output distribution by sampling inevitably suffer from sampling error, which typically precludes exact incentive compatibility.   We overcome this barrier by employing and generalizing the computational model in the literature on Bernoulli Factories. In a Bernoulli factory problem, one is given a function mapping the bias of an ""input coin"" to that of an ""output coin"", and the challenge is to efficiently simulate the output coin given sample access to the input coin. We generalize this to the ""expectations from samples"" computational model, in which an instance is specified by a function mapping the expected values of a set of input distributions to a distribution over outcomes. The challenge is to give a polynomial time algorithm that exactly samples from the distribution over outcomes given only sample access to the input distributions. In this model, we give a polynomial time algorithm for the exponential weights: expected values of the input distributions correspond to the weights of alternatives and we wish to select an alternative with probability proportional to an exponential function of its weight. This algorithm is the key ingredient in designing an incentive compatible mechanism for bipartite matching, which can be used to make the approximately incentive compatible reduction of Hartline et al. (2015) exactly incentive compatible.",provid polynomi time reduct bayesian incent compat mechan design bayesian algorithm design welfar maxim problem unlik prior result reduct achiev exact incent compat problem multi dimension continu type space key technic barrier prevent exact incent compat prior black box reduct repair violat incent constraint requir understand distribut mechan output reduct instead estim output distribut sampl inevit suffer sampl error typic preclud exact incent compat overcom barrier employ general comput model literatur bernoulli factori bernoulli factori problem one given function map bias input coin output coin challeng effici simul output coin given sampl access input coin general expect sampl comput model instanc specifi function map expect valu set input distribut distribut outcom challeng give polynomi time algorithm exact sampl distribut outcom given onli sampl access input distribut model give polynomi time algorithm exponenti weight expect valu input distribut correspond weight altern wish select altern probabl proport exponenti function weight algorithm key ingredi design incent compat mechan bipartit match use make approxim incent compat reduct hartlin et al exact incent compat,"['Shaddin Dughmi', 'Jason Hartline', 'Robert Kleinberg', 'Rad Niazadeh']","['cs.GT', 'cs.CC', 'cs.DS', 'math.PR']",False,False,False,False,False,True
126,2017-03-28T14:06:48Z,2017-03-10T16:03:26Z,http://arxiv.org/abs/1703.03734v1,http://arxiv.org/pdf/1703.03734v1,On matrices with displacement structure: generalized operators and   faster algorithms,matric displac structur general oper faster algorithm,"For matrices with displacement structure, basic operations like multiplication, inversion, and linear system solving can all be expressed in terms of the following task: evaluate the product $\mathsf{A}\mathsf{B}$, where $\mathsf{A}$ is a structured $n \times n$ matrix of displacement rank $\alpha$, and $\mathsf{B}$ is an arbitrary $n\times\alpha$ matrix. Given $\mathsf{B}$ and a so-called ""generator"" of $\mathsf{A}$, this product is classically computed with a cost ranging from $O(\alpha^2 \mathscr{M}(n))$ to $O(\alpha^2 \mathscr{M}(n)\log(n))$ arithmetic operations, depending on the type of structure of $\mathsf{A}$; here, $\mathscr{M}$ is a cost function for polynomial multiplication. In this paper, we first generalize classical displacement operators, based on block diagonal matrices with companion diagonal blocks, and then design fast algorithms to perform the task above for this extended class of structured matrices. The cost of these algorithms ranges from $O(\alpha^{\omega-1} \mathscr{M}(n))$ to $O(\alpha^{\omega-1} \mathscr{M}(n)\log(n))$, with $\omega$ such that two $n \times n$ matrices over a field can be multiplied using $O(n^\omega)$ field operations. By combining this result with classical randomized regularization techniques, we obtain faster Las Vegas algorithms for structured inversion and linear system solving.",matric displac structur basic oper like multipl invers linear system solv express term follow task evalu product mathsf mathsf mathsf structur time matrix displac rank alpha mathsf arbitrari time alpha matrix given mathsf call generat mathsf product classic comput cost rang alpha mathscr alpha mathscr log arithmet oper depend type structur mathsf mathscr cost function polynomi multipl paper first general classic displac oper base block diagon matric companion diagon block design fast algorithm perform task abov extend class structur matric cost algorithm rang alpha omega mathscr alpha omega mathscr log omega two time matric field multipli use omega field oper combin result classic random regular techniqu obtain faster las vega algorithm structur invers linear system solv,"['Alin Bostan', 'Claude-Pierre Jeannerod', 'Christophe Mouilleron', 'Éric Schost']","['cs.SC', 'cs.CC', '68W30, 68Q25, 97H60', 'I.1.2']",False,False,False,False,False,True
127,2017-03-28T14:06:48Z,2017-03-10T08:35:24Z,http://arxiv.org/abs/1703.03575v1,http://arxiv.org/pdf/1703.03575v1,Crossing the Logarithmic Barrier for Dynamic Boolean Data Structure   Lower Bounds,cross logarithm barrier dynam boolean data structur lower bound,"This paper proves the first super-logarithmic lower bounds on the cell probe complexity of dynamic boolean (a.k.a. decision) data structure problems, a long-standing milestone in data structure lower bounds.   We introduce a new method for proving dynamic cell probe lower bounds and use it to prove a $\tilde{\Omega}(\log^{1.5} n)$ lower bound on the operational time of a wide range of boolean data structure problems, most notably, on the query time of dynamic range counting over $\mathbb{F}_2$ ([Pat07]). Proving an $\omega(\lg n)$ lower bound for this problem was explicitly posed as one of five important open problems in the late Mihai P\v{a}tra\c{s}cu's obituary [Tho13]. This result also implies the first $\omega(\lg n)$ lower bound for the classical 2D range counting problem, one of the most fundamental data structure problems in computational geometry and spatial databases. We derive similar lower bounds for boolean versions of dynamic polynomial evaluation and 2D rectangle stabbing, and for the (non-boolean) problems of range selection and range median.   Our technical centerpiece is a new way of ""weakly"" simulating dynamic data structures using efficient one-way communication protocols with small advantage over random guessing. This simulation involves a surprising excursion to low-degree (Chebychev) polynomials which may be of independent interest, and offers an entirely new algorithmic angle on the ""cell sampling"" method of Panigrahy et al. [PTW10].",paper prove first super logarithm lower bound cell probe complex dynam boolean decis data structur problem long stand mileston data structur lower bound introduc new method prove dynam cell probe lower bound use prove tild omega log lower bound oper time wide rang boolean data structur problem notabl queri time dynam rang count mathbb pat prove omega lg lower bound problem explicit pose one five import open problem late mihai tra cu obituari tho result also impli first omega lg lower bound classic rang count problem one fundament data structur problem comput geometri spatial databas deriv similar lower bound boolean version dynam polynomi evalu rectangl stab non boolean problem rang select rang median technic centerpiec new way weak simul dynam data structur use effici one way communic protocol small advantag random guess simul involv surpris excurs low degre chebychev polynomi may independ interest offer entir new algorithm angl cell sampl method panigrahi et al ptw,"['Kasper Green Larsen', 'Omri Weinstein', 'Huacheng Yu']","['cs.DS', 'cs.CC', 'cs.CG', 'cs.IT', 'math.IT']",False,False,False,False,False,True
129,2017-03-28T14:06:48Z,2017-03-08T20:14:27Z,http://arxiv.org/abs/1703.03021v1,http://arxiv.org/pdf/1703.03021v1,A dichotomy theorem for nonuniform CSPs,dichotomi theorem nonuniform csps,In this paper we prove the Dichotomy Conjecture on the complexity of nonuniform constraint satisfaction problems posed by Feder and Vardi.,paper prove dichotomi conjectur complex nonuniform constraint satisfact problem pose feder vardi,['Andrei A. Bulatov'],['cs.CC'],False,False,False,False,False,True
130,2017-03-28T14:06:52Z,2017-03-07T16:50:43Z,http://arxiv.org/abs/1703.02469v1,http://arxiv.org/pdf/1703.02469v1,Random CNFs are Hard for Cutting Planes,random cnfs hard cut plane,"The random k-SAT model is the most important and well-studied distribution over k-SAT instances. It is closely connected to statistical physics; it is used as a testbench for satisfiability algorithms, and average-case hardness over this distribution has also been linked to hardness of approximation via Feige's hypothesis. We prove that any Cutting Planes refutation for random k-SAT requires exponential size, for k that is logarithmic in the number of variables, in the (interesting) regime where the number of clauses guarantees that the formula is unsatisfiable with high probability.",random sat model import well studi distribut sat instanc close connect statist physic use testbench satisfi algorithm averag case hard distribut also link hard approxim via feig hypothesi prove ani cut plane refut random sat requir exponenti size logarithm number variabl interest regim number claus guarante formula unsatisfi high probabl,"['Noah Fleming', 'Denis Pankratov', 'Toniann Pitassi', 'Robert Robere']",['cs.CC'],False,False,False,False,False,True
131,2017-03-28T14:06:52Z,2017-03-07T12:45:26Z,http://arxiv.org/abs/1703.02361v1,http://arxiv.org/pdf/1703.02361v1,On the family of 0/1-polytopes with NP-complete non-adjacency relation,famili polytop np complet non adjac relat,"In 1995 T. Matsui considered a special family 0/1-polytopes for which the problem of recognizing the non-adjacency of two arbitrary vertices is NP-complete. In 2012 the author of this paper established that all the polytopes of this family are present as faces in the polytopes associated with the following NP-complete problems: the traveling salesman problem, the 3-satisfiability problem, the knapsack problem, the set covering problem, the partial ordering problem, the cube subgraph problem, and some others. In particular, it follows that for these families the non-adjacency relation is also NP-complete. On the other hand, it is known that the vertex adjacency criterion is polynomial for polytopes of the following NP-complete problems: the maximum independent set problem, the set packing and the set partitioning problem, the three-index assignment problem. It is shown that none of the polytopes of the above-mentioned special family (with the exception of a one-dimensional segment) can be the face of polytopes associated with the problems of the maximum independent set, of a set packing and partitioning, and of 3-assignments.",matsui consid special famili polytop problem recogn non adjac two arbitrari vertic np complet author paper establish polytop famili present face polytop associ follow np complet problem travel salesman problem satisfi problem knapsack problem set cover problem partial order problem cube subgraph problem particular follow famili non adjac relat also np complet hand known vertex adjac criterion polynomi polytop follow np complet problem maximum independ set problem set pack set partit problem three index assign problem shown none polytop abov mention special famili except one dimension segment face polytop associ problem maximum independ set set pack partit assign,['Alexander Maksimenko'],['cs.CC'],False,False,False,False,False,True
132,2017-03-28T14:06:52Z,2017-03-07T11:22:53Z,http://arxiv.org/abs/1703.02332v1,http://arxiv.org/pdf/1703.02332v1,The Minimum Shared Edges Problem on Grid-like Graphs,minimum share edg problem grid like graph,"We study the NP-hard Minimum Shared Edges (MSE) problem on graphs: decide whether it is possible to route $p$ paths from a start vertex to a target vertex in a given graph while using at most $k$ edges more than once. We show that MSE can be decided on bounded grids in linear time when both dimensions are either small or large compared to the number $p$ of paths. On the contrary, we show that MSE remains NP-hard on subgraphs of bounded grids. Finally, we study MSE from a parametrised complexity point of view. It is known that MSE is fixed-parameter tractable with respect to the number $p$ of paths. We show that, under standard complexity-theoretical assumptions, the problem parametrised by the combined parameter $k$, $p$, maximum degree, diameter, and treewidth does not admit a polynomial-size problem kernel, even when restricted to planar graphs.",studi np hard minimum share edg mse problem graph decid whether possibl rout path start vertex target vertex given graph use edg onc show mse decid bound grid linear time dimens either small larg compar number path contrari show mse remain np hard subgraph bound grid final studi mse parametris complex point view known mse fix paramet tractabl respect number path show standard complex theoret assumpt problem parametris combin paramet maximum degre diamet treewidth doe admit polynomi size problem kernel even restrict planar graph,"['Till Fluschnik', 'Meike Hatzel', 'Steffen Härtlein', 'Hendrik Molter', 'Henning Seidler']","['cs.CC', '68Q17, 68Q25, 68R10, 05C10', 'F.1.3; F.2.2; G.2.2']",False,False,False,False,False,True
134,2017-03-28T14:06:52Z,2017-03-05T23:06:03Z,http://arxiv.org/abs/1703.01686v1,http://arxiv.org/pdf/1703.01686v1,Parameterized complexity of finding a spanning tree with minimum reload   cost diameter,parameter complex find span tree minimum reload cost diamet,"We study the minimum diameter spanning tree problem under the reload cost model (DIAMETER-TREE for short) introduced by Wirth and Steffan (2001). In this problem, given an undirected edge-colored graph $G$, reload costs on a path arise at a node where the path uses consecutive edges of different colors. The objective is to find a spanning tree of $G$ of minimum diameter with respect to the reload costs. We initiate a systematic study of the parameterized complexity of the DIAMETER-TREE problem by considering the following parameters: the cost of a solution, and the treewidth and the maximum degree $\Delta$ of the input graph. We prove that DIAMETER-TREE is para-NP-hard for any combination of two of these three parameters, and that it is FPT parameterized by the three of them. We also prove that the problem can be solved in polynomial time on cactus graphs. This result is somehow surprising since we prove DIAMETER-TREE to be NP-hard on graphs of treewidth two, which is best possible as the problem can be trivially solved on forests. When the reload costs satisfy the triangle inequality, Wirth and Steffan (2001) proved that the problem can be solved in polynomial time on graphs with $\Delta = 3$, and Galbiati (2008) proved that it is NP-hard if $\Delta = 4$. Our results show, in particular, that without the requirement of the triangle inequality, the problem is NP-hard if $\Delta = 3$, which is also best possible. Finally, in the case where the reload costs are polynomially bounded by the size of the input graph, we prove that DIAMETER-TREE is in XP and W[1]-hard parameterized by the treewidth plus $\Delta$.",studi minimum diamet span tree problem reload cost model diamet tree short introduc wirth steffan problem given undirect edg color graph reload cost path aris node path use consecut edg differ color object find span tree minimum diamet respect reload cost initi systemat studi parameter complex diamet tree problem consid follow paramet cost solut treewidth maximum degre delta input graph prove diamet tree para np hard ani combin two three paramet fpt parameter three also prove problem solv polynomi time cactus graph result somehow surpris sinc prove diamet tree np hard graph treewidth two best possibl problem trivial solv forest reload cost satisfi triangl inequ wirth steffan prove problem solv polynomi time graph delta galbiati prove np hard delta result show particular without requir triangl inequ problem np hard delta also best possibl final case reload cost polynomi bound size input graph prove diamet tree xp hard parameter treewidth plus delta,"['Julien Baste', 'Didem Gözüpek', 'Christophe Paul', 'Ignasi Sau', 'Mordechai Shalom', 'Dimitrios M. Thilikos']","['cs.DS', 'cs.CC', '05C85, 05C10', 'G.2.2; G.2.3']",False,False,False,False,False,True
135,2017-03-28T14:06:52Z,2017-03-03T13:04:46Z,http://arxiv.org/abs/1703.01143v1,http://arxiv.org/pdf/1703.01143v1,Why is it hard to beat $O(n^2)$ for Longest Common Weakly Increasing   Subsequence?,whi hard beat longest common weak increas subsequ,"The Longest Common Weakly Increasing Subsequence problem (LCWIS) is a variant of the classic Longest Common Subsequence problem (LCS). Both problems can be solved with simple quadratic time algorithms. A recent line of research led to a number of matching conditional lower bounds for LCS and other related problems. However, the status of LCWIS remained open.   In this paper we show that LCWIS cannot be solved in strongly subquadratic time unless the Strong Exponential Time Hypothesis (SETH) is false.   The ideas which we developed can also be used to obtain a lower bound based on a safer assumption of NC-SETH, i.e. a version of SETH which talks about NC circuits instead of less expressive CNF formulas.",longest common weak increas subsequ problem lcwis variant classic longest common subsequ problem lcs problem solv simpl quadrat time algorithm recent line research led number match condit lower bound lcs relat problem howev status lcwis remain open paper show lcwis cannot solv strong subquadrat time unless strong exponenti time hypothesi seth fals idea develop also use obtain lower bound base safer assumpt nc seth version seth talk nc circuit instead less express cnf formula,['Adam Polak'],['cs.CC'],False,False,False,False,False,True
136,2017-03-28T14:06:52Z,2017-03-02T20:25:04Z,http://arxiv.org/abs/1703.00941v1,http://arxiv.org/pdf/1703.00941v1,On the Fine-grained Complexity of One-Dimensional Dynamic Programming,fine grain complex one dimension dynam program,"In this paper, we investigate the complexity of one-dimensional dynamic programming, or more specifically, of the Least-Weight Subsequence (LWS) problem: Given a sequence of $n$ data items together with weights for every pair of the items, the task is to determine a subsequence $S$ minimizing the total weight of the pairs adjacent in $S$. A large number of natural problems can be formulated as LWS problems, yielding obvious $O(n^2)$-time solutions.   In many interesting instances, the $O(n^2)$-many weights can be succinctly represented. Yet except for near-linear time algorithms for some specific special cases, little is known about when an LWS instantiation admits a subquadratic-time algorithm and when it does not. In particular, no lower bounds for LWS instantiations have been known before. In an attempt to remedy this situation, we provide a general approach to study the fine-grained complexity of succinct instantiations of the LWS problem. In particular, given an LWS instantiation we identify a highly parallel core problem that is subquadratically equivalent. This provides either an explanation for the apparent hardness of the problem or an avenue to find improved algorithms as the case may be.   More specifically, we prove subquadratic equivalences between the following pairs (an LWS instantiation and the corresponding core problem) of problems: a low-rank version of LWS and minimum inner product, finding the longest chain of nested boxes and vector domination, and a coin change problem which is closely related to the knapsack problem and (min,+)-convolution. Using these equivalences and known SETH-hardness results for some of the core problems, we deduce tight conditional lower bounds for the corresponding LWS instantiations. We also establish the (min,+)-convolution-hardness of the knapsack problem.",paper investig complex one dimension dynam program specif least weight subsequ lws problem given sequenc data item togeth weight everi pair item task determin subsequ minim total weight pair adjac larg number natur problem formul lws problem yield obvious time solut mani interest instanc mani weight succinct repres yet except near linear time algorithm specif special case littl known lws instanti admit subquadrat time algorithm doe particular lower bound lws instanti known befor attempt remedi situat provid general approach studi fine grain complex succinct instanti lws problem particular given lws instanti identifi high parallel core problem subquadrat equival provid either explan appar hard problem avenu find improv algorithm case may specif prove subquadrat equival follow pair lws instanti correspond core problem problem low rank version lws minimum inner product find longest chain nest box vector domin coin chang problem close relat knapsack problem min convolut use equival known seth hard result core problem deduc tight condit lower bound correspond lws instanti also establish min convolut hard knapsack problem,"['Marvin Künnemann', 'Ramamohan Paturi', 'Stefan Schneider']","['cs.CC', 'cs.DS']",False,False,False,False,False,True
137,2017-03-28T14:06:52Z,2017-03-01T23:15:54Z,http://arxiv.org/abs/1703.00544v1,http://arxiv.org/pdf/1703.00544v1,Simplified Algorithmic Metatheorems Beyond MSO: Treewidth and   Neighborhood Diversity,simplifi algorithm metatheorem beyond mso treewidth neighborhood divers,"This paper settles the computational complexity of model checking of several extensions of the monadic second order (MSO) logic on two classes of graphs: graphs of bounded treewidth and graphs of bounded neighborhood diversity. A classical theorem of Courcelle states that any graph property definable in MSO is decidable in linear time on graphs of bounded treewidth. Algorithmic metatheorems like Courcelle's serve to generalize known positive results on various graph classes. We explore and extend three previously studied MSO extensions: global and local cardinality constraints (CardMSO and MSO-LCC) and optimizing a fair objective function (fairMSO). First, we show how these fragments relate to each other in expressive power and highlight their (non)linearity. On the side of neighborhood diversity, we show that combining the linear variants of local and global cardinality constraints is possible while keeping the linear runtime but removing linearity of either makes this impossible, and we provide a polynomial time algorithm for the hard case. Furthemore, we show that even the combination of the two most powerful fragments is solvable in polynomial time on graphs of bounded treewidth.",paper settl comput complex model check sever extens monad second order mso logic two class graph graph bound treewidth graph bound neighborhood divers classic theorem courcell state ani graph properti defin mso decid linear time graph bound treewidth algorithm metatheorem like courcell serv general known posit result various graph class explor extend three previous studi mso extens global local cardin constraint cardmso mso lcc optim fair object function fairmso first show fragment relat express power highlight non linear side neighborhood divers show combin linear variant local global cardin constraint possibl keep linear runtim remov linear either make imposs provid polynomi time algorithm hard case furthemor show even combin two power fragment solvabl polynomi time graph bound treewidth,"['Dušan Knop', 'Martin Koutecký', 'Tomáš Masařík', 'Tomáš Toufar']","['cs.CC', 'cs.LO', '03D15', 'F.2.2']",False,False,False,False,False,True
138,2017-03-28T14:06:52Z,2017-03-15T08:50:49Z,http://arxiv.org/abs/1703.00242v2,http://arxiv.org/pdf/1703.00242v2,Reordering Method and Hierarchies for Quantum and Classical Ordered   Binary Decision Diagrams,reorder method hierarchi quantum classic order binari decis diagram,"We consider Quantum OBDD model. It is restricted version of read-once Quantum Branching Programs, with respect to ""width"" complexity. It is known that maximal complexity gap between deterministic and quantum model is exponential. But there are few examples of such functions. We present method (called ""reordering""), which allows to build Boolean function $g$ from Boolean Function $f$, such that if for $f$ we have gap between quantum and deterministic OBDD complexity for natural order of variables, then we have almost the same gap for function $g$, but for any order. Using it we construct the total function $REQ$ which deterministic OBDD complexity is $2^{\Omega(n/\log n)}$ and present quantum OBDD of width $O(n^2)$. It is bigger gap for explicit function that was known before for OBDD of width more than linear. Using this result we prove the width hierarchy for complexity classes of Boolean functions for quantum OBDDs.   Additionally, we prove the width hierarchy for complexity classes of Boolean functions for bounded error probabilistic OBDDs. And using ""reordering"" method we extend a hierarchy for $k$-OBDD of polynomial size, for $k=o(n/\log^3n)$. Moreover, we proved a similar hierarchy for bounded error probabilistic $k$-OBDD. And for deterministic and probabilistic $k$-OBDDs of superpolynomial and subexponential size.",consid quantum obdd model restrict version read onc quantum branch program respect width complex known maxim complex gap determinist quantum model exponenti exampl function present method call reorder allow build boolean function boolean function gap quantum determinist obdd complex natur order variabl almost gap function ani order use construct total function req determinist obdd complex omega log present quantum obdd width bigger gap explicit function known befor obdd width linear use result prove width hierarchi complex class boolean function quantum obdd addit prove width hierarchi complex class boolean function bound error probabilist obdd use reorder method extend hierarchi obdd polynomi size log moreov prove similar hierarchi bound error probabilist obdd determinist probabilist obdd superpolynomi subexponenti size,"['Kamil Khadiev', 'Aliya Khadieva']","['cs.CC', 'quant-ph']",False,False,False,False,False,True
139,2017-03-28T14:06:52Z,2017-02-28T20:28:03Z,http://arxiv.org/abs/1703.00043v1,http://arxiv.org/pdf/1703.00043v1,Tree tribes and lower bounds for switching lemmas,tree tribe lower bound switch lemma,"We show tight upper and lower bounds for switching lemmas obtained by the action of random $p$-restrictions on boolean functions that can be expressed as decision trees in which every vertex is at a distance of at most $t$ from some leaf, also called $t$-clipped decision trees. More specifically, we show the following:   $\bullet$ If a boolean function $f$ can be expressed as a $t$-clipped decision tree, then under the action of a random $p$-restriction $\rho$, the probability that the smallest depth decision tree for $f _{\rho}$ has depth greater than $d$ is upper bounded by $(4p2^{t})^{d}$.   $\bullet$ For every $t$, there exists a function $g_{t}$ that can be expressed as a $t$-clipped decision tree, such that under the action of a random $p$-restriction $\rho$, the probability that the smallest depth decision tree for $g_{t} _{\rho}$ has depth greater than $d$ is lower bounded by $(c_{0}p2^{t})^{d}$, for $0\leq p\leq c_{p}2^{-t}$ and $0\leq d\leq c_{d}\frac{\log n}{2^{t}\log t}$, where $c_{0},c_{p},c_{d}$ are universal constants.",show tight upper lower bound switch lemma obtain action random restrict boolean function express decis tree everi vertex distanc leaf also call clip decis tree specif show follow bullet boolean function express clip decis tree action random restrict rho probabl smallest depth decis tree rho depth greater upper bound bullet everi exist function express clip decis tree action random restrict rho probabl smallest depth decis tree rho depth greater lower bound leq leq leq leq frac log log univers constant,['Jenish C. Mehta'],['cs.CC'],False,False,False,False,False,True
140,2017-03-28T14:06:56Z,2017-02-28T16:57:49Z,http://arxiv.org/abs/1702.08862v1,http://arxiv.org/pdf/1702.08862v1,Proportional Representation in Vote Streams,proport represent vote stream,"We consider elections where the voters come one at a time, in a streaming fashion, and devise space-efficient algorithms which identify an approximate winning committee with respect to common multiwinner proportional representation voting rules; specifically, we consider the Approval-based and the Borda-based variants of both the Chamberlin-- ourant rule and the Monroe rule. We complement our algorithms with lower bounds. Somewhat surprisingly, our results imply that, using space which does not depend on the number of voters it is possible to efficiently identify an approximate representative committee of fixed size over vote streams with huge number of voters.",consid elect voter come one time stream fashion devis space effici algorithm identifi approxim win committe respect common multiwinn proport represent vote rule specif consid approv base borda base variant chamberlin ourant rule monro rule complement algorithm lower bound somewhat surpris result impli use space doe depend number voter possibl effici identifi approxim repres committe fix size vote stream huge number voter,"['Palash Dey', 'Nimrod Talmon', 'Otniel van Handel']","['cs.GT', 'cs.AI', 'cs.CC', 'cs.DS', 'cs.MA']",False,False,False,False,False,True
141,2017-03-28T14:06:56Z,2017-02-28T15:47:39Z,http://arxiv.org/abs/1702.08830v1,http://arxiv.org/pdf/1702.08830v1,The Complexity of Translationally-Invariant Low-Dimensional Spin   Lattices in 3D,complex translate invari low dimension spin lattic,"In this paper, we consider spin systems in three spatial dimensions, and prove that the local Hamiltonian problem for 3D lattices with face-centered cubic unit cells, 4-local translationally-invariant interactions between spin-3/2 particles and open boundary conditions is QMAEXP-complete. We go beyond a mere embedding of past hard 1D history state constructions, and utilize a classical Wang tiling problem as binary counter in order to translate one cube side length into a binary description for the verifier input. We further make use of a recently-developed computational model especially well-suited for history state constructions, and combine it with a specific circuit encoding shown to be universal for quantum computation. These novel techniques allow us to significantly lower the local spin dimension, surpassing the best translationally-invariant result to date by two orders of magnitude (in the number of degrees of freedom per coupling). This brings our models en par with the best non-translationally-invariant construction.",paper consid spin system three spatial dimens prove local hamiltonian problem lattic face center cubic unit cell local translate invari interact spin particl open boundari condit qmaexp complet go beyond mere embed past hard histori state construct util classic wang tile problem binari counter order translat one cube side length binari descript verifi input make use recent develop comput model especi well suit histori state construct combin specif circuit encod shown univers quantum comput novel techniqu allow us signific lower local spin dimens surpass best translate invari result date two order magnitud number degre freedom per coupl bring model en par best non translate invari construct,"['Johannes Bausch', 'Stephen Piddock']","['quant-ph', 'cs.CC', '68Q17, 81V70, 68Q10, 82D25']",False,False,False,False,False,True
143,2017-03-28T14:06:56Z,2017-03-05T02:19:58Z,http://arxiv.org/abs/1702.08660v2,http://arxiv.org/pdf/1702.08660v2,Complexity of short generating functions,complex short generat function,"We give complexity analysis of the class of short generating functions (GF). Assuming $\#P \not\subseteq FP/poly$, we show that this class is not closed under taking many intersections, unions or projections of GFs, in the sense that these operations can increase the bitlength of coefficients of GFs by a super-polynomial factor. We also prove that truncated theta functions are hard in this class.",give complex analysi class short generat function gf assum subseteq fp poli show class close take mani intersect union project gfs sens oper increas bitlength coeffici gfs super polynomi factor also prove truncat theta function hard class,"['Danny Nguyen', 'Igor Pak']","['math.CO', 'cs.CC', 'cs.DM', 'cs.LO', 'math.LO']",False,False,True,False,False,True
144,2017-03-28T14:06:56Z,2017-02-27T19:46:15Z,http://arxiv.org/abs/1702.08489v1,http://arxiv.org/pdf/1702.08489v1,Depth Separation for Neural Networks,depth separ neural network,"Let $f:\mathbb{S}^{d-1}\times \mathbb{S}^{d-1}\to\mathbb{S}$ be a function of the form $f(\mathbf{x},\mathbf{x}') = g(\langle\mathbf{x},\mathbf{x}'\rangle)$ for $g:[-1,1]\to \mathbb{R}$. We give a simple proof that shows that poly-size depth two neural networks with (exponentially) bounded weights cannot approximate $f$ whenever $g$ cannot be approximated by a low degree polynomial. Moreover, for many $g$'s, such as $g(x)=\sin(\pi d^3x)$, the number of neurons must be $2^{\Omega\left(d\log(d)\right)}$. Furthermore, the result holds w.r.t.\ the uniform distribution on $\mathbb{S}^{d-1}\times \mathbb{S}^{d-1}$. As many functions of the above form can be well approximated by poly-size depth three networks with poly-bounded weights, this establishes a separation between depth two and depth three networks w.r.t.\ the uniform distribution on $\mathbb{S}^{d-1}\times \mathbb{S}^{d-1}$.",let mathbb time mathbb mathbb function form mathbf mathbf langl mathbf mathbf rangl mathbb give simpl proof show poli size depth two neural network exponenti bound weight cannot approxim whenev cannot approxim low degre polynomi moreov mani sin pi number neuron must omega left log right furthermor result hold uniform distribut mathbb time mathbb mani function abov form well approxim poli size depth three network poli bound weight establish separ depth two depth three network uniform distribut mathbb time mathbb,['Amit Daniely'],"['cs.LG', 'cs.CC', 'stat.ML']",False,False,False,False,False,True
145,2017-03-28T14:06:56Z,2017-02-27T19:26:15Z,http://arxiv.org/abs/1702.08483v1,http://arxiv.org/pdf/1702.08483v1,The computational landscape of general physical theories,comput landscap general physic theori,"The emergence of quantum computers has challenged long-held beliefs about what is efficiently computable given our current physical theories. However, going back to the work of Abrams and Lloyd, changing one aspect of quantum theory can result in yet more dramatic increases in computational power, as well as violations of fundamental physical principles. Here we focus on efficient computation within a framework of general physical theories that make good operational sense. In prior work, Lee and Barrett showed that in any theory satisfying the principle of tomographic locality (roughly, local measurements suffice for tomography of multipartite states) the complexity bound on efficient computation is AWPP. This bound holds independently of whether the principle of causality (roughly, no signalling from the future) is satisfied. In this work we show that this bound is tight: there exists a theory satisfying both the principles of tomographic locality and causality which can efficiently decide everything in AWPP, and in particular can simulate any efficient quantum computation. Thus the class AWPP has a natural physical interpretation: it is precisely the class of problems that can be solved efficiently in tomographically-local theories. This theory is built upon a model of computing involving Turing machines with quasi-probabilities, to wit, machines with transition weights that can be negative but sum to unity over all branches. In analogy with the study of non-local quantum correlations, this leads us to question what physical principles recover the power of quantum computing. Along this line, we give some computational complexity evidence that quantum computation does not achieve the bound of AWPP.",emerg quantum comput challeng long held belief effici comput given current physic theori howev go back work abram lloyd chang one aspect quantum theori result yet dramat increas comput power well violat fundament physic principl focus effici comput within framework general physic theori make good oper sens prior work lee barrett show ani theori satisfi principl tomograph local rough local measur suffic tomographi multipartit state complex bound effici comput awpp bound hold independ whether principl causal rough signal futur satisfi work show bound tight exist theori satisfi principl tomograph local causal effici decid everyth awpp particular simul ani effici quantum comput thus class awpp natur physic interpret precis class problem solv effici tomograph local theori theori built upon model comput involv ture machin quasi probabl wit machin transit weight negat sum uniti branch analog studi non local quantum correl lead us question physic principl recov power quantum comput along line give comput complex evid quantum comput doe achiev bound awpp,"['Jonathan Barrett', 'Niel de Beaudrap', 'Matty J. Hoban', 'Ciarán M. Lee']","['quant-ph', 'cs.CC']",False,False,True,False,False,True
146,2017-03-28T14:06:56Z,2017-02-27T12:21:07Z,http://arxiv.org/abs/1702.08255v1,http://arxiv.org/pdf/1702.08255v1,Learning with Errors is easy with quantum samples,learn error easi quantum sampl,"Learning with Errors is one of the fundamental problems in computational learning theory and has in the last years become the cornerstone of post-quantum cryptography. In this work, we study the quantum sample complexity of Learning with Errors and show that there exists an efficient quantum learning algorithm (with polynomial sample and time complexity) for the Learning with Errors problem where the error distribution is the one used in cryptography. While our quantum learning algorithm does not break the LWE-based encryption schemes proposed in the cryptography literature, it does have some interesting implications for cryptography: first, when building an LWE-based scheme, one needs to be careful about the access to the public-key generation algorithm that is given to the adversary; second, our algorithm shows a possible way for attacking LWE-based encryption by using classical samples to approximate the quantum sample state, since then using our quantum learning algorithm would solve LWE.",learn error one fundament problem comput learn theori last year becom cornerston post quantum cryptographi work studi quantum sampl complex learn error show exist effici quantum learn algorithm polynomi sampl time complex learn error problem error distribut one use cryptographi quantum learn algorithm doe break lwe base encrypt scheme propos cryptographi literatur doe interest implic cryptographi first build lwe base scheme one need care access public key generat algorithm given adversari second algorithm show possibl way attack lwe base encrypt use classic sampl approxim quantum sampl state sinc use quantum learn algorithm would solv lwe,"['Alex B. Grilo', 'Iordanis Kerenidis']","['quant-ph', 'cs.CC']",False,False,False,False,False,True
147,2017-03-28T14:06:56Z,2017-02-27T11:24:02Z,http://arxiv.org/abs/1702.08238v1,http://arxiv.org/pdf/1702.08238v1,Consensus Patterns parameterized by input string length is W[1]-hard,consensus pattern parameter input string length hard,"We consider the Consensus Patterns problem, where, given a set of input strings, one is asked to extract a long-enough pattern which appears (with some errors) in all strings. We prove that this problem is W[1]-hard when parameterized by the maximum length of input strings.",consid consensus pattern problem given set input string one ask extract long enough pattern appear error string prove problem hard parameter maximum length input string,['Laurent Bulteau'],['cs.CC'],False,False,False,False,False,True
148,2017-03-28T14:06:56Z,2017-02-27T04:42:03Z,http://arxiv.org/abs/1702.08144v1,http://arxiv.org/pdf/1702.08144v1,Synchronization Problems in Automata without Non-trivial Cycles,synchron problem automata without non trivial cycl,"In this paper, we study the computational complexity of various problems related to synchronization of weakly acyclic automata, a subclass of widely studied aperiodic automata. We provide upper and lower bounds on the length of a shortest word synchronizing a weakly acyclic automaton or, more generally, a subset of its states, and show that the problem of approximating this length is hard. We also show inapproximability of the problem of computing the rank of a subset of states in a binary weakly acyclic automaton and prove that several problems related to recognizing a synchronizing subset of states in such automata are NP-complete.",paper studi comput complex various problem relat synchron weak acycl automata subclass wide studi aperiod automata provid upper lower bound length shortest word synchron weak acycl automaton general subset state show problem approxim length hard also show inapproxim problem comput rank subset state binari weak acycl automaton prove sever problem relat recogn synchron subset state automata np complet,['Andrew Ryzhikov'],"['cs.FL', 'cs.CC', '68Q17', 'F.1.1; F.1.3; F.2.2']",False,False,False,False,False,True
149,2017-03-28T14:06:56Z,2017-02-26T20:53:29Z,http://arxiv.org/abs/1702.08084v1,http://arxiv.org/pdf/1702.08084v1,On Algorithmic Statistics for space-bounded algorithms,algorithm statist space bound algorithm,Algorithmic statistics studies explanations of observed data that are good in the algorithmic sense: an explanation should be simple i.e. should have small Kolmogorov complexity and capture all the algorithmically discoverable regularities in the data. However this idea can not be used in practice because Kolmogorov complexity is not computable.   In this paper we develop algorithmic statistics using space-bounded Kolmogorov complexity. We prove an analogue of one of the main result of `classic' algorithmic statistics (about the connection between optimality and randomness deficiences). The main tool of our proof is the Nisan-Wigderson generator.,algorithm statist studi explan observ data good algorithm sens explan simpl small kolmogorov complex captur algorithm discover regular data howev idea use practic becaus kolmogorov complex comput paper develop algorithm statist use space bound kolmogorov complex prove analogu one main result classic algorithm statist connect optim random defici main tool proof nisan wigderson generat,['Alexey Milovanov'],"['cs.IT', 'cs.CC', 'math.IT']",False,False,False,False,False,True
150,2017-03-28T14:07:01Z,2017-03-27T05:15:38Z,http://arxiv.org/abs/1702.08045v2,http://arxiv.org/pdf/1702.08045v2,"General Upper Bounds for Gate Complexity and Depth of Reversible   Circuits Consisting of NOT, CNOT and 2-CNOT Gates",general upper bound gate complex depth revers circuit consist cnot cnot gate,"The paper discusses the gate complexity and the depth of reversible circuits consisting of NOT, CNOT and 2-CNOT gates in the case, when the number of additional inputs is limited. We study Shannon's gate complexity function $L(n, q)$ and depth function $D(n, q)$ for a reversible circuit implementing a Boolean transformation $f\colon \mathbb Z_2^n \to \mathbb Z_2^n$ with $8n < q \lesssim n2^{n-o(n)}$ additional inputs. The general upper bounds $L(n,q) \lesssim 2^n + 8n2^n \mathop / (\log_2 (q-4n) - \log_2 n - 2)$ and $D(n,q) \lesssim 2^{n+1}(2,5 + \log_2 n - \log_2 (\log_2 (q - 4n) - \log_2 n - 2))$ are proved for this case.",paper discuss gate complex depth revers circuit consist cnot cnot gate case number addit input limit studi shannon gate complex function depth function revers circuit implement boolean transform colon mathbb mathbb lesssim addit input general upper bound lesssim mathop log log lesssim log log log log prove case,['Dmitry V. Zakablukov'],['cs.CC'],False,False,False,False,False,True
151,2017-03-28T14:07:01Z,2017-02-26T07:12:46Z,http://arxiv.org/abs/1702.08443v1,http://arxiv.org/pdf/1702.08443v1,"Elementary Yet Precise Worst-case Analysis of MergeSort, A short version   (SV)",elementari yet precis worst case analysi mergesort short version sv,"This paper offers two elementary yet precise derivations of an exact formula   \[ W(n) = \sum_{i=1} ^{n} \lceil \lg i \rceil = n \lceil \lg n \rceil - 2^{\lceil \lg n \rceil} + 1 \] for the maximum number $ W(n) $ of comparisons of keys performed by $ {\tt MergeSort} $ on an $ n $-element array. The first of the two, due to its structural regularity, is well worth carefully studying in its own right.   Close smooth bounds on $ W(n) $ are derived. It seems interesting that $ W(n) $ is linear between the points $ n = 2^{\lfloor \lg n \rfloor} $ and it linearly interpolates its own lower bound $ n \lg n - n + 1 $ between these points.",paper offer two elementari yet precis deriv exact formula sum lceil lg rceil lceil lg rceil lceil lg rceil maximum number comparison key perform tt mergesort element array first two due structur regular well worth care studi right close smooth bound deriv seem interest linear point lfloor lg rfloor linear interpol lower bound lg point,['Marek A. Suchenek'],"['cs.DS', 'cs.CC', 'cs.DM', '68W40 Analysis of algorithms', 'F.2.2; G.2.0; G.2.1; G.2.2']",False,False,True,False,False,True
153,2017-03-28T14:07:01Z,2017-02-25T15:07:59Z,http://arxiv.org/abs/1702.07902v1,http://arxiv.org/pdf/1702.07902v1,Approval Voting with Intransitive Preferences,approv vote intransit prefer,"We extend Approval voting to the settings where voters may have intransitive preferences. The major obstacle to applying Approval voting in these settings is that voters are not able to clearly determine who they should approve or disapprove, due to the intransitivity of their preferences. An approach to address this issue is to apply tournament solutions to help voters make the decision. We study a class of voting systems where first each voter casts a vote defined as a tournament, then a well-defined tournament solution is applied to select the candidates who are assumed to be approved by the voter. Winners are the ones receiving the most approvals. We study axiomatic properties of this class of voting systems and complexity of control and bribery problems for these voting systems.",extend approv vote set voter may intransit prefer major obstacl appli approv vote set voter abl clear determin approv disapprov due intransit prefer approach address issu appli tournament solut help voter make decis studi class vote system first voter cast vote defin tournament well defin tournament solut appli select candid assum approv voter winner one receiv approv studi axiomat properti class vote system complex control briberi problem vote system,['Yongjie Yang'],"['cs.GT', 'cs.CC', 'cs.DM']",False,False,True,False,False,True
154,2017-03-28T14:07:01Z,2017-02-24T17:18:02Z,http://arxiv.org/abs/1702.07669v1,http://arxiv.org/pdf/1702.07669v1,"On problems equivalent to (min,+)-convolution",problem equival min convolut,"In the recent years, significant progress has been made in explaining apparent hardness of improving over naive solutions for many fundamental polynomially solvable problems. This came in the form of conditional lower bounds - reductions to one of problems assumed to be hard. These include 3SUM, All-Pairs Shortest Paths, SAT and Orthogonal Vectors, and others.   In the (min,+)-convolution problem, the goal is to compute a sequence $(c[i])^{n-1}_{i=0}$, where $c[k] = \min_{i=0,\ldots,k} \{a[i]+b[k-i]\}$, given sequences $(a[i])^{n-1}_{i=0}$ and $(b[i])_{i=0}^{n-1}$. This can easily be done in $O(n^2)$ time, but no $O(n^{2-\varepsilon})$ algorithm is known for $\varepsilon > 0$. In this paper we undertake a systematic study of the (min,+)-convolution problem as a hardness assumption.   As the first step, we establish equivalence of this problem to a group of other problems, including variants of the classic knapsack problem and problems related to subadditive sequences. The (min,+)-convolution has been used as a building block in algorithms for many problems, notably problems in stringology. It has also already appeared as an ad hoc hardness assumption. We investigate some of these connections and provide new reductions and other results.",recent year signific progress made explain appar hard improv naiv solut mani fundament polynomi solvabl problem came form condit lower bound reduct one problem assum hard includ sum pair shortest path sat orthogon vector min convolut problem goal comput sequenc min ldot given sequenc easili done time varepsilon algorithm known varepsilon paper undertak systemat studi min convolut problem hard assumpt first step establish equival problem group problem includ variant classic knapsack problem problem relat subaddit sequenc min convolut use build block algorithm mani problem notabl problem stringolog also alreadi appear ad hoc hard assumpt investig connect provid new reduct result,"['Marek Cygan', 'Marcin Mucha', 'Karol Węgrzycki', 'Michał Włodarczyk']","['cs.DS', 'cs.CC', 'F.1.3; F.2']",False,False,False,False,False,True
156,2017-03-28T14:07:01Z,2017-02-23T11:38:36Z,http://arxiv.org/abs/1702.07180v1,http://arxiv.org/pdf/1702.07180v1,Small hitting-sets for tiny arithmetic circuits or: How to turn bad   designs into good,small hit set tini arithmet circuit turn bad design good,"We show that if we can design poly($s$)-time hitting-sets for $\Sigma\wedge^a\Sigma\Pi^{O(\log s)}$ circuits of size $s$, where $a=\omega(1)$ is arbitrarily small and the number of variables, or arity $n$, is $O(\log s)$, then we can derandomize blackbox PIT for general circuits in quasipolynomial time. This also establishes that either E$\not\subseteq$\#P/poly or that VP$\ne$VNP. In fact, we show that one only needs a poly($s$)-time hitting-set against individual-degree $a'=\omega(1)$ polynomials that are computable by a size-$s$ arity-$(\log s)$ $\Sigma\Pi\Sigma$ circuit (note: $\Pi$ fanin may be $s$). Alternatively, we claim that, to understand VP one only needs to find hitting-sets, for depth-$3$, that have a small parameterized complexity. Another tiny family of interest is when we restrict the arity $n=\omega(1)$ to be arbitrarily small. We show that if we can design poly($s,\mu(n)$)-time hitting-sets for size-$s$ arity-$n$ $\Sigma\Pi\Sigma\wedge$ circuits (resp.~$\Sigma\wedge^a\Sigma\Pi$), where function $\mu$ is arbitrary, then we can solve PIT for VP in quasipoly-time, and prove the corresponding lower bounds. Our methods are strong enough to prove a surprising {\em arity reduction} for PIT-- to solve the general problem completely it suffices to find a blackbox PIT with time-complexity $sd2^{O(n)}$. We give several examples of ($\log s$)-variate circuits where a new measure (called cone-size) helps in devising poly-time hitting-sets, but the same question for their $s$-variate versions is open till date: For eg., diagonal depth-$3$ circuits, and in general, models that have a {\em small} partial derivative space. We also introduce a new concept, called cone-closed basis isolation, and provide example models where it occurs, or can be achieved by a small shift.",show design poli time hit set sigma wedg sigma pi log circuit size omega arbitrarili small number variabl ariti log derandom blackbox pit general circuit quasipolynomi time also establish either subseteq poli vp ne vnp fact show one onli need poli time hit set individu degre omega polynomi comput size ariti log sigma pi sigma circuit note pi fanin may altern claim understand vp one onli need find hit set depth small parameter complex anoth tini famili interest restrict ariti omega arbitrarili small show design poli mu time hit set size ariti sigma pi sigma wedg circuit resp sigma wedg sigma pi function mu arbitrari solv pit vp quasipoli time prove correspond lower bound method strong enough prove surpris em ariti reduct pit solv general problem complet suffic find blackbox pit time complex sd give sever exampl log variat circuit new measur call cone size help devis poli time hit set question variat version open till date eg diagon depth circuit general model em small partial deriv space also introduc new concept call cone close basi isol provid exampl model occur achiev small shift,"['Manindra Agrawal', 'Michael Forbes', 'Sumanta Ghosh', 'Nitin Saxena']","['cs.CC', 'F.1.1; I.1.2; F.1.3']",False,False,False,False,False,True
157,2017-03-28T14:07:01Z,2017-02-23T08:02:25Z,http://arxiv.org/abs/1702.07128v1,http://arxiv.org/pdf/1702.07128v1,The Facets of the Bases Polytope of a Matroid and Two Consequences,facet base polytop matroid two consequ,"Let $M$ to be a matroid defined on a finite set $E$ and $L\subset E$. $L$ is locked in $M$ if $M L$ and $M^* (E\backslash L)$ are 2-connected, and $min\{r(L), r^*(E\backslash L)\} \geq 2$. In this paper, we prove that the nontrivial facets of the bases polytope of $M$ are described by the locked subsets. We deduce that finding the maximum--weight basis of $M$ is a polynomial time problem for matroids with a polynomial number of locked subsets. This class of matroids is closed under 2-sums and contains the class of uniform matroids, the V\'amos matroid and all the excluded minors of 2-sums of uniform matroids. We deduce also a matroid oracle for testing uniformity of matroids after one call of this oracle.",let matroid defin finit set subset lock backslash connect min backslash geq paper prove nontrivi facet base polytop describ lock subset deduc find maximum weight basi polynomi time problem matroid polynomi number lock subset class matroid close sum contain class uniform matroid amo matroid exclud minor sum uniform matroid deduc also matroid oracl test uniform matroid one call oracl,['Brahim Chaourar'],"['cs.CC', 'Primary 90C27, Secondary 90C57, 52B40']",False,False,False,False,False,True
159,2017-03-28T14:07:01Z,2017-02-22T20:38:35Z,http://arxiv.org/abs/1702.06997v1,http://arxiv.org/pdf/1702.06997v1,Beyond Talagrand Functions: New Lower Bounds for Testing Monotonicity   and Unateness,beyond talagrand function new lower bound test monoton unat,"We prove a lower bound of $\tilde{\Omega}(n^{1/3})$ for the query complexity of any two-sided and adaptive algorithm that tests whether an unknown Boolean function $f:\{0,1\}^n\rightarrow \{0,1\}$ is monotone or far from monotone. This improves the recent bound of $\tilde{\Omega}(n^{1/4})$ for the same problem by Belovs and Blais [BB15]. Our result builds on a new family of random Boolean functions that can be viewed as a two-level extension of Talagrand's random DNFs.   Beyond monotonicity, we also prove a lower bound of $\tilde{\Omega}(\sqrt{n})$ for any two-sided and adaptive algorithm, and a lower bound of $\tilde{\Omega}(n)$ for any one-sided and non-adaptive algorithm for testing unateness, a natural generalization of monotonicity. The latter matches the recent linear upper bounds by Khot and Shinkar [KS15] and by Chakrabarty and Seshadhri [CS16].",prove lower bound tild omega queri complex ani two side adapt algorithm test whether unknown boolean function rightarrow monoton far monoton improv recent bound tild omega problem belov blai bb result build new famili random boolean function view two level extens talagrand random dnfs beyond monoton also prove lower bound tild omega sqrt ani two side adapt algorithm lower bound tild omega ani one side non adapt algorithm test unat natur general monoton latter match recent linear upper bound khot shinkar ks chakrabarti seshadhri cs,"['Xi Chen', 'Erik Waingarten', 'Jinyu Xie']",['cs.CC'],False,False,False,False,False,True
160,2017-03-28T14:07:05Z,2017-02-22T15:29:15Z,http://arxiv.org/abs/1702.06844v1,http://arxiv.org/pdf/1702.06844v1,Parameterized Shifted Combinatorial Optimization,parameter shift combinatori optim,"Shifted combinatorial optimization is a new nonlinear optimization framework which is a broad extension of standard combinatorial optimization, involving the choice of several feasible solutions at a time. This framework captures well studied and diverse problems ranging from so-called vulnerability problems to sharing and partitioning problems. In particular, every standard combinatorial optimization problem has its shifted counterpart, which is typically much harder. Already with explicitly given input set the shifted problem may be NP-hard. In this article we initiate a study of the parameterized complexity of this framework. First we show that shifting over an explicitly given set with its cardinality as the parameter may be in XP, FPT or P, depending on the objective function. Second, we study the shifted problem over sets definable in MSO logic (which includes, e.g., the well known MSO partitioning problems). Our main results here are that shifted combinatorial optimization over MSO definable sets is in XP with respect to the MSO formula and the treewidth (or more generally clique-width) of the input graph, and is W[1]-hard even under further severe restrictions.",shift combinatori optim new nonlinear optim framework broad extens standard combinatori optim involv choic sever feasibl solut time framework captur well studi divers problem rang call vulner problem share partit problem particular everi standard combinatori optim problem shift counterpart typic much harder alreadi explicit given input set shift problem may np hard articl initi studi parameter complex framework first show shift explicit given set cardin paramet may xp fpt depend object function second studi shift problem set defin mso logic includ well known mso partit problem main result shift combinatori optim mso defin set xp respect mso formula treewidth general cliqu width input graph hard even sever restrict,"['Jakub Gajarský', 'Petr Hliněný', 'Martin Koutecký', 'Shmuel Onn']",['cs.CC'],False,False,True,False,False,True
161,2017-03-28T14:07:05Z,2017-02-21T23:06:56Z,http://arxiv.org/abs/1702.06616v1,http://arxiv.org/pdf/1702.06616v1,TC^0 circuits for algorithmic problems in nilpotent groups,tc circuit algorithm problem nilpot group,"Recently, MacDonald et. al. showed that many algorithmic problems for nilpotent groups including computation of normal forms, the subgroup membership problem, the conjugacy problem, and computation of presentations of subgroups can be done in Logspace. Here we follow their approach and show that all these problems are actually complete for the uniform circuit class TC^0 -- uniformly for all r-generated nilpotent groups of class at most c for fixed r and c.   Moreover, if we allow a certain binary representation of the inputs, then the word problem and computation of normal forms is still in uniform TC^0, while all the other problems we examine are shown to be TC^0-Turing reducible to the problem of computing greatest common divisors and expressing them as a linear combination.",recent macdonald et al show mani algorithm problem nilpot group includ comput normal form subgroup membership problem conjugaci problem comput present subgroup done logspac follow approach show problem actual complet uniform circuit class tc uniform generat nilpot group class fix moreov allow certain binari represent input word problem comput normal form still uniform tc problem examin shown tc ture reduc problem comput greatest common divisor express linear combin,"['Alexei Myasnikov', 'Armin Weiß']","['math.GR', 'cs.CC', 'F.2.2; G.2.0']",False,False,False,False,False,True
164,2017-03-28T14:07:05Z,2017-02-23T02:48:22Z,http://arxiv.org/abs/1702.06237v2,http://arxiv.org/pdf/1702.06237v2,Exact tensor completion with sum-of-squares,exact tensor complet sum squar,"We obtain the first polynomial-time algorithm for exact tensor completion that improves over the bound implied by reduction to matrix completion. The algorithm recovers an unknown 3-tensor with $r$ incoherent, orthogonal components in $\mathbb R^n$ from $r\cdot \tilde O(n^{1.5})$ randomly observed entries of the tensor. This bound improves over the previous best one of $r\cdot \tilde O(n^{2})$ by reduction to exact matrix completion. Our bound also matches the best known results for the easier problem of approximate tensor completion (Barak & Moitra, 2015).   Our algorithm and analysis extends seminal results for exact matrix completion (Candes & Recht, 2009) to the tensor setting via the sum-of-squares method. The main technical challenge is to show that a small number of randomly chosen monomials are enough to construct a degree-3 polynomial with precisely planted orthogonal global optima over the sphere and that this fact can be certified within the sum-of-squares proof system.",obtain first polynomi time algorithm exact tensor complet improv bound impli reduct matrix complet algorithm recov unknown tensor incoher orthogon compon mathbb cdot tild random observ entri tensor bound improv previous best one cdot tild reduct exact matrix complet bound also match best known result easier problem approxim tensor complet barak moitra algorithm analysi extend semin result exact matrix complet cand recht tensor set via sum squar method main technic challeng show small number random chosen monomi enough construct degre polynomi precis plant orthogon global optima sphere fact certifi within sum squar proof system,"['Aaron Potechin', 'David Steurer']","['cs.LG', 'cs.CC', 'cs.DS', 'cs.IT', 'math.IT', 'stat.ML']",False,False,False,False,False,True
165,2017-03-28T14:07:05Z,2017-02-20T15:39:13Z,http://arxiv.org/abs/1702.06017v1,http://arxiv.org/pdf/1702.06017v1,CLS: New Problems and Completeness,cls new problem complet,"The complexity class CLS was introduced by Daskalakis and Papadimitriou with the goal of capturing the complexity of some well-known problems in PPAD$~\cap~$PLS that have resisted, in some cases for decades, attempts to put them in polynomial time. No complete problem was known for CLS, and in previous work, the problems ContractionMap, i.e., the problem of finding an approximate fixpoint of a contraction map, and PLCP, i.e., the problem of solving a P-matrix Linear Complementarity Problem, were identified as prime candidates.   First, we present a new CLS-complete problem MetaMetricContractionMap, which is closely related to the ContractionMap. Second, we introduce EndOfPotentialLine, which captures aspects of PPAD and PLS directly via a monotonic directed path, and show that EndOfPotentialLine is in CLS via a two-way reduction to EndOfMeteredLine. The latter was defined to keep track of how far a vertex is on the PPAD path via a restricted potential function. Third, we reduce PLCP to EndOfPotentialLine, thus making EndOfPotentialLine and EndOfMeteredLine at least as likely to be hard for CLS as PLCP. This last result leverages the monotonic structure of Lemke paths for PLCP problems, making EndOfPotentialLine a likely candidate to capture the exact complexity of PLCP; we note that the structure of Lemke-Howson paths for finding a Nash equilibrium in a two-player game very directly motivated the definition of the complexity class PPAD, which eventually ended up capturing this problem's complexity exactly.",complex class cls introduc daskalaki papadimitriou goal captur complex well known problem ppad cap pls resist case decad attempt put polynomi time complet problem known cls previous work problem contractionmap problem find approxim fixpoint contract map plcp problem solv matrix linear complementar problem identifi prime candid first present new cls complet problem metametriccontractionmap close relat contractionmap second introduc endofpotentiallin captur aspect ppad pls direct via monoton direct path show endofpotentiallin cls via two way reduct endofmeteredlin latter defin keep track far vertex ppad path via restrict potenti function third reduc plcp endofpotentiallin thus make endofpotentiallin endofmeteredlin least like hard cls plcp last result leverag monoton structur lemk path plcp problem make endofpotentiallin like candid captur exact complex plcp note structur lemk howson path find nash equilibrium two player game veri direct motiv definit complex class ppad eventu end captur problem complex exact,"['John Fearnley', 'Spencer Gordon', 'Ruta Mehta', 'Rahul Savani']",['cs.CC'],False,False,False,False,False,True
166,2017-03-28T14:07:05Z,2017-02-20T11:04:29Z,http://arxiv.org/abs/1702.05927v1,http://arxiv.org/pdf/1702.05927v1,How to implement a genuine Parrondo's paradox with quantum walks?,implement genuin parrondo paradox quantum walk,"Parrondo's paradox is ubiquitous in games, ratchets and random walks.The apparent paradox, devised by Juan M. R. Parrondo, that two losing games A and B can produce an winning outcome has been adapted in many physical and biological systems to explain their working. However, proposals on demonstrating Parrondo's paradox using quantum walks failed in the asymptotic limits. In this work, we show that instead of a single coin if we consider a two coin initial state which may or may not be entangled, we can observe a genuine Parrondo's paradox with quantum walks. The implications of our results for observing quantum ratchet like behavior using quantum walks is also discussed.",parrondo paradox ubiquit game ratchet random walk appar paradox devis juan parrondo two lose game produc win outcom adapt mani physic biolog system explain work howev propos demonstr parrondo paradox use quantum walk fail asymptot limit work show instead singl coin consid two coin initi state may may entangl observ genuin parrondo paradox quantum walk implic result observ quantum ratchet like behavior use quantum walk also discuss,"['Jishnu Rajendran', 'Colin Benjamin']","['quant-ph', 'cond-mat.mes-hall', 'cs.CC']",False,False,False,False,False,True
167,2017-03-28T14:07:05Z,2017-02-19T15:48:11Z,http://arxiv.org/abs/1702.05760v1,http://arxiv.org/pdf/1702.05760v1,Hypercube LSH for approximate near neighbors,hypercub lsh approxim near neighbor,"A celebrated technique for finding near neighbors for the angular distance involves using a set of \textit{random} hyperplanes to partition the space into hash regions [Charikar, STOC 2002]. Experiments later showed that using a set of \textit{orthogonal} hyperplanes, thereby partitioning the space into the Voronoi regions induced by a hypercube, leads to even better results [Terasawa and Tanaka, WADS 2007]. However, no theoretical explanation for this improvement was ever given, and it remained unclear how the resulting hypercube hash method scales in high dimensions.   In this work, we provide explicit asymptotics for the collision probabilities when using hypercubes to partition the space. For instance, two near-orthogonal vectors are expected to collide with probability $(\frac{1}{\pi})^{d + o(d)}$ in dimension $d$, compared to $(\frac{1}{2})^d$ when using random hyperplanes. Vectors at angle $\frac{\pi}{3}$ collide with probability $(\frac{\sqrt{3}}{\pi})^{d + o(d)}$, compared to $(\frac{2}{3})^d$ for random hyperplanes, and near-parallel vectors collide with similar asymptotic probabilities in both cases.   For $c$-approximate nearest neighbor searching, this translates to a decrease in the exponent $\rho$ of locality-sensitive hashing (LSH) methods of a factor up to $\log_2(\pi) \approx 1.652$ compared to hyperplane LSH. For $c = 2$, we obtain $\rho \approx 0.302 + o(1)$ for hypercube LSH, improving upon the $\rho \approx 0.377$ for hyperplane LSH. We further describe how to use hypercube LSH in practice, and we consider an example application in the area of lattice algorithms.",celebr techniqu find near neighbor angular distanc involv use set textit random hyperplan partit space hash region charikar stoc experi later show use set textit orthogon hyperplan therebi partit space voronoi region induc hypercub lead even better result terasawa tanaka wad howev theoret explan improv ever given remain unclear result hypercub hash method scale high dimens work provid explicit asymptot collis probabl use hypercub partit space instanc two near orthogon vector expect collid probabl frac pi dimens compar frac use random hyperplan vector angl frac pi collid probabl frac sqrt pi compar frac random hyperplan near parallel vector collid similar asymptot probabl case approxim nearest neighbor search translat decreas expon rho local sensit hash lsh method factor log pi approx compar hyperplan lsh obtain rho approx hypercub lsh improv upon rho approx hyperplan lsh describ use hypercub lsh practic consid exampl applic area lattic algorithm,['Thijs Laarhoven'],"['cs.DS', 'cs.CC', 'cs.CG', 'cs.CR']",False,False,False,False,False,True
170,2017-03-28T14:07:09Z,2017-02-17T23:43:14Z,http://arxiv.org/abs/1702.05543v1,http://arxiv.org/pdf/1702.05543v1,A Fixed-Parameter Perspective on #BIS,fix paramet perspect bis,"The complexity of approximately counting independent sets in bipartite graphs (#BIS) is a central open problem in approximate counting, and it is widely believed to be neither easy nor NP-hard. We study several natural parameterised variants of #BIS, both from the polynomial-time and from the fixed-parameter viewpoint: counting independent sets of a given size; counting independent sets with a given number of vertices in one vertex class; and counting maximum independent sets among those with a given number of vertices in one vertex class. Among other things, we show that all these problems are NP-hard to approximate within any polynomial ratio. We also show that the first problem is #W[1]-hard to solve exactly but admits an FPTRAS, and the other two are W[1]-hard to approximate within any polynomial ratio. Finally, we show that when restricted to graphs of bounded degree, all three problems admit exact fixed-parameter algorithms with reasonable time complexity.",complex approxim count independ set bipartit graph bis central open problem approxim count wide believ neither easi np hard studi sever natur parameteris variant bis polynomi time fix paramet viewpoint count independ set given size count independ set given number vertic one vertex class count maximum independ set among given number vertic one vertex class among thing show problem np hard approxim within ani polynomi ratio also show first problem hard solv exact admit fptras two hard approxim within ani polynomi ratio final show restrict graph bound degre three problem admit exact fix paramet algorithm reason time complex,"['Radu Curticapean', 'Holger Dell', 'Fedor Fomin', 'Leslie Ann Goldberg', 'John Lapinskas']","['cs.CC', 'F.2.2; G.2.1; G.2.2']",False,False,False,False,False,True
171,2017-03-28T14:07:09Z,2017-02-17T17:48:41Z,http://arxiv.org/abs/1702.05456v1,http://arxiv.org/pdf/1702.05456v1,LCL problems on grids,lcl problem grid,"LCLs or locally checkable labelling problems (e.g. maximal independent set, maximal matching, and vertex colouring) in the LOCAL model of computation are very well-understood in cycles (toroidal 1-dimensional grids): every problem has a complexity of $O(1)$, $\Theta(\log^* n)$, or $\Theta(n)$, and the design of optimal algorithms can be fully automated.   This work develops the complexity theory of LCL problems for toroidal 2-dimensional grids. The complexity classes are the same as in the 1-dimensional case: $O(1)$, $\Theta(\log^* n)$, and $\Theta(n)$. However, given an LCL problem it is undecidable whether its complexity is $\Theta(\log^* n)$ or $\Theta(n)$ in 2-dimensional grids.   Nevertheless, if we correctly guess that the complexity of a problem is $\Theta(\log^* n)$, we can completely automate the design of optimal algorithms. For any problem we can find an algorithm that is of a normal form $A' \circ S_k$, where $A'$ is a finite function, $S_k$ is an algorithm for finding a maximal independent set in $k$th power of the grid, and $k$ is a constant.   With the help of this technique, we study several concrete \lcl{} problems, also in more general settings. For example, for all $d \ge 2$, we prove that:   - $d$-dimensional grids can be $k$-vertex coloured in time $O(\log^* n)$ iff $k \ge 4$,   - $d$-dimensional grids can be $k$-edge coloured in time $O(\log^* n)$ iff $k \ge 2d+1$.   The proof that $3$-colouring of $2$-dimensional grids requires $\Theta(n)$ time introduces a new topological proof technique, which can also be applied to e.g. orientation problems.",lcls local checkabl label problem maxim independ set maxim match vertex colour local model comput veri well understood cycl toroid dimension grid everi problem complex theta log theta design optim algorithm fulli autom work develop complex theori lcl problem toroid dimension grid complex class dimension case theta log theta howev given lcl problem undecid whether complex theta log theta dimension grid nevertheless correct guess complex problem theta log complet autom design optim algorithm ani problem find algorithm normal form circ finit function algorithm find maxim independ set th power grid constant help techniqu studi sever concret lcl problem also general set exampl ge prove dimension grid vertex colour time log iff ge dimension grid edg colour time log iff ge proof colour dimension grid requir theta time introduc new topolog proof techniqu also appli orient problem,"['Sebastian Brandt', 'Juho Hirvonen', 'Janne H. Korhonen', 'Tuomo Lempiäinen', 'Patric R. J. Östergård', 'Christopher Purcell', 'Joel Rybicki', 'Jukka Suomela', 'Przemysław Uznański']","['cs.DC', 'cs.CC', 'cs.DS']",False,False,False,False,False,True
172,2017-03-28T14:07:09Z,2017-02-17T17:20:21Z,http://arxiv.org/abs/1702.05447v1,http://arxiv.org/pdf/1702.05447v1,Counting edge-injective homomorphisms and matchings on restricted graph   classes,count edg inject homomorph match restrict graph class,"We consider the parameterized problem of counting all matchings with exactly $k$ edges in a given input graph $G$. This problem is #W[1]-hard (Curticapean, ICALP 2013), so it is unlikely to admit $f(k)\cdot n^{O(1)}$ time algorithms. We show that #W[1]-hardness persists even when the input graph $G$ comes from restricted graph classes, such as line graphs and bipartite graphs of arbitrary constant girth and maximum degree two on one side. To prove the result for line graphs, we observe that $k$-matchings in line graphs can be equivalently viewed as edge-injective homomorphisms from the disjoint union of $k$ paths of length two into (arbitrary) host graphs. Here, a homomorphism from $H$ to $G$ is edge-injective if it maps any two distinct edges of $H$ to distinct edges in $G$. We show that edge-injective homomorphisms from a pattern graph $H$ can be counted in polynomial time if $H$ has bounded vertex-cover number after removing isolated edges. For hereditary classes $\mathcal{H}$ of pattern graphs, we obtain a full complexity dichotomy theorem by proving that counting edge-injective homomorphisms, restricted to patterns from $\mathcal{H}$, is #W[1]-hard if no such bound exists. Our proofs rely on an edge-colored variant of Holant problems and a delicate interpolation argument; both may be of independent interest.",consid parameter problem count match exact edg given input graph problem hard curticapean icalp unlik admit cdot time algorithm show hard persist even input graph come restrict graph class line graph bipartit graph arbitrari constant girth maximum degre two one side prove result line graph observ match line graph equival view edg inject homomorph disjoint union path length two arbitrari host graph homomorph edg inject map ani two distinct edg distinct edg show edg inject homomorph pattern graph count polynomi time bound vertex cover number remov isol edg hereditari class mathcal pattern graph obtain full complex dichotomi theorem prove count edg inject homomorph restrict pattern mathcal hard bound exist proof reli edg color variant holant problem delic interpol argument may independ interest,"['Radu Curticapean', 'Holger Dell', 'Marc Roth']",['cs.CC'],False,False,False,False,False,True
173,2017-03-28T14:07:09Z,2017-02-17T13:07:58Z,http://arxiv.org/abs/1702.05328v1,http://arxiv.org/pdf/1702.05328v1,On algebraic branching programs of small width,algebra branch program small width,"In 1979 Valiant showed that the complexity class VP_e of families with polynomially bounded formula size is contained in the class VP_s of families that have algebraic branching programs (ABPs) of polynomially bounded size. Motivated by the problem of separating these classes we study the topological closure VP_e-bar, i.e. the class of polynomials that can be approximated arbitrarily closely by polynomials in VP_e. We describe VP_e-bar with a strikingly simple complete polynomial (in characteristic different from 2) whose recursive definition is similar to the Fibonacci numbers. Further understanding this polynomial seems to be a promising route to new formula lower bounds.   Our methods are rooted in the study of ABPs of small constant width. In 1992 Ben-Or and Cleve showed that formula size is polynomially equivalent to width-3 ABP size. We extend their result (in characteristic different from 2) by showing that approximate formula size is polynomially equivalent to approximate width-2 ABP size. This is surprising because in 2011 Allender and Wang gave explicit polynomials that cannot be computed by width-2 ABPs at all! The details of our construction lead to the aforementioned characterization of VP_e-bar.   As a natural continuation of this work we prove that the class VNP can be described as the class of families that admit a hypercube summation of polynomially bounded dimension over a product of polynomially many affine linear forms. This gives the first separations of algebraic complexity classes from their nondeterministic analogs.",valiant show complex class vp famili polynomi bound formula size contain class vp famili algebra branch program abp polynomi bound size motiv problem separ class studi topolog closur vp bar class polynomi approxim arbitrarili close polynomi vp describ vp bar strike simpl complet polynomi characterist differ whose recurs definit similar fibonacci number understand polynomi seem promis rout new formula lower bound method root studi abp small constant width ben cleve show formula size polynomi equival width abp size extend result characterist differ show approxim formula size polynomi equival approxim width abp size surpris becaus allend wang gave explicit polynomi cannot comput width abp detail construct lead aforement character vp bar natur continu work prove class vnp describ class famili admit hypercub summat polynomi bound dimens product polynomi mani affin linear form give first separ algebra complex class nondeterminist analog,"['Karl Bringmann', 'Christian Ikenmeyer', 'Jeroen Zuiddam']","['cs.CC', '68Q15', 'F.1.3']",False,False,False,False,False,True
174,2017-03-28T14:07:09Z,2017-02-16T23:21:34Z,http://arxiv.org/abs/1702.05183v1,http://arxiv.org/pdf/1702.05183v1,Courcelle's Theorem Made Dynamic,courcell theorem made dynam,"Dynamic complexity is concerned with updating the output of a problem when the input is slightly changed. We study the dynamic complexity of model checking a fixed monadic second-order formula over evolving subgraphs of a fixed maximal graph having bounded tree-width; here the subgraph evolves by losing or gaining edges (from the maximal graph). We show that this problem is in DynFO (with LOGSPACE precomputation), via a reduction to a Dyck reachability problem on an acyclic automaton.",dynam complex concern updat output problem input slight chang studi dynam complex model check fix monad second order formula evolv subgraph fix maxim graph bound tree width subgraph evolv lose gain edg maxim graph show problem dynfo logspac precomput via reduct dyck reachabl problem acycl automaton,"['Patricia Bouyer-Decitre', 'Vincent Jugé', 'Nicolas Markey']","['cs.CC', 'cs.FL']",False,False,False,False,False,True
175,2017-03-28T14:07:09Z,2017-02-16T20:02:12Z,http://arxiv.org/abs/1702.05139v1,http://arxiv.org/pdf/1702.05139v1,On the Bit Complexity of Sum-of-Squares Proofs,bit complex sum squar proof,"It has often been claimed in recent papers that one can find a degree d Sum-of-Squares proof if one exists via the Ellipsoid algorithm. In [O17], Ryan O'Donnell notes this widely quoted claim is not necessarily true. He presents an example of a polynomial system with bounded coeffcients that admits low-degree proofs of non-negativity, but these proofs necessarily involve numbers with an exponential number of bits, causing the Ellipsoid algorithm to take exponential time. In this paper we obtain both positive and negative results on the bit complexity of SoS proofs. First, we propose a suffcient condition on a polynomial system that implies a bound on the coefficients in an SoS proof. We demonstrate that this sufficient condition is applicable for common use-cases of the SoS algorithm, such as Max-CSP, Balanced Separator, Max- Clique, Max-Bisection, and Unit-Vector constraints. On the negative side, O'Donnell asked whether every polynomial system containing Boolean constraints admits proofs of polynomial bit complexity. We answer this question in the negative, giving a counterexample system and non-negative polynomial which has degree two SoS proofs, but no SoS proof with small coefficients until degree Omega(sqrt(n))",often claim recent paper one find degre sum squar proof one exist via ellipsoid algorithm ryan donnel note wide quot claim necessarili true present exampl polynomi system bound coeffcient admit low degre proof non negat proof necessarili involv number exponenti number bit caus ellipsoid algorithm take exponenti time paper obtain posit negat result bit complex sos proof first propos suffcient condit polynomi system impli bound coeffici sos proof demonstr suffici condit applic common use case sos algorithm max csp balanc separ max cliqu max bisect unit vector constraint negat side donnel ask whether everi polynomi system contain boolean constraint admit proof polynomi bit complex answer question negat give counterexampl system non negat polynomi degre two sos proof sos proof small coeffici degre omega sqrt,"['Prasad Raghavendra', 'Benjamin Weitz']",['cs.CC'],False,False,False,False,False,True
176,2017-03-28T14:07:09Z,2017-02-15T21:21:34Z,http://arxiv.org/abs/1702.04779v1,http://arxiv.org/pdf/1702.04779v1,Compression Complexity,compress complex,"The Kolmogorov complexity of x, denoted C(x), is the length of the shortest program that generates x. For such a simple definition, Kolmogorov complexity has a rich and deep theory, as well as applications to a wide variety of topics including learning theory, complexity lower bounds and SAT algorithms.   Kolmogorov complexity typically focuses on decompression, going from the compressed program to the original string. This paper develops a dual notion of compression, the mapping from a string to its compressed version. Typical lossless compression algorithms such as Lempel-Ziv or Huffman Encoding always produce a string that will decompress to the original. We define a general compression concept based on this observation.   For every m, we exhibit a single compression algorithm q of length about m which for n and strings x of length n >= m, the output of q will have length within n-m+O(1) bits of C(x). We also show this bound is tight in a strong way, for every n >= m there is an x of length n with C(x) about m such that no compression program of size slightly less than m can compress x at all.   We also consider a polynomial time-bounded version of compression complexity and show that similar results for this version would rule out cryptographic one-way functions.",kolmogorov complex denot length shortest program generat simpl definit kolmogorov complex rich deep theori well applic wide varieti topic includ learn theori complex lower bound sat algorithm kolmogorov complex typic focus decompress go compress program origin string paper develop dual notion compress map string compress version typic lossless compress algorithm lempel ziv huffman encod alway produc string decompress origin defin general compress concept base observ everi exhibit singl compress algorithm length string length output length within bit also show bound tight strong way everi length compress program size slight less compress also consid polynomi time bound version compress complex show similar result version would rule cryptograph one way function,"['Stephen Fenner', 'Lance Fortnow']",['cs.CC'],False,False,False,False,False,True
178,2017-03-28T14:07:09Z,2017-02-16T18:09:15Z,http://arxiv.org/abs/1702.04679v2,http://arxiv.org/pdf/1702.04679v2,The complexity of Boolean surjective general-valued CSPs,complex boolean surject general valu csps,"Valued constraint satisfaction problems (VCSPs) are discrete optimisation problems with a $\overline{\mathbb{Q}}$-valued objective function given as a sum of fixed-arity functions, where $\overline{\mathbb{Q}}=\mathbb{Q}\cup\{\infty\}$ is the set of extended rationals.   In Boolean surjective VCSPs variables take on labels from $D=\{0,1\}$ and an optimal assignment is required to use both labels from $D$. A classic example is the global min-cut problem in graphs. Building on the work of Uppman, we establish a dichotomy theorem and thus give a complete complexity classification of Boolean surjective VCSPs. The newly discovered tractable case has an interesting structure related to projections of downsets and upsets. Our work generalises the dichotomy for $\{0,\infty\}$-valued constraint languages (corresponding to CSPs) obtained by Creignou and H\'ebrard, and the dichotomy for $\{0,1\}$-valued constraint languages (corresponding to Min-CSPs) obtained by Uppman.",valu constraint satisfact problem vcsps discret optimis problem overlin mathbb valu object function given sum fix ariti function overlin mathbb mathbb cup infti set extend ration boolean surject vcsps variabl take label optim assign requir use label classic exampl global min cut problem graph build work uppman establish dichotomi theorem thus give complet complex classif boolean surject vcsps newli discov tractabl case interest structur relat project downset upset work generalis dichotomi infti valu constraint languag correspond csps obtain creignou ebrard dichotomi valu constraint languag correspond min csps obtain uppman,"['Peter Fulla', 'Stanislav Zivny']","['cs.CC', 'cs.DM', 'F.2.0']",False,False,False,False,False,True
179,2017-03-28T14:07:09Z,2017-02-15T01:05:51Z,http://arxiv.org/abs/1702.04432v1,http://arxiv.org/pdf/1702.04432v1,Vertex isoperimetry and independent set stability for tensor powers of   cliques,vertex isoperimetri independ set stabil tensor power cliqu,"The tensor power of the clique on $t$ vertices (denoted by $K_t^n$) is the graph on vertex set $\{1, ..., t\}^n$ such that two vertices $x, y \in \{1, ..., t\}^n$ are connected if and only if $x_i \neq y_i$ for all $i \in \{1, ..., n\}$. Let the density of a subset $S$ of $K_t^n$ to be $\mu(S) := \frac{ S }{t^n}$, and let the vertex boundary of a set $S$ to be vertices which are incident to some vertex of $S$, perhaps including points of $S$. We investigate two similar problems on such graphs.   First, we study the vertex isoperimetry problem. Given a density $\nu \in [0, 1]$ what is the smallest possible density of the vertex boundary of a subset of $K_t^n$ of density $\nu$? Let $\Phi_t(\nu)$ be the infimum of these minimum densities as $n \to \infty$. We find a recursive relation allows one to compute $\Phi_t(\nu)$ in time polynomial to the number of desired bits of precision.   Second, we study given an independent set $I \subseteq K_t^n$ of density $\mu(I) = \frac{1}{t}(1-\epsilon)$, how close it is to a maximum-sized independent set $J$ of density $\frac{1}{t}$. We show that this deviation (measured by $\mu(I \setminus J)$) is at most $4\epsilon^{\frac{\log t}{\log t - \log(t-1)}}$ as long as $\epsilon < 1 - \frac{3}{t} + \frac{2}{t^2}$. This substantially improves on results of Alon, Dinur, Friedgut, and Sudakov (2004) and Ghandehari and Hatami (2008) which had an $O(\epsilon)$ upper bound. We also show the exponent $\frac{\log t}{\log t - \log(t-1)}$ is optimal assuming $n$ tending to infinity and $\epsilon$ tending to $0$. The methods have similarity to recent work by Ellis, Keller, and Lifshitz (2016) in the context of Kneser graphs and other settings.   The author hopes that these results have potential applications in hardness of approximation, particularly in approximate graph coloring and independent set problems.",tensor power cliqu vertic denot graph vertex set two vertic connect onli neq let densiti subset mu frac let vertex boundari set vertic incid vertex perhap includ point investig two similar problem graph first studi vertex isoperimetri problem given densiti nu smallest possibl densiti vertex boundari subset densiti nu let phi nu infimum minimum densiti infti find recurs relat allow one comput phi nu time polynomi number desir bit precis second studi given independ set subseteq densiti mu frac epsilon close maximum size independ set densiti frac show deviat measur mu setminus epsilon frac log log log long epsilon frac frac substanti improv result alon dinur friedgut sudakov ghandehari hatami epsilon upper bound also show expon frac log log log optim assum tend infin epsilon tend method similar recent work elli keller lifshitz context kneser graph set author hope result potenti applic hard approxim particular approxim graph color independ set problem,['Joshua Brakensiek'],"['math.CO', 'cs.CC', 'cs.DM']",False,False,True,False,False,True
180,2017-03-28T14:07:13Z,2017-02-14T18:21:28Z,http://arxiv.org/abs/1702.04322v1,http://arxiv.org/pdf/1702.04322v1,Parameterized Algorithms for Recognizing Monopolar and 2-Subcolorable   Graphs,parameter algorithm recogn monopolar subcolor graph,"A graph $G$ is a $(\Pi_A,\Pi_B)$-graph if $V(G)$ can be bipartitioned into $A$ and $B$ such that $G[A]$ satisfies property $\Pi_A$ and $G[B]$ satisfies property $\Pi_B$. The $(\Pi_{A},\Pi_{B})$-Recognition problem is to recognize whether a given graph is a $(\Pi_A,\Pi_B)$-graph. There are many $(\Pi_{A},\Pi_{B})$-Recognition problems, including the recognition problems for bipartite, split, and unipolar graphs. We present efficient algorithms for many cases of $(\Pi_A,\Pi_B)$-Recognition based on a technique which we dub inductive recognition. In particular, we give fixed-parameter algorithms for two NP-hard $(\Pi_{A},\Pi_{B})$-Recognition problems, Monopolar Recognition and 2-Subcoloring. We complement our algorithmic results with several hardness results for $(\Pi_{A},\Pi_{B})$-Recognition.",graph pi pi graph bipartit satisfi properti pi satisfi properti pi pi pi recognit problem recogn whether given graph pi pi graph mani pi pi recognit problem includ recognit problem bipartit split unipolar graph present effici algorithm mani case pi pi recognit base techniqu dub induct recognit particular give fix paramet algorithm two np hard pi pi recognit problem monopolar recognit subcolor complement algorithm result sever hard result pi pi recognit,"['Iyad Kanj', 'Christian Komusiewicz', 'Manuel Sorge', 'Erik Jan van Leeuwen']","['cs.CC', 'cs.DS']",False,False,False,False,True,True
181,2017-03-28T14:07:13Z,2017-02-14T17:23:31Z,http://arxiv.org/abs/1702.04300v1,http://arxiv.org/pdf/1702.04300v1,Optimality condition and complexity analysis for linearly-constrained   optimization without differentiability on the boundary,optim condit complex analysi linear constrain optim without differenti boundari,"In this paper we consider the minimization of a continuous function that is potentially not differentiable or not twice differentiable on the boundary of the feasible region. By exploiting an interior point technique, we present first- and second-order optimality conditions for this problem that reduces to classical ones when the derivative on the boundary is available. For this type of problems, existing necessary conditions often rely on the notion of subdifferential or become non-trivially weaker than the KKT condition in the (twice-)differentiable counterpart problems. In contrast, this paper presents a new set of first- and second-order necessary conditions that are derived without the use of subdifferential and reduces to exactly the KKT condition when (twice-)differentiability holds. As a result, these conditions are stronger than some existing ones considered for the discussed minimization problem when only non-negativity constraints are present. To solve for these optimality conditions in the special but important case of linearly constrained problems, we present two novel interior trust-region point algorithms and show that their worst-case computational efficiency in achieving the potentially stronger optimality conditions match the best known complexity bounds. Since this work considers a more general problem than the literature, our results also indicate that best known complexity bounds hold for a wider class of nonlinear programming problems.",paper consid minim continu function potenti differenti twice differenti boundari feasibl region exploit interior point techniqu present first second order optim condit problem reduc classic one deriv boundari avail type problem exist necessari condit often reli notion subdifferenti becom non trivial weaker kkt condit twice differenti counterpart problem contrast paper present new set first second order necessari condit deriv without use subdifferenti reduc exact kkt condit twice differenti hold result condit stronger exist one consid discuss minim problem onli non negat constraint present solv optim condit special import case linear constrain problem present two novel interior trust region point algorithm show worst case comput effici achiev potenti stronger optim condit match best known complex bound sinc work consid general problem literatur result also indic best known complex bound hold wider class nonlinear program problem,"['Gabriel Haeser', 'Hongcheng Liu', 'Yinyu Ye']","['cs.CC', 'math.OC', '90C30, 90C51, 90C60, 68Q25']",False,False,False,False,False,True
182,2017-03-28T14:07:13Z,2017-02-14T03:19:55Z,http://arxiv.org/abs/1702.04059v1,http://arxiv.org/pdf/1702.04059v1,Computing geometric Lorenz attractors with arbitrary precision,comput geometr lorenz attractor arbitrari precis,"The Lorenz attractor was introduced in 1963 by E. N. Lorenz as one of the first examples of \emph{strange attractors}. However Lorenz' research was mainly based on (non-rigourous) numerical simulations and, until recently, the proof of the existence of the Lorenz attractor remained elusive. To address that problem some authors introduced geometric Lorenz models and proved that geometric Lorenz models have a strange attractor. In 2002 it was shown that the original Lorenz model behaves like a geometric Lorenz model and thus has a strange attractor. In this paper we show that geometric Lorenz attractors are computable, as well as their physical measures.",lorenz attractor introduc lorenz one first exampl emph strang attractor howev lorenz research main base non rigour numer simul recent proof exist lorenz attractor remain elus address problem author introduc geometr lorenz model prove geometr lorenz model strang attractor shown origin lorenz model behav like geometr lorenz model thus strang attractor paper show geometr lorenz attractor comput well physic measur,"['Daniel Graca', 'Cristobal Rojas', 'Ning Zhong']","['math.DS', 'cs.CC', 'nlin.CD']",False,False,False,False,False,True
183,2017-03-28T14:07:13Z,2017-02-13T10:16:54Z,http://arxiv.org/abs/1702.03700v1,http://arxiv.org/pdf/1702.03700v1,Assortment Optimization under a Single Transition Model,assort optim singl transit model,"In this paper, we consider a Markov chain choice model with single transition. In this model, customers arrive at each product with a certain probability. If the arrived product is unavailable, then the seller can recommend a subset of available products to the customer and the customer will purchase one of the recommended products or choose not to purchase with certain transition probabilities. The distinguishing features of the model are that the seller can control which products to recommend depending on the arrived product and that each customer either purchases a product or leaves the market after one transition.   We study the assortment optimization problem under this model. Particularly, we show that this problem is generally NP-Hard even if each product could only transit to at most two products. Despite the complexity of the problem, we provide polynomial time algorithms for several special cases, such as when the transition probabilities are homogeneous with respect to the starting point, or when each product can only transit to one other product. We also provide a tight performance bound for revenue-ordered assortments. In addition, we propose a compact mixed integer program formulation that can solve this problem of large size. Through extensive numerical experiments, we show that the proposed algorithms can solve the problem efficiently and the obtained assortments could significantly improve the revenue of the seller than under the Markov chain choice model.",paper consid markov chain choic model singl transit model custom arriv product certain probabl arriv product unavail seller recommend subset avail product custom custom purchas one recommend product choos purchas certain transit probabl distinguish featur model seller control product recommend depend arriv product custom either purchas product leav market one transit studi assort optim problem model particular show problem general np hard even product could onli transit two product despit complex problem provid polynomi time algorithm sever special case transit probabl homogen respect start point product onli transit one product also provid tight perform bound revenu order assort addit propos compact mix integ program formul solv problem larg size extens numer experi show propos algorithm solv problem effici obtain assort could signific improv revenu seller markov chain choic model,"['Kameng Nip', 'Zhenbo Wang', 'Zizhuo Wang']","['math.OC', 'cs.CC']",False,False,True,False,False,True
184,2017-03-28T14:07:13Z,2017-02-13T04:23:37Z,http://arxiv.org/abs/1702.03625v1,http://arxiv.org/pdf/1702.03625v1,Separation of AC$^0[\oplus]$ Formulas and Circuits,separ ac oplus formula circuit,"This paper gives the first separation between the power of {\em formulas} and {\em circuits} of equal depth in the $\mathrm{AC}^0[\oplus]$ basis (unbounded fan-in AND, OR, NOT and MOD$_2$ gates). We show, for all $d(n) \le O(\frac{\log n}{\log\log n})$, that there exist {\em polynomial-size depth-$d$ circuits} that are not equivalent to {\em depth-$d$ formulas of size $n^{o(d)}$} (moreover, this is optimal in that $n^{o(d)}$ cannot be improved to $n^{O(d)}$). This result is obtained by a combination of new lower and upper bounds for {\em Approximate Majorities}, the class of Boolean functions $\{0,1\}^n \to \{0,1\}$ that agree with the Majority function on $3/4$ fraction of inputs.   $\mathrm{AC}^0[\oplus]$ formula lower bound: We show that every depth-$d$ $\mathrm{AC}^0[\oplus]$ formula of size $s$ has a {\em $1/8$-error polynomial approximation} over $\mathbb{F}_2$ of degree $O(\frac{1}{d}\log s)^{d-1}$. This strengthens a classic $O(\log s)^{d-1}$ degree approximation for \underline{circuits} due to Razborov. Since the Majority function has approximate degree $\Theta(\sqrt n)$, this result implies an $\exp(\Omega(dn^{1/2(d-1)}))$ lower bound on the depth-$d$ $\mathrm{AC}^0[\oplus]$ formula size of all Approximate Majority functions for all $d(n) \le O(\log n)$.   Monotone $\mathrm{AC}^0$ circuit upper bound: For all $d(n) \le O(\frac{\log n}{\log\log n})$, we give a randomized construction of depth-$d$ monotone $\mathrm{AC}^0$ circuits (without NOT or MOD$_2$ gates) of size $\exp(O(n^{1/2(d-1)}))$ that compute an Approximate Majority function. This strengthens a construction of \underline{formulas} of size $\exp(O(dn^{1/2(d-1)}))$ due to Amano.",paper give first separ power em formula em circuit equal depth mathrm ac oplus basi unbound fan mod gate show le frac log log log exist em polynomi size depth circuit equival em depth formula size moreov optim cannot improv result obtain combin new lower upper bound em approxim major class boolean function agre major function fraction input mathrm ac oplus formula lower bound show everi depth mathrm ac oplus formula size em error polynomi approxim mathbb degre frac log strengthen classic log degre approxim underlin circuit due razborov sinc major function approxim degre theta sqrt result impli exp omega dn lower bound depth mathrm ac oplus formula size approxim major function le log monoton mathrm ac circuit upper bound le frac log log log give random construct depth monoton mathrm ac circuit without mod gate size exp comput approxim major function strengthen construct underlin formula size exp dn due amano,"['Benjamin Rossman', 'Srikanth Srinivasan']",['cs.CC'],False,False,False,False,False,True
185,2017-03-28T14:07:13Z,2017-02-10T12:43:46Z,http://arxiv.org/abs/1702.03152v1,http://arxiv.org/pdf/1702.03152v1,A Variation of Levin Search for All Well-Defined Problems,variat levin search well defin problem,"In 1973, L.A. Levin published an algorithm that solves any inversion problem $\pi$ as quickly as the fastest algorithm $p^*$ computing a solution for $\pi$ in time bounded by $2^{l(p^*)}.t^*$, where $l(p^*)$ is the length of the binary encoding of $p^*$, and $t^*$ is the runtime of $p^*$ plus the time to verify its correctness. In 2002, M. Hutter published an algorithm that solves any well-defined problem $\pi$ as quickly as the fastest algorithm $p^*$ computing a solution for $\pi$ in time bounded by $5.t_{p}(x)+d_p.time_{t_{p}}(x)+c_p$, where $d_p=40.2^{l(p)+l(t_{p})}$ and $c_p=40.2^{l(f)+1}.O(l(f)^2)$, where $l(f)$ is the length of the binary encoding of a proof $f$ that produces a pair $(p,t_p)$, where $t_p(x)$ is a provable time bound on the runtime of the fastest program $p$ provably equivalent to $p^*$. In this paper, we rewrite Levin Search using the ideas of Hutter so that we have a new simple algorithm that solves any well-defined problem $\pi$ as quickly as the fastest algorithm $p^*$ computing a solution for $\pi$ in time bounded by $O(l(f)^2).t_p(x)$.",levin publish algorithm solv ani invers problem pi quick fastest algorithm comput solut pi time bound length binari encod runtim plus time verifi correct hutter publish algorithm solv ani well defin problem pi quick fastest algorithm comput solut pi time bound time length binari encod proof produc pair provabl time bound runtim fastest program provabl equival paper rewrit levin search use idea hutter new simpl algorithm solv ani well defin problem pi quick fastest algorithm comput solut pi time bound,['Fouad B. Chedid'],"['cs.CC', 'cs.DS']",False,False,False,False,False,True
186,2017-03-28T14:07:13Z,2017-02-09T16:50:23Z,http://arxiv.org/abs/1702.02890v1,http://arxiv.org/pdf/1702.02890v1,Answer Set Solving with Bounded Treewidth Revisited,answer set solv bound treewidth revisit,"Parameterized algorithms are a way to solve hard problems more efficiently, given that a specific parameter of the input is small. In this paper, we apply this idea to the field of answer set programming (ASP). To this end, we propose two kinds of graph representations of programs to exploit their treewidth as a parameter. Treewidth roughly measures to which extent the internal structure of a program resembles a tree. Our main contribution is the design of parameterized dynamic programming algorithms, which run in linear time if the treewidth and weights of the given program are bounded. Compared to previous work, our algorithms handle the full syntax of ASP. Finally, we report on an empirical evaluation that shows good runtime behaviour for benchmark instances of low treewidth, especially for counting answer sets.",parameter algorithm way solv hard problem effici given specif paramet input small paper appli idea field answer set program asp end propos two kind graph represent program exploit treewidth paramet treewidth rough measur extent intern structur program resembl tree main contribut design parameter dynam program algorithm run linear time treewidth weight given program bound compar previous work algorithm handl full syntax asp final report empir evalu show good runtim behaviour benchmark instanc low treewidth especi count answer set,"['Johannes Fichte', 'Markus Hecher', 'Michael Morak', 'Stefan Woltran']","['cs.LO', 'cs.AI', 'cs.CC']",False,False,False,False,False,True
187,2017-03-28T14:07:13Z,2017-02-09T16:32:26Z,http://arxiv.org/abs/1702.02885v1,http://arxiv.org/pdf/1702.02885v1,Sparse Approximation is Provably Hard under Coherent Dictionaries,spars approxim provabl hard coher dictionari,"It is well known that sparse approximation problem is \textsf{NP}-hard under general dictionaries. Several algorithms have been devised and analyzed in the past decade under various assumptions on the \emph{coherence} $\mu$ of the dictionary represented by an $M \times N$ matrix from which a subset of $k$ column vectors is selected. All these results assume $\mu=O(k^{-1})$. This article is an attempt to bridge the big gap between the negative result of \textsf{NP}-hardness under general dictionaries and the positive results under this restrictive assumption. In particular, it suggests that the aforementioned assumption might be asymptotically the best one can make to arrive at any efficient algorithmic result under well-known conjectures of complexity theory. In establishing the results, we make use of a new simple multilayered PCP which is tailored to give a matrix with small coherence combined with our reduction.",well known spars approxim problem textsf np hard general dictionari sever algorithm devis analyz past decad various assumpt emph coher mu dictionari repres time matrix subset column vector select result assum mu articl attempt bridg big gap negat result textsf np hard general dictionari posit result restrict assumpt particular suggest aforement assumpt might asymptot best one make arriv ani effici algorithm result well known conjectur complex theori establish result make use new simpl multilay pcp tailor give matrix small coher combin reduct,['Ali Çivril'],"['cs.CC', 'cs.IT', 'math.IT']",False,False,False,False,False,True
188,2017-03-28T14:07:13Z,2017-02-20T16:33:24Z,http://arxiv.org/abs/1702.02882v4,http://arxiv.org/pdf/1702.02882v4,Improved Inapproximability Results for Steiner Tree via Long Code Based   Reductions,improv inapproxim result steiner tree via long code base reduct,"The best algorithm for approximating Steiner tree has performance ratio $\ln(4)+\epsilon \approx 1.386$ [J. Byrka et al., \textit{Proceedings of the 42th Annual ACM Symposium on Theory of Computing (STOC)}, 2010, pp. 583-592], whereas the inapproximability result stays at the factor $\frac{96}{95} \approx 1.0105$ [M. Chleb\'ik and J. Chleb\'ikov\'a, \textit{Proceedings of the 8th Scandinavian Workshop on Algorithm Theory (SWAT)}, 2002, pp. 170-179]. In this article, we take a step forward to bridge this gap and show that there is no polynomial time algorithm approximating Steiner tree with constant ratio better than $\frac{19}{18} \approx 1.0555$ unless \textsf{P = NP}. We also relate the problem to the Unique Games Conjecture by showing that it is \textsf{UG}-hard to find a constant approximation ratio better than $\frac{17}{16} = 1.0625$. In the special case of quasi-bipartite graphs, we prove an inapproximability factor of $\frac{25}{24} \approx 1.0416$ unless \textsf{P = NP}, which improves upon the previous bound of $\frac{128}{127} \approx 1.0078$. The reductions that we present for all the cases are of the same spirit with appropriate modifications. Our main technical contribution is an adaptation of a Set-Cover type reduction in which the Long Code is used to the geometric setting of the problems we consider.",best algorithm approxim steiner tree perform ratio ln epsilon approx byrka et al textit proceed th annual acm symposium theori comput stoc pp wherea inapproxim result stay factor frac approx chleb ik chleb ikov textit proceed th scandinavian workshop algorithm theori swat pp articl take step forward bridg gap show polynomi time algorithm approxim steiner tree constant ratio better frac approx unless textsf np also relat problem uniqu game conjectur show textsf ug hard find constant approxim ratio better frac special case quasi bipartit graph prove inapproxim factor frac approx unless textsf np improv upon previous bound frac approx reduct present case spirit appropri modif main technic contribut adapt set cover type reduct long code use geometr set problem consid,['Ali Çivril'],['cs.CC'],False,False,False,False,False,True
190,2017-03-28T14:07:17Z,2017-02-09T13:18:08Z,http://arxiv.org/abs/1702.02821v1,http://arxiv.org/pdf/1702.02821v1,Phase Transitions of the Typical Algorithmic Complexity of the Random   Satisfiability Problem Studied with Linear Programming,phase transit typic algorithm complex random satisfi problem studi linear program,"The Boolean Satisfiability problem asks if a Boolean formula is satisfiable by some assignment of the variables or not. It belongs to the NP-complete complexity class and hence no algorithm with polynomial time worst-case complexity is known, i.e., the problem is hard. The K-SAT problem is the subset of the Boolean Satisfiability problem, for which the Boolean formula has the conjunctive normal form with K literals per clause. This problem is still NP-complete for $K \ge 3$. Although the worst case complexity of NP-complete problems is conjectured to be exponential, there might be subsets of the realizations where solutions can typically be found in polynomial time. In fact, random $K$-SAT, with the number of clauses to number of variables ratio $\alpha$ as control parameter, shows a phase transition between a satisfiable phase and an unsatisfiable phase, at which the hardest problems are located. We use here several linear programming approaches to reveal further ""easy-hard"" transition points at which the typical hardness of the problems increases which means that such algorithms can solve the problem on one side efficiently but not beyond this point. For one of these transitions, we observed a coincidence with a structural transition of the literal factor graphs of the problem instances. We also investigated cutting-plane approaches, which often increase the computational efficiency. Also we tried out a mapping to another NP-complete optimization problem using a specific algorithm for that problem. In both cases, no improvement of the performance was observed, i.e., no shift of the easy-hard transition to higher values of $\alpha$.",boolean satisfi problem ask boolean formula satisfi assign variabl belong np complet complex class henc algorithm polynomi time worst case complex known problem hard sat problem subset boolean satisfi problem boolean formula conjunct normal form liter per claus problem still np complet ge although worst case complex np complet problem conjectur exponenti might subset realize solut typic found polynomi time fact random sat number claus number variabl ratio alpha control paramet show phase transit satisfi phase unsatisfi phase hardest problem locat use sever linear program approach reveal easi hard transit point typic hard problem increas mean algorithm solv problem one side effici beyond point one transit observ coincid structur transit liter factor graph problem instanc also investig cut plane approach often increas comput effici also tri map anoth np complet optim problem use specif algorithm problem case improv perform observ shift easi hard transit higher valu alpha,"['Hendrik Schawe', 'Roman Bleim', 'Alexander K. Hartmann']","['cond-mat.dis-nn', 'cond-mat.stat-mech', 'cs.AI', 'cs.CC']",False,False,False,False,False,True
191,2017-03-28T14:07:17Z,2017-02-09T03:51:01Z,http://arxiv.org/abs/1702.02693v1,http://arxiv.org/pdf/1702.02693v1,Dichotomy for Real Holant$^c$ Problems,dichotomi real holant problem,"Holant problems capture a class of Sum-of-Product computations such as counting matchings. It is inspired by holographic algorithms and is equivalent to tensor networks, with counting CSP being a special case. A classification for Holant problems is more difficult to prove, not only because it implies a classification for counting CSP, but also due to the deeper reason that there exist more intricate polynomial time tractable problems in the broader framework.   We discover a new family of constraint functions $\mathscr{L}$ which define polynomial time computable counting problems. These do not appear in counting CSP, and no newly discovered tractable constraints can be symmetric. It has a delicate support structure related to error-correcting codes. Local holographic transformations is fundamental in its tractability. We prove a complexity dichotomy theorem for all Holant problems defined by any real valued constraint function set on Boolean variables and contains two 0-1 pinning functions. Previously, dichotomy for the same framework was only known for symmetric constraint functions. he set $\mathscr{L}$ supplies the last piece of tractability. We also prove a dichotomy for a variant of counting CSP as a technical component toward this Holant dichotomy.",holant problem captur class sum product comput count match inspir holograph algorithm equival tensor network count csp special case classif holant problem difficult prove onli becaus impli classif count csp also due deeper reason exist intric polynomi time tractabl problem broader framework discov new famili constraint function mathscr defin polynomi time comput count problem appear count csp newli discov tractabl constraint symmetr delic support structur relat error correct code local holograph transform fundament tractabl prove complex dichotomi theorem holant problem defin ani real valu constraint function set boolean variabl contain two pin function previous dichotomi framework onli known symmetr constraint function set mathscr suppli last piec tractabl also prove dichotomi variant count csp technic compon toward holant dichotomi,"['Jin-Yi Cai', 'Pinyan Lu', 'Mingji Xia']","['cs.CC', 'cs.DS']",False,False,False,False,False,True
192,2017-03-28T14:07:17Z,2017-02-08T12:08:22Z,http://arxiv.org/abs/1702.01666v2,http://arxiv.org/pdf/1702.01666v2,On the Complexity of Estimating Renyi Divergences,complex estim renyi diverg,"This paper studies the complexity of estimating Renyi divergences of discrete distributions: $p$ observed from samples and the baseline distribution $q$ known \emph{a priori}. Extending the results of Acharya et al. (SODA'15) on estimating Renyi entropy, we present improved estimation techniques together with upper and lower bounds on the sample complexity.   We show that, contrarily to estimating Renyi entropy where a sublinear (in the alphabet size) number of samples suffices, the sample complexity is heavily dependent on \emph{events occurring unlikely} in $q$, and is unbounded in general (no matter what an estimation technique is used). For any divergence of order bigger than $1$, we provide upper and lower bounds on the number of samples dependent on probabilities of $p$ and $q$. We conclude that the worst-case sample complexity is polynomial in the alphabet size if and only if the probabilities of $q$ are non-negligible.   This gives theoretical insights into heuristics used in applied papers to handle numerical instability, which occurs for small probabilities of $q$. Our result explains that small probabilities should be handled with care not only because of numerical issues, but also because of a blow up in sample complexity.",paper studi complex estim renyi diverg discret distribut observ sampl baselin distribut known emph priori extend result acharya et al soda estim renyi entropi present improv estim techniqu togeth upper lower bound sampl complex show contrarili estim renyi entropi sublinear alphabet size number sampl suffic sampl complex heavili depend emph event occur unlik unbound general matter estim techniqu use ani diverg order bigger provid upper lower bound number sampl depend probabl conclud worst case sampl complex polynomi alphabet size onli probabl non neglig give theoret insight heurist use appli paper handl numer instabl occur small probabl result explain small probabl handl care onli becaus numer issu also becaus blow sampl complex,['Maciej Skorski'],"['cs.IT', 'cs.CC', 'math.IT', 'H.1.1']",False,False,False,False,False,True
193,2017-03-28T14:07:17Z,2017-02-05T21:24:18Z,http://arxiv.org/abs/1702.01454v1,http://arxiv.org/pdf/1702.01454v1,Property Testing of Joint Distributions using Conditional Samples,properti test joint distribut use condit sampl,"In this paper, we present the first non-trivial property tester for joint probability distributions in the recently introduced conditional sampling model. The conditional sampling framework provides an oracle for a distribution $\mu$ that takes as input a subset $S$ of the domain $\Omega$ and returns a sample from the distribution $\mu$ conditioned on $S$.For a joint distribution of dimension $n$, we give a $\tilde{\mathcal{O}}(n^3)$-query uniformity tester, a $\tilde{\mathcal{O}}(n^3)$-query identity tester with a known distribution, and a $\tilde{\mathcal{O}}(n^6)$-query tester for testing independence of marginals. Our technique involves an elegant chain rule which can be proved using basic techniques of probability theory, yet powerful enough to avoid the curse of dimensionality.   We also prove a sample complexity lower bound of $\Omega(\sqrt[4]{n})$ for testing uniformity of a joint distribution when the tester is only allowed to condition independently on the marginals. Our technique involves novel relations between Hellinger distance and total variational distance, and may be of independent interest.",paper present first non trivial properti tester joint probabl distribut recent introduc condit sampl model condit sampl framework provid oracl distribut mu take input subset domain omega return sampl distribut mu condit joint distribut dimens give tild mathcal queri uniform tester tild mathcal queri ident tester known distribut tild mathcal queri tester test independ margin techniqu involv eleg chain rule prove use basic techniqu probabl theori yet power enough avoid curs dimension also prove sampl complex lower bound omega sqrt test uniform joint distribut tester onli allow condit independ margin techniqu involv novel relat helling distanc total variat distanc may independ interest,"['Rishiraj Bhattacharyya', 'Sourav Chakraborty']",['cs.CC'],False,False,False,False,False,True
194,2017-03-28T14:07:17Z,2017-02-05T16:21:35Z,http://arxiv.org/abs/1702.01423v1,http://arxiv.org/pdf/1702.01423v1,Deciding Irreducibility/Indecomposability of Feedback Shift Registers is   NP-hard,decid irreduc indecompos feedback shift regist np hard,Feedback shift registers(FSRs) are a fundamental component in electronics and secure communication. An FSR $f$ is said to be reducible if all the output sequences of another FSR $g$ can also be generated by $f$ and the FSR $g$ has less memory than $f$. An FSR is said to be decomposable if it has the same set of output sequences as a cascade connection of two FSRs. It is proved that deciding whether FSRs are irreducible/indecomposable is NP-hard.,feedback shift regist fsrs fundament compon electron secur communic fsr said reduc output sequenc anoth fsr also generat fsr less memori fsr said decompos set output sequenc cascad connect two fsrs prove decid whether fsrs irreduc indecompos np hard,['Lin Wang'],"['cs.CC', '68Q25, 94A55, 94C15']",False,False,True,False,False,True
195,2017-03-28T14:07:17Z,2017-02-03T22:34:34Z,http://arxiv.org/abs/1702.02017v1,http://arxiv.org/pdf/1702.02017v1,Pushing the Bounds for Matrix-Matrix Multiplication,push bound matrix matrix multipl,"A tight lower bound for required I/O when computing a matrix-matrix multiplication on a processor with two layers of memory is established. Prior work obtained weaker lower bounds by reasoning about the number of \textit{phases} needed to perform $C:=AB$, where each phase is a series of operations involving $S$ reads and writes to and from fast memory, and $S$ is the size of fast memory. A lower bound on the number of phases was then determined by obtaining an upper bound on the number of scalar multiplications performed per phase. This paper follows the same high level approach, but improves the lower bound by considering $C:=AB+C$ instead of $C:=AB$, and obtains the maximum number of scalar fused multiply-adds (FMAs) per phase instead of scalar additions. Key to obtaining the new result is the decoupling of the per-phase I/O from the size of fast memory. The new lower bound is $2mnk/\sqrt{S}-2S$. The constant for the leading term is an improvement of a factor $4\sqrt{2}$. A theoretical algorithm that attains the lower bound is given, and how the state-of-the-art Goto's algorithm also in some sense meets the lower bound is discussed.",tight lower bound requir comput matrix matrix multipl processor two layer memori establish prior work obtain weaker lower bound reason number textit phase need perform ab phase seri oper involv read write fast memori size fast memori lower bound number phase determin obtain upper bound number scalar multipl perform per phase paper follow high level approach improv lower bound consid ab instead ab obtain maximum number scalar fuse multipli add fmas per phase instead scalar addit key obtain new result decoupl per phase size fast memori new lower bound mnk sqrt constant lead term improv factor sqrt theoret algorithm attain lower bound given state art goto algorithm also sens meet lower bound discuss,"['Tyler Michael Smith', 'Robert A. van de Geijn']",['cs.CC'],False,False,False,False,False,True
196,2017-03-28T14:07:17Z,2017-02-02T18:01:03Z,http://arxiv.org/abs/1702.00767v1,http://arxiv.org/pdf/1702.00767v1,A new Holant dichotomy inspired by quantum computation,new holant dichotomi inspir quantum comput,"Holant problems are a framework for the analysis of counting complexity problems on graphs. This framework is simultaneously general enough to encompass many other counting problems on graphs and specific enough to allow the derivation of dichotomy results, partitioning all problem instances into those which can be solved in polynomial time and those which are #P-hard. The Holant framework is based on the theory of holographic algorithms, which was originally inspired by concepts from quantum computation, but this connection appears not to have been explored before.   Here, we employ quantum information theory to explain existing results in a concise way and to derive a dichotomy for a new family of problems, which we call Holant$^+$. This family sits in between the known families of Holant$^*$, for which a full dichotomy is known, and Holant$^c$, for which only a restricted dichotomy is known. Using knowledge from entanglement theory -- both previously existing work and new results of our own -- we prove a full dichotomy theorem for Holant$^+$, which is very similar to the restricted Holant$^c$ dichotomy. Other than the dichotomy for #R$_3$-CSP, ours is the first Holant dichotomy in which the allowed functions are not restricted and in which only a finite number of functions are assumed to be freely available.",holant problem framework analysi count complex problem graph framework simultan general enough encompass mani count problem graph specif enough allow deriv dichotomi result partit problem instanc solv polynomi time hard holant framework base theori holograph algorithm origin inspir concept quantum comput connect appear explor befor employ quantum inform theori explain exist result concis way deriv dichotomi new famili problem call holant famili sit known famili holant full dichotomi known holant onli restrict dichotomi known use knowledg entangl theori previous exist work new result prove full dichotomi theorem holant veri similar restrict holant dichotomi dichotomi csp first holant dichotomi allow function restrict onli finit number function assum freeli avail,['Miriam Backens'],"['quant-ph', 'cs.CC']",False,False,False,False,False,True
197,2017-03-28T14:07:17Z,2017-02-02T07:20:18Z,http://arxiv.org/abs/1702.00558v1,http://arxiv.org/pdf/1702.00558v1,Irreducibility and r-th root finding over finite fields,irreduc th root find finit field,"Constructing $r$-th nonresidue over a finite field is a fundamental computational problem. A related problem is to construct an irreducible polynomial of degree $r^e$ (where $r$ is a prime) over a given finite field $\mathbb{F}_q$ of characteristic $p$ (equivalently, constructing the bigger field $\mathbb{F}_{q^{r^e}}$). Both these problems have famous randomized algorithms but the derandomization is an open question. We give some new connections between these two problems and their variants.   In 1897, Stickelberger proved that if a polynomial has an odd number of even degree factors, then its discriminant is a quadratic nonresidue in the field. We give an extension of Stickelberger's Lemma; we construct $r$-th nonresidues from a polynomial $f$ for which there is a $d$, such that, $r d$ and $r\nmid\,$#(irreducible factor of $f(x)$ of degree $d$). Our theorem has the following interesting consequences: (1) we can construct $\mathbb{F}_{q^m}$ in deterministic poly(deg($f$),$m\log q$)-time if $m$ is an $r$-power and $f$ is known; (2) we can find $r$-th roots in $\mathbb{F}_{p^m}$ in deterministic poly($m\log p$)-time if $r$ is constant and $r \gcd(m,p-1)$.   We also discuss a conjecture significantly weaker than the Generalized Riemann hypothesis to get a deterministic poly-time algorithm for $r$-th root finding.",construct th nonresidu finit field fundament comput problem relat problem construct irreduc polynomi degre prime given finit field mathbb characterist equival construct bigger field mathbb problem famous random algorithm derandom open question give new connect two problem variant stickelberg prove polynomi odd number even degre factor discrimin quadrat nonresidu field give extens stickelberg lemma construct th nonresidu polynomi nmid irreduc factor degre theorem follow interest consequ construct mathbb determinist poli deg log time power known find th root mathbb determinist poli log time constant gcd also discuss conjectur signific weaker general riemann hypothesi get determinist poli time algorithm th root find,"['Vishwas Bhargava', 'Gábor Ivanyos', 'Rajat Mittal', 'Nitin Saxena']","['cs.CC', 'math.AC', 'math.NT']",False,False,False,False,False,True
198,2017-03-28T14:07:17Z,2017-02-02T03:23:39Z,http://arxiv.org/abs/1702.00533v1,http://arxiv.org/pdf/1702.00533v1,Complexity results for $k$-domination and $α$-domination problems   and their variants,complex result domin domin problem variant,"Let $G=(V, E)$ be a simple and undirected graph. For some integer $k\geq 1$, a set $D\subseteq V$ is said to be a k-dominating set in $G$ if every vertex $v$ of $G$ outside $D$ has at least $k$ neighbors in $D$. Furthermore, for some real number $\alpha$ with $0<\alpha\leq1$, a set $D\subseteq V$ is called an $\alpha$-dominating set in $G$ if every vertex $v$ of $G$ outside $D$ has at least $\alpha\times d_v$ neighbors in $D$, where $d_v$ is the degree of $v$ in $G$. The cardinality of a minimum $k$-dominating set and a minimum $\alpha$-dominating set in $G$ is said to be the $k$-domination number and the $\alpha$-domination number of $G$, respectively. In this paper, we present some approximability and inapproximability results on the problem of finding $k$-domination number and $\alpha$-domination number of some classes of graphs. Moreover, we introduce a generalization of $\alpha$-dominating set which we call an $f$-dominating set. Given a function $f:\mathbb{N}\rightarrow \mathbb{R}$, where $\mathbb{N}=\{1, 2, 3, \ldots\}$, a set $D\subseteq V$ is said to be an $f$-dominating set in $G$ if every vertex $v$ of $G$ outside $D$ has at least $f(d_v)$ neighbors in $D$. We prove NP-hardness of the problem of finding a minimum $f$-dominating set in $G$, for a large family of functions $f$.",let simpl undirect graph integ geq set subseteq said domin set everi vertex outsid least neighbor furthermor real number alpha alpha leq set subseteq call alpha domin set everi vertex outsid least alpha time neighbor degre cardin minimum domin set minimum alpha domin set said domin number alpha domin number respect paper present approxim inapproxim result problem find domin number alpha domin number class graph moreov introduc general alpha domin set call domin set given function mathbb rightarrow mathbb mathbb ldot set subseteq said domin set everi vertex outsid least neighbor prove np hard problem find minimum domin set larg famili function,"['Davood Bakhshesh', 'Mohammad Farshi', 'Mahdieh Hasheminezhad']","['cs.CC', 'math.CO', '05C69, 68R05, 68Q25']",False,False,False,False,False,True
199,2017-03-28T14:07:17Z,2017-02-01T21:54:41Z,http://arxiv.org/abs/1702.00467v1,http://arxiv.org/pdf/1702.00467v1,"The Computer Science and Physics of Community Detection: Landscapes,   Phase Transitions, and Hardness",comput scienc physic communiti detect landscap phase transit hard,"Community detection in graphs is the problem of finding groups of vertices which are more densely connected than they are to the rest of the graph. This problem has a long history, but it is currently motivated by social and biological networks. While there are many ways to formalize it, one of the most popular is as an inference problem, where there is a planted ""ground truth"" community structure around which the graph is generated probabilistically. Our task is then to recover the ground truth knowing only the graph.   Recently it was discovered, first heuristically in physics and then rigorously in probability and computer science, that this problem has a phase transition at which it suddenly becomes impossible. Namely, if the graph is too sparse, or the probabilistic process that generates it is too noisy, then no algorithm can find a partition that is correlated with the planted one---or even tell if there are communities, i.e., distinguish the graph from a purely random one with high probability. Above this information-theoretic threshold, there is a second threshold beyond which polynomial-time algorithms are known to succeed; in between, there is a regime in which community detection is possible, but conjectured to be exponentially hard.   For computer scientists, this field offers a wealth of new ideas and open questions, with connections to probability and combinatorics, message-passing algorithms, and random matrix theory. Perhaps more importantly, it provides a window into the cultures of statistical physics and statistical inference, and how those cultures think about distributions of instances, landscapes of solutions, and hardness.",communiti detect graph problem find group vertic dens connect rest graph problem long histori current motiv social biolog network mani way formal one popular infer problem plant ground truth communiti structur around graph generat probabilist task recov ground truth know onli graph recent discov first heurist physic rigor probabl comput scienc problem phase transit sudden becom imposs name graph spars probabilist process generat noisi algorithm find partit correl plant one even tell communiti distinguish graph pure random one high probabl abov inform theoret threshold second threshold beyond polynomi time algorithm known succeed regim communiti detect possibl conjectur exponenti hard comput scientist field offer wealth new idea open question connect probabl combinator messag pass algorithm random matrix theori perhap import provid window cultur statist physic statist infer cultur think distribut instanc landscap solut hard,['Cristopher Moore'],"['cs.CC', 'cond-mat.stat-mech', 'cs.SI', 'math.PR', 'physics.soc-ph']",False,False,False,False,False,True
202,2017-03-28T14:07:16Z,2017-03-21T18:50:24Z,http://arxiv.org/abs/1703.07387v1,http://arxiv.org/pdf/1703.07387v1,"Topological Analysis of Nerves, Reeb Spaces, Mappers, and Multiscale   Mappers",topolog analysi nerv reeb space mapper multiscal mapper,"Data analysis often concerns not only the space where data come from, but also various types of maps attached to data. In recent years, several related structures have been used to study maps on data, including Reeb spaces, mappers and multiscale mappers. The construction of these structures also relies on the so-called \emph{nerve} of a cover of the domain.   In this paper, we aim to analyze the topological information encoded in these structures in order to provide better understanding of these structures and facilitate their practical usage.   More specifically, we show that the one-dimensional homology of the nerve complex $N(\mathcal{U})$ of a path-connected cover $\mathcal{U}$ of a domain $X$ cannot be richer than that of the domain $X$ itself. Intuitively, this result means that no new $H_1$-homology class can be ""created"" under a natural map from $X$ to the nerve complex $N(\mathcal{U})$. Equipping $X$ with a pseudometric $d$, we further refine this result and characterize the classes of $H_1(X)$ that may survive in the nerve complex using the notion of \emph{size} of the covering elements in $\mathcal{U}$. These fundamental results about nerve complexes then lead to an analysis of the $H_1$-homology of Reeb spaces, mappers and multiscale mappers.   The analysis of $H_1$-homology groups unfortunately does not extend to higher dimensions. Nevertheless, by using a map-induced metric, establishing a Gromov-Hausdorff convergence result between mappers and the domain, and interleaving relevant modules, we can still analyze the persistent homology groups of (multiscale) mappers to establish a connection to Reeb spaces.",data analysi often concern onli space data come also various type map attach data recent year sever relat structur use studi map data includ reeb space mapper multiscal mapper construct structur also reli call emph nerv cover domain paper aim analyz topolog inform encod structur order provid better understand structur facilit practic usag specif show one dimension homolog nerv complex mathcal path connect cover mathcal domain cannot richer domain intuit result mean new homolog class creat natur map nerv complex mathcal equip pseudometr refin result character class may surviv nerv complex use notion emph size cover element mathcal fundament result nerv complex lead analysi homolog reeb space mapper multiscal mapper analysi homolog group unfortun doe extend higher dimens nevertheless use map induc metric establish gromov hausdorff converg result mapper domain interleav relev modul still analyz persist homolog group multiscal mapper establish connect reeb space,"['Tamal K. Dey', 'Facundo Memoli', 'Yusu Wang']","['cs.CG', 'math.AT']",False,False,True,False,False,True
204,2017-03-28T14:07:16Z,2017-03-19T22:13:17Z,http://arxiv.org/abs/1703.06526v1,http://arxiv.org/pdf/1703.06526v1,On Optimal 2- and 3-Planar Graphs,optim planar graph,"A graph is $k$-planar if it can be drawn in the plane such that no edge is crossed more than $k$ times. While for $k=1$, optimal $1$-planar graphs, i.e., those with $n$ vertices and exactly $4n-8$ edges, have been completely characterized, this has not been the case for $k \geq 2$. For $k=2,3$ and $4$, upper bounds on the edge density have been developed for the case of simple graphs by Pach and T\'oth, Pach et al. and Ackerman, which have been used to improve the well-known ""Crossing Lemma"". Recently, we proved that these bounds also apply to non-simple $2$- and $3$-planar graphs without homotopic parallel edges and self-loops.   In this paper, we completely characterize optimal $2$- and $3$-planar graphs, i.e., those that achieve the aforementioned upper bounds. We prove that they have a remarkably simple regular structure, although they might be non-simple. The new characterization allows us to develop notable insights concerning new inclusion relationships with other graph classes.",graph planar drawn plane edg cross time optim planar graph vertic exact edg complet character case geq upper bound edg densiti develop case simpl graph pach oth pach et al ackerman use improv well known cross lemma recent prove bound also appli non simpl planar graph without homotop parallel edg self loop paper complet character optim planar graph achiev aforement upper bound prove remark simpl regular structur although might non simpl new character allow us develop notabl insight concern new inclus relationship graph class,"['Michael A. Bekos', 'Michael Kaufmann', 'Chrysanthi N. Raftopoulou']","['cs.CG', 'cs.DM']",False,False,True,False,False,True
205,2017-03-28T14:07:16Z,2017-03-19T18:48:06Z,http://arxiv.org/abs/1703.06487v1,http://arxiv.org/pdf/1703.06487v1,Anisotropic triangulations via discrete Riemannian Voronoi diagrams,anisotrop triangul via discret riemannian voronoi diagram,"The construction of anisotropic triangulations is desirable for various applications, such as the numerical solving of partial differential equations and the representation of surfaces in graphics. To solve this notoriously difficult problem in a practical way, we introduce the discrete Riemannian Voronoi diagram, a discrete structure that approximates the Riemannian Voronoi diagram. This structure has been implemented and was shown to lead to good triangulations in $\mathbb{R}^2$ and on surfaces embedded in $\mathbb{R}^3$ as detailed in our experimental companion paper.   In this paper, we study theoretical aspects of our structure. Given a finite set of points $\cal P$ in a domain $\Omega$ equipped with a Riemannian metric, we compare the discrete Riemannian Voronoi diagram of $\cal P$ to its Riemannian Voronoi diagram. Both diagrams have dual structures called the discrete Riemannian Delaunay and the Riemannian Delaunay complex. We provide conditions that guarantee that these dual structures are identical. It then follows from previous results that the discrete Riemannian Delaunay complex can be embedded in $\Omega$ under sufficient conditions, leading to an anisotropic triangulation with curved simplices. Furthermore, we show that, under similar conditions, the simplices of this triangulation can be straightened.",construct anisotrop triangul desir various applic numer solv partial differenti equat represent surfac graphic solv notori difficult problem practic way introduc discret riemannian voronoi diagram discret structur approxim riemannian voronoi diagram structur implement shown lead good triangul mathbb surfac embed mathbb detail experiment companion paper paper studi theoret aspect structur given finit set point cal domain omega equip riemannian metric compar discret riemannian voronoi diagram cal riemannian voronoi diagram diagram dual structur call discret riemannian delaunay riemannian delaunay complex provid condit guarante dual structur ident follow previous result discret riemannian delaunay complex embed omega suffici condit lead anisotrop triangul curv simplic furthermor show similar condit simplic triangul straighten,"['Jean-Daniel Boissonnat', 'Mael Rouxel-Labbé', 'Mathijs Wintraecken']",['cs.CG'],False,False,False,False,False,True
206,2017-03-28T14:07:16Z,2017-03-18T15:35:10Z,http://arxiv.org/abs/1703.06307v1,http://arxiv.org/pdf/1703.06307v1,Definition of geometric space around analytic fractal trees using   derivative coordinate funtions,definit geometr space around analyt fractal tree use deriv coordin funtion,The concept of derivative coordinate functions proved useful in the formulation of analytic fractal functions to represent smooth symmetric binary fractal trees [1]. In this paper we introduce a new geometry that defines the fractal space around these fractal trees. We present the canonical and degenerate form of this fractal space and extend the fractal geometrical space to R3 specifically and Rn by a recurrence relation. We also discuss the usage of such fractal geometry.,concept deriv coordin function prove use formul analyt fractal function repres smooth symmetr binari fractal tree paper introduc new geometri defin fractal space around fractal tree present canon degener form fractal space extend fractal geometr space specif rn recurr relat also discuss usag fractal geometri,['Henk Mulder'],['cs.CG'],False,False,False,False,False,True
207,2017-03-28T14:07:16Z,2017-03-18T14:58:57Z,http://arxiv.org/abs/1703.06305v1,http://arxiv.org/pdf/1703.06305v1,Hardness of almost embedding simplicial complexes in $\mathbb R^d$,hard almost embed simplici complex mathbb,"A map $f\colon K\to \mathbb R^d$ of a simplicial complex is an almost embedding if $f(\sigma)\cap f(\tau)=\emptyset$ whenever $\sigma,\tau$ are disjoint simplices of $K$.   Theorem. Fix integers $d,k\ge2$ such that $d=\frac{3k}2+1$.   (a) Assume that $P\ne NP$. Then there exists a finite $k$-dimensional complex $K$ that does not admit an almost embedding in $\mathbb R^d$ but for which there exists an equivariant map $\tilde K\to S^{d-1}$.   (b) The algorithmic problem of recognition almost embeddability of finite $k$-dimensional complexes in $\mathbb R^d$ is NP hard.   The proof is based on the technique from the Matou\v{s}ek-Tancer-Wagner paper (proving an analogous result for embeddings), and on singular versions of the higher-dimensional Borromean rings lemma and a generalized van Kampen--Flores theorem.",map colon mathbb simplici complex almost embed sigma cap tau emptyset whenev sigma tau disjoint simplic theorem fix integ ge frac assum ne np exist finit dimension complex doe admit almost embed mathbb exist equivari map tild algorithm problem recognit almost embedd finit dimension complex mathbb np hard proof base techniqu matou ek tancer wagner paper prove analog result embed singular version higher dimension borromean ring lemma general van kampen flore theorem,"['Arkadiy Skopenkov', 'Martin Tancer']","['math.GT', 'cs.CG']",False,False,False,False,False,True
208,2017-03-28T14:07:16Z,2017-03-17T17:13:58Z,http://arxiv.org/abs/1703.06107v1,http://arxiv.org/pdf/1703.06107v1,Self-approaching paths in simple polygons,self approach path simpl polygon,"We study self-approaching paths that are contained in a simple polygon. A self-approaching path is a directed curve connecting two points such that the Euclidean distance between a point moving along the path and any future position does not increase, that is, for all points $a$, $b$, and $c$ that appear in that order along the curve, $ ac  \ge  bc $. We analyze the properties, and present a characterization of shortest self-approaching paths. In particular, we show that a shortest self-approaching path connecting two points inside a polygon can be forced to use a general class of non-algebraic curves. While this makes it difficult to design an exact algorithm, we show how to find a self-approaching path inside a polygon connecting two points under a model of computation which assumes that we can calculate involute curves of high order. Lastly, we provide an algorithm to test if a given simple polygon is self-approaching, that is, if there exists a self-approaching path for any two points inside the polygon.",studi self approach path contain simpl polygon self approach path direct curv connect two point euclidean distanc point move along path ani futur posit doe increas point appear order along curv ac ge bc analyz properti present character shortest self approach path particular show shortest self approach path connect two point insid polygon forc use general class non algebra curv make difficult design exact algorithm show find self approach path insid polygon connect two point model comput assum calcul involut curv high order last provid algorithm test given simpl polygon self approach exist self approach path ani two point insid polygon,"['Prosenjit Bose', 'Irina Kostitsyna', 'Stefan Langerman']",['cs.CG'],False,False,False,False,False,True
210,2017-03-28T14:07:20Z,2017-03-16T10:22:34Z,http://arxiv.org/abs/1703.05549v1,http://arxiv.org/pdf/1703.05549v1,Minimum Perimeter-Sum Partitions in the Plane,minimum perimet sum partit plane,"Let $P$ be a set of $n$ points in the plane. We consider the problem of partitioning $P$ into two subsets $P_1$ and $P_2$ such that the sum of the perimeters of $\text{CH}(P_1)$ and $\text{CH}(P_2)$ is minimized, where $\text{CH}(P_i)$ denotes the convex hull of $P_i$. The problem was first studied by Mitchell and Wynters in 1991 who gave an $O(n^2)$ time algorithm. Despite considerable progress on related problems, no subquadratic time algorithm for this problem was found so far. We present an exact algorithm solving the problem in $O(n \log^4 n)$ time and a $(1+\varepsilon)$-approximation algorithm running in $O(n + 1/\varepsilon^2\cdot\log^4(1/\varepsilon))$ time.",let set point plane consid problem partit two subset sum perimet text ch text ch minim text ch denot convex hull problem first studi mitchel wynter gave time algorithm despit consider progress relat problem subquadrat time algorithm problem found far present exact algorithm solv problem log time varepsilon approxim algorithm run varepsilon cdot log varepsilon time,"['Mikkel Abrahamsen', 'Mark de Berg', 'Kevin Buchin', 'Mehran Mehr', 'Ali D. Mehrabi']",['cs.CG'],False,False,False,False,False,True
211,2017-03-28T14:07:20Z,2017-03-19T22:20:57Z,http://arxiv.org/abs/1703.05475v2,http://arxiv.org/pdf/1703.05475v2,A quest to unravel the metric structure behind perturbed networks,quest unravel metric structur behind perturb network,"Graphs and network data are ubiquitous across a wide spectrum of scientific and application domains. Often in practice, an input graph can be considered as an observed snapshot of a (potentially continuous) hidden domain or process. Subsequent analysis, processing, and inferences are then performed on this observed graph. In this paper we advocate the perspective that an observed graph is often a noisy version of some discretized 1-skeleton of a hidden domain, and specifically we will consider the following natural network model: We assume that there is a true graph ${G^*}$ which is a certain proximity graph for points sampled from a hidden domain $\mathcal{X}$; while the observed graph $G$ is an Erd$\""{o}$s-R$\'{e}$nyi type perturbed version of ${G^*}$.   Our network model is related to, and slightly generalizes, the much-celebrated small-world network model originally proposed by Watts and Strogatz. However, the main question we aim to answer is orthogonal to the usual studies of network models (which often focuses on characterizing / predicting behaviors and properties of real-world networks). Specifically, we aim to recover the metric structure of ${G^*}$ (which reflects that of the hidden space $\mathcal{X}$ as we will show) from the observed graph $G$. Our main result is that a simple filtering process based on the \emph{Jaccard index} can recover this metric within a multiplicative factor of $2$ under our network model. Our work makes one step towards the general question of inferring structure of a hidden space from its observed noisy graph representation. In addition, our results also provide a theoretical understanding for Jaccard-Index-based denoising approaches.",graph network data ubiquit across wide spectrum scientif applic domain often practic input graph consid observ snapshot potenti continu hidden domain process subsequ analysi process infer perform observ graph paper advoc perspect observ graph often noisi version discret skeleton hidden domain specif consid follow natur network model assum true graph certain proxim graph point sampl hidden domain mathcal observ graph erd nyi type perturb version network model relat slight general much celebr small world network model origin propos watt strogatz howev main question aim answer orthogon usual studi network model often focus character predict behavior properti real world network specif aim recov metric structur reflect hidden space mathcal show observ graph main result simpl filter process base emph jaccard index recov metric within multipl factor network model work make one step toward general question infer structur hidden space observ noisi graph represent addit result also provid theoret understand jaccard index base denois approach,"['Srinivasan Parthasarathy', 'David Sivakoff', 'Minghao Tian', 'Yusu Wang']","['cs.CG', 'F.2.2; G.2.2']",False,False,False,False,False,True
213,2017-03-28T14:07:20Z,2017-03-14T22:19:50Z,http://arxiv.org/abs/1703.04774v1,http://arxiv.org/pdf/1703.04774v1,Self-Assembly of 4-sided Fractals in the Two-handed Tile Assembly Model,self assembl side fractal two hand tile assembl model,"In this paper, we consider the strict self-assembly of fractals in one of the most well-studied models of tile based self-assembling systems known as the Two-handed Tile Assembly Model (2HAM). We are particularly interested in a class of fractals called discrete self-similar fractals (a class of fractals that includes the discrete Sierpinski's carpet). We present a 2HAM system that strictly self-assembles the discrete Sierpinski's carpet with scale factor 1. Moreover, the 2HAM system that we give lends itself to being generalized and we describe how this system can be modified to obtain a 2HAM system that strictly self-assembles one of any fractal from an infinite set of fractals which we call 4-sided fractals. The 2HAM systems we give in this paper are the first examples of systems that strictly self-assemble discrete self-similar fractals at scale factor 1 in a purely growth model of self-assembly. Finally, we give an example of a 3-sided fractal (which is not a tree fractal) that cannot be strictly self-assembled by any 2HAM system.",paper consid strict self assembl fractal one well studi model tile base self assembl system known two hand tile assembl model ham particular interest class fractal call discret self similar fractal class fractal includ discret sierpinski carpet present ham system strict self assembl discret sierpinski carpet scale factor moreov ham system give lend general describ system modifi obtain ham system strict self assembl one ani fractal infinit set fractal call side fractal ham system give paper first exampl system strict self assembl discret self similar fractal scale factor pure growth model self assembl final give exampl side fractal tree fractal cannot strict self assembl ani ham system,"['Jacob Hendricks', 'Joseph Opseth']","['cs.ET', 'cs.CG']",False,False,False,False,False,True
214,2017-03-28T14:07:20Z,2017-03-14T22:13:58Z,http://arxiv.org/abs/1703.04758v1,http://arxiv.org/pdf/1703.04758v1,Approximation Schemes for Independent Set and Sparse Subsets of Polygons,approxim scheme independ set spars subset polygon,"We present an $(1+\varepsilon)$-approximation algorithm with quasi-polynomial running time for computing the maximum weight independent set of polygons out of a given set of polygons in the plane (specifically, the running time is $n^{O( \mathrm{poly}( \log n, 1/\varepsilon))}$). Contrasting this, the best known polynomial time algorithm for the problem has an approximation ratio of~$n^{\varepsilon}$. Surprisingly, we can extend the algorithm to the problem of computing the maximum weight subset of the given set of polygons whose intersection graph fulfills some sparsity condition. For example, we show that one can approximate the maximum weight subset of polygons, such that the intersection graph of the subset is planar or does not contain a cycle of length $4$ (i.e., $K_{2,2}$). Our algorithm relies on a recursive partitioning scheme, whose backbone is the existence of balanced cuts with small complexity that intersect polygons from the optimal solution of a small total weight.   For the case of large axis-parallel rectangles, we provide a polynomial time $(1+\varepsilon)$-approximation for the maximum weight independent set. Specifically, we consider the problem where each rectangle has one edge whose length is at least a constant fraction of the length of the corresponding edge of the bounding box of all the input elements. This is now the most general case for which a PTAS is known, and it requires a new and involved partitioning scheme, which should be of independent interest.",present varepsilon approxim algorithm quasi polynomi run time comput maximum weight independ set polygon given set polygon plane specif run time mathrm poli log varepsilon contrast best known polynomi time algorithm problem approxim ratio varepsilon surpris extend algorithm problem comput maximum weight subset given set polygon whose intersect graph fulfil sparsiti condit exampl show one approxim maximum weight subset polygon intersect graph subset planar doe contain cycl length algorithm reli recurs partit scheme whose backbon exist balanc cut small complex intersect polygon optim solut small total weight case larg axi parallel rectangl provid polynomi time varepsilon approxim maximum weight independ set specif consid problem rectangl one edg whose length least constant fraction length correspond edg bound box input element general case ptas known requir new involv partit scheme independ interest,"['Anna Adamaszek', 'Sariel Har-Peled', 'Andreas Wiese']",['cs.CG'],False,False,False,False,False,True
217,2017-03-28T14:07:20Z,2017-03-13T08:00:31Z,http://arxiv.org/abs/1703.04283v1,http://arxiv.org/pdf/1703.04283v1,Universal Slope Sets for 1-Bend Planar Drawings,univers slope set bend planar draw,"We describe a set of $\Delta -1$ slopes that are universal for 1-bend planar drawings of planar graphs of maximum degree $\Delta \geq 4$; this establishes a new upper bound of $\Delta-1$ on the 1-bend planar slope number. By universal we mean that every planar graph of degree $\Delta$ has a planar drawing with at most one bend per edge and such that the slopes of the segments forming the edges belong to the given set of slopes. This improves over previous results in two ways: Firstly, the best previously known upper bound for the 1-bend planar slope number was $\frac{3}{2} (\Delta -1)$ (the known lower bound being $\frac{3}{4} (\Delta -1)$); secondly, all the known algorithms to construct 1-bend planar drawings with $O(\Delta)$ slopes use a different set of slopes for each graph and can have bad angular resolution, while our algorithm uses a universal set of slopes, which also guarantees that the minimum angle between any two edges incident to a vertex is $\frac{\pi}{(\Delta-1)}$.",describ set delta slope univers bend planar draw planar graph maximum degre delta geq establish new upper bound delta bend planar slope number univers mean everi planar graph degre delta planar draw one bend per edg slope segment form edg belong given set slope improv previous result two way first best previous known upper bound bend planar slope number frac delta known lower bound frac delta second known algorithm construct bend planar draw delta slope use differ set slope graph bad angular resolut algorithm use univers set slope also guarante minimum angl ani two edg incid vertex frac pi delta,"['Patrizio Angelini', 'Michael A. Bekos', 'Giuseppe Liotta', 'Fabrizio Montecchiani']",['cs.CG'],False,False,False,False,False,True
219,2017-03-28T14:07:20Z,2017-03-11T23:16:23Z,http://arxiv.org/abs/1703.04040v1,http://arxiv.org/abs/1703.04040v1,Locality-sensitive hashing of curves,local sensit hash curv,"We study data structures for storing a set of polygonal curves in ${\rm R}^d$ such that, given a query curve, we can efficiently retrieve similar curves from the set, where similarity is measured using the discrete Fr\'echet distance or the dynamic time warping distance. To this end we devise the first locality-sensitive hashing schemes for these distance measures. A major challenge is posed by the fact that these distance measures internally optimize the alignment between the curves. We give solutions for different types of alignments including constrained and unconstrained versions. For unconstrained alignments, we improve over a result by Indyk from 2002 for short curves. Let $n$ be the number of input curves and let $m$ be the maximum complexity of a curve in the input. In the particular case where $m \leq \frac{\alpha}{4d} \log n$, for some fixed $\alpha>0$, our solutions imply an approximate near-neighbor data structure for the discrete Fr\'echet distance that uses space in $O(n^{1+\alpha}\log n)$ and achieves query time in $O(n^{\alpha}\log^2 n)$ and constant approximation factor. Furthermore, our solutions provide a trade-off between approximation quality and computational performance: for any parameter $k \in [m]$, we can give a data structure that uses space in $O(2^{2k}m^{k-1} n \log n + nm)$, answers queries in $O( 2^{2k} m^{k}\log n)$ time and achieves approximation factor in $O(m/k)$.",studi data structur store set polygon curv rm given queri curv effici retriev similar curv set similar measur use discret fr echet distanc dynam time warp distanc end devis first local sensit hash scheme distanc measur major challeng pose fact distanc measur intern optim align curv give solut differ type align includ constrain unconstrain version unconstrain align improv result indyk short curv let number input curv let maximum complex curv input particular case leq frac alpha log fix alpha solut impli approxim near neighbor data structur discret fr echet distanc use space alpha log achiev queri time alpha log constant approxim factor furthermor solut provid trade approxim qualiti comput perform ani paramet give data structur use space log nm answer queri log time achiev approxim factor,"['Anne Driemel', 'Francesco Silvestri']","['cs.CG', 'cs.DS', 'cs.IR', 'F.2.2']",False,False,False,False,False,True
220,2017-03-28T14:07:25Z,2017-03-10T13:54:29Z,http://arxiv.org/abs/1703.03687v1,http://arxiv.org/pdf/1703.03687v1,Best Laid Plans of Lions and Men,best laid plan lion men,"We answer the following question dating back to J.E. Littlewood (1885 - 1977): Can two lions catch a man in a bounded area with rectifiable lakes? The lions and the man are all assumed to be points moving with at most unit speed. That the lakes are rectifiable means that their boundaries are finitely long. This requirement is to avoid pathological examples where the man survives forever because any path to the lions is infinitely long. We show that the answer to the question is not always ""yes"" by giving an example of a region $R$ in the plane where the man has a strategy to survive forever. $R$ is a polygonal region with holes and the exterior and interior boundaries are pairwise disjoint, simple polygons. Our construction is the first truly two-dimensional example where the man can survive.   Next, we consider the following game played on the entire plane instead of a bounded area: There is any finite number of unit speed lions and one fast man who can run with speed $1+\varepsilon$ for some value $\varepsilon>0$. Can the man always survive? We answer the question in the affirmative for any constant $\varepsilon>0$.",answer follow question date back littlewood two lion catch man bound area rectifi lake lion man assum point move unit speed lake rectifi mean boundari finit long requir avoid patholog exampl man surviv forev becaus ani path lion infinit long show answer question alway yes give exampl region plane man strategi surviv forev polygon region hole exterior interior boundari pairwis disjoint simpl polygon construct first truli two dimension exampl man surviv next consid follow game play entir plane instead bound area ani finit number unit speed lion one fast man run speed varepsilon valu varepsilon man alway surviv answer question affirm ani constant varepsilon,"['Mikkel Abrahamsen', 'Jacob Holm', 'Eva Rotenberg', 'Christian Wulff-Nilsen']","['cs.CG', 'cs.GT']",False,False,True,False,False,True
221,2017-03-28T14:07:25Z,2017-03-10T08:35:24Z,http://arxiv.org/abs/1703.03575v1,http://arxiv.org/pdf/1703.03575v1,Crossing the Logarithmic Barrier for Dynamic Boolean Data Structure   Lower Bounds,cross logarithm barrier dynam boolean data structur lower bound,"This paper proves the first super-logarithmic lower bounds on the cell probe complexity of dynamic boolean (a.k.a. decision) data structure problems, a long-standing milestone in data structure lower bounds.   We introduce a new method for proving dynamic cell probe lower bounds and use it to prove a $\tilde{\Omega}(\log^{1.5} n)$ lower bound on the operational time of a wide range of boolean data structure problems, most notably, on the query time of dynamic range counting over $\mathbb{F}_2$ ([Pat07]). Proving an $\omega(\lg n)$ lower bound for this problem was explicitly posed as one of five important open problems in the late Mihai P\v{a}tra\c{s}cu's obituary [Tho13]. This result also implies the first $\omega(\lg n)$ lower bound for the classical 2D range counting problem, one of the most fundamental data structure problems in computational geometry and spatial databases. We derive similar lower bounds for boolean versions of dynamic polynomial evaluation and 2D rectangle stabbing, and for the (non-boolean) problems of range selection and range median.   Our technical centerpiece is a new way of ""weakly"" simulating dynamic data structures using efficient one-way communication protocols with small advantage over random guessing. This simulation involves a surprising excursion to low-degree (Chebychev) polynomials which may be of independent interest, and offers an entirely new algorithmic angle on the ""cell sampling"" method of Panigrahy et al. [PTW10].",paper prove first super logarithm lower bound cell probe complex dynam boolean decis data structur problem long stand mileston data structur lower bound introduc new method prove dynam cell probe lower bound use prove tild omega log lower bound oper time wide rang boolean data structur problem notabl queri time dynam rang count mathbb pat prove omega lg lower bound problem explicit pose one five import open problem late mihai tra cu obituari tho result also impli first omega lg lower bound classic rang count problem one fundament data structur problem comput geometri spatial databas deriv similar lower bound boolean version dynam polynomi evalu rectangl stab non boolean problem rang select rang median technic centerpiec new way weak simul dynam data structur use effici one way communic protocol small advantag random guess simul involv surpris excurs low degre chebychev polynomi may independ interest offer entir new algorithm angl cell sampl method panigrahi et al ptw,"['Kasper Green Larsen', 'Omri Weinstein', 'Huacheng Yu']","['cs.DS', 'cs.CC', 'cs.CG', 'cs.IT', 'math.IT']",False,False,False,False,False,True
222,2017-03-28T14:07:25Z,2017-03-08T21:50:06Z,http://arxiv.org/abs/1703.03048v1,http://arxiv.org/pdf/1703.03048v1,Quickest Visibility Queries in Polygonal Domains,quickest visibl queri polygon domain,"Let $s$ be a point in a polygonal domain $\mathcal{P}$ of $h-1$ holes and $n$ vertices. We consider a quickest visibility query problem. Given a query point $q$ in $\mathcal{P}$, the goal is to find a shortest path in $\mathcal{P}$ to move from $s$ to see $q$ as quickly as possible. Previously, Arkin et al. (SoCG 2015) built a data structure of size $O(n^22^{\alpha(n)}\log n)$ that can answer each query in $O(K\log^2 n)$ time, where $\alpha(n)$ is the inverse Ackermann function and $K$ is the size of the visibility polygon of $q$ in $\mathcal{P}$ (and $K$ can be $\Theta(n)$ in the worst case). In this paper, we present a new data structure of size $O(n\log h + h^2)$ that can answer each query in $O(h\log h\log n)$ time. Our result improves the previous work when $h$ is relatively small. In particular, if $h$ is a constant, then our result even matches the best result for the simple polygon case (i.e., $h=1$), which is optimal. As a by-product, we also have a new algorithm for a shortest-path-to-segment query problem. Given a query line segment $\tau$ in $\mathcal{P}$, the query seeks a shortest path from $s$ to all points of $\tau$. Previously, Arkin et al. gave a data structure of size $O(n^22^{\alpha(n)}\log n)$ that can answer each query in $O(\log^2 n)$ time, and another data structure of size $O(n^3\log n)$ with $O(\log n)$ query time. We present a data structure of size $O(n)$ with query time $O(h\log \frac{n}{h})$, which also favors small values of $h$ and is optimal when $h=O(1)$.",let point polygon domain mathcal hole vertic consid quickest visibl queri problem given queri point mathcal goal find shortest path mathcal move see quick possibl previous arkin et al socg built data structur size alpha log answer queri log time alpha invers ackermann function size visibl polygon mathcal theta worst case paper present new data structur size log answer queri log log time result improv previous work relat small particular constant result even match best result simpl polygon case optim product also new algorithm shortest path segment queri problem given queri line segment tau mathcal queri seek shortest path point tau previous arkin et al gave data structur size alpha log answer queri log time anoth data structur size log log queri time present data structur size queri time log frac also favor small valu optim,['Haitao Wang'],"['cs.CG', 'cs.DS']",False,False,False,False,False,True
223,2017-03-28T14:07:25Z,2017-03-08T16:32:05Z,http://arxiv.org/abs/1703.02901v1,http://arxiv.org/pdf/1703.02901v1,Local Equivalence and Intrinsic Metrics between Reeb Graphs,local equival intrins metric reeb graph,"As graphical summaries for topological spaces and maps, Reeb graphs are common objects in the computer graphics or topological data analysis literature. Defining good metrics between these objects has become an important question for applications, where it matters to quantify the extent by which two given Reeb graphs differ. Recent contributions emphasize this aspect, proposing novel distances such as {\em functional distortion} or {\em interleaving} that are provably more discriminative than the so-called {\em bottleneck distance}, being true metrics whereas the latter is only a pseudo-metric. Their main drawback compared to the bottleneck distance is to be comparatively hard (if at all possible) to evaluate. Here we take the opposite view on the problem and show that the bottleneck distance is in fact good enough {\em locally}, in the sense that it is able to discriminate a Reeb graph from any other Reeb graph in a small enough neighborhood, as efficiently as the other metrics do. This suggests considering the {\em intrinsic metrics} induced by these distances, which turn out to be all {\em globally} equivalent. This novel viewpoint on the study of Reeb graphs has a potential impact on applications, where one may not only be interested in discriminating between data but also in interpolating between them.",graphic summari topolog space map reeb graph common object comput graphic topolog data analysi literatur defin good metric object becom import question applic matter quantifi extent two given reeb graph differ recent contribut emphas aspect propos novel distanc em function distort em interleav provabl discrimin call em bottleneck distanc true metric wherea latter onli pseudo metric main drawback compar bottleneck distanc compar hard possibl evalu take opposit view problem show bottleneck distanc fact good enough em local sens abl discrimin reeb graph ani reeb graph small enough neighborhood effici metric suggest consid em intrins metric induc distanc turn em global equival novel viewpoint studi reeb graph potenti impact applic one may onli interest discrimin data also interpol,"['Mathieu Carrière', 'Steve Oudot']","['cs.CG', 'math.AT']",False,False,True,False,False,True
224,2017-03-28T14:07:25Z,2017-03-08T02:12:35Z,http://arxiv.org/abs/1703.02671v1,http://arxiv.org/pdf/1703.02671v1,"Symmetric Assembly Puzzles are Hard, Beyond a Few Pieces",symmetr assembl puzzl hard beyond piec,"We study the complexity of symmetric assembly puzzles: given a collection of simple polygons, can we translate, rotate, and possibly flip them so that their interior-disjoint union is line symmetric? On the negative side, we show that the problem is strongly NP-complete even if the pieces are all polyominos. On the positive side, we show that the problem can be solved in polynomial time if the number of pieces is a fixed constant.",studi complex symmetr assembl puzzl given collect simpl polygon translat rotat possibl flip interior disjoint union line symmetr negat side show problem strong np complet even piec polyomino posit side show problem solv polynomi time number piec fix constant,"['Erik D. Demaine', 'Matias Korman', 'Jason S. Ku', 'Joseph S. B. Mitchell', 'Yota Otachi', 'André van Renssen', 'Marcel Roeloffzen', 'Ryuhei Uehara', 'Yushi Uno']",['cs.CG'],False,False,False,False,False,True
225,2017-03-28T14:07:25Z,2017-03-07T23:22:46Z,http://arxiv.org/abs/1703.02637v1,http://arxiv.org/pdf/1703.02637v1,Effective identifiability criteria for tensors and polynomials,effect identifi criteria tensor polynomi,"A tensor $T$, in a given tensor space, is said to be $h$-identifiable if it admits a unique decomposition as a sum of $h$ rank one tensors. A criterion for $h$-identifiability is called effective if it is satisfied in a dense, open subset of the set of rank $h$ tensors. In this paper we give effective $h$-identifiability criteria for a large class of tensors. We then improve these criteria for some symmetric tensors. For instance, this allows us to give a complete set of effective identifiability criteria for ternary quintic polynomial. Finally, we implement our identifiability algorithms in Macaulay2.",tensor given tensor space said identifi admit uniqu decomposit sum rank one tensor criterion identifi call effect satisfi dens open subset set rank tensor paper give effect identifi criteria larg class tensor improv criteria symmetr tensor instanc allow us give complet set effect identifi criteria ternari quintic polynomi final implement identifi algorithm macaulay,"['Alex Massarenti', 'Massimiliano Mella', 'Giovanni Staglianò']","['math.AG', 'cs.CG', '15A69, 15A72, 11P05 (Primary), 14N05, 15A69 (Secondary)']",False,False,False,False,False,True
228,2017-03-28T14:07:25Z,2017-03-05T23:29:51Z,http://arxiv.org/abs/1703.01691v1,http://arxiv.org/pdf/1703.01691v1,Drawing Planar Graphs with Few Geometric Primitives,draw planar graph geometr primit,"We define the visual complexity of a plane graph drawing to be the number of geometric objects needed to represent all its edges. In particular, one object may represent multiple edges (e.g., one needs only one line segment to draw two collinear edges of the same vertex). Let $n$ denote the number of vertices of a graph. We show that trees can be drawn with $3n/4$ straight-line segments on a polynomial grid, and with $n/2$ straight-line segments on a quasi-polynomial grid. Further, we present an algorithm for drawing planar 3-trees with $(8n-17)/3$ segments on an $O(n)\times O(n^2)$ grid. This algorithm can also be used with a small modification to draw maximal outerplanar graphs with $3n/2$ edges on an $O(n)\times O(n^2)$ grid. We also study the problem of drawing maximal planar graphs with circular arcs and provide an algorithm to draw such graphs using only $(5n - 11)/3$ arcs. This provides a significant improvement over the lower bound of $2n$ for line segments for a nontrivial graph class.",defin visual complex plane graph draw number geometr object need repres edg particular one object may repres multipl edg one need onli one line segment draw two collinear edg vertex let denot number vertic graph show tree drawn straight line segment polynomi grid straight line segment quasi polynomi grid present algorithm draw planar tree segment time grid algorithm also use small modif draw maxim outerplanar graph edg time grid also studi problem draw maxim planar graph circular arc provid algorithm draw graph use onli arc provid signific improv lower bound line segment nontrivi graph class,"['Gregor Hültenschmidt', 'Philipp Kindermann', 'Wouter Meulemans', 'André Schulz']",['cs.CG'],False,False,False,False,False,True
229,2017-03-28T14:07:25Z,2017-03-05T19:10:17Z,http://arxiv.org/abs/1703.01646v1,http://arxiv.org/pdf/1703.01646v1,A PTAS for TSP with Neighborhoods Among Fat Regions in the Plane,ptas tsp neighborhood among fat region plane,"The Euclidean TSP with neighborhoods (TSPN) problem seeks a shortest tour that visits a given collection of $n$ regions ({\em neighborhoods}). We present the first polynomial-time approximation scheme for TSPN for a set of regions given by arbitrary disjoint fat regions in the plane. This improves substantially upon the known approximation algorithms, and is the first PTAS for TSPN on regions of non-comparable sizes. Our result is based on a novel extension of the $m$-guillotine method. The result applies to regions that are ""fat"" in a very weak sense: each region $P_i$ has area $\Omega([diam(P_i)]^2)$, but is otherwise arbitrary.",euclidean tsp neighborhood tspn problem seek shortest tour visit given collect region em neighborhood present first polynomi time approxim scheme tspn set region given arbitrari disjoint fat region plane improv substanti upon known approxim algorithm first ptas tspn region non compar size result base novel extens guillotin method result appli region fat veri weak sens region area omega diam otherwis arbitrari,['Joseph S. B. Mitchell'],['cs.CG'],False,False,True,False,False,True
230,2017-03-28T14:07:29Z,2017-03-05T18:24:23Z,http://arxiv.org/abs/1703.01640v1,http://arxiv.org/abs/1703.01640v1,Approximation algorithms for TSP with neighborhoods in the plane,approxim algorithm tsp neighborhood plane,"In the Euclidean TSP with neighborhoods (TSPN), we are given a collection of n regions (neighborhoods) and we seek a shortest tour that visits each region. As a generalization of the classical Euclidean TSP, TSPN is also NP-hard. In this paper, we present new approximation results for the TSPN, including (1) a constant-factor approximation algorithm for the case of arbitrary connected neighborhoods having comparable diameters; and (2) a PTAS for the important special case of disjoint unit disk neighborhoods (or nearly disjoint, nearly-unit disks). Our methods also yield improved approximation ratios for various special classes of neighborhoods, which have previously been studied. Further, we give a linear-time O(1)-approximation algorithm for the case of neighborhoods that are (infinite) straight lines.",euclidean tsp neighborhood tspn given collect region neighborhood seek shortest tour visit region general classic euclidean tsp tspn also np hard paper present new approxim result tspn includ constant factor approxim algorithm case arbitrari connect neighborhood compar diamet ptas import special case disjoint unit disk neighborhood near disjoint near unit disk method also yield improv approxim ratio various special class neighborhood previous studi give linear time approxim algorithm case neighborhood infinit straight line,"['Adrian Dumitrescu', 'Joseph S. B. Mitchell']","['cs.CG', 'cs.DS']",False,False,False,False,False,True
231,2017-03-28T14:07:29Z,2017-03-05T01:41:08Z,http://arxiv.org/abs/1703.01544v1,http://arxiv.org/pdf/1703.01544v1,L-Graphs and Monotone L-Graphs,graph monoton graph,"In an $\mathsf{L}$-embedding of a graph, each vertex is represented by an $\mathsf{L}$-segment, and two segments intersect each other if and only if the corresponding vertices are adjacent in the graph. If the corner of each $\mathsf{L}$-segment in an $\mathsf{L}$-embedding lies on a straight line, we call it a monotone $\mathsf{L}$-embedding. In this paper we give a full characterization of monotone $\mathsf{L}$-embeddings by introducing a new class of graphs which we call ""non-jumping"" graphs. We show that a graph admits a monotone $\mathsf{L}$-embedding if and only if the graph is a non-jumping graph. Further, we show that outerplanar graphs, convex bipartite graphs, interval graphs, 3-leaf power graphs, and complete graphs are subclasses of non-jumping graphs. Finally, we show that distance-hereditary graphs and $k$-leaf power graphs ($k\le 4$) admit $\mathsf{L}$-embeddings.",mathsf embed graph vertex repres mathsf segment two segment intersect onli correspond vertic adjac graph corner mathsf segment mathsf embed lie straight line call monoton mathsf embed paper give full character monoton mathsf embed introduc new class graph call non jump graph show graph admit monoton mathsf embed onli graph non jump graph show outerplanar graph convex bipartit graph interv graph leaf power graph complet graph subclass non jump graph final show distanc hereditari graph leaf power graph le admit mathsf embed,"['Abu Reyan Ahmed', 'Felice De Luca', 'Sabin Devkota', 'Alon Efrat', 'Md Iqbal Hossain', 'Stephen Kobourov', 'Jixian Li', 'Sammi Abida Salma', 'Eric Welch']",['cs.CG'],False,False,False,False,False,True
232,2017-03-28T14:07:29Z,2017-03-04T10:59:56Z,http://arxiv.org/abs/1703.01439v1,http://arxiv.org/pdf/1703.01439v1,On the set of optimal homeomorphisms for the natural pseudo-distance   associated with the Lie group S^1,set optim homeomorph natur pseudo distanc associ lie group,"If $\varphi$ and $\psi$ are two continuous real-valued functions defined on a compact topological space $X$ and $G$ is a subgroup of the group of all homeomorphisms of $X$ onto itself, the natural pseudo-distance $d_G(\varphi,\psi)$ is defined as the infimum of $\mathcal{L}(g)=\ \varphi-\psi \circ g \ _\infty$, as $g$ varies in $G$. In this paper, we make a first step towards extending the study of this concept to the case of Lie groups, by assuming $X=G=S^1$. In particular, we study the set of the optimal homeomorphisms for $d_G$, i.e. the elements $\rho_\alpha$ of $S^1$ such that $\mathcal{L}(\rho_\alpha)$ is equal to $d_G(\varphi,\psi)$. As our main results, we give conditions that a homeomorphism has to meet in order to be optimal, and we prove that the set of the optimal homeomorphisms is finite under suitable conditions.",varphi psi two continu real valu function defin compact topolog space subgroup group homeomorph onto natur pseudo distanc varphi psi defin infimum mathcal varphi psi circ infti vari paper make first step toward extend studi concept case lie group assum particular studi set optim homeomorph element rho alpha mathcal rho alpha equal varphi psi main result give condit homeomorph meet order optim prove set optim homeomorph finit suitabl condit,['Alessandro De Gregorio'],"['cs.CG', 'math.AT', 'Primary 57S05, Secondary 55N99']",False,False,False,False,False,True
235,2017-03-28T14:07:29Z,2017-02-28T05:47:10Z,http://arxiv.org/abs/1702.08654v1,http://arxiv.org/pdf/1702.08654v1,An Improved Algorithm for General Position Subset Selection,improv algorithm general posit subset select,"In the General Position Subset Selection (GPSS) problem, the goal is to find the largest possible subset of a set of points, such that no three of its members are collinear. If $s_{\textrm{GPSS}}$ is the size the optimal solution, $\sqrt{s_{\textrm{GPSS}}}$ is the current best guarantee for the size of the solution obtained using a polynomial time algorithm. In this paper we present an algorithm for GPSS to improve this bound based on the number of collinear pairs of points.",general posit subset select gpss problem goal find largest possibl subset set point three member collinear textrm gpss size optim solut sqrt textrm gpss current best guarante size solut obtain use polynomi time algorithm paper present algorithm gpss improv bound base number collinear pair point,['Ali Gholami Rudi'],"['cs.CG', '65D18, 05C69', 'G.2.1; I.3.5; G.2.2']",False,False,False,False,False,True
236,2017-03-28T14:07:29Z,2017-02-28T02:18:54Z,http://arxiv.org/abs/1702.08607v1,http://arxiv.org/pdf/1702.08607v1,Faster DB-scan and HDB-scan in Low-Dimensional Euclidean Spaces,faster db scan hdb scan low dimension euclidean space,"We present a new algorithm for the widely used density-based clustering method DBscan. Our algorithm computes the DBscan-clustering in $O(n\log n)$ time in $\mathbb{R}^2$, irrespective of the scale parameter $\varepsilon$ (and assuming the second parameter MinPts is set to a fixed constant, as is the case in practice). Experiments show that the new algorithm is not only fast in theory, but that a slightly simplified version is competitive in practice and much less sensitive to the choice of $\varepsilon$ than the original DBscan algorithm. We also present an $O(n\log n)$ randomized algorithm for HDBscan in the plane---HDBscan is a hierarchical version of DBscan introduced recently---and we show how to compute an approximate version of HDBscan in near-linear time in any fixed dimension.",present new algorithm wide use densiti base cluster method dbscan algorithm comput dbscan cluster log time mathbb irrespect scale paramet varepsilon assum second paramet minpt set fix constant case practic experi show new algorithm onli fast theori slight simplifi version competit practic much less sensit choic varepsilon origin dbscan algorithm also present log random algorithm hdbscan plane hdbscan hierarch version dbscan introduc recent show comput approxim version hdbscan near linear time ani fix dimens,"['Mark de Berg', 'Ade Gunawan', 'Marcel Roeloffzen']",['cs.CG'],False,False,False,False,False,True
238,2017-03-28T14:07:29Z,2017-02-27T22:25:57Z,http://arxiv.org/abs/1703.01350v1,http://arxiv.org/pdf/1703.01350v1,Approximate Convex Hulls,approxim convex hull,We investigate the PPI algorithm as a means for computing ap- proximate convex hull. We explain how the algorithm computes the curvature of points and prove consistency and convergence. We also extend the algorithm to compute approximate convex hulls described in terms of hyperplanes.,investig ppi algorithm mean comput ap proxim convex hull explain algorithm comput curvatur point prove consist converg also extend algorithm comput approxim convex hull describ term hyperplan,"['Robert Graham', 'Adam M. Oberman']","['cs.CG', 'math.CO', '05-04']",False,False,False,False,False,True
239,2017-03-28T14:07:29Z,2017-02-27T17:07:31Z,http://arxiv.org/abs/1702.08380v1,http://arxiv.org/pdf/1702.08380v1,Exploring Increasing-Chord Paths and Trees,explor increas chord path tree,"A straight-line drawing $\Gamma$ of a graph $G=(V,E)$ is a drawing of $G$ in the Euclidean plane, where every vertex in $G$ is mapped to a distinct point, and every edge in $G$ is mapped to a straight line segment between their endpoints. A path $P$ in $\Gamma$ is called increasing-chord if for every four points (not necessarily vertices) $a,b,c,d$ on $P$ in this order, the Euclidean distance between $b,c$ is at most the Euclidean distance between $a,d$. A spanning tree $T$ rooted at some vertex $r$ in $\Gamma$ is called increasing-chord if $T$ contains an increasing-chord path from $r$ to every vertex in $T$. In this paper we prove that given a vertex $r$ in a straight-line drawing $\Gamma$, it is NP-complete to determine whether $\Gamma$ contains an increasing-chord spanning tree rooted at $r$. We conjecture that finding an increasing-chord path between a pair of vertices in $\Gamma$, which is an intriguing open problem posed by Alamdari et al., is also NP-complete, and show a (non-polynomial) reduction from the 3-SAT problem.",straight line draw gamma graph draw euclidean plane everi vertex map distinct point everi edg map straight line segment endpoint path gamma call increas chord everi four point necessarili vertic order euclidean distanc euclidean distanc span tree root vertex gamma call increas chord contain increas chord path everi vertex paper prove given vertex straight line draw gamma np complet determin whether gamma contain increas chord span tree root conjectur find increas chord path pair vertic gamma intrigu open problem pose alamdari et al also np complet show non polynomi reduct sat problem,"['Yeganeh Bahoo', 'Stephane Durocher', 'Sahar Mehrpour', 'Debajyoti Mondal']",['cs.CG'],False,False,True,False,False,True
240,2017-03-28T14:07:33Z,2017-02-25T13:55:53Z,http://arxiv.org/abs/1702.07893v1,http://arxiv.org/pdf/1702.07893v1,The Persistent Homotopy Type Distance,persist homotopi type distanc,"We introduce the persistent homotopy type distance dHT to compare real valued functions defined on possibly different homotopy equivalent topological spaces. The underlying idea in the definition of dHT is to measure the minimal shift that is necessary to apply to one of the two functions in order that the sublevel sets of the two functions become homotopically equivalent. This distance is interesting in connection with persistent homology. Indeed, our main result states that dHT still provides an upper bound for the bottleneck distance between the persistence diagrams of the intervening functions. Moreover, because homotopy equivalences are weaker than homeomorphisms, this implies a lifting of the standard stability results provided by the L-infty distance and the natural pseudo-distance dNP. From a different standpoint, we prove that dHT extends the L-infty distance and dNP in two ways. First, we show that, appropriately restricting the category of objects to which dHT applies, it can be made to coincide with the other two distances. Finally, we show that dHT has an interpretation in terms of interleavings that naturally places it in the family of distances used in persistence theory.",introduc persist homotopi type distanc dht compar real valu function defin possibl differ homotopi equival topolog space idea definit dht measur minim shift necessari appli one two function order sublevel set two function becom homotop equival distanc interest connect persist homolog inde main result state dht still provid upper bound bottleneck distanc persist diagram interven function moreov becaus homotopi equival weaker homeomorph impli lift standard stabil result provid infti distanc natur pseudo distanc dnp differ standpoint prove dht extend infti distanc dnp two way first show appropri restrict categori object dht appli made coincid two distanc final show dht interpret term interleav natur place famili distanc use persist theori,"['Patrizio Frosini', 'Claudia Landi', 'Facundo Memoli']","['cs.CG', 'math.AT']",False,False,False,False,False,True
241,2017-03-28T14:07:33Z,2017-02-24T14:06:31Z,http://arxiv.org/abs/1702.07589v1,http://arxiv.org/pdf/1702.07589v1,Generalization of Schnyder woods to orientable surfaces and applications,general schnyder wood orient surfac applic,Schnyder woods are particularly elegant combinatorial structures with numerous applications concerning planar triangulations and more generally 3-connected planar maps. We propose a simple generalization of Schnyder woods from the plane to maps on orientable surfaces of any genus with a special emphasis on the toroidal case. We provide a natural partition of the set of Schnyder woods of a given map into distributive lattices depending on the surface homology. In the toroidal case we show the existence of particular Schnyder woods with some global properties that are useful for optimal encoding or graph drawing purpose.,schnyder wood particular eleg combinatori structur numer applic concern planar triangul general connect planar map propos simpl general schnyder wood plane map orient surfac ani genus special emphasi toroid case provid natur partit set schnyder wood given map distribut lattic depend surfac homolog toroid case show exist particular schnyder wood global properti use optim encod graph draw purpos,['Benjamin Lévêque'],"['cs.DM', 'cs.CG', 'math.CO']",False,False,False,False,False,True
243,2017-03-28T14:07:33Z,2017-02-23T21:32:10Z,http://arxiv.org/abs/1702.07399v1,http://arxiv.org/pdf/1702.07399v1,An Optimal Algorithm for Computing the Spherical Depth of Points in the   Plane,optim algorithm comput spheric depth point plane,"For a distribution function $F$ on $\mathbb{R}^d$ and a point $q\in \mathbb{R}^d$, the \emph{spherical depth} $\SphD(q;F)$ is defined to be the probability that a point $q$ is contained inside a random closed hyper-ball obtained from a pair of points from $F$. The spherical depth $\SphD(q;S)$ is also defined for an arbitrary data set $S\subseteq \mathbb{R}^d$ and $q\in \mathbb{R}^d$. This definition is based on counting all of the closed hyper-balls, obtained from pairs of points in $S$, that contain $q$. The significant advantage of using the spherical depth in multivariate data analysis is related to its complexity of computation. Unlike most other data depths, the time complexity of the spherical depth grows linearly rather than exponentially in the dimension $d$. The straightforward algorithm for computing the spherical depth in dimension $d$ takes $O(dn^2)$. The main result of this paper is an optimal algorithm that we present for computing the bivariate spherical depth. The algorithm takes $O(n \log n)$ time. By reducing the problem of \textit{Element Uniqueness}, we prove that computing the spherical depth requires $\Omega(n \log n)$ time. Some geometric properties of spherical depth are also investigated in this paper. These properties indicate that \emph{simplicial depth} ($\SD$) (Liu, 1990) is linearly bounded by spherical depth (in particular, $\SphD\geq \frac{2}{3}SD$). To illustrate this relationship between the spherical depth and the simplicial depth, some experimental results are provided. The obtained experimental bound ($\SphD\geq 2\SD$) indicates that, perhaps, a stronger theoretical bound can be achieved.",distribut function mathbb point mathbb emph spheric depth sphd defin probabl point contain insid random close hyper ball obtain pair point spheric depth sphd also defin arbitrari data set subseteq mathbb mathbb definit base count close hyper ball obtain pair point contain signific advantag use spheric depth multivari data analysi relat complex comput unlik data depth time complex spheric depth grow linear rather exponenti dimens straightforward algorithm comput spheric depth dimens take dn main result paper optim algorithm present comput bivari spheric depth algorithm take log time reduc problem textit element uniqu prove comput spheric depth requir omega log time geometr properti spheric depth also investig paper properti indic emph simplici depth sd liu linear bound spheric depth particular sphd geq frac sd illustr relationship spheric depth simplici depth experiment result provid obtain experiment bound sphd geq sd indic perhap stronger theoret bound achiev,"['David Bremner', 'Rasoul Shahsavarifar']",['cs.CG'],False,False,True,False,False,True
245,2017-03-28T14:07:33Z,2017-02-20T20:14:46Z,http://arxiv.org/abs/1702.06163v1,http://arxiv.org/pdf/1702.06163v1,1-Fan-Bundle-Planar Drawings of Graphs,fan bundl planar draw graph,"Edge bundling is an important concept, heavily used for graph visualization purposes. To enable the comparison with other established nearly-planarity models in graph drawing, we formulate a new edge-bundling model which is inspired by the recently introduced fan-planar graphs. In particular, we restrict the bundling to the endsegments of the edges. As in 1-planarity, we call our model 1-fan-bundle-planarity, as we allow at most one crossing per bundle.   For the two variants where we allow either one or, more naturally, both endsegments of each edge to be part of bundles, we present edge density results and consider various recognition questions, not only for general graphs, but also for the outer and 2-layer variants. We conclude with a series of challenging questions.",edg bundl import concept heavili use graph visual purpos enabl comparison establish near planar model graph draw formul new edg bundl model inspir recent introduc fan planar graph particular restrict bundl endseg edg planar call model fan bundl planar allow one cross per bundl two variant allow either one natur endseg edg part bundl present edg densiti result consid various recognit question onli general graph also outer layer variant conclud seri challeng question,"['Patrizio Angelini', 'Michael A. Bekos', 'Michael Kaufmann', 'Philipp Kindermann', 'Thomas Schneck']","['cs.CG', 'math.CO']",False,False,False,False,False,True
246,2017-03-28T14:07:33Z,2017-02-20T08:56:40Z,http://arxiv.org/abs/1702.05900v1,http://arxiv.org/pdf/1702.05900v1,$δ$-Greedy $t$-spanner,greedi spanner,"We introduce a new geometric spanner, $\delta$-Greedy, whose construction is based on a generalization of the known Path-Greedy and Gap-Greedy spanners. The $\delta$-Greedy spanner combines the most desirable properties of geometric spanners both in theory and in practice. More specifically, it has the same theoretical and practical properties as the Path-Greedy spanner: a natural definition, small degree, linear number of edges, low weight, and strong $(1+\varepsilon)$-spanner for every $\varepsilon>0$. The $\delta$-Greedy algorithm is an improvement over the Path-Greedy algorithm with respect to the number of shortest path queries and hence with respect to its construction time. We show how to construct such a spanner for a set of $n$ points in the plane in $O(n^2 \log n)$ time.   The $\delta$-Greedy spanner has an additional parameter, $\delta$, which indicates how close it is to the Path-Greedy spanner on the account of the number of shortest path queries. For $\delta = t$ the output spanner is identical to the Path-Greedy spanner, while the number of shortest path queries is, in practice, linear.   Finally, we show that for a set of $n$ points placed independently at random in a unit square the expected construction time of the $\delta$-Greedy algorithm is $O(n \log n)$. Our analysis indicates that the $\delta$-Greedy spanner gives the best results among the known spanners of expected $O(n \log n)$ time for random point sets. Moreover, the analysis implies that by setting $\delta = t$, the $\delta$-Greedy algorithm provides a spanner identical to the Path-Greedy spanner in expected $O(n \log n)$ time.",introduc new geometr spanner delta greedi whose construct base general known path greedi gap greedi spanner delta greedi spanner combin desir properti geometr spanner theori practic specif theoret practic properti path greedi spanner natur definit small degre linear number edg low weight strong varepsilon spanner everi varepsilon delta greedi algorithm improv path greedi algorithm respect number shortest path queri henc respect construct time show construct spanner set point plane log time delta greedi spanner addit paramet delta indic close path greedi spanner account number shortest path queri delta output spanner ident path greedi spanner number shortest path queri practic linear final show set point place independ random unit squar expect construct time delta greedi algorithm log analysi indic delta greedi spanner give best result among known spanner expect log time random point set moreov analysi impli set delta delta greedi algorithm provid spanner ident path greedi spanner expect log time,"['Gali Bar-On', 'Paz Carmi']",['cs.CG'],False,False,True,False,False,True
247,2017-03-28T14:07:33Z,2017-02-19T15:48:11Z,http://arxiv.org/abs/1702.05760v1,http://arxiv.org/pdf/1702.05760v1,Hypercube LSH for approximate near neighbors,hypercub lsh approxim near neighbor,"A celebrated technique for finding near neighbors for the angular distance involves using a set of \textit{random} hyperplanes to partition the space into hash regions [Charikar, STOC 2002]. Experiments later showed that using a set of \textit{orthogonal} hyperplanes, thereby partitioning the space into the Voronoi regions induced by a hypercube, leads to even better results [Terasawa and Tanaka, WADS 2007]. However, no theoretical explanation for this improvement was ever given, and it remained unclear how the resulting hypercube hash method scales in high dimensions.   In this work, we provide explicit asymptotics for the collision probabilities when using hypercubes to partition the space. For instance, two near-orthogonal vectors are expected to collide with probability $(\frac{1}{\pi})^{d + o(d)}$ in dimension $d$, compared to $(\frac{1}{2})^d$ when using random hyperplanes. Vectors at angle $\frac{\pi}{3}$ collide with probability $(\frac{\sqrt{3}}{\pi})^{d + o(d)}$, compared to $(\frac{2}{3})^d$ for random hyperplanes, and near-parallel vectors collide with similar asymptotic probabilities in both cases.   For $c$-approximate nearest neighbor searching, this translates to a decrease in the exponent $\rho$ of locality-sensitive hashing (LSH) methods of a factor up to $\log_2(\pi) \approx 1.652$ compared to hyperplane LSH. For $c = 2$, we obtain $\rho \approx 0.302 + o(1)$ for hypercube LSH, improving upon the $\rho \approx 0.377$ for hyperplane LSH. We further describe how to use hypercube LSH in practice, and we consider an example application in the area of lattice algorithms.",celebr techniqu find near neighbor angular distanc involv use set textit random hyperplan partit space hash region charikar stoc experi later show use set textit orthogon hyperplan therebi partit space voronoi region induc hypercub lead even better result terasawa tanaka wad howev theoret explan improv ever given remain unclear result hypercub hash method scale high dimens work provid explicit asymptot collis probabl use hypercub partit space instanc two near orthogon vector expect collid probabl frac pi dimens compar frac use random hyperplan vector angl frac pi collid probabl frac sqrt pi compar frac random hyperplan near parallel vector collid similar asymptot probabl case approxim nearest neighbor search translat decreas expon rho local sensit hash lsh method factor log pi approx compar hyperplan lsh obtain rho approx hypercub lsh improv upon rho approx hyperplan lsh describ use hypercub lsh practic consid exampl applic area lattic algorithm,['Thijs Laarhoven'],"['cs.DS', 'cs.CC', 'cs.CG', 'cs.CR']",False,False,False,False,False,True
248,2017-03-28T14:07:33Z,2017-02-18T17:02:58Z,http://arxiv.org/abs/1702.05633v1,http://arxiv.org/pdf/1702.05633v1,Approximation Algorithms for Independence and Domination on B$_1$-VPG   and B$_1$-EPG Graphs,approxim algorithm independ domin vpg epg graph,"A graph $G$ is called B$_k$-VPG (resp., B$_k$-EPG), for some constant $k\geq 0$, if it has a string representation on a grid such that each vertex is an orthogonal path with at most $k$ bends and two vertices are adjacent in $G$ if and only if the corresponding strings intersect (resp., the corresponding strings share at least one grid edge). If two adjacent strings of a B$_k$-VPG graph intersect exactly once, then the graph is called a one-string B$_k$-VPG graph.   In this paper, we study the Maximum Independent Set and Minimum Dominating Set problems on B$_1$-VPG and B$_1$-EPG graphs. We first give a simple $O(\log n)$-approximation algorithm for the Maximum Independent Set problem on B$_1$-VPG graphs, improving the previous $O((\log n)^2)$-approximation algorithm of Lahiri et al. (COCOA 2015). Then, we consider the Minimum Dominating Set problem. We give an $O(1)$-approximation algorithm for this problem on one-string B$_1$-VPG graphs, providing the first constant-factor approximation algorithm for this problem. Moreover, we show that the Minimum Dominating Set problem is APX-hard on B$_1$-EPG graphs, ruling out the possibility of a PTAS unless P=NP. Finally, we give constant-factor approximation algorithms for this problem on two non-trivial subclasses of B$_1$-EPG graphs. To our knowledge, these are the first results for the Minimum Dominating Set problem on B$_1$-EPG graphs, partially answering a question posed by Epstein et al. (WADS 2013).",graph call vpg resp epg constant geq string represent grid vertex orthogon path bend two vertic adjac onli correspond string intersect resp correspond string share least one grid edg two adjac string vpg graph intersect exact onc graph call one string vpg graph paper studi maximum independ set minimum domin set problem vpg epg graph first give simpl log approxim algorithm maximum independ set problem vpg graph improv previous log approxim algorithm lahiri et al cocoa consid minimum domin set problem give approxim algorithm problem one string vpg graph provid first constant factor approxim algorithm problem moreov show minimum domin set problem apx hard epg graph rule possibl ptas unless np final give constant factor approxim algorithm problem two non trivial subclass epg graph knowledg first result minimum domin set problem epg graph partial answer question pose epstein et al wad,['Saeed Mehrabi'],['cs.CG'],False,False,False,False,False,True
250,2017-03-28T14:07:37Z,2017-02-17T14:34:08Z,http://arxiv.org/abs/1702.05358v1,http://arxiv.org/pdf/1702.05358v1,Computational topology of graphs on surfaces,comput topolog graph surfac,"Computational topology is an area that revisits topological problems from an algorithmic point of view, and develops topological tools for improved algorithms. We survey results in computational topology that are concerned with graphs drawn on surfaces. Typical questions include representing surfaces and graphs embedded on them computationally, deciding whether a graph embeds on a surface, solving computational problems related to homotopy, optimizing curves and graphs on surfaces, and solving standard graph algorithm problems more efficiently in the case of surface-embedded graphs.",comput topolog area revisit topolog problem algorithm point view develop topolog tool improv algorithm survey result comput topolog concern graph drawn surfac typic question includ repres surfac graph embed comput decid whether graph emb surfac solv comput problem relat homotopi optim curv graph surfac solv standard graph algorithm problem effici case surfac embed graph,['Éric Colin de Verdière'],"['cs.CG', 'cs.DM', 'cs.DS', 'math.AT', 'math.CO', '68U05, 05C10, 57M15, 68R10', 'F.2.2; G.2.2; I.3.5']",False,False,False,True,False,True
253,2017-03-28T14:07:37Z,2017-02-14T15:20:23Z,http://arxiv.org/abs/1702.04259v1,http://arxiv.org/pdf/1702.04259v1,On the metastable Mabillard-Wagner conjecture,metast mabillard wagner conjectur,"The purpose of this note is to attract attention to the following conjecture (metastable $r$-fold Whitney trick) by clarifying its status as not having a complete proof, in the sense described in the paper.   Assume that $D=D_1\sqcup\ldots\sqcup D_r$ is disjoint union of $r$ disks of dimension $s$, $f:D\to B^d$ a proper PL map such that $f\partial D_1\cap\ldots\cap f\partial D_r=\emptyset$, $rd\ge (r+1)s+3$ and $d\ge s+3$. If the map $$f^r:\partial(D_1\times\ldots\times D_r)\to (B^d)^r-\{(x,x,\ldots,x)\in(B^d)^r\  \ x\in B^d\}$$ extends to $D_1\times\ldots\times D_r$, then there is a PL map $\overline f:D\to B^d$ such that $$\overline f=f \quad\text{on}\quad D_r\cup\partial D\quad\text{and}\quad \overline fD_1\cap\ldots\cap \overline fD_r=\emptyset.$$",purpos note attract attent follow conjectur metast fold whitney trick clarifi status complet proof sens describ paper assum sqcup ldot sqcup disjoint union disk dimens proper pl map partial cap ldot cap partial emptyset rd ge ge map partial time ldot time ldot extend time ldot time pl map overlin overlin quad text quad cup partial quad text quad overlin fd cap ldot cap overlin fd emptyset,['A. Skopenkov'],"['math.GT', 'cs.CG', '57Q35, 57R65, 52B99']",False,False,True,False,False,True
254,2017-03-28T14:07:37Z,2017-03-19T08:52:47Z,http://arxiv.org/abs/1702.03676v2,http://arxiv.org/pdf/1702.03676v2,Epsilon-approximations and epsilon-nets,epsilon approxim epsilon net,The use of random samples to approximate properties of geometric configurations has been an influential idea for both combinatorial and algorithmic purposes. This chapter considers two related notions---$\epsilon$-approximations and $\epsilon$-nets---that capture the most important quantitative properties that one would expect from a random sample with respect to an underlying geometric configuration.,use random sampl approxim properti geometr configur influenti idea combinatori algorithm purpos chapter consid two relat notion epsilon approxim epsilon net captur import quantit properti one would expect random sampl respect geometr configur,"['Nabil H. Mustafa', 'Kasturi R. Varadarajan']","['cs.CG', 'math.CO', 'math.PR']",False,False,False,False,False,True
255,2017-03-28T14:07:37Z,2017-02-11T01:20:50Z,http://arxiv.org/abs/1702.03364v1,http://arxiv.org/pdf/1702.03364v1,Techniques in Lattice Basis Reduction,techniqu lattic basi reduct,"The credit on {\it reduction theory} goes back to the work of Lagrange, Gauss, Hermite, Korkin, Zolotarev, and Minkowski. Modern reduction theory is voluminous and includes the work of A. Lenstra, H. Lenstra and L. Lovasz who created the well known LLL algorithm, and many other researchers such as L. Babai and C. P. Schnorr who created significant new variants of basis reduction algorithms. In this paper, we propose and investigate the efficacy of new optimization techniques to be used along with LLL algorithm. The techniques we have proposed are: i) {\it hill climbing (HC)}, ii) {\it lattice diffusion-sub lattice fusion (LDSF)}, and iii) {\it multistage hybrid LDSF-HC}. The first technique relies on the sensitivity of LLL to permutations of the input basis $B$, and optimization ideas over the symmetric group $S_m$ viewed as a metric space. The second technique relies on partitioning the lattice into sublattices, performing basis reduction in the partition sublattice blocks, fusing the sublattices, and repeating. We also point out places where parallel computation can reduce run-times achieving almost linear speedup. The multistage hybrid technique relies on the lattice diffusion and sublattice fusion and hill climbing algorithms.",credit reduct theori goe back work lagrang gauss hermit korkin zolotarev minkowski modern reduct theori volumin includ work lenstra lenstra lovasz creat well known lll algorithm mani research babai schnorr creat signific new variant basi reduct algorithm paper propos investig efficaci new optim techniqu use along lll algorithm techniqu propos hill climb hc ii lattic diffus sub lattic fusion ldsf iii multistag hybrid ldsf hc first techniqu reli sensit lll permut input basi optim idea symmetr group view metric space second techniqu reli partit lattic sublattic perform basi reduct partit sublattic block fuse sublattic repeat also point place parallel comput reduc run time achiev almost linear speedup multistag hybrid techniqu reli lattic diffus sublattic fusion hill climb algorithm,"['Bal K. Khadka', 'Spyros M. Magliveras']",['cs.CG'],False,False,False,False,False,True
256,2017-03-28T14:07:37Z,2017-02-10T17:54:59Z,http://arxiv.org/abs/1702.03266v1,http://arxiv.org/pdf/1702.03266v1,Two Optimization Problems for Unit Disks,two optim problem unit disk,"We present an implementation of a recent algorithm to compute shortest-path trees in unit disk graphs in $O(n\log n)$ worst-case time, where $n$ is the number of disks.   In the minimum-separation problem, we are given $n$ unit disks and two points $s$ and $t$, not contained in any of the disks, and we want to compute the minimum number of disks one needs to retain so that any curve connecting $s$ to $t$ intersects some of the retained disks. We present a new algorithm solving this problem in $O(n^2\log^3 n)$ worst-case time and its implementation.",present implement recent algorithm comput shortest path tree unit disk graph log worst case time number disk minimum separ problem given unit disk two point contain ani disk want comput minimum number disk one need retain ani curv connect intersect retain disk present new algorithm solv problem log worst case time implement,"['Sergio Cabello', 'Lazar Milinković']",['cs.CG'],False,False,False,False,False,True
257,2017-03-28T14:07:37Z,2017-02-10T14:37:00Z,http://arxiv.org/abs/1702.03187v1,http://arxiv.org/pdf/1702.03187v1,On vertices and facets of combinatorial 2-level polytopes,vertic facet combinatori level polytop,"2-level polytopes naturally appear in several areas of pure and applied mathematics, including combinatorial optimization, polyhedral combinatorics, communication complexity, and statistics. In this paper, we present a polyhedral study of 2-level polytopes arising in combinatorial settings. For all the known (to the best of our knowledge) such polytopes P, we show that v(P).f(P) is upper bounded by d2^(d+1). Here v(P) (resp. f(P)) is the number of vertices (resp. facets) of P, and d is its dimension. Whether this holds for all 2-level polytopes was asked in [Bohn et al., ESA 2015], where experimental results showed it true up to dimension 6. The key to most of our proofs is an understanding of the combinatorial structures underlying those polytopes. This leads to a number of results that we believe to be of independent interest: a trade-off formula for the number of cliques and stable sets in a graph; a description of the facets of the base polytope of the 2-sum of matroids; a linear-size description of the base polytope of matroids that are 2-level in terms of cuts of an associated tree. We also give a self-contained proof of the characterization of the last class, a result first obtained by Grande and Sanyal.",level polytop natur appear sever area pure appli mathemat includ combinatori optim polyhedr combinator communic complex statist paper present polyhedr studi level polytop aris combinatori set known best knowledg polytop show upper bound resp number vertic resp facet dimens whether hold level polytop ask bohn et al esa experiment result show true dimens key proof understand combinatori structur polytop lead number result believ independ interest trade formula number cliqu stabl set graph descript facet base polytop sum matroid linear size descript base polytop matroid level term cut associ tree also give self contain proof character last class result first obtain grand sanyal,"['Manuel Aprile', 'Alfonso Cevallos', 'Yuri Faenza']","['math.CO', 'cs.CG']",False,False,False,False,False,True
258,2017-03-28T14:07:37Z,2017-02-09T14:04:20Z,http://arxiv.org/abs/1702.02838v1,http://arxiv.org/pdf/1702.02838v1,The DTM-signature for a geometric comparison of metric-measure spaces   from samples,dtm signatur geometr comparison metric measur space sampl,"In this paper, we introduce the notion of DTM-signature, a measure on R + that can be associated to any metric-measure space. This signature is based on the distance to a measure (DTM) introduced by Chazal, Cohen-Steiner and M\'erigot. It leads to a pseudo-metric between metric-measure spaces, upper-bounded by the Gromov-Wasserstein distance. Under some geometric assumptions, we derive lower bounds for this pseudo-metric. Given two N-samples, we also build an asymptotic statistical test based on the DTM-signature, to reject the hypothesis of equality of the two underlying metric-measure spaces, up to a measure-preserving isometry. We give strong theoretical justifications for this test and propose an algorithm for its implementation.",paper introduc notion dtm signatur measur associ ani metric measur space signatur base distanc measur dtm introduc chazal cohen steiner erigot lead pseudo metric metric measur space upper bound gromov wasserstein distanc geometr assumpt deriv lower bound pseudo metric given two sampl also build asymptot statist test base dtm signatur reject hypothesi equal two metric measur space measur preserv isometri give strong theoret justif test propos algorithm implement,['Claire Brécheteau'],"['cs.CG', 'math.PR', 'math.ST', 'stat.TH']",False,False,False,False,False,True
259,2017-03-28T14:07:37Z,2017-02-07T01:11:38Z,http://arxiv.org/abs/1702.01836v1,http://arxiv.org/pdf/1702.01836v1,Linear Time Approximation Schemes for Geometric Maximum Coverage,linear time approxim scheme geometr maximum coverag,"We study approximation algorithms for the following geometric version of the maximum coverage problem: Let $\mathcal{P}$ be a set of $n$ weighted points in the plane. Let $D$ represent a planar object, such as a rectangle, or a disk. We want to place $m$ copies of $D$ such that the sum of the weights of the points in $\mathcal{P}$ covered by these copies is maximized. For any fixed $\varepsilon>0$, we present efficient approximation schemes that can find a $(1-\varepsilon)$-approximation to the optimal solution. In particular, for $m=1$ and for the special case where $D$ is a rectangle, our algorithm runs in time $O(n\log (\frac{1}{\varepsilon}))$, improving on the previous result. For $m>1$ and the rectangular case, our algorithm runs in $O(\frac{n}{\varepsilon}\log (\frac{1}{\varepsilon})+\frac{m}{\varepsilon}\log m +m(\frac{1}{\varepsilon})^{O(\min(\sqrt{m},\frac{1}{\varepsilon}))})$ time. For a more general class of shapes (including disks, polygons with $O(1)$ edges), our algorithm runs in $O(n(\frac{1}{\varepsilon})^{O(1)}+\frac{m}{\epsilon}\log m + m(\frac{1}{\varepsilon})^{O(\min(m,\frac{1}{\varepsilon^2}))})$ time.",studi approxim algorithm follow geometr version maximum coverag problem let mathcal set weight point plane let repres planar object rectangl disk want place copi sum weight point mathcal cover copi maxim ani fix varepsilon present effici approxim scheme find varepsilon approxim optim solut particular special case rectangl algorithm run time log frac varepsilon improv previous result rectangular case algorithm run frac varepsilon log frac varepsilon frac varepsilon log frac varepsilon min sqrt frac varepsilon time general class shape includ disk polygon edg algorithm run frac varepsilon frac epsilon log frac varepsilon min frac varepsilon time,"['Kai Jin', 'Jian Li', 'Haitao Wang', 'Bowei Zhang', 'Ningye Zhang']","['cs.CG', 'F.2.2']",False,False,False,False,False,True
260,2017-03-28T14:07:41Z,2017-02-06T21:31:57Z,http://arxiv.org/abs/1702.01799v1,http://arxiv.org/pdf/1702.01799v1,Radial Contour Labeling with Straight Leaders,radial contour label straight leader,"The usefulness of technical drawings as well as scientific illustrations such as medical drawings of human anatomy essentially depends on the placement of labels that describe all relevant parts of the figure. In order to not spoil or clutter the figure with text, the labels are often placed around the figure and are associated by thin connecting lines to their features, respectively. This labeling technique is known as external label placement.   In this paper we introduce a flexible and general approach for external label placement assuming a contour of the figure prescribing the possible positions of the labels. While much research on external label placement aims for fast labeling procedures for interactive systems, we focus on highest-quality illustrations. Based on interviews with domain experts and a semi-automatic analysis of 202 handmade anatomical drawings, we identify a set of 18 layout quality criteria, naturally not all of equal importance. We design a new geometric label placement algorithm that is based only on the most important criteria. Yet, other criteria can flexibly be included in the algorithm, either as hard constraints not to be violated or as soft constraints whose violation is penalized by a general cost function. We formally prove that our approach yields labelings that satisfy all hard constraints and have minimum overall cost. Introducing several speedup techniques, we further demonstrate how to deploy our approach in practice. In an experimental evaluation on real-world anatomical drawings we show that the resulting labelings are of high quality and can be produced in adequate time.",use technic draw well scientif illustr medic draw human anatomi essenti depend placement label describ relev part figur order spoil clutter figur text label often place around figur associ thin connect line featur respect label techniqu known extern label placement paper introduc flexibl general approach extern label placement assum contour figur prescrib possibl posit label much research extern label placement aim fast label procedur interact system focus highest qualiti illustr base interview domain expert semi automat analysi handmad anatom draw identifi set layout qualiti criteria natur equal import design new geometr label placement algorithm base onli import criteria yet criteria flexibl includ algorithm either hard constraint violat soft constraint whose violat penal general cost function formal prove approach yield label satisfi hard constraint minimum overal cost introduc sever speedup techniqu demonstr deploy approach practic experiment evalu real world anatom draw show result label high qualiti produc adequ time,"['Benjamin Niedermann', 'Martin Nöllenburg', 'Ignaz Rutter']",['cs.CG'],False,False,True,False,False,True
261,2017-03-28T14:07:41Z,2017-02-06T17:38:26Z,http://arxiv.org/abs/1702.01719v1,http://arxiv.org/pdf/1702.01719v1,A 2-Approximation for the Height of Maximal Outerplanar Graph Drawings,approxim height maxim outerplanar graph draw,"In this paper, we study planar drawings of maximal outerplanar graphs with the objective of achieving small height. A recent paper gave an algorithm for such drawings that is within a factor of 4 of the optimum height. In this paper, we substantially improve the approximation factor to become 2. The main ingredient is to define a new parameter of outerplanar graphs (the so-called umbrella depth, obtained by recursively splitting the graph into graphs called umbrellas). We argue that the height of any poly-line drawing must be at least the umbrella depth, and then devise an algorithm that achieves height at most twice the umbrella depth.",paper studi planar draw maxim outerplanar graph object achiev small height recent paper gave algorithm draw within factor optimum height paper substanti improv approxim factor becom main ingredi defin new paramet outerplanar graph call umbrella depth obtain recurs split graph graph call umbrella argu height ani poli line draw must least umbrella depth devis algorithm achiev height twice umbrella depth,"['Therese Biedl', 'Philippe Demontigny']","['cs.DS', 'cs.CG']",False,False,False,False,False,True
262,2017-03-28T14:07:41Z,2017-02-06T08:01:16Z,http://arxiv.org/abs/1702.01524v1,http://arxiv.org/pdf/1702.01524v1,Edge N-Level Sparse Visibility Graphs: Fast Optimal Any-Angle   Pathfinding Using Hierarchical Taut Paths,edg level spars visibl graph fast optim ani angl pathfind use hierarch taut path,"In the Any-Angle Pathfinding problem, the goal is to find the shortest path between a pair of vertices on a uniform square grid, that is not constrained to any fixed number of possible directions over the grid. Visibility Graphs are a known optimal algorithm for solving the problem with the use of pre-processing. However, Visibility Graphs are known to perform poorly in terms of running time, especially on large, complex maps. In this paper, we introduce two improvements over the Visibility Graph Algorithm to compute optimal paths. Sparse Visibility Graphs (SVGs) are constructed by pruning unnecessary edges from the original Visibility Graph. Edge N-Level Sparse Visibility Graphs (ENLSVGs) is a hierarchical SVG built by iteratively pruning non-taut paths. We also introduce Line-of-Sight Scans, a faster algorithm for building Visibility Graphs over a grid. SVGs run much faster than Visibility Graphs by reducing the average vertex degree. ENLSVGs, a hierarchical algorithm, improves this further, especially on larger maps. On large maps, with the use of pre-processing, these algorithms are orders of magnitude faster than existing algorithms like Visibility Graphs and Theta*.",ani angl pathfind problem goal find shortest path pair vertic uniform squar grid constrain ani fix number possibl direct grid visibl graph known optim algorithm solv problem use pre process howev visibl graph known perform poor term run time especi larg complex map paper introduc two improv visibl graph algorithm comput optim path spars visibl graph svgs construct prune unnecessari edg origin visibl graph edg level spars visibl graph enlsvg hierarch svg built iter prune non taut path also introduc line sight scan faster algorithm build visibl graph grid svgs run much faster visibl graph reduc averag vertex degre enlsvg hierarch algorithm improv especi larger map larg map use pre process algorithm order magnitud faster exist algorithm like visibl graph theta,"['Shunhao Oh', 'Hon Wai Leong']",['cs.CG'],False,False,False,False,False,True
263,2017-03-28T14:07:41Z,2017-02-09T01:46:20Z,http://arxiv.org/abs/1702.01446v2,http://arxiv.org/pdf/1702.01446v2,Efficient Algorithms for k-Regret Minimizing Sets,effici algorithm regret minim set,"A regret minimizing set Q is a small size representation of a much larger database P so that user queries executed on Q return answers whose scores are not much worse than those on the full dataset. In particular, a k-regret minimizing set has the property that the regret ratio between the score of the top-1 item in Q and the score of the top-k item in P is minimized, where the score of an item is the inner product of the item's attributes with a user's weight (preference) vector. The problem is challenging because we want to find a single representative set Q whose regret ratio is small with respect to all possible user weight vectors.   We show that k-regret minimization is NP-Complete for all dimensions d >= 3. This settles an open problem from Chester et al. [VLDB 2014], and resolves the complexity status of the problem for all d: the problem is known to have polynomial-time solution for d <= 2. In addition, we propose two new approximation schemes for regret minimization, both with provable guarantees, one based on coresets and another based on hitting sets. We also carry out extensive experimental evaluation, and show that our schemes compute regret-minimizing sets comparable in size to the greedy algorithm proposed in [VLDB 14] but our schemes are significantly faster and scalable to large data sets.",regret minim set small size represent much larger databas user queri execut return answer whose score much wors full dataset particular regret minim set properti regret ratio score top item score top item minim score item inner product item attribut user weight prefer vector problem challeng becaus want find singl repres set whose regret ratio small respect possibl user weight vector show regret minim np complet dimens settl open problem chester et al vldb resolv complex status problem problem known polynomi time solut addit propos two new approxim scheme regret minim provabl guarante one base coreset anoth base hit set also carri extens experiment evalu show scheme comput regret minim set compar size greedi algorithm propos vldb scheme signific faster scalabl larg data set,"['Pankaj K. Agarwal', 'Nirman Kumar', 'Stavros Sintos', 'Subhash Suri']","['cs.DS', 'cs.CG', 'cs.DB']",False,False,False,False,False,True
265,2017-03-28T14:07:41Z,2017-02-04T11:51:44Z,http://arxiv.org/abs/1702.01275v1,http://arxiv.org/pdf/1702.01275v1,Geometric Biplane Graphs I: Maximal Graphs,geometr biplan graph maxim graph,"We study biplane graphs drawn on a finite planar point set $S$ in general position. This is the family of geometric graphs whose vertex set is $S$ and can be decomposed into two plane graphs. We show that two maximal biplane graphs---in the sense that no edge can be added while staying biplane---may differ in the number of edges, and we provide an efficient algorithm for adding edges to a biplane graph to make it maximal. We also study extremal properties of maximal biplane graphs such as the maximum number of edges and the largest maximum connectivity over $n$-element point sets.",studi biplan graph drawn finit planar point set general posit famili geometr graph whose vertex set decompos two plane graph show two maxim biplan graph sens edg ad stay biplan may differ number edg provid effici algorithm ad edg biplan graph make maxim also studi extrem properti maxim biplan graph maximum number edg largest maximum connect element point set,"['Alfredo García', 'Ferran Hurtado', 'Matias Korman', 'Inês Matos', 'Maria Saumell', 'Rodrigo I. Silveira', 'Javier Tejel', 'Csaba D. Tóth']",['cs.CG'],False,False,True,False,False,True
267,2017-03-28T14:07:41Z,2017-02-01T16:55:41Z,http://arxiv.org/abs/1702.00353v1,http://arxiv.org/pdf/1702.00353v1,The non-cooperative tile assembly model is not intrinsically universal   or capable of bounded Turing machine simulation,non cooper tile assembl model intrins univers capabl bound ture machin simul,"The field of algorithmic self-assembly is concerned with the computational and expressive power of nanoscale self-assembling molecular systems. In the well-studied cooperative, or temperature 2, abstract tile assembly model it is known that there is a tile set to simulate any Turing machine and an intrinsically universal tile set that simulates the shapes and dynamics of any instance of the model, up to spatial rescaling. It has been an open question as to whether the seemingly simpler noncooperative, or temperature 1, model is capable of such behaviour. Here we show that this is not the case, by showing that there is no tile set in the noncooperative model that is intrinsically universal, nor one capable of time-bounded Turing machine simulation within a bounded region of the plane.   Although the noncooperative model intuitively seems to lack the complexity and power of the cooperative model it has been exceedingly hard to prove this. One reason is that there have been few tools to analyse the structure of complicated paths in the plane. This paper provides a number of such tools. A second reason is that almost every obvious and small generalisation to the model (e.g. allowing error, 3D, non-square tiles, signals/wires on tiles, tiles that repel each other, parallel synchronous growth) endows it with great computational, and sometimes simulation, power. Our main results show that all of these generalisations provably increase computational and/or simulation power. Our results hold for both deterministic and nondeterministic noncooperative systems. Our first main result stands in stark contrast with the fact that for both the cooperative tile assembly model, and for 3D noncooperative tile assembly, there are respective intrinsically universal tilesets. Our second main result gives a new technique (reduction to simulation) for proving negative results about computation in tile assembly.",field algorithm self assembl concern comput express power nanoscal self assembl molecular system well studi cooper temperatur abstract tile assembl model known tile set simul ani ture machin intrins univers tile set simul shape dynam ani instanc model spatial rescal open question whether seem simpler noncoop temperatur model capabl behaviour show case show tile set noncoop model intrins univers one capabl time bound ture machin simul within bound region plane although noncoop model intuit seem lack complex power cooper model exceed hard prove one reason tool analys structur complic path plane paper provid number tool second reason almost everi obvious small generalis model allow error non squar tile signal wire tile tile repel parallel synchron growth endow great comput sometim simul power main result show generalis provabl increas comput simul power result hold determinist nondeterminist noncoop system first main result stand stark contrast fact cooper tile assembl model noncoop tile assembl respect intrins univers tileset second main result give new techniqu reduct simul prove negat result comput tile assembl,"['Pierre-Étienne Meunier', 'Damien Woods']","['cs.CC', 'cs.CG', 'cs.DS']",False,False,False,False,False,True
268,2017-03-28T14:07:41Z,2017-02-01T06:45:40Z,http://arxiv.org/abs/1702.00146v1,http://arxiv.org/pdf/1702.00146v1,Untangling Planar Curves,untangl planar curv,"Any generic closed curve in the plane can be transformed into a simple closed curve by a finite sequence of local transformations called homotopy moves. We prove that simplifying a planar closed curve with $n$ self-crossings requires $\Theta(n^{3/2})$ homotopy moves in the worst case. Our algorithm improves the best previous upper bound $O(n^2)$, which is already implicit in the classical work of Steinitz; the matching lower bound follows from the construction of closed curves with large defect, a topological invariant of generic closed curves introduced by Aicardi and Arnold. Our lower bound also implies that $\Omega(n^{3/2})$ facial electrical transformations are required to reduce any plane graph with treewidth $\Omega(\sqrt{n})$ to a single vertex, matching known upper bounds for rectangular and cylindrical grid graphs. More generally, we prove that transforming one immersion of $k$ circles with at most $n$ self-crossings into another requires $\Theta(n^{3/2} + nk + k^2)$ homotopy moves in the worst case. Finally, we prove that transforming one noncontractible closed curve to another on any orientable surface requires $\Omega(n^2)$ homotopy moves in the worst case; this lower bound is tight if the curve is homotopic to a simple closed curve.",ani generic close curv plane transform simpl close curv finit sequenc local transform call homotopi move prove simplifi planar close curv self cross requir theta homotopi move worst case algorithm improv best previous upper bound alreadi implicit classic work steinitz match lower bound follow construct close curv larg defect topolog invari generic close curv introduc aicardi arnold lower bound also impli omega facial electr transform requir reduc ani plane graph treewidth omega sqrt singl vertex match known upper bound rectangular cylindr grid graph general prove transform one immers circl self cross anoth requir theta nk homotopi move worst case final prove transform one noncontract close curv anoth ani orient surfac requir omega homotopi move worst case lower bound tight curv homotop simpl close curv,"['Hsien-Chih Chang', 'Jeff Erickson']","['cs.CG', 'math.GT']",False,False,False,False,False,True
269,2017-03-28T14:07:41Z,2017-01-29T19:55:27Z,http://arxiv.org/abs/1701.08423v1,http://arxiv.org/pdf/1701.08423v1,One Size Fits All : Effectiveness of Local Search on Structured Data,one size fit effect local search structur data,"In this paper, we analyze the performance of a simple and standard Local Search algorithm for clustering on well behaved data. Since the seminal paper by Ostrovsky, Rabani, Schulman and Swamy [FOCS 2006], much progress has been made to characterize real-world instances. We distinguish the three main definitions -- Distribution Stability (Awasthi, Blum, Sheffet, FOCS 2010) -- Spectral Separability (Kumar, Kannan, FOCS 2010) -- Perturbation Resilience (Bilu, Linial, ICS 2010) We show that Local Search performs well on the instances with the aforementioned stability properties. Specifically, for the $k$-means and $k$-median objective, we show that Local Search exactly recovers the optimal clustering if the dataset is $3+\varepsilon$-perturbation resilient, and is a PTAS for distribution stability and spectral separability. This implies the first PTAS for instances satisfying the spectral separability condition. For the distribution stability condition we also go beyond previous work by showing that the clustering output by the algorithm and the optimal clustering are very similar. This is a significant step toward understanding the success of Local Search heuristics in clustering applications and supports the legitimacy of the stability conditions: They characterize some of the structure of real-world instances that make Local Search a popular heuristic.",paper analyz perform simpl standard local search algorithm cluster well behav data sinc semin paper ostrovski rabani schulman swami foc much progress made character real world instanc distinguish three main definit distribut stabil awasthi blum sheffet foc spectral separ kumar kannan foc perturb resili bilu linial ic show local search perform well instanc aforement stabil properti specif mean median object show local search exact recov optim cluster dataset varepsilon perturb resili ptas distribut stabil spectral separ impli first ptas instanc satisfi spectral separ condit distribut stabil condit also go beyond previous work show cluster output algorithm optim cluster veri similar signific step toward understand success local search heurist cluster applic support legitimaci stabil condit character structur real world instanc make local search popular heurist,"['Vincent Cohen-Addad', 'Chris Schwiegelshohn']","['cs.DS', 'cs.CG', 'cs.LG']",False,False,False,False,False,True
270,2017-03-28T14:07:47Z,2017-01-19T18:01:10Z,http://arxiv.org/abs/1701.05532v1,http://arxiv.org/pdf/1701.05532v1,Tighter Bounds for the Discrepancy of Boxes and Polytopes,tighter bound discrep box polytop,"Combinatorial discrepancy is a complexity measure of a collection of sets which quantifies how well the sets in the collection can be simultaneously balanced. More precisely, we are given an n-point set $P$, and a collection $\mathcal{F} = \{F_1, ..., F_m\}$ of subsets of $P$, and our goal is color $P$ with two colors, red and blue, so that the largest absolute difference between the number of red elements and the number of blue elements (i.e. the discrepancy) in any $F_i$ is minimized. Combinatorial discrepancy has many applications in mathematics and computer science, including constructions of uniformly distributed point sets, and lower bounds for data structures and private data analysis algorithms.   We investigate the combinatorial discrepancy of geometrically defined set systems, in which $P$ is an n-point set in $d$-dimensional space, and $\mathcal{F}$ is the collection of subsets of $P$ induced by dilations and translations of a fixed convex polytope $B$. Such set systems include sets induced by axis-aligned boxes, whose discrepancy is the subject of the well known Tusnady problem. We prove new discrepancy upper bounds for such set systems by extending the approach based on factorization norms previously used by the author and Matousek. We improve the best known upper bound for the Tusnady problem by a logarithmic factor, using a result of Banaszczyk on signed series of vectors. We extend this improvement to any arbitrary convex polytope B by using a decomposition due to Matousek.",combinatori discrep complex measur collect set quantifi well set collect simultan balanc precis given point set collect mathcal subset goal color two color red blue largest absolut differ number red element number blue element discrep ani minim combinatori discrep mani applic mathemat comput scienc includ construct uniform distribut point set lower bound data structur privat data analysi algorithm investig combinatori discrep geometr defin set system point set dimension space mathcal collect subset induc dilat translat fix convex polytop set system includ set induc axi align box whose discrep subject well known tusnadi problem prove new discrep upper bound set system extend approach base factor norm previous use author matousek improv best known upper bound tusnadi problem logarithm factor use result banaszczyk sign seri vector extend improv ani arbitrari convex polytop use decomposit due matousek,['Aleksandar Nikolov'],"['math.CO', 'cs.CG']",False,False,False,False,False,True
273,2017-03-28T14:07:47Z,2017-01-19T03:57:28Z,http://arxiv.org/abs/1701.05290v1,http://arxiv.org/pdf/1701.05290v1,Range-efficient consistent sampling and locality-sensitive hashing for   polygons,rang effici consist sampl local sensit hash polygon,"Locality-sensitive hashing (LSH) is a fundamental technique for similarity search and similarity estimation in high-dimensional spaces. The basic idea is that similar objects should produce hash collisions with probability significantly larger than objects with low similarity. We consider LSH for objects that can be represented as point sets in either one or two dimensions. To make the point sets finite size we consider the subset of points on a grid. Directly applying LSH (e.g. min-wise hashing) to these point sets would require time proportional to the number of points. We seek to achieve time that is much lower than direct approaches.   Technically, we introduce new primitives for range-efficient consistent sampling (of independent interest), and show how to turn such samples into LSH values. Another application of our technique is a data structure for quickly estimating the size of the intersection or union of a set of preprocessed polygons. Curiously, our consistent sampling method uses transformation to a geometric problem.",local sensit hash lsh fundament techniqu similar search similar estim high dimension space basic idea similar object produc hash collis probabl signific larger object low similar consid lsh object repres point set either one two dimens make point set finit size consid subset point grid direct appli lsh min wise hash point set would requir time proport number point seek achiev time much lower direct approach technic introduc new primit rang effici consist sampl independ interest show turn sampl lsh valu anoth applic techniqu data structur quick estim size intersect union set preprocess polygon curious consist sampl method use transform geometr problem,"['Joachim Gudmundsson', 'Rasmus Pagh']","['cs.CG', '68U05', 'F.2.2']",False,False,False,False,False,True
274,2017-03-28T14:07:47Z,2017-01-19T03:20:51Z,http://arxiv.org/abs/1701.05286v1,http://arxiv.org/pdf/1701.05286v1,Algorithms For Longest Chains In Pseudo- Transitive Graphs,algorithm longest chain pseudo transit graph,"A directed acyclic graph G = (V, E) is pseudo-transitive with respect to a given subset of edges E1, if for any edge ab in E1 and any edge bc in E, we have ac in E. We give algorithms for computing longest chains and demonstrate geometric applications that unify and improves some important past results. (For specific applications see the introduction.)",direct acycl graph pseudo transit respect given subset edg ani edg ab ani edg bc ac give algorithm comput longest chain demonstr geometr applic unifi improv import past result specif applic see introduct,['Farhad Shahrokhi'],"['cs.CG', 'math.CO']",False,False,False,False,False,True
276,2017-03-28T14:07:47Z,2017-01-13T15:08:46Z,http://arxiv.org/abs/1701.03693v1,http://arxiv.org/pdf/1701.03693v1,Multivariate Analysis for Computing Maxima in High Dimensions,multivari analysi comput maxima high dimens,"We study the problem of computing the \textsc{Maxima} of a set of $n$ $d$-dimensional points. For dimensions 2 and 3, there are algorithms to solve the problem with order-oblivious instance-optimal running time. However, in higher dimensions there is still room for improvements. We present an algorithm sensitive to the structural entropy of the input set, which improves the running time, for large classes of instances, on the best solution for \textsc{Maxima} to date for $d \ge 4$.",studi problem comput textsc maxima set dimension point dimens algorithm solv problem order oblivi instanc optim run time howev higher dimens still room improv present algorithm sensit structur entropi input set improv run time larg class instanc best solut textsc maxima date ge,"['Jérémy Barbay', 'Javiel Rojas']","['cs.CG', 'cs.DS', 'F.2.2']",False,False,False,False,False,True
278,2017-03-28T14:07:47Z,2017-01-12T04:51:15Z,http://arxiv.org/abs/1701.03230v1,http://arxiv.org/pdf/1701.03230v1,Surface Reconstruction with Data-driven Exemplar Priors,surfac reconstruct data driven exemplar prior,"In this paper, we propose a framework to reconstruct 3D models from raw scanned points by learning the prior knowledge of a specific class of objects. Unlike previous work that heuristically specifies particular regularities and defines parametric models, our shape priors are learned directly from existing 3D models under a framework based on affinity propagation. Given a database of 3D models within the same class of objects, we build a comprehensive library of 3D local shape priors. We then formulate the problem to select as-few-as-possible priors from the library, referred to as exemplar priors. These priors are sufficient to represent the 3D shapes of the whole class of objects from where they are generated. By manipulating these priors, we are able to reconstruct geometrically faithful models with the same class of objects from raw point clouds. Our framework can be easily generalized to reconstruct various categories of 3D objects that have more geometrically or topologically complex structures. Comprehensive experiments exhibit the power of our exemplar priors for gracefully solving several problems in 3D shape reconstruction such as preserving sharp features, recovering fine details and so on.",paper propos framework reconstruct model raw scan point learn prior knowledg specif class object unlik previous work heurist specifi particular regular defin parametr model shape prior learn direct exist model framework base affin propag given databas model within class object build comprehens librari local shape prior formul problem select possibl prior librari refer exemplar prior prior suffici repres shape whole class object generat manipul prior abl reconstruct geometr faith model class object raw point cloud framework easili general reconstruct various categori object geometr topolog complex structur comprehens experi exhibit power exemplar prior grace solv sever problem shape reconstruct preserv sharp featur recov fine detail,"['Oussama Remil', 'Qian Xie', 'Xingyu Xie', 'Kai Xu', 'Jun Wang']","['cs.CG', 'cs.GR']",False,False,False,False,False,True
281,2017-03-28T14:07:51Z,2017-01-09T15:18:28Z,http://arxiv.org/abs/1701.02208v1,http://arxiv.org/pdf/1701.02208v1,Barcodes of Towers and a Streaming Algorithm for Persistent Homology,barcod tower stream algorithm persist homolog,"A tower is a sequence of simplicial complexes connected by simplicial maps. We show how to compute a filtration, a sequence of nested simplicial complexes, with the same persistent barcode as the tower. Our approach is based on the coning strategy by Dey et al. (SoCG 2014). We show that a variant of this approach yields a filtration that is asymptotically only marginally larger than the tower and can be efficiently computed by a streaming algorithm, both in theory and in practice. Furthermore, we show that our approach can be combined with a streaming algorithm to compute the barcode of the tower via matrix reduction. The space complexity of the algorithm does not depend on the length of the tower, but the maximal size of any subcomplex within the tower.",tower sequenc simplici complex connect simplici map show comput filtrat sequenc nest simplici complex persist barcod tower approach base cone strategi dey et al socg show variant approach yield filtrat asymptot onli margin larger tower effici comput stream algorithm theori practic furthermor show approach combin stream algorithm comput barcod tower via matrix reduct space complex algorithm doe depend length tower maxim size ani subcomplex within tower,"['Michael Kerber', 'Hannah Schreiber']","['math.AT', 'cs.CG']",False,False,False,False,False,True
283,2017-03-28T14:07:51Z,2017-01-05T12:00:37Z,http://arxiv.org/abs/1701.06430v1,http://arxiv.org/pdf/1701.06430v1,An Upper Bound of the Minimal Dispersion via Delta Covers,upper bound minim dispers via delta cover,"For a point set of $n$ elements in the $d$-dimensional unit cube and a class of test sets we are interested in the largest volume of a test set which does not contain any point. For all natural numbers $n$, $d$ and under the assumption of a $delta$-cover with cardinality $\vert \Gamma_\delta \vert$ we prove that there is a point set, such that the largest volume of such a test set without any point is bounded by $\frac{\log \vert \Gamma_\delta \vert}{n} + \delta$. For axis-parallel boxes on the unit cube this leads to a volume of at most $\frac{4d}{n}\log(\frac{9n}{d})$ and on the torus to $\frac{4d}{n}\log (2n)$.",point set element dimension unit cube class test set interest largest volum test set doe contain ani point natur number assumpt delta cover cardin vert gamma delta vert prove point set largest volum test set without ani point bound frac log vert gamma delta vert delta axi parallel box unit cube lead volum frac log frac torus frac log,['Daniel Rudolf'],"['cs.CG', 'math.NA', '52B55, 68Q25']",False,False,True,False,False,True
284,2017-03-28T14:07:51Z,2017-01-04T07:44:19Z,http://arxiv.org/abs/1701.00921v1,http://arxiv.org/pdf/1701.00921v1,Towards Faithful Graph Visualizations,toward faith graph visual,"Readability criteria have been addressed as a measurement of the quality of graph visualizations. In this paper, we argue that readability criteria are necessary but not sufficient. We propose a new kind of criteria, namely faithfulness, to evaluate the quality of graph layouts. We introduce a general model for quantify faithfulness, and contrast it with the well established readability criteria. We show examples of common visualization techniques, such as multidimensional scaling, edge bundling and several other visualization metaphors for the study of faithfulness.",readabl criteria address measur qualiti graph visual paper argu readabl criteria necessari suffici propos new kind criteria name faith evalu qualiti graph layout introduc general model quantifi faith contrast well establish readabl criteria show exampl common visual techniqu multidimension scale edg bundl sever visual metaphor studi faith,"['Quan Hoang Nguyen', 'Peter Eades', 'Seok-Hee Hong']",['cs.CG'],False,False,False,False,False,True
285,2017-03-28T14:07:51Z,2017-01-03T12:41:18Z,http://arxiv.org/abs/1701.00679v1,http://arxiv.org/pdf/1701.00679v1,Removing Depth-Order Cycles Among Triangles: An Efficient Algorithm   Generating Triangular Fragments,remov depth order cycl among triangl effici algorithm generat triangular fragment,"We present an algorithm that cuts any collection of n disjoint triangles in R^3 into O(n^{7/4} polylog n) triangular fragments such that all cycles in the depth-order relation are eliminated. The running time of our algorithm is O(n^{3.69}). We also prove a refined bound, which depends on the number, K, of intersections between the projections of the triangle edges onto the xy-plane. More precisely, we show that O(n^{1+\eps} + n^{1/4} K^{3/4} polylog n) fragments suffice to eliminate all cycles. This result extends to xy-monotone surface patches bounded by a constant number of bounded-degree algebraic arcs in general position.",present algorithm cut ani collect disjoint triangl polylog triangular fragment cycl depth order relat elimin run time algorithm also prove refin bound depend number intersect project triangl edg onto xy plane precis show ep polylog fragment suffic elimin cycl result extend xy monoton surfac patch bound constant number bound degre algebra arc general posit,['Mark de Berg'],"['cs.CG', 'F.2.2']",False,False,False,False,False,True
286,2017-03-28T14:07:51Z,2017-01-02T16:45:18Z,http://arxiv.org/abs/1701.00441v1,http://arxiv.org/pdf/1701.00441v1,"Collecting a Swarm in a Grid Environment Using Shared, Global Inputs",collect swarm grid environ use share global input,"This paper investigates efficient techniques to collect and concentrate an under-actuated particle swarm despite obstacles. Concentrating a swarm of particles is of critical importance in health-care for targeted drug delivery, where micro-scale particles must be steered to a goal location. Individual particles must be small in order to navigate through micro-vasculature, but decreasing size brings new challenges. Individual particles are too small to contain on-board power or computation and are instead controlled by a global input, such as an applied fluidic flow or electric field.   To make progress, this paper considers a swarm of robots initialized in a grid world in which each position is either free-space or obstacle. This paper provides algorithms that collect all the robots to one position and compares these algorithms on the basis of efficiency and implementation time.",paper investig effici techniqu collect concentr actuat particl swarm despit obstacl concentr swarm particl critic import health care target drug deliveri micro scale particl must steer goal locat individu particl must small order navig micro vasculatur decreas size bring new challeng individu particl small contain board power comput instead control global input appli fluidic flow electr field make progress paper consid swarm robot initi grid world posit either free space obstacl paper provid algorithm collect robot one posit compar algorithm basi effici implement time,"['Arun V. Mahadev', 'Dominik Krupke', 'Jan-Marc Reinhardt', 'Sándor P. Fekete', 'Aaron T. Becker']","['cs.RO', 'cs.CG', 'I.2.11; F.2.2']",False,False,False,False,False,True
289,2017-03-28T14:07:51Z,2016-12-31T17:05:53Z,http://arxiv.org/abs/1701.00146v1,http://arxiv.org/pdf/1701.00146v1,Even $1 \times n$ Edge-Matching and Jigsaw Puzzles are Really Hard,even time edg match jigsaw puzzl realli hard,"We prove the computational intractability of rotating and placing $n$ square tiles into a $1 \times n$ array such that adjacent tiles are compatible--either equal edge colors, as in edge-matching puzzles, or matching tab/pocket shapes, as in jigsaw puzzles. Beyond basic NP-hardness, we prove that it is NP-hard even to approximately maximize the number of placed tiles (allowing blanks), while satisfying the compatibility constraint between nonblank tiles, within a factor of 0.9999999851. (On the other hand, there is an easy $1 \over 2$-approximation.) This is the first (correct) proof of inapproximability for edge-matching and jigsaw puzzles. Along the way, we prove NP-hardness of distinguishing, for a directed graph on $n$ nodes, between having a Hamiltonian path (length $n-1$) and having at most $0.999999284 (n-1)$ edges that form a vertex-disjoint union of paths. We use this gap hardness and gap-preserving reductions to establish similar gap hardness for $1 \times n$ jigsaw and edge-matching puzzles.",prove comput intract rotat place squar tile time array adjac tile compat either equal edg color edg match puzzl match tab pocket shape jigsaw puzzl beyond basic np hard prove np hard even approxim maxim number place tile allow blank satisfi compat constraint nonblank tile within factor hand easi approxim first correct proof inapproxim edg match jigsaw puzzl along way prove np hard distinguish direct graph node hamiltonian path length edg form vertex disjoint union path use gap hard gap preserv reduct establish similar gap hard time jigsaw edg match puzzl,"['Jeffrey Bosboom', 'Erik D. Demaine', 'Martin L. Demaine', 'Adam Hesterberg', 'Pasin Manurangsi', 'Anak Yodpinyanee']","['cs.CC', 'cs.CG']",False,False,True,False,False,True
290,2017-03-28T14:07:55Z,2016-12-30T09:33:07Z,http://arxiv.org/abs/1612.09434v1,http://arxiv.org/pdf/1612.09434v1,Data driven estimation of Laplace-Beltrami operator,data driven estim laplac beltrami oper,"Approximations of Laplace-Beltrami operators on manifolds through graph Lapla-cians have become popular tools in data analysis and machine learning. These discretized operators usually depend on bandwidth parameters whose tuning remains a theoretical and practical problem. In this paper, we address this problem for the unnormalized graph Laplacian by establishing an oracle inequality that opens the door to a well-founded data-driven procedure for the bandwidth selection. Our approach relies on recent results by Lacour and Massart [LM15] on the so-called Lepski's method.",approxim laplac beltrami oper manifold graph lapla cian becom popular tool data analysi machin learn discret oper usual depend bandwidth paramet whose tune remain theoret practic problem paper address problem unnorm graph laplacian establish oracl inequ open door well found data driven procedur bandwidth select approach reli recent result lacour massart lm call lepski method,"['Frédéric Chazal', 'Ilaria Giulini', 'Bertrand Michel']","['cs.CG', 'cs.LG', 'math.ST', 'stat.TH']",False,False,True,False,False,True
291,2017-03-28T14:07:55Z,2017-01-02T14:15:21Z,http://arxiv.org/abs/1612.09277v2,http://arxiv.org/pdf/1612.09277v2,On Planar Greedy Drawings of 3-Connected Planar Graphs,planar greedi draw connect planar graph,"A graph drawing is $\textit{greedy}$ if, for every ordered pair of vertices $(x,y)$, there is a path from $x$ to $y$ such that the Euclidean distance to $y$ decreases monotonically at every vertex of the path. Greedy drawings support a simple geometric routing scheme, in which any node that has to send a packet to a destination ""greedily"" forwards the packet to any neighbor that is closer to the destination than itself, according to the Euclidean distance in the drawing. In a greedy drawing such a neighbor always exists and hence this routing scheme is guaranteed to succeed.   In 2004 Papadimitriou and Ratajczak stated two conjectures related to greedy drawings. The $\textit{greedy embedding conjecture}$ states that every $3$-connected planar graph admits a greedy drawing. The $\textit{convex greedy embedding conjecture}$ asserts that every $3$-connected planar graph admits a planar greedy drawing in which the faces are delimited by convex polygons. In 2008 the greedy embedding conjecture was settled in the positive by Leighton and Moitra.   In this paper we prove that every $3$-connected planar graph admits a $\textit{planar}$ greedy drawing. Apart from being a strengthening of Leighton and Moitra's result, this theorem constitutes a natural intermediate step towards a proof of the convex greedy embedding conjecture.",graph draw textit greedi everi order pair vertic path euclidean distanc decreas monoton everi vertex path greedi draw support simpl geometr rout scheme ani node send packet destin greedili forward packet ani neighbor closer destin accord euclidean distanc draw greedi draw neighbor alway exist henc rout scheme guarante succeed papadimitriou ratajczak state two conjectur relat greedi draw textit greedi embed conjectur state everi connect planar graph admit greedi draw textit convex greedi embed conjectur assert everi connect planar graph admit planar greedi draw face delimit convex polygon greedi embed conjectur settl posit leighton moitra paper prove everi connect planar graph admit textit planar greedi draw apart strengthen leighton moitra result theorem constitut natur intermedi step toward proof convex greedi embed conjectur,"['Giordano Da Lozzo', ""Anthony D'Angelo"", 'Fabrizio Frati']",['cs.CG'],False,False,False,False,False,True
292,2017-03-28T14:07:55Z,2016-12-24T09:00:37Z,http://arxiv.org/abs/1612.08153v1,http://arxiv.org/pdf/1612.08153v1,EgoReID: Cross-view Self-Identification and Human Re-identification in   Egocentric and Surveillance Videos,egoreid cross view self identif human identif egocentr surveil video,"Human identification remains to be one of the challenging tasks in computer vision community due to drastic changes in visual features across different viewpoints, lighting conditions, occlusion, etc. Most of the literature has been focused on exploring human re-identification across viewpoints that are not too drastically different in nature. Cameras usually capture oblique or side views of humans, leaving room for a lot of geometric and visual reasoning. Given the recent popularity of egocentric and top-view vision, re-identification across these two drastically different views can now be explored. Having an egocentric and a top view video, our goal is to identify the cameraman in the content of the top-view video, and also re-identify the people visible in the egocentric video, by matching them to the identities present in the top-view video. We propose a CRF-based method to address the two problems. Our experimental results demonstrates the efficiency of the proposed approach over a variety of video recorded from two views.",human identif remain one challeng task comput vision communiti due drastic chang visual featur across differ viewpoint light condit occlus etc literatur focus explor human identif across viewpoint drastic differ natur camera usual captur obliqu side view human leav room lot geometr visual reason given recent popular egocentr top view vision identif across two drastic differ view explor egocentr top view video goal identifi cameraman content top view video also identifi peopl visibl egocentr video match ident present top view video propos crf base method address two problem experiment result demonstr effici propos approach varieti video record two view,"['Shervin Ardeshir', 'Sandesh Sharma', 'Ali Broji']","['cs.CV', 'cs.CG']",False,False,False,False,False,True
293,2017-03-28T14:07:55Z,2016-12-22T00:55:29Z,http://arxiv.org/abs/1612.07405v1,http://arxiv.org/pdf/1612.07405v1,Practical linear-space Approximate Near Neighbors in high dimension,practic linear space approxim near neighbor high dimens,"The $c$-approximate Near Neighbor problem in high dimensional spaces has been mainly addressed by Locality Sensitive Hashing (LSH), which offers polynomial dependence on the dimension, query time sublinear in the size of the dataset, and subquadratic space requirement. For practical applications, linear space is typically imperative. Most previous work in the linear space regime focuses on the case that $c$ exceeds $1$ by a constant term. In a recently accepted paper, optimal bounds have been achieved for any $c>1$ \cite{ALRW17}.   Towards practicality, we present a new and simple data structure using linear space and sublinear query time for any $c>1$ including $c\to 1^+$. Given an LSH family of functions for some metric space, we randomly project points to the Hamming cube of dimension $\log n$, where $n$ is the number of input points. The projected space contains strings which serve as keys for buckets containing the input points. The query algorithm simply projects the query point, then examines points which are assigned to the same or nearby vertices on the Hamming cube. We analyze in detail the query time for some standard LSH families.   To illustrate our claim of practicality, we offer an open-source implementation in {\tt C++}, and report on several experiments in dimension up to 1000 and $n$ up to $10^6$. Our algorithm is one to two orders of magnitude faster than brute force search. Experiments confirm the sublinear dependence on $n$ and the linear dependence on the dimension. We have compared against state-of-the-art LSH-based library {\tt FALCONN}: our search is somewhat slower, but memory usage and preprocessing time are significantly smaller.",approxim near neighbor problem high dimension space main address local sensit hash lsh offer polynomi depend dimens queri time sublinear size dataset subquadrat space requir practic applic linear space typic imper previous work linear space regim focus case exceed constant term recent accept paper optim bound achiev ani cite alrw toward practic present new simpl data structur use linear space sublinear queri time ani includ given lsh famili function metric space random project point ham cube dimens log number input point project space contain string serv key bucket contain input point queri algorithm simpli project queri point examin point assign nearbi vertic ham cube analyz detail queri time standard lsh famili illustr claim practic offer open sourc implement tt report sever experi dimens algorithm one two order magnitud faster brute forc search experi confirm sublinear depend linear depend dimens compar state art lsh base librari tt falconn search somewhat slower memori usag preprocess time signific smaller,"['Georgia Avarikioti', 'Ioannis Z. Emiris', 'Ioannis Psarros', 'Georgios Samaras']",['cs.CG'],False,False,False,False,False,True
294,2017-03-28T14:07:55Z,2016-12-21T18:58:17Z,http://arxiv.org/abs/1612.07276v1,http://arxiv.org/pdf/1612.07276v1,Splitting $B_2$-VPG graphs into outer-string and co-comparability graphs,split vpg graph outer string co compar graph,"In this paper, we show that any $B_2$-VPG graph (i.e., an intersection graph of orthogonal curves with at most 2 bends) can be decomposed into $O(\log n)$ outerstring graphs or $O(\log^3 n)$ permutation graphs. This leads to better approximation algorithms for hereditary graph problems, such as independent set, clique and clique cover, on $B_2$-VPG graphs.",paper show ani vpg graph intersect graph orthogon curv bend decompos log outerstr graph log permut graph lead better approxim algorithm hereditari graph problem independ set cliqu cliqu cover vpg graph,"['Martin Derka', 'Therese Biedl']","['cs.CG', 'cs.DS']",False,False,False,False,False,True
295,2017-03-28T14:07:55Z,2017-02-27T10:00:30Z,http://arxiv.org/abs/1701.00541v2,http://arxiv.org/pdf/1701.00541v2,Packing Unequal Circles into a Square Container by Partitioning Narrow   Action Spaces and Circle Items,pack unequ circl squar contain partit narrow action space circl item,"We address the NP-hard problem of finding a non-overlapping dense packing pattern for n Unequal Circle items in a two-dimensional Square Container (PUC-SC) such that the size of the container is minimized. Based on our previous work on an Action Space based Global Optimization (ASGO) that approximates each circle item as a square item to efficiently find the large unoccupied spaces, we propose an optimization algorithm based on the Partitioned Action Space and Partitioned Circle Items (PAS-PCI). The PAS is to partition the narrow action space on the long side to find two equal action spaces to fully utilize the unoccupied spaces. The PCI is to partition the circle items into four groups based on size for the basin hopping strategy. Experiments on two sets of benchmark instances show the effectiveness of the proposed method. In comparison with our previous algorithm ASGO on the 68 tested instances that ASGO published, PAS-PCI not only gains smaller containers in 64 instances and matches the other 4 but also runs faster in most instances. In comparison with the best record of the Packomania website on a total of 98 instances, PAS-PCI finds smaller containers on 82 and matches the other 16. Note that we updated 19 records for (47-48, 51-54, 57, 61-72) that had been kept unchanged since 2013.",address np hard problem find non overlap dens pack pattern unequ circl item two dimension squar contain puc sc size contain minim base previous work action space base global optim asgo approxim circl item squar item effici find larg unoccupi space propos optim algorithm base partit action space partit circl item pas pci pas partit narrow action space long side find two equal action space fulli util unoccupi space pci partit circl item four group base size basin hop strategi experi two set benchmark instanc show effect propos method comparison previous algorithm asgo test instanc asgo publish pas pci onli gain smaller contain instanc match also run faster instanc comparison best record packomania websit total instanc pas pci find smaller contain match note updat record kept unchang sinc,"['Kun He', 'Mohammed Dosh', 'Shenghao Zou']","['cs.CG', 'cs.DS', '52C26']",False,False,False,False,False,True
296,2017-03-28T14:07:55Z,2017-01-03T14:51:30Z,http://arxiv.org/abs/1612.06954v2,http://arxiv.org/pdf/1612.06954v2,Colored stochastic dominance problems,color stochast domin problem,"In this paper, we study the dominance relation under a stochastic setting. Let $\mathcal{S}$ be a set of $n$ colored stochastic points in $\mathbb{R}^d$, each of which is associated with an existence probability. We investigate the problem of computing the probability that a realization of $\mathcal{S}$ contains inter-color dominances, which we call the \textit{colored stochastic dominance} (CSD) problem. We propose the first algorithm to solve the CSD problem for $d=2$ in $O(n^2 \log^2 n)$ time. On the other hand, we prove that, for $d \geq 3$, even the CSD problem with a restricted color pattern is #P-hard. In addition, even if the existence probabilities are restricted to be $\frac{1}{2}$, the problem remains #P-hard for $d \geq 7$. A simple FPRAS is then provided to approximate the desired probability in any dimension. We also study a variant of the CSD problem in which the dominance relation is considered with respect to not only the standard basis but any orthogonal basis of $\mathbb{R}^d$. Specifically, this variant, which we call the {\em free-basis colored stochastic dominance} (FBCSD) problem, considers the probability that a realization of $\mathcal{S}$ contains inter-color dominances with respect to any orthogonal basis of $\mathbb{R}^d$. We show that the CSD problem is polynomial-time reducible to the FBCSD problem in the same dimension, which proves the #P-hardness of the latter for $d \geq 3$. Conversely, we reduce the FBCSD problem in $\mathbb{R}^2$ to the CSD problem in $\mathbb{R}^2$, by which an $O(n^4 \log^2 n)$ time algorithm for the former is obtained.",paper studi domin relat stochast set let mathcal set color stochast point mathbb associ exist probabl investig problem comput probabl realize mathcal contain inter color domin call textit color stochast domin csd problem propos first algorithm solv csd problem log time hand prove geq even csd problem restrict color pattern hard addit even exist probabl restrict frac problem remain hard geq simpl fpras provid approxim desir probabl ani dimens also studi variant csd problem domin relat consid respect onli standard basi ani orthogon basi mathbb specif variant call em free basi color stochast domin fbcsd problem consid probabl realize mathcal contain inter color domin respect ani orthogon basi mathbb show csd problem polynomi time reduc fbcsd problem dimens prove hard latter geq convers reduc fbcsd problem mathbb csd problem mathbb log time algorithm former obtain,"['Jie Xue', 'Yuan Li']","['cs.CG', 'F.2.2']",False,False,False,False,False,True
297,2017-03-28T14:07:55Z,2017-03-09T14:09:06Z,http://arxiv.org/abs/1612.05101v2,http://arxiv.org/pdf/1612.05101v2,Open problem on risk-aware planning in the plane,open problem risk awar plan plane,"We consider the problem of planning a collision-free path of a robot in the presence of risk zones. The robot is allowed to travel in these zones but is penalized in a super-linear fashion for consecutive accumulative time spent there. We recently suggested a natural cost function that balances path length and risk-exposure time. When no risk zones exists, our problem resorts to computing minimal-length paths which is known to be computationally hard in the number of dimensions. It is well known that in two-dimensions computing minimal-length paths can be done efficiently. Thus, a natural question we pose is ""Is our problem computationally hard or not?"" If the problem is hard, we wish to find an approximation algorithm to compute a near-optimal path. If not, then a polynomial-time algorithm should be found.",consid problem plan collis free path robot presenc risk zone robot allow travel zone penal super linear fashion consecut accumul time spent recent suggest natur cost function balanc path length risk exposur time risk zone exist problem resort comput minim length path known comput hard number dimens well known two dimens comput minim length path done effici thus natur question pose problem comput hard problem hard wish find approxim algorithm comput near optim path polynomi time algorithm found,"['Oren Salzman', 'Siddhartha Srinivasa']",['cs.CG'],False,False,False,False,False,True
298,2017-03-28T14:07:55Z,2016-12-15T00:17:43Z,http://arxiv.org/abs/1612.04890v1,http://arxiv.org/pdf/1612.04890v1,Stochastic closest-pair problem and most-likely nearest-neighbor search   in tree spaces,stochast closest pair problem like nearest neighbor search tree space,"Let $T$ be a tree space (or tree network) represented by a weighted tree with $t$ vertices, and $S$ be a set of $n$ stochastic points in $T$, each of which has a fixed location with an independent existence probability. We investigate two fundamental problems under such a stochastic setting, the closest-pair problem and the nearest-neighbor search. For the former, we study the computation of the $\ell$-threshold probability and the expectation of the closest-pair distance of a realization of $S$. We propose the first algorithm to compute the $\ell$-threshold probability in $O(t+n\log n+ \min\{tn,n^2\})$ time for any given threshold $\ell$, which immediately results in an $O(t+\min\{tn^3,n^4\})$-time algorithm for computing the expected closest-pair distance. Based on this, we further show that one can compute a $(1+\varepsilon)$-approximation for the expected closest-pair distance in $O(t+\varepsilon^{-1}\min\{tn^2,n^3\})$ time, by arguing that the expected closest-pair distance can be approximated via $O(\varepsilon^{-1}n)$ threshold probability queries. For the latter, we study the $k$ most-likely nearest-neighbor search ($k$-LNN) via a notion called $k$ most-likely Voronoi Diagram ($k$-LVD). We show that the size of the $k$-LVD $\varPsi_T^S$ of $S$ on $T$ is bounded by $O(kn)$ if the existence probabilities of the points in $S$ are constant-far from 0. Furthermore, we establish an $O(kn)$ average-case upper bound for the size of $\varPsi_T^S$, by regarding the existence probabilities as i.i.d. random variables drawn from some fixed distribution. Our results imply the existence of an LVD data structure which answers $k$-LNN queries in $O(\log n+k)$ time using average-case $O(t+k^2n)$ space, and worst-case $O(t+kn^2)$ space if the existence probabilities are constant-far from 0. Finally, we also give an $O(t+ n^2\log n+n^2k)$-time algorithm to construct the LVD data structure.",let tree space tree network repres weight tree vertic set stochast point fix locat independ exist probabl investig two fundament problem stochast set closest pair problem nearest neighbor search former studi comput ell threshold probabl expect closest pair distanc realize propos first algorithm comput ell threshold probabl log min tn time ani given threshold ell immedi result min tn time algorithm comput expect closest pair distanc base show one comput varepsilon approxim expect closest pair distanc varepsilon min tn time argu expect closest pair distanc approxim via varepsilon threshold probabl queri latter studi like nearest neighbor search lnn via notion call like voronoi diagram lvd show size lvd varpsi bound kn exist probabl point constant far furthermor establish kn averag case upper bound size varpsi regard exist probabl random variabl drawn fix distribut result impli exist lvd data structur answer lnn queri log time use averag case space worst case kn space exist probabl constant far final also give log time algorithm construct lvd data structur,"['Jie Xue', 'Yuan Li']","['cs.CG', 'F.2.2']",False,False,False,False,False,True
299,2017-03-28T14:07:55Z,2016-12-14T19:33:00Z,http://arxiv.org/abs/1612.04780v1,http://arxiv.org/pdf/1612.04780v1,Minimum Weight Connectivity Augmentation for Planar Straight-Line Graphs,minimum weight connect augment planar straight line graph,"We consider edge insertion and deletion operations that increase the connectivity of a given planar straight-line graph (PSLG), while minimizing the total edge length of the output. We show that every connected PSLG $G=(V,E)$ in general position can be augmented to a 2-connected PSLG $(V,E\cup E^+)$ by adding new edges of total Euclidean length $\ E^+\ \leq 2\ E\ $, and this bound is the best possible. An optimal edge set $E^+$ can be computed in $O( V ^4)$ time; however the problem becomes NP-hard when $G$ is disconnected. Further, there is a sequence of edge insertions and deletions that transforms a connected PSLG $G=(V,E)$ into a planar straight-line cycle $G'=(V,E')$ such that $\ E'\ \leq 2\ {\rm MST}(V)\ $, and the graph remains connected with edge length below $\ E\ +\ {\rm MST}(V)\ $ at all stages. These bounds are the best possible.",consid edg insert delet oper increas connect given planar straight line graph pslg minim total edg length output show everi connect pslg general posit augment connect pslg cup ad new edg total euclidean length leq bound best possibl optim edg set comput time howev problem becom np hard disconnect sequenc edg insert delet transform connect pslg planar straight line cycl leq rm mst graph remain connect edg length rm mst stage bound best possibl,"['Hugo A. Akitaya', 'Rajasekhar Inkulu', 'Torrie L. Nichols', 'Diane L. Souvaine', 'Csaba D. Tóth', 'Charles R. Winston']","['cs.CG', '05C40, 05C85, 68R10', 'I.3.5']",False,False,True,False,False,True
301,2017-03-28T14:05:58Z,2017-03-27T13:11:33Z,http://arxiv.org/abs/1703.09046v1,http://arxiv.org/pdf/1703.09046v1,Bootstrapping a Lexicon for Emotional Arousal in Software Engineering,bootstrap lexicon emot arous softwar engin,"Emotional arousal increases activation and performance but may also lead to burnout in software development. We present the first version of a Software Engineering Arousal lexicon (SEA) that is specifically designed to address the problem of emotional arousal in the software developer ecosystem. SEA is built using a bootstrapping approach that combines word embedding model trained on issue-tracking data and manual scoring of items in the lexicon. We show that our lexicon is able to differentiate between issue priorities, which are a source of emotional activation and then act as a proxy for arousal. The best performance is obtained by combining SEA (428 words) with a previously created general purpose lexicon by Warriner et al. (13,915 words) and it achieves Cohen's d effect sizes up to 0.5.",emot arous increas activ perform may also lead burnout softwar develop present first version softwar engin arous lexicon sea specif design address problem emot arous softwar develop ecosystem sea built use bootstrap approach combin word embed model train issu track data manual score item lexicon show lexicon abl differenti issu prioriti sourc emot activ act proxi arous best perform obtain combin sea word previous creat general purpos lexicon warrin et al word achiev cohen effect size,"['Mika V. Mäntylä', 'Nicole Novielli', 'Filippo Lanubile', 'Maëlick Claes', 'Miikka Kuutila']","['cs.SE', 'cs.CL']",False,False,False,False,False,True
303,2017-03-28T14:05:58Z,2017-03-26T23:48:06Z,http://arxiv.org/abs/1703.08885v1,http://arxiv.org/pdf/1703.08885v1,Question Answering from Unstructured Text by Retrieval and Comprehension,question answer unstructur text retriev comprehens,"Open domain Question Answering (QA) systems must interact with external knowledge sources, such as web pages, to find relevant information. Information sources like Wikipedia, however, are not well structured and difficult to utilize in comparison with Knowledge Bases (KBs). In this work we present a two-step approach to question answering from unstructured text, consisting of a retrieval step and a comprehension step. For comprehension, we present an RNN based attention model with a novel mixture mechanism for selecting answers from either retrieved articles or a fixed vocabulary. For retrieval we introduce a hand-crafted model and a neural model for ranking relevant articles. We achieve state-of-the-art performance on W IKI M OVIES dataset, reducing the error by 40%. Our experimental results further demonstrate the importance of each of the introduced components.",open domain question answer qa system must interact extern knowledg sourc web page find relev inform inform sourc like wikipedia howev well structur difficult util comparison knowledg base kbs work present two step approach question answer unstructur text consist retriev step comprehens step comprehens present rnn base attent model novel mixtur mechan select answer either retriev articl fix vocabulari retriev introduc hand craft model neural model rank relev articl achiev state art perform iki ovi dataset reduc error experiment result demonstr import introduc compon,"['Yusuke Watanabe', 'Bhuwan Dhingra', 'Ruslan Salakhutdinov']",['cs.CL'],False,False,False,False,False,True
305,2017-03-28T14:05:58Z,2017-03-26T00:30:38Z,http://arxiv.org/abs/1703.08748v1,http://arxiv.org/pdf/1703.08748v1,LEPOR: An Augmented Machine Translation Evaluation Metric,lepor augment machin translat evalu metric,"Machine translation (MT) was developed as one of the hottest research topics in the natural language processing (NLP) literature. One important issue in MT is that how to evaluate the MT system reasonably and tell us whether the translation system makes an improvement or not. The traditional manual judgment methods are expensive, time-consuming, unrepeatable, and sometimes with low agreement. On the other hand, the popular automatic MT evaluation methods have some weaknesses. Firstly, they tend to perform well on the language pairs with English as the target language, but weak when English is used as source. Secondly, some methods rely on many additional linguistic features to achieve good performance, which makes the metric unable to replicate and apply to other language pairs easily. Thirdly, some popular metrics utilize incomprehensive factors, which result in low performance on some practical tasks. In this thesis, to address the existing problems, we design novel MT evaluation methods and investigate their performances on different languages. Firstly, we design augmented factors to yield highly accurate evaluation.Secondly, we design a tunable evaluation model where weighting of factors can be optimised according to the characteristics of languages. Thirdly, in the enhanced version of our methods, we design concise linguistic feature using POS to show that our methods can yield even higher performance when using some external linguistic resources. Finally, we introduce the practical performance of our metrics in the ACL-WMT workshop shared tasks, which show that the proposed methods are robust across different languages.",machin translat mt develop one hottest research topic natur languag process nlp literatur one import issu mt evalu mt system reason tell us whether translat system make improv tradit manual judgment method expens time consum unrepeat sometim low agreement hand popular automat mt evalu method weak first tend perform well languag pair english target languag weak english use sourc second method reli mani addit linguist featur achiev good perform make metric unabl replic appli languag pair easili third popular metric util incomprehens factor result low perform practic task thesi address exist problem design novel mt evalu method investig perform differ languag first design augment factor yield high accur evalu second design tunabl evalu model weight factor optimis accord characterist languag third enhanc version method design concis linguist featur use pos show method yield even higher perform use extern linguist resourc final introduc practic perform metric acl wmt workshop share task show propos method robust across differ languag,['Lifeng Han'],['cs.CL'],False,False,False,False,False,True
307,2017-03-28T14:05:58Z,2017-03-25T14:56:27Z,http://arxiv.org/abs/1703.08701v1,http://arxiv.org/pdf/1703.08701v1,Morphological Analysis for the Maltese Language: The Challenges of a   Hybrid System,morpholog analysi maltes languag challeng hybrid system,"Maltese is a morphologically rich language with a hybrid morphological system which features both concatenative and non-concatenative processes. This paper analyses the impact of this hybridity on the performance of machine learning techniques for morphological labelling and clustering. In particular, we analyse a dataset of morphologically related word clusters to evaluate the difference in results for concatenative and nonconcatenative clusters. We also describe research carried out in morphological labelling, with a particular focus on the verb category. Two evaluations were carried out, one using an unseen dataset, and another one using a gold standard dataset which was manually labelled. The gold standard dataset was split into concatenative and non-concatenative to analyse the difference in results between the two morphological systems.",maltes morpholog rich languag hybrid morpholog system featur concaten non concaten process paper analys impact hybrid perform machin learn techniqu morpholog label cluster particular analys dataset morpholog relat word cluster evalu differ result concaten nonconcaten cluster also describ research carri morpholog label particular focus verb categori two evalu carri one use unseen dataset anoth one use gold standard dataset manual label gold standard dataset split concaten non concaten analys differ result two morpholog system,"['Claudia Borg', 'Albert Gatt']","['cs.CL', 'I.2.7']",False,False,False,False,False,True
311,2017-03-28T14:06:02Z,2017-03-24T17:13:08Z,http://arxiv.org/abs/1703.08513v1,http://arxiv.org/pdf/1703.08513v1,Interactive Natural Language Acquisition in a Multi-modal Recurrent   Neural Architecture,interact natur languag acquisit multi modal recurr neural architectur,"The human brain is one of the most complex dynamic systems that enables us to communicate in natural language. We have a good understanding of some principles underlying natural languages and language processing, some knowledge about socio-cultural conditions framing acquisition, and some insights about where activity is occurring in the brain. However, we were not yet able to understand the behavioural and mechanistic characteristics for natural language and how mechanisms in the brain allow to acquire and process language.   In an effort to bridge the gap between insights from behavioural psychology and neuroscience, the goal of this paper is to contribute a computational understanding of the appropriate characteristics that favour language acquisition, in a brain-inspired neural architecture. Accordingly, we provide concepts and refinements in cognitive modelling regarding principles and mechanisms in the brain - such as the hierarchical abstraction of context - in a plausible recurrent architecture. On this basis, we propose neurocognitively plausible model for embodied language acquisition from real world interaction of a humanoid robot with its environment. The model is capable of learning language production grounded in both, temporal dynamic somatosensation and vision. In particular, the architecture consists of a continuous time recurrent neural network, where parts have different leakage characteristics and thus operate on multiple timescales for every modality and the association of the higher level nodes of all modalities into cell assemblies. Thus, this model features hierarchical concept abstraction in sensation as well as concept decomposition in production, multi-modal integration, and self-organisation of latent representations.",human brain one complex dynam system enabl us communic natur languag good understand principl natur languag languag process knowledg socio cultur condit frame acquisit insight activ occur brain howev yet abl understand behaviour mechanist characterist natur languag mechan brain allow acquir process languag effort bridg gap insight behaviour psycholog neurosci goal paper contribut comput understand appropri characterist favour languag acquisit brain inspir neural architectur accord provid concept refin cognit model regard principl mechan brain hierarch abstract context plausibl recurr architectur basi propos neurocognit plausibl model embodi languag acquisit real world interact humanoid robot environ model capabl learn languag product ground tempor dynam somatosens vision particular architectur consist continu time recurr neural network part differ leakag characterist thus oper multipl timescal everi modal associ higher level node modal cell assembl thus model featur hierarch concept abstract sensat well concept decomposit product multi modal integr self organis latent represent,"['Stefan Heinrich', 'Stefan Wermter']","['cs.CL', 'q-bio.NC']",False,False,False,False,False,True
327,2017-03-28T14:06:07Z,2017-03-22T18:20:07Z,http://arxiv.org/abs/1703.07805v1,http://arxiv.org/abs/1703.07805v1,Supervised Typing of Big Graphs using Semantic Embeddings,supervis type big graph use semant embed,"We propose a supervised algorithm for generating type embeddings in the same semantic vector space as a given set of entity embeddings. The algorithm is agnostic to the derivation of the underlying entity embeddings. It does not require any manual feature engineering, generalizes well to hundreds of types and achieves near-linear scaling on Big Graphs containing many millions of triples and instances by virtue of an incremental execution. We demonstrate the utility of the embeddings on a type recommendation task, outperforming a non-parametric feature-agnostic baseline while achieving 15x speedup and near-constant memory usage on a full partition of DBpedia. Using state-of-the-art visualization, we illustrate the agreement of our extensionally derived DBpedia type embeddings with the manually curated domain ontology. Finally, we use the embeddings to probabilistically cluster about 4 million DBpedia instances into 415 types in the DBpedia ontology.",propos supervis algorithm generat type embed semant vector space given set entiti embed algorithm agnost deriv entiti embed doe requir ani manual featur engin general well hundr type achiev near linear scale big graph contain mani million tripl instanc virtu increment execut demonstr util embed type recommend task outperform non parametr featur agnost baselin achiev speedup near constant memori usag full partit dbpedia use state art visual illustr agreement extension deriv dbpedia type embed manual curat domain ontolog final use embed probabilist cluster million dbpedia instanc type dbpedia ontolog,"['Mayank Kejriwal', 'Pedro Szekely']","['cs.CL', 'cs.AI']",False,False,False,False,False,True
332,2017-03-28T14:06:12Z,2017-03-21T21:36:28Z,http://arxiv.org/abs/1703.07438v1,http://arxiv.org/pdf/1703.07438v1,The NLTK FrameNet API: Designing for Discoverability with a Rich   Linguistic Resource,nltk framenet api design discover rich linguist resourc,"A new Python API, integrated within the NLTK suite, offers access to the FrameNet 1.7 lexical database. The lexicon (structured in terms of frames) as well as annotated sentences can be processed programatically, or browsed with human-readable displays via the interactive Python prompt.",new python api integr within nltk suit offer access framenet lexic databas lexicon structur term frame well annot sentenc process programat brows human readabl display via interact python prompt,"['Nathan Schneider', 'Chuck Wooters']",['cs.CL'],False,False,True,False,False,True
334,2017-03-28T14:06:12Z,2017-03-21T04:56:14Z,http://arxiv.org/abs/1703.07055v1,http://arxiv.org/pdf/1703.07055v1,Investigation of Language Understanding Impact for Reinforcement   Learning Based Dialogue Systems,investig languag understand impact reinforc learn base dialogu system,"Language understanding is a key component in a spoken dialogue system. In this paper, we investigate how the language understanding module influences the dialogue system performance by conducting a series of systematic experiments on a task-oriented neural dialogue system in a reinforcement learning based setting. The empirical study shows that among different types of language understanding errors, slot-level errors can have more impact on the overall performance of a dialogue system compared to intent-level errors. In addition, our experiments demonstrate that the reinforcement learning based dialogue system is able to learn when and what to confirm in order to achieve better performance and greater robustness.",languag understand key compon spoken dialogu system paper investig languag understand modul influenc dialogu system perform conduct seri systemat experi task orient neural dialogu system reinforc learn base set empir studi show among differ type languag understand error slot level error impact overal perform dialogu system compar intent level error addit experi demonstr reinforc learn base dialogu system abl learn confirm order achiev better perform greater robust,"['Xiujun Li', 'Yun-Nung Chen', 'Lihong Li', 'Jianfeng Gao', 'Asli Celikyilmaz']","['cs.CL', 'cs.AI', 'cs.LG']",False,False,False,False,False,True
338,2017-03-28T14:06:12Z,2017-03-21T17:41:23Z,http://arxiv.org/abs/1703.06585v2,http://arxiv.org/pdf/1703.06585v2,Learning Cooperative Visual Dialog Agents with Deep Reinforcement   Learning,learn cooper visual dialog agent deep reinforc learn,"We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative 'image guessing' game between two agents -- Qbot and Abot -- who communicate in natural language dialog so that Qbot can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end -- from pixels to multi-agent multi-round dialog to game reward.   We demonstrate two experimental results.   First, as a 'sanity check' demonstration of pure RL (from scratch), we show results on a synthetic world, where the agents communicate in ungrounded vocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes (shape/color/style). Thus, we demonstrate the emergence of grounded language and communication among 'visual' dialog agents with no human supervision.   Second, we conduct large-scale real-image experiments on the VisDial dataset, where we pretrain with supervised dialog data and show that the RL 'fine-tuned' agents significantly outperform SL agents. Interestingly, the RL Qbot learns to ask questions that Abot is good at, ultimately resulting in more informative dialog and a better team.",introduc first goal driven train visual question answer dialog agent specif pose cooper imag guess game two agent qbot abot communic natur languag dialog qbot select unseen imag lineup imag use deep reinforc learn rl learn polici agent end end pixel multi agent multi round dialog game reward demonstr two experiment result first saniti check demonstr pure rl scratch show result synthet world agent communic unground vocabulari symbol pre specifi mean find two bot invent communic protocol start use certain symbol ask answer certain visual attribut shape color style thus demonstr emerg ground languag communic among visual dialog agent human supervis second conduct larg scale real imag experi visdial dataset pretrain supervis dialog data show rl fine tune agent signific outperform sl agent interest rl qbot learn ask question abot good ultim result inform dialog better team,"['Abhishek Das', 'Satwik Kottur', 'José M. F. Moura', 'Stefan Lee', 'Dhruv Batra']","['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG']",False,False,False,False,False,True
339,2017-03-28T14:06:12Z,2017-03-19T23:42:28Z,http://arxiv.org/abs/1703.06541v1,http://arxiv.org/pdf/1703.06541v1,Native Language Identification using Stacked Generalization,nativ languag identif use stack general,"Ensemble methods using multiple classifiers have proven to be the most successful approach for the task of Native Language Identification (NLI), achieving the current state of the art. However, a systematic examination of ensemble methods for NLI has yet to be conducted. Additionally, deeper ensemble architectures such as classifier stacking have not been closely evaluated. We present a set of experiments using three ensemble-based models, testing each with multiple configurations and algorithms. This includes a rigorous application of meta-classification models for NLI, achieving state-of-the-art results on three datasets from different languages. We also present the first use of statistical significance testing for comparing NLI systems, showing that our results are significantly better than the previous state of the art. We make available a collection of test set predictions to facilitate future statistical tests.",ensembl method use multipl classifi proven success approach task nativ languag identif nli achiev current state art howev systemat examin ensembl method nli yet conduct addit deeper ensembl architectur classifi stack close evalu present set experi use three ensembl base model test multipl configur algorithm includ rigor applic meta classif model nli achiev state art result three dataset differ languag also present first use statist signific test compar nli system show result signific better previous state art make avail collect test set predict facilit futur statist test,"['Shervin Malmasi', 'Mark Dras']",['cs.CL'],False,False,False,False,False,True
347,2017-03-28T14:06:16Z,2017-03-17T00:02:42Z,http://arxiv.org/abs/1703.05851v1,http://arxiv.org/pdf/1703.05851v1,Temporal Information Extraction for Question Answering Using Syntactic   Dependencies in an LSTM-based Architecture,tempor inform extract question answer use syntact depend lstm base architectur,"In this paper, we propose to use a set of simple, uniform in architecture LSTM-based models to recover different kinds of temporal relations from text. Using the shortest dependency path between entities as input, the same architecture is used to extract intra-sentence, cross-sentence, and document creation time relations. A ""double-checking"" technique reverses entity pairs in classification, boosting the recall of positive cases and reducing misclassifications between opposite classes. An efficient pruning algorithm resolves conflicts globally. Evaluated on QA-TempEval (SemEval2015 Task 5), our proposed technique outperforms state-of-the-art methods by a large margin.",paper propos use set simpl uniform architectur lstm base model recov differ kind tempor relat text use shortest depend path entiti input architectur use extract intra sentenc cross sentenc document creation time relat doubl check techniqu revers entiti pair classif boost recal posit case reduc misclassif opposit class effici prune algorithm resolv conflict global evalu qa tempev semev task propos techniqu outperform state art method larg margin,"['Yuanliang Meng', 'Anna Rumshisky', 'Alexey Romanov']","['cs.IR', 'cs.CL']",False,False,False,False,False,True
351,2017-03-28T14:06:20Z,2017-03-15T23:34:20Z,http://arxiv.org/abs/1703.05423v1,http://arxiv.org/pdf/1703.05423v1,End-to-end optimization of goal-driven and visually grounded dialogue   systems,end end optim goal driven visual ground dialogu system,"End-to-end design of dialogue systems has recently become a popular research topic thanks to powerful tools such as encoder-decoder architectures for sequence-to-sequence learning. Yet, most current approaches cast human-machine dialogue management as a supervised learning problem, aiming at predicting the next utterance of a participant given the full history of the dialogue. This vision is too simplistic to render the intrinsic planning problem inherent to dialogue as well as its grounded nature, making the context of a dialogue larger than the sole history. This is why only chit-chat and question answering tasks have been addressed so far using end-to-end architectures. In this paper, we introduce a Deep Reinforcement Learning method to optimize visually grounded task-oriented dialogues, based on the policy gradient algorithm. This approach is tested on a dataset of 120k dialogues collected through Mechanical Turk and provides encouraging results at solving both the problem of generating natural dialogues and the task of discovering a specific object in a complex picture.",end end design dialogu system recent becom popular research topic thank power tool encod decod architectur sequenc sequenc learn yet current approach cast human machin dialogu manag supervis learn problem aim predict next utter particip given full histori dialogu vision simplist render intrins plan problem inher dialogu well ground natur make context dialogu larger sole histori whi onli chit chat question answer task address far use end end architectur paper introduc deep reinforc learn method optim visual ground task orient dialogu base polici gradient algorithm approach test dataset dialogu collect mechan turk provid encourag result solv problem generat natur dialogu task discov specif object complex pictur,"['Florian Strub', 'Harm de Vries', 'Jeremie Mary', 'Bilal Piot', 'Aaron Courville', 'Olivier Pietquin']",['cs.CL'],False,False,False,False,False,True
360,2017-03-28T14:06:24Z,2017-03-15T01:54:52Z,http://arxiv.org/abs/1703.04879v1,http://arxiv.org/pdf/1703.04879v1,Sparse Named Entity Classification using Factorization Machines,spars name entiti classif use factor machin,"Named entity classification is the task of classifying text-based elements into various categories, including places, names, dates, times, and monetary values. A bottleneck in named entity classification, however, is the data problem of sparseness, because new named entities continually emerge, making it rather difficult to maintain a dictionary for named entity classification. Thus, in this paper, we address the problem of named entity classification using matrix factorization to overcome the problem of feature sparsity. Experimental results show that our proposed model, with fewer features and a smaller size, achieves competitive accuracy to state-of-the-art models.",name entiti classif task classifi text base element various categori includ place name date time monetari valu bottleneck name entiti classif howev data problem spars becaus new name entiti continu emerg make rather difficult maintain dictionari name entiti classif thus paper address problem name entiti classif use matrix factor overcom problem featur sparsiti experiment result show propos model fewer featur smaller size achiev competit accuraci state art model,"['Ai Hirata', 'Mamoru Komachi']",['cs.CL'],False,False,False,False,False,True
365,2017-03-28T14:06:24Z,2017-03-14T19:14:32Z,http://arxiv.org/abs/1703.04677v1,http://arxiv.org/pdf/1703.04677v1,A computational investigation of sources of variability in sentence   comprehension difficulty in aphasia,comput investig sourc variabl sentenc comprehens difficulti aphasia,"We present a computational evaluation of three hypotheses about sources of deficit in sentence comprehension in aphasia: slowed processing, intermittent deficiency, and resource reduction. The ACT-R based Lewis & Vasishth 2005 model is used to implement these three proposals. Slowed processing is implemented as slowed default production-rule firing time; intermittent deficiency as increased random noise in activation of chunks in memory; and resource reduction as reduced goal activation. As data, we considered subject vs. object relatives presented in a self-paced listening modality to 56 individuals with aphasia (IWA) and 46 matched controls. The participants heard the sentences and carried out a picture verification task to decide on an interpretation of the sentence. These response accuracies are used to identify the best parameters (for each participant) that correspond to the three hypotheses mentioned above. We show that controls have more tightly clustered (less variable) parameter values than IWA; specifically, compared to controls, among IWA there are more individuals with low goal activations, high noise, and slow default action times. This suggests that (i) individual patients show differential amounts of deficit along the three dimensions of slowed processing, intermittent deficient, and resource reduction, (ii) overall, there is evidence for all three sources of deficit playing a role, and (iii) IWA have a more variable range of parameter values than controls. In sum, this study contributes a proof of concept of a quantitative implementation of, and evidence for, these three accounts of comprehension deficits in aphasia.",present comput evalu three hypothes sourc deficit sentenc comprehens aphasia slow process intermitt defici resourc reduct act base lewi vasishth model use implement three propos slow process implement slow default product rule fire time intermitt defici increas random nois activ chunk memori resourc reduct reduc goal activ data consid subject vs object relat present self pace listen modal individu aphasia iwa match control particip heard sentenc carri pictur verif task decid interpret sentenc respons accuraci use identifi best paramet particip correspond three hypothes mention abov show control tight cluster less variabl paramet valu iwa specif compar control among iwa individu low goal activ high nois slow default action time suggest individu patient show differenti amount deficit along three dimens slow process intermitt defici resourc reduct ii overal evid three sourc deficit play role iii iwa variabl rang paramet valu control sum studi contribut proof concept quantit implement evid three account comprehens deficit aphasia,"['Paul Mätzig', 'Shravan Vasishth', 'Felix Engelmann', 'David Caplan']","['cs.CL', 'cs.AI']",False,False,True,False,False,True
369,2017-03-28T14:06:24Z,2017-03-13T17:13:51Z,http://arxiv.org/abs/1703.04489v1,http://arxiv.org/pdf/1703.04489v1,Reinforcement Learning for Transition-Based Mention Detection,reinforc learn transit base mention detect,"This paper describes an application of reinforcement learning to the mention detection task. We define a novel action-based formulation for the mention detection task, in which a model can flexibly revise past labeling decisions by grouping together tokens and assigning partial mention labels. We devise a method to create mention-level episodes and we train a model by rewarding correctly labeled complete mentions, irrespective of the inner structure created. The model yields results which are on par with a competitive supervised counterpart while being more flexible in terms of achieving targeted behavior through reward modeling and generating internal mention structure, especially on longer mentions.",paper describ applic reinforc learn mention detect task defin novel action base formul mention detect task model flexibl revis past label decis group togeth token assign partial mention label devis method creat mention level episod train model reward correct label complet mention irrespect inner structur creat model yield result par competit supervis counterpart flexibl term achiev target behavior reward model generat intern mention structur especi longer mention,"['Georgiana Dinu', 'Wael Hamza', 'Radu Florian']","['cs.CL', 'cs.AI']",False,False,False,False,False,True
370,2017-03-28T14:06:28Z,2017-03-13T16:50:36Z,http://arxiv.org/abs/1703.04481v1,http://arxiv.org/pdf/1703.04481v1,Geometrical morphology,geometr morpholog,"We explore inflectional morphology as an example of the relationship of the discrete and the continuous in linguistics. The grammar requests a form of a lexeme by specifying a set of feature values, which corresponds to a corner M of a hypercube in feature value space. The morphology responds to that request by providing a morpheme, or a set of morphemes, whose vector sum is geometrically closest to the corner M. In short, the chosen morpheme $\mu$ is the morpheme (or set of morphemes) that maximizes the inner product of $\mu$ and M.",explor inflect morpholog exampl relationship discret continu linguist grammar request form lexem specifi set featur valu correspond corner hypercub featur valu space morpholog respond request provid morphem set morphem whose vector sum geometr closest corner short chosen morphem mu morphem set morphem maxim inner product mu,"['John Goldsmith', 'Eric Rosen']",['cs.CL'],False,False,False,False,False,True
371,2017-03-28T14:06:28Z,2017-03-13T16:36:38Z,http://arxiv.org/abs/1703.04474v1,http://arxiv.org/pdf/1703.04474v1,DRAGNN: A Transition-based Framework for Dynamically Connected Neural   Networks,dragnn transit base framework dynam connect neural network,"In this work, we present a compact, modular framework for constructing novel recurrent neural architectures. Our basic module is a new generic unit, the Transition Based Recurrent Unit (TBRU). In addition to hidden layer activations, TBRUs have discrete state dynamics that allow network connections to be built dynamically as a function of intermediate activations. By connecting multiple TBRUs, we can extend and combine commonly used architectures such as sequence-to-sequence, attention mechanisms, and re-cursive tree-structured models. A TBRU can also serve as both an encoder for downstream tasks and as a decoder for its own task simultaneously, resulting in more accurate multi-task learning. We call our approach Dynamic Recurrent Acyclic Graphical Neural Networks, or DRAGNN. We show that DRAGNN is significantly more accurate and efficient than seq2seq with attention for syntactic dependency parsing and yields more accurate multi-task learning for extractive summarization tasks.",work present compact modular framework construct novel recurr neural architectur basic modul new generic unit transit base recurr unit tbru addit hidden layer activ tbrus discret state dynam allow network connect built dynam function intermedi activ connect multipl tbrus extend combin common use architectur sequenc sequenc attent mechan cursiv tree structur model tbru also serv encod downstream task decod task simultan result accur multi task learn call approach dynam recurr acycl graphic neural network dragnn show dragnn signific accur effici seqseq attent syntact depend pars yield accur multi task learn extract summar task,"['Lingpeng Kong', 'Chris Alberti', 'Daniel Andor', 'Ivan Bogatyy', 'David Weiss']",['cs.CL'],False,False,False,False,False,True
372,2017-03-28T14:06:28Z,2017-03-13T14:34:23Z,http://arxiv.org/abs/1703.04417v1,http://arxiv.org/pdf/1703.04417v1,El Lenguaje Natural como Lenguaje Formal,el lenguaj natur como lenguaj formal,"Formal languages theory is useful for the study of natural language. In particular, it is of interest to study the adequacy of the grammatical formalisms to express syntactic phenomena present in natural language. First, it helps to draw hypothesis about the nature and complexity of the speaker-hearer linguistic competence, a fundamental question in linguistics and other cognitive sciences. Moreover, from an engineering point of view, it allows the knowledge of practical limitations of applications based on those formalisms. In this article I introduce the adequacy problem of grammatical formalisms for natural language, also introducing some formal language theory concepts required for this discussion. Then, I review the formalisms that have been proposed in history, and the arguments that have been given to support or reject their adequacy.   -----   La teor\'ia de lenguajes formales es \'util para el estudio de los lenguajes naturales. En particular, resulta de inter\'es estudiar la adecuaci\'on de los formalismos gramaticales para expresar los fen\'omenos sint\'acticos presentes en el lenguaje natural. Primero, ayuda a trazar hip\'otesis acerca de la naturaleza y complejidad de las competencias ling\""u\'isticas de los hablantes-oyentes del lenguaje, un interrogante fundamental de la ling\""u\'istica y otras ciencias cognitivas. Adem\'as, desde el punto de vista de la ingenier\'ia, permite conocer limitaciones pr\'acticas de las aplicaciones basadas en dichos formalismos. En este art\'iculo hago una introducci\'on al problema de la adecuaci\'on de los formalismos gramaticales para el lenguaje natural, introduciendo tambi\'en algunos conceptos de la teor\'ia de lenguajes formales necesarios para esta discusi\'on. Luego, hago un repaso de los formalismos que han sido propuestos a lo largo de la historia, y de los argumentos que se han dado para sostener o refutar su adecuaci\'on.",formal languag theori use studi natur languag particular interest studi adequaci grammat formal express syntact phenomena present natur languag first help draw hypothesi natur complex speaker hearer linguist compet fundament question linguist cognit scienc moreov engin point view allow knowledg practic limit applic base formal articl introduc adequaci problem grammat formal natur languag also introduc formal languag theori concept requir discuss review formal propos histori argument given support reject adequaci la teor ia de lenguaj formal es util para el estudio de los lenguaj natural en particular resulta de inter es estudiar la adecuaci de los formalismo gramatical para expresar los fen omeno sint actico present en el lenguaj natur primero ayuda trazar hip otesi acerca de la naturaleza complejidad de las competencia ling istica de los hablant oyent del lenguaj un interrogant fundament de la ling istica otra ciencia cognitiva adem desd el punto de vista de la ingeni ia permit conoc limitacion pr actica de las aplicacion basada en dicho formalismo en est art iculo hago una introducci al problema de la adecuaci de los formalismo gramatical para el lenguaj natur introduciendo tambi en alguno concepto de la teor ia de lenguaj formal necesario para esta discusi luego hago un repaso de los formalismo que han sido propuesto lo largo de la historia de los argumento que se han dado para sosten refutar su adecuaci,['Franco M. Luque'],"['cs.CL', 'cs.FL']",False,False,False,False,False,True
378,2017-03-28T14:06:28Z,2017-03-23T14:00:55Z,http://arxiv.org/abs/1703.04178v2,http://arxiv.org/pdf/1703.04178v2,Why we have switched from building full-fledged taxonomies to simply   detecting hypernymy relations,whi switch build full fledg taxonomi simpli detect hypernymi relat,"The study of taxonomies and hypernymy relations has been extensive on the Natural Language Processing (NLP) literature. However, the evaluation of taxonomy learning approaches has been traditionally troublesome, as it mainly relies on ad-hoc experiments which are hardly reproducible and manually expensive. Partly because of this, current research has been lately focusing on the hypernymy detection task. In this paper we reflect on this trend, analyzing issues related to current evaluation procedures. Finally, we propose three potential avenues for future work so that is-a relations and resources based on them play a more important role in downstream NLP applications.",studi taxonomi hypernymi relat extens natur languag process nlp literatur howev evalu taxonomi learn approach tradit troublesom main reli ad hoc experi hard reproduc manual expens part becaus current research late focus hypernymi detect task paper reflect trend analyz issu relat current evalu procedur final propos three potenti avenu futur work relat resourc base play import role downstream nlp applic,['Jose Camacho-Collados'],['cs.CL'],False,False,False,False,False,True
379,2017-03-28T14:06:28Z,2017-03-12T08:11:29Z,http://arxiv.org/abs/1703.04081v1,http://arxiv.org/pdf/1703.04081v1,Feature overwriting as a finite mixture process: Evidence from   comprehension data,featur overwrit finit mixtur process evid comprehens data,"The ungrammatical sentence ""The key to the cabinets are on the table"" is known to lead to an illusion of grammaticality. As discussed in the meta-analysis by Jaeger et al., 2017, faster reading times are observed at the verb are in the agreement-attraction sentence above compared to the equally ungrammatical sentence ""The key to the cabinet are on the table"". One explanation for this facilitation effect is the feature percolation account: the plural feature on cabinets percolates up to the head noun key, leading to the illusion. An alternative account is in terms of cue-based retrieval (Lewis & Vasishth, 2005), which assumes that the non-subject noun cabinets is misretrieved due to a partial feature-match when a dependency completion process at the auxiliary initiates a memory access for a subject with plural marking. We present evidence for yet another explanation for the observed facilitation. Because the second sentence has two nouns with identical number, it is possible that these are, in some proportion of trials, more difficult to keep distinct, leading to slower reading times at the verb in the first sentence above; this is the feature overwriting account of Nairne, 1990. We show that the feature overwriting proposal can be implemented as a finite mixture process. We reanalysed ten published data-sets, fitting hierarchical Bayesian mixture models to these data assuming a two-mixture distribution. We show that in nine out of the ten studies, a mixture distribution corresponding to feature overwriting furnishes a superior fit over both the feature percolation and the cue-based retrieval accounts.",ungrammat sentenc key cabinet tabl known lead illus grammat discuss meta analysi jaeger et al faster read time observ verb agreement attract sentenc abov compar equal ungrammat sentenc key cabinet tabl one explan facilit effect featur percol account plural featur cabinet percol head noun key lead illus altern account term cue base retriev lewi vasishth assum non subject noun cabinet misretriev due partial featur match depend complet process auxiliari initi memori access subject plural mark present evid yet anoth explan observ facilit becaus second sentenc two noun ident number possibl proport trial difficult keep distinct lead slower read time verb first sentenc abov featur overwrit account nairn show featur overwrit propos implement finit mixtur process reanalys ten publish data set fit hierarch bayesian mixtur model data assum two mixtur distribut show nine ten studi mixtur distribut correspond featur overwrit furnish superior fit featur percol cue base retriev account,"['Shravan Vasishth', 'Lena A. Jaeger', 'Bruno Nicenboim']","['stat.ML', 'cs.CL', 'stat.AP']",False,False,False,False,False,True
385,2017-03-28T14:06:31Z,2017-03-21T20:34:59Z,http://arxiv.org/abs/1703.03906v2,http://arxiv.org/pdf/1703.03906v2,Massive Exploration of Neural Machine Translation Architectures,massiv explor neural machin translat architectur,"Neural Machine Translation (NMT) has shown remarkable progress over the past few years with production systems now being deployed to end-users. One major drawback of current architectures is that they are expensive to train, typically requiring days to weeks of GPU time to converge. This makes exhaustive hyperparameter search, as is commonly done with other neural network architectures, prohibitively expensive. In this work, we present the first large-scale analysis of NMT architecture hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on the standard WMT English to German translation task. Our experiments lead to novel insights and practical advice for building and extending NMT architectures. As part of this contribution, we release an open-source NMT framework that enables researchers to easily experiment with novel techniques and reproduce state of the art results.",neural machin translat nmt shown remark progress past year product system deploy end user one major drawback current architectur expens train typic requir day week gpu time converg make exhaust hyperparamet search common done neural network architectur prohibit expens work present first larg scale analysi nmt architectur hyperparamet report empir result varianc number sever hundr experiment run correspond gpu hour standard wmt english german translat task experi lead novel insight practic advic build extend nmt architectur part contribut releas open sourc nmt framework enabl research easili experi novel techniqu reproduc state art result,"['Denny Britz', 'Anna Goldie', 'Minh-Thang Luong', 'Quoc Le']",['cs.CL'],False,False,False,False,False,True
388,2017-03-28T14:06:32Z,2017-03-10T15:27:45Z,http://arxiv.org/abs/1703.03714v1,http://arxiv.org/pdf/1703.03714v1,Applying the Wizard-of-Oz Technique to Multimodal Human-Robot Dialogue,appli wizard oz techniqu multimod human robot dialogu,"Our overall program objective is to provide more natural ways for soldiers to interact and communicate with robots, much like how soldiers communicate with other soldiers today. We describe how the Wizard-of-Oz (WOz) method can be applied to multimodal human-robot dialogue in a collaborative exploration task. While the WOz method can help design robot behaviors, traditional approaches place the burden of decisions on a single wizard. In this work, we consider two wizards to stand in for robot navigation and dialogue management software components. The scenario used to elicit data is one in which a human-robot team is tasked with exploring an unknown environment: a human gives verbal instructions from a remote location and the robot follows them, clarifying possible misunderstandings as needed via dialogue. We found the division of labor between wizards to be workable, which holds promise for future software development.",overal program object provid natur way soldier interact communic robot much like soldier communic soldier today describ wizard oz woz method appli multimod human robot dialogu collabor explor task woz method help design robot behavior tradit approach place burden decis singl wizard work consid two wizard stand robot navig dialogu manag softwar compon scenario use elicit data one human robot team task explor unknown environ human give verbal instruct remot locat robot follow clarifi possibl misunderstand need via dialogu found divis labor wizard workabl hold promis futur softwar develop,"['Matthew Marge', 'Claire Bonial', 'Brendan Byrne', 'Taylor Cassidy', 'A. William Evans', 'Susan G. Hill', 'Clare Voss']","['cs.CL', 'cs.AI', 'cs.HC', 'cs.RO']",False,False,False,False,False,True
392,2017-03-28T14:06:36Z,2017-03-09T19:50:00Z,http://arxiv.org/abs/1703.03442v1,http://arxiv.org/pdf/1703.03442v1,The cognitive roots of regularization in language,cognit root regular languag,"Regularization occurs when the output a learner produces is less variable than the linguistic data they observed. In an artificial language learning experiment, we show that there exist at least two independent sources of regularization bias in cognition: a domain-general source based on cognitive load and a domain-specific source triggered by linguistic stimuli. Both of these factors modulate how frequency information is encoded and produced, but only the production-side modulations result in regularization (i.e. cause learners to eliminate variation from the observed input). We formalize the definition of regularization as the reduction of entropy and find that entropy measures are better at identifying regularization behavior than frequency-based analyses. We also use a model of cultural transmission to extrapolate from our experimental data in order to predict the amount of regularization which would develop in each experimental condition if the artificial language was transmitted over several generations of learners. Here we find an interaction between cognitive load and linguistic domain, suggesting that the effect of cognitive constraints can become more complex when put into the context of cultural evolution: although learning biases certainly carry information about the course of language evolution, we should not expect a one-to-one correspondence between the micro-level processes that regularize linguistic datasets and the macro-level evolution of linguistic regularity.",regular occur output learner produc less variabl linguist data observ artifici languag learn experi show exist least two independ sourc regular bias cognit domain general sourc base cognit load domain specif sourc trigger linguist stimuli factor modul frequenc inform encod produc onli product side modul result regular caus learner elimin variat observ input formal definit regular reduct entropi find entropi measur better identifi regular behavior frequenc base analys also use model cultur transmiss extrapol experiment data order predict amount regular would develop experiment condit artifici languag transmit sever generat learner find interact cognit load linguist domain suggest effect cognit constraint becom complex put context cultur evolut although learn bias certain carri inform cours languag evolut expect one one correspond micro level process regular linguist dataset macro level evolut linguist regular,"['Vanessa Ferdinand', 'Simon Kirby', 'Kenny Smith']","['cs.CL', 'q-bio.NC']",False,False,False,False,False,True
393,2017-03-28T14:06:36Z,2017-03-09T19:16:14Z,http://arxiv.org/abs/1703.03429v1,http://arxiv.org/pdf/1703.03429v1,What can you do with a rock? Affordance extraction via word embeddings,rock afford extract via word embed,"Autonomous agents must often detect affordances: the set of behaviors enabled by a situation. Affordance detection is particularly helpful in domains with large action spaces, allowing the agent to prune its search space by avoiding futile behaviors. This paper presents a method for affordance extraction via word embeddings trained on a Wikipedia corpus. The resulting word vectors are treated as a common knowledge database which can be queried using linear algebra. We apply this method to a reinforcement learning agent in a text-only environment and show that affordance-based action selection improves performance most of the time. Our method increases the computational complexity of each learning step but significantly reduces the total number of steps needed. In addition, the agent's action selections begin to resemble those a human would choose.",autonom agent must often detect afford set behavior enabl situat afford detect particular help domain larg action space allow agent prune search space avoid futil behavior paper present method afford extract via word embed train wikipedia corpus result word vector treat common knowledg databas queri use linear algebra appli method reinforc learn agent text onli environ show afford base action select improv perform time method increas comput complex learn step signific reduc total number step need addit agent action select begin resembl human would choos,"['Nancy Fulda', 'Daniel Ricks', 'Ben Murdoch', 'David Wingate']","['cs.AI', 'cs.CL']",False,False,False,False,False,True
396,2017-03-28T14:06:36Z,2017-03-09T06:20:49Z,http://arxiv.org/abs/1703.03149v1,http://arxiv.org/pdf/1703.03149v1,Detecting Sockpuppets in Deceptive Opinion Spam,detect sockpuppet decept opinion spam,"This paper explores the problem of sockpuppet detection in deceptive opinion spam using authorship attribution and verification approaches. Two methods are explored. The first is a feature subsampling scheme that uses the KL-Divergence on stylistic language models of an author to find discriminative features. The second is a transduction scheme, spy induction that leverages the diversity of authors in the unlabeled test set by sending a set of spies (positive samples) from the training set to retrieve hidden samples in the unlabeled test set using nearest and farthest neighbors. Experiments using ground truth sockpuppet data show the effectiveness of the proposed schemes.",paper explor problem sockpuppet detect decept opinion spam use authorship attribut verif approach two method explor first featur subsampl scheme use kl diverg stylist languag model author find discrimin featur second transduct scheme spi induct leverag divers author unlabel test set send set spi posit sampl train set retriev hidden sampl unlabel test set use nearest farthest neighbor experi use ground truth sockpuppet data show effect propos scheme,"['Marjan Hosseinia', 'Arjun Mukherjee']",['cs.CL'],False,False,False,False,False,True
397,2017-03-28T14:06:36Z,2017-03-09T04:42:30Z,http://arxiv.org/abs/1703.03130v1,http://arxiv.org/pdf/1703.03130v1,A Structured Self-attentive Sentence Embedding,structur self attent sentenc embed,"This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.",paper propos new model extract interpret sentenc embed introduc self attent instead use vector use matrix repres embed row matrix attend differ part sentenc also propos self attent mechan special regular term model side effect embed come easi way visual specif part sentenc encod embed evalu model differ task author profil sentiment classif textual entail result show model yield signific perform gain compar sentenc embed method task,"['Zhouhan Lin', 'Minwei Feng', 'Cicero Nogueira dos Santos', 'Mo Yu', 'Bing Xiang', 'Bowen Zhou', 'Yoshua Bengio']","['cs.CL', 'cs.AI', 'cs.LG', 'cs.NE']",False,False,False,False,False,True
398,2017-03-28T14:06:36Z,2017-03-09T01:28:00Z,http://arxiv.org/abs/1703.03097v1,http://arxiv.org/abs/1703.03097v1,Information Extraction in Illicit Domains,inform extract illicit domain,"Extracting useful entities and attribute values from illicit domains such as human trafficking is a challenging problem with the potential for widespread social impact. Such domains employ atypical language models, have `long tails' and suffer from the problem of concept drift. In this paper, we propose a lightweight, feature-agnostic Information Extraction (IE) paradigm specifically designed for such domains. Our approach uses raw, unlabeled text from an initial corpus, and a few (12-120) seed annotations per domain-specific attribute, to learn robust IE models for unobserved pages and websites. Empirically, we demonstrate that our approach can outperform feature-centric Conditional Random Field baselines by over 18\% F-Measure on five annotated sets of real-world human trafficking datasets in both low-supervision and high-supervision settings. We also show that our approach is demonstrably robust to concept drift, and can be efficiently bootstrapped even in a serial computing environment.",extract use entiti attribut valu illicit domain human traffick challeng problem potenti widespread social impact domain employ atyp languag model long tail suffer problem concept drift paper propos lightweight featur agnost inform extract ie paradigm specif design domain approach use raw unlabel text initi corpus seed annot per domain specif attribut learn robust ie model unobserv page websit empir demonstr approach outperform featur centric condit random field baselin measur five annot set real world human traffick dataset low supervis high supervis set also show approach demonstr robust concept drift effici bootstrap even serial comput environ,"['Mayank Kejriwal', 'Pedro Szekely']","['cs.CL', 'cs.AI']",False,False,False,False,False,True
406,2017-03-28T14:08:43Z,2017-03-27T16:09:20Z,http://arxiv.org/abs/1703.09161v1,http://arxiv.org/pdf/1703.09161v1,A Dynamic Programming Solution to Bounded Dejittering Problems,dynam program solut bound dejitt problem,"We propose a dynamic programming solution to image dejittering problems with bounded displacements and obtain efficient algorithms for the removal of line jitter, line pixel jitter, and pixel jitter.",propos dynam program solut imag dejitt problem bound displac obtain effici algorithm remov line jitter line pixel jitter pixel jitter,['Lukas F. Lang'],"['math.OC', 'cs.CV']",False,False,False,False,False,True
412,2017-03-28T14:08:48Z,2017-03-27T12:14:07Z,http://arxiv.org/abs/1703.09026v1,http://arxiv.org/pdf/1703.09026v1,Trespassing the Boundaries: Labeling Temporal Bounds for Object   Interactions in Egocentric Video,trespass boundari label tempor bound object interact egocentr video,"Manual annotations of temporal bounds for object interactions (i.e. start and end times) are typical training input to recognition, localization and detection algorithms. For three publicly available egocentric datasets, we uncover inconsistencies in ground truth temporal bounds within and across annotators and datasets. We systematically assess the robustness of state-of-the-art approaches to changes in labeled temporal bounds, for object interaction recognition. As boundaries are trespassed, a drop of up to 10% is observed for both Improved Dense Trajectories and Two-Stream Convolutional Neural Network. We demonstrate that such disagreement stems from a limited understanding of the distinct phases of an action, and propose annotating based on the Rubicon Boundaries, inspired by a similarly named cognitive model, for consistent temporal bounds of object interactions. Evaluated on a public dataset, we report a 4% increase in overall accuracy, and an increase in accuracy for 55% of classes when Rubicon Boundaries are used for temporal annotations.",manual annot tempor bound object interact start end time typic train input recognit local detect algorithm three public avail egocentr dataset uncov inconsist ground truth tempor bound within across annot dataset systemat assess robust state art approach chang label tempor bound object interact recognit boundari trespass drop observ improv dens trajectori two stream convolut neural network demonstr disagr stem limit understand distinct phase action propos annot base rubicon boundari inspir similar name cognit model consist tempor bound object interact evalu public dataset report increas overal accuraci increas accuraci class rubicon boundari use tempor annot,"['Davide Moltisanti', 'Michael Wray', 'Walterio Mayol-Cuevas', 'Dima Damen']",['cs.CV'],False,False,False,False,False,True
416,2017-03-28T14:08:48Z,2017-03-27T03:50:51Z,http://arxiv.org/abs/1703.08919v1,http://arxiv.org/pdf/1703.08919v1,MIHash: Online Hashing with Mutual Information,mihash onlin hash mutual inform,"Learning-based adaptive hashing methods are widely used for nearest neighbor retrieval. Recently, online hashing methods have demonstrated a good performance-complexity tradeoff by learning hash functions from streaming data. In this paper, we aim to advance the state-of-the-art for online hashing. We first address a key challenge that has often been ignored: the binary codes for indexed data must be recomputed to keep pace with updates to the hash functions. We propose an efficient quality measure for hash functions, based on an information-theoretic quantity, mutual information, and use it successfully as a criterion to eliminate unnecessary hash table updates. Next, we show that mutual information can also be used as an objective in learning hash functions, using gradient-based optimization. Experiments on image retrieval benchmarks (including a 2.5M image dataset) confirm the effectiveness of our formulation, both in reducing hash table recomputations and in learning high-quality hash functions.",learn base adapt hash method wide use nearest neighbor retriev recent onlin hash method demonstr good perform complex tradeoff learn hash function stream data paper aim advanc state art onlin hash first address key challeng often ignor binari code index data must recomput keep pace updat hash function propos effici qualiti measur hash function base inform theoret quantiti mutual inform use success criterion elimin unnecessari hash tabl updat next show mutual inform also use object learn hash function use gradient base optim experi imag retriev benchmark includ imag dataset confirm effect formul reduc hash tabl recomput learn high qualiti hash function,"['Fatih Cakir', 'Kun He', 'Sarah Adel Bargal', 'Stan Sclaroff']",['cs.CV'],False,False,False,False,False,True
417,2017-03-28T14:08:48Z,2017-03-27T03:46:58Z,http://arxiv.org/abs/1703.08917v1,http://arxiv.org/pdf/1703.08917v1,A Visual Measure of Changes to Weighted Self-Organizing Map Patterns,visual measur chang weight self organ map pattern,"Estimating output changes by input changes is the main task in causal analysis. In previous work, input and output Self-Organizing Maps (SOMs) were associated for causal analysis of multivariate and nonlinear data. Based on the association, a weight distribution of the output conditional on a given input was obtained over the output map space. Such a weighted SOM pattern of the output changes when the input changes. In order to analyze the change, it is important to measure the difference of the patterns. Many methods have been proposed for the dissimilarity measure of patterns. However, it remains a major challenge when attempting to measure how the patterns change. In this paper, we propose a visualization approach that simplifies the comparison of the difference in terms of the pattern property. Using this approach, the change can be analyzed by integrating colors and star glyph shapes representing the property dissimilarity. Ecological data is used to demonstrate the usefulness of our approach and the experimental results show that our approach provides the change information effectively.",estim output chang input chang main task causal analysi previous work input output self organ map som associ causal analysi multivari nonlinear data base associ weight distribut output condit given input obtain output map space weight som pattern output chang input chang order analyz chang import measur differ pattern mani method propos dissimilar measur pattern howev remain major challeng attempt measur pattern chang paper propos visual approach simplifi comparison differ term pattern properti use approach chang analyz integr color star glyph shape repres properti dissimilar ecolog data use demonstr use approach experiment result show approach provid chang inform effect,"['Younjin Chung', 'Joachim Gudmundsson', 'Masahiro Takatsuka']",['cs.CV'],False,False,False,True,False,True
418,2017-03-28T14:08:48Z,2017-03-27T03:08:58Z,http://arxiv.org/abs/1703.08912v1,http://arxiv.org/pdf/1703.08912v1,Exploiting Color Name Space for Salient Object Detection,exploit color name space salient object detect,"In this paper, we will investigate the contribution of color names for salient object detection. Each input image is first converted to the color name space, which is consisted of 11 probabilistic channels. By exploring the topological structure relationship between the figure and the ground, we obtain a saliency map through a linear combination of a set of sequential attention maps. To overcome the limitation of only exploiting the surroundedness cue, two global cues with respect to color names are invoked for guiding the computation of another weighted saliency map. Finally, we integrate the two saliency maps into a unified framework to infer the saliency result. In addition, an improved post-processing procedure is introduced to effectively suppress the background while uniformly highlight the salient objects. Experimental results show that the proposed model produces more accurate saliency maps and performs well against 23 saliency models in terms of three evaluation metrics on three public datasets.",paper investig contribut color name salient object detect input imag first convert color name space consist probabilist channel explor topolog structur relationship figur ground obtain salienc map linear combin set sequenti attent map overcom limit onli exploit surrounded cue two global cue respect color name invok guid comput anoth weight salienc map final integr two salienc map unifi framework infer salienc result addit improv post process procedur introduc effect suppress background uniform highlight salient object experiment result show propos model produc accur salienc map perform well salienc model term three evalu metric three public dataset,"['Jing Lou', 'Huan Wang', 'Longtao Chen', 'Qingyuan Xia', 'Wei Zhu', 'Mingwu Ren']","['cs.CV', 'I.4']",False,False,False,False,False,True
419,2017-03-28T14:08:48Z,2017-03-27T01:44:41Z,http://arxiv.org/abs/1703.08897v1,http://arxiv.org/pdf/1703.08897v1,Transductive Zero-Shot Learning with Adaptive Structural Embedding,transduct zero shot learn adapt structur embed,"Zero-shot learning (ZSL) endows the computer vision system with the inferential capability to recognize instances of a new category that has never seen before. Two fundamental challenges in it are visual-semantic embedding and domain adaptation in cross-modality learning and unseen class prediction steps, respectively. To address both challenges, this paper presents two corresponding methods named Adaptive STructural Embedding (ASTE) and Self-PAsed Selective Strategy (SPASS), respectively. Specifically, ASTE formulates the visualsemantic interactions in a latent structural SVM framework to adaptively adjust the slack variables to embody the different reliableness among training instances. In this way, the reliable instances are imposed with small punishments, wheras the less reliable instances are imposed with more severe punishments. Thus, it ensures a more discriminative embedding. On the other hand, SPASS offers a framework to alleviate the domain shift problem in ZSL, which exploits the unseen data in an easy to hard fashion. Particularly, SPASS borrows the idea from selfpaced learning by iteratively selecting the unseen instances from reliable to less reliable to gradually adapt the knowledge from the seen domain to the unseen domain. Subsequently, by combining SPASS and ASTE, we present a self-paced Transductive ASTE (TASTE) method to progressively reinforce the classification capacity. Extensive experiments on three benchmark datasets (i.e., AwA, CUB, and aPY) demonstrate the superiorities of ASTE and TASTE. Furthermore, we also propose a fast training (FT) strategy to improve the efficiency of most of existing ZSL methods. The FT strategy is surprisingly simple and general enough, which can speed up the training time of most existing methods by 4~300 times while holding the previous performance.",zero shot learn zsl endow comput vision system inferenti capabl recogn instanc new categori never seen befor two fundament challeng visual semant embed domain adapt cross modal learn unseen class predict step respect address challeng paper present two correspond method name adapt structur embed ast self pase select strategi spass respect specif ast formul visualsemant interact latent structur svm framework adapt adjust slack variabl embodi differ reliabl among train instanc way reliabl instanc impos small punish whera less reliabl instanc impos sever punish thus ensur discrimin embed hand spass offer framework allevi domain shift problem zsl exploit unseen data easi hard fashion particular spass borrow idea selfpac learn iter select unseen instanc reliabl less reliabl gradual adapt knowledg seen domain unseen domain subsequ combin spass ast present self pace transduct ast tast method progress reinforc classif capac extens experi three benchmark dataset awa cub api demonstr superior ast tast furthermor also propos fast train ft strategi improv effici exist zsl method ft strategi surpris simpl general enough speed train time exist method time hold previous perform,"['Yunlong Yu', 'Zhong Ji', 'Jichang Guo', 'Yanwei Pang']",['cs.CV'],False,False,False,False,False,True
420,2017-03-28T14:08:52Z,2017-03-27T01:36:38Z,http://arxiv.org/abs/1703.08893v1,http://arxiv.org/pdf/1703.08893v1,Transductive Zero-Shot Learning with a Self-training dictionary approach,transduct zero shot learn self train dictionari approach,"As an important and challenging problem in computer vision, zero-shot learning (ZSL) aims at automatically recognizing the instances from unseen object classes without training data. To address this problem, ZSL is usually carried out in the following two aspects: 1) capturing the domain distribution connections between seen classes data and unseen classes data; and 2) modeling the semantic interactions between the image feature space and the label embedding space. Motivated by these observations, we propose a bidirectional mapping based semantic relationship modeling scheme that seeks for crossmodal knowledge transfer by simultaneously projecting the image features and label embeddings into a common latent space. Namely, we have a bidirectional connection relationship that takes place from the image feature space to the latent space as well as from the label embedding space to the latent space. To deal with the domain shift problem, we further present a transductive learning approach that formulates the class prediction problem in an iterative refining process, where the object classification capacity is progressively reinforced through bootstrapping-based model updating over highly reliable instances. Experimental results on three benchmark datasets (AwA, CUB and SUN) demonstrate the effectiveness of the proposed approach against the state-of-the-art approaches.",import challeng problem comput vision zero shot learn zsl aim automat recogn instanc unseen object class without train data address problem zsl usual carri follow two aspect captur domain distribut connect seen class data unseen class data model semant interact imag featur space label embed space motiv observ propos bidirect map base semant relationship model scheme seek crossmod knowledg transfer simultan project imag featur label embed common latent space name bidirect connect relationship take place imag featur space latent space well label embed space latent space deal domain shift problem present transduct learn approach formul class predict problem iter refin process object classif capac progress reinforc bootstrap base model updat high reliabl instanc experiment result three benchmark dataset awa cub sun demonstr effect propos approach state art approach,"['Yunlong Yu', 'Zhong Ji', 'Xi Li', 'Jichang Guo', 'Zhongfei Zhang', 'Haibin Ling', 'Fei Wu']",['cs.CV'],False,False,False,True,False,True
422,2017-03-28T14:08:52Z,2017-03-26T16:20:36Z,http://arxiv.org/abs/1703.08840v1,http://arxiv.org/pdf/1703.08840v1,Inferring The Latent Structure of Human Decision-Making from Raw Visual   Inputs,infer latent structur human decis make raw visual input,"The goal of imitation learning is to match example expert behavior, without access to a reinforcement signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are not explicitly modeled. We introduce an extension to the Generative Adversarial Imitation Learning method that can infer the latent structure of human decision-making in an unsupervised way. Our method can not only imitate complex behaviors, but also learn interpretable and meaningful representations. We demonstrate that the approach is applicable to high-dimensional environments including raw visual inputs. In the highway driving domain, we show that a model learned from demonstrations is able to both produce different styles of human-like driving behaviors and accurately anticipate human actions. Our method surpasses various baselines in terms of performance and functionality.",goal imit learn match exampl expert behavior without access reinforc signal expert demonstr provid human howev often show signific variabl due latent factor explicit model introduc extens generat adversari imit learn method infer latent structur human decis make unsupervis way method onli imit complex behavior also learn interpret meaning represent demonstr approach applic high dimension environ includ raw visual input highway drive domain show model learn demonstr abl produc differ style human like drive behavior accur anticip human action method surpass various baselin term perform function,"['Yunzhu Li', 'Jiaming Song', 'Stefano Ermon']","['cs.LG', 'cs.AI', 'cs.CV']",False,False,False,False,False,True
425,2017-03-28T14:08:52Z,2017-03-26T06:34:45Z,http://arxiv.org/abs/1703.08774v1,http://arxiv.org/pdf/1703.08774v1,Who Said What: Modeling Individual Labelers Improves Classification,said model individu label improv classif,"Data are often labeled by many different experts with each expert only labeling a small fraction of the data and each data point being labeled by several experts. This reduces the workload on individual experts and also gives a better estimate of the unobserved ground truth. When experts disagree, the standard approaches are to treat the majority opinion as the correct label or to model the correct label as a distribution. These approaches, however, do not make any use of potentially valuable information about which expert produced which label. To make use of this extra information, we propose modeling the experts individually and then learning averaging weights for combining them, possibly in sample-specific ways. This allows us to give more weight to more reliable experts and take advantage of the unique strengths of individual experts at classifying certain types of data. Here we show that our approach leads to improvements in computer-aided diagnosis of diabetic retinopathy. We also show that our method performs better than competing algorithms by Welinder and Perona, and by Mnih and Hinton. Our work offers an innovative approach for dealing with the myriad real-world settings that use expert opinions to define labels for training.",data often label mani differ expert expert onli label small fraction data data point label sever expert reduc workload individu expert also give better estim unobserv ground truth expert disagre standard approach treat major opinion correct label model correct label distribut approach howev make ani use potenti valuabl inform expert produc label make use extra inform propos model expert individu learn averag weight combin possibl sampl specif way allow us give weight reliabl expert take advantag uniqu strength individu expert classifi certain type data show approach lead improv comput aid diagnosi diabet retinopathi also show method perform better compet algorithm welind perona mnih hinton work offer innov approach deal myriad real world set use expert opinion defin label train,"['Melody Y. Guan', 'Varun Gulshan', 'Andrew M. Dai', 'Geoffrey E. Hinton']","['cs.LG', 'cs.CV']",False,False,True,False,False,True
429,2017-03-28T14:08:52Z,2017-03-26T04:15:10Z,http://arxiv.org/abs/1703.08764v1,http://arxiv.org/pdf/1703.08764v1,Structured Learning of Tree Potentials in CRF for Image Segmentation,structur learn tree potenti crf imag segment,"We propose a new approach to image segmentation, which exploits the advantages of both conditional random fields (CRFs) and decision trees. In the literature, the potential functions of CRFs are mostly defined as a linear combination of some pre-defined parametric models, and then methods like structured support vector machines (SSVMs) are applied to learn those linear coefficients. We instead formulate the unary and pairwise potentials as nonparametric forests---ensembles of decision trees, and learn the ensemble parameters and the trees in a unified optimization problem within the large-margin framework. In this fashion, we easily achieve nonlinear learning of potential functions on both unary and pairwise terms in CRFs. Moreover, we learn class-wise decision trees for each object that appears in the image. Due to the rich structure and flexibility of decision trees, our approach is powerful in modelling complex data likelihoods and label relationships. The resulting optimization problem is very challenging because it can have exponentially many variables and constraints. We show that this challenging optimization can be efficiently solved by combining a modified column generation and cutting-planes techniques. Experimental results on both binary (Graz-02, Weizmann horse, Oxford flower) and multi-class (MSRC-21, PASCAL VOC 2012) segmentation datasets demonstrate the power of the learned nonlinear nonparametric potentials.",propos new approach imag segment exploit advantag condit random field crfs decis tree literatur potenti function crfs defin linear combin pre defin parametr model method like structur support vector machin ssvms appli learn linear coeffici instead formul unari pairwis potenti nonparametr forest ensembl decis tree learn ensembl paramet tree unifi optim problem within larg margin framework fashion easili achiev nonlinear learn potenti function unari pairwis term crfs moreov learn class wise decis tree object appear imag due rich structur flexibl decis tree approach power model complex data likelihood label relationship result optim problem veri challeng becaus exponenti mani variabl constraint show challeng optim effici solv combin modifi column generat cut plane techniqu experiment result binari graz weizmann hors oxford flower multi class msrc pascal voc segment dataset demonstr power learn nonlinear nonparametr potenti,"['Fayao Liu', 'Guosheng Lin', 'Ruizhi Qiao', 'Chunhua Shen']",['cs.CV'],False,False,False,False,False,True
430,2017-03-28T14:08:55Z,2017-03-25T20:33:45Z,http://arxiv.org/abs/1703.08738v1,http://arxiv.org/pdf/1703.08738v1,Sketch-based Face Editing in Video Using Identity Deformation Transfer,sketch base face edit video use ident deform transfer,"We address the problem of using hand-drawn sketch to edit facial identity, such as enlarging the shape or modifying the position of eyes or mouth, in the whole video. This task is formulated as a 3D face model reconstruction and deformation problem. We first introduce a two-stage real-time 3D face model fitting schema to recover facial identity and expressions from the video. We recognize the user's editing intention from the input sketch as a set of facial modifications. A novel identity deformation algorithm is then proposed to transfer these deformations from 2D space to 3D facial identity directly, while preserving the facial expressions. Finally, these changes are propagated to the whole video with the modified identity. Experimental results demonstrate that our method can effectively edit facial identity in video based on the input sketch with high consistency and fidelity.",address problem use hand drawn sketch edit facial ident enlarg shape modifi posit eye mouth whole video task formul face model reconstruct deform problem first introduc two stage real time face model fit schema recov facial ident express video recogn user edit intent input sketch set facial modif novel ident deform algorithm propos transfer deform space facial ident direct preserv facial express final chang propag whole video modifi ident experiment result demonstr method effect edit facial ident video base input sketch high consist fidel,"['Long Zhao', 'Fangda Han', 'Mubbasir Kapadia', 'Vladimir Pavlovic', 'Dimitris Metaxas']",['cs.CV'],False,False,False,False,False,True
432,2017-03-28T14:08:56Z,2017-03-25T14:36:12Z,http://arxiv.org/abs/1703.08697v1,http://arxiv.org/abs/1703.08697v1,Improving the Accuracy of the CogniLearn System for Cognitive Behavior   Assessment,improv accuraci cognilearn system cognit behavior assess,"HTKS is a game-like cognitive assessment method, designed for children between four and eight years of age. During the HTKS assessment, a child responds to a sequence of requests, such as ""touch your head"" or ""touch your toes"". The cognitive challenge stems from the fact that the children are instructed to interpret these requests not literally, but by touching a different body part than the one stated. In prior work, we have developed the CogniLearn system, that captures data from subjects performing the HTKS game, and analyzes the motion of the subjects. In this paper we propose some specific improvements that make the motion analysis module more accurate. As a result of these improvements, the accuracy in recognizing cases where subjects touch their toes has gone from 76.46% in our previous work to 97.19% in this paper.",htks game like cognit assess method design children four eight year age dure htks assess child respond sequenc request touch head touch toe cognit challeng stem fact children instruct interpret request liter touch differ bodi part one state prior work develop cognilearn system captur data subject perform htks game analyz motion subject paper propos specif improv make motion analysi modul accur result improv accuraci recogn case subject touch toe gone previous work paper,"['Amir Ghaderi', 'Srujana Gattupalli', 'Dylan Ebert', 'Ali Sharifara', 'Vassilis Athitsos', 'Fillia Makedon']",['cs.CV'],False,False,False,False,False,True
434,2017-03-28T14:08:56Z,2017-03-25T05:51:42Z,http://arxiv.org/abs/1703.08651v1,http://arxiv.org/pdf/1703.08651v1,More is Less: A More Complicated Network with Less Inference Complexity,less complic network less infer complex,"In this paper, we present a novel and general network structure towards accelerating the inference process of convolutional neural networks, which is more complicated in network structure yet with less inference complexity. The core idea is to equip each original convolutional layer with another low-cost collaborative layer (LCCL), and the element-wise multiplication of the ReLU outputs of these two parallel layers produces the layer-wise output. The combined layer is potentially more discriminative than the original convolutional layer, and its inference is faster for two reasons: 1) the zero cells of the LCCL feature maps will remain zero after element-wise multiplication, and thus it is safe to skip the calculation of the corresponding high-cost convolution in the original convolutional layer, 2) LCCL is very fast if it is implemented as a 1*1 convolution or only a single filter shared by all channels. Extensive experiments on the CIFAR-10, CIFAR-100 and ILSCRC-2012 benchmarks show that our proposed network structure can accelerate the inference process by 32\% on average with negligible performance drop.",paper present novel general network structur toward acceler infer process convolut neural network complic network structur yet less infer complex core idea equip origin convolut layer anoth low cost collabor layer lccl element wise multipl relu output two parallel layer produc layer wise output combin layer potenti discrimin origin convolut layer infer faster two reason zero cell lccl featur map remain zero element wise multipl thus safe skip calcul correspond high cost convolut origin convolut layer lccl veri fast implement convolut onli singl filter share channel extens experi cifar cifar ilscrc benchmark show propos network structur acceler infer process averag neglig perform drop,"['Xuanyi Dong', 'Junshi Huang', 'Yi Yang', 'Shuicheng Yan']",['cs.CV'],False,False,False,False,False,True
441,2017-03-28T14:09:00Z,2017-03-24T16:28:57Z,http://arxiv.org/abs/1703.08493v1,http://arxiv.org/pdf/1703.08493v1,Multi-stage Multi-recursive-input Fully Convolutional Networks for   Neuronal Boundary Detection,multi stage multi recurs input fulli convolut network neuron boundari detect,"In the field of connectomics, neuroscientists seek to identify cortical connectivity comprehensively. Neuronal boundary detection from the Electron Microscopy (EM) images is often done to assist the automatic reconstruction of neuronal circuit. But the segmentation of EM images is a challenging problem, as it requires the detector to be able to detect both filament-like thin and blob-like thick membrane, while suppressing the ambiguous intracellular structure. In this paper, we propose multi-stage multi-recursive-input fully convolutional networks to address this problem. The multiple recursive inputs for one stage, i.e., the multiple side outputs with different receptive field sizes learned from the lower stage, provide multi-scale contextual boundary information for the consecutive learning. This design is biologically-plausible, as it likes a human visual system to compare different possible segmentation solutions to address the ambiguous boundary issue. Our multi-stage networks are trained end-to-end. It achieves promising results on a public available mouse piriform cortex dataset, which significantly outperforms other competitors.",field connectom neuroscientist seek identifi cortic connect comprehens neuron boundari detect electron microscopi em imag often done assist automat reconstruct neuron circuit segment em imag challeng problem requir detector abl detect filament like thin blob like thick membran suppress ambigu intracellular structur paper propos multi stage multi recurs input fulli convolut network address problem multipl recurs input one stage multipl side output differ recept field size learn lower stage provid multi scale contextu boundari inform consecut learn design biolog plausibl like human visual system compar differ possibl segment solut address ambigu boundari issu multi stage network train end end achiev promis result public avail mous piriform cortex dataset signific outperform competitor,"['Wei Shen', 'Bin Wang', 'Yuan Jiang', 'Yan Wang', 'Alan Yuille']",['cs.CV'],False,False,False,False,False,True
459,2017-03-28T14:09:05Z,2017-03-23T12:55:34Z,http://arxiv.org/abs/1703.08050v1,http://arxiv.org/pdf/1703.08050v1,Is Second-order Information Helpful for Large-scale Visual Recognition?,second order inform help larg scale visual recognit,"By stacking deeper layers of convolutions and nonlinearity, convolutional networks (ConvNets) effectively learn from low-level to high-level features and discriminative representations. Since the end goal of large-scale recognition is to delineate the complex boundaries of thousands of classes in a large-dimensional space, adequate exploration of feature distributions is important for realizing full potentials of ConvNets. However, state-of-the-art works concentrate only on deeper or wider architecture design, while rarely exploring feature statistics higher than first-order. We take a step towards addressing this problem. Our method consists in covariance pooling, instead of the most commonly used first-order pooling, of high-level convolutional features. The main challenges involved are robust covariance estimation given a small sample of large-dimensional features and usage of the manifold structure of covariance matrices. To address these challenges, we present a Matrix Power Normalized Covariance (MPN-COV) method. We develop the forward and backward propagation formulas regarding the nonlinear matrix functions such that MPN-COV can be trained end-to-end. In addition, we analyze both qualitatively and quantitatively its advantage over the widely used Log-Euclidean metric. On the ImageNet 2012 validation set, by combining MPN-COV we achieve over 4%, 3% and 2.5% gains for AlexNet, VGG-M and VGG-16, respectively; integration of MPN-COV into 50-layer ResNet outperforms ResNet-101 and is comparable to ResNet-152, both of which use first-order, global average pooling.",stack deeper layer convolut nonlinear convolut network convnet effect learn low level high level featur discrimin represent sinc end goal larg scale recognit delin complex boundari thousand class larg dimension space adequ explor featur distribut import realiz full potenti convnet howev state art work concentr onli deeper wider architectur design rare explor featur statist higher first order take step toward address problem method consist covari pool instead common use first order pool high level convolut featur main challeng involv robust covari estim given small sampl larg dimension featur usag manifold structur covari matric address challeng present matrix power normal covari mpn cov method develop forward backward propag formula regard nonlinear matrix function mpn cov train end end addit analyz qualit quantit advantag wide use log euclidean metric imagenet valid set combin mpn cov achiev gain alexnet vgg vgg respect integr mpn cov layer resnet outperform resnet compar resnet use first order global averag pool,"['Peihua Li', 'Jiangtao Xie', 'Qilong Wang', 'Wangmeng Zuo']",['cs.CV'],False,False,False,False,False,True
461,2017-03-28T14:09:09Z,2017-03-26T02:43:13Z,http://arxiv.org/abs/1703.08025v2,http://arxiv.org/pdf/1703.08025v2,Saliency-guided video classification via adaptively weighted learning,salienc guid video classif via adapt weight learn,"Video classification is productive in many practical applications, and the recent deep learning has greatly improved its accuracy. However, existing works often model video frames indiscriminately, but from the view of motion, video frames can be decomposed into salient and non-salient areas naturally. Salient and non-salient areas should be modeled with different networks, for the former present both appearance and motion information, and the latter present static background information. To address this problem, in this paper, video saliency is predicted by optical flow without supervision firstly. Then two streams of 3D CNN are trained individually for raw frames and optical flow on salient areas, and another 2D CNN is trained for raw frames on non-salient areas. For the reason that these three streams play different roles for each class, the weights of each stream are adaptively learned for each class. Experimental results show that saliency-guided modeling and adaptively weighted learning can reinforce each other, and we achieve the state-of-the-art results.",video classif product mani practic applic recent deep learn great improv accuraci howev exist work often model video frame indiscrimin view motion video frame decompos salient non salient area natur salient non salient area model differ network former present appear motion inform latter present static background inform address problem paper video salienc predict optic flow without supervis first two stream cnn train individu raw frame optic flow salient area anoth cnn train raw frame non salient area reason three stream play differ role class weight stream adapt learn class experiment result show salienc guid model adapt weight learn reinforc achiev state art result,"['Yunzhen Zhao', 'Yuxin Peng']",['cs.CV'],False,False,False,False,False,True
462,2017-03-28T14:09:09Z,2017-03-24T08:24:07Z,http://arxiv.org/abs/1703.08014v2,http://arxiv.org/pdf/1703.08014v2,Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse   IMUs,spars inerti poser automat human pose estim spars imus,"We address the problem of making human motion capture in the wild more practical by using a small set of inertial sensors attached to the body. Since the problem is heavily under-constrained, previous methods either use a large number of sensors, which is intrusive, or they require additional video input. We take a different approach and constrain the problem by: (i) making use of a realistic statistical body model that includes anthropometric constraints and (ii) using a joint optimization framework to fit the model to orientation and acceleration measurements over multiple frames. The resulting tracker Sparse Inertial Poser (SIP) enables 3D human pose estimation using only 6 sensors (attached to the wrists, lower legs, back and head) and works for arbitrary human motions. Experiments on the recently released TNT15 dataset show that, using the same number of sensors, SIP achieves higher accuracy than the dataset baseline without using any video data. We further demonstrate the effectiveness of SIP on newly recorded challenging motions in outdoor scenarios such as climbing or jumping over a wall.",address problem make human motion captur wild practic use small set inerti sensor attach bodi sinc problem heavili constrain previous method either use larg number sensor intrus requir addit video input take differ approach constrain problem make use realist statist bodi model includ anthropometr constraint ii use joint optim framework fit model orient acceler measur multipl frame result tracker spars inerti poser sip enabl human pose estim use onli sensor attach wrist lower leg back head work arbitrari human motion experi recent releas tnt dataset show use number sensor sip achiev higher accuraci dataset baselin without use ani video data demonstr effect sip newli record challeng motion outdoor scenario climb jump wall,"['Timo von Marcard', 'Bodo Rosenhahn', 'Michael J. Black', 'Gerard Pons-Moll']","['cs.CV', 'cs.GR']",False,False,False,False,False,True
468,2017-03-28T14:09:09Z,2017-03-23T07:52:31Z,http://arxiv.org/abs/1703.07957v1,http://arxiv.org/pdf/1703.07957v1,Robust SfM with Little Image Overlap,robust sfm littl imag overlap,"Usual Structure-from-Motion (SfM) techniques require at least trifocal overlaps to calibrate cameras and reconstruct a scene. We consider here scenarios of reduced image sets with little overlap, possibly as low as two images at most seeing the same part of the scene. We propose a new method, based on line coplanarity hypotheses, for estimating the relative scale of two independent bifocal calibrations sharing a camera, without the need of any trifocal information or Manhattan-world assumption. We use it to compute SfM in a chain of up-to-scale relative motions. For accuracy, we however also make use of trifocal information for line and/or point features, when present, relaxing usual trifocal constraints. For robustness to wrong assumptions and mismatches, we embed all constraints in a parameterless RANSAC-like approach. Experiments show that we can calibrate datasets that previously could not, and that this wider applicability does not come at the cost of inaccuracy.",usual structur motion sfm techniqu requir least trifoc overlap calibr camera reconstruct scene consid scenario reduc imag set littl overlap possibl low two imag see part scene propos new method base line coplanar hypothes estim relat scale two independ bifoc calibr share camera without need ani trifoc inform manhattan world assumpt use comput sfm chain scale relat motion accuraci howev also make use trifoc inform line point featur present relax usual trifoc constraint robust wrong assumpt mismatch emb constraint parameterless ransac like approach experi show calibr dataset previous could wider applic doe come cost inaccuraci,"['Yohann Salaun', 'Renaud Marlet', 'Pascal Monasse']",['cs.CV'],False,False,False,False,False,True
475,2017-03-28T14:09:13Z,2017-03-22T23:35:51Z,http://arxiv.org/abs/1703.07886v1,http://arxiv.org/pdf/1703.07886v1,Robust Kronecker-Decomposable Component Analysis for Low Rank Modeling,robust kroneck decompos compon analysi low rank model,"Dictionary learning and component analysis are part of one of the most well-studied and active research fields, at the intersection of signal and image processing, computer vision, and statistical machine learning. In dictionary learning, the current methods of choice are arguably K-SVD and its variants, which learn a dictionary (i.e., a decomposition) for sparse coding via Singular Value Decomposition. In robust component analysis, leading methods derive from Principal Component Pursuit (PCP), which recovers a low-rank matrix from sparse corruptions of unknown magnitude and support. While K-SVD is sensitive to the presence of noise and outliers in the training set, PCP does not provide a dictionary that respects the structure of the data (e.g., images), and requires expensive SVD computations when solved by convex relaxation. In this paper, we introduce a new robust decomposition of images by combining ideas from sparse dictionary learning and PCP. We propose a novel Kronecker-decomposable component analysis which is robust to gross corruption, can be used for low-rank modeling, and leverages separability to solve significantly smaller problems. We design an efficient learning algorithm by drawing links with a restricted form of tensor factorization. The effectiveness of the proposed approach is demonstrated on real-world applications, namely background subtraction and image denoising, by performing a thorough comparison with the current state of the art.",dictionari learn compon analysi part one well studi activ research field intersect signal imag process comput vision statist machin learn dictionari learn current method choic arguabl svd variant learn dictionari decomposit spars code via singular valu decomposit robust compon analysi lead method deriv princip compon pursuit pcp recov low rank matrix spars corrupt unknown magnitud support svd sensit presenc nois outlier train set pcp doe provid dictionari respect structur data imag requir expens svd comput solv convex relax paper introduc new robust decomposit imag combin idea spars dictionari learn pcp propos novel kroneck decompos compon analysi robust gross corrupt use low rank model leverag separ solv signific smaller problem design effici learn algorithm draw link restrict form tensor factor effect propos approach demonstr real world applic name background subtract imag denois perform thorough comparison current state art,"['Mehdi Bahri', 'Yannis Panagakis', 'Stefanos Zafeiriou']","['stat.ML', 'cs.CV']",False,False,False,False,False,True
485,2017-03-28T14:09:17Z,2017-03-22T09:25:49Z,http://arxiv.org/abs/1703.07579v1,http://arxiv.org/pdf/1703.07579v1,An End-to-End Approach to Natural Language Object Retrieval via   Context-Aware Deep Reinforcement Learning,end end approach natur languag object retriev via context awar deep reinforc learn,"We propose an end-to-end approach to the natural language object retrieval task, which localizes an object within an image according to a natural language description, i.e., referring expression. Previous works divide this problem into two independent stages: first, compute region proposals from the image without the exploration of the language description; second, score the object proposals with regard to the referring expression and choose the top-ranked proposals. The object proposals are generated independently from the referring expression, which makes the proposal generation redundant and even irrelevant to the referred object. In this work, we train an agent with deep reinforcement learning, which learns to move and reshape a bounding box to localize the object according to the referring expression. We incorporate both the spatial and temporal context information into the training procedure. By simultaneously exploiting local visual information, the spatial and temporal context and the referring language a priori, the agent selects an appropriate action to take at each time. A special action is defined to indicate when the agent finds the referred object, and terminate the procedure. We evaluate our model on various datasets, and our algorithm significantly outperforms the compared algorithms. Notably, the accuracy improvement of our method over the recent method GroundeR and SCRC on the ReferItGame dataset are 7.67% and 18.25%, respectively.",propos end end approach natur languag object retriev task local object within imag accord natur languag descript refer express previous work divid problem two independ stage first comput region propos imag without explor languag descript second score object propos regard refer express choos top rank propos object propos generat independ refer express make propos generat redund even irrelev refer object work train agent deep reinforc learn learn move reshap bound box local object accord refer express incorpor spatial tempor context inform train procedur simultan exploit local visual inform spatial tempor context refer languag priori agent select appropri action take time special action defin indic agent find refer object termin procedur evalu model various dataset algorithm signific outperform compar algorithm notabl accuraci improv method recent method grounder scrc referitgam dataset respect,"['Fan Wu', 'Zhongwen Xu', 'Yi Yang']",['cs.CV'],False,False,False,False,False,True
491,2017-03-28T14:09:21Z,2017-03-22T00:51:14Z,http://arxiv.org/abs/1703.07479v1,http://arxiv.org/pdf/1703.07479v1,Knowledge Transfer for Melanoma Screening with Deep Learning,knowledg transfer melanoma screen deep learn,"Knowledge transfer impacts the performance of deep learning -- the state of the art for image classification tasks, including automated melanoma screening. Deep learning's greed for large amounts of training data poses a challenge for medical tasks, which we can alleviate by recycling knowledge from models trained on different tasks, in a scheme called transfer learning. Although much of the best art on automated melanoma screening employs some form of transfer learning, a systematic evaluation was missing. Here we investigate the presence of transfer, from which task the transfer is sourced, and the application of fine tuning (i.e., retraining of the deep learning model after transfer). We also test the impact of picking deeper (and more expensive) models. Our results favor deeper models, pre-trained over ImageNet, with fine-tuning, reaching an AUC of 80.7% and 84.5% for the two skin-lesion datasets evaluated.",knowledg transfer impact perform deep learn state art imag classif task includ autom melanoma screen deep learn greed larg amount train data pose challeng medic task allevi recycl knowledg model train differ task scheme call transfer learn although much best art autom melanoma screen employ form transfer learn systemat evalu miss investig presenc transfer task transfer sourc applic fine tune retrain deep learn model transfer also test impact pick deeper expens model result favor deeper model pre train imagenet fine tune reach auc two skin lesion dataset evalu,"['Afonso Menegola', 'Michel Fornaciali', 'Ramon Pires', 'Flávia Vasques Bittencourt', 'Sandra Avila', 'Eduardo Valle']",['cs.CV'],False,False,False,False,False,True
494,2017-03-28T14:09:21Z,2017-03-21T23:56:51Z,http://arxiv.org/abs/1703.07473v1,http://arxiv.org/pdf/1703.07473v1,Episode-Based Active Learning with Bayesian Neural Networks,episod base activ learn bayesian neural network,"We investigate different strategies for active learning with Bayesian deep neural networks. We focus our analysis on scenarios where new, unlabeled data is obtained episodically, such as commonly encountered in mobile robotics applications. An evaluation of different strategies for acquisition, updating, and final training on the CIFAR-10 dataset shows that incremental network updates with final training on the accumulated acquisition set are essential for best performance, while limiting the amount of required human labeling labor.",investig differ strategi activ learn bayesian deep neural network focus analysi scenario new unlabel data obtain episod common encount mobil robot applic evalu differ strategi acquisit updat final train cifar dataset show increment network updat final train accumul acquisit set essenti best perform limit amount requir human label labor,"['Feras Dayoub', 'Niko Sünderhauf', 'Peter Corke']","['cs.CV', 'cs.LG', 'stat.ML']",False,False,False,False,False,True
500,2017-03-28T14:09:26Z,2017-03-27T13:57:31Z,http://arxiv.org/abs/1703.09083v1,http://arxiv.org/pdf/1703.09083v1,The weighted stable matching problem,weight stabl match problem,"We study the stable matching problem in non-bipartite graphs with incomplete but strict preference lists, where the edges have weights and the goal is to compute a stable matching of minimum or maximum weight. This problem is known to be NP-hard in general. Our contribution is two fold: a polyhedral characterization and an approximation algorithm. Previously Chen et al. have shown that the stable matching polytope is integral if and only if the subgraph obtained after running phase one of Irving's algorithm is bipartite. We improve upon this result by showing that there are instances where this subgraph might not be bipartite but one can further eliminate some edges and arrive at a bipartite subgraph. Our elimination procedure ensures that the set of stable matchings remains the same, and thus the stable matching polytope of the final subgraph contains the incidence vectors of all stable matchings of our original graph. This allows us to characterize a larger class of instances for which the weighted stable matching problem is polynomial-time solvable. We also show that our edge elimination procedure is best possible, meaning that if the subgraph we arrive at is not bipartite, then there is no bipartite subgraph that has the same set of stable matchings as the original graph. We complement these results with a $2$-approximation algorithm for the minimum weight stable matching problem for instances where each agent has at most two possible partners in any stable matching. This is the first approximation result for any class of instances with general weights.",studi stabl match problem non bipartit graph incomplet strict prefer list edg weight goal comput stabl match minimum maximum weight problem known np hard general contribut two fold polyhedr character approxim algorithm previous chen et al shown stabl match polytop integr onli subgraph obtain run phase one irv algorithm bipartit improv upon result show instanc subgraph might bipartit one elimin edg arriv bipartit subgraph elimin procedur ensur set stabl match remain thus stabl match polytop final subgraph contain incid vector stabl match origin graph allow us character larger class instanc weight stabl match problem polynomi time solvabl also show edg elimin procedur best possibl mean subgraph arriv bipartit bipartit subgraph set stabl match origin graph complement result approxim algorithm minimum weight stabl match problem instanc agent two possibl partner ani stabl match first approxim result ani class instanc general weight,"['Linda Farczadi', 'Natália Guričanová']","['cs.GT', 'cs.DS']",False,False,False,False,False,True
502,2017-03-28T14:09:26Z,2017-03-27T05:48:36Z,http://arxiv.org/abs/1703.08940v1,http://arxiv.org/pdf/1703.08940v1,Tree Edit Distance Cannot be Computed in Strongly Subcubic Time (unless   APSP can),tree edit distanc cannot comput strong subcub time unless apsp,"The edit distance between two rooted ordered trees with $n$ nodes labeled from an alphabet~$\Sigma$ is the minimum cost of transforming one tree into the other by a sequence of elementary operations consisting of deleting and relabeling existing nodes, as well as inserting new nodes. Tree edit distance is a well known generalization of string edit distance. The fastest known algorithm for tree edit distance runs in cubic $O(n^3)$ time and is based on a similar dynamic programming solution as string edit distance. In this paper we show that a truly subcubic $O(n^{3-\varepsilon})$ time algorithm for tree edit distance is unlikely: For $ \Sigma  = \Omega(n)$, a truly subcubic algorithm for tree edit distance implies a truly subcubic algorithm for the all pairs shortest paths problem. For $ \Sigma  = O(1)$, a truly subcubic algorithm for tree edit distance implies an $O(n^{k-\varepsilon})$ algorithm for finding a maximum weight $k$-clique.   Thus, while in terms of upper bounds string edit distance and tree edit distance are highly related, in terms of lower bounds string edit distance exhibits the hardness of the strong exponential time hypothesis [Backurs, Indyk STOC'15] whereas tree edit distance exhibits the hardness of all pairs shortest paths. Our result provides a matching conditional lower bound for one of the last remaining classic dynamic programming problems.",edit distanc two root order tree node label alphabet sigma minimum cost transform one tree sequenc elementari oper consist delet relabel exist node well insert new node tree edit distanc well known general string edit distanc fastest known algorithm tree edit distanc run cubic time base similar dynam program solut string edit distanc paper show truli subcub varepsilon time algorithm tree edit distanc unlik sigma omega truli subcub algorithm tree edit distanc impli truli subcub algorithm pair shortest path problem sigma truli subcub algorithm tree edit distanc impli varepsilon algorithm find maximum weight cliqu thus term upper bound string edit distanc tree edit distanc high relat term lower bound string edit distanc exhibit hard strong exponenti time hypothesi backur indyk stoc wherea tree edit distanc exhibit hard pair shortest path result provid match condit lower bound one last remain classic dynam program problem,"['Karl Bringmann', 'Paweł Gawrychowski', 'Shay Mozes', 'Oren Weimann']",['cs.DS'],False,False,False,False,False,True
504,2017-03-28T14:09:26Z,2017-03-26T09:03:07Z,http://arxiv.org/abs/1703.08790v1,http://arxiv.org/pdf/1703.08790v1,Steiner Point Removal --- Distant Terminals Don't (Really) Bother,steiner point remov distant termin realli bother,"Given a weighted graph $G=(V,E,w)$ with a set of $k$ terminals $T\subset V$, the Steiner Point Removal problem seeks for a minor of the graph with vertex set $T$, such that the distance between every pair of terminals is preserved within a small multiplicative distortion. Kamma, Krauthgamer and Nguyen (SODA 2014, SICOMP 2015) used a ball-growing algorithm to show that the distortion is at most $\mathcal{O}(\log^5 k)$ for general graphs.   In this paper, we improve the distortion bound to $\mathcal{O}(\log^2 k)$. The improvement is achieved based on a known algorithm that constructs terminal-distance exact-preservation minor with $\mathcal{O}(k^4)$ (which is independent of $ V $) vertices, and also two tail bounds on the sum of independent exponential random variables, which allow us to show that it is unlikely for a non-terminal being contracted to a distant terminal.",given weight graph set termin subset steiner point remov problem seek minor graph vertex set distanc everi pair termin preserv within small multipl distort kamma krauthgam nguyen soda sicomp use ball grow algorithm show distort mathcal log general graph paper improv distort bound mathcal log improv achiev base known algorithm construct termin distanc exact preserv minor mathcal independ vertic also two tail bound sum independ exponenti random variabl allow us show unlik non termin contract distant termin,['Yun Kuen Cheung'],"['cs.DS', 'cs.DM', 'math.CO', 'math.PR']",False,False,False,False,False,True
505,2017-03-28T14:09:26Z,2017-03-25T15:03:49Z,http://arxiv.org/abs/1703.08702v1,http://arxiv.org/pdf/1703.08702v1,Randomized Load Balancing on Networks with Stochastic Inputs,random load balanc network stochast input,"Iterative load balancing algorithms for indivisible tokens have been studied intensively in the past. Complementing previous worst-case analyses, we study an average-case scenario where the load inputs are drawn from a fixed probability distribution. For cycles, tori, hypercubes and expanders, we obtain almost matching upper and lower bounds on the discrepancy, the difference between the maximum and the minimum load. Our bounds hold for a variety of probability distributions including the uniform and binomial distribution but also distributions with unbounded range such as the Poisson and geometric distribution. For graphs with slow convergence like cycles and tori, our results demonstrate a substantial difference between the convergence in the worst- and average-case. An important ingredient in our analysis is new upper bound on the t-step transition probability of a general Markov chain, which is derived by invoking the evolving set process.",iter load balanc algorithm indivis token studi intens past complement previous worst case analys studi averag case scenario load input drawn fix probabl distribut cycl tori hypercub expand obtain almost match upper lower bound discrep differ maximum minimum load bound hold varieti probabl distribut includ uniform binomi distribut also distribut unbound rang poisson geometr distribut graph slow converg like cycl tori result demonstr substanti differ converg worst averag case import ingredi analysi new upper bound step transit probabl general markov chain deriv invok evolv set process,"['Leran Cai', 'Thomas Sauerwald']","['cs.DC', 'cs.DS', 'G.3']",False,False,False,False,False,True
509,2017-03-28T14:09:26Z,2017-03-24T14:44:04Z,http://arxiv.org/abs/1703.08433v1,http://arxiv.org/pdf/1703.08433v1,Metric random matchings with applications,metric random match applic,"Let $(\{1,2,\ldots,n\},d)$ be a metric space. We analyze the expected value and the variance of $\sum_{i=1}^{\lfloor n/2\rfloor}\,d({\boldsymbol{\pi}}(2i-1),{\boldsymbol{\pi}}(2i))$ for a uniformly random permutation ${\boldsymbol{\pi}}$ of $\{1,2,\ldots,n\}$, leading to the following results: (I) Consider the problem of finding a point in $\{1,2,\ldots,n\}$ with the minimum sum of distances to all points. We show that this problem has a randomized algorithm that (1) always outputs a $(2+\epsilon)$-approximate solution in expected $O(n/\epsilon^2)$ time and that (2) inherits Indyk's~\cite{Ind99, Ind00} algorithm to output a $(1+\epsilon)$-approximate solution in $O(n/\epsilon^2)$ time with probability $\Omega(1)$, where $\epsilon\in(0,1)$. (II) The average distance in $(\{1,2,\ldots,n\},d)$ can be approximated in $O(n/\epsilon)$ time to within a multiplicative factor in $[\,1/2-\epsilon,1\,]$ with probability $1/2+\Omega(1)$, where $\epsilon>0$. (III) Assume $d$ to be a graph metric. Then the average distance in $(\{1,2,\ldots,n\},d)$ can be approximated in $O(n)$ time to within a multiplicative factor in $[\,1-\epsilon,1+\epsilon\,]$ with probability $1/2+\Omega(1)$, where $\epsilon=\omega(1/n^{1/4})$.",let ldot metric space analyz expect valu varianc sum lfloor rfloor boldsymbol pi boldsymbol pi uniform random permut boldsymbol pi ldot lead follow result consid problem find point ldot minimum sum distanc point show problem random algorithm alway output epsilon approxim solut expect epsilon time inherit indyk cite ind ind algorithm output epsilon approxim solut epsilon time probabl omega epsilon ii averag distanc ldot approxim epsilon time within multipl factor epsilon probabl omega epsilon iii assum graph metric averag distanc ldot approxim time within multipl factor epsilon epsilon probabl omega epsilon omega,['Ching-Lueh Chang'],['cs.DS'],False,False,False,False,False,True
510,2017-03-28T14:09:30Z,2017-03-24T02:59:51Z,http://arxiv.org/abs/1703.08273v1,http://arxiv.org/pdf/1703.08273v1,An Asymptotically Tighter Bound on Sampling for Frequent Itemsets Mining,asymptot tighter bound sampl frequent itemset mine,"In this paper we present a new error bound on sampling algorithms for frequent itemsets mining. We show that the new bound is asymptotically tighter than the state-of-art bounds, i.e., given the chosen samples, for small enough error probability, the new error bound is roughly half of the existing bounds. Based on the new bound, we give a new approximation algorithm, which is much simpler compared to the existing approximation algorithms, but can also guarantee the worst approximation error with precomputed sample size. We also give an algorithm which can approximate the top-$k$ frequent itemsets with high accuracy and efficiency.",paper present new error bound sampl algorithm frequent itemset mine show new bound asymptot tighter state art bound given chosen sampl small enough error probabl new error bound rough half exist bound base new bound give new approxim algorithm much simpler compar exist approxim algorithm also guarante worst approxim error precomput sampl size also give algorithm approxim top frequent itemset high accuraci effici,"['Shiyu Ji', 'Kun Wan']","['cs.DS', 'cs.DB']",False,False,False,False,False,True
511,2017-03-28T14:09:30Z,2017-03-23T16:50:03Z,http://arxiv.org/abs/1703.08139v1,http://arxiv.org/pdf/1703.08139v1,"Optimal lower bounds for universal relation, samplers, and finding   duplicates",optim lower bound univers relat sampler find duplic,"In the communication problem $\mathbf{UR}$ (universal relation) [KRW95], Alice and Bob respectively receive $x$ and $y$ in $\{0,1\}^n$ with the promise that $x\neq y$. The last player to receive a message must output an index $i$ such that $x_i\neq y_i$. We prove that the randomized one-way communication complexity of this problem in the public coin model is exactly $\Theta(\min\{n, \log(1/\delta)\log^2(\frac{n}{\log(1/\delta)})\})$ bits for failure probability $\delta$. Our lower bound holds even if promised $\mathop{support}(y)\subset \mathop{support}(x)$. As a corollary, we obtain optimal lower bounds for $\ell_p$-sampling in strict turnstile streams for $0\le p < 2$, as well as for the problem of finding duplicates in a stream. Our lower bounds do not need to use large weights, and hold even if it is promised that $x\in\{0,1\}^n$ at all points in the stream.   Our lower bound demonstrates that any algorithm $\mathcal{A}$ solving sampling problems in turnstile streams in low memory can be used to encode subsets of $[n]$ of certain sizes into a number of bits below the information theoretic minimum. Our encoder makes adaptive queries to $\mathcal{A}$ throughout its execution, but done carefully so as to not violate correctness. This is accomplished by injecting random noise into the encoder's interactions with $\mathcal{A}$, which is loosely motivated by techniques in differential privacy. Our correctness analysis involves understanding the ability of $\mathcal{A}$ to correctly answer adaptive queries which have positive but bounded mutual information with $\mathcal{A}$'s internal randomness, and may be of independent interest in the newly emerging area of adaptive data analysis with a theoretical computer science lens.",communic problem mathbf ur univers relat krw alic bob respect receiv promis neq last player receiv messag must output index neq prove random one way communic complex problem public coin model exact theta min log delta log frac log delta bit failur probabl delta lower bound hold even promis mathop support subset mathop support corollari obtain optim lower bound ell sampl strict turnstil stream le well problem find duplic stream lower bound need use larg weight hold even promis point stream lower bound demonstr ani algorithm mathcal solv sampl problem turnstil stream low memori use encod subset certain size number bit inform theoret minimum encod make adapt queri mathcal throughout execut done care violat correct accomplish inject random nois encod interact mathcal loos motiv techniqu differenti privaci correct analysi involv understand abil mathcal correct answer adapt queri posit bound mutual inform mathcal intern random may independ interest newli emerg area adapt data analysi theoret comput scienc len,"['Jelani Nelson', 'Jakub Pachocki', 'Zhengyu Wang']","['cs.CC', 'cs.DS']",False,False,False,False,False,True
513,2017-03-28T14:09:30Z,2017-03-23T08:37:54Z,http://arxiv.org/abs/1703.07964v1,http://arxiv.org/abs/1703.07964v1,Minimum Cuts and Shortest Cycles in Directed Planar Graphs via   Noncrossing Shortest Paths,minimum cut shortest cycl direct planar graph via noncross shortest path,"Let $G$ be an $n$-node simple directed planar graph with nonnegative edge weights. We study the fundamental problems of computing (1) a global cut of $G$ with minimum weight and (2) a~cycle of $G$ with minimum weight. The best previously known algorithm for the former problem, running in $O(n\log^3 n)$ time, can be obtained from the algorithm of \Lacki, Nussbaum, Sankowski, and Wulff-Nilsen for single-source all-sinks maximum flows. The best previously known result for the latter problem is the $O(n\log^3 n)$-time algorithm of Wulff-Nilsen. By exploiting duality between the two problems in planar graphs, we solve both problems in $O(n\log n\log\log n)$ time via a divide-and-conquer algorithm that finds a shortest non-degenerate cycle. The kernel of our result is an $O(n\log\log n)$-time algorithm for computing noncrossing shortest paths among nodes well ordered on a common face of a directed plane graph, which is extended from the algorithm of Italiano, Nussbaum, Sankowski, and Wulff-Nilsen for an undirected plane graph.",let node simpl direct planar graph nonneg edg weight studi fundament problem comput global cut minimum weight cycl minimum weight best previous known algorithm former problem run log time obtain algorithm lacki nussbaum sankowski wulff nilsen singl sourc sink maximum flow best previous known result latter problem log time algorithm wulff nilsen exploit dualiti two problem planar graph solv problem log log log time via divid conquer algorithm find shortest non degener cycl kernel result log log time algorithm comput noncross shortest path among node well order common face direct plane graph extend algorithm italiano nussbaum sankowski wulff nilsen undirect plane graph,"['Hung-Chun Liang', 'Hsueh-I Lu']","['cs.DS', '05C38, 05C10, 05C85, 68P05']",False,False,False,False,False,True
514,2017-03-28T14:09:30Z,2017-03-22T21:52:05Z,http://arxiv.org/abs/1703.07867v1,http://arxiv.org/pdf/1703.07867v1,Distance-sensitive hashing,distanc sensit hash,"We initiate the study of distance-sensitive hashing, a generalization of locality-sensitive hashing that seeks a family of hash functions such that the probability of two points having the same hash value is a given function of the distance between them. More precisely, given a distance space $(X, \text{dist})$ and a ""collision probability function"" (CPF) $f\colon \mathbb{R}\rightarrow [0,1]$ we seek a distribution over pairs of functions $(h,g)$ such that for every pair of points $x, y \in X$ the collision probability is $\Pr[h(x)=g(y)] = f(\text{dist}(x,y))$. Locality-sensitive hashing is the study of how fast a CPF can decrease as the distance grows. For many spaces $f$ can be made exponentially decreasing even if we restrict attention to the symmetric case where $g=h$. In this paper we study how asymmetry makes it possible to achieve CPFs that are, for example, increasing or unimodal. Our original motivation comes from annulus queries where we are interested in searching for points at distance approximately $r$ from a query point, but we believe that distance-sensitive hashing is of interest beyond this application.",initi studi distanc sensit hash general local sensit hash seek famili hash function probabl two point hash valu given function distanc precis given distanc space text dist collis probabl function cpf colon mathbb rightarrow seek distribut pair function everi pair point collis probabl pr text dist local sensit hash studi fast cpf decreas distanc grow mani space made exponenti decreas even restrict attent symmetr case paper studi asymmetri make possibl achiev cpfs exampl increas unimod origin motiv come annulus queri interest search point distanc approxim queri point believ distanc sensit hash interest beyond applic,"['Martin Aumüller', 'Tobias Christiani', 'Rasmus Pagh', 'Francesco Silvestri']","['cs.DS', 'H.3.3']",False,False,False,False,False,True
515,2017-03-28T14:09:30Z,2017-03-22T16:28:17Z,http://arxiv.org/abs/1703.07734v1,http://arxiv.org/pdf/1703.07734v1,On the Probe Complexity of Local Computation Algorithms,probe complex local comput algorithm,"The Local Computation Algorithms (LCA) model is a computational model aimed at problem instances with huge inputs and output. For graph problems, the input graph is accessed using probes: strong probes (SP) specify a vertex $v$ and receive as a reply a list of $v$'s neighbors, and weak probes (WP) specify a vertex $v$ and a port number $i$ and receive as a reply $v$'s $i^{th}$ neighbor. Given a local query (e.g., ""is a certain vertex in the vertex cover of the input graph?""), an LCA should compute the corresponding local output (e.g., ""yes"" or ""no"") while making only a small number of probes, with the requirement that all local outputs form a single global solution (e.g., a legal vertex cover). We study the probe complexity of LCAs that are required to work on graphs that may have arbitrarily large degrees. In particular, such LCAs are expected to probe the graph a number of times that is significantly smaller than the maximum, average, or even minimum degree.   For weak probes, we focus on the weak coloring problem. Among our results we show a separation between weak 3-coloring and weak 2-coloring for deterministic LCAs: $\log^* n + O(1)$ weak probes suffice for weak 3-coloring, but $\Omega\left(\frac{\log n}{\log\log n}\right)$ weak probes are required for weak 2-coloring.   For strong probes, we consider randomized LCAs for vertex cover and maximal/maximum matching. Our negative results include showing that there are graphs for which finding a \emph{maximal} matching requires $\Omega(\sqrt{n})$ strong probes. On the positive side, we design a randomized LCA that finds a $(1-\epsilon)$ approximation to \emph{maximum} matching in regular graphs, and uses $\frac{1}{\epsilon }^{O\left( \frac{1}{\epsilon ^2}\right)}$ probes, independently of the number of vertices and of their degrees.",local comput algorithm lca model comput model aim problem instanc huge input output graph problem input graph access use probe strong probe sp specifi vertex receiv repli list neighbor weak probe wp specifi vertex port number receiv repli th neighbor given local queri certain vertex vertex cover input graph lca comput correspond local output yes make onli small number probe requir local output form singl global solut legal vertex cover studi probe complex lcas requir work graph may arbitrarili larg degre particular lcas expect probe graph number time signific smaller maximum averag even minimum degre weak probe focus weak color problem among result show separ weak color weak color determinist lcas log weak probe suffic weak color omega left frac log log log right weak probe requir weak color strong probe consid random lcas vertex cover maxim maximum match negat result includ show graph find emph maxim match requir omega sqrt strong probe posit side design random lca find epsilon approxim emph maximum match regular graph use frac epsilon left frac epsilon right probe independ number vertic degre,"['Uriel Feige', 'Boaz Patt-Shamir', 'Shai Vardi']",['cs.DS'],False,False,False,False,False,True
517,2017-03-28T14:09:30Z,2017-03-21T21:05:27Z,http://arxiv.org/abs/1703.07432v1,http://arxiv.org/pdf/1703.07432v1,Efficient PAC Learning from the Crowd,effici pac learn crowd,"In recent years crowdsourcing has become the method of choice for gathering labeled training data for learning algorithms. Standard approaches to crowdsourcing view the process of acquiring labeled data separately from the process of learning a classifier from the gathered data. This can give rise to computational and statistical challenges. For example, in most cases there are no known computationally efficient learning algorithms that are robust to the high level of noise that exists in crowdsourced data, and efforts to eliminate noise through voting often require a large number of queries per example.   In this paper, we show how by interleaving the process of labeling and learning, we can attain computational efficiency with much less overhead in the labeling cost. In particular, we consider the realizable setting where there exists a true target function in $\mathcal{F}$ and consider a pool of labelers. When a noticeable fraction of the labelers are perfect, and the rest behave arbitrarily, we show that any $\mathcal{F}$ that can be efficiently learned in the traditional realizable PAC model can be learned in a computationally efficient manner by querying the crowd, despite high amounts of noise in the responses. Moreover, we show that this can be done while each labeler only labels a constant number of examples and the number of labels requested per example, on average, is a constant. When no perfect labelers exist, a related task is to find a set of the labelers which are good but not perfect. We show that we can identify all good labelers, when at least the majority of labelers are good.",recent year crowdsourc becom method choic gather label train data learn algorithm standard approach crowdsourc view process acquir label data separ process learn classifi gather data give rise comput statist challeng exampl case known comput effici learn algorithm robust high level nois exist crowdsourc data effort elimin nois vote often requir larg number queri per exampl paper show interleav process label learn attain comput effici much less overhead label cost particular consid realiz set exist true target function mathcal consid pool label notic fraction label perfect rest behav arbitrarili show ani mathcal effici learn tradit realiz pac model learn comput effici manner queri crowd despit high amount nois respons moreov show done label onli label constant number exampl number label request per exampl averag constant perfect label exist relat task find set label good perfect show identifi good label least major label good,"['Pranjal Awasthi', 'Avrim Blum', 'Nika Haghtalab', 'Yishay Mansour']","['cs.LG', 'cs.DS']",False,False,True,False,False,True
520,2017-03-28T14:09:34Z,2017-03-21T15:57:42Z,http://arxiv.org/abs/1703.07290v1,http://arxiv.org/pdf/1703.07290v1,Just-in-Time Batch Scheduling Problem with Two-dimensional Bin Packing   Constraints,time batch schedul problem two dimension bin pack constraint,"This paper introduces and approximately solves a multi-component problem where small rectangular items are produced from large rectangular bins via guillotine cuts. An item is characterized by its width, height, due date, and earliness and tardiness penalties per unit time. Each item induces a cost that is proportional to its earliness and tardiness. Items cut from the same bin form a batch, whose processing and completion times depend on its assigned items. The items of a batch have the completion time of their bin. The objective is to find a cutting plan that minimizes the weighted sum of earliness and tardiness penalties. We address this problem via a constraint programming based heuristic (CP) and an agent based modelling heuristic (AB). CP is an impact-based search strategy, implemented in the general-purpose solver IBM CP Optimizer. AB is constructive. It builds a solution through repeated negotiations between the set of agents representing the items and the set representing the bins. The agents cooperate to minimize the weighted earliness-tardiness penalties. The computational investigation shows that CP outperforms AB on small-sized instances while the opposite prevails for larger instances.",paper introduc approxim solv multi compon problem small rectangular item produc larg rectangular bin via guillotin cut item character width height due date earli tardi penalti per unit time item induc cost proport earli tardi item cut bin form batch whose process complet time depend assign item item batch complet time bin object find cut plan minim weight sum earli tardi penalti address problem via constraint program base heurist cp agent base model heurist ab cp impact base search strategi implement general purpos solver ibm cp optim ab construct build solut repeat negoti set agent repres item set repres bin agent cooper minim weight earli tardi penalti comput investig show cp outperform ab small size instanc opposit prevail larger instanc,"['S. Polyakovskiy', 'A. Makarowsky', ""R. M'Hallah""]",['cs.DS'],False,False,False,False,False,True
522,2017-03-28T14:09:34Z,2017-03-21T14:39:35Z,http://arxiv.org/abs/1703.07244v1,http://arxiv.org/pdf/1703.07244v1,A Hybrid Feasibility Constraints-Guided Search to the Two-Dimensional   Bin Packing Problem with Due Dates,hybrid feasibl constraint guid search two dimension bin pack problem due date,"The two-dimensional non-oriented bin packing problem with due dates packs a set of rectangular items, which may be rotated by 90 degrees, into identical rectangular bins. The bins have equal processing times. An item's lateness is the difference between its due date and the completion time of its bin. The problem packs all items without overlap as to minimize maximum lateness Lmax.   The paper proposes a tight lower bound that enhances an existing bound on Lmax for 24.07% of the benchmark instances and matches it in 30.87% cases. In addition, it models the problem using mixed integer programming (MIP), and solves small-sized instances exactly using CPLEX. It approximately solves larger-sized instances using a two-stage heuristic. The first stage constructs an initial solution via a first-fit heuristic that applies an iterative constraint programming (CP)-based neighborhood search. The second stage, which is iterative too, approximately solves a series of assignment low-level MIPs that are guided by feasibility constraints. It then enhances the solution via a high-level random local search. The approximate approach improves existing upper bounds by 27.45% on average, and obtains the optimum for 33.93% of the instances. Overall, the exact and approximate approaches identify the optimum for 39.07% cases.   The proposed approach is applicable to complex problems. It applies CP and MIP sequentially, while exploring their advantages, and hybridizes heuristic search with MIP. It embeds a new lookahead strategy that guards against infeasible search directions and constrains the search to improving directions only; thus, differs from traditional lookahead beam searches.",two dimension non orient bin pack problem due date pack set rectangular item may rotat degre ident rectangular bin bin equal process time item late differ due date complet time bin problem pack item without overlap minim maximum late lmax paper propos tight lower bound enhanc exist bound lmax benchmark instanc match case addit model problem use mix integ program mip solv small size instanc exact use cplex approxim solv larger size instanc use two stage heurist first stage construct initi solut via first fit heurist appli iter constraint program cp base neighborhood search second stage iter approxim solv seri assign low level mip guid feasibl constraint enhanc solut via high level random local search approxim approach improv exist upper bound averag obtain optimum instanc overal exact approxim approach identifi optimum case propos approach applic complex problem appli cp mip sequenti explor advantag hybrid heurist search mip emb new lookahead strategi guard infeas search direct constrain search improv direct onli thus differ tradit lookahead beam search,"['S. Polyakovskiy', ""R. M'Hallah""]",['cs.DS'],False,False,False,False,False,True
523,2017-03-28T14:09:34Z,2017-03-21T09:37:16Z,http://arxiv.org/abs/1703.07107v1,http://arxiv.org/pdf/1703.07107v1,On the Interplay between Strong Regularity and Graph Densification,interplay strong regular graph densif,"In this paper we analyze the practical implications of Szemer\'edi's regularity lemma in the preservation of metric information contained in large graphs. To this end, we present a heuristic algorithm to find regular partitions. Our experiments show that this method is quite robust to the natural sparsification of proximity graphs. In addition, this robustness can be enforced by graph densification.",paper analyz practic implic szemer edi regular lemma preserv metric inform contain larg graph end present heurist algorithm find regular partit experi show method quit robust natur sparsif proxim graph addit robust enforc graph densif,"['Marco Fiorucci', 'Alessandro Torcinovich', 'Manuel Curado', 'Francisco Escolano', 'Marcello Pelillo']","['cs.DS', 'cs.CV']",False,False,False,False,False,True
524,2017-03-28T14:09:34Z,2017-03-20T11:17:39Z,http://arxiv.org/abs/1703.06680v1,http://arxiv.org/pdf/1703.06680v1,Parallel Sort-Based Matching for Data Distribution Management on   Shared-Memory Multiprocessors,parallel sort base match data distribut manag share memori multiprocessor,"In this paper we consider the problem of identifying intersections between two sets of d-dimensional axis-parallel rectangles. This is a common problem that arises in many agent-based simulation studies, and is of central importance in the context of High Level Architecture (HLA), where it is at the core of the Data Distribution Management (DDM) service. Several realizations of the DDM service have been proposed; however, many of them are either inefficient or inherently sequential. These are serious limitations since multicore processors are now ubiquitous, and DDM algorithms -- being CPU-intensive -- could benefit from additional computing power. We propose a parallel version of the Sort-Based Matching algorithm for shared-memory multiprocessors. Sort-Based Matching is one of the most efficient serial algorithms for the DDM problem, but is quite difficult to parallelize due to data dependencies. We describe the algorithm and compute its asymptotic running time; we complete the analysis by assessing its performance and scalability through extensive experiments on two commodity multicore systems based on a dual socket Intel Xeon processor, and a single socket Intel Core i7 processor.",paper consid problem identifi intersect two set dimension axi parallel rectangl common problem aris mani agent base simul studi central import context high level architectur hla core data distribut manag ddm servic sever realize ddm servic propos howev mani either ineffici inher sequenti serious limit sinc multicor processor ubiquit ddm algorithm cpu intens could benefit addit comput power propos parallel version sort base match algorithm share memori multiprocessor sort base match one effici serial algorithm ddm problem quit difficult parallel due data depend describ algorithm comput asymptot run time complet analysi assess perform scalabl extens experi two commod multicor system base dual socket intel xeon processor singl socket intel core processor,"['Moreno Marzolla', ""Gabriele D'Angelo""]","['cs.DC', 'cs.DS', 'cs.MA']",False,False,False,False,False,True
525,2017-03-28T14:09:34Z,2017-03-20T09:34:51Z,http://arxiv.org/abs/1703.06644v1,http://arxiv.org/pdf/1703.06644v1,Reoptimization of the Closest Substring Problem under Pattern Length   Modification,reoptim closest substr problem pattern length modif,"This study investigates whether reoptimization can help in solving the closest substring problem. We are dealing with the following reoptimization scenario. Suppose, we have an optimal l-length closest substring of a given set of sequences S. How can this information be beneficial in obtaining an (l+k)-length closest substring for S? In this study, we show that the problem is still computationally hard even with k=1. We present greedy approximation algorithms that make use of the given information and prove that it has an additive error that grows as the parameter k increases. Furthermore, we present hard instances for each algorithm to show that the computed approximation ratio is tight. We also show that we can slightly improve the running-time of the existing polynomial-time approximation scheme (PTAS) for the original problem through reoptimization.",studi investig whether reoptim help solv closest substr problem deal follow reoptim scenario suppos optim length closest substr given set sequenc inform benefici obtain length closest substr studi show problem still comput hard even present greedi approxim algorithm make use given inform prove addit error grow paramet increas furthermor present hard instanc algorithm show comput approxim ratio tight also show slight improv run time exist polynomi time approxim scheme ptas origin problem reoptim,"['Jhoirene B. Clemente', 'Henry N. Adorna']","['cs.DS', '68W25', 'G.2.1']",False,False,False,False,False,True
526,2017-03-28T14:09:34Z,2017-03-18T18:12:17Z,http://arxiv.org/abs/1703.06327v1,http://arxiv.org/pdf/1703.06327v1,Spectrum Estimation from a Few Entries,spectrum estim entri,"Singular values of a data in a matrix form provide insights on the structure of the data, the effective dimensionality, and the choice of hyper-parameters on higher-level data analysis tools. However, in many practical applications such as collaborative filtering and network analysis, we only get a partial observation. Under such scenarios, we consider the fundamental problem of recovering spectral properties of the underlying matrix from a sampling of its entries. We are particularly interested in directly recovering the spectrum, which is the set of singular values, and also in sample-efficient approaches for recovering a spectral sum function, which is an aggregate sum of the same function applied to each of the singular values. We propose first estimating the Schatten $k$-norms of a matrix, and then applying Chebyshev approximation to the spectral sum function or applying moment matching in Wasserstein distance to recover the singular values. The main technical challenge is in accurately estimating the Schatten norms from a sampling of a matrix. We introduce a novel unbiased estimator based on counting small structures in a graph and provide guarantees that match its empirical performance. Our theoretical analysis shows that Schatten norms can be recovered accurately from strictly smaller number of samples compared to what is needed to recover the underlying low-rank matrix. Numerical experiments suggest that we significantly improve upon a competing approach of using matrix completion methods.",singular valu data matrix form provid insight structur data effect dimension choic hyper paramet higher level data analysi tool howev mani practic applic collabor filter network analysi onli get partial observ scenario consid fundament problem recov spectral properti matrix sampl entri particular interest direct recov spectrum set singular valu also sampl effici approach recov spectral sum function aggreg sum function appli singular valu propos first estim schatten norm matrix appli chebyshev approxim spectral sum function appli moment match wasserstein distanc recov singular valu main technic challeng accur estim schatten norm sampl matrix introduc novel unbias estim base count small structur graph provid guarante match empir perform theoret analysi show schatten norm recov accur strict smaller number sampl compar need recov low rank matrix numer experi suggest signific improv upon compet approach use matrix complet method,"['Ashish Khetan', 'Sewoong Oh']","['stat.ML', 'cs.DS', 'cs.LG', 'cs.NA']",False,False,False,False,False,True
527,2017-03-28T14:09:34Z,2017-03-18T17:21:14Z,http://arxiv.org/abs/1703.06320v1,http://arxiv.org/pdf/1703.06320v1,Hardware-Efficient Schemes of Quaternion Multiplying Units for 2D   Discrete Quaternion Fourier Transform Processors,hardwar effici scheme quaternion multipli unit discret quaternion fourier transform processor,"In this paper, we offer and discuss three efficient structural solutions for the hardware-oriented implementation of discrete quaternion Fourier transform basic operations with reduced implementation complexities. The first solution: a scheme for calculating sq product, the second solution: a scheme for calculating qt product, and the third solution: a scheme for calculating sqt product, where s is a so-called i-quaternion, t is an j-quaternion, and q is an usual quaternion. The direct multiplication of two usual quaternions requires 16 real multiplications (or two-operand multipliers in the case of fully parallel hardware implementation) and 12 real additions (or binary adders). At the same time, our solutions allow to design the computation units, which consume only 6 multipliers plus 6 two input adders for implementation of sq or qt basic operations and 9 binary multipliers plus 6 two-input adders and 4 four-input adders for implementation of sqt basic operation.",paper offer discuss three effici structur solut hardwar orient implement discret quaternion fourier transform basic oper reduc implement complex first solut scheme calcul sq product second solut scheme calcul qt product third solut scheme calcul sqt product call quaternion quaternion usual quaternion direct multipl two usual quaternion requir real multipl two operand multipli case fulli parallel hardwar implement real addit binari adder time solut allow design comput unit consum onli multipli plus two input adder implement sq qt basic oper binari multipli plus two input adder four input adder implement sqt basic oper,"['Aleksandr Cariow', 'Galina Cariowa', 'Marina Chicheva']","['cs.DS', 'cs.AR', '65T50, 15A04, 15A66, 15A66, 15A69, 03D15, 65Y20, 65Y10', 'F.2.1; I.1.2; C.1.4; C.3']",False,False,False,False,False,True
530,2017-03-28T14:09:38Z,2017-03-17T16:18:31Z,http://arxiv.org/abs/1703.06074v1,http://arxiv.org/pdf/1703.06074v1,Robust Assignments with Vulnerable Nodes,robust assign vulner node,"Various real-life planning problems require making upfront decisions before all parameters of the problem have been disclosed. An important special case of such problem especially arises in scheduling and staff rostering problems, where a set of tasks needs to be assigned to an available set of resources (personnel or machines), in a way that each task is assigned to one resource, while no task is allowed to share a resource with another task. In its nominal form, the resulting computational problem reduces to the well-known assignment problem that can be modeled as matching problems on bipartite graphs.   In recent work \cite{adjiashvili_bindewald_michaels_icalp2016}, a new robust model for the assignment problem was introduced that can deal with situations in which certain resources, i.e.\ nodes or edges of the underlying bipartite graph, are vulnerable and may become unavailable after a solution has been chosen. In the original version from \cite{adjiashvili_bindewald_michaels_icalp2016} the resources subject to uncertainty are the edges of the underlying bipartite graph.   In this follow-up work, we complement our previous study by considering nodes as being vulnerable, instead of edges. The goal is now to choose a minimum-cost collection of nodes such that, if any vulnerable node becomes unavailable, the remaining part of the solution still contains sufficient nodes to perform all tasks. From a practical point of view, such type of unavailability is interesting as it is typically caused e.g.\ by an employee's sickness, or machine failure. We present algorithms and hardness of approximation results for several variants of the problem.",various real life plan problem requir make upfront decis befor paramet problem disclos import special case problem especi aris schedul staff roster problem set task need assign avail set resourc personnel machin way task assign one resourc task allow share resourc anoth task nomin form result comput problem reduc well known assign problem model match problem bipartit graph recent work cite adjiashvili bindewald michael icalp new robust model assign problem introduc deal situat certain resourc node edg bipartit graph vulner may becom unavail solut chosen origin version cite adjiashvili bindewald michael icalp resourc subject uncertainti edg bipartit graph follow work complement previous studi consid node vulner instead edg goal choos minimum cost collect node ani vulner node becom unavail remain part solut still contain suffici node perform task practic point view type unavail interest typic caus employe sick machin failur present algorithm hard approxim result sever variant problem,"['David Adjiashvili', 'Viktor Bindewald', 'Dennis Michaels']","['cs.DS', 'cs.DM', '90C27', 'I.1.2; G.2.2; G.1.6']",False,False,False,False,False,True
531,2017-03-28T14:09:38Z,2017-03-17T16:08:23Z,http://arxiv.org/abs/1703.06065v1,http://arxiv.org/pdf/1703.06065v1,Block CUR : Decomposing Large Distributed Matrices,block cur decompos larg distribut matric,"A common problem in large-scale data analysis is to approximate a matrix using a combination of specifically sampled rows and columns, known as CUR decomposition. Unfortunately, in many real-world environments, the ability to sample specific individual rows or columns of the matrix is limited by either system constraints or cost. In this paper, we consider matrix approximation by sampling predefined blocks of columns (or rows) from the matrix. This regime is commonly found when data is distributed across multiple nodes in a compute cluster, where such blocks correspond to columns (or rows) of the matrix stored on the same node, which can be retrieved with much less overhead than retrieving individual columns stored across different nodes. We propose a novel algorithm for sampling useful column blocks and provide guarantees for the quality of the approximation. We demonstrate the practical utility of this algorithm for computing the block CUR decomposition of large matrices in a distributed setting using Apache Spark. Using our proposed block CUR algorithms, we can achieve a significant speed-up compared to a regular CUR decomposition with the same quality of approximation.",common problem larg scale data analysi approxim matrix use combin specif sampl row column known cur decomposit unfortun mani real world environ abil sampl specif individu row column matrix limit either system constraint cost paper consid matrix approxim sampl predefin block column row matrix regim common found data distribut across multipl node comput cluster block correspond column row matrix store node retriev much less overhead retriev individu column store across differ node propos novel algorithm sampl use column block provid guarante qualiti approxim demonstr practic util algorithm comput block cur decomposit larg matric distribut set use apach spark use propos block cur algorithm achiev signific speed compar regular cur decomposit qualiti approxim,"['Urvashi Oswal', 'Swayambhoo Jain', 'Kevin S. Xu', 'Brian Eriksson']","['stat.ML', 'cs.DC', 'cs.DS', 'cs.LG']",False,False,False,False,False,True
532,2017-03-28T14:09:38Z,2017-03-17T15:56:50Z,http://arxiv.org/abs/1703.06061v1,http://arxiv.org/pdf/1703.06061v1,Approximation ratio of RePair,approxim ratio repair,"In a seminal paper of Charikar et al.~on the smallest grammar problem, the authors derive upper and lower bounds on the approximation ratios for several grammar-based compressors. Here we improve the lower bound for the famous {\sf RePair} algorithm from $\Omega(\sqrt{\log n})$ to $\Omega(\log n/\log\log n)$. The family of words used in our proof is defined over a binary alphabet, while the lower bound from Charikar et al. needs an alphabet of logarithmic size in the length of the provided words.",semin paper charikar et al smallest grammar problem author deriv upper lower bound approxim ratio sever grammar base compressor improv lower bound famous sf repair algorithm omega sqrt log omega log log log famili word use proof defin binari alphabet lower bound charikar et al need alphabet logarithm size length provid word,"['Danny Hucke', 'Artur Jez', 'Markus Lohrey']","['cs.DS', 'F.2.2, E.4']",False,False,False,False,False,True
533,2017-03-28T14:09:38Z,2017-03-21T12:29:29Z,http://arxiv.org/abs/1703.06053v2,http://arxiv.org/pdf/1703.06053v2,Fast Non-Monotone Submodular Maximisation Subject to a Matroid   Constraint,fast non monoton submodular maximis subject matroid constraint,"In this work we present the first practical $\left(\frac{1}{e}-\epsilon\right)$-approximation algorithm to maximise a general non-negative submodular function subject to a matroid constraint. Our algorithm is based on combining the decreasing-threshold procedure of Badanidiyuru and Vondrak (SODA 2014) with a smoother version of the measured continuous greedy algorithm of Feldman et al. (FOCS 2011). This enables us to obtain an algorithm that requires $O(\frac{nr^2}{\epsilon^4} \big(\frac{a+b}{a}\big)^2 \log^2({\frac{n}{\epsilon}}))$ value oracle calls, where $n$ is the cardinality of the ground set, $r$ is the matroid rank, and $ b, a \in \mathbb{R}^+$ are the absolute values of the minimum and maximum marginal values that the function $f$ can take i.e.: $ -b \leq f_S(i) \leq a$, for all $i\in E$ and $S\subseteq E$, (here, $E$ is the ground set). The additional value oracle calls with respect to the work of Badanidiyuru and Vondrak come from the greater spread in the sampling of the multilinear extension that the possibility of negative marginal values introduce.",work present first practic left frac epsilon right approxim algorithm maximis general non negat submodular function subject matroid constraint algorithm base combin decreas threshold procedur badanidiyuru vondrak soda smoother version measur continu greedi algorithm feldman et al foc enabl us obtain algorithm requir frac nr epsilon big frac big log frac epsilon valu oracl call cardin ground set matroid rank mathbb absolut valu minimum maximum margin valu function take leq leq subseteq ground set addit valu oracl call respect work badanidiyuru vondrak come greater spread sampl multilinear extens possibl negat margin valu introduc,"['Pau Segui-Gasco', 'Hyo-Sang Shin']",['cs.DS'],False,False,False,False,False,True
534,2017-03-28T14:09:38Z,2017-03-17T15:08:17Z,http://arxiv.org/abs/1703.06048v1,http://arxiv.org/pdf/1703.06048v1,An FPTAS for the Knapsack Problem with Parametric Weights,fptas knapsack problem parametr weight,"In this paper, we investigate the parametric weight knapsack problem, in which the item weights are affine functions of the form $w_i(\lambda) = a_i + \lambda \cdot b_i$ for $i \in \{1,\ldots,n\}$ depending on a real-valued parameter $\lambda$. The aim is to provide a solution for all values of the parameter. It is well-known that any exact algorithm for the problem may need to output an exponential number of knapsack solutions. We present the first fully polynomial-time approximation scheme (FPTAS) for the problem that, for any desired precision $\varepsilon \in (0,1)$, computes $(1-\varepsilon)$-approximate solutions for all values of the parameter. Our FPTAS is based on two different approaches and achieves a running time of $\mathcal{O}(n^3/\varepsilon^2 \cdot \min\{ \log^2 P, n^2 \} \cdot \min\{\log M, n \log (n/\varepsilon) / \log(n \log (n/\varepsilon) )\})$ where $P$ is an upper bound on the optimal profit and $M := \max\{W, n \cdot \max\{a_i,b_i: i \in \{1,\ldots,n\}\}\}$ for a knapsack with capacity $W$.",paper investig parametr weight knapsack problem item weight affin function form lambda lambda cdot ldot depend real valu paramet lambda aim provid solut valu paramet well known ani exact algorithm problem may need output exponenti number knapsack solut present first fulli polynomi time approxim scheme fptas problem ani desir precis varepsilon comput varepsilon approxim solut valu paramet fptas base two differ approach achiev run time mathcal varepsilon cdot min log cdot min log log varepsilon log log varepsilon upper bound optim profit max cdot max ldot knapsack capac,"['Michael Holzhauser', 'Sven O. Krumke']","['cs.DS', 'cs.CC', 'math.OC']",False,False,False,False,False,True
535,2017-03-28T14:09:38Z,2017-03-17T14:53:55Z,http://arxiv.org/abs/1703.06040v1,http://arxiv.org/pdf/1703.06040v1,Towards a Topology-Shape-Metrics Framework for Ortho-Radial Drawings,toward topolog shape metric framework ortho radial draw,"Ortho-Radial drawings are a generalization of orthogonal drawings to grids that are formed by concentric circles and straight-line spokes emanating from the circles' center. Such drawings have applications in schematic graph layouts, e.g., for metro maps and destination maps.   A plane graph is a planar graph with a fixed planar embedding. We give a combinatorial characterization of the plane graphs that admit a planar ortho-radial drawing without bends. Previously, such a characterization was only known for paths, cycles, and theta graphs, and in the special case of rectangular drawings for cubic graphs, where the contour of each face is required to be a rectangle.   The characterization is expressed in terms of an ortho-radial representation that, similar to Tamassia's orthogonal representations for orthogonal drawings describes such a drawing combinatorially in terms of angles around vertices and bends on the edges. In this sense our characterization can be seen as a first step towards generalizing the Topology-Shape-Metrics framework of Tamassia to ortho-radial drawings.",ortho radial draw general orthogon draw grid form concentr circl straight line spoke eman circl center draw applic schemat graph layout metro map destin map plane graph planar graph fix planar embed give combinatori character plane graph admit planar ortho radial draw without bend previous character onli known path cycl theta graph special case rectangular draw cubic graph contour face requir rectangl character express term ortho radial represent similar tamassia orthogon represent orthogon draw describ draw combinatori term angl around vertic bend edg sens character seen first step toward general topolog shape metric framework tamassia ortho radial draw,"['Lukas Barth', 'Benjamin Niedermann', 'Ignaz Rutter', 'Matthias Wolf']","['cs.DM', 'cs.DS']",False,False,False,False,False,True
536,2017-03-28T14:09:38Z,2017-03-17T12:57:18Z,http://arxiv.org/abs/1703.05997v1,http://arxiv.org/pdf/1703.05997v1,Connection Scan Algorithm,connect scan algorithm,"We introduce the Connection Scan Algorithm (CSA) to efficiently answer queries to timetable information systems. The input consists, in the simplest setting, of a source position and a desired target position. The output consist is a sequence of vehicles such as trains or buses that a traveler should take to get from the source to the target. We study several problem variations such as the earliest arrival and profile problems. We present algorithm variants that only optimize the arrival time or additionally optimize the number of transfers in the Pareto sense. An advantage of CSA is that is can easily adjust to changes in the timetable, allowing the easy incorporation of known vehicle delays. We additionally introduce the Minimum Expected Arrival Time (MEAT) problem to handle possible, uncertain, future vehicle delays. We present a solution to the MEAT problem that is based upon CSA. Finally, we extend CSA using the multilevel overlay paradigm to answer complex queries on nation-wide integrated timetables with trains and buses.",introduc connect scan algorithm csa effici answer queri timet inform system input consist simplest set sourc posit desir target posit output consist sequenc vehicl train buse travel take get sourc target studi sever problem variat earliest arriv profil problem present algorithm variant onli optim arriv time addit optim number transfer pareto sens advantag csa easili adjust chang timet allow easi incorpor known vehicl delay addit introduc minimum expect arriv time meat problem handl possibl uncertain futur vehicl delay present solut meat problem base upon csa final extend csa use multilevel overlay paradigm answer complex queri nation wide integr timet train buse,"['Julian Dibbelt', 'Thomas Pajor', 'Ben Strasser', 'Dorothea Wagner']",['cs.DS'],False,False,False,False,False,True
537,2017-03-28T14:09:38Z,2017-03-16T13:10:29Z,http://arxiv.org/abs/1703.05598v1,http://arxiv.org/pdf/1703.05598v1,Linear-Time Algorithm for Maximum-Cardinality Matching on   Cocomparability Graphs,linear time algorithm maximum cardin match cocompar graph,"Finding maximum-cardinality matchings in undirected graphs is arguably one of the most central graph problems. For general m-edge and n-vertex graphs, it is well-known to be solvable in $O(m \sqrt{n})$ time. We develop the first linear-time algorithm to find maximum-cardinality matchings on cocomparability graphs, a prominent subclass of perfect graphs that contains interval graphs as well as permutation graphs. Our algorithm is based on the recently discovered Lexicographic Depth First Search (LDFS).",find maximum cardin match undirect graph arguabl one central graph problem general edg vertex graph well known solvabl sqrt time develop first linear time algorithm find maximum cardin match cocompar graph promin subclass perfect graph contain interv graph well permut graph algorithm base recent discov lexicograph depth first search ldfs,"['George B. Mertzios', 'André Nichterlein', 'Rolf Niedermeier']","['cs.DS', 'F.2.2']",False,False,False,False,False,True
542,2017-03-28T14:09:43Z,2017-03-15T23:02:07Z,http://arxiv.org/abs/1703.05418v1,http://arxiv.org/pdf/1703.05418v1,A Local Algorithm for the Sparse Spanning Graph Problem,local algorithm spars span graph problem,"Constructing a sparse \emph{spanning subgraph} is a fundamental primitive in graph theory. In this paper, we study this problem in the Centralized Local model, where the goal is to decide whether an edge is part of the spanning subgraph by examining only a small part of the input; yet, answers must be globally consistent and independent of prior queries.   Unfortunately, maximally sparse spanning subgraphs, i.e., spanning trees, cannot be constructed efficiently in this model. Therefore, we settle for a spanning subgraph containing at most $(1+\varepsilon)n$ edges (where $n$ is the number of vertices and $\varepsilon$ is a given approximation/sparsity parameter). We achieve query complexity of $\tilde{O}(poly(\Delta/\varepsilon)n^{2/3})$,\footnote{$\tilde{O}$-notation hides polylogarithmic factors in $n$.} where $\Delta$ is the maximum degree of the input graph. Our algorithm is the first to do so on arbitrary graphs. Moreover, we achieve the additional property that our algorithm outputs a \emph{spanner,} i.e., distances are approximately preserved. With high probability, for each deleted edge there is a path of $O(poly(\Delta/\varepsilon)\log^2 n)$ hops in the output that connects its endpoints.",construct spars emph span subgraph fundament primit graph theori paper studi problem central local model goal decid whether edg part span subgraph examin onli small part input yet answer must global consist independ prior queri unfortun maxim spars span subgraph span tree cannot construct effici model therefor settl span subgraph contain varepsilon edg number vertic varepsilon given approxim sparsiti paramet achiev queri complex tild poli delta varepsilon footnot tild notat hide polylogarithm factor delta maximum degre input graph algorithm first arbitrari graph moreov achiev addit properti algorithm output emph spanner distanc approxim preserv high probabl delet edg path poli delta varepsilon log hop output connect endpoint,"['Christoph Lenzen', 'Reut Levi']",['cs.DS'],False,False,False,False,False,True
543,2017-03-28T14:09:43Z,2017-03-15T15:10:16Z,http://arxiv.org/abs/1703.05199v1,http://arxiv.org/pdf/1703.05199v1,Optimal Unateness Testers for Real-Valued Functions: Adaptivity Helps,optim unat tester real valu function adapt help,"We study the problem of testing unateness of functions $f:\{0,1\}^d \to \mathbb{R}.$ We give a $O(\frac{d}{\epsilon} \cdot \log\frac{d}{\epsilon})$-query nonadaptive tester and a $O(\frac{d}{\epsilon})$-query adaptive tester and show that both testers are optimal for a fixed distance parameter $\epsilon$. Previously known unateness testers worked only for Boolean functions, and their query complexity had worse dependence on the dimension both for the adaptive and the nonadaptive case. Moreover, no lower bounds for testing unateness were known. We also generalize our results to obtain optimal unateness testers for functions $f:[n]^d \to \mathbb{R}$.   Our results establish that adaptivity helps with testing unateness of real-valued functions on domains of the form $\{0,1\}^d$ and, more generally, $[n]^d$. This stands in contrast to the situation for monotonicity testing where there is no adaptivity gap for functions $f:[n]^d \to \mathbb{R}$.",studi problem test unat function mathbb give frac epsilon cdot log frac epsilon queri nonadapt tester frac epsilon queri adapt tester show tester optim fix distanc paramet epsilon previous known unat tester work onli boolean function queri complex wors depend dimens adapt nonadapt case moreov lower bound test unat known also general result obtain optim unat tester function mathbb result establish adapt help test unat real valu function domain form general stand contrast situat monoton test adapt gap function mathbb,"['Roksana Baleshzar', 'Deeparnab Chakrabarty', 'Ramesh Krishnan S. Pallavoor', 'Sofya Raskhodnikova', 'C. Seshadhri']","['cs.DS', 'cs.DM']",False,False,False,False,False,True
544,2017-03-28T14:09:43Z,2017-03-15T14:01:21Z,http://arxiv.org/abs/1703.05160v1,http://arxiv.org/pdf/1703.05160v1,A New Unbiased and Efficient Class of LSH-Based Samplers and Estimators   for Partition Function Computation in Log-Linear Models,new unbias effici class lsh base sampler estim partit function comput log linear model,"Log-linear models are arguably the most successful class of graphical models for large-scale applications because of their simplicity and tractability. Learning and inference with these models require calculating the partition function, which is a major bottleneck and intractable for large state spaces. Importance Sampling (IS) and MCMC-based approaches are lucrative. However, the condition of having a ""good"" proposal distribution is often not satisfied in practice.   In this paper, we add a new dimension to efficient estimation via sampling. We propose a new sampling scheme and an unbiased estimator that estimates the partition function accurately in sub-linear time. Our samples are generated in near-constant time using locality sensitive hashing (LSH), and so are correlated and unnormalized. We demonstrate the effectiveness of our proposed approach by comparing the accuracy and speed of estimating the partition function against other state-of-the-art estimation techniques including IS and the efficient variant of Gumbel-Max sampling. With our efficient sampling scheme, we accurately train real-world language models using only 1-2% of computations.",log linear model arguabl success class graphic model larg scale applic becaus simplic tractabl learn infer model requir calcul partit function major bottleneck intract larg state space import sampl mcmc base approach lucrat howev condit good propos distribut often satisfi practic paper add new dimens effici estim via sampl propos new sampl scheme unbias estim estim partit function accur sub linear time sampl generat near constant time use local sensit hash lsh correl unnorm demonstr effect propos approach compar accuraci speed estim partit function state art estim techniqu includ effici variant gumbel max sampl effici sampl scheme accur train real world languag model use onli comput,"['Ryan Spring', 'Anshumali Shrivastava']","['stat.ML', 'cs.DB', 'cs.DS', 'cs.LG']",False,False,False,False,False,True
545,2017-03-28T14:09:43Z,2017-03-15T13:51:23Z,http://arxiv.org/abs/1703.05156v1,http://arxiv.org/pdf/1703.05156v1,Complexity Dichotomies for the Minimum F-Overlay Problem,complex dichotomi minimum overlay problem,"For a (possibly infinite) fixed family of graphs F, we say that a graph G overlays F on a hypergraph H if V(H) is equal to V(G) and the subgraph of G induced by every hyperedge of H contains some member of F as a spanning subgraph.While it is easy to see that the complete graph on  V(H)  overlays F on a hypergraph H whenever the problem admits a solution, the Minimum F-Overlay problem asks for such a graph with the minimum number of edges.This problem allows to generalize some natural problems which may arise in practice. For instance, if the family F contains all connected graphs, then Minimum F-Overlay corresponds to the Minimum Connectivity Inference problem (also known as Subset Interconnection Design problem) introduced for the low-resolution reconstruction of macro-molecular assembly in structural biology, or for the design of networks.Our main contribution is a strong dichotomy result regarding the polynomial vs. NP-hard status with respect to the considered family F. Roughly speaking, we show that the easy cases one can think of (e.g. when edgeless graphs of the right sizes are in F, or if F contains only cliques) are the only families giving rise to a polynomial problem: all others are NP-complete.We then investigate the parameterized complexity of the problem and give similar sufficient conditions on F that give rise to W[1]-hard, W[2]-hard or FPT problems when the parameter is the size of the solution.This yields an FPT/W[1]-hard dichotomy for a relaxed problem, where every hyperedge of H must contain some member of F as a (non necessarily spanning) subgraph.",possibl infinit fix famili graph say graph overlay hypergraph equal subgraph induc everi hyperedg contain member span subgraph easi see complet graph overlay hypergraph whenev problem admit solut minimum overlay problem ask graph minimum number edg problem allow general natur problem may aris practic instanc famili contain connect graph minimum overlay correspond minimum connect infer problem also known subset interconnect design problem introduc low resolut reconstruct macro molecular assembl structur biolog design network main contribut strong dichotomi result regard polynomi vs np hard status respect consid famili rough speak show easi case one think edgeless graph right size contain onli cliqu onli famili give rise polynomi problem np complet investig parameter complex problem give similar suffici condit give rise hard hard fpt problem paramet size solut yield fpt hard dichotomi relax problem everi hyperedg must contain member non necessarili span subgraph,"['Nathann Cohen', 'Frédéric Havet', 'Dorian Mazauric', 'Ignasi Sau', 'Rémi Watrigant']","['cs.DS', 'cs.CC']",False,False,False,False,False,True
548,2017-03-28T14:09:43Z,2017-03-15T06:21:59Z,http://arxiv.org/abs/1703.04954v1,http://arxiv.org/pdf/1703.04954v1,Faster STR-IC-LCS computation via RLE,faster str ic lcs comput via rle,"The constrained LCS problem asks one to find a longest common subsequence of two input strings $A$ and $B$ with some constraints. The STR-IC-LCS problem is a variant of the constrained LCS problem, where the solution must include a given constraint string $C$ as a substring. Given two strings $A$ and $B$ of respective lengths $M$ and $N$, and a constraint string $C$ of length at most $\min\{M, N\}$, the best known algorithm for the STR-IC-LCS problem, proposed by Deorowicz~({\em Inf. Process. Lett.}, 11:423--426, 2012), runs in $O(MN)$ time. In this work, we present an $O(mN + nM)$-time solution to the STR-IC-LCS problem, where $m$ and $n$ denote the sizes of the run-length encodings of $A$ and $B$, respectively. Since $m \leq M$ and $n \leq N$ always hold, our algorithm is always as fast as Deorowicz's algorithm, and is faster when input strings are compressible via RLE.",constrain lcs problem ask one find longest common subsequ two input string constraint str ic lcs problem variant constrain lcs problem solut must includ given constraint string substr given two string respect length constraint string length min best known algorithm str ic lcs problem propos deorowicz em inf process lett run mn time work present mn nm time solut str ic lcs problem denot size run length encod respect sinc leq leq alway hold algorithm alway fast deorowicz algorithm faster input string compress via rle,"['Keita Kuboi', 'Yuta Fujishige', 'Shunsuke Inenaga', 'Hideo Bannai', 'Masayuki Takeda']",['cs.DS'],False,False,False,False,False,True
549,2017-03-28T14:09:43Z,2017-03-14T23:06:33Z,http://arxiv.org/abs/1703.04814v1,http://arxiv.org/pdf/1703.04814v1,Near-Optimal Compression for the Planar Graph Metric,near optim compress planar graph metric,"The Planar Graph Metric Compression Problem is to compactly encode the distances among $k$ nodes in a planar graph of size $n$. Two na\""ive solutions are to store the graph using $O(n)$ bits, or to explicitly store the distance matrix with $O(k^2 \log{n})$ bits. The only lower bounds are from the seminal work of Gavoille, Peleg, Prennes, and Raz [SODA'01], who rule out compressions into a polynomially smaller number of bits, for {\em weighted} planar graphs, but leave a large gap for unweighted planar graphs. For example, when $k=\sqrt{n}$, the upper bound is $O(n)$ and their constructions imply an $\Omega(n^{3/4})$ lower bound. This gap is directly related to other major open questions in labelling schemes, dynamic algorithms, and compact routing.   Our main result is a new compression of the planar graph metric into $\tilde{O}(\min (k^2 , \sqrt{k\cdot n}))$ bits, which is optimal up to log factors. Our data structure breaks an $\Omega(k^2)$ lower bound of Krauthgamer, Nguyen, and Zondiner [SICOMP'14] for compression using minors, and the lower bound of Gavoille et al. for compression of weighted planar graphs. This is an unexpected and decisive proof that weights can make planar graphs inherently more complex. Moreover, we design a new {\em Subset Distance Oracle} for planar graphs with $\tilde O(\sqrt{k\cdot n})$ space, and $\tilde O(n^{3/4})$ query time.   Our work carries strong messages to related fields. In particular, the famous $O(n^{1/2})$ vs. $\Omega(n^{1/3})$ gap for distance labelling schemes in planar graphs {\em cannot} be resolved with the current lower bound techniques.",planar graph metric compress problem compact encod distanc among node planar graph size two na ive solut store graph use bit explicit store distanc matrix log bit onli lower bound semin work gavoill peleg prenn raz soda rule compress polynomi smaller number bit em weight planar graph leav larg gap unweight planar graph exampl sqrt upper bound construct impli omega lower bound gap direct relat major open question label scheme dynam algorithm compact rout main result new compress planar graph metric tild min sqrt cdot bit optim log factor data structur break omega lower bound krauthgam nguyen zondin sicomp compress use minor lower bound gavoill et al compress weight planar graph unexpect decis proof weight make planar graph inher complex moreov design new em subset distanc oracl planar graph tild sqrt cdot space tild queri time work carri strong messag relat field particular famous vs omega gap distanc label scheme planar graph em cannot resolv current lower bound techniqu,"['Amir Abboud', 'Pawel Gawrychowski', 'Shay Mozes', 'Oren Weimann']",['cs.DS'],False,False,False,False,False,True
550,2017-03-28T14:09:47Z,2017-03-14T22:16:53Z,http://arxiv.org/abs/1703.04769v1,http://arxiv.org/pdf/1703.04769v1,The Stochastic Container Relocation Problem,stochast contain reloc problem,"The Container Relocation Problem (CRP) is concerned with finding a sequence of moves of containers that minimizes the number of relocations needed to retrieve all containers, while respecting a given order of retrieval. However, the assumption of knowing the full retrieval order of containers is particularly unrealistic in real operations. This paper studies the stochastic CRP (SCRP), which relaxes this assumption. A new multi-stage stochastic model, called the batch model, is introduced, motivated, and compared with an existing model (the online model). The two main contributions are an optimal algorithm called Pruning-Best-First-Search (PBFS) and a randomized approximate algorithm called PBFS-Approximate with a bounded average error. Both algorithms, applicable in the batch and online models, are based on a new family of lower bounds for which we show some theoretical properties. Moreover, we introduce two new heuristics outperforming the best existing heuristics. Algorithms, bounds and heuristics are tested in an extensive computational section. Finally, based on strong computational evidence, we conjecture the optimality of the ""Leveling"" heuristic in a special ""no information"" case, where at any retrieval stage, any of the remaining containers is equally likely to be retrieved next.",contain reloc problem crp concern find sequenc move contain minim number reloc need retriev contain respect given order retriev howev assumpt know full retriev order contain particular unrealist real oper paper studi stochast crp scrp relax assumpt new multi stage stochast model call batch model introduc motiv compar exist model onlin model two main contribut optim algorithm call prune best first search pbfs random approxim algorithm call pbfs approxim bound averag error algorithm applic batch onlin model base new famili lower bound show theoret properti moreov introduc two new heurist outperform best exist heurist algorithm bound heurist test extens comput section final base strong comput evid conjectur optim level heurist special inform case ani retriev stage ani remain contain equal like retriev next,"['Virgile Galle', 'Setareh Borjian Boroujeni', 'Vahideh H. Manshadi', 'Cynthia Barnhart', 'Patrick Jaillet']",['cs.DS'],False,False,False,False,False,True
551,2017-03-28T14:09:47Z,2017-03-14T18:49:57Z,http://arxiv.org/abs/1703.04664v1,http://arxiv.org/pdf/1703.04664v1,Optimal Densification for Fast and Accurate Minwise Hashing,optim densif fast accur minwis hash,"Minwise hashing is a fundamental and one of the most successful hashing algorithm in the literature. Recent advances based on the idea of densification~\cite{Proc:OneHashLSH_ICML14,Proc:Shrivastava_UAI14} have shown that it is possible to compute $k$ minwise hashes, of a vector with $d$ nonzeros, in mere $(d + k)$ computations, a significant improvement over the classical $O(dk)$. These advances have led to an algorithmic improvement in the query complexity of traditional indexing algorithms based on minwise hashing. Unfortunately, the variance of the current densification techniques is unnecessarily high, which leads to significantly poor accuracy compared to vanilla minwise hashing, especially when the data is sparse. In this paper, we provide a novel densification scheme which relies on carefully tailored 2-universal hashes. We show that the proposed scheme is variance-optimal, and without losing the runtime efficiency, it is significantly more accurate than existing densification techniques. As a result, we obtain a significantly efficient hashing scheme which has the same variance and collision probability as minwise hashing. Experimental evaluations on real sparse and high-dimensional datasets validate our claims. We believe that given the significant advantages, our method will replace minwise hashing implementations in practice.",minwis hash fundament one success hash algorithm literatur recent advanc base idea densif cite proc onehashlsh icml proc shrivastava uai shown possibl comput minwis hash vector nonzero mere comput signific improv classic dk advanc led algorithm improv queri complex tradit index algorithm base minwis hash unfortun varianc current densif techniqu unnecessarili high lead signific poor accuraci compar vanilla minwis hash especi data spars paper provid novel densif scheme reli care tailor univers hash show propos scheme varianc optim without lose runtim effici signific accur exist densif techniqu result obtain signific effici hash scheme varianc collis probabl minwis hash experiment evalu real spars high dimension dataset valid claim believ given signific advantag method replac minwis hash implement practic,['Anshumali Shrivastava'],"['cs.DS', 'cs.LG']",False,False,False,False,False,True
553,2017-03-28T14:09:47Z,2017-03-13T13:31:17Z,http://arxiv.org/abs/1703.04381v1,http://arxiv.org/pdf/1703.04381v1,On the Transformation Capability of Feasible Mechanisms for Programmable   Matter,transform capabl feasibl mechan programm matter,"In this work, we study theoretical models of \emph{programmable matter} systems. The systems under consideration consist of spherical modules, kept together by magnetic forces and able to perform two minimal mechanical operations (or movements): \emph{rotate} around a neighbor and \emph{slide} over a line. In terms of modeling, there are $n$ nodes arranged in a 2-dimensional grid and forming some initial \emph{shape}. The goal is for the initial shape $A$ to \emph{transform} to some target shape $B$ by a sequence of movements. Most of the paper focuses on \emph{transformability} questions, meaning whether it is in principle feasible to transform a given shape to another. We first consider the case in which only rotation is available to the nodes. Our main result is that deciding whether two given shapes $A$ and $B$ can be transformed to each other, is in $\mathbf{P}$. We then insist on rotation only and impose the restriction that the nodes must maintain global connectivity throughout the transformation. We prove that the corresponding transformability question is in $\mathbf{PSPACE}$ and study the problem of determining the minimum \emph{seeds} that can make feasible, otherwise infeasible transformations. Next we allow both rotations and slidings and prove universality: any two connected shapes $A,B$ of the same order, can be transformed to each other without breaking connectivity. The worst-case number of movements of the generic strategy is $\Omega(n^2)$. We improve this to $O(n)$ parallel time, by a pipelining strategy, and prove optimality of both by matching lower bounds. In the last part of the paper, we turn our attention to distributed transformations. The nodes are now distributed processes able to perform communicate-compute-move rounds. We provide distributed algorithms for a general type of transformations.",work studi theoret model emph programm matter system system consider consist spheric modul kept togeth magnet forc abl perform two minim mechan oper movement emph rotat around neighbor emph slide line term model node arrang dimension grid form initi emph shape goal initi shape emph transform target shape sequenc movement paper focus emph transform question mean whether principl feasibl transform given shape anoth first consid case onli rotat avail node main result decid whether two given shape transform mathbf insist rotat onli impos restrict node must maintain global connect throughout transform prove correspond transform question mathbf pspace studi problem determin minimum emph seed make feasibl otherwis infeas transform next allow rotat slide prove univers ani two connect shape order transform without break connect worst case number movement generic strategi omega improv parallel time pipelin strategi prove optim match lower bound last part paper turn attent distribut transform node distribut process abl perform communic comput move round provid distribut algorithm general type transform,"['Othon Michail', 'George Skretas', 'Paul G. Spirakis']","['cs.DS', 'cs.DC', 'cs.RO']",False,False,True,False,False,True
554,2017-03-28T14:09:47Z,2017-03-13T02:57:50Z,http://arxiv.org/abs/1703.04230v1,http://arxiv.org/pdf/1703.04230v1,Improved approximation algorithms for $k$-connected $m$-dominating set   problems,improv approxim algorithm connect domin set problem,"A graph is $k$-connected if it has $k$ internally-disjoint paths between every pair of nodes. A subset $S$ of nodes in a graph $G$ is a $k$-connected set if the subgraph $G[S]$ induced by $S$ is $k$-connected; $S$ is an $m$-dominating set if every $v \in V \setminus S$ has at least $m$ neighbors in $S$. If $S$ is both $k$-connected and $m$-dominating then $S$ is a $k$-connected $m$-dominating set, or $(k,m)$-cds for short. In the $k$-Connected $m$-Dominating Set ($(k,m)$-CDS) problem the goal is to find a minimum weight $(k,m)$-cds in a node-weighted graph. We consider the case $m \geq k$ and obtain the following approximation ratios. For unit disc-graphs we obtain ratio $O(k\ln k)$, improving the previous ratio $O(k^2 \ln k)$. For general graphs we obtain the first non-trivial approximation ratio $O(k^2 \ln n)$.",graph connect intern disjoint path everi pair node subset node graph connect set subgraph induc connect domin set everi setminus least neighbor connect domin connect domin set cds short connect domin set cds problem goal find minimum weight cds node weight graph consid case geq obtain follow approxim ratio unit disc graph obtain ratio ln improv previous ratio ln general graph obtain first non trivial approxim ratio ln,['Zeev Nutov'],['cs.DS'],False,False,False,False,False,True
555,2017-03-28T14:09:47Z,2017-03-12T17:11:49Z,http://arxiv.org/abs/1703.04143v1,http://arxiv.org/pdf/1703.04143v1,Bernoulli Factories and Black-Box Reductions in Mechanism Design,bernoulli factori black box reduct mechan design,"We provide a polynomial time reduction from Bayesian incentive compatible mechanism design to Bayesian algorithm design for welfare maximization problems. Unlike prior results, our reduction achieves exact incentive compatibility for problems with multi-dimensional and continuous type spaces. The key technical barrier preventing exact incentive compatibility in prior black-box reductions is that repairing violations of incentive constraints requires understanding the distribution of the mechanism's output. Reductions that instead estimate the output distribution by sampling inevitably suffer from sampling error, which typically precludes exact incentive compatibility.   We overcome this barrier by employing and generalizing the computational model in the literature on Bernoulli Factories. In a Bernoulli factory problem, one is given a function mapping the bias of an ""input coin"" to that of an ""output coin"", and the challenge is to efficiently simulate the output coin given sample access to the input coin. We generalize this to the ""expectations from samples"" computational model, in which an instance is specified by a function mapping the expected values of a set of input distributions to a distribution over outcomes. The challenge is to give a polynomial time algorithm that exactly samples from the distribution over outcomes given only sample access to the input distributions. In this model, we give a polynomial time algorithm for the exponential weights: expected values of the input distributions correspond to the weights of alternatives and we wish to select an alternative with probability proportional to an exponential function of its weight. This algorithm is the key ingredient in designing an incentive compatible mechanism for bipartite matching, which can be used to make the approximately incentive compatible reduction of Hartline et al. (2015) exactly incentive compatible.",provid polynomi time reduct bayesian incent compat mechan design bayesian algorithm design welfar maxim problem unlik prior result reduct achiev exact incent compat problem multi dimension continu type space key technic barrier prevent exact incent compat prior black box reduct repair violat incent constraint requir understand distribut mechan output reduct instead estim output distribut sampl inevit suffer sampl error typic preclud exact incent compat overcom barrier employ general comput model literatur bernoulli factori bernoulli factori problem one given function map bias input coin output coin challeng effici simul output coin given sampl access input coin general expect sampl comput model instanc specifi function map expect valu set input distribut distribut outcom challeng give polynomi time algorithm exact sampl distribut outcom given onli sampl access input distribut model give polynomi time algorithm exponenti weight expect valu input distribut correspond weight altern wish select altern probabl proport exponenti function weight algorithm key ingredi design incent compat mechan bipartit match use make approxim incent compat reduct hartlin et al exact incent compat,"['Shaddin Dughmi', 'Jason Hartline', 'Robert Kleinberg', 'Rad Niazadeh']","['cs.GT', 'cs.CC', 'cs.DS', 'math.PR']",False,False,False,False,False,True
556,2017-03-28T14:09:47Z,2017-03-11T23:16:23Z,http://arxiv.org/abs/1703.04040v1,http://arxiv.org/abs/1703.04040v1,Locality-sensitive hashing of curves,local sensit hash curv,"We study data structures for storing a set of polygonal curves in ${\rm R}^d$ such that, given a query curve, we can efficiently retrieve similar curves from the set, where similarity is measured using the discrete Fr\'echet distance or the dynamic time warping distance. To this end we devise the first locality-sensitive hashing schemes for these distance measures. A major challenge is posed by the fact that these distance measures internally optimize the alignment between the curves. We give solutions for different types of alignments including constrained and unconstrained versions. For unconstrained alignments, we improve over a result by Indyk from 2002 for short curves. Let $n$ be the number of input curves and let $m$ be the maximum complexity of a curve in the input. In the particular case where $m \leq \frac{\alpha}{4d} \log n$, for some fixed $\alpha>0$, our solutions imply an approximate near-neighbor data structure for the discrete Fr\'echet distance that uses space in $O(n^{1+\alpha}\log n)$ and achieves query time in $O(n^{\alpha}\log^2 n)$ and constant approximation factor. Furthermore, our solutions provide a trade-off between approximation quality and computational performance: for any parameter $k \in [m]$, we can give a data structure that uses space in $O(2^{2k}m^{k-1} n \log n + nm)$, answers queries in $O( 2^{2k} m^{k}\log n)$ time and achieves approximation factor in $O(m/k)$.",studi data structur store set polygon curv rm given queri curv effici retriev similar curv set similar measur use discret fr echet distanc dynam time warp distanc end devis first local sensit hash scheme distanc measur major challeng pose fact distanc measur intern optim align curv give solut differ type align includ constrain unconstrain version unconstrain align improv result indyk short curv let number input curv let maximum complex curv input particular case leq frac alpha log fix alpha solut impli approxim near neighbor data structur discret fr echet distanc use space alpha log achiev queri time alpha log constant approxim factor furthermor solut provid trade approxim qualiti comput perform ani paramet give data structur use space log nm answer queri log time achiev approxim factor,"['Anne Driemel', 'Francesco Silvestri']","['cs.CG', 'cs.DS', 'cs.IR', 'F.2.2']",False,False,False,False,False,True
557,2017-03-28T14:09:47Z,2017-03-11T16:53:04Z,http://arxiv.org/abs/1703.03998v1,http://arxiv.org/pdf/1703.03998v1,The Weighted Matching Approach to Maximum Cardinality Matching,weight match approach maximum cardin match,"Several papers have achieved time $O(\sqrt n m)$ for cardinality matching, starting from first principles. This results in a long derivation. We simplify the task by employing well-known concepts for maximum weight matching. We use Edmonds' algorithm to derive the structure of shortest augmenting paths. We extend this to a complete algorithm for maximum cardinality matching in time $O(\sqrt n m)$.",sever paper achiev time sqrt cardin match start first principl result long deriv simplifi task employ well known concept maximum weight match use edmond algorithm deriv structur shortest augment path extend complet algorithm maximum cardin match time sqrt,['Harold N. Gabow'],['cs.DS'],False,False,False,False,False,True
558,2017-03-28T14:09:47Z,2017-03-11T12:24:19Z,http://arxiv.org/abs/1703.03963v1,http://arxiv.org/pdf/1703.03963v1,On Solving Travelling Salesman Problem with Vertex Requisitions,solv travel salesman problem vertex requisit,"We consider the Travelling Salesman Problem with Vertex Requisitions, where for each position of the tour at most two possible vertices are given. It is known that the problem is strongly NP-hard. The proposed algorithm for this problem has less time complexity compared to the previously known one. In particular, almost all feasible instances of the problem are solvable in O(n) time using the new algorithm, where n is the number of vertices. The developed approach also helps in fast enumeration of a neighborhood in the local search and yields an integer programming model with O(n) binary variables for the problem.",consid travel salesman problem vertex requisit posit tour two possibl vertic given known problem strong np hard propos algorithm problem less time complex compar previous known one particular almost feasibl instanc problem solvabl time use new algorithm number vertic develop approach also help fast enumer neighborhood local search yield integ program model binari variabl problem,"['Anton Eremeev', 'Yulia Kovalenko']",['cs.DS'],False,False,False,False,False,True
559,2017-03-28T14:09:47Z,2017-03-11T03:28:30Z,http://arxiv.org/abs/1703.03900v1,http://arxiv.org/pdf/1703.03900v1,Core Maintenance in Dynamic Graphs: A Parallel Approach based on   Matching,core mainten dynam graph parallel approach base match,"The core number of a vertex is a basic index depicting cohesiveness of a graph, and has been widely used in large-scale graph analytics. In this paper, we study the update of core numbers of vertices in dynamic graphs with edge insertions/deletions, which is known as the core maintenance problem. Different from previous approaches that just focus on the case of single-edge insertion/deletion and sequentially handle the edges when multiple edges are inserted/deleted, we investigate the parallelism in the core maintenance procedure. Specifically, we show that if the inserted/deleted edges constitute a matching, the core number update with respect to each inserted/deleted edge can be handled in parallel. Based on this key observation, we propose parallel algorithms for core maintenance in both cases of edge insertions and deletions. We conduct extensive experiments to evaluate the efficiency, stability, parallelism and scalability of our algorithms on different types of real-world and synthetic graphs. Comparing with sequential approaches, our algorithms can improve the core maintenance efficiency significantly.",core number vertex basic index depict cohes graph wide use larg scale graph analyt paper studi updat core number vertic dynam graph edg insert delet known core mainten problem differ previous approach focus case singl edg insert delet sequenti handl edg multipl edg insert delet investig parallel core mainten procedur specif show insert delet edg constitut match core number updat respect insert delet edg handl parallel base key observ propos parallel algorithm core mainten case edg insert delet conduct extens experi evalu effici stabil parallel scalabl algorithm differ type real world synthet graph compar sequenti approach algorithm improv core mainten effici signific,"['Na Wang', 'Dongxiao Yu', 'Hai Jin', 'Qiang-Sheng Hua', 'Xuanhua Shi', 'Xia Xie']",['cs.DS'],False,False,False,False,False,True
560,2017-03-28T14:09:51Z,2017-03-11T02:10:17Z,http://arxiv.org/abs/1703.06113v1,http://arxiv.org/pdf/1703.06113v1,Toward an enumeration of unlabeled trees,toward enumer unlabel tree,"We present an algorithm that, on input $n$, lists every unlabeled tree of order $n$.",present algorithm input list everi unlabel tree order,['Pedro Recuero'],"['cs.DS', 'math.CO', '05C05']",False,False,False,False,False,True
562,2017-03-28T14:09:51Z,2017-03-10T21:51:50Z,http://arxiv.org/abs/1703.03849v1,http://arxiv.org/pdf/1703.03849v1,A note on approximate strengths of edges in a hypergraph,note approxim strength edg hypergraph,"Let $H=(V,E)$ be an edge-weighted hypergraph of rank $r$. Kogan and Krauthgamer extended Bencz\'{u}r and Karger's random sampling scheme for cut sparsification from graphs to hypergraphs. The sampling requires an algorithm for computing the approximate strengths of edges. In this note we extend the algorithm for graphs to hypergraphs and describe a near-linear time algorithm to compute approximate strengths of edges; we build on a sparsification result for hypergraphs from our recent work. Combined with prior results we obtain faster algorithms for finding $(1+\epsilon)$-approximate mincuts when the rank of the hypergraph is small.",let edg weight hypergraph rank kogan krauthgam extend bencz karger random sampl scheme cut sparsif graph hypergraph sampl requir algorithm comput approxim strength edg note extend algorithm graph hypergraph describ near linear time algorithm comput approxim strength edg build sparsif result hypergraph recent work combin prior result obtain faster algorithm find epsilon approxim mincut rank hypergraph small,"['Chandra Chekuri', 'Chao Xu']",['cs.DS'],False,False,False,False,False,True
563,2017-03-28T14:09:51Z,2017-03-10T09:58:40Z,http://arxiv.org/abs/1703.03603v1,http://arxiv.org/pdf/1703.03603v1,The Densest Subgraph Problem with a Convex/Concave Size Function,densest subgraph problem convex concav size function,"In the densest subgraph problem, given an edge-weighted undirected graph $G=(V,E,w)$, we are asked to find $S\subseteq V$ that maximizes the density, i.e., $w(S)/ S $, where $w(S)$ is the sum of weights of the edges in the subgraph induced by $S$. This problem has often been employed in a wide variety of graph mining applications. However, the problem has a drawback; it may happen that the obtained subset is too large or too small in comparison with the size desired in the application at hand. In this study, we address the size issue of the densest subgraph problem by generalizing the density of $S\subseteq V$. Specifically, we introduce the $f$-density of $S\subseteq V$, which is defined as $w(S)/f( S )$, where $f:\mathbb{Z}_{\geq 0}\rightarrow \mathbb{R}_{\geq 0}$ is a monotonically non-decreasing function. In the $f$-densest subgraph problem ($f$-DS), we aim to find $S\subseteq V$ that maximizes the $f$-density $w(S)/f( S )$. Although $f$-DS does not explicitly specify the size of the output subset of vertices, we can handle the above size issue using a convex/concave size function $f$ appropriately. For $f$-DS with convex function $f$, we propose a nearly-linear-time algorithm with a provable approximation guarantee. On the other hand, for $f$-DS with concave function $f$, we propose an LP-based exact algorithm, a flow-based $O( V ^3)$-time exact algorithm for unweighted graphs, and a nearly-linear-time approximation algorithm.",densest subgraph problem given edg weight undirect graph ask find subseteq maxim densiti sum weight edg subgraph induc problem often employ wide varieti graph mine applic howev problem drawback may happen obtain subset larg small comparison size desir applic hand studi address size issu densest subgraph problem general densiti subseteq specif introduc densiti subseteq defin mathbb geq rightarrow mathbb geq monoton non decreas function densest subgraph problem ds aim find subseteq maxim densiti although ds doe explicit specifi size output subset vertic handl abov size issu use convex concav size function appropri ds convex function propos near linear time algorithm provabl approxim guarante hand ds concav function propos lp base exact algorithm flow base time exact algorithm unweight graph near linear time approxim algorithm,"['Yasushi Kawase', 'Atsushi Miyauchi']","['cs.DS', 'cs.DM', 'cs.SI']",False,False,False,False,False,True
564,2017-03-28T14:09:51Z,2017-03-10T08:35:24Z,http://arxiv.org/abs/1703.03575v1,http://arxiv.org/pdf/1703.03575v1,Crossing the Logarithmic Barrier for Dynamic Boolean Data Structure   Lower Bounds,cross logarithm barrier dynam boolean data structur lower bound,"This paper proves the first super-logarithmic lower bounds on the cell probe complexity of dynamic boolean (a.k.a. decision) data structure problems, a long-standing milestone in data structure lower bounds.   We introduce a new method for proving dynamic cell probe lower bounds and use it to prove a $\tilde{\Omega}(\log^{1.5} n)$ lower bound on the operational time of a wide range of boolean data structure problems, most notably, on the query time of dynamic range counting over $\mathbb{F}_2$ ([Pat07]). Proving an $\omega(\lg n)$ lower bound for this problem was explicitly posed as one of five important open problems in the late Mihai P\v{a}tra\c{s}cu's obituary [Tho13]. This result also implies the first $\omega(\lg n)$ lower bound for the classical 2D range counting problem, one of the most fundamental data structure problems in computational geometry and spatial databases. We derive similar lower bounds for boolean versions of dynamic polynomial evaluation and 2D rectangle stabbing, and for the (non-boolean) problems of range selection and range median.   Our technical centerpiece is a new way of ""weakly"" simulating dynamic data structures using efficient one-way communication protocols with small advantage over random guessing. This simulation involves a surprising excursion to low-degree (Chebychev) polynomials which may be of independent interest, and offers an entirely new algorithmic angle on the ""cell sampling"" method of Panigrahy et al. [PTW10].",paper prove first super logarithm lower bound cell probe complex dynam boolean decis data structur problem long stand mileston data structur lower bound introduc new method prove dynam cell probe lower bound use prove tild omega log lower bound oper time wide rang boolean data structur problem notabl queri time dynam rang count mathbb pat prove omega lg lower bound problem explicit pose one five import open problem late mihai tra cu obituari tho result also impli first omega lg lower bound classic rang count problem one fundament data structur problem comput geometri spatial databas deriv similar lower bound boolean version dynam polynomi evalu rectangl stab non boolean problem rang select rang median technic centerpiec new way weak simul dynam data structur use effici one way communic protocol small advantag random guess simul involv surpris excurs low degre chebychev polynomi may independ interest offer entir new algorithm angl cell sampl method panigrahi et al ptw,"['Kasper Green Larsen', 'Omri Weinstein', 'Huacheng Yu']","['cs.DS', 'cs.CC', 'cs.CG', 'cs.IT', 'math.IT']",False,False,False,False,False,True
565,2017-03-28T14:09:51Z,2017-03-09T22:47:10Z,http://arxiv.org/abs/1703.03484v1,http://arxiv.org/pdf/1703.03484v1,Combinatorial Auctions with Online XOS Bidders,combinatori auction onlin xos bidder,"In combinatorial auctions, a designer must decide how to allocate a set of indivisible items amongst a set of bidders. Each bidder has a valuation function which gives the utility they obtain from any subset of the items. Our focus is specifically on welfare maximization, where the objective is to maximize the sum of valuations that the bidders place on the items that they were allocated (the valuation functions are assumed to be reported truthfully). We analyze an online problem in which the algorithm is not given the set of bidders in advance. Instead, the bidders are revealed sequentially in a uniformly random order, similarly to secretary problems. The algorithm must make an irrevocable decision about which items to allocate to the current bidder before the next one is revealed. When the valuation functions lie in the class $XOS$ (which includes submodular functions), we provide a black box reduction from offline to online optimization. Specifically, given an $\alpha$-approximation algorithm for offline welfare maximization, we show how to create a $(0.199 \alpha)$-approximation algorithm for the online problem. Our algorithm draws on connections to secretary problems; in fact, we show that the online welfare maximization problem itself can be viewed as a particular kind of secretary problem with nonuniform arrival order.",combinatori auction design must decid alloc set indivis item amongst set bidder bidder valuat function give util obtain ani subset item focus specif welfar maxim object maxim sum valuat bidder place item alloc valuat function assum report truth analyz onlin problem algorithm given set bidder advanc instead bidder reveal sequenti uniform random order similar secretari problem algorithm must make irrevoc decis item alloc current bidder befor next one reveal valuat function lie class xos includ submodular function provid black box reduct offlin onlin optim specif given alpha approxim algorithm offlin welfar maxim show creat alpha approxim algorithm onlin problem algorithm draw connect secretari problem fact show onlin welfar maxim problem view particular kind secretari problem nonuniform arriv order,"['Shaddin Dughmi', 'Bryan Wilder']","['cs.GT', 'cs.DS']",False,False,False,False,False,True
567,2017-03-28T14:09:51Z,2017-03-09T06:08:01Z,http://arxiv.org/abs/1703.03147v1,http://arxiv.org/pdf/1703.03147v1,Juggling Functions Inside a Database,juggl function insid databas,"We define and study the Functional Aggregate Query (FAQ) problem, which captures common computational tasks across a very wide range of domains including relational databases, logic, matrix and tensor computation, probabilistic graphical models, constraint satisfaction, and signal processing. Simply put, an FAQ is a declarative way of defining a new function from a database of input functions.   We present ""InsideOut"", a dynamic programming algorithm, to evaluate an FAQ. The algorithm rewrites the input query into a set of easier-to-compute FAQ sub-queries. Each sub-query is then evaluated using a worst-case optimal relational join algorithm. The topic of designing algorithms to optimally evaluate the classic multiway join problem has seen exciting developments in the past few years. Our framework tightly connects these new ideas in database theory with a vast number of application areas in a coherent manner, showing potentially that a good database engine can be a general-purpose constraint solver, relational data store, graphical model inference engine, and matrix/tensor computation processor all at once.   The InsideOut algorithm is very simple, as shall be described in this paper. Yet, in spite of solving an extremely general problem, its runtime either is as good as or improves upon the best known algorithm for the applications that FAQ specializes to. These corollaries include computational tasks in graphical model inference, matrix/tensor operations, relational joins, and logic. Better yet, InsideOut can be used within any database engine, because it is basically a principled way of rewriting queries. Indeed, it is already part of the LogicBlox database engine, helping efficiently answer traditional database queries, graphical model inference queries, and train a large class of machine learning models inside the database itself.",defin studi function aggreg queri faq problem captur common comput task across veri wide rang domain includ relat databas logic matrix tensor comput probabilist graphic model constraint satisfact signal process simpli put faq declar way defin new function databas input function present insideout dynam program algorithm evalu faq algorithm rewrit input queri set easier comput faq sub queri sub queri evalu use worst case optim relat join algorithm topic design algorithm optim evalu classic multiway join problem seen excit develop past year framework tight connect new idea databas theori vast number applic area coher manner show potenti good databas engin general purpos constraint solver relat data store graphic model infer engin matrix tensor comput processor onc insideout algorithm veri simpl shall describ paper yet spite solv extrem general problem runtim either good improv upon best known algorithm applic faq special corollari includ comput task graphic model infer matrix tensor oper relat join logic better yet insideout use within ani databas engin becaus basic principl way rewrit queri inde alreadi part logicblox databas engin help effici answer tradit databas queri graphic model infer queri train larg class machin learn model insid databas,"['Mahmoud Abo Khamis', 'Hung Q. Ngo', 'Atri Rudra']","['cs.DB', 'cs.DS', 'cs.LO']",False,False,True,False,False,True
568,2017-03-28T14:09:51Z,2017-03-08T21:50:06Z,http://arxiv.org/abs/1703.03048v1,http://arxiv.org/pdf/1703.03048v1,Quickest Visibility Queries in Polygonal Domains,quickest visibl queri polygon domain,"Let $s$ be a point in a polygonal domain $\mathcal{P}$ of $h-1$ holes and $n$ vertices. We consider a quickest visibility query problem. Given a query point $q$ in $\mathcal{P}$, the goal is to find a shortest path in $\mathcal{P}$ to move from $s$ to see $q$ as quickly as possible. Previously, Arkin et al. (SoCG 2015) built a data structure of size $O(n^22^{\alpha(n)}\log n)$ that can answer each query in $O(K\log^2 n)$ time, where $\alpha(n)$ is the inverse Ackermann function and $K$ is the size of the visibility polygon of $q$ in $\mathcal{P}$ (and $K$ can be $\Theta(n)$ in the worst case). In this paper, we present a new data structure of size $O(n\log h + h^2)$ that can answer each query in $O(h\log h\log n)$ time. Our result improves the previous work when $h$ is relatively small. In particular, if $h$ is a constant, then our result even matches the best result for the simple polygon case (i.e., $h=1$), which is optimal. As a by-product, we also have a new algorithm for a shortest-path-to-segment query problem. Given a query line segment $\tau$ in $\mathcal{P}$, the query seeks a shortest path from $s$ to all points of $\tau$. Previously, Arkin et al. gave a data structure of size $O(n^22^{\alpha(n)}\log n)$ that can answer each query in $O(\log^2 n)$ time, and another data structure of size $O(n^3\log n)$ with $O(\log n)$ query time. We present a data structure of size $O(n)$ with query time $O(h\log \frac{n}{h})$, which also favors small values of $h$ and is optimal when $h=O(1)$.",let point polygon domain mathcal hole vertic consid quickest visibl queri problem given queri point mathcal goal find shortest path mathcal move see quick possibl previous arkin et al socg built data structur size alpha log answer queri log time alpha invers ackermann function size visibl polygon mathcal theta worst case paper present new data structur size log answer queri log log time result improv previous work relat small particular constant result even match best result simpl polygon case optim product also new algorithm shortest path segment queri problem given queri line segment tau mathcal queri seek shortest path point tau previous arkin et al gave data structur size alpha log answer queri log time anoth data structur size log log queri time present data structur size queri time log frac also favor small valu optim,['Haitao Wang'],"['cs.CG', 'cs.DS']",False,False,False,False,False,True
569,2017-03-28T14:09:51Z,2017-03-08T15:16:11Z,http://arxiv.org/abs/1703.02867v1,http://arxiv.org/pdf/1703.02867v1,Electoral District Design via Constrained Clustering,elector district design via constrain cluster,The paper studies the electoral district design problem where municipalities of a state have to be grouped into districts of nearly equal population while obeying certain politically motivated requirements. We develop a general framework for electoral district design that is based on the close connection of constrained geometric clustering and diagrams. The approach is computationally efficient and flexible enough to pursue various conflicting juridical demands for the shape of the districts. We demonstrate the practicability of our methodology for electoral districting in Germany.,paper studi elector district design problem municip state group district near equal popul obey certain polit motiv requir develop general framework elector district design base close connect constrain geometr cluster diagram approach comput effici flexibl enough pursu various conflict jurid demand shape district demonstr practic methodolog elector district germani,"['Andreas Brieden', 'Peter Gritzmann', 'Fabian Klemm']","['cs.DS', 'math.CO', '90C90']",False,False,True,False,False,True
570,2017-03-28T14:09:55Z,2017-03-08T15:16:05Z,http://arxiv.org/abs/1703.02866v1,http://arxiv.org/pdf/1703.02866v1,The Half-integral Erdös-Pósa Property for Non-null Cycles,half integr erd sa properti non null cycl,"A Group Labeled Graph is a pair $(G,\Lambda)$ where $G$ is an oriented graph and $\Lambda$ is a mapping from the arcs of $G$ to elements of a group. A (not necessarily directed) cycle $C$ is called non-null if for any cyclic ordering of the arcs in $C$, the group element obtained by `adding' the labels on forward arcs and `subtracting' the labels on reverse arcs is not the identity element of the group. Non-null cycles in group labeled graphs generalize several well-known graph structures, including odd cycles.   In this paper, we prove that non-null cycles on Group Labeled Graphs have the half-integral Erd\""os-P\'osa property. That is, there is a function $f:{\mathbb N}\to {\mathbb N}$ such that for any $k\in {\mathbb N}$, any group labeled graph $(G,\Lambda)$ has a set of $k$ non-null cycles such that each vertex of $G$ appears in at most two of these cycles or there is a set of at most $f(k)$ vertices that intersects every non-null cycle. Since it is known that non-null cycles do not have the integeral Erd\""os-P\'osa property in general, a half-integral Erd\""os-P\'osa result is the best one could hope for.",group label graph pair lambda orient graph lambda map arc element group necessarili direct cycl call non null ani cyclic order arc group element obtain ad label forward arc subtract label revers arc ident element group non null cycl group label graph general sever well known graph structur includ odd cycl paper prove non null cycl group label graph half integr erd os osa properti function mathbb mathbb ani mathbb ani group label graph lambda set non null cycl vertex appear two cycl set vertic intersect everi non null cycl sinc known non null cycl integer erd os osa properti general half integr erd os osa result best one could hope,"['Daniel Lokshtanov', 'M. S. Ramanujan', 'Saket Saurabh']","['cs.DM', 'cs.DS']",False,False,True,False,False,True
571,2017-03-28T14:09:55Z,2017-03-08T10:56:03Z,http://arxiv.org/abs/1703.02784v1,http://arxiv.org/pdf/1703.02784v1,$K$-Best Solutions of MSO Problems on Tree-Decomposable Graphs,best solut mso problem tree decompos graph,"We show that, for any graph optimization problem in which the feasible solutions can be expressed by a formula in monadic second-order logic describing sets of vertices or edges and in which the goal is to minimize the sum of the weights in the selected sets, we can find the $k$ best solutions for $n$-vertex graphs of bounded treewidth in time $\mathcal O(n+k\log n)$. In particular, this applies to the problem of finding the $k$ shortest simple paths between given vertices in directed graphs of bounded treewidth, giving an exponential speedup in the per-path cost over previous algorithms.",show ani graph optim problem feasibl solut express formula monad second order logic describ set vertic edg goal minim sum weight select set find best solut vertex graph bound treewidth time mathcal log particular appli problem find shortest simpl path given vertic direct graph bound treewidth give exponenti speedup per path cost previous algorithm,"['David Eppstein', 'Denis Kurz']","['cs.DS', 'G.2.2']",False,False,True,False,False,True
572,2017-03-28T14:09:55Z,2017-03-08T04:18:58Z,http://arxiv.org/abs/1703.02693v1,http://arxiv.org/pdf/1703.02693v1,Stream Aggregation Through Order Sampling,stream aggreg order sampl,"This paper introduces a new single-pass reservoir weighted-sampling stream aggregation algorithm, Priority Sample and Hold. PrSH combines aspects of the well-known Sample and Hold algorithm with Priority Sampling. In particular, it achieves a reduced computational cost for rate adaptation in a fixed cache by using a single persistent random variable across the lifetime of each key in the cache. The basic approach can be supplemented with a Sample and Hold pre-sampling stage with a sampling rate adaptation controlled by PrSH. We prove that PrSH provides unbiased estimates of the true aggregates. We analyze the computational complexity of PrSH and its variants, and provide a detailed evaluation of its accuracy on synthetic and trace data. Weighted relative error is reduced by 40% to 65% at sampling rates of 5% to 17%, relative to Adaptive Sample and Hold; there is also substantial improvement for rank queries.",paper introduc new singl pass reservoir weight sampl stream aggreg algorithm prioriti sampl hold prsh combin aspect well known sampl hold algorithm prioriti sampl particular achiev reduc comput cost rate adapt fix cach use singl persist random variabl across lifetim key cach basic approach supplement sampl hold pre sampl stage sampl rate adapt control prsh prove prsh provid unbias estim true aggreg analyz comput complex prsh variant provid detail evalu accuraci synthet trace data weight relat error reduc sampl rate relat adapt sampl hold also substanti improv rank queri,"['Nick Duffield', 'Yunhong Xu', 'Liangzhen Xia', 'Nesreen Ahmed', 'Minlan Yu']",['cs.DS'],False,False,False,False,False,True
573,2017-03-28T14:09:55Z,2017-03-08T03:56:27Z,http://arxiv.org/abs/1703.02690v1,http://arxiv.org/pdf/1703.02690v1,Leveraging Sparsity for Efficient Submodular Data Summarization,leverag sparsiti effici submodular data summar,"The facility location problem is widely used for summarizing large datasets and has additional applications in sensor placement, image retrieval, and clustering. One difficulty of this problem is that submodular optimization algorithms require the calculation of pairwise benefits for all items in the dataset. This is infeasible for large problems, so recent work proposed to only calculate nearest neighbor benefits. One limitation is that several strong assumptions were invoked to obtain provable approximation guarantees. In this paper we establish that these extra assumptions are not necessary---solving the sparsified problem will be almost optimal under the standard assumptions of the problem. We then analyze a different method of sparsification that is a better model for methods such as Locality Sensitive Hashing to accelerate the nearest neighbor computations and extend the use of the problem to a broader family of similarities. We validate our approach by demonstrating that it rapidly generates interpretable summaries.",facil locat problem wide use summar larg dataset addit applic sensor placement imag retriev cluster one difficulti problem submodular optim algorithm requir calcul pairwis benefit item dataset infeas larg problem recent work propos onli calcul nearest neighbor benefit one limit sever strong assumpt invok obtain provabl approxim guarante paper establish extra assumpt necessari solv sparsifi problem almost optim standard assumpt problem analyz differ method sparsif better model method local sensit hash acceler nearest neighbor comput extend use problem broader famili similar valid approach demonstr rapid generat interpret summari,"['Erik M. Lindgren', 'Shanshan Wu', 'Alexandros G. Dimakis']","['stat.ML', 'cs.DS', 'cs.IT', 'cs.LG', 'math.IT']",False,False,False,False,False,True
574,2017-03-28T14:09:55Z,2017-03-08T03:55:27Z,http://arxiv.org/abs/1703.02689v1,http://arxiv.org/pdf/1703.02689v1,Exact MAP Inference by Avoiding Fractional Vertices,exact map infer avoid fraction vertic,"Given a graphical model, one essential problem is MAP inference, that is, finding the most likely configuration of states according to the model. Although this problem is NP-hard, large instances can be solved in practice. A major open question is to explain why this is true. We give a natural condition under which we can provably perform MAP inference in polynomial time. We require that the number of fractional vertices in the LP relaxation exceeding the optimal solution is bounded by a polynomial in the problem size. This resolves an open question by Dimakis, Gohari, and Wainwright. In contrast, for general LP relaxations of integer programs, known techniques can only handle a constant number of fractional vertices whose value exceeds the optimal solution. We experimentally verify this condition and demonstrate how efficient various integer programming methods are at removing fractional solutions.",given graphic model one essenti problem map infer find like configur state accord model although problem np hard larg instanc solv practic major open question explain whi true give natur condit provabl perform map infer polynomi time requir number fraction vertic lp relax exceed optim solut bound polynomi problem size resolv open question dimaki gohari wainwright contrast general lp relax integ program known techniqu onli handl constant number fraction vertic whose valu exceed optim solut experiment verifi condit demonstr effici various integ program method remov fraction solut,"['Erik M. Lindgren', 'Alexandros G. Dimakis', 'Adam Klivans']","['stat.ML', 'cs.DS', 'cs.IT', 'cs.LG', 'math.IT']",False,False,False,False,False,True
575,2017-03-28T14:09:55Z,2017-03-07T22:18:35Z,http://arxiv.org/abs/1703.02625v1,http://arxiv.org/pdf/1703.02625v1,On Sampling from Massive Graph Streams,sampl massiv graph stream,"We propose Graph Priority Sampling (GPS), a new paradigm for order-based reservoir sampling from massive streams of graph edges. GPS provides a general way to weight edge sampling according to auxiliary and/or size variables so as to accomplish various estimation goals of graph properties. In the context of subgraph counting, we show how edge sampling weights can be chosen so as to minimize the estimation variance of counts of specified sets of subgraphs. In distinction with many prior graph sampling schemes, GPS separates the functions of edge sampling and subgraph estimation. We propose two estimation frameworks: (1) Post-Stream estimation, to allow GPS to construct a reference sample of edges to support retrospective graph queries, and (2) In-Stream estimation, to allow GPS to obtain lower variance estimates by incrementally updating the subgraph count estimates during stream processing. Unbiasedness of subgraph estimators is established through a new Martingale formulation of graph stream order sampling, which shows that subgraph estimators, written as a product of constituent edge estimators are unbiased, even when computed at different points in the stream. The separation of estimation and sampling enables significant resource savings relative to previous work. We illustrate our framework with applications to triangle and wedge counting. We perform a large-scale experimental study on real-world graphs from various domains and types. GPS achieves high accuracy with less than 1% error for triangle and wedge counting, while storing a small fraction of the graph with average update times of a few microseconds per edge. Notably, for a large Twitter graph with more than 260M edges, GPS accurately estimates triangle counts with less than 1% error, while storing only 40K edges.",propos graph prioriti sampl gps new paradigm order base reservoir sampl massiv stream graph edg gps provid general way weight edg sampl accord auxiliari size variabl accomplish various estim goal graph properti context subgraph count show edg sampl weight chosen minim estim varianc count specifi set subgraph distinct mani prior graph sampl scheme gps separ function edg sampl subgraph estim propos two estim framework post stream estim allow gps construct refer sampl edg support retrospect graph queri stream estim allow gps obtain lower varianc estim increment updat subgraph count estim dure stream process unbiased subgraph estim establish new martingal formul graph stream order sampl show subgraph estim written product constitu edg estim unbias even comput differ point stream separ estim sampl enabl signific resourc save relat previous work illustr framework applic triangl wedg count perform larg scale experiment studi real world graph various domain type gps achiev high accuraci less error triangl wedg count store small fraction graph averag updat time microsecond per edg notabl larg twitter graph edg gps accur estim triangl count less error store onli edg,"['Nesreen K. Ahmed', 'Nick Duffield', 'Theodore Willke', 'Ryan A. Rossi']","['cs.SI', 'cs.DS', 'cs.IR', 'math.ST', 'stat.TH']",False,False,False,False,False,True
576,2017-03-28T14:09:55Z,2017-03-07T17:35:51Z,http://arxiv.org/abs/1703.02485v1,http://arxiv.org/pdf/1703.02485v1,Certifying coloring algorithms for graphs without long induced paths,certifi color algorithm graph without long induc path,"Let $P_k$ be a path, $C_k$ a cycle on $k$ vertices, and $K_{k,k}$ a complete bipartite graph with $k$ vertices on each side of the bipartition. We prove that (1) for any integers $k, t>0$ and a graph $H$ there are finitely many subgraph minimal graphs with no induced $P_k$ and $K_{t,t}$ that are not $H$-colorable and (2) for any integer $k>4$ there are finitely many subgraph minimal graphs with no induced $P_k$ that are not $C_{k-2}$-colorable.   The former generalizes the result of Hell and Huang [Complexity of coloring graphs without paths and cycles, Discrete Appl. Math. 216: 211--232 (2017)] and the latter extends a result of Bruce, Hoang, and Sawada [A certifying algorithm for 3-colorability of $P_5$-Free Graphs, ISAAC 2009: 594--604]. Both our results lead to polynomial-time certifying algorithms for the corresponding coloring problems.",let path cycl vertic complet bipartit graph vertic side bipartit prove ani integ graph finit mani subgraph minim graph induc color ani integ finit mani subgraph minim graph induc color former general result hell huang complex color graph without path cycl discret appl math latter extend result bruce hoang sawada certifi algorithm color free graph isaac result lead polynomi time certifi algorithm correspond color problem,"['Marcin Kamiński', 'Anna Pstrucha']","['math.CO', 'cs.DS']",False,False,False,False,False,True
577,2017-03-28T14:09:55Z,2017-03-07T14:48:15Z,http://arxiv.org/abs/1703.02411v1,http://arxiv.org/pdf/1703.02411v1,"A Simple Deterministic Distributed MST Algorithm, with Near-Optimal Time   and Message Complexities",simpl determinist distribut mst algorithm near optim time messag complex,"Distributed minimum spanning tree (MST) problem is one of the most central and fundamental problems in distributed graph algorithms. Garay et al. \cite{GKP98,KP98} devised an algorithm with running time $O(D + \sqrt{n} \cdot \log^* n)$, where $D$ is the hop-diameter of the input $n$-vertex $m$-edge graph, and with message complexity $O(m + n^{3/2})$. Peleg and Rubinovich \cite{PR99} showed that the running time of the algorithm of \cite{KP98} is essentially tight, and asked if one can achieve near-optimal running time **together with near-optimal message complexity**.   In a recent breakthrough, Pandurangan et al. \cite{PRS16} answered this question in the affirmative, and devised a **randomized** algorithm with time $\tilde{O}(D+ \sqrt{n})$ and message complexity $\tilde{O}(m)$. They asked if such a simultaneous time- and message-optimality can be achieved by a **deterministic** algorithm.   In this paper, building upon the work of \cite{PRS16}, we answer this question in the affirmative, and devise a **deterministic** algorithm that computes MST in time $O((D + \sqrt{n}) \cdot \log n)$, using $O(m \cdot \log n + n \log n \cdot \log^* n)$ messages. The polylogarithmic factors in the time and message complexities of our algorithm are significantly smaller than the respective factors in the result of \cite{PRS16}. Also, our algorithm and its analysis are very **simple** and self-contained, as opposed to rather complicated previous sublinear-time algorithms \cite{GKP98,KP98,E04b,PRS16}.",distribut minimum span tree mst problem one central fundament problem distribut graph algorithm garay et al cite gkp kp devis algorithm run time sqrt cdot log hop diamet input vertex edg graph messag complex peleg rubinovich cite pr show run time algorithm cite kp essenti tight ask one achiev near optim run time togeth near optim messag complex recent breakthrough pandurangan et al cite prs answer question affirm devis random algorithm time tild sqrt messag complex tild ask simultan time messag optim achiev determinist algorithm paper build upon work cite prs answer question affirm devis determinist algorithm comput mst time sqrt cdot log use cdot log log cdot log messag polylogarithm factor time messag complex algorithm signific smaller respect factor result cite prs also algorithm analysi veri simpl self contain oppos rather complic previous sublinear time algorithm cite gkp kp eb prs,['Michael Elkin'],['cs.DS'],False,False,False,False,False,True
579,2017-03-28T14:09:55Z,2017-03-07T05:43:56Z,http://arxiv.org/abs/1703.02224v1,http://arxiv.org/pdf/1703.02224v1,Space-efficient K-MER algorithm for generalized suffix tree,space effici mer algorithm general suffix tree,"Suffix trees have emerged to be very fast for pattern searching yielding O (m) time, where m is the pattern size. Unfortunately their high memory requirements make it impractical to work with huge amounts of data. We present a memory efficient algorithm of a generalized suffix tree which reduces the space size by a factor of 10 when the size of the pattern is known beforehand. Experiments on the chromosomes and Pizza&Chili corpus show significant advantages of our algorithm over standard linear time suffix tree construction in terms of memory usage for pattern searching.",suffix tree emerg veri fast pattern search yield time pattern size unfortun high memori requir make impract work huge amount data present memori effici algorithm general suffix tree reduc space size factor size pattern known beforehand experi chromosom pizza chili corpus show signific advantag algorithm standard linear time suffix tree construct term memori usag pattern search,"['Freeson Kaniwa', 'Venu Madhav Kuthadi', 'Otlhapile Dinakenyane', 'Heiko Schroeder']",['cs.DS'],False,False,False,False,False,True
580,2017-03-28T14:09:59Z,2017-03-06T20:28:23Z,http://arxiv.org/abs/1703.02100v1,http://arxiv.org/pdf/1703.02100v1,Guarantees for Greedy Maximization of Non-submodular Functions with   Applications,guarante greedi maxim non submodular function applic,"We investigate the performance of the Greedy algorithm for cardinality constrained maximization of non-submodular nondecreasing set functions. While there are strong theoretical guarantees on the performance of Greedy for maximizing submodular functions, there are few guarantees for non-submodular ones. However, Greedy enjoys strong empirical performance for many important non-submodular functions, e.g., the Bayesian A-optimality objective in experimental design. We prove theoretical guarantees supporting the empirical performance. Our guarantees are characterized by the (generalized) submodularity ratio $\gamma$ and the (generalized) curvature $\alpha$. In particular, we prove that Greedy enjoys a tight approximation guarantee of $\frac{1}{\alpha}(1- e^{-\gamma\alpha})$ for cardinality constrained maximization. In addition, we bound the submodularity ratio and curvature for several important real-world objectives, e.g., the Bayesian A-optimality objective, the determinantal function of a square submatrix and certain linear programs with combinatorial constraints. We experimentally validate our theoretical findings for several real-world applications.",investig perform greedi algorithm cardin constrain maxim non submodular nondecreas set function strong theoret guarante perform greedi maxim submodular function guarante non submodular one howev greedi enjoy strong empir perform mani import non submodular function bayesian optim object experiment design prove theoret guarante support empir perform guarante character general submodular ratio gamma general curvatur alpha particular prove greedi enjoy tight approxim guarante frac alpha gamma alpha cardin constrain maxim addit bound submodular ratio curvatur sever import real world object bayesian optim object determinant function squar submatrix certain linear program combinatori constraint experiment valid theoret find sever real world applic,"['Andrew An Bian', 'Joachim M. Buhmann', 'Andreas Krause', 'Sebastian Tschiatschek']","['cs.DM', 'cs.AI', 'cs.DS', 'cs.LG', 'math.OC']",False,False,False,False,False,True
582,2017-03-28T14:09:59Z,2017-03-07T14:17:51Z,http://arxiv.org/abs/1703.01939v2,http://arxiv.org/pdf/1703.01939v2,Distributed Exact Shortest Paths in Sublinear Time,distribut exact shortest path sublinear time,"The distributed single-source shortest paths problem is one of the most fundamental and central problems in the message-passing distributed computing. Classical Bellman-Ford algorithm solves it in $O(n)$ time, where $n$ is the number of vertices in the input graph $G$. Peleg and Rubinovich (FOCS'99) showed a lower bound of $\tilde{\Omega}(D + \sqrt{n})$ for this problem, where $D$ is the hop-diameter of $G$.   Whether or not this problem can be solved in $o(n)$ time when $D$ is relatively small is a major notorious open question. Despite intensive research \cite{LP13,N14,HKN15,EN16,BKKL16} that yielded near-optimal algorithms for the approximate variant of this problem, no progress was reported for the original problem.   In this paper we answer this question in the affirmative. We devise an algorithm that requires $O((n \log n)^{5/6})$ time, for $D = O(\sqrt{n \log n})$, and $O(D^{1/3} \cdot (n \log n)^{2/3})$ time, for larger $D$. This running time is sublinear in $n$ in almost the entire range of parameters, specifically, for $D = o(n/\log^2 n)$.   We also devise the first algorithm with non-trivial complexity guarantees for computing exact shortest paths in the multipass semi-streaming model of computation.   From the technical viewpoint, our algorithm computes a hopset $G""$ of a skeleton graph $G'$ of $G$ without first computing $G'$ itself. We then conduct a Bellman-Ford exploration in $G' \cup G""$, while computing the required edges of $G'$ on the fly. As a result, our algorithm computes exactly those edges of $G'$ that it really needs, rather than computing approximately the entire $G'$.",distribut singl sourc shortest path problem one fundament central problem messag pass distribut comput classic bellman ford algorithm solv time number vertic input graph peleg rubinovich foc show lower bound tild omega sqrt problem hop diamet whether problem solv time relat small major notori open question despit intens research cite lp hkn en bkkl yield near optim algorithm approxim variant problem progress report origin problem paper answer question affirm devis algorithm requir log time sqrt log cdot log time larger run time sublinear almost entir rang paramet specif log also devis first algorithm non trivial complex guarante comput exact shortest path multipass semi stream model comput technic viewpoint algorithm comput hopset skeleton graph without first comput conduct bellman ford explor cup comput requir edg fli result algorithm comput exact edg realli need rather comput approxim entir,['Michael Elkin'],['cs.DS'],False,False,False,False,False,True
583,2017-03-28T14:09:59Z,2017-03-06T15:03:55Z,http://arxiv.org/abs/1703.01913v1,http://arxiv.org/pdf/1703.01913v1,Near-Optimal Closeness Testing of Discrete Histogram Distributions,near optim close test discret histogram distribut,"We investigate the problem of testing the equivalence between two discrete histograms. A {\em $k$-histogram} over $[n]$ is a probability distribution that is piecewise constant over some set of $k$ intervals over $[n]$. Histograms have been extensively studied in computer science and statistics. Given a set of samples from two $k$-histogram distributions $p, q$ over $[n]$, we want to distinguish (with high probability) between the cases that $p = q$ and $\ p-q\ _1 \geq \epsilon$. The main contribution of this paper is a new algorithm for this testing problem and a nearly matching information-theoretic lower bound. Specifically, the sample complexity of our algorithm matches our lower bound up to a logarithmic factor, improving on previous work by polynomial factors in the relevant parameters. Our algorithmic approach applies in a more general setting and yields improved sample upper bounds for testing closeness of other structured distributions as well.",investig problem test equival two discret histogram em histogram probabl distribut piecewis constant set interv histogram extens studi comput scienc statist given set sampl two histogram distribut want distinguish high probabl case geq epsilon main contribut paper new algorithm test problem near match inform theoret lower bound specif sampl complex algorithm match lower bound logarithm factor improv previous work polynomi factor relev paramet algorithm approach appli general set yield improv sampl upper bound test close structur distribut well,"['Ilias Diakonikolas', 'Daniel M. Kane', 'Vladimir Nikishkin']","['cs.DS', 'cs.IT', 'cs.LG', 'math.IT', 'math.ST', 'stat.TH']",False,False,False,False,False,True
584,2017-03-28T14:09:59Z,2017-03-24T15:42:05Z,http://arxiv.org/abs/1703.01905v2,http://arxiv.org/pdf/1703.01905v2,"A randomized, efficient algorithm for 3SAT",random effici algorithm sat,"In this paper I present a 3SAT algorithm based on the randomized algorithm of Papadimitriou from 1991, and Schoning from 1991. We also show that this algorithm finds a solution (if it exists) for a 3SAT problem with high probability in polynomial time.",paper present sat algorithm base random algorithm papadimitriou schone also show algorithm find solut exist sat problem high probabl polynomi time,['Cristian Dumitrescu'],['cs.DS'],False,False,False,False,False,True
585,2017-03-28T14:09:59Z,2017-03-06T12:55:21Z,http://arxiv.org/abs/1703.01847v1,http://arxiv.org/pdf/1703.01847v1,Tight Space-Approximation Tradeoff for the Multi-Pass Streaming Set   Cover Problem,tight space approxim tradeoff multi pass stream set cover problem,"We study the classic set cover problem in the streaming model: the sets that comprise the instance are revealed one by one in a stream and the goal is to solve the problem by making one or few passes over the stream while maintaining a sublinear space $o(mn)$ in the input size; here $m$ denotes the number of the sets and $n$ is the universe size. Notice that in this model, we are mainly concerned with the space requirement of the algorithms and hence do not restrict their computation time.   Our main result is a resolution of the space-approximation tradeoff for the streaming set cover problem: we show that any $\alpha$-approximation algorithm for the set cover problem requires $\widetilde{\Omega}(mn^{1/\alpha})$ space, even if it is allowed polylog${(n)}$ passes over the stream, and even if the sets are arriving in a random order in the stream. This space-approximation tradeoff matches the best known bounds achieved by the recent algorithm of Har-Peled et.al. (PODS 2016) that requires only $O(\alpha)$ passes over the stream in an adversarial order, hence settling the space complexity of approximating the set cover problem in data streams in a quite robust manner. Additionally, our approach yields tight lower bounds for the space complexity of $(1- \epsilon)$-approximating the streaming maximum coverage problem studied in several recent works.",studi classic set cover problem stream model set compris instanc reveal one one stream goal solv problem make one pass stream maintain sublinear space mn input size denot number set univers size notic model main concern space requir algorithm henc restrict comput time main result resolut space approxim tradeoff stream set cover problem show ani alpha approxim algorithm set cover problem requir widetild omega mn alpha space even allow polylog pass stream even set arriv random order stream space approxim tradeoff match best known bound achiev recent algorithm har pele et al pod requir onli alpha pass stream adversari order henc settl space complex approxim set cover problem data stream quit robust manner addit approach yield tight lower bound space complex epsilon approxim stream maximum coverag problem studi sever recent work,['Sepehr Assadi'],['cs.DS'],False,False,False,False,False,True
586,2017-03-28T14:09:59Z,2017-03-06T12:06:58Z,http://arxiv.org/abs/1703.01830v1,http://arxiv.org/pdf/1703.01830v1,Decomposable Submodular Function Minimization: Discrete and Continuous,decompos submodular function minim discret continu,"This paper investigates connections between discrete and continuous approaches for decomposable submodular function minimization. We provide improved running time estimates for the state-of-the-art continuous algorithms for the problem using combinatorial arguments. We also provide a systematic experimental comparison of the two types of methods, based on a clear distinction between level-0 and level-1 algorithms.",paper investig connect discret continu approach decompos submodular function minim provid improv run time estim state art continu algorithm problem use combinatori argument also provid systemat experiment comparison two type method base clear distinct level level algorithm,"['Alina Ene', 'Huy L. Nguyen', 'László A. Végh']","['cs.LG', 'cs.DS']",False,False,False,False,False,True
588,2017-03-28T14:09:59Z,2017-03-05T23:06:03Z,http://arxiv.org/abs/1703.01686v1,http://arxiv.org/pdf/1703.01686v1,Parameterized complexity of finding a spanning tree with minimum reload   cost diameter,parameter complex find span tree minimum reload cost diamet,"We study the minimum diameter spanning tree problem under the reload cost model (DIAMETER-TREE for short) introduced by Wirth and Steffan (2001). In this problem, given an undirected edge-colored graph $G$, reload costs on a path arise at a node where the path uses consecutive edges of different colors. The objective is to find a spanning tree of $G$ of minimum diameter with respect to the reload costs. We initiate a systematic study of the parameterized complexity of the DIAMETER-TREE problem by considering the following parameters: the cost of a solution, and the treewidth and the maximum degree $\Delta$ of the input graph. We prove that DIAMETER-TREE is para-NP-hard for any combination of two of these three parameters, and that it is FPT parameterized by the three of them. We also prove that the problem can be solved in polynomial time on cactus graphs. This result is somehow surprising since we prove DIAMETER-TREE to be NP-hard on graphs of treewidth two, which is best possible as the problem can be trivially solved on forests. When the reload costs satisfy the triangle inequality, Wirth and Steffan (2001) proved that the problem can be solved in polynomial time on graphs with $\Delta = 3$, and Galbiati (2008) proved that it is NP-hard if $\Delta = 4$. Our results show, in particular, that without the requirement of the triangle inequality, the problem is NP-hard if $\Delta = 3$, which is also best possible. Finally, in the case where the reload costs are polynomially bounded by the size of the input graph, we prove that DIAMETER-TREE is in XP and W[1]-hard parameterized by the treewidth plus $\Delta$.",studi minimum diamet span tree problem reload cost model diamet tree short introduc wirth steffan problem given undirect edg color graph reload cost path aris node path use consecut edg differ color object find span tree minimum diamet respect reload cost initi systemat studi parameter complex diamet tree problem consid follow paramet cost solut treewidth maximum degre delta input graph prove diamet tree para np hard ani combin two three paramet fpt parameter three also prove problem solv polynomi time cactus graph result somehow surpris sinc prove diamet tree np hard graph treewidth two best possibl problem trivial solv forest reload cost satisfi triangl inequ wirth steffan prove problem solv polynomi time graph delta galbiati prove np hard delta result show particular without requir triangl inequ problem np hard delta also best possibl final case reload cost polynomi bound size input graph prove diamet tree xp hard parameter treewidth plus delta,"['Julien Baste', 'Didem Gözüpek', 'Christophe Paul', 'Ignasi Sau', 'Mordechai Shalom', 'Dimitrios M. Thilikos']","['cs.DS', 'cs.CC', '05C85, 05C10', 'G.2.2; G.2.3']",False,False,False,False,False,True
589,2017-03-28T14:09:59Z,2017-03-05T18:24:23Z,http://arxiv.org/abs/1703.01640v1,http://arxiv.org/abs/1703.01640v1,Approximation algorithms for TSP with neighborhoods in the plane,approxim algorithm tsp neighborhood plane,"In the Euclidean TSP with neighborhoods (TSPN), we are given a collection of n regions (neighborhoods) and we seek a shortest tour that visits each region. As a generalization of the classical Euclidean TSP, TSPN is also NP-hard. In this paper, we present new approximation results for the TSPN, including (1) a constant-factor approximation algorithm for the case of arbitrary connected neighborhoods having comparable diameters; and (2) a PTAS for the important special case of disjoint unit disk neighborhoods (or nearly disjoint, nearly-unit disks). Our methods also yield improved approximation ratios for various special classes of neighborhoods, which have previously been studied. Further, we give a linear-time O(1)-approximation algorithm for the case of neighborhoods that are (infinite) straight lines.",euclidean tsp neighborhood tspn given collect region neighborhood seek shortest tour visit region general classic euclidean tsp tspn also np hard paper present new approxim result tspn includ constant factor approxim algorithm case arbitrari connect neighborhood compar diamet ptas import special case disjoint unit disk neighborhood near disjoint near unit disk method also yield improv approxim ratio various special class neighborhood previous studi give linear time approxim algorithm case neighborhood infinit straight line,"['Adrian Dumitrescu', 'Joseph S. B. Mitchell']","['cs.CG', 'cs.DS']",False,False,False,False,False,True
590,2017-03-28T14:10:04Z,2017-03-05T18:12:06Z,http://arxiv.org/abs/1703.01638v1,http://arxiv.org/pdf/1703.01638v1,Conditional Hardness for Sensitivity Problems,condit hard sensit problem,"In recent years it has become popular to study dynamic problems in a sensitivity setting: Instead of allowing for an arbitrary sequence of updates, the sensitivity model only allows to apply batch updates of small size to the original input data. The sensitivity model is particularly appealing since recent strong conditional lower bounds ruled out fast algorithms for many dynamic problems, such as shortest paths, reachability, or subgraph connectivity.   In this paper we prove conditional lower bounds for sensitivity problems. For example, we show that under the Boolean Matrix Multiplication (BMM) conjecture combinatorial algorithms cannot compute the (4/3 - {\epsilon})-approximate diameter of an undirected unweighted dense graph with truly subcubic preprocessing time and truly subquadratic update/query time. This result is surprising since in the static setting it is not clear whether a reduction from BMM to diameter is possible. We further show under the BMM conjecture that many problems, such as reachability or approximate shortest paths, cannot be solved faster than by recomputation from scratch even after only one or two edge insertions. We give more lower bounds under the Strong Exponential Time Hypothesis and the All Pairs Shortest Paths Conjecture. Many of our lower bounds also hold for static oracle data structures where no sensitivity is required. Finally, we give the first algorithm for the (1 + {\epsilon})-approximate radius, diameter, and eccentricity problems in directed or undirected unweighted graphs in case of single edges failures. The algorithm has a truly subcubic running time for graphs with a truly subquadratic number of edges; it is tight w.r.t. the conditional lower bounds we obtain.",recent year becom popular studi dynam problem sensit set instead allow arbitrari sequenc updat sensit model onli allow appli batch updat small size origin input data sensit model particular appeal sinc recent strong condit lower bound rule fast algorithm mani dynam problem shortest path reachabl subgraph connect paper prove condit lower bound sensit problem exampl show boolean matrix multipl bmm conjectur combinatori algorithm cannot comput epsilon approxim diamet undirect unweight dens graph truli subcub preprocess time truli subquadrat updat queri time result surpris sinc static set clear whether reduct bmm diamet possibl show bmm conjectur mani problem reachabl approxim shortest path cannot solv faster recomput scratch even onli one two edg insert give lower bound strong exponenti time hypothesi pair shortest path conjectur mani lower bound also hold static oracl data structur sensit requir final give first algorithm epsilon approxim radius diamet eccentr problem direct undirect unweight graph case singl edg failur algorithm truli subcub run time graph truli subquadrat number edg tight condit lower bound obtain,"['Monika Henzinger', 'Andrea Lincoln', 'Stefan Neumann', 'Virginia Vassilevska Williams']","['cs.DS', 'F.2.2']",False,False,False,False,False,True
593,2017-03-28T14:10:04Z,2017-03-04T22:56:03Z,http://arxiv.org/abs/1703.01532v1,http://arxiv.org/pdf/1703.01532v1,Using Matching to Detect Infeasibility of Some Integer Programs,use match detect infeas integ program,"A novel matching based heuristic algorithm designed to detect specially formulated infeasible zero-one IPs is presented. The algorithm input is a set of nested doubly stochastic subsystems and a set E of instance defining variables set at zero level. The algorithm deduces additional variables at zero level until either a constraint is violated (the IP is infeasible), or no more variables can be deduced zero (the IP is undecided). All feasible IPs, and all infeasible IPs not detected infeasible are undecided. We successfully apply the algorithm to a small set of specially formulated infeasible zero-one IP instances of the Hamilton cycle decision problem. We show how to model both the graph and subgraph isomorphism decision problems for input to the algorithm. Increased levels of nested doubly stochastic subsystems can be implemented dynamically. The algorithm is designed for parallel processing, and for inclusion of techniques in addition to matching.",novel match base heurist algorithm design detect special formul infeas zero one ip present algorithm input set nest doubli stochast subsystem set instanc defin variabl set zero level algorithm deduc addit variabl zero level either constraint violat ip infeas variabl deduc zero ip undecid feasibl ip infeas ip detect infeas undecid success appli algorithm small set special formul infeas zero one ip instanc hamilton cycl decis problem show model graph subgraph isomorph decis problem input algorithm increas level nest doubli stochast subsystem implement dynam algorithm design parallel process inclus techniqu addit match,"['S. J. Gismondi', 'E. R. Swart']","['cs.DS', 'cs.DM']",False,False,False,False,False,True
596,2017-03-28T14:10:04Z,2017-03-04T15:13:41Z,http://arxiv.org/abs/1703.01474v1,http://arxiv.org/pdf/1703.01474v1,Sharp bounds for population recovery,sharp bound popul recoveri,"The population recovery problem is a basic problem in noisy unsupervised learning that has attracted significant research attention in recent years [WY12,DRWY12, MS13, BIMP13, LZ15,DST16]. A number of different variants of this problem have been studied, often under assumptions on the unknown distribution (such as that it has restricted support size). In this work we study the sample complexity and algorithmic complexity of the most general version of the problem, under both bit-flip noise and erasure noise model. We give essentially matching upper and lower sample complexity bounds for both noise models, and efficient algorithms matching these sample complexity bounds up to polynomial factors.",popul recoveri problem basic problem noisi unsupervis learn attract signific research attent recent year wy drwi ms bimp lz dst number differ variant problem studi often assumpt unknown distribut restrict support size work studi sampl complex algorithm complex general version problem bit flip nois erasur nois model give essenti match upper lower sampl complex bound nois model effici algorithm match sampl complex bound polynomi factor,"['Anindya De', ""Ryan O'Donnell"", 'Rocco Servedio']","['cs.DS', 'cs.LG', 'math.ST', 'stat.TH']",False,False,False,False,False,True
597,2017-03-28T14:10:04Z,2017-03-06T09:05:51Z,http://arxiv.org/abs/1703.01166v2,http://arxiv.org/pdf/1703.01166v2,Efficient Network Measurements through Approximated Windows,effici network measur approxim window,"Many networking applications require timely access to recent network measurements, which can be captured using a sliding window model. Maintaining such measurements is a challenging task due to the fast line speed and scarcity of fast memory in routers. In this work, we study the efficiency factor that can be gained by approximating the window size. That is, we allow the algorithm to dynamically adjust the window size between $W$ and $W(1+\tau)$ where $\tau$ is a small positive parameter. For example, consider the \emph{basic summing} problem of computing the sum of the last $W$ elements in a stream whose items are integers in $\{0,1\ldots,R\}$, where $R=\text{poly}(W)$. While it is known that $\Omega(W\log{R})$ bits are needed in the exact window model, we show that approximate windows allow an exponential space reduction for constant $\tau$.   Specifically, we present a lower bound of $\Omega(\tau^{-1}\log(RW\tau))$ bits for the basic summing problem. Further, an $(1+\epsilon)$ multiplicative approximation of this problem requires $\Omega(\log({W/\epsilon}+\log\log{R}))$ bits for constant $\tau$. Additionally, for $RW\epsilon$ additive approximations, we show an $\Omega(\tau^{-1}\log\lfloor{1+\tau/\epsilon}\rfloor+\log({W/\epsilon}))$ lower bound~\footnote{ We also provide an optimal bound and algorithm for the $\tau<\epsilon$ case.}. For all three settings, we provide memory optimal algorithms that operate in constant time. Finally, we demonstrate the generality of the approximated window model by applying it to counting the number of distinct flows in a sliding window over a network stream. We present an algorithm that solves this problem while requiring asymptotically less space than previous sliding window methods when $\tau=O(1)$.",mani network applic requir time access recent network measur captur use slide window model maintain measur challeng task due fast line speed scarciti fast memori router work studi effici factor gain approxim window size allow algorithm dynam adjust window size tau tau small posit paramet exampl consid emph basic sum problem comput sum last element stream whose item integ ldot text poli known omega log bit need exact window model show approxim window allow exponenti space reduct constant tau specif present lower bound omega tau log rw tau bit basic sum problem epsilon multipl approxim problem requir omega log epsilon log log bit constant tau addit rw epsilon addit approxim show omega tau log lfloor tau epsilon rfloor log epsilon lower bound footnot also provid optim bound algorithm tau epsilon case three set provid memori optim algorithm oper constant time final demonstr general approxim window model appli count number distinct flow slide window network stream present algorithm solv problem requir asymptot less space previous slide window method tau,"['Ran Ben Basat', 'Gil Einziger', 'Roy Friedman']",['cs.DS'],False,False,False,False,False,True
601,2017-03-28T14:08:00Z,2017-03-27T13:57:31Z,http://arxiv.org/abs/1703.09083v1,http://arxiv.org/pdf/1703.09083v1,The weighted stable matching problem,weight stabl match problem,"We study the stable matching problem in non-bipartite graphs with incomplete but strict preference lists, where the edges have weights and the goal is to compute a stable matching of minimum or maximum weight. This problem is known to be NP-hard in general. Our contribution is two fold: a polyhedral characterization and an approximation algorithm. Previously Chen et al. have shown that the stable matching polytope is integral if and only if the subgraph obtained after running phase one of Irving's algorithm is bipartite. We improve upon this result by showing that there are instances where this subgraph might not be bipartite but one can further eliminate some edges and arrive at a bipartite subgraph. Our elimination procedure ensures that the set of stable matchings remains the same, and thus the stable matching polytope of the final subgraph contains the incidence vectors of all stable matchings of our original graph. This allows us to characterize a larger class of instances for which the weighted stable matching problem is polynomial-time solvable. We also show that our edge elimination procedure is best possible, meaning that if the subgraph we arrive at is not bipartite, then there is no bipartite subgraph that has the same set of stable matchings as the original graph. We complement these results with a $2$-approximation algorithm for the minimum weight stable matching problem for instances where each agent has at most two possible partners in any stable matching. This is the first approximation result for any class of instances with general weights.",studi stabl match problem non bipartit graph incomplet strict prefer list edg weight goal comput stabl match minimum maximum weight problem known np hard general contribut two fold polyhedr character approxim algorithm previous chen et al shown stabl match polytop integr onli subgraph obtain run phase one irv algorithm bipartit improv upon result show instanc subgraph might bipartit one elimin edg arriv bipartit subgraph elimin procedur ensur set stabl match remain thus stabl match polytop final subgraph contain incid vector stabl match origin graph allow us character larger class instanc weight stabl match problem polynomi time solvabl also show edg elimin procedur best possibl mean subgraph arriv bipartit bipartit subgraph set stabl match origin graph complement result approxim algorithm minimum weight stabl match problem instanc agent two possibl partner ani stabl match first approxim result ani class instanc general weight,"['Linda Farczadi', 'Natália Guričanová']","['cs.GT', 'cs.DS']",False,False,False,False,False,True
602,2017-03-28T14:08:00Z,2017-03-27T04:46:58Z,http://arxiv.org/abs/1703.08928v1,http://arxiv.org/pdf/1703.08928v1,Resource-monotonicity and Population-monotonicity in Connected   Cake-cutting,resourc monoton popul monoton connect cake cut,"In the classic cake-cutting problem (Steinhaus, 1948), a heterogeneous resource has to be divided among n agents with different valuations in a proportional way --- giving each agent a piece with a value of at least 1/n of the total. In many applications, such as dividing a land-estate or a time-interval, it is also important that the pieces are connected. We propose two additional requirements: resource-monotonicity (RM) and population-monotonicity (PM). When either the cake or the set of agents changes and the cake is re-divided using the same rule, the utility of all remaining agents must change in the same direction.   Classic cake-cutting protocols are neither RM nor PM. Moreover, we prove that no Pareto-optimal proportional division rule can be either RM or PM. Motivated by this negative result, we search for division rules that are weakly-Pareto-optimal --- no other division is strictly better for all agents.   We present two such rules. The relative-equitable rule, which assigns the maximum possible relative value equal for all agents, is proportional and PM. The so-called rightmost-mark rule, which is an improved version of the Cut and Choose protocol, is proportional and RM for two agents.",classic cake cut problem steinhaus heterogen resourc divid among agent differ valuat proport way give agent piec valu least total mani applic divid land estat time interv also import piec connect propos two addit requir resourc monoton rm popul monoton pm either cake set agent chang cake divid use rule util remain agent must chang direct classic cake cut protocol neither rm pm moreov prove pareto optim proport divis rule either rm pm motiv negat result search divis rule weak pareto optim divis strict better agent present two rule relat equit rule assign maximum possibl relat valu equal agent proport pm call rightmost mark rule improv version cut choos protocol proport rm two agent,"['Erel Segal-Halevi', 'Balázs Sziklai']",['cs.GT'],False,False,False,False,False,True
603,2017-03-28T14:08:00Z,2017-03-26T06:49:54Z,http://arxiv.org/abs/1703.08776v1,http://arxiv.org/pdf/1703.08776v1,Assortative Mixing Equilibria in Social Network Games,assort mix equilibria social network game,"It is known that individuals in social networks tend to exhibit homophily (a.k.a. assortative mixing) in their social ties, which implies that they prefer bonding with others of their own kind. But what are the reasons for this phenomenon? Is it that such relations are more convenient and easier to maintain? Or are there also some more tangible benefits to be gained from this collective behaviour?   The current work takes a game-theoretic perspective on this phenomenon, and studies the conditions under which different assortative mixing strategies lead to equilibrium in an evolving social network. We focus on a biased preferential attachment model where the strategy of each group (e.g., political or social minority) determines the level of bias of its members toward other group members and non-members. Our first result is that if the utility function that the group attempts to maximize is the degree centrality of the group, interpreted as the sum of degrees of the group members in the network, then the only strategy achieving Nash equilibrium is a perfect homophily, which implies that cooperation with other groups is harmful to this utility function. A second, and perhaps more surprising, result is that if a reward for inter-group cooperation is added to the utility function (e.g., externally enforced by an authority as a regulation), then there are only two possible equilibria, namely, perfect homophily or perfect heterophily, and it is possible to characterize their feasibility spaces. Interestingly, these results hold regardless of the minority-majority ratio in the population.   We believe that these results, as well as the game-theoretic perspective presented herein, may contribute to a better understanding of the forces that shape the groups and communities of our society.",known individu social network tend exhibit homophili assort mix social tie impli prefer bond kind reason phenomenon relat conveni easier maintain also tangibl benefit gain collect behaviour current work take game theoret perspect phenomenon studi condit differ assort mix strategi lead equilibrium evolv social network focus bias preferenti attach model strategi group polit social minor determin level bias member toward group member non member first result util function group attempt maxim degre central group interpret sum degre group member network onli strategi achiev nash equilibrium perfect homophili impli cooper group harm util function second perhap surpris result reward inter group cooper ad util function extern enforc author regul onli two possibl equilibria name perfect homophili perfect heterophili possibl character feasibl space interest result hold regardless minor major ratio popul believ result well game theoret perspect present herein may contribut better understand forc shape group communiti societi,"['Chen Avin', 'Hadassa Daltrophe', 'Zvi Lotker', 'David Peleg']","['cs.SI', 'cs.GT', 'physics.soc-ph']",False,False,True,False,False,True
605,2017-03-28T14:08:00Z,2017-03-25T14:39:40Z,http://arxiv.org/abs/1703.08698v1,http://arxiv.org/pdf/1703.08698v1,Hiring Expert Consultants in E-Healthcare: A Two Sided Matching Approach,hire expert consult healthcar two side match approach,"Very often in some censorious healthcare scenario, there may be a need to have some expert consultancies (especially by doctors) that are not available in-house to the hospital. With the advancement in technologies (such as video conferencing, smartphone, etc.), it has become reality that, for the critical medical cases in the hospitals, expert consultants (ECs) from around the world could be hired, who will serve the patients by their physical or virtual presence. Earlier, this interesting healthcare scenario of hiring the ECs (mainly doctors) from outside of the hospitals had been studied with the robust concepts of mechanism design with or without money. We have tried to model the ECs (mainly doctors) hiring problem as a two-sided matching problem. In this paper, for the first time, to the best of our knowledge, we explore the more realistic two-sided matching in our set-up, where the members of the two participating communities, namely patients and doctors are revealing the strict preference ordering over all the members of the opposite community for a stipulated amount of time. We assume that patients and doctors are strategic in nature. With the theoretical analysis, we demonstrate that the proposed mechanism that results in a stable allocation of doctors to patients is strategy-proof (or truthful) and optimal. The proposed mechanism is also validated with exhaustive experiments.",veri often censori healthcar scenario may need expert consult especi doctor avail hous hospit advanc technolog video conferenc smartphon etc becom realiti critic medic case hospit expert consult ec around world could hire serv patient physic virtual presenc earlier interest healthcar scenario hire ec main doctor outsid hospit studi robust concept mechan design without money tri model ec main doctor hire problem two side match problem paper first time best knowledg explor realist two side match set member two particip communiti name patient doctor reveal strict prefer order member opposit communiti stipul amount time assum patient doctor strateg natur theoret analysi demonstr propos mechan result stabl alloc doctor patient strategi proof truth optim propos mechan also valid exhaust experi,"['Vikash Kumar Singh', 'Sajal Mukhopadhyay', 'Aniruddh Sharma', 'Arpan Roy']",['cs.GT'],False,False,False,False,False,True
607,2017-03-28T14:08:00Z,2017-03-24T21:39:34Z,http://arxiv.org/abs/1703.08607v1,http://arxiv.org/pdf/1703.08607v1,Aversion to Uncertainty and Its Implications for Revenue Maximization,avers uncertainti implic revenu maxim,"We study a model of risk in which the agent undervalues uncertain outcomes. Under our model, an event occurring with probability $x<1$ is worth strictly less than $x$ times the value of the event when it occurs with certainty. This property can be formalized in the form of an uncertainty weighting function. Our model is a special case of models considered under prospect theory, an alternative to the heavily studied expected utility theory in economics. Our goal is to understand the implications of this kind of attitude towards risk on mechanism design.   We specifically examine three aspects of revenue optimal mechanism design as they relate to risk. First, how does risk aversion affect the use of randomization within mechanism design? Second, in dynamic settings where the buyer's lack of information about future values allows the seller to extract more revenue, how does risk aversion affect the ability of the seller to exploit the uncertainty of future events? Finally, is it possible to obtain approximation guarantees for revenue that are robust to the specific risk model?   We present three main results. First, we characterize optimal mechanisms in the single-shot setting as menus of binary lotteries and show that under extreme risk aversion, for any value distribution, almost the entire social welfare can be extracted as revenue. Second, we show that under a reasonable bounded-risk-aversion assumption, posted pricing obtains a constant risk-robust approximation to the optimal revenue. Third, in contrast to this positive result, we show that in dynamic settings it is not possible to achieve any constant factor approximation to revenue in a risk-robust manner.",studi model risk agent undervalu uncertain outcom model event occur probabl worth strict less time valu event occur certainti properti formal form uncertainti weight function model special case model consid prospect theori altern heavili studi expect util theori econom goal understand implic kind attitud toward risk mechan design specif examin three aspect revenu optim mechan design relat risk first doe risk avers affect use random within mechan design second dynam set buyer lack inform futur valu allow seller extract revenu doe risk avers affect abil seller exploit uncertainti futur event final possibl obtain approxim guarante revenu robust specif risk model present three main result first character optim mechan singl shot set menus binari lotteri show extrem risk avers ani valu distribut almost entir social welfar extract revenu second show reason bound risk avers assumpt post price obtain constant risk robust approxim optim revenu third contrast posit result show dynam set possibl achiev ani constant factor approxim revenu risk robust manner,"['Shuchi Chawla', 'Kira Goldner', 'J. Benjamin Miller', 'Emmanouil Pountourakis']",['cs.GT'],False,False,True,False,False,True
608,2017-03-28T14:08:00Z,2017-03-24T17:05:58Z,http://arxiv.org/abs/1703.08509v1,http://arxiv.org/pdf/1703.08509v1,Generalized Nash Equilibrium Problem by the Alternating Direction Method   of Multipliers,general nash equilibrium problem altern direct method multipli,"In this paper, the problem of finding a generalized Nash equilibrium (GNE) of a networked game is studied. Players are only able to choose their decisions from a feasible action set. The feasible set is considered to be a private linear equality constraint that is coupled through decisions of the other players. We consider that each player has his own private constraint and it has not to be shared with the other players. This general case also embodies the one with shared constraints between players and it can be also simply extended to the case with inequality constraints. Since the players don't have access to other players' actions, they need to exchange estimates of others' actions and a local copy of the Lagrangian multiplier with their neighbors over a connected communication graph. We develop a relatively fast algorithm by reformulating the conservative GNE problem within the framework of inexact-ADMM. The convergence of the algorithm is guaranteed under a few mild assumptions on cost functions. Finally, the algorithm is simulated for a wireless ad-hoc network.",paper problem find general nash equilibrium gne network game studi player onli abl choos decis feasibl action set feasibl set consid privat linear equal constraint coupl decis player consid player privat constraint share player general case also embodi one share constraint player also simpli extend case inequ constraint sinc player access player action need exchang estim action local copi lagrangian multipli neighbor connect communic graph develop relat fast algorithm reformul conserv gne problem within framework inexact admm converg algorithm guarante mild assumpt cost function final algorithm simul wireless ad hoc network,"['Farzad Salehisadaghiani', 'Lacra Pavel']","['cs.GT', 'cs.SY']",False,False,True,False,False,True
611,2017-03-28T14:08:03Z,2017-03-22T15:05:20Z,http://arxiv.org/abs/1703.07695v1,http://arxiv.org/pdf/1703.07695v1,Selfish Cops and Adversarial Robber: Multi-Player Pursuit Evasion on   Graphs,selfish cop adversari robber multi player pursuit evas graph,We introduce and study the game of Selfish Cops and Adversarial Robber (SCAR) which is an N-player generalization of the classic two-player cops and robbers (CR) game. We prove that SCAR has a Nash equilibrium in deterministic strategies.,introduc studi game selfish cop adversari robber scar player general classic two player cop robber cr game prove scar nash equilibrium determinist strategi,"['G. Konstantinidis', 'Ath. Kehagias']","['cs.DM', 'cs.GT']",False,False,True,False,False,True
612,2017-03-28T14:08:03Z,2017-03-22T13:37:23Z,http://arxiv.org/abs/1703.07647v1,http://arxiv.org/pdf/1703.07647v1,Algorithms for Nash and Pareto Equilibria for Resource Allocation in   Multiple Femtocells,algorithm nash pareto equilibria resourc alloc multipl femtocel,"We consider a cellular system with multiple Femtocells operating in a Macrocell. They are sharing a set of communication channels. Each Femtocell has multiple users requiring certain minimum rate guarantees. Each channel has a peak power constraint to limit interference to the Macro Base Station (BS). We formulate the problem of channel allocation and power control at the Femtocells as a noncooperative Game. We develop decentralized algorithms to obtain a Coarse Correlated equilibrium that satisfies the QoS of each user. If the QoS of all the users cannot be satisfied, then we obtain a fair equilibrium. Finally we also provide a decentralized algorithm to reach a Pareto and a Nash Bargaining solution which has a much lower complexity than the algorithm to compute the NE.",consid cellular system multipl femtocel oper macrocel share set communic channel femtocel multipl user requir certain minimum rate guarante channel peak power constraint limit interfer macro base station bs formul problem channel alloc power control femtocel noncoop game develop decentr algorithm obtain coars correl equilibrium satisfi qos user qos user cannot satisfi obtain fair equilibrium final also provid decentr algorithm reach pareto nash bargain solut much lower complex algorithm comput ne,"['V. Udaya Sankar', 'Vinod Sharma']","['cs.GT', 'cs.NI']",False,False,True,False,False,True
613,2017-03-28T14:08:03Z,2017-03-22T02:57:19Z,http://arxiv.org/abs/1703.07499v1,http://arxiv.org/pdf/1703.07499v1,Hardware Trojan Detection Game: A Prospect-Theoretic Approach,hardwar trojan detect game prospect theoret approach,"Outsourcing integrated circuit (IC) manufacturing to offshore foundries has grown exponentially in recent years. Given the critical role of ICs in the control and operation of vehicular systems and other modern engineering designs, such offshore outsourcing has led to serious security threats due to the potential of insertion of hardware trojans - malicious designs that, when activated, can lead to highly detrimental consequences. In this paper, a novel game-theoretic framework is proposed to analyze the interactions between a hardware manufacturer, acting as attacker, and an IC testing facility, acting as defender. The problem is formulated as a noncooperative game in which the attacker must decide on the type of trojan that it inserts while taking into account the detection penalty as well as the damage caused by the trojan. Meanwhile, the resource-constrained defender must decide on the best testing strategy that allows optimizing its overall utility which accounts for both damages and the fines. The proposed game is based on the robust behavioral framework of prospect theory (PT) which allows capturing the potential uncertainty, risk, and irrational behavior in the decision making of both the attacker and defender. For both, the standard rational expected utility (EUT) case and the PT case, a novel algorithm based on fictitious play is proposed and shown to converge to a mixed-strategy Nash equilibrium. For an illustrative case study, thorough analytical results are derived for both EUT and PT to study the properties of the reached equilibrium as well as the impact of key system parameters such as the defender-set fine. Simulation results assess the performance of the proposed framework under both EUT and PT and show that the use of PT will provide invaluable insights on the outcomes of the proposed hardware trojan game, in particular, and system security, in general.",outsourc integr circuit ic manufactur offshor foundri grown exponenti recent year given critic role ic control oper vehicular system modern engin design offshor outsourc led serious secur threat due potenti insert hardwar trojan malici design activ lead high detriment consequ paper novel game theoret framework propos analyz interact hardwar manufactur act attack ic test facil act defend problem formul noncoop game attack must decid type trojan insert take account detect penalti well damag caus trojan meanwhil resourc constrain defend must decid best test strategi allow optim overal util account damag fine propos game base robust behavior framework prospect theori pt allow captur potenti uncertainti risk irrat behavior decis make attack defend standard ration expect util eut case pt case novel algorithm base fictiti play propos shown converg mix strategi nash equilibrium illustr case studi thorough analyt result deriv eut pt studi properti reach equilibrium well impact key system paramet defend set fine simul result assess perform propos framework eut pt show use pt provid invalu insight outcom propos hardwar trojan game particular system secur general,"['Walid Saad', 'Anibal Sanjab', 'Yunpeng Wang', 'Charles Kamhoua', 'Kevin Kwiat']","['cs.IT', 'cs.CR', 'cs.GT', 'math.IT']",False,False,True,False,False,True
614,2017-03-28T14:08:03Z,2017-03-21T03:52:20Z,http://arxiv.org/abs/1703.07043v1,http://arxiv.org/pdf/1703.07043v1,Energy Efficient Power Control for the Two-tier Networks with Small   Cells and Massive MIMO,energi effici power control two tier network small cell massiv mimo,"In this paper, energy efficient power control for the uplink two-tier networks where a macrocell tier with a massive multiple-input multiple-output (MIMO) base station is overlaid with a small cell tier is investigated. We propose a distributed energy efficient power control algorithm which allows each user in the two-tier network taking individual decisions to optimize its own energy efficiency (EE) for the multi-user and multi-cell scenario. The distributed power control algorithm is implemented by decoupling the EE optimization problem into two steps. In the first step, we propose to assign the users on the same resource into the same group and each group can optimize its own EE, respectively. In the second step, multiple power control games based on evolutionary game theory (EGT) are formulated for each group, which allows each user optimizing its own EE. In the EGT-based power control games, each player selects a strategy giving a higher payoff than the average payoff, which can improve the fairness among the users. The proposed algorithm has a linear complexity with respect to the number of subcarriers and the number of cells in comparison with the brute force approach which has an exponential complexity. Simulation results show the remarkable improvements in terms of fairness by using the proposed algorithm.",paper energi effici power control uplink two tier network macrocel tier massiv multipl input multipl output mimo base station overlaid small cell tier investig propos distribut energi effici power control algorithm allow user two tier network take individu decis optim energi effici ee multi user multi cell scenario distribut power control algorithm implement decoupl ee optim problem two step first step propos assign user resourc group group optim ee respect second step multipl power control game base evolutionari game theori egt formul group allow user optim ee egt base power control game player select strategi give higher payoff averag payoff improv fair among user propos algorithm linear complex respect number subcarri number cell comparison brute forc approach exponenti complex simul result show remark improv term fair use propos algorithm,"['Ningning Lu', 'Yanxiang Jiang', 'Fuchun Zheng', 'Xiaohu You']","['cs.NI', 'cs.GT', 'cs.IT', 'math.IT']",False,False,True,False,False,True
618,2017-03-28T14:08:03Z,2017-03-16T14:32:23Z,http://arxiv.org/abs/1703.05641v1,http://arxiv.org/pdf/1703.05641v1,Distributed Mechanism Design with Learning Guarantees,distribut mechan design learn guarante,"In this paper, we consider two common resource allocation problems: sharing $ K $ infinitely divisible resources among strategic agents for their private consumption (private goods problem) and determining the level of a single infinitely divisible common resource which is consumed simultaneously by strategic agents (public goods problem). For each problem, we present a distributed mechanism for a set of agents who communicate through a given network. We prove that the mechanism produces a unique Nash Equilibrium (NE) and it fully implements the social welfare maximizing allocation. In addition, the mechanism is budget-balanced at NE. We also show that the mechanism induces a game with contractive best-response, leading to guaranteed convergence for all learning strategies within the Adaptive Best-Response (ABR) dynamics class. The convergent point is the unique (and efficient) Nash equilibrium.",paper consid two common resourc alloc problem share infinit divis resourc among strateg agent privat consumpt privat good problem determin level singl infinit divis common resourc consum simultan strateg agent public good problem problem present distribut mechan set agent communic given network prove mechan produc uniqu nash equilibrium ne fulli implement social welfar maxim alloc addit mechan budget balanc ne also show mechan induc game contract best respons lead guarante converg learn strategi within adapt best respons abr dynam class converg point uniqu effici nash equilibrium,"['Abhinav Sinha', 'Achilleas Anastasopoulos']",['cs.GT'],False,False,True,False,False,True
619,2017-03-28T14:08:03Z,2017-03-16T09:15:51Z,http://arxiv.org/abs/1703.05519v1,http://arxiv.org/pdf/1703.05519v1,Arrovian Aggregation via Pairwise Utilitarianism,arrovian aggreg via pairwis utilitarian,"We consider Arrovian aggregation of preferences over lotteries that are represented by skew-symmetric bilinear (SSB) utility functions, a significant generalization of von Neumann-Morgenstern utility functions due to Fishburn, in which utility is assigned to pairs of alternatives. We show that the largest domain of preferences that simultaneously allows for independence of irrelevant alternatives and Pareto optimality when comparing lotteries based on accumulated SSB welfare is a domain in which preferences over lotteries are completely determined by ordinal preferences over pure alternatives. In particular, a lottery is preferred to another lottery if and only if the former is more likely to return a preferred alternative. Preferences over pure alternatives are unrestricted. We argue that SSB welfare maximization for this domain constitutes an appealing probabilistic social choice function.",consid arrovian aggreg prefer lotteri repres skew symmetr bilinear ssb util function signific general von neumann morgenstern util function due fishburn util assign pair altern show largest domain prefer simultan allow independ irrelev altern pareto optim compar lotteri base accumul ssb welfar domain prefer lotteri complet determin ordin prefer pure altern particular lotteri prefer anoth lotteri onli former like return prefer altern prefer pure altern unrestrict argu ssb welfar maxim domain constitut appeal probabilist social choic function,"['Florian Brandl', 'Felix Brandt']","['cs.GT', 'cs.MA']",False,False,False,False,False,True
620,2017-03-28T14:08:07Z,2017-03-15T21:16:15Z,http://arxiv.org/abs/1703.05388v1,http://arxiv.org/pdf/1703.05388v1,A distributed primal-dual algorithm for computation of generalized Nash   equilibria with shared affine coupling constraints via operator splitting   methods,distribut primal dual algorithm comput general nash equilibria share affin coupl constraint via oper split method,"In this paper, we propose a distributed primal-dual algorithm for computation of a generalized Nash equilibrium (GNE) in noncooperative games over network systems. In the considered game, not only each player's local objective function depends on other players' decisions, but also the feasible decision sets of all the players are coupled together with a globally shared affine inequality constraint. Adopting the variational GNE, that is the solution of a variational inequality, as a refinement of GNE, we introduce a primal-dual algorithm that players can use to seek it in a distributed manner. Each player only needs to know its local objective function, local feasible set, and a local block of the affine constraint. Meanwhile, each player only needs to observe the decisions on which its local objective function explicitly depends through the interference graph and share information related to multipliers with its neighbors through a multiplier graph. Through a primal-dual analysis and an augmentation of variables, we reformulate the problem as finding the zeros of a sum of monotone operators. Our distributed primal-dual algorithm is based on forward-backward operator splitting methods. We prove its convergence to the variational GNE for fixed step-sizes under some mild assumptions. Then a distributed algorithm with inertia is also introduced and analyzed for variational GNE seeking. Finally, numerical simulations for network Cournot competition are given to illustrate the algorithm efficiency and performance.",paper propos distribut primal dual algorithm comput general nash equilibrium gne noncoop game network system consid game onli player local object function depend player decis also feasibl decis set player coupl togeth global share affin inequ constraint adopt variat gne solut variat inequ refin gne introduc primal dual algorithm player use seek distribut manner player onli need know local object function local feasibl set local block affin constraint meanwhil player onli need observ decis local object function explicit depend interfer graph share inform relat multipli neighbor multipli graph primal dual analysi augment variabl reformul problem find zero sum monoton oper distribut primal dual algorithm base forward backward oper split method prove converg variat gne fix step size mild assumpt distribut algorithm inertia also introduc analyz variat gne seek final numer simul network cournot competit given illustr algorithm effici perform,"['Peng Yi', 'Lacra Pavel']","['math.OC', 'cs.GT', 'cs.SY']",False,False,False,False,False,True
621,2017-03-28T14:08:07Z,2017-03-14T22:13:20Z,http://arxiv.org/abs/1703.04756v1,http://arxiv.org/pdf/1703.04756v1,Weighted Voting Via No-Regret Learning,weight vote via regret learn,"Voting systems typically treat all voters equally. We argue that perhaps they should not: Voters who have supported good choices in the past should be given higher weight than voters who have supported bad ones. To develop a formal framework for desirable weighting schemes, we draw on no-regret learning. Specifically, given a voting rule, we wish to design a weighting scheme such that applying the voting rule, with voters weighted by the scheme, leads to choices that are almost as good as those endorsed by the best voter in hindsight. We derive possibility and impossibility results for the existence of such weighting schemes, depending on whether the voting rule and the weighting scheme are deterministic or randomized, as well as on the social choice axioms satisfied by the voting rule.",vote system typic treat voter equal argu perhap voter support good choic past given higher weight voter support bad one develop formal framework desir weight scheme draw regret learn specif given vote rule wish design weight scheme appli vote rule voter weight scheme lead choic almost good endors best voter hindsight deriv possibl imposs result exist weight scheme depend whether vote rule weight scheme determinist random well social choic axiom satisfi vote rule,"['Nika Haghtalab', 'Ritesh Noothigattu', 'Ariel D. Procaccia']","['cs.GT', 'cs.AI', 'cs.LG', 'cs.MA']",False,False,False,False,False,True
622,2017-03-28T14:08:07Z,2017-03-13T02:05:58Z,http://arxiv.org/abs/1703.04225v1,http://arxiv.org/pdf/1703.04225v1,New algorithms for matching problems,new algorithm match problem,"The standard two-sided and one-sided matching problems, and the closely related school choice problem, have been widely studied from an axiomatic viewpoint. A small number of algorithms dominate the literature. For two-sided matching, the Gale-Shapley algorithm; for one-sided matching, (random) Serial Dictatorship and Probabilistic Serial rule; for school choice, Gale-Shapley and the Boston mechanisms.   The main reason for the dominance of these algorithms is their good (worst-case) axiomatic behaviour with respect to notions of efficiency and strategyproofness. However if we shift the focus to fairness, social welfare, tradeoffs between incompatible axioms, and average-case analysis, it is far from clear that these algorithms are optimal.   We investigate new algorithms several of which have not appeared (to our knowledge) in the literature before. We give a unified presentation in which algorithms for 2-sided matching yield 1-sided matching algorithms in a systematic way. In addition to axiomatic properties, we investigate agent welfare using both theoretical and computational approaches. We find that some of the new algorithms are worthy of consideration for certain applications. In particular, when considering welfare under truthful preferences, some of the new algorithms outperform the classic ones.",standard two side one side match problem close relat school choic problem wide studi axiomat viewpoint small number algorithm domin literatur two side match gale shapley algorithm one side match random serial dictatorship probabilist serial rule school choic gale shapley boston mechan main reason domin algorithm good worst case axiomat behaviour respect notion effici strategyproof howev shift focus fair social welfar tradeoff incompat axiom averag case analysi far clear algorithm optim investig new algorithm sever appear knowledg literatur befor give unifi present algorithm side match yield side match algorithm systemat way addit axiomat properti investig agent welfar use theoret comput approach find new algorithm worthi consider certain applic particular consid welfar truth prefer new algorithm outperform classic one,"['Jacky Lo', 'Mark C. Wilson']","['cs.GT', '91B68', 'J.4']",False,False,False,False,False,True
623,2017-03-28T14:08:07Z,2017-03-12T17:11:49Z,http://arxiv.org/abs/1703.04143v1,http://arxiv.org/pdf/1703.04143v1,Bernoulli Factories and Black-Box Reductions in Mechanism Design,bernoulli factori black box reduct mechan design,"We provide a polynomial time reduction from Bayesian incentive compatible mechanism design to Bayesian algorithm design for welfare maximization problems. Unlike prior results, our reduction achieves exact incentive compatibility for problems with multi-dimensional and continuous type spaces. The key technical barrier preventing exact incentive compatibility in prior black-box reductions is that repairing violations of incentive constraints requires understanding the distribution of the mechanism's output. Reductions that instead estimate the output distribution by sampling inevitably suffer from sampling error, which typically precludes exact incentive compatibility.   We overcome this barrier by employing and generalizing the computational model in the literature on Bernoulli Factories. In a Bernoulli factory problem, one is given a function mapping the bias of an ""input coin"" to that of an ""output coin"", and the challenge is to efficiently simulate the output coin given sample access to the input coin. We generalize this to the ""expectations from samples"" computational model, in which an instance is specified by a function mapping the expected values of a set of input distributions to a distribution over outcomes. The challenge is to give a polynomial time algorithm that exactly samples from the distribution over outcomes given only sample access to the input distributions. In this model, we give a polynomial time algorithm for the exponential weights: expected values of the input distributions correspond to the weights of alternatives and we wish to select an alternative with probability proportional to an exponential function of its weight. This algorithm is the key ingredient in designing an incentive compatible mechanism for bipartite matching, which can be used to make the approximately incentive compatible reduction of Hartline et al. (2015) exactly incentive compatible.",provid polynomi time reduct bayesian incent compat mechan design bayesian algorithm design welfar maxim problem unlik prior result reduct achiev exact incent compat problem multi dimension continu type space key technic barrier prevent exact incent compat prior black box reduct repair violat incent constraint requir understand distribut mechan output reduct instead estim output distribut sampl inevit suffer sampl error typic preclud exact incent compat overcom barrier employ general comput model literatur bernoulli factori bernoulli factori problem one given function map bias input coin output coin challeng effici simul output coin given sampl access input coin general expect sampl comput model instanc specifi function map expect valu set input distribut distribut outcom challeng give polynomi time algorithm exact sampl distribut outcom given onli sampl access input distribut model give polynomi time algorithm exponenti weight expect valu input distribut correspond weight altern wish select altern probabl proport exponenti function weight algorithm key ingredi design incent compat mechan bipartit match use make approxim incent compat reduct hartlin et al exact incent compat,"['Shaddin Dughmi', 'Jason Hartline', 'Robert Kleinberg', 'Rad Niazadeh']","['cs.GT', 'cs.CC', 'cs.DS', 'math.PR']",False,False,False,False,False,True
624,2017-03-28T14:08:07Z,2017-03-11T05:35:09Z,http://arxiv.org/abs/1703.03912v1,http://arxiv.org/pdf/1703.03912v1,The Curse of Correlation in Security Games and Principle of Max-Entropy,curs correl secur game principl max entropi,"In this paper, we identify and study a fundamental, yet underexplored, phenomenon in security games, which we term the Curse of Correlation (CoC). Specifically, we observe that there is inevitable correlation among the protection status of different targets. Such correlation is a crucial concern, especially in spatio-temporal domains like conservation area patrolling, where attackers can monitor patrollers at certain areas and then infer their patrolling routes using such correlation. To mitigate this issue, we introduce the principle of max-entropy to security games, and focus on designing entropy-maximizing defending strategies for the spatio-temporal security game -- a major victim of CoC. We prove that the problem is #P-hard in general, but propose efficient algorithms in well-motivated special settings. Our experiments show significant advantages of the max-entropy algorithms against previous algorithms.",paper identifi studi fundament yet underexplor phenomenon secur game term curs correl coc specif observ inevit correl among protect status differ target correl crucial concern especi spatio tempor domain like conserv area patrol attack monitor patrol certain area infer patrol rout use correl mitig issu introduc principl max entropi secur game focus design entropi maxim defend strategi spatio tempor secur game major victim coc prove problem hard general propos effici algorithm well motiv special set experi show signific advantag max entropi algorithm previous algorithm,"['Haifeng Xu', 'Milind Tambe', 'Shaddin Dughmi', 'Venil Loyd Noronha']","['cs.GT', 'cs.AI', 'cs.CR']",False,False,True,False,False,True
625,2017-03-28T14:08:08Z,2017-03-10T21:31:32Z,http://arxiv.org/abs/1703.03846v1,http://arxiv.org/pdf/1703.03846v1,Socially Optimal Mining Pools,social optim mine pool,"Mining for Bitcoins is a high-risk high-reward activity. Miners, seeking to reduce their variance and earn steadier rewards, collaborate in pooling strategies where they jointly mine for Bitcoins. Whenever some pool participant is successful, the earned rewards are appropriately split among all pool participants. Currently a dozen of different pooling strategies (i.e., methods for distributing the rewards) are in use for Bitcoin mining.   We here propose a formal model of utility and social welfare for Bitcoin mining (and analogous mining systems) based on the theory of discounted expected utility, and next study pooling strategies that maximize the social welfare of miners. Our main result shows that one of the pooling strategies actually employed in practice--the so-called geometric pay pool--achieves the optimal steady-state utility for miners when its parameters are set appropriately.   Our results apply not only to Bitcoin mining pools, but any other form of pooled mining or crowdsourcing computations where the participants engage in repeated random trials towards a common goal, and where ""partial"" solutions can be efficiently verified.",mine bitcoin high risk high reward activ miner seek reduc varianc earn steadier reward collabor pool strategi joint mine bitcoin whenev pool particip success earn reward appropri split among pool particip current dozen differ pool strategi method distribut reward use bitcoin mine propos formal model util social welfar bitcoin mine analog mine system base theori discount expect util next studi pool strategi maxim social welfar miner main result show one pool strategi actual employ practic call geometr pay pool achiev optim steadi state util miner paramet set appropri result appli onli bitcoin mine pool ani form pool mine crowdsourc comput particip engag repeat random trial toward common goal partial solut effici verifi,"['Ben A. Fisch', 'Rafael Pass', 'Abhi Shelat']",['cs.GT'],False,False,True,False,False,True
627,2017-03-28T14:08:08Z,2017-03-10T13:54:29Z,http://arxiv.org/abs/1703.03687v1,http://arxiv.org/pdf/1703.03687v1,Best Laid Plans of Lions and Men,best laid plan lion men,"We answer the following question dating back to J.E. Littlewood (1885 - 1977): Can two lions catch a man in a bounded area with rectifiable lakes? The lions and the man are all assumed to be points moving with at most unit speed. That the lakes are rectifiable means that their boundaries are finitely long. This requirement is to avoid pathological examples where the man survives forever because any path to the lions is infinitely long. We show that the answer to the question is not always ""yes"" by giving an example of a region $R$ in the plane where the man has a strategy to survive forever. $R$ is a polygonal region with holes and the exterior and interior boundaries are pairwise disjoint, simple polygons. Our construction is the first truly two-dimensional example where the man can survive.   Next, we consider the following game played on the entire plane instead of a bounded area: There is any finite number of unit speed lions and one fast man who can run with speed $1+\varepsilon$ for some value $\varepsilon>0$. Can the man always survive? We answer the question in the affirmative for any constant $\varepsilon>0$.",answer follow question date back littlewood two lion catch man bound area rectifi lake lion man assum point move unit speed lake rectifi mean boundari finit long requir avoid patholog exampl man surviv forev becaus ani path lion infinit long show answer question alway yes give exampl region plane man strategi surviv forev polygon region hole exterior interior boundari pairwis disjoint simpl polygon construct first truli two dimension exampl man surviv next consid follow game play entir plane instead bound area ani finit number unit speed lion one fast man run speed varepsilon valu varepsilon man alway surviv answer question affirm ani constant varepsilon,"['Mikkel Abrahamsen', 'Jacob Holm', 'Eva Rotenberg', 'Christian Wulff-Nilsen']","['cs.CG', 'cs.GT']",False,False,True,False,False,True
628,2017-03-28T14:08:08Z,2017-03-10T01:47:23Z,http://arxiv.org/abs/1703.03511v1,http://arxiv.org/pdf/1703.03511v1,Towards Computing Victory Margins in STV Elections,toward comput victori margin stv elect,"The Single Transferable Vote (STV) is a system of preferential voting employed in multi-seat elections. Each vote cast by a voter is a (potentially partial) ranking over a set of candidates. No techniques currently exist for computing the margin of victory (MOV) in STV elections. The MOV is the smallest number of vote manipulations (changes, additions, and deletions) required to bring about a change in the set of elected candidates. Knowledge of the MOV of an election gives greater insight into both how much time and money should be spent on the auditing of the election, and whether uncovered mistakes (such as ballot box losses) throw the election result into doubt---requiring a costly repeat election---or can be safely ignored. In this paper, we present algorithms for computing lower and upper bounds on the MOV in STV elections. In small instances, these algorithms are able to compute exact margins.",singl transfer vote stv system preferenti vote employ multi seat elect vote cast voter potenti partial rank set candid techniqu current exist comput margin victori mov stv elect mov smallest number vote manipul chang addit delet requir bring chang set elect candid knowledg mov elect give greater insight much time money spent audit elect whether uncov mistak ballot box loss throw elect result doubt requir cost repeat elect safe ignor paper present algorithm comput lower upper bound mov stv elect small instanc algorithm abl comput exact margin,"['Michelle Blom', 'Peter J. Stuckey', 'Vanessa J. Teague']",['cs.GT'],False,False,True,False,False,True
629,2017-03-28T14:08:08Z,2017-03-09T22:47:10Z,http://arxiv.org/abs/1703.03484v1,http://arxiv.org/pdf/1703.03484v1,Combinatorial Auctions with Online XOS Bidders,combinatori auction onlin xos bidder,"In combinatorial auctions, a designer must decide how to allocate a set of indivisible items amongst a set of bidders. Each bidder has a valuation function which gives the utility they obtain from any subset of the items. Our focus is specifically on welfare maximization, where the objective is to maximize the sum of valuations that the bidders place on the items that they were allocated (the valuation functions are assumed to be reported truthfully). We analyze an online problem in which the algorithm is not given the set of bidders in advance. Instead, the bidders are revealed sequentially in a uniformly random order, similarly to secretary problems. The algorithm must make an irrevocable decision about which items to allocate to the current bidder before the next one is revealed. When the valuation functions lie in the class $XOS$ (which includes submodular functions), we provide a black box reduction from offline to online optimization. Specifically, given an $\alpha$-approximation algorithm for offline welfare maximization, we show how to create a $(0.199 \alpha)$-approximation algorithm for the online problem. Our algorithm draws on connections to secretary problems; in fact, we show that the online welfare maximization problem itself can be viewed as a particular kind of secretary problem with nonuniform arrival order.",combinatori auction design must decid alloc set indivis item amongst set bidder bidder valuat function give util obtain ani subset item focus specif welfar maxim object maxim sum valuat bidder place item alloc valuat function assum report truth analyz onlin problem algorithm given set bidder advanc instead bidder reveal sequenti uniform random order similar secretari problem algorithm must make irrevoc decis item alloc current bidder befor next one reveal valuat function lie class xos includ submodular function provid black box reduct offlin onlin optim specif given alpha approxim algorithm offlin welfar maxim show creat alpha approxim algorithm onlin problem algorithm draw connect secretari problem fact show onlin welfar maxim problem view particular kind secretari problem nonuniform arriv order,"['Shaddin Dughmi', 'Bryan Wilder']","['cs.GT', 'cs.DS']",False,False,False,False,False,True
631,2017-03-28T14:08:12Z,2017-03-09T02:50:49Z,http://arxiv.org/abs/1703.03111v1,http://arxiv.org/pdf/1703.03111v1,Statistical Cost Sharing,statist cost share,"We study the cost sharing problem for cooperative games in situations where the cost function $C$ is not available via oracle queries, but must instead be derived from data, represented as tuples $(S, C(S))$, for different subsets $S$ of players. We formalize this approach, which we call statistical cost sharing, and consider the computation of the core and the Shapley value, when the tuples are drawn from some distribution $\mathcal{D}$.   Previous work by Balcan et al. in this setting showed how to compute cost shares that satisfy the core property with high probability for limited classes of functions. We expand on their work and give an algorithm that computes such cost shares for any function with a non-empty core. We complement these results by proving an inapproximability lower bound for a weaker relaxation.   We then turn our attention to the Shapley value. We first show that when cost functions come from the family of submodular functions with bounded curvature, $\kappa$, the Shapley value can be approximated from samples up to a $\sqrt{1 - \kappa}$ factor, and that the bound is tight. We then define statistical analogues of the Shapley axioms, and derive a notion of statistical Shapley value. We show that these can always be approximated arbitrarily well for general functions over any distribution $\mathcal{D}$.",studi cost share problem cooper game situat cost function avail via oracl queri must instead deriv data repres tupl differ subset player formal approach call statist cost share consid comput core shapley valu tupl drawn distribut mathcal previous work balcan et al set show comput cost share satisfi core properti high probabl limit class function expand work give algorithm comput cost share ani function non empti core complement result prove inapproxim lower bound weaker relax turn attent shapley valu first show cost function come famili submodular function bound curvatur kappa shapley valu approxim sampl sqrt kappa factor bound tight defin statist analogu shapley axiom deriv notion statist shapley valu show alway approxim arbitrarili well general function ani distribut mathcal,"['Eric Balkanski', 'Umar Syed', 'Sergei Vassilvitskii']","['cs.GT', 'cs.LG']",False,False,False,False,False,True
636,2017-03-28T14:08:12Z,2017-03-06T13:01:12Z,http://arxiv.org/abs/1703.01851v1,http://arxiv.org/pdf/1703.01851v1,Approximation Algorithms for Maximin Fair Division,approxim algorithm maximin fair divis,"We consider the problem of dividing indivisible goods fairly among $n$ agents who have additive and submodular valuations for the goods. Our fairness guarantees are in terms of the maximin share, that is defined to be the maximum value that an agent can ensure for herself, if she were to partition the goods into $n$ bundles, and then receive a minimum valued bundle. Since maximin fair allocations (i.e., allocations in which each agent gets at least her maximin share) do not always exist, prior work has focussed on approximation results that aim to find allocations in which the value of the bundle allocated to each agent is (multiplicatively) as close to her maximin share as possible. In particular, Procaccia and Wang (2014) along with Amanatidis et al. (2015) have shown that under a $2/3$-approximate maximin fair allocation always exists and can be found in polynomial time. We complement these results by developing a simple and efficient algorithm that achieves the same approximation guarantee.   Furthermore, we initiate the study of approximate maximin fair division under submodular valuations. Specifically, we show that when the valuations of the agents are nonnegative, monotone, and submodular, then a $1/10$-approximate maximin fair allocation is guaranteed to exist. In fact, we show that such an allocation (with a slightly worse approximation guarantee) can be efficiently found by a simple round-robin algorithm. A technical contribution of the paper is to analyze the performance of this combinatorial algorithm by employing the concept of multilinear extensions.",consid problem divid indivis good fair among agent addit submodular valuat good fair guarante term maximin share defin maximum valu agent ensur partit good bundl receiv minimum valu bundl sinc maximin fair alloc alloc agent get least maximin share alway exist prior work focuss approxim result aim find alloc valu bundl alloc agent multipl close maximin share possibl particular procaccia wang along amanatidi et al shown approxim maximin fair alloc alway exist found polynomi time complement result develop simpl effici algorithm achiev approxim guarante furthermor initi studi approxim maximin fair divis submodular valuat specif show valuat agent nonneg monoton submodular approxim maximin fair alloc guarante exist fact show alloc slight wors approxim guarante effici found simpl round robin algorithm technic contribut paper analyz perform combinatori algorithm employ concept multilinear extens,"['Siddharth Barman', 'Sanath Kumar Krishna Murthy']",['cs.GT'],False,False,False,False,False,True
637,2017-03-28T14:08:12Z,2017-03-22T17:07:13Z,http://arxiv.org/abs/1703.01649v2,http://arxiv.org/pdf/1703.01649v2,Fair Allocation of Indivisible Goods to Asymmetric Agents,fair alloc indivis good asymmetr agent,"We study fair allocation of indivisible goods to agents with unequal entitlements. Fair allocation has been the subject of many studies in both divisible and indivisible settings. Our emphasis is on the case where the goods are indivisible and agents have unequal entitlements. This problem is a generalization of the work by Procaccia and Wang wherein the agents are assumed to be symmetric with respect to their entitlements. Although Procaccia and Wang show an almost fair (constant approximation) allocation exists in their setting, our main result is in sharp contrast to their observation. We show that, in some cases with $n$ agents, no allocation can guarantee better than $1/n$ approximation of a fair allocation when the entitlements are not necessarily equal. Furthermore, we devise a simple algorithm that ensures a $1/n$ approximation guarantee. Our second result is for a restricted version of the problem where the valuation of every agent for each good is bounded by the total value he wishes to receive in a fair allocation. Although this assumption might seem w.l.o.g, we show it enables us to find a $1/2$ approximation fair allocation via a greedy algorithm. Finally, we run some experiments on real-world data and show that, in practice, a fair allocation is likely to exist. We also support our experiments by showing positive results for two stochastic variants of the problem, namely stochastic agents and stochastic items.",studi fair alloc indivis good agent unequ entitl fair alloc subject mani studi divis indivis set emphasi case good indivis agent unequ entitl problem general work procaccia wang wherein agent assum symmetr respect entitl although procaccia wang show almost fair constant approxim alloc exist set main result sharp contrast observ show case agent alloc guarante better approxim fair alloc entitl necessarili equal furthermor devis simpl algorithm ensur approxim guarante second result restrict version problem valuat everi agent good bound total valu wish receiv fair alloc although assumpt might seem show enabl us find approxim fair alloc via greedi algorithm final run experi real world data show practic fair alloc like exist also support experi show posit result two stochast variant problem name stochast agent stochast item,"['Alireza Farhadi', 'Mohammad Ghodsi', 'MohammadTaghi Hajiaghayi', 'Sebastien Lahaie', 'David Pennock', 'Masoud Seddighin', 'Saeed Seddighin', 'Hadi Yami']",['cs.GT'],False,False,False,False,False,True
638,2017-03-28T14:08:12Z,2017-03-07T13:05:15Z,http://arxiv.org/abs/1703.01599v2,http://arxiv.org/pdf/1703.01599v2,How bad is selfish routing in practice?,bad selfish rout practic,"Routing games are one of the most successful domains of application of game theory. It is well understood that simple dynamics converge to equilibria, whose performance is nearly optimal regardless of the size of the network or the number of agents. These strong theoretical assertions prompt a natural question: How well do these pen-and-paper calculations agree with the reality of everyday traffic routing? We focus on a semantically rich dataset from Singapore's National Science Experiment that captures detailed information about the daily behavior of thousands of Singaporean students. Using this dataset, we can identify the routes as well as the modes of transportation used by the students, e.g. car (driving or being driven to school) versus bus or metro, estimate source and sink destinations (home-school) and trip duration, as well as their mode-dependent available routes. We quantify both the system and individual optimality. Our estimate of the Empirical Price of Anarchy lies between 1.11 and 1.22. Individually, the typical behavior is consistent from day to day and nearly optimal, with low regret for not deviating to alternative paths.",rout game one success domain applic game theori well understood simpl dynam converg equilibria whose perform near optim regardless size network number agent strong theoret assert prompt natur question well pen paper calcul agre realiti everyday traffic rout focus semant rich dataset singapor nation scienc experi captur detail inform daili behavior thousand singaporean student use dataset identifi rout well mode transport use student car drive driven school versus bus metro estim sourc sink destin home school trip durat well mode depend avail rout quantifi system individu optim estim empir price anarchi lie individu typic behavior consist day day near optim low regret deviat altern path,"['Barnabé Monnot', 'Francisco Benita', 'Georgios Piliouras']",['cs.GT'],False,False,True,False,False,True
639,2017-03-28T14:08:12Z,2017-03-05T13:53:05Z,http://arxiv.org/abs/1703.06824v1,http://arxiv.org/abs/1703.06824v1,Energy Efficient Non-Cooperative Power Control in Small Cell Networks,energi effici non cooper power control small cell network,"In this paper, energy efficient power control for small cells underlaying a macro cellular network is investigated. We formulate the power control problem in self-organizing small cell networks as a non-cooperative game, and propose a distributed energy efficient power control scheme, which allows the small base stations (SBSs) to take individual decisions for attaining the Nash equilibrium (NE) with minimum information exchange. Specially, in the non-cooperative power control game, a non-convex optimization problem is formulated for each SBS to maximize their energy efficiency (EE). By exploiting the properties of parameter-free fractional programming and the concept of perspective function, the non-convex optimization problem for each SBS is transformed into an equivalent constrained convex optimization problem. Then, the constrained convex optimization problem is converted into an unconstrained convex optimization problem by exploiting the mixed penalty function method. The inequality constraints are eliminated by introducing the logarithmic barrier functions and the equality constraint is eliminated by introducing the quadratic penalty function. We also theoretically show the existence and the uniqueness of the NE in the non-cooperative power control game. Simulation results show remarkable improvements in terms of EE by using the proposed scheme.",paper energi effici power control small cell underlay macro cellular network investig formul power control problem self organ small cell network non cooper game propos distribut energi effici power control scheme allow small base station sbss take individu decis attain nash equilibrium ne minimum inform exchang special non cooper power control game non convex optim problem formul sbs maxim energi effici ee exploit properti paramet free fraction program concept perspect function non convex optim problem sbs transform equival constrain convex optim problem constrain convex optim problem convert unconstrain convex optim problem exploit mix penalti function method inequ constraint elimin introduc logarithm barrier function equal constraint elimin introduc quadrat penalti function also theoret show exist uniqu ne non cooper power control game simul result show remark improv term ee use propos scheme,"['Yanxiang Jiang', 'Ningning Lu', 'Yan Chen', 'Mehdi Bennis', 'Fuchun Zheng', 'Xiqi Gao', 'Xiaohu You']","['cs.GT', 'cs.IT', 'math.IT']",False,False,False,False,False,True
641,2017-03-28T14:08:16Z,2017-03-03T12:57:26Z,http://arxiv.org/abs/1703.01138v1,http://arxiv.org/pdf/1703.01138v1,"Multiplicative Weights Update with Constant Step-Size in Congestion   Games: Convergence, Limit Cycles and Chaos",multipl weight updat constant step size congest game converg limit cycl chao,"The Multiplicative Weights Update (MWU) method is a ubiquitous meta-algorithm that works as follows: A distribution is maintained on a certain set, and at each step the probability assigned to element $\gamma$ is multiplied by $(1 -\epsilon C(\gamma))>0$ where $C(\gamma)$ is the ""cost"" of element $\gamma$ and then rescaled to ensure that the new values form a distribution. We analyze MWU in congestion games where agents use \textit{arbitrary admissible constants} as learning rates $\epsilon$ and prove convergence to \textit{exact Nash equilibria}. Our proof leverages a novel connection between MWU and the Baum-Welch algorithm, the standard instantiation of the Expectation-Maximization (EM) algorithm for hidden Markov models (HMM). Interestingly, this convergence result does not carry over to the nearly homologous MWU variant where at each step the probability assigned to element $\gamma$ is multiplied by $(1 -\epsilon)^{C(\gamma)}$ even for the most innocuous case of two-agent, two-strategy load balancing games, where such dynamics can provably lead to limit cycles or even chaotic behavior.",multipl weight updat mwu method ubiquit meta algorithm work follow distribut maintain certain set step probabl assign element gamma multipli epsilon gamma gamma cost element gamma rescal ensur new valu form distribut analyz mwu congest game agent use textit arbitrari admiss constant learn rate epsilon prove converg textit exact nash equilibria proof leverag novel connect mwu baum welch algorithm standard instanti expect maxim em algorithm hidden markov model hmm interest converg result doe carri near homolog mwu variant step probabl assign element gamma multipli epsilon gamma even innocu case two agent two strategi load balanc game dynam provabl lead limit cycl even chaotic behavior,"['Gerasimos Palaiopanos', 'Ioannis Panageas', 'Georgios Piliouras']",['cs.GT'],False,False,False,False,False,True
642,2017-03-28T14:08:16Z,2017-03-03T12:10:13Z,http://arxiv.org/abs/1703.01121v1,http://arxiv.org/pdf/1703.01121v1,On Parameterized Complexity of Group Activity Selection Problems on   Social Networks,parameter complex group activ select problem social network,"In Group Activity Selection Problem (GASP), players form coalitions to participate in activities and have preferences over pairs of the form (activity, group size). Recently, Igarashi et al. have initiated the study of group activity selection problems on social networks (gGASP): a group of players can engage in the same activity if the members of the group form a connected subset of the underlying communication structure. Igarashi et al. have primarily focused on Nash stable outcomes, and showed that many associated algorithmic questions are computationally hard even for very simple networks. In this paper we study the parameterized complexity of gGASP with respect to the number of activities as well as with respect to the number of players, for several solution concepts such as Nash stability, individual stability and core stability. The first parameter we consider in the number of activities. For this parameter, we propose an FPT algorithm for Nash stability for the case where the social network is acyclic and obtain a W[1]-hardness result for cliques (i.e., for classic GASP); similar results hold for individual stability. In contrast, finding a core stable outcome is hard even if the number of activities is bounded by a small constant, both for classic GASP and when the social network is a star. Another parameter we study is the number of players. While all solution concepts we consider become polynomial-time computable when this parameter is bounded by a constant, we prove W[1]-hardness results for cliques (i.e., for classic GASP).",group activ select problem gasp player form coalit particip activ prefer pair form activ group size recent igarashi et al initi studi group activ select problem social network ggasp group player engag activ member group form connect subset communic structur igarashi et al primarili focus nash stabl outcom show mani associ algorithm question comput hard even veri simpl network paper studi parameter complex ggasp respect number activ well respect number player sever solut concept nash stabil individu stabil core stabil first paramet consid number activ paramet propos fpt algorithm nash stabil case social network acycl obtain hard result cliqu classic gasp similar result hold individu stabil contrast find core stabl outcom hard even number activ bound small constant classic gasp social network star anoth paramet studi number player solut concept consid becom polynomi time comput paramet bound constant prove hard result cliqu classic gasp,"['Ayumi Igarashi', 'Robert Bredereck', 'Edith Elkind']",['cs.GT'],False,False,False,False,False,True
645,2017-03-28T14:08:16Z,2017-03-02T09:42:47Z,http://arxiv.org/abs/1703.00683v1,http://arxiv.org/pdf/1703.00683v1,"Parity Games, Imperfect Information and Structural Complexity",pariti game imperfect inform structur complex,"We address the problem of solving parity games with imperfect information on finite graphs of bounded structural complexity. It is a major open problem whether parity games with perfect information can be solved in PTIME. Restricting the structural complexity of the game arenas, however, often leads to efficient algorithms for parity games. Such results are known for graph classes of bounded tree-width, DAG-width, directed path-width, and entanglement, which we describe in terms of cops and robber games. Conversely, the introduction of imperfect information makes the problem more difficult, it becomes EXPTIME-hard. We analyse the interaction of both approaches.   We use a simple method to measure the amount of ""unawareness""' of a player, the amount of imperfect information. It turns out that if it is unbounded, low structural complexity does not make the problem simpler. It remains EXPTIME-hard or PSPACE-hard even on very simple graphs.   For games with bounded imperfect information we analyse the powerset construction, which is commonly used to convert a game of imperfect information into an equivalent game with perfect information. This construction preserves boundedness of directed path-width and DAG-width, but not of entanglement or of tree-width. Hence, if directed path-width or DAG-width are bounded, parity games with bounded imperfect information can be solved in PTIME. For DAG-width we follow two approaches. One leads to a generalization of the known fact that perfect information parity games are in PTIME if DAG-width is bounded. We prove this theorem for non-monotone DAG-width. The other approach introduces a cops and robbers game (with multiple robbers) on directed graphs, considered Richerby and Thilikos forundirected graphs. We show a tight linear bound for the number of additional cops needed to capture an additional robber.",address problem solv pariti game imperfect inform finit graph bound structur complex major open problem whether pariti game perfect inform solv ptime restrict structur complex game arena howev often lead effici algorithm pariti game result known graph class bound tree width dag width direct path width entangl describ term cop robber game convers introduct imperfect inform make problem difficult becom exptim hard analys interact approach use simpl method measur amount unawar player amount imperfect inform turn unbound low structur complex doe make problem simpler remain exptim hard pspace hard even veri simpl graph game bound imperfect inform analys powerset construct common use convert game imperfect inform equival game perfect inform construct preserv bounded direct path width dag width entangl tree width henc direct path width dag width bound pariti game bound imperfect inform solv ptime dag width follow two approach one lead general known fact perfect inform pariti game ptime dag width bound prove theorem non monoton dag width approach introduc cop robber game multipl robber direct graph consid richerbi thiliko forundirect graph show tight linear bound number addit cop need captur addit robber,"['Bernd Puchala', 'Roman Rabinovich']",['cs.GT'],False,False,False,False,False,True
646,2017-03-28T14:08:16Z,2017-03-02T05:36:16Z,http://arxiv.org/abs/1703.00632v1,http://arxiv.org/pdf/1703.00632v1,"A Dominant Strategy Truthful, Deterministic Multi-Armed Bandit Mechanism   with Logarithmic Regret",domin strategi truth determinist multi arm bandit mechan logarithm regret,"Stochastic multi-armed bandit (MAB) mechanisms are widely used in sponsored search auctions, crowdsourcing, online procurement, etc. Existing stochastic MAB mechanisms with a deterministic payment rule, proposed in the literature, necessarily suffer a regret of $\Omega(T^{2/3})$, where $T$ is the number of time steps. This happens because the existing mechanisms consider the worst case scenario where the means of the agents' stochastic rewards are separated by a very small amount that depends on $T$. We make, and, exploit the crucial observation that in most scenarios, the separation between the agents' rewards is rarely a function of $T$. Moreover, in the case that the rewards of the arms are arbitrarily close, the regret contributed by such sub-optimal arms is minimal. Our idea is to allow the center to indicate the resolution, $\Delta$, with which the agents must be distinguished. This immediately leads us to introduce the notion of $\Delta$-Regret. Using sponsored search auctions as a concrete example (the same idea applies for other applications as well), we propose a dominant strategy incentive compatible (DSIC) and individually rational (IR), deterministic MAB mechanism, based on ideas from the Upper Confidence Bound (UCB) family of MAB algorithms. Remarkably, the proposed mechanism $\Delta$-UCB achieves a $\Delta$-regret of $O(\log T)$ for the case of sponsored search auctions. We first establish the results for single slot sponsored search auctions and then non-trivially extend the results to the case where multiple slots are to be allocated.",stochast multi arm bandit mab mechan wide use sponsor search auction crowdsourc onlin procur etc exist stochast mab mechan determinist payment rule propos literatur necessarili suffer regret omega number time step happen becaus exist mechan consid worst case scenario mean agent stochast reward separ veri small amount depend make exploit crucial observ scenario separ agent reward rare function moreov case reward arm arbitrarili close regret contribut sub optim arm minim idea allow center indic resolut delta agent must distinguish immedi lead us introduc notion delta regret use sponsor search auction concret exampl idea appli applic well propos domin strategi incent compat dsic individu ration ir determinist mab mechan base idea upper confid bound ucb famili mab algorithm remark propos mechan delta ucb achiev delta regret log case sponsor search auction first establish result singl slot sponsor search auction non trivial extend result case multipl slot alloc,"['Divya Padmanabhan', 'Satyanath Bhat', 'Prabuchandran K. J.', 'Shirish Shevade', 'Y. Narahari']",['cs.GT'],False,False,True,False,False,True
647,2017-03-28T14:08:16Z,2017-03-01T20:09:43Z,http://arxiv.org/abs/1703.00484v1,http://arxiv.org/pdf/1703.00484v1,Truth and Regret in Online Scheduling,truth regret onlin schedul,"We consider a scheduling problem where a cloud service provider has multiple units of a resource available over time. Selfish clients submit jobs, each with an arrival time, deadline, length, and value. The service provider's goal is to implement a truthful online mechanism for scheduling jobs so as to maximize the social welfare of the schedule. Recent work shows that under a stochastic assumption on job arrivals, there is a single-parameter family of mechanisms that achieves near-optimal social welfare. We show that given any such family of near-optimal online mechanisms, there exists an online mechanism that in the worst case performs nearly as well as the best of the given mechanisms. Our mechanism is truthful whenever the mechanisms in the given family are truthful and prompt, and achieves optimal (within constant factors) regret.   We model the problem of competing against a family of online scheduling mechanisms as one of learning from expert advice. A primary challenge is that any scheduling decisions we make affect not only the payoff at the current step, but also the resource availability and payoffs in future steps. Furthermore, switching from one algorithm (a.k.a. expert) to another in an online fashion is challenging both because it requires synchronization with the state of the latter algorithm as well as because it affects the incentive structure of the algorithms. We further show how to adapt our algorithm to a non-clairvoyant setting where job lengths are unknown until jobs are run to completion. Once again, in this setting, we obtain truthfulness along with asymptotically optimal regret (within poly-logarithmic factors).",consid schedul problem cloud servic provid multipl unit resourc avail time selfish client submit job arriv time deadlin length valu servic provid goal implement truth onlin mechan schedul job maxim social welfar schedul recent work show stochast assumpt job arriv singl paramet famili mechan achiev near optim social welfar show given ani famili near optim onlin mechan exist onlin mechan worst case perform near well best given mechan mechan truth whenev mechan given famili truth prompt achiev optim within constant factor regret model problem compet famili onlin schedul mechan one learn expert advic primari challeng ani schedul decis make affect onli payoff current step also resourc avail payoff futur step furthermor switch one algorithm expert anoth onlin fashion challeng becaus requir synchron state latter algorithm well becaus affect incent structur algorithm show adapt algorithm non clairvoy set job length unknown job run complet onc set obtain truth along asymptot optim regret within poli logarithm factor,"['Shuchi Chawla', 'Nikhil Devanur', 'Janardhan Kulkarni', 'Rad Niazadeh']","['cs.GT', 'cs.AI', 'cs.DS', 'cs.LG']",False,False,False,False,False,True
648,2017-03-28T14:08:16Z,2017-03-01T14:42:20Z,http://arxiv.org/abs/1703.00320v1,http://arxiv.org/pdf/1703.00320v1,Investigating the Characteristics of One-Sided Matching Mechanisms Under   Various Preferences and Risk Attitudes,investig characterist one side match mechan various prefer risk attitud,"One-sided matching mechanisms are fundamental for assigning a set of indivisible objects to a set of self-interested agents when monetary transfers are not allowed. Two widely-studied randomized mechanisms in multiagent settings are the Random Serial Dictatorship (RSD) and the Probabilistic Serial Rule (PS). Both mechanisms require only that agents specify ordinal preferences and have a number of desirable economic and computational properties. However, the induced outcomes of the mechanisms are often incomparable and thus there are challenges when it comes to deciding which mechanism to adopt in practice. In this paper, we first consider the space of general ordinal preferences and provide empirical results on the (in)comparability of RSD and PS. We analyze their respective economic properties under general and lexicographic preferences. We then instantiate utility functions with the goal of gaining insights on the manipulability, efficiency, and envyfreeness of the mechanisms under different risk-attitude models. Our results hold under various preference distribution models, which further confirm the broad use of RSD in most practical applications.",one side match mechan fundament assign set indivis object set self interest agent monetari transfer allow two wide studi random mechan multiag set random serial dictatorship rsd probabilist serial rule ps mechan requir onli agent specifi ordin prefer number desir econom comput properti howev induc outcom mechan often incompar thus challeng come decid mechan adopt practic paper first consid space general ordin prefer provid empir result compar rsd ps analyz respect econom properti general lexicograph prefer instanti util function goal gain insight manipul effici envyfre mechan differ risk attitud model result hold various prefer distribut model confirm broad use rsd practic applic,"['Hadi Hosseini', 'Kate Larson', 'Robin Cohen']","['cs.GT', 'cs.AI', 'cs.MA', 'I.2.11; J.4']",False,False,True,False,False,True
650,2017-03-28T14:08:19Z,2017-02-28T16:57:49Z,http://arxiv.org/abs/1702.08862v1,http://arxiv.org/pdf/1702.08862v1,Proportional Representation in Vote Streams,proport represent vote stream,"We consider elections where the voters come one at a time, in a streaming fashion, and devise space-efficient algorithms which identify an approximate winning committee with respect to common multiwinner proportional representation voting rules; specifically, we consider the Approval-based and the Borda-based variants of both the Chamberlin-- ourant rule and the Monroe rule. We complement our algorithms with lower bounds. Somewhat surprisingly, our results imply that, using space which does not depend on the number of voters it is possible to efficiently identify an approximate representative committee of fixed size over vote streams with huge number of voters.",consid elect voter come one time stream fashion devis space effici algorithm identifi approxim win committe respect common multiwinn proport represent vote rule specif consid approv base borda base variant chamberlin ourant rule monro rule complement algorithm lower bound somewhat surpris result impli use space doe depend number voter possibl effici identifi approxim repres committe fix size vote stream huge number voter,"['Palash Dey', 'Nimrod Talmon', 'Otniel van Handel']","['cs.GT', 'cs.AI', 'cs.CC', 'cs.DS', 'cs.MA']",False,False,False,False,False,True
652,2017-03-28T14:08:19Z,2017-02-28T13:56:57Z,http://arxiv.org/abs/1702.08789v1,http://arxiv.org/pdf/1702.08789v1,Nash and Wardrop equilibria in aggregative games with coupling   constraints,nash wardrop equilibria aggreg game coupl constraint,"We consider the framework of aggregative games, in which the cost function of each agent depends on his own strategy and on the average population strategy. As first contribution, we investigate the relations between the concepts of Nash and Wardrop equilibrium. By exploiting a characterization of the two equilibria as solutions of variational inequalities, we bound their distance with a decreasing function of the population size. As second contribution, we propose two decentralized algorithms that converge to such equilibria and are capable of coping with constraints coupling the strategies of different agents. Finally, we study the applications of charging of electric vehicles and of route choice on a road network.",consid framework aggreg game cost function agent depend strategi averag popul strategi first contribut investig relat concept nash wardrop equilibrium exploit character two equilibria solut variat inequ bound distanc decreas function popul size second contribut propos two decentr algorithm converg equilibria capabl cope constraint coupl strategi differ agent final studi applic charg electr vehicl rout choic road network,"['Basilio Gentile', 'Francesca Parise', 'Dario Paccagnan', 'Maryam Kamgarpour', 'John Lygeros']","['cs.SY', 'cs.GT', 'math.OC']",False,False,True,False,False,True
654,2017-03-28T14:08:19Z,2017-02-27T18:07:12Z,http://arxiv.org/abs/1702.08405v1,http://arxiv.org/pdf/1702.08405v1,Game-Theoretic Semantics for ATL+ with Applications to Model Checking,game theoret semant atl applic model check,"We develop game-theoretic semantics (GTS) for the fragment ATL+ of the full Alternating-time Temporal Logic ATL*, essentially extending a recently introduced GTS for ATL. We first show that the new game-theoretic semantics is equivalent to the standard semantics of ATL+ (based on perfect recall strategies). We then provide an analysis, based on the new semantics, of the memory and time resources needed for model checking ATL+. Based on that, we establish that strategies that use only a very limited amount of memory suffice for ATL+. Furthermore, using the GTS we provide a new algorithm for model checking of ATL+ and identify a natural hierarchy of tractable fragments of ATL+ that extend ATL.",develop game theoret semant gts fragment atl full altern time tempor logic atl essenti extend recent introduc gts atl first show new game theoret semant equival standard semant atl base perfect recal strategi provid analysi base new semant memori time resourc need model check atl base establish strategi use onli veri limit amount memori suffic atl furthermor use gts provid new algorithm model check atl identifi natur hierarchi tractabl fragment atl extend atl,"['Valentin Goranko', 'Antti Kuusisto', 'Raine Rönnholm']","['math.LO', 'cs.GT', 'cs.LO', 'F.4.1; I.2.11']",False,False,True,False,False,True
655,2017-03-28T14:08:19Z,2017-02-27T15:41:56Z,http://arxiv.org/abs/1702.08334v1,http://arxiv.org/pdf/1702.08334v1,Stochastic Stability Analysis of Perturbed Learning Automata with   Constant Step-Size in Strategic-Form Games,stochast stabil analysi perturb learn automata constant step size strateg form game,"This paper considers a class of reinforcement-learning that belongs to the family of Learning Automata and provides a stochastic-stability analysis in strategic-form games. For this class of dynamics, convergence to pure Nash equilibria has been demonstrated only for the fine class of potential games. Prior work primarily provides convergence properties of the dynamics through stochastic approximations, where the asymptotic behavior can be associated with the limit points of an ordinary-differential equation (ODE). However, analyzing global convergence through the ODE-approximation requires the existence of a Lyapunov or a potential function, which naturally restricts the applicabity of these algorithms to a fine class of games. To overcome these limitations, this paper introduces an alternative framework for analyzing stochastic-stability that is based upon an explicit characterization of the (unique) invariant probability measure of the induced Markov chain.",paper consid class reinforc learn belong famili learn automata provid stochast stabil analysi strateg form game class dynam converg pure nash equilibria demonstr onli fine class potenti game prior work primarili provid converg properti dynam stochast approxim asymptot behavior associ limit point ordinari differenti equat ode howev analyz global converg ode approxim requir exist lyapunov potenti function natur restrict applicab algorithm fine class game overcom limit paper introduc altern framework analyz stochast stabil base upon explicit character uniqu invari probabl measur induc markov chain,['Georgios C. Chasparis'],['cs.GT'],False,False,False,False,False,True
658,2017-03-28T14:08:19Z,2017-02-26T03:24:31Z,http://arxiv.org/abs/1702.07984v1,http://arxiv.org/abs/1702.07984v1,Collaborative Optimization for Collective Decision-making in Continuous   Spaces,collabor optim collect decis make continu space,"Many societal decision problems lie in high-dimensional continuous spaces not amenable to the voting techniques common for their discrete or single-dimensional counterparts. These problems are typically discretized before running an election or decided upon through negotiation by representatives. We propose a meta-algorithm called \emph{Iterative Local Voting} for collective decision-making in this setting, in which voters are sequentially sampled and asked to modify a candidate solution within some local neighborhood of its current value, as defined by a ball in some chosen norm. In general, such schemes do not converge, or, when they do, the resulting solution does not have a natural description.   We first prove the convergence of this algorithm under appropriate choices of neighborhoods to plausible solutions in certain natural settings: when the voters' utilities can be expressed in terms of some form of distance from their ideal solution, and when these utilities are additively decomposable across dimensions. In many of these cases, we obtain convergence to the societal welfare maximizing solution.   We then describe an experiment in which we test our algorithm for the decision of the U.S. Federal Budget on Mechanical Turk with over 4,000 workers, employing neighborhoods defined by $\mathcal{L}^1, \mathcal{L}^2$ and $\mathcal{L}^\infty$ balls. We make several observations that inform future implementations of such a procedure.",mani societ decis problem lie high dimension continu space amen vote techniqu common discret singl dimension counterpart problem typic discret befor run elect decid upon negoti repres propos meta algorithm call emph iter local vote collect decis make set voter sequenti sampl ask modifi candid solut within local neighborhood current valu defin ball chosen norm general scheme converg result solut doe natur descript first prove converg algorithm appropri choic neighborhood plausibl solut certain natur set voter util express term form distanc ideal solut util addit decompos across dimens mani case obtain converg societ welfar maxim solut describ experi test algorithm decis feder budget mechan turk worker employ neighborhood defin mathcal mathcal mathcal infti ball make sever observ inform futur implement procedur,"['Nikhil Garg', 'Vijay Kamble', 'Ashish Goel', 'David Marn', 'Kamesh Munagala']","['cs.MA', 'cs.CY', 'cs.GT']",False,False,False,False,False,True
659,2017-03-28T14:08:19Z,2017-02-25T17:56:07Z,http://arxiv.org/abs/1702.07932v1,http://arxiv.org/pdf/1702.07932v1,The role of quantum correlations in Cop and Robber game,role quantum correl cop robber game,"We introduce and study quantized versions of Cop and Robber game. We achieve this by using graph-preserving unitary operations, which are the quantum analogue of stochastic operations preserving the graph. We provide the tight bound for the number of operations required to reach the given state. By extending them to controlled operations, we define a quantum controlled Cop and Robber game, which expands the classical Cop and Robber game, as well as classically controlled quantum Cop and Robber game. In contrast to the typical scheme for introducing quantum games, we assume that both parties can utilise full information about the opponent's strategy. We show that the utilisation of the full knowledge about the opponent's state does not provide the advantage. Moreover, the chances of catching the Robber decreases for classically cop-win graphs. The result does not depend on the chosen model of evolution. On the other hand, the possibility to execute controlled quantum operations allows catching the Robber on almost all classically cop-win graphs. To provide interesting, non-trivial quantized Cop and Robber game, we need to enrich the structure of correlations between the players' systems. This result demonstrates that the ability to utilise quantum controlled operations is significantly stronger that the control restricted operating on classical selecting quantum operations only.",introduc studi quantiz version cop robber game achiev use graph preserv unitari oper quantum analogu stochast oper preserv graph provid tight bound number oper requir reach given state extend control oper defin quantum control cop robber game expand classic cop robber game well classic control quantum cop robber game contrast typic scheme introduc quantum game assum parti utilis full inform oppon strategi show utilis full knowledg oppon state doe provid advantag moreov chanc catch robber decreas classic cop win graph result doe depend chosen model evolut hand possibl execut control quantum oper allow catch robber almost classic cop win graph provid interest non trivial quantiz cop robber game need enrich structur correl player system result demonstr abil utilis quantum control oper signific stronger control restrict oper classic select quantum oper onli,"['Adam Glos', 'Jarosław Adam Miszczak']","['quant-ph', 'cs.DM', 'cs.GT', '05C57 (Primary), 91A46, 81P40 (Secondary)', 'G.2.2']",False,False,True,False,False,True
660,2017-03-28T14:08:25Z,2017-02-25T15:07:59Z,http://arxiv.org/abs/1702.07902v1,http://arxiv.org/pdf/1702.07902v1,Approval Voting with Intransitive Preferences,approv vote intransit prefer,"We extend Approval voting to the settings where voters may have intransitive preferences. The major obstacle to applying Approval voting in these settings is that voters are not able to clearly determine who they should approve or disapprove, due to the intransitivity of their preferences. An approach to address this issue is to apply tournament solutions to help voters make the decision. We study a class of voting systems where first each voter casts a vote defined as a tournament, then a well-defined tournament solution is applied to select the candidates who are assumed to be approved by the voter. Winners are the ones receiving the most approvals. We study axiomatic properties of this class of voting systems and complexity of control and bribery problems for these voting systems.",extend approv vote set voter may intransit prefer major obstacl appli approv vote set voter abl clear determin approv disapprov due intransit prefer approach address issu appli tournament solut help voter make decis studi class vote system first voter cast vote defin tournament well defin tournament solut appli select candid assum approv voter winner one receiv approv studi axiomat properti class vote system complex control briberi problem vote system,['Yongjie Yang'],"['cs.GT', 'cs.CC', 'cs.DM']",False,False,True,False,False,True
663,2017-03-28T14:08:25Z,2017-02-24T17:09:22Z,http://arxiv.org/abs/1702.07665v1,http://arxiv.org/pdf/1702.07665v1,Truthful Mechanisms for Delivery with Mobile Agents,truth mechan deliveri mobil agent,"We study the game-theoretic task of selecting mobile agents to deliver multiple items on a network. An instance is given by $m$ messages (physical objects) which have to be transported between specified source-target pairs in a weighted undirected graph, and $k$ mobile heterogeneous agents, each being able to transport one message at a time. Following a recent model by [B\""artschi et al. 2016], each agent $i$ consumes energy proportional to the distance it travels in the graph, where the different rates of energy consumption are given by weight factors $w_i$. We are interested in optimizing or approximating the total energy consumption over all selected agents.   Unlike previous research, we assume the weights to be private values known only to the respective agents. We present three different mechanisms which select, route and pay the agents in a truthful way that guarantees voluntary participation of the agents, while approximating the optimum energy consumption by a constant factor. To this end we analyze a previous structural result and an approximation algorithm given by [B\""artschi et al. 2017]. Finally, we show that for some instances in the case of a single message ($m=1$), the sum of the payments can be bounded in terms of the optimum as well.",studi game theoret task select mobil agent deliv multipl item network instanc given messag physic object transport specifi sourc target pair weight undirect graph mobil heterogen agent abl transport one messag time follow recent model artschi et al agent consum energi proport distanc travel graph differ rate energi consumpt given weight factor interest optim approxim total energi consumpt select agent unlik previous research assum weight privat valu known onli respect agent present three differ mechan select rout pay agent truth way guarante voluntari particip agent approxim optimum energi consumpt constant factor end analyz previous structur result approxim algorithm given artschi et al final show instanc case singl messag sum payment bound term optimum well,"['Andreas Bärtschi', 'Daniel Graf', 'Paolo Penna']","['cs.GT', 'cs.DS']",False,False,True,False,False,True
664,2017-03-28T14:08:25Z,2017-02-24T02:30:15Z,http://arxiv.org/abs/1702.07450v1,http://arxiv.org/pdf/1702.07450v1,Strongly-Typed Agents are Guaranteed to Interact Safely,strong type agent guarante interact safe,"As artificial agents proliferate, it is becoming increasingly important to ensure that their interactions with one another are well-behaved. In this paper, we formalize a common-sense notion of when algorithms are well-behaved: an algorithm is safe if it does no harm. Motivated by recent progress in deep learning, we focus on the specific case where agents update their actions according to gradient descent. The first result is that gradient descent converges to a Nash equilibrium in safe games.   The paper provides sufficient conditions that guarantee safe interactions. The main contribution is to define strongly-typed agents and show they are guaranteed to interact safely. A series of examples show that strong-typing generalizes certain key features of convexity and is closely related to blind source separation. The analysis introduce a new perspective on classical multilinear games based on tensor decomposition.",artifici agent prolifer becom increas import ensur interact one anoth well behav paper formal common sens notion algorithm well behav algorithm safe doe harm motiv recent progress deep learn focus specif case agent updat action accord gradient descent first result gradient descent converg nash equilibrium safe game paper provid suffici condit guarante safe interact main contribut defin strong type agent show guarante interact safe seri exampl show strong type general certain key featur convex close relat blind sourc separ analysi introduc new perspect classic multilinear game base tensor decomposit,['David Balduzzi'],"['cs.LG', 'cs.AI', 'cs.GT']",False,False,False,False,False,True
665,2017-03-28T14:08:25Z,2017-02-24T01:51:48Z,http://arxiv.org/abs/1702.07444v1,http://arxiv.org/pdf/1702.07444v1,Bandits with Movement Costs and Adaptive Pricing,bandit movement cost adapt price,"We extend the model of Multi-armed Bandit with unit switching cost to incorporate a metric between the actions. We consider the case where the metric over the actions can be modeled by a complete binary tree, and the distance between two leaves is the size of the subtree of their least common ancestor, which abstracts the case that the actions are points on the continuous interval $[0,1]$ and the switching cost is their distance. In this setting, we give a new algorithm that establishes a regret of $\widetilde{O}(\sqrt{kT} + T/k)$, where $k$ is the number of actions and $T$ is the time horizon. When the set of actions corresponds to whole $[0,1]$ interval we can exploit our method for the task of bandit learning with Lipschitz loss functions, where our algorithm achieves an optimal regret rate of $\widetilde{\Theta}(T^{2/3})$, which is the same rate one obtains when there is no penalty for movements. As our main application, we use our new algorithm to solve an adaptive pricing problem. Specifically, we consider the case of a single seller faced with a stream of patient buyers. Each buyer has a private value and a window of time in which they are interested in buying, and they buy at the lowest price in the window, if it is below their value. We show that with an appropriate discretization of the prices, the seller can achieve a regret of $\widetilde{O}(T^{2/3})$ compared to the best fixed price in hindsight, which outperform the previous regret bound of $\widetilde{O}(T^{3/4})$ for the problem.",extend model multi arm bandit unit switch cost incorpor metric action consid case metric action model complet binari tree distanc two leav size subtre least common ancestor abstract case action point continu interv switch cost distanc set give new algorithm establish regret widetild sqrt kt number action time horizon set action correspond whole interv exploit method task bandit learn lipschitz loss function algorithm achiev optim regret rate widetild theta rate one obtain penalti movement main applic use new algorithm solv adapt price problem specif consid case singl seller face stream patient buyer buyer privat valu window time interest buy buy lowest price window valu show appropri discret price seller achiev regret widetild compar best fix price hindsight outperform previous regret bound widetild problem,"['Tomer Koren', 'Roi Livni', 'Yishay Mansour']","['cs.LG', 'cs.GT']",False,False,True,False,False,True
666,2017-03-28T14:08:25Z,2017-02-23T17:54:28Z,http://arxiv.org/abs/1702.07311v1,http://arxiv.org/abs/1702.07311v1,ERA: A Framework for Economic Resource Allocation for the Cloud,era framework econom resourc alloc cloud,"Cloud computing has reached significant maturity from a systems perspective, but currently deployed solutions rely on rather basic economics mechanisms that yield suboptimal allocation of the costly hardware resources. In this paper we present Economic Resource Allocation (ERA), a complete framework for scheduling and pricing cloud resources, aimed at increasing the efficiency of cloud resources usage by allocating resources according to economic principles. The ERA architecture carefully abstracts the underlying cloud infrastructure, enabling the development of scheduling and pricing algorithms independently of the concrete lower-level cloud infrastructure and independently of its concerns. Specifically, ERA is designed as a flexible layer that can sit on top of any cloud system and interfaces with both the cloud resource manager and with the users who reserve resources to run their jobs. The jobs are scheduled based on prices that are dynamically calculated according to the predicted demand. Additionally, ERA provides a key internal API to pluggable algorithmic modules that include scheduling, pricing and demand prediction. We provide a proof-of-concept software and demonstrate the effectiveness of the architecture by testing ERA over both public and private cloud systems -- Azure Batch of Microsoft and Hadoop/YARN. A broader intent of our work is to foster collaborations between economics and system communities. To that end, we have developed a simulation platform via which economics and system experts can test their algorithmic implementations.",cloud comput reach signific matur system perspect current deploy solut reli rather basic econom mechan yield suboptim alloc cost hardwar resourc paper present econom resourc alloc era complet framework schedul price cloud resourc aim increas effici cloud resourc usag alloc resourc accord econom principl era architectur care abstract cloud infrastructur enabl develop schedul price algorithm independ concret lower level cloud infrastructur independ concern specif era design flexibl layer sit top ani cloud system interfac cloud resourc manag user reserv resourc run job job schedul base price dynam calcul accord predict demand addit era provid key intern api pluggabl algorithm modul includ schedul price demand predict provid proof concept softwar demonstr effect architectur test era public privat cloud system azur batch microsoft hadoop yarn broader intent work foster collabor econom system communiti end develop simul platform via econom system expert test algorithm implement,"['Moshe Babaioff', 'Yishay Mansour', 'Noam Nisan', 'Gali Noti', 'Carlo Curino', 'Nar Ganapathy', 'Ishai Menache', 'Omer Reingold', 'Moshe Tennenholtz', 'Erez Timnat']","['cs.GT', 'cs.DC']",False,False,True,False,False,True
670,2017-03-28T14:08:29Z,2017-02-22T22:34:57Z,http://arxiv.org/abs/1702.07031v1,http://arxiv.org/pdf/1702.07031v1,Proactive Resource Management in LTE-U Systems: A Deep Learning   Perspective,proactiv resourc manag lte system deep learn perspect,"LTE in unlicensed spectrum (LTE-U) is a promising approach to overcome the wireless spectrum scarcity. However, to reap the benefits of LTE-U, a fair coexistence mechanism with other incumbent WiFi deployments is required. In this paper, a novel deep learning approach is proposed for modeling the resource allocation problem of LTE-U small base stations (SBSs). The proposed approach enables multiple SBSs to proactively perform dynamic channel selection, carrier aggregation, and fractional spectrum access while guaranteeing fairness with existing WiFi networks and other LTE-U operators. Adopting a proactive coexistence mechanism enables future delay-intolerant LTE-U data demands to be served within a given prediction window ahead of their actual arrival time thus avoiding the underutilization of the unlicensed spectrum during off-peak hours while maximizing the total served LTE-U traffic load. To this end, a noncooperative game model is formulated in which SBSs are modeled as Homo Egualis agents that aim at predicting a sequence of future actions and thus achieving long-term equal weighted fairness with WLAN and other LTE-U operators over a given time horizon. The proposed deep learning algorithm is then shown to reach a mixed-strategy Nash equilibrium (NE), when it converges. Simulation results using real data traces show that the proposed scheme can yield up to 28% and 11% gains over a conventional reactive approach and a proportional fair coexistence mechanism, respectively. The results also show that the proposed framework prevents WiFi performance degradation for a densely deployed LTE-U network.",lte unlicens spectrum lte promis approach overcom wireless spectrum scarciti howev reap benefit lte fair coexist mechan incumb wifi deploy requir paper novel deep learn approach propos model resourc alloc problem lte small base station sbss propos approach enabl multipl sbss proactiv perform dynam channel select carrier aggreg fraction spectrum access guarante fair exist wifi network lte oper adopt proactiv coexist mechan enabl futur delay intoler lte data demand serv within given predict window ahead actual arriv time thus avoid underutil unlicens spectrum dure peak hour maxim total serv lte traffic load end noncoop game model formul sbss model homo eguali agent aim predict sequenc futur action thus achiev long term equal weight fair wlan lte oper given time horizon propos deep learn algorithm shown reach mix strategi nash equilibrium ne converg simul result use real data trace show propos scheme yield gain convent reactiv approach proport fair coexist mechan respect result also show propos framework prevent wifi perform degrad dens deploy lte network,"['Ursula Challita', 'Li Dong', 'Walid Saad']","['cs.IT', 'cs.AI', 'cs.GT', 'math.IT']",False,False,True,False,False,True
671,2017-03-28T14:08:29Z,2017-02-22T18:06:24Z,http://arxiv.org/abs/1702.06922v1,http://arxiv.org/pdf/1702.06922v1,Formation of coalition structures as a non-cooperative game,format coalit structur non cooper game,"The paper defines a family of nested non-cooperative simultaneous finite games to study coalition structure formation with intra and inter-coalition externalities. Every game has two outcomes - an allocation of players over coalitions and a payoff profile for every player.   Every game in the family has an equilibrium in mixed strategies. The equilibrium can generate more than one coalition with a presence of intra and inter group externalities. These properties make it different from the Shapley value, strong Nash, coalition-proof equilibrium, core, kernel, nucleolus. The paper demonstrates some applications: non-cooperative cooperation, Bayesian game, stochastic games and construction of a non-cooperative criterion of coalition structure stability for studying focal points. An example demonstrates that a payoff profile in the Prisoners' Dilemma is non-informative to deduce a cooperation of players.",paper defin famili nest non cooper simultan finit game studi coalit structur format intra inter coalit extern everi game two outcom alloc player coalit payoff profil everi player everi game famili equilibrium mix strategi equilibrium generat one coalit presenc intra inter group extern properti make differ shapley valu strong nash coalit proof equilibrium core kernel nucleolus paper demonstr applic non cooper cooper bayesian game stochast game construct non cooper criterion coalit structur stabil studi focal point exampl demonstr payoff profil prison dilemma non inform deduc cooper player,['Dmitry Levando'],"['math.OC', 'cs.GT']",False,False,True,False,False,True
672,2017-03-28T14:08:29Z,2017-02-22T15:39:12Z,http://arxiv.org/abs/1702.06858v1,http://arxiv.org/pdf/1702.06858v1,Emptiness of zero automata is decidable,empti zero automata decid,"Zero automata are a probabilistic extension of parity automata on infinite trees. The satisfiability of a certain probabilistic variant of mso, called tmso + zero, reduces to the emptiness problem for zero automata. We introduce a variant of zero automata called nonzero automata. We prove that for every zero automaton there is an equivalent nonzero automaton of quadratic size and the emptiness problem of nonzero automata is decidable, with complexity np. These results imply that tmso + zero has decidable satisfiability.",zero automata probabilist extens pariti automata infinit tree satisfi certain probabilist variant mso call tmso zero reduc empti problem zero automata introduc variant zero automata call nonzero automata prove everi zero automaton equival nonzero automaton quadrat size empti problem nonzero automata decid complex np result impli tmso zero decid satisfi,"['Mikolaj Bojańczyk', 'Hugo Gimbert', 'Edon Kelmendi']","['cs.FL', 'cs.GT']",False,False,False,False,False,True
676,2017-03-28T14:08:29Z,2017-02-21T15:19:56Z,http://arxiv.org/abs/1702.06436v1,http://arxiv.org/pdf/1702.06436v1,Contract-Theoretic Resource Allocation for Critical Infrastructure   Protection,contract theoret resourc alloc critic infrastructur protect,"Critical infrastructure protection (CIP) is envisioned to be one of the most challenging security problems in the coming decade. One key challenge in CIP is the ability to allocate resources, either personnel or cyber, to critical infrastructures with different vulnerability and criticality levels. In this work, a contract-theoretic approach is proposed to solve the problem of resource allocation in critical infrastructure with asymmetric information. A control center (CC) is used to design contracts and offer them to infrastructures' owners. A contract can be seen as an agreement between the CC and infrastructures using which the CC allocates resources and gets rewards in return. Contracts are designed in a way to maximize the CC's benefit and motivate each infrastructure to accept a contract and obtain proper resources for its protection. Infrastructures are defined by both vulnerability levels and criticality levels which are unknown to the CC. Therefore, each infrastructure can claim that it is the most vulnerable or critical to gain more resources. A novel mechanism is developed to handle such an asymmetric information while providing the optimal contract that motivates each infrastructure to reveal its actual type. The necessary and sufficient conditions for such resource allocation contracts under asymmetric information are derived. Simulation results show that the proposed contract-theoretic approach maximizes the CC's utility while ensuring that no infrastructure has an incentive to ask for another contract, despite the lack of exact information at the CC.",critic infrastructur protect cip envis one challeng secur problem come decad one key challeng cip abil alloc resourc either personnel cyber critic infrastructur differ vulner critic level work contract theoret approach propos solv problem resourc alloc critic infrastructur asymmetr inform control center cc use design contract offer infrastructur owner contract seen agreement cc infrastructur use cc alloc resourc get reward return contract design way maxim cc benefit motiv infrastructur accept contract obtain proper resourc protect infrastructur defin vulner level critic level unknown cc therefor infrastructur claim vulner critic gain resourc novel mechan develop handl asymmetr inform provid optim contract motiv infrastructur reveal actual type necessari suffici condit resourc alloc contract asymmetr inform deriv simul result show propos contract theoret approach maxim cc util ensur infrastructur incent ask anoth contract despit lack exact inform cc,"['AbdelRahman Eldosouky', 'Walid Saad', 'Charles Kamhoua', 'and Kevin Kwiat']","['cs.CR', 'cs.GT']",False,False,False,False,False,True
677,2017-03-28T14:08:29Z,2017-02-20T21:54:26Z,http://arxiv.org/abs/1702.06189v1,http://arxiv.org/pdf/1702.06189v1,A Graphical Evolutionary Game Approach to Social Learning,graphic evolutionari game approach social learn,"In this work, we study the social learning problem, in which agents of a networked system collaborate to detect the state of the nature based on their private signals. A novel distributed graphical evolutionary game theoretic learning method is proposed. In the proposed game-theoretic method, agents only need to communicate their binary decisions rather than the real-valued beliefs with their neighbors, which endows the method with low communication complexity. Under mean field approximations, we theoretically analyze the steady state equilibria of the game and show that the evolutionarily stable states (ESSs) coincide with the decisions of the benchmark centralized detector. Numerical experiments are implemented to confirm the effectiveness of the proposed game-theoretic learning method.",work studi social learn problem agent network system collabor detect state natur base privat signal novel distribut graphic evolutionari game theoret learn method propos propos game theoret method agent onli need communic binari decis rather real valu belief neighbor endow method low communic complex mean field approxim theoret analyz steadi state equilibria game show evolutionarili stabl state esss coincid decis benchmark central detector numer experi implement confirm effect propos game theoret learn method,"['Xuanyu Cao', 'K. J. Ray Liu']",['cs.GT'],False,False,True,False,False,True
679,2017-03-28T14:08:29Z,2017-02-22T01:48:29Z,http://arxiv.org/abs/1702.06062v2,http://arxiv.org/pdf/1702.06062v2,Simple vs Optimal Mechanisms in Auctions with Convex Payments,simpl vs optim mechan auction convex payment,"We investigate approximately optimal mechanisms in settings where bidders' utility functions are non-linear; specifically, convex, with respect to payments (such settings arise, for instance, in procurement auctions for energy). We provide constant factor approximation guarantees for mechanisms that are independent of bidders' private information (i.e., prior-free), and for mechanisms that rely to an increasing extent on that information (i.e., detail free). We also describe experiments, which show that for randomly drawn monotone hazard rate distributions, our mechanisms achieve at least 80\% of the optimal revenue, on average. Both our theoretical and experimental results show that in the convex payment setting, it is desirable to allocate across multiple bidders, rather than only to bidders with the highest (virtual) value, as in the traditional quasi-linear utility setting.",investig approxim optim mechan set bidder util function non linear specif convex respect payment set aris instanc procur auction energi provid constant factor approxim guarante mechan independ bidder privat inform prior free mechan reli increas extent inform detail free also describ experi show random drawn monoton hazard rate distribut mechan achiev least optim revenu averag theoret experiment result show convex payment set desir alloc across multipl bidder rather onli bidder highest virtual valu tradit quasi linear util set,"['Amy Greenwald', 'Takehiro Oyakawa', 'Vasilis Syrgkanis']",['cs.GT'],False,False,True,False,False,True
680,2017-03-28T14:08:34Z,2017-02-20T00:34:00Z,http://arxiv.org/abs/1702.05825v1,http://arxiv.org/pdf/1702.05825v1,Sustainable Fair Division,sustain fair divis,"In this paper, I summarize our work on online fair division. In particular, I present two models for online fair division: (1) one existing model for fair division in food banks and (2) one new model for fair division of deceased organs to patients. I further discuss simple mechanisms for these models that allocate the resources as they arrive to agents. In practice, agents are often risk-averse having imperfect information. Within this assumption, I report several interesting axiomatic and complexity results for these mechanisms and conclude with future work.",paper summar work onlin fair divis particular present two model onlin fair divis one exist model fair divis food bank one new model fair divis deceas organ patient discuss simpl mechan model alloc resourc arriv agent practic agent often risk avers imperfect inform within assumpt report sever interest axiomat complex result mechan conclud futur work,['Martin Aleksandrov'],['cs.GT'],False,False,True,False,False,True
681,2017-03-28T14:08:34Z,2017-02-19T18:23:47Z,http://arxiv.org/abs/1702.05778v1,http://arxiv.org/pdf/1702.05778v1,The Absent-Minded Driver Problem Redux,absent mind driver problem redux,"This paper reconsiders the problem of the absent-minded driver who must choose between alternatives with different payoff with imperfect recall and varying degrees of knowledge of the system. The classical absent-minded driver problem represents the case with limited information and it has bearing on the general area of communication and learning, social choice, mechanism design, auctions, theories of knowledge, belief, and rational agency. Within the framework of extensive games, this problem has applications to many artificial intelligence scenarios. It is obvious that the performance of the agent improves as information available increases. It is shown that a non-uniform assignment strategy for successive choices does better than a fixed probability strategy. We consider both classical and quantum approaches to the problem. We argue that the superior performance of quantum decisions with access to entanglement cannot be fairly compared to a classical algorithm. If the cognitive systems of agents are taken to have access to quantum resources, or have a quantum mechanical basis, then that can be leveraged into superior performance.",paper reconsid problem absent mind driver must choos altern differ payoff imperfect recal vari degre knowledg system classic absent mind driver problem repres case limit inform bear general area communic learn social choic mechan design auction theori knowledg belief ration agenc within framework extens game problem applic mani artifici intellig scenario obvious perform agent improv inform avail increas shown non uniform assign strategi success choic doe better fix probabl strategi consid classic quantum approach problem argu superior perform quantum decis access entangl cannot fair compar classic algorithm cognit system agent taken access quantum resourc quantum mechan basi leverag superior perform,['Subhash Kak'],"['cs.AI', 'cs.GT']",False,False,False,False,False,True
682,2017-03-28T14:08:34Z,2017-02-18T18:23:17Z,http://arxiv.org/abs/1702.05640v1,http://arxiv.org/pdf/1702.05640v1,Obvious Strategyproofness Needs Monitoring for Good Approximations,obvious strategyproof need monitor good approxim,"Obvious strategyproofness (OSP) is an appealing concept as it allows to maintain incentive compatibility even in the presence of agents that are not fully rational, e.g., those who struggle with contingent reasoning [Li, 2015]. However, it has been shown to impose some limitations, e.g., no OSP mechanism can return a stable matching [Ashlagi and Gonczarowski, 2015].   We here deepen the study of the limitations of OSP mechanisms by looking at their approximation guarantees for basic optimization problems paradigmatic of the area, i.e., machine scheduling and facility location. We prove a number of bounds on the approximation guarantee of OSP mechanisms, which show that OSP can come at a significant cost. However, rather surprisingly, we prove that OSP mechanisms can return optimal solutions when they use monitoring -- a novel mechanism design paradigm that introduces a mild level of scrutiny on agents' declarations [Kovacs et al., 2015].",obvious strategyproof osp appeal concept allow maintain incent compat even presenc agent fulli ration struggl conting reason li howev shown impos limit osp mechan return stabl match ashlagi gonczarowski deepen studi limit osp mechan look approxim guarante basic optim problem paradigmat area machin schedul facil locat prove number bound approxim guarante osp mechan show osp come signific cost howev rather surpris prove osp mechan return optim solut use monitor novel mechan design paradigm introduc mild level scrutini agent declar kovac et al,"['Diodato Ferraioli', 'Carmine Ventre']",['cs.GT'],False,False,True,False,False,True
683,2017-03-28T14:08:34Z,2017-02-17T22:39:37Z,http://arxiv.org/abs/1702.05536v1,http://arxiv.org/pdf/1702.05536v1,Beyond the Hazard Rate: More Perturbation Algorithms for Adversarial   Multi-armed Bandits,beyond hazard rate perturb algorithm adversari multi arm bandit,"Recent work on follow the perturbed leader (FTPL) algorithms for the adversarial multi-armed bandit problem has highlighted the role of the hazard rate of the distribution generating the perturbations. Assuming that the hazard rate is bounded, it is possible to provide regret analyses for a variety of FTPL algorithms for the multi-armed bandit problem. This paper pushes the inquiry into regret bounds for FTPL algorithms beyond the bounded hazard rate condition. There are good reasons to do so: natural distributions such as the uniform and Gaussian violate the condition. We give regret bounds for both bounded support and unbounded support distributions without assuming the hazard rate condition. We also disprove a conjecture that the Gaussian distribution cannot lead to a low-regret algorithm. In fact, it turns out that it leads to near optimal regret, up to logarithmic factors. A key ingredient in our approach is the introduction of a new notion called the generalized hazard rate.",recent work follow perturb leader ftpl algorithm adversari multi arm bandit problem highlight role hazard rate distribut generat perturb assum hazard rate bound possibl provid regret analys varieti ftpl algorithm multi arm bandit problem paper push inquiri regret bound ftpl algorithm beyond bound hazard rate condit good reason natur distribut uniform gaussian violat condit give regret bound bound support unbound support distribut without assum hazard rate condit also disprov conjectur gaussian distribut cannot lead low regret algorithm fact turn lead near optim regret logarithm factor key ingredi approach introduct new notion call general hazard rate,"['Zifan Li', 'Ambuj Tewari']","['cs.LG', 'cs.GT', 'stat.ML']",False,False,False,False,False,True
684,2017-03-28T14:08:34Z,2017-02-17T18:52:11Z,http://arxiv.org/abs/1702.05472v1,http://arxiv.org/pdf/1702.05472v1,Threshold Constraints with Guarantees for Parity Objectives in Markov   Decision Processes,threshold constraint guarante pariti object markov decis process,"The beyond worst-case synthesis problem was introduced recently by Bruy\`ere et al. [BFRR14]: it aims at building system controllers that provide strict worst-case performance guarantees against an antagonistic environment while ensuring higher expected performance against a stochastic model of the environment. Our work extends the framework of [BFRR14] and follow-up papers, which focused on quantitative objectives, by addressing the case of $\omega$-regular conditions encoded as parity objectives, a natural way to represent functional requirements of systems.   We build strategies that satisfy a main parity objective on all plays, while ensuring a secondary one with sufficient probability. This setting raises new challenges in comparison to quantitative objectives, as one cannot easily mix different strategies without endangering the functional properties of the system. We establish that, for all variants of this problem, deciding the existence of a strategy lies in ${\sf NP} \cap {\sf coNP}$, the same complexity class as classical parity games. Hence, our framework provides additional modeling power while staying in the same complexity class.   [BFRR14] V\'eronique Bruy\`ere, Emmanuel Filiot, Mickael Randour, and Jean-Fran\c{c}ois Raskin. Meet your expectations with guarantees: Beyond worst-case synthesis in quantitative games. In Ernst W. Mayr and Natacha Portier, editors, 31st International Symposium on Theoretical Aspects of Computer Science, STACS 2014, March 5-8, 2014, Lyon, France, volume 25 of LIPIcs, pages 199-213. Schloss Dagstuhl - Leibniz - Zentrum fuer Informatik, 2014.",beyond worst case synthesi problem introduc recent bruy ere et al bfrr aim build system control provid strict worst case perform guarante antagonist environ ensur higher expect perform stochast model environ work extend framework bfrr follow paper focus quantit object address case omega regular condit encod pariti object natur way repres function requir system build strategi satisfi main pariti object play ensur secondari one suffici probabl set rais new challeng comparison quantit object one cannot easili mix differ strategi without endang function properti system establish variant problem decid exist strategi lie sf np cap sf conp complex class classic pariti game henc framework provid addit model power stay complex class bfrr eroniqu bruy ere emmanuel filiot mickael randour jean fran oi raskin meet expect guarante beyond worst case synthesi quantit game ernst mayr natacha portier editor st intern symposium theoret aspect comput scienc stac march lyon franc volum lipic page schloss dagstuhl leibniz zentrum fuer informatik,"['Raphaël Berthon', 'Mickael Randour', 'Jean-François Raskin']","['cs.LO', 'cs.AI', 'cs.FL', 'cs.GT', 'math.PR']",False,False,False,False,False,True
685,2017-03-28T14:08:34Z,2017-02-25T08:57:27Z,http://arxiv.org/abs/1702.05371v2,http://arxiv.org/pdf/1702.05371v2,"Distributionally Robust Games, Part I: f-Divergence and Learning",distribut robust game part diverg learn,"In this paper we introduce the novel framework of distributionally robust games. These are multi-player games where each player models the state of nature using a worst-case distribution, also called adversarial distribution. Thus each player's payoff depends on the other players' decisions and on the decision of a virtual player (nature) who selects an adversarial distribution of scenarios. This paper provides three main contributions. Firstly, the distributionally robust game is formulated using the statistical notions of f-divergence between two distributions, here represented by the adversarial distribution, and the exact distribution. Secondly, the complexity of the problem is significantly reduced by means of triality theory. Thirdly, stochastic Bregman learning algorithms are proposed to speedup the computation of robust equilibria. Finally, the theoretical findings are illustrated in a convex setting and its limitations are tested with a non-convex non-concave function.",paper introduc novel framework distribut robust game multi player game player model state natur use worst case distribut also call adversari distribut thus player payoff depend player decis decis virtual player natur select adversari distribut scenario paper provid three main contribut first distribut robust game formul use statist notion diverg two distribut repres adversari distribut exact distribut second complex problem signific reduc mean trialiti theori third stochast bregman learn algorithm propos speedup comput robust equilibria final theoret find illustr convex set limit test non convex non concav function,"['Dario Bauso', 'Jian Gao', 'Hamidou Tembine']","['math.OC', 'cs.GT']",False,False,False,False,False,True
686,2017-03-28T14:08:34Z,2017-02-25T09:20:25Z,http://arxiv.org/abs/1702.05361v2,http://arxiv.org/pdf/1702.05361v2,Empathy in One-Shot Prisoner Dilemma,empathi one shot prison dilemma,"Strategic decision making involves affective and cognitive functions like reasoning, cognitive and emotional empathy which may be subject to age and gender differences. However, empathy-related changes in strategic decision-making and their relation to age, gender and neuropsychological functions have not been studied widely. In this article, we study a one-shot prisoner dilemma from a psychological game theory viewpoint. Forty seven participants (28 women and 19 men), aged 18 to 42 years, were tested with a empathy questionnaire and a one-shot prisoner dilemma questionnaire comprising a closiness option with the other participant. The percentage of cooperation and defection decisions was analyzed. A new empathetic payoff model was calculated to fit the observations from the test whether multi-dimensional empathy levels matter in the outcome. A significant level of cooperation is observed in the experimental one-shot game. The collected data suggests that perspective taking, empathic concern and fantasy scale are strongly correlated and have an important effect on cooperative decisions. However, their effect in the payoff is not additive. Mixed scales as well as other non-classified subscales (25+8 out of 47) were observed from the data.",strateg decis make involv affect cognit function like reason cognit emot empathi may subject age gender differ howev empathi relat chang strateg decis make relat age gender neuropsycholog function studi wide articl studi one shot prison dilemma psycholog game theori viewpoint forti seven particip women men age year test empathi questionnair one shot prison dilemma questionnair compris closi option particip percentag cooper defect decis analyz new empathet payoff model calcul fit observ test whether multi dimension empathi level matter outcom signific level cooper observ experiment one shot game collect data suggest perspect take empath concern fantasi scale strong correl import effect cooper decis howev effect payoff addit mix scale well non classifi subscal observ data,"['Giulia Rossi', 'Alain Tcheukam', 'Hamidou Tembine']",['cs.GT'],False,False,False,False,False,True
688,2017-03-28T14:08:34Z,2017-02-16T19:23:12Z,http://arxiv.org/abs/1702.05119v1,http://arxiv.org/pdf/1702.05119v1,Evolutionary prisoner's dilemma games coevolving on adaptive networks,evolutionari prison dilemma game coevolv adapt network,"We study a model for switching strategies in the Prisoner's Dilemma game on adaptive networks of player pairings that coevolve as players attempt to maximize their return. We use a node-based strategy model with each player following one strategy (cooperate or defect) at a time with all of its neighbors. We improve on the existing pair approximation (PA) model for this system by using approximate master equations (AMEs). We explore the parameter space demonstrating the accuracy of the approximation as compared with simulations. We study two variations of this partner-switching model to investigate the evolution, predict stationary states, and compare the total utilities and other qualitative differences between these two variants.",studi model switch strategi prison dilemma game adapt network player pair coevolv player attempt maxim return use node base strategi model player follow one strategi cooper defect time neighbor improv exist pair approxim pa model system use approxim master equat ame explor paramet space demonstr accuraci approxim compar simul studi two variat partner switch model investig evolut predict stationari state compar total util qualit differ two variant,"['Hsuan-Wei Lee', 'Nishant Malik', 'Peter J. Mucha']","['cs.SI', 'cs.GT', '91']",False,False,True,False,False,True
689,2017-03-28T14:08:34Z,2017-02-16T03:39:07Z,http://arxiv.org/abs/1702.04849v1,http://arxiv.org/pdf/1702.04849v1,Theoretical and Practical Advances on Smoothing for Extensive-Form Games,theoret practic advanc smooth extens form game,"Sparse iterative methods, in particular first-order methods, are known to be among the most effective in solving large-scale two-player zero-sum extensive-form games. The convergence rates of these methods depend heavily on the properties of the distance-generating function that they are based on. We investigate the acceleration of first-order methods for solving extensive-form games through better design of the dilated entropy function---a class of distance-generating functions related to the domains associated with the extensive-form games. By introducing a new weighting scheme for the dilated entropy function, we develop the first distance-generating function for the strategy spaces of sequential games that has no dependence on the branching factor of the player. This result improves the convergence rate of several first-order methods by a factor of $\Omega(b^dd)$, where $b$ is the branching factor of the player, and $d$ is the depth of the game tree.   Thus far, counterfactual regret minimization methods have been faster in practice, and more popular, than first-order methods despite their theoretically inferior convergence rates. Using our new weighting scheme and practical tuning we show that, for the first time, the excessive gap technique can be made faster than the fastest counterfactual regret minimization algorithm, CFR+, in practice.",spars iter method particular first order method known among effect solv larg scale two player zero sum extens form game converg rate method depend heavili properti distanc generat function base investig acceler first order method solv extens form game better design dilat entropi function class distanc generat function relat domain associ extens form game introduc new weight scheme dilat entropi function develop first distanc generat function strategi space sequenti game depend branch factor player result improv converg rate sever first order method factor omega dd branch factor player depth game tree thus far counterfactu regret minim method faster practic popular first order method despit theoret inferior converg rate use new weight scheme practic tune show first time excess gap techniqu made faster fastest counterfactu regret minim algorithm cfr practic,"['Christian Kroer', 'Kevin Waugh', 'Fatma Kilinc-Karzan', 'Tuomas Sandholm']","['cs.GT', 'cs.AI']",False,False,True,False,False,True
690,2017-03-28T14:08:38Z,2017-02-16T17:04:36Z,http://arxiv.org/abs/1702.04254v2,http://arxiv.org/pdf/1702.04254v2,"A ""Quantal Regret"" Method for Structural Econometrics in Repeated Games",quantal regret method structur econometr repeat game,"We suggest a general method for inferring players' values from their actions in repeated games. The method extends and improves upon the recent suggestion of (Nekipelov et al., EC 2015) and is based on the assumption that players are more likely to exhibit sequences of actions that have lower regret.   We evaluate this ""quantal regret"" method on two different datasets from experiments of repeated games with controlled player values: those of (Selten and Chmura, AER 2008) on a variety of two-player 2x2 games and our own experiment on ad-auctions (Noti et al., WWW 2014). We find that the quantal regret method is consistently and significantly more precise than either ""classic"" econometric methods that are based on Nash equilibria, or the ""min-regret"" method of (Nekipelov et al., EC 2015).",suggest general method infer player valu action repeat game method extend improv upon recent suggest nekipelov et al ec base assumpt player like exhibit sequenc action lower regret evalu quantal regret method two differ dataset experi repeat game control player valu selten chmura aer varieti two player game experi ad auction noti et al www find quantal regret method consist signific precis either classic econometr method base nash equilibria min regret method nekipelov et al ec,"['Noam Nisan', 'Gali Noti']",['cs.GT'],False,False,True,False,False,True
694,2017-03-28T14:08:38Z,2017-02-13T04:08:17Z,http://arxiv.org/abs/1702.03620v1,http://arxiv.org/pdf/1702.03620v1,Complexity of mixed equilibria in Boolean games,complex mix equilibria boolean game,"Boolean games are a succinct representation of strategic games wherein a player seeks to satisfy a formula of propositional logic by selecting a truth assignment to a set of propositional variables under his control.   The framework has proven popular within the multiagent community, however, almost invariably, the work to date has been restricted to the case of pure strategies. Such a focus is highly restrictive as the notion of randomised play is fundamental to the theory of strategic games -- even very simple games can fail to have pure-strategy equilibria, but every finite game has at least one equilibrium in mixed strategies.   To address this, the present work focuses on the complexity of algorithmic problems dealing with mixed strategies in Boolean games. The main result is that the problem of determining whether a two-player game has an equilibrium satisfying a given payoff constraint is NEXP-complete. Based on this result, we then demonstrate that a number of other decision problems, such as the uniqueness of an equilibrium or the satisfaction of a given formula in equilibrium, are either NEXP or coNEXP-complete. The proof techniques developed in the course of this are then used to show that the problem of deciding whether a given profile is in equilibrium is coNP^#P-hard, and the problem of deciding whether a Boolean game has a rational-valued equilibrium is NEXP-hard, and whether a two-player Boolean game has an irrational-valued equilibrium is NEXP-complete. Finally, we show that determining whether the value of a two-player zero-sum game exceeds a given threshold is EXP-complete.",boolean game succinct represent strateg game wherein player seek satisfi formula proposit logic select truth assign set proposit variabl control framework proven popular within multiag communiti howev almost invari work date restrict case pure strategi focus high restrict notion randomis play fundament theori strateg game even veri simpl game fail pure strategi equilibria everi finit game least one equilibrium mix strategi address present work focus complex algorithm problem deal mix strategi boolean game main result problem determin whether two player game equilibrium satisfi given payoff constraint nexp complet base result demonstr number decis problem uniqu equilibrium satisfact given formula equilibrium either nexp conexp complet proof techniqu develop cours use show problem decid whether given profil equilibrium conp hard problem decid whether boolean game ration valu equilibrium nexp hard whether two player boolean game irrat valu equilibrium nexp complet final show determin whether valu two player zero sum game exceed given threshold exp complet,['Egor Ianovski'],['cs.GT'],False,False,False,False,False,True
695,2017-03-28T14:08:38Z,2017-02-13T03:07:49Z,http://arxiv.org/abs/1702.03615v1,http://arxiv.org/pdf/1702.03615v1,Online Prediction with Selfish Experts,onlin predict selfish expert,"We consider the problem of binary prediction with expert advice in settings where experts have agency and seek to maximize their credibility. This paper makes three main contributions. First, it defines a model to reason formally about settings with selfish experts, and demonstrates that ""incentive compatible"" (IC) algorithms are closely related to the design of proper scoring rules. Designing a good IC algorithm is easy if the designer's loss function is quadratic, but for other loss functions, novel techniques are required. Second, we design IC algorithms with good performance guarantees for the absolute loss function. Third, we give a formal separation between the power of online prediction with selfish experts and online prediction with honest experts by proving lower bounds for both IC and non-IC algorithms. In particular, with selfish experts and the absolute loss function, there is no (randomized) algorithm for online prediction-IC or otherwise-with asymptotically vanishing regret.",consid problem binari predict expert advic set expert agenc seek maxim credibl paper make three main contribut first defin model reason formal set selfish expert demonstr incent compat ic algorithm close relat design proper score rule design good ic algorithm easi design loss function quadrat loss function novel techniqu requir second design ic algorithm good perform guarante absolut loss function third give formal separ power onlin predict selfish expert onlin predict honest expert prove lower bound ic non ic algorithm particular selfish expert absolut loss function random algorithm onlin predict ic otherwis asymptot vanish regret,"['Tim Roughgarden', 'Okke Schrijvers']",['cs.GT'],False,False,False,False,False,True
697,2017-03-28T14:08:38Z,2017-02-10T01:48:40Z,http://arxiv.org/abs/1702.03037v1,http://arxiv.org/pdf/1702.03037v1,Multi-agent Reinforcement Learning in Sequential Social Dilemmas,multi agent reinforc learn sequenti social dilemma,"Matrix games like Prisoner's Dilemma have guided research on social dilemmas for decades. However, they necessarily treat the choice to cooperate or defect as an atomic action. In real-world social dilemmas these choices are temporally extended. Cooperativeness is a property that applies to policies, not elementary actions. We introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. We analyze the dynamics of policies learned by multiple self-interested independent learning agents, each using its own deep Q-network, on two Markov games we introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We characterize how learned behavior in each domain changes as a function of environmental factors including resource abundance. Our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation.",matrix game like prison dilemma guid research social dilemma decad howev necessarili treat choic cooper defect atom action real world social dilemma choic tempor extend cooper properti appli polici elementari action introduc sequenti social dilemma share mix incent structur matrix game social dilemma also requir agent learn polici implement strateg intent analyz dynam polici learn multipl self interest independ learn agent use deep network two markov game introduc fruit gather game wolfpack hunt game character learn behavior domain chang function environment factor includ resourc abund experi show conflict emerg competit share resourc shed light sequenti natur real world social dilemma affect cooper,"['Joel Z. Leibo', 'Vinicius Zambaldi', 'Marc Lanctot', 'Janusz Marecki', 'Thore Graepel']","['cs.MA', 'cs.AI', 'cs.GT', 'cs.LG']",False,False,False,False,False,True
699,2017-03-28T14:08:38Z,2017-02-09T23:48:27Z,http://arxiv.org/abs/1702.03018v1,http://arxiv.org/pdf/1702.03018v1,Counterexamples to conjectures about Subset Takeaway and counting linear   extensions of a Boolean lattice,counterexampl conjectur subset takeaway count linear extens boolean lattic,"We study the game known as Subset Takeaway (Chomp on a hypercube), and give unexpected answers to questions of Gale and Neyman. We show that the number of linear extensions of the lattice of a 7-cube is 630470261306055898099742878692134361829979979674711225065761605059425237453564989302659882866111738567871048772795838071474370002961694720 (roughly $6.3 \cdot 10^{137}$).",studi game known subset takeaway chomp hypercub give unexpect answer question gale neyman show number linear extens lattic cube rough cdot,"['Andries E. Brouwer', 'J. Daniel Christensen']","['math.CO', 'cs.GT', '91A46, 06A07']",False,False,False,False,False,True
700,2017-03-28T14:10:09Z,2017-03-27T14:03:24Z,http://arxiv.org/abs/1703.09087v1,http://arxiv.org/pdf/1703.09087v1,Automating decision making to help establish norm-based regulations,autom decis make help establish norm base regul,"Norms have been extensively proposed as coordination mechanisms for both agent and human societies. Nevertheless, choosing the norms to regulate a society is by no means straightforward. The reasons are twofold. First, the norms to choose from may not be independent (i.e, they can be related to each other). Second, different preference criteria may be applied when choosing the norms to enact. This paper advances the state of the art by modeling a series of decision-making problems that regulation authorities confront when choosing the policies to establish. In order to do so, we first identify three different norm relationships -namely, generalisation, exclusivity, and substitutability- and we then consider norm representation power, cost, and associated moral values as alternative preference criteria. Thereafter, we show that the decision-making problems faced by policy makers can be encoded as linear programs, and hence solved with the aid of state-of-the-art solvers.",norm extens propos coordin mechan agent human societi nevertheless choos norm regul societi mean straightforward reason twofold first norm choos may independ relat second differ prefer criteria may appli choos norm enact paper advanc state art model seri decis make problem regul author confront choos polici establish order first identifi three differ norm relationship name generalis exclus substitut consid norm represent power cost associ moral valu altern prefer criteria thereaft show decis make problem face polici maker encod linear program henc solv aid state art solver,"['Maite Lopez-Sanchez', 'Marc Serramia', 'Juan A. Rodriguez-Aguilar', 'Javier Morales', 'Michael Wooldridge']",['cs.MA'],False,False,False,False,False,True
702,2017-03-28T14:10:09Z,2017-03-24T10:24:47Z,http://arxiv.org/abs/1703.08342v1,http://arxiv.org/abs/1703.08342v1,Event-based State Estimation: An Emulation-based Approach,event base state estim emul base approach,"An event-based state estimation approach for reducing communication in a networked control system is proposed. Multiple distributed sensor agents observe a dynamic process and sporadically transmit their measurements to estimator agents over a shared bus network. Local event-triggering protocols ensure that data is transmitted only when necessary to meet a desired estimation accuracy. The event-based design is shown to emulate the performance of a centralised state observer design up to guaranteed bounds, but with reduced communication. The stability results for state estimation are extended to the distributed control system that results when the local estimates are used for feedback control. Results from numerical simulations and hardware experiments illustrate the effectiveness of the proposed approach in reducing network communication.",event base state estim approach reduc communic network control system propos multipl distribut sensor agent observ dynam process sporad transmit measur estim agent share bus network local event trigger protocol ensur data transmit onli necessari meet desir estim accuraci event base design shown emul perform centralis state observ design guarante bound reduc communic stabil result state estim extend distribut control system result local estim use feedback control result numer simul hardwar experi illustr effect propos approach reduc network communic,['Sebastian Trimpe'],"['cs.SY', 'cs.MA']",False,False,False,False,False,True
704,2017-03-28T14:10:09Z,2017-03-22T21:29:38Z,http://arxiv.org/abs/1703.07865v1,http://arxiv.org/pdf/1703.07865v1,Weight Design of Distributed Approximate Newton Algorithms for   Constrained Optimization,weight design distribut approxim newton algorithm constrain optim,"Motivated by economic dispatch and linearly-constrained resource allocation problems, this paper proposes a novel Distributed Approx-Newton algorithm that approximates the standard Newton optimization method. A main property of this distributed algorithm is that it only requires agents to exchange constant-size communication messages. The convergence of this algorithm is discussed and rigorously analyzed. In addition, we aim to address the problem of designing communication topologies and weightings that are optimal for second-order methods. To this end, we propose an effective approximation which is loosely based on completing the square to address the NP-hard bilinear optimization involved in the design. Simulations demonstrate that our proposed weight design applied to the Distributed Approx-Newton algorithm has a superior convergence property compared to existing weighted and distributed first-order gradient descent methods.",motiv econom dispatch linear constrain resourc alloc problem paper propos novel distribut approx newton algorithm approxim standard newton optim method main properti distribut algorithm onli requir agent exchang constant size communic messag converg algorithm discuss rigor analyz addit aim address problem design communic topolog weight optim second order method end propos effect approxim loos base complet squar address np hard bilinear optim involv design simul demonstr propos weight design appli distribut approx newton algorithm superior converg properti compar exist weight distribut first order gradient descent method,"['Tor Anderson', 'Chin-Yao Chang', 'Sonia Martinez']","['cs.NA', 'cs.MA', 'math.OC']",False,False,False,False,False,True
705,2017-03-28T14:10:09Z,2017-03-24T03:00:40Z,http://arxiv.org/abs/1703.07306v2,http://arxiv.org/pdf/1703.07306v2,Controllability to Equilibria of the 1-D Fokker-Planck Equation with   Zero-Flux Boundary Condition,control equilibria fokker planck equat zero flux boundari condit,"We consider the problem of controlling the spatiotemporal probability distribution of a robotic swarm that evolves according to a reflected diffusion process, using the space- and time-dependent drift vector field parameter as the control variable. In contrast to previous work on control of the Fokker-Planck equation, a zero-flux boundary condition is imposed on the partial differential equation that governs the swarm probability distribution, and only bounded vector fields are considered to be admissible as control parameters. Under these constraints, we show that any initial probability distribution can be transported to a target probability distribution under certain assumptions on the regularity of the target distribution. In particular, we show that if the target distribution is (essentially) bounded, has bounded first-order and second-order partial derivatives, and is bounded from below by a strictly positive constant, then this distribution can be reached exactly using a drift vector field that is bounded in space and time. Our proof is constructive and based on classical linear semigroup theoretic concepts.",consid problem control spatiotempor probabl distribut robot swarm evolv accord reflect diffus process use space time depend drift vector field paramet control variabl contrast previous work control fokker planck equat zero flux boundari condit impos partial differenti equat govern swarm probabl distribut onli bound vector field consid admiss control paramet constraint show ani initi probabl distribut transport target probabl distribut certain assumpt regular target distribut particular show target distribut essenti bound bound first order second order partial deriv bound strict posit constant distribut reach exact use drift vector field bound space time proof construct base classic linear semigroup theoret concept,"['Karthik Elamvazhuthi', 'Hendrik Kuiper', 'Spring Berman']","['cs.SY', 'cs.MA', 'math.OC']",False,False,False,False,False,True
706,2017-03-28T14:10:09Z,2017-03-21T15:38:50Z,http://arxiv.org/abs/1703.07280v1,http://arxiv.org/pdf/1703.07280v1,Resilient Monotone Submodular Function Maximization,resili monoton submodular function maxim,"In this paper, we focus on applications in machine learning, optimization, and control that call for the resilient selection of a few elements, e.g. features, sensors, or leaders, against a number of adversarial denial-of-service attacks or failures. In general, such resilient optimization problems are hard, and cannot be solved exactly in polynomial time, even though they often involve objective functions that are monotone and submodular. Notwithstanding, in this paper we provide the first scalable, curvature-dependent algorithm for their approximate solution, that is valid for any number of attacks or failures, and which, for functions with low curvature, guarantees superior approximation performance. Notably, the curvature has been known to tighten approximations for several non-resilient maximization problems, yet its effect on resilient maximization had hitherto been unknown. We complement our theoretical analyses with supporting empirical evaluations.",paper focus applic machin learn optim control call resili select element featur sensor leader number adversari denial servic attack failur general resili optim problem hard cannot solv exact polynomi time even though often involv object function monoton submodular notwithstand paper provid first scalabl curvatur depend algorithm approxim solut valid ani number attack failur function low curvatur guarante superior approxim perform notabl curvatur known tighten approxim sever non resili maxim problem yet effect resili maxim hitherto unknown complement theoret analys support empir evalu,"['Vasileios Tzoumas', 'Konstantinos Gatsis', 'Ali Jadbabaie', 'George J. Pappas']","['math.OC', 'cs.MA', 'cs.SY']",False,False,False,False,False,True
708,2017-03-28T14:10:09Z,2017-03-20T11:17:39Z,http://arxiv.org/abs/1703.06680v1,http://arxiv.org/pdf/1703.06680v1,Parallel Sort-Based Matching for Data Distribution Management on   Shared-Memory Multiprocessors,parallel sort base match data distribut manag share memori multiprocessor,"In this paper we consider the problem of identifying intersections between two sets of d-dimensional axis-parallel rectangles. This is a common problem that arises in many agent-based simulation studies, and is of central importance in the context of High Level Architecture (HLA), where it is at the core of the Data Distribution Management (DDM) service. Several realizations of the DDM service have been proposed; however, many of them are either inefficient or inherently sequential. These are serious limitations since multicore processors are now ubiquitous, and DDM algorithms -- being CPU-intensive -- could benefit from additional computing power. We propose a parallel version of the Sort-Based Matching algorithm for shared-memory multiprocessors. Sort-Based Matching is one of the most efficient serial algorithms for the DDM problem, but is quite difficult to parallelize due to data dependencies. We describe the algorithm and compute its asymptotic running time; we complete the analysis by assessing its performance and scalability through extensive experiments on two commodity multicore systems based on a dual socket Intel Xeon processor, and a single socket Intel Core i7 processor.",paper consid problem identifi intersect two set dimension axi parallel rectangl common problem aris mani agent base simul studi central import context high level architectur hla core data distribut manag ddm servic sever realize ddm servic propos howev mani either ineffici inher sequenti serious limit sinc multicor processor ubiquit ddm algorithm cpu intens could benefit addit comput power propos parallel version sort base match algorithm share memori multiprocessor sort base match one effici serial algorithm ddm problem quit difficult parallel due data depend describ algorithm comput asymptot run time complet analysi assess perform scalabl extens experi two commod multicor system base dual socket intel xeon processor singl socket intel core processor,"['Moreno Marzolla', ""Gabriele D'Angelo""]","['cs.DC', 'cs.DS', 'cs.MA']",False,False,False,False,False,True
709,2017-03-28T14:10:09Z,2017-03-19T10:35:17Z,http://arxiv.org/abs/1703.06416v1,http://arxiv.org/pdf/1703.06416v1,A Passivity-Based Distributed Reference Governor for Constrained Robotic   Networks,passiv base distribut refer governor constrain robot network,"This paper focuses on a passivity-based distributed reference governor (RG) applied to a pre-stabilized mobile robotic network. The novelty of this paper lies in the method used to solve the RG problem, where a passivity-based distributed optimization scheme is proposed. In particular, the gradient descent method minimizes the global objective function while the dual ascent method maximizes the Hamiltonian. To make the agents converge to the agreed optimal solution, a proportional-integral consensus estimator is used. This paper proves the convergence of the state estimates of the RG to the optimal solution through passivity arguments, considering the physical system static. Then, the effectiveness of the scheme considering the dynamics of the physical system is demonstrated through simulations and experiments.",paper focus passiv base distribut refer governor rg appli pre stabil mobil robot network novelti paper lie method use solv rg problem passiv base distribut optim scheme propos particular gradient descent method minim global object function dual ascent method maxim hamiltonian make agent converg agre optim solut proport integr consensus estim use paper prove converg state estim rg optim solut passiv argument consid physic system static effect scheme consid dynam physic system demonstr simul experi,"['Tam Nguyen', 'Takeshi Hatanaka', 'Mamoru Doi', 'Emanuele Garone', 'Masayuki Fujita']","['cs.MA', 'cs.DC', 'cs.RO', 'cs.SY']",False,False,False,False,False,True
710,2017-03-28T14:10:13Z,2017-03-18T07:11:53Z,http://arxiv.org/abs/1703.06261v1,http://arxiv.org/pdf/1703.06261v1,Cooperative Localisation of a GPS-Denied UAV in 3-Dimensional Space   Using Direction of Arrival Measurements,cooper localis gps deni uav dimension space use direct arriv measur,"This paper presents a novel approach for localising a GPS (Global Positioning System)-denied Unmanned Aerial Vehicle (UAV) with the aid of a GPS-equipped UAV in three-dimensional space. The GPS-equipped UAV makes discrete-time broadcasts of its global coordinates. The GPS-denied UAV simultaneously receives the broadcast and takes direction of arrival (DOA) measurements towards the origin of the broadcast in its local coordinate frame (obtained via an inertial navigation system (INS)). The aim is to determine the difference between the local and global frames, described by a rotation and a translation. In the noiseless case, global coordinates were recovered exactly by solving a system of linear equations. When DOA measurements are contaminated with noise, rank relaxed semidefinite programming (SDP) and the Orthogonal Procrustes algorithm are employed. Simulations are provided and factors affecting accuracy, such as noise levels and number of measurements, are explored.",paper present novel approach localis gps global posit system deni unman aerial vehicl uav aid gps equip uav three dimension space gps equip uav make discret time broadcast global coordin gps deni uav simultan receiv broadcast take direct arriv doa measur toward origin broadcast local coordin frame obtain via inerti navig system aim determin differ local global frame describ rotat translat noiseless case global coordin recov exact solv system linear equat doa measur contamin nois rank relax semidefinit program sdp orthogon procrust algorithm employ simul provid factor affect accuraci nois level number measur explor,"['James Russell', 'Mengbin Ye', 'Brian D. O. Anderson', 'Hatem Hmam', 'Peter Sarunic']","['cs.RO', 'cs.MA', 'cs.SY']",False,False,False,False,False,True
711,2017-03-28T14:10:13Z,2017-03-16T14:04:38Z,http://arxiv.org/abs/1703.05626v1,http://arxiv.org/pdf/1703.05626v1,Scalable Accelerated Decentralized Multi-Robot Policy Search in   Continuous Observation Spaces,scalabl acceler decentr multi robot polici search continu observ space,"This paper presents the first ever approach for solving \emph{continuous-observation} Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) and their semi-Markovian counterparts, Dec-POSMDPs. This contribution is especially important in robotics, where a vast number of sensors provide continuous observation data. A continuous-observation policy representation is introduced using Stochastic Kernel-based Finite State Automata (SK-FSAs). An SK-FSA search algorithm titled Entropy-based Policy Search using Continuous Kernel Observations (EPSCKO) is introduced and applied to the first ever continuous-observation Dec-POMDP/Dec-POSMDP domain, where it significantly outperforms state-of-the-art discrete approaches. This methodology is equally applicable to Dec-POMDPs and Dec-POSMDPs, though the empirical analysis presented focuses on Dec-POSMDPs due to their higher scalability. To improve convergence, an entropy injection policy search acceleration approach for both continuous and discrete observation cases is also developed and shown to improve convergence rates without degrading policy quality.",paper present first ever approach solv emph continu observ decentr partial observ markov decis process dec pomdp semi markovian counterpart dec posmdp contribut especi import robot vast number sensor provid continu observ data continu observ polici represent introduc use stochast kernel base finit state automata sk fsas sk fsa search algorithm titl entropi base polici search use continu kernel observ epscko introduc appli first ever continu observ dec pomdp dec posmdp domain signific outperform state art discret approach methodolog equal applic dec pomdp dec posmdp though empir analysi present focus dec posmdp due higher scalabl improv converg entropi inject polici search acceler approach continu discret observ case also develop shown improv converg rate without degrad polici qualiti,"['Shayegan Omidshafiei', 'Christopher Amato', 'Miao Liu', 'Michael Everett', 'Jonathan P. How', 'John Vian']","['cs.MA', 'cs.RO']",False,False,False,False,False,True
712,2017-03-28T14:10:13Z,2017-03-16T13:59:48Z,http://arxiv.org/abs/1703.05623v1,http://arxiv.org/pdf/1703.05623v1,Semantic-level Decentralized Multi-Robot Decision-Making using   Probabilistic Macro-Observations,semant level decentr multi robot decis make use probabilist macro observ,"Robust environment perception is essential for decision-making on robots operating in complex domains. Intelligent task execution requires principled treatment of uncertainty sources in a robot's observation model. This is important not only for low-level observations (e.g., accelerometer data), but also for high-level observations such as semantic object labels. This paper formalizes the concept of macro-observations in Decentralized Partially Observable Semi-Markov Decision Processes (Dec-POSMDPs), allowing scalable semantic-level multi-robot decision making. A hierarchical Bayesian approach is used to model noise statistics of low-level classifier outputs, while simultaneously allowing sharing of domain noise characteristics between classes. Classification accuracy of the proposed macro-observation scheme, called Hierarchical Bayesian Noise Inference (HBNI), is shown to exceed existing methods. The macro-observation scheme is then integrated into a Dec-POSMDP planner, with hardware experiments running onboard a team of dynamic quadrotors in a challenging domain where noise-agnostic filtering fails. To the best of our knowledge, this is the first demonstration of a real-time, convolutional neural net-based classification framework running fully onboard a team of quadrotors in a multi-robot decision-making domain.",robust environ percept essenti decis make robot oper complex domain intellig task execut requir principl treatment uncertainti sourc robot observ model import onli low level observ acceleromet data also high level observ semant object label paper formal concept macro observ decentr partial observ semi markov decis process dec posmdp allow scalabl semant level multi robot decis make hierarch bayesian approach use model nois statist low level classifi output simultan allow share domain nois characterist class classif accuraci propos macro observ scheme call hierarch bayesian nois infer hbni shown exceed exist method macro observ scheme integr dec posmdp planner hardwar experi run onboard team dynam quadrotor challeng domain nois agnost filter fail best knowledg first demonstr real time convolut neural net base classif framework run fulli onboard team quadrotor multi robot decis make domain,"['Shayegan Omidshafiei', 'Shih-Yuan Liu', 'Michael Everett', 'Brett T. Lopez', 'Christopher Amato', 'Miao Liu', 'Jonathan P. How', 'John Vian']","['cs.MA', 'cs.RO']",False,False,False,False,False,True
713,2017-03-28T14:10:13Z,2017-03-16T09:15:51Z,http://arxiv.org/abs/1703.05519v1,http://arxiv.org/pdf/1703.05519v1,Arrovian Aggregation via Pairwise Utilitarianism,arrovian aggreg via pairwis utilitarian,"We consider Arrovian aggregation of preferences over lotteries that are represented by skew-symmetric bilinear (SSB) utility functions, a significant generalization of von Neumann-Morgenstern utility functions due to Fishburn, in which utility is assigned to pairs of alternatives. We show that the largest domain of preferences that simultaneously allows for independence of irrelevant alternatives and Pareto optimality when comparing lotteries based on accumulated SSB welfare is a domain in which preferences over lotteries are completely determined by ordinal preferences over pure alternatives. In particular, a lottery is preferred to another lottery if and only if the former is more likely to return a preferred alternative. Preferences over pure alternatives are unrestricted. We argue that SSB welfare maximization for this domain constitutes an appealing probabilistic social choice function.",consid arrovian aggreg prefer lotteri repres skew symmetr bilinear ssb util function signific general von neumann morgenstern util function due fishburn util assign pair altern show largest domain prefer simultan allow independ irrelev altern pareto optim compar lotteri base accumul ssb welfar domain prefer lotteri complet determin ordin prefer pure altern particular lotteri prefer anoth lotteri onli former like return prefer altern prefer pure altern unrestrict argu ssb welfar maxim domain constitut appeal probabilist social choic function,"['Florian Brandl', 'Felix Brandt']","['cs.GT', 'cs.MA']",False,False,False,False,False,True
714,2017-03-28T14:10:13Z,2017-03-21T17:38:04Z,http://arxiv.org/abs/1703.05240v2,http://arxiv.org/pdf/1703.05240v2,Humans of Simulated New York (HOSNY): an exploratory comprehensive model   of city life,human simul new york hosni exploratori comprehens model citi life,"The model presented in this paper experiments with a comprehensive simulant agent in order to provide an exploratory platform in which simulation modelers may try alternative scenarios and participation in policy decision-making. The framework is built in a computationally distributed online format in which users can join in and visually explore the results. Modeled activity involves daily routine errands, such as shopping, visiting the doctor or engaging in the labor market. Further, agents make everyday decisions based on individual behavioral attributes and minimal requirements, according to social and contagion networks. Fully developed firms and governments are also included in the model allowing for taxes collection, production decisions, bankruptcy and change in ownership. The contributions to the literature are multifold. They include (a) a comprehensive model with detailing of the agents and firms' activities and processes and original use of simultaneously (b) reinforcement learning for firm pricing and demand allocation; (c) social contagion for disease spreading and social network for hiring opportunities; and (d) Bayesian networks for demographic-like generation of agents. All of that within a (e) visually rich environment and multiple use of databases. Hence, the model provides a comprehensive framework from where interactions among citizens, firms and governments can be easily explored allowing for learning and visualization of policies and scenarios.",model present paper experi comprehens simul agent order provid exploratori platform simul model may tri altern scenario particip polici decis make framework built comput distribut onlin format user join visual explor result model activ involv daili routin errand shop visit doctor engag labor market agent make everyday decis base individu behavior attribut minim requir accord social contagion network fulli develop firm govern also includ model allow tax collect product decis bankruptci chang ownership contribut literatur multifold includ comprehens model detail agent firm activ process origin use simultan reinforc learn firm price demand alloc social contagion diseas spread social network hire opportun bayesian network demograph like generat agent within visual rich environ multipl use databas henc model provid comprehens framework interact among citizen firm govern easili explor allow learn visual polici scenario,"['Francis Tseng', 'Fei Liu', 'Bernardo Alves Furtado']","['cs.MA', 'q-fin.EC']",False,False,False,False,False,True
716,2017-03-28T14:10:13Z,2017-03-15T02:59:10Z,http://arxiv.org/abs/1703.04901v1,http://arxiv.org/pdf/1703.04901v1,On the Analysis of the DeGroot-Friedkin Model with Dynamic Relative   Interaction Matrices,analysi degroot friedkin model dynam relat interact matric,"This paper analyses the DeGroot-Friedkin model for evolution of the individuals' social powers in a social network when the network topology varies dynamically (described by dynamic relative interaction matrices). The DeGroot-Friedkin model describes how individual social power (self-appraisal, self-weight) evolves as a network of individuals discuss a sequence of issues. We seek to study dynamically changing relative interactions because interactions may change depending on the issue being discussed. In order to explore the problem in detail, two different cases of issue-dependent network topologies are studied. First, if the topology varies between issues in a periodic manner, it is shown that the individuals' self-appraisals admit a periodic solution. Second, if the topology changes arbitrarily, under the assumption that each relative interaction matrix is doubly stochastic and irreducible, the individuals' self-appraisals asymptotically converge to a unique non-trivial equilibrium.",paper analys degroot friedkin model evolut individu social power social network network topolog vari dynam describ dynam relat interact matric degroot friedkin model describ individu social power self apprais self weight evolv network individu discuss sequenc issu seek studi dynam chang relat interact becaus interact may chang depend issu discuss order explor problem detail two differ case issu depend network topolog studi first topolog vari issu period manner shown individu self apprais admit period solut second topolog chang arbitrarili assumpt relat interact matrix doubli stochast irreduc individu self apprais asymptot converg uniqu non trivial equilibrium,"['Mengbin Ye', 'Ji Liu', 'Brian David Outram Anderson', 'Changbin Yu', 'Tamer Başar']","['cs.SI', 'cs.MA', 'cs.SY', 'physics.soc-ph']",False,False,True,False,False,True
717,2017-03-28T14:10:13Z,2017-03-14T22:13:20Z,http://arxiv.org/abs/1703.04756v1,http://arxiv.org/pdf/1703.04756v1,Weighted Voting Via No-Regret Learning,weight vote via regret learn,"Voting systems typically treat all voters equally. We argue that perhaps they should not: Voters who have supported good choices in the past should be given higher weight than voters who have supported bad ones. To develop a formal framework for desirable weighting schemes, we draw on no-regret learning. Specifically, given a voting rule, we wish to design a weighting scheme such that applying the voting rule, with voters weighted by the scheme, leads to choices that are almost as good as those endorsed by the best voter in hindsight. We derive possibility and impossibility results for the existence of such weighting schemes, depending on whether the voting rule and the weighting scheme are deterministic or randomized, as well as on the social choice axioms satisfied by the voting rule.",vote system typic treat voter equal argu perhap voter support good choic past given higher weight voter support bad one develop formal framework desir weight scheme draw regret learn specif given vote rule wish design weight scheme appli vote rule voter weight scheme lead choic almost good endors best voter hindsight deriv possibl imposs result exist weight scheme depend whether vote rule weight scheme determinist random well social choic axiom satisfi vote rule,"['Nika Haghtalab', 'Ritesh Noothigattu', 'Ariel D. Procaccia']","['cs.GT', 'cs.AI', 'cs.LG', 'cs.MA']",False,False,False,False,False,True
718,2017-03-28T14:10:13Z,2017-03-11T10:29:57Z,http://arxiv.org/abs/1703.03943v1,http://arxiv.org/abs/1703.03943v1,A norm knockout method on indirect reciprocity to reveal indispensable   norms,norm knockout method indirect reciproc reveal indispens norm,"Although various norms for reciprocity-based cooperation have been suggested that are evolutionarily stable against invasion from free riders, the process of alternation of norms and the role of diversified norms remain unclear in the evolution of cooperation. We clarify the co-evolutionary dynamics of norms and cooperation in indirect reciprocity and also identify the indispensable norms for the evolution of cooperation. Inspired by the gene knockout method, a genetic engineering technique, we developed the norm knockout method and clarified the norms necessary for the establishment of cooperation. The results of numerical investigations revealed that the majority of norms gradually transitioned to tolerant norms after defectors are eliminated by strict norms. Furthermore, no cooperation emerges when specific norms that are intolerant to defectors are knocked out.",although various norm reciproc base cooper suggest evolutionarili stabl invas free rider process altern norm role diversifi norm remain unclear evolut cooper clarifi co evolutionari dynam norm cooper indirect reciproc also identifi indispens norm evolut cooper inspir gene knockout method genet engin techniqu develop norm knockout method clarifi norm necessari establish cooper result numer investig reveal major norm gradual transit toler norm defector elimin strict norm furthermor cooper emerg specif norm intoler defector knock,"['Hitoshi Yamamoto', 'Isamu Okada', 'Satoshi Uchida', 'Tatsuya Sasaki']","['physics.soc-ph', 'cs.MA', 'cs.NE', 'q-bio.PE']",False,False,False,False,False,True
719,2017-03-28T14:10:13Z,2017-03-08T04:58:51Z,http://arxiv.org/abs/1703.02702v1,http://arxiv.org/pdf/1703.02702v1,Robust Adversarial Reinforcement Learning,robust adversari reinforc learn,"Deep neural networks coupled with fast simulation and improved computation have led to recent successes in the field of reinforcement learning (RL). However, most current RL-based approaches fail to generalize since: (a) the gap between simulation and real world is so large that policy-learning approaches fail to transfer; (b) even if policy learning is done in real world, the data scarcity leads to failed generalization from training to test scenarios (e.g., due to different friction or object masses). Inspired from H-infinity control methods, we note that both modeling errors and differences in training and test scenarios can be viewed as extra forces/disturbances in the system. This paper proposes the idea of robust adversarial reinforcement learning (RARL), where we train an agent to operate in the presence of a destabilizing adversary that applies disturbance forces to the system. The jointly trained adversary is reinforced -- that is, it learns an optimal destabilization policy. We formulate the policy learning as a zero-sum, minimax objective function. Extensive experiments in multiple environments (InvertedPendulum, HalfCheetah, Swimmer, Hopper and Walker2d) conclusively demonstrate that our method (a) improves training stability; (b) is robust to differences in training/test conditions; and c) outperform the baseline even in the absence of the adversary.",deep neural network coupl fast simul improv comput led recent success field reinforc learn rl howev current rl base approach fail general sinc gap simul real world larg polici learn approach fail transfer even polici learn done real world data scarciti lead fail general train test scenario due differ friction object mass inspir infin control method note model error differ train test scenario view extra forc disturb system paper propos idea robust adversari reinforc learn rarl train agent oper presenc destabil adversari appli disturb forc system joint train adversari reinforc learn optim destabil polici formul polici learn zero sum minimax object function extens experi multipl environ invertedpendulum halfcheetah swimmer hopper walkerd conclus demonstr method improv train stabil robust differ train test condit outperform baselin even absenc adversari,"['Lerrel Pinto', 'James Davidson', 'Rahul Sukthankar', 'Abhinav Gupta']","['cs.LG', 'cs.AI', 'cs.MA', 'cs.RO']",False,False,False,False,False,True
721,2017-03-28T14:10:17Z,2017-03-07T14:33:00Z,http://arxiv.org/abs/1703.02399v1,http://arxiv.org/pdf/1703.02399v1,On time and consistency in multi-level agent-based simulations,time consist multi level agent base simul,"The integration of multiple viewpoints became an increasingly popular approach to deal with agent-based simulations. Despite their disparities, recent approaches successfully manage to run such multi-level simulations. Yet, are they doing it appropriately?   This paper tries to answer that question, with an analysis based on a generic model of the temporal dynamics of multi-level simulations. This generic model is then used to build an orthogonal approach to multi-level simulation called SIMILAR. In this approach, most time-related issues are explicitly modeled, owing to an implementation-oriented approach based on the influence/reaction principle.",integr multipl viewpoint becam increas popular approach deal agent base simul despit dispar recent approach success manag run multi level simul yet appropri paper tri answer question analysi base generic model tempor dynam multi level simul generic model use build orthogon approach multi level simul call similar approach time relat issu explicit model owe implement orient approach base influenc reaction principl,"['Gildas Morvan', 'Yoann Kubera']",['cs.MA'],False,False,True,False,False,True
722,2017-03-28T14:10:17Z,2017-03-07T13:18:14Z,http://arxiv.org/abs/1703.02367v1,http://arxiv.org/pdf/1703.02367v1,Vocabulary Alignment in Openly Specified Interactions,vocabulari align open specifi interact,"The problem of achieving common understanding between agents that use different vocabularies has been mainly addressed by designing techniques that explicitly negotiate mappings between their vocabularies, requiring agents to share a meta-language. In this paper we consider the case of agents that use different vocabularies and have no meta-language in common, but share the knowledge of how to perform a task, given by the specification of an interaction protocol. For this situation, we present a framework that lets agents learn a vocabulary alignment from the experience of interacting. Unlike previous work in this direction, we use open protocols that constrain possible actions instead of defining procedures, making our approach more general. We present two techniques that can be used either to learn an alignment from scratch or to repair an existent one, and we evaluate experimentally their performance.",problem achiev common understand agent use differ vocabulari main address design techniqu explicit negoti map vocabulari requir agent share meta languag paper consid case agent use differ vocabulari meta languag common share knowledg perform task given specif interact protocol situat present framework let agent learn vocabulari align experi interact unlik previous work direct use open protocol constrain possibl action instead defin procedur make approach general present two techniqu use either learn align scratch repair exist one evalu experiment perform,"['Paula Chocron', 'Marco Schorlemmer']",['cs.MA'],False,False,False,False,False,True
723,2017-03-28T14:10:17Z,2017-03-07T03:16:22Z,http://arxiv.org/abs/1703.02196v1,http://arxiv.org/abs/1703.02196v1,Cooperative Epistemic Multi-Agent Planning for Implicit Coordination,cooper epistem multi agent plan implicit coordin,"Epistemic planning can be used for decision making in multi-agent situations with distributed knowledge and capabilities. Recently, Dynamic Epistemic Logic (DEL) has been shown to provide a very natural and expressive framework for epistemic planning. We extend the DEL-based epistemic planning framework to include perspective shifts, allowing us to define new notions of sequential and conditional planning with implicit coordination. With these, it is possible to solve planning tasks with joint goals in a decentralized manner without the agents having to negotiate about and commit to a joint policy at plan time. First we define the central planning notions and sketch the implementation of a planning system built on those notions. Afterwards we provide some case studies in order to evaluate the planner empirically and to show that the concept is useful for multi-agent systems in practice.",epistem plan use decis make multi agent situat distribut knowledg capabl recent dynam epistem logic del shown provid veri natur express framework epistem plan extend del base epistem plan framework includ perspect shift allow us defin new notion sequenti condit plan implicit coordin possibl solv plan task joint goal decentr manner without agent negoti commit joint polici plan time first defin central plan notion sketch implement plan system built notion afterward provid case studi order evalu planner empir show concept use multi agent system practic,"['Thorsten Engesser', 'Thomas Bolander', 'Robert Mattmüller', 'Bernhard Nebel']","['cs.AI', 'cs.LO', 'cs.MA']",False,False,True,False,False,True
724,2017-03-28T14:10:17Z,2017-03-07T03:15:08Z,http://arxiv.org/abs/1703.02192v1,http://arxiv.org/abs/1703.02192v1,A Gentle Introduction to Epistemic Planning: The DEL Approach,gentl introduct epistem plan del approach,"Epistemic planning can be used for decision making in multi-agent situations with distributed knowledge and capabilities. Dynamic Epistemic Logic (DEL) has been shown to provide a very natural and expressive framework for epistemic planning. In this paper, we aim to give an accessible introduction to DEL-based epistemic planning. The paper starts with the most classical framework for planning, STRIPS, and then moves towards epistemic planning in a number of smaller steps, where each step is motivated by the need to be able to model more complex planning scenarios.",epistem plan use decis make multi agent situat distribut knowledg capabl dynam epistem logic del shown provid veri natur express framework epistem plan paper aim give access introduct del base epistem plan paper start classic framework plan strip move toward epistem plan number smaller step step motiv need abl model complex plan scenario,['Thomas Bolander'],"['cs.AI', 'cs.LO', 'cs.MA']",False,False,False,False,False,True
725,2017-03-28T14:10:17Z,2017-03-06T15:44:10Z,http://arxiv.org/abs/1703.01931v1,http://arxiv.org/pdf/1703.01931v1,Context-Based Concurrent Experience Sharing in Multiagent Systems,context base concurr experi share multiag system,"One of the key challenges for multi-agent learning is scalability. In this paper, we introduce a technique for speeding up multi-agent learning by exploiting concurrent and incremental experience sharing. This solution adaptively identifies opportunities to transfer experiences between agents and allows for the rapid acquisition of appropriate policies in large-scale, stochastic, homogeneous multi-agent systems. We introduce an online, distributed, supervisor-directed transfer technique for constructing high-level characterizations of an agent's dynamic learning environment---called contexts---which are used to identify groups of agents operating under approximately similar dynamics within a short temporal window. A set of supervisory agents computes contextual information for groups of subordinate agents, thereby identifying candidates for experience sharing. Our method uses a tiered architecture to propagate, with low communication overhead, state, action, and reward data amongst the members of each dynamically-identified information-sharing group. We applied this method to a large-scale distributed task allocation problem with hundreds of information-sharing agents operating in an unknown, non-stationary environment. We demonstrate that our approach results in significant performance gains, that it is robust to noise-corrupted or suboptimal context features, and that communication costs scale linearly with the supervisor-to-subordinate ratio.",one key challeng multi agent learn scalabl paper introduc techniqu speed multi agent learn exploit concurr increment experi share solut adapt identifi opportun transfer experi agent allow rapid acquisit appropri polici larg scale stochast homogen multi agent system introduc onlin distribut supervisor direct transfer techniqu construct high level character agent dynam learn environ call context use identifi group agent oper approxim similar dynam within short tempor window set supervisori agent comput contextu inform group subordin agent therebi identifi candid experi share method use tier architectur propag low communic overhead state action reward data amongst member dynam identifi inform share group appli method larg scale distribut task alloc problem hundr inform share agent oper unknown non stationari environ demonstr approach result signific perform gain robust nois corrupt suboptim context featur communic cost scale linear supervisor subordin ratio,"['Dan Garant', 'Bruno da Silva', 'Victor Lesser', 'Chongjie Zhang']",['cs.MA'],False,False,True,False,False,True
726,2017-03-28T14:10:17Z,2017-03-01T14:42:20Z,http://arxiv.org/abs/1703.00320v1,http://arxiv.org/pdf/1703.00320v1,Investigating the Characteristics of One-Sided Matching Mechanisms Under   Various Preferences and Risk Attitudes,investig characterist one side match mechan various prefer risk attitud,"One-sided matching mechanisms are fundamental for assigning a set of indivisible objects to a set of self-interested agents when monetary transfers are not allowed. Two widely-studied randomized mechanisms in multiagent settings are the Random Serial Dictatorship (RSD) and the Probabilistic Serial Rule (PS). Both mechanisms require only that agents specify ordinal preferences and have a number of desirable economic and computational properties. However, the induced outcomes of the mechanisms are often incomparable and thus there are challenges when it comes to deciding which mechanism to adopt in practice. In this paper, we first consider the space of general ordinal preferences and provide empirical results on the (in)comparability of RSD and PS. We analyze their respective economic properties under general and lexicographic preferences. We then instantiate utility functions with the goal of gaining insights on the manipulability, efficiency, and envyfreeness of the mechanisms under different risk-attitude models. Our results hold under various preference distribution models, which further confirm the broad use of RSD in most practical applications.",one side match mechan fundament assign set indivis object set self interest agent monetari transfer allow two wide studi random mechan multiag set random serial dictatorship rsd probabilist serial rule ps mechan requir onli agent specifi ordin prefer number desir econom comput properti howev induc outcom mechan often incompar thus challeng come decid mechan adopt practic paper first consid space general ordin prefer provid empir result compar rsd ps analyz respect econom properti general lexicograph prefer instanti util function goal gain insight manipul effici envyfre mechan differ risk attitud model result hold various prefer distribut model confirm broad use rsd practic applic,"['Hadi Hosseini', 'Kate Larson', 'Robin Cohen']","['cs.GT', 'cs.AI', 'cs.MA', 'I.2.11; J.4']",False,False,True,False,False,True
727,2017-03-28T14:10:17Z,2017-02-28T17:56:41Z,http://arxiv.org/abs/1702.08887v1,http://arxiv.org/pdf/1702.08887v1,Stabilising Experience Replay for Deep Multi-Agent Reinforcement   Learning,stabilis experi replay deep multi agent reinforc learn,"Many real-world problems, such as network packet routing and urban traffic control, are naturally modeled as multi-agent reinforcement learning (RL) problems. However, existing multi-agent RL methods typically scale poorly in the problem size. Therefore, a key challenge is to translate the success of deep learning on single-agent RL to the multi-agent setting. A key stumbling block is that independent Q-learning, the most popular multi-agent RL method, introduces nonstationarity that makes it incompatible with the experience replay memory on which deep RL relies. This paper proposes two methods that address this problem: 1) conditioning each agent's value function on a footprint that disambiguates the age of the data sampled from the replay memory and 2) using a multi-agent variant of importance sampling to naturally decay obsolete data. Results on a challenging decentralised variant of StarCraft unit micromanagement confirm that these methods enable the successful combination of experience replay with multi-agent RL.",mani real world problem network packet rout urban traffic control natur model multi agent reinforc learn rl problem howev exist multi agent rl method typic scale poor problem size therefor key challeng translat success deep learn singl agent rl multi agent set key stumbl block independ learn popular multi agent rl method introduc nonstationar make incompat experi replay memori deep rl reli paper propos two method address problem condit agent valu function footprint disambigu age data sampl replay memori use multi agent variant import sampl natur decay obsolet data result challeng decentralis variant starcraft unit micromanag confirm method enabl success combin experi replay multi agent rl,"['Jakob Foerster', 'Nantas Nardelli', 'Gregory Farquhar', 'Philip. H. S. Torr', 'Pushmeet Kohli', 'Shimon Whiteson']","['cs.AI', 'cs.LG', 'cs.MA']",False,False,False,False,False,True
728,2017-03-28T14:10:17Z,2017-02-28T16:57:49Z,http://arxiv.org/abs/1702.08862v1,http://arxiv.org/pdf/1702.08862v1,Proportional Representation in Vote Streams,proport represent vote stream,"We consider elections where the voters come one at a time, in a streaming fashion, and devise space-efficient algorithms which identify an approximate winning committee with respect to common multiwinner proportional representation voting rules; specifically, we consider the Approval-based and the Borda-based variants of both the Chamberlin-- ourant rule and the Monroe rule. We complement our algorithms with lower bounds. Somewhat surprisingly, our results imply that, using space which does not depend on the number of voters it is possible to efficiently identify an approximate representative committee of fixed size over vote streams with huge number of voters.",consid elect voter come one time stream fashion devis space effici algorithm identifi approxim win committe respect common multiwinn proport represent vote rule specif consid approv base borda base variant chamberlin ourant rule monro rule complement algorithm lower bound somewhat surpris result impli use space doe depend number voter possibl effici identifi approxim repres committe fix size vote stream huge number voter,"['Palash Dey', 'Nimrod Talmon', 'Otniel van Handel']","['cs.GT', 'cs.AI', 'cs.CC', 'cs.DS', 'cs.MA']",False,False,False,False,False,True
729,2017-03-28T14:10:17Z,2017-02-28T10:49:36Z,http://arxiv.org/abs/1702.08736v1,http://arxiv.org/pdf/1702.08736v1,Analysing Congestion Problems in Multi-agent Reinforcement Learning,analys congest problem multi agent reinforc learn,"Congestion problems are omnipresent in today's complex networks and represent a challenge in many research domains. In the context of Multi-agent Reinforcement Learning (MARL), approaches like difference rewards and resource abstraction have shown promising results in tackling such problems. Resource abstraction was shown to be an ideal candidate for solving large-scale resource allocation problems in a fully decentralized manner. However, its performance and applicability strongly depends on some, until now, undocumented assumptions. Two of the main congestion benchmark problems considered in the literature are: the Beach Problem Domain and the Traffic Lane Domain. In both settings the highest system utility is achieved when overcrowding one resource and keeping the rest at optimum capacity. We analyse how abstract grouping can promote this behaviour and how feasible it is to apply this approach in a real-world domain (i.e., what assumptions need to be satisfied and what knowledge is necessary). We introduce a new test problem, the Road Network Domain (RND), where the resources are no longer independent, but rather part of a network (e.g., road network), thus choosing one path will also impact the load on other paths having common road segments. We demonstrate the application of state-of-the-art MARL methods for this new congestion model and analyse their performance. RND allows us to highlight an important limitation of resource abstraction and show that the difference rewards approach manages to better capture and inform the agents about the dynamics of the environment.",congest problem omnipres today complex network repres challeng mani research domain context multi agent reinforc learn marl approach like differ reward resourc abstract shown promis result tackl problem resourc abstract shown ideal candid solv larg scale resourc alloc problem fulli decentr manner howev perform applic strong depend undocu assumpt two main congest benchmark problem consid literatur beach problem domain traffic lane domain set highest system util achiev overcrowd one resourc keep rest optimum capac analys abstract group promot behaviour feasibl appli approach real world domain assumpt need satisfi knowledg necessari introduc new test problem road network domain rnd resourc longer independ rather part network road network thus choos one path also impact load path common road segment demonstr applic state art marl method new congest model analys perform rnd allow us highlight import limit resourc abstract show differ reward approach manag better captur inform agent dynam environ,"['Roxana Rădulescu', 'Peter Vrancx', 'Ann Nowé']","['cs.MA', 'cs.AI', '68T05', 'I.2.11']",False,False,False,False,False,True
730,2017-03-28T14:10:21Z,2017-02-27T21:03:26Z,http://arxiv.org/abs/1702.08529v1,http://arxiv.org/pdf/1702.08529v1,Multi-agent systems and decentralized artificial superintelligence,multi agent system decentr artifici superintellig,"Multi-agents systems communication is a technology, which provides a way for multiple interacting intelligent agents to communicate with each other and with environment. Multiple-agent systems are used to solve problems that are difficult for solving by individual agent. Multiple-agent communication technologies can be used for management and organization of computing fog and act as a global, distributed operating system. In present publication we suggest technology, which combines decentralized P2P BOINC general-purpose computing tasks distribution, multiple-agents communication protocol and smart-contract based rewards, powered by Ethereum blockchain. Such system can be used as distributed P2P computing power market, protected from any central authority. Such decentralized market can further be updated to system, which learns the most efficient way for software-hardware combinations usage and optimization. Once system learns to optimize software-hardware efficiency it can be updated to general-purpose distributed intelligence, which acts as combination of single-purpose AI.",multi agent system communic technolog provid way multipl interact intellig agent communic environ multipl agent system use solv problem difficult solv individu agent multipl agent communic technolog use manag organ comput fog act global distribut oper system present public suggest technolog combin decentr pp boinc general purpos comput task distribut multipl agent communic protocol smart contract base reward power ethereum blockchain system use distribut pp comput power market protect ani central author decentr market updat system learn effici way softwar hardwar combin usag optim onc system learn optim softwar hardwar effici updat general purpos distribut intellig act combin singl purpos ai,"['S. Ponomarev', 'A. E. Voronkov']",['cs.MA'],False,False,True,False,False,True
731,2017-03-28T14:10:21Z,2017-02-26T03:24:31Z,http://arxiv.org/abs/1702.07984v1,http://arxiv.org/abs/1702.07984v1,Collaborative Optimization for Collective Decision-making in Continuous   Spaces,collabor optim collect decis make continu space,"Many societal decision problems lie in high-dimensional continuous spaces not amenable to the voting techniques common for their discrete or single-dimensional counterparts. These problems are typically discretized before running an election or decided upon through negotiation by representatives. We propose a meta-algorithm called \emph{Iterative Local Voting} for collective decision-making in this setting, in which voters are sequentially sampled and asked to modify a candidate solution within some local neighborhood of its current value, as defined by a ball in some chosen norm. In general, such schemes do not converge, or, when they do, the resulting solution does not have a natural description.   We first prove the convergence of this algorithm under appropriate choices of neighborhoods to plausible solutions in certain natural settings: when the voters' utilities can be expressed in terms of some form of distance from their ideal solution, and when these utilities are additively decomposable across dimensions. In many of these cases, we obtain convergence to the societal welfare maximizing solution.   We then describe an experiment in which we test our algorithm for the decision of the U.S. Federal Budget on Mechanical Turk with over 4,000 workers, employing neighborhoods defined by $\mathcal{L}^1, \mathcal{L}^2$ and $\mathcal{L}^\infty$ balls. We make several observations that inform future implementations of such a procedure.",mani societ decis problem lie high dimension continu space amen vote techniqu common discret singl dimension counterpart problem typic discret befor run elect decid upon negoti repres propos meta algorithm call emph iter local vote collect decis make set voter sequenti sampl ask modifi candid solut within local neighborhood current valu defin ball chosen norm general scheme converg result solut doe natur descript first prove converg algorithm appropri choic neighborhood plausibl solut certain natur set voter util express term form distanc ideal solut util addit decompos across dimens mani case obtain converg societ welfar maxim solut describ experi test algorithm decis feder budget mechan turk worker employ neighborhood defin mathcal mathcal mathcal infti ball make sever observ inform futur implement procedur,"['Nikhil Garg', 'Vijay Kamble', 'Ashish Goel', 'David Marn', 'Kamesh Munagala']","['cs.MA', 'cs.CY', 'cs.GT']",False,False,False,False,False,True
732,2017-03-28T14:10:21Z,2017-02-25T18:10:37Z,http://arxiv.org/abs/1702.07934v1,http://arxiv.org/pdf/1702.07934v1,A decentralized algorithm for control of autonomous agents coupled by   feasibility constraints,decentr algorithm control autonom agent coupl feasibl constraint,"In this paper a decentralized control algorithm for systems composed of $N$ dynamically decoupled agents, coupled by feasibility constraints, is presented. The control problem is divided into $N$ optimal control sub-problems and a communication scheme is proposed to decouple computations. The derivative of the solution of each sub-problem is used to approximate the evolution of the system allowing the algorithm to decentralize and parallelize computations. The effectiveness of the proposed algorithm is shown through simulations in a cooperative driving scenario.",paper decentr control algorithm system compos dynam decoupl agent coupl feasibl constraint present control problem divid optim control sub problem communic scheme propos decoupl comput deriv solut sub problem use approxim evolut system allow algorithm decentr parallel comput effect propos algorithm shown simul cooper drive scenario,"['Ugo Rosolia', 'Francesco Braghin', 'Andrew G. Alleyne', 'Stijn De Bruyne', 'Edoardo Sabbioni']","['cs.MA', 'cs.SY']",False,False,False,False,False,True
733,2017-03-28T14:10:21Z,2017-02-24T11:39:00Z,http://arxiv.org/abs/1702.07544v1,http://arxiv.org/pdf/1702.07544v1,Scalable Multiagent Coordination with Distributed Online Open Loop   Planning,scalabl multiag coordin distribut onlin open loop plan,"We propose distributed online open loop planning (DOOLP), a general framework for online multiagent coordination and decision making under uncertainty. DOOLP is based on online heuristic search in the space defined by a generative model of the domain dynamics, which is exploited by agents to simulate and evaluate the consequences of their potential choices.   We also propose distributed online Thompson sampling (DOTS) as an effective instantiation of the DOOLP framework. DOTS models sequences of agent choices by concatenating a number of multiarmed bandits for each agent and uses Thompson sampling for dealing with action value uncertainty. The Bayesian approach underlying Thompson sampling allows to effectively model and estimate uncertainty about (a) own action values and (b) other agents' behavior. This approach yields a principled and statistically sound solution to the exploration-exploitation dilemma when exploring large search spaces with limited resources.   We implemented DOTS in a smart factory case study with positive empirical results. We observed effective, robust and scalable planning and coordination capabilities even when only searching a fraction of the potential search space.",propos distribut onlin open loop plan doolp general framework onlin multiag coordin decis make uncertainti doolp base onlin heurist search space defin generat model domain dynam exploit agent simul evalu consequ potenti choic also propos distribut onlin thompson sampl dot effect instanti doolp framework dot model sequenc agent choic concaten number multiarm bandit agent use thompson sampl deal action valu uncertainti bayesian approach thompson sampl allow effect model estim uncertainti action valu agent behavior approach yield principl statist sound solut explor exploit dilemma explor larg search space limit resourc implement dot smart factori case studi posit empir result observ effect robust scalabl plan coordin capabl even onli search fraction potenti search space,"['Lenz Belzner', 'Thomas Gabor']","['cs.MA', 'cs.SY']",False,False,False,False,False,True
735,2017-03-28T14:10:21Z,2017-02-21T00:18:14Z,http://arxiv.org/abs/1702.06219v1,http://arxiv.org/pdf/1702.06219v1,An Online Optimization Approach for Multi-Agent Tracking of Dynamic   Parameters in the Presence of Adversarial Noise,onlin optim approach multi agent track dynam paramet presenc adversari nois,"This paper addresses tracking of a moving target in a multi-agent network. The target follows a linear dynamics corrupted by an adversarial noise, i.e., the noise is not generated from a statistical distribution. The location of the target at each time induces a global time-varying loss function, and the global loss is a sum of local losses, each of which is associated to one agent. Agents noisy observations could be nonlinear. We formulate this problem as a distributed online optimization where agents communicate with each other to track the minimizer of the global loss. We then propose a decentralized version of the Mirror Descent algorithm and provide the non-asymptotic analysis of the problem. Using the notion of dynamic regret, we measure the performance of our algorithm versus its offline counterpart in the centralized setting. We prove that the bound on dynamic regret scales inversely in the network spectral gap, and it represents the adversarial noise causing deviation with respect to the linear dynamics. Our result subsumes a number of results in the distributed optimization literature. Finally, in a numerical experiment, we verify that our algorithm can be simply implemented for multi-agent tracking with nonlinear observations.",paper address track move target multi agent network target follow linear dynam corrupt adversari nois nois generat statist distribut locat target time induc global time vari loss function global loss sum local loss associ one agent agent noisi observ could nonlinear formul problem distribut onlin optim agent communic track minim global loss propos decentr version mirror descent algorithm provid non asymptot analysi problem use notion dynam regret measur perform algorithm versus offlin counterpart central set prove bound dynam regret scale invers network spectral gap repres adversari nois caus deviat respect linear dynam result subsum number result distribut optim literatur final numer experi verifi algorithm simpli implement multi agent track nonlinear observ,"['Shahin Shahrampour', 'Ali Jadbabaie']","['math.OC', 'cs.MA', 'stat.ML']",False,False,False,False,False,True
736,2017-03-28T14:10:21Z,2017-02-19T11:06:28Z,http://arxiv.org/abs/1702.05739v1,http://arxiv.org/pdf/1702.05739v1,Social learning in a simple task allocation game,social learn simpl task alloc game,"We investigate the effects of social interactions in task al- location using Evolutionary Game Theory (EGT). We propose a simple task-allocation game and study how different learning mechanisms can give rise to specialised and non- specialised colonies under different ecological conditions. By combining agent-based simulations and adaptive dynamics we show that social learning can result in colonies of generalists or specialists, depending on ecological parameters. Agent-based simulations further show that learning dynamics play a crucial role in task allocation. In particular, introspective individual learning readily favours the emergence of specialists, while a process resembling task recruitment favours the emergence of generalists.",investig effect social interact task al locat use evolutionari game theori egt propos simpl task alloc game studi differ learn mechan give rise specialis non specialis coloni differ ecolog condit combin agent base simul adapt dynam show social learn result coloni generalist specialist depend ecolog paramet agent base simul show learn dynam play crucial role task alloc particular introspect individu learn readili favour emerg specialist process resembl task recruit favour emerg generalist,"['Rui Chen', 'Garcia Julian', 'Meyer Bernd']","['q-bio.PE', 'cs.MA']",False,False,False,False,False,True
739,2017-03-28T14:10:21Z,2017-02-15T18:09:18Z,http://arxiv.org/abs/1702.04700v1,http://arxiv.org/pdf/1702.04700v1,Target assignment for robots constrained by limited communication range,target assign robot constrain limit communic rang,"This paper investigates the task assignment problem for multiple dispersed robots constrained by limited communication range. The robots are initially randomly distributed and need to visit several target locations while minimizing the total travel time. A centralized rendezvous-based algorithm is proposed, under which all the robots first move towards a rendezvous position until communication paths are established between every pair of robots either directly or through intermediate peers, and then one robot is chosen as the leader to make a centralized task assignment for the other robots. Furthermore, we propose a decentralized algorithm based on a single-traveling-salesman tour, which does not require all the robots to be connected through communication. We investigate the variation of the quality of the assignment solutions as the level of information sharing increases and as the communication range grows, respectively. The proposed algorithms are compared with a centralized algorithm with shared global information and a decentralized greedy algorithm respectively. Monte Carlo simulation results show the satisfying performance of the proposed algorithms.",paper investig task assign problem multipl dispers robot constrain limit communic rang robot initi random distribut need visit sever target locat minim total travel time central rendezv base algorithm propos robot first move toward rendezv posit communic path establish everi pair robot either direct intermedi peer one robot chosen leader make central task assign robot furthermor propos decentr algorithm base singl travel salesman tour doe requir robot connect communic investig variat qualiti assign solut level inform share increas communic rang grow respect propos algorithm compar central algorithm share global inform decentr greedi algorithm respect mont carlo simul result show satisfi perform propos algorithm,"['Xiaoshan Bai', 'Weisheng Yan', 'Ming Cao', 'Jie Huang']","['math.OC', 'cs.MA', 'cs.RO', 'math.PR']",False,False,False,False,False,True
741,2017-03-28T14:10:25Z,2017-02-13T02:50:55Z,http://arxiv.org/abs/1702.03614v1,http://arxiv.org/pdf/1702.03614v1,Multitask diffusion adaptation over networks with common latent   representations,multitask diffus adapt network common latent represent,"Online learning with streaming data in a distributed and collaborative manner can be useful in a wide range of applications. This topic has been receiving considerable attention in recent years with emphasis on both single-task and multitask scenarios. In single-task adaptation, agents cooperate to track an objective of common interest, while in multitask adaptation agents track multiple objectives simultaneously. Regularization is one useful technique to promote and exploit similarity among tasks in the latter scenario. This work examines an alternative way to model relations among tasks by assuming that they all share a common latent feature representation. As a result, a new multitask learning formulation is presented and algorithms are developed for its solution in a distributed online manner. We present a unified framework to analyze the mean-square-error performance of the adaptive strategies, and conduct simulations to illustrate the theoretical findings and potential applications.",onlin learn stream data distribut collabor manner use wide rang applic topic receiv consider attent recent year emphasi singl task multitask scenario singl task adapt agent cooper track object common interest multitask adapt agent track multipl object simultan regular one use techniqu promot exploit similar among task latter scenario work examin altern way model relat among task assum share common latent featur represent result new multitask learn formul present algorithm develop solut distribut onlin manner present unifi framework analyz mean squar error perform adapt strategi conduct simul illustr theoret find potenti applic,"['Jie Chen', 'Cédric Richard', 'Ali H. Sayed']","['cs.MA', 'stat.ML']",False,False,True,False,False,True
743,2017-03-28T14:10:25Z,2017-02-11T22:56:22Z,http://arxiv.org/abs/1702.03466v1,http://arxiv.org/pdf/1702.03466v1,Safe Open-Loop Strategies for Handling Intermittent Communications in   Multi-Robot Systems,safe open loop strategi handl intermitt communic multi robot system,"In multi-robot systems where a central decision maker is specifying the movement of each individual robot, a communication failure can severely impair the performance of the system. This paper develops a motion strategy that allows robots to safely handle critical communication failures for such multi-robot architectures. For each robot, the proposed algorithm computes a time horizon over which collisions with other robots are guaranteed not to occur. These safe time horizons are included in the commands being transmitted to the individual robots. In the event of a communication failure, the robots execute the last received velocity commands for the corresponding safe time horizons leading to a provably safe open-loop motion strategy. The resulting algorithm is computationally effective and is agnostic to the task that the robots are performing. The efficacy of the strategy is verified in simulation as well as on a team of differential-drive mobile robots.",multi robot system central decis maker specifi movement individu robot communic failur sever impair perform system paper develop motion strategi allow robot safe handl critic communic failur multi robot architectur robot propos algorithm comput time horizon collis robot guarante occur safe time horizon includ command transmit individu robot event communic failur robot execut last receiv veloc command correspond safe time horizon lead provabl safe open loop motion strategi result algorithm comput effect agnost task robot perform efficaci strategi verifi simul well team differenti drive mobil robot,"['Siddharth Mayya', 'Magnus Egerstedt']","['cs.MA', 'cs.RO']",False,False,False,False,False,True
744,2017-03-28T14:10:25Z,2017-02-22T12:34:24Z,http://arxiv.org/abs/1702.03400v2,http://arxiv.org/pdf/1702.03400v2,"Gathering Anonymous, Oblivious Robots on a Grid",gather anonym oblivi robot grid,"We consider a swarm of $n$ autonomous mobile robots, distributed on a 2-dimensional grid. A basic task for such a swarm is the gathering process: all robots have to gather at one (not predefined) place. The work in this paper is motivated by the following insight: On one side, for swarms of robots distributed in the 2-dimensional Euclidean space, several gathering algorithms are known for extremely simple robots that are oblivious, have bounded viewing radius, no compass, and no ""flags"" to communicate a status to others. On the other side, in case of the 2-dimensional grid, the only known gathering algorithms for robots with bounded viewing radius without compass, need to memorize a constant number of rounds and need flags.   In this paper we contribute the, to the best of our knowledge, first gathering algorithm on the grid, which works for anonymous, oblivious robots with bounded viewing range, without any further means of communication and without any memory. We prove its correctness and an $O(n^2)$ time bound. This time bound matches those of the best known algorithms for the Euclidean plane mentioned above.",consid swarm autonom mobil robot distribut dimension grid basic task swarm gather process robot gather one predefin place work paper motiv follow insight one side swarm robot distribut dimension euclidean space sever gather algorithm known extrem simpl robot oblivi bound view radius compass flag communic status side case dimension grid onli known gather algorithm robot bound view radius without compass need memor constant number round need flag paper contribut best knowledg first gather algorithm grid work anonym oblivi robot bound view rang without ani mean communic without ani memori prove correct time bound time bound match best known algorithm euclidean plane mention abov,"['Matthias Fischer', 'Daniel Jung', 'Friedhelm Meyer auf der Heide']","['cs.DC', 'cs.MA', 'cs.RO']",False,False,False,False,False,True
746,2017-03-28T14:10:25Z,2017-02-10T01:48:40Z,http://arxiv.org/abs/1702.03037v1,http://arxiv.org/pdf/1702.03037v1,Multi-agent Reinforcement Learning in Sequential Social Dilemmas,multi agent reinforc learn sequenti social dilemma,"Matrix games like Prisoner's Dilemma have guided research on social dilemmas for decades. However, they necessarily treat the choice to cooperate or defect as an atomic action. In real-world social dilemmas these choices are temporally extended. Cooperativeness is a property that applies to policies, not elementary actions. We introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. We analyze the dynamics of policies learned by multiple self-interested independent learning agents, each using its own deep Q-network, on two Markov games we introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We characterize how learned behavior in each domain changes as a function of environmental factors including resource abundance. Our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation.",matrix game like prison dilemma guid research social dilemma decad howev necessarili treat choic cooper defect atom action real world social dilemma choic tempor extend cooper properti appli polici elementari action introduc sequenti social dilemma share mix incent structur matrix game social dilemma also requir agent learn polici implement strateg intent analyz dynam polici learn multipl self interest independ learn agent use deep network two markov game introduc fruit gather game wolfpack hunt game character learn behavior domain chang function environment factor includ resourc abund experi show conflict emerg competit share resourc shed light sequenti natur real world social dilemma affect cooper,"['Joel Z. Leibo', 'Vinicius Zambaldi', 'Marc Lanctot', 'Janusz Marecki', 'Thore Graepel']","['cs.MA', 'cs.AI', 'cs.GT', 'cs.LG']",False,False,False,False,False,True
747,2017-03-28T14:10:25Z,2017-02-09T01:35:31Z,http://arxiv.org/abs/1702.04299v1,http://arxiv.org/pdf/1702.04299v1,Cyclic Dominance in the Spatial Coevolutionary Optional Prisoner's   Dilemma Game,cyclic domin spatial coevolutionari option prison dilemma game,"This paper studies scenarios of cyclic dominance in a coevolutionary spatial model in which game strategies and links between agents adaptively evolve over time. The Optional Prisoner's Dilemma (OPD) game is employed. The OPD is an extended version of the traditional Prisoner's Dilemma where players have a third option to abstain from playing the game. We adopt an agent-based simulation approach and use Monte Carlo methods to perform the OPD with coevolutionary rules. The necessary conditions to break the scenarios of cyclic dominance are also investigated. This work highlights that cyclic dominance is essential in the sustenance of biodiversity. Moreover, we also discuss the importance of a spatial coevolutionary model in maintaining cyclic dominance in adverse conditions.",paper studi scenario cyclic domin coevolutionari spatial model game strategi link agent adapt evolv time option prison dilemma opd game employ opd extend version tradit prison dilemma player third option abstain play game adopt agent base simul approach use mont carlo method perform opd coevolutionari rule necessari condit break scenario cyclic domin also investig work highlight cyclic domin essenti susten biodivers moreov also discuss import spatial coevolutionari model maintain cyclic domin advers condit,"['Marcos Cardinot', 'Josephine Griffith', ""Colm O'Riordan""]","['cs.GT', 'cs.MA', 'math.DS', 'physics.soc-ph']",False,False,True,False,False,True
748,2017-03-28T14:10:25Z,2017-02-08T19:50:39Z,http://arxiv.org/abs/1702.02597v1,http://arxiv.org/pdf/1702.02597v1,Structurally Observable Distributed Networks of Agents under Cost and   Robustness Constraints,structur observ distribut network agent cost robust constraint,"In many problems, agents cooperate locally so that a leader or fusion center can infer the state of every agent from probing the state of only a small number of agents. Versions of this problem arise when a fusion center reconstructs an extended physical field by accessing the state of just a few of the sensors measuring the field, or a leader monitors the formation of a team of robots. Given a link cost, the paper presents a polynomial time algorithm to design a minimum cost coordinated network dynamics followed by the agents, under an observability constraint. The problem is placed in the context of structural observability and solved even when up to k agents in the coordinated network dynamics fail.",mani problem agent cooper local leader fusion center infer state everi agent probe state onli small number agent version problem aris fusion center reconstruct extend physic field access state sensor measur field leader monitor format team robot given link cost paper present polynomi time algorithm design minimum cost coordin network dynam follow agent observ constraint problem place context structur observ solv even agent coordin network dynam fail,"['Stephen Kruzick', 'Sérgio Pequito', 'Soummya Kar', 'José M. F. Moura', 'A. Pedro Aguiar']",['cs.MA'],False,False,False,False,False,True
749,2017-03-28T14:10:25Z,2017-02-08T17:49:31Z,http://arxiv.org/abs/1702.02541v1,http://arxiv.org/pdf/1702.02541v1,Modelling community formation driven by the status of individual in a   society,model communiti format driven status individu societi,"In human societies, people's willingness to compete and strive for better social status as well as being envious of those perceived in some way superior lead to social structures that are intrinsically hierarchical. Here we propose an agent-based, network model to mimic the ranking behaviour of individuals and its possible repercussions in human society. The main ingredient of the model is the assumption that the relevant feature of social interactions is each individual's keenness to maximise his or her status relative to others. The social networks produced by the model are homophilous and assortative, as frequently observed in human communities and most of the network properties seem quite independent of its size. However, it is seen that for small number of agents the resulting network consists of disjoint weakly connected communities while being highly assortative and homophilic. On the other hand larger networks turn out to be more cohesive with larger communities but less homophilic. We find that the reason for these changes is that larger network size allows agents to use new strategies for maximizing their social status allowing for more diverse links between them.",human societi peopl willing compet strive better social status well envious perceiv way superior lead social structur intrins hierarch propos agent base network model mimic rank behaviour individu possibl repercuss human societi main ingredi model assumpt relev featur social interact individu keen maximis status relat social network produc model homophil assort frequent observ human communiti network properti seem quit independ size howev seen small number agent result network consist disjoint weak connect communiti high assort homophil hand larger network turn cohes larger communiti less homophil find reason chang larger network size allow agent use new strategi maxim social status allow divers link,"['Jan E. Snellman', 'Gerardo Iñiguez', 'Tzipe Govezensky', 'Rafael A. Barrio', 'Kimmo K. Kaski']","['cs.MA', 'cs.SI', 'physics.soc-ph', 'I.2.11']",False,False,True,False,False,True
751,2017-03-28T14:10:29Z,2017-02-01T14:45:48Z,http://arxiv.org/abs/1702.00290v1,http://arxiv.org/pdf/1702.00290v1,Attacking the V: On the Resiliency of Adaptive-Horizon MPC,attack resili adapt horizon mpc,"We introduce the concept of a V-formation game between a controller and an attacker, where controller's goal is to maneuver the plant (a simple model of flocking dynamics) into a V-formation, and the goal of the attacker is to prevent the controller from doing so. Controllers in V-formation games utilize a new formulation of model-predictive control we call Adaptive-Horizon MPC (AMPC), giving them extraordinary power: we prove that under certain controllability assumptions, an AMPC controller is able to attain V-formation with probability 1.   We define several classes of attackers, including those that in one move can remove R birds from the flock, or introduce random displacement into flock dynamics. We consider both naive attackers, whose strategies are purely probabilistic, and AMPC-enabled attackers, putting them on par strategically with the controllers. While an AMPC-enabled controller is expected to win every game with probability 1, in practice, it is resource-constrained: its maximum prediction horizon and the maximum number of game execution steps are fixed. Under these conditions, an attacker has a much better chance of winning a V-formation game.   Our extensive performance evaluation of V-formation games uses statistical model checking to estimate the probability an attacker can thwart the controller. Our results show that for the bird-removal game with R = 1, the controller almost always wins (restores the flock to a V-formation). For R = 2, the game outcome critically depends on which two birds are removed. For the displacement game, our results again demonstrate that an intelligent attacker, i.e. one that uses AMPC in this case, significantly outperforms its naive counterpart that randomly executes its attack.",introduc concept format game control attack control goal maneuv plant simpl model flock dynam format goal attack prevent control control format game util new formul model predict control call adapt horizon mpc ampc give extraordinari power prove certain control assumpt ampc control abl attain format probabl defin sever class attack includ one move remov bird flock introduc random displac flock dynam consid naiv attack whose strategi pure probabilist ampc enabl attack put par strateg control ampc enabl control expect win everi game probabl practic resourc constrain maximum predict horizon maximum number game execut step fix condit attack much better chanc win format game extens perform evalu format game use statist model check estim probabl attack thwart control result show bird remov game control almost alway win restor flock format game outcom critic depend two bird remov displac game result demonstr intellig attack one use ampc case signific outperform naiv counterpart random execut attack,"['Scott A. Smolka', 'Ashish Tiwari', 'Lukas Esterle', 'Anna Lukina', 'Junxing Yang', 'Radu Grosu']","['cs.SY', 'cs.MA']",False,False,True,False,False,True
752,2017-03-28T14:10:29Z,2017-01-31T16:16:41Z,http://arxiv.org/abs/1701.09112v1,http://arxiv.org/pdf/1701.09112v1,Socio-Affective Agents as Models of Human Behaviour in the Networked   Prisoner's Dilemma,socio affect agent model human behaviour network prison dilemma,"Affect Control Theory (ACT) is a powerful and general sociological model of human affective interaction. ACT provides an empirically derived mathematical model of culturally shared sentiments as heuristic guides for human decision making. BayesACT, a variant on classical ACT, combines affective reasoning with cognitive (denotative or logical) reasoning as is traditionally found in AI. Bayes\-ACT allows for the creation of agents that are both emotionally guided and goal-directed. In this work, we simulate BayesACT agents in the Iterated Networked Prisoner's Dilemma (INPD), and we show four out of five known properties of human play in INPD are replicated by these socio-affective agents. In particular, we show how the observed human behaviours of network structure invariance, anti-correlation of cooperation and reward, and player type stratification are all clearly emergent properties of the networked BayesACT agents. We further show that decision hyteresis (Moody Conditional Cooperation) is replicated by BayesACT agents in over $2/3$ of the cases we have considered. In contrast, previously used imitation-based agents are only able to replicate one of the five properties. We discuss the implications of these findings in the development of human-agent societies.",affect control theori act power general sociolog model human affect interact act provid empir deriv mathemat model cultur share sentiment heurist guid human decis make bayesact variant classic act combin affect reason cognit denot logic reason tradit found ai bay act allow creation agent emot guid goal direct work simul bayesact agent iter network prison dilemma inpd show four five known properti human play inpd replic socio affect agent particular show observ human behaviour network structur invari anti correl cooper reward player type stratif clear emerg properti network bayesact agent show decis hyteresi moodi condit cooper replic bayesact agent case consid contrast previous use imit base agent onli abl replic one five properti discuss implic find develop human agent societi,"['Joshua D. A. Jung', 'Jesse Hoey']",['cs.MA'],False,False,True,False,False,True
753,2017-03-28T14:10:29Z,2017-01-27T17:35:56Z,http://arxiv.org/abs/1701.08125v1,http://arxiv.org/pdf/1701.08125v1,Organic Computing in the Spotlight,organ comput spotlight,"Organic Computing is an initiative in the field of systems engineering that proposed to make use of concepts such as self-adaptation and self-organisation to increase the robustness of technical systems. Based on the observation that traditional design and operation concepts reach their limits, transferring more autonomy to the systems themselves should result in a reduction of complexity for users, administrators, and developers. However, there seems to be a need for an updated definition of the term ""Organic Computing"", of desired properties of technical, organic systems, and the objectives of the Organic Computing initiative. With this article, we will address these points.",organ comput initi field system engin propos make use concept self adapt self organis increas robust technic system base observ tradit design oper concept reach limit transfer autonomi system themselv result reduct complex user administr develop howev seem need updat definit term organ comput desir properti technic organ system object organ comput initi articl address point,"['Sven Tomforde', 'Bernhard Sick', 'Christian Müller-Schloer']","['cs.MA', 'cs.AI', '68T05', 'I.2.8, I.2.11']",False,False,True,False,False,True
758,2017-03-28T14:10:29Z,2017-02-23T11:26:23Z,http://arxiv.org/abs/1701.07248v2,http://arxiv.org/abs/1701.07248v2,Distributed methods for synchronization of orthogonal matrices over   graphs,distribut method synchron orthogon matric graph,"This paper addresses the problem of synchronizing orthogonal matrices over directed graphs. For synchronized transformations (or matrices), composite transformations over loops equal the identity. We formulate the synchronization problem as a least-squares optimization problem with nonlinear constraints. The synchronization problem appears as one of the key components in applications ranging from 3D-localization to image registration. The main contributions of this work can be summarized as the introduction of two novel algorithms; one for symmetric graphs and one for graphs that are possibly asymmetric. Under general conditions, the former has guaranteed convergence to the solution of a spectral relaxation to the synchronization problem. The latter is stable for small step sizes when the graph is quasi-strongly connected. The proposed methods are verified in numerical simulations.",paper address problem synchron orthogon matric direct graph synchron transform matric composit transform loop equal ident formul synchron problem least squar optim problem nonlinear constraint synchron problem appear one key compon applic rang local imag registr main contribut work summar introduct two novel algorithm one symmetr graph one graph possibl asymmetr general condit former guarante converg solut spectral relax synchron problem latter stabl small step size graph quasi strong connect propos method verifi numer simul,"['Johan Thunberg', 'Florian Bernard', 'Jorge Goncalves']","['math.OC', 'cs.CV', 'cs.DC', 'cs.MA']",False,False,False,False,False,True
760,2017-03-28T14:10:34Z,2017-01-20T18:24:43Z,http://arxiv.org/abs/1701.05880v1,http://arxiv.org/pdf/1701.05880v1,Separable and Localized System Level Synthesis for Large-Scale Systems,separ local system level synthesi larg scale system,"A major challenge faced in the design of large-scale cyber-physical systems, such as power systems, the Internet of Things or intelligent transportation systems, is that traditional distributed optimal control methods do not scale gracefully, neither in controller synthesis nor in controller implementation, to systems composed of millions, billions or even trillions of interacting subsystems. This paper shows that this challenge can now be addressed by leveraging the recently introduced System Level Approach (SLA) to controller synthesis. In particular, in the context of the SLA, we define suitable notions of separability for control objective functions and system constraints such that the global optimization problem (or iterate update problems of a distributed optimization algorithm) can be decomposed into parallel subproblems. We then further show that if additional locality (i.e., sparsity) constraints are imposed, then these subproblems can be solved using local models and local decision variables. The SLA is essential to maintaining the convexity of the aforementioned problems under locality constraints. As a consequence, the resulting synthesis methods have $O(1)$ complexity relative to the size of the global system. We further show that many optimal control problems of interest, such as (localized) LQR and LQG, $\mathcal{H}_2$ optimal control with joint actuator and sensor regularization, and (localized) mixed $\mathcal{H}_2/\mathcal{L}_1$ optimal control problems, satisfy these notions of separability, and use these problems to explore tradeoffs in performance, actuator and sensing density, and average versus worst-case performance for a large-scale power inspired system.",major challeng face design larg scale cyber physic system power system internet thing intellig transport system tradit distribut optim control method scale grace neither control synthesi control implement system compos million billion even trillion interact subsystem paper show challeng address leverag recent introduc system level approach sla control synthesi particular context sla defin suitabl notion separ control object function system constraint global optim problem iter updat problem distribut optim algorithm decompos parallel subproblem show addit local sparsiti constraint impos subproblem solv use local model local decis variabl sla essenti maintain convex aforement problem local constraint consequ result synthesi method complex relat size global system show mani optim control problem interest local lqr lqg mathcal optim control joint actuat sensor regular local mix mathcal mathcal optim control problem satisfi notion separ use problem explor tradeoff perform actuat sens densiti averag versus worst case perform larg scale power inspir system,"['Yuh-Shyang Wang', 'Nikolai Matni', 'John C. Doyle']","['math.OC', 'cs.MA']",False,False,False,False,False,True
761,2017-03-28T14:10:34Z,2017-01-18T15:36:46Z,http://arxiv.org/abs/1701.05108v1,http://arxiv.org/pdf/1701.05108v1,On the Computational Complexity of Variants of Combinatorial Voter   Control in Elections,comput complex variant combinatori voter control elect,"Voter control problems model situations in which an external agent tries toaffect the result of an election by adding or deleting the fewest number of voters. The goal of the agent is to make a specific candidate either win (\emph{constructive} control) or lose (\emph{destructive} control) the election. We study the constructive and destructive voter control problems whenadding and deleting voters have a \emph{combinatorial flavor}: If we add (resp.\ delete) a voter~$v$, we also add (resp.\ delete) a bundle~$\kappa(v) $ of voters that are associated with~$v$. While the bundle~$\kappa(v)$ may have more than one voter, a voter may also be associated with more than one voter. We analyze the computational complexity of the four voter control problems for the Plurality rule. We obtain that, in general, making a candidate lose is computationally easier than making her win. In particular, if the bundling relation is symmetric (i.e.\ $\forall w\colon w \in \kappa(v) \Leftrightarrow v \in \kappa(w) $), and if each voter has at most two voters associated with him, then destructive control is polynomial-time solvable while the constructive variant remains $\NP$-hard. Even if the bundles are disjoint (i.e.\ $\forall w\colon w \in \kappa(v) \Leftrightarrow \kappa(v) = \kappa(w) $), the constructive problem variants remain intractable. Finally, the minimization variant of constructive control by adding voters does not admit an efficient approximation algorithm, unless P=NP.",voter control problem model situat extern agent tri toaffect result elect ad delet fewest number voter goal agent make specif candid either win emph construct control lose emph destruct control elect studi construct destruct voter control problem whenad delet voter emph combinatori flavor add resp delet voter also add resp delet bundl kappa voter associ bundl kappa may one voter voter may also associ one voter analyz comput complex four voter control problem plural rule obtain general make candid lose comput easier make win particular bundl relat symmetr foral colon kappa leftrightarrow kappa voter two voter associ destruct control polynomi time solvabl construct variant remain np hard even bundl disjoint foral colon kappa leftrightarrow kappa kappa construct problem variant remain intract final minim variant construct control ad voter doe admit effici approxim algorithm unless np,"['Leon Kellerhals', 'Viatcheslav Korenwein', 'Philipp Zschoche', 'Robert Bredereck', 'Jiehua Chen']",['cs.MA'],False,False,False,False,False,True
762,2017-03-28T14:10:34Z,2017-01-12T21:22:39Z,http://arxiv.org/abs/1701.03508v1,http://arxiv.org/pdf/1701.03508v1,Coordination of multi-agent systems via asynchronous cloud communication,coordin multi agent system via asynchron cloud communic,"In this work we study a multi-agent coordination problem in which agents are only able to communicate with each other intermittently through a cloud server. To reduce the amount of required communication, we develop a self-triggered algorithm that allows agents to communicate with the cloud only when necessary rather than at some fixed period. Unlike the vast majority of similar works that propose distributed event- and/or self-triggered control laws, this work doesn't assume agents can be ""listening"" continuously. In other words, when an event is triggered by one agent, neighboring agents will not be aware of this until the next time they establish communication with the cloud themselves. Using a notion of ""promises"" about future control inputs, agents are able to keep track of higher quality estimates about their neighbors allowing them to stay disconnected from the cloud for longer periods of time while still guaranteeing a positive contribution to the global task. We show that our self-triggered coordination algorithm guarantees that the system asymptotically reaches the set of desired states. Simulations illustrate our results.",work studi multi agent coordin problem agent onli abl communic intermitt cloud server reduc amount requir communic develop self trigger algorithm allow agent communic cloud onli necessari rather fix period unlik vast major similar work propos distribut event self trigger control law work assum agent listen continu word event trigger one agent neighbor agent awar next time establish communic cloud themselv use notion promis futur control input agent abl keep track higher qualiti estim neighbor allow stay disconnect cloud longer period time still guarante posit contribut global task show self trigger coordin algorithm guarante system asymptot reach set desir state simul illustr result,"['Sean L. Bowman', 'Cameron Nowzari', 'George J. Pappas']","['math.OC', 'cs.MA']",False,False,False,False,False,True
763,2017-03-28T14:10:34Z,2017-01-05T12:02:34Z,http://arxiv.org/abs/1701.01289v1,http://arxiv.org/pdf/1701.01289v1,Applying DCOP to User Association Problem in Heterogeneous Networks with   Markov Chain Based Algorithm,appli dcop user associ problem heterogen network markov chain base algorithm,"Multi-agent systems (MAS) is able to characterize the behavior of individual agent and the interaction between agents. Thus, it motivates us to leverage the distributed constraint optimization problem (DCOP), a framework of modeling MAS, to solve the user association problem in heterogeneous networks (HetNets). Two issues we have to consider when we take DCOP into the application of HetNet including: (i) How to set up an effective model by DCOP taking account of the negtive impact of the increment of users on the modeling process (ii) Which kind of algorithms is more suitable to balance the time consumption and the quality of soltuion. Aiming to overcome these issues, we firstly come up with an ECAV-$\eta$ (Each Connection As Variable) model in which a parameter $\eta$ with an adequate assignment ($\eta=3$ in this paper) is able to control the scale of the model. After that, a Markov chain (MC) based algorithm is proposed on the basis of log-sum-exp function. Experimental results show that the solution obtained by DCOP framework is better than the one obtained by the Max-SINR algorithm. Comparing with the Lagrange dual decomposition based method (LDD), the solution performance has been improved since there is no need to transform original problem into a satisfied one. In addition, it is also apparent that the DCOP based method has better robustness than LDD when the number of users increases but the available resource at base stations are limited.",multi agent system mas abl character behavior individu agent interact agent thus motiv us leverag distribut constraint optim problem dcop framework model mas solv user associ problem heterogen network hetnet two issu consid take dcop applic hetnet includ set effect model dcop take account negtiv impact increment user model process ii kind algorithm suitabl balanc time consumpt qualiti soltuion aim overcom issu first come ecav eta connect variabl model paramet eta adequ assign eta paper abl control scale model markov chain mc base algorithm propos basi log sum exp function experiment result show solut obtain dcop framework better one obtain max sinr algorithm compar lagrang dual decomposit base method ldd solut perform improv sinc need transform origin problem satisfi one addit also appar dcop base method better robust ldd number user increas avail resourc base station limit,"['Peibo Duan', 'Guoqiang Mao', 'Changsheng Zhang', 'Bin Zhang']",['cs.MA'],False,False,True,False,False,True
766,2017-03-28T14:10:34Z,2016-12-28T10:35:05Z,http://arxiv.org/abs/1612.08845v1,http://arxiv.org/pdf/1612.08845v1,"The formal-logical characterisation of lies, deception, and associated   notions",formal logic characteris lie decept associ notion,Defining various dishonest notions in a formal way is a key step to enable intelligent agents to act in untrustworthy environments. This review evaluates the literature for this topic by looking at formal definitions based on modal logic as well as other formal approaches. Criteria from philosophical groundwork is used to assess the definitions for correctness and completeness. The key contribution of this review is to show that only a few definitions fully comply with this gold standard and to point out the missing steps towards a successful application of these definitions in an actual agent environment.,defin various dishonest notion formal way key step enabl intellig agent act untrustworthi environ review evalu literatur topic look formal definit base modal logic well formal approach criteria philosoph groundwork use assess definit correct complet key contribut review show onli definit fulli compli gold standard point miss step toward success applic definit actual agent environ,['Toni Heidenreich'],"['cs.LO', 'cs.AI', 'cs.MA']",False,False,False,False,False,True
768,2017-03-28T14:10:34Z,2016-12-26T09:41:56Z,http://arxiv.org/abs/1612.08351v1,http://arxiv.org/pdf/1612.08351v1,"Network, Popularity and Social Cohesion: A Game-Theoretic Approach",network popular social cohes game theoret approach,"In studies of social dynamics, cohesion refers to a group's tendency to stay in unity, which -- as argued in sociometry -- arises from the network topology of interpersonal ties between members of the group. We follow this idea and propose a game-based model of cohesion that not only relies on the social network, but also reflects individuals' social needs. In particular, our model is a type of cooperative games where players may gain popularity by strategically forming groups. A group is socially cohesive if the grand coalition is core stable. We study social cohesion in some special types of graphs and draw a link between social cohesion and the classical notion of structural cohesion. We then focus on the problem of deciding whether a given social network is socially cohesive and show that this problem is CoNP-complete. Nevertheless, we give two efficient heuristics for coalition structures where players enjoy high popularity and experimentally evaluate their performances.",studi social dynam cohes refer group tendenc stay uniti argu sociometri aris network topolog interperson tie member group follow idea propos game base model cohes onli reli social network also reflect individu social need particular model type cooper game player may gain popular strateg form group group social cohes grand coalit core stabl studi social cohes special type graph draw link social cohes classic notion structur cohes focus problem decid whether given social network social cohes show problem conp complet nevertheless give two effici heurist coalit structur player enjoy high popular experiment evalu perform,"['Jiamou Liu', 'Ziheng Wei']","['cs.SI', 'cs.CC', 'cs.GT', 'cs.MA', '91D30, 91Cxx, 91A40, 91A12, 68T42, 68Q15', 'I.2.11; J.4; F.1.3']",False,False,True,False,False,True
769,2017-03-28T14:10:34Z,2016-12-24T03:12:16Z,http://arxiv.org/abs/1612.08126v1,http://arxiv.org/pdf/1612.08126v1,Brain-Swarm Interface (BSI): Controlling a Swarm of Robots with Brain   and Eye Signals from an EEG Headset,brain swarm interfac bsi control swarm robot brain eye signal eeg headset,"This work presents a novel marriage of Swarm Robotics and Brain Computer Interface technology to produce an interface which connects a user to a swarm of robots. The proposed interface enables the user to control the swarm's size and motion employing just thoughts and eye movements. The thoughts and eye movements are recorded as electrical signals from the scalp by an off-the-shelf Electroencephalogram (EEG) headset. Signal processing techniques are used to filter out noise and decode the user's eye movements from raw signals, while a Hidden Markov Model technique is employed to decipher the user's thoughts from filtered signals. The dynamics of the robots are controlled using a swarm controller based on potential fields. The shape and motion parameters of the potential fields are modulated by the human user through the brain-swarm interface to move the robots. The method is demonstrated experimentally with a human controlling a swarm of three M3pi robots in a laboratory environment, as well as controlling a swarm of 128 robots in a computer simulation.",work present novel marriag swarm robot brain comput interfac technolog produc interfac connect user swarm robot propos interfac enabl user control swarm size motion employ thought eye movement thought eye movement record electr signal scalp shelf electroencephalogram eeg headset signal process techniqu use filter nois decod user eye movement raw signal hidden markov model techniqu employ deciph user thought filter signal dynam robot control use swarm control base potenti field shape motion paramet potenti field modul human user brain swarm interfac move robot method demonstr experiment human control swarm three mpi robot laboratori environ well control swarm robot comput simul,"['Aamodh Suresh', 'Mac Schwager']","['cs.RO', 'cs.MA']",False,False,False,False,False,True
771,2017-03-28T14:10:38Z,2017-03-05T21:40:51Z,http://arxiv.org/abs/1612.07182v2,http://arxiv.org/pdf/1612.07182v2,Multi-Agent Cooperation and the Emergence of (Natural) Language,multi agent cooper emerg natur languag,"The current mainstream approach to train natural language systems is to expose them to large amounts of text. This passive learning is problematic if we are interested in developing interactive machines, such as conversational agents. We propose a framework for language learning that relies on multi-agent communication. We study this learning in the context of referential games. In these games, a sender and a receiver see a pair of images. The sender is told one of them is the target and is allowed to send a message from a fixed, arbitrary vocabulary to the receiver. The receiver must rely on this message to identify the target. Thus, the agents develop their own language interactively out of the need to communicate. We show that two networks with simple configurations are able to learn to coordinate in the referential game. We further explore how to make changes to the game environment to cause the ""word meanings"" induced in the game to better reflect intuitive semantic properties of the images. In addition, we present a simple strategy for grounding the agents' code into natural language. Both of these are necessary steps towards developing machines that are able to communicate with humans productively.",current mainstream approach train natur languag system expos larg amount text passiv learn problemat interest develop interact machin convers agent propos framework languag learn reli multi agent communic studi learn context referenti game game sender receiv see pair imag sender told one target allow send messag fix arbitrari vocabulari receiv receiv must reli messag identifi target thus agent develop languag interact need communic show two network simpl configur abl learn coordin referenti game explor make chang game environ caus word mean induc game better reflect intuit semant properti imag addit present simpl strategi ground agent code natur languag necessari step toward develop machin abl communic human product,"['Angeliki Lazaridou', 'Alexander Peysakhovich', 'Marco Baroni']","['cs.CL', 'cs.CV', 'cs.GT', 'cs.LG', 'cs.MA']",False,False,False,False,False,True
772,2017-03-28T14:10:38Z,2016-12-21T11:16:50Z,http://arxiv.org/abs/1612.07059v1,http://arxiv.org/pdf/1612.07059v1,ARES: Adaptive Receding-Horizon Synthesis of Optimal Plans,adapt reced horizon synthesi optim plan,"We introduce ARES, an efficient approximation algorithm for generating optimal plans (action sequences) that take an initial state of a Markov Decision Process (MDP) to a state whose cost is below a specified (convergence) threshold. ARES uses Particle Swarm Optimization, with adaptive sizing for both the receding horizon and the particle swarm. Inspired by Importance Splitting, the length of the horizon and the number of particles are chosen such that at least one particle reaches a next-level state, that is, a state where the cost decreases by a required delta from the previous-level state. The level relation on states and the plans constructed by ARES implicitly define a Lyapunov function and an optimal policy, respectively, both of which could be explicitly generated by applying ARES to all states of the MDP, up to some topological equivalence relation. We also assess the effectiveness of ARES by statistically evaluating its rate of success in generating optimal plans. The ARES algorithm resulted from our desire to clarify if flying in V-formation is a flocking policy that optimizes energy conservation, clear view, and velocity alignment. That is, we were interested to see if one could find optimal plans that bring a flock from an arbitrary initial state to a state exhibiting a single connected V-formation. For flocks with 7 birds, ARES is able to generate a plan that leads to a V-formation in 95% of the 8,000 random initial configurations within 63 seconds, on average. ARES can also be easily customized into a model-predictive controller (MPC) with an adaptive receding horizon and statistical guarantees of convergence. To the best of our knowledge, our adaptive-sizing approach is the first to provide convergence guarantees in receding-horizon techniques.",introduc effici approxim algorithm generat optim plan action sequenc take initi state markov decis process mdp state whose cost specifi converg threshold use particl swarm optim adapt size reced horizon particl swarm inspir import split length horizon number particl chosen least one particl reach next level state state cost decreas requir delta previous level state level relat state plan construct implicit defin lyapunov function optim polici respect could explicit generat appli state mdp topolog equival relat also assess effect statist evalu rate success generat optim plan algorithm result desir clarifi fli format flock polici optim energi conserv clear view veloc align interest see one could find optim plan bring flock arbitrari initi state state exhibit singl connect format flock bird abl generat plan lead format random initi configur within second averag also easili custom model predict control mpc adapt reced horizon statist guarante converg best knowledg adapt size approach first provid converg guarante reced horizon techniqu,"['Anna Lukina', 'Lukas Esterle', 'Christian Hirsch', 'Ezio Bartocci', 'Junxing Yang', 'Ashish Tiwari', 'Scott A. Smolka', 'Radu Grosu']","['cs.AI', 'cs.MA', 'cs.SY']",False,False,True,False,False,True
773,2017-03-28T14:10:38Z,2017-02-20T17:54:11Z,http://arxiv.org/abs/1612.06340v2,http://arxiv.org/pdf/1612.06340v2,Computing Human-Understandable Strategies,comput human understand strategi,"Algorithms for equilibrium computation generally make no attempt to ensure that the computed strategies are understandable by humans. For instance the strategies for the strongest poker agents are represented as massive binary files. In many situations, we would like to compute strategies that can actually be implemented by humans, who may have computational limitations and may only be able to remember a small number of features or components of the strategies that have been computed. We study poker games where private information distributions can be arbitrary. We create a large training set of game instances and solutions, by randomly selecting the information probabilities, and present algorithms that learn from the training instances in order to perform well in games with unseen information distributions. We are able to conclude several new fundamental rules about poker strategy that can be easily implemented by humans.",algorithm equilibrium comput general make attempt ensur comput strategi understand human instanc strategi strongest poker agent repres massiv binari file mani situat would like comput strategi actual implement human may comput limit may onli abl rememb small number featur compon strategi comput studi poker game privat inform distribut arbitrari creat larg train set game instanc solut random select inform probabl present algorithm learn train instanc order perform well game unseen inform distribut abl conclud sever new fundament rule poker strategi easili implement human,"['Sam Ganzfried', 'Farzana Yusuf']","['cs.GT', 'cs.AI', 'cs.LG', 'cs.MA']",False,False,True,False,False,True
774,2017-03-28T14:10:38Z,2016-12-17T01:54:23Z,http://arxiv.org/abs/1612.05693v1,http://arxiv.org/pdf/1612.05693v1,Optimal Target Assignment and Path Finding for Teams of Agents,optim target assign path find team agent,"We study the TAPF (combined target-assignment and path-finding) problem for teams of agents in known terrain, which generalizes both the anonymous and non-anonymous multi-agent path-finding problems. Each of the teams is given the same number of targets as there are agents in the team. Each agent has to move to exactly one target given to its team such that all targets are visited. The TAPF problem is to first assign agents to targets and then plan collision-free paths for the agents to their targets in a way such that the makespan is minimized. We present the CBM (Conflict-Based Min-Cost-Flow) algorithm, a hierarchical algorithm that solves TAPF instances optimally by combining ideas from anonymous and non-anonymous multi-agent path-finding algorithms. On the low level, CBM uses a min-cost max-flow algorithm on a time-expanded network to assign all agents in a single team to targets and plan their paths. On the high level, CBM uses conflict-based search to resolve collisions among agents in different teams. Theoretically, we prove that CBM is correct, complete and optimal. Experimentally, we show the scalability of CBM to TAPF instances with dozens of teams and hundreds of agents and adapt it to a simulated warehouse system.",studi tapf combin target assign path find problem team agent known terrain general anonym non anonym multi agent path find problem team given number target agent team agent move exact one target given team target visit tapf problem first assign agent target plan collis free path agent target way makespan minim present cbm conflict base min cost flow algorithm hierarch algorithm solv tapf instanc optim combin idea anonym non anonym multi agent path find algorithm low level cbm use min cost max flow algorithm time expand network assign agent singl team target plan path high level cbm use conflict base search resolv collis among agent differ team theoret prove cbm correct complet optim experiment show scalabl cbm tapf instanc dozen team hundr agent adapt simul warehous system,"['Hang Ma', 'Sven Koenig']","['cs.AI', 'cs.MA', 'cs.RO']",False,False,False,False,False,True
775,2017-03-28T14:10:38Z,2016-12-15T23:33:41Z,http://arxiv.org/abs/1612.05309v1,http://arxiv.org/pdf/1612.05309v1,Multi-Agent Path Finding with Delay Probabilities,multi agent path find delay probabl,"Several recently developed Multi-Agent Path Finding (MAPF) solvers scale to large MAPF instances by searching for MAPF plans on 2 levels: The high-level search resolves collisions between agents, and the low-level search plans paths for single agents under the constraints imposed by the high-level search. We make the following contributions to solve the MAPF problem with imperfect plan execution with small average makespans: First, we formalize the MAPF Problem with Delay Probabilities (MAPF-DP), define valid MAPF-DP plans and propose the use of robust plan-execution policies for valid MAPF-DP plans to control how each agent proceeds along its path. Second, we discuss 2 classes of decentralized robust plan-execution policies (called Fully Synchronized Policies and Minimal Communication Policies) that prevent collisions during plan execution for valid MAPF-DP plans. Third, we present a 2-level MAPF-DP solver (called Approximate Minimization in Expectation) that generates valid MAPF-DP plans.",sever recent develop multi agent path find mapf solver scale larg mapf instanc search mapf plan level high level search resolv collis agent low level search plan path singl agent constraint impos high level search make follow contribut solv mapf problem imperfect plan execut small averag makespan first formal mapf problem delay probabl mapf dp defin valid mapf dp plan propos use robust plan execut polici valid mapf dp plan control agent proceed along path second discuss class decentr robust plan execut polici call fulli synchron polici minim communic polici prevent collis dure plan execut valid mapf dp plan third present level mapf dp solver call approxim minim expect generat valid mapf dp plan,"['Hang Ma', 'T. K. Satish Kumar', 'Sven Koenig']","['cs.AI', 'cs.MA', 'cs.RO']",False,False,False,False,False,True
776,2017-03-28T14:10:38Z,2016-12-15T19:26:33Z,http://arxiv.org/abs/1612.05201v1,http://arxiv.org/pdf/1612.05201v1,Models of latent consensus,model latent consensus,"The paper studies the problem of achieving consensus in multi-agent systems in the case where the dependency digraph $\Gamma$ has no spanning in-tree. We consider the regularization protocol that amounts to the addition of a dummy agent (hub) uniformly connected to the agents. The presence of such a hub guarantees the achievement of an asymptotic consensus. For the ""evaporation"" of the dummy agent, the strength of its influences on the other agents vanishes, which leads to the concept of latent consensus. We obtain a closed-form expression for the consensus when the connections of the hub are symmetric, in this case, the impact of the hub upon the consensus remains fixed. On the other hand, if the hub is essentially influenced by the agents, whereas its influence on them tends to zero, then the consensus is expressed by the scalar product of the vector of column means of the Laplacian eigenprojection of $\Gamma$ and the initial state vector of the system. Another protocol, which assumes the presence of vanishingly weak uniform background links between the agents, leads to the same latent consensus.",paper studi problem achiev consensus multi agent system case depend digraph gamma span tree consid regular protocol amount addit dummi agent hub uniform connect agent presenc hub guarante achiev asymptot consensus evapor dummi agent strength influenc agent vanish lead concept latent consensus obtain close form express consensus connect hub symmetr case impact hub upon consensus remain fix hand hub essenti influenc agent wherea influenc tend zero consensus express scalar product vector column mean laplacian eigenproject gamma initi state vector system anoth protocol assum presenc vanish weak uniform background link agent lead latent consensus,"['Rafig Agaev', 'Pavel Chebotarev']","['math.OC', 'cs.MA', 'cs.SY', '93A14, 68T42, 15B51, 05C50, 05C05, 60J22']",False,False,False,False,False,True
778,2017-03-28T14:10:38Z,2016-12-13T23:56:58Z,http://arxiv.org/abs/1612.04432v1,http://arxiv.org/pdf/1612.04432v1,An argumentative agent-based model of scientific inquiry,argument agent base model scientif inquiri,"In this paper we present an agent-based model (ABM) of scientific inquiry aimed at investigating how different social networks impact the efficiency of scientists in acquiring knowledge. As such, the ABM is a computational tool for tackling issues in the domain of scientific methodology and science policy. In contrast to existing ABMs of science, our model aims to represent the argumentative dynamics that underlies scientific practice. To this end we employ abstract argumentation theory as the core design feature of the model. This helps to avoid a number of problematic idealizations which are present in other ABMs of science and which impede their relevance for actual scientific practice.",paper present agent base model abm scientif inquiri aim investig differ social network impact effici scientist acquir knowledg abm comput tool tackl issu domain scientif methodolog scienc polici contrast exist abm scienc model aim repres argument dynam scientif practic end employ abstract argument theori core design featur model help avoid number problemat ideal present abm scienc imped relev actual scientif practic,"['Annemarie Borg', 'Daniel Frey', 'Dunja Šešelja', 'Christian Straßer']","['cs.SI', 'cs.AI', 'cs.MA', '91-04', 'I.2.11; I.2.6; I.6.0']",False,False,False,False,False,True
779,2017-03-28T14:10:38Z,2016-12-13T17:51:59Z,http://arxiv.org/abs/1612.04299v1,http://arxiv.org/pdf/1612.04299v1,Algorithms for Graph-Constrained Coalition Formation in the Real World,algorithm graph constrain coalit format real world,"Coalition formation typically involves the coming together of multiple, heterogeneous, agents to achieve both their individual and collective goals. In this paper, we focus on a special case of coalition formation known as Graph-Constrained Coalition Formation (GCCF) whereby a network connecting the agents constrains the formation of coalitions. We focus on this type of problem given that in many real-world applications, agents may be connected by a communication network or only trust certain peers in their social network. We propose a novel representation of this problem based on the concept of edge contraction, which allows us to model the search space induced by the GCCF problem as a rooted tree. Then, we propose an anytime solution algorithm (CFSS), which is particularly efficient when applied to a general class of characteristic functions called $m+a$ functions. Moreover, we show how CFSS can be efficiently parallelised to solve GCCF using a non-redundant partition of the search space. We benchmark CFSS on both synthetic and realistic scenarios, using a real-world dataset consisting of the energy consumption of a large number of households in the UK. Our results show that, in the best case, the serial version of CFSS is 4 orders of magnitude faster than the state of the art, while the parallel version is 9.44 times faster than the serial version on a 12-core machine. Moreover, CFSS is the first approach to provide anytime approximate solutions with quality guarantees for very large systems of agents (i.e., with more than 2700 agents).",coalit format typic involv come togeth multipl heterogen agent achiev individu collect goal paper focus special case coalit format known graph constrain coalit format gccf wherebi network connect agent constrain format coalit focus type problem given mani real world applic agent may connect communic network onli trust certain peer social network propos novel represent problem base concept edg contract allow us model search space induc gccf problem root tree propos anytim solut algorithm cfss particular effici appli general class characterist function call function moreov show cfss effici parallelis solv gccf use non redund partit search space benchmark cfss synthet realist scenario use real world dataset consist energi consumpt larg number household uk result show best case serial version cfss order magnitud faster state art parallel version time faster serial version core machin moreov cfss first approach provid anytim approxim solut qualiti guarante veri larg system agent agent,"['Filippo Bistaffa', 'Alessandro Farinelli', 'Jesús Cerquides', 'Juan A. Rodríguez-Aguilar', 'Sarvapali D. Ramchurn']","['cs.MA', 'cs.AI', 'I.2']",False,False,True,False,False,True
780,2017-03-28T14:10:43Z,2016-12-11T16:22:27Z,http://arxiv.org/abs/1612.03433v1,http://arxiv.org/abs/1612.03433v1,A Model of Multi-Agent Consensus for Vague and Uncertain Beliefs,model multi agent consensus vagu uncertain belief,"Consensus formation is investigated for multi-agent systems in which agents' beliefs are both vague and uncertain. Vagueness is represented by a third truth state meaning \emph{borderline}. This is combined with a probabilistic model of uncertainty. A belief combination operator is then proposed which exploits borderline truth values to enable agents with conflicting beliefs to reach a compromise. A number of simulation experiments are carried out in which agents apply this operator in pairwise interactions, under the bounded confidence restriction that the two agents' beliefs must be sufficiently consistent with each other before agreement can be reached. As well as studying the consensus operator in isolation we also investigate scenarios in which agents are influenced either directly or indirectly by the state of the world. For the former we conduct simulations which combine consensus formation with belief updating based on evidence. For the latter we investigate the effect of assuming that the closer an agent's beliefs are to the truth the more visible they are in the consensus building process. In all cases applying the consensus operators results in the population converging to a single shared belief which is both crisp and certain. Furthermore, simulations which combine consensus formation with evidential updating converge faster to a shared opinion which is closer to the actual state of the world than those in which beliefs are only changed as a result of directly receiving new evidence. Finally, if agent interactions are guided by belief quality measured as similarity to the true state of the world, then applying the consensus operator alone results in the population converging to a high quality shared belief.",consensus format investig multi agent system agent belief vagu uncertain vagu repres third truth state mean emph borderlin combin probabilist model uncertainti belief combin oper propos exploit borderlin truth valu enabl agent conflict belief reach compromis number simul experi carri agent appli oper pairwis interact bound confid restrict two agent belief must suffici consist befor agreement reach well studi consensus oper isol also investig scenario agent influenc either direct indirect state world former conduct simul combin consensus format belief updat base evid latter investig effect assum closer agent belief truth visibl consensus build process case appli consensus oper result popul converg singl share belief crisp certain furthermor simul combin consensus format evidenti updat converg faster share opinion closer actual state world belief onli chang result direct receiv new evid final agent interact guid belief qualiti measur similar true state world appli consensus oper alon result popul converg high qualiti share belief,"['Michael Crosscombe', 'Jonathan Lawry']","['cs.MA', 'cs.AI']",False,False,True,False,False,True
781,2017-03-28T14:10:43Z,2017-03-13T16:08:55Z,http://arxiv.org/abs/1612.02684v3,http://arxiv.org/pdf/1612.02684v3,Fixpoint Approximation of Strategic Abilities under Imperfect   Information,fixpoint approxim strateg abil imperfect inform,"Model checking of strategic ability under imperfect information is known to be hard. The complexity results range from NP-completeness to undecidability, depending on the precise setup of the problem. No less importantly, fixpoint equivalences do not generally hold for imperfect information strategies, which seriously hampers incremental synthesis of winning strategies. In this paper, we propose translations of ATLir formulae that provide lower and upper bounds for their truth values, and are cheaper to verify than the original specifications. That is, if the expression is verified as true then the corresponding formula of ATLir should also hold in the given model. We begin by showing where the straightforward approach does not work. Then, we propose how it can be modified to obtain guaranteed lower bounds. To this end, we alter the next-step operator in such a way that traversing one's indistinguishability relation is seen as atomic activity. Most interestingly, the lower approximation is provided by a fixpoint expression that uses a nonstandard variant of the next-step ability operator. We show the correctness of the translations, establish their computational complexity, and validate the approach by experiments with a scalable scenario of Bridge play.",model check strateg abil imperfect inform known hard complex result rang np complet undecid depend precis setup problem less import fixpoint equival general hold imperfect inform strategi serious hamper increment synthesi win strategi paper propos translat atlir formula provid lower upper bound truth valu cheaper verifi origin specif express verifi true correspond formula atlir also hold given model begin show straightforward approach doe work propos modifi obtain guarante lower bound end alter next step oper way travers one indistinguish relat seen atom activ interest lower approxim provid fixpoint express use nonstandard variant next step abil oper show correct translat establish comput complex valid approach experi scalabl scenario bridg play,"['Wojciech Jamroga', 'Michał Knapik', 'Damian Kurpiewski']","['cs.MA', 'cs.AI', 'cs.LO']",False,False,False,False,False,True
783,2017-03-28T14:10:43Z,2016-12-06T22:52:25Z,http://arxiv.org/abs/1612.02065v1,http://arxiv.org/pdf/1612.02065v1,Collaborative Visual Area Coverage using Unmanned Aerial Vehicles,collabor visual area coverag use unman aerial vehicl,"This article addresses the visual area coverage problem using a team of Unmanned Aerial Vehicles (UAVs). The UAVs are assumed to be equipped with a downward facing camera covering all points of interest within a circle on the ground. The diameter of this circular conic-section increases as the UAV flies at a larger height, yet the quality of the observed area is inverse proportional to the UAV's height. The objective is to provide a distributed control algorithm that maximizes a combined coverage-quality criterion by adjusting the UAV's altitude. Simulation studies are offered to highlight the effectiveness of the suggested scheme.",articl address visual area coverag problem use team unman aerial vehicl uav uav assum equip downward face camera cover point interest within circl ground diamet circular conic section increas uav fli larger height yet qualiti observ area invers proport uav height object provid distribut control algorithm maxim combin coverag qualiti criterion adjust uav altitud simul studi offer highlight effect suggest scheme,"['Sotiris Papatheodorou', 'Anthony Tzes', 'Yiannis Stergiopoulos']","['cs.SY', 'cs.MA']",False,False,False,False,False,True
784,2017-03-28T14:10:43Z,2016-12-07T03:39:59Z,http://arxiv.org/abs/1612.01600v2,http://arxiv.org/pdf/1612.01600v2,Distributed Gaussian Learning over Time-varying Directed Graphs,distribut gaussian learn time vari direct graph,"We present a distributed (non-Bayesian) learning algorithm for the problem of parameter estimation with Gaussian noise. The algorithm is expressed as explicit updates on the parameters of the Gaussian beliefs (i.e. means and precision). We show a convergence rate of $O(1/k)$ with the constant term depending on the number of agents and the topology of the network. Moreover, we show almost sure convergence to the optimal solution of the estimation problem for the general case of time-varying directed graphs.",present distribut non bayesian learn algorithm problem paramet estim gaussian nois algorithm express explicit updat paramet gaussian belief mean precis show converg rate constant term depend number agent topolog network moreov show almost sure converg optim solut estim problem general case time vari direct graph,"['Angelia Nedić', 'Alex Olshevsky', 'César A. Uribe']","['math.OC', 'cs.LG', 'cs.MA', 'cs.SY', 'stat.ML']",False,False,False,False,False,True
785,2017-03-28T14:10:43Z,2016-12-05T17:09:34Z,http://arxiv.org/abs/1612.01434v1,http://arxiv.org/pdf/1612.01434v1,Proportional Rankings,proport rank,"In this paper we extend the principle of proportional representation to rankings. We consider the setting where alternatives need to be ranked based on approval preferences. In this setting, proportional representation requires that cohesive groups of voters are represented proportionally in each initial segment of the ranking. Proportional rankings are desirable in situations where initial segments of different lengths may be relevant, e.g., hiring decisions (if it is unclear how many positions are to be filled), the presentation of competing proposals on a liquid democracy platform (if it is unclear how many proposals participants are taking into consideration), or recommender systems (if a ranking has to accommodate different user types). We study the proportional representation provided by several ranking methods and prove theoretical guarantees. Furthermore, we experimentally evaluate these methods and present preliminary evidence as to which methods are most suitable for producing proportional rankings.",paper extend principl proport represent rank consid set altern need rank base approv prefer set proport represent requir cohes group voter repres proport initi segment rank proport rank desir situat initi segment differ length may relev hire decis unclear mani posit fill present compet propos liquid democraci platform unclear mani propos particip take consider recommend system rank accommod differ user type studi proport represent provid sever rank method prove theoret guarante furthermor experiment evalu method present preliminari evid method suitabl produc proport rank,"['Piotr Skowron', 'Martin Lackner', 'Markus Brill', 'Dominik Peters', 'Edith Elkind']","['cs.GT', 'cs.AI', 'cs.MA']",False,False,True,False,False,True
786,2017-03-28T14:10:43Z,2016-12-05T06:56:19Z,http://arxiv.org/abs/1612.01260v1,http://arxiv.org/pdf/1612.01260v1,Real-time Collision Handling in Railway Network:An Agent-based Approach,real time collis handl railway network agent base approach,"Advancement in intelligent transportation systems with complex operations requires autonomous planning and management to avoid collisions in day-to-day traffic. As failure and/or inadequacy in traffic safety system are life-critical, such collisions must be detected and resolved in an efficient way to manage continuously rising traffic. In this paper, we address different types of collision scenarios along with their early detection and resolution techniques in a complex railway system. In order to handle collisions dynamically in distributed manner, a novel agent based solution approach is proposed using the idea of max-sum algorithm, where each agent (train agent, station agent, and junction agent) communicates and cooperates with others to generate a good feasible solution that keeps the system in a safe state, i.e., collision free. We implement the proposed mechanism in Java Agent DEvelopment Framework (JADE). The results are evaluated with exhaustive experiments and compared with different existing collision handling methods to show the efficiency of our proposed approach.",advanc intellig transport system complex oper requir autonom plan manag avoid collis day day traffic failur inadequaci traffic safeti system life critic collis must detect resolv effici way manag continu rise traffic paper address differ type collis scenario along earli detect resolut techniqu complex railway system order handl collis dynam distribut manner novel agent base solut approach propos use idea max sum algorithm agent train agent station agent junction agent communic cooper generat good feasibl solut keep system safe state collis free implement propos mechan java agent develop framework jade result evalu exhaust experi compar differ exist collis handl method show effici propos approach,"['Poulami Dalapati', 'Abhijeet Padhy', 'Bhawana Mishra', 'Animesh Dutta', 'Swapan Bhattacharya']",['cs.MA'],False,False,True,False,False,True
787,2017-03-28T14:10:43Z,2016-12-03T09:48:55Z,http://arxiv.org/abs/1612.00947v1,http://arxiv.org/pdf/1612.00947v1,Disruptive innovations in RoboCup 2D Soccer Simulation League: from   Cyberoos'98 to Gliders2016,disrupt innov robocup soccer simul leagu cyberoo glider,"We review disruptive innovations introduced in the RoboCup 2D Soccer Simulation League over the twenty years since its inception, and trace the progress of our champion team (Gliders). We conjecture that the League has been developing as an ecosystem shaped by diverse approaches taken by participating teams, increasing in its overall complexity. A common feature is that different champion teams succeeded in finding a way to decompose the enormous search-space of possible single- and multi-agent behaviours, by automating the exploration of the problem space with various techniques which accelerated the software development efforts. These methods included interactive debugging, machine learning, automated planning, and opponent modelling. The winning approach developed by Gliders is centred on human-based evolutionary computation which optimised several components such as an action-dependent evaluation function, dynamic tactics with Voronoi diagrams, information dynamics, and bio-inspired collective behaviour.",review disrupt innov introduc robocup soccer simul leagu twenti year sinc incept trace progress champion team glider conjectur leagu develop ecosystem shape divers approach taken particip team increas overal complex common featur differ champion team succeed find way decompos enorm search space possibl singl multi agent behaviour autom explor problem space various techniqu acceler softwar develop effort method includ interact debug machin learn autom plan oppon model win approach develop glider centr human base evolutionari comput optimis sever compon action depend evalu function dynam tactic voronoi diagram inform dynam bio inspir collect behaviour,"['Mikhail Prokopenko', 'Peter Wang']",['cs.MA'],False,False,False,False,False,True
788,2017-03-28T14:10:43Z,2016-12-03T00:36:50Z,http://arxiv.org/abs/1612.00903v1,http://arxiv.org/pdf/1612.00903v1,Expander Graph and Communication-Efficient Decentralized Optimization,expand graph communic effici decentr optim,"In this paper, we discuss how to design the graph topology to reduce the communication complexity of certain algorithms for decentralized optimization. Our goal is to minimize the total communication needed to achieve a prescribed accuracy. We discover that the so-called expander graphs are near-optimal choices. We propose three approaches to construct expander graphs for different numbers of nodes and node degrees. Our numerical results show that the performance of decentralized optimization is significantly better on expander graphs than other regular graphs.",paper discuss design graph topolog reduc communic complex certain algorithm decentr optim goal minim total communic need achiev prescrib accuraci discov call expand graph near optim choic propos three approach construct expand graph differ number node node degre numer result show perform decentr optim signific better expand graph regular graph,"['Yat-Tin Chow', 'Wei Shi', 'Tianyu Wu', 'Wotao Yin']","['math.OC', 'cs.DC', 'cs.MA']",False,False,True,False,False,True
789,2017-03-28T14:10:43Z,2016-12-01T21:38:03Z,http://arxiv.org/abs/1612.00480v1,http://arxiv.org/pdf/1612.00480v1,A Scalable and Adaptable Multiple-Place Foraging Algorithm for   Ant-Inspired Robot Swarms,scalabl adapt multipl place forag algorithm ant inspir robot swarm,"Individual robots are not effective at exploring large unmapped areas. An alternate approach is to use a swarm of simple robots that work together, rather than a single highly capable robot. The central-place foraging algorithm (CPFA) is effective for coordinating robot swarm search and collection tasks. Robots start at a centrally placed location (nest), explore potential targets in the area without global localization or central control, and return the targets to the nest. The scalability of the CPFA is limited because large numbers of robots produce more inter-robot collisions and large search areas result in substantial travel costs. We address these problems with the multiple-place foraging algorithm (MPFA), which uses multiple nests distributed throughout the search area. Robots start from a randomly assigned home nest but return to the closest nest with found targets. We simulate the foraging behavior of robot swarms in the robot simulator ARGoS and employ a genetic algorithm to discover different optimized foraging strategies as swarm sizes and the number of targets are scaled up. In our experiments, the MPFA always produces higher foraging rates, fewer collisions, and lower travel and search time compared to the CPFA for the partially clustered targets distribution. The main contribution of this paper is that we systematically quantify the advantages of the MPFA (reduced travel time and collisions), the potential disadvantages (less communication among robots), and the ability of a genetic algorithm to tune MPFA parameters to mitigate search inefficiency due to less communication.",individu robot effect explor larg unmap area altern approach use swarm simpl robot work togeth rather singl high capabl robot central place forag algorithm cpfa effect coordin robot swarm search collect task robot start central place locat nest explor potenti target area without global local central control return target nest scalabl cpfa limit becaus larg number robot produc inter robot collis larg search area result substanti travel cost address problem multipl place forag algorithm mpfa use multipl nest distribut throughout search area robot start random assign home nest return closest nest found target simul forag behavior robot swarm robot simul argo employ genet algorithm discov differ optim forag strategi swarm size number target scale experi mpfa alway produc higher forag rate fewer collis lower travel search time compar cpfa partial cluster target distribut main contribut paper systemat quantifi advantag mpfa reduc travel time collis potenti disadvantag less communic among robot abil genet algorithm tune mpfa paramet mitig search ineffici due less communic,"['Qi Lu', 'Melanie E. Moses', 'Joshua P. Hecker']","['cs.MA', 'cs.RO']",False,False,False,False,False,True
790,2017-03-28T14:10:47Z,2017-03-02T23:21:46Z,http://arxiv.org/abs/1612.00150v2,http://arxiv.org/pdf/1612.00150v2,Decentralized Consensus Optimization with Asynchrony and Delays,decentr consensus optim asynchroni delay,"We propose an asynchronous, decentralized algorithm for consensus optimization. The algorithm runs over a network in which the agents communicate with their neighbors and perform local computation. In the proposed algorithm, each agent can compute and communicate independently at different times, for different durations, with the information it has even if the latest information from its neighbors is not yet available. Such an asynchronous algorithm reduces the time that agents would otherwise waste idle because of communication delays or because their neighbors are slower. It also eliminates the need for a global clock for synchronization. Mathematically, the algorithm involves both primal and dual variables, uses fixed step-size parameters, and provably converges to the exact solution under a bounded delay assumption and a random agent assumption. When running synchronously, the algorithm performs just as well as existing competitive synchronous algorithms such as PG-EXTRA, which diverges without synchronization. Numerical experiments confirm the theoretical findings and illustrate the performance of the proposed algorithm.",propos asynchron decentr algorithm consensus optim algorithm run network agent communic neighbor perform local comput propos algorithm agent comput communic independ differ time differ durat inform even latest inform neighbor yet avail asynchron algorithm reduc time agent would otherwis wast idl becaus communic delay becaus neighbor slower also elimin need global clock synchron mathemat algorithm involv primal dual variabl use fix step size paramet provabl converg exact solut bound delay assumpt random agent assumpt run synchron algorithm perform well exist competit synchron algorithm pg extra diverg without synchron numer experi confirm theoret find illustr perform propos algorithm,"['Tianyu Wu', 'Kun Yuan', 'Qing Ling', 'Wotao Yin', 'Ali H. Sayed']","['math.OC', 'cs.DC', 'cs.MA']",False,False,False,False,False,True
791,2017-03-28T14:10:47Z,2016-11-30T14:04:32Z,http://arxiv.org/abs/1611.10154v1,http://arxiv.org/pdf/1611.10154v1,A Majoritarian Representative Voting System,majoritarian repres vote system,"We present an alternative voting system that aims at bridging the gap between proportional representative systems and majoritarian, single winner election systems. The system lets people vote for multiple parties, but then assigns each ballot to a single party. This opens a whole range of possible systems, all representative. We show theoretically that this space is convex. Then among the possible parliaments we present an algorithm to produce the most majoritarian result. We then test the system and compare the results with a pure proportional and a majoritarian voting system showing how the results are comparable with the majoritarian system. Then we simulate the system and show how it tends to produce parties of exponentially decreasing size with always a first, major party. Finally we describe how the system can be used in a context of a parliament made up of two separate houses.",present altern vote system aim bridg gap proport repres system majoritarian singl winner elect system system let peopl vote multipl parti assign ballot singl parti open whole rang possibl system repres show theoret space convex among possibl parliament present algorithm produc majoritarian result test system compar result pure proport majoritarian vote system show result compar majoritarian system simul system show tend produc parti exponenti decreas size alway first major parti final describ system use context parliament made two separ hous,"['Pietro Speroni di Fenizio', 'Daniele A. Gewurz']","['cs.GT', 'cs.MA', 'math.CO']",False,False,True,False,False,True
792,2017-03-28T14:10:47Z,2016-11-30T05:37:11Z,http://arxiv.org/abs/1611.10007v1,http://arxiv.org/abs/1611.10007v1,Structural Controllability of Multi-Agent Networks: Robustness against   Simultaneous Failures,structur control multi agent network robust simultan failur,"In this paper, structural controllability of a leader-follower multi-agent system with multiple leaders is studied from a graph-theoretic point of view. The problem of preservation of structural controllability under simultaneous failures in both the communication links and the agents is investigated. The effects of the loss of agents and communication links on the controllability of an information flow graph are previously studied. In this work, the corresponding results are exploited to introduce some useful indices and importance measures that help characterize and quantify the role of individual links and agents in the controllability of the overall network. Existing results are then extended by considering the effects of losses in both links and agents at the same time. To this end, the concepts of joint (r,s)-controllability and joint t-controllability are introduced as quantitative measures of reliability for a multi-agent system, and their important properties are investigated. Lastly, the class of jointly critical digraphs is introduced and it is stated that if a digraph is jointly critical, then joint t-controllability is a necessary and sufficient condition for remaining controllable following the failure of any set of links and agents, with cardinality less than t. Various examples are exploited throughout the paper to elaborate on the analytical findings.",paper structur control leader follow multi agent system multipl leader studi graph theoret point view problem preserv structur control simultan failur communic link agent investig effect loss agent communic link control inform flow graph previous studi work correspond result exploit introduc use indic import measur help character quantifi role individu link agent control overal network exist result extend consid effect loss link agent time end concept joint control joint control introduc quantit measur reliabl multi agent system import properti investig last class joint critic digraph introduc state digraph joint critic joint control necessari suffici condit remain control follow failur ani set link agent cardin less various exampl exploit throughout paper elabor analyt find,"['M. Amin Rahimian', 'Amir G. Aghdam']","['cs.SY', 'cs.MA', 'cs.RO', 'math.CO', 'math.DS']",False,False,False,False,False,True
795,2017-03-28T14:10:47Z,2016-11-27T22:39:53Z,http://arxiv.org/abs/1611.08927v1,http://arxiv.org/pdf/1611.08927v1,The Complexity of Succinct Elections,complex succinct elect,"The computational study of elections generally assumes that the preferences of the electorate come in as a list of votes. Depending on the context, it may be much more natural to represent the list succinctly, as the distinct votes of the electorate and their counts. We consider how this succinct representation of the voters affects the computational complexity of election problems. Though the succinct representation may be exponentially smaller than the nonsuccinct representation, we find only one natural case where the complexity increases, namely the complexity of winner determination for Kemeny elections. This is in sharp contrast to the case where each voter has a weight, where the complexity usually increases.",comput studi elect general assum prefer elector come list vote depend context may much natur repres list succinct distinct vote elector count consid succinct represent voter affect comput complex elect problem though succinct represent may exponenti smaller nonsuccinct represent find onli one natur case complex increas name complex winner determin kemeni elect sharp contrast case voter weight complex usual increas,"['Zack Fitzsimmons', 'Edith Hemaspaandra']","['cs.GT', 'cs.CC', 'cs.MA']",False,False,False,False,False,True
797,2017-03-28T14:10:47Z,2016-11-25T07:03:21Z,http://arxiv.org/abs/1611.08364v1,http://arxiv.org/pdf/1611.08364v1,Robust Sequential Path Planning Under Disturbances and Adversarial   Intruder,robust sequenti path plan disturb adversari intrud,"Provably safe and scalable multi-vehicle path planning is an important and urgent problem due to the expected increase of automation in civilian airspace in the near future. Although this problem has been studied in the past, there has not been a method that guarantees both goal satisfaction and safety for vehicles with general nonlinear dynamics while taking into account disturbances and potential adversarial agents, to the best of our knowledge. Hamilton-Jacobi (HJ) reachability is the ideal tool for guaranteeing goal satisfaction and safety under such scenarios, and has been successfully applied to many small-scale problems. However, a direct application of HJ reachability in most cases becomes intractable when there are more than two vehicles due to the exponentially scaling computational complexity with respect to system dimension. In this paper, we take advantage of the guarantees HJ reachability provides, and eliminate the computation burden by assigning a strict priority ordering to the vehicles under consideration. Under this sequential path planning (SPP) scheme, vehicles reserve ""space-time"" portions in the airspace, and the space-time portions guarantee dynamic feasibility, collision avoidance, and optimality of the paths given the priority ordering. With a computation complexity that scales quadratically when accounting for both disturbances and an intruder, and linearly when accounting for only disturbances, SPP can tractably solve the multi-vehicle path planning problem for vehicles with general nonlinear dynamics in a practical setting. We demonstrate our theory in representative simulations.",provabl safe scalabl multi vehicl path plan import urgent problem due expect increas autom civilian airspac near futur although problem studi past method guarante goal satisfact safeti vehicl general nonlinear dynam take account disturb potenti adversari agent best knowledg hamilton jacobi hj reachabl ideal tool guarante goal satisfact safeti scenario success appli mani small scale problem howev direct applic hj reachabl case becom intract two vehicl due exponenti scale comput complex respect system dimens paper take advantag guarante hj reachabl provid elimin comput burden assign strict prioriti order vehicl consider sequenti path plan spp scheme vehicl reserv space time portion airspac space time portion guarante dynam feasibl collis avoid optim path given prioriti order comput complex scale quadrat account disturb intrud linear account onli disturb spp tractabl solv multi vehicl path plan problem vehicl general nonlinear dynam practic set demonstr theori repres simul,"['Mo Chen', 'Somil Bansal', 'Jaime F. Fisac', 'Claire J. Tomlin']",['cs.MA'],False,False,False,False,False,True
798,2017-03-28T14:10:47Z,2016-11-22T18:44:08Z,http://arxiv.org/abs/1611.07454v1,http://arxiv.org/pdf/1611.07454v1,An Agent-Based Model of Message Propagation in the Facebook Electronic   Social Network,agent base model messag propag facebook electron social network,"A large scale agent-based model of common Facebook users was designed to develop an understanding of the underlying mechanism of information diffusion within online social networks at a micro-level analysis. The agent-based model network structure is based on a sample from Facebook. Using an erased configuration model and the idea of common neighbours, a new correction procedure was investigated to overcome the problem of missing graph edges to construct a representative sample of the Facebook network graph. The model parameters are based on assumptions and general activity patterns (such as posting rate, time spent on Facebook etc.) taken from general data on Facebook. Using the agent-based model, the impact of post length, post score and publisher's friend count on the spread of wall posts in several scenarios was analyzed. Findings indicated that post content has the highest impact on the success of post propagation. However, amusing and absorbing but lengthy posts (e.g. a funny video) do not spread as well as short but unremarkable ones (e.g. an interesting photo). In contrast to product adoption and disease spread propagation models, the absence of a similar ""epidemic"" threshold in Facebook post diffusion is observed.",larg scale agent base model common facebook user design develop understand mechan inform diffus within onlin social network micro level analysi agent base model network structur base sampl facebook use eras configur model idea common neighbour new correct procedur investig overcom problem miss graph edg construct repres sampl facebook network graph model paramet base assumpt general activ pattern post rate time spent facebook etc taken general data facebook use agent base model impact post length post score publish friend count spread wall post sever scenario analyz find indic post content highest impact success post propag howev amus absorb lengthi post funni video spread well short unremark one interest photo contrast product adopt diseas spread propag model absenc similar epidem threshold facebook post diffus observ,"['Hamid Reza Nasrinpour', 'Marcia R. Friesen', 'Robert D.', 'McLeod']","['cs.SI', 'cs.MA']",False,False,True,False,False,True
799,2017-03-28T14:10:47Z,2016-11-21T16:00:59Z,http://arxiv.org/abs/1611.06858v1,http://arxiv.org/pdf/1611.06858v1,What Do We Elect Committees For? A Voting Committee Model for   Multi-Winner Rules,elect committe vote committe model multi winner rule,"We present a new model that describes the process of electing a group of representatives (e.g., a parliament) for a group of voters. In this model, called the voting committee model, the elected group of representatives runs a number of ballots to make final decisions regarding various issues. The satisfaction of voters comes from the final decisions made by the elected committee. Our results suggest that depending on a decision system used by the committee to make these final decisions, different multi-winner election rules are most suitable for electing the committee. Furthermore, we show that if we allow not only a committee, but also an election rule used to make final decisions, to depend on the voters' preferences, we can obtain an even better representation of the voters.",present new model describ process elect group repres parliament group voter model call vote committe model elect group repres run number ballot make final decis regard various issu satisfact voter come final decis made elect committe result suggest depend decis system use committe make final decis differ multi winner elect rule suitabl elect committe furthermor show allow onli committe also elect rule use make final decis depend voter prefer obtain even better represent voter,['Piotr Skowron'],"['cs.MA', 'cs.GT']",False,False,False,False,False,True
802,2017-03-28T14:10:52Z,2017-03-23T17:07:14Z,http://arxiv.org/abs/1703.08144v1,http://arxiv.org/pdf/1703.08144v1,Note Value Recognition for Rhythm Transcription Using a Markov Random   Field Model for Musical Scores and Performances of Piano Music,note valu recognit rhythm transcript use markov random field model music score perform piano music,"This paper presents a statistical method for music transcription that can estimate score times of note onsets and offsets from polyphonic MIDI performance signals. Because performed note durations can deviate largely from score-indicated values, previous methods had the problem of not being able to accurately estimate offset score times (or note values) and thus could only output incomplete musical scores. Based on observations that the pitch context and onset score times are influential on the configuration of note values, we construct a context-tree model that provides prior distributions of note values using these features and combine it with a performance model in the framework of Markov random fields. Evaluation results showed that our method reduces the average error rate by around 40 percent compared to existing/simple methods. We also confirmed that, in our model, the score model plays a more important role than the performance model, and it automatically captures the voice structure by unsupervised learning.",paper present statist method music transcript estim score time note onset offset polyphon midi perform signal becaus perform note durat deviat larg score indic valu previous method problem abl accur estim offset score time note valu thus could onli output incomplet music score base observ pitch context onset score time influenti configur note valu construct context tree model provid prior distribut note valu use featur combin perform model framework markov random field evalu result show method reduc averag error rate around percent compar exist simpl method also confirm model score model play import role perform model automat captur voic structur unsupervis learn,"['Eita Nakamura', 'Kazuyoshi Yoshii', 'Simon Dixon']","['cs.AI', 'cs.SD']",False,False,False,False,False,True
808,2017-03-28T14:10:52Z,2017-03-22T07:44:55Z,http://arxiv.org/abs/1703.06891v2,http://arxiv.org/pdf/1703.06891v2,Dance Dance Convolution,danc danc convolut,"Dance Dance Revolution (DDR) is a popular rhythm-based video game. Players perform steps on a dance platform in synchronization with music as directed by on-screen step charts. While many step charts are available in standardized packs, users may grow tired of existing charts, or wish to dance to a song for which no chart exists. We introduce the task of learning to choreograph. Given a raw audio track, the goal is to produce a new step chart. This task decomposes naturally into two subtasks: deciding when to place steps and deciding which steps to select. For the step placement task, we combine recurrent and convolutional neural networks to ingest spectrograms of low-level audio features to predict steps, conditioned on chart difficulty. For step selection, we present a conditional LSTM generative model that substantially outperforms n-gram and fixed-window approaches.",danc danc revolut ddr popular rhythm base video game player perform step danc platform synchron music direct screen step chart mani step chart avail standard pack user may grow tire exist chart wish danc song chart exist introduc task learn choreograph given raw audio track goal produc new step chart task decompos natur two subtask decid place step decid step select step placement task combin recurr convolut neural network ingest spectrogram low level audio featur predict step condit chart difficulti step select present condit lstm generat model substanti outperform gram fix window approach,"['Chris Donahue', 'Zachary C. Lipton', 'Julian McAuley']","['cs.LG', 'cs.MM', 'cs.NE', 'cs.SD', 'stat.ML']",False,False,False,False,False,True
825,2017-03-28T14:11:00Z,2017-02-28T18:26:44Z,http://arxiv.org/abs/1703.00009v1,http://arxiv.org/pdf/1703.00009v1,Nonlinear Model and its Inverse of an Audio System,nonlinear model invers audio system,"This computer science master thesis aims at modelling the nonlinearities of a loudspeaker. A piecewise linear approximation is initially explored and then we present a nonlinear Volterra model to simulate the behavior of the system. The general theory of continuous and discrete Volterra series is summarised. A Normalized Least Mean Square algorithm is used to determine the Volterra series to third order. We also present as inverted system which is trained with the same algorithm. Training data for the models were collected measuring a physical speaker using a laser interferometer. Results indicate a decrease in Mean Squared Error compared to the linear model with a dependency on the particular test signal, the order and the parameters of the model.",comput scienc master thesi aim model nonlinear loudspeak piecewis linear approxim initi explor present nonlinear volterra model simul behavior system general theori continu discret volterra seri summaris normal least mean squar algorithm use determin volterra seri third order also present invert system train algorithm train data model collect measur physic speaker use laser interferomet result indic decreas mean squar error compar linear model depend particular test signal order paramet model,['Alessandro Loriga'],['cs.SD'],False,False,False,False,False,True
847,2017-03-28T14:11:09Z,2017-01-12T09:26:22Z,http://arxiv.org/abs/1701.03274v1,http://arxiv.org/pdf/1701.03274v1,Investigating the role of musical genre in human perception of music   stretching resistance,investig role music genr human percept music stretch resist,"To stretch a music piece to a given length is a common demand in people's daily lives, e.g., in audio-video synchronization and animation production. However, it is not always guaranteed that the stretched music piece is acceptable for general audience since music stretching suffers from people's perceptual artefacts. Over-stretching a music piece will make it uncomfortable for human psychoacoustic hearing. The research on music stretching resistance attempts to estimate the maximum stretchability of music pieces to further avoid over-stretch. It has been observed that musical genres can significantly improve the accuracy of automatic estimation of music stretching resistance, but how musical genres are related to music stretching resistance has never been explained or studied in detail in the literature. In this paper, the characteristics of music stretching resistance are compared across different musical genres. It is found that music stretching resistance has strong intra-genre cohesiveness and inter-genre discrepancies in the experiments. Moreover, the ambiguity and the symmetry of music stretching resistance are also observed in the experimental analysis. These findings lead to a new measurement on the similarity between different musical genres based on their music stretching resistance. In addition, the analysis of variance (ANOVA) also supports the findings in this paper by verifying the significance of musical genre in shaping music stretching resistance.",stretch music piec given length common demand peopl daili live audio video synchron anim product howev alway guarante stretch music piec accept general audienc sinc music stretch suffer peopl perceptu artefact stretch music piec make uncomfort human psychoacoust hear research music stretch resist attempt estim maximum stretchabl music piec avoid stretch observ music genr signific improv accuraci automat estim music stretch resist music genr relat music stretch resist never explain studi detail literatur paper characterist music stretch resist compar across differ music genr found music stretch resist strong intra genr cohes inter genr discrep experi moreov ambigu symmetri music stretch resist also observ experiment analysi find lead new measur similar differ music genr base music stretch resist addit analysi varianc anova also support find paper verifi signific music genr shape music stretch resist,"['Jun Chen', 'Chaokun Wang']","['cs.MM', 'cs.SD']",False,False,False,False,False,True
862,2017-03-28T14:11:17Z,2016-12-16T11:05:52Z,http://arxiv.org/abs/1612.05432v1,http://arxiv.org/pdf/1612.05432v1,Basis-Function Modeling of Loudness Variations in Ensemble Performance,basi function model loud variat ensembl perform,"This paper describes a computational model of loudness variations in expressive ensemble performance. The model predicts and explains the continuous variation of loudness as a function of information extracted automatically from the written score. Although such models have been proposed for expressive performance in solo instruments, this is (to the best of our knowledge) the first attempt to define a model for expressive performance in ensembles. To that end, we extend an existing model that was designed to model expressive piano performances, and describe the additional steps necessary for the model to deal with scores of arbitrary instrumentation, including orchestral scores. We test both linear and non-linear variants of the extended model n a data set of audio recordings of symphonic music, in a leave-one-out setting. The experiments reveal that the most successful model variant is a recurrent, non-linear model. Even if the accuracy of the predicted loudness varies from one recording to another, in several cases the model explains well over 50% of the variance in loudness.",paper describ comput model loud variat express ensembl perform model predict explain continu variat loud function inform extract automat written score although model propos express perform solo instrument best knowledg first attempt defin model express perform ensembl end extend exist model design model express piano perform describ addit step necessari model deal score arbitrari instrument includ orchestr score test linear non linear variant extend model data set audio record symphon music leav one set experi reveal success model variant recurr non linear model even accuraci predict loud vari one record anoth sever case model explain well varianc loud,"['Thassilo Gadermaier', 'Maarten Grachten', 'Carlos Eduardo Cancino Chacón']",['cs.SD'],False,False,False,False,False,True
874,2017-03-28T14:11:21Z,2016-12-15T01:48:52Z,http://arxiv.org/abs/1612.04742v2,http://arxiv.org/pdf/1612.04742v2,Imposing higher-level Structure in Polyphonic Music Generation using   Convolutional Restricted Boltzmann Machines and Constraints,impos higher level structur polyphon music generat use convolut restrict boltzmann machin constraint,"We introduce a method for imposing higher-level structure on generated, polyphonic music. A Convolutional Restricted Boltzmann Machine (C-RBM) as a generative model is combined with gradient descent constraint optimization to provide further control over the generation process. Among other things, this allows for the use of a ""template"" piece, from which some structural properties can be extracted, and transferred as constraints to newly generated material. The sampling process is guided with Simulated Annealing in order to avoid local optima, and find solutions that both satisfy the constraints, and are relatively stable with respect to the C-RBM. Results show that with this approach it is possible to control the higher level self-similarity structure, the meter, as well as tonal properties of the resulting musical piece while preserving its local musical coherence.",introduc method impos higher level structur generat polyphon music convolut restrict boltzmann machin rbm generat model combin gradient descent constraint optim provid control generat process among thing allow use templat piec structur properti extract transfer constraint newli generat materi sampl process guid simul anneal order avoid local optima find solut satisfi constraint relat stabl respect rbm result show approach possibl control higher level self similar structur meter well tonal properti result music piec preserv local music coher,"['Stefan Lattner', 'Maarten Grachten', 'Gerhard Widmer']","['cs.SD', 'cs.AI', 'cs.NE']",False,False,False,False,False,True
885,2017-03-28T14:11:25Z,2017-01-31T17:33:34Z,http://arxiv.org/abs/1612.01860v4,http://arxiv.org/pdf/1612.01860v4,An algorithm to assign musical prime commas to every prime number and   construct a universal and compact free Just Intonation musical notation,algorithm assign music prime comma everi prime number construct univers compact free inton music notat,"Musical frequencies in Just Intonation are comprised of rational numbers. The structure of rational numbers is determined by prime factorisations. Just Intonation frequencies can be split into two components. The larger component uses only integer powers of the first two primes, 2 and 3. The smaller component decomposes into a series of microtonal adjustments, one for each prime number 5 and above present in the original frequency. The larger 3-limit component can be notated using scientific pitch notation modified to use Pythagorean tuning. The microtonal adjustments can be notated using rational commas which are built up from prime commas. This gives a notation system for the whole of free-JI, called Rational Comma Notation. RCN is compact since all microtonal adjustments can be represented by a single notational unit based on a rational number. RCN has different versions depending on the choice of algorithm to assign a prime comma to each prime number. Two existing algorithms SAG and KG are found in the literature. A novel algorithm DR is developed based on discussion of mathematical and musical criteria for algorithm design. Results for DR are presented for primes below 1400. Some observations are made about these results and their applications, including shorthand notation and pitch class lattices. Results for DR are compared with those for SAG and KG. Translation is possible between any two free-JI notations and any two versions of RCN since they all represent the same underlying set of rational numbers.",music frequenc inton compris ration number structur ration number determin prime factoris inton frequenc split two compon larger compon use onli integ power first two prime smaller compon decompos seri microton adjust one prime number abov present origin frequenc larger limit compon notat use scientif pitch notat modifi use pythagorean tune microton adjust notat use ration comma built prime comma give notat system whole free ji call ration comma notat rcn compact sinc microton adjust repres singl notat unit base ration number rcn differ version depend choic algorithm assign prime comma prime number two exist algorithm sag kg found literatur novel algorithm dr develop base discuss mathemat music criteria algorithm design result dr present prime observ made result applic includ shorthand notat pitch class lattic result dr compar sag kg translat possibl ani two free ji notat ani two version rcn sinc repres set ration number,['David Ryan'],['cs.SD'],False,False,False,False,False,True
886,2017-03-28T14:11:25Z,2016-12-04T03:36:51Z,http://arxiv.org/abs/1612.01058v1,http://arxiv.org/pdf/1612.01058v1,Algorithmic Songwriting with ALYSIA,algorithm songwrit alysia,"This paper introduces ALYSIA: Automated LYrical SongwrIting Application. ALYSIA is based on a machine learning model using Random Forests, and we discuss its success at pitch and rhythm prediction. Next, we show how ALYSIA was used to create original pop songs that were subsequently recorded and produced. Finally, we discuss our vision for the future of Automated Songwriting for both co-creative and autonomous systems.",paper introduc alysia autom lyric songwrit applic alysia base machin learn model use random forest discuss success pitch rhythm predict next show alysia use creat origin pop song subsequ record produc final discuss vision futur autom songwrit co creativ autonom system,"['Margareta Ackerman', 'David Loker']","['cs.AI', 'cs.LG', 'cs.MM', 'cs.SD']",False,False,False,False,False,True
889,2017-03-28T14:11:25Z,2016-12-01T08:31:23Z,http://arxiv.org/abs/1612.00172v1,http://arxiv.org/pdf/1612.00172v1,A Non Linear Approach towards Automated Emotion Analysis in Hindustani   Music,non linear approach toward autom emot analysi hindustani music,"In North Indian Classical Music, raga forms the basic structure over which individual improvisations is performed by an artist based on his/her creativity. The Alap is the opening section of a typical Hindustani Music (HM) performance, where the raga is introduced and the paths of its development are revealed using all the notes used in that particular raga and allowed transitions between them with proper distribution over time. In India, corresponding to each raga, several emotional flavors are listed, namely erotic love, pathetic, devotional, comic, horrific, repugnant, heroic, fantastic, furious, peaceful. The detection of emotional cues from Hindustani Classical music is a demanding task due to the inherent ambiguity present in the different ragas, which makes it difficult to identify any particular emotion from a certain raga. In this study we took the help of a high resolution mathematical microscope (MFDFA or Multifractal Detrended Fluctuation Analysis) to procure information about the inherent complexities and time series fluctuations that constitute an acoustic signal. With the help of this technique, 3 min alap portion of six conventional ragas of Hindustani classical music namely, Darbari Kanada, Yaman, Mian ki Malhar, Durga, Jay Jayanti and Hamswadhani played in three different musical instruments were analyzed. The results are discussed in detail.",north indian classic music raga form basic structur individu improvis perform artist base creativ alap open section typic hindustani music hm perform raga introduc path develop reveal use note use particular raga allow transit proper distribut time india correspond raga sever emot flavor list name erot love pathet devot comic horrif repugn heroic fantast furious peac detect emot cue hindustani classic music demand task due inher ambigu present differ raga make difficult identifi ani particular emot certain raga studi took help high resolut mathemat microscop mfdfa multifract detrend fluctuat analysi procur inform inher complex time seri fluctuat constitut acoust signal help techniqu min alap portion six convent raga hindustani classic music name darbari kanada yaman mian ki malhar durga jay jayanti hamswadhani play three differ music instrument analyz result discuss detail,"['Shankha Sanyal', 'Archi Banerjee', 'Tarit Guhathakurata', 'Ranjan Sengupta', 'Dipak Ghosh']","['cs.SD', 'nlin.CD']",False,False,False,False,False,True
890,2017-03-28T14:11:30Z,2016-12-01T08:25:45Z,http://arxiv.org/abs/1612.00171v1,http://arxiv.org/pdf/1612.00171v1,A Non Linear Multifractal Study to Illustrate the Evolution of Tagore   Songs Over a Century,non linear multifract studi illustr evolut tagor song centuri,The works of Rabindranath Tagore have been sung by various artistes over generations spanning over almost 100 years. there are few songs which were popular in the early years and have been able to retain their popularity over the years while some others have faded away. In this study we look to find cues for the singing style of these songs which have kept them alive for all these years. For this we took 3 min clip of four Tagore songs which have been sung by five generation of artistes over 100 years and analyze them with the help of latest nonlinear techniques Multifractal Detrended Fluctuation Analysis (MFDFA). The multifractal spectral width is a manifestation of the inherent complexity of the signal and may prove to be an important parameter to identify the singing style of particular generation of singers and how this style varies over different generations. The results are discussed in detail.,work rabindranath tagor sung various artist generat span almost year song popular earli year abl retain popular year fade away studi look find cue sing style song kept aliv year took min clip four tagor song sung five generat artist year analyz help latest nonlinear techniqu multifract detrend fluctuat analysi mfdfa multifract spectral width manifest inher complex signal may prove import paramet identifi sing style particular generat singer style vari differ generat result discuss detail,"['Shankha Sanyal', 'Archi Banerjee', 'Tarit Guhathakurata', 'Ranjan Sengupta', 'Dipak Ghosh']","['cs.SD', 'nlin.CD']",False,False,True,False,False,True
892,2017-03-28T14:11:30Z,2016-11-29T17:19:45Z,http://arxiv.org/abs/1611.09733v1,http://arxiv.org/abs/1611.09733v1,Getting Closer to the Essence of Music: The Con Espressione Manifesto,get closer essenc music con espression manifesto,"This text offers a personal and very subjective view on the current situation of Music Information Research (MIR). Motivated by the desire to build systems with a somewhat deeper understanding of music than the ones we currently have, I try to sketch a number of challenges for the next decade of MIR research, grouped around six simple truths about music that are probably generally agreed on, but often ignored in everyday research.",text offer person veri subject view current situat music inform research mir motiv desir build system somewhat deeper understand music one current tri sketch number challeng next decad mir research group around six simpl truth music probabl general agre often ignor everyday research,['Gerhard Widmer'],['cs.SD'],False,False,True,False,False,True
898,2017-03-28T14:11:30Z,2016-11-27T20:29:53Z,http://arxiv.org/abs/1611.08905v1,http://arxiv.org/pdf/1611.08905v1,SISO and SIMO Accompaniment Cancellation for Live Solo Recordings Based   on Short-Time ERB-Band Wiener Filtering and Spectral Subtraction,siso simo accompani cancel live solo record base short time erb band wiener filter spectral subtract,"Research in collaborative music learning is subject to unresolved problems demanding new technological solutions. One such problem poses the suppression of the accompaniment in a live recording of a performance during practice, which can be for the purposes of self-assessment or further machine-aided analysis. Being able to separate a solo from the accompaniment allows to create learning agents that may act as personal tutors and help the apprentice improve his or her technique. First, we start from the classical adaptive noise cancelling approach, and adjust it to the problem at hand. In a second step, we compare some adaptive and Wiener filtering approaches and assess their performances on the task. Our findings underpin that adaptive filtering is inapt of dealing with music signals and that Wiener filtering in the short-time Fourier transform domain is a much more effective approach. In addition, it is very cheap if carried out in the frequency bands of auditory filters. A double-output extension based on maximal-ratio combining is also proposed.",research collabor music learn subject unresolv problem demand new technolog solut one problem pose suppress accompani live record perform dure practic purpos self assess machin aid analysi abl separ solo accompani allow creat learn agent may act person tutor help apprentic improv techniqu first start classic adapt nois cancel approach adjust problem hand second step compar adapt wiener filter approach assess perform task find underpin adapt filter inapt deal music signal wiener filter short time fourier transform domain much effect approach addit veri cheap carri frequenc band auditori filter doubl output extens base maxim ratio combin also propos,"['Stanislaw Gorlow', 'Mathieu Ramona', 'François Pachet']",['cs.SD'],False,False,False,False,False,True
900,2017-03-28T14:02:27Z,2017-03-27T15:36:40Z,http://arxiv.org/abs/1703.09147v1,http://arxiv.org/pdf/1703.09147v1,Two-part models with stochastic processes for modelling longitudinal   semicontinuous data: computationally efficient inference and modelling the   overall marginal mean,two part model stochast process model longitudin semicontinu data comput effici infer model overal margin mean,"Several researchers have described two-part models with patient-specific stochastic processes for analysing longitudinal semicontinuous data. In theory, such models can offer greater flexibility than the standard two-part model with patient-specific random effects. However, in practice the high dimensional integrations involved in the marginal likelihood (i.e. integrated over the stochastic processes) significantly complicates model fitting. Thus non-standard computationally intensive procedures based on simulating the marginal likelihood have so far only been proposed. In this paper, we describe an efficient method of implementation by demonstrating how the high dimensional integrations involved in the marginal likelihood can be computed efficiently. Specifically, by using a property of the multivariate normal distribution and the standard marginal cumulative distribution function identity, we transform the marginal likelihood so that the high dimensional integrations are contained in the cumulative distribution function of a multivariate normal distribution, which can then be efficiently evaluated. Hence maximum likelihood estimation can be used to obtain parameter estimates and asymptotic standard errors (from the observed information matrix) of model parameters. We describe our proposed efficient implementation procedure for the standard two-part model parameterisation and when it is of interest to directly model the overall marginal mean. The methodology is applied on a psoriatic arthritis data set concerning functional disability.",sever research describ two part model patient specif stochast process analys longitudin semicontinu data theori model offer greater flexibl standard two part model patient specif random effect howev practic high dimension integr involv margin likelihood integr stochast process signific complic model fit thus non standard comput intens procedur base simul margin likelihood far onli propos paper describ effici method implement demonstr high dimension integr involv margin likelihood comput effici specif use properti multivari normal distribut standard margin cumul distribut function ident transform margin likelihood high dimension integr contain cumul distribut function multivari normal distribut effici evalu henc maximum likelihood estim use obtain paramet estim asymptot standard error observ inform matrix model paramet describ propos effici implement procedur standard two part model parameteris interest direct model overal margin mean methodolog appli psoriat arthriti data set concern function disabl,"['Sean Yiu', 'Brian Tom']",['stat.AP'],False,False,False,False,False,True
901,2017-03-28T14:02:27Z,2017-03-27T11:04:00Z,http://arxiv.org/abs/1703.09007v1,http://arxiv.org/pdf/1703.09007v1,Detection of Spatiotemporally Coherent Rainfall Anomalies Using Markov   Random Fields,detect spatiotempor coher rainfal anomali use markov random field,"Precipitation is a large-scale, spatio-temporally heterogeneous phenomenon, with frequent anomalies exhibiting unusually high or low values. We use Markov Random Fields (MRFs) to detect spatio-temporally coherent anomalies in gridded annual rainfall data across India from 1901-2005. MRFs are undirected graphical models where each node is associated with a \{location,year\} pair, with edges connecting nodes representing adjacent locations or years. Some nodes represent observations of precipitation, while the rest represent unobserved (\emph{latent}) states that can take one of three values: high/low/normal. The MRF represents a probability distribution over the variables, using \emph{node potential} and \emph{edge potential} functions defined on nodes and edges of the graph. Optimal values of latent state variables are estimated by maximizing the posterior probability of the observations, using Gibbs sampling. Edge potentials enforce spatial and temporal coherence, and node potentials influence threshold for anomalies by affecting the prior probabilities of the states. The model can be tuned to recover anomalies detected by threshold-based methods. The competing influences of spatial and temporal coherence can be adjusted through edge potentials.   We study spatio-temporal properties of rainfall anomalies discovered by this method, using suitable measures. We identify nonstationarities in occurrence of positive and negative anomalies between the first and second halves of the 20th century. We find that between these periods, there has been decrease in rainfall during June-September (JJAS) and an increase during other months. These effects are highlighted prominently in the statistics of anomalies. Properties of anomalies learnt from this approach could present tests of regional-scale rainfall simulations by climate models and statistical simulators.",precipit larg scale spatio tempor heterogen phenomenon frequent anomali exhibit unusu high low valu use markov random field mrfs detect spatio tempor coher anomali grid annual rainfal data across india mrfs undirect graphic model node associ locat year pair edg connect node repres adjac locat year node repres observ precipit rest repres unobserv emph latent state take one three valu high low normal mrf repres probabl distribut variabl use emph node potenti emph edg potenti function defin node edg graph optim valu latent state variabl estim maxim posterior probabl observ use gibb sampl edg potenti enforc spatial tempor coher node potenti influenc threshold anomali affect prior probabl state model tune recov anomali detect threshold base method compet influenc spatial tempor coher adjust edg potenti studi spatio tempor properti rainfal anomali discov method use suitabl measur identifi nonstationar occurr posit negat anomali first second halv th centuri find period decreas rainfal dure june septemb jjas increas dure month effect highlight promin statist anomali properti anomali learnt approach could present test region scale rainfal simul climat model statist simul,"['Adway Mitra', 'Ashwin K. Seshadri']",['stat.AP'],False,False,False,False,False,True
902,2017-03-28T14:02:27Z,2017-03-27T10:14:54Z,http://arxiv.org/abs/1703.08994v1,http://arxiv.org/pdf/1703.08994v1,Value of Information: Sensitivity Analysis and Research Design in   Bayesian Evidence Synthesis,valu inform sensit analysi research design bayesian evid synthesi,"Suppose we have a Bayesian model which combines evidence from several different sources. We want to know which model parameters most affect the estimate or decision from the model, or which of the parameter uncertainties drive the decision uncertainty. Furthermore we want to prioritise what further data should be collected. These questions can be addressed by Value of Information (VoI) analysis, in which we estimate expected reductions in loss from learning specific parameters or collecting data of a given design. We describe the theory and practice of VoI for Bayesian evidence synthesis, using and extending ideas from health economics, computer modelling and Bayesian design. The methods are general to a range of decision problems including point estimation and choices between discrete actions. We apply them to a model for estimating prevalence of HIV infection, combining indirect information from several surveys, registers and expert beliefs. This analysis shows which parameters contribute most of the uncertainty about each prevalence estimate, and provides the expected improvements in precision from collecting specific amounts of additional data.",suppos bayesian model combin evid sever differ sourc want know model paramet affect estim decis model paramet uncertainti drive decis uncertainti furthermor want prioritis data collect question address valu inform voi analysi estim expect reduct loss learn specif paramet collect data given design describ theori practic voi bayesian evid synthesi use extend idea health econom comput model bayesian design method general rang decis problem includ point estim choic discret action appli model estim preval hiv infect combin indirect inform sever survey regist expert belief analysi show paramet contribut uncertainti preval estim provid expect improv precis collect specif amount addit data,"['Christopher Jackson', 'Anne Presanis', 'Stefano Conti', 'Daniela De Angelis']",['stat.AP'],False,False,True,False,False,True
905,2017-03-28T14:02:27Z,2017-03-25T17:57:31Z,http://arxiv.org/abs/1703.08723v1,http://arxiv.org/pdf/1703.08723v1,Extending Growth Mixture Models Using Continuous Non-Elliptical   Distributions,extend growth mixtur model use continu non ellipt distribut,"Growth mixture models (GMMs) incorporate both conventional random effects growth modeling and latent trajectory classes as in finite mixture modeling; therefore, they offer a way to handle the unobserved heterogeneity between subjects in their development. GMMs with Gaussian random effects dominate the literature. When the data are asymmetric and/or have heavier tails, more than one latent class is required to capture the observed variable distribution. Therefore, a GMM with continuous non-elliptical distributions is proposed to capture skewness and heavier tails in the data set. Specifically, multivariate skew-t distributions and generalized hyperbolic distributions are introduced to extend GMMs. When extending GMMs, four statistical models are considered with differing distributions of measurement errors and random effects. The mathematical development of a GMM with non-elliptical distributions relies on its relationship with the generalized inverse Gaussian distribution. Parameter estimation is outlined within the expectation-maximization framework before the performance of our GMM with non-elliptical distributions is illustrated on simulated and real data.",growth mixtur model gmms incorpor convent random effect growth model latent trajectori class finit mixtur model therefor offer way handl unobserv heterogen subject develop gmms gaussian random effect domin literatur data asymmetr heavier tail one latent class requir captur observ variabl distribut therefor gmm continu non ellipt distribut propos captur skew heavier tail data set specif multivari skew distribut general hyperbol distribut introduc extend gmms extend gmms four statist model consid differ distribut measur error random effect mathemat develop gmm non ellipt distribut reli relationship general invers gaussian distribut paramet estim outlin within expect maxim framework befor perform gmm non ellipt distribut illustr simul real data,"['Yuhong Wei', 'Emilie Shireman', 'Paul D. McNicholas', 'Douglas L. Steinley']","['stat.ME', 'stat.AP', 'stat.CO']",False,False,False,False,False,True
906,2017-03-28T14:02:27Z,2017-03-25T03:44:05Z,http://arxiv.org/abs/1703.08644v1,http://arxiv.org/pdf/1703.08644v1,Exact Spike Train Inference Via $\ell_0$ Optimization,exact spike train infer via ell optim,"In recent years, new technologies in neuroscience have made it possible to measure the activities of large numbers of neurons in behaving animals. For each neuron, a fluorescence trace is measured; this can be seen as a first-order approximation of the neuron's activity over time. Determining the exact time at which a neuron spikes on the basis of its fluorescence trace is an important open problem in the field of computational neuroscience.   Recently, a convex optimization problem involving an $\ell_1$ penalty was proposed for this task. In this paper, we slightly modify that recent proposal by replacing the $\ell_1$ penalty with an $\ell_0$ penalty. In stark contrast to the conventional wisdom that $\ell_0$ optimization problems are computationally intractable, we show that the resulting optimization problem can be efficiently solved for the global optimum using an extremely simple and efficient dynamic programming algorithm. Our R-language implementation of the proposed algorithm runs in a few minutes on fluorescence traces of $100,000$ timesteps. Furthermore, our proposal leads to substantially better results than the previous $\ell_1$ proposal, on synthetic data as well as on two calcium imaging data sets.   R-language software for our proposal is now available at https://github.com/jewellsean/LZeroSpikeInference and will be available soon on CRAN in the package LZeroSpikeInference.",recent year new technolog neurosci made possibl measur activ larg number neuron behav anim neuron fluoresc trace measur seen first order approxim neuron activ time determin exact time neuron spike basi fluoresc trace import open problem field comput neurosci recent convex optim problem involv ell penalti propos task paper slight modifi recent propos replac ell penalti ell penalti stark contrast convent wisdom ell optim problem comput intract show result optim problem effici solv global optimum use extrem simpl effici dynam program algorithm languag implement propos algorithm run minut fluoresc trace timestep furthermor propos lead substanti better result previous ell propos synthet data well two calcium imag data set languag softwar propos avail https github com jewellsean lzerospikeinfer avail soon cran packag lzerospikeinfer,"['Sean Jewell', 'Daniela Witten']",['stat.AP'],False,False,False,False,False,True
909,2017-03-28T14:02:27Z,2017-03-24T14:40:45Z,http://arxiv.org/abs/1703.08429v1,http://arxiv.org/pdf/1703.08429v1,Modeling and Estimation for Self-Exciting Spatio-Temporal Models of   Terrorist Activity,model estim self excit spatio tempor model terrorist activ,"Spatio-temporal hierarchical modeling is an extremely attractive way to model the spread of crime or terrorism data over a given region, especially when the observations are counts and must be modeled discretely. The spatio-temporal diffusion is placed, as a matter of convenience, in the process model allowing for straightforward estimation of the diffusion parameters through Bayesian techniques. However, this method of modeling does not allow for the existence of self-excitation, or a temporal data model dependency, that has been shown to exist in criminal and terrorism data. In this manuscript we will use existing theories on how violence spreads to create models that allow for both spatio-temporal diffusion in the process model as well as temporal diffusion, or self-excitation, in the data model. We will further demonstrate how Laplace approximations similar to their use in Integrated Nested Laplace Approximation can be used to quickly and accurately conduct inference of self-exciting spatio-temporal models allowing practitioners a new way of fitting and comparing multiple process models. We will illustrate this approach by fitting a self-exciting spatio-temporal model to terrorism data in Iraq and demonstrate how choice of process model leads to differing conclusions on the existence of self-excitation in the data and differing conclusions on how violence is spreading spatio-temporally.",spatio tempor hierarch model extrem attract way model spread crime terror data given region especi observ count must model discret spatio tempor diffus place matter conveni process model allow straightforward estim diffus paramet bayesian techniqu howev method model doe allow exist self excit tempor data model depend shown exist crimin terror data manuscript use exist theori violenc spread creat model allow spatio tempor diffus process model well tempor diffus self excit data model demonstr laplac approxim similar use integr nest laplac approxim use quick accur conduct infer self excit spatio tempor model allow practition new way fit compar multipl process model illustr approach fit self excit spatio tempor model terror data iraq demonstr choic process model lead differ conclus exist self excit data differ conclus violenc spread spatio tempor,"['Nicholas J. Clark', 'Philip M. Dixon']",['stat.AP'],False,False,False,False,False,True
911,2017-03-28T14:02:31Z,2017-03-23T15:38:17Z,http://arxiv.org/abs/1703.08111v1,http://arxiv.org/pdf/1703.08111v1,Alternating optimization for GxE modelling with weighted genetic and   environmental scores: examples from the MAVAN study,altern optim gxe model weight genet environment score exampl mavan studi,"Motivated by the goal of expanding currently existing genotype x environment interaction (GxE) models to simultaneously include multiple genetic variants and environmental exposures in a parsimonious way, we developed a novel method to estimate the parameters in a GxE model, where G is a weighted sum of genetic variants (genetic score) and E is a weighted sum of environments (environmental score). The approach uses alternating optimization to estimate the parameters of the GxE model. This is an iterative process where the genetic score weights, the environmental score weights, and the main model parameters are estimated in turn assuming the other parameters to be constant. This technique can be used to construct relatively complex interaction models that are constrained to a particular structure, and hence contain fewer parameters.   We present the model as a two-way interaction longitudinal mixed model, for which ordinary linear regression is a special case, but it can easily be extended to be compatible with k-way interaction models and generalized linear mixed models. The model is implemented in R (LEGIT package) and using SAS macros (LEGIT_SAS). Here we present examples from the Maternal Adversity, Vulnerability, and Neurodevelopment (MAVAN) study where we improve significantly upon already existing models using alternating optimization. Furthermore, through simulations, we demonstrate the power and validity of this approach even with small sample sizes.",motiv goal expand current exist genotyp environ interact gxe model simultan includ multipl genet variant environment exposur parsimoni way develop novel method estim paramet gxe model weight sum genet variant genet score weight sum environ environment score approach use altern optim estim paramet gxe model iter process genet score weight environment score weight main model paramet estim turn assum paramet constant techniqu use construct relat complex interact model constrain particular structur henc contain fewer paramet present model two way interact longitudin mix model ordinari linear regress special case easili extend compat way interact model general linear mix model model implement legit packag use sas macro legit sas present exampl matern advers vulner neurodevelop mavan studi improv signific upon alreadi exist model use altern optim furthermor simul demonstr power valid approach even small sampl size,"['Alexia Jolicoeur-Martineau', 'Ashley Wazana', 'Eszter Székely', 'Meir Steiner', 'Alison S. Fleming', 'James L. Kennedy', 'Michael J. Meaney', 'Celia M. T. Greenwood']",['stat.AP'],False,False,False,False,False,True
912,2017-03-28T14:02:31Z,2017-03-23T13:53:06Z,http://arxiv.org/abs/1703.08071v1,http://arxiv.org/pdf/1703.08071v1,Quantifying and suppressing ranking bias in a large citation network,quantifi suppress rank bias larg citat network,"It is widely recognized that citation counts for papers from different fields cannot be directly compared because different scientific fields adopt different citation practices. Citation counts are also strongly biased by paper age since older papers had more time to attract citations. Various procedures aim at suppressing these biases and give rise to new normalized indicators, such as the relative citation count. We use a large citation dataset from Microsoft Academic Graph and a new statistical framework based on the Mahalanobis distance to show that the rankings by well known indicators, including the relative citation count and Google's PageRank score, are significantly biased by paper field and age. We propose a general normalization procedure motivated by the $z$-score which produces much less biased rankings when applied to citation count and PageRank score.",wide recogn citat count paper differ field cannot direct compar becaus differ scientif field adopt differ citat practic citat count also strong bias paper age sinc older paper time attract citat various procedur aim suppress bias give rise new normal indic relat citat count use larg citat dataset microsoft academ graph new statist framework base mahalanobi distanc show rank well known indic includ relat citat count googl pagerank score signific bias paper field age propos general normal procedur motiv score produc much less bias rank appli citat count pagerank score,"['Giacomo Vaccario', 'Matus Medo', 'Nicolas Wider', 'Manuel Sebastian Mariani']","['physics.soc-ph', 'cs.DL', 'cs.IR', 'physics.data-an', 'stat.AP']",False,False,True,False,False,True
913,2017-03-28T14:02:31Z,2017-03-21T19:53:59Z,http://arxiv.org/abs/1703.07408v1,http://arxiv.org/pdf/1703.07408v1,Maximum a posteriori estimation through simulated annealing for binary   asteroid orbit determination,maximum posteriori estim simul anneal binari asteroid orbit determin,"This paper considers a new method for the binary asteroid orbit determination problem. The method is based on the Bayesian approach with a global optimisation algorithm. The orbital parameters to be determined are modelled through a posteriori, including a priori and likelihood terms. The first term constrains the parameters search space. It allows to introduce knowledge about orbit, if such information is available, but at the same time it does not require a good initial estimation of parameters. The second term is based on given observations, besides it allows us to use and to compare different observational error models. Ones the a posteriori model is build the estimator of the orbital parameters is computed using a global optimisation procedure: the simulated annealing algorithm. The new method was implemented for simulated and real observations, having received successful result, and also verified for ephemeris prediction capability. The new approach can prove useful in case of small numbers of observations and/or in case of non-Gaussian observational errors, when the classical least-squares method can not be applied.",paper consid new method binari asteroid orbit determin problem method base bayesian approach global optimis algorithm orbit paramet determin model posteriori includ priori likelihood term first term constrain paramet search space allow introduc knowledg orbit inform avail time doe requir good initi estim paramet second term base given observ besid allow us use compar differ observ error model one posteriori model build estim orbit paramet comput use global optimis procedur simul anneal algorithm new method implement simul real observ receiv success result also verifi ephemeri predict capabl new approach prove use case small number observ case non gaussian observ error classic least squar method appli,"['Irina D. Kovalenko', 'Radu S. Stoica', 'Nikolay V. Emelyanov']","['astro-ph.IM', 'astro-ph.EP', 'stat.AP']",False,False,True,False,False,True
915,2017-03-28T14:02:31Z,2017-03-21T16:48:50Z,http://arxiv.org/abs/1703.07309v1,http://arxiv.org/pdf/1703.07309v1,Phytoplankton Hotspot Prediction With an Unsupervised Spatial Community   Model,phytoplankton hotspot predict unsupervis spatial communiti model,"Many interesting natural phenomena are sparsely distributed and discrete. Locating the hotspots of such sparsely distributed phenomena is often difficult because their density gradient is likely to be very noisy. We present a novel approach to this search problem, where we model the co-occurrence relations between a robot's observations with a Bayesian nonparametric topic model. This approach makes it possible to produce a robust estimate of the spatial distribution of the target, even in the absence of direct target observations. We apply the proposed approach to the problem of finding the spatial locations of the hotspots of a specific phytoplankton taxon in the ocean. We use classified image data from Imaging FlowCytobot (IFCB), which automatically measures individual microscopic cells and colonies of cells. Given these individual taxon-specific observations, we learn a phytoplankton community model that characterizes the co-occurrence relations between taxa. We present experiments with simulated robot missions drawn from real observation data collected during a research cruise traversing the US Atlantic coast. Our results show that the proposed approach outperforms nearest neighbor and k-means based methods for predicting the spatial distribution of hotspots from in-situ observations.",mani interest natur phenomena spars distribut discret locat hotspot spars distribut phenomena often difficult becaus densiti gradient like veri noisi present novel approach search problem model co occurr relat robot observ bayesian nonparametr topic model approach make possibl produc robust estim spatial distribut target even absenc direct target observ appli propos approach problem find spatial locat hotspot specif phytoplankton taxon ocean use classifi imag data imag flowcytobot ifcb automat measur individu microscop cell coloni cell given individu taxon specif observ learn phytoplankton communiti model character co occurr relat taxa present experi simul robot mission drawn real observ data collect dure research cruis travers us atlant coast result show propos approach outperform nearest neighbor mean base method predict spatial distribut hotspot situ observ,"['Arnold Kalmbach', 'Yogesh Girdhar', 'Heidi M. Sosik', 'Gregory Dudek']","['cs.RO', 'stat.AP']",False,False,False,False,False,True
916,2017-03-28T14:02:31Z,2017-03-21T15:02:45Z,http://arxiv.org/abs/1703.07256v1,http://arxiv.org/pdf/1703.07256v1,Statistical Topology and the Random Interstellar Medium,statist topolog random interstellar medium,"Current astrophysical models of the interstellar medium assume that small scale variation and noise can be modelled as Gaussian random fields or simple transformations thereof, such as lognormal. We use topological methods to investigate this assumption for three regions of the southern sky. We consider Gaussian random fields on two-dimensional lattices and investigate the expected distribution of topological structures quantified through Betti numbers. We demonstrate that there are circumstances where differences in topology can identify differences in distributions when conventional marginal or correlation analyses may not. We propose a non-parametric method for comparing two fields based on the counts of topological features and the geometry of the associated persistence diagrams. When we apply the methods to the astrophysical data, we find strong evidence against a Gaussian random field model for each of the three regions of the interstellar medium that we consider. Further, we show that there are topological differences at a local scale between these different regions.",current astrophys model interstellar medium assum small scale variat nois model gaussian random field simpl transform thereof lognorm use topolog method investig assumpt three region southern sky consid gaussian random field two dimension lattic investig expect distribut topolog structur quantifi betti number demonstr circumst differ topolog identifi differ distribut convent margin correl analys may propos non parametr method compar two field base count topolog featur geometri associ persist diagram appli method astrophys data find strong evid gaussian random field model three region interstellar medium consid show topolog differ local scale differ region,"['Robin Henderson', 'Irina Makarenko', 'Paul Bushby', 'Andrew Fletcher', 'Anvar Shukurov']","['stat.AP', 'astro-ph.GA']",False,False,True,False,False,True
917,2017-03-28T14:02:31Z,2017-03-21T10:45:07Z,http://arxiv.org/abs/1703.07137v1,http://arxiv.org/pdf/1703.07137v1,MRI-based Surgical Planning for Lumbar Spinal Stenosis,mri base surgic plan lumbar spinal stenosi,"The most common reason for spinal surgery in elderly patients is lumbar spinal stenosis(LSS). For LSS, treatment decisions based on clinical and radiological information as well as personal experience of the surgeon shows large variance. Thus a standardized support system is of high value for a more objective and reproducible decision. In this work, we develop an automated algorithm to localize the stenosis causing the symptoms of the patient in magnetic resonance imaging (MRI). With 22 MRI features of each of five spinal levels of 321 patients, we show it is possible to predict the location of lesion triggering the symptoms. To support this hypothesis, we conduct an automated analysis of labeled and unlabeled MRI scans extracted from 788 patients. We confirm quantitatively the importance of radiological information and provide an algorithmic pipeline for working with raw MRI scans.",common reason spinal surgeri elder patient lumbar spinal stenosi lss lss treatment decis base clinic radiolog inform well person experi surgeon show larg varianc thus standard support system high valu object reproduc decis work develop autom algorithm local stenosi caus symptom patient magnet reson imag mri mri featur five spinal level patient show possibl predict locat lesion trigger symptom support hypothesi conduct autom analysi label unlabel mri scan extract patient confirm quantit import radiolog inform provid algorithm pipelin work raw mri scan,"['Gabriele Abbati', 'Stefan Bauer', 'Peter J. Schüffler', 'Jakob Burgstaller', 'Ulrike Held', 'Sebastian Winklhofer', 'Johann Steurer', 'Joachim M. Buhmann']",['stat.AP'],False,False,False,False,False,True
919,2017-03-28T14:02:31Z,2017-03-20T20:26:10Z,http://arxiv.org/abs/1703.06957v1,http://arxiv.org/pdf/1703.06957v1,Nuisance parameter based sample size re-estimation incorporating prior   information,nuisanc paramet base sampl size estim incorpor prior inform,"Prior information is often incorporated informally when planning a clinical trial. Here, we present an approach on how to incorporate prior information, such as data from historical clinical trials, into the nuisance parameter based sample size re-estimation in a design with an internal pilot study. We focus on trials with continuous endpoints in which the outcome variance is the nuisance parameter. For planning and analyzing the trial frequentist methods are considered. Moreover, the external information on the variance are summarized by the Bayesian meta-analytic-predictive approach. To incorporate external information into the sample size re-estimation, we propose to update the MAP prior based on the results of the internal pilot study and to re-estimate the sample size using a Bayes estimator from the posterior. By means of a simulation study, we compare the operating characteristics such as power and sample size distribution of the proposed procedure with the traditional sample size re-estimation approach which uses the pooled variance estimator. The simulation study shows that, if no prior data conflict is present, incorporating external information into the sample size re-estimation improves the operating characteristics compared to the traditional approach. In the case of a prior data conflict, that is when the variance of the ongoing clinical trial is unequal to the prior location, the performance of the traditional sample size re-estimation procedure is in general superior, even when the prior information is robustified. When considering to include prior information in sample size re-estimation, the potential gains should be balanced against the risks.",prior inform often incorpor inform plan clinic trial present approach incorpor prior inform data histor clinic trial nuisanc paramet base sampl size estim design intern pilot studi focus trial continu endpoint outcom varianc nuisanc paramet plan analyz trial frequentist method consid moreov extern inform varianc summar bayesian meta analyt predict approach incorpor extern inform sampl size estim propos updat map prior base result intern pilot studi estim sampl size use bay estim posterior mean simul studi compar oper characterist power sampl size distribut propos procedur tradit sampl size estim approach use pool varianc estim simul studi show prior data conflict present incorpor extern inform sampl size estim improv oper characterist compar tradit approach case prior data conflict varianc ongo clinic trial unequ prior locat perform tradit sampl size estim procedur general superior even prior inform robustifi consid includ prior inform sampl size estim potenti gain balanc risk,"['Tobias Mütze', 'Heinz Schmidli', 'Tim Friede']",['stat.AP'],False,False,False,False,False,True
921,2017-03-28T14:02:35Z,2017-03-20T15:45:44Z,http://arxiv.org/abs/1703.06808v1,http://arxiv.org/pdf/1703.06808v1,Worth Weighting? How to Think About and Use Sample Weights in Survey   Experiments,worth weight think use sampl weight survey experi,"The popularity of online surveys has increased the prominence of sampling weights in claims of representativeness. Yet, much uncertainty remains regarding how these weights should be employed in the analysis of survey experiments: Should they be used or ignored? If they are used, which estimators are preferred? We offer practical advice, rooted in the Neyman-Rubin model, for researchers producing and working with survey experimental data. We examine simple, efficient estimators (Horvitz-Thompson, H\`ajek, ""double-H\`ajek"", and post-stratification) for analyzing these data, along with formulae for biases and variances. We provide simulations that examine these estimators and real examples from experiments administered online through YouGov. We find that for examining the existence of population treatment effects using high-quality, broadly representative samples recruited by top online survey firms, sample quantities, which do not rely on weights, are often sufficient. Sample Average Treatment Effect (SATE) estimates are unlikely to differ substantially from weighted estimates, and they avoid the statistical power loss that accompanies weighting. When precise estimates of Population Average Treatment Effects (PATE) are essential, we analytically show post-stratifying on survey weights and/or covariates highly correlated with the outcome to be a conservative choice.",popular onlin survey increas promin sampl weight claim repres yet much uncertainti remain regard weight employ analysi survey experi use ignor use estim prefer offer practic advic root neyman rubin model research produc work survey experiment data examin simpl effici estim horvitz thompson ajek doubl ajek post stratif analyz data along formula bias varianc provid simul examin estim real exampl experi administ onlin yougov find examin exist popul treatment effect use high qualiti broad repres sampl recruit top onlin survey firm sampl quantiti reli weight often suffici sampl averag treatment effect sate estim unlik differ substanti weight estim avoid statist power loss accompani weight precis estim popul averag treatment effect pate essenti analyt show post stratifi survey weight covari high correl outcom conserv choic,"['Luke W. Miratrix', 'Jasjeet S. Sekhon', 'Alexander G. Theodoridis', 'Luis F. Campos']","['stat.ME', 'stat.AP']",False,False,False,False,False,True
930,2017-03-28T14:02:39Z,2017-03-21T09:36:42Z,http://arxiv.org/abs/1703.05926v2,http://arxiv.org/pdf/1703.05926v2,Quantifying the causal effect of speed cameras on road traffic accidents   via an approximate Bayesian doubly robust estimator,quantifi causal effect speed camera road traffic accid via approxim bayesian doubli robust estim,"This paper develops an approximate Bayesian doubly-robust (DR) estimation method to quantify the causal effect of speed cameras on road traffic accidents. Previous empirical work on this topic, which shows a diverse range of estimated effects, is based largely on outcome regression (OR) models using the Empirical Bayes approach or on simple before and after comparisons. Issues of causality and confounding have received little formal attention. A causal DR approach combines propensity score (PS) and OR models to give an average treatment effect (ATE) estimator that is consistent and asymptotically normal under correct specification of either of the two component models. We develop this approach within a novel approximate Bayesian framework to derive posterior predictive distributions for the ATE of speed cameras on road traffic accidents. Our results for England indicate significant reductions in the number of accidents at speed cameras sites (mean ATE = -30%). Our proposed method offers a promising approach for evaluation of transport safety interventions.",paper develop approxim bayesian doubli robust dr estim method quantifi causal effect speed camera road traffic accid previous empir work topic show divers rang estim effect base larg outcom regress model use empir bay approach simpl befor comparison issu causal confound receiv littl formal attent causal dr approach combin propens score ps model give averag treatment effect ate estim consist asymptot normal correct specif either two compon model develop approach within novel approxim bayesian framework deriv posterior predict distribut ate speed camera road traffic accid result england indic signific reduct number accid speed camera site mean ate propos method offer promis approach evalu transport safeti intervent,"['Daniel J Graham', 'Cian Naik', 'Emma J McCoy']",['stat.AP'],False,False,False,False,False,True
931,2017-03-28T14:02:39Z,2017-03-16T19:11:58Z,http://arxiv.org/abs/1703.05799v1,http://arxiv.org/pdf/1703.05799v1,A new sample-based algorithms to compute the total sensitivity index,new sampl base algorithm comput total sensit index,"Variance based sensitivity indices have established themselves as a reference among practitioners of sensitivity analysis of model output. It is not unusual to consider a variance based sensitivity analysis as informative if it produces at least the first order sensitivity indices Sj and the so-called total-effect sensitivity indices STj or Tj for all the uncertain factors of the mathematical model under analysis. Computational economy is critical in sensitivity analysis. It depends mostly upon the number of model evaluations needed to obtain stable values of the estimates. While for the first order indices efficient estimation procedures are available which are independent from the number of factors under analysis, this is less the case for the total sensitivity indices. When estimating the Tj one can either use a sample based approach, whose computational cost depends from the number of factors, or approaches based on meta-modelling / emulators, e.g. based on Gaussian processes. The present work focuses on sample-based estimation procedures for Tj, and tries different avenues to achieve an algorithmic improvement over the designs proposed in the existing best practices. One among the selected procedures appear to lead to a considerable improvement when the mean absolute error is considered.",varianc base sensit indic establish themselv refer among practition sensit analysi model output unusu consid varianc base sensit analysi inform produc least first order sensit indic sj call total effect sensit indic stj tj uncertain factor mathemat model analysi comput economi critic sensit analysi depend upon number model evalu need obtain stabl valu estim first order indic effici estim procedur avail independ number factor analysi less case total sensit indic estim tj one either use sampl base approach whose comput cost depend number factor approach base meta model emul base gaussian process present work focus sampl base estim procedur tj tri differ avenu achiev algorithm improv design propos exist best practic one among select procedur appear lead consider improv mean absolut error consid,"['Andrea Saltelli', 'Daniel Albrecht', 'Stefano Tarantola', 'Federico Ferretti']","['stat.AP', '00A71', 'G.3; G.4']",False,False,False,False,False,True
933,2017-03-28T14:02:39Z,2017-03-16T10:06:45Z,http://arxiv.org/abs/1703.05545v1,http://arxiv.org/pdf/1703.05545v1,The nature and origin of heavy tails in retweet activity,natur origin heavi tail retweet activ,"Modern social media platforms facilitate the rapid spread of information online. Modelling phenomena such as social contagion and information diffusion are contingent upon a detailed understanding of the information-sharing processes. In Twitter, an important aspect of this occurs with retweets, where users rebroadcast the tweets of other users. To improve our understanding of how these distributions arise, we analyse the distribution of retweet times. We show that a power law with exponential cutoff provides a better fit than the power laws previously suggested. We explain this fit through the burstiness of human behaviour and the priorities individuals place on different tasks.",modern social media platform facilit rapid spread inform onlin model phenomena social contagion inform diffus conting upon detail understand inform share process twitter import aspect occur retweet user rebroadcast tweet user improv understand distribut aris analys distribut retweet time show power law exponenti cutoff provid better fit power law previous suggest explain fit bursti human behaviour prioriti individu place differ task,"['Peter Mathews', 'Lewis Mitchell', 'Giang T. Nguyen', 'Nigel G. Bean']","['physics.soc-ph', 'cs.SI', 'stat.AP']",False,False,True,False,False,True
935,2017-03-28T14:02:39Z,2017-03-16T08:28:11Z,http://arxiv.org/abs/1703.05502v1,http://arxiv.org/pdf/1703.05502v1,Steganographic Generative Adversarial Networks,steganograph generat adversari network,"Steganography is collection of methods to hide secret information (""payload"") within non-secret information (""container""). Its counterpart, Steganalysis, is the practice of determining if a message contains a hidden payload, and recovering it if possible. Presence of hidden payloads is typically detected by a binary classifier. In the present study, we propose a new model for generating image-like containers based on Deep Convolutional Generative Adversarial Networks (DCGAN). This approach allows to generate more setganalysis-secure message embedding using standard steganography algorithms. Experiment results demonstrate that the new model successfully deceives the steganography analyzer, and for this reason, can be used in steganographic applications.",steganographi collect method hide secret inform payload within non secret inform contain counterpart steganalysi practic determin messag contain hidden payload recov possibl presenc hidden payload typic detect binari classifi present studi propos new model generat imag like contain base deep convolut generat adversari network dcgan approach allow generat setganalysi secur messag embed use standard steganographi algorithm experi result demonstr new model success deceiv steganographi analyz reason use steganograph applic,"['Denis Volkhonskiy', 'Ivan Nazarov', 'Boris Borisenko', 'Evgeny Burnaev']","['cs.MM', 'cs.CR', 'cs.CV', 'stat.AP']",False,False,False,False,False,True
938,2017-03-28T14:02:39Z,2017-03-15T14:22:09Z,http://arxiv.org/abs/1703.05172v1,http://arxiv.org/pdf/1703.05172v1,Bayesian adaptive bandit-based designs using the Gittins index for   multi-armed trials with normally distributed endpoints,bayesian adapt bandit base design use gittin index multi arm trial normal distribut endpoint,"Adaptive designs for multi-armed clinical trials have become increasingly popular recently in many areas of medical research because of their potential to shorten development times and to increase patient response. However, developing response-adaptive trial designs that offer patient benefit while ensuring the resulting trial avoids bias and provides a statistically rigorous comparison of the different treatments included is highly challenging. In this paper, the theory of Multi-Armed Bandit Problems is used to define a family of near optimal adaptive designs in the context of a clinical trial with a normally distributed endpoint with known variance. Through simulation studies based on an ongoing trial as a motivation we report the operating characteristics (type I error, power, bias) and patient benefit of these approaches and compare them to traditional and existing alternative designs. These results are then compared to those recently published in the context of Bernoulli endpoints. Many limitations and advantages are similar in both cases but there are also important differences, specially with respect to type I error control. This paper proposes a simulation-based testing procedure to correct for the observed type I error inflation that bandit-based and adaptive rules can induce. Results presented extend recent work by considering a normally distributed endpoint, a very common case in clinical practice yet mostly ignored in the response-adaptive theoretical literature, and illustrate the potential advantages of using these methods in a rare disease context. We also recommend a suitable modified implementation of the bandit-based adaptive designs for the case of common diseases.",adapt design multi arm clinic trial becom increas popular recent mani area medic research becaus potenti shorten develop time increas patient respons howev develop respons adapt trial design offer patient benefit ensur result trial avoid bias provid statist rigor comparison differ treatment includ high challeng paper theori multi arm bandit problem use defin famili near optim adapt design context clinic trial normal distribut endpoint known varianc simul studi base ongo trial motiv report oper characterist type error power bias patient benefit approach compar tradit exist altern design result compar recent publish context bernoulli endpoint mani limit advantag similar case also import differ special respect type error control paper propos simul base test procedur correct observ type error inflat bandit base adapt rule induc result present extend recent work consid normal distribut endpoint veri common case clinic practic yet ignor respons adapt theoret literatur illustr potenti advantag use method rare diseas context also recommend suitabl modifi implement bandit base adapt design case common diseas,"['Adam Smith', 'Sofia S. Villar']",['stat.AP'],False,False,True,False,False,True
939,2017-03-28T14:02:39Z,2017-03-15T12:07:47Z,http://arxiv.org/abs/1703.05103v1,http://arxiv.org/pdf/1703.05103v1,Do pay-for-performance incentives lead to a better health outcome?,pay perform incent lead better health outcom,"Pay-for-performance approaches have been widely adopted in order to drive improvements in the quality of healthcare provision. Previous studies evaluating the impact of these programs are either limited by the number of health outcomes or of medical conditions considered. In this paper, we evaluate the effectiveness of a pay-for-performance program on the basis of five health outcomes and across a wide range of medical conditions. The context of the study is the Lombardy region in Italy, where a rewarding program was introduced in 2012. The policy evaluation is based on a difference-in-differences approach. The model includes multiple dependent outcomes, that allow quantifying the joint effect of the program, and random effects, that account for the heterogeneity of the data at the ward and hospital level. Our results show that the policy had a positive effect on the hospitals' performance in terms of those outcomes that can be more influenced by a managerial activity, namely the number of readmissions, transfers and returns to the surgery room. No significant changes which can be related to the pay-for-performance introduction are observed for the number of voluntary discharges and for mortality. Finally, our study shows evidence that the medical wards have reacted more strongly to the pay-for-performance program than the surgical ones, whereas only limited evidence is found in support of a different policy reaction across different types of hospital ownership.",pay perform approach wide adopt order drive improv qualiti healthcar provis previous studi evalu impact program either limit number health outcom medic condit consid paper evalu effect pay perform program basi five health outcom across wide rang medic condit context studi lombardi region itali reward program introduc polici evalu base differ differ approach model includ multipl depend outcom allow quantifi joint effect program random effect account heterogen data ward hospit level result show polici posit effect hospit perform term outcom influenc manageri activ name number readmiss transfer return surgeri room signific chang relat pay perform introduct observ number voluntari discharg mortal final studi show evid medic ward react strong pay perform program surgic one wherea onli limit evid found support differ polici reaction across differ type hospit ownership,"['Alina Peluso', 'Paolo Berta', 'Veronica Vinciotti']",['stat.AP'],False,False,False,False,False,True
942,2017-03-28T14:02:43Z,2017-03-14T23:04:14Z,http://arxiv.org/abs/1703.04812v1,http://arxiv.org/pdf/1703.04812v1,An alternative representation of the negative binomial-Lindley   distribution. New results and applications,altern represent negat binomi lindley distribut new result applic,"In this paper we present an alternative representation of the Negative Binomial--Lindley distribution recently proposed by Zamani and Ismail (2010) which shows some advantages over the latter model. This new formulation provides a tractable model with attractive properties which makes it suitable for application not only in insurance settings but also in other fields where overdispersion is observed. Basic properties of the new distribution are studied. A recurrence for the probabilities of the new distribution and an integral equation for the probability density function of the compound version, when the claim severities are absolutely continuous, are derived. Estimation methods are discussed and a numerical application is given.",paper present altern represent negat binomi lindley distribut recent propos zamani ismail show advantag latter model new formul provid tractabl model attract properti make suitabl applic onli insur set also field overdispers observ basic properti new distribut studi recurr probabl new distribut integr equat probabl densiti function compound version claim sever absolut continu deriv estim method discuss numer applic given,"['Emilio Gomez-Deniz', 'Enrique Calderin-Ojeda']",['stat.AP'],False,False,True,False,False,True
944,2017-03-28T14:02:43Z,2017-03-14T18:16:06Z,http://arxiv.org/abs/1703.04642v1,http://arxiv.org/pdf/1703.04642v1,Robust Morphometric Analysis based on Landmarks. Applications,robust morphometr analysi base landmark applic,"Procrustes Analysis is a Morphometric method based on Configurations of Landmarks that estimates the superimposition parameters by least-squares; for this reason, the procedure is very sensitive to outliers. In the first part of the paper we robustify this technique to classify individuals from a descriptive point of view. In the literature there are also classical results, based on the normality of the observations, to test whether there are significant differences between individuals. In the second part of the paper we determine a Von Mises plus Saddlepoint approximation for the tail probability of the Procrustes Statistic when the observations come from a model close to the normal. We conclude the paper with some applications using the Geographical Information System QGIS.",procrust analysi morphometr method base configur landmark estim superimposit paramet least squar reason procedur veri sensit outlier first part paper robustifi techniqu classifi individu descript point view literatur also classic result base normal observ test whether signific differ individu second part paper determin von mise plus saddlepoint approxim tail probabl procrust statist observ come model close normal conclud paper applic use geograph inform system qgis,"['A. Garcia-Perez', 'M. A. Cabrero-Ortega']",['stat.AP'],False,False,False,False,False,True
946,2017-03-28T14:02:43Z,2017-03-13T10:14:20Z,http://arxiv.org/abs/1703.04312v1,http://arxiv.org/pdf/1703.04312v1,Assessing Potential Wind Energy Resources in Saudi Arabia with a Skew-t   Distribution,assess potenti wind energi resourc saudi arabia skew distribut,"Facing increasing domestic energy consumption from population growth and industrialization, Saudi Arabia is aiming to reduce its reliance on fossil fuels and to broaden its energy mix by expanding investment in renewable energy sources, including wind energy. A preliminary task in the development of wind energy infrastructure is the assessment of wind energy potential, a key aspect of which is the characterization of its spatio-temporal behavior. In this study we examine the impact of internal climate variability on seasonal wind power density fluctuations using 30 simulations from the Large Ensemble Project (LENS) developed at the National Center for Atmospheric Research. Furthermore, a spatio-temporal model for daily wind speed is proposed with neighbor-based cross-temporal dependence, and a multivariate skew-t distribution to capture the spatial patterns of higher order moments. The model can be used to generate synthetic time series over the entire spatial domain that adequately reproduces the internal variability of the LENS dataset.",face increas domest energi consumpt popul growth industri saudi arabia aim reduc relianc fossil fuel broaden energi mix expand invest renew energi sourc includ wind energi preliminari task develop wind energi infrastructur assess wind energi potenti key aspect character spatio tempor behavior studi examin impact intern climat variabl season wind power densiti fluctuat use simul larg ensembl project len develop nation center atmospher research furthermor spatio tempor model daili wind speed propos neighbor base cross tempor depend multivari skew distribut captur spatial pattern higher order moment model use generat synthet time seri entir spatial domain adequ reproduc intern variabl len dataset,"['Felipe Tagle', 'Stefano Castruccio', 'Paola Crippa', 'Marc G. Genton']",['stat.AP'],False,False,True,False,False,True
947,2017-03-28T14:02:43Z,2017-03-12T08:11:29Z,http://arxiv.org/abs/1703.04081v1,http://arxiv.org/pdf/1703.04081v1,Feature overwriting as a finite mixture process: Evidence from   comprehension data,featur overwrit finit mixtur process evid comprehens data,"The ungrammatical sentence ""The key to the cabinets are on the table"" is known to lead to an illusion of grammaticality. As discussed in the meta-analysis by Jaeger et al., 2017, faster reading times are observed at the verb are in the agreement-attraction sentence above compared to the equally ungrammatical sentence ""The key to the cabinet are on the table"". One explanation for this facilitation effect is the feature percolation account: the plural feature on cabinets percolates up to the head noun key, leading to the illusion. An alternative account is in terms of cue-based retrieval (Lewis & Vasishth, 2005), which assumes that the non-subject noun cabinets is misretrieved due to a partial feature-match when a dependency completion process at the auxiliary initiates a memory access for a subject with plural marking. We present evidence for yet another explanation for the observed facilitation. Because the second sentence has two nouns with identical number, it is possible that these are, in some proportion of trials, more difficult to keep distinct, leading to slower reading times at the verb in the first sentence above; this is the feature overwriting account of Nairne, 1990. We show that the feature overwriting proposal can be implemented as a finite mixture process. We reanalysed ten published data-sets, fitting hierarchical Bayesian mixture models to these data assuming a two-mixture distribution. We show that in nine out of the ten studies, a mixture distribution corresponding to feature overwriting furnishes a superior fit over both the feature percolation and the cue-based retrieval accounts.",ungrammat sentenc key cabinet tabl known lead illus grammat discuss meta analysi jaeger et al faster read time observ verb agreement attract sentenc abov compar equal ungrammat sentenc key cabinet tabl one explan facilit effect featur percol account plural featur cabinet percol head noun key lead illus altern account term cue base retriev lewi vasishth assum non subject noun cabinet misretriev due partial featur match depend complet process auxiliari initi memori access subject plural mark present evid yet anoth explan observ facilit becaus second sentenc two noun ident number possibl proport trial difficult keep distinct lead slower read time verb first sentenc abov featur overwrit account nairn show featur overwrit propos implement finit mixtur process reanalys ten publish data set fit hierarch bayesian mixtur model data assum two mixtur distribut show nine ten studi mixtur distribut correspond featur overwrit furnish superior fit featur percol cue base retriev account,"['Shravan Vasishth', 'Lena A. Jaeger', 'Bruno Nicenboim']","['stat.ML', 'cs.CL', 'stat.AP']",False,False,False,False,False,True
950,2017-03-28T14:02:47Z,2017-03-10T22:03:17Z,http://arxiv.org/abs/1703.03853v1,http://arxiv.org/pdf/1703.03853v1,PairCloneTree: Reconstruction of Tumor Subclone Phylogeny Based on   Mutation Pairs using Next Generation Sequencing Data,pairclonetre reconstruct tumor subclon phylogeni base mutat pair use next generat sequenc data,"We present a latent feature allocation model to reconstruct tumor subclones subject to phylogenetic evolution that mimics tumor evolution. Similar to most current methods, we consider data from next-generation sequencing. Unlike most methods that use information in short reads mapped to single nucleotide variants (SNVs), we consider subclone reconstruction using pairs of two proximal SNVs that can be mapped by the same short reads. As part of the Bayesian inference model, we construct a phylogenetic tree prior. The use of the tree structure in the prior greatly strengthens inference. Only subclones that can be approximated by a phylogenetic tree are assigned non-negligible probability. The proposed Bayesian framework implies posterior distributions on the number of subclones, their genotypes, cellular proportions, and the phylogenetic tree spanned by the inferred subclones. The proposed method is validated against different sets of simulated and real-world data using single and multiple tumor samples. An open source software package is available at http://www.compgenome.org/pairclonetree",present latent featur alloc model reconstruct tumor subclon subject phylogenet evolut mimic tumor evolut similar current method consid data next generat sequenc unlik method use inform short read map singl nucleotid variant snvs consid subclon reconstruct use pair two proxim snvs map short read part bayesian infer model construct phylogenet tree prior use tree structur prior great strengthen infer onli subclon approxim phylogenet tree assign non neglig probabl propos bayesian framework impli posterior distribut number subclon genotyp cellular proport phylogenet tree span infer subclon propos method valid differ set simul real world data use singl multipl tumor sampl open sourc softwar packag avail http www compgenom org pairclonetre,"['Tianjian Zhou', 'Subhajit Sengupta', 'Peter Mueller', 'Yuan Ji']",['stat.AP'],False,False,False,False,False,True
951,2017-03-28T14:02:47Z,2017-03-10T18:41:00Z,http://arxiv.org/abs/1703.03790v1,http://arxiv.org/pdf/1703.03790v1,"Summertime, and the livin is easy: Winter and summer pseudoseasonal life   expectancy in the United States",summertim livin easi winter summer pseudoseason life expect unit state,"In temperate climates, mortality is seasonal with a winter-dominant pattern, due in part to pneumonia and influenza. Cardiac causes, which are the leading cause of death in the United States, are also winter-seasonal although it is not clear why. Interactions between circulating respiratory viruses (f.e., influenza) and cardiac conditions have been suggested as a cause of winter-dominant mortality patterns. We propose and implement a way to estimate an upper bound on mortality attributable to winter-dominant viruses like influenza. We calculate 'pseudo-seasonal' life expectancy, dividing the year into two six-month spans, one encompassing winter the other summer. During the summer when the circulation of respiratory viruses is drastically reduced, life expectancy is about one year longer. We also quantify the seasonal mortality difference in terms of seasonal ""equivalent ages"" (defined herein) and proportional hazards. We suggest that even if viruses cause excess winter cardiac mortality, the population-level mortality reduction of a perfect influenza vaccine would be much more modest than is often recognized.",temper climat mortal season winter domin pattern due part pneumonia influenza cardiac caus lead caus death unit state also winter season although clear whi interact circul respiratori virus influenza cardiac condit suggest caus winter domin mortal pattern propos implement way estim upper bound mortal attribut winter domin virus like influenza calcul pseudo season life expect divid year two six month span one encompass winter summer dure summer circul respiratori virus drastic reduc life expect one year longer also quantifi season mortal differ term season equival age defin herein proport hazard suggest even virus caus excess winter cardiac mortal popul level mortal reduct perfect influenza vaccin would much modest often recogn,"['Tina Ho', 'Andrew Noymer']",['stat.AP'],False,False,False,False,False,True
952,2017-03-28T14:02:47Z,2017-03-10T16:35:40Z,http://arxiv.org/abs/1703.03753v1,http://arxiv.org/pdf/1703.03753v1,Latent Gaussian Mixture Models for Nationwide Kidney Transplant Center   Evaluation,latent gaussian mixtur model nationwid kidney transplant center evalu,"Five year post-transplant survival rate is an important indicator on quality of care delivered by kidney transplant centers in the United States. To provide a fair assessment of each transplant center, an effect that represents the center-specific care quality, along with patient level risk factors, is often included in the risk adjustment model. In the past, the center effects have been modeled as either fixed effects or Gaussian random effects, with various pros and cons. Our numerical analyses reveal that the distributional assumptions do impact the prediction of center effects especially when the effect is extreme. To bridge the gap between these two approaches, we propose to model the transplant center effect as a latent random variable with a finite Gaussian mixture distribution. Such latent Gaussian mixture models provide a convenient framework to study the heterogeneity among the transplant centers. To overcome the weak identifiability issues, we propose to estimate the latent Gaussian mixture model using a penalized likelihood approach, and develop sequential locally restricted likelihood ratio tests to determine the number of components in the Gaussian mixture distribution. The fitted mixture model provides a convenient means of controlling the false discovery rate when screening for underperforming or outperforming transplant centers. The performance of the methods is verified by simulations and by the analysis of the motivating data example.",five year post transplant surviv rate import indic qualiti care deliv kidney transplant center unit state provid fair assess transplant center effect repres center specif care qualiti along patient level risk factor often includ risk adjust model past center effect model either fix effect gaussian random effect various pros con numer analys reveal distribut assumpt impact predict center effect especi effect extrem bridg gap two approach propos model transplant center effect latent random variabl finit gaussian mixtur distribut latent gaussian mixtur model provid conveni framework studi heterogen among transplant center overcom weak identifi issu propos estim latent gaussian mixtur model use penal likelihood approach develop sequenti local restrict likelihood ratio test determin number compon gaussian mixtur distribut fit mixtur model provid conveni mean control fals discoveri rate screen underperform outperform transplant center perform method verifi simul analysi motiv data exampl,"['Lanfeng Pan', 'Yehua Li', 'Kevin He', 'Yanming Li', 'Yi Li']",['stat.AP'],False,False,False,False,False,True
956,2017-03-28T14:02:47Z,2017-03-08T00:47:45Z,http://arxiv.org/abs/1703.02650v1,http://arxiv.org/pdf/1703.02650v1,Joint Multichannel Deconvolution and Blind Source Separation,joint multichannel deconvolut blind sourc separ,"Blind Source Separation (BSS) is a challenging matrix factorization problem that plays a central role in multichannel imaging science. In a large number of applications, such as astrophysics, current unmixing methods are limited since real-world mixtures are generally affected by extra instrumental effects like blurring. Therefore, BSS has to be solved jointly with a deconvolution problem, which requires tackling a new inverse problem: deconvolution BSS (DBSS). In this article, we introduce an innovative DBSS approach, called DecGMCA, based on sparse signal modeling and an efficient alternative projected least square algorithm. Numerical results demonstrate that the DecGMCA algorithm performs very well on simulations. It further highlights the importance of jointly solving BSS and deconvolution instead of considering these two problems independently. Furthermore, the performance of the proposed DecGMCA algorithm is demonstrated on simulated radio-interferometric data.",blind sourc separ bss challeng matrix factor problem play central role multichannel imag scienc larg number applic astrophys current unmix method limit sinc real world mixtur general affect extra instrument effect like blur therefor bss solv joint deconvolut problem requir tackl new invers problem deconvolut bss dbss articl introduc innov dbss approach call decgmca base spars signal model effici altern project least squar algorithm numer result demonstr decgmca algorithm perform veri well simul highlight import joint solv bss deconvolut instead consid two problem independ furthermor perform propos decgmca algorithm demonstr simul radio interferometr data,"['Ming Jiang', 'Jérôme Bobin', 'Jean-Luc Starck']","['stat.AP', 'cs.IT', 'math.IT']",False,False,False,False,False,True
958,2017-03-28T14:02:47Z,2017-03-07T15:43:22Z,http://arxiv.org/abs/1703.02441v1,http://arxiv.org/pdf/1703.02441v1,Statistical Analysis of the Ricker Model,statist analysi ricker model,The Ricker model was introduced in the context of managing fishing stocks. It is a discrete non-linear iterative model given by $N(t+1)=rN(t)\exp(-N(t))$ where $N(t)$ is the population at time $t$. The model treated in this paper includes a random component $N(t+1)=rN(t)\exp(-N(t)+\varepsilon(t+1))$ and what is observed at time $t$ is a Poisson random variable with parameter $\varphi N(t)$. Such a model has been analysed using `synthetic likelihood' and ABC (Approximate Bayesian Computation). In contrast this paper takes a non-likelihood approach and treats the model in a consistent manner as an approximation. The goal is to specify those parameter values if any which are consistent with the data.,ricker model introduc context manag fish stock discret non linear iter model given rn exp popul time model treat paper includ random compon rn exp varepsilon observ time poisson random variabl paramet varphi model analys use synthet likelihood abc approxim bayesian comput contrast paper take non likelihood approach treat model consist manner approxim goal specifi paramet valu ani consist data,['Laurie Davies'],"['stat.AP', '62M99']",False,False,False,False,False,True
961,2017-03-28T14:02:51Z,2017-03-06T21:17:42Z,http://arxiv.org/abs/1703.02112v1,http://arxiv.org/pdf/1703.02112v1,Process convolution approaches for modeling interacting trajectories,process convolut approach model interact trajectori,"Gaussian processes are a fundamental statistical tool used in a wide range of applications. In the spatio-temporal setting, several families of covariance functions exist to accommodate a wide variety of dependence structures arising in different applications. These parametric families can be restrictive and are insufficient in some situations. In contrast, process convolutions represent a flexible, interpretable approach to defining the covariance of a Gaussian process and have modest requirements to ensure validity. We introduce a generalization of the process convolution approach that employs multiple convolutions sequentially to form a ""process convolution chain."" In our proposed multi-stage framework, complex dependencies that arise from a combination of different interacting mechanisms are decomposed into a series of interpretable kernel smoothers. We demonstrate an application of process convolution chains to model killer whale movement, in which the paths taken by multiple individuals are not independent, but reflect dynamic social interactions within the population. Our proposed model for dependent movement provides inference for the latent dynamic social structure in the study population. Additionally, by leveraging the positive dependence among individual paths, we achieve a reduction in uncertainty for the estimated locations of the whales, compared to a model that treats paths as independent.",gaussian process fundament statist tool use wide rang applic spatio tempor set sever famili covari function exist accommod wide varieti depend structur aris differ applic parametr famili restrict insuffici situat contrast process convolut repres flexibl interpret approach defin covari gaussian process modest requir ensur valid introduc general process convolut approach employ multipl convolut sequenti form process convolut chain propos multi stage framework complex depend aris combin differ interact mechan decompos seri interpret kernel smoother demonstr applic process convolut chain model killer whale movement path taken multipl individu independ reflect dynam social interact within popul propos model depend movement provid infer latent dynam social structur studi popul addit leverag posit depend among individu path achiev reduct uncertainti estim locat whale compar model treat path independ,"['Henry R. Scharf', 'Mevin B. Hooten', 'Devin S. Johnson', 'John W. Durban']","['stat.ME', 'stat.AP']",False,False,False,False,False,True
964,2017-03-28T14:02:51Z,2017-03-06T09:24:07Z,http://arxiv.org/abs/1703.01776v1,http://arxiv.org/pdf/1703.01776v1,Online Sequential Monte Carlo smoother for partially observed stochastic   differential equations,onlin sequenti mont carlo smoother partial observ stochast differenti equat,"This paper introduces a new algorithm to approximate smoothed additive functionals for partially observed stochastic differential equations. This method relies on a recent procedure which allows to compute such approximations online, i.e. as the observations are received, and with a computational complexity growing linearly with the number of Monte Carlo samples. This online smoother cannot be used directly in the case of partially observed stochastic differential equations since the transition density of the latent data is usually unknown. We prove that a similar algorithm may still be defined for partially observed continuous processes by replacing this unknown quantity by an unbiased estimator obtained for instance using general Poisson estimators. We prove that this estimator is consistent and its performance are illustrated using data from two models.",paper introduc new algorithm approxim smooth addit function partial observ stochast differenti equat method reli recent procedur allow comput approxim onlin observ receiv comput complex grow linear number mont carlo sampl onlin smoother cannot use direct case partial observ stochast differenti equat sinc transit densiti latent data usual unknown prove similar algorithm may still defin partial observ continu process replac unknown quantiti unbias estim obtain instanc use general poisson estim prove estim consist perform illustr use data two model,"['Pierre Gloaguen', 'Marie-Pierre Etienne', 'Sylvain Le Corff']","['stat.ME', 'stat.AP']",False,False,False,False,False,True
967,2017-03-28T14:02:51Z,2017-03-03T16:29:21Z,http://arxiv.org/abs/1703.01234v1,http://arxiv.org/pdf/1703.01234v1,A Bayesian computer model analysis of Robust Bayesian analyses,bayesian comput model analysi robust bayesian analys,"We harness the power of Bayesian emulation techniques, designed to aid the analysis of complex computer models, to examine the structure of complex Bayesian analyses themselves. These techniques facilitate robust Bayesian analyses and/or sensitivity analyses of complex problems, and hence allow global exploration of the impacts of choices made in both the likelihood and prior specification. We show how previously intractable problems in robustness studies can be overcome using emulation techniques, and how these methods allow other scientists to quickly extract approximations to posterior results corresponding to their own particular subjective specification. The utility and flexibility of our method is demonstrated on a reanalysis of a real application where Bayesian methods were employed to capture beliefs about river flow. We discuss the obvious extensions and directions of future research that such an approach opens up.",har power bayesian emul techniqu design aid analysi complex comput model examin structur complex bayesian analys themselv techniqu facilit robust bayesian analys sensit analys complex problem henc allow global explor impact choic made likelihood prior specif show previous intract problem robust studi overcom use emul techniqu method allow scientist quick extract approxim posterior result correspond particular subject specif util flexibl method demonstr reanalysi real applic bayesian method employ captur belief river flow discuss obvious extens direct futur research approach open,"['Ian Vernon', 'John Paul Gosling']","['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']",False,False,False,False,False,True
971,2017-03-28T14:02:55Z,2017-03-24T13:54:01Z,http://arxiv.org/abs/1703.01000v2,http://arxiv.org/abs/1703.01000v2,The M33 Synoptic Stellar Survey. II. Mira Variables,synopt stellar survey ii mira variabl,"We present the discovery of 1847 Mira candidates in the Local Group galaxy M33 using a novel semi-parametric periodogram technique coupled with a Random Forest classifier. The algorithms were applied to ~2.4x10^5 I-band light curves previously obtained by the M33 Synoptic Stellar Survey. We derive preliminary Period-Luminosity relations at optical, near- & mid-infrared wavelengths and compare them to the corresponding relations in the Large Magellanic Cloud.",present discoveri mira candid local group galaxi use novel semi parametr periodogram techniqu coupl random forest classifi algorithm appli band light curv previous obtain synopt stellar survey deriv preliminari period luminos relat optic near mid infrar wavelength compar correspond relat larg magellan cloud,"['Wenlong Yuan', 'Shiyuan He', 'Lucas M. Macri', 'James Long', 'Jianhua Z. Huang']","['astro-ph.SR', 'stat.AP']",False,False,False,False,False,True
972,2017-03-28T14:02:55Z,2017-03-02T23:03:56Z,http://arxiv.org/abs/1703.00981v1,http://arxiv.org/pdf/1703.00981v1,A Restaurant Process Mixture Model for Connectivity Based Parcellation   of the Cortex,restaur process mixtur model connect base parcel cortex,"One of the primary objectives of human brain mapping is the division of the cortical surface into functionally distinct regions, i.e. parcellation. While it is generally agreed that at macro-scale different regions of the cortex have different functions, the exact number and configuration of these regions is not known. Methods for the discovery of these regions are thus important, particularly as the volume of available information grows. Towards this end, we present a parcellation method based on a Bayesian non-parametric mixture model of cortical connectivity.",one primari object human brain map divis cortic surfac function distinct region parcel general agre macro scale differ region cortex differ function exact number configur region known method discoveri region thus import particular volum avail inform grow toward end present parcel method base bayesian non parametr mixtur model cortic connect,"['Daniel Moyer', 'Boris A Gutman', 'Neda Jahanshad', 'Paul M. Thompson']","['q-bio.NC', 'cs.CE', 'cs.CV', 'q-bio.QM', 'stat.AP']",False,False,False,False,False,True
978,2017-03-28T14:02:55Z,2017-02-27T22:22:03Z,http://arxiv.org/abs/1702.08560v1,http://arxiv.org/pdf/1702.08560v1,"Estimating the reproductive number, total outbreak size, and reporting   rates for Zika epidemics in South and Central America",estim reproduct number total outbreak size report rate zika epidem south central america,"As South and Central American countries prepare for increased birth defects from Zika virus outbreaks and plan for mitigation strategies to minimize ongoing and future outbreaks, understanding important characteristics of Zika outbreaks and how they vary across regions is a challenging and important problem. We developed a mathematical model for the 2015 Zika virus outbreak dynamics in Colombia, El Salvador, and Suriname. We fit the model to publicly available data provided by the Pan American Health Organization, using Approximate Bayesian Computation to estimate parameter distributions and provide uncertainty quantification. An important model input is the at-risk susceptible population, which can vary with a number of factors including climate, elevation, population density, and socio-economic status. We informed this initial condition using the highest historically reported dengue incidence modified by the probable dengue reporting rates in the chosen countries. The model indicated that a country-level analysis was not appropriate for Colombia. We then estimated the basic reproduction number, or the expected number of new human infections arising from a single infected human, to range between 4 and 6 for El Salvador and Suriname with a median of 4.3 and 5.3, respectively. We estimated the reporting rate to be around 16% in El Salvador and 18% in Suriname with estimated total outbreak sizes of 73,395 and 21,647 people, respectively. The uncertainty in parameter estimates highlights a need for research and data collection that will better constrain parameter ranges.",south central american countri prepar increas birth defect zika virus outbreak plan mitig strategi minim ongo futur outbreak understand import characterist zika outbreak vari across region challeng import problem develop mathemat model zika virus outbreak dynam colombia el salvador surinam fit model public avail data provid pan american health organ use approxim bayesian comput estim paramet distribut provid uncertainti quantif import model input risk suscept popul vari number factor includ climat elev popul densiti socio econom status inform initi condit use highest histor report dengu incid modifi probabl dengu report rate chosen countri model indic countri level analysi appropri colombia estim basic reproduct number expect number new human infect aris singl infect human rang el salvador surinam median respect estim report rate around el salvador surinam estim total outbreak size peopl respect uncertainti paramet estim highlight need research data collect better constrain paramet rang,"['Deborah P. Shutt', 'Carrie A. Manore', 'Stephen Pankavich', 'Aaron T. Porter', 'Sara Y. Del Valle']","['q-bio.PE', 'q-bio.QM', 'stat.AP', '92D30']",False,False,True,False,False,True
981,2017-03-28T14:03:00Z,2017-02-27T08:33:26Z,http://arxiv.org/abs/1702.08185v1,http://arxiv.org/pdf/1702.08185v1,An update on statistical boosting in biomedicine,updat statist boost biomedicin,"Statistical boosting algorithms have triggered a lot of research during the last decade. They combine a powerful machine-learning approach with classical statistical modelling, offering various practical advantages like automated variable selection and implicit regularization of effect estimates. They are extremely flexible, as the underlying base-learners (regression functions defining the type of effect for the explanatory variables) can be combined with any kind of loss function (target function to be optimized, defining the type of regression setting). In this review article, we highlight the most recent methodological developments on statistical boosting regarding variable selection, functional regression and advanced time-to-event modelling. Additionally, we provide a short overview on relevant applications of statistical boosting in biomedicine.",statist boost algorithm trigger lot research dure last decad combin power machin learn approach classic statist model offer various practic advantag like autom variabl select implicit regular effect estim extrem flexibl base learner regress function defin type effect explanatori variabl combin ani kind loss function target function optim defin type regress set review articl highlight recent methodolog develop statist boost regard variabl select function regress advanc time event model addit provid short overview relev applic statist boost biomedicin,"['Andreas Mayr', 'Benjamin Hofner', 'Elisabeth Waldmann', 'Tobias Hepp', 'Olaf Gefeller', 'Matthias Schmid']","['stat.AP', 'stat.CO', 'stat.ML']",False,False,False,False,False,True
983,2017-03-28T14:03:00Z,2017-02-26T21:23:33Z,http://arxiv.org/abs/1702.08088v1,http://arxiv.org/pdf/1702.08088v1,Selection of training populations (and other subset selection problems)   with an accelerated genetic algorithm (STPGA: An R-package for selection of   training populations with a genetic algorithm),select train popul subset select problem acceler genet algorithm stpga packag select train popul genet algorithm,"Optimal subset selection is an important task that has numerous algorithms designed for it and has many application areas. STPGA contains a special genetic algorithm supplemented with a tabu memory property (that keeps track of previously tried solutions and their fitness for a number of iterations), and with a regression of the fitness of the solutions on their coding that is used to form the ideal estimated solution (look ahead property) to search for solutions of generic optimal subset selection problems. I have initially developed the programs for the specific problem of selecting training populations for genomic prediction or association problems, therefore I give discussion of the theory behind optimal design of experiments to explain the default optimization criteria in STPGA, and illustrate the use of the programs in this endeavor. Nevertheless, I have picked a few other areas of application: supervised and unsupervised variable selection based on kernel alignment, supervised variable selection with design criteria, influential observation identification for regression, solving mixed integer quadratic optimization problems, balancing gains and inbreeding in a breeding population. Some of these illustrations pertain new statistical approaches.",optim subset select import task numer algorithm design mani applic area stpga contain special genet algorithm supplement tabu memori properti keep track previous tri solut fit number iter regress fit solut code use form ideal estim solut look ahead properti search solut generic optim subset select problem initi develop program specif problem select train popul genom predict associ problem therefor give discuss theori behind optim design experi explain default optim criteria stpga illustr use program endeavor nevertheless pick area applic supervis unsupervis variabl select base kernel align supervis variabl select design criteria influenti observ identif regress solv mix integ quadrat optim problem balanc gain inbreed breed popul illustr pertain new statist approach,['Deniz Akdemir'],"['stat.ME', 'cs.LG', 'q-bio.GN', 'q-bio.QM', 'stat.AP']",False,False,False,False,False,True
984,2017-03-28T14:03:00Z,2017-02-26T10:41:26Z,http://arxiv.org/abs/1703.01977v1,http://arxiv.org/pdf/1703.01977v1,"Linear, Machine Learning and Probabilistic Approaches for Time Series   Analysis",linear machin learn probabilist approach time seri analysi,"In this paper we study different approaches for time series modeling. The forecasting approaches using linear models, ARIMA alpgorithm, XGBoost machine learning algorithm are described. Results of different model combinations are shown. For probabilistic modeling the approaches using copulas and Bayesian inference are considered.",paper studi differ approach time seri model forecast approach use linear model arima alpgorithm xgboost machin learn algorithm describ result differ model combin shown probabilist model approach use copula bayesian infer consid,['B. M. Pavlyshenko'],"['stat.AP', 'cs.LG', 'stat.ME']",False,False,False,False,False,True
985,2017-03-28T14:03:00Z,2017-02-26T03:10:41Z,http://arxiv.org/abs/1702.07981v1,http://arxiv.org/pdf/1702.07981v1,BayCount: A Bayesian Decomposition Method for Inferring Tumor   Heterogeneity using RNA-Seq Counts,baycount bayesian decomposit method infer tumor heterogen use rna seq count,"Tumor is heterogeneous - a tumor sample usually consists of a set of subclones with distinct transcriptional profiles and potentially different degrees of aggressiveness and responses to drugs. Understanding tumor heterogeneity is therefore critical to precise cancer prognosis and treatment. In this paper, we introduce BayCount, a Bayesian decomposition method to infer tumor heterogeneity with highly over-dispersed RNA sequencing count data. Using negative binomial factor analysis, BayCount takes into account both the between-sample and gene-specific random effects on raw counts of sequencing reads mapped to each gene. For posterior inference, we develop an efficient compound Poisson based blocked Gibbs sampler. Through extensive simulation studies and analysis of The Cancer Genome Atlas lung cancer and kidney cancer RNA sequencing count data, we show that BayCount is able to accurately estimate the number of subclones, the proportions of these subclones in each tumor sample, and the gene expression profiles in each subclone. Our method represents the first effort in characterizing tumor heterogeneity using RNA sequencing count data that simultaneously removes the need of normalizing the counts, achieves statistical robustness, and obtains biologically and clinically meaningful insights.",tumor heterogen tumor sampl usual consist set subclon distinct transcript profil potenti differ degre aggress respons drug understand tumor heterogen therefor critic precis cancer prognosi treatment paper introduc baycount bayesian decomposit method infer tumor heterogen high dispers rna sequenc count data use negat binomi factor analysi baycount take account sampl gene specif random effect raw count sequenc read map gene posterior infer develop effici compound poisson base block gibb sampler extens simul studi analysi cancer genom atlas lung cancer kidney cancer rna sequenc count data show baycount abl accur estim number subclon proport subclon tumor sampl gene express profil subclon method repres first effort character tumor heterogen use rna sequenc count data simultan remov need normal count achiev statist robust obtain biolog clinic meaning insight,"['Fangzheng Xie', 'Mingyuan Zhou', 'Yanxun Xu']",['stat.AP'],False,False,False,False,False,True
986,2017-03-28T14:03:00Z,2017-03-04T00:20:43Z,http://arxiv.org/abs/1702.07909v2,http://arxiv.org/pdf/1702.07909v2,Analysis of Urban Vibrancy and Safety in Philadelphia,analysi urban vibranc safeti philadelphia,"Statistical analyses of urban environments have been recently improved through publicly available high resolution data and mapping technologies that have adopted across industries. These technologies allow us to create metrics to empirically investigate urban design principles of the past half-century. Philadelphia is an interesting case study for this work, with its rapid urban development and population increase in the last decade. We focus on features of what urban planners call ""vibrancy"": measures of positive, healthy activity or energy in an area. Historically, vibrancy has been very challenging to measure empirically. We explore the association between safety (violent and non-violent crime) and features of local neighborhood vibrancy such as population, economic measures and land use zoning. Despite rhetoric about the negative effects of population density in the 1960s and 70s, we find very little association between crime and population density. Measures based on land use zoning are not an adequate description of local vibrancy and so we construct a database and set of measures of business activity in each neighborhood. We employ several matching analyses within census block groups to explore the relationship between neighborhood vibrancy and safety at a higher resolution. We find that neighborhoods with more vacancy have higher crime but within neighborhoods, crimes tend not to be located near vacant properties. We also find that more crimes occur near business locations but businesses that are active (open) for longer periods are associated with fewer crimes.",statist analys urban environ recent improv public avail high resolut data map technolog adopt across industri technolog allow us creat metric empir investig urban design principl past half centuri philadelphia interest case studi work rapid urban develop popul increas last decad focus featur urban planner call vibranc measur posit healthi activ energi area histor vibranc veri challeng measur empir explor associ safeti violent non violent crime featur local neighborhood vibranc popul econom measur land use zone despit rhetor negat effect popul densiti find veri littl associ crime popul densiti measur base land use zone adequ descript local vibranc construct databas set measur busi activ neighborhood employ sever match analys within census block group explor relationship neighborhood vibranc safeti higher resolut find neighborhood vacanc higher crime within neighborhood crime tend locat near vacant properti also find crime occur near busi locat busi activ open longer period associ fewer crime,"['Colman Humphrey', 'Shane T. Jensen', 'Dylan Small', 'Rachel Thurston']",['stat.AP'],False,False,True,False,False,True
987,2017-03-28T14:03:00Z,2017-02-25T10:26:59Z,http://arxiv.org/abs/1702.07869v1,http://arxiv.org/pdf/1702.07869v1,Signal Denoising Using the Minimum-Probability-of-Error Criterion,signal denois use minimum probabl error criterion,"We address the problem of signal denoising via transform-domain shrinkage based on a novel $\textit{risk}$ criterion called the minimum probability of error (MPE), which measures the probability that the estimated parameter lies outside an $\epsilon$-neighborhood of the actual value. However, the MPE, similar to the mean-squared error (MSE), depends on the ground-truth parameter, and has to be estimated from the noisy observations. We consider linear shrinkage-based denoising functions, wherein the optimum shrinkage parameter is obtained by minimizing an estimate of the MPE. When the probability of error is integrated over $\epsilon$, it leads to the expected $\ell_1$ distortion. The proposed MPE and $\ell_1$ distortion formulations are applicable to various noise distributions by invoking a Gaussian mixture model approximation. Within the realm of MPE, we also develop an extension of the transform-domain shrinkage by grouping transform coefficients, resulting in $\textit{subband shrinkage}$. The denoising performance obtained within the proposed framework is shown to be better than that obtained using the minimum MSE-based approaches formulated within $\textbf{$\textit {Stein's unbiased risk estimation}$}$ (SURE) framework, especially in the low measurement signal-to-noise ratio (SNR) regime. Performance comparison with three state-of-the-art denoising algorithms, carried out on electrocardiogram signals and two test signals taken from the $\textit{Wavelab}$ toolbox, exhibits that the MPE framework results in consistent SNR gains for input SNRs below $5$ dB.",address problem signal denois via transform domain shrinkag base novel textit risk criterion call minimum probabl error mpe measur probabl estim paramet lie outsid epsilon neighborhood actual valu howev mpe similar mean squar error mse depend ground truth paramet estim noisi observ consid linear shrinkag base denois function wherein optimum shrinkag paramet obtain minim estim mpe probabl error integr epsilon lead expect ell distort propos mpe ell distort formul applic various nois distribut invok gaussian mixtur model approxim within realm mpe also develop extens transform domain shrinkag group transform coeffici result textit subband shrinkag denois perform obtain within propos framework shown better obtain use minimum mse base approach formul within textbf textit stein unbias risk estim sure framework especi low measur signal nois ratio snr regim perform comparison three state art denois algorithm carri electrocardiogram signal two test signal taken textit wavelab toolbox exhibit mpe framework result consist snr gain input snrs db,"['Jishnu Sadasivan', 'Subhadip Mukherjee', 'Chandra Sekhar Seelamantula']",['stat.AP'],False,False,False,False,False,True
988,2017-03-28T14:03:00Z,2017-02-24T05:16:23Z,http://arxiv.org/abs/1702.07465v1,http://arxiv.org/pdf/1702.07465v1,PairClone: A Bayesian Subclone Caller Based on Mutation Pairs,pairclon bayesian subclon caller base mutat pair,"Tumor cell populations can be thought of as being composed of homogeneous cell subpopulations, with each subpopulation being characterized by overlapping sets of single nucleotide variants (SNVs). Such subpopulations are known as subclones and are an important target for precision medicine. Reconstructing such subclones from next-generation sequencing (NGS) data is one of the major challenges in precision medicine. We present PairClone as a new tool to implement this reconstruction. The main idea of PairClone is to model short reads mapped to pairs of proximal SNVs. In contrast, most existing methods use only marginal reads for unpaired SNVs. Using Bayesian nonparametric models, we estimate posterior probabilities of the number, genotypes and population frequencies of subclones in one or more tumor sample. We use the categorical Indian buffet process (cIBP) as a prior probability model for subclones that are represented as vectors of categorical matrices that record the corresponding sets of mutation pairs. Performance of PairClone is assessed using simulated and real datasets. An open source software package can be obtained at http://www.compgenome.org/pairclone.",tumor cell popul thought compos homogen cell subpopul subpopul character overlap set singl nucleotid variant snvs subpopul known subclon import target precis medicin reconstruct subclon next generat sequenc ngs data one major challeng precis medicin present pairclon new tool implement reconstruct main idea pairclon model short read map pair proxim snvs contrast exist method use onli margin read unpair snvs use bayesian nonparametr model estim posterior probabl number genotyp popul frequenc subclon one tumor sampl use categor indian buffet process cibp prior probabl model subclon repres vector categor matric record correspond set mutat pair perform pairclon assess use simul real dataset open sourc softwar packag obtain http www compgenom org pairclon,"['Tianjian Zhou', 'Peter Mueller', 'Subhajit Sengupta', 'Yuan Ji']",['stat.AP'],False,False,False,False,False,True
991,2017-03-28T14:03:04Z,2017-02-22T21:21:54Z,http://arxiv.org/abs/1702.07009v1,http://arxiv.org/pdf/1702.07009v1,The Impact of Confounder Selection in Propensity Scores for Rare Events   Data - with Applications to Birth Defects,impact confound select propens score rare event data applic birth defect,"Our work was motivated by a recent study on birth defects of infants born to pregnant women exposed to a certain medication for treating chronic diseases. Outcomes such as birth defects are rare events in the general population, which often translate to very small numbers of events in the unexposed group. As drug safety studies in pregnancy are typically observational in nature, we control for confounding in this rare events setting using propensity scores (PS). Using our empirical data, we noticed that the estimated odds ratio for birth defects due to exposure varied drastically depending on the specific approach used. The commonly used approaches with PS are matching, stratification, inverse probability weighting (IPW) and regression adjustment. The extremely rare events setting renders the matching or stratification infeasible. In addition, the PS itself may be formed via different approaches to select confounders from a relatively long list of potential confounders. We carried out simulation experiments to compare different combinations of approaches: IPW or regression adjustment, with 1) including all potential confounders without selection, 2) selection based on univariate association between the candidate variable and the outcome, 3) selection based on change in effects (CIE). The simulation showed that IPW without selection leads to extremely large variances in the estimated odds ratio, which help to explain the empirical data analysis results that we had observed. The simulation also showed that IPW with selection based on univariate association with the outcome is preferred over IPW with CIE. Regression adjustment has small variances of the estimated odds ratio regardless of the selection methods used.",work motiv recent studi birth defect infant born pregnant women expos certain medic treat chronic diseas outcom birth defect rare event general popul often translat veri small number event unexpos group drug safeti studi pregnanc typic observ natur control confound rare event set use propens score ps use empir data notic estim odd ratio birth defect due exposur vari drastic depend specif approach use common use approach ps match stratif invers probabl weight ipw regress adjust extrem rare event set render match stratif infeas addit ps may form via differ approach select confound relat long list potenti confound carri simul experi compar differ combin approach ipw regress adjust includ potenti confound without select select base univari associ candid variabl outcom select base chang effect cie simul show ipw without select lead extrem larg varianc estim odd ratio help explain empir data analysi result observ simul also show ipw select base univari associ outcom prefer ipw cie regress adjust small varianc estim odd ratio regardless select method use,"['Ronghui Xu', 'Jue Hou', 'Christina D. Chambers']",['stat.AP'],False,False,False,False,False,True
1000,2017-03-28T14:03:09Z,2017-03-27T16:14:18Z,http://arxiv.org/abs/1703.09163v1,http://arxiv.org/pdf/1703.09163v1,Scalable Bayesian shrinkage and uncertainty quantification in   high-dimensional regression,scalabl bayesian shrinkag uncertainti quantif high dimension regress,"Bayesian shrinkage methods have generated a lot of recent interest as tools for high-dimensional regression and model selection. These methods naturally facilitate tractable uncertainty quantification and incorporation of prior information. A common feature of these models, including the Bayesian lasso, global-local shrinkage priors, and spike-and-slab priors is that the corresponding priors on the regression coefficients can be expressed as scale mixture of normals. While the three-step Gibbs sampler used to sample from the often intractable associated posterior density has been shown to be geometrically ergodic for several of these models (Khare and Hobert, 2013; Pal and Khare, 2014), it has been demonstrated recently that convergence of this sampler can still be quite slow in modern high-dimensional settings despite this apparent theoretical safeguard. We propose a new method to draw from the same posterior via a tractable two-step blocked Gibbs sampler. We demonstrate that our proposed two-step blocked sampler exhibits vastly superior convergence behavior compared to the original three- step sampler in high-dimensional regimes on both real and simulated data. We also provide a detailed theoretical underpinning to the new method in the context of the Bayesian lasso. First, we derive explicit upper bounds for the (geometric) rate of convergence. Furthermore, we demonstrate theoretically that while the original Bayesian lasso chain is not Hilbert-Schmidt, the proposed chain is trace class (and hence Hilbert-Schmidt). The trace class property has useful theoretical and practical implications. It implies that the corresponding Markov operator is compact, and its eigenvalues are summable. It also facilitates a rigorous comparison of the two-step blocked chain with ""sandwich"" algorithms which aim to improve performance of the two-step chain by inserting an inexpensive extra step.",bayesian shrinkag method generat lot recent interest tool high dimension regress model select method natur facilit tractabl uncertainti quantif incorpor prior inform common featur model includ bayesian lasso global local shrinkag prior spike slab prior correspond prior regress coeffici express scale mixtur normal three step gibb sampler use sampl often intract associ posterior densiti shown geometr ergod sever model khare hobert pal khare demonstr recent converg sampler still quit slow modern high dimension set despit appar theoret safeguard propos new method draw posterior via tractabl two step block gibb sampler demonstr propos two step block sampler exhibit vast superior converg behavior compar origin three step sampler high dimension regim real simul data also provid detail theoret underpin new method context bayesian lasso first deriv explicit upper bound geometr rate converg furthermor demonstr theoret origin bayesian lasso chain hilbert schmidt propos chain trace class henc hilbert schmidt trace class properti use theoret practic implic impli correspond markov oper compact eigenvalu summabl also facilit rigor comparison two step block chain sandwich algorithm aim improv perform two step chain insert inexpens extra step,"['Bala Rajaratnam', 'Doug Sparks', 'Kshitij Khare', 'Liyuan Zhang']",['stat.CO'],False,False,False,False,False,True
1001,2017-03-28T14:03:09Z,2017-03-27T13:41:00Z,http://arxiv.org/abs/1703.09074v1,http://arxiv.org/pdf/1703.09074v1,Randomized CP Tensor Decomposition,random cp tensor decomposit,"The CANDECOMP/PARAFAC (CP) tensor decomposition is a popular dimensionality-reduction method for multiway data. Dimensionality reduction is often sought since many high-dimensional tensors have low intrinsic rank relative to the dimension of the ambient measurement space. However, the emergence of `big data' poses significant computational challenges for computing this fundamental tensor decomposition. Leveraging modern randomized algorithms, we demonstrate that the coherent structure can be learned from a smaller representation of the tensor in a fraction of the time. Moreover, the high-dimensional signal can be faithfully approximated from the compressed measurements. Thus, this simple but powerful algorithm enables one to compute the approximate CP decomposition even for massive tensors. The approximation error can thereby be controlled via oversampling and the computation of power iterations. In addition to theoretical results, several empirical results demonstrate the performance of the proposed algorithm.",candecomp parafac cp tensor decomposit popular dimension reduct method multiway data dimension reduct often sought sinc mani high dimension tensor low intrins rank relat dimens ambient measur space howev emerg big data pose signific comput challeng comput fundament tensor decomposit leverag modern random algorithm demonstr coher structur learn smaller represent tensor fraction time moreov high dimension signal faith approxim compress measur thus simpl power algorithm enabl one comput approxim cp decomposit even massiv tensor approxim error therebi control via oversampl comput power iter addit theoret result sever empir result demonstr perform propos algorithm,"['N. Benjamin Erichson', 'Krithika Manohar', 'Steven L. Brunton', 'J. Nathan Kutz']","['cs.NA', 'stat.CO']",False,False,False,False,False,True
1003,2017-03-28T14:03:09Z,2017-03-26T22:49:31Z,http://arxiv.org/abs/1703.08882v1,http://arxiv.org/pdf/1703.08882v1,A Mixture of Matrix Variate Skew-t Distributions,mixtur matrix variat skew distribut,"Clustering is the process of finding underlying group structures in data. Although model-based clustering is firmly established in the multivariate case, there is relative paucity for matrix variate distributions, and there are even fewer examples using matrix variate skew distributions. In this paper, we look at parameter estimation for a finite mixture of matrix variate skew-t distributions in the context of model-based clustering. Simulated data is used for illustrative purposes.",cluster process find group structur data although model base cluster firm establish multivari case relat pauciti matrix variat distribut even fewer exampl use matrix variat skew distribut paper look paramet estim finit mixtur matrix variat skew distribut context model base cluster simul data use illustr purpos,"['Michael P. B. Gallaugher', 'Paul D. McNicholas']","['stat.ME', 'stat.CO']",False,False,True,False,False,True
1004,2017-03-28T14:03:09Z,2017-03-25T17:57:31Z,http://arxiv.org/abs/1703.08723v1,http://arxiv.org/pdf/1703.08723v1,Extending Growth Mixture Models Using Continuous Non-Elliptical   Distributions,extend growth mixtur model use continu non ellipt distribut,"Growth mixture models (GMMs) incorporate both conventional random effects growth modeling and latent trajectory classes as in finite mixture modeling; therefore, they offer a way to handle the unobserved heterogeneity between subjects in their development. GMMs with Gaussian random effects dominate the literature. When the data are asymmetric and/or have heavier tails, more than one latent class is required to capture the observed variable distribution. Therefore, a GMM with continuous non-elliptical distributions is proposed to capture skewness and heavier tails in the data set. Specifically, multivariate skew-t distributions and generalized hyperbolic distributions are introduced to extend GMMs. When extending GMMs, four statistical models are considered with differing distributions of measurement errors and random effects. The mathematical development of a GMM with non-elliptical distributions relies on its relationship with the generalized inverse Gaussian distribution. Parameter estimation is outlined within the expectation-maximization framework before the performance of our GMM with non-elliptical distributions is illustrated on simulated and real data.",growth mixtur model gmms incorpor convent random effect growth model latent trajectori class finit mixtur model therefor offer way handl unobserv heterogen subject develop gmms gaussian random effect domin literatur data asymmetr heavier tail one latent class requir captur observ variabl distribut therefor gmm continu non ellipt distribut propos captur skew heavier tail data set specif multivari skew distribut general hyperbol distribut introduc extend gmms extend gmms four statist model consid differ distribut measur error random effect mathemat develop gmm non ellipt distribut reli relationship general invers gaussian distribut paramet estim outlin within expect maxim framework befor perform gmm non ellipt distribut illustr simul real data,"['Yuhong Wei', 'Emilie Shireman', 'Paul D. McNicholas', 'Douglas L. Steinley']","['stat.ME', 'stat.AP', 'stat.CO']",False,False,False,False,False,True
1005,2017-03-28T14:03:09Z,2017-03-25T11:20:21Z,http://arxiv.org/abs/1703.08676v1,http://arxiv.org/pdf/1703.08676v1,Statistical and Computational Tradeoff in Genetic Algorithm-Based   Estimation,statist comput tradeoff genet algorithm base estim,"When a Genetic Algorithm (GA), or a stochastic algorithm in general, is employed in a statistical problem, the obtained result is affected by both variability due to sampling, that refers to the fact that only a sample is observed, and variability due to the stochastic elements of the algorithm. This topic can be easily set in a framework of statistical and computational tradeoff question, crucial in recent problems, for which statisticians must carefully set statistical and computational part of the analysis, taking account of some resource or time constraints. In the present work we analyze estimation problems tackled by GAs, for which variability of estimates can be decomposed in the two sources of variability, considering some constraints in the form of cost functions, related to both data acquisition and runtime of the algorithm. Simulation studies will be presented to discuss the statistical and computational tradeoff question.",genet algorithm ga stochast algorithm general employ statist problem obtain result affect variabl due sampl refer fact onli sampl observ variabl due stochast element algorithm topic easili set framework statist comput tradeoff question crucial recent problem statistician must care set statist comput part analysi take account resourc time constraint present work analyz estim problem tackl gas variabl estim decompos two sourc variabl consid constraint form cost function relat data acquisit runtim algorithm simul studi present discuss statist comput tradeoff question,"['Manuel Rizzo', 'Francesco Battaglia']",['stat.CO'],False,False,False,False,False,True
1006,2017-03-28T14:03:09Z,2017-03-24T23:49:33Z,http://arxiv.org/abs/1703.08627v1,http://arxiv.org/pdf/1703.08627v1,Random sampling of Latin squares via binary contingency tables and   probabilistic divide-and-conquer,random sampl latin squar via binari conting tabl probabilist divid conquer,"We demonstrate a novel approach for the random sampling of Latin squares of order~$n$ via probabilistic divide-and-conquer. The algorithm divides the entries of the table modulo powers of $2$, and samples a corresponding binary contingency table at each level. The sampling distribution is based on the Boltzmann sampling heuristic, along with probabilistic divide-and-conquer.",demonstr novel approach random sampl latin squar order via probabilist divid conquer algorithm divid entri tabl modulo power sampl correspond binari conting tabl level sampl distribut base boltzmann sampl heurist along probabilist divid conquer,['Stephen DeSalvo'],['stat.CO'],False,False,False,False,False,True
1007,2017-03-28T14:03:09Z,2017-03-24T17:17:45Z,http://arxiv.org/abs/1703.08520v1,http://arxiv.org/pdf/1703.08520v1,Rejection-free Ensemble MCMC with applications to Factorial Hidden   Markov Models,reject free ensembl mcmc applic factori hidden markov model,"Bayesian inference for complex models is challenging due to the need to explore high-dimensional spaces and multimodality and standard Monte Carlo samplers can have difficulties effectively exploring the posterior. We introduce a general purpose rejection-free ensemble Markov Chain Monte Carlo (MCMC) technique to improve on existing poorly mixing samplers. This is achieved by combining parallel tempering and an auxiliary variable move to exchange information between the chains. We demonstrate this ensemble MCMC scheme on Bayesian inference in Factorial Hidden Markov Models. This high-dimensional inference problem is difficult due to the exponentially sized latent variable space. Existing sampling approaches mix slowly and can get trapped in local modes. We show that the performance of these samplers is improved by our rejection-free ensemble technique and that the method is attractive and ""easy-to-use"" since no parameter tuning is required.",bayesian infer complex model challeng due need explor high dimension space multimod standard mont carlo sampler difficulti effect explor posterior introduc general purpos reject free ensembl markov chain mont carlo mcmc techniqu improv exist poor mix sampler achiev combin parallel temper auxiliari variabl move exchang inform chain demonstr ensembl mcmc scheme bayesian infer factori hidden markov model high dimension infer problem difficult due exponenti size latent variabl space exist sampl approach mix slowli get trap local mode show perform sampler improv reject free ensembl techniqu method attract easi use sinc paramet tune requir,"['Kaspar Märtens', 'Michalis K Titsias', 'Christopher Yau']","['stat.CO', 'stat.ME', 'stat.ML']",False,False,False,False,False,True
1008,2017-03-28T14:03:09Z,2017-03-21T15:42:38Z,http://arxiv.org/abs/1703.07285v1,http://arxiv.org/pdf/1703.07285v1,From safe screening rules to working sets for faster Lasso-type solvers,safe screen rule work set faster lasso type solver,"Convex sparsity-promoting regularizations are ubiquitous in modern statistical learning. By construction, they yield solutions with few non-zero coefficients, which correspond to saturated constraints in the dual optimization formulation. Working set (WS) strategies are generic optimization techniques that consist in solving simpler problems that only consider a subset of constraints, whose indices form the WS. Working set methods therefore involve two nested iterations: the outer loop corresponds to the definition of the WS and the inner loop calls a solver for the subproblems. For the Lasso estimator a WS is a set of features, while for a Group Lasso it refers to a set of groups. In practice, WS are generally small in this context so the associated feature Gram matrix can fit in memory. Here we show that the Gauss-Southwell rule (a greedy strategy for block coordinate descent techniques) leads to fast solvers in this case. Combined with a working set strategy based on an aggressive use of so-called Gap Safe screening rules, we propose a solver achieving state-of-the-art performance on sparse learning problems. Results are presented on Lasso and multi-task Lasso estimators.",convex sparsiti promot regular ubiquit modern statist learn construct yield solut non zero coeffici correspond satur constraint dual optim formul work set ws strategi generic optim techniqu consist solv simpler problem onli consid subset constraint whose indic form ws work set method therefor involv two nest iter outer loop correspond definit ws inner loop call solver subproblem lasso estim ws set featur group lasso refer set group practic ws general small context associ featur gram matrix fit memori show gauss southwel rule greedi strategi block coordin descent techniqu lead fast solver case combin work set strategi base aggress use call gap safe screen rule propos solver achiev state art perform spars learn problem result present lasso multi task lasso estim,"['Mathurin Massias', 'Alexandre Gramfort', 'Joseph Salmon']","['stat.ML', 'cs.LG', 'math.OC', 'stat.CO']",False,False,False,False,False,True
1010,2017-03-28T14:03:13Z,2017-03-19T10:51:41Z,http://arxiv.org/abs/1703.06419v1,http://arxiv.org/pdf/1703.06419v1,Multivariate Functional Data Visualization and Outlier Detection,multivari function data visual outlier detect,"This article proposes a new graphical tool, the magnitude-shape (MS) plot, for visualizing both the magnitude and shape outlyingness of multivariate functional data. The proposed tool builds on the recent notion of functional directional outlyingness, which measures the centrality of functional data by simultaneously considering the level and the direction of their deviation from the central region. The MS-plot intuitively presents not only levels but also directions of magnitude outlyingness on the horizontal axis or plane, and demonstrates shape outlyingness on the vertical axis. A dividing curve or surface is provided to separate non-outlying data from the outliers. Both the simulated data and the practical examples confirm that the MS-plot is superior to existing tools for visualizing centrality and detecting outliers for functional data.",articl propos new graphic tool magnitud shape ms plot visual magnitud shape outlying multivari function data propos tool build recent notion function direct outlying measur central function data simultan consid level direct deviat central region ms plot intuit present onli level also direct magnitud outlying horizont axi plane demonstr shape outlying vertic axi divid curv surfac provid separ non data outlier simul data practic exampl confirm ms plot superior exist tool visual central detect outlier function data,"['Wenlin Dai', 'Marc G. Genton']","['stat.ME', 'stat.CO']",False,False,False,False,False,True
1011,2017-03-28T14:03:13Z,2017-03-18T22:12:08Z,http://arxiv.org/abs/1703.06359v1,http://arxiv.org/pdf/1703.06359v1,Fully symmetric kernel quadrature,fulli symmetr kernel quadratur,"Kernel quadratures and other kernel-based approximation methods typically suffer from prohibitive cubic time and quadratic space complexity in the number of function evaluations. The problem arises because a system of linear equations needs to be solved. In this article we show that the weights of a kernel quadrature rule can be computed efficiently and exactly for up to tens of millions of nodes if the kernel, integration domain, and measure are fully symmetric and the node set is a union of fully symmetric sets. This is based on the observations that in such a setting there are only as many distinct weights as there are fully symmetric sets and that these weights can be solved from a linear system of equations constructed out of row sums of certain submatrices of the full kernel matrix. We present several numerical examples that show feasibility, both for a large number of nodes and in high dimensions, of the developed fully symmetric kernel quadrature rules. Most prominent of the fully symmetric kernel quadrature rules we propose are those that use sparse grids.",kernel quadratur kernel base approxim method typic suffer prohibit cubic time quadrat space complex number function evalu problem aris becaus system linear equat need solv articl show weight kernel quadratur rule comput effici exact ten million node kernel integr domain measur fulli symmetr node set union fulli symmetr set base observ set onli mani distinct weight fulli symmetr set weight solv linear system equat construct row sum certain submatric full kernel matrix present sever numer exampl show feasibl larg number node high dimens develop fulli symmetr kernel quadratur rule promin fulli symmetr kernel quadratur rule propos use spars grid,"['Toni Karvonen', 'Simo Särkkä']","['math.NA', 'cs.NA', 'stat.CO']",False,False,True,False,False,True
1012,2017-03-28T14:03:13Z,2017-03-17T21:49:28Z,http://arxiv.org/abs/1703.06206v1,http://arxiv.org/pdf/1703.06206v1,Sequential Monte Carlo Methods in the nimble R Package,sequenti mont carlo method nimbl packag,"nimble is an R package for constructing algorithms and conducting inference on hierarchical models. The nimble package provides a unique combination of flexible model specification and the ability to program model-generic algorithms -- specifically, the package allows users to code models in the BUGS language, and it allows users to write algorithms that can be applied to any appropriately-specified BUGS model. In this paper, we introduce nimble's capabilities for state-space model analysis using Sequential Monte Carlo (SMC) techniques. We first provide an overview of state-space models and commonly used SMC algorithms. We then describe how to build a state-space model and conduct inference using existing SMC algorithms within nimble. SMC algorithms within nimble currently include the bootstrap filter, auxiliary particle filter, Liu and West filter, ensemble Kalman filter, and a particle MCMC sampler. These algorithms can be run in R or compiled into C++ for more efficient execution. Examples of applying SMC algorithms to a random walk model and a stochastic volatility model are provided. Finally, we give an overview of how model-generic algorithms are coded within nimble by providing code for a simple SMC algorithm.",nimbl packag construct algorithm conduct infer hierarch model nimbl packag provid uniqu combin flexibl model specif abil program model generic algorithm specif packag allow user code model bug languag allow user write algorithm appli ani appropri specifi bug model paper introduc nimbl capabl state space model analysi use sequenti mont carlo smc techniqu first provid overview state space model common use smc algorithm describ build state space model conduct infer use exist smc algorithm within nimbl smc algorithm within nimbl current includ bootstrap filter auxiliari particl filter liu west filter ensembl kalman filter particl mcmc sampler algorithm run compil effici execut exampl appli smc algorithm random walk model stochast volatil model provid final give overview model generic algorithm code within nimbl provid code simpl smc algorithm,"['Nicholas Michaud', 'Perry de Valpine', 'Daniel Turek', 'Christopher J. Paciorek']",['stat.CO'],False,False,False,False,False,True
1013,2017-03-28T14:03:13Z,2017-03-17T17:50:44Z,http://arxiv.org/abs/1703.06131v1,http://arxiv.org/pdf/1703.06131v1,Inference via low-dimensional couplings,infer via low dimension coupl,"Integration against an intractable probability measure is among the fundamental challenges of statistical inference, particularly in the Bayesian setting. A principled approach to this problem seeks a deterministic coupling of the measure of interest with a tractable ""reference"" measure (e.g., a standard Gaussian). This coupling is induced by a transport map, and enables direct simulation from the desired measure simply by evaluating the transport map at samples from the reference. Yet characterizing such a map---e.g., representing and evaluating it---grows challenging in high dimensions. The central contribution of this paper is to establish a link between the Markov properties of the target measure and the existence of certain low-dimensional couplings, induced by transport maps that are sparse or decomposable. Our analysis not only facilitates the construction of couplings in high-dimensional settings, but also suggests new inference methodologies. For instance, in the context of nonlinear and non-Gaussian state space models, we describe new online and single-pass variational algorithms that characterize the full posterior distribution of the sequential inference problem using operations only slightly more complex than regular filtering.",integr intract probabl measur among fundament challeng statist infer particular bayesian set principl approach problem seek determinist coupl measur interest tractabl refer measur standard gaussian coupl induc transport map enabl direct simul desir measur simpli evalu transport map sampl refer yet character map repres evalu grow challeng high dimens central contribut paper establish link markov properti target measur exist certain low dimension coupl induc transport map spars decompos analysi onli facilit construct coupl high dimension set also suggest new infer methodolog instanc context nonlinear non gaussian state space model describ new onlin singl pass variat algorithm character full posterior distribut sequenti infer problem use oper onli slight complex regular filter,"['Alessio Spantini', 'Daniele Bigoni', 'Youssef Marzouk']","['stat.ME', 'stat.CO', 'stat.ML']",False,False,False,False,False,True
1014,2017-03-28T14:03:13Z,2017-03-17T17:00:53Z,http://arxiv.org/abs/1703.06098v1,http://arxiv.org/pdf/1703.06098v1,Analysis of the Gibbs Sampler for Gaussian hierarchical models via   multigrid decomposition,analysi gibb sampler gaussian hierarch model via multigrid decomposit,"We study the convergence properties of the Gibbs Sampler in the context of posterior distributions arising from Bayesian analysis of Gaussian hierarchical models. We consider centred and non-centred parameterizations as well as their hybrids including the full family of partially non-centred parameterizations. We develop a novel methodology based on multi-grid decompositions to derive analytic expressions for the convergence rates of the algorithm for an arbitrary number of layers in the hierarchy, while previous work was typically limited to the two-level case. Our work gives a complete understanding for the three-level symmetric case and this gives rise to approximations for the non-symmetric case. We also give analogous, if less explicit, results for models of arbitrary level. This theory gives rise to simple and easy-to-implement guidelines for the practical implementation of Gibbs samplers on conditionally Gaussian hierarchical models.",studi converg properti gibb sampler context posterior distribut aris bayesian analysi gaussian hierarch model consid centr non centr parameter well hybrid includ full famili partial non centr parameter develop novel methodolog base multi grid decomposit deriv analyt express converg rate algorithm arbitrari number layer hierarchi previous work typic limit two level case work give complet understand three level symmetr case give rise approxim non symmetr case also give analog less explicit result model arbitrari level theori give rise simpl easi implement guidelin practic implement gibb sampler condit gaussian hierarch model,"['Giacomo Zanella', 'Gareth Roberts']","['stat.CO', 'math.PR', 'stat.ME', '60J22, 62F15, 65C40, 65C05']",False,False,False,False,False,True
1015,2017-03-28T14:03:13Z,2017-03-17T12:11:34Z,http://arxiv.org/abs/1703.05984v1,http://arxiv.org/pdf/1703.05984v1,A Tutorial on Bridge Sampling,tutori bridg sampl,"The marginal likelihood plays an important role in many areas of Bayesian statistics such as parameter estimation, model comparison, and model averaging. In most applications, however, the marginal likelihood is not analytically tractable and must be approximated using numerical methods. Here we provide a tutorial on bridge sampling (Bennett, 1976; Meng & Wong, 1996), a reliable and relatively straightforward sampling method that allows researchers to obtain the marginal likelihood for models of varying complexity. First, we introduce bridge sampling and three related sampling methods using the beta-binomial model as a running example. We then apply bridge sampling to estimate the marginal likelihood for the Expectancy Valence (EV) model---a popular model for reinforcement learning. Our results indicate that bridge sampling provides accurate estimates for both a single participant and a hierarchical version of the EV model. We conclude that bridge sampling is an attractive method for mathematical psychologists who typically aim to approximate the marginal likelihood for a limited set of possibly high-dimensional models.",margin likelihood play import role mani area bayesian statist paramet estim model comparison model averag applic howev margin likelihood analyt tractabl must approxim use numer method provid tutori bridg sampl bennett meng wong reliabl relat straightforward sampl method allow research obtain margin likelihood model vari complex first introduc bridg sampl three relat sampl method use beta binomi model run exampl appli bridg sampl estim margin likelihood expect valenc ev model popular model reinforc learn result indic bridg sampl provid accur estim singl particip hierarch version ev model conclud bridg sampl attract method mathemat psychologist typic aim approxim margin likelihood limit set possibl high dimension model,"['Quentin F. Gronau', 'Alexandra Sarafoglou', 'Dora Matzke', 'Alexander Ly', 'Udo Boehm', 'Maarten Marsman', 'David S. Leslie', 'Jonathan J. Forster', 'Eric-Jan Wagenmakers', 'Helen Steingroever']",['stat.CO'],False,False,False,False,False,True
1017,2017-03-28T14:03:13Z,2017-03-16T09:01:22Z,http://arxiv.org/abs/1703.05511v1,http://arxiv.org/pdf/1703.05511v1,An Induced Natural Selection Heuristic for Evaluating Optimal Bayesian   Experimental Designs,induc natur select heurist evalu optim bayesian experiment design,"Bayesian optimal experimental design has immense potential to inform the collection of data, so as to subsequently enhance our understanding of a variety of processes. However, a major impediment is the difficulty in evaluating optimal designs for problems with large, or high-dimensional, design spaces. We propose an efficient search heuristic suitable for general optimisation problems, with a particular focus on optimal Bayesian experimental design problems. The heuristic evaluates the objective (utility) function at an initial, randomly generated set of input values. At each generation of the algorithm, input values are ""accepted"" if their corresponding objective (utility) function satisfies some acceptance criteria, and new inputs are sampled about these accepted points. We demonstrate the new algorithm by evaluating the optimal Bayesian experimental designs for two popular stochastic models: a Markovian death model, and a pharmacokinetic model. The designs from this new algorithm are compared to those evaluated by existing algorithms, and computation times are given as a demonstration of the computational efficiency. A comparison to the current ""gold-standard"" method are given, to demonstrate that INSH finds designs that contain a similar amount of information, but more computationally efficiently. We also consider a simple approach to the construction of sampling windows for the pharmacokinetic model using the output of the proposed algorithm.",bayesian optim experiment design immens potenti inform collect data subsequ enhanc understand varieti process howev major impedi difficulti evalu optim design problem larg high dimension design space propos effici search heurist suitabl general optimis problem particular focus optim bayesian experiment design problem heurist evalu object util function initi random generat set input valu generat algorithm input valu accept correspond object util function satisfi accept criteria new input sampl accept point demonstr new algorithm evalu optim bayesian experiment design two popular stochast model markovian death model pharmacokinet model design new algorithm compar evalu exist algorithm comput time given demonstr comput effici comparison current gold standard method given demonstr insh find design contain similar amount inform comput effici also consid simpl approach construct sampl window pharmacokinet model use output propos algorithm,"['David J. Price', 'Nigel G. Bean', 'Joshua V. Ross', 'Jonathan Tuke']",['stat.CO'],False,False,False,False,False,True
1018,2017-03-28T14:03:13Z,2017-03-16T05:00:37Z,http://arxiv.org/abs/1703.05471v1,http://arxiv.org/pdf/1703.05471v1,Model selection and parameter inference in phylogenetics using Nested   Sampling,model select paramet infer phylogenet use nest sampl,"Bayesian inference methods rely on numerical algorithms for both model selection and parameter inference. In general, these algorithms require a high computational effort to yield reliable inferences. One of the major challenges in phylogenetics regards the estimation of the marginal likelihood. This quantity is commonly used for comparing different evolutionary models, but its calculation, even for simple models, incurs high computational cost. Another interesting challenge regards the estimation of the posterior distribution. Often, long Markov chains are required to get sufficient samples to carry out parameter inference, especially for tree distributions. In general, these problems are addressed separately by using different procedures. Nested sampling (NS) is a Bayesian algorithm which provides the means to estimate marginal likelihoods and to sample from the posterior distribution at no extra cost. In this paper, we introduce NS to phylogenetics. Its performance is analysed under different scenarios and compared to established methods. We conclude that NS is a very competitive and attractive algorithm for phylogenetic inference.",bayesian infer method reli numer algorithm model select paramet infer general algorithm requir high comput effort yield reliabl infer one major challeng phylogenet regard estim margin likelihood quantiti common use compar differ evolutionari model calcul even simpl model incur high comput cost anoth interest challeng regard estim posterior distribut often long markov chain requir get suffici sampl carri paramet infer especi tree distribut general problem address separ use differ procedur nest sampl ns bayesian algorithm provid mean estim margin likelihood sampl posterior distribut extra cost paper introduc ns phylogenet perform analys differ scenario compar establish method conclud ns veri competit attract algorithm phylogenet infer,"['Patricio Maturana R.', 'Brendon J. Brewer', 'Steffen Klaere']","['q-bio.QM', 'stat.CO']",False,False,False,False,False,True
1019,2017-03-28T14:03:14Z,2017-03-26T15:00:32Z,http://arxiv.org/abs/1703.05144v2,http://arxiv.org/pdf/1703.05144v2,Bergm: Bayesian exponential random graph models in R,bergm bayesian exponenti random graph model,The Bergm package provides a comprehensive framework for Bayesian inference using Markov chain Monte Carlo (MCMC) algorithms. It can also supply graphical Bayesian goodness-of-fit procedures that address the issue of model adequacy. The package is simple to use and represents an attractive way of analysing network data as it offers the advantage of a complete probabilistic treatment of uncertainty. Bergm is based on the ergm package and therefore it makes use of the same model set-up and network simulation algorithms. The Bergm package has been continually improved in terms of speed performance over the last years and now offers the end-user a feasible option for carrying out Bayesian inference for networks with several thousands of nodes.,bergm packag provid comprehens framework bayesian infer use markov chain mont carlo mcmc algorithm also suppli graphic bayesian good fit procedur address issu model adequaci packag simpl use repres attract way analys network data offer advantag complet probabilist treatment uncertainti bergm base ergm packag therefor make use model set network simul algorithm bergm packag continu improv term speed perform last year offer end user feasibl option carri bayesian infer network sever thousand node,"['Alberto Caimo', 'Nial Friel']",['stat.CO'],False,False,False,False,False,True
1021,2017-03-28T14:03:17Z,2017-03-15T01:18:57Z,http://arxiv.org/abs/1703.04866v1,http://arxiv.org/pdf/1703.04866v1,Multilevel Sequential Monte Carlo with Dimension-Independent   Likelihood-Informed Proposals,multilevel sequenti mont carlo dimens independ likelihood inform propos,"In this article we develop a new sequential Monte Carlo (SMC) method for multilevel (ML) Monte Carlo estimation. In particular, the method can be used to estimate expectations with respect to a target probability distribution over an infinite-dimensional and non-compact space as given, for example, by a Bayesian inverse problem with Gaussian random field prior. Under suitable assumptions the MLSMC method has the optimal $O(\epsilon^{-2})$ bound on the cost to obtain a mean-square error of $O(\epsilon^2)$. The algorithm is accelerated by dimension-independent likelihood-informed (DILI) proposals designed for Gaussian priors, leveraging a novel variation which uses empirical sample covariance information in lieu of Hessian information, hence eliminating the requirement for gradient evaluations. The efficiency of the algorithm is illustrated on two examples: inversion of noisy pressure measurements in a PDE model of Darcy flow to recover the posterior distribution of the permeability field, and inversion of noisy measurements of the solution of an SDE to recover the posterior path measure.",articl develop new sequenti mont carlo smc method multilevel ml mont carlo estim particular method use estim expect respect target probabl distribut infinit dimension non compact space given exampl bayesian invers problem gaussian random field prior suitabl assumpt mlsmc method optim epsilon bound cost obtain mean squar error epsilon algorithm acceler dimens independ likelihood inform dili propos design gaussian prior leverag novel variat use empir sampl covari inform lieu hessian inform henc elimin requir gradient evalu effici algorithm illustr two exampl invers noisi pressur measur pde model darci flow recov posterior distribut permeabl field invers noisi measur solut sde recov posterior path measur,"['Alexandros Beskos', 'Ajay Jasra', 'Kody Law', 'Youssef Marzouk', 'Yan Zhou']",['stat.CO'],False,False,False,False,False,True
1023,2017-03-28T14:03:17Z,2017-03-13T11:19:28Z,http://arxiv.org/abs/1703.04334v1,http://arxiv.org/pdf/1703.04334v1,Probabilistic Matching: Causal Inference under Measurement Errors,probabilist match causal infer measur error,"The abundance of data produced daily from large variety of sources has boosted the need of novel approaches on causal inference analysis from observational data. Observational data often contain noisy or missing entries. Moreover, causal inference studies may require unobserved high-level information which needs to be inferred from other observed attributes. In such cases, inaccuracies of the applied inference methods will result in noisy outputs. In this study, we propose a novel approach for causal inference when one or more key variables are noisy. Our method utilizes the knowledge about the uncertainty of the real values of key variables in order to reduce the bias induced by noisy measurements. We evaluate our approach in comparison with existing methods both on simulated and real scenarios and we demonstrate that our method reduces the bias and avoids false causal inference conclusions in most cases.",abund data produc daili larg varieti sourc boost need novel approach causal infer analysi observ data observ data often contain noisi miss entri moreov causal infer studi may requir unobserv high level inform need infer observ attribut case inaccuraci appli infer method result noisi output studi propos novel approach causal infer one key variabl noisi method util knowledg uncertainti real valu key variabl order reduc bias induc noisi measur evalu approach comparison exist method simul real scenario demonstr method reduc bias avoid fals causal infer conclus case,"['Fani Tsapeli', 'Peter Tino', 'Mirco Musolesi']","['stat.ME', 'stat.CO', 'stat.ML']",False,False,False,False,False,True
1027,2017-03-28T14:03:17Z,2017-03-09T17:17:39Z,http://arxiv.org/abs/1703.03352v1,http://arxiv.org/pdf/1703.03352v1,A log-linear time algorithm for constrained changepoint detection,log linear time algorithm constrain changepoint detect,"Changepoint detection is a central problem in time series and genomic data. For some applications, it is natural to impose constraints on the directions of changes. One example is ChIP-seq data, for which adding an up-down constraint improves peak detection accuracy, but makes the optimization problem more complicated. We show how a recently proposed functional pruning technique can be adapted to solve such constrained changepoint detection problems. This leads to a new algorithm which can solve problems with arbitrary affine constraints on adjacent segment means, and which has empirical time complexity that is log-linear in the amount of data. This algorithm achieves state-of-the-art accuracy in a benchmark of several genomic data sets, and is orders of magnitude faster than existing algorithms that have similar accuracy. Our implementation is available as the PeakSegPDPA function in the coseg R package, https://github.com/tdhock/coseg",changepoint detect central problem time seri genom data applic natur impos constraint direct chang one exampl chip seq data ad constraint improv peak detect accuraci make optim problem complic show recent propos function prune techniqu adapt solv constrain changepoint detect problem lead new algorithm solv problem arbitrari affin constraint adjac segment mean empir time complex log linear amount data algorithm achiev state art accuraci benchmark sever genom data set order magnitud faster exist algorithm similar accuraci implement avail peaksegpdpa function coseg packag https github com tdhock coseg,"['Toby Dylan Hocking', 'Guillem Rigaill', 'Paul Fearnhead', 'Guillaume Bourque']","['stat.CO', 'q-bio.GN', 'stat.ML']",False,False,False,False,False,True
1028,2017-03-28T14:03:17Z,2017-03-10T21:00:26Z,http://arxiv.org/abs/1703.03004v2,http://arxiv.org/pdf/1703.03004v2,New approximation for GARCH parameters estimate,new approxim garch paramet estim,"This paper presents a new approach for the optimization of GARCH parameters estimation. Firstly, we propose a method for the localization of the maximum. Thereafter, using the methods of least squares, we make a local approximation for the projection of the likelihood function curve on two dimensional planes by a polynomial of order two which will be used to calculate an estimation of the maximum.",paper present new approach optim garch paramet estim first propos method local maximum thereaft use method least squar make local approxim project likelihood function curv two dimension plane polynomi order two use calcul estim maximum,"['Yakoub Boularouk', 'Nasr-eddine Hamri']",['stat.CO'],False,False,False,False,False,True
1029,2017-03-28T14:03:17Z,2017-03-08T19:22:11Z,http://arxiv.org/abs/1703.02998v1,http://arxiv.org/pdf/1703.02998v1,A note on quickly sampling a sparse matrix with low rank expectation,note quick sampl spars matrix low rank expect,"Given matrices $X,Y \in R^{n \times K}$ and $S \in R^{K \times K}$ with positive elements, this paper proposes an algorithm fastRG to sample a sparse matrix $A$ with low rank expectation $E(A) = XSY^T$ and independent Poisson elements. This allows for quickly sampling from a broad class of stochastic blockmodel graphs (degree-corrected, mixed membership, overlapping) all of which are specific parameterizations of the generalized random product graph model defined in Section 2.2. The basic idea of fastRG is to first sample the number of edges $m$ and then sample each edge. The key insight is that because of the the low rank expectation, it is easy to sample individual edges. The naive ""element-wise"" algorithm requires $O(n^2)$ operations to generate the $n\times n$ adjacency matrix $A$. In sparse graphs, where $m = O(n)$, ignoring log terms, fastRG runs in time $O(n)$. An implementation in fastRG is available on github. A computational experiment in Section 2.4 simulates graphs up to $n=10,000,000$ nodes with $m = 100,000,000$ edges. For example, on a graph with $n=500,000$ and $m = 5,000,000$, fastRG runs in less than one second on a 3.5 GHz Intel i5.",given matric time time posit element paper propos algorithm fastrg sampl spars matrix low rank expect xsi independ poisson element allow quick sampl broad class stochast blockmodel graph degre correct mix membership overlap specif parameter general random product graph model defin section basic idea fastrg first sampl number edg sampl edg key insight becaus low rank expect easi sampl individu edg naiv element wise algorithm requir oper generat time adjac matrix spars graph ignor log term fastrg run time implement fastrg avail github comput experi section simul graph node edg exampl graph fastrg run less one second ghz intel,"['Karl Rohe', 'Jun Tao', 'Xintian Han', 'Norbert Binkiewicz']",['stat.CO'],False,False,False,False,False,True
1030,2017-03-28T14:03:21Z,2017-03-07T18:36:55Z,http://arxiv.org/abs/1703.02518v1,http://arxiv.org/pdf/1703.02518v1,Faster Coordinate Descent via Adaptive Importance Sampling,faster coordin descent via adapt import sampl,"Coordinate descent methods employ random partial updates of decision variables in order to solve huge-scale convex optimization problems. In this work, we introduce new adaptive rules for the random selection of their updates. By adaptive, we mean that our selection rules are based on the dual residual or the primal-dual gap estimates and can change at each iteration. We theoretically characterize the performance of our selection rules and demonstrate improvements over the state-of-the-art, and extend our theory and algorithms to general convex objectives. Numerical evidence with hinge-loss support vector machines and Lasso confirm that the practice follows the theory.",coordin descent method employ random partial updat decis variabl order solv huge scale convex optim problem work introduc new adapt rule random select updat adapt mean select rule base dual residu primal dual gap estim chang iter theoret character perform select rule demonstr improv state art extend theori algorithm general convex object numer evid hing loss support vector machin lasso confirm practic follow theori,"['Dmytro Perekrestenko', 'Volkan Cevher', 'Martin Jaggi']","['cs.LG', 'cs.CV', 'math.OC', 'stat.CO', 'stat.ML', 'G.1.6']",False,False,False,False,False,True
1031,2017-03-28T14:03:21Z,2017-03-07T15:13:08Z,http://arxiv.org/abs/1703.02428v1,http://arxiv.org/pdf/1703.02428v1,Robust Bayesian Filtering and Smoothing Using Student's t Distribution,robust bayesian filter smooth use student distribut,"State estimation in heavy-tailed process and measurement noise is an important challenge that must be addressed in, e.g., tracking scenarios with agile targets and outlier-corrupted measurements. The performance of the Kalman filter (KF) can deteriorate in such applications because of the close relation to the Gaussian distribution. Therefore, this paper describes the use of Student's t distribution to develop robust, scalable, and simple filtering and smoothing algorithms.   After a discussion of Student's t distribution, exact filtering in linear state-space models with t noise is analyzed. Intermediate approximation steps are used to arrive at filtering and smoothing algorithms that closely resemble the KF and the Rauch-Tung-Striebel (RTS) smoother except for a nonlinear measurement-dependent matrix update. The required approximations are discussed and an undesirable behavior of moment matching for t densities is revealed. A favorable approximation based on minimization of the Kullback-Leibler divergence is presented. Because of its relation to the KF, some properties and algorithmic extensions are inherited by the t filter. Instructive simulation examples demonstrate the performance and robustness of the novel algorithms.",state estim heavi tail process measur nois import challeng must address track scenario agil target outlier corrupt measur perform kalman filter kf deterior applic becaus close relat gaussian distribut therefor paper describ use student distribut develop robust scalabl simpl filter smooth algorithm discuss student distribut exact filter linear state space model nois analyz intermedi approxim step use arriv filter smooth algorithm close resembl kf rauch tung striebel rts smoother except nonlinear measur depend matrix updat requir approxim discuss undesir behavior moment match densiti reveal favor approxim base minim kullback leibler diverg present becaus relat kf properti algorithm extens inherit filter instruct simul exampl demonstr perform robust novel algorithm,"['Michael Roth', 'Tohid Ardeshiri', 'Emre Özkan', 'Fredrik Gustafsson']","['stat.ME', 'cs.SY', 'stat.CO']",False,False,False,False,False,True
1032,2017-03-28T14:03:21Z,2017-03-07T15:01:51Z,http://arxiv.org/abs/1703.02419v1,http://arxiv.org/pdf/1703.02419v1,Probabilistic learning of nonlinear dynamical systems using sequential   Monte Carlo,probabilist learn nonlinear dynam system use sequenti mont carlo,"Probabilistic modeling provides the capability to represent and manipulate uncertainty in data, models, decisions and predictions. We are concerned with the problem of learning probabilistic models of dynamical systems from measured data. Specifically, we consider learning of probabilistic nonlinear state space models. There is no closed-form solution available for this problem, implying that we are forced to use approximations. In this tutorial we will provide a self-contained introduction to one of the state-of-the-art methods---the particle Metropolis-Hastings algorithm---which has proven to offer very practical approximations. This is a Monte Carlo based method, where the so-called particle filter is used to guide a Markov chain Monte Carlo method through the parameter space. One of the key merits of the particle Metropolis-Hastings method is that it is guaranteed to converge to the ""true solution"" under mild assumptions, despite being based on a practical implementation of a particle filter (i.e., using a finite number of particles). We will also provide a motivating numerical example illustrating the method which we have implemented in an in-house developed modeling language, serving the purpose of abstracting away the underlying mathematics of the Monte Carlo approximations from the user. This modeling language will open up the power of sophisticated Monte Carlo methods, including particle Metropolis-Hastings, to a large group of users without requiring them to know all the underlying mathematical details.",probabilist model provid capabl repres manipul uncertainti data model decis predict concern problem learn probabilist model dynam system measur data specif consid learn probabilist nonlinear state space model close form solut avail problem impli forc use approxim tutori provid self contain introduct one state art method particl metropoli hast algorithm proven offer veri practic approxim mont carlo base method call particl filter use guid markov chain mont carlo method paramet space one key merit particl metropoli hast method guarante converg true solut mild assumpt despit base practic implement particl filter use finit number particl also provid motiv numer exampl illustr method implement hous develop model languag serv purpos abstract away mathemat mont carlo approxim user model languag open power sophist mont carlo method includ particl metropoli hast larg group user without requir know mathemat detail,"['Thomas B. Schön', 'Andreas Svensson', 'Lawrence Murray', 'Fredrik Lindsten']","['stat.CO', 'cs.LG', 'cs.SY']",False,False,False,False,False,True
1033,2017-03-28T14:03:21Z,2017-03-07T11:48:45Z,http://arxiv.org/abs/1703.02341v1,http://arxiv.org/pdf/1703.02341v1,An automatic adaptive method to combine summary statistics in   approximate Bayesian computation,automat adapt method combin summari statist approxim bayesian comput,"To infer the parameters of mechanistic models with intractable likelihoods, techniques such as approximate Bayesian computation (ABC) are increasingly being adopted. One of the main disadvantages of ABC in practical situations, however, is that parameter inference must generally rely on summary statistics of the data. This is particularly the case for problems involving high-dimensional data, such as biological imaging experiments. However, some summary statistics contain more information about parameters of interest than others, and it is not always clear how to weight their contributions within the ABC framework. We address this problem by developing an automatic, adaptive algorithm that chooses weights for each summary statistic. Our algorithm aims to maximize the distance between the prior and the approximate posterior by automatically adapting the weights within the ABC distance function. To demonstrate the effectiveness of our algorithm, we apply it to several stochastic models of biochemical reaction networks, and a spatial model of diffusion, and compare our results with existing algorithms.",infer paramet mechanist model intract likelihood techniqu approxim bayesian comput abc increas adopt one main disadvantag abc practic situat howev paramet infer must general reli summari statist data particular case problem involv high dimension data biolog imag experi howev summari statist contain inform paramet interest alway clear weight contribut within abc framework address problem develop automat adapt algorithm choos weight summari statist algorithm aim maxim distanc prior approxim posterior automat adapt weight within abc distanc function demonstr effect algorithm appli sever stochast model biochem reaction network spatial model diffus compar result exist algorithm,"['Jonathan U Harrison', 'Ruth E Baker']",['stat.CO'],False,False,False,False,False,True
1035,2017-03-28T14:03:21Z,2017-03-07T09:33:21Z,http://arxiv.org/abs/1703.02293v1,http://arxiv.org/pdf/1703.02293v1,Variable selection for mixed data clustering: a model-based approach,variabl select mix data cluster model base approach,"We propose two approaches for selecting variables in latent class analysis (i.e.,mixture model assuming within component independence), which is the common model-based clustering method for mixed data. The first approach consists in optimizing the BIC with a modified version of the EM algorithm. This approach simultaneously performs both model selection and parameter inference. The second approach consists in maximizing the MICL, which considers the clustering task, with an algorithm of alternate optimization. This approach performs model selection without requiring the maximum likelihood estimates for model comparison, then parameter inference is done for the unique selected model. Thus, the benefits of both approaches is to avoid the computation of the maximum likelihood estimates for each model comparison. Moreover, they also avoid the use of the standard algorithms for variable selection which are often suboptimal (e.g. stepwise method) and computationally expensive. The case of data with missing values is also discussed. The interest of both proposed criteria is shown on simulated and real data.",propos two approach select variabl latent class analysi mixtur model assum within compon independ common model base cluster method mix data first approach consist optim bic modifi version em algorithm approach simultan perform model select paramet infer second approach consist maxim micl consid cluster task algorithm altern optim approach perform model select without requir maximum likelihood estim model comparison paramet infer done uniqu select model thus benefit approach avoid comput maximum likelihood estim model comparison moreov also avoid use standard algorithm variabl select often suboptim stepwis method comput expens case data miss valu also discuss interest propos criteria shown simul real data,"['Matthieu Marbac', 'Mohammed Sedki']","['stat.CO', '62F15']",False,False,False,False,False,True
1036,2017-03-28T14:03:21Z,2017-03-07T07:44:52Z,http://arxiv.org/abs/1703.02251v1,http://arxiv.org/pdf/1703.02251v1,The Maximum Likelihood Degree of Toric Varieties,maximum likelihood degre toric varieti,"We study the maximum likelihood degree (ML degree) of toric varieties, known as discrete exponential models in statistics. By introducing scaling coefficients to the monomial parameterization of the toric variety, one can change the ML degree. We show that the ML degree is equal to the degree of the toric variety for generic scalings, while it drops if and only if the scaling vector is in the locus of the principal $A$-determinant. We also illustrate how to compute the ML estimate of a toric variety numerically via homotopy continuation from a scaled toric variety with low ML degree. Throughout, we include examples motivated by algebraic geometry and statistics. We compute the ML degree of rational normal scrolls and a large class of Veronese-type varieties. In addition, we investigate the ML degree of scaled Segre varieties, hierarchical loglinear models, and graphical models.",studi maximum likelihood degre ml degre toric varieti known discret exponenti model statist introduc scale coeffici monomi parameter toric varieti one chang ml degre show ml degre equal degre toric varieti generic scale drop onli scale vector locus princip determin also illustr comput ml estim toric varieti numer via homotopi continu scale toric varieti low ml degre throughout includ exampl motiv algebra geometri statist comput ml degre ration normal scroll larg class verones type varieti addit investig ml degre scale segr varieti hierarch loglinear model graphic model,"['Carlos Améndola', 'Nathan Bliss', 'Isaac Burke', 'Courtney R. Gibbons', 'Martin Helmer', 'Serkan Hoşten', 'Evan D. Nash', 'Jose Israel Rodriguez', 'Daniel Smolkin']","['math.AG', 'math.ST', 'stat.CO', 'stat.TH', '14Q15, 14M25, 13P15, 62F10']",False,False,True,False,False,True
1042,2017-03-28T14:03:25Z,2017-03-04T09:12:42Z,http://arxiv.org/abs/1703.01421v1,http://arxiv.org/pdf/1703.01421v1,$l_0$-estimation of piecewise-constant signals on graphs,estim piecewis constant signal graph,"We study recovery of piecewise-constant signals over arbitrary graphs by the estimator minimizing an $l_0$-edge-penalized objective. Although exact minimization of this objective may be computationally intractable, we show that the same statistical risk guarantees are achieved by the alpha-expansion algorithm which approximately minimizes this objective in polynomial time. We establish that for graphs with small average vertex degree, these guarantees are rate-optimal in a minimax sense over classes of edge-sparse signals. For application to spatially inhomogeneous graphs, we propose minimization of an edge-weighted variant of this objective where each edge is weighted by its effective resistance or another measure of its contribution to the graph's connectivity. We establish minimax optimality of the resulting estimators over corresponding edge-weighted sparsity classes. We show theoretically that these risk guarantees are not always achieved by the estimator minimizing the $l_1$/total-variation relaxation, and empirically that the $l_0$-based estimates are more accurate in high signal-to-noise settings.",studi recoveri piecewis constant signal arbitrari graph estim minim edg penal object although exact minim object may comput intract show statist risk guarante achiev alpha expans algorithm approxim minim object polynomi time establish graph small averag vertex degre guarante rate optim minimax sens class edg spars signal applic spatial inhomogen graph propos minim edg weight variant object edg weight effect resist anoth measur contribut graph connect establish minimax optim result estim correspond edg weight sparsiti class show theoret risk guarante alway achiev estim minim total variat relax empir base estim accur high signal nois set,"['Zhou Fan', 'Leying Guan']","['stat.ME', 'math.ST', 'stat.CO', 'stat.TH']",False,False,False,False,False,True
1043,2017-03-28T14:03:25Z,2017-03-03T18:07:15Z,http://arxiv.org/abs/1703.01273v1,http://arxiv.org/pdf/1703.01273v1,Estimating Spatial Econometrics Models with Integrated Nested Laplace   Approximation,estim spatial econometr model integr nest laplac approxim,"Integrated Nested Laplace Approximation provides a fast and effective method for marginal inference on Bayesian hierarchical models. This methodology has been implemented in the R-INLA package which permits INLA to be used from within R statistical software. Although INLA is implemented as a general methodology, its use in practice is limited to the models implemented in the R-INLA package.   Spatial autoregressive models are widely used in spatial econometrics but have until now been missing from the R-INLA package. In this paper, we describe the implementation and application of a new class of latent models in INLA made available through R-INLA. This new latent class implements a standard spatial lag model, which is widely used and that can be used to build more complex models in spatial econometrics.   The implementation of this latent model in R-INLA also means that all the other features of INLA can be used for model fitting, model selection and inference in spatial econometrics, as will be shown in this paper. Finally, we will illustrate the use of this new latent model and its applications with two datasets based on Gaussian and binary outcomes.",integr nest laplac approxim provid fast effect method margin infer bayesian hierarch model methodolog implement inla packag permit inla use within statist softwar although inla implement general methodolog use practic limit model implement inla packag spatial autoregress model wide use spatial econometr miss inla packag paper describ implement applic new class latent model inla made avail inla new latent class implement standard spatial lag model wide use use build complex model spatial econometr implement latent model inla also mean featur inla use model fit model select infer spatial econometr shown paper final illustr use new latent model applic two dataset base gaussian binari outcom,"['Virgilio Gomez-Rubio', 'Roger S. Bivand', 'Håvard Rue']",['stat.CO'],False,False,False,False,False,True
1044,2017-03-28T14:03:25Z,2017-03-03T16:29:21Z,http://arxiv.org/abs/1703.01234v1,http://arxiv.org/pdf/1703.01234v1,A Bayesian computer model analysis of Robust Bayesian analyses,bayesian comput model analysi robust bayesian analys,"We harness the power of Bayesian emulation techniques, designed to aid the analysis of complex computer models, to examine the structure of complex Bayesian analyses themselves. These techniques facilitate robust Bayesian analyses and/or sensitivity analyses of complex problems, and hence allow global exploration of the impacts of choices made in both the likelihood and prior specification. We show how previously intractable problems in robustness studies can be overcome using emulation techniques, and how these methods allow other scientists to quickly extract approximations to posterior results corresponding to their own particular subjective specification. The utility and flexibility of our method is demonstrated on a reanalysis of a real application where Bayesian methods were employed to capture beliefs about river flow. We discuss the obvious extensions and directions of future research that such an approach opens up.",har power bayesian emul techniqu design aid analysi complex comput model examin structur complex bayesian analys themselv techniqu facilit robust bayesian analys sensit analys complex problem henc allow global explor impact choic made likelihood prior specif show previous intract problem robust studi overcom use emul techniqu method allow scientist quick extract approxim posterior result correspond particular subject specif util flexibl method demonstr reanalysi real applic bayesian method employ captur belief river flow discuss obvious extens direct futur research approach open,"['Ian Vernon', 'John Paul Gosling']","['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']",False,False,False,False,False,True
1047,2017-03-28T14:03:25Z,2017-03-19T16:03:04Z,http://arxiv.org/abs/1703.00368v2,http://arxiv.org/pdf/1703.00368v2,Approximate Computational Approaches for Bayesian Sensor Placement in   High Dimensions,approxim comput approach bayesian sensor placement high dimens,"Since the cost of installing and maintaining sensors is usually high, sensor locations are always strategically selected. For those aiming at inferring certain quantities of interest (QoI), it is desirable to explore the dependency between sensor measurements and QoI. One of the most popular metric for the dependency is mutual information which naturally measures how much information about one variable can be obtained given the other. However, computing mutual information is always challenging, and the result is unreliable in high dimension. In this paper, we propose an approach to find an approximate lower bound of mutual information and compute it in a lower dimension. Then, sensors are placed where highest mutual information (lower bound) is achieved and QoI is inferred via Bayes rule given sensor measurements. In addition, Bayesian optimization is introduced to provide a continuous mutual information surface over the domain and thus reduce the number of evaluations. A chemical release accident is simulated where multiple sensors are placed to locate the source of the release. The result shows that the proposed approach is both effective and efficient in inferring QoI.",sinc cost instal maintain sensor usual high sensor locat alway strateg select aim infer certain quantiti interest qoi desir explor depend sensor measur qoi one popular metric depend mutual inform natur measur much inform one variabl obtain given howev comput mutual inform alway challeng result unreli high dimens paper propos approach find approxim lower bound mutual inform comput lower dimens sensor place highest mutual inform lower bound achiev qoi infer via bay rule given sensor measur addit bayesian optim introduc provid continu mutual inform surfac domain thus reduc number evalu chemic releas accid simul multipl sensor place locat sourc releas result show propos approach effect effici infer qoi,"['Xiao Lin', 'Asif Chowdhury', 'Xiaofan Wang', 'Gabriel Terejanu']",['stat.CO'],False,False,False,False,False,True
1048,2017-03-28T14:03:25Z,2017-03-13T17:28:04Z,http://arxiv.org/abs/1702.08896v2,http://arxiv.org/pdf/1702.08896v2,Deep and Hierarchical Implicit Models,deep hierarch implicit model,"Implicit probabilistic models are a flexible class for modeling data. They define a process to simulate observations, and unlike traditional models, they do not require a tractable likelihood function. In this paper, we develop two families of models: hierarchical implicit models and deep implicit models. They combine the idea of implicit densities with hierarchical Bayesian modeling and deep neural networks. The use of implicit models with Bayesian analysis has been limited by our ability to perform accurate and scalable inference. We develop likelihood-free variational inference (LFVI). Key to LFVI is specifying a variational family that is also implicit. This matches the model's flexibility and allows for accurate approximation of the posterior. Our work scales up implicit models to sizes previously not possible and advances their modeling design. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for text generation.",implicit probabilist model flexibl class model data defin process simul observ unlik tradit model requir tractabl likelihood function paper develop two famili model hierarch implicit model deep implicit model combin idea implicit densiti hierarch bayesian model deep neural network use implicit model bayesian analysi limit abil perform accur scalabl infer develop likelihood free variat infer lfvi key lfvi specifi variat famili also implicit match model flexibl allow accur approxim posterior work scale implicit model size previous possibl advanc model design demonstr divers applic larg scale physic simul predat prey popul ecolog bayesian generat adversari network discret data deep implicit model text generat,"['Dustin Tran', 'Rajesh Ranganath', 'David M. Blei']","['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']",False,False,False,False,False,True
1049,2017-03-28T14:03:25Z,2017-02-28T16:31:21Z,http://arxiv.org/abs/1702.08849v1,http://arxiv.org/pdf/1702.08849v1,Multi-Sensor Multi-object Tracking with the Generalized Labeled   Multi-Bernoulli Filter,multi sensor multi object track general label multi bernoulli filter,This paper proposes an efficient implementation of the multi-sensor generalized labeled multi-Bernoulli (GLMB) filter. The solution exploits the GLMB joint prediction and update together with a new technique for truncating the GLMB filtering density based on Gibbs sampling. The resulting algorithm has quadratic complexity in the number of hypothesized object and linear in the number of measurements of each individual sensors.,paper propos effici implement multi sensor general label multi bernoulli glmb filter solut exploit glmb joint predict updat togeth new techniqu truncat glmb filter densiti base gibb sampl result algorithm quadrat complex number hypothes object linear number measur individu sensor,"['Ba Ngu Vo', 'Ba Tuong Vo']",['stat.CO'],False,False,False,False,False,True
1050,2017-03-28T14:03:30Z,2017-02-28T13:34:02Z,http://arxiv.org/abs/1702.08781v1,http://arxiv.org/pdf/1702.08781v1,General Bayesian inference schemes in infinite mixture models,general bayesian infer scheme infinit mixtur model,"Bayesian statistical models allow us to formalise our knowledge about the world and reason about our uncertainty, but there is a need for better procedures to accurately encode its complexity. One way to do so is through compositional models, which are formed by combining blocks consisting of simpler models. One can increase the complexity of the compositional model by either stacking more blocks or by using a not-so-simple model as a building block. This thesis is an example of the latter. One first aim is to expand the choice of Bayesian nonparametric (BNP) blocks for constructing tractable compositional models. So far, most of the models that have a Bayesian nonparametric component use a Dirichlet Process or a Pitman-Yor process because of the availability of tractable and compact representations. This thesis shows how to overcome certain intractabilities in order to obtain analogous compact representations for the class of Poisson-Kingman priors which includes the Dirichlet and Pitman-Yor processes.   A major impediment to the widespread use of Bayesian nonparametric building blocks is that inference is often costly, intractable or difficult to carry out. This is an active research area since dealing with the model's infinite dimensional component forbids the direct use of standard simulation-based methods. The main contribution of this thesis is a variety of inference schemes that tackle this problem: Markov chain Monte Carlo and Sequential Monte Carlo methods, which are exact inference schemes since they target the true posterior. The contributions of this thesis, in a larger context, provide general purpose exact inference schemes in the flavour or probabilistic programming: the user is able to choose from a variety of models, focusing only on the modelling part. Indeed, if the wide enough class of Poisson-Kingman priors is used as one of our blocks, this objective is achieved.",bayesian statist model allow us formalis knowledg world reason uncertainti need better procedur accur encod complex one way composit model form combin block consist simpler model one increas complex composit model either stack block use simpl model build block thesi exampl latter one first aim expand choic bayesian nonparametr bnp block construct tractabl composit model far model bayesian nonparametr compon use dirichlet process pitman yor process becaus avail tractabl compact represent thesi show overcom certain intract order obtain analog compact represent class poisson kingman prior includ dirichlet pitman yor process major impedi widespread use bayesian nonparametr build block infer often cost intract difficult carri activ research area sinc deal model infinit dimension compon forbid direct use standard simul base method main contribut thesi varieti infer scheme tackl problem markov chain mont carlo sequenti mont carlo method exact infer scheme sinc target true posterior contribut thesi larger context provid general purpos exact infer scheme flavour probabilist program user abl choos varieti model focus onli model part inde wide enough class poisson kingman prior use one block object achiev,['Maria Lomeli'],['stat.CO'],False,False,False,False,False,True
1052,2017-03-28T14:03:30Z,2017-02-27T22:52:29Z,http://arxiv.org/abs/1702.08572v1,http://arxiv.org/pdf/1702.08572v1,Comparison of Confidence Interval Estimators: an Index Approach,comparison confid interv estim index approach,"We develop a confidence interval index for comparing confidence interval estimators based on the confidence interval length and coverage probability. We show that the confidence interval index has range of values within the neighborhood of the range of the coverage probability, [0,1]. In addition, a good confidence interval estimator is shown to have an index value approaching 1; and a bad confidence interval has an index value approaching 0. A simulation study is conducted to assess the finite sample performance of the index. Finally, the proposed index is illustrated with a practical example from the literature.",develop confid interv index compar confid interv estim base confid interv length coverag probabl show confid interv index rang valu within neighborhood rang coverag probabl addit good confid interv estim shown index valu approach bad confid interv index valu approach simul studi conduct assess finit sampl perform index final propos index illustr practic exampl literatur,"['Richard Minkah', 'Tertius de Wet']","['stat.ME', 'stat.CO', '62F99, 62G99']",False,False,False,False,False,True
1054,2017-03-28T14:03:30Z,2017-02-27T12:12:46Z,http://arxiv.org/abs/1702.08251v1,http://arxiv.org/pdf/1702.08251v1,Hessian corrections to Hybrid Monte Carlo,hessian correct hybrid mont carlo,"A method for the introduction of second-order derivatives of the log likelihood into HMC algorithms is introduced, which does not require the Hessian to be evaluated at each leapfrog step but only at the start and end of trajectories.",method introduct second order deriv log likelihood hmc algorithm introduc doe requir hessian evalu leapfrog step onli start end trajectori,['Thomas House'],['stat.CO'],False,False,False,False,False,True
1056,2017-03-28T14:03:30Z,2017-02-27T08:42:49Z,http://arxiv.org/abs/1702.08188v1,http://arxiv.org/pdf/1702.08188v1,dotCall64: An Efficient Interface to Compiled C/C++ and Fortran Code   Supporting Long Vectors,dotcal effici interfac compil fortran code support long vector,"The R functions .C() and .Fortran() can be used to call compiled C/C++ and Fortran code from R. This so-called foreign function interface is convenient, since it does not require any interactions with the C API of R. However, it does not support long vectors (i.e., vectors of more than 2^31 elements). To overcome this limitation, the R package dotCall64 provides .C64(), which can be used to call compiled C/C++ and Fortran functions. It transparently supports long vectors and does the necessary castings to pass numeric R vectors to 64-bit integer arguments of the compiled code. Moreover, .C64() features a mechanism to avoid unnecessary copies of function arguments, making it efficient in terms of speed and memory usage.",function fortran use call compil fortran code call foreign function interfac conveni sinc doe requir ani interact api howev doe support long vector vector element overcom limit packag dotcal provid use call compil fortran function transpar support long vector doe necessari cast pass numer vector bit integ argument compil code moreov featur mechan avoid unnecessari copi function argument make effici term speed memori usag,"['Florian Gerber', 'Kaspar Mösinger', 'Reinhard Furrer']",['stat.CO'],False,False,False,False,False,True
1057,2017-03-28T14:03:30Z,2017-02-27T08:33:26Z,http://arxiv.org/abs/1702.08185v1,http://arxiv.org/pdf/1702.08185v1,An update on statistical boosting in biomedicine,updat statist boost biomedicin,"Statistical boosting algorithms have triggered a lot of research during the last decade. They combine a powerful machine-learning approach with classical statistical modelling, offering various practical advantages like automated variable selection and implicit regularization of effect estimates. They are extremely flexible, as the underlying base-learners (regression functions defining the type of effect for the explanatory variables) can be combined with any kind of loss function (target function to be optimized, defining the type of regression setting). In this review article, we highlight the most recent methodological developments on statistical boosting regarding variable selection, functional regression and advanced time-to-event modelling. Additionally, we provide a short overview on relevant applications of statistical boosting in biomedicine.",statist boost algorithm trigger lot research dure last decad combin power machin learn approach classic statist model offer various practic advantag like autom variabl select implicit regular effect estim extrem flexibl base learner regress function defin type effect explanatori variabl combin ani kind loss function target function optim defin type regress set review articl highlight recent methodolog develop statist boost regard variabl select function regress advanc time event model addit provid short overview relev applic statist boost biomedicin,"['Andreas Mayr', 'Benjamin Hofner', 'Elisabeth Waldmann', 'Tobias Hepp', 'Olaf Gefeller', 'Matthias Schmid']","['stat.AP', 'stat.CO', 'stat.ML']",False,False,False,False,False,True
1059,2017-03-28T14:03:30Z,2017-02-28T09:59:57Z,http://arxiv.org/abs/1702.08061v2,http://arxiv.org/pdf/1702.08061v2,The Ensemble Kalman Filter: A Signal Processing Perspective,ensembl kalman filter signal process perspect,"The ensemble Kalman filter (EnKF) is a Monte Carlo based implementation of the Kalman filter (KF) for extremely high-dimensional, possibly nonlinear and non-Gaussian state estimation problems. Its ability to handle state dimensions in the order of millions has made the EnKF a popular algorithm in different geoscientific disciplines. Despite a similarly vital need for scalable algorithms in signal processing, e.g., to make sense of the ever increasing amount of sensor data, the EnKF is hardly discussed in our field.   This self-contained review paper is aimed at signal processing researchers and provides all the knowledge to get started with the EnKF. The algorithm is derived in a KF framework, without the often encountered geoscientific terminology. Algorithmic challenges and required extensions of the EnKF are provided, as well as relations to sigma-point KF and particle filters. The relevant EnKF literature is summarized in an extensive survey and unique simulation examples, including popular benchmark problems, complement the theory with practical insights. The signal processing perspective highlights new directions of research and facilitates the exchange of potentially beneficial ideas, both for the EnKF and high-dimensional nonlinear and non-Gaussian filtering in general.",ensembl kalman filter enkf mont carlo base implement kalman filter kf extrem high dimension possibl nonlinear non gaussian state estim problem abil handl state dimens order million made enkf popular algorithm differ geoscientif disciplin despit similar vital need scalabl algorithm signal process make sens ever increas amount sensor data enkf hard discuss field self contain review paper aim signal process research provid knowledg get start enkf algorithm deriv kf framework without often encount geoscientif terminolog algorithm challeng requir extens enkf provid well relat sigma point kf particl filter relev enkf literatur summar extens survey uniqu simul exampl includ popular benchmark problem complement theori practic insight signal process perspect highlight new direct research facilit exchang potenti benefici idea enkf high dimension nonlinear non gaussian filter general,"['Michael Roth', 'Gustaf Hendeby', 'Carsten Fritsche', 'Fredrik Gustafsson']","['stat.ME', 'cs.SY', 'stat.CO']",False,False,False,False,False,True
1061,2017-03-28T14:03:34Z,2017-02-25T17:47:33Z,http://arxiv.org/abs/1702.07930v1,http://arxiv.org/pdf/1702.07930v1,Upper-Bounding the Regularization Constant for Convex Sparse Signal   Reconstruction,upper bound regular constant convex spars signal reconstruct,Consider reconstructing a signal $x$ by minimizing a weighted sum of a convex differentiable negative log-likelihood (NLL) (data-fidelity) term and a convex regularization term that imposes a convex-set constraint on $x$ and enforces its sparsity using $\ell_1$-norm analysis regularization. We compute upper bounds on the regularization tuning constant beyond which the regularization term overwhelmingly dominates the NLL term so that the set of minimum points of the objective function does not change. Necessary and sufficient conditions for irrelevance of sparse signal regularization and a condition for the existence of finite upper bounds are established. We formulate an optimization problem for finding these bounds when the regularization term can be globally minimized by a feasible $x$ and also develop an alternating direction method of multipliers (ADMM) type method for their computation. Simulation examples show that the derived and empirical bounds match.,consid reconstruct signal minim weight sum convex differenti negat log likelihood nll data fidel term convex regular term impos convex set constraint enforc sparsiti use ell norm analysi regular comput upper bound regular tune constant beyond regular term overwhelm domin nll term set minimum point object function doe chang necessari suffici condit irrelev spars signal regular condit exist finit upper bound establish formul optim problem find bound regular term global minim feasibl also develop altern direct method multipli admm type method comput simul exampl show deriv empir bound match,"['Renliang Gu', 'Aleksandar Dogandžić']","['stat.CO', 'math.OC']",False,False,False,False,False,True
1062,2017-03-28T14:03:34Z,2017-02-25T03:46:20Z,http://arxiv.org/abs/1702.07830v1,http://arxiv.org/pdf/1702.07830v1,A Near-Optimal Sampling Strategy for Sparse Recovery of Polynomial Chaos   Expansions,near optim sampl strategi spars recoveri polynomi chao expans,"Compressive sampling has become a widely used approach to construct polynomial chaos surrogates when the number of available simulation samples is limited. Originally, these expensive simulation samples would be obtained at random locations in the parameter space. It was later shown that the choice of sample locations could significantly impact the accuracy of resulting surrogates. This motivated new sampling strategies or design-of-experiment approaches, such as coherence-optimal sampling, which aim at improving the coherence property. In this paper, we propose a sampling strategy that can identify near-optimal sample locations that lead to improvement in local-coherence property and also enhancement of cross-correlation properties of measurement matrices. We provide theoretical motivations for the proposed sampling strategy along with several numerical examples that show that our near-optimal sampling strategy produces substantially more accurate results, compared to other sampling strategies.",compress sampl becom wide use approach construct polynomi chao surrog number avail simul sampl limit origin expens simul sampl would obtain random locat paramet space later shown choic sampl locat could signific impact accuraci result surrog motiv new sampl strategi design experi approach coher optim sampl aim improv coher properti paper propos sampl strategi identifi near optim sampl locat lead improv local coher properti also enhanc cross correl properti measur matric provid theoret motiv propos sampl strategi along sever numer exampl show near optim sampl strategi produc substanti accur result compar sampl strategi,"['Negin Alemazkoor', 'Hadi Meidani']",['stat.CO'],False,False,False,False,False,True
1066,2017-03-28T14:03:34Z,2017-02-23T04:40:41Z,http://arxiv.org/abs/1702.07094v1,http://arxiv.org/pdf/1702.07094v1,BigVAR: Tools for Modeling Sparse High-Dimensional Multivariate Time   Series,bigvar tool model spars high dimension multivari time seri,"The R package BigVAR allows for the simultaneous estimation of high-dimensional time series by applying structured penalties to the conventional vector autoregression (VAR) and vector autoregression with exogenous variables (VARX) frameworks. Our methods can be utilized in many forecasting applications that make use of time-dependent data such as macroeconomics, finance, and internet traffic. Our package extends solution algorithms from the machine learning and signal processing literatures to a time dependent setting: selecting the regularization parameter by sequential cross validation and provides substantial improvements in forecasting performance over conventional methods. We offer a user-friendly interface that utilizes R's s4 object class structure which makes our methodology easily accessible to practicioners.   In this paper, we present an overview of our notation, the models that comprise BigVAR, and the functionality of our package with a detailed example using publicly available macroeconomic data. In addition, we present a simulation study comparing the performance of several procedures that refit the support selected by a BigVAR procedure according to several variants of least squares and conclude that refitting generally degrades forecast performance.",packag bigvar allow simultan estim high dimension time seri appli structur penalti convent vector autoregress var vector autoregress exogen variabl varx framework method util mani forecast applic make use time depend data macroeconom financ internet traffic packag extend solut algorithm machin learn signal process literatur time depend set select regular paramet sequenti cross valid provid substanti improv forecast perform convent method offer user friend interfac util object class structur make methodolog easili access practicion paper present overview notat model compris bigvar function packag detail exampl use public avail macroeconom data addit present simul studi compar perform sever procedur refit support select bigvar procedur accord sever variant least squar conclud refit general degrad forecast perform,"['William Nicholson', 'David Matteson', 'Jacob Bien']",['stat.CO'],False,False,True,False,False,True
1067,2017-03-28T14:03:34Z,2017-02-22T00:43:01Z,http://arxiv.org/abs/1702.06632v1,http://arxiv.org/pdf/1702.06632v1,A Balanced Algorithm for Sampling Abstract Simplicial Complexes,balanc algorithm sampl abstract simplici complex,"We provide an algorithm for sampling the space of abstract simplicial complexes on a fixed number of vertices that aims to provide a balanced sampling over non-isomorphic complexes. Although sampling uniformly from geometrically distinct complexes is a difficult task with no known analytic algorithm, our generative and descriptive algorithm is designed with heuristics to help balance the combinatorial multiplicities of the states and more widely sample across the space of inequivalent configurations. We provide a formula for the exact probabilities with which this algorithm will produce a requested labeled state, and compare the algorithm to Kahle's multi-parameter model of exponential random simplicial complexes, demonstrating analytically that our algorithm performs better with respect to worst-case probability bounds on a given complex and providing numerical results illustrating the increased sampling efficiency over distinct classes.",provid algorithm sampl space abstract simplici complex fix number vertic aim provid balanc sampl non isomorph complex although sampl uniform geometr distinct complex difficult task known analyt algorithm generat descript algorithm design heurist help balanc combinatori multipl state wide sampl across space inequival configur provid formula exact probabl algorithm produc request label state compar algorithm kahl multi paramet model exponenti random simplici complex demonstr analyt algorithm perform better respect worst case probabl bound given complex provid numer result illustr increas sampl effici distinct class,['John Lombard'],"['stat.CO', 'math.CO', 'math.PR']",False,False,False,False,False,True
1072,2017-03-28T14:03:38Z,2017-02-17T21:16:40Z,http://arxiv.org/abs/1702.05518v1,http://arxiv.org/pdf/1702.05518v1,Sampling Strategies for Fast Updating of Gaussian Markov Random Fields,sampl strategi fast updat gaussian markov random field,"Gaussian Markov random fields (GMRFs) are popular for modeling temporal or spatial dependence in large areal datasets due to their ease of interpretation and computational convenience afforded by conditional independence and their sparse precision matrices needed for random variable generation. Using such models inside a Markov chain Monte Carlo algorithm requires repeatedly simulating random fields. This is a nontrivial issue, especially when the full conditional precision matrix depends on parameters that change at each iteration. Typically in Bayesian computation, GMRFs are updated jointly in a block Gibbs sampler or one location at a time in a single-site sampler. The former approach leads to quicker convergence by updating correlated variables all at once, while the latter avoids solving large matrices. Efficient algorithms for sampling Markov random fields have become the focus of much recent research in the machine learning literature, much of which can be useful to statisticians. We briefly review recently proposed approaches with an eye toward implementation for statisticians without expertise in numerical analysis or advanced computing. In particular, we consider a version of block sampling in which the underlying graph can be cut so that conditionally independent sites are all updated together. This algorithm allows a practitioner to parallelize the updating of a subset locations or to take advantage of `vectorized' calculations in a high-level language such as R. Through both simulation and real data application, we demonstrate computational savings that can be achieved versus both traditional single-site updating and block updating, regardless of whether the data are on a regular or irregular lattice. We argue that this easily-implemented sampling routine provides a good compromise between statistical and computational efficiency when working with large datasets.",gaussian markov random field gmrfs popular model tempor spatial depend larg areal dataset due eas interpret comput conveni afford condit independ spars precis matric need random variabl generat use model insid markov chain mont carlo algorithm requir repeat simul random field nontrivi issu especi full condit precis matrix depend paramet chang iter typic bayesian comput gmrfs updat joint block gibb sampler one locat time singl site sampler former approach lead quicker converg updat correl variabl onc latter avoid solv larg matric effici algorithm sampl markov random field becom focus much recent research machin learn literatur much use statistician briefli review recent propos approach eye toward implement statistician without expertis numer analysi advanc comput particular consid version block sampl graph cut condit independ site updat togeth algorithm allow practition parallel updat subset locat take advantag vector calcul high level languag simul real data applic demonstr comput save achiev versus tradit singl site updat block updat regardless whether data regular irregular lattic argu easili implement sampl routin provid good compromis statist comput effici work larg dataset,"['D. Andrew Brown', 'Christopher S. McMahan']",['stat.CO'],False,False,False,False,False,True
1074,2017-03-28T14:03:38Z,2017-02-15T11:52:14Z,http://arxiv.org/abs/1702.04561v1,http://arxiv.org/pdf/1702.04561v1,Probing for sparse and fast variable selection with model-based boosting,probe spars fast variabl select model base boost,"We present a new variable selection method based on model-based gradient boosting and randomly permuted variables. Model-based boosting is a tool to fit a statistical model while performing variable selection at the same time. A drawback of the fitting lies in the need of multiple model fits on slightly altered data (e.g. cross-validation or bootstrap) to find the optimal number of boosting iterations and prevent overfitting. In our proposed approach, we augment the data set with randomly permuted versions of the true variables, so called shadow variables, and stop the step-wise fitting as soon as such a variable would be added to the model. This allows variable selection in a single fit of the model without requiring further parameter tuning. We show that our probing approach can compete with state-of-the-art selection methods like stability selection in a high-dimensional classification benchmark and apply it on gene expression data for the estimation of riboflavin production of Bacillus subtilis.",present new variabl select method base model base gradient boost random permut variabl model base boost tool fit statist model perform variabl select time drawback fit lie need multipl model fit slight alter data cross valid bootstrap find optim number boost iter prevent overfit propos approach augment data set random permut version true variabl call shadow variabl stop step wise fit soon variabl would ad model allow variabl select singl fit model without requir paramet tune show probe approach compet state art select method like stabil select high dimension classif benchmark appli gene express data estim riboflavin product bacillus subtili,"['Janek Thomas', 'Tobias Hepp', 'Andreas Mayr', 'Bernd Bischl']","['stat.ML', 'stat.CO']",False,False,True,False,False,True
1075,2017-03-28T14:03:38Z,2017-02-14T21:20:23Z,http://arxiv.org/abs/1702.04391v1,http://arxiv.org/pdf/1702.04391v1,Bootstrap-based inferential improvements in beta autoregressive moving   average model,bootstrap base inferenti improv beta autoregress move averag model,"We consider the issue of performing accurate small sample inference in beta autoregressive moving average model, which is useful for modeling and forecasting continuous variables that assumes values in the interval $(0,1)$. The inferences based on conditional maximum likelihood estimation have good asymptotic properties, but their performances in small samples may be poor. This way, we propose bootstrap bias corrections of the point estimators and different bootstrap strategies for confidence interval improvements. Our Monte Carlo simulations show that finite sample inference based on bootstrap corrections is much more reliable than the usual inferences. We also presented an empirical application.",consid issu perform accur small sampl infer beta autoregress move averag model use model forecast continu variabl assum valu interv infer base condit maximum likelihood estim good asymptot properti perform small sampl may poor way propos bootstrap bias correct point estim differ bootstrap strategi confid interv improv mont carlo simul show finit sampl infer base bootstrap correct much reliabl usual infer also present empir applic,"['Bruna Gregory Palm', 'Fábio M. Bayer']",['stat.CO'],False,False,False,False,False,True
1076,2017-03-28T14:03:38Z,2017-02-13T17:23:02Z,http://arxiv.org/abs/1702.03891v1,http://arxiv.org/pdf/1702.03891v1,Spatial Models with the Integrated Nested Laplace Approximation within   Markov Chain Monte Carlo,spatial model integr nest laplac approxim within markov chain mont carlo,"The Integrated Nested Laplace Approximation (INLA) is a convenient way to obtain approximations to the posterior marginals for parameters in Bayesian hierarchical models when the latent effects can be expressed as a Gaussian Markov Random Field (GMRF). In addition, its implementation in the R-INLA package for the R statistical software provides an easy way to fit models using INLA in practice. R-INLA implements a number of widely used latent models, including several spatial models. In addition, R-INLA can fit models in a fraction of the time than other computer intensive methods (e.g. Markov Chain Monte Carlo) take to fit the same model.   Although INLA provides a fast approximation to the marginals of the model parameters, it is difficult to use it with models not implemented in R-INLA. It is also difficult to make multivariate posterior inference on the parameters of the model as INLA focuses on the posterior marginals and not the joint posterior distribution.   In this paper we describe how to use INLA within the Metropolis-Hastings algorithm to fit spatial models and estimate the joint posterior distribution of a reduced number of parameters. We will illustrate the benefits of this new method with two examples on spatial econometrics and disease mapping where complex spatial models with several spatial structures need to be fitted.",integr nest laplac approxim inla conveni way obtain approxim posterior margin paramet bayesian hierarch model latent effect express gaussian markov random field gmrf addit implement inla packag statist softwar provid easi way fit model use inla practic inla implement number wide use latent model includ sever spatial model addit inla fit model fraction time comput intens method markov chain mont carlo take fit model although inla provid fast approxim margin model paramet difficult use model implement inla also difficult make multivari posterior infer paramet model inla focus posterior margin joint posterior distribut paper describ use inla within metropoli hast algorithm fit spatial model estim joint posterior distribut reduc number paramet illustr benefit new method two exampl spatial econometr diseas map complex spatial model sever spatial structur need fit,"['Virgilio Gómez-Rubio', 'Francisco Palmí-Perales']",['stat.CO'],False,False,False,False,False,True
1077,2017-03-28T14:03:38Z,2017-02-13T08:52:58Z,http://arxiv.org/abs/1702.03673v1,http://arxiv.org/pdf/1702.03673v1,Bayesian Probabilistic Numerical Methods,bayesian probabilist numer method,"The emergent field of probabilistic numerics has thus far lacked rigorous statistical principals. This paper establishes Bayesian probabilistic numerical methods as those which can be cast as solutions to certain Bayesian inverse problems, albeit problems that are non-standard. This allows us to establish general conditions under which Bayesian probabilistic numerical methods are well-defined, encompassing both non-linear and non-Gaussian models. For general computation, a numerical approximation scheme is developed and its asymptotic convergence is established. The theoretical development is then extended to pipelines of computation, wherein probabilistic numerical methods are composed to solve more challenging numerical tasks. The contribution highlights an important research frontier at the interface of numerical analysis and uncertainty quantification, with some illustrative applications presented.",emerg field probabilist numer thus far lack rigor statist princip paper establish bayesian probabilist numer method cast solut certain bayesian invers problem albeit problem non standard allow us establish general condit bayesian probabilist numer method well defin encompass non linear non gaussian model general comput numer approxim scheme develop asymptot converg establish theoret develop extend pipelin comput wherein probabilist numer method compos solv challeng numer task contribut highlight import research frontier interfac numer analysi uncertainti quantif illustr applic present,"['Jon Cockayne', 'Chris Oates', 'Tim Sullivan', 'Mark Girolami']","['stat.ME', 'cs.NA', 'math.NA', 'math.ST', 'stat.CO', 'stat.TH']",False,False,False,False,False,True
1078,2017-03-28T14:03:38Z,2017-02-10T12:26:52Z,http://arxiv.org/abs/1702.03146v1,http://arxiv.org/pdf/1702.03146v1,Analysis of a nonlinear importance sampling scheme for Bayesian   parameter estimation in state-space models,analysi nonlinear import sampl scheme bayesian paramet estim state space model,"The Bayesian estimation of the unknown parameters of state-space (dynamical) systems has received considerable attention over the past decade, with a handful of powerful algorithms being introduced. In this paper we tackle the theoretical analysis of the recently proposed {\it nonlinear} population Monte Carlo (NPMC). This is an iterative importance sampling scheme whose key features, compared to conventional importance samplers, are (i) the approximate computation of the importance weights (IWs) assigned to the Monte Carlo samples and (ii) the nonlinear transformation of these IWs in order to prevent the degeneracy problem that flaws the performance of conventional importance samplers. The contribution of the present paper is a rigorous proof of convergence of the nonlinear IS (NIS) scheme as the number of Monte Carlo samples, $M$, increases. Our analysis reveals that the NIS approximation errors converge to 0 almost surely and with the optimal Monte Carlo rate of $M^{-\frac{1}{2}}$. Moreover, we prove that this is achieved even when the mean estimation error of the IWs remains constant, a property that has been termed {\it exact approximation} in the Markov chain Monte Carlo literature. We illustrate these theoretical results by means of a computer simulation example involving the estimation of the parameters of a state-space model typically used for target tracking.",bayesian estim unknown paramet state space dynam system receiv consider attent past decad hand power algorithm introduc paper tackl theoret analysi recent propos nonlinear popul mont carlo npmc iter import sampl scheme whose key featur compar convent import sampler approxim comput import weight iw assign mont carlo sampl ii nonlinear transform iw order prevent degeneraci problem flaw perform convent import sampler contribut present paper rigor proof converg nonlinear nis scheme number mont carlo sampl increas analysi reveal nis approxim error converg almost sure optim mont carlo rate frac moreov prove achiev even mean estim error iw remain constant properti term exact approxim markov chain mont carlo literatur illustr theoret result mean comput simul exampl involv estim paramet state space model typic use target track,"['Joaquin Miguez', 'Ines P. Mariño', 'Manuel A. Vazquez']",['stat.CO'],False,False,False,False,False,True
1079,2017-03-28T14:03:38Z,2017-02-10T10:44:23Z,http://arxiv.org/abs/1702.03126v1,http://arxiv.org/pdf/1702.03126v1,Computational inference without proposal kernels,comput infer without propos kernel,"Likelihood-free methods, such as approximate Bayesian computation, are powerful tools for practical inference problems with intractable likelihood functions. Markov chain Monte Carlo and sequential Monte Carlo variants of approximate Bayesian computation can be effective techniques for sampling posterior distributions without likelihoods. However, the efficiency of these methods depends crucially on the proposal kernel used to generate proposal posterior samples, and a poor choice can lead to extremely low efficiency. We propose a new method for likelihood-free Bayesian inference based upon ideas from multilevel Monte Carlo. Our method is accurate and does not require proposal kernels, thereby overcoming a key obstacle in the use of likelihood-free approaches in real-world situations.",likelihood free method approxim bayesian comput power tool practic infer problem intract likelihood function markov chain mont carlo sequenti mont carlo variant approxim bayesian comput effect techniqu sampl posterior distribut without likelihood howev effici method depend crucial propos kernel use generat propos posterior sampl poor choic lead extrem low effici propos new method likelihood free bayesian infer base upon idea multilevel mont carlo method accur doe requir propos kernel therebi overcom key obstacl use likelihood free approach real world situat,"['David J. Warne', 'Ruth E. Baker', 'Matthew J. Simpson']","['stat.CO', '62F15, 65C05']",False,False,False,False,False,True
1080,2017-03-28T14:03:42Z,2017-02-15T10:14:43Z,http://arxiv.org/abs/1702.03057v2,http://arxiv.org/pdf/1702.03057v2,Unbiased Multi-index Monte Carlo,unbias multi index mont carlo,"We introduce a new class of Monte Carlo based approximations of expectations of random variables defined whose laws are not available directly, but only through certain discretisatizations. Sampling from the discretized versions of these laws can typically introduce a bias. In this paper, we show how to remove that bias, by introducing a new version of multi-index Monte Carlo (MIMC) that has the added advantage of reducing the computational effort, relative to i.i.d. sampling from the most precise discretization, for a given level of error. We cover extensions of results regarding variance and optimality criteria for the new approach. We apply the methodology to the problem of computing an unbiased mollified version of the solution of a partial differential equation with random coefficients. A second application concerns the Bayesian inference (the smoothing problem) of an infinite dimensional signal modelled by the solution of a stochastic partial differential equation that is observed on a discrete space grid and at discrete times. Both applications are complemented by numerical simulations.",introduc new class mont carlo base approxim expect random variabl defin whose law avail direct onli certain discretisat sampl discret version law typic introduc bias paper show remov bias introduc new version multi index mont carlo mimc ad advantag reduc comput effort relat sampl precis discret given level error cover extens result regard varianc optim criteria new approach appli methodolog problem comput unbias mollifi version solut partial differenti equat random coeffici second applic concern bayesian infer smooth problem infinit dimension signal model solut stochast partial differenti equat observ discret space grid discret time applic complement numer simul,"['Dan Crisan', 'Jeremie Houssineau', 'Ajay Jasra']",['stat.CO'],False,False,False,False,False,True
1083,2017-03-28T14:03:42Z,2017-02-06T14:01:20Z,http://arxiv.org/abs/1702.01618v1,http://arxiv.org/pdf/1702.01618v1,Learning of state-space models with highly informative observations: a   tempered Sequential Monte Carlo solution,learn state space model high inform observ temper sequenti mont carlo solut,"Probabilistic (or Bayesian) modeling and learning offers interesting possibilities for systematic representation of uncertainty based on probability theory. Recent advances in Monte Carlo based methods have made previously intractable problem possible to solve using only the computational power available in a standard personal computer. For probabilistic learning of unknown parameters in nonlinear state-space models, methods based on the particle filter have proven useful. However, a notoriously challenging problem occurs when the observations are highly informative, i.e. when there is very little or no measurement noise present. The particle filter will then struggle in estimating one of the basic component in most parameter learning algorithms, the likelihood p(data parameters). To this end we suggest an algorithm which initially assumes that there is artificial measurement noise present. The variance of this noise is sequentially decreased in an adaptive fashion such that we in the end recover the original problem or possibly a very close approximation of it. Computationally the parameters are learned using a sequential Monte Carlo (SMC) sampler, which gives our proposed method a clear resemblance to the SMC^2 method. Another natural link is also made to the ideas underlying the so-called approximate Bayesian computation (ABC). We provide a theoretical justification (implying convergence results) for the suggested approach. We also illustrate it with numerical examples, and in particular show promising results for a challenging Wiener-Hammerstein benchmark.",probabilist bayesian model learn offer interest possibl systemat represent uncertainti base probabl theori recent advanc mont carlo base method made previous intract problem possibl solv use onli comput power avail standard person comput probabilist learn unknown paramet nonlinear state space model method base particl filter proven use howev notori challeng problem occur observ high inform veri littl measur nois present particl filter struggl estim one basic compon paramet learn algorithm likelihood data paramet end suggest algorithm initi assum artifici measur nois present varianc nois sequenti decreas adapt fashion end recov origin problem possibl veri close approxim comput paramet learn use sequenti mont carlo smc sampler give propos method clear resembl smc method anoth natur link also made idea call approxim bayesian comput abc provid theoret justif impli converg result suggest approach also illustr numer exampl particular show promis result challeng wiener hammerstein benchmark,"['Andreas Svensson', 'Thomas B. Schön', 'Fredrik Lindsten']","['stat.CO', 'stat.ML']",False,False,False,False,False,True
1087,2017-03-28T14:03:42Z,2017-02-03T22:07:37Z,http://arxiv.org/abs/1702.01185v1,http://arxiv.org/pdf/1702.01185v1,Basis Adaptive Sample Efficient Polynomial Chaos (BASE-PC),basi adapt sampl effici polynomi chao base pc,"For a large class of orthogonal basis functions, there has been a recent identification of expansion methods for computing accurate, stable approximations of a quantity of interest. This paper presents, within the context of uncertainty quantification, a practical implementation using basis adaptation, and coherence motivated sampling, which under assumptions has satisfying guarantees. This implementation is referred to as Basis Adaptive Sample Efficient Polynomial Chaos (BASE-PC). A key component of this is the use of anisotropic polynomial order which admits evolving global bases for approximation in an efficient manner, leading to consistently stable approximation for a practical class of smooth functionals. This fully adaptive, non-intrusive method, requires no a priori information of the solution, and has satisfying theoretical guarantees of recovery. A key contribution to stability is the use of a presented correction sampling for coherence-optimal sampling in order to improve stability and accuracy within the adaptive basis scheme. Theoretically, the method may dramatically reduce the impact of dimensionality in function approximation, and numerically the method is demonstrated to perform well on problems with dimension up to 1000.",larg class orthogon basi function recent identif expans method comput accur stabl approxim quantiti interest paper present within context uncertainti quantif practic implement use basi adapt coher motiv sampl assumpt satisfi guarante implement refer basi adapt sampl effici polynomi chao base pc key compon use anisotrop polynomi order admit evolv global base approxim effici manner lead consist stabl approxim practic class smooth function fulli adapt non intrus method requir priori inform solut satisfi theoret guarante recoveri key contribut stabil use present correct sampl coher optim sampl order improv stabil accuraci within adapt basi scheme theoret method may dramat reduc impact dimension function approxim numer method demonstr perform well problem dimens,"['Jerrad Hampton', 'Alireza Doostan']","['stat.CO', 'math.PR', 'math.ST', 'stat.TH']",False,False,False,False,False,True
1088,2017-03-28T14:03:42Z,2017-02-03T21:23:46Z,http://arxiv.org/abs/1702.01166v1,http://arxiv.org/pdf/1702.01166v1,Optimal Subsampling for Large Sample Logistic Regression,optim subsampl larg sampl logist regress,"For massive data, the family of subsampling algorithms is popular to downsize the data volume and reduce computational burden. Existing studies focus on approximating the ordinary least squares estimate in linear regression, where statistical leverage scores are often used to define subsampling probabilities. In this paper, we propose fast subsampling algorithms to efficiently approximate the maximum likelihood estimate in logistic regression. We first establish consistency and asymptotic normality of the estimator from a general subsampling algorithm, and then derive optimal subsampling probabilities that minimize the asymptotic mean squared error of the resultant estimator. An alternative minimization criterion is also proposed to further reduce the computational cost. The optimal subsampling probabilities depend on the full data estimate, so we develop a two-step algorithm to approximate the optimal subsampling procedure. This algorithm is computationally efficient and has a significant reduction in computing time compared to the full data approach. Consistency and asymptotic normality of the estimator from a two-step algorithm are also established. Synthetic and real data sets are used to evaluate the practical performance of the proposed method.",massiv data famili subsampl algorithm popular downsiz data volum reduc comput burden exist studi focus approxim ordinari least squar estim linear regress statist leverag score often use defin subsampl probabl paper propos fast subsampl algorithm effici approxim maximum likelihood estim logist regress first establish consist asymptot normal estim general subsampl algorithm deriv optim subsampl probabl minim asymptot mean squar error result estim altern minim criterion also propos reduc comput cost optim subsampl probabl depend full data estim develop two step algorithm approxim optim subsampl procedur algorithm comput effici signific reduct comput time compar full data approach consist asymptot normal estim two step algorithm also establish synthet real data set use evalu practic perform propos method,"['HaiYing Wang', 'Rong Zhu', 'Ping Ma']","['stat.CO', 'stat.ME', 'stat.ML']",False,False,False,False,False,True
1089,2017-03-28T14:03:42Z,2017-02-02T20:08:42Z,http://arxiv.org/abs/1702.00817v1,http://arxiv.org/abs/1702.00817v1,DCT-like Transform for Image Compression Requires 14 Additions Only,dct like transform imag compress requir addit onli,"A low-complexity 8-point orthogonal approximate DCT is introduced. The proposed transform requires no multiplications or bit-shift operations. The derived fast algorithm requires only 14 additions, less than any existing DCT approximation. Moreover, in several image compression scenarios, the proposed transform could outperform the well-known signed DCT, as well as state-of-the-art algorithms.",low complex point orthogon approxim dct introduc propos transform requir multipl bit shift oper deriv fast algorithm requir onli addit less ani exist dct approxim moreov sever imag compress scenario propos transform could outperform well known sign dct well state art algorithm,"['F. M. Bayer', 'R. J. Cintra']","['cs.MM', 'cs.DS', 'stat.AP', 'stat.CO']",False,False,False,False,False,True
1092,2017-03-28T14:03:46Z,2017-02-07T22:13:25Z,http://arxiv.org/abs/1702.00317v2,http://arxiv.org/pdf/1702.00317v2,On SGD's Failure in Practice: Characterizing and Overcoming Stalling,sgd failur practic character overcom stall,"Stochastic Gradient Descent (SGD) is widely used in machine learning problems to efficiently perform empirical risk minimization, yet, in practice, SGD is known to stall before reaching the actual minimizer of the empirical risk. SGD stalling has often been attributed to its sensitivity to the conditioning of the problem; however, as we demonstrate, SGD will stall even when applied to a simple linear regression problem with unity condition number for standard learning rates. Thus, in this work, we numerically demonstrate and mathematically argue that stalling is a crippling and generic limitation of SGD and its variants in practice. Once we have established the problem of stalling, we generalize an existing framework for hedging against its effects, which (1) deters SGD and its variants from stalling, (2) still provides convergence guarantees, and (3) makes SGD and its variants more practical methods for minimization.",stochast gradient descent sgd wide use machin learn problem effici perform empir risk minim yet practic sgd known stall befor reach actual minim empir risk sgd stall often attribut sensit condit problem howev demonstr sgd stall even appli simpl linear regress problem uniti condit number standard learn rate thus work numer demonstr mathemat argu stall crippl generic limit sgd variant practic onc establish problem stall general exist framework hedg effect deter sgd variant stall still provid converg guarante make sgd variant practic method minim,['Vivak Patel'],"['stat.ML', 'cs.LG', 'math.OC', 'stat.CO', '62L20, 62L12, 90C99', 'G.1.6; G.3; I.2.6']",False,False,False,False,False,True
1093,2017-03-28T14:03:46Z,2017-02-01T10:55:12Z,http://arxiv.org/abs/1702.00204v1,http://arxiv.org/pdf/1702.00204v1,Bayesian model selection for the latent position cluster model for   Social Networks,bayesian model select latent posit cluster model social network,"The latent position cluster model is a popular model for the statistical analysis of network data. This model assumes that there is an underlying latent space in which the actors follow a finite mixture distribution. Moreover, actors which are close in this latent space are more likely to be tied by an edge. This is an appealing approach since it allows the model to cluster actors which consequently provides the practitioner with useful qualitative information. However, exploring the uncertainty in the number of underlying latent components in the mixture distribution is a complex task. The current state-of-the-art is to use an approximate form of BIC for this purpose, where an approximation of the log-likelihood is used instead of the true log-likelihood which is unavailable. The main contribution of this paper is to show that through the use of conjugate prior distributions it is possible to analytically integrate out almost all of the model parameters, leaving a posterior distribution which depends on the allocation vector of the mixture model. This enables posterior inference over the number of components in the latent mixture distribution without using trans- dimensional MCMC algorithms such as reversible jump MCMC. Our approach is compared with the state-of-the-art latentnet (Krivitsky & Handcock 2015) and VBLPCM (Salter-Townshend & Murphy 2013) packages.",latent posit cluster model popular model statist analysi network data model assum latent space actor follow finit mixtur distribut moreov actor close latent space like tie edg appeal approach sinc allow model cluster actor consequ provid practition use qualit inform howev explor uncertainti number latent compon mixtur distribut complex task current state art use approxim form bic purpos approxim log likelihood use instead true log likelihood unavail main contribut paper show use conjug prior distribut possibl analyt integr almost model paramet leav posterior distribut depend alloc vector mixtur model enabl posterior infer number compon latent mixtur distribut without use tran dimension mcmc algorithm revers jump mcmc approach compar state art latentnet krivitski handcock vblpcm salter townshend murphi packag,"['Caitriona Ryan', 'Jason Wyse', 'Nial Friel']",['stat.CO'],False,False,False,False,False,True
1095,2017-03-28T14:03:46Z,2017-03-03T15:54:59Z,http://arxiv.org/abs/1701.08142v3,http://arxiv.org/pdf/1701.08142v3,Modelling Preference Data with the Wallenius Distribution,model prefer data wallenius distribut,"The Wallenius distribution is a generalisation of the Hypergeometric distribution where weights are assigned to balls of different colours. This naturally defines a model for ranking categories which can be used for classification purposes. Since, in general, the resulting likelihood is not analytically available, we adopt an approximate Bayesian computational (ABC) approach for estimating the importance of the categories. We illustrate the performance of the estimation procedure on simulated datasets. Finally, we use the new model for analysing two datasets about movies ratings and Italian academic statisticians' journal preferences. The latter is a novel dataset collected by the authors.",wallenius distribut generalis hypergeometr distribut weight assign ball differ colour natur defin model rank categori use classif purpos sinc general result likelihood analyt avail adopt approxim bayesian comput abc approach estim import categori illustr perform estim procedur simul dataset final use new model analys two dataset movi rate italian academ statistician journal prefer latter novel dataset collect author,"['Clara Grazian', 'Fabrizio Leisen', 'Brunero Liseo']","['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']",False,False,False,False,False,True
1096,2017-03-28T14:03:46Z,2017-01-26T19:07:53Z,http://arxiv.org/abs/1701.07844v1,http://arxiv.org/pdf/1701.07844v1,Markov Chain Monte Carlo with the Integrated Nested Laplace   Approximation,markov chain mont carlo integr nest laplac approxim,"The Integrated Nested Laplace Approximation (INLA) has established itself as a widely used method for approximate inference on Bayesian hierarchical models which can be represented as a latent Gaussian model (LGM). INLA is based on producing an accurate approximation to the posterior marginal distributions of the parameters in the model and some other quantities of interest by using repeated approximations to intermediate distributions and integrals that appear in the computation of the posterior marginals.   INLA focuses on models whose latent effects are a Gaussian Markov random field (GMRF). For this reason, we have explored alternative ways of expanding the number of possible models that can be fitted using the INLA methodology. In this paper, we present a novel approach that combines INLA and Markov chain Monte Carlo (MCMC). The aim is to consider a wider range of models that cannot be fitted with INLA unless some of the parameters of the model have been fixed. Hence, conditioning on these parameters the model could be fitted with the R-INLA package. We show how new values of these parameters can be drawn from their posterior by using conditional models fitted with INLA and standard MCMC algorithms, such as Metropolis-Hastings. Hence, this will extend the use of INLA to fit models that can be expressed as a conditional LGM. Also, this new approach can be used to build simpler MCMC samplers for complex models as it allows sampling only on a limited number parameters in the model.   We will demonstrate how our approach can extend the class of models that could benefit from INLA, and how the R-INLA package will ease its implementation. We will go through simple examples of this new approach before we discuss more advanced problems with datasets taken from relevant literature.",integr nest laplac approxim inla establish wide use method approxim infer bayesian hierarch model repres latent gaussian model lgm inla base produc accur approxim posterior margin distribut paramet model quantiti interest use repeat approxim intermedi distribut integr appear comput posterior margin inla focus model whose latent effect gaussian markov random field gmrf reason explor altern way expand number possibl model fit use inla methodolog paper present novel approach combin inla markov chain mont carlo mcmc aim consid wider rang model cannot fit inla unless paramet model fix henc condit paramet model could fit inla packag show new valu paramet drawn posterior use condit model fit inla standard mcmc algorithm metropoli hast henc extend use inla fit model express condit lgm also new approach use build simpler mcmc sampler complex model allow sampl onli limit number paramet model demonstr approach extend class model could benefit inla inla packag eas implement go simpl exampl new approach befor discuss advanc problem dataset taken relev literatur,"['Virgilio Gómez-Rubio', 'Håvard Rue']",['stat.CO'],False,False,False,False,False,True
1097,2017-03-28T14:03:46Z,2017-02-09T12:18:42Z,http://arxiv.org/abs/1701.07787v3,http://arxiv.org/pdf/1701.07787v3,Multi-locus data distinguishes between population growth and multiple   merger coalescents,multi locus data distinguish popul growth multipl merger coalesc,"We introduce a low dimensional function of the site frequency spectrum that is tailor-made for distinguishing coalescent models with multiple mergers from Kingman coalescent models with population growth, and use this function to construct a hypothesis test between these two model classes. The null and alternative sampling distributions of our statistic are intractable, but its low dimensionality renders these distributions amenable to Monte Carlo estimation. We construct kernel density estimates of the sampling distributions based on simulated data, and show that the resulting hypothesis test dramatically improves on the statistical power of a current state-of-the-art method. A key reason for this improvement is the use of multi-locus data, in particular averaging observed site frequency spectra across unlinked loci to reduce sampling variance. We also demonstrate the robustness of our method to nuisance and tuning parameters. Finally we demonstrate that the same kernel density estimates can be used to conduct parameter estimation, and argue that our method is readily generalisable for applications in model selection, parameter inference and experimental design.",introduc low dimension function site frequenc spectrum tailor made distinguish coalesc model multipl merger kingman coalesc model popul growth use function construct hypothesi test two model class null altern sampl distribut statist intract low dimension render distribut amen mont carlo estim construct kernel densiti estim sampl distribut base simul data show result hypothesi test dramat improv statist power current state art method key reason improv use multi locus data particular averag observ site frequenc spectra across unlink loci reduc sampl varianc also demonstr robust method nuisanc tune paramet final demonstr kernel densiti estim use conduct paramet estim argu method readili generalis applic model select paramet infer experiment design,['Jere Koskela'],"['q-bio.PE', 'q-bio.QM', 'stat.CO', 'stat.ME', '92D15 (Primary), 62M02, 62M05 (Secondary)']",False,False,False,False,False,True
1099,2017-03-28T14:03:46Z,2017-01-23T20:26:42Z,http://arxiv.org/abs/1701.06619v1,http://arxiv.org/pdf/1701.06619v1,Bayesian Inference in the Presence of Intractable Normalizing Functions,bayesian infer presenc intract normal function,"Models with intractable normalizing functions arise frequently in statistics. Common examples of such models include exponential random graph models for social networks and Markov point processes for ecology and disease modeling. Inference for these models is complicated because the normalizing functions of their probability distributions include the parameters of interest. In Bayesian analysis they result in so-called doubly intractable posterior distributions which pose significant computational challenges. Several Monte Carlo methods have emerged in recent years to address Bayesian inference for such models. We provide a framework for understanding the algorithms and elucidate connections among them. Through multiple simulated and real data examples, we compare and contrast the computational and statistical efficiency of these algorithms and discuss their theoretical bases. Our study provides practical recommendations for practitioners along with directions for future research for MCMC methodologists.",model intract normal function aris frequent statist common exampl model includ exponenti random graph model social network markov point process ecolog diseas model infer model complic becaus normal function probabl distribut includ paramet interest bayesian analysi result call doubli intract posterior distribut pose signific comput challeng sever mont carlo method emerg recent year address bayesian infer model provid framework understand algorithm elucid connect among multipl simul real data exampl compar contrast comput statist effici algorithm discuss theoret base studi provid practic recommend practition along direct futur research mcmc methodologist,"['Jaewoo Park', 'Murali Haran']","['stat.CO', 'stat.AP']",False,False,False,False,False,True
1100,2017-03-28T14:03:51Z,2017-03-27T14:52:09Z,http://arxiv.org/abs/1703.09124v1,http://arxiv.org/pdf/1703.09124v1,Multi-sensor Transmission Management for Remote State Estimation under   Coordination,multi sensor transmiss manag remot state estim coordin,"This paper considers the remote state estimation in a cyber-physical system (CPS) using multiple sensors. The measurements of each sensor are transmitted to a remote estimator over a shared channel, where simultaneous transmissions from other sensors are regarded as interference signals. In such a competitive environment, each sensor needs to choose its transmission power for sending data packets taking into account of other sensors' behavior. To model this interactive decision-making process among the sensors, we introduce a multi-player non-cooperative game framework. To overcome the inefficiency arising from the Nash equilibrium (NE) solution, we propose a correlation policy, along with the notion of correlation equilibrium (CE). An analytical comparison of the game value between the NE and the CE is provided, with/without the power expenditure constraints for each sensor. Also, numerical simulations demonstrate the comparison results.",paper consid remot state estim cyber physic system cps use multipl sensor measur sensor transmit remot estim share channel simultan transmiss sensor regard interfer signal competit environ sensor need choos transmiss power send data packet take account sensor behavior model interact decis make process among sensor introduc multi player non cooper game framework overcom ineffici aris nash equilibrium ne solut propos correl polici along notion correl equilibrium ce analyt comparison game valu ne ce provid without power expenditur constraint sensor also numer simul demonstr comparison result,"['Kemi Ding', 'Yuzhe Li', 'Subhrakanti Dey', 'Ling Shi']","['stat.ME', 'cs.IT', 'math.IT']",False,False,True,False,False,True
1103,2017-03-28T14:03:51Z,2017-03-27T10:17:00Z,http://arxiv.org/abs/1703.08995v1,http://arxiv.org/pdf/1703.08995v1,On the Limit Imbalanced Logistic Regression by Binary Predictors,limit imbalanc logist regress binari predictor,"In this work, we introduce a modified (rescaled) likelihood for imbalanced logistic regression. This new approach makes easier the use of exponential priors and the computation of lasso regularization path. Precisely, we study a limiting behavior for which class imbalance is artificially increased by replication of the majority class observations. If some strong overlap conditions are satisfied, the maximum likelihood estimate converges towards a finite value close to the initial one (intercept excluded) as shown by simulations with binary predictors. This solution corresponds to the extremum of a concave function that we refer to as ""rescaled"" likelihood. In this context, the use of exponential priors has a clear interpretation as a shift on the predictor means for the minority class. Thanks to the simple binary structure, some random designs give analytic path estimators for the lasso regularization problem. An effective approximate path algorithm by piecewise logarithmic functions based on matrix inversions is also presented. This work was motivated by its potential application to spontaneous reports databases in a pharmacovigilance context.",work introduc modifi rescal likelihood imbalanc logist regress new approach make easier use exponenti prior comput lasso regular path precis studi limit behavior class imbal artifici increas replic major class observ strong overlap condit satisfi maximum likelihood estim converg toward finit valu close initi one intercept exclud shown simul binari predictor solut correspond extremum concav function refer rescal likelihood context use exponenti prior clear interpret shift predictor mean minor class thank simpl binari structur random design give analyt path estim lasso regular problem effect approxim path algorithm piecewis logarithm function base matrix invers also present work motiv potenti applic spontan report databas pharmacovigil context,['Vincent Runge'],"['stat.ME', 'Primary 62J12, 62F12, 62F15, secondary 34E05, 49M29, 62P10']",False,False,False,False,False,True
1104,2017-03-28T14:03:51Z,2017-03-26T22:49:31Z,http://arxiv.org/abs/1703.08882v1,http://arxiv.org/pdf/1703.08882v1,A Mixture of Matrix Variate Skew-t Distributions,mixtur matrix variat skew distribut,"Clustering is the process of finding underlying group structures in data. Although model-based clustering is firmly established in the multivariate case, there is relative paucity for matrix variate distributions, and there are even fewer examples using matrix variate skew distributions. In this paper, we look at parameter estimation for a finite mixture of matrix variate skew-t distributions in the context of model-based clustering. Simulated data is used for illustrative purposes.",cluster process find group structur data although model base cluster firm establish multivari case relat pauciti matrix variat distribut even fewer exampl use matrix variat skew distribut paper look paramet estim finit mixtur matrix variat skew distribut context model base cluster simul data use illustr purpos,"['Michael P. B. Gallaugher', 'Paul D. McNicholas']","['stat.ME', 'stat.CO']",False,False,True,False,False,True
1105,2017-03-28T14:03:51Z,2017-03-25T20:57:56Z,http://arxiv.org/abs/1703.08741v1,http://arxiv.org/pdf/1703.08741v1,Clustering and Variable Selection in the Presence of Mixed Variable   Types and Missing Data,cluster variabl select presenc mix variabl type miss data,"We consider the problem of model-based clustering in the presence of many correlated, mixed continuous and discrete variables, some of which may have missing values. Discrete variables are treated with a latent continuous variable approach and the Dirichlet process is used to construct a mixture model with an unknown number of components. Variable selection is also performed to identify the variables that are most influential for determining cluster membership. The work is motivated by the need to cluster patients thought to potentially have autism spectrum disorder (ASD) on the basis of many cognitive and/or behavioral test scores. There are a modest number of patients (~480) in the data set along with many (~100) test score variables (many of which are discrete valued and/or missing). The goal of the work is to (i) cluster these patients into similar groups to help identify those with similar clinical presentation, and (ii) identify a sparse subset of tests that inform the clusters in order to eliminate unnecessary testing. The proposed approach compares very favorably to other methods via simulation of problems of this type. The results of the ASD analysis suggested three clusters to be most likely, while only four test scores had high (>0.5) posterior probability of being informative. This will result in much more efficient and informative testing. The need to cluster observations on the basis of many correlated, continuous/discrete variables with missing values, is a common problem in the health sciences as well as in many other disciplines.",consid problem model base cluster presenc mani correl mix continu discret variabl may miss valu discret variabl treat latent continu variabl approach dirichlet process use construct mixtur model unknown number compon variabl select also perform identifi variabl influenti determin cluster membership work motiv need cluster patient thought potenti autism spectrum disord asd basi mani cognit behavior test score modest number patient data set along mani test score variabl mani discret valu miss goal work cluster patient similar group help identifi similar clinic present ii identifi spars subset test inform cluster order elimin unnecessari test propos approach compar veri favor method via simul problem type result asd analysi suggest three cluster like onli four test score high posterior probabl inform result much effici inform test need cluster observ basi mani correl continu discret variabl miss valu common problem health scienc well mani disciplin,"['Curtis Storlie', 'Scott Myers', 'S Katusic', 'Amy Weaver', 'Robert Voigt', 'Robert Colligan', 'Paul Croarkin', 'Ruth Stoeckel', 'John Port']",['stat.ME'],False,False,False,False,False,True
1106,2017-03-28T14:03:51Z,2017-03-25T17:57:31Z,http://arxiv.org/abs/1703.08723v1,http://arxiv.org/pdf/1703.08723v1,Extending Growth Mixture Models Using Continuous Non-Elliptical   Distributions,extend growth mixtur model use continu non ellipt distribut,"Growth mixture models (GMMs) incorporate both conventional random effects growth modeling and latent trajectory classes as in finite mixture modeling; therefore, they offer a way to handle the unobserved heterogeneity between subjects in their development. GMMs with Gaussian random effects dominate the literature. When the data are asymmetric and/or have heavier tails, more than one latent class is required to capture the observed variable distribution. Therefore, a GMM with continuous non-elliptical distributions is proposed to capture skewness and heavier tails in the data set. Specifically, multivariate skew-t distributions and generalized hyperbolic distributions are introduced to extend GMMs. When extending GMMs, four statistical models are considered with differing distributions of measurement errors and random effects. The mathematical development of a GMM with non-elliptical distributions relies on its relationship with the generalized inverse Gaussian distribution. Parameter estimation is outlined within the expectation-maximization framework before the performance of our GMM with non-elliptical distributions is illustrated on simulated and real data.",growth mixtur model gmms incorpor convent random effect growth model latent trajectori class finit mixtur model therefor offer way handl unobserv heterogen subject develop gmms gaussian random effect domin literatur data asymmetr heavier tail one latent class requir captur observ variabl distribut therefor gmm continu non ellipt distribut propos captur skew heavier tail data set specif multivari skew distribut general hyperbol distribut introduc extend gmms extend gmms four statist model consid differ distribut measur error random effect mathemat develop gmm non ellipt distribut reli relationship general invers gaussian distribut paramet estim outlin within expect maxim framework befor perform gmm non ellipt distribut illustr simul real data,"['Yuhong Wei', 'Emilie Shireman', 'Paul D. McNicholas', 'Douglas L. Steinley']","['stat.ME', 'stat.AP', 'stat.CO']",False,False,False,False,False,True
1110,2017-03-28T14:03:55Z,2017-03-24T17:17:45Z,http://arxiv.org/abs/1703.08520v1,http://arxiv.org/pdf/1703.08520v1,Rejection-free Ensemble MCMC with applications to Factorial Hidden   Markov Models,reject free ensembl mcmc applic factori hidden markov model,"Bayesian inference for complex models is challenging due to the need to explore high-dimensional spaces and multimodality and standard Monte Carlo samplers can have difficulties effectively exploring the posterior. We introduce a general purpose rejection-free ensemble Markov Chain Monte Carlo (MCMC) technique to improve on existing poorly mixing samplers. This is achieved by combining parallel tempering and an auxiliary variable move to exchange information between the chains. We demonstrate this ensemble MCMC scheme on Bayesian inference in Factorial Hidden Markov Models. This high-dimensional inference problem is difficult due to the exponentially sized latent variable space. Existing sampling approaches mix slowly and can get trapped in local modes. We show that the performance of these samplers is improved by our rejection-free ensemble technique and that the method is attractive and ""easy-to-use"" since no parameter tuning is required.",bayesian infer complex model challeng due need explor high dimension space multimod standard mont carlo sampler difficulti effect explor posterior introduc general purpos reject free ensembl markov chain mont carlo mcmc techniqu improv exist poor mix sampler achiev combin parallel temper auxiliari variabl move exchang inform chain demonstr ensembl mcmc scheme bayesian infer factori hidden markov model high dimension infer problem difficult due exponenti size latent variabl space exist sampl approach mix slowli get trap local mode show perform sampler improv reject free ensembl techniqu method attract easi use sinc paramet tune requir,"['Kaspar Märtens', 'Michalis K Titsias', 'Christopher Yau']","['stat.CO', 'stat.ME', 'stat.ML']",False,False,False,False,False,True
1111,2017-03-28T14:03:55Z,2017-03-24T16:11:19Z,http://arxiv.org/abs/1703.08489v1,http://arxiv.org/pdf/1703.08489v1,regsem: Regularized Structural Equation Modeling,regsem regular structur equat model,"The regsem package in R, an implementation of regularized structural equation modeling (RegSEM; Jacobucci, Grimm, and McArdle 2016), was recently developed with the goal of incorporating various forms of penalized likelihood estimation in a broad array of structural equations models. The forms of regularization include both the ridge (Hoerl and Kennard 1970) and the least absolute shrinkage and selection operator (lasso; Tibshirani 1996), along with sparser extensions. RegSEM is particularly useful for structural equation models that have a small parameter to sample size ratio, as the addition of penalties can reduce the complexity, thus reducing the bias of the parameter estimates. The paper covers the algorithmic details and an overview of the use of regsem with the application of both factor analysis and latent growth curve models.",regsem packag implement regular structur equat model regsem jacobucci grimm mcardl recent develop goal incorpor various form penal likelihood estim broad array structur equat model form regular includ ridg hoerl kennard least absolut shrinkag select oper lasso tibshirani along sparser extens regsem particular use structur equat model small paramet sampl size ratio addit penalti reduc complex thus reduc bias paramet estim paper cover algorithm detail overview use regsem applic factor analysi latent growth curv model,['Ross Jacobucci'],['stat.ME'],False,False,False,False,False,True
1115,2017-03-28T14:03:55Z,2017-03-23T12:43:23Z,http://arxiv.org/abs/1703.08045v1,http://arxiv.org/pdf/1703.08045v1,Profiled deviance for the multivariate linear mixed-effects model   fitting,profil devianc multivari linear mix effect model fit,"This paper focuses on the multivariate linear mixed-effects model, including all the correlations between the random effects when the marginal residual terms are assumed uncorrelated and homoscedastic with possibly different standard deviations. The random effects covariance matrix is Cholesky factorized to directly estimate the variance components of these random effects. This strategy enables a consistent estimate of the random effects covariance matrix which, generally, has a poor estimate when it is grossly (or directly) estimated, using the estimating methods such as the EM algorithm. By using simulated data sets, we compare the estimates based on the present method with the EM algorithm-based estimates. We provide an illustration by using the real-life data concerning the study of the child's immune against malaria in Benin (West Africa).",paper focus multivari linear mix effect model includ correl random effect margin residu term assum uncorrel homoscedast possibl differ standard deviat random effect covari matrix choleski factor direct estim varianc compon random effect strategi enabl consist estim random effect covari matrix general poor estim grossli direct estim use estim method em algorithm use simul data set compar estim base present method em algorithm base estim provid illustr use real life data concern studi child immun malaria benin west africa,"['Eric Adjakossa', 'Grégory Nuel']",['stat.ME'],False,False,False,False,False,True
1116,2017-03-28T14:03:55Z,2017-03-23T09:20:36Z,http://arxiv.org/abs/1703.07975v1,http://arxiv.org/pdf/1703.07975v1,An Adapted Loss Function for Censored Quantile Regression,adapt loss function censor quantil regress,"In this paper, we study a novel approach for the estimation of quantiles when facing potential right censoring of the responses. Contrary to the existing literature on the subject, the adopted strategy of this paper is to tackle censoring at the very level of the loss function usually employed for the computation of quantiles, the so-called ""check"" function. For interpretation purposes, a simple comparison with the latter reveals how censoring is accounted for in the newly proposed loss function. Subsequently, when considering the inclusion of covariates for conditional quantile estimation, by defining a new general loss function, the proposed methodology opens the gate to numerous parametric, semiparametric and nonparametric modelling techniques. In order to illustrate this statement, we consider the well-studied linear regression under the usual assumption of conditional independence between the true response and the censoring variable. For practical minimization of the studied loss function, we also provide a simple algorithmic procedure shown to yield satisfactory results for the proposed estimator with respect to the existing literature in an extensive simulation study. From a more theoretical prospect, consistency of the estimator for linear regression is obtained using very recent results on non-smooth semiparametric estimation equations with an infinite-dimensional nuisance parameter, while numerical examples illustrate the adequateness of a simple bootstrap procedure for inferential purposes. Lastly, an application to a real dataset is used to further illustrate the validity and finite sample performance of the proposed estimator.",paper studi novel approach estim quantil face potenti right censor respons contrari exist literatur subject adopt strategi paper tackl censor veri level loss function usual employ comput quantil call check function interpret purpos simpl comparison latter reveal censor account newli propos loss function subsequ consid inclus covari condit quantil estim defin new general loss function propos methodolog open gate numer parametr semiparametr nonparametr model techniqu order illustr statement consid well studi linear regress usual assumpt condit independ true respons censor variabl practic minim studi loss function also provid simpl algorithm procedur shown yield satisfactori result propos estim respect exist literatur extens simul studi theoret prospect consist estim linear regress obtain use veri recent result non smooth semiparametr estim equat infinit dimension nuisanc paramet numer exampl illustr adequ simpl bootstrap procedur inferenti purpos last applic real dataset use illustr valid finit sampl perform propos estim,"['Mickaël De Backer', 'Anouar El Ghouch', 'Ingrid Van Keilegom']",['stat.ME'],False,False,False,False,False,True
1117,2017-03-28T14:03:55Z,2017-03-23T01:30:17Z,http://arxiv.org/abs/1703.07904v1,http://arxiv.org/pdf/1703.07904v1,Cross-Validation with Confidence,cross valid confid,"Cross-validation is one of the most popular model selection methods in statistics and machine learning. Despite its wide applicability, traditional cross-validation methods tend to select overfitting models, unless the ratio between the training and testing sample sizes is much smaller than conventional choices. We argue that such an overfitting tendency of cross-validation is due to the ignorance of the uncertainty in the testing sample. Starting from this observation, we develop a new, statistically principled inference tool based on cross-validation that takes into account the uncertainty in the testing sample. This new method outputs a small set of highly competitive candidate models containing the best one with guaranteed probability. As a consequence, our method can achieve consistent variable selection in a classical linear regression setting, for which existing cross-validation methods require unconventional split ratios. We demonstrate the performance of the proposed method in several simulated and real data examples.",cross valid one popular model select method statist machin learn despit wide applic tradit cross valid method tend select overfit model unless ratio train test sampl size much smaller convent choic argu overfit tendenc cross valid due ignor uncertainti test sampl start observ develop new statist principl infer tool base cross valid take account uncertainti test sampl new method output small set high competit candid model contain best one guarante probabl consequ method achiev consist variabl select classic linear regress set exist cross valid method requir unconvent split ratio demonstr perform propos method sever simul real data exampl,['Jing Lei'],"['stat.ME', 'stat.ML']",False,False,True,False,False,True
1118,2017-03-28T14:03:55Z,2017-03-22T22:59:20Z,http://arxiv.org/abs/1703.07879v1,http://arxiv.org/pdf/1703.07879v1,How to avoid the curse of dimensionality: scalability of particle   filters with and without importance weights,avoid curs dimension scalabl particl filter without import weight,"Particle filters are a popular and flexible class of numerical algorithms to solve a large class of nonlinear filtering problems. However, standard particle filters with importance weights have been shown to require a sample size that increases exponentially with the dimension D of the state space in order to achieve a certain performance, which precludes their use in very high-dimensional filtering problems. Here, we focus on the dynamic aspect of this curse of dimensionality (COD) in continuous time filtering, which is caused by the degeneracy of importance weights over time. We show that the degeneracy occurs on a time-scale that decreases with increasing D. In order to soften the effects of weight degeneracy, most particle filters use particle resampling and improved proposal functions for the particle motion. We explain why neither of the two can prevent the COD in general. In order to address this fundamental problem, we investigate an existing filtering algorithm based on optimal feedback control that sidesteps the use of importance weights. We use numerical experiments to show that this Feedback Particle Filter (FPF) by Yang et al. (2013) does not exhibit a COD.",particl filter popular flexibl class numer algorithm solv larg class nonlinear filter problem howev standard particl filter import weight shown requir sampl size increas exponenti dimens state space order achiev certain perform preclud use veri high dimension filter problem focus dynam aspect curs dimension cod continu time filter caus degeneraci import weight time show degeneraci occur time scale decreas increas order soften effect weight degeneraci particl filter use particl resampl improv propos function particl motion explain whi neither two prevent cod general order address fundament problem investig exist filter algorithm base optim feedback control sidestep use import weight use numer experi show feedback particl filter fpf yang et al doe exhibit cod,"['Simone Carlo Surace', 'Anna Kutschireiter', 'Jean-Pascal Pfister']","['math.OC', 'math.PR', 'math.ST', 'stat.ME', 'stat.TH']",False,False,False,False,False,True
1119,2017-03-28T14:03:55Z,2017-03-22T21:13:02Z,http://arxiv.org/abs/1703.07856v1,http://arxiv.org/pdf/1703.07856v1,Testing for the Equality of two Distributions on High Dimensional Object   Spaces,test equal two distribut high dimension object space,"Energy statistics are estimators of the energy distance that depend on the distances between observations. The idea behind energy statistics is to consider a statistical potential energy that would parallel Newton's gravitational potential energy. This statistical potential energy is zero if and only if a certain null hypothesis relating two distributions holds true. In Szekely and Rizzo(2004), a nonparametric test for equality of two multivariate distributions was given, based on the Euclidean distance between observations. This test was shown to be effective for high dimensional multivariate data, and was implemented by an appropriate distribution free permutation test. As an extension of Szekely and Rizzo (2013), here we consider the energy distance between to independent random objects X and Y on the object space M, that admits an embedding into an Euclidean space. In the case of a Kendall shape space, we can use its VW-embedding into an Euclidean space of matrices and define the extrinsic distance between two shapes as their VW associated distance. The corresponding energy distance between two distributions of Kendall shapes of k-ads will be called VW-energy distance We test our methodology on, to compare the distributions of Kendall shape of the contour of the midsagittal section of the Corpus Callossum in normal vs ADHD diagnosed individuals. Here we use the VW distance between the shapes of two children CC midsections. Using the CC data coming originally from http://fcon 1000.projects.nitrc.org/indi/adhd200/ it appears that the two Kendall shape distributions are not significantly different.",energi statist estim energi distanc depend distanc observ idea behind energi statist consid statist potenti energi would parallel newton gravit potenti energi statist potenti energi zero onli certain null hypothesi relat two distribut hold true szeke rizzo nonparametr test equal two multivari distribut given base euclidean distanc observ test shown effect high dimension multivari data implement appropri distribut free permut test extens szeke rizzo consid energi distanc independ random object object space admit embed euclidean space case kendal shape space use vw embed euclidean space matric defin extrins distanc two shape vw associ distanc correspond energi distanc two distribut kendal shape ad call vw energi distanc test methodolog compar distribut kendal shape contour midsagitt section corpus callossum normal vs adhd diagnos individu use vw distanc shape two children cc midsect use cc data come origin http fcon project nitrc org indi adhd appear two kendal shape distribut signific differ,"['Ruite Guo', 'Vic Patrangenaru']",['stat.ME'],False,False,False,False,False,True
1121,2017-03-28T14:03:59Z,2017-03-22T11:20:00Z,http://arxiv.org/abs/1703.07603v1,http://arxiv.org/pdf/1703.07603v1,Effect fusion using model-based clustering,effect fusion use model base cluster,"In social and economic studies many of the collected variables are measured on a nominal scale, often with a large number of categories. The definition of categories is usually not unambiguous and different classification schemes using either a finer or a coarser grid are possible. Categorisation has an impact when such a variable is included as covariate in a regression model: a too fine grid will result in imprecise estimates of the corresponding effects, whereas with a too coarse grid important effects will be missed, resulting in biased effect estimates and poor predictive performance.   To achieve automatic grouping of levels with essentially the same effect, we adopt a Bayesian approach and specify the prior on the level effects as a location mixture of spiky normal components. Fusion of level effects is induced by a prior on the mixture weights which encourages empty components. Model-based clustering of the effects during MCMC sampling allows to simultaneously detect categories which have essentially the same effect size and identify variables with no effect at all. The properties of this approach are investigated in simulation studies. Finally, the method is applied to analyse effects of high-dimensional categorical predictors on income in Austria.",social econom studi mani collect variabl measur nomin scale often larg number categori definit categori usual unambigu differ classif scheme use either finer coarser grid possibl categoris impact variabl includ covari regress model fine grid result imprecis estim correspond effect wherea coars grid import effect miss result bias effect estim poor predict perform achiev automat group level essenti effect adopt bayesian approach specifi prior level effect locat mixtur spiki normal compon fusion level effect induc prior mixtur weight encourag empti compon model base cluster effect dure mcmc sampl allow simultan detect categori essenti effect size identifi variabl effect properti approach investig simul studi final method appli analys effect high dimension categor predictor incom austria,"['Gertraud Malsiner-Walli', 'Daniela Pauger', 'Helga Wagner']",['stat.ME'],False,False,False,False,False,True
1122,2017-03-28T14:03:59Z,2017-03-21T16:39:28Z,http://arxiv.org/abs/1703.07305v1,http://arxiv.org/abs/1703.07305v1,Targeting Bayes factors with direct-path non-equilibrium thermodynamic   integration,target bay factor direct path non equilibrium thermodynam integr,"Thermodynamic integration (TI) for computing marginal likelihoods is based on an inverse annealing path from the prior to the posterior distribution. In many cases, the resulting estimator suffers from high variability, which particularly stems from the prior regime. When comparing complex models with differences in a comparatively small number of parameters, intrinsic errors from sampling fluctuations may outweigh the differences in the log marginal likelihood estimates. In the present article, we propose a thermodynamic integration scheme that directly targets the log Bayes factor. The method is based on a modified annealing path between the posterior distributions of the two models compared, which systematically avoids the high variance prior regime. We combine this scheme with the concept of non-equilibrium TI to minimise discretisation errors from numerical integration. Results obtained on Bayesian regression models applied to standard benchmark data, and a complex hierarchical model applied to biopathway inference, demonstrate a significant reduction in estimator variance over state-of-the-art TI methods.",thermodynam integr ti comput margin likelihood base invers anneal path prior posterior distribut mani case result estim suffer high variabl particular stem prior regim compar complex model differ compar small number paramet intrins error sampl fluctuat may outweigh differ log margin likelihood estim present articl propos thermodynam integr scheme direct target log bay factor method base modifi anneal path posterior distribut two model compar systemat avoid high varianc prior regim combin scheme concept non equilibrium ti minimis discretis error numer integr result obtain bayesian regress model appli standard benchmark data complex hierarch model appli biopathway infer demonstr signific reduct estim varianc state art ti method,"['Marco Grzegorczyk', 'Andrej Aderhold', 'Dirk Husmeier']","['stat.ME', 'stat.ML']",False,False,False,False,False,True
1123,2017-03-28T14:03:59Z,2017-03-21T14:41:52Z,http://arxiv.org/abs/1703.07246v1,http://arxiv.org/pdf/1703.07246v1,Sufficient Dimension Reduction via Random-Partitions for Large-p-Small-n   Problem,suffici dimens reduct via random partit larg small problem,"Sufficient dimension reduction (SDR) is continuing an active research field nowadays for high dimensional data. It aims to estimate the central subspace (CS) without making distributional assumption. To overcome the large-$p$-small-$n$ problem we propose a new approach for SDR. Our method combines the following ideas for high dimensional data analysis: (1) Randomly partition the covariates into subsets and use distance correlation (DC) to construct a sketch of envelope subspace with low dimension. (2) Obtain a sketch of the CS by applying conventional SDR method within the constructed envelope subspace. (3) Repeat the above two steps for a few times and integrate these multiple sketches to form the final estimate of the CS. We name the proposed SDR procedure ""integrated random-partition SDR (iRP-SDR)"". Comparing with existing methods, iRP-SDR is less affected by the selection of tuning parameters. Moreover, the estimation procedure of iRP-SDR does not involve the determination of the structural dimension until at the last stage, which makes the method more robust in a high-dimensional setting. Asymptotic properties of iRP-SDR are also established. The advantageous performance of the proposed method is demonstrated via simulation studies and the EEG data analysis.",suffici dimens reduct sdr continu activ research field nowaday high dimension data aim estim central subspac cs without make distribut assumpt overcom larg small problem propos new approach sdr method combin follow idea high dimension data analysi random partit covari subset use distanc correl dc construct sketch envelop subspac low dimens obtain sketch cs appli convent sdr method within construct envelop subspac repeat abov two step time integr multipl sketch form final estim cs name propos sdr procedur integr random partit sdr irp sdr compar exist method irp sdr less affect select tune paramet moreov estim procedur irp sdr doe involv determin structur dimens last stage make method robust high dimension set asymptot properti irp sdr also establish advantag perform propos method demonstr via simul studi eeg data analysi,"['Hung Hung', 'Su-Yun Huang']",['stat.ME'],False,False,True,False,False,True
1125,2017-03-28T14:03:59Z,2017-03-21T12:33:19Z,http://arxiv.org/abs/1703.07169v1,http://arxiv.org/pdf/1703.07169v1,A Deterministic Global Optimization Method for Variational Inference,determinist global optim method variat infer,"Variational inference methods for latent variable statistical models have gained popularity because they are relatively fast, can handle large data sets, and have deterministic convergence guarantees. However, in practice it is unclear whether the fixed point identified by the variational inference algorithm is a local or a global optimum. Here, we propose a method for constructing iterative optimization algorithms for variational inference problems that are guaranteed to converge to the $\epsilon$-global variational lower bound on the log-likelihood. We derive inference algorithms for two variational approximations to a standard Bayesian Gaussian mixture model (BGMM). We present a minimal data set for empirically testing convergence and show that a variational inference algorithm frequently converges to a local optimum while our algorithm always converges to the globally optimal variational lower bound. We characterize the loss incurred by choosing a non-optimal variational approximation distribution suggesting that selection of the approximating variational distribution deserves as much attention as the selection of the original statistical model for a given data set.",variat infer method latent variabl statist model gain popular becaus relat fast handl larg data set determinist converg guarante howev practic unclear whether fix point identifi variat infer algorithm local global optimum propos method construct iter optim algorithm variat infer problem guarante converg epsilon global variat lower bound log likelihood deriv infer algorithm two variat approxim standard bayesian gaussian mixtur model bgmm present minim data set empir test converg show variat infer algorithm frequent converg local optimum algorithm alway converg global optim variat lower bound character loss incur choos non optim variat approxim distribut suggest select approxim variat distribut deserv much attent select origin statist model given data set,"['Hachem Saddiki', 'Andrew C. Trapp', 'Patrick Flaherty']","['stat.ME', 'stat.ML']",False,False,False,False,False,True
1128,2017-03-28T14:03:59Z,2017-03-20T15:45:44Z,http://arxiv.org/abs/1703.06808v1,http://arxiv.org/pdf/1703.06808v1,Worth Weighting? How to Think About and Use Sample Weights in Survey   Experiments,worth weight think use sampl weight survey experi,"The popularity of online surveys has increased the prominence of sampling weights in claims of representativeness. Yet, much uncertainty remains regarding how these weights should be employed in the analysis of survey experiments: Should they be used or ignored? If they are used, which estimators are preferred? We offer practical advice, rooted in the Neyman-Rubin model, for researchers producing and working with survey experimental data. We examine simple, efficient estimators (Horvitz-Thompson, H\`ajek, ""double-H\`ajek"", and post-stratification) for analyzing these data, along with formulae for biases and variances. We provide simulations that examine these estimators and real examples from experiments administered online through YouGov. We find that for examining the existence of population treatment effects using high-quality, broadly representative samples recruited by top online survey firms, sample quantities, which do not rely on weights, are often sufficient. Sample Average Treatment Effect (SATE) estimates are unlikely to differ substantially from weighted estimates, and they avoid the statistical power loss that accompanies weighting. When precise estimates of Population Average Treatment Effects (PATE) are essential, we analytically show post-stratifying on survey weights and/or covariates highly correlated with the outcome to be a conservative choice.",popular onlin survey increas promin sampl weight claim repres yet much uncertainti remain regard weight employ analysi survey experi use ignor use estim prefer offer practic advic root neyman rubin model research produc work survey experiment data examin simpl effici estim horvitz thompson ajek doubl ajek post stratif analyz data along formula bias varianc provid simul examin estim real exampl experi administ onlin yougov find examin exist popul treatment effect use high qualiti broad repres sampl recruit top onlin survey firm sampl quantiti reli weight often suffici sampl averag treatment effect sate estim unlik differ substanti weight estim avoid statist power loss accompani weight precis estim popul averag treatment effect pate essenti analyt show post stratifi survey weight covari high correl outcom conserv choic,"['Luke W. Miratrix', 'Jasjeet S. Sekhon', 'Alexander G. Theodoridis', 'Luis F. Campos']","['stat.ME', 'stat.AP']",False,False,False,False,False,True
1129,2017-03-28T14:03:59Z,2017-03-20T08:51:47Z,http://arxiv.org/abs/1703.06633v1,http://arxiv.org/pdf/1703.06633v1,Variational inference for probabilistic Poisson PCA,variat infer probabilist poisson pca,"Many application domains such as ecology or genomics have to deal with multivariate non Gaussian observations. A typical example is the joint observation of the respective abundances of a set of species in a series of sites, aiming to understand the co-variations between these species. The Gaussian setting provides a canonical way to model such dependencies, but does not apply in general. We consider here the multivariate exponential family framework for which we introduce a generic model with multivariate Gaussian latent variables. We show that approximate maximum likelihood inference can be achieved via a variational algorithm for which gradient descent easily applies. We show that this setting enables us to account for covariates and offsets. We then focus on the case of the Poisson-lognormal model in the context of community ecology.",mani applic domain ecolog genom deal multivari non gaussian observ typic exampl joint observ respect abund set speci seri site aim understand co variat speci gaussian set provid canon way model depend doe appli general consid multivari exponenti famili framework introduc generic model multivari gaussian latent variabl show approxim maximum likelihood infer achiev via variat algorithm gradient descent easili appli show set enabl us account covari offset focus case poisson lognorm model context communiti ecolog,"['Julien Chiquet', 'Mahendra Mariadassou', 'Stéphane Robin']",['stat.ME'],False,False,False,False,False,True
1130,2017-03-28T14:04:03Z,2017-03-20T02:04:24Z,http://arxiv.org/abs/1703.06559v1,http://arxiv.org/pdf/1703.06559v1,Adaptive p-values after cross-validation,adapt valu cross valid,"We describe a way to construct hypothesis tests and confidence intervals after having used the Lasso for feature selection, allowing the regularization parameter to be chosen via an estimate of prediction error. Our estimate of prediction error is a slight variation on cross-validation. Using this variation, we are able to describe an appropriate selection event for choosing a parameter by cross-validation. Adjusting for this selection event, we derive a pivotal quantity that has an asymptotically Unif(0,1) distribution which can be used to test hypotheses or construct intervals. To enhance power, we consider the randomized Lasso with cross-validation. We derive a similar test statistic and develop MCMC sampling scheme to construct valid post-selective confidence intervals empirically. Finally, we demonstrate via simulation that our procedure achieves high-statistical power and FDR control, yielding results comparable to knockoffs (in simulations favorable to knockoffs).",describ way construct hypothesi test confid interv use lasso featur select allow regular paramet chosen via estim predict error estim predict error slight variat cross valid use variat abl describ appropri select event choos paramet cross valid adjust select event deriv pivot quantiti asymptot unif distribut use test hypothes construct interv enhanc power consid random lasso cross valid deriv similar test statist develop mcmc sampl scheme construct valid post select confid interv empir final demonstr via simul procedur achiev high statist power fdr control yield result compar knockoff simul favor knockoff,"['Jelena Markovic', 'Lucy Xia', 'Jonathan Taylor']",['stat.ME'],False,False,False,False,False,True
1132,2017-03-28T14:04:03Z,2017-03-19T10:51:41Z,http://arxiv.org/abs/1703.06419v1,http://arxiv.org/pdf/1703.06419v1,Multivariate Functional Data Visualization and Outlier Detection,multivari function data visual outlier detect,"This article proposes a new graphical tool, the magnitude-shape (MS) plot, for visualizing both the magnitude and shape outlyingness of multivariate functional data. The proposed tool builds on the recent notion of functional directional outlyingness, which measures the centrality of functional data by simultaneously considering the level and the direction of their deviation from the central region. The MS-plot intuitively presents not only levels but also directions of magnitude outlyingness on the horizontal axis or plane, and demonstrates shape outlyingness on the vertical axis. A dividing curve or surface is provided to separate non-outlying data from the outliers. Both the simulated data and the practical examples confirm that the MS-plot is superior to existing tools for visualizing centrality and detecting outliers for functional data.",articl propos new graphic tool magnitud shape ms plot visual magnitud shape outlying multivari function data propos tool build recent notion function direct outlying measur central function data simultan consid level direct deviat central region ms plot intuit present onli level also direct magnitud outlying horizont axi plane demonstr shape outlying vertic axi divid curv surfac provid separ non data outlier simul data practic exampl confirm ms plot superior exist tool visual central detect outlier function data,"['Wenlin Dai', 'Marc G. Genton']","['stat.ME', 'stat.CO']",False,False,False,False,False,True
1133,2017-03-28T14:04:03Z,2017-03-19T10:36:53Z,http://arxiv.org/abs/1703.06417v1,http://arxiv.org/pdf/1703.06417v1,Spectral analysis of stationary random bivariate signals,spectral analysi stationari random bivari signal,"A novel approach towards the spectral analysis of stationary random bivariate signals is proposed. Using the Quaternion Fourier Transform, we introduce a quaternion-valued spectral representation of random bivariate signals seen as complex-valued sequences. This makes possible the definition of a scalar quaternion-valued spectral density for bivariate signals. This spectral density can be meaningfully interpreted in terms of frequency-dependent polarization attributes. A natural decomposition of any random bivariate signal in terms of unpolarized and polarized components is introduced. Nonparametric spectral density estimation is investigated, and we introduce the polarization periodogram of a random bivariate signal. Numerical experiments support our theoretical analysis, illustrating the relevance of the approach on synthetic data.",novel approach toward spectral analysi stationari random bivari signal propos use quaternion fourier transform introduc quaternion valu spectral represent random bivari signal seen complex valu sequenc make possibl definit scalar quaternion valu spectral densiti bivari signal spectral densiti meaning interpret term frequenc depend polar attribut natur decomposit ani random bivari signal term unpolar polar compon introduc nonparametr spectral densiti estim investig introduc polar periodogram random bivari signal numer experi support theoret analysi illustr relev approach synthet data,"['Julien Flamant', 'Nicolas Le Bihan', 'Pierre Chainais']",['stat.ME'],False,False,False,False,False,True
1134,2017-03-28T14:04:03Z,2017-03-19T01:29:12Z,http://arxiv.org/abs/1703.06379v1,http://arxiv.org/pdf/1703.06379v1,Penalized pairwise pseudo likelihood for variable selection with   nonignorable missing data,penal pairwis pseudo likelihood variabl select nonignor miss data,"The regularization approach for variable selection was well developed for a completely observed data set in the past two decades. In the presence of missing values, this approach needs to be tailored to different missing data mechanisms. In this paper, we focus on a flexible and generally applicable missing data mechanism, which contains both ignorable and nonignorable missing data mechanism assumptions. We show how the regularization approach for variable selection can be adapted to the situation under this missing data mechanism. The computational and theoretical properties for variable selection consistency are established. The proposed method is further illustrated by comprehensive simulation studies and real data analyses, for both low and high dimensional settings.",regular approach variabl select well develop complet observ data set past two decad presenc miss valu approach need tailor differ miss data mechan paper focus flexibl general applic miss data mechan contain ignor nonignor miss data mechan assumpt show regular approach variabl select adapt situat miss data mechan comput theoret properti variabl select consist establish propos method illustr comprehens simul studi real data analys low high dimension set,"['Jiwei Zhao', 'Yang Yang', 'Yang Ning']",['stat.ME'],False,False,True,False,False,True
1136,2017-03-28T14:04:03Z,2017-03-18T09:10:51Z,http://arxiv.org/abs/1703.06277v1,http://arxiv.org/pdf/1703.06277v1,Unsupervised Learning of Mixture Regression Models for Longitudinal Data,unsupervis learn mixtur regress model longitudin data,"This paper is concerned with learning of mixture regression models for individuals that are measured repeatedly. The adjective ""unsupervised"" implies that the number of mixing components is unknown and has to be determined, ideally by data driven tools. For this purpose, a novel penalized method is proposed to simultaneously select the number of mixing components and to estimate the mixing proportions and unknown parameters in the models. The proposed method is capable of handling both continuous and discrete responses by only requiring the first two moment conditions of the model distribution. It is shown to be consistent in both selecting the number of components and estimating the mixing proportions and unknown regression parameters. Further, a modified EM algorithm is developed to seamlessly integrate model selection and estimation. Simulation studies are conducted to evaluate the finite sample performance of the proposed procedure. And it is further illustrated via an analysis of a primary biliary cirrhosis data set.",paper concern learn mixtur regress model individu measur repeat adject unsupervis impli number mix compon unknown determin ideal data driven tool purpos novel penal method propos simultan select number mix compon estim mix proport unknown paramet model propos method capabl handl continu discret respons onli requir first two moment condit model distribut shown consist select number compon estim mix proport unknown regress paramet modifi em algorithm develop seamless integr model select estim simul studi conduct evalu finit sampl perform propos procedur illustr via analysi primari biliari cirrhosi data set,"['Peirong Xu', 'Heng Peng', 'Tao Huang']",['stat.ME'],False,False,False,False,False,True
1137,2017-03-28T14:04:03Z,2017-03-18T00:37:18Z,http://arxiv.org/abs/1703.06226v1,http://arxiv.org/pdf/1703.06226v1,Identifying the Support of Rectangular Signals in Gaussian Noise,identifi support rectangular signal gaussian nois,"We consider the problem of identifying the support of the block signal in a sequence when both the length and the location of the block signal are unknown. The multivariate version of this problem is also considered, in which we try to identify the support of the rectangular signal in the hyper- rectangle. We allow the length of the block signal to grow polynomially with the length of the sequence, which greatly generalizes the previous results in [16]. A statistical boundary above which the identification is possible is presented and an asymptotically optimal and computationally efficient procedure is proposed under Gaussian white noise in both the univariate and multivariate settings. The problem of block signal identification is shown to have the same statistical difficulty as the corresponding problem of detection in both the univariate and multivariate cases, in the sense that whenever we can detect the signal, we can identify the support of the signal. Some generalizations are also considered here: (1) We ex- tend our theory to the case of multiple block signals. (2) We also discuss about the robust identification problem when the noise distribution is un- specified and the block signal identification problem under the exponential family setting.",consid problem identifi support block signal sequenc length locat block signal unknown multivari version problem also consid tri identifi support rectangular signal hyper rectangl allow length block signal grow polynomi length sequenc great general previous result statist boundari abov identif possibl present asymptot optim comput effici procedur propos gaussian white nois univari multivari set problem block signal identif shown statist difficulti correspond problem detect univari multivari case sens whenev detect signal identifi support signal general also consid ex tend theori case multipl block signal also discuss robust identif problem nois distribut un specifi block signal identif problem exponenti famili set,['Jiyao Kou'],['stat.ME'],False,False,False,False,False,True
1138,2017-03-28T14:04:03Z,2017-03-18T00:08:59Z,http://arxiv.org/abs/1703.06222v1,http://arxiv.org/pdf/1703.06222v1,A Unified Treatment of Multiple Testing with Prior Knowledge,unifi treatment multipl test prior knowledg,"A significant literature has arisen to study ways to employing prior knowledge to improve power and precision of multiple testing procedures. Some common forms of prior knowledge may include (a) a priori beliefs about which hypotheses are null, modeled by non-uniform prior weights; (b) differing importances of hypotheses, modeled by differing penalties for false discoveries; (c) partitions of the hypotheses into known groups, indicating (dis)similarity of hypotheses; and (d) knowledge of independence, positive dependence or arbitrary dependence between hypotheses or groups, allowing for more aggressive or conservative procedures. We present a general framework for global null testing and false discovery rate (FDR) control that allows the scientist to incorporate all four types of prior knowledge (a)-(d) simultaneously. We unify a number of existing procedures, generalize the conditions under which they are known to work, and simplify their proofs of FDR control under independence, positive and arbitrary dependence. We also present an algorithmic framework that strictly generalizes and unifies the classic algorithms of Benjamini and Hochberg [3] and Simes [25], algorithms that guard against unknown dependence [7, 9], algorithms that employ prior weights [17, 15], algorithms that use penalty weights [4], algorithms that incorporate null-proportion adaptivity [26, 27], and algorithms that make use of multiple arbitrary partitions into groups [1]. Unlike this previous work, we can simultaneously incorporate all of the four types of prior knowledge, combined with all of the three forms of dependence.",signific literatur arisen studi way employ prior knowledg improv power precis multipl test procedur common form prior knowledg may includ priori belief hypothes null model non uniform prior weight differ import hypothes model differ penalti fals discoveri partit hypothes known group indic dis similar hypothes knowledg independ posit depend arbitrari depend hypothes group allow aggress conserv procedur present general framework global null test fals discoveri rate fdr control allow scientist incorpor four type prior knowledg simultan unifi number exist procedur general condit known work simplifi proof fdr control independ posit arbitrari depend also present algorithm framework strict general unifi classic algorithm benjamini hochberg sime algorithm guard unknown depend algorithm employ prior weight algorithm use penalti weight algorithm incorpor null proport adapt algorithm make use multipl arbitrari partit group unlik previous work simultan incorpor four type prior knowledg combin three form depend,"['Aaditya Ramdas', 'Rina Foygel Barber', 'Martin J. Wainwright', 'Michael I. Jordan']","['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']",False,False,False,False,False,True
1139,2017-03-28T14:04:03Z,2017-03-17T19:23:10Z,http://arxiv.org/abs/1703.06176v1,http://arxiv.org/pdf/1703.06176v1,Sampling from a pseudo selective posterior using a primal-dual approach,sampl pseudo select posterior use primal dual approach,"Adopting the Bayesian methodology of adjusting for selection to provide valid inference in Panigrahi (2016), the current work proposes an approximation to a selective posterior, post randomized queries on data. Such a posterior differs from the usual one as it involves a truncated likelihood prepended with a prior belief on parameters in a Bayesian model. The truncation, imposed by selection, leads to intractability of the selective posterior, thereby posing a technical hurdle in sampling from such a posterior. We derive an optimization problem to approximate the otherwise intractable posterior, the efficiency of a sampler targeting the pseudo selective posterior depends on the computational cost involved in solving the approximating optimization problem. We adopt a primal-dual approach in the current work to obtain a reduced optimization problem that allows for scalable Bayesian inference in both low and high dimensional regimes.",adopt bayesian methodolog adjust select provid valid infer panigrahi current work propos approxim select posterior post random queri data posterior differ usual one involv truncat likelihood prepend prior belief paramet bayesian model truncat impos select lead intract select posterior therebi pose technic hurdl sampl posterior deriv optim problem approxim otherwis intract posterior effici sampler target pseudo select posterior depend comput cost involv solv approxim optim problem adopt primal dual approach current work obtain reduc optim problem allow scalabl bayesian infer low high dimension regim,"['Snigdha Panigrahi', 'Jonathan Taylor']",['stat.ME'],False,False,False,False,False,True
1140,2017-03-28T14:04:07Z,2017-03-17T18:31:49Z,http://arxiv.org/abs/1703.06154v1,http://arxiv.org/pdf/1703.06154v1,An MCMC free approach to post-selective inference,mcmc free approach post select infer,"The current work proposes a Monte Carlo free alternative to inference post randomized selection algorithms with a convex loss and a convex penalty. The pivots based on the selective law that is truncated to all selected realizations, typically lack closed form expressions in randomized settings. Inference in these settings relies upon standard Monte Carlo sampling techniques, which can be prove to be unstable for parameters far off from the chosen reference distribution. This work offers an approximation to the selective pivot based on a pseudo selective law and consequently, provides valid inference as confidence intervals and the selective MLE; such an approximation takes the form of an optimization problem in  E  dimensions, where  E  is the size of the active set observed from selection. The guarantees of inference are valid coverage of confidence intervals based on inverting the approximate pivot, which have the additional merit of lengths being comparable to the unadjusted intervals with inducted randomization in the selective analysis.",current work propos mont carlo free altern infer post random select algorithm convex loss convex penalti pivot base select law truncat select realize typic lack close form express random set infer set reli upon standard mont carlo sampl techniqu prove unstabl paramet far chosen refer distribut work offer approxim select pivot base pseudo select law consequ provid valid infer confid interv select mle approxim take form optim problem dimens size activ set observ select guarante infer valid coverag confid interv base invert approxim pivot addit merit length compar unadjust interv induct random select analysi,"['Snigdha Panigrahi', 'Jelena Markovic', 'Jonathan Taylor']",['stat.ME'],False,False,False,False,False,True
1141,2017-03-28T14:04:07Z,2017-03-17T17:50:44Z,http://arxiv.org/abs/1703.06131v1,http://arxiv.org/pdf/1703.06131v1,Inference via low-dimensional couplings,infer via low dimension coupl,"Integration against an intractable probability measure is among the fundamental challenges of statistical inference, particularly in the Bayesian setting. A principled approach to this problem seeks a deterministic coupling of the measure of interest with a tractable ""reference"" measure (e.g., a standard Gaussian). This coupling is induced by a transport map, and enables direct simulation from the desired measure simply by evaluating the transport map at samples from the reference. Yet characterizing such a map---e.g., representing and evaluating it---grows challenging in high dimensions. The central contribution of this paper is to establish a link between the Markov properties of the target measure and the existence of certain low-dimensional couplings, induced by transport maps that are sparse or decomposable. Our analysis not only facilitates the construction of couplings in high-dimensional settings, but also suggests new inference methodologies. For instance, in the context of nonlinear and non-Gaussian state space models, we describe new online and single-pass variational algorithms that characterize the full posterior distribution of the sequential inference problem using operations only slightly more complex than regular filtering.",integr intract probabl measur among fundament challeng statist infer particular bayesian set principl approach problem seek determinist coupl measur interest tractabl refer measur standard gaussian coupl induc transport map enabl direct simul desir measur simpli evalu transport map sampl refer yet character map repres evalu grow challeng high dimens central contribut paper establish link markov properti target measur exist certain low dimension coupl induc transport map spars decompos analysi onli facilit construct coupl high dimension set also suggest new infer methodolog instanc context nonlinear non gaussian state space model describ new onlin singl pass variat algorithm character full posterior distribut sequenti infer problem use oper onli slight complex regular filter,"['Alessio Spantini', 'Daniele Bigoni', 'Youssef Marzouk']","['stat.ME', 'stat.CO', 'stat.ML']",False,False,False,False,False,True
1142,2017-03-28T14:04:07Z,2017-03-17T17:00:53Z,http://arxiv.org/abs/1703.06098v1,http://arxiv.org/pdf/1703.06098v1,Analysis of the Gibbs Sampler for Gaussian hierarchical models via   multigrid decomposition,analysi gibb sampler gaussian hierarch model via multigrid decomposit,"We study the convergence properties of the Gibbs Sampler in the context of posterior distributions arising from Bayesian analysis of Gaussian hierarchical models. We consider centred and non-centred parameterizations as well as their hybrids including the full family of partially non-centred parameterizations. We develop a novel methodology based on multi-grid decompositions to derive analytic expressions for the convergence rates of the algorithm for an arbitrary number of layers in the hierarchy, while previous work was typically limited to the two-level case. Our work gives a complete understanding for the three-level symmetric case and this gives rise to approximations for the non-symmetric case. We also give analogous, if less explicit, results for models of arbitrary level. This theory gives rise to simple and easy-to-implement guidelines for the practical implementation of Gibbs samplers on conditionally Gaussian hierarchical models.",studi converg properti gibb sampler context posterior distribut aris bayesian analysi gaussian hierarch model consid centr non centr parameter well hybrid includ full famili partial non centr parameter develop novel methodolog base multi grid decomposit deriv analyt express converg rate algorithm arbitrari number layer hierarchi previous work typic limit two level case work give complet understand three level symmetr case give rise approxim non symmetr case also give analog less explicit result model arbitrari level theori give rise simpl easi implement guidelin practic implement gibb sampler condit gaussian hierarch model,"['Giacomo Zanella', 'Gareth Roberts']","['stat.CO', 'math.PR', 'stat.ME', '60J22, 62F15, 65C40, 65C05']",False,False,False,False,False,True
1143,2017-03-28T14:04:07Z,2017-03-17T16:30:08Z,http://arxiv.org/abs/1703.06086v1,http://arxiv.org/pdf/1703.06086v1,Propensity score weighting for causal inference with clustered data,propens score weight causal infer cluster data,"Propensity score weighting is a tool for causal inference to adjust for measured confounders in observational studies. In practice, data often present complex structures, such as clustering, which make propensity score modeling and estimation challenging. In addition, for clustered data, there may be unmeasured cluster-specific variables that are related to both the treatment assignment and the outcome. When such unmeasured cluster-specific confounders exist and are omitted in the propensity score model, the subsequent propensity score adjustment may be biased. In this article, we propose a calibration technique for propensity score estimation under the latent ignorable treatment assignment mechanism, i.e., the treatment-outcome relationship is unconfounded given the observed covariates and the latent cluster effects. We then provide a consistent propensity score weighting estimator of the average treatment effect when the propensity score and outcome follow generalized linear mixed effects models. The proposed propensity score weighting estimator is attractive, because it does not require specification of functional forms of the propensity score and outcome models, and therefore is robust to model misspecification. The proposed weighting method can be combined with sampling weights for an integrated solution to handle confounding and sampling designs for causal inference with clustered survey data. In simulation studies, we show that the proposed estimator is superior to other competitors. We estimate the effect of School Body Mass Index Screening on prevalence of overweight and obesity for elementary schools in Pennsylvania.",propens score weight tool causal infer adjust measur confound observ studi practic data often present complex structur cluster make propens score model estim challeng addit cluster data may unmeasur cluster specif variabl relat treatment assign outcom unmeasur cluster specif confound exist omit propens score model subsequ propens score adjust may bias articl propos calibr techniqu propens score estim latent ignor treatment assign mechan treatment outcom relationship unconfound given observ covari latent cluster effect provid consist propens score weight estim averag treatment effect propens score outcom follow general linear mix effect model propos propens score weight estim attract becaus doe requir specif function form propens score outcom model therefor robust model misspecif propos weight method combin sampl weight integr solut handl confound sampl design causal infer cluster survey data simul studi show propos estim superior competitor estim effect school bodi mass index screen preval overweight obes elementari school pennsylvania,['Shu Yang'],['stat.ME'],False,False,False,False,False,True
1146,2017-03-28T14:04:07Z,2017-03-17T05:38:07Z,http://arxiv.org/abs/1703.05899v1,http://arxiv.org/pdf/1703.05899v1,Decomposition analysis to identify intervention targets for reducing   disparities,decomposit analysi identifi intervent target reduc dispar,"There has been considerable interest in using decomposition methods in epidemiology (mediation analysis) and economics (Oaxaca-Blinder decomposition) to understand how health disparities arise and how they might change upon intervention. It has not been clear when estimates from the Oaxaca-Blinder decomposition can be interpreted causally because its implementation does not explicitly address potential confounding of target variables. While mediation analysis does explicitly adjust for confounders of target variables, it does so in a way that entails equalizing confounders across racial groups, which may not reflect the intended intervention. Revisiting prior analyses in the National Longitudinal Survey of Youth on disparities in wages, unemployment, incarceration, and overall health with test scores, taken as a proxy for educational attainment, as a target intervention, we propose and demonstrate a novel decomposition that controls for confounders of test scores (measures of childhood SES) while leaving their association with race intact. We compare this decomposition with others that use standardization (to equalize childhood SES alone), mediation analysis (to equalize test scores within levels of childhood SES), and one that equalizes both childhood SES and test scores. We also show how these decompositions, including our novel proposals, are equivalent to causal implementations of the Oaxaca-Blinder decomposition.",consider interest use decomposit method epidemiolog mediat analysi econom oaxaca blinder decomposit understand health dispar aris might chang upon intervent clear estim oaxaca blinder decomposit interpret causal becaus implement doe explicit address potenti confound target variabl mediat analysi doe explicit adjust confound target variabl doe way entail equal confound across racial group may reflect intend intervent revisit prior analys nation longitudin survey youth dispar wage unemploy incarcer overal health test score taken proxi educ attain target intervent propos demonstr novel decomposit control confound test score measur childhood ses leav associ race intact compar decomposit use standard equal childhood ses alon mediat analysi equal test score within level childhood ses one equal childhood ses test score also show decomposit includ novel propos equival causal implement oaxaca blinder decomposit,"['John W. Jackson', 'Tyler J. VanderWeele']",['stat.ME'],False,False,False,False,False,True
1147,2017-03-28T14:04:07Z,2017-03-16T23:33:24Z,http://arxiv.org/abs/1703.05849v1,http://arxiv.org/pdf/1703.05849v1,Causal Inference through the Method of Direct Estimation,causal infer method direct estim,"The intersection of causal inference and machine learning is a rapidly advancing field. We propose a new approach, the method of direct estimation, that draws on both traditions in order to obtain nonparametric estimates of treatment effects. The approach focuses on estimating the effect of fluctuations in a treatment variable on an outcome. A tensor-spline implementation enables rich interactions between functional bases allowing for the approach to capture treatment/covariate interactions. We show how new innovations in Bayesian sparse modeling readily handle the proposed framework, and then document its performance in simulation and applied examples. Furthermore we show how the method of direct estimation can easily extend to structural estimators commonly used in a variety of disciplines, like instrumental variables, mediation analysis, and sequential g-estimation.",intersect causal infer machin learn rapid advanc field propos new approach method direct estim draw tradit order obtain nonparametr estim treatment effect approach focus estim effect fluctuat treatment variabl outcom tensor spline implement enabl rich interact function base allow approach captur treatment covari interact show new innov bayesian spars model readili handl propos framework document perform simul appli exampl furthermor show method direct estim easili extend structur estim common use varieti disciplin like instrument variabl mediat analysi sequenti estim,"['Marc Ratkovic', 'Dustin Tingley']","['stat.ML', 'stat.ME', '62G08, 46N30, 62P20, 62P25']",False,False,False,False,False,True
1151,2017-03-28T14:04:12Z,2017-03-15T15:27:46Z,http://arxiv.org/abs/1703.05208v1,http://arxiv.org/pdf/1703.05208v1,Understanding the Probabilistic Latent Component Analysis Framework,understand probabilist latent compon analysi framework,"Probabilistic Component Latent Analysis (PLCA) is a statistical modeling method for feature extraction from non-negative data. It has been fruitfully applied to various research fields of information retrieval. However, the EM-solved optimization problem coming with the parameter estimation of PLCA-based models has never been properly posed and justified. We then propose in this short paper to re-define the theoretical framework of this problem, with the motivation of making it clearer to understand, and more admissible for further developments of PLCA-based computational systems.",probabilist compon latent analysi plca statist model method featur extract non negat data fruit appli various research field inform retriev howev em solv optim problem come paramet estim plca base model never proper pose justifi propos short paper defin theoret framework problem motiv make clearer understand admiss develop plca base comput system,"['D. Cazau', 'G. Nuel']",['stat.ME'],False,False,False,False,False,True
1152,2017-03-28T14:04:12Z,2017-03-15T15:19:14Z,http://arxiv.org/abs/1703.05203v1,http://arxiv.org/pdf/1703.05203v1,Growing simplified vine copula trees: improving Dißmann's algorithm,grow simplifi vine copula tree improv di mann algorithm,"Vine copulas are pair-copula constructions enabling multivariate dependence modeling in terms of bivariate building blocks. One of the main tasks of fitting a vine copula is the selection of a suitable tree structure. For this the prevalent method is a heuristic called Di{\ss}mann's algorithm. It sequentially constructs the vine's trees by maximizing dependence at each tree level, where dependence is measured in terms of absolute Kendall's $\tau$. However, the algorithm disregards any implications of the tree structure on the simplifying assumption that is usually made for vine copulas to keep inference tractable. We develop two new algorithms that select tree structures focused on producing simplified vine copulas for which the simplifying assumption is violated as little as possible. For this we make use of a recently developed statistical test of the simplifying assumption. In a simulation study we show that our proposed methods outperform the benchmark given by Di{\ss}mann's algorithm by a great margin. Several real data applications emphasize their practical relevance.",vine copula pair copula construct enabl multivari depend model term bivari build block one main task fit vine copula select suitabl tree structur preval method heurist call di ss mann algorithm sequenti construct vine tree maxim depend tree level depend measur term absolut kendal tau howev algorithm disregard ani implic tree structur simplifi assumpt usual made vine copula keep infer tractabl develop two new algorithm select tree structur focus produc simplifi vine copula simplifi assumpt violat littl possibl make use recent develop statist test simplifi assumpt simul studi show propos method outperform benchmark given di ss mann algorithm great margin sever real data applic emphas practic relev,"['Daniel Kraus', 'Claudia Czado']",['stat.ME'],False,False,False,False,False,True
1153,2017-03-28T14:04:12Z,2017-03-16T09:36:40Z,http://arxiv.org/abs/1703.05189v2,http://arxiv.org/pdf/1703.05189v2,Student-t Process Quadratures for Filtering of Non-Linear Systems with   Heavy-Tailed Noise,student process quadratur filter non linear system heavi tail nois,"The aim of this article is to design a moment transformation for Student- t distributed random variables, which is able to account for the error in the numerically computed mean. We employ Student-t process quadrature, an instance of Bayesian quadrature, which allows us to treat the integral itself as a random variable whose variance provides information about the incurred integration error. Advantage of the Student- t process quadrature over the traditional Gaussian process quadrature, is that the integral variance depends also on the function values, allowing for a more robust modelling of the integration error. The moment transform is applied in nonlinear sigma-point filtering and evaluated on two numerical examples, where it is shown to outperform the state-of-the-art moment transforms.",aim articl design moment transform student distribut random variabl abl account error numer comput mean employ student process quadratur instanc bayesian quadratur allow us treat integr random variabl whose varianc provid inform incur integr error advantag student process quadratur tradit gaussian process quadratur integr varianc depend also function valu allow robust model integr error moment transform appli nonlinear sigma point filter evalu two numer exampl shown outperform state art moment transform,"['Jakub Prüher', 'Filip Tronarp', 'Toni Karvonen', 'Simo Särkkä', 'Ondřej Straka']","['stat.ME', 'stat.ML']",False,False,False,False,False,True
1154,2017-03-28T14:04:12Z,2017-03-15T13:51:52Z,http://arxiv.org/abs/1703.05157v1,http://arxiv.org/pdf/1703.05157v1,One-Sided Cross-Validation for Nonsmooth Density Functions,one side cross valid nonsmooth densiti function,One-sided cross-validation (OSCV) is a bandwidth selection method initially introduced by Hart and Yi (1998) in the context of smooth regression functions. Mart\'{\i}nez-Miranda et al. (2009) developed a version of OSCV for smooth density functions. This article extends the method for nonsmooth densities. It also introduces the fully robust OSCV modification that produces consistent OSCV bandwidths for both smooth and nonsmooth cases. Practical implementations of the OSCV method for smooth and nonsmooth densities are discussed. One of the considered cross-validation kernels has potential for improving the OSCV method's implementation in the regression context.,one side cross valid oscv bandwidth select method initi introduc hart yi context smooth regress function mart nez miranda et al develop version oscv smooth densiti function articl extend method nonsmooth densiti also introduc fulli robust oscv modif produc consist oscv bandwidth smooth nonsmooth case practic implement oscv method smooth nonsmooth densiti discuss one consid cross valid kernel potenti improv oscv method implement regress context,['Olga Y. Savchuk'],"['stat.ME', '62G07']",False,False,True,False,False,True
1156,2017-03-28T14:04:12Z,2017-03-15T08:09:27Z,http://arxiv.org/abs/1703.05312v1,http://arxiv.org/pdf/1703.05312v1,On optimal experimental designs for Sparse Polynomial Chaos Expansions,optim experiment design spars polynomi chao expans,"Uncertainty quantification (UQ) has received much attention in the literature in the past decade. In this context, Sparse Polynomial chaos expansions (PCE) have been shown to be among the most promising methods because of their ability to model highly complex models at relatively low computational costs. A least-square minimization technique may be used to determine the coefficients of the sparse PCE by relying on the so called experimental design (ED), i.e. the sample points where the original computational model is evaluated. An efficient sampling strategy is then needed to generate an accurate PCE at low computational cost. This paper is concerned with the problem of identifying an optimal experimental design that maximizes the accuracy of the surrogate model over the whole input space within a given computational budget. A novel sequential adaptive strategy where the ED is enriched sequentially by capitalizing on the sparsity of the underlying metamodel is introduced. A comparative study between several state-of-the-art methods is performed on four numerical models with varying input dimensionality and computational complexity. It is shown that the optimal sequential design based on the S-value criterion yields accurate, stable and computationally efficient PCE.",uncertainti quantif uq receiv much attent literatur past decad context spars polynomi chao expans pce shown among promis method becaus abil model high complex model relat low comput cost least squar minim techniqu may use determin coeffici spars pce reli call experiment design ed sampl point origin comput model evalu effici sampl strategi need generat accur pce low comput cost paper concern problem identifi optim experiment design maxim accuraci surrog model whole input space within given comput budget novel sequenti adapt strategi ed enrich sequenti capit sparsiti metamodel introduc compar studi sever state art method perform four numer model vari input dimension comput complex shown optim sequenti design base valu criterion yield accur stabl comput effici pce,"['N. Fajraoui', 'S. Marelli', 'B. Sudret']",['stat.ME'],False,False,False,False,False,True
1160,2017-03-28T14:04:16Z,2017-03-13T11:19:28Z,http://arxiv.org/abs/1703.04334v1,http://arxiv.org/pdf/1703.04334v1,Probabilistic Matching: Causal Inference under Measurement Errors,probabilist match causal infer measur error,"The abundance of data produced daily from large variety of sources has boosted the need of novel approaches on causal inference analysis from observational data. Observational data often contain noisy or missing entries. Moreover, causal inference studies may require unobserved high-level information which needs to be inferred from other observed attributes. In such cases, inaccuracies of the applied inference methods will result in noisy outputs. In this study, we propose a novel approach for causal inference when one or more key variables are noisy. Our method utilizes the knowledge about the uncertainty of the real values of key variables in order to reduce the bias induced by noisy measurements. We evaluate our approach in comparison with existing methods both on simulated and real scenarios and we demonstrate that our method reduces the bias and avoids false causal inference conclusions in most cases.",abund data produc daili larg varieti sourc boost need novel approach causal infer analysi observ data observ data often contain noisi miss entri moreov causal infer studi may requir unobserv high level inform need infer observ attribut case inaccuraci appli infer method result noisi output studi propos novel approach causal infer one key variabl noisi method util knowledg uncertainti real valu key variabl order reduc bias induc noisi measur evalu approach comparison exist method simul real scenario demonstr method reduc bias avoid fals causal infer conclus case,"['Fani Tsapeli', 'Peter Tino', 'Mirco Musolesi']","['stat.ME', 'stat.CO', 'stat.ML']",False,False,False,False,False,True
1161,2017-03-28T14:04:16Z,2017-03-13T06:08:51Z,http://arxiv.org/abs/1703.04264v1,http://arxiv.org/pdf/1703.04264v1,Poisson multi-Bernoulli mixture filter: direct derivation and   implementation,poisson multi bernoulli mixtur filter direct deriv implement,"We provide a derivation of the Poisson multi-Bernoulli mixture (PMBM) filter for multi-target tracking with the standard point target measurements without using probability generating functionals or functional derivatives. We also establish the connection with the \delta-generalised labelled multi-Bernoulli (\delta-GLMB) filter, showing that a \delta-GLMB density represents a multi-Bernoulli mixture with labelled targets so it can be seen as a special case of PMBM. In addition, we propose an implementation for linear/Gaussian dynamic and measurement models and how to efficiently obtain typical estimators in the literature from the PMBM. The PMBM filter is shown to outperform other filters in the literature in a challenging scenario",provid deriv poisson multi bernoulli mixtur pmbm filter multi target track standard point target measur without use probabl generat function function deriv also establish connect delta generalis label multi bernoulli delta glmb filter show delta glmb densiti repres multi bernoulli mixtur label target seen special case pmbm addit propos implement linear gaussian dynam measur model effici obtain typic estim literatur pmbm pmbm filter shown outperform filter literatur challeng scenario,"['Ángel F. García-Fernández', 'Jason L. Williams', 'Karl Granström', 'Lennart Svensson']","['cs.CV', 'stat.ME']",False,False,False,False,False,True
1165,2017-03-28T14:04:16Z,2017-03-11T00:54:13Z,http://arxiv.org/abs/1703.03882v1,http://arxiv.org/pdf/1703.03882v1,Generalized Full Matching,general full match,"Matching methods are used to make units comparable on observed characteristics. Full matching can be used to derive optimal matches. However, the method has only been defined in the case of two treatment categories, it places unnecessary restrictions on the matched groups, and existing implementations are computationally intractable in large samples. As a result, the method has not been feasible in studies with large samples or complex designs. We introduce a generalization of full matching that inherits its optimality properties but allows the investigator to specify any desired structure of the matched groups over any number of treatment conditions. We also describe a new approximation algorithm to derive generalized full matchings. In the worst case, the maximum within-group dissimilarity produced by the algorithm is no worse than four times the optimal solution, but it typically performs close to on par with existing optimal algorithms when they exist. Despite its performance, the algorithm is fast and uses little memory: it terminates, on average, in linearithmic time using linear space. This enables investigators to derive well-performing matchings within minutes even in complex studies with samples of several million units.",match method use make unit compar observ characterist full match use deriv optim match howev method onli defin case two treatment categori place unnecessari restrict match group exist implement comput intract larg sampl result method feasibl studi larg sampl complex design introduc general full match inherit optim properti allow investig specifi ani desir structur match group ani number treatment condit also describ new approxim algorithm deriv general full match worst case maximum within group dissimilar produc algorithm wors four time optim solut typic perform close par exist optim algorithm exist despit perform algorithm fast use littl memori termin averag linearithm time use linear space enabl investig deriv well perform match within minut even complex studi sampl sever million unit,"['Fredrik Sävje', 'Michael J. Higgins', 'Jasjeet S. Sekhon']",['stat.ME'],False,False,False,False,False,True
1166,2017-03-28T14:04:16Z,2017-03-09T15:56:58Z,http://arxiv.org/abs/1703.03312v1,http://arxiv.org/pdf/1703.03312v1,Split Sample Empirical Likelihood,split sampl empir likelihood,"We propose a new approach that combines multiple non-parametric likelihood-type components to build a data-driven approximation of the true likelihood function. Our approach is built on empirical likelihood, a non-parametric approximation of the likelihood function. We show the asymptotic behaviors of our approach are identical to those seen in empirical likelihood. We demonstrate that our method performs comparably to empirical likelihood while significantly decreasing computational time.",propos new approach combin multipl non parametr likelihood type compon build data driven approxim true likelihood function approach built empir likelihood non parametr approxim likelihood function show asymptot behavior approach ident seen empir likelihood demonstr method perform compar empir likelihood signific decreas comput time,"['Adam Jaeger', 'Nicole Lazar']",['stat.ME'],False,False,True,False,False,True
1168,2017-03-28T14:04:16Z,2017-03-09T07:27:33Z,http://arxiv.org/abs/1703.03165v1,http://arxiv.org/pdf/1703.03165v1,Perturbation Bootstrap in Adaptive Lasso,perturb bootstrap adapt lasso,"The Adaptive LASSO (ALASSO) was proposed by Zou [J. Amer. Statist. Assoc. 101 (2006) 1418-1429] as a modification of the LASSO for the purpose of simultaneous variable selection and estimation of the parameters in a linear regression model. Zou (2006) established that the ALASSO estimator is variable-selection consistent as well as asymptotically Normal in the indices corresponding to the nonzero regression coefficients in certain fixed-dimensional settings. In an influential paper, Minnier, Tian and Cai [J. Amer. Statist. Assoc. 106 (2011) 1371-1382] proposed a perturbation bootstrap method and established its distributional consistency for the ALASSO estimator in the fixed-dimensional setting. In this paper, however, we show that this (naive) perturbation bootstrap fails to achieve second order correctness in approximating the distribution of the ALASSO estimator. We propose a modification to the perturbation bootstrap objective function and show that a suitably studentized version of our modified perturbation bootstrap ALASSO estimator achieves second-order correctness even when the dimension of the model is allowed to grow to infinity with the sample size. As a consequence, inferences based on the modified perturbation bootstrap will be more accurate than the inferences based on the oracle Normal approximation. We give simulation studies demonstrating good finite-sample properties of our modified perturbation bootstrap method as well as an illustration of our method on a real data set.",adapt lasso alasso propos zou amer statist assoc modif lasso purpos simultan variabl select estim paramet linear regress model zou establish alasso estim variabl select consist well asymptot normal indic correspond nonzero regress coeffici certain fix dimension set influenti paper minnier tian cai amer statist assoc propos perturb bootstrap method establish distribut consist alasso estim fix dimension set paper howev show naiv perturb bootstrap fail achiev second order correct approxim distribut alasso estim propos modif perturb bootstrap object function show suitabl student version modifi perturb bootstrap alasso estim achiev second order correct even dimens model allow grow infin sampl size consequ infer base modifi perturb bootstrap accur infer base oracl normal approxim give simul studi demonstr good finit sampl properti modifi perturb bootstrap method well illustr method real data set,"['Debraj Das', 'Karl Gregory', 'S. N. Lahiri']","['stat.ME', 'math.ST', 'stat.TH']",False,False,False,False,False,True
1169,2017-03-28T14:04:16Z,2017-03-09T03:49:33Z,http://arxiv.org/abs/1703.03123v1,http://arxiv.org/pdf/1703.03123v1,Calibrated Data Augmentation for Scalable Markov Chain Monte Carlo,calibr data augment scalabl markov chain mont carlo,"Data augmentation is a common technique for building tuning-free Markov chain Monte Carlo algorithms. Although these algorithms are very popular, autocorrelations are often high in large samples, leading to poor computational efficiency. This phenomenon has been attributed to a discrepancy between Gibbs step sizes and the rate of posterior concentration. In this article, we propose a family of calibrated data augmentation algorithms, which adjust for this discrepancy by inflating Gibbs step sizes while adjusting for bias. A Metropolis-Hastings step is included to account for the slight discrepancy between the stationary distribution of the resulting sampler and the exact posterior distribution. The approach is applicable to a broad variety of existing data augmentation algorithms, and we focus on three popular models: probit, logistic and Poisson log-linear. Dramatic gains in computational efficiency are shown in applications.",data augment common techniqu build tune free markov chain mont carlo algorithm although algorithm veri popular autocorrel often high larg sampl lead poor comput effici phenomenon attribut discrep gibb step size rate posterior concentr articl propos famili calibr data augment algorithm adjust discrep inflat gibb step size adjust bias metropoli hast step includ account slight discrep stationari distribut result sampler exact posterior distribut approach applic broad varieti exist data augment algorithm focus three popular model probit logist poisson log linear dramat gain comput effici shown applic,"['Leo L. Duan', 'James E. Johndrow', 'David B. Dunson']",['stat.ME'],False,False,False,False,False,True
1172,2017-03-28T14:04:20Z,2017-03-08T20:32:20Z,http://arxiv.org/abs/1703.03023v1,http://arxiv.org/pdf/1703.03023v1,"Elicitation, measuring bias, checking for prior-data conflict and   inference with a Dirichlet prior",elicit measur bias check prior data conflict infer dirichlet prior,Methods are developed for eliciting a Dirichlet prior based upon bounds on the individual probabilities that hold with virtual certainty. This approach to selecting a prior is applied to a contingency table problem where it is demonstrated how to assess the bias in the prior as well as how to check for prior-data conflict. It is shown that the assessment of a hypothesis via relative belief can easily take into account what it means for the falsity of the hypothesis to correspond to a difference of practical importance and provide evidence in favor of a hypothesis.,method develop elicit dirichlet prior base upon bound individu probabl hold virtual certainti approach select prior appli conting tabl problem demonstr assess bias prior well check prior data conflict shown assess hypothesi via relat belief easili take account mean falsiti hypothesi correspond differ practic import provid evid favor hypothesi,"['Michael Evans', 'Irwin Guttman', 'Peiying Li']","['stat.ME', '62F15']",False,False,True,False,False,True
1175,2017-03-28T14:04:20Z,2017-03-08T07:41:13Z,http://arxiv.org/abs/1703.02736v1,http://arxiv.org/pdf/1703.02736v1,Profile Estimation for Partial Functional Partially Linear Single-Index   Model,profil estim partial function partial linear singl index model,"This paper studies a \textit{partial functional partially linear single-index model} that consists of a functional linear component as well as a linear single-index component. This model generalizes many well-known existing models and is suitable for more complicated data structures. However, its estimation inherits the difficulties and complexities from both components and makes it a challenging problem, which calls for new methodology. We propose a novel profile B-spline method to estimate the parameters by approximating the unknown nonparametric link function in the single-index component part with B-spline, while the linear slope function in the functional component part is estimated by the functional principal component basis. The consistency and asymptotic normality of the parametric estimators are derived, and the global convergence of the proposed estimator of the linear slope function is also established. More excitingly, the latter convergence is optimal in the minimax sense. A two-stage procedure is implemented to estimate the nonparametric link function, and the resulting estimator possesses the optimal global rate of convergence. Furthermore, the convergence rate of the mean squared prediction error for a predictor is also obtained. Empirical properties of the proposed procedures are studied through Monte Carlo simulations. A real data example is also analyzed to illustrate the power and flexibility of the proposed methodology.",paper studi textit partial function partial linear singl index model consist function linear compon well linear singl index compon model general mani well known exist model suitabl complic data structur howev estim inherit difficulti complex compon make challeng problem call new methodolog propos novel profil spline method estim paramet approxim unknown nonparametr link function singl index compon part spline linear slope function function compon part estim function princip compon basi consist asymptot normal parametr estim deriv global converg propos estim linear slope function also establish excit latter converg optim minimax sens two stage procedur implement estim nonparametr link function result estim possess optim global rate converg furthermor converg rate mean squar predict error predictor also obtain empir properti propos procedur studi mont carlo simul real data exampl also analyz illustr power flexibl propos methodolog,"['Qingguo Tang', 'Linglong Kong', 'David Ruppert', 'Rohana J. Karunamuni']","['math.ST', 'stat.ME', 'stat.TH']",False,False,False,False,False,True
1176,2017-03-28T14:04:20Z,2017-03-08T06:22:56Z,http://arxiv.org/abs/1703.02724v1,http://arxiv.org/pdf/1703.02724v1,Guaranteed Tensor PCA with Optimality in Statistics and Computation,guarante tensor pca optim statist comput,"Tensors, or high-order arrays, attract much attention in recent research. In this paper, we propose a general framework for tensor principal component analysis (tensor PCA), which focuses on the methodology and theory for extracting the hidden low-rank structure from the high-dimensional tensor data. A unified solution is provided for tensor PCA with considerations in both statistical limits and computational costs. The problem exhibits three different phases according to the signal-noise-ratio (SNR). In particular, with strong SNR, we propose a fast spectral power iteration method that achieves the minimax optimal rate of convergence in estimation; with weak SNR, the information-theoretical lower bound shows that it is impossible to have consistent estimation in general; with moderate SNR, we show that the non-convex maximum likelihood estimation provides optimal solution, but with NP-hard computational cost; moreover, under the hardness hypothesis of hypergraphic planted clique detection, there are no polynomial-time algorithms performing consistently in general. Simulation studies show that the proposed spectral power iteration method have good performance under a variety of settings.",tensor high order array attract much attent recent research paper propos general framework tensor princip compon analysi tensor pca focus methodolog theori extract hidden low rank structur high dimension tensor data unifi solut provid tensor pca consider statist limit comput cost problem exhibit three differ phase accord signal nois ratio snr particular strong snr propos fast spectral power iter method achiev minimax optim rate converg estim weak snr inform theoret lower bound show imposs consist estim general moder snr show non convex maximum likelihood estim provid optim solut np hard comput cost moreov hard hypothesi hypergraph plant cliqu detect polynomi time algorithm perform consist general simul studi show propos spectral power iter method good perform varieti set,"['Anru Zhang', 'Dong Xia']","['math.ST', 'cs.LG', 'stat.ME', 'stat.ML', 'stat.TH']",False,False,True,False,False,True
1177,2017-03-28T14:04:20Z,2017-03-08T03:07:37Z,http://arxiv.org/abs/1703.02679v1,http://arxiv.org/pdf/1703.02679v1,Performance Bounds for Graphical Record Linkage,perform bound graphic record linkag,"Record linkage involves merging records in large, noisy databases to remove duplicate entities. It has become an important area because of its widespread occurrence in bibliometrics, public health, official statistics production, political science, and beyond. Traditional linkage methods directly linking records to one another are computationally infeasible as the number of records grows. As a result, it is increasingly common for researchers to treat record linkage as a clustering task, in which each latent entity is associated with one or more noisy database records. We critically assess performance bounds using the Kullback-Leibler (KL) divergence under a Bayesian record linkage framework, making connections to Kolchin partition models. We provide an upper bound using the KL divergence and a lower bound on the minimum probability of misclassifying a latent entity. We give insights for when our bounds hold using simulated data and provide practical user guidance.",record linkag involv merg record larg noisi databas remov duplic entiti becom import area becaus widespread occurr bibliometr public health offici statist product polit scienc beyond tradit linkag method direct link record one anoth comput infeas number record grow result increas common research treat record linkag cluster task latent entiti associ one noisi databas record critic assess perform bound use kullback leibler kl diverg bayesian record linkag framework make connect kolchin partit model provid upper bound use kl diverg lower bound minimum probabl misclassifi latent entiti give insight bound hold use simul data provid practic user guidanc,"['Rebecca C. Steorts', 'Matt Barnes', 'Willie Neiswanger']","['math.ST', 'cs.IT', 'math.IT', 'stat.ME', 'stat.ML', 'stat.TH']",False,False,False,False,False,True
1180,2017-03-28T14:04:24Z,2017-03-07T15:13:08Z,http://arxiv.org/abs/1703.02428v1,http://arxiv.org/pdf/1703.02428v1,Robust Bayesian Filtering and Smoothing Using Student's t Distribution,robust bayesian filter smooth use student distribut,"State estimation in heavy-tailed process and measurement noise is an important challenge that must be addressed in, e.g., tracking scenarios with agile targets and outlier-corrupted measurements. The performance of the Kalman filter (KF) can deteriorate in such applications because of the close relation to the Gaussian distribution. Therefore, this paper describes the use of Student's t distribution to develop robust, scalable, and simple filtering and smoothing algorithms.   After a discussion of Student's t distribution, exact filtering in linear state-space models with t noise is analyzed. Intermediate approximation steps are used to arrive at filtering and smoothing algorithms that closely resemble the KF and the Rauch-Tung-Striebel (RTS) smoother except for a nonlinear measurement-dependent matrix update. The required approximations are discussed and an undesirable behavior of moment matching for t densities is revealed. A favorable approximation based on minimization of the Kullback-Leibler divergence is presented. Because of its relation to the KF, some properties and algorithmic extensions are inherited by the t filter. Instructive simulation examples demonstrate the performance and robustness of the novel algorithms.",state estim heavi tail process measur nois import challeng must address track scenario agil target outlier corrupt measur perform kalman filter kf deterior applic becaus close relat gaussian distribut therefor paper describ use student distribut develop robust scalabl simpl filter smooth algorithm discuss student distribut exact filter linear state space model nois analyz intermedi approxim step use arriv filter smooth algorithm close resembl kf rauch tung striebel rts smoother except nonlinear measur depend matrix updat requir approxim discuss undesir behavior moment match densiti reveal favor approxim base minim kullback leibler diverg present becaus relat kf properti algorithm extens inherit filter instruct simul exampl demonstr perform robust novel algorithm,"['Michael Roth', 'Tohid Ardeshiri', 'Emre Özkan', 'Fredrik Gustafsson']","['stat.ME', 'cs.SY', 'stat.CO']",False,False,False,False,False,True
1181,2017-03-28T14:04:24Z,2017-03-07T09:39:00Z,http://arxiv.org/abs/1703.02296v1,http://arxiv.org/pdf/1703.02296v1,Low-rank Interaction Contingency Tables,low rank interact conting tabl,"Log-linear models are popular tools to analyze contingency tables, particularly to model row and column effects as well as row-column interactions in two-way tables. In this paper, we introduce a regularized log-linear model designed for denoising and visualizing count data, which can incorporate side information such as row and column features. The estimation is performed through a convex optimization problem where we minimize a negative Poisson log-likelihood penalized by the nuclear norm of the interaction matrix. We derive an upper bound on the Frobenius estimation error, which improves previous rates for Poisson matrix recovery, and an algorithm based on the alternating direction method of multipliers to compute our estimator. To propose a complete methodology to users, we also address automatic selection of the regularization parameter. A Monte Carlo simulation reveals that our estimator is particularly well suited to estimate the rank of the interaction in low signal to noise ratio regimes. We illustrate with two data analyses that the results can be easily interpreted through biplot vizualization. The method is available as an R code.",log linear model popular tool analyz conting tabl particular model row column effect well row column interact two way tabl paper introduc regular log linear model design denois visual count data incorpor side inform row column featur estim perform convex optim problem minim negat poisson log likelihood penal nuclear norm interact matrix deriv upper bound frobenius estim error improv previous rate poisson matrix recoveri algorithm base altern direct method multipli comput estim propos complet methodolog user also address automat select regular paramet mont carlo simul reveal estim particular well suit estim rank interact low signal nois ratio regim illustr two data analys result easili interpret biplot vizual method avail code,"['Geneviève Robin', 'Julie Josse', 'Eric Moulines', 'Sylvain Sardy']",['stat.ME'],False,False,False,False,False,True
1184,2017-03-28T14:04:24Z,2017-03-06T21:19:20Z,http://arxiv.org/abs/1703.02113v1,http://arxiv.org/pdf/1703.02113v1,A fresh look at effect aliasing and interactions: some new wine in old   bottles,fresh look effect alias interact new wine old bottl,"Interactions and effect aliasing are among the fundamental concepts in experimental design. In this paper, some new insights and approaches are provided on these subjects. In the literature, the ""de-aliasing"" of aliased effects is deemed to be impossible. We argue that this ""impossibility"" can indeed be resolved by employing a new approach which consists of reparametrization of effects and exploitation of effect non-orthogonality. This approach is successfully applied to three classes of designs: regular and nonregular two-level fractional factorial designs, and three-level fractional factorial designs. For reparametrization, the notion of conditional main effects (cme's) is employed for two-level regular designs, while the linear-quadratic system is used for three-level designs. For nonregular two-level designs, reparametrization is not needed because the partial aliasing of their effects already induces non-orthogonality. The approach can be extended to general observational data by using a new bi-level variable selection technique based on the cme's. A historical recollection is given on how these ideas were discovered.",interact effect alias among fundament concept experiment design paper new insight approach provid subject literatur de alias alias effect deem imposs argu imposs inde resolv employ new approach consist reparametr effect exploit effect non orthogon approach success appli three class design regular nonregular two level fraction factori design three level fraction factori design reparametr notion condit main effect cme employ two level regular design linear quadrat system use three level design nonregular two level design reparametr need becaus partial alias effect alreadi induc non orthogon approach extend general observ data use new bi level variabl select techniqu base cme histor recollect given idea discov,['C. F. Jeff Wu'],['stat.ME'],False,False,False,False,False,True
1185,2017-03-28T14:04:24Z,2017-03-06T21:17:42Z,http://arxiv.org/abs/1703.02112v1,http://arxiv.org/pdf/1703.02112v1,Process convolution approaches for modeling interacting trajectories,process convolut approach model interact trajectori,"Gaussian processes are a fundamental statistical tool used in a wide range of applications. In the spatio-temporal setting, several families of covariance functions exist to accommodate a wide variety of dependence structures arising in different applications. These parametric families can be restrictive and are insufficient in some situations. In contrast, process convolutions represent a flexible, interpretable approach to defining the covariance of a Gaussian process and have modest requirements to ensure validity. We introduce a generalization of the process convolution approach that employs multiple convolutions sequentially to form a ""process convolution chain."" In our proposed multi-stage framework, complex dependencies that arise from a combination of different interacting mechanisms are decomposed into a series of interpretable kernel smoothers. We demonstrate an application of process convolution chains to model killer whale movement, in which the paths taken by multiple individuals are not independent, but reflect dynamic social interactions within the population. Our proposed model for dependent movement provides inference for the latent dynamic social structure in the study population. Additionally, by leveraging the positive dependence among individual paths, we achieve a reduction in uncertainty for the estimated locations of the whales, compared to a model that treats paths as independent.",gaussian process fundament statist tool use wide rang applic spatio tempor set sever famili covari function exist accommod wide varieti depend structur aris differ applic parametr famili restrict insuffici situat contrast process convolut repres flexibl interpret approach defin covari gaussian process modest requir ensur valid introduc general process convolut approach employ multipl convolut sequenti form process convolut chain propos multi stage framework complex depend aris combin differ interact mechan decompos seri interpret kernel smoother demonstr applic process convolut chain model killer whale movement path taken multipl individu independ reflect dynam social interact within popul propos model depend movement provid infer latent dynam social structur studi popul addit leverag posit depend among individu path achiev reduct uncertainti estim locat whale compar model treat path independ,"['Henry R. Scharf', 'Mevin B. Hooten', 'Devin S. Johnson', 'John W. Durban']","['stat.ME', 'stat.AP']",False,False,False,False,False,True
1188,2017-03-28T14:04:24Z,2017-03-06T10:32:54Z,http://arxiv.org/abs/1703.01805v1,http://arxiv.org/pdf/1703.01805v1,Bayesian Estimation of Kendall's tau Using a Latent Normal Approach,bayesian estim kendal tau use latent normal approach,"The rank-based association between two variables can be modeled by introducing a latent normal level to ordinal data. We demonstrate how this approach yields Bayesian inference for Kendall's rank correlation coefficient, improving on a recent Bayesian solution from asymptotic properties of the test statistic.",rank base associ two variabl model introduc latent normal level ordin data demonstr approach yield bayesian infer kendal rank correl coeffici improv recent bayesian solut asymptot properti test statist,"['Johnny van Doorn', 'Alexander Ly', 'Maarten Marsman', 'Eric-Jan Wagenmakers']",['stat.ME'],False,False,False,False,False,True
1189,2017-03-28T14:04:24Z,2017-03-06T09:24:07Z,http://arxiv.org/abs/1703.01776v1,http://arxiv.org/pdf/1703.01776v1,Online Sequential Monte Carlo smoother for partially observed stochastic   differential equations,onlin sequenti mont carlo smoother partial observ stochast differenti equat,"This paper introduces a new algorithm to approximate smoothed additive functionals for partially observed stochastic differential equations. This method relies on a recent procedure which allows to compute such approximations online, i.e. as the observations are received, and with a computational complexity growing linearly with the number of Monte Carlo samples. This online smoother cannot be used directly in the case of partially observed stochastic differential equations since the transition density of the latent data is usually unknown. We prove that a similar algorithm may still be defined for partially observed continuous processes by replacing this unknown quantity by an unbiased estimator obtained for instance using general Poisson estimators. We prove that this estimator is consistent and its performance are illustrated using data from two models.",paper introduc new algorithm approxim smooth addit function partial observ stochast differenti equat method reli recent procedur allow comput approxim onlin observ receiv comput complex grow linear number mont carlo sampl onlin smoother cannot use direct case partial observ stochast differenti equat sinc transit densiti latent data usual unknown prove similar algorithm may still defin partial observ continu process replac unknown quantiti unbias estim obtain instanc use general poisson estim prove estim consist perform illustr use data two model,"['Pierre Gloaguen', 'Marie-Pierre Etienne', 'Sylvain Le Corff']","['stat.ME', 'stat.AP']",False,False,False,False,False,True
1191,2017-03-28T14:04:28Z,2017-03-05T21:14:28Z,http://arxiv.org/abs/1703.01665v1,http://arxiv.org/pdf/1703.01665v1,Anisotropic functional Laplace deconvolution,anisotrop function laplac deconvolut,"In the present paper we consider the problem of estimating a three-dimensional function $f$ based on observations from its noisy Laplace convolution. Our study is motivated by the analysis of Dynamic Contrast Enhanced (DCE) imaging data. We construct an adaptive wavelet-Laguerre estimator of $f$, derive minimax lower bounds for the $L^2$-risk when $f$ belongs to a three-dimensional Laguerre-Sobolev ball and demonstrate that the wavelet-Laguerre estimator is adaptive and asymptotically near-optimal in a wide range of Laguerre-Sobolev spaces. We carry out a limited simulations study and show that the estimator performs well in a finite sample setting. Finally, we use the technique for the solution of the Laplace deconvolution problem on the basis of DCE Computerized Tomography data.",present paper consid problem estim three dimension function base observ noisi laplac convolut studi motiv analysi dynam contrast enhanc dce imag data construct adapt wavelet laguerr estim deriv minimax lower bound risk belong three dimension laguerr sobolev ball demonstr wavelet laguerr estim adapt asymptot near optim wide rang laguerr sobolev space carri limit simul studi show estim perform well finit sampl set final use techniqu solut laplac deconvolut problem basi dce computer tomographi data,"['Rida Benhaddou', 'Marianna Pensky', 'Rasika Rajapakshage']","['stat.ME', 'Primary 62G05, , secondary 62G08, 62P35']",False,False,False,False,False,True
1193,2017-03-28T14:04:28Z,2017-03-04T09:12:42Z,http://arxiv.org/abs/1703.01421v1,http://arxiv.org/pdf/1703.01421v1,$l_0$-estimation of piecewise-constant signals on graphs,estim piecewis constant signal graph,"We study recovery of piecewise-constant signals over arbitrary graphs by the estimator minimizing an $l_0$-edge-penalized objective. Although exact minimization of this objective may be computationally intractable, we show that the same statistical risk guarantees are achieved by the alpha-expansion algorithm which approximately minimizes this objective in polynomial time. We establish that for graphs with small average vertex degree, these guarantees are rate-optimal in a minimax sense over classes of edge-sparse signals. For application to spatially inhomogeneous graphs, we propose minimization of an edge-weighted variant of this objective where each edge is weighted by its effective resistance or another measure of its contribution to the graph's connectivity. We establish minimax optimality of the resulting estimators over corresponding edge-weighted sparsity classes. We show theoretically that these risk guarantees are not always achieved by the estimator minimizing the $l_1$/total-variation relaxation, and empirically that the $l_0$-based estimates are more accurate in high signal-to-noise settings.",studi recoveri piecewis constant signal arbitrari graph estim minim edg penal object although exact minim object may comput intract show statist risk guarante achiev alpha expans algorithm approxim minim object polynomi time establish graph small averag vertex degre guarante rate optim minimax sens class edg spars signal applic spatial inhomogen graph propos minim edg weight variant object edg weight effect resist anoth measur contribut graph connect establish minimax optim result estim correspond edg weight sparsiti class show theoret risk guarante alway achiev estim minim total variat relax empir base estim accur high signal nois set,"['Zhou Fan', 'Leying Guan']","['stat.ME', 'math.ST', 'stat.CO', 'stat.TH']",False,False,False,False,False,True
1195,2017-03-28T14:04:28Z,2017-03-03T16:29:21Z,http://arxiv.org/abs/1703.01234v1,http://arxiv.org/pdf/1703.01234v1,A Bayesian computer model analysis of Robust Bayesian analyses,bayesian comput model analysi robust bayesian analys,"We harness the power of Bayesian emulation techniques, designed to aid the analysis of complex computer models, to examine the structure of complex Bayesian analyses themselves. These techniques facilitate robust Bayesian analyses and/or sensitivity analyses of complex problems, and hence allow global exploration of the impacts of choices made in both the likelihood and prior specification. We show how previously intractable problems in robustness studies can be overcome using emulation techniques, and how these methods allow other scientists to quickly extract approximations to posterior results corresponding to their own particular subjective specification. The utility and flexibility of our method is demonstrated on a reanalysis of a real application where Bayesian methods were employed to capture beliefs about river flow. We discuss the obvious extensions and directions of future research that such an approach opens up.",har power bayesian emul techniqu design aid analysi complex comput model examin structur complex bayesian analys themselv techniqu facilit robust bayesian analys sensit analys complex problem henc allow global explor impact choic made likelihood prior specif show previous intract problem robust studi overcom use emul techniqu method allow scientist quick extract approxim posterior result correspond particular subject specif util flexibl method demonstr reanalysi real applic bayesian method employ captur belief river flow discuss obvious extens direct futur research approach open,"['Ian Vernon', 'John Paul Gosling']","['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']",False,False,False,False,False,True
1197,2017-03-28T14:04:28Z,2017-03-02T22:17:52Z,http://arxiv.org/abs/1703.00968v1,http://arxiv.org/pdf/1703.00968v1,Bayesian inference for generalized extreme value distribution with   Gaussian copula dependence,bayesian infer general extrem valu distribut gaussian copula depend,"Dependent generalized extreme value (dGEV) models have attracted much attention due to the dependency structure that often appears in real datasets. To construct a dGEV model, a natural approach is to assume that some parameters in the model are time-varying. A previous study has shown that a dependent Gumbel process can be naturally incorporated into a GEV model. The model is a nonlinear state space model with a hidden state that follows a Markov process, with its innovation following a Gumbel distribution. Inference may be made for the model using Bayesian methods, sampling the hidden process from a mixture normal distribution, used to approximate the Gumbel distribution. Thus the response follows an approximate GEV model. We propose a new model in which each marginal distribution is an exact GEV distribution. We use a variable transformation to combine the marginal CDF of a Gumbel distribution with the standard normal copula. Then our model is a nonlinear state space model in which the hidden state equation is Gaussian. We analyze this model using Bayesian methods, and sample the elements of the state vector using particle Gibbs with ancestor sampling (PGAS). The PGAS algorithm turns out to be very efficient in solving nonlinear state space models. We also show our model is flexible enough to incorporate seasonality.",depend general extrem valu dgev model attract much attent due depend structur often appear real dataset construct dgev model natur approach assum paramet model time vari previous studi shown depend gumbel process natur incorpor gev model model nonlinear state space model hidden state follow markov process innov follow gumbel distribut infer may made model use bayesian method sampl hidden process mixtur normal distribut use approxim gumbel distribut thus respons follow approxim gev model propos new model margin distribut exact gev distribut use variabl transform combin margin cdf gumbel distribut standard normal copula model nonlinear state space model hidden state equat gaussian analyz model use bayesian method sampl element state vector use particl gibb ancestor sampl pgas pgas algorithm turn veri effici solv nonlinear state space model also show model flexibl enough incorpor season,"['Bo Ning', 'Peter Bloomfield']",['stat.ME'],False,False,False,False,False,True
1198,2017-03-28T14:04:28Z,2017-03-02T18:20:18Z,http://arxiv.org/abs/1703.00884v1,http://arxiv.org/pdf/1703.00884v1,A Dichotomy for Sampling Barrier-Crossing Events of Random Walks with   Regularly Varying Tails,dichotomi sampl barrier cross event random walk regular vari tail,"We study how to sample paths of a random walk up to the first time it crosses a fixed barrier, in the setting where the step sizes are iid with negative mean and have a regularly varying right tail. We introduce a desirable property for a change of measure to be suitable for exact simulation. We study whether the change of measure of Blanchet and Glynn (2008) satisfies this property and show that it does so if and only if the tail index $\alpha$ of the right tail lies in the interval $(1, \, 3/2)$.",studi sampl path random walk first time cross fix barrier set step size iid negat mean regular vari right tail introduc desir properti chang measur suitabl exact simul studi whether chang measur blanchet glynn satisfi properti show doe onli tail index alpha right tail lie interv,"['Ton Dieker', 'Guido Lagos']","['math.PR', 'stat.ME', '68U20, 60G50, 68W40']",False,False,True,False,False,True
1199,2017-03-28T14:04:28Z,2017-03-02T11:48:24Z,http://arxiv.org/abs/1703.00734v1,http://arxiv.org/pdf/1703.00734v1,Distributed Bayesian Matrix Factorization with Minimal Communication,distribut bayesian matrix factor minim communic,"Bayesian matrix factorization (BMF) is a powerful tool for producing low-rank representations of matrices, and giving principled predictions of missing values. However, scaling up MCMC samplers to large matrices has proven to be difficult with parallel algorithms that require communication between MCMC iterations. On the other hand, designing communication-free algorithms is challenging due to the inherent unidentifiability of BMF solutions. We propose posterior propagation, an embarrassingly parallel inference procedure, which hierarchically introduces dependencies between data subsets and thus alleviates the unidentifiability problem.",bayesian matrix factor bmf power tool produc low rank represent matric give principl predict miss valu howev scale mcmc sampler larg matric proven difficult parallel algorithm requir communic mcmc iter hand design communic free algorithm challeng due inher unidentifi bmf solut propos posterior propag embarrass parallel infer procedur hierarch introduc depend data subset thus allevi unidentifi problem,"['Xiangju Qin', 'Paul Blomstedt', 'Eemeli Leppäaho', 'Pekka Parviainen', 'Samuel Kaski']","['stat.ML', 'cs.DC', 'cs.LG', 'cs.NA', 'stat.ME']",False,False,False,False,False,True
1202,2017-03-28T14:01:45Z,2017-03-27T17:25:02Z,http://arxiv.org/abs/1703.09194v1,http://arxiv.org/pdf/1703.09194v1,Sticking the Landing: An Asymptotically Zero-Variance Gradient Estimator   for Variational Inference,stick land asymptot zero varianc gradient estim variat infer,"We propose a simple and general variant of the standard reparameterized gradient estimator for the variational evidence lower bound. Specifically, we remove a part of the total derivative with respect to the variational parameters that corresponds to the score function. Removing this term produces an unbiased gradient estimator whose variance approaches zero as the approximate posterior approaches the exact posterior. We analyze the behavior of this gradient estimator theoretically and empirically, and generalize it to more complex variational distributions such as mixtures and importance-weighted posteriors.",propos simpl general variant standard reparameter gradient estim variat evid lower bound specif remov part total deriv respect variat paramet correspond score function remov term produc unbias gradient estim whose varianc approach zero approxim posterior approach exact posterior analyz behavior gradient estim theoret empir general complex variat distribut mixtur import weight posterior,"['Geoffrey Roeder', 'Yuhuai Wu', 'David Duvenaud']","['stat.ML', 'cs.LG']",False,False,False,False,False,True
1206,2017-03-28T14:01:45Z,2017-03-27T08:45:57Z,http://arxiv.org/abs/1703.08972v1,http://arxiv.org/pdf/1703.08972v1,Thompson Sampling for Linear-Quadratic Control Problems,thompson sampl linear quadrat control problem,"We consider the exploration-exploitation tradeoff in linear quadratic (LQ) control problems, where the state dynamics is linear and the cost function is quadratic in states and controls. We analyze the regret of Thompson sampling (TS) (a.k.a. posterior-sampling for reinforcement learning) in the frequentist setting, i.e., when the parameters characterizing the LQ dynamics are fixed. Despite the empirical and theoretical success in a wide range of problems from multi-armed bandit to linear bandit, we show that when studying the frequentist regret TS in control problems, we need to trade-off the frequency of sampling optimistic parameters and the frequency of switches in the control policy. This results in an overall regret of $O(T^{2/3})$, which is significantly worse than the regret $O(\sqrt{T})$ achieved by the optimism-in-face-of-uncertainty algorithm in LQ control problems.",consid explor exploit tradeoff linear quadrat lq control problem state dynam linear cost function quadrat state control analyz regret thompson sampl ts posterior sampl reinforc learn frequentist set paramet character lq dynam fix despit empir theoret success wide rang problem multi arm bandit linear bandit show studi frequentist regret ts control problem need trade frequenc sampl optimist paramet frequenc switch control polici result overal regret signific wors regret sqrt achiev optim face uncertainti algorithm lq control problem,"['Marc Abeille', 'Alessandro Lazaric']",['stat.ML'],False,False,False,False,False,True
1207,2017-03-28T14:01:45Z,2017-03-27T05:41:03Z,http://arxiv.org/abs/1703.08937v1,http://arxiv.org/pdf/1703.08937v1,A Scale Free Algorithm for Stochastic Bandits with Bounded Kurtosis,scale free algorithm stochast bandit bound kurtosi,"Existing strategies for finite-armed stochastic bandits mostly depend on a parameter of scale that must be known in advance. Sometimes this is in the form of a bound on the payoffs, or the knowledge of a variance or subgaussian parameter. The notable exceptions are the analysis of Gaussian bandits with unknown mean and variance by Cowan and Katehakis [2015] and of uniform distributions with unknown support [Cowan and Katehakis, 2015]. The results derived in these specialised cases are generalised here to the non-parametric setup, where the learner knows only a bound on the kurtosis of the noise, which is a scale free measure of the extremity of outliers.",exist strategi finit arm stochast bandit depend paramet scale must known advanc sometim form bound payoff knowledg varianc subgaussian paramet notabl except analysi gaussian bandit unknown mean varianc cowan katehaki uniform distribut unknown support cowan katehaki result deriv specialis case generalis non parametr setup learner know onli bound kurtosi nois scale free measur extrem outlier,['Tor Lattimore'],['stat.ML'],False,False,True,False,False,True
1208,2017-03-28T14:01:45Z,2017-03-26T16:01:28Z,http://arxiv.org/abs/1703.08831v1,http://arxiv.org/pdf/1703.08831v1,Token-based Function Computation with Memory,token base function comput memori,"In distributed function computation, each node has an initial value and the goal is to compute a function of these values in a distributed manner. In this paper, we propose a novel token-based approach to compute a wide class of target functions to which we refer as ""Token-based function Computation with Memory"" (TCM) algorithm. In this approach, node values are attached to tokens and travel across the network. Each pair of travelling tokens would coalesce when they meet, forming a token with a new value as a function of the original token values. In contrast to the Coalescing Random Walk (CRW) algorithm, where token movement is governed by random walk, meeting of tokens in our scheme is accelerated by adopting a novel chasing mechanism. We proved that, compared to the CRW algorithm, the TCM algorithm results in a reduction of time complexity by a factor of at least $\sqrt{n/\log(n)}$ in Erd\""os-Renyi and complete graphs, and by a factor of $\log(n)/\log(\log(n))$ in torus networks. Simulation results show that there is at least a constant factor improvement in the message complexity of TCM algorithm in all considered topologies. Robustness of the CRW and TCM algorithms in the presence of node failure is analyzed. We show that their robustness can be improved by running multiple instances of the algorithms in parallel.",distribut function comput node initi valu goal comput function valu distribut manner paper propos novel token base approach comput wide class target function refer token base function comput memori tcm algorithm approach node valu attach token travel across network pair travel token would coalesc meet form token new valu function origin token valu contrast coalesc random walk crw algorithm token movement govern random walk meet token scheme acceler adopt novel chase mechan prove compar crw algorithm tcm algorithm result reduct time complex factor least sqrt log erd os renyi complet graph factor log log log torus network simul result show least constant factor improv messag complex tcm algorithm consid topolog robust crw tcm algorithm presenc node failur analyz show robust improv run multipl instanc algorithm parallel,"['Saber Salehkaleybar', 'S. Jamaloddin Golestani']","['cs.DC', 'stat.ML']",False,False,False,False,False,True
1209,2017-03-28T14:01:45Z,2017-03-26T13:29:25Z,http://arxiv.org/abs/1703.08816v1,http://arxiv.org/pdf/1703.08816v1,Uncertainty Quantification in the Classification of High Dimensional   Data,uncertainti quantif classif high dimension data,"Classification of high dimensional data finds wide-ranging applications. In many of these applications equipping the resulting classification with a measure of uncertainty may be as important as the classification itself. In this paper we introduce, develop algorithms for, and investigate the properties of, a variety of Bayesian models for the task of binary classification; via the posterior distribution on the classification labels, these methods automatically give measures of uncertainty. The methods are all based around the graph formulation of semi-supervised learning.   We provide a unified framework which brings together a variety of methods which have been introduced in different communities within the mathematical sciences. We study probit classification, generalize the level-set method for Bayesian inverse problems to the classification setting, and generalize the Ginzburg-Landau optimization-based classifier to a Bayesian setting; we also show that the probit and level set approaches are natural relaxations of the harmonic function approach.   We introduce efficient numerical methods, suited to large data-sets, for both MCMC-based sampling as well as gradient-based MAP estimation. Through numerical experiments we study classification accuracy and uncertainty quantification for our models; these experiments showcase a suite of datasets commonly used to evaluate graph-based semi-supervised learning algorithms.",classif high dimension data find wide rang applic mani applic equip result classif measur uncertainti may import classif paper introduc develop algorithm investig properti varieti bayesian model task binari classif via posterior distribut classif label method automat give measur uncertainti method base around graph formul semi supervis learn provid unifi framework bring togeth varieti method introduc differ communiti within mathemat scienc studi probit classif general level set method bayesian invers problem classif set general ginzburg landau optim base classifi bayesian set also show probit level set approach natur relax harmon function approach introduc effici numer method suit larg data set mcmc base sampl well gradient base map estim numer experi studi classif accuraci uncertainti quantif model experi showcas suit dataset common use evalu graph base semi supervis learn algorithm,"['Andrea L. Bertozzi', 'Xiyang Luo', 'Andrew M. Stuart', 'Konstantinos C. Zygalakis']","['cs.LG', 'stat.ML']",False,False,False,False,False,True
1211,2017-03-28T14:01:49Z,2017-03-25T20:06:10Z,http://arxiv.org/abs/1703.08737v1,http://arxiv.org/pdf/1703.08737v1,Learning to Predict: A Fast Re-constructive Method to Generate   Multimodal Embeddings,learn predict fast construct method generat multimod embed,"Integrating visual and linguistic information into a single multimodal representation is an unsolved problem with wide-reaching applications to both natural language processing and computer vision. In this paper, we present a simple method to build multimodal representations by learning a language-to-vision mapping and using its output to build multimodal embeddings. In this sense, our method provides a cognitively plausible way of building representations, consistent with the inherently re-constructive and associative nature of human memory. Using seven benchmark concept similarity tests we show that the mapped vectors not only implicitly encode multimodal information, but also outperform strong unimodal baselines and state-of-the-art multimodal methods, thus exhibiting more ""human-like"" judgments---particularly in zero-shot settings.",integr visual linguist inform singl multimod represent unsolv problem wide reach applic natur languag process comput vision paper present simpl method build multimod represent learn languag vision map use output build multimod embed sens method provid cognit plausibl way build represent consist inher construct associ natur human memori use seven benchmark concept similar test show map vector onli implicit encod multimod inform also outperform strong unimod baselin state art multimod method thus exhibit human like judgment particular zero shot set,"['Guillem Collell', 'Teddy Zhang', 'Marie-Francine Moens']",['stat.ML'],False,False,False,False,False,True
1212,2017-03-28T14:01:49Z,2017-03-25T18:45:55Z,http://arxiv.org/abs/1703.08729v1,http://arxiv.org/pdf/1703.08729v1,Solving SDPs for synchronization and MaxCut problems via the   Grothendieck inequality,solv sdps synchron maxcut problem via grothendieck inequ,"A number of statistical estimation problems can be addressed by semidefinite programs (SDP). While SDPs are solvable in polynomial time using interior point methods, in practice generic SDP solvers do not scale well to high-dimensional problems. In order to cope with this problem, Burer and Monteiro proposed a non-convex rank-constrained formulation, which has good performance in practice but is still poorly understood theoretically.   In this paper we study the rank-constrained version of SDPs arising in MaxCut and in synchronization problems. We establish a Grothendieck-type inequality that proves that all the local maxima and dangerous saddle points are within a small multiplicative gap from the global maximum. We use this structural information to prove that SDPs can be solved within a known accuracy, by applying the Riemannian trust-region method to this non-convex problem, while constraining the rank to be of order one. For the MaxCut problem, our inequality implies that any local maximizer of the rank-constrained SDP provides a $ (1 - 1/(k-1)) \times 0.878$ approximation of the MaxCut, when the rank is fixed to $k$.   We then apply our results to data matrices generated according to the Gaussian ${\mathbb Z}_2$ synchronization problem, and the two-groups stochastic block model with large bounded degree. We prove that the error achieved by local maximizers undergoes a phase transition at the same threshold as for information-theoretically optimal methods.",number statist estim problem address semidefinit program sdp sdps solvabl polynomi time use interior point method practic generic sdp solver scale well high dimension problem order cope problem burer monteiro propos non convex rank constrain formul good perform practic still poor understood theoret paper studi rank constrain version sdps aris maxcut synchron problem establish grothendieck type inequ prove local maxima danger saddl point within small multipl gap global maximum use structur inform prove sdps solv within known accuraci appli riemannian trust region method non convex problem constrain rank order one maxcut problem inequ impli ani local maxim rank constrain sdp provid time approxim maxcut rank fix appli result data matric generat accord gaussian mathbb synchron problem two group stochast block model larg bound degre prove error achiev local maxim undergo phase transit threshold inform theoret optim method,"['Song Mei', 'Theodor Misiakiewicz', 'Andrea Montanari', 'Roberto I. Oliveira']","['math.OC', 'stat.ML']",False,False,False,False,False,True
1215,2017-03-28T14:01:49Z,2017-03-24T22:54:17Z,http://arxiv.org/abs/1703.08619v1,http://arxiv.org/pdf/1703.08619v1,Binarsity: a penalization for one-hot encoded features,binars penal one hot encod featur,"This paper deals with the problem of large-scale linear supervised learning in settings where a large number of continuous features are available. We propose to combine the well-known trick of one-hot encoding of continuous features with a new penalization called binarsity. In each group of binary features coming from the one-hot encoding of a single raw continuous feature, this penalization uses total-variation regularization together with an extra linear constraint to avoid collinearity within groups. Non-asymptotic oracle inequalities for generalized linear models are proposed, and numerical experiments illustrate the good performances of our approach on several datasets. It is also noteworthy that our method has a numerical complexity comparable to standard $\ell_1$ penalization.",paper deal problem larg scale linear supervis learn set larg number continu featur avail propos combin well known trick one hot encod continu featur new penal call binars group binari featur come one hot encod singl raw continu featur penal use total variat regular togeth extra linear constraint avoid collinear within group non asymptot oracl inequ general linear model propos numer experi illustr good perform approach sever dataset also noteworthi method numer complex compar standard ell penal,"['Mokhtar Z. Alaya', 'Simon Bussy', 'Stéphane Gaïffas', 'Agathe Guilloux']",['stat.ML'],False,False,False,False,False,True
1217,2017-03-28T14:01:49Z,2017-03-24T17:17:45Z,http://arxiv.org/abs/1703.08520v1,http://arxiv.org/pdf/1703.08520v1,Rejection-free Ensemble MCMC with applications to Factorial Hidden   Markov Models,reject free ensembl mcmc applic factori hidden markov model,"Bayesian inference for complex models is challenging due to the need to explore high-dimensional spaces and multimodality and standard Monte Carlo samplers can have difficulties effectively exploring the posterior. We introduce a general purpose rejection-free ensemble Markov Chain Monte Carlo (MCMC) technique to improve on existing poorly mixing samplers. This is achieved by combining parallel tempering and an auxiliary variable move to exchange information between the chains. We demonstrate this ensemble MCMC scheme on Bayesian inference in Factorial Hidden Markov Models. This high-dimensional inference problem is difficult due to the exponentially sized latent variable space. Existing sampling approaches mix slowly and can get trapped in local modes. We show that the performance of these samplers is improved by our rejection-free ensemble technique and that the method is attractive and ""easy-to-use"" since no parameter tuning is required.",bayesian infer complex model challeng due need explor high dimension space multimod standard mont carlo sampler difficulti effect explor posterior introduc general purpos reject free ensembl markov chain mont carlo mcmc techniqu improv exist poor mix sampler achiev combin parallel temper auxiliari variabl move exchang inform chain demonstr ensembl mcmc scheme bayesian infer factori hidden markov model high dimension infer problem difficult due exponenti size latent variabl space exist sampl approach mix slowli get trap local mode show perform sampler improv reject free ensembl techniqu method attract easi use sinc paramet tune requir,"['Kaspar Märtens', 'Michalis K Titsias', 'Christopher Yau']","['stat.CO', 'stat.ME', 'stat.ML']",False,False,False,False,False,True
1221,2017-03-28T14:01:53Z,2017-03-24T02:31:23Z,http://arxiv.org/abs/1703.08267v1,http://arxiv.org/abs/1703.08267v1,A Nonconvex Splitting Method for Symmetric Nonnegative Matrix   Factorization: Convergence Analysis and Optimality,nonconvex split method symmetr nonneg matrix factor converg analysi optim,"Symmetric nonnegative matrix factorization (SymNMF) has important applications in data analytics problems such as document clustering, community detection and image segmentation. In this paper, we propose a novel nonconvex variable splitting method for solving SymNMF. The proposed algorithm is guaranteed to converge to the set of Karush-Kuhn-Tucker (KKT) points of the nonconvex SymNMF problem. Furthermore, it achieves a global sublinear convergence rate. We also show that the algorithm can be efficiently implemented in parallel. Further, sufficient conditions are provided which guarantee the global and local optimality of the obtained solutions. Extensive numerical results performed on both synthetic and real data sets suggest that the proposed algorithm converges quickly to a local minimum solution.",symmetr nonneg matrix factor symnmf import applic data analyt problem document cluster communiti detect imag segment paper propos novel nonconvex variabl split method solv symnmf propos algorithm guarante converg set karush kuhn tucker kkt point nonconvex symnmf problem furthermor achiev global sublinear converg rate also show algorithm effici implement parallel suffici condit provid guarante global local optim obtain solut extens numer result perform synthet real data set suggest propos algorithm converg quick local minimum solut,"['Songtao Lu', 'Mingyi Hong', 'Zhengdao Wang']","['math.OC', 'stat.ML']",False,False,True,False,False,True
1222,2017-03-28T14:01:53Z,2017-03-23T23:27:12Z,http://arxiv.org/abs/1703.08251v1,http://arxiv.org/pdf/1703.08251v1,The Dependence of Machine Learning on Electronic Medical Record Quality,depend machin learn electron medic record qualiti,"There is growing interest in applying machine learning methods to Electronic Medical Records (EMR). Across different institutions, however, EMR quality can vary widely. This work investigated the impact of this disparity on the performance of three advanced machine learning algorithms: logistic regression, multilayer perceptron, and recurrent neural network. The EMR disparity was emulated using different permutations of the EMR collected at Children's Hospital Los Angeles (CHLA) Pediatric Intensive Care Unit (PICU) and Cardiothoracic Intensive Care Unit (CTICU). The algorithms were trained using patients from the PICU to predict in-ICU mortality for patients in a held out set of PICU and CTICU patients. The disparate patient populations between the PICU and CTICU provide an estimate of generalization errors across different ICUs. We quantified and evaluated the generalization of these algorithms on varying EMR size, input types, and fidelity of data.",grow interest appli machin learn method electron medic record emr across differ institut howev emr qualiti vari wide work investig impact dispar perform three advanc machin learn algorithm logist regress multilay perceptron recurr neural network emr dispar emul use differ permut emr collect children hospit los angel chla pediatr intens care unit picu cardiothorac intens care unit cticu algorithm train use patient picu predict icu mortal patient held set picu cticu patient dispar patient popul picu cticu provid estim general error across differ icus quantifi evalu general algorithm vari emr size input type fidel data,"['Long Ho', 'David Ledbetter', 'Melissa Aczon', 'Randall Wetzel']",['stat.ML'],False,False,True,False,False,True
1224,2017-03-28T14:01:53Z,2017-03-23T14:29:29Z,http://arxiv.org/abs/1703.08085v1,http://arxiv.org/pdf/1703.08085v1,Unifying Framework for Crowd-sourcing via Graphon Estimation,unifi framework crowd sourc via graphon estim,"We consider the question of inferring true answers associated with tasks based on potentially noisy answers obtained through a micro-task crowd-sourcing platform such as Amazon Mechanical Turk. We propose a generic, non-parametric model for this setting: for a given task $i$, $1\leq i \leq T$, the response of worker $j$, $1\leq j\leq W$ for this task is correct with probability $F_{ij}$, where matrix $F = [F_{ij}]_{i\leq T, j\leq W}$ may satisfy one of a collection of regularity conditions including low rank, which can express the popular Dawid-Skene model; piecewise constant, which occurs when there is finitely many worker and task types; monotonic under permutation, when there is some ordering of worker skills and task difficulties; or Lipschitz with respect to an associated latent non-parametric function. This model, contains most, if not all, of the previously proposed models to the best of our knowledge.   We show that the question of estimating the true answers to tasks can be reduced to solving the Graphon estimation problem, for which there has been much recent progress. By leveraging these techniques, we provide a crowdsourcing inference algorithm along with theoretical bounds on the fraction of incorrectly estimated tasks. Subsequently, we have a solution for inferring the true answers for tasks using noisy answers collected from crowd-sourcing platform under a significantly larger class of models. Concretely, we establish that if the $(i,j)$th element of $F$, $F_{ij}$, is equal to a Lipschitz continuous function over latent features associated with the task $i$ and worker $j$ for all $i, j$, then all task answers can be inferred correctly with high probability by soliciting $\tilde{O}(\ln(T)^{3/2})$ responses per task even without any knowledge of the Lipschitz function, task and worker features, or the matrix $F$.",consid question infer true answer associ task base potenti noisi answer obtain micro task crowd sourc platform amazon mechan turk propos generic non parametr model set given task leq leq respons worker leq leq task correct probabl ij matrix ij leq leq may satisfi one collect regular condit includ low rank express popular dawid skene model piecewis constant occur finit mani worker task type monoton permut order worker skill task difficulti lipschitz respect associ latent non parametr function model contain previous propos model best knowledg show question estim true answer task reduc solv graphon estim problem much recent progress leverag techniqu provid crowdsourc infer algorithm along theoret bound fraction incorrect estim task subsequ solut infer true answer task use noisi answer collect crowd sourc platform signific larger class model concret establish th element ij equal lipschitz continu function latent featur associ task worker task answer infer correct high probabl solicit tild ln respons per task even without ani knowledg lipschitz function task worker featur matrix,"['Christina E. Lee', 'Devavrat Shah']",['stat.ML'],False,False,False,False,False,True
1225,2017-03-28T14:01:53Z,2017-03-23T13:41:47Z,http://arxiv.org/abs/1703.08065v1,http://arxiv.org/pdf/1703.08065v1,Robustness of Maximum Correntropy Estimation Against Large Outliers,robust maximum correntropi estim larg outlier,"The maximum correntropy criterion (MCC) has recently been successfully applied in robust regression, classification and adaptive filtering, where the correntropy is maximized instead of minimizing the well-known mean square error (MSE) to improve the robustness with respect to outliers (or impulsive noises). Considerable efforts have been devoted to develop various robust adaptive algorithms under MCC, but so far little insight has been gained as to how the optimal solution will be affected by outliers. In this work, we study this problem in the context of parameter estimation for a simple linear errors-in-variables (EIV) model where all variables are scalar. Under certain conditions, we derive an upper bound on the absolute value of the estimation error and show that the optimal solution under MCC can be very close to the true value of the unknown parameter even with outliers (whose values can be arbitrarily large) in both input and output variables. An illustrative example is presented to verify and clarify the theory.",maximum correntropi criterion mcc recent success appli robust regress classif adapt filter correntropi maxim instead minim well known mean squar error mse improv robust respect outlier impuls nois consider effort devot develop various robust adapt algorithm mcc far littl insight gain optim solut affect outlier work studi problem context paramet estim simpl linear error variabl eiv model variabl scalar certain condit deriv upper bound absolut valu estim error show optim solut mcc veri close true valu unknown paramet even outlier whose valu arbitrarili larg input output variabl illustr exampl present verifi clarifi theori,"['Badong Chen', 'Lei Xing', 'Haiquan Zhao', 'Bin Xu', 'Jose C. Principe']",['stat.ML'],False,False,False,False,False,True
1227,2017-03-28T14:01:53Z,2017-03-23T12:17:00Z,http://arxiv.org/abs/1703.08031v1,http://arxiv.org/pdf/1703.08031v1,Distribution of Gaussian Process Arc Lengths,distribut gaussian process arc length,"We present the first treatment of the arc length of the Gaussian Process (GP) with more than a single output dimension. GPs are commonly used for tasks such as trajectory modelling, where path length is a crucial quantity of interest. Previously, only paths in one dimension have been considered, with no theoretical consideration of higher dimensional problems. We fill the gap in the existing literature by deriving the moments of the arc length for a stationary GP with multiple output dimensions. A new method is used to derive the mean of a one-dimensional GP over a finite interval, by considering the distribution of the arc length integrand. This technique is used to derive an approximate distribution over the arc length of a vector valued GP in $\mathbb{R}^n$ by moment matching the distribution. Numerical simulations confirm our theoretical derivations.",present first treatment arc length gaussian process gp singl output dimens gps common use task trajectori model path length crucial quantiti interest previous onli path one dimens consid theoret consider higher dimension problem fill gap exist literatur deriv moment arc length stationari gp multipl output dimens new method use deriv mean one dimension gp finit interv consid distribut arc length integrand techniqu use deriv approxim distribut arc length vector valu gp mathbb moment match distribut numer simul confirm theoret deriv,"['Justin D. Bewsher', 'Alessandra Tosi', 'Michael A. Osborne', 'Stephen J. Roberts']",['stat.ML'],False,False,False,False,False,True
1228,2017-03-28T14:01:53Z,2017-03-23T07:13:28Z,http://arxiv.org/abs/1703.07948v1,http://arxiv.org/pdf/1703.07948v1,Fast Stochastic Variance Reduced Gradient Method with Momentum   Acceleration for Machine Learning,fast stochast varianc reduc gradient method momentum acceler machin learn,"Recently, research on accelerated stochastic gradient descent methods (e.g., SVRG) has made exciting progress (e.g., linear convergence for strongly convex problems). However, the best-known methods (e.g., Katyusha) requires at least two auxiliary variables and two momentum parameters. In this paper, we propose a fast stochastic variance reduction gradient (FSVRG) method, in which we design a novel update rule with the Nesterov's momentum and incorporate the technique of growing epoch size. FSVRG has only one auxiliary variable and one momentum weight, and thus it is much simpler and has much lower per-iteration complexity. We prove that FSVRG achieves linear convergence for strongly convex problems and the optimal $\mathcal{O}(1/T^2)$ convergence rate for non-strongly convex problems, where $T$ is the number of outer-iterations. We also extend FSVRG to directly solve the problems with non-smooth component functions, such as SVM. Finally, we empirically study the performance of FSVRG for solving various machine learning problems such as logistic regression, ridge regression, Lasso and SVM. Our results show that FSVRG outperforms the state-of-the-art stochastic methods, including Katyusha.",recent research acceler stochast gradient descent method svrg made excit progress linear converg strong convex problem howev best known method katyusha requir least two auxiliari variabl two momentum paramet paper propos fast stochast varianc reduct gradient fsvrg method design novel updat rule nesterov momentum incorpor techniqu grow epoch size fsvrg onli one auxiliari variabl one momentum weight thus much simpler much lower per iter complex prove fsvrg achiev linear converg strong convex problem optim mathcal converg rate non strong convex problem number outer iter also extend fsvrg direct solv problem non smooth compon function svm final empir studi perform fsvrg solv various machin learn problem logist regress ridg regress lasso svm result show fsvrg outperform state art stochast method includ katyusha,"['Fanhua Shang', 'Yuanyuan Liu', 'James Cheng', 'Jiacheng Zhuo']","['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']",False,False,False,False,False,True
1229,2017-03-28T14:01:53Z,2017-03-23T05:23:34Z,http://arxiv.org/abs/1703.07940v1,http://arxiv.org/pdf/1703.07940v1,Unsupervised Basis Function Adaptation for Reinforcement Learning,unsupervis basi function adapt reinforc learn,"When using reinforcement learning (RL) algorithms to evaluate a policy it is common, given a large state space, to introduce some form of approximation architecture for the value function (VF). The exact form of this architecture can have a significant effect on the accuracy of the VF estimate, however, and determining a suitable approximation architecture can often be a highly complex task. Consequently there is a large amount of interest in the potential for allowing RL algorithms to adaptively generate (i.e. to learn) approximation architectures.   We investigate a method of adapting approximation architectures which uses feedback regarding the frequency with which an agent has visited certain states to guide which areas of the state space to approximate with greater detail. We introduce an algorithm based upon this idea which adapts a state aggregation approximation architecture on-line.   Assuming $S$ states, we demonstrate theoretically that - provided the following relatively non-restrictive assumptions are satisfied: (a) the number of cells $X$ in the state aggregation architecture is of order $\sqrt{S}\ln{S}\log_2{S}$ or greater, (b) the policy and transition function are close to deterministic, and (c) the prior for the transition function is uniformly distributed - our algorithm can guarantee, assuming we use an appropriate scoring function to measure VF error, error which is arbitrarily close to zero as $S$ becomes large. It is able to do this despite having only $O(X\log_2{S})$ space complexity (and negligible time complexity). We conclude by generating a set of empirical results which support the theoretical results.",use reinforc learn rl algorithm evalu polici common given larg state space introduc form approxim architectur valu function vf exact form architectur signific effect accuraci vf estim howev determin suitabl approxim architectur often high complex task consequ larg amount interest potenti allow rl algorithm adapt generat learn approxim architectur investig method adapt approxim architectur use feedback regard frequenc agent visit certain state guid area state space approxim greater detail introduc algorithm base upon idea adapt state aggreg approxim architectur line assum state demonstr theoret provid follow relat non restrict assumpt satisfi number cell state aggreg architectur order sqrt ln log greater polici transit function close determinist prior transit function uniform distribut algorithm guarante assum use appropri score function measur vf error error arbitrarili close zero becom larg abl despit onli log space complex neglig time complex conclud generat set empir result support theoret result,"['Edward Barker', 'Charl Ras']","['cs.LG', 'cs.AI', 'stat.ML']",False,False,False,False,False,True
1231,2017-03-28T14:01:57Z,2017-03-23T03:17:14Z,http://arxiv.org/abs/1703.07915v1,http://arxiv.org/pdf/1703.07915v1,Perspective: Energy Landscapes for Machine Learning,perspect energi landscap machin learn,"Machine learning techniques are being increasingly used as flexible non-linear fitting and prediction tools in the physical sciences. Fitting functions that exhibit multiple solutions as local minima can be analysed in terms of the corresponding machine learning landscape. Methods to explore and visualise molecular potential energy landscapes can be applied to these machine learning landscapes to gain new insight into the solution space involved in training and the nature of the corresponding predictions. In particular, we can define quantities analogous to molecular structure, thermodynamics, and kinetics, and relate these emergent properties to the structure of the underlying landscape. This Perspective aims to describe these analogies with examples from recent applications, and suggest avenues for new interdisciplinary research.",machin learn techniqu increas use flexibl non linear fit predict tool physic scienc fit function exhibit multipl solut local minima analys term correspond machin learn landscap method explor visualis molecular potenti energi landscap appli machin learn landscap gain new insight solut space involv train natur correspond predict particular defin quantiti analog molecular structur thermodynam kinet relat emerg properti structur landscap perspect aim describ analog exampl recent applic suggest avenu new interdisciplinari research,"['Andrew J. Ballard', 'Ritankar Das', 'Stefano Martiniani', 'Dhagash Mehta', 'Levent Sagun', 'Jacob D. Stevenson', 'David J. Wales']","['stat.ML', 'cond-mat.dis-nn', 'cs.LG']",False,False,True,False,False,True
1232,2017-03-28T14:01:57Z,2017-03-23T02:40:36Z,http://arxiv.org/abs/1703.07909v1,http://arxiv.org/pdf/1703.07909v1,Data Driven Exploratory Attacks on Black Box Classifiers in Adversarial   Domains,data driven exploratori attack black box classifi adversari domain,"While modern day web applications aim to create impact at the civilization level, they have become vulnerable to adversarial activity, where the next cyber-attack can take any shape and can originate from anywhere. The increasing scale and sophistication of attacks, has prompted the need for a data driven solution, with machine learning forming the core of many cybersecurity systems. Machine learning was not designed with security in mind, and the essential assumption of stationarity, requiring that the training and testing data follow similar distributions, is violated in an adversarial domain. In this paper, an adversary's view point of a classification based system, is presented. Based on a formal adversarial model, the Seed-Explore-Exploit framework is presented, for simulating the generation of data driven and reverse engineering attacks on classifiers. Experimental evaluation, on 10 real world datasets and using the Google Cloud Prediction Platform, demonstrates the innate vulnerability of classifiers and the ease with which evasion can be carried out, without any explicit information about the classifier type, the training data or the application domain. The proposed framework, algorithms and empirical evaluation, serve as a white hat analysis of the vulnerabilities, and aim to foster the development of secure machine learning frameworks.",modern day web applic aim creat impact civil level becom vulner adversari activ next cyber attack take ani shape origin anywher increas scale sophist attack prompt need data driven solut machin learn form core mani cybersecur system machin learn design secur mind essenti assumpt stationar requir train test data follow similar distribut violat adversari domain paper adversari view point classif base system present base formal adversari model seed explor exploit framework present simul generat data driven revers engin attack classifi experiment evalu real world dataset use googl cloud predict platform demonstr innat vulner classifi eas evas carri without ani explicit inform classifi type train data applic domain propos framework algorithm empir evalu serv white hat analysi vulner aim foster develop secur machin learn framework,"['Tegjyot Singh Sethi', 'Mehmed Kantardzic']","['stat.ML', 'cs.CR', 'cs.LG']",False,False,True,False,False,True
1233,2017-03-28T14:01:57Z,2017-03-23T01:30:17Z,http://arxiv.org/abs/1703.07904v1,http://arxiv.org/pdf/1703.07904v1,Cross-Validation with Confidence,cross valid confid,"Cross-validation is one of the most popular model selection methods in statistics and machine learning. Despite its wide applicability, traditional cross-validation methods tend to select overfitting models, unless the ratio between the training and testing sample sizes is much smaller than conventional choices. We argue that such an overfitting tendency of cross-validation is due to the ignorance of the uncertainty in the testing sample. Starting from this observation, we develop a new, statistically principled inference tool based on cross-validation that takes into account the uncertainty in the testing sample. This new method outputs a small set of highly competitive candidate models containing the best one with guaranteed probability. As a consequence, our method can achieve consistent variable selection in a classical linear regression setting, for which existing cross-validation methods require unconventional split ratios. We demonstrate the performance of the proposed method in several simulated and real data examples.",cross valid one popular model select method statist machin learn despit wide applic tradit cross valid method tend select overfit model unless ratio train test sampl size much smaller convent choic argu overfit tendenc cross valid due ignor uncertainti test sampl start observ develop new statist principl infer tool base cross valid take account uncertainti test sampl new method output small set high competit candid model contain best one guarante probabl consequ method achiev consist variabl select classic linear regress set exist cross valid method requir unconvent split ratio demonstr perform propos method sever simul real data exampl,['Jing Lei'],"['stat.ME', 'stat.ML']",False,False,True,False,False,True
1234,2017-03-28T14:01:57Z,2017-03-22T23:35:51Z,http://arxiv.org/abs/1703.07886v1,http://arxiv.org/pdf/1703.07886v1,Robust Kronecker-Decomposable Component Analysis for Low Rank Modeling,robust kroneck decompos compon analysi low rank model,"Dictionary learning and component analysis are part of one of the most well-studied and active research fields, at the intersection of signal and image processing, computer vision, and statistical machine learning. In dictionary learning, the current methods of choice are arguably K-SVD and its variants, which learn a dictionary (i.e., a decomposition) for sparse coding via Singular Value Decomposition. In robust component analysis, leading methods derive from Principal Component Pursuit (PCP), which recovers a low-rank matrix from sparse corruptions of unknown magnitude and support. While K-SVD is sensitive to the presence of noise and outliers in the training set, PCP does not provide a dictionary that respects the structure of the data (e.g., images), and requires expensive SVD computations when solved by convex relaxation. In this paper, we introduce a new robust decomposition of images by combining ideas from sparse dictionary learning and PCP. We propose a novel Kronecker-decomposable component analysis which is robust to gross corruption, can be used for low-rank modeling, and leverages separability to solve significantly smaller problems. We design an efficient learning algorithm by drawing links with a restricted form of tensor factorization. The effectiveness of the proposed approach is demonstrated on real-world applications, namely background subtraction and image denoising, by performing a thorough comparison with the current state of the art.",dictionari learn compon analysi part one well studi activ research field intersect signal imag process comput vision statist machin learn dictionari learn current method choic arguabl svd variant learn dictionari decomposit spars code via singular valu decomposit robust compon analysi lead method deriv princip compon pursuit pcp recov low rank matrix spars corrupt unknown magnitud support svd sensit presenc nois outlier train set pcp doe provid dictionari respect structur data imag requir expens svd comput solv convex relax paper introduc new robust decomposit imag combin idea spars dictionari learn pcp propos novel kroneck decompos compon analysi robust gross corrupt use low rank model leverag separ solv signific smaller problem design effici learn algorithm draw link restrict form tensor factor effect propos approach demonstr real world applic name background subtract imag denois perform thorough comparison current state art,"['Mehdi Bahri', 'Yannis Panagakis', 'Stefanos Zafeiriou']","['stat.ML', 'cs.CV']",False,False,False,False,False,True
1237,2017-03-28T14:01:57Z,2017-03-22T17:27:57Z,http://arxiv.org/abs/1703.07758v1,http://arxiv.org/pdf/1703.07758v1,S-Concave Distributions: Towards Broader Distributions for   Noise-Tolerant and Sample-Efficient Learning Algorithms,concav distribut toward broader distribut nois toler sampl effici learn algorithm,"We provide new results concerning noise-tolerant and sample-efficient learning algorithms under $s$-concave distributions over $\mathbb{R}^n$ for $-\frac{1}{2n+3}\le s\le 0$. The new class of $s$-concave distributions is a broad and natural generalization of log-concavity, and includes many important additional distributions, e.g., the Pareto distribution and $t$-distribution. This class has been studied in the context of efficient sampling, integration, and optimization, but much remains unknown concerning the geometry of this class of distributions and their applications in the context of learning.   The challenge is that unlike the commonly used distributions in learning (uniform or more generally log-concave distributions), this broader class is not closed under the marginalization operator and many such distributions are fat-tailed. In this work, we introduce new convex geometry tools to study the properties of s-concave distributions and use these properties to provide bounds on quantities of interest to learning including the probability of disagreement between two halfspaces, disagreement outside a band, and disagreement coefficient. We use these results to significantly generalize prior results for margin-based active learning, disagreement-based active learning, and passively learning of intersections of halfspaces.   Our analysis of geometric properties of s-concave distributions might be of independent interest to optimization more broadly.",provid new result concern nois toler sampl effici learn algorithm concav distribut mathbb frac le le new class concav distribut broad natur general log concav includ mani import addit distribut pareto distribut distribut class studi context effici sampl integr optim much remain unknown concern geometri class distribut applic context learn challeng unlik common use distribut learn uniform general log concav distribut broader class close margin oper mani distribut fat tail work introduc new convex geometri tool studi properti concav distribut use properti provid bound quantiti interest learn includ probabl disagr two halfspac disagr outsid band disagr coeffici use result signific general prior result margin base activ learn disagr base activ learn passiv learn intersect halfspac analysi geometr properti concav distribut might independ interest optim broad,"['Maria-Florina Balcan', 'Hongyang Zhang']","['stat.ML', 'cs.AI', 'cs.LG']",False,False,False,False,False,True
1239,2017-03-28T14:01:57Z,2017-03-22T15:34:23Z,http://arxiv.org/abs/1703.07710v1,http://arxiv.org/pdf/1703.07710v1,UBEV - A More Practical Algorithm for Episodic RL with Near-Optimal PAC   and Regret Guarantees,ubev practic algorithm episod rl near optim pac regret guarante,"We present UBEV, a simple and efficient reinforcement learning algorithm for fixed-horizon episodic Markov decision processes. The main contribution is a proof that UBEV enjoys a sample-complexity bound that holds for all accuracy levels simultaneously with high probability, and matches the lower bound except for logarithmic terms and one factor of the horizon. A consequence of the fact that our sample-complexity bound holds for all accuracy levels is that the new algorithm achieves a sub-linear regret of O(sqrt(SAT)), which is the first time the dependence on the size of the state space has provably appeared inside the square root. A brief empirical evaluation shows that UBEV is practically superior to existing algorithms with known sample-complexity guarantees.",present ubev simpl effici reinforc learn algorithm fix horizon episod markov decis process main contribut proof ubev enjoy sampl complex bound hold accuraci level simultan high probabl match lower bound except logarithm term one factor horizon consequ fact sampl complex bound hold accuraci level new algorithm achiev sub linear regret sqrt sat first time depend size state space provabl appear insid squar root brief empir evalu show ubev practic superior exist algorithm known sampl complex guarante,"['Christoph Dann', 'Tor Lattimore', 'Emma Brunskill']","['cs.LG', 'cs.AI', 'stat.ML']",False,False,False,False,False,True
1240,2017-03-28T14:02:02Z,2017-03-22T15:11:55Z,http://arxiv.org/abs/1703.07698v1,http://arxiv.org/pdf/1703.07698v1,Characterization of Deterministic and Probabilistic Sampling Patterns   for Finite Completability of Low Tensor-Train Rank Tensor,character determinist probabilist sampl pattern finit complet low tensor train rank tensor,"In this paper, we analyze the fundamental conditions for low-rank tensor completion given the separation or tensor-train (TT) rank, i.e., ranks of unfoldings. We exploit the algebraic structure of the TT decomposition to obtain the deterministic necessary and sufficient conditions on the locations of the samples to ensure finite completability. Specifically, we propose an algebraic geometric analysis on the TT manifold that can incorporate the whole rank vector simultaneously in contrast to the existing approach based on the Grassmannian manifold that can only incorporate one rank component. Our proposed technique characterizes the algebraic independence of a set of polynomials defined based on the sampling pattern and the TT decomposition, which is instrumental to obtaining the deterministic condition on the sampling pattern for finite completability. In addition, based on the proposed analysis, assuming that the entries of the tensor are sampled independently with probability $p$, we derive a lower bound on the sampling probability $p$, or equivalently, the number of sampled entries that ensures finite completability with high probability. Moreover, we also provide the deterministic and probabilistic conditions for unique completability.",paper analyz fundament condit low rank tensor complet given separ tensor train tt rank rank unfold exploit algebra structur tt decomposit obtain determinist necessari suffici condit locat sampl ensur finit complet specif propos algebra geometr analysi tt manifold incorpor whole rank vector simultan contrast exist approach base grassmannian manifold onli incorpor one rank compon propos techniqu character algebra independ set polynomi defin base sampl pattern tt decomposit instrument obtain determinist condit sampl pattern finit complet addit base propos analysi assum entri tensor sampl independ probabl deriv lower bound sampl probabl equival number sampl entri ensur finit complet high probabl moreov also provid determinist probabilist condit uniqu complet,"['Morteza Ashraphijuo', 'Xiaodong Wang']","['cs.LG', 'cs.IT', 'math.AG', 'math.IT', 'stat.ML']",False,False,False,False,False,True
1242,2017-03-28T14:02:02Z,2017-03-22T11:53:53Z,http://arxiv.org/abs/1703.07608v1,http://arxiv.org/pdf/1703.07608v1,Deep Exploration via Randomized Value Functions,deep explor via random valu function,We study the use of randomized value functions to guide deep exploration in reinforcement learning. This offers an elegant means for synthesizing statistically and computationally efficient exploration with common practical approaches to value function learning. We present several reinforcement learning algorithms that leverage randomized value functions and demonstrate their efficacy through computational studies. We also prove a regret bound that establishes statistical efficiency with a tabular representation.,studi use random valu function guid deep explor reinforc learn offer eleg mean synthes statist comput effici explor common practic approach valu function learn present sever reinforc learn algorithm leverag random valu function demonstr efficaci comput studi also prove regret bound establish statist effici tabular represent,"['Ian Osband', 'Daniel Russo', 'Zheng Wen', 'Benjamin Van Roy']","['stat.ML', 'cs.AI', 'cs.LG']",False,False,False,False,False,True
1246,2017-03-28T14:02:02Z,2017-03-21T23:56:51Z,http://arxiv.org/abs/1703.07473v1,http://arxiv.org/pdf/1703.07473v1,Episode-Based Active Learning with Bayesian Neural Networks,episod base activ learn bayesian neural network,"We investigate different strategies for active learning with Bayesian deep neural networks. We focus our analysis on scenarios where new, unlabeled data is obtained episodically, such as commonly encountered in mobile robotics applications. An evaluation of different strategies for acquisition, updating, and final training on the CIFAR-10 dataset shows that incremental network updates with final training on the accumulated acquisition set are essential for best performance, while limiting the amount of required human labeling labor.",investig differ strategi activ learn bayesian deep neural network focus analysi scenario new unlabel data obtain episod common encount mobil robot applic evalu differ strategi acquisit updat final train cifar dataset show increment network updat final train accumul acquisit set essenti best perform limit amount requir human label labor,"['Feras Dayoub', 'Niko Sünderhauf', 'Peter Corke']","['cs.CV', 'cs.LG', 'stat.ML']",False,False,False,False,False,True
1247,2017-03-28T14:02:02Z,2017-03-21T18:05:31Z,http://arxiv.org/abs/1703.07370v1,http://arxiv.org/pdf/1703.07370v1,"REBAR: Low-variance, unbiased gradient estimates for discrete latent   variable models",rebar low varianc unbias gradient estim discret latent variabl model,"Learning in models with discrete latent variables is challenging due to high variance gradient estimators. Generally, approaches have relied on control variates to reduce the variance of the REINFORCE estimator. Recent work (Jang et al. 2016, Maddison et al. 2016) has taken a different approach, introducing a continuous relaxation of discrete variables to produce low-variance, but biased, gradient estimates. In this work, we combine the two approaches through a novel control variate that produces low-variance, unbiased gradient estimates. We present encouraging preliminary results on a toy problem and on learning sigmoid belief networks.",learn model discret latent variabl challeng due high varianc gradient estim general approach reli control variat reduc varianc reinforc estim recent work jang et al maddison et al taken differ approach introduc continu relax discret variabl produc low varianc bias gradient estim work combin two approach novel control variat produc low varianc unbias gradient estim present encourag preliminari result toy problem learn sigmoid belief network,"['George Tucker', 'Andriy Mnih', 'Chris J. Maddison', 'Jascha Sohl-Dickstein']","['cs.LG', 'stat.ML']",False,False,False,False,False,True
1249,2017-03-28T14:02:02Z,2017-03-21T17:58:03Z,http://arxiv.org/abs/1703.07345v1,http://arxiv.org/pdf/1703.07345v1,On The Projection Operator to A Three-view Cardinality Constrained Set,project oper three view cardin constrain set,"The cardinality constraint is an intrinsic way to restrict the solution structure in many domains, for example, sparse learning, feature selection, and compressed sensing. To solve a cardinality constrained problem, the key challenge is to solve the projection onto the cardinality constraint set, which is NP-hard in general when there exist multiple overlapped cardiaiality constraints. In this paper, we consider the scenario where overlapped cardinality constraints satisfy a Three-view Cardinality Structure (TVCS), which reflects the natural restriction in many applications, such as identification of gene regulatory networks and task-worker assignment problem. We cast the projection onto the TVCS set into a linear programming, and prove that its solution can be obtained by finding an integer solution to such linear programming. We further prove that such integer solution can be found with the complexity proportional to the problem scale. We finally use synthetic experiments and two interesting applications in bioinformatics and crowdsourcing to validate the proposed TVCS model and method.",cardin constraint intrins way restrict solut structur mani domain exampl spars learn featur select compress sens solv cardin constrain problem key challeng solv project onto cardin constraint set np hard general exist multipl overlap cardiaial constraint paper consid scenario overlap cardin constraint satisfi three view cardin structur tvcs reflect natur restrict mani applic identif gene regulatori network task worker assign problem cast project onto tvcs set linear program prove solut obtain find integ solut linear program prove integ solut found complex proport problem scale final use synthet experi two interest applic bioinformat crowdsourc valid propos tvcs model method,"['Haichuan Yang', 'Shupeng Gui', 'Chuyang Ke', 'Daniel Stefankovic', 'Ryohei Fujimaki', 'Ji Liu']","['cs.LG', 'stat.ML']",False,False,False,False,False,True
1250,2017-03-28T14:02:06Z,2017-03-21T16:39:28Z,http://arxiv.org/abs/1703.07305v1,http://arxiv.org/abs/1703.07305v1,Targeting Bayes factors with direct-path non-equilibrium thermodynamic   integration,target bay factor direct path non equilibrium thermodynam integr,"Thermodynamic integration (TI) for computing marginal likelihoods is based on an inverse annealing path from the prior to the posterior distribution. In many cases, the resulting estimator suffers from high variability, which particularly stems from the prior regime. When comparing complex models with differences in a comparatively small number of parameters, intrinsic errors from sampling fluctuations may outweigh the differences in the log marginal likelihood estimates. In the present article, we propose a thermodynamic integration scheme that directly targets the log Bayes factor. The method is based on a modified annealing path between the posterior distributions of the two models compared, which systematically avoids the high variance prior regime. We combine this scheme with the concept of non-equilibrium TI to minimise discretisation errors from numerical integration. Results obtained on Bayesian regression models applied to standard benchmark data, and a complex hierarchical model applied to biopathway inference, demonstrate a significant reduction in estimator variance over state-of-the-art TI methods.",thermodynam integr ti comput margin likelihood base invers anneal path prior posterior distribut mani case result estim suffer high variabl particular stem prior regim compar complex model differ compar small number paramet intrins error sampl fluctuat may outweigh differ log margin likelihood estim present articl propos thermodynam integr scheme direct target log bay factor method base modifi anneal path posterior distribut two model compar systemat avoid high varianc prior regim combin scheme concept non equilibrium ti minimis discretis error numer integr result obtain bayesian regress model appli standard benchmark data complex hierarch model appli biopathway infer demonstr signific reduct estim varianc state art ti method,"['Marco Grzegorczyk', 'Andrej Aderhold', 'Dirk Husmeier']","['stat.ME', 'stat.ML']",False,False,False,False,False,True
1251,2017-03-28T14:02:06Z,2017-03-21T15:42:38Z,http://arxiv.org/abs/1703.07285v1,http://arxiv.org/pdf/1703.07285v1,From safe screening rules to working sets for faster Lasso-type solvers,safe screen rule work set faster lasso type solver,"Convex sparsity-promoting regularizations are ubiquitous in modern statistical learning. By construction, they yield solutions with few non-zero coefficients, which correspond to saturated constraints in the dual optimization formulation. Working set (WS) strategies are generic optimization techniques that consist in solving simpler problems that only consider a subset of constraints, whose indices form the WS. Working set methods therefore involve two nested iterations: the outer loop corresponds to the definition of the WS and the inner loop calls a solver for the subproblems. For the Lasso estimator a WS is a set of features, while for a Group Lasso it refers to a set of groups. In practice, WS are generally small in this context so the associated feature Gram matrix can fit in memory. Here we show that the Gauss-Southwell rule (a greedy strategy for block coordinate descent techniques) leads to fast solvers in this case. Combined with a working set strategy based on an aggressive use of so-called Gap Safe screening rules, we propose a solver achieving state-of-the-art performance on sparse learning problems. Results are presented on Lasso and multi-task Lasso estimators.",convex sparsiti promot regular ubiquit modern statist learn construct yield solut non zero coeffici correspond satur constraint dual optim formul work set ws strategi generic optim techniqu consist solv simpler problem onli consid subset constraint whose indic form ws work set method therefor involv two nest iter outer loop correspond definit ws inner loop call solver subproblem lasso estim ws set featur group lasso refer set group practic ws general small context associ featur gram matrix fit memori show gauss southwel rule greedi strategi block coordin descent techniqu lead fast solver case combin work set strategi base aggress use call gap safe screen rule propos solver achiev state art perform spars learn problem result present lasso multi task lasso estim,"['Mathurin Massias', 'Alexandre Gramfort', 'Joseph Salmon']","['stat.ML', 'cs.LG', 'math.OC', 'stat.CO']",False,False,False,False,False,True
1254,2017-03-28T14:02:06Z,2017-03-21T12:33:19Z,http://arxiv.org/abs/1703.07169v1,http://arxiv.org/pdf/1703.07169v1,A Deterministic Global Optimization Method for Variational Inference,determinist global optim method variat infer,"Variational inference methods for latent variable statistical models have gained popularity because they are relatively fast, can handle large data sets, and have deterministic convergence guarantees. However, in practice it is unclear whether the fixed point identified by the variational inference algorithm is a local or a global optimum. Here, we propose a method for constructing iterative optimization algorithms for variational inference problems that are guaranteed to converge to the $\epsilon$-global variational lower bound on the log-likelihood. We derive inference algorithms for two variational approximations to a standard Bayesian Gaussian mixture model (BGMM). We present a minimal data set for empirically testing convergence and show that a variational inference algorithm frequently converges to a local optimum while our algorithm always converges to the globally optimal variational lower bound. We characterize the loss incurred by choosing a non-optimal variational approximation distribution suggesting that selection of the approximating variational distribution deserves as much attention as the selection of the original statistical model for a given data set.",variat infer method latent variabl statist model gain popular becaus relat fast handl larg data set determinist converg guarante howev practic unclear whether fix point identifi variat infer algorithm local global optimum propos method construct iter optim algorithm variat infer problem guarante converg epsilon global variat lower bound log likelihood deriv infer algorithm two variat approxim standard bayesian gaussian mixtur model bgmm present minim data set empir test converg show variat infer algorithm frequent converg local optimum algorithm alway converg global optim variat lower bound character loss incur choos non optim variat approxim distribut suggest select approxim variat distribut deserv much attent select origin statist model given data set,"['Hachem Saddiki', 'Andrew C. Trapp', 'Patrick Flaherty']","['stat.ME', 'stat.ML']",False,False,False,False,False,True
1256,2017-03-28T14:02:06Z,2017-03-21T05:08:33Z,http://arxiv.org/abs/1703.07056v1,http://arxiv.org/pdf/1703.07056v1,Stochastic Primal Dual Coordinate Method with Non-Uniform Sampling Based   on Optimality Violations,stochast primal dual coordin method non uniform sampl base optim violat,"We study primal-dual type stochastic optimization algorithms with non-uniform sampling. Our main theoretical contribution in this paper is to present a convergence analysis of Stochastic Primal Dual Coordinate (SPDC) Method with arbitrary sampling. Based on this theoretical framework, we propose Optimality Violation-based Sampling SPDC (ovsSPDC), a non-uniform sampling method based on Optimality Violation. We also propose two efficient heuristic variants of ovsSPDC called ovsSDPC+ and ovsSDPC++. Through intensive numerical experiments, we demonstrate that the proposed method and its variants are faster than other state-of-the-art primal-dual type stochastic optimization methods.",studi primal dual type stochast optim algorithm non uniform sampl main theoret contribut paper present converg analysi stochast primal dual coordin spdc method arbitrari sampl base theoret framework propos optim violat base sampl spdc ovsspdc non uniform sampl method base optim violat also propos two effici heurist variant ovsspdc call ovssdpc ovssdpc intens numer experi demonstr propos method variant faster state art primal dual type stochast optim method,"['Atsushi Shibagaki', 'Ichiro Takeuchi']","['stat.ML', 'cs.LG', 'math.OC']",False,False,True,False,False,True
1260,2017-03-28T14:02:09Z,2017-03-20T22:32:36Z,http://arxiv.org/abs/1703.06990v1,http://arxiv.org/pdf/1703.06990v1,Metalearning for Feature Selection,metalearn featur select,"A general formulation of optimization problems in which various candidate solutions may use different feature-sets is presented, encompassing supervised classification, automated program learning and other cases. A novel characterization of the concept of a ""good quality feature"" for such an optimization problem is provided; and a proposal regarding the integration of quality based feature selection into metalearning is suggested, wherein the quality of a feature for a problem is estimated using knowledge about related features in the context of related problems. Results are presented regarding extensive testing of this ""feature metalearning"" approach on supervised text classification problems; it is demonstrated that, in this context, feature metalearning can provide significant and sometimes dramatic speedup over standard feature selection heuristics.",general formul optim problem various candid solut may use differ featur set present encompass supervis classif autom program learn case novel character concept good qualiti featur optim problem provid propos regard integr qualiti base featur select metalearn suggest wherein qualiti featur problem estim use knowledg relat featur context relat problem result present regard extens test featur metalearn approach supervis text classif problem demonstr context featur metalearn provid signific sometim dramat speedup standard featur select heurist,"['Ben Goertzel', 'Nil Geisweiller', 'Chris Poulin']","['cs.LG', 'stat.ML']",False,False,False,False,False,True
1261,2017-03-28T14:02:09Z,2017-03-20T21:29:18Z,http://arxiv.org/abs/1703.06975v1,http://arxiv.org/pdf/1703.06975v1,Learning to Generate Samples from Noise through Infusion Training,learn generat sampl nois infus train,"In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net",work investig novel train procedur learn generat model transit oper markov chain appli repeat unstructur random nois sampl denois sampl match target distribut train set novel train procedur learn progress denois oper involv sampl slight differ chain model chain use generat absenc denois target train chain infus inform train target exampl would like chain reach high probabl thus learn transit oper abl produc qualiti vari sampl small number step experi show competit result compar sampl generat basic generat adversari net,"['Florian Bordes', 'Sina Honari', 'Pascal Vincent']","['stat.ML', 'cs.LG']",False,False,False,False,False,True
1262,2017-03-28T14:02:09Z,2017-03-20T19:26:00Z,http://arxiv.org/abs/1703.06934v1,http://arxiv.org/pdf/1703.06934v1,Ensemble representation learning: an analysis of fitness and survival   for wrapper-based genetic programming methods,ensembl represent learn analysi fit surviv wrapper base genet program method,"Recently we proposed a general, ensemble-based feature engineering wrapper (FEW) that was paired with a number of machine learning methods to solve regression problems. Here, we adapt FEW for supervised classification and perform a thorough analysis of fitness and survival methods within this framework. Our tests demonstrate that two fitness metrics, one introduced as an adaptation of the silhouette score, outperform the more commonly used Fisher criterion. We analyze survival methods and demonstrate that $\epsilon$-lexicase survival works best across our test problems, followed by random survival which outperforms both tournament and deterministic crowding. We conduct hyper-parameter optimization for several classification methods using a large set of problems to benchmark the ability of FEW to improve data representations. The results show that FEW can improve the best classifier performance on several problems. We show that FEW generates readable and meaningful features for a biomedical problem with different ML pairings.",recent propos general ensembl base featur engin wrapper pair number machin learn method solv regress problem adapt supervis classif perform thorough analysi fit surviv method within framework test demonstr two fit metric one introduc adapt silhouett score outperform common use fisher criterion analyz surviv method demonstr epsilon lexicas surviv work best across test problem follow random surviv outperform tournament determinist crowd conduct hyper paramet optim sever classif method use larg set problem benchmark abil improv data represent result show improv best classifi perform sever problem show generat readabl meaning featur biomed problem differ ml pair,"['William La Cava', 'Jason H. Moore']","['cs.NE', 'cs.LG', 'stat.ML']",False,False,False,False,False,True
1263,2017-03-28T14:02:09Z,2017-03-22T07:44:55Z,http://arxiv.org/abs/1703.06891v2,http://arxiv.org/pdf/1703.06891v2,Dance Dance Convolution,danc danc convolut,"Dance Dance Revolution (DDR) is a popular rhythm-based video game. Players perform steps on a dance platform in synchronization with music as directed by on-screen step charts. While many step charts are available in standardized packs, users may grow tired of existing charts, or wish to dance to a song for which no chart exists. We introduce the task of learning to choreograph. Given a raw audio track, the goal is to produce a new step chart. This task decomposes naturally into two subtasks: deciding when to place steps and deciding which steps to select. For the step placement task, we combine recurrent and convolutional neural networks to ingest spectrograms of low-level audio features to predict steps, conditioned on chart difficulty. For step selection, we present a conditional LSTM generative model that substantially outperforms n-gram and fixed-window approaches.",danc danc revolut ddr popular rhythm base video game player perform step danc platform synchron music direct screen step chart mani step chart avail standard pack user may grow tire exist chart wish danc song chart exist introduc task learn choreograph given raw audio track goal produc new step chart task decompos natur two subtask decid place step decid step select step placement task combin recurr convolut neural network ingest spectrogram low level audio featur predict step condit chart difficulti step select present condit lstm generat model substanti outperform gram fix window approach,"['Chris Donahue', 'Zachary C. Lipton', 'Julian McAuley']","['cs.LG', 'cs.MM', 'cs.NE', 'cs.SD', 'stat.ML']",False,False,False,False,False,True
1265,2017-03-28T14:02:09Z,2017-03-20T17:18:57Z,http://arxiv.org/abs/1703.06856v1,http://arxiv.org/pdf/1703.06856v1,Counterfactual Fairness,counterfactu fair,"Machine learning has matured to the point to where it is now being considered to automate decisions in loan lending, employee hiring, and predictive policing. In many of these scenarios however, previous decisions have been made that are unfairly biased against certain subpopulations (e.g., those of a particular race, gender, or sexual orientation). Because this past data is often biased, machine learning predictors must account for this to avoid perpetuating discriminatory practices (or incidentally making new ones). In this paper, we develop a framework for modeling fairness in any dataset using tools from counterfactual inference. We propose a definition called counterfactual fairness that captures the intuition that a decision is fair towards an individual if it gives the same predictions in (a) the observed world and (b) a world where the individual had always belonged to a different demographic group, other background causes of the outcome being equal. We demonstrate our framework on two real-world problems: fair prediction of law school success, and fair modeling of an individual's criminality in policing data.",machin learn matur point consid autom decis loan lend employe hire predict polic mani scenario howev previous decis made unfair bias certain subpopul particular race gender sexual orient becaus past data often bias machin learn predictor must account avoid perpetu discriminatori practic incident make new one paper develop framework model fair ani dataset use tool counterfactu infer propos definit call counterfactu fair captur intuit decis fair toward individu give predict observ world world individu alway belong differ demograph group background caus outcom equal demonstr framework two real world problem fair predict law school success fair model individu crimin polic data,"['Matt J. Kusner', 'Joshua R. Loftus', 'Chris Russell', 'Ricardo Silva']","['stat.ML', 'cs.CY', 'cs.LG']",False,False,False,False,False,True
1266,2017-03-28T14:02:09Z,2017-03-20T15:43:10Z,http://arxiv.org/abs/1703.06807v1,http://arxiv.org/pdf/1703.06807v1,Variance Reduced Stochastic Gradient Descent with Sufficient Decrease,varianc reduc stochast gradient descent suffici decreas,"The sufficient decrease technique has been widely used in deterministic optimization, even for non-convex optimization problems, such as line-search techniques. Motivated by those successes, we propose a novel sufficient decrease framework for a class of variance reduced stochastic gradient descent (VR-SGD) methods such as SVRG and SAGA. In order to make sufficient decrease for stochastic optimization, we design a new sufficient decrease criterion. We then introduce a coefficient \theta to satisfy the sufficient decrease property, which takes the decisions to shrink, expand or move in the opposite direction (i.e., \theta x for the variable x), and give two specific update rules for Lasso and ridge regression. Moreover, we analyze the convergence properties of our algorithms for strongly convex problems, which show that both of our algorithms attain linear convergence rates. We also provide the convergence guarantees of both of our algorithms for non-strongly convex problems. Our experimental results further verify that our algorithms achieve better performance than their counterparts.",suffici decreas techniqu wide use determinist optim even non convex optim problem line search techniqu motiv success propos novel suffici decreas framework class varianc reduc stochast gradient descent vr sgd method svrg saga order make suffici decreas stochast optim design new suffici decreas criterion introduc coeffici theta satisfi suffici decreas properti take decis shrink expand move opposit direct theta variabl give two specif updat rule lasso ridg regress moreov analyz converg properti algorithm strong convex problem show algorithm attain linear converg rate also provid converg guarante algorithm non strong convex problem experiment result verifi algorithm achiev better perform counterpart,"['Fanhua Shang', 'Yuanyuan Liu', 'James Cheng', 'Kelvin Kai Wing Ng', 'Yuichi Yoshida']","['cs.LG', 'math.OC', 'stat.ML']",False,False,False,False,False,True
1267,2017-03-28T14:02:09Z,2017-03-20T14:42:27Z,http://arxiv.org/abs/1703.06777v1,http://arxiv.org/pdf/1703.06777v1,On the Use of Default Parameter Settings in the Empirical Evaluation of   Classification Algorithms,use default paramet set empir evalu classif algorithm,"We demonstrate that, for a range of state-of-the-art machine learning algorithms, the differences in generalisation performance obtained using default parameter settings and using parameters tuned via cross-validation can be similar in magnitude to the differences in performance observed between state-of-the-art and uncompetitive learning systems. This means that fair and rigorous evaluation of new learning algorithms requires performance comparison against benchmark methods with best-practice model selection procedures, rather than using default parameter settings. We investigate the sensitivity of three key machine learning algorithms (support vector machine, random forest and rotation forest) to their default parameter settings, and provide guidance on determining sensible default parameter values for implementations of these algorithms. We also conduct an experimental comparison of these three algorithms on 121 classification problems and find that, perhaps surprisingly, rotation forest is significantly more accurate on average than both random forest and a support vector machine.",demonstr rang state art machin learn algorithm differ generalis perform obtain use default paramet set use paramet tune via cross valid similar magnitud differ perform observ state art uncompetit learn system mean fair rigor evalu new learn algorithm requir perform comparison benchmark method best practic model select procedur rather use default paramet set investig sensit three key machin learn algorithm support vector machin random forest rotat forest default paramet set provid guidanc determin sensibl default paramet valu implement algorithm also conduct experiment comparison three algorithm classif problem find perhap surpris rotat forest signific accur averag random forest support vector machin,"['Anthony Bagnall', 'Gavin C. Cawley']","['cs.LG', 'stat.ML']",False,False,False,False,False,True
1269,2017-03-28T14:02:09Z,2017-03-20T12:09:53Z,http://arxiv.org/abs/1703.06700v1,http://arxiv.org/pdf/1703.06700v1,Independence clustering (without a matrix),independ cluster without matrix,"The independence clustering problem is considered in the following formulation: given a set $S$ of random variables, it is required to find the finest partitioning $\{U_1,\dots,U_k\}$ of $S$ into clusters such that the clusters $U_1,\dots,U_k$ are mutually independent. Since mutual independence is the target, pairwise similarity measurements are of no use, and thus traditional clustering algorithms are inapplicable. The distribution of the random variables in $S$ is, in general, unknown, but a sample is available. Thus, the problem is cast in terms of time series. Two forms of sampling are considered: i.i.d.\ and stationary time series, with the main emphasis being on the latter, more general, case. A consistent, computationally tractable algorithm for each of the settings is proposed, and a number of open directions for further research are outlined.",independ cluster problem consid follow formul given set random variabl requir find finest partit dot cluster cluster dot mutual independ sinc mutual independ target pairwis similar measur use thus tradit cluster algorithm inapplic distribut random variabl general unknown sampl avail thus problem cast term time seri two form sampl consid stationari time seri main emphasi latter general case consist comput tractabl algorithm set propos number open direct research outlin,['Daniil Ryabko'],"['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']",False,False,False,False,False,True
1271,2017-03-28T14:02:14Z,2017-03-20T11:26:04Z,http://arxiv.org/abs/1703.06686v1,http://arxiv.org/pdf/1703.06686v1,Copula Index for Detecting Dependence and Monotonicity between   Stochastic Signals,copula index detect depend monoton stochast signal,"This paper introduces a nonparametric copula-based approach for detecting the strength and monotonicity of linear and nonlinear statistical dependence between bivariate continuous, discrete or hybrid random variables and stochastic signals, termed CIM. We show that CIM satisfies the data processing inequality and is consequently a self-equitable metric. Simulation results using synthetic datasets reveal that the CIM compares favorably to other state-of-the-art statistical dependency metrics, including the Maximal Information Coefficient (MIC), Randomized Dependency Coefficient (RDC), distance Correlation (dCor), Copula correlation (Ccor), and Copula Statistic (CoS) in both statistical power and sample size requirements. Simulations using real world data highlight the importance of understanding the monotonicity of the dependence structure.",paper introduc nonparametr copula base approach detect strength monoton linear nonlinear statist depend bivari continu discret hybrid random variabl stochast signal term cim show cim satisfi data process inequ consequ self equit metric simul result use synthet dataset reveal cim compar favor state art statist depend metric includ maxim inform coeffici mic random depend coeffici rdc distanc correl dcor copula correl ccor copula statist cos statist power sampl size requir simul use real world data highlight import understand monoton depend structur,"['Kiran Karra', 'Lamine Mili']","['stat.ML', 'q-bio.QM']",False,False,False,False,False,True
1274,2017-03-28T14:02:14Z,2017-03-19T21:06:51Z,http://arxiv.org/abs/1703.06513v1,http://arxiv.org/pdf/1703.06513v1,Bernoulli Rank-$1$ Bandits for Click Feedback,bernoulli rank bandit click feedback,"The probability that a user will click a search result depends both on its relevance and its position on the results page. The position based model explains this behavior by ascribing to every item an attraction probability, and to every position an examination probability. To be clicked, a result must be both attractive and examined. The probabilities of an item-position pair being clicked thus form the entries of a rank-$1$ matrix. We propose the learning problem of a Bernoulli rank-$1$ bandit where at each step, the learning agent chooses a pair of row and column arms, and receives the product of their Bernoulli-distributed values as a reward. This is a special case of the stochastic rank-$1$ bandit problem considered in recent work that proposed an elimination based algorithm Rank1Elim, and showed that Rank1Elim's regret scales linearly with the number of rows and columns on ""benign"" instances. These are the instances where the minimum of the average row and column rewards $\mu$ is bounded away from zero. The issue with Rank1Elim is that it fails to be competitive with straightforward bandit strategies as $\mu \rightarrow 0$. In this paper we propose Rank1ElimKL which simply replaces the (crude) confidence intervals of Rank1Elim with confidence intervals based on Kullback-Leibler (KL) divergences, and with the help of a novel result concerning the scaling of KL divergences we prove that with this change, our algorithm will be competitive no matter the value of $\mu$. Experiments with synthetic data confirm that on benign instances the performance of Rank1ElimKL is significantly better than that of even Rank1Elim, while experiments with models derived from real data confirm that the improvements are significant across the board, regardless of whether the data is benign or not.",probabl user click search result depend relev posit result page posit base model explain behavior ascrib everi item attract probabl everi posit examin probabl click result must attract examin probabl item posit pair click thus form entri rank matrix propos learn problem bernoulli rank bandit step learn agent choos pair row column arm receiv product bernoulli distribut valu reward special case stochast rank bandit problem consid recent work propos elimin base algorithm rankelim show rankelim regret scale linear number row column benign instanc instanc minimum averag row column reward mu bound away zero issu rankelim fail competit straightforward bandit strategi mu rightarrow paper propos rankelimkl simpli replac crude confid interv rankelim confid interv base kullback leibler kl diverg help novel result concern scale kl diverg prove chang algorithm competit matter valu mu experi synthet data confirm benign instanc perform rankelimkl signific better even rankelim experi model deriv real data confirm improv signific across board regardless whether data benign,"['Sumeet Katariya', 'Branislav Kveton', 'Csaba Szepesvári', 'Claire Vernade', 'Zheng Wen']","['cs.LG', 'stat.ML']",False,False,True,False,False,True
1276,2017-03-28T14:02:14Z,2017-03-18T18:12:17Z,http://arxiv.org/abs/1703.06327v1,http://arxiv.org/pdf/1703.06327v1,Spectrum Estimation from a Few Entries,spectrum estim entri,"Singular values of a data in a matrix form provide insights on the structure of the data, the effective dimensionality, and the choice of hyper-parameters on higher-level data analysis tools. However, in many practical applications such as collaborative filtering and network analysis, we only get a partial observation. Under such scenarios, we consider the fundamental problem of recovering spectral properties of the underlying matrix from a sampling of its entries. We are particularly interested in directly recovering the spectrum, which is the set of singular values, and also in sample-efficient approaches for recovering a spectral sum function, which is an aggregate sum of the same function applied to each of the singular values. We propose first estimating the Schatten $k$-norms of a matrix, and then applying Chebyshev approximation to the spectral sum function or applying moment matching in Wasserstein distance to recover the singular values. The main technical challenge is in accurately estimating the Schatten norms from a sampling of a matrix. We introduce a novel unbiased estimator based on counting small structures in a graph and provide guarantees that match its empirical performance. Our theoretical analysis shows that Schatten norms can be recovered accurately from strictly smaller number of samples compared to what is needed to recover the underlying low-rank matrix. Numerical experiments suggest that we significantly improve upon a competing approach of using matrix completion methods.",singular valu data matrix form provid insight structur data effect dimension choic hyper paramet higher level data analysi tool howev mani practic applic collabor filter network analysi onli get partial observ scenario consid fundament problem recov spectral properti matrix sampl entri particular interest direct recov spectrum set singular valu also sampl effici approach recov spectral sum function aggreg sum function appli singular valu propos first estim schatten norm matrix appli chebyshev approxim spectral sum function appli moment match wasserstein distanc recov singular valu main technic challeng accur estim schatten norm sampl matrix introduc novel unbias estim base count small structur graph provid guarante match empir perform theoret analysi show schatten norm recov accur strict smaller number sampl compar need recov low rank matrix numer experi suggest signific improv upon compet approach use matrix complet method,"['Ashish Khetan', 'Sewoong Oh']","['stat.ML', 'cs.DS', 'cs.LG', 'cs.NA']",False,False,False,False,False,True
1277,2017-03-28T14:02:14Z,2017-03-18T17:49:42Z,http://arxiv.org/abs/1703.06324v1,http://arxiv.org/pdf/1703.06324v1,Deep Tensor Encoding,deep tensor encod,"Learning an encoding of feature vectors in terms of an over-complete dictionary or a probabilistic information geometric (Fisher vectors) construct is wide-spread in statistical signal processing and computer vision. In content based information retrieval using deep-learning classifiers, such encodings are learnt on the flattened last layer, without adherence to the multi-linear structure of the underlying feature tensor. We illustrate a variety of feature encodings incl. sparse dictionary coding and Fisher vectors along with proposing that a structured tensor factorization scheme enables us to perform retrieval that is at par, in terms of average precision, with Fisher vector encoded image signatures. In short, we illustrate how structural constraints increase retrieval fidelity.",learn encod featur vector term complet dictionari probabilist inform geometr fisher vector construct wide spread statist signal process comput vision content base inform retriev use deep learn classifi encod learnt flatten last layer without adher multi linear structur featur tensor illustr varieti featur encod incl spars dictionari code fisher vector along propos structur tensor factor scheme enabl us perform retriev par term averag precis fisher vector encod imag signatur short illustr structur constraint increas retriev fidel,"['B Sengupta', 'E Vasquez', 'Y Qian']","['cs.IR', 'cs.LG', 'stat.ML']",False,False,False,False,False,True
1280,2017-03-28T14:02:18Z,2017-03-18T03:28:40Z,http://arxiv.org/abs/1703.06240v1,http://arxiv.org/pdf/1703.06240v1,Multi-fidelity Bayesian Optimisation with Continuous Approximations,multi fidel bayesian optimis continu approxim,"Bandit methods for black-box optimisation, such as Bayesian optimisation, are used in a variety of applications including hyper-parameter tuning and experiment design. Recently, \emph{multi-fidelity} methods have garnered considerable attention since function evaluations have become increasingly expensive in such applications. Multi-fidelity methods use cheap approximations to the function of interest to speed up the overall optimisation process. However, most multi-fidelity methods assume only a finite number of approximations. In many practical applications however, a continuous spectrum of approximations might be available. For instance, when tuning an expensive neural network, one might choose to approximate the cross validation performance using less data $N$ and/or few training iterations $T$. Here, the approximations are best viewed as arising out of a continuous two dimensional space $(N,T)$. In this work, we develop a Bayesian optimisation method, BOCA, for this setting. We characterise its theoretical properties and show that it achieves better regret than than strategies which ignore the approximations. BOCA outperforms several other baselines in synthetic and real experiments.",bandit method black box optimis bayesian optimis use varieti applic includ hyper paramet tune experi design recent emph multi fidel method garner consider attent sinc function evalu becom increas expens applic multi fidel method use cheap approxim function interest speed overal optimis process howev multi fidel method assum onli finit number approxim mani practic applic howev continu spectrum approxim might avail instanc tune expens neural network one might choos approxim cross valid perform use less data train iter approxim best view aris continu two dimension space work develop bayesian optimis method boca set characteris theoret properti show achiev better regret strategi ignor approxim boca outperform sever baselin synthet real experi,"['Kirthevasan Kandasamy', 'Gautam Dasarathy', 'Jeff Schneider', 'Barnabas Poczos']",['stat.ML'],False,False,False,False,False,True
1281,2017-03-28T14:02:18Z,2017-03-18T00:59:40Z,http://arxiv.org/abs/1703.06229v1,http://arxiv.org/pdf/1703.06229v1,Curriculum Dropout,curriculum dropout,"Dropout is a very effective way of regularizing neural networks. Stochastically ""dropping out"" units with a certain probability discourages over-specific co-adaptations of feature detectors, preventing overfitting and improving network generalization. Besides, Dropout can be interpreted as an approximate model aggregation technique, where an exponential number of smaller networks are averaged in order to get a more powerful ensemble. In this paper, we show that using a fixed dropout probability during training is a suboptimal choice. We thus propose a time scheduling for the probability of retaining neurons in the network. This induces an adaptive regularization scheme that smoothly increases the difficulty of the optimization problem. This idea of ""starting easy"" and adaptively increasing the difficulty of the learning problem has its roots in curriculum learning and allows one to train better models. Indeed, we prove that our optimization strategy implements a very general curriculum scheme, by gradually adding noise to both the input and intermediate feature representations within the network architecture. Experiments on seven image classification datasets and different network architectures show that our method, named Curriculum Dropout, frequently yields to better generalization and, at worst, performs just as well as the standard Dropout method.",dropout veri effect way regular neural network stochast drop unit certain probabl discourag specif co adapt featur detector prevent overfit improv network general besid dropout interpret approxim model aggreg techniqu exponenti number smaller network averag order get power ensembl paper show use fix dropout probabl dure train suboptim choic thus propos time schedul probabl retain neuron network induc adapt regular scheme smooth increas difficulti optim problem idea start easi adapt increas difficulti learn problem root curriculum learn allow one train better model inde prove optim strategi implement veri general curriculum scheme gradual ad nois input intermedi featur represent within network architectur experi seven imag classif dataset differ network architectur show method name curriculum dropout frequent yield better general worst perform well standard dropout method,"['Pietro Morerio', 'Jacopo Cavazza', 'Riccardo Volpi', 'Rene Vidal', 'Vittorio Murino']","['cs.NE', 'cs.LG', 'stat.ML']",False,False,True,False,False,True
1282,2017-03-28T14:02:18Z,2017-03-18T00:08:59Z,http://arxiv.org/abs/1703.06222v1,http://arxiv.org/pdf/1703.06222v1,A Unified Treatment of Multiple Testing with Prior Knowledge,unifi treatment multipl test prior knowledg,"A significant literature has arisen to study ways to employing prior knowledge to improve power and precision of multiple testing procedures. Some common forms of prior knowledge may include (a) a priori beliefs about which hypotheses are null, modeled by non-uniform prior weights; (b) differing importances of hypotheses, modeled by differing penalties for false discoveries; (c) partitions of the hypotheses into known groups, indicating (dis)similarity of hypotheses; and (d) knowledge of independence, positive dependence or arbitrary dependence between hypotheses or groups, allowing for more aggressive or conservative procedures. We present a general framework for global null testing and false discovery rate (FDR) control that allows the scientist to incorporate all four types of prior knowledge (a)-(d) simultaneously. We unify a number of existing procedures, generalize the conditions under which they are known to work, and simplify their proofs of FDR control under independence, positive and arbitrary dependence. We also present an algorithmic framework that strictly generalizes and unifies the classic algorithms of Benjamini and Hochberg [3] and Simes [25], algorithms that guard against unknown dependence [7, 9], algorithms that employ prior weights [17, 15], algorithms that use penalty weights [4], algorithms that incorporate null-proportion adaptivity [26, 27], and algorithms that make use of multiple arbitrary partitions into groups [1]. Unlike this previous work, we can simultaneously incorporate all of the four types of prior knowledge, combined with all of the three forms of dependence.",signific literatur arisen studi way employ prior knowledg improv power precis multipl test procedur common form prior knowledg may includ priori belief hypothes null model non uniform prior weight differ import hypothes model differ penalti fals discoveri partit hypothes known group indic dis similar hypothes knowledg independ posit depend arbitrari depend hypothes group allow aggress conserv procedur present general framework global null test fals discoveri rate fdr control allow scientist incorpor four type prior knowledg simultan unifi number exist procedur general condit known work simplifi proof fdr control independ posit arbitrari depend also present algorithm framework strict general unifi classic algorithm benjamini hochberg sime algorithm guard unknown depend algorithm employ prior weight algorithm use penalti weight algorithm incorpor null proport adapt algorithm make use multipl arbitrari partit group unlik previous work simultan incorpor four type prior knowledg combin three form depend,"['Aaditya Ramdas', 'Rina Foygel Barber', 'Martin J. Wainwright', 'Michael I. Jordan']","['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']",False,False,False,False,False,True
1285,2017-03-28T14:02:18Z,2017-03-17T17:50:44Z,http://arxiv.org/abs/1703.06131v1,http://arxiv.org/pdf/1703.06131v1,Inference via low-dimensional couplings,infer via low dimension coupl,"Integration against an intractable probability measure is among the fundamental challenges of statistical inference, particularly in the Bayesian setting. A principled approach to this problem seeks a deterministic coupling of the measure of interest with a tractable ""reference"" measure (e.g., a standard Gaussian). This coupling is induced by a transport map, and enables direct simulation from the desired measure simply by evaluating the transport map at samples from the reference. Yet characterizing such a map---e.g., representing and evaluating it---grows challenging in high dimensions. The central contribution of this paper is to establish a link between the Markov properties of the target measure and the existence of certain low-dimensional couplings, induced by transport maps that are sparse or decomposable. Our analysis not only facilitates the construction of couplings in high-dimensional settings, but also suggests new inference methodologies. For instance, in the context of nonlinear and non-Gaussian state space models, we describe new online and single-pass variational algorithms that characterize the full posterior distribution of the sequential inference problem using operations only slightly more complex than regular filtering.",integr intract probabl measur among fundament challeng statist infer particular bayesian set principl approach problem seek determinist coupl measur interest tractabl refer measur standard gaussian coupl induc transport map enabl direct simul desir measur simpli evalu transport map sampl refer yet character map repres evalu grow challeng high dimens central contribut paper establish link markov properti target measur exist certain low dimension coupl induc transport map spars decompos analysi onli facilit construct coupl high dimension set also suggest new infer methodolog instanc context nonlinear non gaussian state space model describ new onlin singl pass variat algorithm character full posterior distribut sequenti infer problem use oper onli slight complex regular filter,"['Alessio Spantini', 'Daniele Bigoni', 'Youssef Marzouk']","['stat.ME', 'stat.CO', 'stat.ML']",False,False,False,False,False,True
1286,2017-03-28T14:02:18Z,2017-03-17T17:09:15Z,http://arxiv.org/abs/1703.06104v1,http://arxiv.org/pdf/1703.06104v1,Nonconvex One-bit Single-label Multi-label Learning,nonconvex one bit singl label multi label learn,"We study an extreme scenario in multi-label learning where each training instance is endowed with a single one-bit label out of multiple labels. We formulate this problem as a non-trivial special case of one-bit rank-one matrix sensing and develop an efficient non-convex algorithm based on alternating power iteration. The proposed algorithm is able to recover the underlying low-rank matrix model with linear convergence. For a rank-$k$ model with $d_1$ features and $d_2$ classes, the proposed algorithm achieves $O(\epsilon)$ recovery error after retrieving $O(k^{1.5}d_1 d_2/\epsilon)$ one-bit labels within $O(kd)$ memory. Our bound is nearly optimal in the order of $O(1/\epsilon)$. This significantly improves the state-of-the-art sampling complexity of one-bit multi-label learning. We perform experiments to verify our theory and evaluate the performance of the proposed algorithm.",studi extrem scenario multi label learn train instanc endow singl one bit label multipl label formul problem non trivial special case one bit rank one matrix sens develop effici non convex algorithm base altern power iter propos algorithm abl recov low rank matrix model linear converg rank model featur class propos algorithm achiev epsilon recoveri error retriev epsilon one bit label within kd memori bound near optim order epsilon signific improv state art sampl complex one bit multi label learn perform experi verifi theori evalu perform propos algorithm,"['Shuang Qiu', 'Tingjin Luo', 'Jieping Ye', 'Ming Lin']","['stat.ML', 'cs.LG']",False,False,False,False,False,True
1288,2017-03-28T14:02:18Z,2017-03-17T16:08:23Z,http://arxiv.org/abs/1703.06065v1,http://arxiv.org/pdf/1703.06065v1,Block CUR : Decomposing Large Distributed Matrices,block cur decompos larg distribut matric,"A common problem in large-scale data analysis is to approximate a matrix using a combination of specifically sampled rows and columns, known as CUR decomposition. Unfortunately, in many real-world environments, the ability to sample specific individual rows or columns of the matrix is limited by either system constraints or cost. In this paper, we consider matrix approximation by sampling predefined blocks of columns (or rows) from the matrix. This regime is commonly found when data is distributed across multiple nodes in a compute cluster, where such blocks correspond to columns (or rows) of the matrix stored on the same node, which can be retrieved with much less overhead than retrieving individual columns stored across different nodes. We propose a novel algorithm for sampling useful column blocks and provide guarantees for the quality of the approximation. We demonstrate the practical utility of this algorithm for computing the block CUR decomposition of large matrices in a distributed setting using Apache Spark. Using our proposed block CUR algorithms, we can achieve a significant speed-up compared to a regular CUR decomposition with the same quality of approximation.",common problem larg scale data analysi approxim matrix use combin specif sampl row column known cur decomposit unfortun mani real world environ abil sampl specif individu row column matrix limit either system constraint cost paper consid matrix approxim sampl predefin block column row matrix regim common found data distribut across multipl node comput cluster block correspond column row matrix store node retriev much less overhead retriev individu column store across differ node propos novel algorithm sampl use column block provid guarante qualiti approxim demonstr practic util algorithm comput block cur decomposit larg matric distribut set use apach spark use propos block cur algorithm achiev signific speed compar regular cur decomposit qualiti approxim,"['Urvashi Oswal', 'Swayambhoo Jain', 'Kevin S. Xu', 'Brian Eriksson']","['stat.ML', 'cs.DC', 'cs.DS', 'cs.LG']",False,False,False,False,False,True
1289,2017-03-28T14:02:18Z,2017-03-17T14:59:17Z,http://arxiv.org/abs/1703.06043v1,http://arxiv.org/pdf/1703.06043v1,Pattern representation and recognition with accelerated analog   neuromorphic systems,pattern represent recognit acceler analog neuromorph system,"Despite being originally inspired by the central nervous system, artificial neural networks have diverged from their biological archetypes as they have been remodeled to fit particular tasks. In this paper, we review several possibilites to reverse map these architectures to biologically more realistic spiking networks with the aim of emulating them on fast, low-power neuromorphic hardware. Since many of these devices employ analog components, which cannot be perfectly controlled, finding ways to compensate for the resulting effects represents a key challenge. Here, we discuss three different strategies to address this problem: the addition of auxiliary network components for stabilizing activity, the utilization of inherently robust architectures and a training method for hardware-emulated networks that functions without perfect knowledge of the system's dynamics and parameters. For all three scenarios, we corroborate our theoretical considerations with experimental results on accelerated analog neuromorphic platforms.",despit origin inspir central nervous system artifici neural network diverg biolog archetyp remodel fit particular task paper review sever possibilit revers map architectur biolog realist spike network aim emul fast low power neuromorph hardwar sinc mani devic employ analog compon cannot perfect control find way compens result effect repres key challeng discuss three differ strategi address problem addit auxiliari network compon stabil activ util inher robust architectur train method hardwar emul network function without perfect knowledg system dynam paramet three scenario corrobor theoret consider experiment result acceler analog neuromorph platform,"['Mihai A. Petrovici', 'Sebastian Schmitt', 'Johann Klähn', 'David Stöckel', 'Anna Schroeder', 'Guillaume Bellec', 'Johannes Bill', 'Oliver Breitwieser', 'Ilja Bytschok', 'Andreas Grübl', 'Maurice Güttler', 'Andreas Hartel', 'Stephan Hartmann', 'Dan Husmann', 'Kai Husmann', 'Sebastian Jeltsch', 'Vitali Karasenko', 'Mitja Kleider', 'Christoph Koke', 'Alexander Kononov', 'Christian Mauch', 'Paul Müller', 'Johannes Partzsch', 'Thomas Pfeil', 'Stefan Schiefer', 'Stefan Scholze', 'Anand Subramoney', 'Vasilis Thanasoulis', 'Bernhard Vogginger', 'Robert Legenstein', 'Wolfgang Maass', 'René Schüffny', 'Christian Mayr', 'Johannes Schemmel', 'Karlheinz Meier']","['q-bio.NC', 'cs.NE', 'stat.ML']",False,False,False,False,False,True
1290,2017-03-28T14:02:22Z,2017-03-16T23:33:24Z,http://arxiv.org/abs/1703.05849v1,http://arxiv.org/pdf/1703.05849v1,Causal Inference through the Method of Direct Estimation,causal infer method direct estim,"The intersection of causal inference and machine learning is a rapidly advancing field. We propose a new approach, the method of direct estimation, that draws on both traditions in order to obtain nonparametric estimates of treatment effects. The approach focuses on estimating the effect of fluctuations in a treatment variable on an outcome. A tensor-spline implementation enables rich interactions between functional bases allowing for the approach to capture treatment/covariate interactions. We show how new innovations in Bayesian sparse modeling readily handle the proposed framework, and then document its performance in simulation and applied examples. Furthermore we show how the method of direct estimation can easily extend to structural estimators commonly used in a variety of disciplines, like instrumental variables, mediation analysis, and sequential g-estimation.",intersect causal infer machin learn rapid advanc field propos new approach method direct estim draw tradit order obtain nonparametr estim treatment effect approach focus estim effect fluctuat treatment variabl outcom tensor spline implement enabl rich interact function base allow approach captur treatment covari interact show new innov bayesian spars model readili handl propos framework document perform simul appli exampl furthermor show method direct estim easili extend structur estim common use varieti disciplin like instrument variabl mediat analysi sequenti estim,"['Marc Ratkovic', 'Dustin Tingley']","['stat.ML', 'stat.ME', '62G08, 46N30, 62P20, 62P25']",False,False,False,False,False,True
1292,2017-03-28T14:02:22Z,2017-03-24T20:28:16Z,http://arxiv.org/abs/1703.05840v2,http://arxiv.org/pdf/1703.05840v2,Conditional Accelerated Lazy Stochastic Gradient Descent,condit acceler lazi stochast gradient descent,"In this work we introduce a conditional accelerated lazy stochastic gradient descent algorithm with optimal number of calls to a stochastic first-order oracle and convergence rate $O\left(\frac{1}{\varepsilon^2}\right)$ improving over the projection-free, Online Frank-Wolfe based stochastic gradient descent of Hazan and Kale [2012] with convergence rate $O\left(\frac{1}{\varepsilon^4}\right)$.",work introduc condit acceler lazi stochast gradient descent algorithm optim number call stochast first order oracl converg rate left frac varepsilon right improv project free onlin frank wolf base stochast gradient descent hazan kale converg rate left frac varepsilon right,"['Guanghui Lan', 'Sebastian Pokutta', 'Yi Zhou', 'Daniel Zink']","['cs.LG', 'stat.ML', '90C25', 'G.1.6']",False,False,False,False,False,True
1293,2017-03-28T14:02:22Z,2017-03-16T18:25:21Z,http://arxiv.org/abs/1703.05785v1,http://arxiv.org/pdf/1703.05785v1,Low-rank and Sparse NMF for Joint Endmembers' Number Estimation and   Blind Unmixing of Hyperspectral Images,low rank spars nmf joint endmemb number estim blind unmix hyperspectr imag,"Estimation of the number of endmembers existing in a scene constitutes a critical task in the hyperspectral unmixing process. The accuracy of this estimate plays a crucial role in subsequent unsupervised unmixing steps i.e., the derivation of the spectral signatures of the endmembers (endmembers' extraction) and the estimation of the abundance fractions of the pixels. A common practice amply followed in literature is to treat endmembers' number estimation and unmixing, independently as two separate tasks, providing the outcome of the former as input to the latter. In this paper, we go beyond this computationally demanding strategy. More precisely, we set forth a multiple constrained optimization framework, which encapsulates endmembers' number estimation and unsupervised unmixing in a single task. This is attained by suitably formulating the problem via a low-rank and sparse nonnegative matrix factorization rationale, where low-rankness is promoted with the use of a sophisticated $\ell_2/\ell_1$ norm penalty term. An alternating proximal algorithm is then proposed for minimizing the emerging cost function. The results obtained by simulated and real data experiments verify the effectiveness of the proposed approach.",estim number endmemb exist scene constitut critic task hyperspectr unmix process accuraci estim play crucial role subsequ unsupervis unmix step deriv spectral signatur endmemb endmemb extract estim abund fraction pixel common practic ampli follow literatur treat endmemb number estim unmix independ two separ task provid outcom former input latter paper go beyond comput demand strategi precis set forth multipl constrain optim framework encapsul endmemb number estim unsupervis unmix singl task attain suitabl formul problem via low rank spars nonneg matrix factor rational low rank promot use sophist ell ell norm penalti term altern proxim algorithm propos minim emerg cost function result obtain simul real data experi verifi effect propos approach,"['Paris V. Giampouras', 'Athanasios A. Rontogiannis', 'Konstantinos D. Koutroumbas']","['cs.CV', 'stat.ML']",False,False,False,False,False,True
1295,2017-03-28T14:02:22Z,2017-03-16T15:14:48Z,http://arxiv.org/abs/1703.05667v1,http://arxiv.org/pdf/1703.05667v1,End-to-End Learning for Structured Prediction Energy Networks,end end learn structur predict energi network,"Structured Prediction Energy Networks (Belanger and McCallum, 2016) (SPENs) are a simple, yet expressive family of structured prediction models. An energy function over candidate structured outputs is given by a deep network, and predictions are formed by gradient-based optimization. Unfortunately, we have struggled to apply the structured SVM (SSVM) learning method of Belanger and McCallum, 2016 to applications with more complex structure than multi-label classification. In general, SSVMs are unreliable whenever exact energy minimization is intractable. In response, we present end-to-end learning for SPENs, where the energy function is discriminatively trained by back-propagating through gradient-based prediction. This paper presents a collection of methods necessary to apply the technique to problems with complex structure. For example, we avoid vanishing gradients when learning SPENs for convex relaxations of discrete prediction problems and explicitly train models such that energy minimization converges quickly in practice. Using end-to-end learning, we demonstrate the power of SPENs on 7-Scenes depth image denoising and CoNLL-2005 semantic role labeling tasks. In both, we outperform competitive baselines that employ more simplistic energy functions, but perform exact energy minimization. In particular, for denoising we achieve 40 PSNR, outperforming the previous state-of-the-art of 36.",structur predict energi network belang mccallum spen simpl yet express famili structur predict model energi function candid structur output given deep network predict form gradient base optim unfortun struggl appli structur svm ssvm learn method belang mccallum applic complex structur multi label classif general ssvms unreli whenev exact energi minim intract respons present end end learn spen energi function discrimin train back propag gradient base predict paper present collect method necessari appli techniqu problem complex structur exampl avoid vanish gradient learn spen convex relax discret predict problem explicit train model energi minim converg quick practic use end end learn demonstr power spen scene depth imag denois conll semant role label task outperform competit baselin employ simplist energi function perform exact energi minim particular denois achiev psnr outperform previous state art,"['David Belanger', 'Bishan Yang', 'Andrew McCallum']","['stat.ML', 'cs.LG']",False,False,False,False,False,True
1297,2017-03-28T14:02:22Z,2017-03-16T01:37:25Z,http://arxiv.org/abs/1703.05452v1,http://arxiv.org/pdf/1703.05452v1,Efficient Online Learning for Optimizing Value of Information: Theory   and Application to Interactive Troubleshooting,effici onlin learn optim valu inform theori applic interact troubleshoot,"We consider the optimal value of information (VoI) problem, where the goal is to sequentially select a set of tests with a minimal cost, so that one can efficiently make the best decision based on the observed outcomes. Existing algorithms are either heuristics with no guarantees, or scale poorly (with exponential run time in terms of the number of available tests). Moreover, these methods assume a known distribution over the test outcomes, which is often not the case in practice. We propose an efficient sampling-based online learning framework to address the above issues. First, assuming the distribution over hypotheses is known, we propose a dynamic hypothesis enumeration strategy, which allows efficient information gathering with strong theoretical guarantees. We show that with sufficient amount of samples, one can identify a near-optimal decision with high probability. Second, when the parameters of the hypotheses distribution are unknown, we propose an algorithm which learns the parameters progressively via posterior sampling in an online fashion. We further establish a rigorous bound on the expected regret. We demonstrate the effectiveness of our approach on a real-world interactive troubleshooting application and show that one can efficiently make high-quality decisions with low cost.",consid optim valu inform voi problem goal sequenti select set test minim cost one effici make best decis base observ outcom exist algorithm either heurist guarante scale poor exponenti run time term number avail test moreov method assum known distribut test outcom often case practic propos effici sampl base onlin learn framework address abov issu first assum distribut hypothes known propos dynam hypothesi enumer strategi allow effici inform gather strong theoret guarante show suffici amount sampl one identifi near optim decis high probabl second paramet hypothes distribut unknown propos algorithm learn paramet progress via posterior sampl onlin fashion establish rigor bound expect regret demonstr effect approach real world interact troubleshoot applic show one effici make high qualiti decis low cost,"['Yuxin Chen', 'Jean-Michel Renders', 'Morteza Haghir Chehreghani', 'Andreas Krause']","['cs.AI', 'cs.LG', 'stat.ML']",False,False,False,False,False,True
1298,2017-03-28T14:02:22Z,2017-03-16T01:31:33Z,http://arxiv.org/abs/1703.05449v1,http://arxiv.org/pdf/1703.05449v1,Minimax Regret Bounds for Reinforcement Learning,minimax regret bound reinforc learn,"We consider the problem of efficient exploration in finite horizon MDPs.We show that an optimistic modification to model-based value iteration, can achieve a regret bound $\tilde{O}( \sqrt{HSAT} + H^2S^2A+H\sqrt{T})$ where $H$ is the time horizon, $S$ the number of states, $A$ the number of actions and $T$ the time elapsed. This result improves over the best previous known bound $\tilde{O}(HS \sqrt{AT})$ achieved by the UCRL2 algorithm.The key significance of our new results is that when $T\geq H^3S^3A$ and $SA\geq H$, it leads to a regret of $\tilde{O}(\sqrt{HSAT})$ that matches the established lower bounds of $\Omega(\sqrt{HSAT})$ up to a logarithmic factor. Our analysis contain two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in $S$), and we use ""exploration bonuses"" based on Bernstein's inequality, together with using a recursive -Bellman-type- Law of Total Variance (to improve scaling in $H$).",consid problem effici explor finit horizon mdps show optimist modif model base valu iter achiev regret bound tild sqrt hsat sqrt time horizon number state number action time elaps result improv best previous known bound tild hs sqrt achiev ucrl algorithm key signific new result geq sa geq lead regret tild sqrt hsat match establish lower bound omega sqrt hsat logarithm factor analysi contain two key insight use care applic concentr inequ optim valu function whole rather transit probabl improv scale use explor bonus base bernstein inequ togeth use recurs bellman type law total varianc improv scale,"['Mohammad Gheshlaghi Azar', 'Ian Osband', 'Rémi Munos']","['stat.ML', 'cs.AI', 'cs.LG']",False,False,False,False,False,True
1299,2017-03-28T14:02:22Z,2017-03-15T23:58:19Z,http://arxiv.org/abs/1703.05430v1,http://arxiv.org/pdf/1703.05430v1,Cost-complexity pruning of random forests,cost complex prune random forest,Random forests perform bootstrap-aggregation by sampling the training samples with replacement. This enables the evaluation of out-of-bag error which serves as a internal cross-validation mechanism. Our motivation lies in using the unsampled training samples to improve each decision tree in the ensemble. We study the effect of using the out-of-bag samples to improve the generalization error first of the decision trees and second the random forest by post-pruning. A preliminary empirical study on four UCI repository datasets show consistent decrease in the size of the forests without considerable loss in accuracy.,random forest perform bootstrap aggreg sampl train sampl replac enabl evalu bag error serv intern cross valid mechan motiv lie use unsampl train sampl improv decis tree ensembl studi effect use bag sampl improv general error first decis tree second random forest post prune preliminari empir studi four uci repositori dataset show consist decreas size forest without consider loss accuraci,"['Kiran Bangalore Ravi', 'Jean Serra']","['stat.ML', 'cs.LG']",False,False,False,False,False,True
1300,2017-03-28T14:04:33Z,2017-03-26T16:37:52Z,http://arxiv.org/abs/1703.08843v1,http://arxiv.org/pdf/1703.08843v1,Testing independence with high-dimensional correlated samples,test independ high dimension correl sampl,"Testing independence among a number of (ultra) high-dimensional random samples is a fundamental and challenging problem. By arranging $n$ identically distributed $p$-dimensional random vectors into a $p \times n$ data matrix, we investigate the problem of testing independence among columns under the matrix-variate normal modeling of data. We propose a computationally simple and tuning-free test statistic, characterize its limiting null distribution, analyze the statistical power and prove its minimax optimality. As an important by-product of the test statistic, a ratio-consistent estimator for the quadratic functional of a covariance matrix from correlated samples is developed. We further study the effect of correlation among samples to an important high-dimensional inference problem --- large-scale multiple testing of Pearson's correlation coefficients. Indeed, blindly using classical inference results based on the assumed independence of samples will lead to many false discoveries, which suggests the need for conducting independence testing before applying existing methods. To address the challenge arising from correlation among samples, we propose a ""sandwich estimator"" of Pearson's correlation coefficient by de-correlating the samples. Based on this approach, the resulting multiple testing procedure asymptotically controls the overall false discovery rate at the nominal level while maintaining good statistical power. Both simulated and real data experiments are carried out to demonstrate the advantages of the proposed methods.",test independ among number ultra high dimension random sampl fundament challeng problem arrang ident distribut dimension random vector time data matrix investig problem test independ among column matrix variat normal model data propos comput simpl tune free test statist character limit null distribut analyz statist power prove minimax optim import product test statist ratio consist estim quadrat function covari matrix correl sampl develop studi effect correl among sampl import high dimension infer problem larg scale multipl test pearson correl coeffici inde blind use classic infer result base assum independ sampl lead mani fals discoveri suggest need conduct independ test befor appli exist method address challeng aris correl among sampl propos sandwich estim pearson correl coeffici de correl sampl base approach result multipl test procedur asymptot control overal fals discoveri rate nomin level maintain good statist power simul real data experi carri demonstr advantag propos method,"['Xi Chen', 'Weidong Liu']","['math.ST', 'stat.TH']",False,False,False,False,False,True
1303,2017-03-28T14:04:33Z,2017-03-24T18:45:06Z,http://arxiv.org/abs/1703.08570v1,http://arxiv.org/pdf/1703.08570v1,Stochastic Methods for Composite Optimization Problems,stochast method composit optim problem,"We consider minimization of stochastic functionals that are compositions of a (potentially) non-smooth convex function $h$ and smooth function $c$. We develop two stochastic methods---a stochastic prox-linear algorithm and a stochastic (generalized) sub-gradient procedure---and prove that, under mild technical conditions, each converges to first-order stationary points of the stochastic objective. We provide experiments further investigating our methods on non-smooth phase retrieval problems, the experiments indicate the practical effectiveness of the procedures.",consid minim stochast function composit potenti non smooth convex function smooth function develop two stochast method stochast prox linear algorithm stochast general sub gradient procedur prove mild technic condit converg first order stationari point stochast object provid experi investig method non smooth phase retriev problem experi indic practic effect procedur,"['John Duchi', 'Feng Ruan']","['math.OC', 'math.ST', 'stat.TH']",False,False,False,False,False,True
1305,2017-03-28T14:04:33Z,2017-03-24T11:16:37Z,http://arxiv.org/abs/1703.08358v1,http://arxiv.org/pdf/1703.08358v1,Nonparametric Bayesian analysis for support boundary recovery,nonparametr bayesian analysi support boundari recoveri,"Given a sample of a Poisson point process with intensity $\lambda_f(x,y) = n \mathbf{1}(f(x) \leq y),$ we study recovery of the boundary function $f$ from a nonparametric Bayes perspective. Because of the irregularity of this model, the analysis is non-standard. We derive contraction rates with respect to the $L^1$-norm for several classes of priors, including Gaussian priors, priors based on (truncated) random series, compound Poisson processes, and subordinators. We also investigate the limiting shape of the posterior distribution and derive a nonparametric version of the Bernstein-von Mises theorem for a specific class of priors on a function space with increasing parameter dimension. We show that the marginal posterior of the functional $\vartheta =\int f$ does some automatic bias correction and contracts with a faster rate than the MLE. In this case, $1-\alpha$-credible sets are also asymptotic $1-\alpha$ confidence intervals. It is also shown that the frequentist coverage of credible sets is lost under model misspecification.",given sampl poisson point process intens lambda mathbf leq studi recoveri boundari function nonparametr bay perspect becaus irregular model analysi non standard deriv contract rate respect norm sever class prior includ gaussian prior prior base truncat random seri compound poisson process subordin also investig limit shape posterior distribut deriv nonparametr version bernstein von mise theorem specif class prior function space increas paramet dimens show margin posterior function vartheta int doe automat bias correct contract faster rate mle case alpha credibl set also asymptot alpha confid interv also shown frequentist coverag credibl set lost model misspecif,"['Markus Reiss', 'Johannes Schmidt-Hieber']","['math.ST', 'stat.TH', '62G05, 62G08']",False,False,True,False,False,True
1306,2017-03-28T14:04:33Z,2017-03-24T04:51:03Z,http://arxiv.org/abs/1703.08285v1,http://arxiv.org/pdf/1703.08285v1,The Multi-Armed Bandit Problem: An Efficient Non-Parametric Solution,multi arm bandit problem effici non parametr solut,"Lai and Robbins (1985) and Lai (1987) provided efficient parametric solutions to the multi-armed bandit problem, showing that arm allocation via upper confidence bounds (UCB) achieves minimum regret. These bounds are constructed from the Kullback-Leibler information of the reward distributions, estimated from within a specified parametric family. In recent years there has been renewed interest in the multi-armed bandit problem due to new applications in machine learning algorithms and data analytics. Non-parametric arm allocation procedures like $\epsilon$-greedy and Boltzmann exploration were studied, and modified versions of the UCB procedure were also analyzed under a non-parametric setting. However unlike UCB these non-parametric procedures are not efficient under a parametric setting. In this paper we propose a subsample comparison procedure that is non-parametric, but still efficient under parametric settings.",lai robbin lai provid effici parametr solut multi arm bandit problem show arm alloc via upper confid bound ucb achiev minimum regret bound construct kullback leibler inform reward distribut estim within specifi parametr famili recent year renew interest multi arm bandit problem due new applic machin learn algorithm data analyt non parametr arm alloc procedur like epsilon greedi boltzmann explor studi modifi version ucb procedur also analyz non parametr set howev unlik ucb non parametr procedur effici parametr set paper propos subsampl comparison procedur non parametr still effici parametr set,['Hock Peng Chan'],"['math.ST', 'stat.TH']",False,False,False,False,False,True
1307,2017-03-28T14:04:33Z,2017-03-23T18:01:39Z,http://arxiv.org/abs/1703.08190v1,http://arxiv.org/pdf/1703.08190v1,MSE estimates for multitaper spectral estimation and off-grid   compressive sensing,mse estim multitap spectral estim grid compress sens,"We obtain estimates for the Mean Squared Error (MSE) for the multitaper spectral estimator and certain compressive acquisition methods for multi-band signals. We confirm a fact discovered by Thomson [Spectrum estimation and harmonic analysis, Proc. IEEE, 1982]: assuming bandwidth $W$ and $N$ time domain observations, the average of the square of the first $K=2NW$ Slepian functions approaches, as $K$ grows, an ideal band-pass kernel for the interval $[-W,W]$. We provide an analytic proof of this fact and measure the corresponding rate of convergence in the $L^{1}$ norm. This validates a heuristic approximation used to control the MSE of the multitaper estimator. The estimates have also consequences for the method of compressive acquisition of multi-band signals introduced by Davenport and Wakin, giving MSE approximation bounds for the dictionary formed by modulation of the critical number of prolates.",obtain estim mean squar error mse multitap spectral estim certain compress acquisit method multi band signal confirm fact discov thomson spectrum estim harmon analysi proc ieee assum bandwidth time domain observ averag squar first nw slepian function approach grow ideal band pass kernel interv provid analyt proof fact measur correspond rate converg norm valid heurist approxim use control mse multitap estim estim also consequ method compress acquisit multi band signal introduc davenport wakin give mse approxim bound dictionari form modul critic number prolat,"['Luís Daniel Abreu', 'José Luis Romero']","['cs.IT', 'math.IT', 'math.ST', 'stat.TH']",False,False,False,False,False,True
1308,2017-03-28T14:04:33Z,2017-03-22T22:59:20Z,http://arxiv.org/abs/1703.07879v1,http://arxiv.org/pdf/1703.07879v1,How to avoid the curse of dimensionality: scalability of particle   filters with and without importance weights,avoid curs dimension scalabl particl filter without import weight,"Particle filters are a popular and flexible class of numerical algorithms to solve a large class of nonlinear filtering problems. However, standard particle filters with importance weights have been shown to require a sample size that increases exponentially with the dimension D of the state space in order to achieve a certain performance, which precludes their use in very high-dimensional filtering problems. Here, we focus on the dynamic aspect of this curse of dimensionality (COD) in continuous time filtering, which is caused by the degeneracy of importance weights over time. We show that the degeneracy occurs on a time-scale that decreases with increasing D. In order to soften the effects of weight degeneracy, most particle filters use particle resampling and improved proposal functions for the particle motion. We explain why neither of the two can prevent the COD in general. In order to address this fundamental problem, we investigate an existing filtering algorithm based on optimal feedback control that sidesteps the use of importance weights. We use numerical experiments to show that this Feedback Particle Filter (FPF) by Yang et al. (2013) does not exhibit a COD.",particl filter popular flexibl class numer algorithm solv larg class nonlinear filter problem howev standard particl filter import weight shown requir sampl size increas exponenti dimens state space order achiev certain perform preclud use veri high dimension filter problem focus dynam aspect curs dimension cod continu time filter caus degeneraci import weight time show degeneraci occur time scale decreas increas order soften effect weight degeneraci particl filter use particl resampl improv propos function particl motion explain whi neither two prevent cod general order address fundament problem investig exist filter algorithm base optim feedback control sidestep use import weight use numer experi show feedback particl filter fpf yang et al doe exhibit cod,"['Simone Carlo Surace', 'Anna Kutschireiter', 'Jean-Pascal Pfister']","['math.OC', 'math.PR', 'math.ST', 'stat.ME', 'stat.TH']",False,False,False,False,False,True
1309,2017-03-28T14:04:33Z,2017-03-22T18:32:47Z,http://arxiv.org/abs/1703.07809v1,http://arxiv.org/pdf/1703.07809v1,Empirical Risk Minimization as Parameter Choice Rule for General Linear   Regularization Methods,empir risk minim paramet choic rule general linear regular method,"We consider the statistical inverse problem to recover $f$ from noisy measurements $Y = Tf + \sigma \xi$ where $\xi$ is Gaussian white noise and $T$ a compact operator between Hilbert spaces. Considering general reconstruction methods of the form $\hat f_\alpha = q_\alpha \left(T^*T\right)T^*Y$ with an ordered filter $q_\alpha$, we investigate the choice of the regularization parameter $\alpha$ by minimizing an unbiased estimate of the predictive risk $\mathbb E\left[\Vert Tf - T\hat f_\alpha\Vert^2\right]$. The corresponding parameter $\alpha_{\mathrm{pred}}$ and its usage are well-known in the literature, but oracle inequalities and optimality results in this general setting are unknown. We prove a (generalized) oracle inequality, which relates the direct risk $\mathbb E\left[\Vert f - \hat f_{\alpha_{\mathrm{pred}}}\Vert^2\right]$ with the oracle prediction risk $\inf_{\alpha>0}\mathbb E\left[\Vert Tf - T\hat f_{\alpha}\Vert^2\right]$. From this oracle inequality we are then able to conclude that the investigated parameter choice rule is of optimal order.   Finally we also present numerical simulations, which support the order optimality of the method and the quality of the parameter choice in finite sample situations.",consid statist invers problem recov noisi measur tf sigma xi xi gaussian white nois compact oper hilbert space consid general reconstruct method form hat alpha alpha left right order filter alpha investig choic regular paramet alpha minim unbias estim predict risk mathbb left vert tf hat alpha vert right correspond paramet alpha mathrm pred usag well known literatur oracl inequ optim result general set unknown prove general oracl inequ relat direct risk mathbb left vert hat alpha mathrm pred vert right oracl predict risk inf alpha mathbb left vert tf hat alpha vert right oracl inequ abl conclud investig paramet choic rule optim order final also present numer simul support order optim method qualiti paramet choic finit sampl situat,"['Housen Li', 'Frank Werner']","['math.NA', 'math.ST', 'stat.TH', 'Primary 62G05, Secondary 62G20, 65J22, 65J20']",False,False,True,False,False,True
1312,2017-03-28T14:04:37Z,2017-03-21T14:20:44Z,http://arxiv.org/abs/1703.07233v1,http://arxiv.org/pdf/1703.07233v1,Gibbs Reference Prior for Robust Gaussian Process Emulation,gibb refer prior robust gaussian process emul,"We propose an objective prior distribution on correlation kernel parameters for Simple Kriging models in the spirit of reference priors. Because it is proper and defined through its conditional densities, it and its associated posterior distribution lend themselves well to Gibbs sampling, thus making the full-Bayesian procedure tractable. Numerical examples show it has near-optimal frequentist performance in terms of prediction interval coverage",propos object prior distribut correl kernel paramet simpl krige model spirit refer prior becaus proper defin condit densiti associ posterior distribut lend themselv well gibb sampl thus make full bayesian procedur tractabl numer exampl show near optim frequentist perform term predict interv coverag,['Joseph Muré'],"['math.ST', 'stat.TH', '62F15 (Primary) 62M30, 60G15 (Secondary)']",False,False,False,False,False,True
1313,2017-03-28T14:04:37Z,2017-03-21T06:24:47Z,http://arxiv.org/abs/1703.07072v1,http://arxiv.org/pdf/1703.07072v1,Bayesian Nonparametric Inference for M/G/1 Queueing Systems,bayesian nonparametr infer queue system,"In this work, nonparametric statistical inference is provided for the continuous-time M/G/1 queueing model from a Bayesian point of view. The inference is based on observations of the inter-arrival and service times. Beside other characteristics of the system, particular interest is in the waiting time distribution which is not accessible in closed form. Thus, we use an indirect statistical approach by exploiting the Pollaczek-Khinchine transform formula for the Laplace transform of the waiting time distribution. Due to this, an estimator is defined and its frequentist validation in terms of posterior consistency and posterior normality is studied. It will turn out that we can hereby make inference for the observables separately and compose the results subsequently by suitable techniques.",work nonparametr statist infer provid continu time queue model bayesian point view infer base observ inter arriv servic time besid characterist system particular interest wait time distribut access close form thus use indirect statist approach exploit pollaczek khinchin transform formula laplac transform wait time distribut due estim defin frequentist valid term posterior consist posterior normal studi turn herebi make infer observ separ compos result subsequ suitabl techniqu,"['Cornelia Wichelhaus', 'Moritz von Rohrscheidt']","['math.ST', 'stat.TH']",False,False,True,False,False,True
1315,2017-03-28T14:04:37Z,2017-03-20T15:55:05Z,http://arxiv.org/abs/1703.06810v1,http://arxiv.org/pdf/1703.06810v1,The geometry of hypothesis testing over convex cones: Generalized   likelihood tests and minimax radii,geometri hypothesi test convex cone general likelihood test minimax radii,"We consider a compound testing problem within the Gaussian sequence model in which the null and alternative are specified by a pair of closed, convex cones. Such cone testing problem arise in various applications, including detection of treatment effects, trend detection in econometrics, signal detection in radar processing, and shape-constrained inference in non-parametric statistics. We provide a sharp characterization of the GLRT testing radius up to a universal multiplicative constant in terms of the geometric structure of the underlying convex cones. When applied to concrete examples, this result reveals some interesting phenomena that do not arise in the analogous problems of estimation under convex constraints. In particular, in contrast to estimation error, the testing error no longer depends purely on the problem complexity via a volume-based measure (such as metric entropy or Gaussian complexity), other geometric properties of the cones also play an important role. To address the issue of optimality, we prove information-theoretic lower bounds for minimax testing radius again in terms of geometric quantities. Our general theorems are illustrated by examples including the cases of monotone and orthant cones, and involve some results of independent interest.",consid compound test problem within gaussian sequenc model null altern specifi pair close convex cone cone test problem aris various applic includ detect treatment effect trend detect econometr signal detect radar process shape constrain infer non parametr statist provid sharp character glrt test radius univers multipl constant term geometr structur convex cone appli concret exampl result reveal interest phenomena aris analog problem estim convex constraint particular contrast estim error test error longer depend pure problem complex via volum base measur metric entropi gaussian complex geometr properti cone also play import role address issu optim prove inform theoret lower bound minimax test radius term geometr quantiti general theorem illustr exampl includ case monoton orthant cone involv result independ interest,"['Yuting Wei', 'Martin J. Wainwright', 'Adityanand Guntuboyina']","['math.ST', 'cs.IT', 'math.IT', 'stat.TH']",False,False,False,False,False,True
1316,2017-03-28T14:04:37Z,2017-03-20T05:37:48Z,http://arxiv.org/abs/1703.06610v1,http://arxiv.org/pdf/1703.06610v1,Asymptotic Performance of PCA for High-Dimensional Heteroscedastic Data,asymptot perform pca high dimension heteroscedast data,"Principal Component Analysis (PCA) is a classical method for reducing the dimensionality of data by projecting them onto a subspace that captures most of their variation. Effective use of PCA in modern applications requires understanding its performance for data that are both high-dimensional (i.e., with dimension comparable to or larger than the number of samples) and heteroscedastic (i.e., with noise that has non uniform variance across samples such as outliers). This paper analyzes the statistical performance of PCA in this setting, that is, for high-dimensional data drawn from a low-dimensional subspace and degraded by heteroscedastic noise. We provide simple expressions for the asymptotic PCA recovery of the underlying subspace, subspace amplitudes and subspace coefficients; the expressions enable both easy and efficient calculation and reasoning about the performance of PCA. We exploit the structure of these expressions to show that asymptotic recovery for a fixed average noise variance is maximized when the noise variances are equal (i.e., when the noise is in fact homoscedastic). Hence, while average noise variance is often a practically convenient measure for the overall quality of data, it gives an overly optimistic estimate of the performance of PCA for heteroscedastic data.",princip compon analysi pca classic method reduc dimension data project onto subspac captur variat effect use pca modern applic requir understand perform data high dimension dimens compar larger number sampl heteroscedast nois non uniform varianc across sampl outlier paper analyz statist perform pca set high dimension data drawn low dimension subspac degrad heteroscedast nois provid simpl express asymptot pca recoveri subspac subspac amplitud subspac coeffici express enabl easi effici calcul reason perform pca exploit structur express show asymptot recoveri fix averag nois varianc maxim nois varianc equal nois fact homoscedast henc averag nois varianc often practic conveni measur overal qualiti data give optimist estim perform pca heteroscedast data,"['David Hong', 'Laura Balzano', 'Jeffrey A. Fessler']","['math.ST', 'stat.TH']",False,False,False,False,False,True
1319,2017-03-28T14:04:37Z,2017-03-18T00:08:59Z,http://arxiv.org/abs/1703.06222v1,http://arxiv.org/pdf/1703.06222v1,A Unified Treatment of Multiple Testing with Prior Knowledge,unifi treatment multipl test prior knowledg,"A significant literature has arisen to study ways to employing prior knowledge to improve power and precision of multiple testing procedures. Some common forms of prior knowledge may include (a) a priori beliefs about which hypotheses are null, modeled by non-uniform prior weights; (b) differing importances of hypotheses, modeled by differing penalties for false discoveries; (c) partitions of the hypotheses into known groups, indicating (dis)similarity of hypotheses; and (d) knowledge of independence, positive dependence or arbitrary dependence between hypotheses or groups, allowing for more aggressive or conservative procedures. We present a general framework for global null testing and false discovery rate (FDR) control that allows the scientist to incorporate all four types of prior knowledge (a)-(d) simultaneously. We unify a number of existing procedures, generalize the conditions under which they are known to work, and simplify their proofs of FDR control under independence, positive and arbitrary dependence. We also present an algorithmic framework that strictly generalizes and unifies the classic algorithms of Benjamini and Hochberg [3] and Simes [25], algorithms that guard against unknown dependence [7, 9], algorithms that employ prior weights [17, 15], algorithms that use penalty weights [4], algorithms that incorporate null-proportion adaptivity [26, 27], and algorithms that make use of multiple arbitrary partitions into groups [1]. Unlike this previous work, we can simultaneously incorporate all of the four types of prior knowledge, combined with all of the three forms of dependence.",signific literatur arisen studi way employ prior knowledg improv power precis multipl test procedur common form prior knowledg may includ priori belief hypothes null model non uniform prior weight differ import hypothes model differ penalti fals discoveri partit hypothes known group indic dis similar hypothes knowledg independ posit depend arbitrari depend hypothes group allow aggress conserv procedur present general framework global null test fals discoveri rate fdr control allow scientist incorpor four type prior knowledg simultan unifi number exist procedur general condit known work simplifi proof fdr control independ posit arbitrari depend also present algorithm framework strict general unifi classic algorithm benjamini hochberg sime algorithm guard unknown depend algorithm employ prior weight algorithm use penalti weight algorithm incorpor null proport adapt algorithm make use multipl arbitrari partit group unlik previous work simultan incorpor four type prior knowledg combin three form depend,"['Aaditya Ramdas', 'Rina Foygel Barber', 'Martin J. Wainwright', 'Michael I. Jordan']","['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']",False,False,False,False,False,True
1321,2017-03-28T14:04:42Z,2017-03-16T13:48:45Z,http://arxiv.org/abs/1703.05619v1,http://arxiv.org/pdf/1703.05619v1,Nonparametric intensity estimation from indirect point process   observations under unknown error distribution,nonparametr intens estim indirect point process observ unknown error distribut,"We consider the nonparametric estimation of the intensity function of a Poisson point process in a circular model from indirect observations $N_1,\ldots,N_n$. These observations emerge from hidden point process realizations with the target intensity through contamination with additive error. Under the assumption that the error distribution is unknown and only available by means of an additional sample $Y_1,\ldots,Y_m$ we derive minimax rates of convergence with respect to the sample sizes $n$ and $m$ under abstract smoothness conditions and propose an orthonormal series estimator which attains the optimal rate of convergence. The performance of the estimator depends on the correct specification of a dimension parameter whose optimal choice relies on smoothness characteristics of both the intensity and the error density. Since a priori knowledge of such characteristics is a too strong assumption, we propose a data-driven choice of the dimension parameter based on model selection and show that the adaptive estimator either attains the minimax optimal rate or is suboptimal only by a logarithmic factor.",consid nonparametr estim intens function poisson point process circular model indirect observ ldot observ emerg hidden point process realize target intens contamin addit error assumpt error distribut unknown onli avail mean addit sampl ldot deriv minimax rate converg respect sampl size abstract smooth condit propos orthonorm seri estim attain optim rate converg perform estim depend correct specif dimens paramet whose optim choic reli smooth characterist intens error densiti sinc priori knowledg characterist strong assumpt propos data driven choic dimens paramet base model select show adapt estim either attain minimax optim rate suboptim onli logarithm factor,['Martin Kroll'],"['math.ST', 'stat.TH', '62G05, 60G55']",False,False,True,False,False,True
1322,2017-03-28T14:04:42Z,2017-03-15T12:04:34Z,http://arxiv.org/abs/1703.05101v1,http://arxiv.org/pdf/1703.05101v1,Optimal graphon estimation in cut distance,optim graphon estim cut distanc,"Consider the twin problems of estimating the connection probability matrix of an inhomogeneous random graph and the graphon of a W-random graph. We establish the minimax estimation rates with respect to the cut metric for classes of block constant matrices and step function graphons. Surprisingly, our results imply that, from the minimax point of view, the raw data, that is, the adjacency matrix of the observed graph, is already optimal and more involved procedures cannot improve the convergence rates for this metric. This phenomenon contrasts with optimal rates of convergence with respect to other classical distances for graphons such as the l 1 or l 2 metrics.",consid twin problem estim connect probabl matrix inhomogen random graph graphon random graph establish minimax estim rate respect cut metric class block constant matric step function graphon surpris result impli minimax point view raw data adjac matrix observ graph alreadi optim involv procedur cannot improv converg rate metric phenomenon contrast optim rate converg respect classic distanc graphon metric,"['Olga Klopp', 'Nicolas Verzelen']","['math.ST', 'stat.TH']",False,False,False,False,False,True
1324,2017-03-28T14:04:42Z,2017-03-15T06:26:43Z,http://arxiv.org/abs/1703.04955v1,http://arxiv.org/pdf/1703.04955v1,Theoretical Limits of Record Linkage and Microclustering,theoret limit record linkag microclust,"There has been substantial recent interest in record linkage, attempting to group the records pertaining to the same entities from a large database lacking unique identifiers. This can be viewed as a type of ""microclustering,"" with few observations per cluster and a very large number of clusters. A variety of methods have been proposed, but there is a lack of literature providing theoretical guarantees on performance. We show that the problem is fundamentally hard from a theoretical perspective, and even in idealized cases, accurate entity resolution is effectively impossible when the number of entities is small relative to the number of records and/or the separation among records from different entities is not extremely large. To characterize the fundamental difficulty, we focus on entity resolution based on multivariate Gaussian mixture models, but our conclusions apply broadly and are supported by simulation studies inspired by human rights applications. These results suggest conservatism in interpretation of the results of record linkage, support collection of additional data to more accurately disambiguate the entities, and motivate a focus on coarser inference. For example, results from a simulation study suggest that sometimes one may obtain accurate results for population size estimation even when fine scale entity resolution is inaccurate.",substanti recent interest record linkag attempt group record pertain entiti larg databas lack uniqu identifi view type microclust observ per cluster veri larg number cluster varieti method propos lack literatur provid theoret guarante perform show problem fundament hard theoret perspect even ideal case accur entiti resolut effect imposs number entiti small relat number record separ among record differ entiti extrem larg character fundament difficulti focus entiti resolut base multivari gaussian mixtur model conclus appli broad support simul studi inspir human right applic result suggest conservat interpret result record linkag support collect addit data accur disambigu entiti motiv focus coarser infer exampl result simul studi suggest sometim one may obtain accur result popul size estim even fine scale entiti resolut inaccur,"['James E. Johndrow', 'Kristian Lum', 'David B. Dunson']","['math.ST', 'stat.TH']",False,False,True,False,False,True
1325,2017-03-28T14:04:42Z,2017-03-15T02:25:31Z,http://arxiv.org/abs/1703.04886v1,http://arxiv.org/pdf/1703.04886v1,Towards Optimal Sparse Inverse Covariance Selection through Non-Convex   Optimization,toward optim spars invers covari select non convex optim,"We study the problem of reconstructing the graph of a sparse Gaussian Graphical Model from independent observations, which is equivalent to finding non-zero elements of an inverse covariance matrix. For a model of size $p$ and maximum degree $d$, information theoretic lower bounds established in prior works require that the number of samples needed for recovering the graph perfectly is at least $d \log p/\kappa^2$, where $\kappa$ is the minimum normalized non-zero entry of the inverse covariance matrix. Existing algorithms require additional assumptions to guarantee perfect graph reconstruction, and consequently, their sample complexity is dependent on parameters that are not present in the information theoretic lower bound. We propose an estimator, called SLICE, that consists of a cardinality constrained least-squares regression followed by a thresholding procedure. Without any additional assumptions we show that SLICE attains a sample complexity of $\frac{64}{\kappa^4}d \log p$, which differs from the lower bound by only a factor proportional to $1/\kappa^2$ and depends only on parameters present in the lower bound.",studi problem reconstruct graph spars gaussian graphic model independ observ equival find non zero element invers covari matrix model size maximum degre inform theoret lower bound establish prior work requir number sampl need recov graph perfect least log kappa kappa minimum normal non zero entri invers covari matrix exist algorithm requir addit assumpt guarante perfect graph reconstruct consequ sampl complex depend paramet present inform theoret lower bound propos estim call slice consist cardin constrain least squar regress follow threshold procedur without ani addit assumpt show slice attain sampl complex frac kappa log differ lower bound onli factor proport kappa depend onli paramet present lower bound,"['Sidhant Misra', 'Marc Vuffray', 'Andrey Y. Lokhov', 'Michael Chertkov']","['cs.LG', 'cs.IT', 'math.IT', 'math.ST', 'stat.TH']",False,False,False,False,False,True
1326,2017-03-28T14:04:42Z,2017-03-14T22:49:50Z,http://arxiv.org/abs/1703.04799v1,http://arxiv.org/pdf/1703.04799v1,Multi-parameter One-Sided Monitoring Test,multi paramet one side monitor test,"Multi-parameter one-sided hypothesis test problems arise naturally in many applications. We are particularly interested in effective tests for monitoring multiple quality indices in forestry products. Our search reveals that there are many effective statistical methods in the literature for normal data, and that they can easily be adapted for non-normal data. We find that the beautiful likelihood ratio test is unsatisfactory, because in order to control the size, it must cope with the least favorable distributions at the cost of power. In this paper, we find a novel way to slightly ease the size control, obtaining a much more powerful test. Simulation confirms that the new test retains good control of the type I error and is markedly more powerful than the likelihood ratio test as well as many competitors based on normal data. The new method performs well in the context of monitoring multiple quality indices.",multi paramet one side hypothesi test problem aris natur mani applic particular interest effect test monitor multipl qualiti indic forestri product search reveal mani effect statist method literatur normal data easili adapt non normal data find beauti likelihood ratio test unsatisfactori becaus order control size must cope least favor distribut cost power paper find novel way slight eas size control obtain much power test simul confirm new test retain good control type error mark power likelihood ratio test well mani competitor base normal data new method perform well context monitor multipl qualiti indic,"['Guangyu Zhu', 'Jiahua Chen']","['math.ST', 'stat.TH']",False,False,True,False,False,True
1327,2017-03-28T14:04:42Z,2017-03-14T22:36:09Z,http://arxiv.org/abs/1703.04790v1,http://arxiv.org/pdf/1703.04790v1,Robust Power System Dynamic State Estimator with Non-Gaussian   Measurement Noise: Part I--Theory,robust power system dynam state estim non gaussian measur nois part theori,"This paper develops the theoretical framework and the equations of a new robust Generalized Maximum-likelihood-type Unscented Kalman Filter (GM-UKF) that is able to suppress observation and innovation outliers while filtering out non-Gaussian measurement noise. Because the errors of the real and reactive power measurements calculated using Phasor Measurement Units (PMUs) follow long-tailed probability distributions, the conventional UKF provides strongly biased state estimates since it relies on the weighted least squares estimator. By contrast, the state estimates and residuals of our GM-UKF are proved to be roughly Gaussian, allowing the sigma points to reliably approximate the mean and the covariance matrices of the predicted and corrected state vectors. To develop our GM-UKF, we first derive a batch-mode regression form by processing the predictions and observations simultaneously, where the statistical linearization approach is used. We show that the set of equations so derived are equivalent to those of the unscented transformation. Then, a robust GM-estimator that minimizes a convex Huber cost function while using weights calculated via Projection Statistics (PS's) is proposed. The PS's are applied to a two-dimensional matrix that consists of serially correlated predicted state and innovation vectors to detect observation and innovation outliers. These outliers are suppressed by the GM-estimator using the iteratively reweighted least squares algorithm. Finally, the asymptotic error covariance matrix of the GM-UKF state estimates is derived from the total influence function. In the companion paper, extensive simulation results will be shown to verify the effectiveness and robustness of the proposed method.",paper develop theoret framework equat new robust general maximum likelihood type unscent kalman filter gm ukf abl suppress observ innov outlier filter non gaussian measur nois becaus error real reactiv power measur calcul use phasor measur unit pmus follow long tail probabl distribut convent ukf provid strong bias state estim sinc reli weight least squar estim contrast state estim residu gm ukf prove rough gaussian allow sigma point reliabl approxim mean covari matric predict correct state vector develop gm ukf first deriv batch mode regress form process predict observ simultan statist linear approach use show set equat deriv equival unscent transform robust gm estim minim convex huber cost function use weight calcul via project statist ps propos ps appli two dimension matrix consist serial correl predict state innov vector detect observ innov outlier outlier suppress gm estim use iter reweight least squar algorithm final asymptot error covari matrix gm ukf state estim deriv total influenc function companion paper extens simul result shown verifi effect robust propos method,"['Junbo Zhao', 'Lamine Mili']","['math.ST', 'stat.TH']",False,False,False,False,False,True
1328,2017-03-28T14:04:42Z,2017-03-14T20:19:08Z,http://arxiv.org/abs/1703.04697v1,http://arxiv.org/pdf/1703.04697v1,On the benefits of output sparsity for multi-label classification,benefit output sparsiti multi label classif,"The multi-label classification framework, where each observation can be associated with a set of labels, has generated a tremendous amount of attention over recent years. The modern multi-label problems are typically large-scale in terms of number of observations, features and labels, and the amount of labels can even be comparable with the amount of observations. In this context, different remedies have been proposed to overcome the curse of dimensionality. In this work, we aim at exploiting the output sparsity by introducing a new loss, called the sparse weighted Hamming loss. This proposed loss can be seen as a weighted version of classical ones, where active and inactive labels are weighted separately. Leveraging the influence of sparsity in the loss function, we provide improved generalization bounds for the empirical risk minimizer, a suitable property for large-scale problems. For this new loss, we derive rates of convergence linear in the underlying output-sparsity rather than linear in the number of labels. In practice, minimizing the associated risk can be performed efficiently by using convex surrogates and modern convex optimization algorithms. We provide experiments on various real-world datasets demonstrating the pertinence of our approach when compared to non-weighted techniques.",multi label classif framework observ associ set label generat tremend amount attent recent year modern multi label problem typic larg scale term number observ featur label amount label even compar amount observ context differ remedi propos overcom curs dimension work aim exploit output sparsiti introduc new loss call spars weight ham loss propos loss seen weight version classic one activ inact label weight separ leverag influenc sparsiti loss function provid improv general bound empir risk minim suitabl properti larg scale problem new loss deriv rate converg linear output sparsiti rather linear number label practic minim associ risk perform effici use convex surrog modern convex optim algorithm provid experi various real world dataset demonstr pertin approach compar non weight techniqu,"['Evgenii Chzhen', 'Christophe Denis', 'Mohamed Hebiri', 'Joseph Salmon']","['math.ST', 'cs.LG', 'stat.ML', 'stat.TH']",False,False,False,False,False,True
1331,2017-03-28T14:04:46Z,2017-03-13T14:36:48Z,http://arxiv.org/abs/1703.04419v1,http://arxiv.org/pdf/1703.04419v1,Iterated failure rate monotonicity and ordering relations within Gamma   and Weibull distributions,iter failur rate monoton order relat within gamma weibul distribut,"Stochastic ordering of distributions of random variables may be defined by the relative convexity of the tail functions. This has been extended to higher order stochastic orderings, by iteratively reassigning tail-weights. The actual verification of those stochastic orderings is not simple, as this depends on inverting distribution functions for which there may be no explicit expression. The iterative definition of distributions, of course, contributes to make that verification even harder. We have a look at the stochastic ordering, introducing a method that allows for explicit usage, applying it to the Gamma and Weibull distributions, giving a complete description of the order relations within each of those families.",stochast order distribut random variabl may defin relat convex tail function extend higher order stochast order iter reassign tail weight actual verif stochast order simpl depend invert distribut function may explicit express iter definit distribut cours contribut make verif even harder look stochast order introduc method allow explicit usag appli gamma weibul distribut give complet descript order relat within famili,"['Idir Arab', 'Paulo Eduardo Oliveira']","['math.ST', 'stat.TH', '60E15, 26A51']",False,False,False,False,False,True
1336,2017-03-28T14:04:46Z,2017-03-10T12:42:32Z,http://arxiv.org/abs/1703.03658v1,http://arxiv.org/pdf/1703.03658v1,Construction of Non-asymptotic Confidence Sets in 2-Wasserstein Space,construct non asymptot confid set wasserstein space,"In this paper, we consider a probabilistic setting where the probability measures are considered to be random objects. We propose a procedure of construction non-asymptotic confidence sets for empirical barycenters in 2-Wasserstein space and develop the idea further to construction of a non-parametric two-sample test that is then applied to the detection of structural breaks in data with complex geometry. Both procedures mainly rely on the idea of multiplier bootstrap (Spokoiny and Zhilova (2015), Chernozhukov et al. (2014)). The main focus lies on probability measures that have commuting covariance matrices and belong to the same scatter-location family: we proof the validity of a bootstrap procedure that allows to compute confidence sets and critical values for a Wasserstein-based two-sample test.",paper consid probabilist set probabl measur consid random object propos procedur construct non asymptot confid set empir barycent wasserstein space develop idea construct non parametr two sampl test appli detect structur break data complex geometri procedur main reli idea multipli bootstrap spokoini zhilova chernozhukov et al main focus lie probabl measur commut covari matric belong scatter locat famili proof valid bootstrap procedur allow comput confid set critic valu wasserstein base two sampl test,"['Johannes Ebert', 'Vladimir Spokoiny', 'Alexandra Suvorikova']","['math.ST', 'stat.TH']",False,False,False,False,False,True
1337,2017-03-28T14:04:46Z,2017-03-09T19:00:01Z,http://arxiv.org/abs/1703.03412v1,http://arxiv.org/pdf/1703.03412v1,Uniform estimation of a class of random graph functionals,uniform estim class random graph function,"We consider estimation of certain functionals of random graphs. The random graph is generated by a stochastic block model (SBM). The number of classes is fixed or grows with the number of vertices. Minimax lower and upper bounds of estimation along specific submodels are derived. The results are nonasymptotic and imply that uniform estimation of a single connectivity parameter is much slower than the expected asymptotic pointwise rate. Specifically, the uniform quadratic rate does not scale as the number of edges, but only as the number of vertices. The lower bounds are local around any possible SBM. An analogous result is derived for functionals of a class of smooth graphons.",consid estim certain function random graph random graph generat stochast block model sbm number class fix grow number vertic minimax lower upper bound estim along specif submodel deriv result nonasymptot impli uniform estim singl connect paramet much slower expect asymptot pointwis rate specif uniform quadrat rate doe scale number edg onli number vertic lower bound local around ani possibl sbm analog result deriv function class smooth graphon,"['Ismaël Castillo', 'Peter Orbanz']","['math.ST', 'stat.TH']",False,False,False,False,False,True
1338,2017-03-28T14:04:46Z,2017-03-09T17:18:21Z,http://arxiv.org/abs/1703.03353v1,http://arxiv.org/pdf/1703.03353v1,A Note on Bayesian Model Selection for Discrete Data Using Proper   Scoring Rules,note bayesian model select discret data use proper score rule,"We consider the problem of choosing between parametric models for a discrete observable, taking a Bayesian approach in which the within-model prior distributions are allowed to be improper. In order to avoid the ambiguity in the marginal likelihood function in such a case, we apply a homogeneous scoring rule. For the particular case of distinguishing between Poisson and Negative Binomial models, we conduct simulations that indicate that, applied prequentially, the method will consistently select the true model.",consid problem choos parametr model discret observ take bayesian approach within model prior distribut allow improp order avoid ambigu margin likelihood function case appli homogen score rule particular case distinguish poisson negat binomi model conduct simul indic appli prequenti method consist select true model,"['A. Philip Dawid', 'Monica Musio', 'Silvia Columbu']","['math.ST', 'stat.TH', 'Primary 62C99, secondary 62F15, 62A99']",False,False,True,False,False,True
1339,2017-03-28T14:04:46Z,2017-03-10T16:16:31Z,http://arxiv.org/abs/1703.03282v2,http://arxiv.org/pdf/1703.03282v2,Confidence intervals in high-dimensional regression based on regularized   pseudoinverses,confid interv high dimension regress base regular pseudoinvers,"In modern data sets, the number of available variables can greatly exceed the number of observations. In this paper we show how valid confidence intervals can be constructed by approximating the inverse covariance matrix by a scaled Moore-Penrose pseudoinverse, and using the lasso to perform a bias correction. In addition, we propose random least squares, a new regularization technique which yields narrower confidence intervals with the same theoretical validity. Random least squares estimates the inverse covariance matrix using multiple low-dimensional random projections of the data. This is shown to be equivalent to a generalized form of ridge regularization. The methods are illustrated in Monte Carlo experiments and an empirical example using quarterly data from the FRED-QD database, where gross domestic product is explained by a large number of macroeconomic and financial indicators.",modern data set number avail variabl great exceed number observ paper show valid confid interv construct approxim invers covari matrix scale moor penros pseudoinvers use lasso perform bias correct addit propos random least squar new regular techniqu yield narrow confid interv theoret valid random least squar estim invers covari matrix use multipl low dimension random project data shown equival general form ridg regular method illustr mont carlo experi empir exampl use quarter data fred qd databas gross domest product explain larg number macroeconom financi indic,"['Tom Boot', 'Didier Nibbering']","['math.ST', 'stat.TH']",False,False,False,False,False,True
1340,2017-03-28T14:04:50Z,2017-03-09T11:42:34Z,http://arxiv.org/abs/1703.03237v1,http://arxiv.org/pdf/1703.03237v1,Fractional compound Poisson processes with multiple internal states,fraction compound poisson process multipl intern state,"For the particles undergoing the anomalous diffusion with different waiting time distributions for different internal states, we derive the Fokker-Planck and Feymann-Kac equations, respectively, describing positions of the particles and functional distributions of the trajectories of particles; in particular, the equations governing the functional distribution of internal states are also obtained. The dynamics of the stochastic processes are analyzed and the applications, calculating the distribution of the first passage time and the distribution of the fraction of the occupation time, of the equations are given.",particl undergo anomal diffus differ wait time distribut differ intern state deriv fokker planck feymann kac equat respect describ posit particl function distribut trajectori particl particular equat govern function distribut intern state also obtain dynam stochast process analyz applic calcul distribut first passag time distribut fraction occup time equat given,"['Pengbo Xu', 'Weihua Deng']","['math.ST', 'cond-mat.stat-mech', 'stat.TH']",False,False,False,False,False,True
1341,2017-03-28T14:04:50Z,2017-03-09T07:40:53Z,http://arxiv.org/abs/1703.03167v1,http://arxiv.org/pdf/1703.03167v1,Cross-validation,cross valid,"This text is a survey on cross-validation. We define all classical cross-validation procedures, and we study their properties for two different goals: estimating the risk of a given estimator, and selecting the best estimator among a given family. For the risk estimation problem, we compute the bias (which can also be corrected) and the variance of cross-validation methods. For estimator selection, we first provide a first-order analysis (based on expectations). Then, we explain how to take into account second-order terms (from variance computations, and by taking into account the usefulness of overpenalization). This allows, in the end, to provide some guidelines for choosing the best cross-validation method for a given learning problem.",text survey cross valid defin classic cross valid procedur studi properti two differ goal estim risk given estim select best estim among given famili risk estim problem comput bias also correct varianc cross valid method estim select first provid first order analysi base expect explain take account second order term varianc comput take account use overpen allow end provid guidelin choos best cross valid method given learn problem,['Sylvain Arlot'],"['math.ST', 'stat.ML', 'stat.TH']",False,False,False,False,False,True
1342,2017-03-28T14:04:50Z,2017-03-09T07:27:33Z,http://arxiv.org/abs/1703.03165v1,http://arxiv.org/pdf/1703.03165v1,Perturbation Bootstrap in Adaptive Lasso,perturb bootstrap adapt lasso,"The Adaptive LASSO (ALASSO) was proposed by Zou [J. Amer. Statist. Assoc. 101 (2006) 1418-1429] as a modification of the LASSO for the purpose of simultaneous variable selection and estimation of the parameters in a linear regression model. Zou (2006) established that the ALASSO estimator is variable-selection consistent as well as asymptotically Normal in the indices corresponding to the nonzero regression coefficients in certain fixed-dimensional settings. In an influential paper, Minnier, Tian and Cai [J. Amer. Statist. Assoc. 106 (2011) 1371-1382] proposed a perturbation bootstrap method and established its distributional consistency for the ALASSO estimator in the fixed-dimensional setting. In this paper, however, we show that this (naive) perturbation bootstrap fails to achieve second order correctness in approximating the distribution of the ALASSO estimator. We propose a modification to the perturbation bootstrap objective function and show that a suitably studentized version of our modified perturbation bootstrap ALASSO estimator achieves second-order correctness even when the dimension of the model is allowed to grow to infinity with the sample size. As a consequence, inferences based on the modified perturbation bootstrap will be more accurate than the inferences based on the oracle Normal approximation. We give simulation studies demonstrating good finite-sample properties of our modified perturbation bootstrap method as well as an illustration of our method on a real data set.",adapt lasso alasso propos zou amer statist assoc modif lasso purpos simultan variabl select estim paramet linear regress model zou establish alasso estim variabl select consist well asymptot normal indic correspond nonzero regress coeffici certain fix dimension set influenti paper minnier tian cai amer statist assoc propos perturb bootstrap method establish distribut consist alasso estim fix dimension set paper howev show naiv perturb bootstrap fail achiev second order correct approxim distribut alasso estim propos modif perturb bootstrap object function show suitabl student version modifi perturb bootstrap alasso estim achiev second order correct even dimens model allow grow infin sampl size consequ infer base modifi perturb bootstrap accur infer base oracl normal approxim give simul studi demonstr good finit sampl properti modifi perturb bootstrap method well illustr method real data set,"['Debraj Das', 'Karl Gregory', 'S. N. Lahiri']","['stat.ME', 'math.ST', 'stat.TH']",False,False,False,False,False,True
1346,2017-03-28T14:04:50Z,2017-03-08T07:41:13Z,http://arxiv.org/abs/1703.02736v1,http://arxiv.org/pdf/1703.02736v1,Profile Estimation for Partial Functional Partially Linear Single-Index   Model,profil estim partial function partial linear singl index model,"This paper studies a \textit{partial functional partially linear single-index model} that consists of a functional linear component as well as a linear single-index component. This model generalizes many well-known existing models and is suitable for more complicated data structures. However, its estimation inherits the difficulties and complexities from both components and makes it a challenging problem, which calls for new methodology. We propose a novel profile B-spline method to estimate the parameters by approximating the unknown nonparametric link function in the single-index component part with B-spline, while the linear slope function in the functional component part is estimated by the functional principal component basis. The consistency and asymptotic normality of the parametric estimators are derived, and the global convergence of the proposed estimator of the linear slope function is also established. More excitingly, the latter convergence is optimal in the minimax sense. A two-stage procedure is implemented to estimate the nonparametric link function, and the resulting estimator possesses the optimal global rate of convergence. Furthermore, the convergence rate of the mean squared prediction error for a predictor is also obtained. Empirical properties of the proposed procedures are studied through Monte Carlo simulations. A real data example is also analyzed to illustrate the power and flexibility of the proposed methodology.",paper studi textit partial function partial linear singl index model consist function linear compon well linear singl index compon model general mani well known exist model suitabl complic data structur howev estim inherit difficulti complex compon make challeng problem call new methodolog propos novel profil spline method estim paramet approxim unknown nonparametr link function singl index compon part spline linear slope function function compon part estim function princip compon basi consist asymptot normal parametr estim deriv global converg propos estim linear slope function also establish excit latter converg optim minimax sens two stage procedur implement estim nonparametr link function result estim possess optim global rate converg furthermor converg rate mean squar predict error predictor also obtain empir properti propos procedur studi mont carlo simul real data exampl also analyz illustr power flexibl propos methodolog,"['Qingguo Tang', 'Linglong Kong', 'David Ruppert', 'Rohana J. Karunamuni']","['math.ST', 'stat.ME', 'stat.TH']",False,False,False,False,False,True
1347,2017-03-28T14:04:50Z,2017-03-08T06:22:56Z,http://arxiv.org/abs/1703.02724v1,http://arxiv.org/pdf/1703.02724v1,Guaranteed Tensor PCA with Optimality in Statistics and Computation,guarante tensor pca optim statist comput,"Tensors, or high-order arrays, attract much attention in recent research. In this paper, we propose a general framework for tensor principal component analysis (tensor PCA), which focuses on the methodology and theory for extracting the hidden low-rank structure from the high-dimensional tensor data. A unified solution is provided for tensor PCA with considerations in both statistical limits and computational costs. The problem exhibits three different phases according to the signal-noise-ratio (SNR). In particular, with strong SNR, we propose a fast spectral power iteration method that achieves the minimax optimal rate of convergence in estimation; with weak SNR, the information-theoretical lower bound shows that it is impossible to have consistent estimation in general; with moderate SNR, we show that the non-convex maximum likelihood estimation provides optimal solution, but with NP-hard computational cost; moreover, under the hardness hypothesis of hypergraphic planted clique detection, there are no polynomial-time algorithms performing consistently in general. Simulation studies show that the proposed spectral power iteration method have good performance under a variety of settings.",tensor high order array attract much attent recent research paper propos general framework tensor princip compon analysi tensor pca focus methodolog theori extract hidden low rank structur high dimension tensor data unifi solut provid tensor pca consider statist limit comput cost problem exhibit three differ phase accord signal nois ratio snr particular strong snr propos fast spectral power iter method achiev minimax optim rate converg estim weak snr inform theoret lower bound show imposs consist estim general moder snr show non convex maximum likelihood estim provid optim solut np hard comput cost moreov hard hypothesi hypergraph plant cliqu detect polynomi time algorithm perform consist general simul studi show propos spectral power iter method good perform varieti set,"['Anru Zhang', 'Dong Xia']","['math.ST', 'cs.LG', 'stat.ME', 'stat.ML', 'stat.TH']",False,False,True,False,False,True
1349,2017-03-28T14:04:50Z,2017-03-08T03:07:37Z,http://arxiv.org/abs/1703.02679v1,http://arxiv.org/pdf/1703.02679v1,Performance Bounds for Graphical Record Linkage,perform bound graphic record linkag,"Record linkage involves merging records in large, noisy databases to remove duplicate entities. It has become an important area because of its widespread occurrence in bibliometrics, public health, official statistics production, political science, and beyond. Traditional linkage methods directly linking records to one another are computationally infeasible as the number of records grows. As a result, it is increasingly common for researchers to treat record linkage as a clustering task, in which each latent entity is associated with one or more noisy database records. We critically assess performance bounds using the Kullback-Leibler (KL) divergence under a Bayesian record linkage framework, making connections to Kolchin partition models. We provide an upper bound using the KL divergence and a lower bound on the minimum probability of misclassifying a latent entity. We give insights for when our bounds hold using simulated data and provide practical user guidance.",record linkag involv merg record larg noisi databas remov duplic entiti becom import area becaus widespread occurr bibliometr public health offici statist product polit scienc beyond tradit linkag method direct link record one anoth comput infeas number record grow result increas common research treat record linkag cluster task latent entiti associ one noisi databas record critic assess perform bound use kullback leibler kl diverg bayesian record linkag framework make connect kolchin partit model provid upper bound use kl diverg lower bound minimum probabl misclassifi latent entiti give insight bound hold use simul data provid practic user guidanc,"['Rebecca C. Steorts', 'Matt Barnes', 'Willie Neiswanger']","['math.ST', 'cs.IT', 'math.IT', 'stat.ME', 'stat.ML', 'stat.TH']",False,False,False,False,False,True
1350,2017-03-28T14:04:54Z,2017-03-07T22:18:35Z,http://arxiv.org/abs/1703.02625v1,http://arxiv.org/pdf/1703.02625v1,On Sampling from Massive Graph Streams,sampl massiv graph stream,"We propose Graph Priority Sampling (GPS), a new paradigm for order-based reservoir sampling from massive streams of graph edges. GPS provides a general way to weight edge sampling according to auxiliary and/or size variables so as to accomplish various estimation goals of graph properties. In the context of subgraph counting, we show how edge sampling weights can be chosen so as to minimize the estimation variance of counts of specified sets of subgraphs. In distinction with many prior graph sampling schemes, GPS separates the functions of edge sampling and subgraph estimation. We propose two estimation frameworks: (1) Post-Stream estimation, to allow GPS to construct a reference sample of edges to support retrospective graph queries, and (2) In-Stream estimation, to allow GPS to obtain lower variance estimates by incrementally updating the subgraph count estimates during stream processing. Unbiasedness of subgraph estimators is established through a new Martingale formulation of graph stream order sampling, which shows that subgraph estimators, written as a product of constituent edge estimators are unbiased, even when computed at different points in the stream. The separation of estimation and sampling enables significant resource savings relative to previous work. We illustrate our framework with applications to triangle and wedge counting. We perform a large-scale experimental study on real-world graphs from various domains and types. GPS achieves high accuracy with less than 1% error for triangle and wedge counting, while storing a small fraction of the graph with average update times of a few microseconds per edge. Notably, for a large Twitter graph with more than 260M edges, GPS accurately estimates triangle counts with less than 1% error, while storing only 40K edges.",propos graph prioriti sampl gps new paradigm order base reservoir sampl massiv stream graph edg gps provid general way weight edg sampl accord auxiliari size variabl accomplish various estim goal graph properti context subgraph count show edg sampl weight chosen minim estim varianc count specifi set subgraph distinct mani prior graph sampl scheme gps separ function edg sampl subgraph estim propos two estim framework post stream estim allow gps construct refer sampl edg support retrospect graph queri stream estim allow gps obtain lower varianc estim increment updat subgraph count estim dure stream process unbiased subgraph estim establish new martingal formul graph stream order sampl show subgraph estim written product constitu edg estim unbias even comput differ point stream separ estim sampl enabl signific resourc save relat previous work illustr framework applic triangl wedg count perform larg scale experiment studi real world graph various domain type gps achiev high accuraci less error triangl wedg count store small fraction graph averag updat time microsecond per edg notabl larg twitter graph edg gps accur estim triangl count less error store onli edg,"['Nesreen K. Ahmed', 'Nick Duffield', 'Theodore Willke', 'Ryan A. Rossi']","['cs.SI', 'cs.DS', 'cs.IR', 'math.ST', 'stat.TH']",False,False,False,False,False,True
1353,2017-03-28T14:04:54Z,2017-03-13T15:44:35Z,http://arxiv.org/abs/1703.02307v2,http://arxiv.org/pdf/1703.02307v2,Post hoc inference via joint family-wise error rate control,post hoc infer via joint famili wise error rate control,"We introduce a general methodology for post hoc inference in a large-scale multiple testing framework. The approach is called "" user-agnostic "" in the sense that the statistical guarantee on the number of correct rejections holds for any set of candidate items selected by the user (after having seen the data). This task is investigated by defining a suitable criterion, named the joint-family-wise-error rate (JER for short). We propose several procedures for controlling the JER, with a special focus on incorporating dependencies while adapting to the unknown quantity of signal (via a step-down approach). We show that our proposed setting incorporates as particular cases a version of the higher criticism as well as the closed testing based approach of Goeman and Solari (2011). Our theoretical statements are supported by numerical experiments.",introduc general methodolog post hoc infer larg scale multipl test framework approach call user agnost sens statist guarante number correct reject hold ani set candid item select user seen data task investig defin suitabl criterion name joint famili wise error rate jer short propos sever procedur control jer special focus incorpor depend adapt unknown quantiti signal via step approach show propos set incorpor particular case version higher critic well close test base approach goeman solari theoret statement support numer experi,"['Gilles Blanchard', 'Pierre Neuvial', 'Etienne Roquain']","['math.ST', 'stat.TH']",False,False,True,False,False,True
1354,2017-03-28T14:04:54Z,2017-03-07T07:44:52Z,http://arxiv.org/abs/1703.02251v1,http://arxiv.org/pdf/1703.02251v1,The Maximum Likelihood Degree of Toric Varieties,maximum likelihood degre toric varieti,"We study the maximum likelihood degree (ML degree) of toric varieties, known as discrete exponential models in statistics. By introducing scaling coefficients to the monomial parameterization of the toric variety, one can change the ML degree. We show that the ML degree is equal to the degree of the toric variety for generic scalings, while it drops if and only if the scaling vector is in the locus of the principal $A$-determinant. We also illustrate how to compute the ML estimate of a toric variety numerically via homotopy continuation from a scaled toric variety with low ML degree. Throughout, we include examples motivated by algebraic geometry and statistics. We compute the ML degree of rational normal scrolls and a large class of Veronese-type varieties. In addition, we investigate the ML degree of scaled Segre varieties, hierarchical loglinear models, and graphical models.",studi maximum likelihood degre ml degre toric varieti known discret exponenti model statist introduc scale coeffici monomi parameter toric varieti one chang ml degre show ml degre equal degre toric varieti generic scale drop onli scale vector locus princip determin also illustr comput ml estim toric varieti numer via homotopi continu scale toric varieti low ml degre throughout includ exampl motiv algebra geometri statist comput ml degre ration normal scroll larg class verones type varieti addit investig ml degre scale segr varieti hierarch loglinear model graphic model,"['Carlos Améndola', 'Nathan Bliss', 'Isaac Burke', 'Courtney R. Gibbons', 'Martin Helmer', 'Serkan Hoşten', 'Evan D. Nash', 'Jose Israel Rodriguez', 'Daniel Smolkin']","['math.AG', 'math.ST', 'stat.CO', 'stat.TH', '14Q15, 14M25, 13P15, 62F10']",False,False,True,False,False,True
1355,2017-03-28T14:04:54Z,2017-03-06T15:03:55Z,http://arxiv.org/abs/1703.01913v1,http://arxiv.org/pdf/1703.01913v1,Near-Optimal Closeness Testing of Discrete Histogram Distributions,near optim close test discret histogram distribut,"We investigate the problem of testing the equivalence between two discrete histograms. A {\em $k$-histogram} over $[n]$ is a probability distribution that is piecewise constant over some set of $k$ intervals over $[n]$. Histograms have been extensively studied in computer science and statistics. Given a set of samples from two $k$-histogram distributions $p, q$ over $[n]$, we want to distinguish (with high probability) between the cases that $p = q$ and $\ p-q\ _1 \geq \epsilon$. The main contribution of this paper is a new algorithm for this testing problem and a nearly matching information-theoretic lower bound. Specifically, the sample complexity of our algorithm matches our lower bound up to a logarithmic factor, improving on previous work by polynomial factors in the relevant parameters. Our algorithmic approach applies in a more general setting and yields improved sample upper bounds for testing closeness of other structured distributions as well.",investig problem test equival two discret histogram em histogram probabl distribut piecewis constant set interv histogram extens studi comput scienc statist given set sampl two histogram distribut want distinguish high probabl case geq epsilon main contribut paper new algorithm test problem near match inform theoret lower bound specif sampl complex algorithm match lower bound logarithm factor improv previous work polynomi factor relev paramet algorithm approach appli general set yield improv sampl upper bound test close structur distribut well,"['Ilias Diakonikolas', 'Daniel M. Kane', 'Vladimir Nikishkin']","['cs.DS', 'cs.IT', 'cs.LG', 'math.IT', 'math.ST', 'stat.TH']",False,False,False,False,False,True
1356,2017-03-28T14:04:54Z,2017-03-06T09:25:09Z,http://arxiv.org/abs/1703.01777v1,http://arxiv.org/pdf/1703.01777v1,D-optimal design for multivariate polynomial regression via the   Christoffel function and semidefinite relaxations,optim design multivari polynomi regress via christoffel function semidefinit relax,We present a new approach to the design of D-optimal experiments with multivariate polynomial regressions on compact semi-algebraic design spaces. We apply the moment-sum-of-squares hierarchy of semidefinite programming problems to solve numerically and approximately the optimal design problem. The geometry of the design is recovered with semidefinite programming duality theory and the Christoffel polynomial.,present new approach design optim experi multivari polynomi regress compact semi algebra design space appli moment sum squar hierarchi semidefinit program problem solv numer approxim optim design problem geometri design recov semidefinit program dualiti theori christoffel polynomi,"['Yohann De Castro', 'F Gamboa', 'D Henrion', 'R Hess', 'J. -B Lasserre']","['math.ST', 'math.OC', 'stat.TH']",False,False,False,False,False,True
1357,2017-03-28T14:04:54Z,2017-03-06T04:31:36Z,http://arxiv.org/abs/1703.01721v1,http://arxiv.org/pdf/1703.01721v1,The Bennett-Orlicz norm,bennett orlicz norm,"Lederer and van de Geer (2013) introduced a new Orlicz norm, the Bernstein-Orlicz norm, which is connected to Bernstein type inequalities. Here we introduce another Orlicz norm, the Bennett-Orlicz norm, which is connected to Bennett type inequalities. The new Bennett-Orlicz norm yields inequalities for expectations of maxima which are potentially somewhat tighter than those resulting from the Bernstein-Orlicz norm when they are both applicable. We discuss cross connections between these norms, exponential inequalities of the Bernstein, Bennett, and Prokhorov types, and make comparisons with results of Talagrand (1989, 1994), and Boucheron, Lugosi, and Massart (2013).",leder van de geer introduc new orlicz norm bernstein orlicz norm connect bernstein type inequ introduc anoth orlicz norm bennett orlicz norm connect bennett type inequ new bennett orlicz norm yield inequ expect maxima potenti somewhat tighter result bernstein orlicz norm applic discuss cross connect norm exponenti inequ bernstein bennett prokhorov type make comparison result talagrand boucheron lugosi massart,['Jon A. Wellner'],"['math.ST', 'stat.TH', '60E15, 62E17, 62H10, 46E30']",False,False,True,False,False,True
1359,2017-03-28T14:04:54Z,2017-03-05T19:37:52Z,http://arxiv.org/abs/1703.01654v1,http://arxiv.org/pdf/1703.01654v1,"À propos des tentatives visant à construire un estimateur   ""universel"" à partir de données indépendantes",propo des tentat visant construir un estimateur universel partir de donn es ind pendant,"This paper is based on our personal notes for the short course we gave on January 5, 2017 at Institut Henri Poincar\'e, after an invitation of the SFdS. Our purpose is to give an overview of the method of $\rho$-estimation and of the optimality and robustness properties of the estimators built according to this procedure. This method can be viewed as the sequel of a long series of researches which were devoted to the construction of estimators with good properties in various statistical frameworks. We shall emphasize the connection between the $\rho$-estimators and the previous ones, in particular the maximum likelihood estimator, and we shall show, via some typical examples, that the $\rho$-estimators perform better from various points of view.   ------   Cet article est fond\'e sur les notes du mini-cours que nous avons donn\'e le 5 janvier 2017 \`a l'Institut Henri Poincar\'e \`a l'occasion d'une journ\'ee organis\'ee par la SFdS et consacr\'ee \`a la Statistique Math\'ematique. Il vise \`a donner un aper\c{c}u de la m\'ethode de $\rho$-estimation ainsi que des propri\'et\'es d'optimalit\'e et de robustesse des estimateurs construits selon cette proc\'edure. Cette m\'ethode s'inscrit dans une longue lign\'ee de recherches dont l'objectif a \'et\'e de produire des estimateurs poss\'edant de bonnes propri\'et\'es pour un ensemble de cadres statistiques aussi vaste que possible. Nous mettrons en lumi\`ere les liens forts qui existent entre les $\rho$-estimateurs et ces pr\'ed\'ecesseurs, notamment les estimateurs du maximum de vraisemblance, mais montrerons \'egalement, au travers d'exemples choisis, que les $\rho$-estimateurs les surpassent sur bien des aspects.",paper base person note short cours gave januari institut henri poincar invit sfds purpos give overview method rho estim optim robust properti estim built accord procedur method view sequel long seri research devot construct estim good properti various statist framework shall emphas connect rho estim previous one particular maximum likelihood estim shall show via typic exampl rho estim perform better various point view cet articl est fond sur les note du mini cour que nous avon donn le janvier institut henri poincar occas une journ ee organi ee par la sfds et consacr ee la statistiqu math ematiqu il vise donner un aper de la ethod de rho estim ainsi que des propri et es optimalit et de robustess des estimateur construit selon cett proc edur cett ethod inscrit dan une longu lign ee de recherch dont objectif et de produir des estimateur poss edant de bonn propri et es pour un ensembl de cadr statistiqu aussi vast que possibl nous mettron en lumi ere les lien fort qui exist entr les rho estimateur et ces pr ed ecesseur notam les estimateur du maximum de vraisembl mai montreron egal au traver exempl choisi que les rho estimateur les surpass sur bien des aspect,"['Yannick Baraud', 'Lucien Birgé']","['math.ST', 'stat.TH', '62G05']",False,False,True,False,False,True
1360,2017-03-28T14:04:58Z,2017-03-04T22:15:27Z,http://arxiv.org/abs/1703.01527v1,http://arxiv.org/pdf/1703.01527v1,Power Allocation for Full-Duplex Relay Selection in Underlay Cognitive   Radio Networks: Coherent versus Non-Coherent Scenarios,power alloc full duplex relay select underlay cognit radio network coher versus non coher scenario,"This paper investigates power control and relay selection in Full Duplex Cognitive Relay Networks (FDCRNs), where the secondary-user (SU) relays can simultaneously receive data from the SU source and forward them to the SU destination. We study both non-coherent and coherent scenarios. In the non-coherent case, the SU relay forwards the signal from the SU source without regulating the phase; while in the coherent scenario, the SU relay regulates the phase when forwarding the signal to minimize the interference at the primary-user (PU) receiver. We consider the problem of maximizing the transmission rate from the SU source to the SU destination subject to the interference constraint at the PU receiver and power constraints at both the SU source and SU relay. We then develop a mathematical model to analyze the data rate performance of the FDCRN considering the self-interference effects at the FD relay. We develop low-complexity and high-performance joint power control and relay selection algorithms. Extensive numerical results are presented to illustrate the impacts of power level parameters and the self-interference cancellation quality on the rate performance. Moreover, we demonstrate the significant gain of phase regulation at the SU relay.",paper investig power control relay select full duplex cognit relay network fdcrns secondari user su relay simultan receiv data su sourc forward su destin studi non coher coher scenario non coher case su relay forward signal su sourc without regul phase coher scenario su relay regul phase forward signal minim interfer primari user pu receiv consid problem maxim transmiss rate su sourc su destin subject interfer constraint pu receiv power constraint su sourc su relay develop mathemat model analyz data rate perform fdcrn consid self interfer effect fd relay develop low complex high perform joint power control relay select algorithm extens numer result present illustr impact power level paramet self interfer cancel qualiti rate perform moreov demonstr signific gain phase regul su relay,"['Le Thanh Tan', 'Lei Ying', 'Daniel W. Bliss']","['cs.IT', 'cs.NI', 'math.IT', 'math.ST', 'stat.TH']",False,False,False,False,False,True
1361,2017-03-28T14:04:58Z,2017-03-04T21:48:41Z,http://arxiv.org/abs/1703.01525v1,http://arxiv.org/pdf/1703.01525v1,Power Control and Relay Selection in Full-Duplex Cognitive Relay   Networks: Coherent versus Non-coherent Scenarios,power control relay select full duplex cognit relay network coher versus non coher scenario,"This paper investigates power control and relay selection in Full Duplex Cognitive Relay Networks (FDCRNs), where the secondary-user (SU) relays can simultaneously receive and forward the signal from the SU source. We study both non-coherent and coherent scenarios. In the non-coherent case, the SU relay forwards the signal from the SU source without regulating the phase, while in the coherent scenario, the SU relay regulates the phase when forwarding the signal to minimize the interference at the primary-user (PU) receiver. We consider the problem of maximizing the transmission rate from the SU source to the SU destination subject to the interference constraint at the PU receiver and power constraints at both the SU source and SU relay. We develop low-complexity and high-performance joint power control and relay selection algorithms. The superior performance of the proposed algorithms are confirmed using extensive numerical evaluation. In particular, we demonstrate the significant gain of phase regulation at the SU relay (i.e., the gain of the coherent mechanism over the noncoherent mechanism).",paper investig power control relay select full duplex cognit relay network fdcrns secondari user su relay simultan receiv forward signal su sourc studi non coher coher scenario non coher case su relay forward signal su sourc without regul phase coher scenario su relay regul phase forward signal minim interfer primari user pu receiv consid problem maxim transmiss rate su sourc su destin subject interfer constraint pu receiv power constraint su sourc su relay develop low complex high perform joint power control relay select algorithm superior perform propos algorithm confirm use extens numer evalu particular demonstr signific gain phase regul su relay gain coher mechan noncoher mechan,"['Le Thanh Tan', 'Lei Ying', 'Daniel W. Bliss']","['cs.IT', 'cs.NI', 'math.IT', 'math.ST', 'stat.TH']",False,False,False,False,False,True
1362,2017-03-28T14:04:58Z,2017-03-04T15:13:41Z,http://arxiv.org/abs/1703.01474v1,http://arxiv.org/pdf/1703.01474v1,Sharp bounds for population recovery,sharp bound popul recoveri,"The population recovery problem is a basic problem in noisy unsupervised learning that has attracted significant research attention in recent years [WY12,DRWY12, MS13, BIMP13, LZ15,DST16]. A number of different variants of this problem have been studied, often under assumptions on the unknown distribution (such as that it has restricted support size). In this work we study the sample complexity and algorithmic complexity of the most general version of the problem, under both bit-flip noise and erasure noise model. We give essentially matching upper and lower sample complexity bounds for both noise models, and efficient algorithms matching these sample complexity bounds up to polynomial factors.",popul recoveri problem basic problem noisi unsupervis learn attract signific research attent recent year wy drwi ms bimp lz dst number differ variant problem studi often assumpt unknown distribut restrict support size work studi sampl complex algorithm complex general version problem bit flip nois erasur nois model give essenti match upper lower sampl complex bound nois model effici algorithm match sampl complex bound polynomi factor,"['Anindya De', ""Ryan O'Donnell"", 'Rocco Servedio']","['cs.DS', 'cs.LG', 'math.ST', 'stat.TH']",False,False,False,False,False,True
1363,2017-03-28T14:04:58Z,2017-03-04T09:12:42Z,http://arxiv.org/abs/1703.01421v1,http://arxiv.org/pdf/1703.01421v1,$l_0$-estimation of piecewise-constant signals on graphs,estim piecewis constant signal graph,"We study recovery of piecewise-constant signals over arbitrary graphs by the estimator minimizing an $l_0$-edge-penalized objective. Although exact minimization of this objective may be computationally intractable, we show that the same statistical risk guarantees are achieved by the alpha-expansion algorithm which approximately minimizes this objective in polynomial time. We establish that for graphs with small average vertex degree, these guarantees are rate-optimal in a minimax sense over classes of edge-sparse signals. For application to spatially inhomogeneous graphs, we propose minimization of an edge-weighted variant of this objective where each edge is weighted by its effective resistance or another measure of its contribution to the graph's connectivity. We establish minimax optimality of the resulting estimators over corresponding edge-weighted sparsity classes. We show theoretically that these risk guarantees are not always achieved by the estimator minimizing the $l_1$/total-variation relaxation, and empirically that the $l_0$-based estimates are more accurate in high signal-to-noise settings.",studi recoveri piecewis constant signal arbitrari graph estim minim edg penal object although exact minim object may comput intract show statist risk guarante achiev alpha expans algorithm approxim minim object polynomi time establish graph small averag vertex degre guarante rate optim minimax sens class edg spars signal applic spatial inhomogen graph propos minim edg weight variant object edg weight effect resist anoth measur contribut graph connect establish minimax optim result estim correspond edg weight sparsiti class show theoret risk guarante alway achiev estim minim total variat relax empir base estim accur high signal nois set,"['Zhou Fan', 'Leying Guan']","['stat.ME', 'math.ST', 'stat.CO', 'stat.TH']",False,False,False,False,False,True
1365,2017-03-28T14:04:58Z,2017-03-14T22:44:25Z,http://arxiv.org/abs/1703.01332v2,http://arxiv.org/pdf/1703.01332v2,Optimistic lower bounds for convex regularized least-squares,optimist lower bound convex regular least squar,"Minimax lower bounds are pessimistic in nature: for any given estimator, minimax lower bounds yield the existence of a worst-case target vector $\beta^*_{worst}$ for which the prediction error of the given estimator is bounded from below. However, minimax lower bounds shed no light on the prediction error of the given estimator for target vectors different than $\beta^*_{worst}$. A characterization of the prediction error of any convex regularized least-squares is given. This characterization provide both a lower bound and an upper bound on the prediction error. This produces lower bounds that are applicable for any target vector and not only for a single, worst-case $\beta^*_{worst}$. Finally, these lower and upper bounds on the prediction error are applied to the Lasso is sparse linear regression. We obtain a lower bound involving the compatibility constant for any tuning parameter, matching upper and lower bounds for the universal choice of the tuning parameter, and a lower bound for the Lasso with small tuning parameter.",minimax lower bound pessimist natur ani given estim minimax lower bound yield exist worst case target vector beta worst predict error given estim bound howev minimax lower bound shed light predict error given estim target vector differ beta worst character predict error ani convex regular least squar given character provid lower bound upper bound predict error produc lower bound applic ani target vector onli singl worst case beta worst final lower upper bound predict error appli lasso spars linear regress obtain lower bound involv compat constant ani tune paramet match upper lower bound univers choic tune paramet lower bound lasso small tune paramet,['Pierre C Bellec'],"['math.ST', 'stat.TH']",False,False,False,False,False,True
1366,2017-03-28T14:04:58Z,2017-03-03T20:14:59Z,http://arxiv.org/abs/1703.01326v1,http://arxiv.org/pdf/1703.01326v1,Prediction based on the Kennedy-O'Hagan calibration model: asymptotic   consistency and other properties,predict base kennedi hagan calibr model asymptot consist properti,"Kennedy and O'Hagan (2001) propose a model for calibrating some unknown parameters in a computer model and estimating the discrepancy between the computer output and physical response. This model is known to have certain identifiability issues. Tuo and Wu (2016) show that there are examples for which the Kennedy-O'Hagan method renders unreasonable results in calibration. In spite of its unstable performance in calibration, the Kennedy-O'Hagan approach has a more robust behavior in predicting the physical response. In this work, we present some theoretical analysis to show the consistency of predictor based on their calibration model in the context of radial basis functions.",kennedi hagan propos model calibr unknown paramet comput model estim discrep comput output physic respons model known certain identifi issu tuo wu show exampl kennedi hagan method render unreason result calibr spite unstabl perform calibr kennedi hagan approach robust behavior predict physic respons work present theoret analysi show consist predictor base calibr model context radial basi function,"['Rui Tuo', 'C. F. Jeff Wu']","['math.ST', 'stat.TH']",False,False,False,False,False,True
1367,2017-03-28T14:04:58Z,2017-03-07T08:29:02Z,http://arxiv.org/abs/1703.01232v2,http://arxiv.org/pdf/1703.01232v2,Inconsistency of Template Estimation with the Fr{é}chet mean in   Quotient Space,inconsist templat estim fr chet mean quotient space,"We tackle the problem of template estimation when data have been randomly transformed under an isometric group action in the presence of noise. In order to estimate the template, one often minimizes the variance when the influence of the transformations have been removed (computation of the Fr{\'e}chet mean in quotient space). The consistency bias is defined as the distance (possibly zero) between the orbit of the template and the orbit of one element which minimizes the variance. In this article we establish an asymptotic behavior of the consistency bias with respect to the noise level. This behavior is linear with respect to the noise level. As a result the inconsistency is unavoidable as soon as the noise is large enough. In practice, the template estimation with a finite sample is often done with an algorithm called max-max. We show the convergence of this algorithm to an empirical Karcher mean. Finally, our numerical experiments show that the bias observed in practice cannot be attributed to the small sample size or to a convergence problem but is indeed due to the previously studied inconsistency.",tackl problem templat estim data random transform isometr group action presenc nois order estim templat one often minim varianc influenc transform remov comput fr chet mean quotient space consist bias defin distanc possibl zero orbit templat orbit one element minim varianc articl establish asymptot behavior consist bias respect nois level behavior linear respect nois level result inconsist unavoid soon nois larg enough practic templat estim finit sampl often done algorithm call max max show converg algorithm empir karcher mean final numer experi show bias observ practic cannot attribut small sampl size converg problem inde due previous studi inconsist,"['Loïc Devilliers', 'Xavier Pennec', 'Stéphanie Allassonnière']","['math.ST', 'stat.TH']",False,False,False,False,False,True
1368,2017-03-28T14:04:58Z,2017-03-02T19:05:00Z,http://arxiv.org/abs/1703.00918v1,http://arxiv.org/pdf/1703.00918v1,A note on conditional covariance matrices for elliptical distributions,note condit covari matric ellipt distribut,"In this short note we provide an analytical formula for the conditional covariance matrices of the elliptically distributed random vectors, when the conditioning is based on the values of any linear combination of the marginal random variables. We show that one could introduce the univariate invariant depending solely on the conditioning set, which greatly simplifies the calculations. As an application, we show that one could define uniquely defined quantile-based sets on which conditional covariance matrices must be equal to each other if only the vector is multivariate normal. The similar results are obtained for conditional correlation matrices of the general elliptic case.",short note provid analyt formula condit covari matric ellipt distribut random vector condit base valu ani linear combin margin random variabl show one could introduc univari invari depend sole condit set great simplifi calcul applic show one could defin uniqu defin quantil base set condit covari matric must equal onli vector multivari normal similar result obtain condit correl matric general ellipt case,"['Piotr Jaworski', 'Marcin Pitera']","['math.PR', 'math.ST', 'q-fin.RM', 'stat.TH', '62H05, 60E05']",False,False,False,False,False,True
1369,2017-03-28T14:04:58Z,2017-03-02T17:48:18Z,http://arxiv.org/abs/1703.00871v1,http://arxiv.org/pdf/1703.00871v1,Bootstrap confidence sets for spectral projectors of sample covariance,bootstrap confid set spectral projector sampl covari,"Let $X_{1},\ldots,X_{n}$ be i.i.d. sample in $\mathbb{R}^{p}$ with zero mean and the covariance matrix $\mathbf{\Sigma}$. The problem of recovering the projector onto an eigenspace of $\mathbf{\Sigma}$ from these observations naturally arises in many applications. Recent technique from [Koltchinskii, Lounici, 2015] helps to study the asymptotic distribution of the distance in the Frobenius norm $\  \mathbf{P}_r - \widehat{\mathbf{P}}_r \ _{2}$ between the true projector $\mathbf{P}_r$ on the subspace of the $r$-th eigenvalue and its empirical counterpart $\widehat{\mathbf{P}}_r$ in terms of the effective rank of $\mathbf{\Sigma}$. This paper offers a bootstrap procedure for building sharp confidence sets for the true projector $\mathbf{P}_r$ from the given data. This procedure does not rely on the asymptotic distribution of $\  \mathbf{P}_r - \widehat{\mathbf{P}}_r \ _{2}$ and its moments. It could be applied for small or moderate sample size $n$ and large dimension $p$. The main result states the validity of the proposed procedure for finite samples with an explicit error bound for the error of bootstrap approximation. This bound involves some new sharp results on Gaussian comparison and Gaussian anti-concentration in high-dimensional spaces. Numeric results confirm a good performance of the method in realistic examples.",let ldot sampl mathbb zero mean covari matrix mathbf sigma problem recov projector onto eigenspac mathbf sigma observ natur aris mani applic recent techniqu koltchinskii lounici help studi asymptot distribut distanc frobenius norm mathbf widehat mathbf true projector mathbf subspac th eigenvalu empir counterpart widehat mathbf term effect rank mathbf sigma paper offer bootstrap procedur build sharp confid set true projector mathbf given data procedur doe reli asymptot distribut mathbf widehat mathbf moment could appli small moder sampl size larg dimens main result state valid propos procedur finit sampl explicit error bound error bootstrap approxim bound involv new sharp result gaussian comparison gaussian anti concentr high dimension space numer result confirm good perform method realist exampl,"['Alexey Naumov', 'Vladimir Spokoiny', 'Vladimir Ulyanov']","['math.ST', 'math.PR', 'stat.TH']",False,False,False,False,False,True
1371,2017-03-28T14:05:02Z,2017-03-01T23:08:33Z,http://arxiv.org/abs/1703.00542v1,http://arxiv.org/pdf/1703.00542v1,A note on the approximate admissibility of regularized estimators in the   Gaussian sequence model,note approxim admiss regular estim gaussian sequenc model,"We study the problem of estimating an unknown vector $\theta$ from an observation $X$ drawn according to the normal distribution with mean $\theta$ and identity covariance matrix under the knowledge that $\theta$ belongs to a known closed convex set $\Theta$. In this general setting, Chatterjee (2014) proved that the natural constrained least squares estimator is ""approximately admissible"" for every $\Theta$. We extend this result by proving that the same property holds for all convex penalized estimators as well. Moreover, we simplify and shorten the original proof considerably. We also provide explicit upper and lower bounds for the universal constant underlying the notion of approximate admissibility.",studi problem estim unknown vector theta observ drawn accord normal distribut mean theta ident covari matrix knowledg theta belong known close convex set theta general set chatterje prove natur constrain least squar estim approxim admiss everi theta extend result prove properti hold convex penal estim well moreov simplifi shorten origin proof consider also provid explicit upper lower bound univers constant notion approxim admiss,"['Xi Chen', 'Adityanand Guntuboyina', 'Yuchen Zhang']","['math.ST', 'stat.TH']",False,False,False,False,False,True
1372,2017-03-28T14:05:02Z,2017-03-01T22:53:13Z,http://arxiv.org/abs/1703.00539v1,http://arxiv.org/pdf/1703.00539v1,Learning Determinantal Point Processes with Moments and Cycles,learn determinant point process moment cycl,"Determinantal Point Processes (DPPs) are a family of probabilistic models that have a repulsive behavior, and lend themselves naturally to many tasks in machine learning where returning a diverse set of objects is important. While there are fast algorithms for sampling, marginalization and conditioning, much less is known about learning the parameters of a DPP. Our contribution is twofold: (i) we establish the optimal sample complexity achievable in this problem and show that it is governed by a natural parameter, which we call the \emph{cycle sparsity}; (ii) we propose a provably fast combinatorial algorithm that implements the method of moments efficiently and achieves optimal sample complexity. Finally, we give experimental results that confirm our theoretical findings.",determinant point process dpps famili probabilist model repuls behavior lend themselv natur mani task machin learn return divers set object import fast algorithm sampl margin condit much less known learn paramet dpp contribut twofold establish optim sampl complex achiev problem show govern natur paramet call emph cycl sparsiti ii propos provabl fast combinatori algorithm implement method moment effici achiev optim sampl complex final give experiment result confirm theoret find,"['John Urschel', 'Victor-Emmanuel Brunel', 'Ankur Moitra', 'Philippe Rigollet']","['math.ST', 'stat.TH', '62M30, 60G55, 62C20, 05C38']",False,False,False,False,False,True
1373,2017-03-28T14:05:02Z,2017-03-01T19:25:14Z,http://arxiv.org/abs/1703.00471v1,http://arxiv.org/pdf/1703.00471v1,Multidimensional Sampling of Isotropically Bandlimited Signals,multidimension sampl isotrop bandlimit signal,"A new lower bound on the average reconstruction error variance of multidimensional sampling and reconstruction is presented. It applies to sampling on arbitrary lattices in arbitrary dimensions, assuming a stochastic process with constant, isotropically bandlimited spectrum and reconstruction by the best linear interpolator. The lower bound is exact for any lattice at sufficiently high and low sampling rates. The two threshold rates where the error variance deviates from the lower bound gives two optimality criteria for sampling lattices. It is proved that at low rates, near the first threshold, the optimal lattice is the dual of the best sphere-covering lattice, which for the first time establishes a rigorous relation between optimal sampling and optimal sphere covering. A previously known result is confirmed at high rates, near the second threshold, namely, that the optimal lattice is the dual of the best sphere-packing lattice. Numerical results quantify the performance of various lattices for sampling and support the theoretical optimality criteria.",new lower bound averag reconstruct error varianc multidimension sampl reconstruct present appli sampl arbitrari lattic arbitrari dimens assum stochast process constant isotrop bandlimit spectrum reconstruct best linear interpol lower bound exact ani lattic suffici high low sampl rate two threshold rate error varianc deviat lower bound give two optim criteria sampl lattic prove low rate near first threshold optim lattic dual best sphere cover lattic first time establish rigor relat optim sampl optim sphere cover previous known result confirm high rate near second threshold name optim lattic dual best sphere pack lattic numer result quantifi perform various lattic sampl support theoret optim criteria,"['Erik Agrell', 'Balázs Csébfalvi']","['cs.IT', 'math.IT', 'math.ST', 'stat.TH']",False,False,False,False,False,True
1374,2017-03-28T14:05:02Z,2017-03-01T19:08:00Z,http://arxiv.org/abs/1703.00469v1,http://arxiv.org/pdf/1703.00469v1,Confidence Bands for Coefficients in High Dimensional Linear Models with   Error-in-variables,confid band coeffici high dimension linear model error variabl,"We study high-dimensional linear models with error-in-variables. Such models are motivated by various applications in econometrics, finance and genetics. These models are challenging because of the need to account for measurement errors to avoid non-vanishing biases in addition to handle the high dimensionality of the parameters. A recent growing literature has proposed various estimators that achieve good rates of convergence. Our main contribution complements this literature with the construction of simultaneous confidence regions for the parameters of interest in such high-dimensional linear models with error-in-variables.   These confidence regions are based on the construction of moment conditions that have an additional orthogonal property with respect to nuisance parameters. We provide a construction that requires us to estimate an additional high-dimensional linear model with error-in-variables for each component of interest. We use a multiplier bootstrap to compute critical values for simultaneous confidence intervals for a subset $S$ of the components. We show its validity despite of possible model selection mistakes, and allowing for the cardinality of $S$ to be larger than the sample size.   We apply and discuss the implications of our results to two examples and conduct Monte Carlo simulations to illustrate the performance of the proposed procedure.",studi high dimension linear model error variabl model motiv various applic econometr financ genet model challeng becaus need account measur error avoid non vanish bias addit handl high dimension paramet recent grow literatur propos various estim achiev good rate converg main contribut complement literatur construct simultan confid region paramet interest high dimension linear model error variabl confid region base construct moment condit addit orthogon properti respect nuisanc paramet provid construct requir us estim addit high dimension linear model error variabl compon interest use multipli bootstrap comput critic valu simultan confid interv subset compon show valid despit possibl model select mistak allow cardin larger sampl size appli discuss implic result two exampl conduct mont carlo simul illustr perform propos procedur,"['Alexandre Belloni', 'Victor Chernozhukov', 'Abhishek Kaul']","['math.ST', 'stat.TH']",False,False,False,False,False,True
1375,2017-03-28T14:05:02Z,2017-03-01T15:49:03Z,http://arxiv.org/abs/1703.00353v1,http://arxiv.org/pdf/1703.00353v1,Matrix product moments in normal variables,matrix product moment normal variabl,"Let ${\cal X }=XX^{\prime}$ be a random matrix associated with a centered $r$-column centered Gaussian vector $X$ with a covariance matrix $P$. In this article we compute expectations of matrix-products of the form $\prod_{1\leq i\leq n}({\cal X } P^{v_i})$ for any $n\geq 1$ and any multi-index parameters $v_i\in\mathbb{N}$. We derive closed form formulae and a simple sequential algorithm to compute these matrices w.r.t. the parameter $n$. The second part of the article is dedicated to a non commutative binomial formula for the central matrix-moments $\mathbb{E}\left(\left[{\cal X }-P\right]^n\right)$. The matrix product moments discussed in this study are expressed in terms of polynomial formulae w.r.t. the powers of the covariance matrix, with coefficients depending on the trace of these matrices. We also derive a series of estimates w.r.t. the Loewner order on quadratic forms. For instance we shall prove the rather crude estimate $\mathbb{E}\left(\left[{\cal X }-P\right]^n\right)\leq \mathbb{E}\left({\cal X }^n-P^n\right)$, for any $n\geq 1$",let cal xx prime random matrix associ center column center gaussian vector covari matrix articl comput expect matrix product form prod leq leq cal ani geq ani multi index paramet mathbb deriv close form formula simpl sequenti algorithm comput matric paramet second part articl dedic non commut binomi formula central matrix moment mathbb left left cal right right matrix product moment discuss studi express term polynomi formula power covari matrix coeffici depend trace matric also deriv seri estim loewner order quadrat form instanc shall prove rather crude estim mathbb left left cal right right leq mathbb left cal right ani geq,"['Pierre Del Moral', 'Adrian N. Bishop']","['math.ST', 'stat.TH', '15B52, 60B20, 46L53, 05A10']",False,False,False,False,False,True
1376,2017-03-28T14:05:02Z,2017-03-01T15:03:04Z,http://arxiv.org/abs/1703.00329v1,http://arxiv.org/pdf/1703.00329v1,Convergence rate of a simulated annealing algorithm with noisy   observations,converg rate simul anneal algorithm noisi observ,"In this paper we propose a modified version of the simulated annealing algorithm for solving a stochastic global optimization problem. More precisely, we address the problem of finding a global minimizer of a function with noisy evaluations. We provide a rate of convergence and its optimized parametrization to ensure a minimal number of evaluations for a given accuracy and a confidence level close to 1. This work is completed with a set of numerical experimentations and assesses the practical performance both on benchmark test cases and on real world examples.",paper propos modifi version simul anneal algorithm solv stochast global optim problem precis address problem find global minim function noisi evalu provid rate converg optim parametr ensur minim number evalu given accuraci confid level close work complet set numer experiment assess practic perform benchmark test case real world exampl,"['Clément Bouttier', 'Ioana Gavra']","['stat.ML', 'math.OC', 'math.ST', 'stat.TH']",False,False,False,False,False,True
1377,2017-03-28T14:05:02Z,2017-03-01T07:57:26Z,http://arxiv.org/abs/1703.00167v1,http://arxiv.org/pdf/1703.00167v1,Adaptive estimation of the sparsity in the Gaussian vector model,adapt estim sparsiti gaussian vector model,"Consider the Gaussian vector model with mean value {\theta}. We study the twin problems of estimating the number  {\theta} _0 of non-zero components of {\theta} and testing whether  {\theta} _0 is smaller than some value. For testing, we establish the minimax separation distances for this model and introduce a minimax adaptive test. Extensions to the case of unknown variance are also discussed. Rewriting the estimation of  {\theta} _0 as a multiple testing problem of all hypotheses { {\theta} _0 <= q}, we both derive a new way of assessing the optimality of a sparsity estimator and we exhibit such an optimal procedure. This general approach provides a roadmap for estimating the complexity of the signal in various statistical models.",consid gaussian vector model mean valu theta studi twin problem estim number theta non zero compon theta test whether theta smaller valu test establish minimax separ distanc model introduc minimax adapt test extens case unknown varianc also discuss rewrit estim theta multipl test problem hypothes theta deriv new way assess optim sparsiti estim exhibit optim procedur general approach provid roadmap estim complex signal various statist model,"['Alexandra Carpentier', 'Nicolas Verzelen']","['math.ST', 'stat.TH', '62C20, 62G10, 62B10']",False,False,False,False,False,True
1379,2017-03-28T14:05:02Z,2017-02-28T18:28:07Z,http://arxiv.org/abs/1702.08895v1,http://arxiv.org/pdf/1702.08895v1,Minimax density estimation for growing dimension,minimax densiti estim grow dimens,"This paper presents minimax rates for density estimation when the data dimension $d$ is allowed to grow with the number of observations $n$ rather than remaining fixed as in previous analyses. We prove a non-asymptotic lower bound which gives the worst-case rate over standard classes of smooth densities, and we show that kernel density estimators achieve this rate. We also give oracle choices for the bandwidth and derive the fastest rate $d$ can grow with $n$ to maintain estimation consistency.",paper present minimax rate densiti estim data dimens allow grow number observ rather remain fix previous analys prove non asymptot lower bound give worst case rate standard class smooth densiti show kernel densiti estim achiev rate also give oracl choic bandwidth deriv fastest rate grow maintain estim consist,['Daniel J. McDonald'],"['math.ST', 'stat.TH']",False,False,False,False,False,True
1380,2017-03-28T14:05:06Z,2017-02-28T13:50:56Z,http://arxiv.org/abs/1702.08787v1,http://arxiv.org/pdf/1702.08787v1,Compound Poisson approximation to estimate the Lévy density,compound poisson approxim estim vy densiti,"We construct an estimator of the L\'evy density, with respect to the Lebesgue measure, of a pure jump L\'evy process from high frequency observations: we observe one trajectory of the L\'evy process over [0, T] at the sampling rate $\Delta$, where $\Delta$ $\rightarrow$ 0 as T $\rightarrow$ $\infty$. The main novelty of our result is that we directly estimate the L\'evy density in cases where the process may present infinite activity. Moreover, we study the risk of the estimator with respect to L\_p loss functions, 1 $\le$ p \textless{} $\infty$, whereas existing results only focus on p $\in$ {2, $\infty$}. The main idea behind the estimation procedure that we propose is to use that ""every infinitely divisible distribution is the limit of a sequence of compound Poisson distributions"" (see e.g. Corollary 8.8 in Sato (1999)) and to take advantage of the fact that it is well known how to estimate the L\'evy density of a compound Poisson process in the high frequency setting. We consider linear wavelet estimators and the performance of our procedure is studied in term of L\_p loss functions, p $\ge$ 1, over Besov balls. The results are illustrated on several examples.",construct estim evi densiti respect lebesgu measur pure jump evi process high frequenc observ observ one trajectori evi process sampl rate delta delta rightarrow rightarrow infti main novelti result direct estim evi densiti case process may present infinit activ moreov studi risk estim respect loss function le textless infti wherea exist result onli focus infti main idea behind estim procedur propos use everi infinit divis distribut limit sequenc compound poisson distribut see corollari sato take advantag fact well known estim evi densiti compound poisson process high frequenc set consid linear wavelet estim perform procedur studi term loss function ge besov ball result illustr sever exampl,"['Céline Duval', 'Ester Mariucci']","['math.PR', 'math.ST', 'stat.TH']",False,False,False,False,False,True
1381,2017-03-28T14:05:06Z,2017-02-28T02:54:56Z,http://arxiv.org/abs/1702.08615v1,http://arxiv.org/pdf/1702.08615v1,Bridging Finite and Super Population Causal Inference,bridg finit super popul causal infer,"There are two general views in causal analysis of experimental data: the super population view that the units are an independent sample from some hypothetical infinite populations, and the finite population view that the potential outcomes of the experimental units are fixed and the randomness comes solely from the physical randomization of the treatment assignment. These two views differs conceptually and mathematically, resulting in different sampling variances of the usual difference-in-means estimator of the average causal effect. Practically, however, these two views result in identical variance estimators. By recalling a variance decomposition and exploiting a completeness-type argument, we establish a connection between these two views in completely randomized experiments. This alternative formulation could serve as a template for bridging finite and super population causal inference in other scenarios.",two general view causal analysi experiment data super popul view unit independ sampl hypothet infinit popul finit popul view potenti outcom experiment unit fix random come sole physic random treatment assign two view differ conceptu mathemat result differ sampl varianc usual differ mean estim averag causal effect practic howev two view result ident varianc estim recal varianc decomposit exploit complet type argument establish connect two view complet random experi altern formul could serv templat bridg finit super popul causal infer scenario,"['Peng Ding', 'Xinran Li', 'Luke W. Miratrix']","['math.ST', 'stat.TH']",False,False,False,False,False,True
1382,2017-03-28T14:05:06Z,2017-02-27T21:52:17Z,http://arxiv.org/abs/1702.08546v1,http://arxiv.org/pdf/1702.08546v1,Optimal rates of estimation for multi-reference alignment,optim rate estim multi refer align,"This paper describes optimal rates of adaptive estimation of a vector in the multi-reference alignment model, a problem with important applications in fields such as signal processing, image processing, and computer vision, among others. We describe how this model can be viewed as a multivariate Gaussian mixture model under the constraint that the centers belong to the orbit of a group. This enables us to derive matching upper and lower bounds that feature an interesting dependence on the signal-to-noise ratio of the model. Both upper and lower bounds are articulated around a tight local control of Kullback-Leibler divergences that showcases the central role of moment tensors in this problem.",paper describ optim rate adapt estim vector multi refer align model problem import applic field signal process imag process comput vision among describ model view multivari gaussian mixtur model constraint center belong orbit group enabl us deriv match upper lower bound featur interest depend signal nois ratio model upper lower bound articul around tight local control kullback leibler diverg showcas central role moment tensor problem,"['Afonso Bandeira', 'Philippe Rigollet', 'Jonathan Weed']","['math.ST', 'stat.TH']",False,False,False,False,False,True
1384,2017-03-28T14:05:06Z,2017-02-27T10:01:36Z,http://arxiv.org/abs/1702.08211v1,http://arxiv.org/pdf/1702.08211v1,"Online Nonparametric Learning, Chaining, and the Role of Partial   Feedback",onlin nonparametr learn chain role partial feedback,"We investigate contextual online learning with nonparametric (Lipschitz) comparison classes under different assumptions on losses and feedback information. For full information feedback and Lipschitz losses, we characterize the minimax regret up to log factors by proving an upper bound matching a previously known lower bound. In a partial feedback model motivated by second-price auctions, we prove upper bounds for Lipschitz and semi-Lipschitz losses that improve on the known bounds for standard bandit feedback. Our analysis combines novel results for contextual second-price auctions with a novel algorithmic approach based on chaining. When the context space is Euclidean, our chaining approach is efficient and delivers an even better regret bound.",investig contextu onlin learn nonparametr lipschitz comparison class differ assumpt loss feedback inform full inform feedback lipschitz loss character minimax regret log factor prove upper bound match previous known lower bound partial feedback model motiv second price auction prove upper bound lipschitz semi lipschitz loss improv known bound standard bandit feedback analysi combin novel result contextu second price auction novel algorithm approach base chain context space euclidean chain approach effici deliv even better regret bound,"['Nicolò Cesa-Bianchi', 'Pierre Gaillard', 'Claudio Gentile', 'Sébastien Gerchinovitz']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",False,False,True,False,False,True
1385,2017-03-28T14:05:06Z,2017-02-26T22:59:02Z,http://arxiv.org/abs/1702.08109v1,http://arxiv.org/pdf/1702.08109v1,Constrained Maximum Likelihood Estimators for Densities,constrain maximum likelihood estim densiti,"We put forward a framework for nonparametric density estimation in situations where the sample is supplemented by information and assumptions about shape, support, continuity, slope, location of modes, density values, etc. These supplements are incorporated as constraints that in conjunction with a maximum likelihood criterion lead to constrained infinite-dimensional optimization problems that we formulate over spaces of semicontinuous functions. These spaces, when equipped with an appropriate metric, offer a series of advantages including simple conditions for existence of estimators and their limits and, in particular, guarantee the convergence of modes of densities. Relying on the approximation theory---epi-convergence---for optimization problems, we provide general conditions under which estimators subject to essentially arbitrary constraints are consistent and illustrate the framework with a number of examples that span classical and novel shape constraints.",put forward framework nonparametr densiti estim situat sampl supplement inform assumpt shape support continu slope locat mode densiti valu etc supplement incorpor constraint conjunct maximum likelihood criterion lead constrain infinit dimension optim problem formul space semicontinu function space equip appropri metric offer seri advantag includ simpl condit exist estim limit particular guarante converg mode densiti reli approxim theori epi converg optim problem provid general condit estim subject essenti arbitrari constraint consist illustr framework number exampl span classic novel shape constraint,"['Johannes O. Royset', 'Roger J-B Wets']","['math.ST', 'stat.TH']",False,False,False,False,False,True
1386,2017-03-28T14:05:06Z,2017-02-25T14:53:53Z,http://arxiv.org/abs/1702.07899v1,http://arxiv.org/pdf/1702.07899v1,Are there needles in a moving haystack? Adaptive sensing for detection   of dynamically evolving signals,needl move haystack adapt sens detect dynam evolv signal,"In this paper we investigate the problem of detecting dynamically evolving signals. We model the signal as an $n$ dimensional vector that is either zero or has $s$ non-zero components. At each time step $t\in \mathbb{N}$ the non-zero components change their location independently with probability $p$. The statistical problem is to decide whether the signal is a zero vector or in fact it has non-zero components. This decision is based on $m$ noisy observations of individual signal components collected at times $t=1,\ldots,m$. We consider two different sensing paradigms, namely adaptive and non-adaptive sensing. For non-adaptive sensing the choice of components to measure has to be decided before the data collection process started, while for adaptive sensing one can adjust the sensing process based on observations collected earlier. We characterize the difficulty of this detection problem in both sensing paradigms in terms of the aforementioned parameters, with special interest to the speed of change of the active components. In addition we provide an adaptive sensing algorithm for this problem and contrast its performance to that of non-adaptive detection algorithms.",paper investig problem detect dynam evolv signal model signal dimension vector either zero non zero compon time step mathbb non zero compon chang locat independ probabl statist problem decid whether signal zero vector fact non zero compon decis base noisi observ individu signal compon collect time ldot consid two differ sens paradigm name adapt non adapt sens non adapt sens choic compon measur decid befor data collect process start adapt sens one adjust sens process base observ collect earlier character difficulti detect problem sens paradigm term aforement paramet special interest speed chang activ compon addit provid adapt sens algorithm problem contrast perform non adapt detect algorithm,"['Rui M. Castro', 'Ervin Tánczos']","['math.ST', 'stat.TH']",False,False,False,False,False,True
1387,2017-03-28T14:05:06Z,2017-02-24T23:43:06Z,http://arxiv.org/abs/1702.07803v1,http://arxiv.org/pdf/1702.07803v1,Nonparanormal Information Estimation,nonparanorm inform estim,"We study the problem of using i.i.d. samples from an unknown multivariate probability distribution $p$ to estimate the mutual information of $p$. This problem has recently received attention in two settings: (1) where $p$ is assumed to be Gaussian and (2) where $p$ is assumed only to lie in a large nonparametric smoothness class. Estimators proposed for the Gaussian case converge in high dimensions when the Gaussian assumption holds, but are brittle, failing dramatically when $p$ is not Gaussian. Estimators proposed for the nonparametric case fail to converge with realistic sample sizes except in very low dimensions. As a result, there is a lack of robust mutual information estimators for many realistic data. To address this, we propose estimators for mutual information when $p$ is assumed to be a nonparanormal (a.k.a., Gaussian copula) model, a semiparametric compromise between Gaussian and nonparametric extremes. Using theoretical bounds and experiments, we show these estimators strike a practical balance between robustness and scaling with dimensionality.",studi problem use sampl unknown multivari probabl distribut estim mutual inform problem recent receiv attent two set assum gaussian assum onli lie larg nonparametr smooth class estim propos gaussian case converg high dimens gaussian assumpt hold brittl fail dramat gaussian estim propos nonparametr case fail converg realist sampl size except veri low dimens result lack robust mutual inform estim mani realist data address propos estim mutual inform assum nonparanorm gaussian copula model semiparametr compromis gaussian nonparametr extrem use theoret bound experi show estim strike practic balanc robust scale dimension,"['Shashank Singh', 'Barnabás Pøczos']","['math.ST', 'cs.IT', 'math.IT', 'stat.ML', 'stat.TH']",False,False,False,False,False,True
1388,2017-03-28T14:05:06Z,2017-02-24T23:31:04Z,http://arxiv.org/abs/1702.07801v1,http://arxiv.org/pdf/1702.07801v1,Consistent structure estimation of exponential-family random graph   models with additional structure,consist structur estim exponenti famili random graph model addit structur,"We consider the challenging problem of statistical inference for exponential-family random graph models given one observation of a random graph with complex dependence (e.g., transitivity). To facilitate statistical inference, we endow random graphs with additional structure. The basic idea is that random graphs are composed of subgraphs with complex dependence. We have shown elsewhere that when the composition of random graphs is known, $M$-estimators of canonical and curved exponential families with complex dependence are consistent. In practice, the composition is known in some applications, but is unknown in others. If the composition is unknown, the first and foremost question is whether it can be recovered. The main consistency results of the paper show that it is possible to do so as long as exponential families satisfy weak dependence and smoothness conditions. These results confirm that exponential-family random graph models with additional structure constitute a promising direction of statistical network analysis.",consid challeng problem statist infer exponenti famili random graph model given one observ random graph complex depend transit facilit statist infer endow random graph addit structur basic idea random graph compos subgraph complex depend shown elsewher composit random graph known estim canon curv exponenti famili complex depend consist practic composit known applic unknown composit unknown first foremost question whether recov main consist result paper show possibl long exponenti famili satisfi weak depend smooth condit result confirm exponenti famili random graph model addit structur constitut promis direct statist network analysi,['Michael Schweinberger'],"['math.ST', 'stat.TH']",False,False,False,False,False,True
1389,2017-03-28T14:05:06Z,2017-03-01T00:17:04Z,http://arxiv.org/abs/1702.07795v2,http://arxiv.org/pdf/1702.07795v2,A Study of the Allan Variance for Constant-Mean Non-Stationary Processes,studi allan varianc constant mean non stationari process,"The Allan Variance (AV) is a widely used quantity in areas focusing on error measurement as well as in the general analysis of variance for autocorrelated processes in domains such as engineering and, more specifically, metrology. The form of this quantity is widely used to detect noise patterns and indications of stability within signals. However, the properties of this quantity are not known for commonly occurring processes whose covariance structure is non-stationary and, in these cases, an erroneous interpretation of the AV could lead to misleading conclusions. This paper generalizes the theoretical form of the AV to some non-stationary processes while at the same time being valid also for weakly stationary processes. Some simulation examples show how this new form can help to understand the processes for which the AV is able to distinguish these from the stationary cases and hence allow for a better interpretation of this quantity in applied cases.",allan varianc av wide use quantiti area focus error measur well general analysi varianc autocorrel process domain engin specif metrolog form quantiti wide use detect nois pattern indic stabil within signal howev properti quantiti known common occur process whose covari structur non stationari case erron interpret av could lead mislead conclus paper general theoret form av non stationari process time valid also weak stationari process simul exampl show new form help understand process av abl distinguish stationari case henc allow better interpret quantiti appli case,"['Haotian Xu', 'Stéphane Guerrier', 'Roberto Molinari', 'Yuming Zhang']","['math.ST', 'stat.TH']",False,False,False,False,False,True
1390,2017-03-28T14:05:11Z,2017-02-24T02:09:04Z,http://arxiv.org/abs/1702.07448v1,http://arxiv.org/pdf/1702.07448v1,Optimal Bayesian Minimax Rates for Unconstrained Large Covariance   Matrices,optim bayesian minimax rate unconstrain larg covari matric,"We obtain the optimal Bayesian minimax rate for the unconstrained large covariance matrix of multivariate normal sample with mean zero, when both the sample size, n, and the dimension, p, of the covariance matrix tend to infinity. Traditionally the posterior convergence rate is used to compare the frequentist asymptotic performance of priors, but defining the optimality with it is elusive. We propose a new decision theoretic framework for prior selection and define Bayesian minimax rate. Under the proposed framework, we obtain the optimal Bayesian minimax rate for the spectral norm for all rates of p. We also considered Frobenius norm, Bregman divergence and squared log-determinant loss and obtain the optimal Bayesian minimax rate under certain rate conditions on p. A simulation study is conducted to support the theoretical results.",obtain optim bayesian minimax rate unconstrain larg covari matrix multivari normal sampl mean zero sampl size dimens covari matrix tend infin tradit posterior converg rate use compar frequentist asymptot perform prior defin optim elus propos new decis theoret framework prior select defin bayesian minimax rate propos framework obtain optim bayesian minimax rate spectral norm rate also consid frobenius norm bregman diverg squar log determin loss obtain optim bayesian minimax rate certain rate condit simul studi conduct support theoret result,"['Kyoungjae Lee', 'Jaeyong Lee']","['math.ST', 'stat.TH']",False,False,True,False,False,True
1391,2017-03-28T14:05:11Z,2017-02-23T13:49:57Z,http://arxiv.org/abs/1702.07211v1,http://arxiv.org/pdf/1702.07211v1,A minimax and asymptotically optimal algorithm for stochastic bandits,minimax asymptot optim algorithm stochast bandit,"We propose the kl-UCB ++ algorithm for regret minimization in stochastic bandit models with exponential families of distributions. We prove that it is simultaneously asymptotically optimal (in the sense of Lai and Robbins' lower bound) and minimax optimal. This is the first algorithm proved to enjoy these two properties at the same time. This work thus merges two different lines of research, with simple proofs involving no complexity overhead.",propos kl ucb algorithm regret minim stochast bandit model exponenti famili distribut prove simultan asymptot optim sens lai robbin lower bound minimax optim first algorithm prove enjoy two properti time work thus merg two differ line research simpl proof involv complex overhead,"['Pierre Ménard', 'Aurélien Garivier']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",False,False,False,False,False,True
1394,2017-03-28T14:05:11Z,2017-02-26T20:06:00Z,http://arxiv.org/abs/1702.07027v2,http://arxiv.org/pdf/1702.07027v2,Nonparametric Inference via Bootstrapping the Debiased Estimator,nonparametr infer via bootstrap debias estim,"In this paper, we propose to construct confidence bands by bootstrapping the debiased kernel density estimator (for density estimation) and the debiased local polynomial regression estimator (for regression analysis). The idea of using a debiased estimator was first introduced in Calonico et al. (2015), where they construct a confidence interval of the density function (and regression function) at a given point by explicitly estimating stochastic variations. We extend their ideas and propose a bootstrap approach for constructing confidence bands that is uniform for every point in the support. We prove that the resulting bootstrap confidence band is asymptotically valid and is compatible with most tuning parameter selection approaches, such as the rule of thumb and cross-validation. We further generalize our method to confidence sets of density level sets and inverse regression problems. Simulation studies confirm the validity of the proposed confidence bands/sets.",paper propos construct confid band bootstrap debias kernel densiti estim densiti estim debias local polynomi regress estim regress analysi idea use debias estim first introduc calonico et al construct confid interv densiti function regress function given point explicit estim stochast variat extend idea propos bootstrap approach construct confid band uniform everi point support prove result bootstrap confid band asymptot valid compat tune paramet select approach rule thumb cross valid general method confid set densiti level set invers regress problem simul studi confirm valid propos confid band set,['Yen-Chi Chen'],"['stat.ME', 'math.ST', 'stat.TH', 'Primary 62G15, secondary 62G09, 62G07, 62G08']",False,False,False,False,False,True
1395,2017-03-28T14:05:11Z,2017-02-22T19:25:29Z,http://arxiv.org/abs/1702.06975v1,http://arxiv.org/pdf/1702.06975v1,High dimensional deformed rectangle matrices with applications in matrix   denoising,high dimension deform rectangl matric applic matrix denois,"We consider the recovery of a low rank $M \times N$ matrix $S$ from its noisy observation $\tilde{S}$ in two different regimes. Under the assumption that $M$ is comparable to $N$, we propose two optimal estimators for $S$. Our analysis rely on the local behavior of the large dimensional rectangle matrices with finite rank perturbation. We also derive the convergent limits and rates for the singular values and vectors of such matrices.",consid recoveri low rank time matrix noisi observ tild two differ regim assumpt compar propos two optim estim analysi reli local behavior larg dimension rectangl matric finit rank perturb also deriv converg limit rate singular valu vector matric,['Xiucai Ding'],"['math.ST', 'stat.TH']",False,False,False,False,False,True
1396,2017-03-28T14:05:11Z,2017-02-22T19:22:55Z,http://arxiv.org/abs/1702.06972v1,http://arxiv.org/pdf/1702.06972v1,Approximations of the Restless Bandit Problem,approxim restless bandit problem,"The multi-armed restless bandit problem is studied in the case where the pay-offs are not necessarily independent over time nor across the arms. Even though this version of the problem provides a more realistic model for most real-world applications, it cannot be optimally solved in practice since it is known to be PSPACE-hard. The objective of this paper is to characterize special sub-classes of the problem where good approximate solutions can be found using tractable approaches. Specifically, it is shown that in the case where the joint distribution over the arms is $\varphi$-mixing, and under some conditions on the $\varphi$-mixing coefficients, a modified version of UCB can prove optimal. On the other hand, it is shown that when the pay-off distributions are strongly dependent, simple switching strategies may be devised which leverage the strong inter-dependencies. To this end, an example is provided using Gaussian Processes. The techniques developed in this paper apply, more generally, to the problem of online sampling under dependence.",multi arm restless bandit problem studi case pay necessarili independ time across arm even though version problem provid realist model real world applic cannot optim solv practic sinc known pspace hard object paper character special sub class problem good approxim solut found use tractabl approach specif shown case joint distribut arm varphi mix condit varphi mix coeffici modifi version ucb prove optim hand shown pay distribut strong depend simpl switch strategi may devis leverag strong inter depend end exampl provid use gaussian process techniqu develop paper appli general problem onlin sampl depend,"['Steffen Grunewalder', 'Azadeh Khaleghi']","['math.ST', 'cs.LG', 'math.PR', 'stat.ML', 'stat.TH']",False,False,False,False,False,True
1399,2017-03-28T14:05:11Z,2017-02-20T14:34:46Z,http://arxiv.org/abs/1702.05985v1,http://arxiv.org/pdf/1702.05985v1,Fano's inequality for random variables,fano inequ random variabl,"We extend Fano's inequality, which controls the average probability of (disjoint) events in terms of the average of some Kullback-Leibler divergences, to work with arbitrary [0,1]-valued random variables. Our simple two-step methodology is general enough to cover the case of an arbitrary (possibly continuously infinite) family of distributions as well as [0,1]-valued random variables not necessarily summing up to 1. Several novel applications are provided, in which the consideration of random variables is particularly handy. The most important applications deal with the problem of Bayesian posterior concentration (minimax or distribution-dependent) rates and with a lower bound on the regret in non-stochastic sequential learning. We also improve in passing some earlier fundamental results: in particular, we provide a simple and enlightening proof of the refined Pinsker's inequality of Ordentlich and Weinberger and derive a sharper Bretagnolle-Huber inequality.",extend fano inequ control averag probabl disjoint event term averag kullback leibler diverg work arbitrari valu random variabl simpl two step methodolog general enough cover case arbitrari possibl continu infinit famili distribut well valu random variabl necessarili sum sever novel applic provid consider random variabl particular handi import applic deal problem bayesian posterior concentr minimax distribut depend rate lower bound regret non stochast sequenti learn also improv pass earlier fundament result particular provid simpl enlighten proof refin pinsker inequ ordentlich weinberg deriv sharper bretagnoll huber inequ,"['Sebastien Gerchinovitz', 'Pierre Ménard', 'Gilles Stoltz']","['math.ST', 'cs.IT', 'math.IT', 'stat.TH']",False,False,False,False,False,True
