,プログラム実行日時,論文更新日時,論文リンク,PDFリンク,元論文タイトル,論文タイトル,元サマリ,サマリ,著者,事前付与ジャンル,ニューラルネットワーク,自然言語処理,マーケティング,画像解析,音声解析,強化学習
6,2017-03-28T14:05:16Z,2017-03-26T05:44:56Z,http://arxiv.org/abs/1703.08769v1,http://arxiv.org/pdf/1703.08769v1,Open Vocabulary Scene Parsing,open vocabulari scene pars,"Recognizing arbitrary objects in the wild has been a challenging problem due to the limitations of existing classification models and datasets. In this paper, we propose a new task that aims at parsing scene with a large and open vocabulary, and several evaluation metrics are explored for this problem. Our proposed approach to this problem is a joint image pixel and word concept embeddings framework, where word concepts are connected by semantic relations. We validate the open vocabulary prediction ability of our framework on ADE20K dataset which covers a wide variety of scenes and objects. We further explore the trained joint embedding space to show its interpretability.",recogn arbitrari object wild challeng problem due limit exist classif model dataset paper propos new task aim pars scene larg open vocabulari sever evalu metric explor problem propos approach problem joint imag pixel word concept embed framework word concept connect semant relat valid open vocabulari predict abil framework adek dataset cover wide varieti scene object explor train joint embed space show interpret,"['Hang Zhao', 'Xavier Puig', 'Bolei Zhou', 'Sanja Fidler', 'Antonio Torralba']","['cs.CV', 'cs.AI']",False,True,False,False,False,False
8,2017-03-28T14:05:16Z,2017-03-25T15:37:09Z,http://arxiv.org/abs/1703.08705v1,http://arxiv.org/pdf/1703.08705v1,Comparing Rule-Based and Deep Learning Models for Patient Phenotyping,compar rule base deep learn model patient phenotyp,"Objective: We investigate whether deep learning techniques for natural language processing (NLP) can be used efficiently for patient phenotyping. Patient phenotyping is a classification task for determining whether a patient has a medical condition, and is a crucial part of secondary analysis of healthcare data. We assess the performance of deep learning algorithms and compare them with classical NLP approaches.   Materials and Methods: We compare convolutional neural networks (CNNs), n-gram models, and approaches based on cTAKES that extract pre-defined medical concepts from clinical notes and use them to predict patient phenotypes. The performance is tested on 10 different phenotyping tasks using 1,610 discharge summaries extracted from the MIMIC-III database.   Results: CNNs outperform other phenotyping algorithms in all 10 tasks. The average F1-score of our model is 76 (PPV of 83, and sensitivity of 71) with our model having an F1-score up to 37 points higher than alternative approaches. We additionally assess the interpretability of our model by presenting a method that extracts the most salient phrases for a particular prediction.   Conclusion: We show that NLP methods based on deep learning improve the performance of patient phenotyping. Our CNN-based algorithm automatically learns the phrases associated with each patient phenotype. As such, it reduces the annotation complexity for clinical domain experts, who are normally required to develop task-specific annotation rules and identify relevant phrases. Our method performs well in terms of both performance and interpretability, which indicates that deep learning is an effective approach to patient phenotyping based on clinicians' notes.",object investig whether deep learn techniqu natur languag process nlp use effici patient phenotyp patient phenotyp classif task determin whether patient medic condit crucial part secondari analysi healthcar data assess perform deep learn algorithm compar classic nlp approach materi method compar convolut neural network cnns gram model approach base ctake extract pre defin medic concept clinic note use predict patient phenotyp perform test differ phenotyp task use discharg summari extract mimic iii databas result cnns outperform phenotyp algorithm task averag score model ppv sensit model score point higher altern approach addit assess interpret model present method extract salient phrase particular predict conclus show nlp method base deep learn improv perform patient phenotyp cnn base algorithm automat learn phrase associ patient phenotyp reduc annot complex clinic domain expert normal requir develop task specif annot rule identifi relev phrase method perform well term perform interpret indic deep learn effect approach patient phenotyp base clinician note,"['Sebastian Gehrmann', 'Franck Dernoncourt', 'Yeran Li', 'Eric T. Carlson', 'Joy T. Wu', 'Jonathan Welt', 'John Foote Jr.', 'Edward T. Moseley', 'David W. Grant', 'Patrick D. Tyler', 'Leo Anthony Celi']","['cs.CL', 'cs.AI', 'cs.NE', 'stat.ML']",False,True,False,False,False,False
15,2017-03-28T14:05:20Z,2017-03-23T15:15:26Z,http://arxiv.org/abs/1703.08098v1,http://arxiv.org/pdf/1703.08098v1,An overview of embedding models of entities and relationships for   knowledge base completion,overview embed model entiti relationship knowledg base complet,"Knowledge bases of real-world facts about entities and their relationships are useful resources for a variety of natural language processing tasks. However, because knowledge bases are typically incomplete, it is useful to be able to perform knowledge base completion, i.e., predict whether a relationship not in the knowledge base is likely to be true. This article presents an overview of embedding models of entities and relationships for knowledge base completion, with up-to-date experimental results on two standard evaluation tasks of link prediction (i.e. entity prediction) and triple classification.",knowledg base real world fact entiti relationship use resourc varieti natur languag process task howev becaus knowledg base typic incomplet use abl perform knowledg base complet predict whether relationship knowledg base like true articl present overview embed model entiti relationship knowledg base complet date experiment result two standard evalu task link predict entiti predict tripl classif,['Dat Quoc Nguyen'],"['cs.CL', 'cs.AI', 'cs.IR']",False,True,False,False,False,False
30,2017-03-28T14:05:28Z,2017-03-21T18:34:34Z,http://arxiv.org/abs/1703.07384v1,http://arxiv.org/pdf/1703.07384v1,Ontology Based Pivoted normalization using Vector Based Approach for   information Retrieval,ontolog base pivot normal use vector base approach inform retriev,"The proposed methodology is procedural i.e. it follows finite number of steps that extracts relevant documents according to users query. It is based on principles of Data Mining for analyzing web data. Data Mining first adapts integration of data to generate warehouse. Then, it extracts useful information with the help of algorithm. The task of representing extracted documents is done by using Vector Based Statistical Approach that represents each document in set of Terms.",propos methodolog procedur follow finit number step extract relev document accord user queri base principl data mine analyz web data data mine first adapt integr data generat warehous extract use inform help algorithm task repres extract document done use vector base statist approach repres document set term,"['Vishal Jain', 'Dr. Mayank Singh']","['cs.IR', 'cs.AI']",False,True,False,False,False,False
67,2017-03-28T14:05:41Z,2017-03-15T17:01:20Z,http://arxiv.org/abs/1703.05260v1,http://arxiv.org/pdf/1703.05260v1,InScript: Narrative texts annotated with script information,inscript narrat text annot script inform,"This paper presents the InScript corpus (Narrative Texts Instantiating Script structure). InScript is a corpus of 1,000 stories centered around 10 different scenarios. Verbs and noun phrases are annotated with event and participant types, respectively. Additionally, the text is annotated with coreference information. The corpus shows rich lexical variation and will serve as a unique resource for the study of the role of script knowledge in natural language processing.",paper present inscript corpus narrat text instanti script structur inscript corpus stori center around differ scenario verb noun phrase annot event particip type respect addit text annot corefer inform corpus show rich lexic variat serv uniqu resourc studi role script knowledg natur languag process,"['Ashutosh Modi', 'Tatjana Anikina', 'Simon Ostermann', 'Manfred Pinkal']","['cs.CL', 'cs.AI']",False,True,False,False,False,False
73,2017-03-28T14:05:45Z,2017-03-15T03:30:13Z,http://arxiv.org/abs/1703.04908v1,http://arxiv.org/pdf/1703.04908v1,Emergence of Grounded Compositional Language in Multi-Agent Populations,emerg ground composit languag multi agent popul,"By capturing statistical patterns in large corpora, machine learning has enabled significant advances in natural language processing, including in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply capturing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. Towards this end, we propose a multi-agent learning environment and learning methods that bring about emergence of a basic compositional language. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of non-verbal communication such as pointing and guiding when language communication is unavailable.",captur statist pattern larg corpora machin learn enabl signific advanc natur languag process includ machin translat question answer sentiment analysi howev agent intellig interact human simpli captur statist pattern insuffici paper investig ground composit languag emerg mean achiev goal multi agent popul toward end propos multi agent learn environ learn method bring emerg basic composit languag languag repres stream abstract discret symbol utter agent time nonetheless coher structur possess defin vocabulari syntax also observ emerg non verbal communic point guid languag communic unavail,"['Igor Mordatch', 'Pieter Abbeel']","['cs.AI', 'cs.CL']",False,True,False,False,False,False
82,2017-03-28T14:05:50Z,2017-03-13T17:34:18Z,http://arxiv.org/abs/1703.04498v1,http://arxiv.org/pdf/1703.04498v1,High-Throughput and Language-Agnostic Entity Disambiguation and Linking   on User Generated Data,high throughput languag agnost entiti disambigu link user generat data,"The Entity Disambiguation and Linking (EDL) task matches entity mentions in text to a unique Knowledge Base (KB) identifier such as a Wikipedia or Freebase id. It plays a critical role in the construction of a high quality information network, and can be further leveraged for a variety of information retrieval and NLP tasks such as text categorization and document tagging. EDL is a complex and challenging problem due to ambiguity of the mentions and real world text being multi-lingual. Moreover, EDL systems need to have high throughput and should be lightweight in order to scale to large datasets and run on off-the-shelf machines. More importantly, these systems need to be able to extract and disambiguate dense annotations from the data in order to enable an Information Retrieval or Extraction task running on the data to be more efficient and accurate. In order to address all these challenges, we present the Lithium EDL system and algorithm - a high-throughput, lightweight, language-agnostic EDL system that extracts and correctly disambiguates 75% more entities than state-of-the-art EDL systems and is significantly faster than them.",entiti disambigu link edl task match entiti mention text uniqu knowledg base kb identifi wikipedia freebas id play critic role construct high qualiti inform network leverag varieti inform retriev nlp task text categor document tag edl complex challeng problem due ambigu mention real world text multi lingual moreov edl system need high throughput lightweight order scale larg dataset run shelf machin import system need abl extract disambigu dens annot data order enabl inform retriev extract task run data effici accur order address challeng present lithium edl system algorithm high throughput lightweight languag agnost edl system extract correct disambigu entiti state art edl system signific faster,"['Preeti Bhargava', 'Nemanja Spasojevic', 'Guoning Hu']","['cs.IR', 'cs.AI', 'cs.CL']",False,True,False,False,False,False
302,2017-03-28T14:05:58Z,2017-03-27T11:15:58Z,http://arxiv.org/abs/1703.09013v1,http://arxiv.org/pdf/1703.09013v1,A Sentence Simplification System for Improving Relation Extraction,sentenc simplif system improv relat extract,"In this demo paper, we present a text simplification approach that is directed at improving the performance of state-of-the-art Open Relation Extraction (RE) systems. As syntactically complex sentences often pose a challenge for current Open RE approaches, we have developed a simplification framework that performs a pre-processing step by taking a single sentence as input and using a set of syntactic-based transformation rules to create a textual input that is easier to process for subsequently applied Open RE systems.",demo paper present text simplif approach direct improv perform state art open relat extract system syntact complex sentenc often pose challeng current open approach develop simplif framework perform pre process step take singl sentenc input use set syntact base transform rule creat textual input easier process subsequ appli open system,"['Christina Niklaus', 'Bernhard Bermeitinger', 'Siegfried Handschuh', 'André Freitas']",['cs.CL'],False,True,False,False,False,False
306,2017-03-28T14:05:58Z,2017-03-25T15:37:09Z,http://arxiv.org/abs/1703.08705v1,http://arxiv.org/pdf/1703.08705v1,Comparing Rule-Based and Deep Learning Models for Patient Phenotyping,compar rule base deep learn model patient phenotyp,"Objective: We investigate whether deep learning techniques for natural language processing (NLP) can be used efficiently for patient phenotyping. Patient phenotyping is a classification task for determining whether a patient has a medical condition, and is a crucial part of secondary analysis of healthcare data. We assess the performance of deep learning algorithms and compare them with classical NLP approaches.   Materials and Methods: We compare convolutional neural networks (CNNs), n-gram models, and approaches based on cTAKES that extract pre-defined medical concepts from clinical notes and use them to predict patient phenotypes. The performance is tested on 10 different phenotyping tasks using 1,610 discharge summaries extracted from the MIMIC-III database.   Results: CNNs outperform other phenotyping algorithms in all 10 tasks. The average F1-score of our model is 76 (PPV of 83, and sensitivity of 71) with our model having an F1-score up to 37 points higher than alternative approaches. We additionally assess the interpretability of our model by presenting a method that extracts the most salient phrases for a particular prediction.   Conclusion: We show that NLP methods based on deep learning improve the performance of patient phenotyping. Our CNN-based algorithm automatically learns the phrases associated with each patient phenotype. As such, it reduces the annotation complexity for clinical domain experts, who are normally required to develop task-specific annotation rules and identify relevant phrases. Our method performs well in terms of both performance and interpretability, which indicates that deep learning is an effective approach to patient phenotyping based on clinicians' notes.",object investig whether deep learn techniqu natur languag process nlp use effici patient phenotyp patient phenotyp classif task determin whether patient medic condit crucial part secondari analysi healthcar data assess perform deep learn algorithm compar classic nlp approach materi method compar convolut neural network cnns gram model approach base ctake extract pre defin medic concept clinic note use predict patient phenotyp perform test differ phenotyp task use discharg summari extract mimic iii databas result cnns outperform phenotyp algorithm task averag score model ppv sensit model score point higher altern approach addit assess interpret model present method extract salient phrase particular predict conclus show nlp method base deep learn improv perform patient phenotyp cnn base algorithm automat learn phrase associ patient phenotyp reduc annot complex clinic domain expert normal requir develop task specif annot rule identifi relev phrase method perform well term perform interpret indic deep learn effect approach patient phenotyp base clinician note,"['Sebastian Gehrmann', 'Franck Dernoncourt', 'Yeran Li', 'Eric T. Carlson', 'Joy T. Wu', 'Jonathan Welt', 'John Foote Jr.', 'Edward T. Moseley', 'David W. Grant', 'Patrick D. Tyler', 'Leo Anthony Celi']","['cs.CL', 'cs.AI', 'cs.NE', 'stat.ML']",False,True,False,False,False,False
310,2017-03-28T14:06:02Z,2017-03-24T17:55:33Z,http://arxiv.org/abs/1703.08537v1,http://arxiv.org/pdf/1703.08537v1,Crowdsourcing Universal Part-Of-Speech Tags for Code-Switching,crowdsourc univers part speech tag code switch,"Code-switching is the phenomenon by which bilingual speakers switch between multiple languages during communication. The importance of developing language technologies for codeswitching data is immense, given the large populations that routinely code-switch. High-quality linguistic annotations are extremely valuable for any NLP task, and performance is often limited by the amount of high-quality labeled data. However, little such data exists for code-switching. In this paper, we describe crowd-sourcing universal part-of-speech tags for the Miami Bangor Corpus of Spanish-English code-switched speech. We split the annotation task into three subtasks: one in which a subset of tokens are labeled automatically, one in which questions are specifically designed to disambiguate a subset of high frequency words, and a more general cascaded approach for the remaining data in which questions are displayed to the worker following a decision tree structure. Each subtask is extended and adapted for a multilingual setting and the universal tagset. The quality of the annotation process is measured using hidden check questions annotated with gold labels. The overall agreement between gold standard labels and the majority vote is between 0.95 and 0.96 for just three labels and the average recall across part-of-speech tags is between 0.87 and 0.99, depending on the task.",code switch phenomenon bilingu speaker switch multipl languag dure communic import develop languag technolog codeswitch data immens given larg popul routin code switch high qualiti linguist annot extrem valuabl ani nlp task perform often limit amount high qualiti label data howev littl data exist code switch paper describ crowd sourc univers part speech tag miami bangor corpus spanish english code switch speech split annot task three subtask one subset token label automat one question specif design disambigu subset high frequenc word general cascad approach remain data question display worker follow decis tree structur subtask extend adapt multilingu set univers tagset qualiti annot process measur use hidden check question annot gold label overal agreement gold standard label major vote three label averag recal across part speech tag depend task,"['Victor Soto', 'Julia Hirschberg']",['cs.CL'],False,True,False,False,False,False
313,2017-03-28T14:06:02Z,2017-03-24T14:49:58Z,http://arxiv.org/abs/1703.08544v1,http://arxiv.org/pdf/1703.08544v1,D.TRUMP: Data-mining Textual Responses to Uncover Misconception Patterns,trump data mine textual respons uncov misconcept pattern,"An important, yet largely unstudied, problem in student data analysis is to detect misconceptions from students' responses to open-response questions. Misconception detection enables instructors to deliver more targeted feedback on the misconceptions exhibited by many students in their class, thus improving the quality of instruction. In this paper, we propose D.TRUMP, a new natural language processing-based framework to detect the common misconceptions among students' textual responses to short-answer questions. We propose a probabilistic model for students' textual responses involving misconceptions and experimentally validate it on a real-world student-response dataset. Experimental results show that D.TRUMP excels at classifying whether a response exhibits one or more misconceptions. More importantly, it can also automatically detect the common misconceptions exhibited across responses from multiple students to multiple questions; this property is especially important at large scale, since instructors will no longer need to manually specify all possible misconceptions that students might exhibit.",import yet larg unstudi problem student data analysi detect misconcept student respons open respons question misconcept detect enabl instructor deliv target feedback misconcept exhibit mani student class thus improv qualiti instruct paper propos trump new natur languag process base framework detect common misconcept among student textual respons short answer question propos probabilist model student textual respons involv misconcept experiment valid real world student respons dataset experiment result show trump excel classifi whether respons exhibit one misconcept import also automat detect common misconcept exhibit across respons multipl student multipl question properti especi import larg scale sinc instructor longer need manual specifi possibl misconcept student might exhibit,"['Joshua J. Michalenko', 'Andrew S. Lan', 'Richard G. Baraniuk']","['stat.ML', 'cs.CL']",False,True,False,False,False,False
315,2017-03-28T14:06:02Z,2017-03-24T09:32:23Z,http://arxiv.org/abs/1703.08324v1,http://arxiv.org/pdf/1703.08324v1,Are crossing dependencies really scarce?,cross depend realli scarc,"The syntactic structure of a sentence can be modelled as a tree, where vertices correspond to words and edges indicate syntactic dependencies. It has been claimed recurrently that the number of edge crossings in real sentences is small. However, a baseline or null hypothesis has been lacking. Here we quantify the amount of crossings of real sentences and compare it to the predictions of a series of baselines. We conclude that crossings are really scarce in real sentences. Their scarcity is unexpected by the hubiness of the trees. Indeed, real sentences are close to linear trees, where the potential number of crossings is maximized.",syntact structur sentenc model tree vertic correspond word edg indic syntact depend claim recurr number edg cross real sentenc small howev baselin null hypothesi lack quantifi amount cross real sentenc compar predict seri baselin conclud cross realli scarc real sentenc scarciti unexpect hubi tree inde real sentenc close linear tree potenti number cross maxim,"['Ramon Ferrer-i-Cancho', 'Carlos Gomez-Rodriguez', 'J. L. Esteban']","['physics.soc-ph', 'cond-mat.stat-mech', 'cs.CL', 'physics.data-an']",False,True,False,False,False,False
321,2017-03-28T14:06:07Z,2017-03-23T15:15:26Z,http://arxiv.org/abs/1703.08098v1,http://arxiv.org/pdf/1703.08098v1,An overview of embedding models of entities and relationships for   knowledge base completion,overview embed model entiti relationship knowledg base complet,"Knowledge bases of real-world facts about entities and their relationships are useful resources for a variety of natural language processing tasks. However, because knowledge bases are typically incomplete, it is useful to be able to perform knowledge base completion, i.e., predict whether a relationship not in the knowledge base is likely to be true. This article presents an overview of embedding models of entities and relationships for knowledge base completion, with up-to-date experimental results on two standard evaluation tasks of link prediction (i.e. entity prediction) and triple classification.",knowledg base real world fact entiti relationship use resourc varieti natur languag process task howev becaus knowledg base typic incomplet use abl perform knowledg base complet predict whether relationship knowledg base like true articl present overview embed model entiti relationship knowledg base complet date experiment result two standard evalu task link predict entiti predict tripl classif,['Dat Quoc Nguyen'],"['cs.CL', 'cs.AI', 'cs.IR']",False,True,False,False,False,False
323,2017-03-28T14:06:07Z,2017-03-23T14:20:52Z,http://arxiv.org/abs/1703.08084v1,http://arxiv.org/pdf/1703.08084v1,Multimodal Compact Bilinear Pooling for Multimodal Neural Machine   Translation,multimod compact bilinear pool multimod neural machin translat,"In state-of-the-art Neural Machine Translation, an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions. Approaches to pool two modalities usually include element-wise product, sum or concatenation. In this paper, we evaluate the more advanced Multimodal Compact Bilinear pooling method, which takes the outer product of two vectors to combine the attention features for the two modalities. This has been previously investigated for visual question answering. We try out this approach for multimodal image caption translation and show improvements compared to basic combination methods.",state art neural machin translat attent mechan use dure decod enhanc translat everi step decod use mechan focus differ part sourc sentenc gather use inform befor output target word recent effect attent mechan also explor multimod task becom possibl focus sentenc part imag region approach pool two modal usual includ element wise product sum concaten paper evalu advanc multimod compact bilinear pool method take outer product two vector combin attent featur two modal previous investig visual question answer tri approach multimod imag caption translat show improv compar basic combin method,"['Jean-Benoit Delbrouck', 'Stephane Dupont']",['cs.CL'],False,True,False,False,False,False
331,2017-03-28T14:06:12Z,2017-03-22T00:37:33Z,http://arxiv.org/abs/1703.07476v1,http://arxiv.org/pdf/1703.07476v1,Topic Identification for Speech without ASR,topic identif speech without asr,"Modern topic identification (topic ID) systems for speech use automatic speech recognition (ASR) to produce speech transcripts, and perform supervised classification on such ASR outputs. However, under resource-limited conditions, the manually transcribed speech required to develop standard ASR systems can be severely limited or unavailable. In this paper, we investigate alternative unsupervised solutions to obtaining tokenizations of speech in terms of a vocabulary of automatically discovered word-like or phoneme-like units, without depending on the supervised training of ASR systems. Moreover, using automatic phoneme-like tokenizations, we demonstrate that a convolutional neural network based framework for learning spoken document representations provides competitive performance compared to a standard bag-of-words representation, as evidenced by comprehensive topic ID evaluations on both single-label and multi-label classification tasks.",modern topic identif topic id system speech use automat speech recognit asr produc speech transcript perform supervis classif asr output howev resourc limit condit manual transcrib speech requir develop standard asr system sever limit unavail paper investig altern unsupervis solut obtain token speech term vocabulari automat discov word like phonem like unit without depend supervis train asr system moreov use automat phonem like token demonstr convolut neural network base framework learn spoken document represent provid competit perform compar standard bag word represent evidenc comprehens topic id evalu singl label multi label classif task,"['Chunxi Liu', 'Jan Trmal', 'Matthew Wiesner', 'Craig Harman', 'Sanjeev Khudanpur']",['cs.CL'],False,True,False,False,True,False
337,2017-03-28T14:06:12Z,2017-03-20T08:19:43Z,http://arxiv.org/abs/1703.06630v1,http://arxiv.org/pdf/1703.06630v1,Automatic Text Summarization Approaches to Speed up Topic Model Learning   Process,automat text summar approach speed topic model learn process,"The number of documents available into Internet moves each day up. For this reason, processing this amount of information effectively and expressibly becomes a major concern for companies and scientists. Methods that represent a textual document by a topic representation are widely used in Information Retrieval (IR) to process big data such as Wikipedia articles. One of the main difficulty in using topic model on huge data collection is related to the material resources (CPU time and memory) required for model estimate. To deal with this issue, we propose to build topic spaces from summarized documents. In this paper, we present a study of topic space representation in the context of big data. The topic space representation behavior is analyzed on different languages. Experiments show that topic spaces estimated from text summaries are as relevant as those estimated from the complete documents. The real advantage of such an approach is the processing time gain: we showed that the processing time can be drastically reduced using summarized documents (more than 60\% in general). This study finally points out the differences between thematic representations of documents depending on the targeted languages such as English or latin languages.",number document avail internet move day reason process amount inform effect express becom major concern compani scientist method repres textual document topic represent wide use inform retriev ir process big data wikipedia articl one main difficulti use topic model huge data collect relat materi resourc cpu time memori requir model estim deal issu propos build topic space summar document paper present studi topic space represent context big data topic space represent behavior analyz differ languag experi show topic space estim text summari relev estim complet document real advantag approach process time gain show process time drastic reduc use summar document general studi final point differ themat represent document depend target languag english latin languag,"['Mohamed Morchid', 'Juan-Manuel Torres-Moreno', 'Richard Dufour', 'Javier Ramírez-Rodríguez', 'Georges Linarès']","['cs.IR', 'cs.CL']",False,True,False,False,False,False
343,2017-03-28T14:06:16Z,2017-03-17T17:16:02Z,http://arxiv.org/abs/1703.06108v1,http://arxiv.org/abs/1703.06108v1,Global Entity Ranking Across Multiple Languages,global entiti rank across multipl languag,"We present work on building a global long-tailed ranking of entities across multiple languages using Wikipedia and Freebase knowledge bases. We identify multiple features and build a model to rank entities using a ground-truth dataset of more than 10 thousand labels. The final system ranks 27 million entities with 75% precision and 48% F1 score. We provide performance evaluation and empirical evidence of the quality of ranking across languages, and open the final ranked lists for future research.",present work build global long tail rank entiti across multipl languag use wikipedia freebas knowledg base identifi multipl featur build model rank entiti use ground truth dataset thousand label final system rank million entiti precis score provid perform evalu empir evid qualiti rank across languag open final rank list futur research,"['Prantik Bhattacharyya', 'Nemanja Spasojevic']","['cs.IR', 'cs.CL', 'cs.SI', 'H.3.1']",False,True,False,False,False,False
348,2017-03-28T14:06:16Z,2017-03-17T04:03:36Z,http://arxiv.org/abs/1703.05706v2,http://arxiv.org/pdf/1703.05706v2,Improving Document Clustering by Eliminating Unnatural Language,improv document cluster elimin unnatur languag,"Technical documents contain a fair amount of unnatural language, such as tables, formulas, pseudo-codes, etc. Unnatural language can be an important factor of confusing existing NLP tools. This paper presents an effective method of distinguishing unnatural language from natural language, and evaluates the impact of unnatural language detection on NLP tasks such as document clustering. We view this problem as an information extraction task and build a multiclass classification model identifying unnatural language components into four categories. First, we create a new annotated corpus by collecting slides and papers in various formats, PPT, PDF, and HTML, where unnatural language components are annotated into four categories. We then explore features available from plain text to build a statistical model that can handle any format as long as it is converted into plain text. Our experiments show that removing unnatural language components gives an absolute improvement in document clustering up to 15%. Our corpus and tool are publicly available.",technic document contain fair amount unnatur languag tabl formula pseudo code etc unnatur languag import factor confus exist nlp tool paper present effect method distinguish unnatur languag natur languag evalu impact unnatur languag detect nlp task document cluster view problem inform extract task build multiclass classif model identifi unnatur languag compon four categori first creat new annot corpus collect slide paper various format ppt pdf html unnatur languag compon annot four categori explor featur avail plain text build statist model handl ani format long convert plain text experi show remov unnatur languag compon give absolut improv document cluster corpus tool public avail,"['Myungha Jang', 'Jinho D. Choi', 'James Allan']","['cs.IR', 'cs.CL']",False,True,False,False,False,False
349,2017-03-28T14:06:16Z,2017-03-16T03:15:22Z,http://arxiv.org/abs/1703.05465v1,http://arxiv.org/pdf/1703.05465v1,Neobility at SemEval-2017 Task 1: An Attention-based Sentence Similarity   Model,neobil semev task attent base sentenc similar model,"This paper describes a neural-network model which performed competitively (top 6) at the SemEval 2017 cross-lingual Semantic Textual Similarity (STS) task. Our system employs an attention-based recurrent neural network model that optimizes the sentence similarity. In this paper, we describe our participation in the multilingual STS task which measures similarity across English, Spanish, and Arabic.",paper describ neural network model perform competit top semev cross lingual semant textual similar sts task system employ attent base recurr neural network model optim sentenc similar paper describ particip multilingu sts task measur similar across english spanish arab,"['Wenli Zhuang', 'Ernie Chang']",['cs.CL'],False,True,False,False,False,False
353,2017-03-28T14:06:20Z,2017-03-15T17:01:20Z,http://arxiv.org/abs/1703.05260v1,http://arxiv.org/pdf/1703.05260v1,InScript: Narrative texts annotated with script information,inscript narrat text annot script inform,"This paper presents the InScript corpus (Narrative Texts Instantiating Script structure). InScript is a corpus of 1,000 stories centered around 10 different scenarios. Verbs and noun phrases are annotated with event and participant types, respectively. Additionally, the text is annotated with coreference information. The corpus shows rich lexical variation and will serve as a unique resource for the study of the role of script knowledge in natural language processing.",paper present inscript corpus narrat text instanti script structur inscript corpus stori center around differ scenario verb noun phrase annot event particip type respect addit text annot corefer inform corpus show rich lexic variat serv uniqu resourc studi role script knowledg natur languag process,"['Ashutosh Modi', 'Tatjana Anikina', 'Simon Ostermann', 'Manfred Pinkal']","['cs.CL', 'cs.AI']",False,True,False,False,False,False
354,2017-03-28T14:06:20Z,2017-03-16T08:57:29Z,http://arxiv.org/abs/1703.05123v2,http://arxiv.org/pdf/1703.05123v2,Character-based Neural Embeddings for Tweet Clustering,charact base neural embed tweet cluster,In this paper we show how the performance of tweet clustering can be improved by leveraging character-based neural networks. The proposed approach overcomes the limitations related to the vocabulary explosion in the word-based models and allows for the seamless processing of the multilingual content. Our evaluation results and code are available on-line at https://github.com/vendi12/tweet2vec_clustering,paper show perform tweet cluster improv leverag charact base neural network propos approach overcom limit relat vocabulari explos word base model allow seamless process multilingu content evalu result code avail line https github com vendi tweetvec cluster,"['Svitlana Vakulenko', 'Lyndon Nixon', 'Mihai Lupu']","['cs.IR', 'cs.CL']",False,True,False,False,True,False
356,2017-03-28T14:06:20Z,2017-03-15T04:57:17Z,http://arxiv.org/abs/1703.04929v1,http://arxiv.org/pdf/1703.04929v1,SyntaxNet Models for the CoNLL 2017 Shared Task,syntaxnet model conll share task,"We describe a baseline dependency parsing system for the CoNLL2017 Shared Task. This system, which we call ""ParseySaurus,"" uses the DRAGNN framework [Kong et al, 2017] to combine transition-based recurrent parsing and tagging with character-based word representations. On the v1.3 Universal Dependencies Treebanks, the new system outpeforms the publicly available, state-of-the-art ""Parsey's Cousins"" models by 3.47% absolute Labeled Accuracy Score (LAS) across 52 treebanks.",describ baselin depend pars system conll share task system call parseysaurus use dragnn framework kong et al combin transit base recurr pars tag charact base word represent univers depend treebank new system outpeform public avail state art parsey cousin model absolut label accuraci score las across treebank,"['Chris Alberti', 'Daniel Andor', 'Ivan Bogatyy', 'Michael Collins', 'Dan Gillick', 'Lingpeng Kong', 'Terry Koo', 'Ji Ma', 'Mark Omernick', 'Slav Petrov', 'Chayut Thanapirom', 'Zora Tung', 'David Weiss']",['cs.CL'],False,True,False,False,False,False
358,2017-03-28T14:06:20Z,2017-03-15T03:30:13Z,http://arxiv.org/abs/1703.04908v1,http://arxiv.org/pdf/1703.04908v1,Emergence of Grounded Compositional Language in Multi-Agent Populations,emerg ground composit languag multi agent popul,"By capturing statistical patterns in large corpora, machine learning has enabled significant advances in natural language processing, including in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply capturing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. Towards this end, we propose a multi-agent learning environment and learning methods that bring about emergence of a basic compositional language. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of non-verbal communication such as pointing and guiding when language communication is unavailable.",captur statist pattern larg corpora machin learn enabl signific advanc natur languag process includ machin translat question answer sentiment analysi howev agent intellig interact human simpli captur statist pattern insuffici paper investig ground composit languag emerg mean achiev goal multi agent popul toward end propos multi agent learn environ learn method bring emerg basic composit languag languag repres stream abstract discret symbol utter agent time nonetheless coher structur possess defin vocabulari syntax also observ emerg non verbal communic point guid languag communic unavail,"['Igor Mordatch', 'Pieter Abbeel']","['cs.AI', 'cs.CL']",False,True,False,False,False,False
359,2017-03-28T14:06:20Z,2017-03-15T02:26:25Z,http://arxiv.org/abs/1703.04887v1,http://arxiv.org/pdf/1703.04887v1,Improving Neural Machine Translation with Conditional Sequence   Generative Adversarial Nets,improv neural machin translat condit sequenc generat adversari net,"This paper proposes a new route for applying the generative adversarial nets (GANs) to NLP tasks (taking the neural machine translation as an instance) and the widespread perspective that GANs can't work well in the NLP area turns out to be unreasonable. In this work, we build a conditional sequence generative adversarial net which comprises of two adversarial sub models, a generative model (generator) which translates the source sentence into the target sentence as the traditional NMT models do and a discriminative model (discriminator) which discriminates the machine-translated target sentence from the human-translated sentence. From the perspective of Turing test, the proposed model is to generate the translation which is indistinguishable from the human-translated one. Experiments show that the proposed model achieves significant improvements than the traditional NMT model. In Chinese-English translation tasks, we obtain up to +2.0 BLEU points improvement. To the best of our knowledge, this is the first time that the quantitative results about the application of GANs in the traditional NLP task is reported. Meanwhile, we present detailed strategies for GAN training. In addition, We find that the discriminator of the proposed model shows great capability in data cleaning.",paper propos new rout appli generat adversari net gan nlp task take neural machin translat instanc widespread perspect gan work well nlp area turn unreason work build condit sequenc generat adversari net compris two adversari sub model generat model generat translat sourc sentenc target sentenc tradit nmt model discrimin model discrimin discrimin machin translat target sentenc human translat sentenc perspect ture test propos model generat translat indistinguish human translat one experi show propos model achiev signific improv tradit nmt model chines english translat task obtain bleu point improv best knowledg first time quantit result applic gan tradit nlp task report meanwhil present detail strategi gan train addit find discrimin propos model show great capabl data clean,"['Zhen Yang', 'Wei Chen', 'Feng Wang', 'Bo Xu']",['cs.CL'],False,True,False,False,False,False
362,2017-03-28T14:06:24Z,2017-03-14T23:25:34Z,http://arxiv.org/abs/1703.04826v1,http://arxiv.org/pdf/1703.04826v1,Encoding Sentences with Graph Convolutional Networks for Semantic Role   Labeling,encod sentenc graph convolut network semant role label,"Semantic role labeling (SRL) is the task of identifying the predicate-argument structure of a sentence. It is typically regarded as an important step in the standard natural language processing pipeline, providing information to downstream tasks such as information extraction and question answering. As the semantic representations are closely related to syntactic ones, we exploit syntactic information in our model. We propose a version of graph convolutional networks (GCNs), a recent class of multilayer neural networks operating on graphs, suited to modeling syntactic dependency graphs. GCNs over syntactic dependency trees are used as sentence encoders, producing latent feature representations of words in a sentence and capturing information relevant to predicting the semantic representations. We observe that GCN layers are complementary to LSTM ones: when we stack both GCN and LSTM layers, we obtain a substantial improvement over an already state-of-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009) both for Chinese and English.",semant role label srl task identifi predic argument structur sentenc typic regard import step standard natur languag process pipelin provid inform downstream task inform extract question answer semant represent close relat syntact one exploit syntact inform model propos version graph convolut network gcns recent class multilay neural network oper graph suit model syntact depend graph gcns syntact depend tree use sentenc encod produc latent featur represent word sentenc captur inform relev predict semant represent observ gcn layer complementari lstm one stack gcn lstm layer obtain substanti improv alreadi state art lstm srl model result best report score standard benchmark conll chines english,"['Diego Marcheggiani', 'Ivan Titov']","['cs.CL', 'cs.LG']",False,True,False,False,False,False
368,2017-03-28T14:06:24Z,2017-03-13T17:34:18Z,http://arxiv.org/abs/1703.04498v1,http://arxiv.org/pdf/1703.04498v1,High-Throughput and Language-Agnostic Entity Disambiguation and Linking   on User Generated Data,high throughput languag agnost entiti disambigu link user generat data,"The Entity Disambiguation and Linking (EDL) task matches entity mentions in text to a unique Knowledge Base (KB) identifier such as a Wikipedia or Freebase id. It plays a critical role in the construction of a high quality information network, and can be further leveraged for a variety of information retrieval and NLP tasks such as text categorization and document tagging. EDL is a complex and challenging problem due to ambiguity of the mentions and real world text being multi-lingual. Moreover, EDL systems need to have high throughput and should be lightweight in order to scale to large datasets and run on off-the-shelf machines. More importantly, these systems need to be able to extract and disambiguate dense annotations from the data in order to enable an Information Retrieval or Extraction task running on the data to be more efficient and accurate. In order to address all these challenges, we present the Lithium EDL system and algorithm - a high-throughput, lightweight, language-agnostic EDL system that extracts and correctly disambiguates 75% more entities than state-of-the-art EDL systems and is significantly faster than them.",entiti disambigu link edl task match entiti mention text uniqu knowledg base kb identifi wikipedia freebas id play critic role construct high qualiti inform network leverag varieti inform retriev nlp task text categor document tag edl complex challeng problem due ambigu mention real world text multi lingual moreov edl system need high throughput lightweight order scale larg dataset run shelf machin import system need abl extract disambigu dens annot data order enabl inform retriev extract task run data effici accur order address challeng present lithium edl system algorithm high throughput lightweight languag agnost edl system extract correct disambigu entiti state art edl system signific faster,"['Preeti Bhargava', 'Nemanja Spasojevic', 'Guoning Hu']","['cs.IR', 'cs.AI', 'cs.CL']",False,True,False,False,False,False
374,2017-03-28T14:06:28Z,2017-03-13T11:19:56Z,http://arxiv.org/abs/1703.04336v1,http://arxiv.org/pdf/1703.04336v1,A Visual Representation of Wittgenstein's Tractatus Logico-Philosophicus,visual represent wittgenstein tractatus logico philosophicus,"In this paper we present a data visualization method together with its potential usefulness in digital humanities and philosophy of language. We compile a multilingual parallel corpus from different versions of Wittgenstein's Tractatus Logico-Philosophicus, including the original in German and translations into English, Spanish, French, and Russian. Using this corpus, we compute a similarity measure between propositions and render a visual network of relations for different languages.",paper present data visual method togeth potenti use digit human philosophi languag compil multilingu parallel corpus differ version wittgenstein tractatus logico philosophicus includ origin german translat english spanish french russian use corpus comput similar measur proposit render visual network relat differ languag,"['Anca Bucur', 'Sergiu Nisioi']","['cs.IR', 'cs.CL']",False,True,False,False,False,False
375,2017-03-28T14:06:28Z,2017-03-13T11:03:40Z,http://arxiv.org/abs/1703.04330v1,http://arxiv.org/pdf/1703.04330v1,Story Cloze Ending Selection Baselines and Data Examination,stori cloze end select baselin data examin,"This paper describes two supervised baseline systems for the Story Cloze Test Shared Task (Mostafazadeh et al., 2016a). We first build a classifier using features based on word embeddings and semantic similarity computation. We further implement a neural LSTM system with different encoding strategies that try to model the relation between the story and the provided endings. Our experiments show that a model using representation features based on average word embedding vectors over the given story words and the candidate ending sentences words, joint with similarity features between the story and candidate ending representations performed better than the neural models. Our best model achieves an accuracy of 72.42, ranking 3rd in the official evaluation.",paper describ two supervis baselin system stori cloze test share task mostafazadeh et al first build classifi use featur base word embed semant similar comput implement neural lstm system differ encod strategi tri model relat stori provid end experi show model use represent featur base averag word embed vector given stori word candid end sentenc word joint similar featur stori candid end represent perform better neural model best model achiev accuraci rank rd offici evalu,"['Todor Mihaylov', 'Anette Frank']",['cs.CL'],False,True,False,False,False,False
377,2017-03-28T14:06:28Z,2017-03-14T20:26:32Z,http://arxiv.org/abs/1703.04213v2,http://arxiv.org/pdf/1703.04213v2,MetaPAD: Meta Pattern Discovery from Massive Text Corpora,metapad meta pattern discoveri massiv text corpora,"Mining textual patterns in news, tweets, papers, and many other kinds of text corpora has been an active theme in text mining and NLP research. Previous studies adopt a dependency parsing-based pattern discovery approach. However, the parsing results lose rich context around entities in the patterns, and the process is costly for a corpus of large scale. In this study, we propose a novel typed textual pattern structure, called meta pattern, which is extended to a frequent, informative, and precise subsequence pattern in certain context. We propose an efficient framework, called MetaPAD, which discovers meta patterns from massive corpora with three techniques: (1) it develops a context-aware segmentation method to carefully determine the boundaries of patterns with a learnt pattern quality assessment function, which avoids costly dependency parsing and generates high-quality patterns; (2) it identifies and groups synonymous meta patterns from multiple facets---their types, contexts, and extractions; and (3) it examines type distributions of entities in the instances extracted by each group of patterns, and looks for appropriate type levels to make discovered patterns precise. Experiments demonstrate that our proposed framework discovers high-quality typed textual patterns efficiently from different genres of massive corpora and facilitates information extraction.",mine textual pattern news tweet paper mani kind text corpora activ theme text mine nlp research previous studi adopt depend pars base pattern discoveri approach howev pars result lose rich context around entiti pattern process cost corpus larg scale studi propos novel type textual pattern structur call meta pattern extend frequent inform precis subsequ pattern certain context propos effici framework call metapad discov meta pattern massiv corpora three techniqu develop context awar segment method care determin boundari pattern learnt pattern qualiti assess function avoid cost depend pars generat high qualiti pattern identifi group synonym meta pattern multipl facet type context extract examin type distribut entiti instanc extract group pattern look appropri type level make discov pattern precis experi demonstr propos framework discov high qualiti type textual pattern effici differ genr massiv corpora facilit inform extract,"['Meng Jiang', 'Jingbo Shang', 'Taylor Cassidy', 'Xiang Ren', 'Lance M. Kaplan', 'Timothy P. Hanratty', 'Jiawei Han']",['cs.CL'],False,True,False,False,False,False
383,2017-03-28T14:06:31Z,2017-03-11T07:37:37Z,http://arxiv.org/abs/1703.04718v1,http://arxiv.org/pdf/1703.04718v1,Extending Automatic Discourse Segmentation for Texts in Spanish to   Catalan,extend automat discours segment text spanish catalan,"At present, automatic discourse analysis is a relevant research topic in the field of NLP. However, discourse is one of the phenomena most difficult to process. Although discourse parsers have been already developed for several languages, this tool does not exist for Catalan. In order to implement this kind of parser, the first step is to develop a discourse segmenter. In this article we present the first discourse segmenter for texts in Catalan. This segmenter is based on Rhetorical Structure Theory (RST) for Spanish, and uses lexical and syntactic information to translate rules valid for Spanish into rules for Catalan. We have evaluated the system by using a gold standard corpus including manually segmented texts and results are promising.",present automat discours analysi relev research topic field nlp howev discours one phenomena difficult process although discours parser alreadi develop sever languag tool doe exist catalan order implement kind parser first step develop discours segment articl present first discours segment text catalan segment base rhetor structur theori rst spanish use lexic syntact inform translat rule valid spanish rule catalan evalu system use gold standard corpus includ manual segment text result promis,"['Iria da Cunha', 'Eric SanJuan', 'Juan-Manuel Torres-Moreno', 'Irene Castellón']",['cs.CL'],False,True,False,False,False,False
384,2017-03-28T14:06:31Z,2017-03-11T07:35:28Z,http://arxiv.org/abs/1703.03923v1,http://arxiv.org/pdf/1703.03923v1,A German Corpus for Text Similarity Detection Tasks,german corpus text similar detect task,"Text similarity detection aims at measuring the degree of similarity between a pair of texts. Corpora available for text similarity detection are designed to evaluate the algorithms to assess the paraphrase level among documents. In this paper we present a textual German corpus for similarity detection. The purpose of this corpus is to automatically assess the similarity between a pair of texts and to evaluate different similarity measures, both for whole documents or for individual sentences. Therefore we have calculated several simple measures on our corpus based on a library of similarity functions.",text similar detect aim measur degre similar pair text corpora avail text similar detect design evalu algorithm assess paraphras level among document paper present textual german corpus similar detect purpos corpus automat assess similar pair text evalu differ similar measur whole document individu sentenc therefor calcul sever simpl measur corpus base librari similar function,"['Juan-Manuel Torres-Moreno', 'Gerardo Sierra', 'Peter Peinl']","['cs.IR', 'cs.CL']",False,True,False,False,False,False
387,2017-03-28T14:06:31Z,2017-03-10T17:27:38Z,http://arxiv.org/abs/1703.03771v1,http://arxiv.org/pdf/1703.03771v1,Coping with Construals in Broad-Coverage Semantic Annotation of   Adpositions,cope construal broad coverag semant annot adposit,"We consider the semantics of prepositions, revisiting a broad-coverage annotation scheme used for annotating all 4,250 preposition tokens in a 55,000 word corpus of English. Attempts to apply the scheme to adpositions and case markers in other languages, as well as some problematic cases in English, have led us to reconsider the assumption that a preposition's lexical contribution is equivalent to the role/relation that it mediates. Our proposal is to embrace the potential for construal in adposition use, expressing such phenomena directly at the token level to manage complexity and avoid sense proliferation. We suggest a framework to represent both the scene role and the adposition's lexical function so they can be annotated at scale---supporting automatic, statistical processing of domain-general language---and sketch how this representation would inform a constructional analysis.",consid semant preposit revisit broad coverag annot scheme use annot preposit token word corpus english attempt appli scheme adposit case marker languag well problemat case english led us reconsid assumpt preposit lexic contribut equival role relat mediat propos embrac potenti construal adposit use express phenomena direct token level manag complex avoid sens prolifer suggest framework repres scene role adposit lexic function annot scale support automat statist process domain general languag sketch represent would inform construct analysi,"['Jena D. Hwang', 'Archna Bhatia', 'Na-Rae Han', ""Tim O'Gorman"", 'Vivek Srikumar', 'Nathan Schneider']",['cs.CL'],False,True,False,False,False,False
390,2017-03-28T14:06:36Z,2017-03-10T11:58:48Z,http://arxiv.org/abs/1703.03640v1,http://arxiv.org/pdf/1703.03640v1,A Study of Metrics of Distance and Correlation Between Ranked Lists for   Compositionality Detection,studi metric distanc correl rank list composit detect,"Compositionality in language refers to how much the meaning of some phrase can be decomposed into the meaning of its constituents and the way these constituents are combined. Based on the premise that substitution by synonyms is meaning-preserving, compositionality can be approximated as the semantic similarity between a phrase and a version of that phrase where words have been replaced by their synonyms. Different ways of representing such phrases exist (e.g., vectors [1] or language models [2]), and the choice of representation affects the measurement of semantic similarity.   We propose a new compositionality detection method that represents phrases as ranked lists of term weights. Our method approximates the semantic similarity between two ranked list representations using a range of well-known distance and correlation metrics. In contrast to most state-of-the-art approaches in compositionality detection, our method is completely unsupervised. Experiments with a publicly available dataset of 1048 human-annotated phrases shows that, compared to strong supervised baselines, our approach provides superior measurement of compositionality using any of the distance and correlation metrics considered.",composit languag refer much mean phrase decompos mean constitu way constitu combin base premis substitut synonym mean preserv composit approxim semant similar phrase version phrase word replac synonym differ way repres phrase exist vector languag model choic represent affect measur semant similar propos new composit detect method repres phrase rank list term weight method approxim semant similar two rank list represent use rang well known distanc correl metric contrast state art approach composit detect method complet unsupervis experi public avail dataset human annot phrase show compar strong supervis baselin approach provid superior measur composit use ani distanc correl metric consid,"['Christina Lioma', 'Niels Dalum Hansen']",['cs.CL'],False,True,False,False,False,False
428,2017-03-28T14:08:52Z,2017-03-26T05:44:56Z,http://arxiv.org/abs/1703.08769v1,http://arxiv.org/pdf/1703.08769v1,Open Vocabulary Scene Parsing,open vocabulari scene pars,"Recognizing arbitrary objects in the wild has been a challenging problem due to the limitations of existing classification models and datasets. In this paper, we propose a new task that aims at parsing scene with a large and open vocabulary, and several evaluation metrics are explored for this problem. Our proposed approach to this problem is a joint image pixel and word concept embeddings framework, where word concepts are connected by semantic relations. We validate the open vocabulary prediction ability of our framework on ADE20K dataset which covers a wide variety of scenes and objects. We further explore the trained joint embedding space to show its interpretability.",recogn arbitrari object wild challeng problem due limit exist classif model dataset paper propos new task aim pars scene larg open vocabulari sever evalu metric explor problem propos approach problem joint imag pixel word concept embed framework word concept connect semant relat valid open vocabulari predict abil framework adek dataset cover wide varieti scene object explor train joint embed space show interpret,"['Hang Zhao', 'Xavier Puig', 'Bolei Zhou', 'Sanja Fidler', 'Antonio Torralba']","['cs.CV', 'cs.AI']",False,True,False,False,False,False
442,2017-03-28T14:09:00Z,2017-03-24T16:27:57Z,http://arxiv.org/abs/1703.08492v1,http://arxiv.org/pdf/1703.08492v1,Content-Based Image Retrieval Based on Late Fusion of Binary and Local   Descriptors,content base imag retriev base late fusion binari local descriptor,"One of the challenges in Content-Based Image Retrieval (CBIR) is to reduce the semantic gaps between low-level features and high-level semantic concepts. In CBIR, the images are represented in the feature space and the performance of CBIR depends on the type of selected feature representation. Late fusion also known as visual words integration is applied to enhance the performance of image retrieval. The recent advances in image retrieval diverted the focus of research towards the use of binary descriptors as they are reported computationally efficient. In this paper, we aim to investigate the late fusion of Fast Retina Keypoint (FREAK) and Scale Invariant Feature Transform (SIFT). The late fusion of binary and local descriptor is selected because among binary descriptors, FREAK has shown good results in classification-based problems while SIFT is robust to translation, scaling, rotation and small distortions. The late fusion of FREAK and SIFT integrates the performance of both feature descriptors for an effective image retrieval. Experimental results and comparisons show that the proposed late fusion enhances the performances of image retrieval.",one challeng content base imag retriev cbir reduc semant gap low level featur high level semant concept cbir imag repres featur space perform cbir depend type select featur represent late fusion also known visual word integr appli enhanc perform imag retriev recent advanc imag retriev divert focus research toward use binari descriptor report comput effici paper aim investig late fusion fast retina keypoint freak scale invari featur transform sift late fusion binari local descriptor select becaus among binari descriptor freak shown good result classif base problem sift robust translat scale rotat small distort late fusion freak sift integr perform featur descriptor effect imag retriev experiment result comparison show propos late fusion enhanc perform imag retriev,"['Nouman Ali', 'Danish Ali Mazhar', 'Zeshan Iqbal', 'Rehan Ashraf', 'Jawad Ahmed', 'Farrukh Zeeshan Khan']",['cs.CV'],False,True,False,True,False,False
503,2017-03-28T14:09:26Z,2017-03-27T05:09:06Z,http://arxiv.org/abs/1703.08931v1,http://arxiv.org/pdf/1703.08931v1,Palindromic Decompositions with Gaps and Errors,palindrom decomposit gap error,"Identifying palindromes in sequences has been an interesting line of research in combinatorics on words and also in computational biology, after the discovery of the relation of palindromes in the DNA sequence with the HIV virus. Efficient algorithms for the factorization of sequences into palindromes and maximal palindromes have been devised in recent years. We extend these studies by allowing gaps in decompositions and errors in palindromes, and also imposing a lower bound to the length of acceptable palindromes.   We first present an algorithm for obtaining a palindromic decomposition of a string of length n with the minimal total gap length in time O(n log n * g) and space O(n g), where g is the number of allowed gaps in the decomposition. We then consider a decomposition of the string in maximal \delta-palindromes (i.e. palindromes with \delta errors under the edit or Hamming distance) and g allowed gaps. We present an algorithm to obtain such a decomposition with the minimal total gap length in time O(n (g + \delta)) and space O(n g).",identifi palindrom sequenc interest line research combinator word also comput biolog discoveri relat palindrom dna sequenc hiv virus effici algorithm factor sequenc palindrom maxim palindrom devis recent year extend studi allow gap decomposit error palindrom also impos lower bound length accept palindrom first present algorithm obtain palindrom decomposit string length minim total gap length time log space number allow gap decomposit consid decomposit string maxim delta palindrom palindrom delta error edit ham distanc allow gap present algorithm obtain decomposit minim total gap length time delta space,"['Michał Adamczyk', 'Mai Alzamel', 'Panagiotis Charalampopoulos', 'Costas S. Iliopoulos', 'Jakub Radoszewski']",['cs.DS'],False,True,False,False,False,False
599,2017-03-28T14:10:04Z,2017-03-06T04:41:19Z,http://arxiv.org/abs/1703.01009v2,http://arxiv.org/pdf/1703.01009v2,Optimal Time and Space Construction of Suffix Arrays and LCP Arrays for   Integer Alphabets,optim time space construct suffix array lcp array integ alphabet,"Suffix arrays and LCP arrays are one of the most fundamental data structures widely used for various kinds of string processing. Many problems can be solved efficiently by using suffix arrays, or a pair of suffix arrays and LCP arrays. In this paper, we consider two problems for a string of length $N$, the characters of which are represented as integers in $[1, \dots, \sigma]$ for $1 \leq \sigma \leq N$; the string contains $\sigma$ distinct characters, (1) construction of the suffix array and (2) simultaneous construction of both the suffix array and the LCP array. In the word RAM model, we propose algorithms to solve both the problems in $O(N)$ time using $O(1)$ extra words, which are optimal in time and space. Extra words mean the required space except for the space of the input string and output suffix array and LCP array. Our contribution improves the previous most efficient algorithm that runs in $O(N)$ time using $\sigma+O(1)$ extra words for the suffix array construction proposed by [Nong, TOIS 2013], and it improves the previous most efficient solution that runs in $O(N)$ time using $\sigma + O(1)$ extra words for both suffix array and LCP array construction using the combination of [Nong, TOIS 2013] and [Manzini, SWAT 2004].   Another optimal time and space algorithm to construct the suffix array was proposed by [Li et al, arXiv 2016] very recently and independently. Our algorithm is simpler than theirs, and it allows us to solve the second problem in optimal time and space.",suffix array lcp array one fundament data structur wide use various kind string process mani problem solv effici use suffix array pair suffix array lcp array paper consid two problem string length charact repres integ dot sigma leq sigma leq string contain sigma distinct charact construct suffix array simultan construct suffix array lcp array word ram model propos algorithm solv problem time use extra word optim time space extra word mean requir space except space input string output suffix array lcp array contribut improv previous effici algorithm run time use sigma extra word suffix array construct propos nong toi improv previous effici solut run time use sigma extra word suffix array lcp array construct use combin nong toi manzini swat anoth optim time space algorithm construct suffix array propos li et al arxiv veri recent independ algorithm simpler allow us solv second problem optim time space,['Keisuke Goto'],['cs.DS'],False,True,False,False,False,False
1214,2017-03-28T14:01:49Z,2017-03-25T15:37:09Z,http://arxiv.org/abs/1703.08705v1,http://arxiv.org/pdf/1703.08705v1,Comparing Rule-Based and Deep Learning Models for Patient Phenotyping,compar rule base deep learn model patient phenotyp,"Objective: We investigate whether deep learning techniques for natural language processing (NLP) can be used efficiently for patient phenotyping. Patient phenotyping is a classification task for determining whether a patient has a medical condition, and is a crucial part of secondary analysis of healthcare data. We assess the performance of deep learning algorithms and compare them with classical NLP approaches.   Materials and Methods: We compare convolutional neural networks (CNNs), n-gram models, and approaches based on cTAKES that extract pre-defined medical concepts from clinical notes and use them to predict patient phenotypes. The performance is tested on 10 different phenotyping tasks using 1,610 discharge summaries extracted from the MIMIC-III database.   Results: CNNs outperform other phenotyping algorithms in all 10 tasks. The average F1-score of our model is 76 (PPV of 83, and sensitivity of 71) with our model having an F1-score up to 37 points higher than alternative approaches. We additionally assess the interpretability of our model by presenting a method that extracts the most salient phrases for a particular prediction.   Conclusion: We show that NLP methods based on deep learning improve the performance of patient phenotyping. Our CNN-based algorithm automatically learns the phrases associated with each patient phenotype. As such, it reduces the annotation complexity for clinical domain experts, who are normally required to develop task-specific annotation rules and identify relevant phrases. Our method performs well in terms of both performance and interpretability, which indicates that deep learning is an effective approach to patient phenotyping based on clinicians' notes.",object investig whether deep learn techniqu natur languag process nlp use effici patient phenotyp patient phenotyp classif task determin whether patient medic condit crucial part secondari analysi healthcar data assess perform deep learn algorithm compar classic nlp approach materi method compar convolut neural network cnns gram model approach base ctake extract pre defin medic concept clinic note use predict patient phenotyp perform test differ phenotyp task use discharg summari extract mimic iii databas result cnns outperform phenotyp algorithm task averag score model ppv sensit model score point higher altern approach addit assess interpret model present method extract salient phrase particular predict conclus show nlp method base deep learn improv perform patient phenotyp cnn base algorithm automat learn phrase associ patient phenotyp reduc annot complex clinic domain expert normal requir develop task specif annot rule identifi relev phrase method perform well term perform interpret indic deep learn effect approach patient phenotyp base clinician note,"['Sebastian Gehrmann', 'Franck Dernoncourt', 'Yeran Li', 'Eric T. Carlson', 'Joy T. Wu', 'Jonathan Welt', 'John Foote Jr.', 'Edward T. Moseley', 'David W. Grant', 'Patrick D. Tyler', 'Leo Anthony Celi']","['cs.CL', 'cs.AI', 'cs.NE', 'stat.ML']",False,True,False,False,False,False
1218,2017-03-28T14:01:49Z,2017-03-24T14:49:58Z,http://arxiv.org/abs/1703.08544v1,http://arxiv.org/pdf/1703.08544v1,D.TRUMP: Data-mining Textual Responses to Uncover Misconception Patterns,trump data mine textual respons uncov misconcept pattern,"An important, yet largely unstudied, problem in student data analysis is to detect misconceptions from students' responses to open-response questions. Misconception detection enables instructors to deliver more targeted feedback on the misconceptions exhibited by many students in their class, thus improving the quality of instruction. In this paper, we propose D.TRUMP, a new natural language processing-based framework to detect the common misconceptions among students' textual responses to short-answer questions. We propose a probabilistic model for students' textual responses involving misconceptions and experimentally validate it on a real-world student-response dataset. Experimental results show that D.TRUMP excels at classifying whether a response exhibits one or more misconceptions. More importantly, it can also automatically detect the common misconceptions exhibited across responses from multiple students to multiple questions; this property is especially important at large scale, since instructors will no longer need to manually specify all possible misconceptions that students might exhibit.",import yet larg unstudi problem student data analysi detect misconcept student respons open respons question misconcept detect enabl instructor deliv target feedback misconcept exhibit mani student class thus improv qualiti instruct paper propos trump new natur languag process base framework detect common misconcept among student textual respons short answer question propos probabilist model student textual respons involv misconcept experiment valid real world student respons dataset experiment result show trump excel classifi whether respons exhibit one misconcept import also automat detect common misconcept exhibit across respons multipl student multipl question properti especi import larg scale sinc instructor longer need manual specifi possibl misconcept student might exhibit,"['Joshua J. Michalenko', 'Andrew S. Lan', 'Richard G. Baraniuk']","['stat.ML', 'cs.CL']",False,True,False,False,False,False
