,プログラム実行日時,論文更新日時,論文リンク,PDFリンク,元論文タイトル,論文タイトル,元サマリ,サマリ,著者,事前付与ジャンル,ニューラルネットワーク,自然言語処理,マーケティング,画像解析,音声解析,強化学習
2,2017-03-28T14:05:16Z,2017-03-27T04:03:56Z,http://arxiv.org/abs/1703.08922v1,http://arxiv.org/pdf/1703.08922v1,On Automating the Doctrine of Double Effect,autom doctrin doubl effect,"The doctrine of double effect ($\mathcal{DDE}$) is a long-studied ethical principle that governs when actions that have both positive and negative effects are to be allowed. The goal in this paper is to automate $\mathcal{DDE}$. We briefly present $\mathcal{DDE}$, and use a first-order modal logic, the deontic cognitive event calculus, as our framework to formalize the doctrine. We present formalizations of increasingly stronger versions of the principle, including what is known as the doctrine of triple effect. We then use our framework to simulate successfully scenarios that have been used to test for the presence of the principle in human subjects. Our framework can be used in two different modes: One can use it to build $\mathcal{DDE}$-compliant autonomous systems from scratch, or one can use it to verify that a given AI system is $\mathcal{DDE}$-compliant, by applying a $\mathcal{DDE}$ layer on an existing system or model. For the latter mode, the underlying AI system can be built using any architecture (planners, deep neural networks, bayesian networks, knowledge-representation systems, or a hybrid); as long as the system exposes a few parameters in its model, such verification is possible. The role of the $\mathcal{DDE}$ layer here is akin to a (dynamic or static) software verifier that examines existing software modules. Finally, we end by presenting initial work on how one can apply our $\mathcal{DDE}$ layer to the STRIPS-style planning model, and to a modified POMDP model.",doctrin doubl effect mathcal dde long studi ethic principl govern action posit negat effect allow goal paper autom mathcal dde briefli present mathcal dde use first order modal logic deontic cognit event calculus framework formal doctrin present formal increas stronger version principl includ known doctrin tripl effect use framework simul success scenario use test presenc principl human subject framework use two differ mode one use build mathcal dde compliant autonom system scratch one use verifi given ai system mathcal dde compliant appli mathcal dde layer exist system model latter mode ai system built use ani architectur planner deep neural network bayesian network knowledg represent system hybrid long system expos paramet model verif possibl role mathcal dde layer akin dynam static softwar verifi examin exist softwar modul final end present initi work one appli mathcal dde layer strip style plan model modifi pomdp model,"['Naveen Sundar Govindarajulu', 'Selmer Bringsjord']","['cs.AI', 'cs.LO', 'cs.RO']",False,False,True,False,False,True
5,2017-03-28T14:05:16Z,2017-03-26T15:26:34Z,http://arxiv.org/abs/1703.08825v1,http://arxiv.org/pdf/1703.08825v1,Surrogate Model of Multi-Period Flexibility from a Home Energy   Management System,surrog model multi period flexibl home energi manag system,"Near-future electric distribution grids operation will have to rely on demand-side flexibility, both by implementation of demand response strategies and by taking advantage of the intelligent management of increasingly common small-scale energy storage. Home energy management systems (HEMS) will play a crucial role on the flexibility provision to both system operators and market players like aggregators. Modeling multi-period flexibility from residential consumers (HEMS flexibility), such as battery storage and electric water heater, while complying with internal constraints (comfort levels, data privacy) and uncertainty is a complex task. This paper describes a computational method that is capable of efficiently define and learn the feasible flexibility set from controllable resources connected to a HEMS. An Evolutionary Particle Swarm Optimization (EPSO) algorithm is adopted and reshaped to derive a set of feasible temporal trajectories for the residential net-load, considering storage, flexible appliances, and predefined costumer preferences, as well as load and photovoltaic (PV) forecast uncertainty. A support vector data description (SVDD) algorithm is used to build models capable of classifying feasible and unfeasible HEMS operating trajectories upon request from an optimization/control algorithm operated by a DSO or market player.",near futur electr distribut grid oper reli demand side flexibl implement demand respons strategi take advantag intellig manag increas common small scale energi storag home energi manag system hem play crucial role flexibl provis system oper market player like aggreg model multi period flexibl residenti consum hem flexibl batteri storag electr water heater compli intern constraint comfort level data privaci uncertainti complex task paper describ comput method capabl effici defin learn feasibl flexibl set control resourc connect hem evolutionari particl swarm optim epso algorithm adopt reshap deriv set feasibl tempor trajectori residenti net load consid storag flexibl applianc predefin costum prefer well load photovolta pv forecast uncertainti support vector data descript svdd algorithm use build model capabl classifi feasibl unfeas hem oper trajectori upon request optim control algorithm oper dso market player,"['Rui Pinto', 'Ricardo Bessa', 'Manuel Matos']","['cs.NE', 'cs.AI']",False,False,True,False,False,False
7,2017-03-28T14:05:16Z,2017-03-26T03:47:54Z,http://arxiv.org/abs/1703.08762v1,http://arxiv.org/pdf/1703.08762v1,Team Formation for Scheduling Educational Material in Massive Online   Classes,team format schedul educ materi massiv onlin class,"Whether teaching in a classroom or a Massive Online Open Course it is crucial to present the material in a way that benefits the audience as a whole. We identify two important tasks to solve towards this objective, 1 group students so that they can maximally benefit from peer interaction and 2 find an optimal schedule of the educational material for each group. Thus, in this paper, we solve the problem of team formation and content scheduling for education. Given a time frame d, a set of students S with their required need to learn different activities T and given k as the number of desired groups, we study the problem of finding k group of students. The goal is to teach students within time frame d such that their potential for learning is maximized and find the best schedule for each group. We show this problem to be NP-hard and develop a polynomial algorithm for it. We show our algorithm to be effective both on synthetic as well as a real data set. For our experiments, we use real data on students' grades in a Computer Science department. As part of our contribution, we release a semi-synthetic dataset that mimics the properties of the real data.",whether teach classroom massiv onlin open cours crucial present materi way benefit audienc whole identifi two import task solv toward object group student maxim benefit peer interact find optim schedul educ materi group thus paper solv problem team format content schedul educ given time frame set student requir need learn differ activ given number desir group studi problem find group student goal teach student within time frame potenti learn maxim find best schedul group show problem np hard develop polynomi algorithm show algorithm effect synthet well real data set experi use real data student grade comput scienc depart part contribut releas semi synthet dataset mimic properti real data,"['Sanaz Bahargam', 'Dóra Erdos', 'Azer Bestavros', 'Evimaria Terzi']",['cs.AI'],False,False,True,False,False,True
10,2017-03-28T14:05:20Z,2017-03-24T14:40:31Z,http://arxiv.org/abs/1703.08428v1,http://arxiv.org/abs/1703.08428v1,Calendar.help: Designing a Workflow-Based Scheduling Agent with Humans   in the Loop,calendar help design workflow base schedul agent human loop,"Although information workers may complain about meetings, they are an essential part of their work life. Consequently, busy people spend a significant amount of time scheduling meetings. We present Calendar.help, a system that provides fast, efficient scheduling through structured workflows. Users interact with the system via email, delegating their scheduling needs to the system as if it were a human personal assistant. Common scheduling scenarios are broken down using well-defined workflows and completed as a series of microtasks that are automated when possible and executed by a human otherwise. Unusual scenarios fall back to a trained human assistant who executes them as unstructured macrotasks. We describe the iterative approach we used to develop Calendar.help, and share the lessons learned from scheduling thousands of meetings during a year of real-world deployments. Our findings provide insight into how complex information tasks can be broken down into repeatable components that can be executed efficiently to improve productivity.",although inform worker may complain meet essenti part work life consequ busi peopl spend signific amount time schedul meet present calendar help system provid fast effici schedul structur workflow user interact system via email deleg schedul need system human person assist common schedul scenario broken use well defin workflow complet seri microtask autom possibl execut human otherwis unusu scenario fall back train human assist execut unstructur macrotask describ iter approach use develop calendar help share lesson learn schedul thousand meet dure year real world deploy find provid insight complex inform task broken repeat compon execut effici improv product,"['Justin Cranshaw', 'Emad Elwany', 'Todd Newman', 'Rafal Kocielnik', 'Bowen Yu', 'Sandeep Soni', 'Jaime Teevan', 'Andrés Monroy-Hernández']","['cs.HC', 'cs.AI', 'cs.CL']",False,False,True,False,False,False
20,2017-03-28T14:05:24Z,2017-03-23T04:26:46Z,http://arxiv.org/abs/1703.07929v1,http://arxiv.org/pdf/1703.07929v1,Diversification-Based Learning in Computing and Optimization,diversif base learn comput optim,"Diversification-Based Learning (DBL) derives from a collection of principles and methods introduced in the field of metaheuristics that have broad applications in computing and optimization. We show that the DBL framework goes significantly beyond that of the more recent Opposition-based learning (OBL) framework introduced in Tizhoosh (2005), which has become the focus of numerous research initiatives in machine learning and metaheuristic optimization. We unify and extend earlier proposals in metaheuristic search (Glover, 1997, Glover and Laguna, 1997) to give a collection of approaches that are more flexible and comprehensive than OBL for creating intensification and diversification strategies in metaheuristic search. We also describe potential applications of DBL to various subfields of machine learning and optimization.",diversif base learn dbl deriv collect principl method introduc field metaheurist broad applic comput optim show dbl framework goe signific beyond recent opposit base learn obl framework introduc tizhoosh becom focus numer research initi machin learn metaheurist optim unifi extend earlier propos metaheurist search glover glover laguna give collect approach flexibl comprehens obl creat intensif diversif strategi metaheurist search also describ potenti applic dbl various subfield machin learn optim,"['Fred Glover', 'Jin-Kao Hao']",['cs.AI'],False,False,True,False,False,True
25,2017-03-28T14:05:24Z,2017-03-24T01:00:03Z,http://arxiv.org/abs/1703.07726v3,http://arxiv.org/pdf/1703.07726v3,\$1 Today or \$2 Tomorrow? The Answer is in Your Facebook Likes,today tomorrow answer facebook like,"In economics and psychology, delay discounting is often used to characterize how individuals choose between a smaller immediate reward and a larger delayed reward. People with higher delay discounting rate (DDR) often choose smaller but more immediate rewards (a ""today person""). In contrast, people with a lower discounting rate often choose a larger future rewards (a ""tomorrow person""). Since the ability to modulate the desire of immediate gratification for long term rewards plays an important role in our decision-making, the lower discounting rate often predicts better social, academic and health outcomes. In contrast, the higher discounting rate is often associated with problematic behaviors such as alcohol/drug abuse, pathological gambling and credit card default. Thus, research on understanding and moderating delay discounting has the potential to produce substantial societal benefits.",econom psycholog delay discount often use character individu choos smaller immedi reward larger delay reward peopl higher delay discount rate ddr often choos smaller immedi reward today person contrast peopl lower discount rate often choos larger futur reward tomorrow person sinc abil modul desir immedi gratif long term reward play import role decis make lower discount rate often predict better social academ health outcom contrast higher discount rate often associ problemat behavior alcohol drug abus patholog gambl credit card default thus research understand moder delay discount potenti produc substanti societ benefit,"['Tao Ding', 'Warren K. Bickel', 'Shimei Pan']","['cs.AI', 'cs.CY', 'cs.SI']",False,False,True,False,False,False
31,2017-03-28T14:05:28Z,2017-03-21T18:29:05Z,http://arxiv.org/abs/1703.07381v1,http://arxiv.org/pdf/1703.07381v1,Improving Statistical Multimedia Information Retrieval Model by using   Ontology,improv statist multimedia inform retriev model use ontolog,"A typical IR system that delivers and stores information is affected by problem of matching between user query and available content on web. Use of Ontology represents the extracted terms in form of network graph consisting of nodes, edges, index terms etc. The above mentioned IR approaches provide relevance thus satisfying users query. The paper also emphasis on analyzing multimedia documents and performs calculation for extracted terms using different statistical formulas. The proposed model developed reduces semantic gap and satisfies user needs efficiently.",typic ir system deliv store inform affect problem match user queri avail content web use ontolog repres extract term form network graph consist node edg index term etc abov mention ir approach provid relev thus satisfi user queri paper also emphasi analyz multimedia document perform calcul extract term use differ statist formula propos model develop reduc semant gap satisfi user need effici,"['Gagandeep Singh Narula', 'Vishal Jain']","['cs.IR', 'cs.AI']",False,False,True,False,False,False
43,2017-03-28T14:05:32Z,2017-03-20T04:47:14Z,http://arxiv.org/abs/1703.06597v1,http://arxiv.org/pdf/1703.06597v1,Artificial Intelligence and Economic Theories,artifici intellig econom theori,"The advent of artificial intelligence has changed many disciplines such as engineering, social science and economics. Artificial intelligence is a computational technique which is inspired by natural intelligence such as the swarming of birds, the working of the brain and the pathfinding of the ants. These techniques have impact on economic theories. This book studies the impact of artificial intelligence on economic theories, a subject that has not been extensively studied. The theories that are considered are: demand and supply, asymmetrical information, pricing, rational choice, rational expectation, game theory, efficient market hypotheses, mechanism design, prospect, bounded rationality, portfolio theory, rational counterfactual and causality. The benefit of this book is that it evaluates existing theories of economics and update them based on the developments in artificial intelligence field.",advent artifici intellig chang mani disciplin engin social scienc econom artifici intellig comput techniqu inspir natur intellig swarm bird work brain pathfind ant techniqu impact econom theori book studi impact artifici intellig econom theori subject extens studi theori consid demand suppli asymmetr inform price ration choic ration expect game theori effici market hypothes mechan design prospect bound ration portfolio theori ration counterfactu causal benefit book evalu exist theori econom updat base develop artifici intellig field,"['Tshilidzi Marwala', 'Evan Hurwitz']",['cs.AI'],False,False,True,False,False,False
45,2017-03-28T14:05:32Z,2017-03-20T02:29:53Z,http://arxiv.org/abs/1703.06565v1,http://arxiv.org/pdf/1703.06565v1,Evidence Updating for Stream-Processing in Big-Data: Robust Conditioning   in Soft and Hard Fusion Environments,evid updat stream process big data robust condit soft hard fusion environ,"Conditioning is the primary method for belief revision in data fusion systems employing probabilistic inferencing. However, big-data environments, where soft (i.e., human or human-based) sources are commonly utilized in addition to hard (i.e., physics-based sensors, pose several challenges to traditional conditioning tasks primarily due to the numerous data/source imperfections that are characteristic of such data. The objective of this paper is to investigate the most natural extension of Bayes conditioning based evidence updates in the presence of such large-scale data uncertainties and source/sensor imperfections. By viewing the evidence updating process as a thought experiment, we devise an elegant strategy for robust evidence updating in the presence of extreme uncertainties characteristic of big-data environments. In particular, we look at the formulation of a belief theoretic evidence updating mechanism that is derived as a natural extension of Bayes conditional approach when the incoming evidence takes the form of a general belief function. Proposed method generalizes the belief theoretic Fagin-Halpern conditional notion, and provides a novel evidence updating strategy that is derived as a natural extension of Bayes conditional applied in a highly uncertain and complex fusion scenario that is characteristic of big-data environments. The presented extension differs fundamentally from the previously published work on Conditional Update Equation (CUE) as well as authors own work. An overview of this development is provided via illustrative examples. Furthermore, insights into parameter selection under various fusion contexts are also provided.",condit primari method belief revis data fusion system employ probabilist inferenc howev big data environ soft human human base sourc common util addit hard physic base sensor pose sever challeng tradit condit task primarili due numer data sourc imperfect characterist data object paper investig natur extens bay condit base evid updat presenc larg scale data uncertainti sourc sensor imperfect view evid updat process thought experi devis eleg strategi robust evid updat presenc extrem uncertainti characterist big data environ particular look formul belief theoret evid updat mechan deriv natur extens bay condit approach incom evid take form general belief function propos method general belief theoret fagin halpern condit notion provid novel evid updat strategi deriv natur extens bay condit appli high uncertain complex fusion scenario characterist big data environ present extens differ fundament previous publish work condit updat equat cue well author work overview develop provid via illustr exampl furthermor insight paramet select various fusion context also provid,['Thanuka Wickramarathne'],['cs.AI'],False,False,True,False,False,False
48,2017-03-28T14:05:32Z,2017-03-18T21:25:29Z,http://arxiv.org/abs/1703.06354v1,http://arxiv.org/pdf/1703.06354v1,Goal Conflict in Designing an Autonomous Artificial System,goal conflict design autonom artifici system,"Research on human self-regulation has shown that people hold many goals simultaneously and have complex self-regulation mechanisms to deal with this goal conflict. Artificial autonomous systems may also need to find ways to cope with conflicting goals. Indeed, the intricate interplay among different goals may be critical to the design as well as long-term safety and stability of artificial autonomous systems. I discuss some of the critical features of the human self-regulation system and how it might be applied to an artificial system. Furthermore, the implications of goal conflict for the reliability and stability of artificial autonomous systems and ensuring their alignment with human goals and ethics is examined.",research human self regul shown peopl hold mani goal simultan complex self regul mechan deal goal conflict artifici autonom system may also need find way cope conflict goal inde intric interplay among differ goal may critic design well long term safeti stabil artifici autonom system discuss critic featur human self regul system might appli artifici system furthermor implic goal conflict reliabl stabil artifici autonom system ensur align human goal ethic examin,['Mark Muraven'],['cs.AI'],False,False,True,False,False,True
54,2017-03-28T14:05:37Z,2017-03-17T17:09:14Z,http://arxiv.org/abs/1703.06103v1,http://arxiv.org/pdf/1703.06103v1,Modeling Relational Data with Graph Convolutional Networks,model relat data graph convolut network,"Knowledge bases play a crucial role in many applications, for example question answering and information retrieval. Despite the great effort invested in creating and maintaining them, even the largest representatives (e.g., Yago, DBPedia or Wikidata) are highly incomplete. We introduce relational graph convolutional networks (R-GCNs) and apply them to two standard knowledge base completion tasks: link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing attributes of entities). R-GCNs are a generalization of graph convolutional networks, a recent class of neural networks operating on graphs, and are developed specifically to deal with highly multi-relational data, characteristic of realistic knowledge bases. Our methods achieve competitive results on standard benchmarks for both tasks.",knowledg base play crucial role mani applic exampl question answer inform retriev despit great effort invest creat maintain even largest repres yago dbpedia wikidata high incomplet introduc relat graph convolut network gcns appli two standard knowledg base complet task link predict recoveri miss fact subject predic object tripl entiti classif recoveri miss attribut entiti gcns general graph convolut network recent class neural network oper graph develop specif deal high multi relat data characterist realist knowledg base method achiev competit result standard benchmark task,"['Michael Schlichtkrull', 'Thomas N. Kipf', 'Peter Bloem', 'Rianne van den Berg', 'Ivan Titov', 'Max Welling']","['stat.ML', 'cs.AI', 'cs.DB', 'cs.LG']",False,False,True,False,False,False
57,2017-03-28T14:05:37Z,2017-03-16T13:36:41Z,http://arxiv.org/abs/1703.05614v1,http://arxiv.org/pdf/1703.05614v1,ParaGraphE: A Library for Parallel Knowledge Graph Embedding,paragraph librari parallel knowledg graph embed,"Knowledge graph embedding aims at translating the knowledge graph into numerical representations by transforming the entities and relations into con- tinuous low-dimensional vectors. Recently, many methods [1, 5, 3, 2, 6] have been proposed to deal with this problem, but existing single-thread implemen- tations of them are time-consuming for large-scale knowledge graphs. Here, we design a unified parallel framework to parallelize these methods, which achieves a significant time reduction without in uencing the accuracy. We name our framework as ParaGraphE, which provides a library for parallel knowledge graph embedding. The source code can be downloaded from https: //github.com/LIBBLE/LIBBLE-MultiThread/tree/master/ParaGraphE.",knowledg graph embed aim translat knowledg graph numer represent transform entiti relat con tinuous low dimension vector recent mani method propos deal problem exist singl thread implemen tation time consum larg scale knowledg graph design unifi parallel framework parallel method achiev signific time reduct without uenc accuraci name framework paragraph provid librari parallel knowledg graph embed sourc code download https github com libbl libbl multithread tree master paragraph,"['Xiao-Fan Niu', 'Wu-Jun Li']",['cs.AI'],False,False,True,False,False,False
60,2017-03-28T14:05:41Z,2017-03-16T03:36:28Z,http://arxiv.org/abs/1703.05468v1,http://arxiv.org/abs/1703.05468v1,Database Learning: Toward a Database that Becomes Smarter Every Time,databas learn toward databas becom smarter everi time,"In today's databases, previous query answers rarely benefit answering future queries. For the first time, to the best of our knowledge, we change this paradigm in an approximate query processing (AQP) context. We make the following observation: the answer to each query reveals some degree of knowledge about the answer to another query because their answers stem from the same underlying distribution that has produced the entire dataset. Exploiting and refining this knowledge should allow us to answer queries more analytically, rather than by reading enormous amounts of raw data. Also, processing more queries should continuously enhance our knowledge of the underlying distribution, and hence lead to increasingly faster response times for future queries.   We call this novel idea---learning from past query answers---Database Learning. We exploit the principle of maximum entropy to produce answers, which are in expectation guaranteed to be more accurate than existing sample-based approximations. Empowered by this idea, we build a query engine on top of Spark SQL, called Verdict. We conduct extensive experiments on real-world query traces from a large customer of a major database vendor. Our results demonstrate that database learning supports 73.7% of these queries, speeding them up by up to 23.0x for the same accuracy level compared to existing AQP systems.",today databas previous queri answer rare benefit answer futur queri first time best knowledg chang paradigm approxim queri process aqp context make follow observ answer queri reveal degre knowledg answer anoth queri becaus answer stem distribut produc entir dataset exploit refin knowledg allow us answer queri analyt rather read enorm amount raw data also process queri continu enhanc knowledg distribut henc lead increas faster respons time futur queri call novel idea learn past queri answer databas learn exploit principl maximum entropi produc answer expect guarante accur exist sampl base approxim empow idea build queri engin top spark sql call verdict conduct extens experi real world queri trace larg custom major databas vendor result demonstr databas learn support queri speed accuraci level compar exist aqp system,"['Yongjoo Park', 'Ahmad Shahab Tajik', 'Michael Cafarella', 'Barzan Mozafari']","['cs.DB', 'cs.AI']",False,False,True,False,False,True
68,2017-03-28T14:05:41Z,2017-03-15T15:19:28Z,http://arxiv.org/abs/1703.05204v1,http://arxiv.org/pdf/1703.05204v1,On Inconsistency Indices and Inconsistency Axioms in Pairwise   Comparisons,inconsist indic inconsist axiom pairwis comparison,"Pairwise comparisons are an important tool of modern (multiple criteria) decision making. Since human judgments are often inconsistent, many studies focused on the ways how to express and measure this inconsistency, and several inconsistency indices were proposed as an alternative to Saaty inconsistency index and inconsistency ratio for reciprocal pairwise comparisons matrices. This paper aims to: firstly, introduce a new measure of inconsistency of pairwise comparisons and to prove its basic properties; secondly, to postulate an additional axiom, an upper boundary axiom, to an existing set of axioms; and the last, but not least, the paper provides proofs of satisfaction of this additional axiom by selected inconsistency indices as well as it provides their numerical comparison.",pairwis comparison import tool modern multipl criteria decis make sinc human judgment often inconsist mani studi focus way express measur inconsist sever inconsist indic propos altern saati inconsist index inconsist ratio reciproc pairwis comparison matric paper aim first introduc new measur inconsist pairwis comparison prove basic properti second postul addit axiom upper boundari axiom exist set axiom last least paper provid proof satisfact addit axiom select inconsist indic well provid numer comparison,['Jiri Mazurek'],['cs.AI'],False,False,True,False,False,True
74,2017-03-28T14:05:45Z,2017-03-15T01:04:49Z,http://arxiv.org/abs/1703.04862v1,http://arxiv.org/pdf/1703.04862v1,Exploring the Combination Rules of D Numbers From a Perspective of   Conflict Redistribution,explor combin rule number perspect conflict redistribut,"Dempster-Shafer theory of evidence is widely applied to uncertainty modelling and knowledge reasoning because of its advantages in dealing with uncertain information. But some conditions or requirements, such as exclusiveness hypothesis and completeness constraint, limit the development and application of that theory to a large extend. To overcome the shortcomings and enhance its capability of representing the uncertainty, a novel model, called D numbers, has been proposed recently. However, many key issues, for example how to implement the combination of D numbers, remain unsolved. In the paper, we have explored the combination of D Numbers from a perspective of conflict redistribution, and proposed two combination rules being suitable for different situations for the fusion of two D numbers. The proposed combination rules can reduce to the classical Dempster's rule in Dempster-Shafer theory under a certain conditions. Numerical examples and discussion about the proposed rules are also given in the paper.",dempster shafer theori evid wide appli uncertainti model knowledg reason becaus advantag deal uncertain inform condit requir exclus hypothesi complet constraint limit develop applic theori larg extend overcom shortcom enhanc capabl repres uncertainti novel model call number propos recent howev mani key issu exampl implement combin number remain unsolv paper explor combin number perspect conflict redistribut propos two combin rule suitabl differ situat fusion two number propos combin rule reduc classic dempster rule dempster shafer theori certain condit numer exampl discuss propos rule also given paper,"['Xinyang Deng', 'Wen Jiang']",['cs.AI'],False,False,True,False,False,True
79,2017-03-28T14:05:45Z,2017-03-14T19:14:32Z,http://arxiv.org/abs/1703.04677v1,http://arxiv.org/pdf/1703.04677v1,A computational investigation of sources of variability in sentence   comprehension difficulty in aphasia,comput investig sourc variabl sentenc comprehens difficulti aphasia,"We present a computational evaluation of three hypotheses about sources of deficit in sentence comprehension in aphasia: slowed processing, intermittent deficiency, and resource reduction. The ACT-R based Lewis & Vasishth 2005 model is used to implement these three proposals. Slowed processing is implemented as slowed default production-rule firing time; intermittent deficiency as increased random noise in activation of chunks in memory; and resource reduction as reduced goal activation. As data, we considered subject vs. object relatives presented in a self-paced listening modality to 56 individuals with aphasia (IWA) and 46 matched controls. The participants heard the sentences and carried out a picture verification task to decide on an interpretation of the sentence. These response accuracies are used to identify the best parameters (for each participant) that correspond to the three hypotheses mentioned above. We show that controls have more tightly clustered (less variable) parameter values than IWA; specifically, compared to controls, among IWA there are more individuals with low goal activations, high noise, and slow default action times. This suggests that (i) individual patients show differential amounts of deficit along the three dimensions of slowed processing, intermittent deficient, and resource reduction, (ii) overall, there is evidence for all three sources of deficit playing a role, and (iii) IWA have a more variable range of parameter values than controls. In sum, this study contributes a proof of concept of a quantitative implementation of, and evidence for, these three accounts of comprehension deficits in aphasia.",present comput evalu three hypothes sourc deficit sentenc comprehens aphasia slow process intermitt defici resourc reduct act base lewi vasishth model use implement three propos slow process implement slow default product rule fire time intermitt defici increas random nois activ chunk memori resourc reduct reduc goal activ data consid subject vs object relat present self pace listen modal individu aphasia iwa match control particip heard sentenc carri pictur verif task decid interpret sentenc respons accuraci use identifi best paramet particip correspond three hypothes mention abov show control tight cluster less variabl paramet valu iwa specif compar control among iwa individu low goal activ high nois slow default action time suggest individu patient show differenti amount deficit along three dimens slow process intermitt defici resourc reduct ii overal evid three sourc deficit play role iii iwa variabl rang paramet valu control sum studi contribut proof concept quantit implement evid three account comprehens deficit aphasia,"['Paul Mätzig', 'Shravan Vasishth', 'Felix Engelmann', 'David Caplan']","['cs.CL', 'cs.AI']",False,False,True,False,False,True
84,2017-03-28T14:05:50Z,2017-03-13T13:45:13Z,http://arxiv.org/abs/1703.04389v1,http://arxiv.org/pdf/1703.04389v1,Bayesian Optimization with Gradients,bayesian optim gradient,"In recent years, Bayesian optimization has proven successful for global optimization of expensive-to-evaluate multimodal objective functions. However, unlike most optimization methods, Bayesian optimization typically does not use derivative information. In this paper we show how Bayesian optimization can exploit derivative information to decrease the number of objective function evaluations required for good performance. In particular, we develop a novel Bayesian optimization algorithm, the derivative-enabled knowledge-gradient (dKG), for which we show one-step Bayes-optimality, asymptotic consistency, and greater one-step value of information than is possible in the derivative-free setting. Our procedure accommodates noisy and incomplete derivative information, and comes in both sequential and batch forms. We show dKG provides state-of-the-art performance compared to a wide range of optimization procedures with and without gradients, on benchmarks including logistic regression, kernel learning, and k-nearest neighbors.",recent year bayesian optim proven success global optim expens evalu multimod object function howev unlik optim method bayesian optim typic doe use deriv inform paper show bayesian optim exploit deriv inform decreas number object function evalu requir good perform particular develop novel bayesian optim algorithm deriv enabl knowledg gradient dkg show one step bay optim asymptot consist greater one step valu inform possibl deriv free set procedur accommod noisi incomplet deriv inform come sequenti batch form show dkg provid state art perform compar wide rang optim procedur without gradient benchmark includ logist regress kernel learn nearest neighbor,"['Jian Wu', 'Matthias Poloczek', 'Andrew Gordon Wilson', 'Peter I. Frazier']","['stat.ML', 'cs.AI', 'cs.LG', 'math.OC']",False,False,True,False,False,True
89,2017-03-28T14:05:50Z,2017-03-13T03:29:23Z,http://arxiv.org/abs/1703.04232v1,http://arxiv.org/pdf/1703.04232v1,Numerical Integration and Dynamic Discretization in Heuristic Search   Planning over Hybrid Domains,numer integr dynam discret heurist search plan hybrid domain,"In this paper we look into the problem of planning over hybrid domains, where change can be both discrete and instantaneous, or continuous over time. In addition, it is required that each state on the trajectory induced by the execution of plans complies with a given set of global constraints. We approach the computation of plans for such domains as the problem of searching over a deterministic state model. In this model, some of the successor states are obtained by solving numerically the so-called initial value problem over a set of ordinary differential equations (ODE) given by the current plan prefix. These equations hold over time intervals whose duration is determined dynamically, according to whether zero crossing events take place for a set of invariant conditions. The resulting planner, FS+, incorporates these features together with effective heuristic guidance. FS+ does not impose any of the syntactic restrictions on process effects often found on the existing literature on Hybrid Planning. A key concept of our approach is that a clear separation is struck between planning and simulation time steps. The former is the time allowed to observe the evolution of a given dynamical system before committing to a future course of action, whilst the later is part of the model of the environment. FS+ is shown to be a robust planner over a diverse set of hybrid domains, taken from the existing literature on hybrid planning and systems.",paper look problem plan hybrid domain chang discret instantan continu time addit requir state trajectori induc execut plan compli given set global constraint approach comput plan domain problem search determinist state model model successor state obtain solv numer call initi valu problem set ordinari differenti equat ode given current plan prefix equat hold time interv whose durat determin dynam accord whether zero cross event take place set invari condit result planner fs incorpor featur togeth effect heurist guidanc fs doe impos ani syntact restrict process effect often found exist literatur hybrid plan key concept approach clear separ struck plan simul time step former time allow observ evolut given dynam system befor commit futur cours action whilst later part model environ fs shown robust planner divers set hybrid domain taken exist literatur hybrid plan system,"['Miquel Ramirez', 'Enrico Scala', 'Patrik Haslum', 'Sylvie Thiebaux']","['cs.AI', 'I.2.8; F.2.2; I.6; J.2']",False,False,True,False,False,False
92,2017-03-28T14:05:54Z,2017-03-12T13:17:08Z,http://arxiv.org/abs/1703.04115v1,http://arxiv.org/pdf/1703.04115v1,"BetaRun 2017 Team Description Paper: Variety, Complexity, and Learning",betarun team descript paper varieti complex learn,"RoboCup offers a set of benchmark problems for Artificial Intelligence in form of official world championships since 1997. The most tactical advanced and richest in terms of behavioural complexity of these is the 2D Soccer Simulation League, a simulated robotic soccer competition. BetaRun is a new attempt combining both machine learning and manual programming approaches, with the ultimate goal to arrive at a team that is trained entirely from observing and playing games, and a successor of the World Champion team Gliders 2016.",robocup offer set benchmark problem artifici intellig form offici world championship sinc tactic advanc richest term behaviour complex soccer simul leagu simul robot soccer competit betarun new attempt combin machin learn manual program approach ultim goal arriv team train entir observ play game successor world champion team glider,"['Olivia Michael', 'Oliver Obst']",['cs.AI'],False,False,True,False,False,True
97,2017-03-28T14:05:54Z,2017-03-11T07:46:51Z,http://arxiv.org/abs/1703.03924v1,http://arxiv.org/pdf/1703.03924v1,Real-Time Machine Learning: The Missing Pieces,real time machin learn miss piec,"Machine learning applications are increasingly deployed not only to serve predictions using static models, but also as tightly-integrated components of feedback loops involving dynamic, real-time decision making. These applications pose a new set of requirements, none of which are difficult to achieve in isolation, but the combination of which creates a challenge for existing distributed execution frameworks: computation with millisecond latency at high throughput, adaptive construction of arbitrary task graphs, and execution of heterogeneous kernels over diverse sets of resources. We assert that a new distributed execution framework is needed for such ML applications and propose a candidate approach with a proof-of-concept architecture that achieves a 63x performance improvement over a state-of-the-art execution framework for a representative application.",machin learn applic increas deploy onli serv predict use static model also tight integr compon feedback loop involv dynam real time decis make applic pose new set requir none difficult achiev isol combin creat challeng exist distribut execut framework comput millisecond latenc high throughput adapt construct arbitrari task graph execut heterogen kernel divers set resourc assert new distribut execut framework need ml applic propos candid approach proof concept architectur achiev perform improv state art execut framework repres applic,"['Robert Nishihara', 'Philipp Moritz', 'Stephanie Wang', 'Alexey Tumanov', 'William Paul', 'Johann Schleier-Smith', 'Richard Liaw', 'Michael I. Jordan', 'Ion Stoica']","['cs.DC', 'cs.AI', 'cs.LG']",False,False,True,False,False,False
99,2017-03-28T14:05:54Z,2017-03-11T05:35:09Z,http://arxiv.org/abs/1703.03912v1,http://arxiv.org/pdf/1703.03912v1,The Curse of Correlation in Security Games and Principle of Max-Entropy,curs correl secur game principl max entropi,"In this paper, we identify and study a fundamental, yet underexplored, phenomenon in security games, which we term the Curse of Correlation (CoC). Specifically, we observe that there is inevitable correlation among the protection status of different targets. Such correlation is a crucial concern, especially in spatio-temporal domains like conservation area patrolling, where attackers can monitor patrollers at certain areas and then infer their patrolling routes using such correlation. To mitigate this issue, we introduce the principle of max-entropy to security games, and focus on designing entropy-maximizing defending strategies for the spatio-temporal security game -- a major victim of CoC. We prove that the problem is #P-hard in general, but propose efficient algorithms in well-motivated special settings. Our experiments show significant advantages of the max-entropy algorithms against previous algorithms.",paper identifi studi fundament yet underexplor phenomenon secur game term curs correl coc specif observ inevit correl among protect status differ target correl crucial concern especi spatio tempor domain like conserv area patrol attack monitor patrol certain area infer patrol rout use correl mitig issu introduc principl max entropi secur game focus design entropi maxim defend strategi spatio tempor secur game major victim coc prove problem hard general propos effici algorithm well motiv special set experi show signific advantag max entropi algorithm previous algorithm,"['Haifeng Xu', 'Milind Tambe', 'Shaddin Dughmi', 'Venil Loyd Noronha']","['cs.GT', 'cs.AI', 'cs.CR']",False,False,True,False,False,True
100,2017-03-28T14:06:40Z,2017-03-25T22:50:20Z,http://arxiv.org/abs/1703.08746v1,http://arxiv.org/pdf/1703.08746v1,Proof Verification Can Be Hard!,proof verif hard,"The generally accepted wisdom in computational circles is that pure proof verification is a solved problem and that the computationally hard elements and fertile areas of study lie in proof discovery. This wisdom presumably does hold for conventional proof systems such as first-order logic with a standard proof calculus such as natural deduction or resolution. But this folk belief breaks down when we consider more user-friendly/powerful inference rules. One such rule is the restricted {\omega}-rule, which is not even semi-decidable when added to a standard proof calculus of a nice theory. While presumably not a novel result, we feel that the hardness of proof verification is under-appreciated in most communities that deal with proofs. A proof-sketch follows.",general accept wisdom comput circl pure proof verif solv problem comput hard element fertil area studi lie proof discoveri wisdom presum doe hold convent proof system first order logic standard proof calculus natur deduct resolut folk belief break consid user friend power infer rule one rule restrict omega rule even semi decid ad standard proof calculus nice theori presum novel result feel hard proof verif appreci communiti deal proof proof sketch follow,"['Naveen Sundar Govindarajulu', 'Selmer Bringsjord']","['cs.LO', 'cs.CC']",False,False,True,False,False,True
103,2017-03-28T14:06:40Z,2017-03-22T19:56:27Z,http://arxiv.org/abs/1703.07833v1,http://arxiv.org/pdf/1703.07833v1,"Information complexity of the AND function in the two-Party, and   multiparty settings",inform complex function two parti multiparti set,"In a recent breakthrough paper [M. Braverman, A. Garg, D. Pankratov, and O. Weinstein, From information to exact communication, STOC'13] Braverman et al. developed a local characterization for the zero-error information complexity in the two party model, and used it to compute the exact internal and external information complexity of the 2-bit AND function, which was then applied to determine the exact asymptotic of randomized communication complexity of the set disjointness problem.   In this article, we extend their results on AND function to the multi-party number-in-hand model by proving that the generalization of their protocol has optimal internal and external information cost for certain distributions. Our proof has new components, and in particular it fixes some minor gaps in the proof of Braverman et al.",recent breakthrough paper braverman garg pankratov weinstein inform exact communic stoc braverman et al develop local character zero error inform complex two parti model use comput exact intern extern inform complex bit function appli determin exact asymptot random communic complex set disjoint problem articl extend result function multi parti number hand model prove general protocol optim intern extern inform cost certain distribut proof new compon particular fix minor gap proof braverman et al,"['Yuval Filmus', 'Hamed Hatami', 'Yaqiao Li', 'Suzin You']",['cs.CC'],False,False,True,False,False,True
112,2017-03-28T14:06:44Z,2017-03-17T17:43:59Z,http://arxiv.org/abs/1703.06127v1,http://arxiv.org/pdf/1703.06127v1,"Tusnády's problem, the transference principle, and non-uniform QMC   sampling",tusn dy problem transfer principl non uniform qmc sampl,"It is well-known that for every $N \geq 1$ and $d \geq 1$ there exist point sets $x_1, \dots, x_N \in [0,1]^d$ whose discrepancy with respect to the Lebesgue measure is of order at most $(\log N)^{d-1} N^{-1}$. In a more general setting, the first author proved together with Josef Dick that for any normalized measure $\mu$ on $[0,1]^d$ there exist points $x_1, \dots, x_N$ whose discrepancy with respect to $\mu$ is of order at most $(\log N)^{(3d+1)/2} N^{-1}$. The proof used methods from combinatorial mathematics, and in particular a result of Banaszczyk on balancings of vectors. In the present note we use a version of the so-called transference principle together with recent results on the discrepancy of red-blue colorings to show that for any $\mu$ there even exist points having discrepancy of order at most $(\log N)^{d-\frac12} N^{-1}$, which is almost as good as the discrepancy bound in the case of the Lebesgue measure.",well known everi geq geq exist point set dot whose discrep respect lebesgu measur order log general set first author prove togeth josef dick ani normal measur mu exist point dot whose discrep respect mu order log proof use method combinatori mathemat particular result banaszczyk balanc vector present note use version call transfer principl togeth recent result discrep red blue color show ani mu even exist point discrep order log frac almost good discrep bound case lebesgu measur,"['Christoph Aistleitner', 'Dmitriy Bilyk', 'Aleksandar Nikolov']","['math.CO', 'cs.CC', 'math.NA', 'math.PR']",False,False,True,False,False,False
116,2017-03-28T14:06:44Z,2017-03-15T18:00:05Z,http://arxiv.org/abs/1703.05332v1,http://arxiv.org/pdf/1703.05332v1,Complexity of sampling as an order parameter,complex sampl order paramet,"We consider the classical complexity of approximately simulating time evolution under spatially local quadratic bosonic Hamiltonians for time $t$. We obtain upper and lower bounds on the scaling of $t$ with the number of bosons, $n$, for which simulation, cast as a sampling problem, is classically efficient and provably hard, respectively. We view these results in the light of classifying phases of physical systems based on parameters in the Hamiltonian and conjecture a link to dynamical phase transitions. In doing so, we combine ideas from mathematical physics and computational complexity to gain insight into the behavior of condensed matter systems.",consid classic complex approxim simul time evolut spatial local quadrat boson hamiltonian time obtain upper lower bound scale number boson simul cast sampl problem classic effici provabl hard respect view result light classifi phase physic system base paramet hamiltonian conjectur link dynam phase transit combin idea mathemat physic comput complex gain insight behavior condens matter system,"['Abhinav Deshpande', 'Bill Fefferman', 'Michael Foss-Feig', 'Alexey V. Gorshkov']","['quant-ph', 'cond-mat.quant-gas', 'cs.CC']",False,False,True,False,False,True
128,2017-03-28T14:06:48Z,2017-03-09T13:45:45Z,http://arxiv.org/abs/1703.03262v1,http://arxiv.org/pdf/1703.03262v1,Does Nash Envy Immunity,doe nash envi immun,"The most popular stability notion in games should be Nash equilibrium under the rationality of players who maximize their own payoff individually. In contrast, in many scenarios, players can be (partly) irrational with some unpredictable factors. Hence a strategy profile can be more robust if it is resilient against certain irrational behaviors. In this paper, we propose a stability notion that is resilient against envy. A strategy profile is said to be envy-proof if each player cannot gain a competitive edge with respect to the change in utility over the other players by deviation. Together with Nash equilibrium and another stability notion called immunity, we show how these separate notions are related to each other, whether they exist in games, and whether and when a strategy profile satisfying these notions can be efficiently found. We answer these questions by starting with the general two player game and extend the discussion for the approximate stability and for the corresponding fault-tolerance notions in multi-player games.",popular stabil notion game nash equilibrium ration player maxim payoff individu contrast mani scenario player part irrat unpredict factor henc strategi profil robust resili certain irrat behavior paper propos stabil notion resili envi strategi profil said envi proof player cannot gain competit edg respect chang util player deviat togeth nash equilibrium anoth stabil notion call immun show separ notion relat whether exist game whether strategi profil satisfi notion effici found answer question start general two player game extend discuss approxim stabil correspond fault toler notion multi player game,['Ching-Hua Yu'],"['cs.GT', 'cs.CC', 'cs.CR', 'I.2.1; J.4; K.6.0; C.4']",False,False,True,False,False,False
133,2017-03-28T14:06:52Z,2017-03-06T15:42:36Z,http://arxiv.org/abs/1703.01928v1,http://arxiv.org/pdf/1703.01928v1,On The Complexity of Enumeration,complex enumer,"We investigate the relationship between several enumeration complexity classes and focus in particular on the incremental polynomial time and the polynomial delay (IncP and DelayP). We prove, modulo the Exponential Time Hypothesis, that IncP contains a strict hierarchy of subclasses. Since DelayP is included in IncP_1, the first class of the hierarchy, it is separated from IncP. We prove for some algorithms that we can turn an average delay into a worst case delay, suggesting that IncP_1 = DelayP even with a polynomially bounded memory. Finally we relate the uniform generation of solutions to probabilistic enumeration algorithms with polynomial delay.",investig relationship sever enumer complex class focus particular increment polynomi time polynomi delay incp delayp prove modulo exponenti time hypothesi incp contain strict hierarchi subclass sinc delayp includ incp first class hierarchi separ incp prove algorithm turn averag delay worst case delay suggest incp delayp even polynomi bound memori final relat uniform generat solut probabilist enumer algorithm polynomi delay,"['Florent Capelli', 'Yann Strozecki']",['cs.CC'],False,False,True,False,False,False
143,2017-03-28T14:06:56Z,2017-03-05T02:19:58Z,http://arxiv.org/abs/1702.08660v2,http://arxiv.org/pdf/1702.08660v2,Complexity of short generating functions,complex short generat function,"We give complexity analysis of the class of short generating functions (GF). Assuming $\#P \not\subseteq FP/poly$, we show that this class is not closed under taking many intersections, unions or projections of GFs, in the sense that these operations can increase the bitlength of coefficients of GFs by a super-polynomial factor. We also prove that truncated theta functions are hard in this class.",give complex analysi class short generat function gf assum subseteq fp poli show class close take mani intersect union project gfs sens oper increas bitlength coeffici gfs super polynomi factor also prove truncat theta function hard class,"['Danny Nguyen', 'Igor Pak']","['math.CO', 'cs.CC', 'cs.DM', 'cs.LO', 'math.LO']",False,False,True,False,False,True
145,2017-03-28T14:06:56Z,2017-02-27T19:26:15Z,http://arxiv.org/abs/1702.08483v1,http://arxiv.org/pdf/1702.08483v1,The computational landscape of general physical theories,comput landscap general physic theori,"The emergence of quantum computers has challenged long-held beliefs about what is efficiently computable given our current physical theories. However, going back to the work of Abrams and Lloyd, changing one aspect of quantum theory can result in yet more dramatic increases in computational power, as well as violations of fundamental physical principles. Here we focus on efficient computation within a framework of general physical theories that make good operational sense. In prior work, Lee and Barrett showed that in any theory satisfying the principle of tomographic locality (roughly, local measurements suffice for tomography of multipartite states) the complexity bound on efficient computation is AWPP. This bound holds independently of whether the principle of causality (roughly, no signalling from the future) is satisfied. In this work we show that this bound is tight: there exists a theory satisfying both the principles of tomographic locality and causality which can efficiently decide everything in AWPP, and in particular can simulate any efficient quantum computation. Thus the class AWPP has a natural physical interpretation: it is precisely the class of problems that can be solved efficiently in tomographically-local theories. This theory is built upon a model of computing involving Turing machines with quasi-probabilities, to wit, machines with transition weights that can be negative but sum to unity over all branches. In analogy with the study of non-local quantum correlations, this leads us to question what physical principles recover the power of quantum computing. Along this line, we give some computational complexity evidence that quantum computation does not achieve the bound of AWPP.",emerg quantum comput challeng long held belief effici comput given current physic theori howev go back work abram lloyd chang one aspect quantum theori result yet dramat increas comput power well violat fundament physic principl focus effici comput within framework general physic theori make good oper sens prior work lee barrett show ani theori satisfi principl tomograph local rough local measur suffic tomographi multipartit state complex bound effici comput awpp bound hold independ whether principl causal rough signal futur satisfi work show bound tight exist theori satisfi principl tomograph local causal effici decid everyth awpp particular simul ani effici quantum comput thus class awpp natur physic interpret precis class problem solv effici tomograph local theori theori built upon model comput involv ture machin quasi probabl wit machin transit weight negat sum uniti branch analog studi non local quantum correl lead us question physic principl recov power quantum comput along line give comput complex evid quantum comput doe achiev bound awpp,"['Jonathan Barrett', 'Niel de Beaudrap', 'Matty J. Hoban', 'Ciarán M. Lee']","['quant-ph', 'cs.CC']",False,False,True,False,False,True
151,2017-03-28T14:07:01Z,2017-02-26T07:12:46Z,http://arxiv.org/abs/1702.08443v1,http://arxiv.org/pdf/1702.08443v1,"Elementary Yet Precise Worst-case Analysis of MergeSort, A short version   (SV)",elementari yet precis worst case analysi mergesort short version sv,"This paper offers two elementary yet precise derivations of an exact formula   \[ W(n) = \sum_{i=1} ^{n} \lceil \lg i \rceil = n \lceil \lg n \rceil - 2^{\lceil \lg n \rceil} + 1 \] for the maximum number $ W(n) $ of comparisons of keys performed by $ {\tt MergeSort} $ on an $ n $-element array. The first of the two, due to its structural regularity, is well worth carefully studying in its own right.   Close smooth bounds on $ W(n) $ are derived. It seems interesting that $ W(n) $ is linear between the points $ n = 2^{\lfloor \lg n \rfloor} $ and it linearly interpolates its own lower bound $ n \lg n - n + 1 $ between these points.",paper offer two elementari yet precis deriv exact formula sum lceil lg rceil lceil lg rceil lceil lg rceil maximum number comparison key perform tt mergesort element array first two due structur regular well worth care studi right close smooth bound deriv seem interest linear point lfloor lg rfloor linear interpol lower bound lg point,['Marek A. Suchenek'],"['cs.DS', 'cs.CC', 'cs.DM', '68W40 Analysis of algorithms', 'F.2.2; G.2.0; G.2.1; G.2.2']",False,False,True,False,False,True
152,2017-03-28T14:07:01Z,2017-02-25T19:07:15Z,http://arxiv.org/abs/1702.07938v1,http://arxiv.org/pdf/1702.07938v1,Complexity Classification of the Eight-Vertex Model,complex classif eight vertex model,"We prove a complexity dichotomy theorem for the eight-vertex model. For every setting of the parameters of the model, we prove that computing the partition function is either solvable in polynomial time or \#P-hard. The dichotomy criterion is explicit. For tractability, we find some new classes of problems computable in polynomial time. For \#P-hardness, we employ M\""{o}bius transformations to prove the success of interpolations.",prove complex dichotomi theorem eight vertex model everi set paramet model prove comput partit function either solvabl polynomi time hard dichotomi criterion explicit tractabl find new class problem comput polynomi time hard employ bius transform prove success interpol,"['Jin-Yi Cai', 'Zhiguo Fu']",['cs.CC'],False,False,True,False,False,False
153,2017-03-28T14:07:01Z,2017-02-25T15:07:59Z,http://arxiv.org/abs/1702.07902v1,http://arxiv.org/pdf/1702.07902v1,Approval Voting with Intransitive Preferences,approv vote intransit prefer,"We extend Approval voting to the settings where voters may have intransitive preferences. The major obstacle to applying Approval voting in these settings is that voters are not able to clearly determine who they should approve or disapprove, due to the intransitivity of their preferences. An approach to address this issue is to apply tournament solutions to help voters make the decision. We study a class of voting systems where first each voter casts a vote defined as a tournament, then a well-defined tournament solution is applied to select the candidates who are assumed to be approved by the voter. Winners are the ones receiving the most approvals. We study axiomatic properties of this class of voting systems and complexity of control and bribery problems for these voting systems.",extend approv vote set voter may intransit prefer major obstacl appli approv vote set voter abl clear determin approv disapprov due intransit prefer approach address issu appli tournament solut help voter make decis studi class vote system first voter cast vote defin tournament well defin tournament solut appli select candid assum approv voter winner one receiv approv studi axiomat properti class vote system complex control briberi problem vote system,['Yongjie Yang'],"['cs.GT', 'cs.CC', 'cs.DM']",False,False,True,False,False,True
158,2017-03-28T14:07:01Z,2017-02-22T22:43:45Z,http://arxiv.org/abs/1702.07032v1,http://arxiv.org/pdf/1702.07032v1,On the Complexity of Bundle-Pricing and Simple Mechanisms,complex bundl price simpl mechan,"We show that the problem of finding an optimal bundle-pricing for a single additive buyer is #P-hard, even when the distributions have support size 2 for each item and the optimal solution is guaranteed to be a simple one: the seller picks a price for the grand bundle and a price for each individual item; the buyer can purchase either the grand bundle at the given price or any bundle of items at their total individual prices. We refer to this simple and natural family of pricing schemes as discounted item-pricings. In addition to the hardness result, we show that when the distributions are i.i.d. with support size 2, a discounted item-pricing can achieve the optimal revenue obtainable by lottery-pricings and it can be found in polynomial time.",show problem find optim bundl price singl addit buyer hard even distribut support size item optim solut guarante simpl one seller pick price grand bundl price individu item buyer purchas either grand bundl given price ani bundl item total individu price refer simpl natur famili price scheme discount item price addit hard result show distribut support size discount item price achiev optim revenu obtain lotteri price found polynomi time,"['Xi Chen', 'George Matikas', 'Dimitris Paparas', 'Mihalis Yannakakis']","['cs.GT', 'cs.CC', 'cs.DS']",False,False,True,False,False,False
160,2017-03-28T14:07:05Z,2017-02-22T15:29:15Z,http://arxiv.org/abs/1702.06844v1,http://arxiv.org/pdf/1702.06844v1,Parameterized Shifted Combinatorial Optimization,parameter shift combinatori optim,"Shifted combinatorial optimization is a new nonlinear optimization framework which is a broad extension of standard combinatorial optimization, involving the choice of several feasible solutions at a time. This framework captures well studied and diverse problems ranging from so-called vulnerability problems to sharing and partitioning problems. In particular, every standard combinatorial optimization problem has its shifted counterpart, which is typically much harder. Already with explicitly given input set the shifted problem may be NP-hard. In this article we initiate a study of the parameterized complexity of this framework. First we show that shifting over an explicitly given set with its cardinality as the parameter may be in XP, FPT or P, depending on the objective function. Second, we study the shifted problem over sets definable in MSO logic (which includes, e.g., the well known MSO partitioning problems). Our main results here are that shifted combinatorial optimization over MSO definable sets is in XP with respect to the MSO formula and the treewidth (or more generally clique-width) of the input graph, and is W[1]-hard even under further severe restrictions.",shift combinatori optim new nonlinear optim framework broad extens standard combinatori optim involv choic sever feasibl solut time framework captur well studi divers problem rang call vulner problem share partit problem particular everi standard combinatori optim problem shift counterpart typic much harder alreadi explicit given input set shift problem may np hard articl initi studi parameter complex framework first show shift explicit given set cardin paramet may xp fpt depend object function second studi shift problem set defin mso logic includ well known mso partit problem main result shift combinatori optim mso defin set xp respect mso formula treewidth general cliqu width input graph hard even sever restrict,"['Jakub Gajarský', 'Petr Hliněný', 'Martin Koutecký', 'Shmuel Onn']",['cs.CC'],False,False,True,False,False,True
162,2017-03-28T14:07:05Z,2017-02-21T18:13:40Z,http://arxiv.org/abs/1702.06503v1,http://arxiv.org/pdf/1702.06503v1,When can Graph Hyperbolicity be computed in Linear Time?,graph hyperbol comput linear time,"Hyperbolicity measures, in terms of (distance) metrics, how close a given graph is to being a tree. Due to its relevance in modeling real-world networks, hyperbolicity has seen intensive research over the last years. Unfortunately, the best known algorithms for computing the hyperbolicity number of a graph (the smaller, the more tree-like) have running time $O(n^4)$, where $n$ is the number of graph vertices. Exploiting the framework of parameterized complexity analysis, we explore possibilities for ""linear-time FPT"" algorithms to compute hyperbolicity. For instance, we show that hyperbolicity can be computed in time $O(2^{O(k)} + n +m)$ ($m$ being the number of graph edges) while at the same time, unless the SETH fails, there is no $2^{o(k)}n^2$-time algorithm.",hyperbol measur term distanc metric close given graph tree due relev model real world network hyperbol seen intens research last year unfortun best known algorithm comput hyperbol number graph smaller tree like run time number graph vertic exploit framework parameter complex analysi explor possibl linear time fpt algorithm comput hyperbol instanc show hyperbol comput time number graph edg time unless seth fail time algorithm,"['Till Fluschnik', 'Christian Komusiewicz', 'George B. Mertzios', 'André Nichterlein', 'Rolf Niedermeier', 'Nimrod Talmon']","['cs.CC', 'cs.DS', '05C12, 68R10, 68Q25, 68Q17', 'F.2.2; G.2.2']",False,False,True,False,False,False
168,2017-03-28T14:07:05Z,2017-02-21T09:07:53Z,http://arxiv.org/abs/1702.05704v2,http://arxiv.org/pdf/1702.05704v2,Computational Complexity of Atomic Chemical Reaction Networks,comput complex atom chemic reaction network,"Informally, a chemical reaction network is ""atomic"" if each reaction may be interpreted as the rearrangement of indivisible units of matter. There are several reasonable definitions formalizing this idea. We investigate the computational complexity of deciding whether a given network is atomic according to each of these definitions.   Our first definition, primitive atomic, which requires each reaction to preserve the total number of atoms, is to shown to be equivalent to mass conservation. Since it is known that it can be decided in polynomial time whether a given chemical reaction network is mass-conserving, the equivalence gives an efficient algorithm to decide primitive atomicity.   Another definition, subset atomic, further requires that all atoms are species. We show that deciding whether a given network is subset atomic is in $\textsf{NP}$, and the problem ""is a network subset atomic with respect to a given atom set"" is strongly $\textsf{NP}$-$\textsf{Complete}$.   A third definition, reachably atomic, studied by Adleman, Gopalkrishnan et al., further requires that each species has a sequence of reactions splitting it into its constituent atoms. We show that there is a polynomial-time algorithm to decide whether a given network is reachably atomic, improving upon the result of Adleman et al. that the problem is decidable. We show that the reachability problem for reachably atomic networks is $\textsf{Pspace}$-$\textsf{Complete}$.   Finally, we demonstrate equivalence relationships between our definitions and some special cases of another existing definition of atomicity due to Gnacadja.",inform chemic reaction network atom reaction may interpret rearrang indivis unit matter sever reason definit formal idea investig comput complex decid whether given network atom accord definit first definit primit atom requir reaction preserv total number atom shown equival mass conserv sinc known decid polynomi time whether given chemic reaction network mass conserv equival give effici algorithm decid primit atom anoth definit subset atom requir atom speci show decid whether given network subset atom textsf np problem network subset atom respect given atom set strong textsf np textsf complet third definit reachabl atom studi adleman gopalkrishnan et al requir speci sequenc reaction split constitu atom show polynomi time algorithm decid whether given network reachabl atom improv upon result adleman et al problem decid show reachabl problem reachabl atom network textsf pspace textsf complet final demonstr equival relationship definit special case anoth exist definit atom due gnacadja,"['David Doty', 'Shaopeng Zhu']","['cs.CC', 'F.1.1']",False,False,True,False,False,False
169,2017-03-28T14:07:05Z,2017-02-18T00:19:02Z,http://arxiv.org/abs/1702.05547v1,http://arxiv.org/pdf/1702.05547v1,Nontrivial Turmites are Turing-universal,nontrivi turmit ture univers,"A Turmit is a Turing machine that works over a two-dimensional grid, that is, an agent that moves, reads and writes symbols over the cells of the grid. Its state is an arrow and, depending on the symbol that it reads, it turns to the left or to the right, switching the symbol at the same time. Several symbols are admitted, and the rule is specified by the turning sense that the machine has over each symbol. Turmites are a generalization of Langtons ant, and they present very complex and diverse behaviors. We prove that any Turmite, except for those whose rule does not depend on the symbol, can simulate any Turing Machine. We also prove the P-completeness of prediction their future behavior by explicitly giving a log-space reduction from the Topological Circuit Value Problem. A similar result was already established for Langtons ant; here we use a similar technique but prove a stronger notion of simulation, and for a more general family.",turmit ture machin work two dimension grid agent move read write symbol cell grid state arrow depend symbol read turn left right switch symbol time sever symbol admit rule specifi turn sens machin symbol turmit general langton ant present veri complex divers behavior prove ani turmit except whose rule doe depend symbol simul ani ture machin also prove complet predict futur behavior explicit give log space reduct topolog circuit valu problem similar result alreadi establish langton ant use similar techniqu prove stronger notion simul general famili,"['Diego Maldonado', 'Anahí Gajardo', 'Benjamin Hellouin de Menibus', 'Andrés Moreira']","['cs.CC', 'nlin.CG', '68Q17, 68Q05', 'F.1.1; F.1.3']",False,False,True,False,False,False
179,2017-03-28T14:07:09Z,2017-02-15T01:05:51Z,http://arxiv.org/abs/1702.04432v1,http://arxiv.org/pdf/1702.04432v1,Vertex isoperimetry and independent set stability for tensor powers of   cliques,vertex isoperimetri independ set stabil tensor power cliqu,"The tensor power of the clique on $t$ vertices (denoted by $K_t^n$) is the graph on vertex set $\{1, ..., t\}^n$ such that two vertices $x, y \in \{1, ..., t\}^n$ are connected if and only if $x_i \neq y_i$ for all $i \in \{1, ..., n\}$. Let the density of a subset $S$ of $K_t^n$ to be $\mu(S) := \frac{ S }{t^n}$, and let the vertex boundary of a set $S$ to be vertices which are incident to some vertex of $S$, perhaps including points of $S$. We investigate two similar problems on such graphs.   First, we study the vertex isoperimetry problem. Given a density $\nu \in [0, 1]$ what is the smallest possible density of the vertex boundary of a subset of $K_t^n$ of density $\nu$? Let $\Phi_t(\nu)$ be the infimum of these minimum densities as $n \to \infty$. We find a recursive relation allows one to compute $\Phi_t(\nu)$ in time polynomial to the number of desired bits of precision.   Second, we study given an independent set $I \subseteq K_t^n$ of density $\mu(I) = \frac{1}{t}(1-\epsilon)$, how close it is to a maximum-sized independent set $J$ of density $\frac{1}{t}$. We show that this deviation (measured by $\mu(I \setminus J)$) is at most $4\epsilon^{\frac{\log t}{\log t - \log(t-1)}}$ as long as $\epsilon < 1 - \frac{3}{t} + \frac{2}{t^2}$. This substantially improves on results of Alon, Dinur, Friedgut, and Sudakov (2004) and Ghandehari and Hatami (2008) which had an $O(\epsilon)$ upper bound. We also show the exponent $\frac{\log t}{\log t - \log(t-1)}$ is optimal assuming $n$ tending to infinity and $\epsilon$ tending to $0$. The methods have similarity to recent work by Ellis, Keller, and Lifshitz (2016) in the context of Kneser graphs and other settings.   The author hopes that these results have potential applications in hardness of approximation, particularly in approximate graph coloring and independent set problems.",tensor power cliqu vertic denot graph vertex set two vertic connect onli neq let densiti subset mu frac let vertex boundari set vertic incid vertex perhap includ point investig two similar problem graph first studi vertex isoperimetri problem given densiti nu smallest possibl densiti vertex boundari subset densiti nu let phi nu infimum minimum densiti infti find recurs relat allow one comput phi nu time polynomi number desir bit precis second studi given independ set subseteq densiti mu frac epsilon close maximum size independ set densiti frac show deviat measur mu setminus epsilon frac log log log long epsilon frac frac substanti improv result alon dinur friedgut sudakov ghandehari hatami epsilon upper bound also show expon frac log log log optim assum tend infin epsilon tend method similar recent work elli keller lifshitz context kneser graph set author hope result potenti applic hard approxim particular approxim graph color independ set problem,['Joshua Brakensiek'],"['math.CO', 'cs.CC', 'cs.DM']",False,False,True,False,False,True
183,2017-03-28T14:07:13Z,2017-02-13T10:16:54Z,http://arxiv.org/abs/1702.03700v1,http://arxiv.org/pdf/1702.03700v1,Assortment Optimization under a Single Transition Model,assort optim singl transit model,"In this paper, we consider a Markov chain choice model with single transition. In this model, customers arrive at each product with a certain probability. If the arrived product is unavailable, then the seller can recommend a subset of available products to the customer and the customer will purchase one of the recommended products or choose not to purchase with certain transition probabilities. The distinguishing features of the model are that the seller can control which products to recommend depending on the arrived product and that each customer either purchases a product or leaves the market after one transition.   We study the assortment optimization problem under this model. Particularly, we show that this problem is generally NP-Hard even if each product could only transit to at most two products. Despite the complexity of the problem, we provide polynomial time algorithms for several special cases, such as when the transition probabilities are homogeneous with respect to the starting point, or when each product can only transit to one other product. We also provide a tight performance bound for revenue-ordered assortments. In addition, we propose a compact mixed integer program formulation that can solve this problem of large size. Through extensive numerical experiments, we show that the proposed algorithms can solve the problem efficiently and the obtained assortments could significantly improve the revenue of the seller than under the Markov chain choice model.",paper consid markov chain choic model singl transit model custom arriv product certain probabl arriv product unavail seller recommend subset avail product custom custom purchas one recommend product choos purchas certain transit probabl distinguish featur model seller control product recommend depend arriv product custom either purchas product leav market one transit studi assort optim problem model particular show problem general np hard even product could onli transit two product despit complex problem provid polynomi time algorithm sever special case transit probabl homogen respect start point product onli transit one product also provid tight perform bound revenu order assort addit propos compact mix integ program formul solv problem larg size extens numer experi show propos algorithm solv problem effici obtain assort could signific improv revenu seller markov chain choic model,"['Kameng Nip', 'Zhenbo Wang', 'Zizhuo Wang']","['math.OC', 'cs.CC']",False,False,True,False,False,True
189,2017-03-28T14:07:13Z,2017-02-09T15:41:48Z,http://arxiv.org/abs/1702.02863v1,http://arxiv.org/pdf/1702.02863v1,Complexity Classification Of The Six-Vertex Model,complex classif six vertex model,"We prove a complexity dichotomy theorem for the six-vertex model. For every setting of the parameters of the model, we prove that computing the partition function is either solvable in polynomial time or #P-hard. The dichotomy criterion is explicit.",prove complex dichotomi theorem six vertex model everi set paramet model prove comput partit function either solvabl polynomi time hard dichotomi criterion explicit,"['Jin-Yi Cai', 'Zhiguo Fu', 'Mingji Xia']",['cs.CC'],False,False,True,False,False,False
194,2017-03-28T14:07:17Z,2017-02-05T16:21:35Z,http://arxiv.org/abs/1702.01423v1,http://arxiv.org/pdf/1702.01423v1,Deciding Irreducibility/Indecomposability of Feedback Shift Registers is   NP-hard,decid irreduc indecompos feedback shift regist np hard,Feedback shift registers(FSRs) are a fundamental component in electronics and secure communication. An FSR $f$ is said to be reducible if all the output sequences of another FSR $g$ can also be generated by $f$ and the FSR $g$ has less memory than $f$. An FSR is said to be decomposable if it has the same set of output sequences as a cascade connection of two FSRs. It is proved that deciding whether FSRs are irreducible/indecomposable is NP-hard.,feedback shift regist fsrs fundament compon electron secur communic fsr said reduc output sequenc anoth fsr also generat fsr less memori fsr said decompos set output sequenc cascad connect two fsrs prove decid whether fsrs irreduc indecompos np hard,['Lin Wang'],"['cs.CC', '68Q25, 94A55, 94C15']",False,False,True,False,False,True
200,2017-03-28T14:07:16Z,2017-03-24T16:55:40Z,http://arxiv.org/abs/1703.08504v1,http://arxiv.org/pdf/1703.08504v1,Shingle 2.0: generalising self-consistent and automated domain   discretisation for multi-scale geophysical models,shingl generalis self consist autom domain discretis multi scale geophys model,"The approaches taken to describe and develop spatial discretisations of the domains required for geophysical simulation models are commonly ad hoc, model or application specific and under-documented. This is particularly acute for simulation models that are flexible in their use of multi-scale, anisotropic, fully unstructured meshes where a relatively large number of heterogeneous parameters are required to constrain their full description. As a consequence, it can be difficult to reproduce simulations, ensure a provenance in model data handling and initialisation, and a challenge to conduct model intercomparisons rigorously. This paper takes a novel approach to spatial discretisation, considering it much like a numerical simulation model problem of its own. It introduces a generalised, extensible, self-documenting approach to carefully describe, and necessarily fully, the constraints over the heterogeneous parameter space that determine how a domain is spatially discretised. This additionally provides a method to accurately record these constraints, using high-level natural language based abstractions, that enables full accounts of provenance, sharing and distribution. Together with this description, a generalised consistent approach to unstructured mesh generation for geophysical models is developed, that is automated, robust and repeatable, quick-to-draft, rigorously verified and consistent to the source data throughout. This interprets the description above to execute a self-consistent spatial discretisation process, which is automatically validated to expected discrete characteristics and metrics.",approach taken describ develop spatial discretis domain requir geophys simul model common ad hoc model applic specif document particular acut simul model flexibl use multi scale anisotrop fulli unstructur mesh relat larg number heterogen paramet requir constrain full descript consequ difficult reproduc simul ensur proven model data handl initialis challeng conduct model intercomparison rigor paper take novel approach spatial discretis consid much like numer simul model problem introduc generalis extens self document approach care describ necessarili fulli constraint heterogen paramet space determin domain spatial discretis addit provid method accur record constraint use high level natur languag base abstract enabl full account proven share distribut togeth descript generalis consist approach unstructur mesh generat geophys model develop autom robust repeat quick draft rigor verifi consist sourc data throughout interpret descript abov execut self consist spatial discretis process automat valid expect discret characterist metric,"['Adam S. Candy', 'Julie D. Pietrzak']","['physics.geo-ph', 'cs.CG', 'physics.ao-ph', 'physics.comp-ph', 'physics.flu-dyn']",False,False,True,False,False,False
201,2017-03-28T14:07:16Z,2017-03-24T16:27:52Z,http://arxiv.org/abs/1703.08491v1,http://arxiv.org/pdf/1703.08491v1,A consistent approach to unstructured mesh generation for geophysical   models,consist approach unstructur mesh generat geophys model,"Geophysical model domains typically contain irregular, complex fractal-like boundaries and physical processes that act over a wide range of scales. Constructing geographically constrained boundary-conforming spatial discretizations of these domains with flexible use of anisotropically, fully unstructured meshes is a challenge. The problem contains a wide range of scales and a relatively large, heterogeneous constraint parameter space. Approaches are commonly ad hoc, model or application specific and insufficiently described. Development of new spatial domains is frequently time-consuming, hard to repeat, error prone and difficult to ensure consistent due to the significant human input required. As a consequence, it is difficult to reproduce simulations, ensure a provenance in model data handling and initialization, and a challenge to conduct model intercomparisons rigorously. Moreover, for flexible unstructured meshes, there is additionally a greater potential for inconsistencies in model initialization and forcing parameters. This paper introduces a consistent approach to unstructured mesh generation for geophysical models, that is automated, quick-to-draft and repeat, and provides a rigorous and robust approach that is consistent to the source data throughout. The approach is enabling further new research in complex multi-scale domains, difficult or not possible to achieve with existing methods. Examples being actively pursued in a range of geophysical modeling efforts are presented alongside the approach, together with the implementation library Shingle and a selection of its verification test cases.",geophys model domain typic contain irregular complex fractal like boundari physic process act wide rang scale construct geograph constrain boundari conform spatial discret domain flexibl use anisotrop fulli unstructur mesh challeng problem contain wide rang scale relat larg heterogen constraint paramet space approach common ad hoc model applic specif insuffici describ develop new spatial domain frequent time consum hard repeat error prone difficult ensur consist due signific human input requir consequ difficult reproduc simul ensur proven model data handl initi challeng conduct model intercomparison rigor moreov flexibl unstructur mesh addit greater potenti inconsist model initi forc paramet paper introduc consist approach unstructur mesh generat geophys model autom quick draft repeat provid rigor robust approach consist sourc data throughout approach enabl new research complex multi scale domain difficult possibl achiev exist method exampl activ pursu rang geophys model effort present alongsid approach togeth implement librari shingl select verif test case,['Adam S. Candy'],"['physics.geo-ph', 'cs.CG', 'physics.ao-ph', 'physics.comp-ph', 'physics.flu-dyn']",False,False,True,False,False,False
202,2017-03-28T14:07:16Z,2017-03-21T18:50:24Z,http://arxiv.org/abs/1703.07387v1,http://arxiv.org/pdf/1703.07387v1,"Topological Analysis of Nerves, Reeb Spaces, Mappers, and Multiscale   Mappers",topolog analysi nerv reeb space mapper multiscal mapper,"Data analysis often concerns not only the space where data come from, but also various types of maps attached to data. In recent years, several related structures have been used to study maps on data, including Reeb spaces, mappers and multiscale mappers. The construction of these structures also relies on the so-called \emph{nerve} of a cover of the domain.   In this paper, we aim to analyze the topological information encoded in these structures in order to provide better understanding of these structures and facilitate their practical usage.   More specifically, we show that the one-dimensional homology of the nerve complex $N(\mathcal{U})$ of a path-connected cover $\mathcal{U}$ of a domain $X$ cannot be richer than that of the domain $X$ itself. Intuitively, this result means that no new $H_1$-homology class can be ""created"" under a natural map from $X$ to the nerve complex $N(\mathcal{U})$. Equipping $X$ with a pseudometric $d$, we further refine this result and characterize the classes of $H_1(X)$ that may survive in the nerve complex using the notion of \emph{size} of the covering elements in $\mathcal{U}$. These fundamental results about nerve complexes then lead to an analysis of the $H_1$-homology of Reeb spaces, mappers and multiscale mappers.   The analysis of $H_1$-homology groups unfortunately does not extend to higher dimensions. Nevertheless, by using a map-induced metric, establishing a Gromov-Hausdorff convergence result between mappers and the domain, and interleaving relevant modules, we can still analyze the persistent homology groups of (multiscale) mappers to establish a connection to Reeb spaces.",data analysi often concern onli space data come also various type map attach data recent year sever relat structur use studi map data includ reeb space mapper multiscal mapper construct structur also reli call emph nerv cover domain paper aim analyz topolog inform encod structur order provid better understand structur facilit practic usag specif show one dimension homolog nerv complex mathcal path connect cover mathcal domain cannot richer domain intuit result mean new homolog class creat natur map nerv complex mathcal equip pseudometr refin result character class may surviv nerv complex use notion emph size cover element mathcal fundament result nerv complex lead analysi homolog reeb space mapper multiscal mapper analysi homolog group unfortun doe extend higher dimens nevertheless use map induc metric establish gromov hausdorff converg result mapper domain interleav relev modul still analyz persist homolog group multiscal mapper establish connect reeb space,"['Tamal K. Dey', 'Facundo Memoli', 'Yusu Wang']","['cs.CG', 'math.AT']",False,False,True,False,False,True
204,2017-03-28T14:07:16Z,2017-03-19T22:13:17Z,http://arxiv.org/abs/1703.06526v1,http://arxiv.org/pdf/1703.06526v1,On Optimal 2- and 3-Planar Graphs,optim planar graph,"A graph is $k$-planar if it can be drawn in the plane such that no edge is crossed more than $k$ times. While for $k=1$, optimal $1$-planar graphs, i.e., those with $n$ vertices and exactly $4n-8$ edges, have been completely characterized, this has not been the case for $k \geq 2$. For $k=2,3$ and $4$, upper bounds on the edge density have been developed for the case of simple graphs by Pach and T\'oth, Pach et al. and Ackerman, which have been used to improve the well-known ""Crossing Lemma"". Recently, we proved that these bounds also apply to non-simple $2$- and $3$-planar graphs without homotopic parallel edges and self-loops.   In this paper, we completely characterize optimal $2$- and $3$-planar graphs, i.e., those that achieve the aforementioned upper bounds. We prove that they have a remarkably simple regular structure, although they might be non-simple. The new characterization allows us to develop notable insights concerning new inclusion relationships with other graph classes.",graph planar drawn plane edg cross time optim planar graph vertic exact edg complet character case geq upper bound edg densiti develop case simpl graph pach oth pach et al ackerman use improv well known cross lemma recent prove bound also appli non simpl planar graph without homotop parallel edg self loop paper complet character optim planar graph achiev aforement upper bound prove remark simpl regular structur although might non simpl new character allow us develop notabl insight concern new inclus relationship graph class,"['Michael A. Bekos', 'Michael Kaufmann', 'Chrysanthi N. Raftopoulou']","['cs.CG', 'cs.DM']",False,False,True,False,False,True
209,2017-03-28T14:07:16Z,2017-03-17T01:31:12Z,http://arxiv.org/abs/1703.05863v1,http://arxiv.org/pdf/1703.05863v1,Packing Short Plane Spanning Graphs in Complete Geometric Graphs,pack short plane span graph complet geometr graph,"Given a set of points in the plane, we want to establish a connection network between these points that consists of several disjoint layers. Motivated by sensor networks, we want that each layer is spanning and plane, and that no edge is very long (when compared to the minimum length needed to obtain a spanning graph).   We consider two different approaches: first we show an almost optimal centralized approach to extract two graphs. Then we show a constant factor approximation for a distributed model in which each point can compute its adjacencies using only local information. In both cases the obtained layers are plane",given set point plane want establish connect network point consist sever disjoint layer motiv sensor network want layer span plane edg veri long compar minimum length need obtain span graph consid two differ approach first show almost optim central approach extract two graph show constant factor approxim distribut model point comput adjac use onli local inform case obtain layer plane,"['Oswin Aichholzer', 'Thomas Hackl', 'Matias Korman', 'Alexander Pilz', 'Günter Rote', 'André van Renssen', 'Marcel Roeloffzen', 'Birgit Vogtenhuber']",['cs.CG'],False,False,True,False,False,False
215,2017-03-28T14:07:20Z,2017-03-13T16:18:01Z,http://arxiv.org/abs/1703.04466v1,http://arxiv.org/pdf/1703.04466v1,Bicriteria Rectilinear Shortest Paths among Rectilinear Obstacles in the   Plane,bicriteria rectilinear shortest path among rectilinear obstacl plane,"Given a rectilinear domain $\mathcal{P}$ of $h$ pairwise-disjoint rectilinear obstacles with a total of $n$ vertices in the plane, we study the problem of computing bicriteria rectilinear shortest paths between two points $s$ and $t$ in $\mathcal{P}$. Three types of bicriteria rectilinear paths are considered: minimum-link shortest paths, shortest minimum-link paths, and minimum-cost paths where the cost of a path is a non-decreasing function of both the number of edges and the length of the path. The one-point and two-point path queries are also considered. Algorithms for these problems have been given previously. Our contributions are threefold. First, we find a critical error in all previous algorithms. Second, we correct the error in a not-so-trivial way. Third, we further improve the algorithms so that they are even faster than the previous (incorrect) algorithms when $h$ is relatively small. For example, for the minimum-link shortest paths, we obtain the following results. Our algorithm computes a minimum-link shortest $s$-$t$ path in $O(n+h\log^{3/2} h)$ time. For the one-point queries, we build a data structure of size $O(n+ h\log h)$ in $O(n+h\log^{3/2} h)$ time for a source point $s$, such that given any query point $t$, a minimum-link shortest $s$-$t$ path can be determined in $O(\log n)$ time. For the two-point queries, with $O(n+h^2\log^2 h)$ time and space preprocessing, a minimum-link shortest $s$-$t$ path can be determined in $O(\log n+\log^2 h)$ time for any two query points $s$ and $t$; alternatively, with $O(n+h^2\cdot \log^{2} h \cdot 4^{\sqrt{\log h}})$ time and $O(n+h^2\cdot \log h \cdot 4^{\sqrt{\log h}})$ space preprocessing, we can answer each two-point query in $O(\log n)$ time.",given rectilinear domain mathcal pairwis disjoint rectilinear obstacl total vertic plane studi problem comput bicriteria rectilinear shortest path two point mathcal three type bicriteria rectilinear path consid minimum link shortest path shortest minimum link path minimum cost path cost path non decreas function number edg length path one point two point path queri also consid algorithm problem given previous contribut threefold first find critic error previous algorithm second correct error trivial way third improv algorithm even faster previous incorrect algorithm relat small exampl minimum link shortest path obtain follow result algorithm comput minimum link shortest path log time one point queri build data structur size log log time sourc point given ani queri point minimum link shortest path determin log time two point queri log time space preprocess minimum link shortest path determin log log time ani two queri point altern cdot log cdot sqrt log time cdot log cdot sqrt log space preprocess answer two point queri log time,['Haitao Wang'],"['cs.CG', 'cs.DS']",False,False,True,False,False,False
220,2017-03-28T14:07:25Z,2017-03-10T13:54:29Z,http://arxiv.org/abs/1703.03687v1,http://arxiv.org/pdf/1703.03687v1,Best Laid Plans of Lions and Men,best laid plan lion men,"We answer the following question dating back to J.E. Littlewood (1885 - 1977): Can two lions catch a man in a bounded area with rectifiable lakes? The lions and the man are all assumed to be points moving with at most unit speed. That the lakes are rectifiable means that their boundaries are finitely long. This requirement is to avoid pathological examples where the man survives forever because any path to the lions is infinitely long. We show that the answer to the question is not always ""yes"" by giving an example of a region $R$ in the plane where the man has a strategy to survive forever. $R$ is a polygonal region with holes and the exterior and interior boundaries are pairwise disjoint, simple polygons. Our construction is the first truly two-dimensional example where the man can survive.   Next, we consider the following game played on the entire plane instead of a bounded area: There is any finite number of unit speed lions and one fast man who can run with speed $1+\varepsilon$ for some value $\varepsilon>0$. Can the man always survive? We answer the question in the affirmative for any constant $\varepsilon>0$.",answer follow question date back littlewood two lion catch man bound area rectifi lake lion man assum point move unit speed lake rectifi mean boundari finit long requir avoid patholog exampl man surviv forev becaus ani path lion infinit long show answer question alway yes give exampl region plane man strategi surviv forev polygon region hole exterior interior boundari pairwis disjoint simpl polygon construct first truli two dimension exampl man surviv next consid follow game play entir plane instead bound area ani finit number unit speed lion one fast man run speed varepsilon valu varepsilon man alway surviv answer question affirm ani constant varepsilon,"['Mikkel Abrahamsen', 'Jacob Holm', 'Eva Rotenberg', 'Christian Wulff-Nilsen']","['cs.CG', 'cs.GT']",False,False,True,False,False,True
223,2017-03-28T14:07:25Z,2017-03-08T16:32:05Z,http://arxiv.org/abs/1703.02901v1,http://arxiv.org/pdf/1703.02901v1,Local Equivalence and Intrinsic Metrics between Reeb Graphs,local equival intrins metric reeb graph,"As graphical summaries for topological spaces and maps, Reeb graphs are common objects in the computer graphics or topological data analysis literature. Defining good metrics between these objects has become an important question for applications, where it matters to quantify the extent by which two given Reeb graphs differ. Recent contributions emphasize this aspect, proposing novel distances such as {\em functional distortion} or {\em interleaving} that are provably more discriminative than the so-called {\em bottleneck distance}, being true metrics whereas the latter is only a pseudo-metric. Their main drawback compared to the bottleneck distance is to be comparatively hard (if at all possible) to evaluate. Here we take the opposite view on the problem and show that the bottleneck distance is in fact good enough {\em locally}, in the sense that it is able to discriminate a Reeb graph from any other Reeb graph in a small enough neighborhood, as efficiently as the other metrics do. This suggests considering the {\em intrinsic metrics} induced by these distances, which turn out to be all {\em globally} equivalent. This novel viewpoint on the study of Reeb graphs has a potential impact on applications, where one may not only be interested in discriminating between data but also in interpolating between them.",graphic summari topolog space map reeb graph common object comput graphic topolog data analysi literatur defin good metric object becom import question applic matter quantifi extent two given reeb graph differ recent contribut emphas aspect propos novel distanc em function distort em interleav provabl discrimin call em bottleneck distanc true metric wherea latter onli pseudo metric main drawback compar bottleneck distanc compar hard possibl evalu take opposit view problem show bottleneck distanc fact good enough em local sens abl discrimin reeb graph ani reeb graph small enough neighborhood effici metric suggest consid em intrins metric induc distanc turn em global equival novel viewpoint studi reeb graph potenti impact applic one may onli interest discrimin data also interpol,"['Mathieu Carrière', 'Steve Oudot']","['cs.CG', 'math.AT']",False,False,True,False,False,True
227,2017-03-28T14:07:25Z,2017-03-06T16:05:05Z,http://arxiv.org/abs/1703.01943v1,http://arxiv.org/pdf/1703.01943v1,Enumeration of $2$-level polytopes,enumer level polytop,"A (convex) polytope $P$ is said to be $2$-level if for every direction of hyperplanes which is facet-defining for $P$, the vertices of $P$ can be covered with two hyperplanes of that direction. The study of these polytopes is motivated by questions in combinatorial optimization and communication complexity, among others. In this paper, we present the first algorithm for enumerating all combinatorial types of $2$-level polytopes of a given dimension $d$, and provide complete experimental results for $d \leqslant 7$. Our approach is inductive: for each fixed $(d-1)$-dimensional $2$-level polytope $P_0$, we enumerate all $d$-dimensional $2$-level polytopes $P$ that have $P_0$ as a facet. This relies on the enumeration of the closed sets of a closure operator over a finite ground set. By varying the prescribed facet $P_0$, we obtain all $2$-level polytopes in dimension $d$.",convex polytop said level everi direct hyperplan facet defin vertic cover two hyperplan direct studi polytop motiv question combinatori optim communic complex among paper present first algorithm enumer combinatori type level polytop given dimens provid complet experiment result leqslant approach induct fix dimension level polytop enumer dimension level polytop facet reli enumer close set closur oper finit ground set vari prescrib facet obtain level polytop dimens,"['Adam Bohn', 'Yuri Faenza', 'Samuel Fiorini', 'Vissarion Fisikopoulos', 'Marco Macchia', 'Kanstantsin Pashkovich']","['math.CO', 'cs.CG', 'cs.DM', 'math.OC', '05A15, 05C17, 52B12, 52B55, 68W05, 90C22']",False,False,True,False,False,False
229,2017-03-28T14:07:25Z,2017-03-05T19:10:17Z,http://arxiv.org/abs/1703.01646v1,http://arxiv.org/pdf/1703.01646v1,A PTAS for TSP with Neighborhoods Among Fat Regions in the Plane,ptas tsp neighborhood among fat region plane,"The Euclidean TSP with neighborhoods (TSPN) problem seeks a shortest tour that visits a given collection of $n$ regions ({\em neighborhoods}). We present the first polynomial-time approximation scheme for TSPN for a set of regions given by arbitrary disjoint fat regions in the plane. This improves substantially upon the known approximation algorithms, and is the first PTAS for TSPN on regions of non-comparable sizes. Our result is based on a novel extension of the $m$-guillotine method. The result applies to regions that are ""fat"" in a very weak sense: each region $P_i$ has area $\Omega([diam(P_i)]^2)$, but is otherwise arbitrary.",euclidean tsp neighborhood tspn problem seek shortest tour visit given collect region em neighborhood present first polynomi time approxim scheme tspn set region given arbitrari disjoint fat region plane improv substanti upon known approxim algorithm first ptas tspn region non compar size result base novel extens guillotin method result appli region fat veri weak sens region area omega diam otherwis arbitrari,['Joseph S. B. Mitchell'],['cs.CG'],False,False,True,False,False,True
233,2017-03-28T14:07:29Z,2017-03-01T02:48:12Z,http://arxiv.org/abs/1703.00112v1,http://arxiv.org/pdf/1703.00112v1,Minimum Enclosing Circle of a Set of Static Points with Dynamic Weight   from One Free Point,minimum enclos circl set static point dynam weight one free point,"Given a set $S$ of $n$ static points and a free point $p$ in the Euclidean plane, we study a new variation of the minimum enclosing circle problem, in which a dynamic weight that equals to the reciprocal of the distance from the free point $p$ to the undetermined circle center is included. In this work, we prove the optimal solution of the new problem is unique and lies on the boundary of the farthest-point Voronoi diagram of $S$, once $p$ does not coincide with any vertex of the convex hull of $S$. We propose a tree structure constructed from the boundary of the farthest-point Voronoi diagram and use the hierarchical relationship between edges to locate the optimal solution. The plane could be divide into at most $3n-4$ non-overlapping regions. When $p$ lies in one of the regions, the optimal solution locates at one node or lies on the interior of one edge in the boundary of the farthest-point Voronoi diagram. Moreover, we apply the new variation to calculate the maximum displacement of one point $p$ under the condition that the displacements of points in $S$ are restricted in 2D rigid motion.",given set static point free point euclidean plane studi new variat minimum enclos circl problem dynam weight equal reciproc distanc free point undetermin circl center includ work prove optim solut new problem uniqu lie boundari farthest point voronoi diagram onc doe coincid ani vertex convex hull propos tree structur construct boundari farthest point voronoi diagram use hierarch relationship edg locat optim solut plane could divid non overlap region lie one region optim solut locat one node lie interior one edg boundari farthest point voronoi diagram moreov appli new variat calcul maximum displac one point condit displac point restrict rigid motion,"['Lei Qiu', 'Yu Zhang', 'Li Zhang']",['cs.CG'],False,False,True,False,False,False
239,2017-03-28T14:07:29Z,2017-02-27T17:07:31Z,http://arxiv.org/abs/1702.08380v1,http://arxiv.org/pdf/1702.08380v1,Exploring Increasing-Chord Paths and Trees,explor increas chord path tree,"A straight-line drawing $\Gamma$ of a graph $G=(V,E)$ is a drawing of $G$ in the Euclidean plane, where every vertex in $G$ is mapped to a distinct point, and every edge in $G$ is mapped to a straight line segment between their endpoints. A path $P$ in $\Gamma$ is called increasing-chord if for every four points (not necessarily vertices) $a,b,c,d$ on $P$ in this order, the Euclidean distance between $b,c$ is at most the Euclidean distance between $a,d$. A spanning tree $T$ rooted at some vertex $r$ in $\Gamma$ is called increasing-chord if $T$ contains an increasing-chord path from $r$ to every vertex in $T$. In this paper we prove that given a vertex $r$ in a straight-line drawing $\Gamma$, it is NP-complete to determine whether $\Gamma$ contains an increasing-chord spanning tree rooted at $r$. We conjecture that finding an increasing-chord path between a pair of vertices in $\Gamma$, which is an intriguing open problem posed by Alamdari et al., is also NP-complete, and show a (non-polynomial) reduction from the 3-SAT problem.",straight line draw gamma graph draw euclidean plane everi vertex map distinct point everi edg map straight line segment endpoint path gamma call increas chord everi four point necessarili vertic order euclidean distanc euclidean distanc span tree root vertex gamma call increas chord contain increas chord path everi vertex paper prove given vertex straight line draw gamma np complet determin whether gamma contain increas chord span tree root conjectur find increas chord path pair vertic gamma intrigu open problem pose alamdari et al also np complet show non polynomi reduct sat problem,"['Yeganeh Bahoo', 'Stephane Durocher', 'Sahar Mehrpour', 'Debajyoti Mondal']",['cs.CG'],False,False,True,False,False,True
242,2017-03-28T14:07:33Z,2017-02-24T12:25:43Z,http://arxiv.org/abs/1702.07555v1,http://arxiv.org/pdf/1702.07555v1,A generalization of crossing families,general cross famili,"For a set of points in the plane, a \emph{crossing family} is a set of line segments, each joining two of the points, such that any two line segments cross. We investigate the following generalization of crossing families: a \emph{spoke set} is a set of lines drawn through a point set such that each unbounded region of the induced line arrangement contains at least one point of the point set. We show that every point set has a spoke set of size $\sqrt{\frac{n}{8}}$. We also characterize the matchings obtained by selecting exactly one point in each unbounded region and connecting every such point to the point in the antipodal unbounded region.",set point plane emph cross famili set line segment join two point ani two line segment cross investig follow general cross famili emph spoke set set line drawn point set unbound region induc line arrang contain least one point point set show everi point set spoke set size sqrt frac also character match obtain select exact one point unbound region connect everi point point antipod unbound region,['Patrick Schnider'],['cs.CG'],False,False,True,False,False,False
243,2017-03-28T14:07:33Z,2017-02-23T21:32:10Z,http://arxiv.org/abs/1702.07399v1,http://arxiv.org/pdf/1702.07399v1,An Optimal Algorithm for Computing the Spherical Depth of Points in the   Plane,optim algorithm comput spheric depth point plane,"For a distribution function $F$ on $\mathbb{R}^d$ and a point $q\in \mathbb{R}^d$, the \emph{spherical depth} $\SphD(q;F)$ is defined to be the probability that a point $q$ is contained inside a random closed hyper-ball obtained from a pair of points from $F$. The spherical depth $\SphD(q;S)$ is also defined for an arbitrary data set $S\subseteq \mathbb{R}^d$ and $q\in \mathbb{R}^d$. This definition is based on counting all of the closed hyper-balls, obtained from pairs of points in $S$, that contain $q$. The significant advantage of using the spherical depth in multivariate data analysis is related to its complexity of computation. Unlike most other data depths, the time complexity of the spherical depth grows linearly rather than exponentially in the dimension $d$. The straightforward algorithm for computing the spherical depth in dimension $d$ takes $O(dn^2)$. The main result of this paper is an optimal algorithm that we present for computing the bivariate spherical depth. The algorithm takes $O(n \log n)$ time. By reducing the problem of \textit{Element Uniqueness}, we prove that computing the spherical depth requires $\Omega(n \log n)$ time. Some geometric properties of spherical depth are also investigated in this paper. These properties indicate that \emph{simplicial depth} ($\SD$) (Liu, 1990) is linearly bounded by spherical depth (in particular, $\SphD\geq \frac{2}{3}SD$). To illustrate this relationship between the spherical depth and the simplicial depth, some experimental results are provided. The obtained experimental bound ($\SphD\geq 2\SD$) indicates that, perhaps, a stronger theoretical bound can be achieved.",distribut function mathbb point mathbb emph spheric depth sphd defin probabl point contain insid random close hyper ball obtain pair point spheric depth sphd also defin arbitrari data set subseteq mathbb mathbb definit base count close hyper ball obtain pair point contain signific advantag use spheric depth multivari data analysi relat complex comput unlik data depth time complex spheric depth grow linear rather exponenti dimens straightforward algorithm comput spheric depth dimens take dn main result paper optim algorithm present comput bivari spheric depth algorithm take log time reduc problem textit element uniqu prove comput spheric depth requir omega log time geometr properti spheric depth also investig paper properti indic emph simplici depth sd liu linear bound spheric depth particular sphd geq frac sd illustr relationship spheric depth simplici depth experiment result provid obtain experiment bound sphd geq sd indic perhap stronger theoret bound achiev,"['David Bremner', 'Rasoul Shahsavarifar']",['cs.CG'],False,False,True,False,False,True
246,2017-03-28T14:07:33Z,2017-02-20T08:56:40Z,http://arxiv.org/abs/1702.05900v1,http://arxiv.org/pdf/1702.05900v1,$δ$-Greedy $t$-spanner,greedi spanner,"We introduce a new geometric spanner, $\delta$-Greedy, whose construction is based on a generalization of the known Path-Greedy and Gap-Greedy spanners. The $\delta$-Greedy spanner combines the most desirable properties of geometric spanners both in theory and in practice. More specifically, it has the same theoretical and practical properties as the Path-Greedy spanner: a natural definition, small degree, linear number of edges, low weight, and strong $(1+\varepsilon)$-spanner for every $\varepsilon>0$. The $\delta$-Greedy algorithm is an improvement over the Path-Greedy algorithm with respect to the number of shortest path queries and hence with respect to its construction time. We show how to construct such a spanner for a set of $n$ points in the plane in $O(n^2 \log n)$ time.   The $\delta$-Greedy spanner has an additional parameter, $\delta$, which indicates how close it is to the Path-Greedy spanner on the account of the number of shortest path queries. For $\delta = t$ the output spanner is identical to the Path-Greedy spanner, while the number of shortest path queries is, in practice, linear.   Finally, we show that for a set of $n$ points placed independently at random in a unit square the expected construction time of the $\delta$-Greedy algorithm is $O(n \log n)$. Our analysis indicates that the $\delta$-Greedy spanner gives the best results among the known spanners of expected $O(n \log n)$ time for random point sets. Moreover, the analysis implies that by setting $\delta = t$, the $\delta$-Greedy algorithm provides a spanner identical to the Path-Greedy spanner in expected $O(n \log n)$ time.",introduc new geometr spanner delta greedi whose construct base general known path greedi gap greedi spanner delta greedi spanner combin desir properti geometr spanner theori practic specif theoret practic properti path greedi spanner natur definit small degre linear number edg low weight strong varepsilon spanner everi varepsilon delta greedi algorithm improv path greedi algorithm respect number shortest path queri henc respect construct time show construct spanner set point plane log time delta greedi spanner addit paramet delta indic close path greedi spanner account number shortest path queri delta output spanner ident path greedi spanner number shortest path queri practic linear final show set point place independ random unit squar expect construct time delta greedi algorithm log analysi indic delta greedi spanner give best result among known spanner expect log time random point set moreov analysi impli set delta delta greedi algorithm provid spanner ident path greedi spanner expect log time,"['Gali Bar-On', 'Paz Carmi']",['cs.CG'],False,False,True,False,False,True
252,2017-03-28T14:07:37Z,2017-02-15T14:59:10Z,http://arxiv.org/abs/1702.04641v1,http://arxiv.org/pdf/1702.04641v1,Filling missing data in point clouds by merging structured and   unstructured point clouds,fill miss data point cloud merg structur unstructur point cloud,"Point clouds arising from structured data, mainly as a result of CT scans, provides special properties on the distribution of points and the distances between those. Yet often, the amount of data provided can not compare to unstructured point clouds, i.e. data that arises from 3D light scans or laser scans. This article hereby proposes an approach to extend structured data and enhance the quality by inserting selected points from an unstructured point cloud. The resulting point cloud still has a partial structure that is called ""half-structure"". In this way, missing data that can not be optimally recovered through other surface reconstruction methods can be completed.",point cloud aris structur data main result ct scan provid special properti distribut point distanc yet often amount data provid compar unstructur point cloud data aris light scan laser scan articl herebi propos approach extend structur data enhanc qualiti insert select point unstructur point cloud result point cloud still partial structur call half structur way miss data optim recov surfac reconstruct method complet,"['Franziska Lippoldt', 'Hartmut Schwandt']","['cs.CG', 'cs.CV', 'cs.DM', '53A05', 'F.2.2; G.2.1; I.3.5']",False,False,True,False,False,False
253,2017-03-28T14:07:37Z,2017-02-14T15:20:23Z,http://arxiv.org/abs/1702.04259v1,http://arxiv.org/pdf/1702.04259v1,On the metastable Mabillard-Wagner conjecture,metast mabillard wagner conjectur,"The purpose of this note is to attract attention to the following conjecture (metastable $r$-fold Whitney trick) by clarifying its status as not having a complete proof, in the sense described in the paper.   Assume that $D=D_1\sqcup\ldots\sqcup D_r$ is disjoint union of $r$ disks of dimension $s$, $f:D\to B^d$ a proper PL map such that $f\partial D_1\cap\ldots\cap f\partial D_r=\emptyset$, $rd\ge (r+1)s+3$ and $d\ge s+3$. If the map $$f^r:\partial(D_1\times\ldots\times D_r)\to (B^d)^r-\{(x,x,\ldots,x)\in(B^d)^r\  \ x\in B^d\}$$ extends to $D_1\times\ldots\times D_r$, then there is a PL map $\overline f:D\to B^d$ such that $$\overline f=f \quad\text{on}\quad D_r\cup\partial D\quad\text{and}\quad \overline fD_1\cap\ldots\cap \overline fD_r=\emptyset.$$",purpos note attract attent follow conjectur metast fold whitney trick clarifi status complet proof sens describ paper assum sqcup ldot sqcup disjoint union disk dimens proper pl map partial cap ldot cap partial emptyset rd ge ge map partial time ldot time ldot extend time ldot time pl map overlin overlin quad text quad cup partial quad text quad overlin fd cap ldot cap overlin fd emptyset,['A. Skopenkov'],"['math.GT', 'cs.CG', '57Q35, 57R65, 52B99']",False,False,True,False,False,True
260,2017-03-28T14:07:41Z,2017-02-06T21:31:57Z,http://arxiv.org/abs/1702.01799v1,http://arxiv.org/pdf/1702.01799v1,Radial Contour Labeling with Straight Leaders,radial contour label straight leader,"The usefulness of technical drawings as well as scientific illustrations such as medical drawings of human anatomy essentially depends on the placement of labels that describe all relevant parts of the figure. In order to not spoil or clutter the figure with text, the labels are often placed around the figure and are associated by thin connecting lines to their features, respectively. This labeling technique is known as external label placement.   In this paper we introduce a flexible and general approach for external label placement assuming a contour of the figure prescribing the possible positions of the labels. While much research on external label placement aims for fast labeling procedures for interactive systems, we focus on highest-quality illustrations. Based on interviews with domain experts and a semi-automatic analysis of 202 handmade anatomical drawings, we identify a set of 18 layout quality criteria, naturally not all of equal importance. We design a new geometric label placement algorithm that is based only on the most important criteria. Yet, other criteria can flexibly be included in the algorithm, either as hard constraints not to be violated or as soft constraints whose violation is penalized by a general cost function. We formally prove that our approach yields labelings that satisfy all hard constraints and have minimum overall cost. Introducing several speedup techniques, we further demonstrate how to deploy our approach in practice. In an experimental evaluation on real-world anatomical drawings we show that the resulting labelings are of high quality and can be produced in adequate time.",use technic draw well scientif illustr medic draw human anatomi essenti depend placement label describ relev part figur order spoil clutter figur text label often place around figur associ thin connect line featur respect label techniqu known extern label placement paper introduc flexibl general approach extern label placement assum contour figur prescrib possibl posit label much research extern label placement aim fast label procedur interact system focus highest qualiti illustr base interview domain expert semi automat analysi handmad anatom draw identifi set layout qualiti criteria natur equal import design new geometr label placement algorithm base onli import criteria yet criteria flexibl includ algorithm either hard constraint violat soft constraint whose violat penal general cost function formal prove approach yield label satisfi hard constraint minimum overal cost introduc sever speedup techniqu demonstr deploy approach practic experiment evalu real world anatom draw show result label high qualiti produc adequ time,"['Benjamin Niedermann', 'Martin Nöllenburg', 'Ignaz Rutter']",['cs.CG'],False,False,True,False,False,True
265,2017-03-28T14:07:41Z,2017-02-04T11:51:44Z,http://arxiv.org/abs/1702.01275v1,http://arxiv.org/pdf/1702.01275v1,Geometric Biplane Graphs I: Maximal Graphs,geometr biplan graph maxim graph,"We study biplane graphs drawn on a finite planar point set $S$ in general position. This is the family of geometric graphs whose vertex set is $S$ and can be decomposed into two plane graphs. We show that two maximal biplane graphs---in the sense that no edge can be added while staying biplane---may differ in the number of edges, and we provide an efficient algorithm for adding edges to a biplane graph to make it maximal. We also study extremal properties of maximal biplane graphs such as the maximum number of edges and the largest maximum connectivity over $n$-element point sets.",studi biplan graph drawn finit planar point set general posit famili geometr graph whose vertex set decompos two plane graph show two maxim biplan graph sens edg ad stay biplan may differ number edg provid effici algorithm ad edg biplan graph make maxim also studi extrem properti maxim biplan graph maximum number edg largest maximum connect element point set,"['Alfredo García', 'Ferran Hurtado', 'Matias Korman', 'Inês Matos', 'Maria Saumell', 'Rodrigo I. Silveira', 'Javier Tejel', 'Csaba D. Tóth']",['cs.CG'],False,False,True,False,False,True
271,2017-03-28T14:07:47Z,2017-01-19T16:24:27Z,http://arxiv.org/abs/1701.05500v1,http://arxiv.org/pdf/1701.05500v1,The number of realizations of a Laman graph,number realize laman graph,"Laman graphs model planar frameworks that are rigid for a general choice of distances between the vertices. There are finitely many ways, up to isometries, to realize a Laman graph in the plane. Such realizations can be seen as solutions of systems of quadratic equations prescribing the distances between pairs of points. Using ideas from algebraic and tropical geometry, we provide a recursion formula for the number of complex solutions of such systems.",laman graph model planar framework rigid general choic distanc vertic finit mani way isometri realiz laman graph plane realize seen solut system quadrat equat prescrib distanc pair point use idea algebra tropic geometri provid recurs formula number complex solut system,"['Jose Capco', 'Matteo Gallet', 'Georg Grasegger', 'Christoph Koutschan', 'Niels Lubbes', 'Josef Schicho']","['math.AG', 'cs.CG', 'cs.SC', 'math.CO', '14T05, 14N99, 52C25, 05C99']",False,False,True,False,False,False
275,2017-03-28T14:07:47Z,2017-01-18T16:46:08Z,http://arxiv.org/abs/1701.05141v1,http://arxiv.org/pdf/1701.05141v1,The Explicit Corridor Map: A Medial Axis-Based Navigation Mesh for   Multi-Layered Environments,explicit corridor map medial axi base navig mesh multi layer environ,"Path planning for walking characters in complicated virtual environments is a fundamental task in simulations and games. In this paper, we present an improved definition of the Explicit Corridor Map (ECM), a navigation mesh that allows efficient path planning and crowd simulation for disk-shaped characters of any radius. The ECM is a medial axis (MA) annotated with nearest-obstacle information. For a planar environment with $n$ obstacle vertices, the ECM has size $O(n)$ and can be computed in $O(n \log n)$ time.   We also introduce multi-layered environments (MLEs), in which multiple planar layers are connected by line segment connections. Typical real-world examples are multi-storey buildings, train stations, and sports stadiums. We define the MA and the ECM for multi-layered environments, based on projected distances on the ground plane. For an MLE with $n$ obstacle points and $k$ connections, the MA has size $O(n)$. We present an improved algorithm that constructs the MA and ECM in $O(n \log n \log k)$ time.   Our implementations show that the ECM can be computed efficiently for large 2D and multi-layered environments, and that it can be used to compute paths within milliseconds. This enables simulations of large virtual crowds of heterogeneous characters in real-time.",path plan walk charact complic virtual environ fundament task simul game paper present improv definit explicit corridor map ecm navig mesh allow effici path plan crowd simul disk shape charact ani radius ecm medial axi annot nearest obstacl inform planar environ obstacl vertic ecm size comput log time also introduc multi layer environ mles multipl planar layer connect line segment connect typic real world exampl multi storey build train station sport stadium defin ecm multi layer environ base project distanc ground plane mle obstacl point connect size present improv algorithm construct ecm log log time implement show ecm comput effici larg multi layer environ use comput path within millisecond enabl simul larg virtual crowd heterogen charact real time,"['Wouter van Toll', 'Atlas F. Cook IV', 'Marc J. van Kreveld', 'Roland Geraerts']","['cs.CG', 'cs.DS']",False,False,True,False,False,False
277,2017-03-28T14:07:47Z,2017-01-12T16:01:50Z,http://arxiv.org/abs/1701.03388v1,http://arxiv.org/pdf/1701.03388v1,Dynamic and Kinetic Conflict-Free Coloring of Intervals with Respect to   Points,dynam kinet conflict free color interv respect point,"We introduce the dynamic conflict-free coloring problem for a set $S$ of intervals in $\mathbb{R}^1$ with respect to points, where the goal is to maintain a conflict-free coloring for $S$ under insertions and deletions. We investigate trade-offs between the number of colors used and the number of intervals that are recolored upon insertion or deletion of an interval. Our results include:   - a lower bound on the number of recolorings as a function of the number of colors, which implies that with $O(1)$ recolorings per update the worst-case number of colors is $\Omega(\log n/\log\log n)$, and that any strategy using $O(1/\varepsilon)$ colors needs $\Omega(\varepsilon n^{\varepsilon})$ recolorings;   - a coloring strategy that uses $O(\log n)$ colors at the cost of $O(\log n)$ recolorings, and another strategy that uses $O(1/\varepsilon)$ colors at the cost of $O(n^{\varepsilon}/\varepsilon)$ recolorings;   - stronger upper and lower bounds for special cases.   We also consider the kinetic setting where the intervals move continuously (but there are no insertions or deletions); here we show how to maintain a coloring with only four colors at the cost of three recolorings per event and show this is tight.",introduc dynam conflict free color problem set interv mathbb respect point goal maintain conflict free color insert delet investig trade number color use number interv recolor upon insert delet interv result includ lower bound number recolor function number color impli recolor per updat worst case number color omega log log log ani strategi use varepsilon color need omega varepsilon varepsilon recolor color strategi use log color cost log recolor anoth strategi use varepsilon color cost varepsilon varepsilon recolor stronger upper lower bound special case also consid kinet set interv move continu insert delet show maintain color onli four color cost three recolor per event show tight,"['Mark de Berg', 'Tim Leijsen', 'André van Renssen', 'Marcel Roeloffzen', 'Aleksandar Markovic', 'Gerhard Woeginger']",['cs.CG'],False,False,True,False,False,False
279,2017-03-28T14:07:47Z,2017-01-11T04:04:43Z,http://arxiv.org/abs/1701.02843v1,http://arxiv.org/pdf/1701.02843v1,Solve Partial Differential Equations on Manifold From Incomplete   Inter-Point Distance,solv partial differenti equat manifold incomplet inter point distanc,"Solutions of partial differential equations (PDEs) on manifolds have provided important applications in different fields in science and engineering. Existing methods are majorly based on discretization of manifolds as implicit functions, triangle meshes, or point clouds, where the manifold structure is approximated by either zero level set of an implicit function or a set of points. In many applications, manifolds might be only provided as an inter-point distance matrix with possible missing values. This paper discusses a framework to discretize PDEs on manifolds represented as incomplete distance information. Without conducting a time-consuming global coordinates reconstruction, we propose a more efficient strategy by discretizing differential operators only based on point-wisely local reconstruction. Our local reconstruction model is based on the recent advances of low-rank matrix completion theory, where only a very small random portion of distance information is required. This method enables us to conduct analyses of incomplete distance data using solutions of special designed PDEs such as the Laplace-Beltrami (LB) eigen-system. As an application, we demonstrate a new way of manifold reconstruction from an incomplete distance by stitching patches using the spectrum of the LB operator. Intensive numerical experiments demonstrate the effectiveness of the proposed methods.",solut partial differenti equat pdes manifold provid import applic differ field scienc engin exist method major base discret manifold implicit function triangl mesh point cloud manifold structur approxim either zero level set implicit function set point mani applic manifold might onli provid inter point distanc matrix possibl miss valu paper discuss framework discret pdes manifold repres incomplet distanc inform without conduct time consum global coordin reconstruct propos effici strategi discret differenti oper onli base point wise local reconstruct local reconstruct model base recent advanc low rank matrix complet theori onli veri small random portion distanc inform requir method enabl us conduct analys incomplet distanc data use solut special design pdes laplac beltrami lb eigen system applic demonstr new way manifold reconstruct incomplet distanc stitch patch use spectrum lb oper intens numer experi demonstr effect propos method,"['Rongjie Lai', 'Jia Li']","['math.NA', 'cs.CG', '65D18, 65D25, 65N25']",False,False,True,False,False,False
282,2017-03-28T14:07:51Z,2017-02-22T20:40:08Z,http://arxiv.org/abs/1701.02200v2,http://arxiv.org/pdf/1701.02200v2,Bounding a global red-blue proportion using local conditions,bound global red blue proport use local condit,"We study the following local-to-global phenomenon: Let $B$ and $R$ be two finite sets of (blue and red) points in the Euclidean plane $\mathbb{R}^2$. Suppose that in each ""neighborhood"" of a red point, the number of blue points is at least as large as the number of red points. We show that in this case the total number of blue points is at least one fifth of the total number of red points. We also show that this bound is optimal and we generalize the result to arbitrary dimension and arbitrary norm using results from Minkowski arrangements.",studi follow local global phenomenon let two finit set blue red point euclidean plane mathbb suppos neighborhood red point number blue point least larg number red point show case total number blue point least one fifth total number red point also show bound optim general result arbitrari dimens arbitrari norm use result minkowski arrang,"['Márton Naszódi', 'Leonardo Martínez Sandoval', 'Shakhar Smorodinsky']",['cs.CG'],False,False,True,False,False,False
283,2017-03-28T14:07:51Z,2017-01-05T12:00:37Z,http://arxiv.org/abs/1701.06430v1,http://arxiv.org/pdf/1701.06430v1,An Upper Bound of the Minimal Dispersion via Delta Covers,upper bound minim dispers via delta cover,"For a point set of $n$ elements in the $d$-dimensional unit cube and a class of test sets we are interested in the largest volume of a test set which does not contain any point. For all natural numbers $n$, $d$ and under the assumption of a $delta$-cover with cardinality $\vert \Gamma_\delta \vert$ we prove that there is a point set, such that the largest volume of such a test set without any point is bounded by $\frac{\log \vert \Gamma_\delta \vert}{n} + \delta$. For axis-parallel boxes on the unit cube this leads to a volume of at most $\frac{4d}{n}\log(\frac{9n}{d})$ and on the torus to $\frac{4d}{n}\log (2n)$.",point set element dimension unit cube class test set interest largest volum test set doe contain ani point natur number assumpt delta cover cardin vert gamma delta vert prove point set largest volum test set without ani point bound frac log vert gamma delta vert delta axi parallel box unit cube lead volum frac log frac torus frac log,['Daniel Rudolf'],"['cs.CG', 'math.NA', '52B55, 68Q25']",False,False,True,False,False,True
288,2017-03-28T14:07:51Z,2016-12-31T21:53:09Z,http://arxiv.org/abs/1701.00169v1,http://arxiv.org/pdf/1701.00169v1,Tree segmentation in multi-story stands within small-footprint airborne   LiDAR data,tree segment multi stori stand within small footprint airborn lidar data,"Airborne LiDAR point cloud of a forest contains three dimensional data, from which vertical stand structure (including information about under-story trees) can be derived. This paper presents a segmentation approach for multi-story stands that strips the point cloud to its canopy layers, identifies individual tree segments within each layer using a DSM-based tree identification method as a building block, and combines the segments of immediate layers in order to fix potential over-segmentation of tree crowns across the layers. We introduce local layering that analyzes the vertical distributions of LiDAR points within their local neighborhoods in order to locally determine the height thresholds for layering the canopy. Unlike the previous work that stripped stiff layers within constrained areas, the local layering method strips flexible (in thickness and elevation) and narrower canopy layers within unconstrained areas. Statistical analyses showed that layering in general strongly improves identifying (specifically under-story) trees for the cost of moderately increasing over-segmentation rate of the identified trees. Combining tree segments across the immediate layers did not seem to improve tree identification accuracy remarkably, suggesting that layers separated canopy layers rather precisely.",airborn lidar point cloud forest contain three dimension data vertic stand structur includ inform stori tree deriv paper present segment approach multi stori stand strip point cloud canopi layer identifi individu tree segment within layer use dsm base tree identif method build block combin segment immedi layer order fix potenti segment tree crown across layer introduc local layer analyz vertic distribut lidar point within local neighborhood order local determin height threshold layer canopi unlik previous work strip stiff layer within constrain area local layer method strip flexibl thick elev narrow canopi layer within unconstrain area statist analys show layer general strong improv identifi specif stori tree cost moder increas segment rate identifi tree combin tree segment across immedi layer seem improv tree identif accuraci remark suggest layer separ canopi layer rather precis,"['Hamid Hamraz', 'Marco A. Contreras', 'Jun Zhang']","['cs.CV', 'cs.CE', 'cs.CG']",False,False,True,False,False,False
289,2017-03-28T14:07:51Z,2016-12-31T17:05:53Z,http://arxiv.org/abs/1701.00146v1,http://arxiv.org/pdf/1701.00146v1,Even $1 \times n$ Edge-Matching and Jigsaw Puzzles are Really Hard,even time edg match jigsaw puzzl realli hard,"We prove the computational intractability of rotating and placing $n$ square tiles into a $1 \times n$ array such that adjacent tiles are compatible--either equal edge colors, as in edge-matching puzzles, or matching tab/pocket shapes, as in jigsaw puzzles. Beyond basic NP-hardness, we prove that it is NP-hard even to approximately maximize the number of placed tiles (allowing blanks), while satisfying the compatibility constraint between nonblank tiles, within a factor of 0.9999999851. (On the other hand, there is an easy $1 \over 2$-approximation.) This is the first (correct) proof of inapproximability for edge-matching and jigsaw puzzles. Along the way, we prove NP-hardness of distinguishing, for a directed graph on $n$ nodes, between having a Hamiltonian path (length $n-1$) and having at most $0.999999284 (n-1)$ edges that form a vertex-disjoint union of paths. We use this gap hardness and gap-preserving reductions to establish similar gap hardness for $1 \times n$ jigsaw and edge-matching puzzles.",prove comput intract rotat place squar tile time array adjac tile compat either equal edg color edg match puzzl match tab pocket shape jigsaw puzzl beyond basic np hard prove np hard even approxim maxim number place tile allow blank satisfi compat constraint nonblank tile within factor hand easi approxim first correct proof inapproxim edg match jigsaw puzzl along way prove np hard distinguish direct graph node hamiltonian path length edg form vertex disjoint union path use gap hard gap preserv reduct establish similar gap hard time jigsaw edg match puzzl,"['Jeffrey Bosboom', 'Erik D. Demaine', 'Martin L. Demaine', 'Adam Hesterberg', 'Pasin Manurangsi', 'Anak Yodpinyanee']","['cs.CC', 'cs.CG']",False,False,True,False,False,True
290,2017-03-28T14:07:55Z,2016-12-30T09:33:07Z,http://arxiv.org/abs/1612.09434v1,http://arxiv.org/pdf/1612.09434v1,Data driven estimation of Laplace-Beltrami operator,data driven estim laplac beltrami oper,"Approximations of Laplace-Beltrami operators on manifolds through graph Lapla-cians have become popular tools in data analysis and machine learning. These discretized operators usually depend on bandwidth parameters whose tuning remains a theoretical and practical problem. In this paper, we address this problem for the unnormalized graph Laplacian by establishing an oracle inequality that opens the door to a well-founded data-driven procedure for the bandwidth selection. Our approach relies on recent results by Lacour and Massart [LM15] on the so-called Lepski's method.",approxim laplac beltrami oper manifold graph lapla cian becom popular tool data analysi machin learn discret oper usual depend bandwidth paramet whose tune remain theoret practic problem paper address problem unnorm graph laplacian establish oracl inequ open door well found data driven procedur bandwidth select approach reli recent result lacour massart lm call lepski method,"['Frédéric Chazal', 'Ilaria Giulini', 'Bertrand Michel']","['cs.CG', 'cs.LG', 'math.ST', 'stat.TH']",False,False,True,False,False,True
299,2017-03-28T14:07:55Z,2016-12-14T19:33:00Z,http://arxiv.org/abs/1612.04780v1,http://arxiv.org/pdf/1612.04780v1,Minimum Weight Connectivity Augmentation for Planar Straight-Line Graphs,minimum weight connect augment planar straight line graph,"We consider edge insertion and deletion operations that increase the connectivity of a given planar straight-line graph (PSLG), while minimizing the total edge length of the output. We show that every connected PSLG $G=(V,E)$ in general position can be augmented to a 2-connected PSLG $(V,E\cup E^+)$ by adding new edges of total Euclidean length $\ E^+\ \leq 2\ E\ $, and this bound is the best possible. An optimal edge set $E^+$ can be computed in $O( V ^4)$ time; however the problem becomes NP-hard when $G$ is disconnected. Further, there is a sequence of edge insertions and deletions that transforms a connected PSLG $G=(V,E)$ into a planar straight-line cycle $G'=(V,E')$ such that $\ E'\ \leq 2\ {\rm MST}(V)\ $, and the graph remains connected with edge length below $\ E\ +\ {\rm MST}(V)\ $ at all stages. These bounds are the best possible.",consid edg insert delet oper increas connect given planar straight line graph pslg minim total edg length output show everi connect pslg general posit augment connect pslg cup ad new edg total euclidean length leq bound best possibl optim edg set comput time howev problem becom np hard disconnect sequenc edg insert delet transform connect pslg planar straight line cycl leq rm mst graph remain connect edg length rm mst stage bound best possibl,"['Hugo A. Akitaya', 'Rajasekhar Inkulu', 'Torrie L. Nichols', 'Diane L. Souvaine', 'Csaba D. Tóth', 'Charles R. Winston']","['cs.CG', '05C40, 05C85, 68R10', 'I.3.5']",False,False,True,False,False,True
314,2017-03-28T14:06:02Z,2017-03-24T14:40:31Z,http://arxiv.org/abs/1703.08428v1,http://arxiv.org/abs/1703.08428v1,Calendar.help: Designing a Workflow-Based Scheduling Agent with Humans   in the Loop,calendar help design workflow base schedul agent human loop,"Although information workers may complain about meetings, they are an essential part of their work life. Consequently, busy people spend a significant amount of time scheduling meetings. We present Calendar.help, a system that provides fast, efficient scheduling through structured workflows. Users interact with the system via email, delegating their scheduling needs to the system as if it were a human personal assistant. Common scheduling scenarios are broken down using well-defined workflows and completed as a series of microtasks that are automated when possible and executed by a human otherwise. Unusual scenarios fall back to a trained human assistant who executes them as unstructured macrotasks. We describe the iterative approach we used to develop Calendar.help, and share the lessons learned from scheduling thousands of meetings during a year of real-world deployments. Our findings provide insight into how complex information tasks can be broken down into repeatable components that can be executed efficiently to improve productivity.",although inform worker may complain meet essenti part work life consequ busi peopl spend signific amount time schedul meet present calendar help system provid fast effici schedul structur workflow user interact system via email deleg schedul need system human person assist common schedul scenario broken use well defin workflow complet seri microtask autom possibl execut human otherwis unusu scenario fall back train human assist execut unstructur macrotask describ iter approach use develop calendar help share lesson learn schedul thousand meet dure year real world deploy find provid insight complex inform task broken repeat compon execut effici improv product,"['Justin Cranshaw', 'Emad Elwany', 'Todd Newman', 'Rafal Kocielnik', 'Bowen Yu', 'Sandeep Soni', 'Jaime Teevan', 'Andrés Monroy-Hernández']","['cs.HC', 'cs.AI', 'cs.CL']",False,False,True,False,False,False
322,2017-03-28T14:06:07Z,2017-03-25T00:00:00Z,http://arxiv.org/abs/1703.08088v2,http://arxiv.org/abs/1703.08088v2,Rapid-Rate: A Framework for Semi-supervised Real-time Sentiment Trend   Detection in Unstructured Big Data,rapid rate framework semi supervis real time sentiment trend detect unstructur big data,"Commercial establishments like restaurants, service centres and retailers have several sources of customer feedback about products and services, most of which need not be as structured as rated reviews provided by services like Yelp, or Amazon, in terms of sentiment conveyed. For instance, Amazon provides a fine-grained score on a numeric scale for product reviews. Some sources, however, like social media (Twitter, Facebook), mailing lists (Google Groups) and forums (Quora) contain text data that is much more voluminous, but unstructured and unlabelled. It might be in the best interests of a business establishment to assess the general sentiment towards their brand on these platforms as well. This text could be pipelined into a system with a built-in prediction model, with the objective of generating real-time graphs on opinion and sentiment trends. Although such tasks like the one described about have been explored with respect to document classification problems in the past, the implementation described in this paper, by virtue of learning a continuous function rather than a discrete one, offers a lot more depth of insight as compared to document classification approaches. This study aims to explore the validity of such a continuous function predicting model to quantify sentiment about an entity, without the additional overhead of manual labelling, and computational preprocessing & feature extraction. This research project also aims to design and implement a re-usable document regression pipeline as a framework, Rapid-Rate, that can be used to predict document scores in real-time.",commerci establish like restaur servic centr retail sever sourc custom feedback product servic need structur rate review provid servic like yelp amazon term sentiment convey instanc amazon provid fine grain score numer scale product review sourc howev like social media twitter facebook mail list googl group forum quora contain text data much volumin unstructur unlabel might best interest busi establish assess general sentiment toward brand platform well text could pipelin system built predict model object generat real time graph opinion sentiment trend although task like one describ explor respect document classif problem past implement describ paper virtu learn continu function rather discret one offer lot depth insight compar document classif approach studi aim explor valid continu function predict model quantifi sentiment entiti without addit overhead manual label comput preprocess featur extract research project also aim design implement usabl document regress pipelin framework rapid rate use predict document score real time,['Vineet John'],"['cs.CL', '68T50']",False,False,True,False,False,False
332,2017-03-28T14:06:12Z,2017-03-21T21:36:28Z,http://arxiv.org/abs/1703.07438v1,http://arxiv.org/pdf/1703.07438v1,The NLTK FrameNet API: Designing for Discoverability with a Rich   Linguistic Resource,nltk framenet api design discover rich linguist resourc,"A new Python API, integrated within the NLTK suite, offers access to the FrameNet 1.7 lexical database. The lexicon (structured in terms of frames) as well as annotated sentences can be processed programatically, or browsed with human-readable displays via the interactive Python prompt.",new python api integr within nltk suit offer access framenet lexic databas lexicon structur term frame well annot sentenc process programat brows human readabl display via interact python prompt,"['Nathan Schneider', 'Chuck Wooters']",['cs.CL'],False,False,True,False,False,True
340,2017-03-28T14:06:16Z,2017-03-19T19:56:25Z,http://arxiv.org/abs/1703.06501v1,http://arxiv.org/pdf/1703.06501v1,Métodos de Otimização Combinatória Aplicados ao Problema de   Compressão MultiFrases,todo de otimiza combinat ria aplicado ao problema de compress multifras,"The Internet has led to a dramatic increase in the amount of available information. In this context, reading and understanding this flow of information have become costly tasks. In the last years, to assist people to understand textual data, various Natural Language Processing (NLP) applications based on Combinatorial Optimization have been devised. However, for Multi-Sentences Compression (MSC), method which reduces the sentence length without removing core information, the insertion of optimization methods requires further study to improve the performance of MSC. This article describes a method for MSC using Combinatorial Optimization and Graph Theory to generate more informative sentences while maintaining their grammaticality. An experiment led on a corpus of 40 clusters of sentences shows that our system has achieved a very good quality and is better than the state-of-the-art.",internet led dramat increas amount avail inform context read understand flow inform becom cost task last year assist peopl understand textual data various natur languag process nlp applic base combinatori optim devis howev multi sentenc compress msc method reduc sentenc length without remov core inform insert optim method requir studi improv perform msc articl describ method msc use combinatori optim graph theori generat inform sentenc maintain grammat experi led corpus cluster sentenc show system achiev veri good qualiti better state art,"['Elvys Linhares Pontes', 'Thiago Gouveia da Silva', 'Andréa Carneiro Linhares', 'Juan-Manuel Torres-Moreno', 'Stéphane Huet']",['cs.CL'],False,False,True,False,False,False
357,2017-03-28T14:06:20Z,2017-03-15T04:00:27Z,http://arxiv.org/abs/1703.04914v1,http://arxiv.org/pdf/1703.04914v1,Ensemble of Neural Classifiers for Scoring Knowledge Base Triples,ensembl neural classifi score knowledg base tripl,"This paper describes our approach for the triple scoring task at WSDM Cup 2017. The task aims to assign a relevance score for each pair of entities and their types in a knowledge base in order to enhance the ranking results in entity retrieval tasks. We propose an approach wherein the outputs of multiple neural network classifiers are combined using a supervised machine learning model. The experimental results show that our proposed method achieves the best performance in one out of three measures, and performs competitively in the other two measures.",paper describ approach tripl score task wsdm cup task aim assign relev score pair entiti type knowledg base order enhanc rank result entiti retriev task propos approach wherein output multipl neural network classifi combin use supervis machin learn model experiment result show propos method achiev best perform one three measur perform competit two measur,"['Ikuya Yamada', 'Motoki Sato', 'Hiroyuki Shindo']","['cs.CL', 'cs.IR']",False,False,True,False,False,False
361,2017-03-28T14:06:24Z,2017-03-15T00:47:28Z,http://arxiv.org/abs/1703.04854v1,http://arxiv.org/pdf/1703.04854v1,Distributed-Representation Based Hybrid Recommender System with Short   Item Descriptions,distribut represent base hybrid recommend system short item descript,"Collaborative filtering (CF) aims to build a model from users' past behaviors and/or similar decisions made by other users, and use the model to recommend items for users. Despite of the success of previous collaborative filtering approaches, they are all based on the assumption that there are sufficient rating scores available for building high-quality recommendation models. In real world applications, however, it is often difficult to collect sufficient rating scores, especially when new items are introduced into the system, which makes the recommendation task challenging. We find that there are often ""short"" texts describing features of items, based on which we can approximate the similarity of items and make recommendation together with rating scores. In this paper we ""borrow"" the idea of vector representation of words to capture the information of short texts and embed it into a matrix factorization framework. We empirically show that our approach is effective by comparing it with state-of-the-art approaches.",collabor filter cf aim build model user past behavior similar decis made user use model recommend item user despit success previous collabor filter approach base assumpt suffici rate score avail build high qualiti recommend model real world applic howev often difficult collect suffici rate score especi new item introduc system make recommend task challeng find often short text describ featur item base approxim similar item make recommend togeth rate score paper borrow idea vector represent word captur inform short text emb matrix factor framework empir show approach effect compar state art approach,"['Junhua He', 'Hankz Hankui Zhuo', 'Jarvan Law']","['cs.IR', 'cs.CL']",False,False,True,False,False,False
365,2017-03-28T14:06:24Z,2017-03-14T19:14:32Z,http://arxiv.org/abs/1703.04677v1,http://arxiv.org/pdf/1703.04677v1,A computational investigation of sources of variability in sentence   comprehension difficulty in aphasia,comput investig sourc variabl sentenc comprehens difficulti aphasia,"We present a computational evaluation of three hypotheses about sources of deficit in sentence comprehension in aphasia: slowed processing, intermittent deficiency, and resource reduction. The ACT-R based Lewis & Vasishth 2005 model is used to implement these three proposals. Slowed processing is implemented as slowed default production-rule firing time; intermittent deficiency as increased random noise in activation of chunks in memory; and resource reduction as reduced goal activation. As data, we considered subject vs. object relatives presented in a self-paced listening modality to 56 individuals with aphasia (IWA) and 46 matched controls. The participants heard the sentences and carried out a picture verification task to decide on an interpretation of the sentence. These response accuracies are used to identify the best parameters (for each participant) that correspond to the three hypotheses mentioned above. We show that controls have more tightly clustered (less variable) parameter values than IWA; specifically, compared to controls, among IWA there are more individuals with low goal activations, high noise, and slow default action times. This suggests that (i) individual patients show differential amounts of deficit along the three dimensions of slowed processing, intermittent deficient, and resource reduction, (ii) overall, there is evidence for all three sources of deficit playing a role, and (iii) IWA have a more variable range of parameter values than controls. In sum, this study contributes a proof of concept of a quantitative implementation of, and evidence for, these three accounts of comprehension deficits in aphasia.",present comput evalu three hypothes sourc deficit sentenc comprehens aphasia slow process intermitt defici resourc reduct act base lewi vasishth model use implement three propos slow process implement slow default product rule fire time intermitt defici increas random nois activ chunk memori resourc reduct reduc goal activ data consid subject vs object relat present self pace listen modal individu aphasia iwa match control particip heard sentenc carri pictur verif task decid interpret sentenc respons accuraci use identifi best paramet particip correspond three hypothes mention abov show control tight cluster less variabl paramet valu iwa specif compar control among iwa individu low goal activ high nois slow default action time suggest individu patient show differenti amount deficit along three dimens slow process intermitt defici resourc reduct ii overal evid three sourc deficit play role iii iwa variabl rang paramet valu control sum studi contribut proof concept quantit implement evid three account comprehens deficit aphasia,"['Paul Mätzig', 'Shravan Vasishth', 'Felix Engelmann', 'David Caplan']","['cs.CL', 'cs.AI']",False,False,True,False,False,True
376,2017-03-28T14:06:28Z,2017-03-13T04:55:19Z,http://arxiv.org/abs/1703.04247v1,http://arxiv.org/pdf/1703.04247v1,DeepFM: A Factorization-Machine based Neural Network for CTR Prediction,deepfm factor machin base neural network ctr predict,"Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide \& Deep model from Google, DeepFM has a shared input to its ""wide"" and ""deep"" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.",learn sophist featur interact behind user behavior critic maxim ctr recommend system despit great progress exist method seem strong bias toward low high order interact requir expertis featur engin paper show possibl deriv end end learn model emphas low high order featur interact propos model deepfm combin power factor machin recommend deep learn featur learn new neural network architectur compar latest wide deep model googl deepfm share input wide deep part need featur engin besid raw featur comprehens experi conduct demonstr effect effici deepfm exist model ctr predict benchmark data commerci data,"['Huifeng Guo', 'Ruiming Tang', 'Yunming Ye', 'Zhenguo Li', 'Xiuqiang He']","['cs.IR', 'cs.CL']",False,False,True,False,False,False
381,2017-03-28T14:06:31Z,2017-03-11T17:14:55Z,http://arxiv.org/abs/1703.04001v1,http://arxiv.org/pdf/1703.04001v1,Language Use Matters: Analysis of the Linguistic Structure of Question   Texts Can Characterize Answerability in Quora,languag use matter analysi linguist structur question text character answer quora,"Quora is one of the most popular community Q&A sites of recent times. However, many question posts on this Q&A site often do not get answered. In this paper, we quantify various linguistic activities that discriminates an answered question from an unanswered one. Our central finding is that the way users use language while writing the question text can be a very effective means to characterize answerability. This characterization helps us to predict early if a question remaining unanswered for a specific time period t will eventually be answered or not and achieve an accuracy of 76.26% (t = 1 month) and 68.33% (t = 3 months). Notably, features representing the language use patterns of the users are most discriminative and alone account for an accuracy of 74.18%. We also compare our method with some of the similar works (Dror et al., Yang et al.) achieving a maximum improvement of ~39% in terms of accuracy.",quora one popular communiti site recent time howev mani question post site often get answer paper quantifi various linguist activ discrimin answer question unansw one central find way user use languag write question text veri effect mean character answer character help us predict earli question remain unansw specif time period eventu answer achiev accuraci month month notabl featur repres languag use pattern user discrimin alon account accuraci also compar method similar work dror et al yang et al achiev maximum improv term accuraci,"['Suman Kalyan Maity', 'Aman Kharb', 'Animesh Mukherjee']","['cs.CL', 'cs.SI']",False,False,True,False,False,False
389,2017-03-28T14:06:32Z,2017-03-10T12:59:52Z,http://arxiv.org/abs/1703.03666v1,http://arxiv.org/pdf/1703.03666v1,Comparison of SMT and RBMT; The Requirement of Hybridization for   Marathi-Hindi MT,comparison smt rbmt requir hybrid marathi hindi mt,"We present in this paper our work on comparison between Statistical Machine Translation (SMT) and Rule-based machine translation for translation from Marathi to Hindi. Rule Based systems although robust take lots of time to build. On the other hand statistical machine translation systems are easier to create, maintain and improve upon. We describe the development of a basic Marathi-Hindi SMT system and evaluate its performance. Through a detailed error analysis, we, point out the relative strengths and weaknesses of both systems. Effectively, we shall see that even with a small amount of training corpus a statistical machine translation system has many advantages for high quality domain specific machine translation over that of a rule-based counterpart.",present paper work comparison statist machin translat smt rule base machin translat translat marathi hindi rule base system although robust take lot time build hand statist machin translat system easier creat maintain improv upon describ develop basic marathi hindi smt system evalu perform detail error analysi point relat strength weak system effect shall see even small amount train corpus statist machin translat system mani advantag high qualiti domain specif machin translat rule base counterpart,"['Sreelekha. S', 'Pushpak Bhattacharyya']",['cs.CL'],False,False,True,False,False,False
391,2017-03-28T14:06:36Z,2017-03-10T10:17:27Z,http://arxiv.org/abs/1703.03609v1,http://arxiv.org/abs/1703.03609v1,NetSpam: a Network-based Spam Detection Framework for Reviews in Online   Social Media,netspam network base spam detect framework review onlin social media,"Nowadays, a big part of people rely on available content in social media in their decisions (e.g. reviews and feedback on a topic or product). The possibility that anybody can leave a review provide a golden opportunity for spammers to write spam reviews about products and services for different interests. Identifying these spammers and the spam content is a hot topic of research and although a considerable number of studies have been done recently toward this end, but so far the methodologies put forth still barely detect spam reviews, and none of them show the importance of each extracted feature type. In this study, we propose a novel framework, named NetSpam, which utilizes spam features for modeling review datasets as heterogeneous information networks to map spam detection procedure into a classification problem in such networks. Using the importance of spam features help us to obtain better results in terms of different metrics experimented on real-world review datasets from Yelp and Amazon websites. The results show that NetSpam outperforms the existing methods and among four categories of features; including review-behavioral, user-behavioral, reviewlinguistic, user-linguistic, the first type of features performs better than the other categories.",nowaday big part peopl reli avail content social media decis review feedback topic product possibl anybodi leav review provid golden opportun spammer write spam review product servic differ interest identifi spammer spam content hot topic research although consider number studi done recent toward end far methodolog put forth still bare detect spam review none show import extract featur type studi propos novel framework name netspam util spam featur model review dataset heterogen inform network map spam detect procedur classif problem network use import spam featur help us obtain better result term differ metric experi real world review dataset yelp amazon websit result show netspam outperform exist method among four categori featur includ review behavior user behavior reviewlinguist user linguist first type featur perform better categori,"['Saeedreza Shehnepoor', 'Mostafa Salehi', 'Reza Farahbakhsh', 'Noel Crespi']","['cs.SI', 'cs.CL', 'cs.IR', 'physics.soc-ph']",False,False,True,False,False,False
394,2017-03-28T14:06:36Z,2017-03-09T18:37:50Z,http://arxiv.org/abs/1703.03386v1,http://arxiv.org/pdf/1703.03386v1,Loyalty in Online Communities,loyalti onlin communiti,"Loyalty is an essential component of multi-community engagement. When users have the choice to engage with a variety of different communities, they often become loyal to just one, focusing on that community at the expense of others. However, it is unclear how loyalty is manifested in user behavior, or whether loyalty is encouraged by certain community characteristics.   In this paper we operationalize loyalty as a user-community relation: users loyal to a community consistently prefer it over all others; loyal communities retain their loyal users over time. By exploring this relation using a large dataset of discussion communities from Reddit, we reveal that loyalty is manifested in remarkably consistent behaviors across a wide spectrum of communities. Loyal users employ language that signals collective identity and engage with more esoteric, less popular content, indicating they may play a curational role in surfacing new material. Loyal communities have denser user-user interaction networks and lower rates of triadic closure, suggesting that community-level loyalty is associated with more cohesive interactions and less fragmentation into subgroups. We exploit these general patterns to predict future rates of loyalty. Our results show that a user's propensity to become loyal is apparent from their first interactions with a community, suggesting that some users are intrinsically loyal from the very beginning.",loyalti essenti compon multi communiti engag user choic engag varieti differ communiti often becom loyal one focus communiti expens howev unclear loyalti manifest user behavior whether loyalti encourag certain communiti characterist paper operation loyalti user communiti relat user loyal communiti consist prefer loyal communiti retain loyal user time explor relat use larg dataset discuss communiti reddit reveal loyalti manifest remark consist behavior across wide spectrum communiti loyal user employ languag signal collect ident engag esoter less popular content indic may play curat role surfac new materi loyal communiti denser user user interact network lower rate triadic closur suggest communiti level loyalti associ cohes interact less fragment subgroup exploit general pattern predict futur rate loyalti result show user propens becom loyal appar first interact communiti suggest user intrins loyal veri begin,"['William L. Hamilton', 'Justine Zhang', 'Cristian Danescu-Niculescu-Mizil', 'Dan Jurafsky', 'Jure Leskovec']","['cs.SI', 'cs.CL']",False,False,True,False,False,False
399,2017-03-28T14:06:36Z,2017-03-09T01:04:07Z,http://arxiv.org/abs/1703.03091v1,http://arxiv.org/pdf/1703.03091v1,Deep Learning applied to NLP,deep learn appli nlp,"Convolutional Neural Network (CNNs) are typically associated with Computer Vision. CNNs are responsible for major breakthroughs in Image Classification and are the core of most Computer Vision systems today. More recently CNNs have been applied to problems in Natural Language Processing and gotten some interesting results. In this paper, we will try to explain the basics of CNNs, its different variations and how they have been applied to NLP.",convolut neural network cnns typic associ comput vision cnns respons major breakthrough imag classif core comput vision system today recent cnns appli problem natur languag process gotten interest result paper tri explain basic cnns differ variat appli nlp,"['Marc Moreno Lopez', 'Jugal Kalita']",['cs.CL'],False,False,True,False,False,False
400,2017-03-28T14:08:43Z,2017-03-27T17:52:55Z,http://arxiv.org/abs/1703.09211v1,http://arxiv.org/pdf/1703.09211v1,Coherent Online Video Style Transfer,coher onlin video style transfer,"Training a feed-forward network for fast neural style transfer of images is proven to be successful. However, the naive extension to process video frame by frame is prone to producing flickering results. We propose the first end-to-end network for online video style transfer, which generates temporally coherent stylized video sequences in near real-time. Two key ideas include an efficient network by incorporating short-term coherence, and propagating short-term coherence to long-term, which ensures the consistency over larger period of time. Our network can incorporate different image stylization networks. We show that the proposed method clearly outperforms the per-frame baseline both qualitatively and quantitatively. Moreover, it can achieve visually comparable coherence to optimization-based video style transfer, but is three orders of magnitudes faster in runtime.",train feed forward network fast neural style transfer imag proven success howev naiv extens process video frame frame prone produc flicker result propos first end end network onlin video style transfer generat tempor coher styliz video sequenc near real time two key idea includ effici network incorpor short term coher propag short term coher long term ensur consist larger period time network incorpor differ imag stylize network show propos method clear outperform per frame baselin qualit quantit moreov achiev visual compar coher optim base video style transfer three order magnitud faster runtim,"['Dongdong Chen', 'Jing Liao', 'Yuan Lu', 'Nenghai Yu', 'Gang Hua']",['cs.CV'],False,False,True,False,False,False
415,2017-03-28T14:08:48Z,2017-03-27T07:49:43Z,http://arxiv.org/abs/1703.08961v1,http://arxiv.org/pdf/1703.08961v1,Scaling the Scattering Transform: Deep Hybrid Networks,scale scatter transform deep hybrid network,"We use the scattering network as a generic and fixed initialization of the first layers of a supervised hybrid deep network. We show that early layers do not necessarily need to be learned, providing the best results to-date with pre-defined representations while being competitive with Deep CNNs. Using a shallow cascade of 1x1 convolutions, which encodes scattering coefficients that correspond to spatial windows of very small sizes, permits to obtain AlexNet accuracy on the imagenet ILSVRC2012. We demonstrate that this local encoding explicitly learns in-variance w.r.t. rotations. Combining scattering networks with a modern ResNet, we achieve a single-crop top 5 error of 11.4% on imagenet ILSVRC2012, comparable to the Resnet-18 architecture, while utilizing only 10 layers. We also find that hybrid architectures can yield excellent performance in the small sample regime, exceeding their end-to-end counterparts, through their ability to incorporate geometrical priors. We demonstrate this on subsets of the CIFAR-10 dataset and by setting a new state-of-the-art on the STL-10 dataset.",use scatter network generic fix initi first layer supervis hybrid deep network show earli layer necessarili need learn provid best result date pre defin represent competit deep cnns use shallow cascad convolut encod scatter coeffici correspond spatial window veri small size permit obtain alexnet accuraci imagenet ilsvrc demonstr local encod explicit learn varianc rotat combin scatter network modern resnet achiev singl crop top error imagenet ilsvrc compar resnet architectur util onli layer also find hybrid architectur yield excel perform small sampl regim exceed end end counterpart abil incorpor geometr prior demonstr subset cifar dataset set new state art stl dataset,"['Edouard Oyallon', 'Eugene Belilovsky', 'Sergey Zagoruyko']","['cs.CV', 'cs.LG']",False,False,True,False,False,False
425,2017-03-28T14:08:52Z,2017-03-26T06:34:45Z,http://arxiv.org/abs/1703.08774v1,http://arxiv.org/pdf/1703.08774v1,Who Said What: Modeling Individual Labelers Improves Classification,said model individu label improv classif,"Data are often labeled by many different experts with each expert only labeling a small fraction of the data and each data point being labeled by several experts. This reduces the workload on individual experts and also gives a better estimate of the unobserved ground truth. When experts disagree, the standard approaches are to treat the majority opinion as the correct label or to model the correct label as a distribution. These approaches, however, do not make any use of potentially valuable information about which expert produced which label. To make use of this extra information, we propose modeling the experts individually and then learning averaging weights for combining them, possibly in sample-specific ways. This allows us to give more weight to more reliable experts and take advantage of the unique strengths of individual experts at classifying certain types of data. Here we show that our approach leads to improvements in computer-aided diagnosis of diabetic retinopathy. We also show that our method performs better than competing algorithms by Welinder and Perona, and by Mnih and Hinton. Our work offers an innovative approach for dealing with the myriad real-world settings that use expert opinions to define labels for training.",data often label mani differ expert expert onli label small fraction data data point label sever expert reduc workload individu expert also give better estim unobserv ground truth expert disagre standard approach treat major opinion correct label model correct label distribut approach howev make ani use potenti valuabl inform expert produc label make use extra inform propos model expert individu learn averag weight combin possibl sampl specif way allow us give weight reliabl expert take advantag uniqu strength individu expert classifi certain type data show approach lead improv comput aid diagnosi diabet retinopathi also show method perform better compet algorithm welind perona mnih hinton work offer innov approach deal myriad real world set use expert opinion defin label train,"['Melody Y. Guan', 'Varun Gulshan', 'Andrew M. Dai', 'Geoffrey E. Hinton']","['cs.LG', 'cs.CV']",False,False,True,False,False,True
426,2017-03-28T14:08:52Z,2017-03-26T05:53:39Z,http://arxiv.org/abs/1703.08772v1,http://arxiv.org/pdf/1703.08772v1,Multivariate Regression with Gross Errors on Manifold-valued Data,multivari regress gross error manifold valu data,"We consider the topic of multivariate regression on manifold-valued output, that is, for a multivariate observation, its output response lies on a manifold. Moreover, we propose a new regression model to deal with the presence of grossly corrupted manifold-valued responses, a bottleneck issue commonly encountered in practical scenarios. Our model first takes a correction step on the grossly corrupted responses via geodesic curves on the manifold, and then performs multivariate linear regression on the corrected data. This results in a nonconvex and nonsmooth optimization problem on manifolds. To this end, we propose a dedicated approach named PALMR, by utilizing and extending the proximal alternating linearized minimization techniques. Theoretically, we investigate its convergence property, where it is shown to converge to a critical point under mild conditions. Empirically, we test our model on both synthetic and real diffusion tensor imaging data, and show that our model outperforms other multivariate regression models when manifold-valued responses contain gross errors, and is effective in identifying gross errors.",consid topic multivari regress manifold valu output multivari observ output respons lie manifold moreov propos new regress model deal presenc grossli corrupt manifold valu respons bottleneck issu common encount practic scenario model first take correct step grossli corrupt respons via geodes curv manifold perform multivari linear regress correct data result nonconvex nonsmooth optim problem manifold end propos dedic approach name palmr util extend proxim altern linear minim techniqu theoret investig converg properti shown converg critic point mild condit empir test model synthet real diffus tensor imag data show model outperform multivari regress model manifold valu respons contain gross error effect identifi gross error,"['Xiaowei Zhang', 'Xudong Shi', 'Yu Sun', 'Li Cheng']","['stat.ML', 'cs.CV', 'math.OC']",False,False,True,False,False,False
448,2017-03-28T14:09:00Z,2017-03-24T11:17:00Z,http://arxiv.org/abs/1703.08359v1,http://arxiv.org/pdf/1703.08359v1,Scalable Person Re-identification on Supervised Smoothed Manifold,scalabl person identif supervis smooth manifold,"Most existing person re-identification algorithms either extract robust visual features or learn discriminative metrics for person images. However, the underlying manifold which those images reside on is rarely investigated. That raises a problem that the learned metric is not smooth with respect to the local geometry structure of the data manifold.   In this paper, we study person re-identification with manifold-based affinity learning, which did not receive enough attention from this area. An unconventional manifold-preserving algorithm is proposed, which can 1) make the best use of supervision from training data, whose label information is given as pairwise constraints; 2) scale up to large repositories with low on-line time complexity; and 3) be plunged into most existing algorithms, serving as a generic postprocessing procedure to further boost the identification accuracies. Extensive experimental results on five popular person re-identification benchmarks consistently demonstrate the effectiveness of our method. Especially, on the largest CUHK03 and Market-1501, our method outperforms the state-of-the-art alternatives by a large margin with high efficiency, which is more appropriate for practical applications.",exist person identif algorithm either extract robust visual featur learn discrimin metric person imag howev manifold imag resid rare investig rais problem learn metric smooth respect local geometri structur data manifold paper studi person identif manifold base affin learn receiv enough attent area unconvent manifold preserv algorithm propos make best use supervis train data whose label inform given pairwis constraint scale larg repositori low line time complex plung exist algorithm serv generic postprocess procedur boost identif accuraci extens experiment result five popular person identif benchmark consist demonstr effect method especi largest cuhk market method outperform state art altern larg margin high effici appropri practic applic,"['Song Bai', 'Xiang Bai', 'Qi Tian']",['cs.CV'],False,False,True,False,False,False
453,2017-03-28T14:09:05Z,2017-03-23T21:25:48Z,http://arxiv.org/abs/1703.08238v1,http://arxiv.org/pdf/1703.08238v1,Semi-Automatic Segmentation and Ultrasonic Characterization of Solid   Breast Lesions,semi automat segment ultrason character solid breast lesion,"Characterization of breast lesions is an essential prerequisite to detect breast cancer in an early stage. Automatic segmentation makes this categorization method robust by freeing it from subjectivity and human error. Both spectral and morphometric features are successfully used for differentiating between benign and malignant breast lesions. In this thesis, we used empirical mode decomposition method for semi-automatic segmentation. Sonographic features like ehcogenicity, heterogeneity, FNPA, margin definition, Hurst coefficient, compactness, roundness, aspect ratio, convexity, solidity, form factor were calculated to be used as our characterization parameters. All of these parameters did not give desired comparative results. But some of them namely echogenicity, heterogeneity, margin definition, aspect ratio and convexity gave good results and were used for characterization.",character breast lesion essenti prerequisit detect breast cancer earli stage automat segment make categor method robust free subject human error spectral morphometr featur success use differenti benign malign breast lesion thesi use empir mode decomposit method semi automat segment sonograph featur like ehcogen heterogen fnpa margin definit hurst coeffici compact round aspect ratio convex solid form factor calcul use character paramet paramet give desir compar result name echogen heterogen margin definit aspect ratio convex gave good result use character,"['Mohammad Saad Billah', 'Tahmida Binte Mahmud']",['cs.CV'],False,False,True,False,False,False
457,2017-03-28T14:09:05Z,2017-03-23T15:56:33Z,http://arxiv.org/abs/1703.08119v1,http://arxiv.org/pdf/1703.08119v1,Quality Resilient Deep Neural Networks,qualiti resili deep neural network,"We study deep neural networks for classification of images with quality distortions. We first show that networks fine-tuned on distorted data greatly outperform the original networks when tested on distorted data. However, fine-tuned networks perform poorly on quality distortions that they have not been trained for. We propose a mixture of experts ensemble method that is robust to different types of distortions. The ""experts"" in our model are trained on a particular type of distortion. The output of the model is a weighted sum of the expert models, where the weights are determined by a separate gating network. The gating network is trained to predict optimal weights for a particular distortion type and level. During testing, the network is blind to the distortion level and type, yet can still assign appropriate weights to the expert models. We additionally investigate weight sharing methods for the mixture model and show that improved performance can be achieved with a large reduction in the number of unique network parameters.",studi deep neural network classif imag qualiti distort first show network fine tune distort data great outperform origin network test distort data howev fine tune network perform poor qualiti distort train propos mixtur expert ensembl method robust differ type distort expert model train particular type distort output model weight sum expert model weight determin separ gate network gate network train predict optim weight particular distort type level dure test network blind distort level type yet still assign appropri weight expert model addit investig weight share method mixtur model show improv perform achiev larg reduct number uniqu network paramet,"['Samuel Dodge', 'Lina Karam']",['cs.CV'],False,False,True,False,False,False
479,2017-03-28T14:09:13Z,2017-03-27T13:39:01Z,http://arxiv.org/abs/1703.07737v2,http://arxiv.org/pdf/1703.07737v2,In Defense of the Triplet Loss for Person Re-Identification,defens triplet loss person identif,"In the past few years, the field of computer vision has gone through a revolution fueled mainly by the advent of large datasets and the adoption of deep convolutional neural networks for end-to-end learning. The person re-identification subfield is no exception to this, thanks to the notable publication of the Market-1501 and MARS datasets and several strong deep learning approaches. Unfortunately, a prevailing belief in the community seems to be that the triplet loss is inferior to using surrogate losses (classification, verification) followed by a separate metric learning step. We show that, for models trained from scratch as well as pretrained ones, using a variant of the triplet loss to perform end-to-end deep metric learning outperforms any other published method by a large margin.",past year field comput vision gone revolut fuel main advent larg dataset adopt deep convolut neural network end end learn person identif subfield except thank notabl public market mar dataset sever strong deep learn approach unfortun prevail belief communiti seem triplet loss inferior use surrog loss classif verif follow separ metric learn step show model train scratch well pretrain one use variant triplet loss perform end end deep metric learn outperform ani publish method larg margin,"['Alexander Hermans', 'Lucas Beyer', 'Bastian Leibe']","['cs.CV', 'cs.NE']",False,False,True,False,False,False
490,2017-03-28T14:09:21Z,2017-03-22T04:21:41Z,http://arxiv.org/abs/1703.07511v1,http://arxiv.org/pdf/1703.07511v1,Deep Photo Style Transfer,deep photo style transfer,"This paper introduces a deep-learning approach to photographic style transfer that handles a large variety of image content while faithfully transferring the reference style. Our approach builds upon recent work on painterly transfer that separates style from the content of an image by considering different layers of a neural network. However, as is, this approach is not suitable for photorealistic style transfer. Even when both the input and reference images are photographs, the output still exhibits distortions reminiscent of a painting. Our contribution is to constrain the transformation from the input to the output to be locally affine in colorspace, and to express this constraint as a custom CNN layer through which we can backpropagate. We show that this approach successfully suppresses distortion and yields satisfying photorealistic style transfers in a broad variety of scenarios, including transfer of the time of day, weather, season, and artistic edits.",paper introduc deep learn approach photograph style transfer handl larg varieti imag content faith transfer refer style approach build upon recent work painter transfer separ style content imag consid differ layer neural network howev approach suitabl photorealist style transfer even input refer imag photograph output still exhibit distort reminisc paint contribut constrain transform input output local affin colorspac express constraint custom cnn layer backpropag show approach success suppress distort yield satisfi photorealist style transfer broad varieti scenario includ transfer time day weather season artist edit,"['Fujun Luan', 'Sylvain Paris', 'Eli Shechtman', 'Kavita Bala']",['cs.CV'],False,False,True,False,False,False
495,2017-03-28T14:09:21Z,2017-03-24T21:17:05Z,http://arxiv.org/abs/1703.07464v2,http://arxiv.org/pdf/1703.07464v2,No Fuss Distance Metric Learning using Proxies,fuss distanc metric learn use proxi,"We address the problem of distance metric learning (DML), defined as learning a distance consistent with a notion of semantic similarity. Traditionally, for this problem supervision is expressed in the form of sets of points that follow an ordinal relationship -- an anchor point $x$ is similar to a set of positive points $Y$, and dissimilar to a set of negative points $Z$, and a loss defined over these distances is minimized.   While the specifics of the optimization differ, in this work we collectively call this type of supervision Triplets and all methods that follow this pattern Triplet-Based methods. These methods are challenging to optimize. A main issue is the need for finding informative triplets, which is usually achieved by a variety of tricks such as increasing the batch size, hard or semi-hard triplet mining, etc, but even with these tricks, the convergence rate of such methods is slow. In this paper we propose to optimize the triplet loss on a different space of triplets, consisting of an anchor data point and similar and dissimilar proxy points. These proxies approximate the original data points, so that a triplet loss over the proxies is a tight upper bound of the original loss. This proxy-based loss is empirically better behaved. As a result, the proxy-loss improves on state-of-art results for three standard zero-shot learning datasets, by up to 15% points, while converging three times as fast as other triplet-based losses.",address problem distanc metric learn dml defin learn distanc consist notion semant similar tradit problem supervis express form set point follow ordin relationship anchor point similar set posit point dissimilar set negat point loss defin distanc minim specif optim differ work collect call type supervis triplet method follow pattern triplet base method method challeng optim main issu need find inform triplet usual achiev varieti trick increas batch size hard semi hard triplet mine etc even trick converg rate method slow paper propos optim triplet loss differ space triplet consist anchor data point similar dissimilar proxi point proxi approxim origin data point triplet loss proxi tight upper bound origin loss proxi base loss empir better behav result proxi loss improv state art result three standard zero shot learn dataset point converg three time fast triplet base loss,"['Yair Movshovitz-Attias', 'Alexander Toshev', 'Thomas K. Leung', 'Sergey Ioffe', 'Saurabh Singh']",['cs.CV'],False,False,True,False,False,False
497,2017-03-28T14:09:21Z,2017-03-21T19:40:25Z,http://arxiv.org/abs/1703.07402v1,http://arxiv.org/pdf/1703.07402v1,Simple Online and Realtime Tracking with a Deep Association Metric,simpl onlin realtim track deep associ metric,"Simple Online and Realtime Tracking (SORT) is a pragmatic approach to multiple object tracking with a focus on simple, effective algorithms. In this paper, we integrate appearance information to improve the performance of SORT. Due to this extension we are able to track objects through longer periods of occlusions, effectively reducing the number of identity switches. In spirit of the original framework we place much of the computational complexity into an offline pre-training stage where we learn a deep association metric on a large-scale person re-identification dataset. During online application, we establish measurement-to-track associations using nearest neighbor queries in visual appearance space. Experimental evaluation shows that our extensions reduce the number of identity switches by 45%, achieving overall competitive performance at high frame rates.",simpl onlin realtim track sort pragmat approach multipl object track focus simpl effect algorithm paper integr appear inform improv perform sort due extens abl track object longer period occlus effect reduc number ident switch spirit origin framework place much comput complex offlin pre train stage learn deep associ metric larg scale person identif dataset dure onlin applic establish measur track associ use nearest neighbor queri visual appear space experiment evalu show extens reduc number ident switch achiev overal competit perform high frame rate,"['Nicolai Wojke', 'Alex Bewley', 'Dietrich Paulus']",['cs.CV'],False,False,True,False,False,False
507,2017-03-28T14:09:26Z,2017-03-24T20:21:52Z,http://arxiv.org/abs/1703.08589v1,http://arxiv.org/pdf/1703.08589v1,Polynomial-Time Methods to Solve Unimodular Quadratic Programs With   Performance Guarantees,polynomi time method solv unimodular quadrat program perform guarante,"We develop polynomial-time heuristic methods to solve unimodular quadratic programs (UQPs) approximately, which are known to be NP-hard. In the UQP framework, we maximize a quadratic function of a vector of complex variables with unit modulus. Several problems in active sensing and wireless communication applications boil down to UQP. With this motivation, we present three new heuristic methods with polynomial-time complexity to solve the UQP approximately. The first method is called dominant-eigenvector-matching; here the solution is picked that matches the complex arguments of the dominant eigenvector of the Hermitian matrix in the UQP formulation. We also provide a performance guarantee for this method. The second method, a greedy strategy, is shown to provide a performance guarantee of (1-1/e) with respect to the optimal objective value given that the objective function possesses a property called string submodularity. The third heuristic method is called row-swap greedy strategy, which is an extension to the greedy strategy and utilizes certain properties of the UQP to provide a better performance than the greedy strategy at the expense of an increase in computational complexity. We present numerical results to demonstrate the performance of these heuristic methods, and also compare the performance of these methods against a standard heuristic method called semidefinite relaxation.",develop polynomi time heurist method solv unimodular quadrat program uqp approxim known np hard uqp framework maxim quadrat function vector complex variabl unit modulus sever problem activ sens wireless communic applic boil uqp motiv present three new heurist method polynomi time complex solv uqp approxim first method call domin eigenvector match solut pick match complex argument domin eigenvector hermitian matrix uqp formul also provid perform guarante method second method greedi strategi shown provid perform guarante respect optim object valu given object function possess properti call string submodular third heurist method call row swap greedi strategi extens greedi strategi util certain properti uqp provid better perform greedi strategi expens increas comput complex present numer result demonstr perform heurist method also compar perform method standard heurist method call semidefinit relax,"['Shankarachary Ragi', 'Edwin K. P. Chong', 'Hans D. Mittelmann']","['math.OC', 'cs.DS']",False,False,True,False,False,False
508,2017-03-28T14:09:26Z,2017-03-24T17:10:23Z,http://arxiv.org/abs/1703.08511v1,http://arxiv.org/pdf/1703.08511v1,ALLSAT compressed with wildcards. Part 2: All k-models of a BDD,allsat compress wildcard part model bdd,"If f is a Boolean function given by a BDD then it is well known how to calculate the number of models (i.e. bitstrings x with f(x)=1). Let  x  be the number of 1's in x. How to calculate the number of k-models x (i.e. having  x =k) is lesser known; we review a nice method due to Knuth. The main topic however is enumeration (=generation) as opposed to counting. Again, that ALL models can be enumerated in polynomial total time, is well known. Apparently new is the fact that also all k-models (for any fixed k) can be enumerated in polynomial total time. Using suitable wildcards this can be achieved in a compressed format.",boolean function given bdd well known calcul number model bitstr let number calcul number model lesser known review nice method due knuth main topic howev enumer generat oppos count model enumer polynomi total time well known appar new fact also model ani fix enumer polynomi total time use suitabl wildcard achiev compress format,['Marcel Wild'],['cs.DS'],False,False,True,False,False,False
516,2017-03-28T14:09:30Z,2017-03-22T12:50:15Z,http://arxiv.org/abs/1703.07625v1,http://arxiv.org/pdf/1703.07625v1,Clustering for Different Scales of Measurement - the Gap-Ratio Weighted   K-means Algorithm,cluster differ scale measur gap ratio weight mean algorithm,"This paper describes a method for clustering data that are spread out over large regions and which dimensions are on different scales of measurement. Such an algorithm was developed to implement a robotics application consisting in sorting and storing objects in an unsupervised way. The toy dataset used to validate such application consists of Lego bricks of different shapes and colors. The uncontrolled lighting conditions together with the use of RGB color features, respectively involve data with a large spread and different levels of measurement between data dimensions. To overcome the combination of these two characteristics in the data, we have developed a new weighted K-means algorithm, called gap-ratio K-means, which consists in weighting each dimension of the feature space before running the K-means algorithm. The weight associated with a feature is proportional to the ratio of the biggest gap between two consecutive data points, and the average of all the other gaps. This method is compared with two other variants of K-means on the Lego bricks clustering problem as well as two other common classification datasets.",paper describ method cluster data spread larg region dimens differ scale measur algorithm develop implement robot applic consist sort store object unsupervis way toy dataset use valid applic consist lego brick differ shape color uncontrol light condit togeth use rgb color featur respect involv data larg spread differ level measur data dimens overcom combin two characterist data develop new weight mean algorithm call gap ratio mean consist weight dimens featur space befor run mean algorithm weight associ featur proport ratio biggest gap two consecut data point averag gap method compar two variant mean lego brick cluster problem well two common classif dataset,"['Joris Guérin', 'Olivier Gibaru', 'Stéphane Thiery', 'Eric Nyiri']","['cs.LG', 'cs.DS', 'stat.ML']",False,False,True,False,False,False
517,2017-03-28T14:09:30Z,2017-03-21T21:05:27Z,http://arxiv.org/abs/1703.07432v1,http://arxiv.org/pdf/1703.07432v1,Efficient PAC Learning from the Crowd,effici pac learn crowd,"In recent years crowdsourcing has become the method of choice for gathering labeled training data for learning algorithms. Standard approaches to crowdsourcing view the process of acquiring labeled data separately from the process of learning a classifier from the gathered data. This can give rise to computational and statistical challenges. For example, in most cases there are no known computationally efficient learning algorithms that are robust to the high level of noise that exists in crowdsourced data, and efforts to eliminate noise through voting often require a large number of queries per example.   In this paper, we show how by interleaving the process of labeling and learning, we can attain computational efficiency with much less overhead in the labeling cost. In particular, we consider the realizable setting where there exists a true target function in $\mathcal{F}$ and consider a pool of labelers. When a noticeable fraction of the labelers are perfect, and the rest behave arbitrarily, we show that any $\mathcal{F}$ that can be efficiently learned in the traditional realizable PAC model can be learned in a computationally efficient manner by querying the crowd, despite high amounts of noise in the responses. Moreover, we show that this can be done while each labeler only labels a constant number of examples and the number of labels requested per example, on average, is a constant. When no perfect labelers exist, a related task is to find a set of the labelers which are good but not perfect. We show that we can identify all good labelers, when at least the majority of labelers are good.",recent year crowdsourc becom method choic gather label train data learn algorithm standard approach crowdsourc view process acquir label data separ process learn classifi gather data give rise comput statist challeng exampl case known comput effici learn algorithm robust high level nois exist crowdsourc data effort elimin nois vote often requir larg number queri per exampl paper show interleav process label learn attain comput effici much less overhead label cost particular consid realiz set exist true target function mathcal consid pool label notic fraction label perfect rest behav arbitrarili show ani mathcal effici learn tradit realiz pac model learn comput effici manner queri crowd despit high amount nois respons moreov show done label onli label constant number exampl number label request per exampl averag constant perfect label exist relat task find set label good perfect show identifi good label least major label good,"['Pranjal Awasthi', 'Avrim Blum', 'Nika Haghtalab', 'Yishay Mansour']","['cs.LG', 'cs.DS']",False,False,True,False,False,True
521,2017-03-28T14:09:34Z,2017-03-21T14:46:39Z,http://arxiv.org/abs/1703.07247v1,http://arxiv.org/pdf/1703.07247v1,A Note on the Tree Augmentation Problem,note tree augment problem,"In the Tree Augmentation problem we are given a tree $T=(V,F)$ and an additional set $E \subseteq V \times V$ of edges, called ""links"", with positive integer costs $\{c_e:e \in E\}$. The goal is to augment $T$ by a minimum cost set of links $J \subseteq E$ such that $T \cup J$ is $2$-edge-connected. Let $M$ denote the maximum cost of a link. Recently, Adjiashvili introduced a novel LP for the problem and used it to break the natural $2$-approximation barrier for instances when $M$ is a constant. Specifically, his algorithm computes a $1.96418+\epsilon$ approximate solution in time $n^{O(M/\epsilon^2)}$. Using a slightly weaker LP we achieve ratio $\frac{12}{7}+\epsilon$ for arbitrary costs and ratio $1.6+\epsilon$ for unit costs in time $2^{O(M/\epsilon^2)}$.",tree augment problem given tree addit set subseteq time edg call link posit integ cost goal augment minimum cost set link subseteq cup edg connect let denot maximum cost link recent adjiashvili introduc novel lp problem use break natur approxim barrier instanc constant specif algorithm comput epsilon approxim solut time epsilon use slight weaker lp achiev ratio frac epsilon arbitrari cost ratio epsilon unit cost time epsilon,['Zeev Nutov'],['cs.DS'],False,False,True,False,False,False
528,2017-03-28T14:09:34Z,2017-03-18T00:44:38Z,http://arxiv.org/abs/1703.06227v1,http://arxiv.org/pdf/1703.06227v1,Discriminative Distance-Based Network Indices and the Tiny-World   Property,discrimin distanc base network indic tini world properti,"Distance-based indices, including closeness centrality, average path length, eccentricity and average eccentricity, are important tools for network analysis. In these indices, the distance between two vertices is measured by the size of shortest paths between them. However, this measure has shortcomings. A well-studied shortcoming is that extending it to disconnected graphs (and also directed graphs) is controversial. The second shortcoming is that when this measure is used in real-world networks, a huge number of vertices may have exactly the same closeness/eccentricity scores. The third shortcoming is that in many applications, the distance between two vertices not only depends on the size of shortest paths, but also on the number of shortest paths between them. In this paper, we develop a new distance measure between vertices of a graph that yields discriminative distance-based centrality indices. This measure is proportional to the size of shortest paths and inversely proportional to the number of shortest paths. We present algorithms for exact computation of the proposed discriminative indices. We then develop randomized algorithms that precisely estimate average discriminative path length and average discriminative eccentricity and show that they give $(\epsilon,\delta)$-approximations of these indices. Finally, we preform extensive experiments over several real-world networks from different domains and show that compared to the traditional indices, discriminative indices have usually much more discriminability. Our experiments reveal that real-world networks have usually a tiny average discriminative path length, bounded by a constant (e.g., 2). We refer to this property as the tiny-world property.",distanc base indic includ close central averag path length eccentr averag eccentr import tool network analysi indic distanc two vertic measur size shortest path howev measur shortcom well studi shortcom extend disconnect graph also direct graph controversi second shortcom measur use real world network huge number vertic may exact close eccentr score third shortcom mani applic distanc two vertic onli depend size shortest path also number shortest path paper develop new distanc measur vertic graph yield discrimin distanc base central indic measur proport size shortest path invers proport number shortest path present algorithm exact comput propos discrimin indic develop random algorithm precis estim averag discrimin path length averag discrimin eccentr show give epsilon delta approxim indic final preform extens experi sever real world network differ domain show compar tradit indic discrimin indic usual much discrimin experi reveal real world network usual tini averag discrimin path length bound constant refer properti tini world properti,"['Mostafa Haghir Chehreghani', 'Albert Bifet', 'Talel Abdessalem']","['cs.DS', 'cs.SI']",False,False,True,False,False,False
540,2017-03-28T14:09:43Z,2017-03-16T08:52:21Z,http://arxiv.org/abs/1703.05509v1,http://arxiv.org/pdf/1703.05509v1,VieM v1.00 -- Vienna Mapping and Sparse Quadratic Assignment User Guide,viem vienna map spars quadrat assign user guid,This paper severs as a user guide to the mapping framework VieM (Vienna Mapping and Sparse Quadratic Assignment). We give a rough overview of the techniques used within the framework and describe the user interface as well as the file formats used.,paper sever user guid map framework viem vienna map spars quadrat assign give rough overview techniqu use within framework describ user interfac well file format use,"['Christian Schulz', 'Jesper Larsson Träff']","['cs.DC', 'cs.DS', 'math.CO']",False,False,True,False,False,False
541,2017-03-28T14:09:43Z,2017-03-16T08:10:30Z,http://arxiv.org/abs/1703.05496v1,http://arxiv.org/pdf/1703.05496v1,Data Delivery by Mobile Agents with Energy Constraints over a fixed path,data deliveri mobil agent energi constraint fix path,"We consider $k$ mobile agents of limited energy that are initially located at vertices of an edge-weighted graph $G$ and have to collectively deliver data from a source vertex $s$ to a target vertex $t$. The data are to be collected by an agent reaching $s$, who can carry and then hand them over another agent etc., until some agent with the data reaches $t$. The data can be carried only over a fixed $s-t$ path of $G$; each agent has an initial energy budget and each time it passes an edge, it consumes the edge's weights in energy units and stalls if its energy is not anymore sufficient to move. The main result of this paper is a 3-approximation polynomial time algorithm for the data delivery problem over a fixed $s-t$ path in the graph, for identical initial energy budgets and at most one allowed data hand-over per agent.",consid mobil agent limit energi initi locat vertic edg weight graph collect deliv data sourc vertex target vertex data collect agent reach carri hand anoth agent etc agent data reach data carri onli fix path agent initi energi budget time pass edg consum edg weight energi unit stall energi anymor suffici move main result paper approxim polynomi time algorithm data deliveri problem fix path graph ident initi energi budget one allow data hand per agent,"['Aristotelis Giannakos', 'Mhand Hifi', 'Gregory Karagiorgos']",['cs.DS'],False,False,True,False,False,False
547,2017-03-28T14:09:43Z,2017-03-15T11:57:53Z,http://arxiv.org/abs/1703.05097v1,http://arxiv.org/pdf/1703.05097v1,A cubic-time algorithm for computing the trinet distance between level-1   networks,cubic time algorithm comput trinet distanc level network,"In evolutionary biology, phylogenetic networks are constructed to represent the evolution of species in which reticulate events are thought to have occurred, such as recombination and hybridization. It is therefore useful to have efficiently computable metrics with which to systematically compare such networks. Through developing an optimal algorithm to enumerate all trinets displayed by a level-1 network (a type of network that is slightly more general than an evolutionary tree), here we propose a cubic-time algorithm to compute the trinet distance between two level-1 networks. Employing simulations, we also present a comparison between the trinet metric and the so-called Robinson-Foulds phylogenetic network metric restricted to level-1 networks. The algorithms described in this paper have been implemented in JAVA and are freely available at https://www.uea.ac.uk/computing/TriLoNet.",evolutionari biolog phylogenet network construct repres evolut speci reticul event thought occur recombin hybrid therefor use effici comput metric systemat compar network develop optim algorithm enumer trinet display level network type network slight general evolutionari tree propos cubic time algorithm comput trinet distanc two level network employ simul also present comparison trinet metric call robinson fould phylogenet network metric restrict level network algorithm describ paper implement java freeli avail https www uea ac uk comput trilonet,"['Vincent Moulton', 'James Oldman', 'Taoyang Wu']","['q-bio.PE', 'cs.DM', 'cs.DS']",False,False,True,False,True,False
552,2017-03-28T14:09:47Z,2017-03-13T16:18:01Z,http://arxiv.org/abs/1703.04466v1,http://arxiv.org/pdf/1703.04466v1,Bicriteria Rectilinear Shortest Paths among Rectilinear Obstacles in the   Plane,bicriteria rectilinear shortest path among rectilinear obstacl plane,"Given a rectilinear domain $\mathcal{P}$ of $h$ pairwise-disjoint rectilinear obstacles with a total of $n$ vertices in the plane, we study the problem of computing bicriteria rectilinear shortest paths between two points $s$ and $t$ in $\mathcal{P}$. Three types of bicriteria rectilinear paths are considered: minimum-link shortest paths, shortest minimum-link paths, and minimum-cost paths where the cost of a path is a non-decreasing function of both the number of edges and the length of the path. The one-point and two-point path queries are also considered. Algorithms for these problems have been given previously. Our contributions are threefold. First, we find a critical error in all previous algorithms. Second, we correct the error in a not-so-trivial way. Third, we further improve the algorithms so that they are even faster than the previous (incorrect) algorithms when $h$ is relatively small. For example, for the minimum-link shortest paths, we obtain the following results. Our algorithm computes a minimum-link shortest $s$-$t$ path in $O(n+h\log^{3/2} h)$ time. For the one-point queries, we build a data structure of size $O(n+ h\log h)$ in $O(n+h\log^{3/2} h)$ time for a source point $s$, such that given any query point $t$, a minimum-link shortest $s$-$t$ path can be determined in $O(\log n)$ time. For the two-point queries, with $O(n+h^2\log^2 h)$ time and space preprocessing, a minimum-link shortest $s$-$t$ path can be determined in $O(\log n+\log^2 h)$ time for any two query points $s$ and $t$; alternatively, with $O(n+h^2\cdot \log^{2} h \cdot 4^{\sqrt{\log h}})$ time and $O(n+h^2\cdot \log h \cdot 4^{\sqrt{\log h}})$ space preprocessing, we can answer each two-point query in $O(\log n)$ time.",given rectilinear domain mathcal pairwis disjoint rectilinear obstacl total vertic plane studi problem comput bicriteria rectilinear shortest path two point mathcal three type bicriteria rectilinear path consid minimum link shortest path shortest minimum link path minimum cost path cost path non decreas function number edg length path one point two point path queri also consid algorithm problem given previous contribut threefold first find critic error previous algorithm second correct error trivial way third improv algorithm even faster previous incorrect algorithm relat small exampl minimum link shortest path obtain follow result algorithm comput minimum link shortest path log time one point queri build data structur size log log time sourc point given ani queri point minimum link shortest path determin log time two point queri log time space preprocess minimum link shortest path determin log log time ani two queri point altern cdot log cdot sqrt log time cdot log cdot sqrt log space preprocess answer two point queri log time,['Haitao Wang'],"['cs.CG', 'cs.DS']",False,False,True,False,False,False
553,2017-03-28T14:09:47Z,2017-03-13T13:31:17Z,http://arxiv.org/abs/1703.04381v1,http://arxiv.org/pdf/1703.04381v1,On the Transformation Capability of Feasible Mechanisms for Programmable   Matter,transform capabl feasibl mechan programm matter,"In this work, we study theoretical models of \emph{programmable matter} systems. The systems under consideration consist of spherical modules, kept together by magnetic forces and able to perform two minimal mechanical operations (or movements): \emph{rotate} around a neighbor and \emph{slide} over a line. In terms of modeling, there are $n$ nodes arranged in a 2-dimensional grid and forming some initial \emph{shape}. The goal is for the initial shape $A$ to \emph{transform} to some target shape $B$ by a sequence of movements. Most of the paper focuses on \emph{transformability} questions, meaning whether it is in principle feasible to transform a given shape to another. We first consider the case in which only rotation is available to the nodes. Our main result is that deciding whether two given shapes $A$ and $B$ can be transformed to each other, is in $\mathbf{P}$. We then insist on rotation only and impose the restriction that the nodes must maintain global connectivity throughout the transformation. We prove that the corresponding transformability question is in $\mathbf{PSPACE}$ and study the problem of determining the minimum \emph{seeds} that can make feasible, otherwise infeasible transformations. Next we allow both rotations and slidings and prove universality: any two connected shapes $A,B$ of the same order, can be transformed to each other without breaking connectivity. The worst-case number of movements of the generic strategy is $\Omega(n^2)$. We improve this to $O(n)$ parallel time, by a pipelining strategy, and prove optimality of both by matching lower bounds. In the last part of the paper, we turn our attention to distributed transformations. The nodes are now distributed processes able to perform communicate-compute-move rounds. We provide distributed algorithms for a general type of transformations.",work studi theoret model emph programm matter system system consider consist spheric modul kept togeth magnet forc abl perform two minim mechan oper movement emph rotat around neighbor emph slide line term model node arrang dimension grid form initi emph shape goal initi shape emph transform target shape sequenc movement paper focus emph transform question mean whether principl feasibl transform given shape anoth first consid case onli rotat avail node main result decid whether two given shape transform mathbf insist rotat onli impos restrict node must maintain global connect throughout transform prove correspond transform question mathbf pspace studi problem determin minimum emph seed make feasibl otherwis infeas transform next allow rotat slide prove univers ani two connect shape order transform without break connect worst case number movement generic strategi omega improv parallel time pipelin strategi prove optim match lower bound last part paper turn attent distribut transform node distribut process abl perform communic comput move round provid distribut algorithm general type transform,"['Othon Michail', 'George Skretas', 'Paul G. Spirakis']","['cs.DS', 'cs.DC', 'cs.RO']",False,False,True,False,False,True
561,2017-03-28T14:09:51Z,2017-03-10T22:25:56Z,http://arxiv.org/abs/1703.03859v1,http://arxiv.org/abs/1703.03859v1,Markov Chain Lifting and Distributed ADMM,markov chain lift distribut admm,"The time to converge to the steady state of a finite Markov chain can be greatly reduced by a lifting operation, which creates a new Markov chain on an expanded state space. For a class of quadratic objectives, we show an analogous behavior where a distributed ADMM algorithm can be seen as a lifting of Gradient Descent algorithm. This provides a deep insight for its faster convergence rate under optimal parameter tuning. We conjecture that this gain is always present, as opposed to the lifting of a Markov chain which sometimes only provides a marginal speedup.",time converg steadi state finit markov chain great reduc lift oper creat new markov chain expand state space class quadrat object show analog behavior distribut admm algorithm seen lift gradient descent algorithm provid deep insight faster converg rate optim paramet tune conjectur gain alway present oppos lift markov chain sometim onli provid margin speedup,"['Guilherme França', 'José Bento']","['stat.ML', 'cs.DS', 'cs.IT', 'cs.LG', 'math.IT', 'math.OC']",False,False,True,False,False,False
567,2017-03-28T14:09:51Z,2017-03-09T06:08:01Z,http://arxiv.org/abs/1703.03147v1,http://arxiv.org/pdf/1703.03147v1,Juggling Functions Inside a Database,juggl function insid databas,"We define and study the Functional Aggregate Query (FAQ) problem, which captures common computational tasks across a very wide range of domains including relational databases, logic, matrix and tensor computation, probabilistic graphical models, constraint satisfaction, and signal processing. Simply put, an FAQ is a declarative way of defining a new function from a database of input functions.   We present ""InsideOut"", a dynamic programming algorithm, to evaluate an FAQ. The algorithm rewrites the input query into a set of easier-to-compute FAQ sub-queries. Each sub-query is then evaluated using a worst-case optimal relational join algorithm. The topic of designing algorithms to optimally evaluate the classic multiway join problem has seen exciting developments in the past few years. Our framework tightly connects these new ideas in database theory with a vast number of application areas in a coherent manner, showing potentially that a good database engine can be a general-purpose constraint solver, relational data store, graphical model inference engine, and matrix/tensor computation processor all at once.   The InsideOut algorithm is very simple, as shall be described in this paper. Yet, in spite of solving an extremely general problem, its runtime either is as good as or improves upon the best known algorithm for the applications that FAQ specializes to. These corollaries include computational tasks in graphical model inference, matrix/tensor operations, relational joins, and logic. Better yet, InsideOut can be used within any database engine, because it is basically a principled way of rewriting queries. Indeed, it is already part of the LogicBlox database engine, helping efficiently answer traditional database queries, graphical model inference queries, and train a large class of machine learning models inside the database itself.",defin studi function aggreg queri faq problem captur common comput task across veri wide rang domain includ relat databas logic matrix tensor comput probabilist graphic model constraint satisfact signal process simpli put faq declar way defin new function databas input function present insideout dynam program algorithm evalu faq algorithm rewrit input queri set easier comput faq sub queri sub queri evalu use worst case optim relat join algorithm topic design algorithm optim evalu classic multiway join problem seen excit develop past year framework tight connect new idea databas theori vast number applic area coher manner show potenti good databas engin general purpos constraint solver relat data store graphic model infer engin matrix tensor comput processor onc insideout algorithm veri simpl shall describ paper yet spite solv extrem general problem runtim either good improv upon best known algorithm applic faq special corollari includ comput task graphic model infer matrix tensor oper relat join logic better yet insideout use within ani databas engin becaus basic principl way rewrit queri inde alreadi part logicblox databas engin help effici answer tradit databas queri graphic model infer queri train larg class machin learn model insid databas,"['Mahmoud Abo Khamis', 'Hung Q. Ngo', 'Atri Rudra']","['cs.DB', 'cs.DS', 'cs.LO']",False,False,True,False,False,True
569,2017-03-28T14:09:51Z,2017-03-08T15:16:11Z,http://arxiv.org/abs/1703.02867v1,http://arxiv.org/pdf/1703.02867v1,Electoral District Design via Constrained Clustering,elector district design via constrain cluster,The paper studies the electoral district design problem where municipalities of a state have to be grouped into districts of nearly equal population while obeying certain politically motivated requirements. We develop a general framework for electoral district design that is based on the close connection of constrained geometric clustering and diagrams. The approach is computationally efficient and flexible enough to pursue various conflicting juridical demands for the shape of the districts. We demonstrate the practicability of our methodology for electoral districting in Germany.,paper studi elector district design problem municip state group district near equal popul obey certain polit motiv requir develop general framework elector district design base close connect constrain geometr cluster diagram approach comput effici flexibl enough pursu various conflict jurid demand shape district demonstr practic methodolog elector district germani,"['Andreas Brieden', 'Peter Gritzmann', 'Fabian Klemm']","['cs.DS', 'math.CO', '90C90']",False,False,True,False,False,True
570,2017-03-28T14:09:55Z,2017-03-08T15:16:05Z,http://arxiv.org/abs/1703.02866v1,http://arxiv.org/pdf/1703.02866v1,The Half-integral Erdös-Pósa Property for Non-null Cycles,half integr erd sa properti non null cycl,"A Group Labeled Graph is a pair $(G,\Lambda)$ where $G$ is an oriented graph and $\Lambda$ is a mapping from the arcs of $G$ to elements of a group. A (not necessarily directed) cycle $C$ is called non-null if for any cyclic ordering of the arcs in $C$, the group element obtained by `adding' the labels on forward arcs and `subtracting' the labels on reverse arcs is not the identity element of the group. Non-null cycles in group labeled graphs generalize several well-known graph structures, including odd cycles.   In this paper, we prove that non-null cycles on Group Labeled Graphs have the half-integral Erd\""os-P\'osa property. That is, there is a function $f:{\mathbb N}\to {\mathbb N}$ such that for any $k\in {\mathbb N}$, any group labeled graph $(G,\Lambda)$ has a set of $k$ non-null cycles such that each vertex of $G$ appears in at most two of these cycles or there is a set of at most $f(k)$ vertices that intersects every non-null cycle. Since it is known that non-null cycles do not have the integeral Erd\""os-P\'osa property in general, a half-integral Erd\""os-P\'osa result is the best one could hope for.",group label graph pair lambda orient graph lambda map arc element group necessarili direct cycl call non null ani cyclic order arc group element obtain ad label forward arc subtract label revers arc ident element group non null cycl group label graph general sever well known graph structur includ odd cycl paper prove non null cycl group label graph half integr erd os osa properti function mathbb mathbb ani mathbb ani group label graph lambda set non null cycl vertex appear two cycl set vertic intersect everi non null cycl sinc known non null cycl integer erd os osa properti general half integr erd os osa result best one could hope,"['Daniel Lokshtanov', 'M. S. Ramanujan', 'Saket Saurabh']","['cs.DM', 'cs.DS']",False,False,True,False,False,True
571,2017-03-28T14:09:55Z,2017-03-08T10:56:03Z,http://arxiv.org/abs/1703.02784v1,http://arxiv.org/pdf/1703.02784v1,$K$-Best Solutions of MSO Problems on Tree-Decomposable Graphs,best solut mso problem tree decompos graph,"We show that, for any graph optimization problem in which the feasible solutions can be expressed by a formula in monadic second-order logic describing sets of vertices or edges and in which the goal is to minimize the sum of the weights in the selected sets, we can find the $k$ best solutions for $n$-vertex graphs of bounded treewidth in time $\mathcal O(n+k\log n)$. In particular, this applies to the problem of finding the $k$ shortest simple paths between given vertices in directed graphs of bounded treewidth, giving an exponential speedup in the per-path cost over previous algorithms.",show ani graph optim problem feasibl solut express formula monad second order logic describ set vertic edg goal minim sum weight select set find best solut vertex graph bound treewidth time mathcal log particular appli problem find shortest simpl path given vertic direct graph bound treewidth give exponenti speedup per path cost previous algorithm,"['David Eppstein', 'Denis Kurz']","['cs.DS', 'G.2.2']",False,False,True,False,False,True
578,2017-03-28T14:09:55Z,2017-03-23T13:11:52Z,http://arxiv.org/abs/1703.02375v2,http://arxiv.org/pdf/1703.02375v2,Graph sketching-based Massive Data Clustering,graph sketch base massiv data cluster,"In this paper, we address the problem of recovering arbitrary-shaped data clusters from massive datasets. We present DBMSTClu a new density-based non-parametric method working on a limited number of linear measurements i.e. a sketched version of the similarity graph $G$ between the $N$ objects to cluster. Unlike $k$-means, $k$-medians or $k$-medoids algorithms, it does not fail at distinguishing clusters with particular structures. No input parameter is needed contrarily to DBSCAN or the Spectral Clustering method. DBMSTClu as a graph-based technique relies on the similarity graph $G$ which costs theoretically $O(N^2)$ in memory. However, our algorithm follows the dynamic semi-streaming model by handling $G$ as a stream of edge weight updates and sketches it in one pass over the data into a compact structure requiring $O(N \operatorname{poly} \operatorname{log} (N))$ space. Thanks to the property of the Minimum Spanning Tree (MST) for expressing the underlying structure of a graph, our algorithm successfully detects the right number of non-convex clusters by recovering an approximate MST from the graph sketch of $G$. We provide theoretical guarantees on the quality of the clustering partition and also demonstrate its advantage over the existing state-of-the-art on several datasets.",paper address problem recov arbitrari shape data cluster massiv dataset present dbmstclu new densiti base non parametr method work limit number linear measur sketch version similar graph object cluster unlik mean median medoid algorithm doe fail distinguish cluster particular structur input paramet need contrarili dbscan spectral cluster method dbmstclu graph base techniqu reli similar graph cost theoret memori howev algorithm follow dynam semi stream model handl stream edg weight updat sketch one pass data compact structur requir operatornam poli operatornam log space thank properti minimum span tree mst express structur graph algorithm success detect right number non convex cluster recov approxim mst graph sketch provid theoret guarante qualiti cluster partit also demonstr advantag exist state art sever dataset,"['Anne Morvan', 'Krzysztof Choromanski', 'Cédric Gouy-Pailler', 'Jamal Atif']","['cs.LG', 'cs.DS']",False,False,True,False,False,False
581,2017-03-28T14:09:59Z,2017-03-06T19:01:03Z,http://arxiv.org/abs/1703.02059v1,http://arxiv.org/pdf/1703.02059v1,Cheshire: An Online Algorithm for Activity Maximization in Social   Networks,cheshir onlin algorithm activ maxim social network,"User engagement in social networks depends critically on the number of online actions their users take in the network. Can we design an algorithm that finds when to incentivize users to take actions to maximize the overall activity in a social network? In this paper, we model the number of online actions over time using multidimensional Hawkes processes, derive an alternate representation of these processes based on stochastic differential equations (SDEs) with jumps and, exploiting this alternate representation, address the above question from the perspective of stochastic optimal control of SDEs with jumps. We find that the optimal level of incentivized actions depends linearly on the current level of overall actions. Moreover, the coefficients of this linear relationship can be found by solving a matrix Riccati differential equation, which can be solved efficiently, and a first order differential equation, which has a closed form solution. As a result, we are able to design an efficient online algorithm, Cheshire, to sample the optimal times of the users' incentivized actions. Experiments on both synthetic and real data gathered from Twitter show that our algorithm is able to consistently maximize the number of online actions more effectively than the state of the art.",user engag social network depend critic number onlin action user take network design algorithm find incentiv user take action maxim overal activ social network paper model number onlin action time use multidimension hawk process deriv altern represent process base stochast differenti equat sdes jump exploit altern represent address abov question perspect stochast optim control sdes jump find optim level incentiv action depend linear current level overal action moreov coeffici linear relationship found solv matrix riccati differenti equat solv effici first order differenti equat close form solut result abl design effici onlin algorithm cheshir sampl optim time user incentiv action experi synthet real data gather twitter show algorithm abl consist maxim number onlin action effect state art,"['Ali Zarezade', 'Abir De', 'Hamid Rabiee', 'Manuel Gomez Rodriguez']","['stat.ML', 'cs.DS', 'cs.LG', 'cs.SI']",False,False,True,False,False,False
587,2017-03-28T14:09:59Z,2017-03-06T05:14:07Z,http://arxiv.org/abs/1703.01727v1,http://arxiv.org/pdf/1703.01727v1,Frequent Query Matching in Dynamic Data Warehousing,frequent queri match dynam data wareh,"With the need for flexible and on-demand decision support, Dynamic Data Warehouses (DDW) provide benefits over traditional data warehouses due to their dynamic characteristics in structuring and access mechanism. A DDW is a data framework that accommodates data source changes easily to allow seamless querying to users. Materialized Views (MV) are proven to be an effective methodology to enhance the process of retrieving data from a DDW as results are pre-computed and stored in it. However, due to the static nature of materialized views, the level of dynamicity that can be provided at the MV access layer is restricted. As a result, the collection of materialized views is not compatible with ever-changing reporting requirements. It is important that the MV collection is consistent with current and upcoming queries. The solution to the above problem must consider the following aspects: (a) MV must be matched against an OLAP query in order to recognize whether the MV can answer the query, (b) enable scalability in the MV collection, an intuitive mechanism to prune it and retrieve closely matching MVs must be incorporated, (c) MV collection must be able to evolve in correspondence to the regularly changing user query patterns. Therefore, the primary objective of this paper is to explore these aspects and provide a well-rounded solution for the MV access layer to remove the mismatch between the MV collection and reporting requirements. Our contribution to solve the problem includes a Query Matching Technique, a Domain Matching Technique and Maintenance of the MV collection. We developed an experimental platform using real data-sets to evaluate the effectiveness in terms of performance and precision of the proposed techniques.",need flexibl demand decis support dynam data warehous ddw provid benefit tradit data warehous due dynam characterist structur access mechan ddw data framework accommod data sourc chang easili allow seamless queri user materi view mv proven effect methodolog enhanc process retriev data ddw result pre comput store howev due static natur materi view level dynam provid mv access layer restrict result collect materi view compat ever chang report requir import mv collect consist current upcom queri solut abov problem must consid follow aspect mv must match olap queri order recogn whether mv answer queri enabl scalabl mv collect intuit mechan prune retriev close match mvs must incorpor mv collect must abl evolv correspond regular chang user queri pattern therefor primari object paper explor aspect provid well round solut mv access layer remov mismatch mv collect report requir contribut solv problem includ queri match techniqu domain match techniqu mainten mv collect develop experiment platform use real data set evalu effect term perform precis propos techniqu,"['Charles H. Goonetilleke', 'J. Wenny Rahayu', 'Md. Saiful Islam']","['cs.DB', 'cs.DS']",False,False,True,False,False,False
592,2017-03-28T14:10:04Z,2017-03-05T01:08:29Z,http://arxiv.org/abs/1703.01539v1,http://arxiv.org/pdf/1703.01539v1,Distributed Partial Clustering,distribut partial cluster,"Recent years have witnessed an increasing popularity of algorithm design for distributed data, largely due to the fact that massive datasets are often collected and stored in different locations. In the distributed setting communication typically dominates the query processing time. Thus it becomes crucial to design communication efficient algorithms for queries on distributed data. Simultaneously, it has been widely recognized that partial optimizations, where we are allowed to disregard a small part of the data, provide us significantly better solutions. The motivation for disregarded points often arise from noise and other phenomena that are pervasive in large data scenarios.   In this paper we focus on partial clustering problems, $k$-center, $k$-median and $k$-means, in the distributed model, and provide algorithms with communication sublinear of the input size. As a consequence we develop the first algorithms for the partial $k$-median and means objectives that run in subquadratic running time. We also initiate the study of distributed algorithms for clustering uncertain data, where each data point can possibly fall into multiple locations under certain probability distribution.",recent year wit increas popular algorithm design distribut data larg due fact massiv dataset often collect store differ locat distribut set communic typic domin queri process time thus becom crucial design communic effici algorithm queri distribut data simultan wide recogn partial optim allow disregard small part data provid us signific better solut motiv disregard point often aris nois phenomena pervas larg data scenario paper focus partial cluster problem center median mean distribut model provid algorithm communic sublinear input size consequ develop first algorithm partial median mean object run subquadrat run time also initi studi distribut algorithm cluster uncertain data data point possibl fall multipl locat certain probabl distribut,"['Sudipto Guha', 'Yi Li', 'Qin Zhang']",['cs.DS'],False,False,True,False,False,False
594,2017-03-28T14:10:04Z,2017-03-04T19:08:22Z,http://arxiv.org/abs/1703.01507v1,http://arxiv.org/pdf/1703.01507v1,Machine Learning Friendly Set Version of Johnson-Lindenstrauss Lemma,machin learn friend set version johnson lindenstrauss lemma,"In this paper we make a novel use of the Johnson-Lindenstrauss Lemma. The Lemma has an existential form saying that there exists a JL transformation $f$ of the data points into lower dimensional space such that all of them fall into predefined error range $\delta$. We formulate in this paper a theorem stating that we can choose the target dimensionality in a random projection type JL linear transformation in such a way that with probability $1-\epsilon$ all of them fall into predefined error range $\delta$ for any user-predefined failure probability $\epsilon$. This result is important for applications such a data clustering where we want to have a priori dimensionality reducing transformation instead of trying out a (large) number of them, as with traditional Johnson-Lindenstrauss Lemma.",paper make novel use johnson lindenstrauss lemma lemma existenti form say exist jl transform data point lower dimension space fall predefin error rang delta formul paper theorem state choos target dimension random project type jl linear transform way probabl epsilon fall predefin error rang delta ani user predefin failur probabl epsilon result import applic data cluster want priori dimension reduc transform instead tri larg number tradit johnson lindenstrauss lemma,['Mieczysław A. Kłopotek'],"['cs.DS', 'cs.LG']",False,False,True,False,False,False
598,2017-03-28T14:10:04Z,2017-03-03T06:41:02Z,http://arxiv.org/abs/1703.01054v1,http://arxiv.org/abs/1703.01054v1,When Hashes Met Wedges: A Distributed Algorithm for Finding High   Similarity Vectors,hash met wedg distribut algorithm find high similar vector,"Finding similar user pairs is a fundamental task in social networks, with numerous applications in ranking and personalization tasks such as link prediction and tie strength detection. A common manifestation of user similarity is based upon network structure: each user is represented by a vector that represents the user's network connections, where pairwise cosine similarity among these vectors defines user similarity. The predominant task for user similarity applications is to discover all similar pairs that have a pairwise cosine similarity value larger than a given threshold $\tau$. In contrast to previous work where $\tau$ is assumed to be quite close to 1, we focus on recommendation applications where $\tau$ is small, but still meaningful. The all pairs cosine similarity problem is computationally challenging on networks with billions of edges, and especially so for settings with small $\tau$. To the best of our knowledge, there is no practical solution for computing all user pairs with, say $\tau = 0.2$ on large social networks, even using the power of distributed algorithms.   Our work directly addresses this challenge by introducing a new algorithm --- WHIMP --- that solves this problem efficiently in the MapReduce model. The key insight in WHIMP is to combine the ""wedge-sampling"" approach of Cohen-Lewis for approximate matrix multiplication with the SimHash random projection techniques of Charikar. We provide a theoretical analysis of WHIMP, proving that it has near optimal communication costs while maintaining computation cost comparable with the state of the art. We also empirically demonstrate WHIMP's scalability by computing all highly similar pairs on four massive data sets, and show that it accurately finds high similarity pairs. In particular, we note that WHIMP successfully processes the entire Twitter network, which has tens of billions of edges.",find similar user pair fundament task social network numer applic rank person task link predict tie strength detect common manifest user similar base upon network structur user repres vector repres user network connect pairwis cosin similar among vector defin user similar predomin task user similar applic discov similar pair pairwis cosin similar valu larger given threshold tau contrast previous work tau assum quit close focus recommend applic tau small still meaning pair cosin similar problem comput challeng network billion edg especi set small tau best knowledg practic solut comput user pair say tau larg social network even use power distribut algorithm work direct address challeng introduc new algorithm whimp solv problem effici mapreduc model key insight whimp combin wedg sampl approach cohen lewi approxim matrix multipl simhash random project techniqu charikar provid theoret analysi whimp prove near optim communic cost maintain comput cost compar state art also empir demonstr whimp scalabl comput high similar pair four massiv data set show accur find high similar pair particular note whimp success process entir twitter network ten billion edg,"['Aneesh Sharma', 'C. Seshadhri', 'Ashish Goel']","['cs.SI', 'cs.DC', 'cs.DS']",False,False,True,False,False,False
600,2017-03-28T14:08:00Z,2017-03-27T16:44:49Z,http://arxiv.org/abs/1703.09177v1,http://arxiv.org/pdf/1703.09177v1,Nash Equilibrium in Social Media,nash equilibrium social media,"In this work, we investigate an application of a Nash equilibrium seeking algorithm in a social network. In a networked game each player (user) takes action in response to other players' actions in order to decrease (increase) his cost (profit) in the network. We assume that the players' cost functions are not necessarily dependent on the actions of all players. This is due to better mimicking the standard social media rules. A communication graph is defined for the game through which players are able to share their information with only their neighbors. We assume that the communication neighbors necessarily affect the players' cost functions while the reverse is not always true. In this game, the players are only aware of their own cost functions and actions. Thus, each of them maintains an estimate of the others' actions and share it with the neighbors to update his action and estimates.",work investig applic nash equilibrium seek algorithm social network network game player user take action respons player action order decreas increas cost profit network assum player cost function necessarili depend action player due better mimick standard social media rule communic graph defin game player abl share inform onli neighbor assum communic neighbor necessarili affect player cost function revers alway true game player onli awar cost function action thus maintain estim action share neighbor updat action estim,['Farzad Salehisadaghiani'],"['cs.GT', 'cs.SI', 'cs.SY']",False,False,True,False,False,False
603,2017-03-28T14:08:00Z,2017-03-26T06:49:54Z,http://arxiv.org/abs/1703.08776v1,http://arxiv.org/pdf/1703.08776v1,Assortative Mixing Equilibria in Social Network Games,assort mix equilibria social network game,"It is known that individuals in social networks tend to exhibit homophily (a.k.a. assortative mixing) in their social ties, which implies that they prefer bonding with others of their own kind. But what are the reasons for this phenomenon? Is it that such relations are more convenient and easier to maintain? Or are there also some more tangible benefits to be gained from this collective behaviour?   The current work takes a game-theoretic perspective on this phenomenon, and studies the conditions under which different assortative mixing strategies lead to equilibrium in an evolving social network. We focus on a biased preferential attachment model where the strategy of each group (e.g., political or social minority) determines the level of bias of its members toward other group members and non-members. Our first result is that if the utility function that the group attempts to maximize is the degree centrality of the group, interpreted as the sum of degrees of the group members in the network, then the only strategy achieving Nash equilibrium is a perfect homophily, which implies that cooperation with other groups is harmful to this utility function. A second, and perhaps more surprising, result is that if a reward for inter-group cooperation is added to the utility function (e.g., externally enforced by an authority as a regulation), then there are only two possible equilibria, namely, perfect homophily or perfect heterophily, and it is possible to characterize their feasibility spaces. Interestingly, these results hold regardless of the minority-majority ratio in the population.   We believe that these results, as well as the game-theoretic perspective presented herein, may contribute to a better understanding of the forces that shape the groups and communities of our society.",known individu social network tend exhibit homophili assort mix social tie impli prefer bond kind reason phenomenon relat conveni easier maintain also tangibl benefit gain collect behaviour current work take game theoret perspect phenomenon studi condit differ assort mix strategi lead equilibrium evolv social network focus bias preferenti attach model strategi group polit social minor determin level bias member toward group member non member first result util function group attempt maxim degre central group interpret sum degre group member network onli strategi achiev nash equilibrium perfect homophili impli cooper group harm util function second perhap surpris result reward inter group cooper ad util function extern enforc author regul onli two possibl equilibria name perfect homophili perfect heterophili possibl character feasibl space interest result hold regardless minor major ratio popul believ result well game theoret perspect present herein may contribut better understand forc shape group communiti societi,"['Chen Avin', 'Hadassa Daltrophe', 'Zvi Lotker', 'David Peleg']","['cs.SI', 'cs.GT', 'physics.soc-ph']",False,False,True,False,False,True
604,2017-03-28T14:08:00Z,2017-03-26T01:18:35Z,http://arxiv.org/abs/1703.08750v1,http://arxiv.org/pdf/1703.08750v1,Game-Theoretic Protection Against Networked SIS Epidemics by Human   Decision-Makers,game theoret protect network sis epidem human decis maker,"We study decentralized protection strategies by human decision-makers against Susceptible-Infected-Susceptible (SIS) epidemics on networks. Specifically, we examine the impact of behavioral (mis)-perceptions of infection probabilities (captured by Prospect theory) on the Nash equilibrium strategies in two classes of games. In the first class of games, nodes choose their curing rates to minimize the steady-state infection probability under the degree-based mean-field approximation plus the cost of their selected curing rate. We establish the existence of pure Nash equilibria under both risk neutral and behavioral decision-makers. When the per-unit cost of curing rate is sufficiently high, we show that risk neutral players choose the curing rate to be zero at the equilibrium, while curing rate is nonzero under behavioral decision-making for any finite cost. In the second class of games, the nodes choose whether or not to vaccinate themselves. We establish the existence of unique threshold equilibria where nodes with degrees larger than a certain threshold vaccinate. When the vaccination cost is sufficiently high, fewer behavioral players vaccinate compared to risk neutral players, and vice versa. Finally, we provide a rigorous comparison of the equilibrium thresholds under behavioral and risk neutral players in networks with power-law degree distributions.",studi decentr protect strategi human decis maker suscept infect suscept sis epidem network specif examin impact behavior mis percept infect probabl captur prospect theori nash equilibrium strategi two class game first class game node choos cure rate minim steadi state infect probabl degre base mean field approxim plus cost select cure rate establish exist pure nash equilibria risk neutral behavior decis maker per unit cost cure rate suffici high show risk neutral player choos cure rate zero equilibrium cure rate nonzero behavior decis make ani finit cost second class game node choos whether vaccin themselv establish exist uniqu threshold equilibria node degre larger certain threshold vaccin vaccin cost suffici high fewer behavior player vaccin compar risk neutral player vice versa final provid rigor comparison equilibrium threshold behavior risk neutral player network power law degre distribut,"['Ashish R. Hota', 'Shreyas Sundaram']","['cs.GT', 'cs.SY', 'q-fin.EC']",False,False,True,False,False,False
606,2017-03-28T14:08:00Z,2017-03-25T01:37:23Z,http://arxiv.org/abs/1703.08636v1,http://arxiv.org/pdf/1703.08636v1,Informational Substitutes,inform substitut,"We propose definitions of substitutes and complements for pieces of information (""signals"") in the context of a decision or optimization problem, with game-theoretic and algorithmic applications. In a game-theoretic context, substitutes capture diminishing marginal value of information to a rational decision maker. We use the definitions to address the question of how and when information is aggregated in prediction markets. Substitutes characterize ""best-possible"" equilibria with immediate information aggregation, while complements characterize ""worst-possible"", delayed aggregation. Game-theoretic applications also include settings such as crowdsourcing contests and Q\&A forums. In an algorithmic context, where substitutes capture diminishing marginal improvement of information to an optimization problem, substitutes imply efficient approximation algorithms for a very general class of (adaptive) information acquisition problems.   In tandem with these broad applications, we examine the structure and design of informational substitutes and complements. They have equivalent, intuitive definitions from disparate perspectives: submodularity, geometry, and information theory. We also consider the design of scoring rules or optimization problems so as to encourage substitutability or complementarity, with positive and negative results. Taken as a whole, the results give some evidence that, in parallel with substitutable items, informational substitutes play a natural conceptual and formal role in game theory and algorithms.",propos definit substitut complement piec inform signal context decis optim problem game theoret algorithm applic game theoret context substitut captur diminish margin valu inform ration decis maker use definit address question inform aggreg predict market substitut character best possibl equilibria immedi inform aggreg complement character worst possibl delay aggreg game theoret applic also includ set crowdsourc contest forum algorithm context substitut captur diminish margin improv inform optim problem substitut impli effici approxim algorithm veri general class adapt inform acquisit problem tandem broad applic examin structur design inform substitut complement equival intuit definit dispar perspect submodular geometri inform theori also consid design score rule optim problem encourag substitut complementar posit negat result taken whole result give evid parallel substitut item inform substitut play natur conceptu formal role game theori algorithm,"['Yiling Chen', 'Bo Waggoner']",['cs.GT'],False,False,True,False,False,False
607,2017-03-28T14:08:00Z,2017-03-24T21:39:34Z,http://arxiv.org/abs/1703.08607v1,http://arxiv.org/pdf/1703.08607v1,Aversion to Uncertainty and Its Implications for Revenue Maximization,avers uncertainti implic revenu maxim,"We study a model of risk in which the agent undervalues uncertain outcomes. Under our model, an event occurring with probability $x<1$ is worth strictly less than $x$ times the value of the event when it occurs with certainty. This property can be formalized in the form of an uncertainty weighting function. Our model is a special case of models considered under prospect theory, an alternative to the heavily studied expected utility theory in economics. Our goal is to understand the implications of this kind of attitude towards risk on mechanism design.   We specifically examine three aspects of revenue optimal mechanism design as they relate to risk. First, how does risk aversion affect the use of randomization within mechanism design? Second, in dynamic settings where the buyer's lack of information about future values allows the seller to extract more revenue, how does risk aversion affect the ability of the seller to exploit the uncertainty of future events? Finally, is it possible to obtain approximation guarantees for revenue that are robust to the specific risk model?   We present three main results. First, we characterize optimal mechanisms in the single-shot setting as menus of binary lotteries and show that under extreme risk aversion, for any value distribution, almost the entire social welfare can be extracted as revenue. Second, we show that under a reasonable bounded-risk-aversion assumption, posted pricing obtains a constant risk-robust approximation to the optimal revenue. Third, in contrast to this positive result, we show that in dynamic settings it is not possible to achieve any constant factor approximation to revenue in a risk-robust manner.",studi model risk agent undervalu uncertain outcom model event occur probabl worth strict less time valu event occur certainti properti formal form uncertainti weight function model special case model consid prospect theori altern heavili studi expect util theori econom goal understand implic kind attitud toward risk mechan design specif examin three aspect revenu optim mechan design relat risk first doe risk avers affect use random within mechan design second dynam set buyer lack inform futur valu allow seller extract revenu doe risk avers affect abil seller exploit uncertainti futur event final possibl obtain approxim guarante revenu robust specif risk model present three main result first character optim mechan singl shot set menus binari lotteri show extrem risk avers ani valu distribut almost entir social welfar extract revenu second show reason bound risk avers assumpt post price obtain constant risk robust approxim optim revenu third contrast posit result show dynam set possibl achiev ani constant factor approxim revenu risk robust manner,"['Shuchi Chawla', 'Kira Goldner', 'J. Benjamin Miller', 'Emmanouil Pountourakis']",['cs.GT'],False,False,True,False,False,True
608,2017-03-28T14:08:00Z,2017-03-24T17:05:58Z,http://arxiv.org/abs/1703.08509v1,http://arxiv.org/pdf/1703.08509v1,Generalized Nash Equilibrium Problem by the Alternating Direction Method   of Multipliers,general nash equilibrium problem altern direct method multipli,"In this paper, the problem of finding a generalized Nash equilibrium (GNE) of a networked game is studied. Players are only able to choose their decisions from a feasible action set. The feasible set is considered to be a private linear equality constraint that is coupled through decisions of the other players. We consider that each player has his own private constraint and it has not to be shared with the other players. This general case also embodies the one with shared constraints between players and it can be also simply extended to the case with inequality constraints. Since the players don't have access to other players' actions, they need to exchange estimates of others' actions and a local copy of the Lagrangian multiplier with their neighbors over a connected communication graph. We develop a relatively fast algorithm by reformulating the conservative GNE problem within the framework of inexact-ADMM. The convergence of the algorithm is guaranteed under a few mild assumptions on cost functions. Finally, the algorithm is simulated for a wireless ad-hoc network.",paper problem find general nash equilibrium gne network game studi player onli abl choos decis feasibl action set feasibl set consid privat linear equal constraint coupl decis player consid player privat constraint share player general case also embodi one share constraint player also simpli extend case inequ constraint sinc player access player action need exchang estim action local copi lagrangian multipli neighbor connect communic graph develop relat fast algorithm reformul conserv gne problem within framework inexact admm converg algorithm guarante mild assumpt cost function final algorithm simul wireless ad hoc network,"['Farzad Salehisadaghiani', 'Lacra Pavel']","['cs.GT', 'cs.SY']",False,False,True,False,False,True
609,2017-03-28T14:08:00Z,2017-03-24T05:15:19Z,http://arxiv.org/abs/1703.08286v1,http://arxiv.org/pdf/1703.08286v1,Evolutionary Stability of Reputation Management System in Peer to Peer   Networks,evolutionari stabil reput manag system peer peer network,"Each participant in peer-to-peer network prefers to free-ride on the contribution of other participants. Reputation based resource sharing is a way to control the free riding. Instead of classical game theory we use evolutionary game theory to analyse the reputation based resource sharing in peer to peer system. Classical game-theoretical approach requires global information of the population. However, the evolutionary games only assumes light cognitive capabilities of users, that is, each user imitates the behavior of other user with better payoff. We find that without any extra benefit reputation strategy is not stable in the system. We also find the fraction of users who calculate the reputation for controlling the free riding in equilibrium. In this work first we made a game theoretical model for the reputation system and then we calculate the threshold of the fraction of users with which the reputation strategy is sustainable in the system. We found that in simplistic conditions reputation calculation is not evolutionarily stable strategy but if we impose some initial payment to all users and then distribute that payment among the users who are calculating reputation then reputation is evolutionary stable strategy.",particip peer peer network prefer free ride contribut particip reput base resourc share way control free ride instead classic game theori use evolutionari game theori analys reput base resourc share peer peer system classic game theoret approach requir global inform popul howev evolutionari game onli assum light cognit capabl user user imit behavior user better payoff find without ani extra benefit reput strategi stabl system also find fraction user calcul reput control free ride equilibrium work first made game theoret model reput system calcul threshold fraction user reput strategi sustain system found simplist condit reput calcul evolutionarili stabl strategi impos initi payment user distribut payment among user calcul reput reput evolutionari stabl strategi,"['Antriksh Goswami', 'Ruchir Gupta']",['cs.GT'],False,False,True,False,False,False
610,2017-03-28T14:08:03Z,2017-03-23T17:27:51Z,http://arxiv.org/abs/1703.08150v1,http://arxiv.org/pdf/1703.08150v1,Competitive Equilibria with Indivisible Goods and Generic Budgets,competit equilibria indivis good generic budget,"We study competitive equilibria in the basic Fisher market model, but with indivisible goods. Such equilibria fail to exist in the simplest possible market of two players with equal budgets and a single good, yet this is a knife's edge instance as equilibria exist once budgets are not precisely equal. Is non-existence of equilibria also a knife-edge phenomenon in complex markets with multiple goods? Our computerized search has indicated that equilibria often exist when budgets are ""generic"". We prove several existence results both for the case of general preferences and for the special case of additive preferences, and relate competitive equilibria to notions of fair allocation of indivisible items.",studi competit equilibria basic fisher market model indivis good equilibria fail exist simplest possibl market two player equal budget singl good yet knife edg instanc equilibria exist onc budget precis equal non exist equilibria also knife edg phenomenon complex market multipl good computer search indic equilibria often exist budget generic prove sever exist result case general prefer special case addit prefer relat competit equilibria notion fair alloc indivis item,"['Moshe Babaioff', 'Noam Nisan', 'Inbal Talgam-Cohen']",['cs.GT'],False,False,True,False,False,False
611,2017-03-28T14:08:03Z,2017-03-22T15:05:20Z,http://arxiv.org/abs/1703.07695v1,http://arxiv.org/pdf/1703.07695v1,Selfish Cops and Adversarial Robber: Multi-Player Pursuit Evasion on   Graphs,selfish cop adversari robber multi player pursuit evas graph,We introduce and study the game of Selfish Cops and Adversarial Robber (SCAR) which is an N-player generalization of the classic two-player cops and robbers (CR) game. We prove that SCAR has a Nash equilibrium in deterministic strategies.,introduc studi game selfish cop adversari robber scar player general classic two player cop robber cr game prove scar nash equilibrium determinist strategi,"['G. Konstantinidis', 'Ath. Kehagias']","['cs.DM', 'cs.GT']",False,False,True,False,False,True
612,2017-03-28T14:08:03Z,2017-03-22T13:37:23Z,http://arxiv.org/abs/1703.07647v1,http://arxiv.org/pdf/1703.07647v1,Algorithms for Nash and Pareto Equilibria for Resource Allocation in   Multiple Femtocells,algorithm nash pareto equilibria resourc alloc multipl femtocel,"We consider a cellular system with multiple Femtocells operating in a Macrocell. They are sharing a set of communication channels. Each Femtocell has multiple users requiring certain minimum rate guarantees. Each channel has a peak power constraint to limit interference to the Macro Base Station (BS). We formulate the problem of channel allocation and power control at the Femtocells as a noncooperative Game. We develop decentralized algorithms to obtain a Coarse Correlated equilibrium that satisfies the QoS of each user. If the QoS of all the users cannot be satisfied, then we obtain a fair equilibrium. Finally we also provide a decentralized algorithm to reach a Pareto and a Nash Bargaining solution which has a much lower complexity than the algorithm to compute the NE.",consid cellular system multipl femtocel oper macrocel share set communic channel femtocel multipl user requir certain minimum rate guarante channel peak power constraint limit interfer macro base station bs formul problem channel alloc power control femtocel noncoop game develop decentr algorithm obtain coars correl equilibrium satisfi qos user qos user cannot satisfi obtain fair equilibrium final also provid decentr algorithm reach pareto nash bargain solut much lower complex algorithm comput ne,"['V. Udaya Sankar', 'Vinod Sharma']","['cs.GT', 'cs.NI']",False,False,True,False,False,True
613,2017-03-28T14:08:03Z,2017-03-22T02:57:19Z,http://arxiv.org/abs/1703.07499v1,http://arxiv.org/pdf/1703.07499v1,Hardware Trojan Detection Game: A Prospect-Theoretic Approach,hardwar trojan detect game prospect theoret approach,"Outsourcing integrated circuit (IC) manufacturing to offshore foundries has grown exponentially in recent years. Given the critical role of ICs in the control and operation of vehicular systems and other modern engineering designs, such offshore outsourcing has led to serious security threats due to the potential of insertion of hardware trojans - malicious designs that, when activated, can lead to highly detrimental consequences. In this paper, a novel game-theoretic framework is proposed to analyze the interactions between a hardware manufacturer, acting as attacker, and an IC testing facility, acting as defender. The problem is formulated as a noncooperative game in which the attacker must decide on the type of trojan that it inserts while taking into account the detection penalty as well as the damage caused by the trojan. Meanwhile, the resource-constrained defender must decide on the best testing strategy that allows optimizing its overall utility which accounts for both damages and the fines. The proposed game is based on the robust behavioral framework of prospect theory (PT) which allows capturing the potential uncertainty, risk, and irrational behavior in the decision making of both the attacker and defender. For both, the standard rational expected utility (EUT) case and the PT case, a novel algorithm based on fictitious play is proposed and shown to converge to a mixed-strategy Nash equilibrium. For an illustrative case study, thorough analytical results are derived for both EUT and PT to study the properties of the reached equilibrium as well as the impact of key system parameters such as the defender-set fine. Simulation results assess the performance of the proposed framework under both EUT and PT and show that the use of PT will provide invaluable insights on the outcomes of the proposed hardware trojan game, in particular, and system security, in general.",outsourc integr circuit ic manufactur offshor foundri grown exponenti recent year given critic role ic control oper vehicular system modern engin design offshor outsourc led serious secur threat due potenti insert hardwar trojan malici design activ lead high detriment consequ paper novel game theoret framework propos analyz interact hardwar manufactur act attack ic test facil act defend problem formul noncoop game attack must decid type trojan insert take account detect penalti well damag caus trojan meanwhil resourc constrain defend must decid best test strategi allow optim overal util account damag fine propos game base robust behavior framework prospect theori pt allow captur potenti uncertainti risk irrat behavior decis make attack defend standard ration expect util eut case pt case novel algorithm base fictiti play propos shown converg mix strategi nash equilibrium illustr case studi thorough analyt result deriv eut pt studi properti reach equilibrium well impact key system paramet defend set fine simul result assess perform propos framework eut pt show use pt provid invalu insight outcom propos hardwar trojan game particular system secur general,"['Walid Saad', 'Anibal Sanjab', 'Yunpeng Wang', 'Charles Kamhoua', 'Kevin Kwiat']","['cs.IT', 'cs.CR', 'cs.GT', 'math.IT']",False,False,True,False,False,True
614,2017-03-28T14:08:03Z,2017-03-21T03:52:20Z,http://arxiv.org/abs/1703.07043v1,http://arxiv.org/pdf/1703.07043v1,Energy Efficient Power Control for the Two-tier Networks with Small   Cells and Massive MIMO,energi effici power control two tier network small cell massiv mimo,"In this paper, energy efficient power control for the uplink two-tier networks where a macrocell tier with a massive multiple-input multiple-output (MIMO) base station is overlaid with a small cell tier is investigated. We propose a distributed energy efficient power control algorithm which allows each user in the two-tier network taking individual decisions to optimize its own energy efficiency (EE) for the multi-user and multi-cell scenario. The distributed power control algorithm is implemented by decoupling the EE optimization problem into two steps. In the first step, we propose to assign the users on the same resource into the same group and each group can optimize its own EE, respectively. In the second step, multiple power control games based on evolutionary game theory (EGT) are formulated for each group, which allows each user optimizing its own EE. In the EGT-based power control games, each player selects a strategy giving a higher payoff than the average payoff, which can improve the fairness among the users. The proposed algorithm has a linear complexity with respect to the number of subcarriers and the number of cells in comparison with the brute force approach which has an exponential complexity. Simulation results show the remarkable improvements in terms of fairness by using the proposed algorithm.",paper energi effici power control uplink two tier network macrocel tier massiv multipl input multipl output mimo base station overlaid small cell tier investig propos distribut energi effici power control algorithm allow user two tier network take individu decis optim energi effici ee multi user multi cell scenario distribut power control algorithm implement decoupl ee optim problem two step first step propos assign user resourc group group optim ee respect second step multipl power control game base evolutionari game theori egt formul group allow user optim ee egt base power control game player select strategi give higher payoff averag payoff improv fair among user propos algorithm linear complex respect number subcarri number cell comparison brute forc approach exponenti complex simul result show remark improv term fair use propos algorithm,"['Ningning Lu', 'Yanxiang Jiang', 'Fuchun Zheng', 'Xiaohu You']","['cs.NI', 'cs.GT', 'cs.IT', 'math.IT']",False,False,True,False,False,True
615,2017-03-28T14:08:03Z,2017-03-18T23:22:23Z,http://arxiv.org/abs/1703.06367v1,http://arxiv.org/pdf/1703.06367v1,Optimal Learning from Multiple Information Sources,optim learn multipl inform sourc,"Decision-makers often learn by acquiring information from distinct sources that possibly provide complementary information. We consider a decision-maker who sequentially samples from a finite set of Gaussian signals, and wants to predict a persistent multi-dimensional state at an unknown final period. What signal should he choose to observe in each period? Related problems about optimal experimentation and dynamic learning tend to have solutions that can only be approximated or implicitly characterized. In contrast, we find that in our problem, the dynamically optimal path of signal acquisitions generically: (1) eventually coincides at every period with the myopic path of signal acquisitions, and (2) eventually achieves ""total optimality,"" so that at every large period, the decision-maker will not want to revise his previous signal acquisitions, even if given this opportunity. In special classes of environments that we describe, these properties attain not only eventually, but from period 1. Finally, we characterize the asymptotic frequency with which each signal is chosen, and how this depends on primitives of the informational environment.",decis maker often learn acquir inform distinct sourc possibl provid complementari inform consid decis maker sequenti sampl finit set gaussian signal want predict persist multi dimension state unknown final period signal choos observ period relat problem optim experiment dynam learn tend solut onli approxim implicit character contrast find problem dynam optim path signal acquisit generic eventu coincid everi period myopic path signal acquisit eventu achiev total optim everi larg period decis maker want revis previous signal acquisit even given opportun special class environ describ properti attain onli eventu period final character asymptot frequenc signal chosen depend primit inform environ,"['Annie Liang', 'Xiaosheng Mu', 'Vasilis Syrgkanis']","['cs.GT', 'cs.LG', 'math.ST', 'stat.TH']",False,False,True,False,False,False
616,2017-03-28T14:08:03Z,2017-03-17T15:50:41Z,http://arxiv.org/abs/1703.06058v1,http://arxiv.org/pdf/1703.06058v1,Computation Peer Offloading for Energy-Constrained Mobile Edge Computing   in Small-Cell Networks,comput peer offload energi constrain mobil edg comput small cell network,"The (ultra-)dense deployment of small-cell base stations (SBSs) endowed with cloud-like computing functionalities paves the way for pervasive mobile edge computing (MEC), enabling ultra-low latency and location-awareness for a variety of emerging mobile applications and the Internet of Things. To handle spatially uneven computation workloads in the network, cooperation among SBSs via workload peer offloading is essential to avoid large computation latency at overloaded SBSs and provide high quality of service to end users. However, performing effective peer offloading faces many unique challenges in small cell networks due to limited energy resources committed by self-interested SBS owners, uncertainties in the system dynamics and co-provisioning of radio access and computing services. This paper develops a novel online SBS peer offloading framework, called OPEN, by leveraging the Lyapunov technique, in order to maximize the long-term system performance while keeping the energy consumption of SBSs below individual long-term constraints. OPEN works online without requiring information about future system dynamics, yet provides provably near-optimal performance compared to the oracle solution that has the complete future information. In addition, this paper formulates a novel peer offloading game among SBSs, analyzes its equilibrium and efficiency loss in terms of the price of anarchy in order to thoroughly understand SBSs' strategic behaviors, thereby enabling decentralized and autonomous peer offloading decision making. Extensive simulations are carried out and show that peer offloading among SBSs dramatically improves the edge computing performance.",ultra dens deploy small cell base station sbss endow cloud like comput function pave way pervas mobil edg comput mec enabl ultra low latenc locat awar varieti emerg mobil applic internet thing handl spatial uneven comput workload network cooper among sbss via workload peer offload essenti avoid larg comput latenc overload sbss provid high qualiti servic end user howev perform effect peer offload face mani uniqu challeng small cell network due limit energi resourc commit self interest sbs owner uncertainti system dynam co provis radio access comput servic paper develop novel onlin sbs peer offload framework call open leverag lyapunov techniqu order maxim long term system perform keep energi consumpt sbss individu long term constraint open work onlin without requir inform futur system dynam yet provid provabl near optim perform compar oracl solut complet futur inform addit paper formul novel peer offload game among sbss analyz equilibrium effici loss term price anarchi order thorough understand sbss strateg behavior therebi enabl decentr autonom peer offload decis make extens simul carri show peer offload among sbss dramat improv edg comput perform,"['Lixing Chen', 'Sheng Zhou', 'Jie Xu']","['cs.GT', 'cs.DC']",False,False,True,False,False,False
617,2017-03-28T14:08:03Z,2017-03-17T06:03:31Z,http://arxiv.org/abs/1703.05902v1,http://arxiv.org/pdf/1703.05902v1,A Contract-based Incentive Mechanism for Energy Harvesting-based   Internet of Things,contract base incent mechan energi harvest base internet thing,"By enabling wireless devices to be charged wirelessly and remotely, radio frequency energy harvesting (RFEH) has become a promising technology to power the unattended Internet of Things (IoT) low-power devices. To enable this, in future IoT networks, besides the conventional data access points (DAPs) responsible for collecting data from IoT devices, energy access points (EAPs) should be deployed to transfer radio frequency (RF) energy to IoT devices to maintain their sustainable operations. In practice, the DAPs and EAPs may be operated by different operators and a DAP should provide certain incentives to motivate the surrounding EAPs to charge its associated IoT device(s) to assist its data collection. Motivated by this, in this paper we develop a contract theory-based incentive mechanism for the energy trading in RFEH assisted IoT systems. The necessary and sufficient condition for the feasibility of the formulated contract is analyzed. The optimal contract is derived to maximize the DAP's expected utility as well as the social welfare. Simulation results demonstrate the feasibility and effectiveness of the proposed incentive mechanism.",enabl wireless devic charg wireless remot radio frequenc energi harvest rfeh becom promis technolog power unattend internet thing iot low power devic enabl futur iot network besid convent data access point dap respons collect data iot devic energi access point eap deploy transfer radio frequenc rf energi iot devic maintain sustain oper practic dap eap may oper differ oper dap provid certain incent motiv surround eap charg associ iot devic assist data collect motiv paper develop contract theori base incent mechan energi trade rfeh assist iot system necessari suffici condit feasibl formul contract analyz optim contract deriv maxim dap expect util well social welfar simul result demonstr feasibl effect propos incent mechan,"['Zhanwei Hou', 'He Chen', 'Yonghui Li', 'Zhu Han', 'Branka Vucetic']","['cs.GT', 'cs.IT', 'cs.NI', 'math.IT']",False,False,True,False,False,False
618,2017-03-28T14:08:03Z,2017-03-16T14:32:23Z,http://arxiv.org/abs/1703.05641v1,http://arxiv.org/pdf/1703.05641v1,Distributed Mechanism Design with Learning Guarantees,distribut mechan design learn guarante,"In this paper, we consider two common resource allocation problems: sharing $ K $ infinitely divisible resources among strategic agents for their private consumption (private goods problem) and determining the level of a single infinitely divisible common resource which is consumed simultaneously by strategic agents (public goods problem). For each problem, we present a distributed mechanism for a set of agents who communicate through a given network. We prove that the mechanism produces a unique Nash Equilibrium (NE) and it fully implements the social welfare maximizing allocation. In addition, the mechanism is budget-balanced at NE. We also show that the mechanism induces a game with contractive best-response, leading to guaranteed convergence for all learning strategies within the Adaptive Best-Response (ABR) dynamics class. The convergent point is the unique (and efficient) Nash equilibrium.",paper consid two common resourc alloc problem share infinit divis resourc among strateg agent privat consumpt privat good problem determin level singl infinit divis common resourc consum simultan strateg agent public good problem problem present distribut mechan set agent communic given network prove mechan produc uniqu nash equilibrium ne fulli implement social welfar maxim alloc addit mechan budget balanc ne also show mechan induc game contract best respons lead guarante converg learn strategi within adapt best respons abr dynam class converg point uniqu effici nash equilibrium,"['Abhinav Sinha', 'Achilleas Anastasopoulos']",['cs.GT'],False,False,True,False,False,True
624,2017-03-28T14:08:07Z,2017-03-11T05:35:09Z,http://arxiv.org/abs/1703.03912v1,http://arxiv.org/pdf/1703.03912v1,The Curse of Correlation in Security Games and Principle of Max-Entropy,curs correl secur game principl max entropi,"In this paper, we identify and study a fundamental, yet underexplored, phenomenon in security games, which we term the Curse of Correlation (CoC). Specifically, we observe that there is inevitable correlation among the protection status of different targets. Such correlation is a crucial concern, especially in spatio-temporal domains like conservation area patrolling, where attackers can monitor patrollers at certain areas and then infer their patrolling routes using such correlation. To mitigate this issue, we introduce the principle of max-entropy to security games, and focus on designing entropy-maximizing defending strategies for the spatio-temporal security game -- a major victim of CoC. We prove that the problem is #P-hard in general, but propose efficient algorithms in well-motivated special settings. Our experiments show significant advantages of the max-entropy algorithms against previous algorithms.",paper identifi studi fundament yet underexplor phenomenon secur game term curs correl coc specif observ inevit correl among protect status differ target correl crucial concern especi spatio tempor domain like conserv area patrol attack monitor patrol certain area infer patrol rout use correl mitig issu introduc principl max entropi secur game focus design entropi maxim defend strategi spatio tempor secur game major victim coc prove problem hard general propos effici algorithm well motiv special set experi show signific advantag max entropi algorithm previous algorithm,"['Haifeng Xu', 'Milind Tambe', 'Shaddin Dughmi', 'Venil Loyd Noronha']","['cs.GT', 'cs.AI', 'cs.CR']",False,False,True,False,False,True
625,2017-03-28T14:08:08Z,2017-03-10T21:31:32Z,http://arxiv.org/abs/1703.03846v1,http://arxiv.org/pdf/1703.03846v1,Socially Optimal Mining Pools,social optim mine pool,"Mining for Bitcoins is a high-risk high-reward activity. Miners, seeking to reduce their variance and earn steadier rewards, collaborate in pooling strategies where they jointly mine for Bitcoins. Whenever some pool participant is successful, the earned rewards are appropriately split among all pool participants. Currently a dozen of different pooling strategies (i.e., methods for distributing the rewards) are in use for Bitcoin mining.   We here propose a formal model of utility and social welfare for Bitcoin mining (and analogous mining systems) based on the theory of discounted expected utility, and next study pooling strategies that maximize the social welfare of miners. Our main result shows that one of the pooling strategies actually employed in practice--the so-called geometric pay pool--achieves the optimal steady-state utility for miners when its parameters are set appropriately.   Our results apply not only to Bitcoin mining pools, but any other form of pooled mining or crowdsourcing computations where the participants engage in repeated random trials towards a common goal, and where ""partial"" solutions can be efficiently verified.",mine bitcoin high risk high reward activ miner seek reduc varianc earn steadier reward collabor pool strategi joint mine bitcoin whenev pool particip success earn reward appropri split among pool particip current dozen differ pool strategi method distribut reward use bitcoin mine propos formal model util social welfar bitcoin mine analog mine system base theori discount expect util next studi pool strategi maxim social welfar miner main result show one pool strategi actual employ practic call geometr pay pool achiev optim steadi state util miner paramet set appropri result appli onli bitcoin mine pool ani form pool mine crowdsourc comput particip engag repeat random trial toward common goal partial solut effici verifi,"['Ben A. Fisch', 'Rafael Pass', 'Abhi Shelat']",['cs.GT'],False,False,True,False,False,True
626,2017-03-28T14:08:08Z,2017-03-10T16:10:09Z,http://arxiv.org/abs/1703.03741v1,http://arxiv.org/pdf/1703.03741v1,Opinion-Based Centrality in Multiplex Networks: A Convex Optimization   Approach,opinion base central multiplex network convex optim approach,"Most people simultaneously belong to several distinct social networks, in which their relations can be different. They have opinions about certain topics, which they share and spread on these networks, and are influenced by the opinions of other persons. In this paper, we build upon this observation to propose a new nodal centrality measure for multiplex networks. Our measure, called Opinion centrality, is based on a stochastic model representing opinion propagation dynamics in such a network. We formulate an optimization problem consisting in maximizing the opinion of the whole network when controlling an external influence able to affect each node individually. We find a mathematical closed form of this problem, and use its solution to derive our centrality measure. According to the opinion centrality, the more a node is worth investing external influence, and the more it is central. We perform an empirical study of the proposed centrality over a toy network, as well as a collection of real-world networks. Our measure is generally negatively correlated with existing multiplex centrality measures, and highlights different types of nodes, accordingly to its definition.",peopl simultan belong sever distinct social network relat differ opinion certain topic share spread network influenc opinion person paper build upon observ propos new nodal central measur multiplex network measur call opinion central base stochast model repres opinion propag dynam network formul optim problem consist maxim opinion whole network control extern influenc abl affect node individu find mathemat close form problem use solut deriv central measur accord opinion central node worth invest extern influenc central perform empir studi propos central toy network well collect real world network measur general negat correl exist multiplex central measur highlight differ type node accord definit,"['Alexandre Reiffers-Masson', 'Vincent Labatut']","['cs.SI', 'cs.GT', 'physics.soc-ph']",False,False,True,False,False,False
627,2017-03-28T14:08:08Z,2017-03-10T13:54:29Z,http://arxiv.org/abs/1703.03687v1,http://arxiv.org/pdf/1703.03687v1,Best Laid Plans of Lions and Men,best laid plan lion men,"We answer the following question dating back to J.E. Littlewood (1885 - 1977): Can two lions catch a man in a bounded area with rectifiable lakes? The lions and the man are all assumed to be points moving with at most unit speed. That the lakes are rectifiable means that their boundaries are finitely long. This requirement is to avoid pathological examples where the man survives forever because any path to the lions is infinitely long. We show that the answer to the question is not always ""yes"" by giving an example of a region $R$ in the plane where the man has a strategy to survive forever. $R$ is a polygonal region with holes and the exterior and interior boundaries are pairwise disjoint, simple polygons. Our construction is the first truly two-dimensional example where the man can survive.   Next, we consider the following game played on the entire plane instead of a bounded area: There is any finite number of unit speed lions and one fast man who can run with speed $1+\varepsilon$ for some value $\varepsilon>0$. Can the man always survive? We answer the question in the affirmative for any constant $\varepsilon>0$.",answer follow question date back littlewood two lion catch man bound area rectifi lake lion man assum point move unit speed lake rectifi mean boundari finit long requir avoid patholog exampl man surviv forev becaus ani path lion infinit long show answer question alway yes give exampl region plane man strategi surviv forev polygon region hole exterior interior boundari pairwis disjoint simpl polygon construct first truli two dimension exampl man surviv next consid follow game play entir plane instead bound area ani finit number unit speed lion one fast man run speed varepsilon valu varepsilon man alway surviv answer question affirm ani constant varepsilon,"['Mikkel Abrahamsen', 'Jacob Holm', 'Eva Rotenberg', 'Christian Wulff-Nilsen']","['cs.CG', 'cs.GT']",False,False,True,False,False,True
628,2017-03-28T14:08:08Z,2017-03-10T01:47:23Z,http://arxiv.org/abs/1703.03511v1,http://arxiv.org/pdf/1703.03511v1,Towards Computing Victory Margins in STV Elections,toward comput victori margin stv elect,"The Single Transferable Vote (STV) is a system of preferential voting employed in multi-seat elections. Each vote cast by a voter is a (potentially partial) ranking over a set of candidates. No techniques currently exist for computing the margin of victory (MOV) in STV elections. The MOV is the smallest number of vote manipulations (changes, additions, and deletions) required to bring about a change in the set of elected candidates. Knowledge of the MOV of an election gives greater insight into both how much time and money should be spent on the auditing of the election, and whether uncovered mistakes (such as ballot box losses) throw the election result into doubt---requiring a costly repeat election---or can be safely ignored. In this paper, we present algorithms for computing lower and upper bounds on the MOV in STV elections. In small instances, these algorithms are able to compute exact margins.",singl transfer vote stv system preferenti vote employ multi seat elect vote cast voter potenti partial rank set candid techniqu current exist comput margin victori mov stv elect mov smallest number vote manipul chang addit delet requir bring chang set elect candid knowledg mov elect give greater insight much time money spent audit elect whether uncov mistak ballot box loss throw elect result doubt requir cost repeat elect safe ignor paper present algorithm comput lower upper bound mov stv elect small instanc algorithm abl comput exact margin,"['Michelle Blom', 'Peter J. Stuckey', 'Vanessa J. Teague']",['cs.GT'],False,False,True,False,False,True
630,2017-03-28T14:08:12Z,2017-03-09T13:45:45Z,http://arxiv.org/abs/1703.03262v1,http://arxiv.org/pdf/1703.03262v1,Does Nash Envy Immunity,doe nash envi immun,"The most popular stability notion in games should be Nash equilibrium under the rationality of players who maximize their own payoff individually. In contrast, in many scenarios, players can be (partly) irrational with some unpredictable factors. Hence a strategy profile can be more robust if it is resilient against certain irrational behaviors. In this paper, we propose a stability notion that is resilient against envy. A strategy profile is said to be envy-proof if each player cannot gain a competitive edge with respect to the change in utility over the other players by deviation. Together with Nash equilibrium and another stability notion called immunity, we show how these separate notions are related to each other, whether they exist in games, and whether and when a strategy profile satisfying these notions can be efficiently found. We answer these questions by starting with the general two player game and extend the discussion for the approximate stability and for the corresponding fault-tolerance notions in multi-player games.",popular stabil notion game nash equilibrium ration player maxim payoff individu contrast mani scenario player part irrat unpredict factor henc strategi profil robust resili certain irrat behavior paper propos stabil notion resili envi strategi profil said envi proof player cannot gain competit edg respect chang util player deviat togeth nash equilibrium anoth stabil notion call immun show separ notion relat whether exist game whether strategi profil satisfi notion effici found answer question start general two player game extend discuss approxim stabil correspond fault toler notion multi player game,['Ching-Hua Yu'],"['cs.GT', 'cs.CC', 'cs.CR', 'I.2.1; J.4; K.6.0; C.4']",False,False,True,False,False,False
632,2017-03-28T14:08:12Z,2017-03-08T14:41:30Z,http://arxiv.org/abs/1703.02851v1,http://arxiv.org/pdf/1703.02851v1,On the Importance of Correlations in Rational Choice: A Case for   Non-Nashian Game Theory,import correl ration choic case non nashian game theori,"The Nash equilibrium paradigm, and Rational Choice Theory in general, rely on agents acting independently from each other. This note shows how this assumption is crucial in the definition of Rational Choice Theory. It explains how a consistent Alternate Rational Choice Theory, as suggested by Jean-Pierre Dupuy, can be built on the exact opposite assumption, and how it provides a viable account for alternate, actually observed behavior of rational agents that is based on correlations between their decisions.   The end goal of this note is three-fold: (i) to motivate that the Perfect Prediction Equilibrium, implementing Dupuy's notion of projected time and previously called ""projected equilibrium"", is a reasonable approach in certain real situations and a meaningful complement to the Nash paradigm, (ii) to summarize common misconceptions about this equilibrium, and (iii) to give a concise motivation for future research on non-Nashian game theory.",nash equilibrium paradigm ration choic theori general reli agent act independ note show assumpt crucial definit ration choic theori explain consist altern ration choic theori suggest jean pierr dupuy built exact opposit assumpt provid viabl account altern actual observ behavior ration agent base correl decis end goal note three fold motiv perfect predict equilibrium implement dupuy notion project time previous call project equilibrium reason approach certain real situat meaning complement nash paradigm ii summar common misconcept equilibrium iii give concis motiv futur research non nashian game theori,['Ghislain Fourny'],"['cs.GT', '91A35', 'J.4']",False,False,True,False,False,False
633,2017-03-28T14:08:12Z,2017-03-07T19:33:50Z,http://arxiv.org/abs/1703.02567v1,http://arxiv.org/pdf/1703.02567v1,Online Learning of Optimal Bidding Strategy in Repeated Multi-Commodity   Auctions,onlin learn optim bid strategi repeat multi commod auction,"We study the online learning problem of a bidder who participates in repeated auctions. With the goal of maximizing his total T-period payoff, the bidder wants to determine the optimal allocation of his fixed budget among his bids for $K$ different goods at each period. As a bidding strategy, we propose a polynomial time algorithm, referred to as dynamic programming on discrete set (DPDS), which is inspired by the dynamic programming approach to Knapsack problems. We show that DPDS achieves the regret order of $O(\sqrt{T\log{T}})$. Also, by showing that the regret growth rate is lower bounded by $\Omega(\sqrt{T})$ for any bidding strategy, we conclude that DPDS algorithm is order optimal up to a $\sqrt{\log{T}}$ term. We also evaluate the performance of DPDS empirically in the context of virtual bidding in wholesale electricity markets by using historical data from the New York energy market.",studi onlin learn problem bidder particip repeat auction goal maxim total period payoff bidder want determin optim alloc fix budget among bid differ good period bid strategi propos polynomi time algorithm refer dynam program discret set dpds inspir dynam program approach knapsack problem show dpds achiev regret order sqrt log also show regret growth rate lower bound omega sqrt ani bid strategi conclud dpds algorithm order optim sqrt log term also evalu perform dpds empir context virtual bid wholesal electr market use histor data new york energi market,"['Sevi Baltaoglu', 'Lang Tong', 'Qing Zhao']","['cs.GT', 'cs.LG']",False,False,True,False,False,False
634,2017-03-28T14:08:12Z,2017-03-06T16:35:31Z,http://arxiv.org/abs/1703.01957v1,http://arxiv.org/pdf/1703.01957v1,Solving Two-Player Zero-Sum Repeated Bayesian Games,solv two player zero sum repeat bayesian game,"This paper studies two-player zero-sum repeated Bayesian games in which every player has a private type that is unknown to the other player, and the initial probability of the type of every player is publicly known. The types of players are independently chosen according to the initial probabilities, and are kept the same all through the game. At every stage, players simultaneously choose actions, and announce their actions publicly. For finite horizon cases, an explicit linear program is provided to compute players' security strategies. Moreover, based on the existing results in [1], this paper shows that a player's sufficient statistics, which is independent of the strategy of the other player, consists of the belief over the player's own type, the regret with respect to the other player's type, and the stage. Explicit linear programs are provided to compute the initial regrets, and the security strategies that only depends on the sufficient statistics. For discounted cases, following the same idea in the finite horizon, this paper shows that a player's sufficient statistics consists of the belief of the player's own type and the anti-discounted regret with respect to the other player's type. Besides, an approximated security strategy depending on the sufficient statistics is provided, and an explicit linear program to compute the approximated security strategy is given. This paper also obtains a bound on the performance difference between the approximated security strategy and the security strategy.",paper studi two player zero sum repeat bayesian game everi player privat type unknown player initi probabl type everi player public known type player independ chosen accord initi probabl kept game everi stage player simultan choos action announc action public finit horizon case explicit linear program provid comput player secur strategi moreov base exist result paper show player suffici statist independ strategi player consist belief player type regret respect player type stage explicit linear program provid comput initi regret secur strategi onli depend suffici statist discount case follow idea finit horizon paper show player suffici statist consist belief player type anti discount regret respect player type besid approxim secur strategi depend suffici statist provid explicit linear program comput approxim secur strategi given paper also obtain bound perform differ approxim secur strategi secur strategi,"['Lichun Li', 'Cedric Langbort', 'Jeff Shamma']",['cs.GT'],False,False,True,False,False,False
635,2017-03-28T14:08:12Z,2017-03-06T16:23:50Z,http://arxiv.org/abs/1703.01952v1,http://arxiv.org/pdf/1703.01952v1,Efficient Strategy Computation in Zero-Sum Asymmetric Repeated Games,effici strategi comput zero sum asymmetr repeat game,"Zero-sum asymmetric games model decision making scenarios involving two competing players who have different information about the game being played. A particular case is that of nested information, where one (informed) player has superior information over the other (uninformed) player. This paper considers the case of nested information in repeated zero-sum games and studies the computation of strategies for both the informed and uninformed players for finite-horizon and discounted infinite-horizon nested information games. For finite-horizon settings, we exploit that for both players, the security strategy, i.e. Nash equilibrium, and also the opponent's corresponding best response depend only on the informed player's history of actions. Using this property, we refine the sequence form, and formulate an LP computation of player strategies that is linear in the size of the uninformed player's action set. For the infinite-horizon discounted game, we construct LP formulations to compute the approximated security strategies for both players, and provide a bound on the performance difference between the suboptimal strategies and the security strategies. Finally, we illustrate the results on a network interdiction game between an informed system administrator and uniformed intruder.",zero sum asymmetr game model decis make scenario involv two compet player differ inform game play particular case nest inform one inform player superior inform uninform player paper consid case nest inform repeat zero sum game studi comput strategi inform uninform player finit horizon discount infinit horizon nest inform game finit horizon set exploit player secur strategi nash equilibrium also oppon correspond best respons depend onli inform player histori action use properti refin sequenc form formul lp comput player strategi linear size uninform player action set infinit horizon discount game construct lp formul comput approxim secur strategi player provid bound perform differ suboptim strategi secur strategi final illustr result network interdict game inform system administr uniform intrud,"['Lichun Li', 'Jeff S. Shamma']",['cs.GT'],False,False,True,False,False,False
638,2017-03-28T14:08:12Z,2017-03-07T13:05:15Z,http://arxiv.org/abs/1703.01599v2,http://arxiv.org/pdf/1703.01599v2,How bad is selfish routing in practice?,bad selfish rout practic,"Routing games are one of the most successful domains of application of game theory. It is well understood that simple dynamics converge to equilibria, whose performance is nearly optimal regardless of the size of the network or the number of agents. These strong theoretical assertions prompt a natural question: How well do these pen-and-paper calculations agree with the reality of everyday traffic routing? We focus on a semantically rich dataset from Singapore's National Science Experiment that captures detailed information about the daily behavior of thousands of Singaporean students. Using this dataset, we can identify the routes as well as the modes of transportation used by the students, e.g. car (driving or being driven to school) versus bus or metro, estimate source and sink destinations (home-school) and trip duration, as well as their mode-dependent available routes. We quantify both the system and individual optimality. Our estimate of the Empirical Price of Anarchy lies between 1.11 and 1.22. Individually, the typical behavior is consistent from day to day and nearly optimal, with low regret for not deviating to alternative paths.",rout game one success domain applic game theori well understood simpl dynam converg equilibria whose perform near optim regardless size network number agent strong theoret assert prompt natur question well pen paper calcul agre realiti everyday traffic rout focus semant rich dataset singapor nation scienc experi captur detail inform daili behavior thousand singaporean student use dataset identifi rout well mode transport use student car drive driven school versus bus metro estim sourc sink destin home school trip durat well mode depend avail rout quantifi system individu optim estim empir price anarchi lie individu typic behavior consist day day near optim low regret deviat altern path,"['Barnabé Monnot', 'Francisco Benita', 'Georgios Piliouras']",['cs.GT'],False,False,True,False,False,True
640,2017-03-28T14:08:16Z,2017-03-04T02:12:37Z,http://arxiv.org/abs/1703.01380v1,http://arxiv.org/pdf/1703.01380v1,Internalization of Externalities in Interdependent Security: Large   Network Cases,intern extern interdepend secur larg network case,"With increasing connectivity among comprising agents or (sub-)systems in large, complex systems, there is a growing interest in understanding interdependent security and dealing with inefficiency in security investments. Making use of a population game model and the well-known Chung-Lu random graph model, we study how one could encourage selfish agents to invest more in security by internalizing the externalities produced by their security investments.   To this end, we first establish an interesting relation between the local minimizers of social cost and the Nash equilibria of a population game with slightly altered costs. Secondly, under a mild technical assumption, we demonstrate that there exists a unique minimizer of social cost and it coincides with the unique Nash equilibrium of the population game. This finding tells us how to modify the private cost functions of selfish agents in order to enhance the overall security and reduce social cost. In addition, it reveals how the sensitivity of overall security to security investments of agents influences their externalities and, consequently, penalties or taxes that should be imposed for internalization of externalities. Finally, we illustrate how the degree distribution of agents influences their security investments and overall security at both the NEs of population games and social optima.",increas connect among compris agent sub system larg complex system grow interest understand interdepend secur deal ineffici secur invest make use popul game model well known chung lu random graph model studi one could encourag selfish agent invest secur intern extern produc secur invest end first establish interest relat local minim social cost nash equilibria popul game slight alter cost second mild technic assumpt demonstr exist uniqu minim social cost coincid uniqu nash equilibrium popul game find tell us modifi privat cost function selfish agent order enhanc overal secur reduc social cost addit reveal sensit overal secur secur invest agent influenc extern consequ penalti tax impos intern extern final illustr degre distribut agent influenc secur invest overal secur nes popul game social optima,['Richard J. La'],"['cs.SI', 'cs.GT']",False,False,True,False,False,False
643,2017-03-28T14:08:16Z,2017-03-17T22:49:39Z,http://arxiv.org/abs/1703.00972v3,http://arxiv.org/pdf/1703.00972v3,Eliciting Private User Information for Residential Demand Response,elicit privat user inform residenti demand respons,"Residential Demand Response has emerged as a viable tool to alleviate supply and demand imbalances of electricity, particularly during times when the electric grid is strained due a shortage of supply. Demand Response providers bid reduction capacity into the wholesale electricity market by asking their customers under contract to temporarily reduce their consumption in exchange for a monetary incentive. To contribute to the analysis of consumer behavior in response to such incentives, this paper formulates Demand Response as a Mechanism Design problem, where a Demand Response Provider elicits private information of its rational, profit-maximizing customers who derive positive expected utility by participating in reduction events. By designing an incentive compatible and individually rational mechanism to collect users' price elasticities of demand, the Demand Response provider can target the most susceptible users to incentives. We measure reductions by comparing the materialized consumption to the projected consumption, which we model as the ""10-in-10""-baseline, the regulatory standard set by the California Independent System Operator. Due to the suboptimal performance of this baseline, we show, using consumption data of residential customers in California, that Demand Response Providers receive payments for ""virtual reductions"", which exist due to the inaccuracies of the baseline rather than actual reductions. Improving the accuracy of the baseline diminishes the contribution of these virtual reductions.",residenti demand respons emerg viabl tool allevi suppli demand imbal electr particular dure time electr grid strain due shortag suppli demand respons provid bid reduct capac wholesal electr market ask custom contract temporarili reduc consumpt exchang monetari incent contribut analysi consum behavior respons incent paper formul demand respons mechan design problem demand respons provid elicit privat inform ration profit maxim custom deriv posit expect util particip reduct event design incent compat individu ration mechan collect user price elast demand demand respons provid target suscept user incent measur reduct compar materi consumpt project consumpt model baselin regulatori standard set california independ system oper due suboptim perform baselin show use consumpt data residenti custom california demand respons provid receiv payment virtual reduct exist due inaccuraci baselin rather actual reduct improv accuraci baselin diminish contribut virtual reduct,"['Datong P. Zhou', 'Munther A. Dahleh', 'Claire J. Tomlin']",['cs.GT'],False,False,True,False,False,False
644,2017-03-28T14:08:16Z,2017-03-02T19:35:48Z,http://arxiv.org/abs/1703.00927v1,http://arxiv.org/pdf/1703.00927v1,On the asymptotic behavior of the price of anarchy: Is selfish routing   bad in highly congested networks?,asymptot behavior price anarchi selfish rout bad high congest network,"This paper examines the asymptotic behavior of the price of anarchy as a function of the total traffic inflow in nonatomic congestion games with multiple origin-destination pairs. We first show that the price of anarchy may remain bounded away from 1, even in simple three-link parallel networks with convex cost functions. On the other hand, empirical studies show that the price of anarchy is close to 1 in highly congested real-world networks, thus begging the question: under what assumptions can this behavior be justified analytically? To that end, we prove a general result showing that for a large class of cost functions (defined in terms of regular variation and including all polynomials), the price of anarchy converges to 1 in the high congestion limit. In particular, specializing to networks with polynomial costs, we show that this convergence follows a power law whose degree can be computed explicitly.",paper examin asymptot behavior price anarchi function total traffic inflow nonatom congest game multipl origin destin pair first show price anarchi may remain bound away even simpl three link parallel network convex cost function hand empir studi show price anarchi close high congest real world network thus beg question assumpt behavior justifi analyt end prove general result show larg class cost function defin term regular variat includ polynomi price anarchi converg high congest limit particular special network polynomi cost show converg follow power law whose degre comput explicit,"['Riccardo Colini Baldeschi', 'Roberto Cominetti', 'Panayotis Mertikopoulos', 'Marco Scarsini']","['cs.GT', 'math.OC', 'Primary 91A13, secondary 91A43']",False,False,True,False,False,False
646,2017-03-28T14:08:16Z,2017-03-02T05:36:16Z,http://arxiv.org/abs/1703.00632v1,http://arxiv.org/pdf/1703.00632v1,"A Dominant Strategy Truthful, Deterministic Multi-Armed Bandit Mechanism   with Logarithmic Regret",domin strategi truth determinist multi arm bandit mechan logarithm regret,"Stochastic multi-armed bandit (MAB) mechanisms are widely used in sponsored search auctions, crowdsourcing, online procurement, etc. Existing stochastic MAB mechanisms with a deterministic payment rule, proposed in the literature, necessarily suffer a regret of $\Omega(T^{2/3})$, where $T$ is the number of time steps. This happens because the existing mechanisms consider the worst case scenario where the means of the agents' stochastic rewards are separated by a very small amount that depends on $T$. We make, and, exploit the crucial observation that in most scenarios, the separation between the agents' rewards is rarely a function of $T$. Moreover, in the case that the rewards of the arms are arbitrarily close, the regret contributed by such sub-optimal arms is minimal. Our idea is to allow the center to indicate the resolution, $\Delta$, with which the agents must be distinguished. This immediately leads us to introduce the notion of $\Delta$-Regret. Using sponsored search auctions as a concrete example (the same idea applies for other applications as well), we propose a dominant strategy incentive compatible (DSIC) and individually rational (IR), deterministic MAB mechanism, based on ideas from the Upper Confidence Bound (UCB) family of MAB algorithms. Remarkably, the proposed mechanism $\Delta$-UCB achieves a $\Delta$-regret of $O(\log T)$ for the case of sponsored search auctions. We first establish the results for single slot sponsored search auctions and then non-trivially extend the results to the case where multiple slots are to be allocated.",stochast multi arm bandit mab mechan wide use sponsor search auction crowdsourc onlin procur etc exist stochast mab mechan determinist payment rule propos literatur necessarili suffer regret omega number time step happen becaus exist mechan consid worst case scenario mean agent stochast reward separ veri small amount depend make exploit crucial observ scenario separ agent reward rare function moreov case reward arm arbitrarili close regret contribut sub optim arm minim idea allow center indic resolut delta agent must distinguish immedi lead us introduc notion delta regret use sponsor search auction concret exampl idea appli applic well propos domin strategi incent compat dsic individu ration ir determinist mab mechan base idea upper confid bound ucb famili mab algorithm remark propos mechan delta ucb achiev delta regret log case sponsor search auction first establish result singl slot sponsor search auction non trivial extend result case multipl slot alloc,"['Divya Padmanabhan', 'Satyanath Bhat', 'Prabuchandran K. J.', 'Shirish Shevade', 'Y. Narahari']",['cs.GT'],False,False,True,False,False,True
648,2017-03-28T14:08:16Z,2017-03-01T14:42:20Z,http://arxiv.org/abs/1703.00320v1,http://arxiv.org/pdf/1703.00320v1,Investigating the Characteristics of One-Sided Matching Mechanisms Under   Various Preferences and Risk Attitudes,investig characterist one side match mechan various prefer risk attitud,"One-sided matching mechanisms are fundamental for assigning a set of indivisible objects to a set of self-interested agents when monetary transfers are not allowed. Two widely-studied randomized mechanisms in multiagent settings are the Random Serial Dictatorship (RSD) and the Probabilistic Serial Rule (PS). Both mechanisms require only that agents specify ordinal preferences and have a number of desirable economic and computational properties. However, the induced outcomes of the mechanisms are often incomparable and thus there are challenges when it comes to deciding which mechanism to adopt in practice. In this paper, we first consider the space of general ordinal preferences and provide empirical results on the (in)comparability of RSD and PS. We analyze their respective economic properties under general and lexicographic preferences. We then instantiate utility functions with the goal of gaining insights on the manipulability, efficiency, and envyfreeness of the mechanisms under different risk-attitude models. Our results hold under various preference distribution models, which further confirm the broad use of RSD in most practical applications.",one side match mechan fundament assign set indivis object set self interest agent monetari transfer allow two wide studi random mechan multiag set random serial dictatorship rsd probabilist serial rule ps mechan requir onli agent specifi ordin prefer number desir econom comput properti howev induc outcom mechan often incompar thus challeng come decid mechan adopt practic paper first consid space general ordin prefer provid empir result compar rsd ps analyz respect econom properti general lexicograph prefer instanti util function goal gain insight manipul effici envyfre mechan differ risk attitud model result hold various prefer distribut model confirm broad use rsd practic applic,"['Hadi Hosseini', 'Kate Larson', 'Robin Cohen']","['cs.GT', 'cs.AI', 'cs.MA', 'I.2.11; J.4']",False,False,True,False,False,True
649,2017-03-28T14:08:16Z,2017-03-01T10:30:36Z,http://arxiv.org/abs/1703.00216v1,http://arxiv.org/pdf/1703.00216v1,Congestion-Aware Distributed Network Selection for Integrated Cellular   and Wi-Fi Networks,congest awar distribut network select integr cellular wi fi network,"Intelligent network selection plays an important role in achieving an effective data offloading in the integrated cellular and Wi-Fi networks. However, previously proposed network selection schemes mainly focused on offloading as much data traffic to Wi-Fi as possible, without systematically considering the Wi-Fi network congestion and the ping-pong effect, both of which may lead to a poor overall user quality of experience. Thus, in this paper, we study a more practical network selection problem by considering both the impacts of the network congestion and switching penalties. More specifically, we formulate the users' interactions as a Bayesian network selection game (NSG) under the incomplete information of the users' mobilities. We prove that it is a Bayesian potential game and show the existence of a pure Bayesian Nash equilibrium that can be easily reached. We then propose a distributed network selection (DNS) algorithm based on the network congestion statistics obtained from the operator. Furthermore, we show that computing the optimal centralized network allocation is an NP-hard problem, which further justifies our distributed approach. Simulation results show that the DNS algorithm achieves the highest user utility and a good fairness among users, as compared with the on-the-spot offloading and cellular-only benchmark schemes.",intellig network select play import role achiev effect data offload integr cellular wi fi network howev previous propos network select scheme main focus offload much data traffic wi fi possibl without systemat consid wi fi network congest ping pong effect may lead poor overal user qualiti experi thus paper studi practic network select problem consid impact network congest switch penalti specif formul user interact bayesian network select game nsg incomplet inform user mobil prove bayesian potenti game show exist pure bayesian nash equilibrium easili reach propos distribut network select dns algorithm base network congest statist obtain oper furthermor show comput optim central network alloc np hard problem justifi distribut approach simul result show dns algorithm achiev highest user util good fair among user compar spot offload cellular onli benchmark scheme,"['Man Hon Cheung', 'Fen Hou', 'Jianwei Huang', 'Richard Southwell']","['cs.NI', 'cs.GT']",False,False,True,False,False,False
651,2017-03-28T14:08:19Z,2017-02-28T14:16:00Z,http://arxiv.org/abs/1702.08794v1,http://arxiv.org/pdf/1702.08794v1,Lowest Unique Bid Auctions with Resubmission Opportunities,lowest uniqu bid auction resubmiss opportun,"The recent online platforms propose multiple items for bidding. The state of the art, however, is limited to the analysis of one item auction without resubmission. In this paper we study multi-item lowest unique bid auctions (LUBA) with resubmission in discrete bid spaces under budget constraints. We show that the game does not have pure Bayes-Nash equilibria (except in very special cases). However, at least one mixed Bayes-Nash equilibria exists for arbitrary number of bidders and items. The equilibrium is explicitly computed for two-bidder setup with resubmission possibilities. In the general setting we propose a distributed strategic learning algorithm to approximate equilibria. Computer simulations indicate that the error quickly decays in few number of steps. When the number of bidders per item follows a Poisson distribution, it is shown that the seller can get a non-negligible revenue on several items, and hence making a partial revelation of the true value of the items. Finally, the attitude of the bidders towards the risk is considered. In contrast to risk-neutral agents who bids very small values, the cumulative distribution and the bidding support of risk-sensitive agents are more distributed.",recent onlin platform propos multipl item bid state art howev limit analysi one item auction without resubmiss paper studi multi item lowest uniqu bid auction luba resubmiss discret bid space budget constraint show game doe pure bay nash equilibria except veri special case howev least one mix bay nash equilibria exist arbitrari number bidder item equilibrium explicit comput two bidder setup resubmiss possibl general set propos distribut strateg learn algorithm approxim equilibria comput simul indic error quick decay number step number bidder per item follow poisson distribut shown seller get non neglig revenu sever item henc make partial revel true valu item final attitud bidder toward risk consid contrast risk neutral agent bid veri small valu cumul distribut bid support risk sensit agent distribut,"['Yida Xu', 'Hamidou Tembine']",['cs.GT'],False,False,True,False,False,False
652,2017-03-28T14:08:19Z,2017-02-28T13:56:57Z,http://arxiv.org/abs/1702.08789v1,http://arxiv.org/pdf/1702.08789v1,Nash and Wardrop equilibria in aggregative games with coupling   constraints,nash wardrop equilibria aggreg game coupl constraint,"We consider the framework of aggregative games, in which the cost function of each agent depends on his own strategy and on the average population strategy. As first contribution, we investigate the relations between the concepts of Nash and Wardrop equilibrium. By exploiting a characterization of the two equilibria as solutions of variational inequalities, we bound their distance with a decreasing function of the population size. As second contribution, we propose two decentralized algorithms that converge to such equilibria and are capable of coping with constraints coupling the strategies of different agents. Finally, we study the applications of charging of electric vehicles and of route choice on a road network.",consid framework aggreg game cost function agent depend strategi averag popul strategi first contribut investig relat concept nash wardrop equilibrium exploit character two equilibria solut variat inequ bound distanc decreas function popul size second contribut propos two decentr algorithm converg equilibria capabl cope constraint coupl strategi differ agent final studi applic charg electr vehicl rout choic road network,"['Basilio Gentile', 'Francesca Parise', 'Dario Paccagnan', 'Maryam Kamgarpour', 'John Lygeros']","['cs.SY', 'cs.GT', 'math.OC']",False,False,True,False,False,True
653,2017-03-28T14:08:19Z,2017-02-27T21:13:57Z,http://arxiv.org/abs/1702.08533v1,http://arxiv.org/pdf/1702.08533v1,Competing Bandits: Learning under Competition,compet bandit learn competit,"Most modern systems strive to learn from interactions with users, and many engage in \emph{exploration}: making potentially suboptimal choices for the sake of acquiring new information. We initiate a study of the interplay between \emph{exploration and competition}---how such systems balance the exploration for learning and the competition for users. Here the users play three distinct roles: they are customers that generate revenue, they are sources of data for learning, and they are self-interested agents which choose among the competing systems.   As a model, we consider competition between two multi-armed bandit algorithms faced with the same bandit instance. Users arrive one by one and choose among the two algorithms, so that each algorithm makes progress if and only if it is chosen. We ask whether and to which extent competition incentivizes \emph{innovation}: adoption of better algorithms. We investigate this issue for several models of user response, as we vary the degree of rationality and competitiveness in the model. Effectively, we map out the ""competition vs. innovation"" relationship, a well-studied theme in economics.",modern system strive learn interact user mani engag emph explor make potenti suboptim choic sake acquir new inform initi studi interplay emph explor competit system balanc explor learn competit user user play three distinct role custom generat revenu sourc data learn self interest agent choos among compet system model consid competit two multi arm bandit algorithm face bandit instanc user arriv one one choos among two algorithm algorithm make progress onli chosen ask whether extent competit incentiv emph innov adopt better algorithm investig issu sever model user respons vari degre ration competit model effect map competit vs innov relationship well studi theme econom,"['Yishay Mansour', 'Aleksandrs Slivkins', 'Zhiwei Steven Wu']","['cs.GT', 'cs.LG']",False,False,True,False,False,False
654,2017-03-28T14:08:19Z,2017-02-27T18:07:12Z,http://arxiv.org/abs/1702.08405v1,http://arxiv.org/pdf/1702.08405v1,Game-Theoretic Semantics for ATL+ with Applications to Model Checking,game theoret semant atl applic model check,"We develop game-theoretic semantics (GTS) for the fragment ATL+ of the full Alternating-time Temporal Logic ATL*, essentially extending a recently introduced GTS for ATL. We first show that the new game-theoretic semantics is equivalent to the standard semantics of ATL+ (based on perfect recall strategies). We then provide an analysis, based on the new semantics, of the memory and time resources needed for model checking ATL+. Based on that, we establish that strategies that use only a very limited amount of memory suffice for ATL+. Furthermore, using the GTS we provide a new algorithm for model checking of ATL+ and identify a natural hierarchy of tractable fragments of ATL+ that extend ATL.",develop game theoret semant gts fragment atl full altern time tempor logic atl essenti extend recent introduc gts atl first show new game theoret semant equival standard semant atl base perfect recal strategi provid analysi base new semant memori time resourc need model check atl base establish strategi use onli veri limit amount memori suffic atl furthermor use gts provid new algorithm model check atl identifi natur hierarchi tractabl fragment atl extend atl,"['Valentin Goranko', 'Antti Kuusisto', 'Raine Rönnholm']","['math.LO', 'cs.GT', 'cs.LO', 'F.4.1; I.2.11']",False,False,True,False,False,True
656,2017-03-28T14:08:19Z,2017-02-27T13:54:44Z,http://arxiv.org/abs/1702.08286v1,http://arxiv.org/pdf/1702.08286v1,Balancing Lexicographic Fairness and a Utilitarian Objective with   Application to Kidney Exchange,balanc lexicograph fair utilitarian object applic kidney exchang,"Balancing fairness and efficiency in resource allocation is a classical economic and computational problem. The price of fairness measures the worst-case loss of economic efficiency when using an inefficient but fair allocation rule; for indivisible goods in many settings, this price is unacceptably high. In this work, we propose a hybrid fairness rule that balances a strict lexicographic preference ordering over classes of agents and a utilitarian objective that maximizes economic efficiency. We develop a utility function that favors disadvantaged groups lexicographically; but if cost to overall efficiency becomes too high, it smoothly switches to a utilitarian objective. This rule has only one parameter which is proportional to a bound on the price of fairness, and can be adjusted by policymakers. We apply this rule to kidney exchange, where needy patients swap willing but incompatible donors, and demonstrate on real data from a large exchange that our hybrid rule produces more reliable outcomes than other fairness rules.",balanc fair effici resourc alloc classic econom comput problem price fair measur worst case loss econom effici use ineffici fair alloc rule indivis good mani set price unaccept high work propos hybrid fair rule balanc strict lexicograph prefer order class agent utilitarian object maxim econom effici develop util function favor disadvantag group lexicograph cost overal effici becom high smooth switch utilitarian object rule onli one paramet proport bound price fair adjust policymak appli rule kidney exchang needi patient swap incompat donor demonstr real data larg exchang hybrid rule produc reliabl outcom fair rule,"['Duncan C. McElfresh', 'John P. Dickerson']","['cs.GT', 'cs.AI', 'I.2.11; J.4']",False,False,True,False,False,False
657,2017-03-28T14:08:19Z,2017-03-08T03:37:42Z,http://arxiv.org/abs/1703.02091v2,http://arxiv.org/pdf/1703.02091v2,Optimized Cost per Click in Taobao Display Advertising,optim cost per click taobao display advertis,"Taobao, as the largest online retail platform in the world, provides billions of online display advertising impressions for millions of advertisers every day. For commercial purposes, the advertisers bid for specific spots and target crowds to compete for business traffic. The platform chooses the most suitable ads to display in tens of milliseconds. Common pricing methods include cost per mille (CPM) and cost per click (CPC). Traditional advertising systems target certain traits of users and ad placements with fixed bids, essentially regarded as coarse-grained matching of bid and traffic quality. However, the fixed bids set by the advertisers competing for different quality requests cannot fully optimize the advertisers' key requirements. Moreover, the platform has to be responsible for the business revenue and user experience. Thus, we proposed a bid optimizing strategy called optimized cost per click (OCPC) which automatically adjusts the bid to achieve finer matching of bid and traffic quality of page view (PV) request granularity. Our approach optimizes advertisers' demands, platform business revenue and user experience and as a whole improves traffic allocation efficiency. We have validated our approach in Taobao display advertising system in production. The online A/B test shows our algorithm yields substantially better results than previous fixed bid manner.",taobao largest onlin retail platform world provid billion onlin display advertis impress million advertis everi day commerci purpos advertis bid specif spot target crowd compet busi traffic platform choos suitabl ad display ten millisecond common price method includ cost per mill cpm cost per click cpc tradit advertis system target certain trait user ad placement fix bid essenti regard coars grain match bid traffic qualiti howev fix bid set advertis compet differ qualiti request cannot fulli optim advertis key requir moreov platform respons busi revenu user experi thus propos bid optim strategi call optim cost per click ocpc automat adjust bid achiev finer match bid traffic qualiti page view pv request granular approach optim advertis demand platform busi revenu user experi whole improv traffic alloc effici valid approach taobao display advertis system product onlin test show algorithm yield substanti better result previous fix bid manner,"['Han Zhu', 'Junqi Jin', 'Chang Tan', 'Fei Pan', 'Yifan Zeng', 'Han Li', 'Kun Gai']","['cs.GT', 'stat.ML']",False,False,True,False,False,False
659,2017-03-28T14:08:19Z,2017-02-25T17:56:07Z,http://arxiv.org/abs/1702.07932v1,http://arxiv.org/pdf/1702.07932v1,The role of quantum correlations in Cop and Robber game,role quantum correl cop robber game,"We introduce and study quantized versions of Cop and Robber game. We achieve this by using graph-preserving unitary operations, which are the quantum analogue of stochastic operations preserving the graph. We provide the tight bound for the number of operations required to reach the given state. By extending them to controlled operations, we define a quantum controlled Cop and Robber game, which expands the classical Cop and Robber game, as well as classically controlled quantum Cop and Robber game. In contrast to the typical scheme for introducing quantum games, we assume that both parties can utilise full information about the opponent's strategy. We show that the utilisation of the full knowledge about the opponent's state does not provide the advantage. Moreover, the chances of catching the Robber decreases for classically cop-win graphs. The result does not depend on the chosen model of evolution. On the other hand, the possibility to execute controlled quantum operations allows catching the Robber on almost all classically cop-win graphs. To provide interesting, non-trivial quantized Cop and Robber game, we need to enrich the structure of correlations between the players' systems. This result demonstrates that the ability to utilise quantum controlled operations is significantly stronger that the control restricted operating on classical selecting quantum operations only.",introduc studi quantiz version cop robber game achiev use graph preserv unitari oper quantum analogu stochast oper preserv graph provid tight bound number oper requir reach given state extend control oper defin quantum control cop robber game expand classic cop robber game well classic control quantum cop robber game contrast typic scheme introduc quantum game assum parti utilis full inform oppon strategi show utilis full knowledg oppon state doe provid advantag moreov chanc catch robber decreas classic cop win graph result doe depend chosen model evolut hand possibl execut control quantum oper allow catch robber almost classic cop win graph provid interest non trivial quantiz cop robber game need enrich structur correl player system result demonstr abil utilis quantum control oper signific stronger control restrict oper classic select quantum oper onli,"['Adam Glos', 'Jarosław Adam Miszczak']","['quant-ph', 'cs.DM', 'cs.GT', '05C57 (Primary), 91A46, 81P40 (Secondary)', 'G.2.2']",False,False,True,False,False,True
660,2017-03-28T14:08:25Z,2017-02-25T15:07:59Z,http://arxiv.org/abs/1702.07902v1,http://arxiv.org/pdf/1702.07902v1,Approval Voting with Intransitive Preferences,approv vote intransit prefer,"We extend Approval voting to the settings where voters may have intransitive preferences. The major obstacle to applying Approval voting in these settings is that voters are not able to clearly determine who they should approve or disapprove, due to the intransitivity of their preferences. An approach to address this issue is to apply tournament solutions to help voters make the decision. We study a class of voting systems where first each voter casts a vote defined as a tournament, then a well-defined tournament solution is applied to select the candidates who are assumed to be approved by the voter. Winners are the ones receiving the most approvals. We study axiomatic properties of this class of voting systems and complexity of control and bribery problems for these voting systems.",extend approv vote set voter may intransit prefer major obstacl appli approv vote set voter abl clear determin approv disapprov due intransit prefer approach address issu appli tournament solut help voter make decis studi class vote system first voter cast vote defin tournament well defin tournament solut appli select candid assum approv voter winner one receiv approv studi axiomat properti class vote system complex control briberi problem vote system,['Yongjie Yang'],"['cs.GT', 'cs.CC', 'cs.DM']",False,False,True,False,False,True
661,2017-03-28T14:08:25Z,2017-02-25T00:05:57Z,http://arxiv.org/abs/1702.07810v1,http://arxiv.org/pdf/1702.07810v1,A Decomposition of Forecast Error in Prediction Markets,decomposit forecast error predict market,"We introduce and analyze sources of error in prediction market forecasts in order to characterize and bound the difference between a security's price and its ground truth value. We consider cost-function-based prediction markets in which an automated market maker adjusts security prices according to the history of trade. We decompose the forecasting error into four components: \emph{sampling error}, occurring because traders only possess noisy estimates of ground truth; \emph{risk-aversion effect}, arising because traders reveal beliefs only through self-interested trade; \emph{market-maker bias}, resulting from the use of a particular market maker (i.e., cost function) to facilitate trade; and finally, \emph{convergence error}, arising because, at any point in time, market prices may still be in flux. Our goal is to understand the tradeoffs between these error components, and how they are influenced by design decisions such as the functional form of the cost function and the amount of liquidity in the market. We specifically consider a model in which traders have exponential utility and exponential-family beliefs drawn with an independent noise relative to ground truth. In this setting, sampling error and risk-aversion effect vanish as the number of traders grows, but there is a tradeoff between the other two components: decreasing the market maker's liquidity results in smaller market-maker bias, but may also slow down convergence. We provide both upper and lower bounds on market-maker bias and convergence error, and demonstrate via numerical simulations that these bounds are tight. Our results yield new insights into the question of how to set the market's liquidity parameter, and into the extent to which markets that enforce coherent prices across securities produce better predictions than markets that price securities independently.",introduc analyz sourc error predict market forecast order character bound differ secur price ground truth valu consid cost function base predict market autom market maker adjust secur price accord histori trade decompos forecast error four compon emph sampl error occur becaus trader onli possess noisi estim ground truth emph risk avers effect aris becaus trader reveal belief onli self interest trade emph market maker bias result use particular market maker cost function facilit trade final emph converg error aris becaus ani point time market price may still flux goal understand tradeoff error compon influenc design decis function form cost function amount liquid market specif consid model trader exponenti util exponenti famili belief drawn independ nois relat ground truth set sampl error risk avers effect vanish number trader grow tradeoff two compon decreas market maker liquid result smaller market maker bias may also slow converg provid upper lower bound market maker bias converg error demonstr via numer simul bound tight result yield new insight question set market liquid paramet extent market enforc coher price across secur produc better predict market price secur independ,"['Miroslav Dudík', 'Sébastien Lahaie', 'Ryan Rogers', 'Jennifer Wortman Vaughan']",['cs.GT'],False,False,True,False,False,False
662,2017-03-28T14:08:25Z,2017-02-25T00:00:36Z,http://arxiv.org/abs/1702.07806v1,http://arxiv.org/pdf/1702.07806v1,When Does Diversity of User Preferences Improve Outcomes in Selfish   Routing?,doe divers user prefer improv outcom selfish rout,"We seek to understand when heterogeneity in user preferences yields improved outcomes in terms of overall cost. That this might be hoped for is based on the common belief that diversity is advantageous in many settings. We investigate this in the context of routing. Our main result is a sharp characterization of the network settings in which diversity always helps, versus those in which it is sometimes harmful.   Specifically, we consider routing games, where diversity arises in the way that users trade-off two criteria (such as time and money, or, in the case of stochastic delays, expectation and variance of delay). We consider both linear and non-linear combinations of the two criteria and view our main contributions as the following: 1) A participant-oriented measure of cost in the presence of user diversity, together with the identification of the natural benchmark: the same cost measure for an appropriately defined average of the diversity. 2) A full characterization of those network topologies for which diversity always helps, for all latency functions and demands. For single-commodity routings, these are series-parallel graphs, while for multi-commodity routings, they are the newly-defined ""block-matching"" networks. The latter comprise a suitable interweaving of multiple series-parallel graphs each connecting a distinct source-sink pair.   While the result for the single-commodity case may seem intuitive in light of the well-known Braess paradox, the two problems are different. But the main technical challenge is to establish the ""only if"" direction of the result for multi-commodity networks. This follows by constructing an instance where diversity hurts, and showing how to embed it in any network which is not block-matching, by carefully exploiting the way the simple source-sink paths of the commodities intersect in the ""non-block-matching"" portion of the network.",seek understand heterogen user prefer yield improv outcom term overal cost might hope base common belief divers advantag mani set investig context rout main result sharp character network set divers alway help versus sometim harm specif consid rout game divers aris way user trade two criteria time money case stochast delay expect varianc delay consid linear non linear combin two criteria view main contribut follow particip orient measur cost presenc user divers togeth identif natur benchmark cost measur appropri defin averag divers full character network topolog divers alway help latenc function demand singl commod rout seri parallel graph multi commod rout newli defin block match network latter compris suitabl interweav multipl seri parallel graph connect distinct sourc sink pair result singl commod case may seem intuit light well known braess paradox two problem differ main technic challeng establish onli direct result multi commod network follow construct instanc divers hurt show emb ani network block match care exploit way simpl sourc sink path commod intersect non block match portion network,"['Richard Cole', 'Thanasis Lianeas', 'Evdokia Nikolova']",['cs.GT'],False,False,True,False,False,False
663,2017-03-28T14:08:25Z,2017-02-24T17:09:22Z,http://arxiv.org/abs/1702.07665v1,http://arxiv.org/pdf/1702.07665v1,Truthful Mechanisms for Delivery with Mobile Agents,truth mechan deliveri mobil agent,"We study the game-theoretic task of selecting mobile agents to deliver multiple items on a network. An instance is given by $m$ messages (physical objects) which have to be transported between specified source-target pairs in a weighted undirected graph, and $k$ mobile heterogeneous agents, each being able to transport one message at a time. Following a recent model by [B\""artschi et al. 2016], each agent $i$ consumes energy proportional to the distance it travels in the graph, where the different rates of energy consumption are given by weight factors $w_i$. We are interested in optimizing or approximating the total energy consumption over all selected agents.   Unlike previous research, we assume the weights to be private values known only to the respective agents. We present three different mechanisms which select, route and pay the agents in a truthful way that guarantees voluntary participation of the agents, while approximating the optimum energy consumption by a constant factor. To this end we analyze a previous structural result and an approximation algorithm given by [B\""artschi et al. 2017]. Finally, we show that for some instances in the case of a single message ($m=1$), the sum of the payments can be bounded in terms of the optimum as well.",studi game theoret task select mobil agent deliv multipl item network instanc given messag physic object transport specifi sourc target pair weight undirect graph mobil heterogen agent abl transport one messag time follow recent model artschi et al agent consum energi proport distanc travel graph differ rate energi consumpt given weight factor interest optim approxim total energi consumpt select agent unlik previous research assum weight privat valu known onli respect agent present three differ mechan select rout pay agent truth way guarante voluntari particip agent approxim optimum energi consumpt constant factor end analyz previous structur result approxim algorithm given artschi et al final show instanc case singl messag sum payment bound term optimum well,"['Andreas Bärtschi', 'Daniel Graf', 'Paolo Penna']","['cs.GT', 'cs.DS']",False,False,True,False,False,True
665,2017-03-28T14:08:25Z,2017-02-24T01:51:48Z,http://arxiv.org/abs/1702.07444v1,http://arxiv.org/pdf/1702.07444v1,Bandits with Movement Costs and Adaptive Pricing,bandit movement cost adapt price,"We extend the model of Multi-armed Bandit with unit switching cost to incorporate a metric between the actions. We consider the case where the metric over the actions can be modeled by a complete binary tree, and the distance between two leaves is the size of the subtree of their least common ancestor, which abstracts the case that the actions are points on the continuous interval $[0,1]$ and the switching cost is their distance. In this setting, we give a new algorithm that establishes a regret of $\widetilde{O}(\sqrt{kT} + T/k)$, where $k$ is the number of actions and $T$ is the time horizon. When the set of actions corresponds to whole $[0,1]$ interval we can exploit our method for the task of bandit learning with Lipschitz loss functions, where our algorithm achieves an optimal regret rate of $\widetilde{\Theta}(T^{2/3})$, which is the same rate one obtains when there is no penalty for movements. As our main application, we use our new algorithm to solve an adaptive pricing problem. Specifically, we consider the case of a single seller faced with a stream of patient buyers. Each buyer has a private value and a window of time in which they are interested in buying, and they buy at the lowest price in the window, if it is below their value. We show that with an appropriate discretization of the prices, the seller can achieve a regret of $\widetilde{O}(T^{2/3})$ compared to the best fixed price in hindsight, which outperform the previous regret bound of $\widetilde{O}(T^{3/4})$ for the problem.",extend model multi arm bandit unit switch cost incorpor metric action consid case metric action model complet binari tree distanc two leav size subtre least common ancestor abstract case action point continu interv switch cost distanc set give new algorithm establish regret widetild sqrt kt number action time horizon set action correspond whole interv exploit method task bandit learn lipschitz loss function algorithm achiev optim regret rate widetild theta rate one obtain penalti movement main applic use new algorithm solv adapt price problem specif consid case singl seller face stream patient buyer buyer privat valu window time interest buy buy lowest price window valu show appropri discret price seller achiev regret widetild compar best fix price hindsight outperform previous regret bound widetild problem,"['Tomer Koren', 'Roi Livni', 'Yishay Mansour']","['cs.LG', 'cs.GT']",False,False,True,False,False,True
666,2017-03-28T14:08:25Z,2017-02-23T17:54:28Z,http://arxiv.org/abs/1702.07311v1,http://arxiv.org/abs/1702.07311v1,ERA: A Framework for Economic Resource Allocation for the Cloud,era framework econom resourc alloc cloud,"Cloud computing has reached significant maturity from a systems perspective, but currently deployed solutions rely on rather basic economics mechanisms that yield suboptimal allocation of the costly hardware resources. In this paper we present Economic Resource Allocation (ERA), a complete framework for scheduling and pricing cloud resources, aimed at increasing the efficiency of cloud resources usage by allocating resources according to economic principles. The ERA architecture carefully abstracts the underlying cloud infrastructure, enabling the development of scheduling and pricing algorithms independently of the concrete lower-level cloud infrastructure and independently of its concerns. Specifically, ERA is designed as a flexible layer that can sit on top of any cloud system and interfaces with both the cloud resource manager and with the users who reserve resources to run their jobs. The jobs are scheduled based on prices that are dynamically calculated according to the predicted demand. Additionally, ERA provides a key internal API to pluggable algorithmic modules that include scheduling, pricing and demand prediction. We provide a proof-of-concept software and demonstrate the effectiveness of the architecture by testing ERA over both public and private cloud systems -- Azure Batch of Microsoft and Hadoop/YARN. A broader intent of our work is to foster collaborations between economics and system communities. To that end, we have developed a simulation platform via which economics and system experts can test their algorithmic implementations.",cloud comput reach signific matur system perspect current deploy solut reli rather basic econom mechan yield suboptim alloc cost hardwar resourc paper present econom resourc alloc era complet framework schedul price cloud resourc aim increas effici cloud resourc usag alloc resourc accord econom principl era architectur care abstract cloud infrastructur enabl develop schedul price algorithm independ concret lower level cloud infrastructur independ concern specif era design flexibl layer sit top ani cloud system interfac cloud resourc manag user reserv resourc run job job schedul base price dynam calcul accord predict demand addit era provid key intern api pluggabl algorithm modul includ schedul price demand predict provid proof concept softwar demonstr effect architectur test era public privat cloud system azur batch microsoft hadoop yarn broader intent work foster collabor econom system communiti end develop simul platform via econom system expert test algorithm implement,"['Moshe Babaioff', 'Yishay Mansour', 'Noam Nisan', 'Gali Noti', 'Carlo Curino', 'Nar Ganapathy', 'Ishai Menache', 'Omer Reingold', 'Moshe Tennenholtz', 'Erez Timnat']","['cs.GT', 'cs.DC']",False,False,True,False,False,True
667,2017-03-28T14:08:25Z,2017-02-23T17:49:40Z,http://arxiv.org/abs/1702.07309v1,http://arxiv.org/pdf/1702.07309v1,Bounding the inefficiency of compromise,bound ineffici compromis,"Social networks on the Internet have seen an enormous growth recently and play a crucial role in different aspects of today's life. They have facilitated information dissemination in ways that have been beneficial for their users but they are often used strategically in order to spread information that only serves the objectives of particular users. These properties have inspired a revision of classical opinion formation models from sociology using game-theoretic notions and tools. We follow the same modeling approach, focusing on scenarios where the opinion expressed by each user is a compromise between her internal belief and the opinions of a small number of neighbors among her social acquaintances. We formulate simple games that capture this behavior and quantify the inefficiency of equilibria using the well-known notion of the price of anarchy. Our results indicate that compromise comes at a cost that strongly depends on the neighborhood size.",social network internet seen enorm growth recent play crucial role differ aspect today life facilit inform dissemin way benefici user often use strateg order spread inform onli serv object particular user properti inspir revis classic opinion format model sociolog use game theoret notion tool follow model approach focus scenario opinion express user compromis intern belief opinion small number neighbor among social acquaint formul simpl game captur behavior quantifi ineffici equilibria use well known notion price anarchi result indic compromis come cost strong depend neighborhood size,"['Ioannis Caragiannis', 'Panagiotis Kanellopoulos', 'Alexandros A. Voudouris']",['cs.GT'],False,False,True,False,False,False
668,2017-03-28T14:08:25Z,2017-02-23T04:18:53Z,http://arxiv.org/abs/1702.07638v1,http://arxiv.org/pdf/1702.07638v1,Reward-penalty Mechanism for Reverse Supply Chain Network with   Asymmetric Information and Carbon Emission Constraints,reward penalti mechan revers suppli chain network asymmetr inform carbon emiss constraint,"We discuss the government's reward and penalty mechanism in the presence of asymmetric information and carbon emission constraint when downstream retailers compete in a reverse supply chain network. Considering five game models which are different in terms of the coordination structure of the reverse supply chain network and power structure of the reward-penalty mechanism: (1) the reverse supply chain network centralized decision-making model; (2) the reverse supply chain network centralized decision-making model with carbon emission constraint; (3) the retailers' competition reverse supply chain network decentralized decision-making model; (4) the retailers' competition reverse supply chain network decentralized decision-making model with carbon emission constraint; (5) the retailers' competition reverse supply chain network decentralized decision-making model with carbon emission constraint and the government's reward-penalty mechanism. Building the participation-incentive contract under each model use the principal-agent theory, and solving the model use the Lagrange multiplier method. We can get the following conclusion: 1) when the government implements the reward-penalty mechanism for carbon emission and recycling simultaneously, the recycling rate as well as the buy-back price offered by the manufacturer are higher than those when the government conducts reward-penalty mechanism exclusively for carbon emission; 2) when the government implements carbon emission constraint, both retailers' selling prices of the new product are higher than those when no carbon emission constraint is forced; 3) there is no certain relationship between the two retailers' selling prices of the new product when the government implements the reward-penalty mechanism only for carbon emission and when it implements the mechanism for carbon emission as well as recycling.",discuss govern reward penalti mechan presenc asymmetr inform carbon emiss constraint downstream retail compet revers suppli chain network consid five game model differ term coordin structur revers suppli chain network power structur reward penalti mechan revers suppli chain network central decis make model revers suppli chain network central decis make model carbon emiss constraint retail competit revers suppli chain network decentr decis make model retail competit revers suppli chain network decentr decis make model carbon emiss constraint retail competit revers suppli chain network decentr decis make model carbon emiss constraint govern reward penalti mechan build particip incent contract model use princip agent theori solv model use lagrang multipli method get follow conclus govern implement reward penalti mechan carbon emiss recycl simultan recycl rate well buy back price offer manufactur higher govern conduct reward penalti mechan exclus carbon emiss govern implement carbon emiss constraint retail sell price new product higher carbon emiss constraint forc certain relationship two retail sell price new product govern implement reward penalti mechan onli carbon emiss implement mechan carbon emiss well recycl,"['Xiao-qing Zhang', 'Xi-gang Yuan']","['math.OC', 'cs.GT']",False,False,True,False,False,False
669,2017-03-28T14:08:25Z,2017-02-22T22:43:45Z,http://arxiv.org/abs/1702.07032v1,http://arxiv.org/pdf/1702.07032v1,On the Complexity of Bundle-Pricing and Simple Mechanisms,complex bundl price simpl mechan,"We show that the problem of finding an optimal bundle-pricing for a single additive buyer is #P-hard, even when the distributions have support size 2 for each item and the optimal solution is guaranteed to be a simple one: the seller picks a price for the grand bundle and a price for each individual item; the buyer can purchase either the grand bundle at the given price or any bundle of items at their total individual prices. We refer to this simple and natural family of pricing schemes as discounted item-pricings. In addition to the hardness result, we show that when the distributions are i.i.d. with support size 2, a discounted item-pricing can achieve the optimal revenue obtainable by lottery-pricings and it can be found in polynomial time.",show problem find optim bundl price singl addit buyer hard even distribut support size item optim solut guarante simpl one seller pick price grand bundl price individu item buyer purchas either grand bundl given price ani bundl item total individu price refer simpl natur famili price scheme discount item price addit hard result show distribut support size discount item price achiev optim revenu obtain lotteri price found polynomi time,"['Xi Chen', 'George Matikas', 'Dimitris Paparas', 'Mihalis Yannakakis']","['cs.GT', 'cs.CC', 'cs.DS']",False,False,True,False,False,False
670,2017-03-28T14:08:29Z,2017-02-22T22:34:57Z,http://arxiv.org/abs/1702.07031v1,http://arxiv.org/pdf/1702.07031v1,Proactive Resource Management in LTE-U Systems: A Deep Learning   Perspective,proactiv resourc manag lte system deep learn perspect,"LTE in unlicensed spectrum (LTE-U) is a promising approach to overcome the wireless spectrum scarcity. However, to reap the benefits of LTE-U, a fair coexistence mechanism with other incumbent WiFi deployments is required. In this paper, a novel deep learning approach is proposed for modeling the resource allocation problem of LTE-U small base stations (SBSs). The proposed approach enables multiple SBSs to proactively perform dynamic channel selection, carrier aggregation, and fractional spectrum access while guaranteeing fairness with existing WiFi networks and other LTE-U operators. Adopting a proactive coexistence mechanism enables future delay-intolerant LTE-U data demands to be served within a given prediction window ahead of their actual arrival time thus avoiding the underutilization of the unlicensed spectrum during off-peak hours while maximizing the total served LTE-U traffic load. To this end, a noncooperative game model is formulated in which SBSs are modeled as Homo Egualis agents that aim at predicting a sequence of future actions and thus achieving long-term equal weighted fairness with WLAN and other LTE-U operators over a given time horizon. The proposed deep learning algorithm is then shown to reach a mixed-strategy Nash equilibrium (NE), when it converges. Simulation results using real data traces show that the proposed scheme can yield up to 28% and 11% gains over a conventional reactive approach and a proportional fair coexistence mechanism, respectively. The results also show that the proposed framework prevents WiFi performance degradation for a densely deployed LTE-U network.",lte unlicens spectrum lte promis approach overcom wireless spectrum scarciti howev reap benefit lte fair coexist mechan incumb wifi deploy requir paper novel deep learn approach propos model resourc alloc problem lte small base station sbss propos approach enabl multipl sbss proactiv perform dynam channel select carrier aggreg fraction spectrum access guarante fair exist wifi network lte oper adopt proactiv coexist mechan enabl futur delay intoler lte data demand serv within given predict window ahead actual arriv time thus avoid underutil unlicens spectrum dure peak hour maxim total serv lte traffic load end noncoop game model formul sbss model homo eguali agent aim predict sequenc futur action thus achiev long term equal weight fair wlan lte oper given time horizon propos deep learn algorithm shown reach mix strategi nash equilibrium ne converg simul result use real data trace show propos scheme yield gain convent reactiv approach proport fair coexist mechan respect result also show propos framework prevent wifi perform degrad dens deploy lte network,"['Ursula Challita', 'Li Dong', 'Walid Saad']","['cs.IT', 'cs.AI', 'cs.GT', 'math.IT']",False,False,True,False,False,True
671,2017-03-28T14:08:29Z,2017-02-22T18:06:24Z,http://arxiv.org/abs/1702.06922v1,http://arxiv.org/pdf/1702.06922v1,Formation of coalition structures as a non-cooperative game,format coalit structur non cooper game,"The paper defines a family of nested non-cooperative simultaneous finite games to study coalition structure formation with intra and inter-coalition externalities. Every game has two outcomes - an allocation of players over coalitions and a payoff profile for every player.   Every game in the family has an equilibrium in mixed strategies. The equilibrium can generate more than one coalition with a presence of intra and inter group externalities. These properties make it different from the Shapley value, strong Nash, coalition-proof equilibrium, core, kernel, nucleolus. The paper demonstrates some applications: non-cooperative cooperation, Bayesian game, stochastic games and construction of a non-cooperative criterion of coalition structure stability for studying focal points. An example demonstrates that a payoff profile in the Prisoners' Dilemma is non-informative to deduce a cooperation of players.",paper defin famili nest non cooper simultan finit game studi coalit structur format intra inter coalit extern everi game two outcom alloc player coalit payoff profil everi player everi game famili equilibrium mix strategi equilibrium generat one coalit presenc intra inter group extern properti make differ shapley valu strong nash coalit proof equilibrium core kernel nucleolus paper demonstr applic non cooper cooper bayesian game stochast game construct non cooper criterion coalit structur stabil studi focal point exampl demonstr payoff profil prison dilemma non inform deduc cooper player,['Dmitry Levando'],"['math.OC', 'cs.GT']",False,False,True,False,False,True
673,2017-03-28T14:08:29Z,2017-02-25T23:52:16Z,http://arxiv.org/abs/1702.06810v2,http://arxiv.org/pdf/1702.06810v2,Pricing average price advertisement options when underlying spot market   prices are discontinuous,price averag price advertis option spot market price discontinu,"Advertisement (ad) options have been recently studied as a novel guaranteed delivery (GD) system in online advertising. In essence, an ad option is a contract that gives an advertiser a right but not obligation to enter into transactions to purchase ad inventories such as page views or link clicks from a specific slot at one or multiple pre-specified prices in a specific future period. Compared to guaranteed contracts, the advertiser pays a lower upfront fee but can have greater flexibility and more control in advertising. So far ad option studies have been restricted to the situations where the option payoff is determined by the underlying auction payment price at a specific time point and the price evolution over time is assumed to be continuous. The former leads to a biased option payoff calculation and the latter is invalid empirically for many ad slots. This paper discusses a new option pricing framework which can be applied to a general situation. The option payoff is calculated based on the average price over a specific future period. As we use the general mean, our framework contains different payoff functions as special cases. Further, we use jump-diffusion stochastic models to describe the auction payment price movement, which have Markov and price discontinuity properties, and those properties are validated by our statistical investigation of ad auctions from different datasets. In the paper, we propose a general option pricing solution based on Monte Carlo simulation and also give an explicit pricing formula for a special case. The latter is also a generalisation of the option pricing models in some other recent developments.",advertis ad option recent studi novel guarante deliveri gd system onlin advertis essenc ad option contract give advertis right oblig enter transact purchas ad inventori page view link click specif slot one multipl pre specifi price specif futur period compar guarante contract advertis pay lower upfront fee greater flexibl control advertis far ad option studi restrict situat option payoff determin auction payment price specif time point price evolut time assum continu former lead bias option payoff calcul latter invalid empir mani ad slot paper discuss new option price framework appli general situat option payoff calcul base averag price specif futur period use general mean framework contain differ payoff function special case use jump diffus stochast model describ auction payment price movement markov price discontinu properti properti valid statist investig ad auction differ dataset paper propos general option price solut base mont carlo simul also give explicit price formula special case latter also generalis option price model recent develop,"['Bowei Chen', 'Mohan S. Kankanhalli']",['cs.GT'],False,False,True,False,False,False
674,2017-03-28T14:08:29Z,2017-02-22T02:07:25Z,http://arxiv.org/abs/1702.06645v1,http://arxiv.org/pdf/1702.06645v1,Resource Sharing Among mmWave Cellular Service Providers in a Vertically   Differentiated Duopoly,resourc share among mmwave cellular servic provid vertic differenti duopoli,"With the increasing interest in the use of millimeter wave bands for 5G cellular systems comes renewed interest in resource sharing. Properties of millimeter wave bands such as massive bandwidth, highly directional antennas, high penetration loss, and susceptibility to shadowing, suggest technical advantages to spectrum and infrastructure sharing in millimeter wave cellular networks. However, technical advantages do not necessarily translate to increased profit for service providers, or increased consumer surplus. In this paper, detailed network simulations are used to better understand the economic implications of resource sharing in a vertically differentiated duopoly market for cellular service. The results suggest that resource sharing is less often profitable for millimeter wave service providers compared to microwave cellular service providers, and does not necessarily increase consumer surplus.",increas interest use millimet wave band cellular system come renew interest resourc share properti millimet wave band massiv bandwidth high direct antenna high penetr loss suscept shadow suggest technic advantag spectrum infrastructur share millimet wave cellular network howev technic advantag necessarili translat increas profit servic provid increas consum surplus paper detail network simul use better understand econom implic resourc share vertic differenti duopoli market cellular servic result suggest resourc share less often profit millimet wave servic provid compar microwav cellular servic provid doe necessarili increas consum surplus,"['Fraida Fund', 'Shahram Shahsavari', 'Shivendra S. Panwar', 'Elza Erkip', 'Sundeep Rangan']","['cs.NI', 'cs.GT']",False,False,True,False,False,False
675,2017-03-28T14:08:29Z,2017-02-21T15:30:28Z,http://arxiv.org/abs/1702.06439v1,http://arxiv.org/pdf/1702.06439v1,Admissibility in Concurrent Games,admiss concurr game,"In this paper, we study the notion of admissibility for randomised strategies in concurrent games. Intuitively, an admissible strategy is one where the player plays `as well as possible', because there is no other strategy that dominates it, i.e., that wins (almost surely) against a super set of adversarial strategies. We prove that admissible strategies always exist in concurrent games, and we characterise them precisely. Then, when the objectives of the players are omega-regular, we show how to perform assume-admissible synthesis, i.e., how to compute admissible strategies that win (almost surely) under the hypothesis that the other players play admissible",paper studi notion admiss randomis strategi concurr game intuit admiss strategi one player play well possibl becaus strategi domin win almost sure super set adversari strategi prove admiss strategi alway exist concurr game characteris precis object player omega regular show perform assum admiss synthesi comput admiss strategi win almost sure hypothesi player play admiss,"['Nicolas Basset', 'Gilles Geeraerts', 'Jean-François Raskin', 'Ocan Sankur']","['cs.GT', 'cs.LO']",False,False,True,False,False,False
677,2017-03-28T14:08:29Z,2017-02-20T21:54:26Z,http://arxiv.org/abs/1702.06189v1,http://arxiv.org/pdf/1702.06189v1,A Graphical Evolutionary Game Approach to Social Learning,graphic evolutionari game approach social learn,"In this work, we study the social learning problem, in which agents of a networked system collaborate to detect the state of the nature based on their private signals. A novel distributed graphical evolutionary game theoretic learning method is proposed. In the proposed game-theoretic method, agents only need to communicate their binary decisions rather than the real-valued beliefs with their neighbors, which endows the method with low communication complexity. Under mean field approximations, we theoretically analyze the steady state equilibria of the game and show that the evolutionarily stable states (ESSs) coincide with the decisions of the benchmark centralized detector. Numerical experiments are implemented to confirm the effectiveness of the proposed game-theoretic learning method.",work studi social learn problem agent network system collabor detect state natur base privat signal novel distribut graphic evolutionari game theoret learn method propos propos game theoret method agent onli need communic binari decis rather real valu belief neighbor endow method low communic complex mean field approxim theoret analyz steadi state equilibria game show evolutionarili stabl state esss coincid decis benchmark central detector numer experi implement confirm effect propos game theoret learn method,"['Xuanyu Cao', 'K. J. Ray Liu']",['cs.GT'],False,False,True,False,False,True
678,2017-03-28T14:08:29Z,2017-02-20T17:56:50Z,http://arxiv.org/abs/1703.00807v1,http://arxiv.org/abs/1703.00807v1,Privacy Management and Optimal Pricing in People-Centric Sensing,privaci manag optim price peopl centric sens,"With the emerging sensing technologies such as mobile crowdsensing and Internet of Things (IoT), people-centric data can be efficiently collected and used for analytics and optimization purposes. This data is typically required to develop and render people-centric services. In this paper, we address the privacy implication, optimal pricing, and bundling of people-centric services. We first define the inverse correlation between the service quality and privacy level from data analytics perspectives. We then present the profit maximization models of selling standalone, complementary, and substitute services. Specifically, the closed-form solutions of the optimal privacy level and subscription fee are derived to maximize the gross profit of service providers. For interrelated people-centric services, we show that cooperation by service bundling of complementary services is profitable compared to the separate sales but detrimental for substitutes. We also show that the market value of a service bundle is correlated with the degree of contingency between the interrelated services. Finally, we incorporate the profit sharing models from game theory for dividing the bundling profit among the cooperative service providers.",emerg sens technolog mobil crowdsens internet thing iot peopl centric data effici collect use analyt optim purpos data typic requir develop render peopl centric servic paper address privaci implic optim price bundl peopl centric servic first defin invers correl servic qualiti privaci level data analyt perspect present profit maxim model sell standalon complementari substitut servic specif close form solut optim privaci level subscript fee deriv maxim gross profit servic provid interrel peopl centric servic show cooper servic bundl complementari servic profit compar separ sale detriment substitut also show market valu servic bundl correl degre conting interrel servic final incorpor profit share model game theori divid bundl profit among cooper servic provid,"['Mohammad Abu Alsheikh', 'Dusit Niyato', 'Derek Leong', 'Ping Wang', 'Zhu Han']",['cs.GT'],False,False,True,False,False,False
679,2017-03-28T14:08:29Z,2017-02-22T01:48:29Z,http://arxiv.org/abs/1702.06062v2,http://arxiv.org/pdf/1702.06062v2,Simple vs Optimal Mechanisms in Auctions with Convex Payments,simpl vs optim mechan auction convex payment,"We investigate approximately optimal mechanisms in settings where bidders' utility functions are non-linear; specifically, convex, with respect to payments (such settings arise, for instance, in procurement auctions for energy). We provide constant factor approximation guarantees for mechanisms that are independent of bidders' private information (i.e., prior-free), and for mechanisms that rely to an increasing extent on that information (i.e., detail free). We also describe experiments, which show that for randomly drawn monotone hazard rate distributions, our mechanisms achieve at least 80\% of the optimal revenue, on average. Both our theoretical and experimental results show that in the convex payment setting, it is desirable to allocate across multiple bidders, rather than only to bidders with the highest (virtual) value, as in the traditional quasi-linear utility setting.",investig approxim optim mechan set bidder util function non linear specif convex respect payment set aris instanc procur auction energi provid constant factor approxim guarante mechan independ bidder privat inform prior free mechan reli increas extent inform detail free also describ experi show random drawn monoton hazard rate distribut mechan achiev least optim revenu averag theoret experiment result show convex payment set desir alloc across multipl bidder rather onli bidder highest virtual valu tradit quasi linear util set,"['Amy Greenwald', 'Takehiro Oyakawa', 'Vasilis Syrgkanis']",['cs.GT'],False,False,True,False,False,True
680,2017-03-28T14:08:34Z,2017-02-20T00:34:00Z,http://arxiv.org/abs/1702.05825v1,http://arxiv.org/pdf/1702.05825v1,Sustainable Fair Division,sustain fair divis,"In this paper, I summarize our work on online fair division. In particular, I present two models for online fair division: (1) one existing model for fair division in food banks and (2) one new model for fair division of deceased organs to patients. I further discuss simple mechanisms for these models that allocate the resources as they arrive to agents. In practice, agents are often risk-averse having imperfect information. Within this assumption, I report several interesting axiomatic and complexity results for these mechanisms and conclude with future work.",paper summar work onlin fair divis particular present two model onlin fair divis one exist model fair divis food bank one new model fair divis deceas organ patient discuss simpl mechan model alloc resourc arriv agent practic agent often risk avers imperfect inform within assumpt report sever interest axiomat complex result mechan conclud futur work,['Martin Aleksandrov'],['cs.GT'],False,False,True,False,False,True
682,2017-03-28T14:08:34Z,2017-02-18T18:23:17Z,http://arxiv.org/abs/1702.05640v1,http://arxiv.org/pdf/1702.05640v1,Obvious Strategyproofness Needs Monitoring for Good Approximations,obvious strategyproof need monitor good approxim,"Obvious strategyproofness (OSP) is an appealing concept as it allows to maintain incentive compatibility even in the presence of agents that are not fully rational, e.g., those who struggle with contingent reasoning [Li, 2015]. However, it has been shown to impose some limitations, e.g., no OSP mechanism can return a stable matching [Ashlagi and Gonczarowski, 2015].   We here deepen the study of the limitations of OSP mechanisms by looking at their approximation guarantees for basic optimization problems paradigmatic of the area, i.e., machine scheduling and facility location. We prove a number of bounds on the approximation guarantee of OSP mechanisms, which show that OSP can come at a significant cost. However, rather surprisingly, we prove that OSP mechanisms can return optimal solutions when they use monitoring -- a novel mechanism design paradigm that introduces a mild level of scrutiny on agents' declarations [Kovacs et al., 2015].",obvious strategyproof osp appeal concept allow maintain incent compat even presenc agent fulli ration struggl conting reason li howev shown impos limit osp mechan return stabl match ashlagi gonczarowski deepen studi limit osp mechan look approxim guarante basic optim problem paradigmat area machin schedul facil locat prove number bound approxim guarante osp mechan show osp come signific cost howev rather surpris prove osp mechan return optim solut use monitor novel mechan design paradigm introduc mild level scrutini agent declar kovac et al,"['Diodato Ferraioli', 'Carmine Ventre']",['cs.GT'],False,False,True,False,False,True
687,2017-03-28T14:08:34Z,2017-02-25T09:23:38Z,http://arxiv.org/abs/1702.05355v2,http://arxiv.org/pdf/1702.05355v2,How Much Does Users' Psychology Matter in Engineering Mean-Field-Type   Games,much doe user psycholog matter engin mean field type game,"Until now mean-field-type game theory was not focused on cognitively-plausible models of choices in humans, animals, machines, robots, software-defined and mobile devices strategic interactions. This work presents some effects of users' psychology in mean-field-type games. In addition to the traditional ""material"" payoff modelling, psychological patterns are introduced in order to better capture and understand behaviors that are observed in engineering practice or in experimental settings. The psychological payoff value depends upon choices, mean-field states, mean-field actions, empathy and beliefs. It is shown that the affective empathy enforces mean-field equilibrium payoff equity and improves fairness between the players. It establishes equilibrium systems for such interactive decision-making problems. Basic empathy concepts are illustrated in several important problems in engineering including resource sharing, packet collision minimization, energy markets, and forwarding in Device-to-Device communications. The work conducts also an experiment with 47 people who have to decide whether to cooperate or not. The basic Interpersonal Reactivity Index of empathy metrics were used to measure the empathy distribution of each participant. Android app called Empathizer is developed to analyze systematically the data obtained from the participants. The experimental results reveal that the dominated strategies of the classical game theory are not dominated any more when users' psychology is involved, and a significant level of cooperation is observed among the users who are positively partially empathetic.",mean field type game theori focus cognit plausibl model choic human anim machin robot softwar defin mobil devic strateg interact work present effect user psycholog mean field type game addit tradit materi payoff model psycholog pattern introduc order better captur understand behavior observ engin practic experiment set psycholog payoff valu depend upon choic mean field state mean field action empathi belief shown affect empathi enforc mean field equilibrium payoff equiti improv fair player establish equilibrium system interact decis make problem basic empathi concept illustr sever import problem engin includ resourc share packet collis minim energi market forward devic devic communic work conduct also experi peopl decid whether cooper basic interperson reactiv index empathi metric use measur empathi distribut particip android app call empath develop analyz systemat data obtain particip experiment result reveal domin strategi classic game theori domin ani user psycholog involv signific level cooper observ among user posit partial empathet,"['Giulia Rossi', 'Alain Tcheukam', 'Hamidou Tembine']","['cs.GT', 'cs.MA']",False,False,True,False,False,False
688,2017-03-28T14:08:34Z,2017-02-16T19:23:12Z,http://arxiv.org/abs/1702.05119v1,http://arxiv.org/pdf/1702.05119v1,Evolutionary prisoner's dilemma games coevolving on adaptive networks,evolutionari prison dilemma game coevolv adapt network,"We study a model for switching strategies in the Prisoner's Dilemma game on adaptive networks of player pairings that coevolve as players attempt to maximize their return. We use a node-based strategy model with each player following one strategy (cooperate or defect) at a time with all of its neighbors. We improve on the existing pair approximation (PA) model for this system by using approximate master equations (AMEs). We explore the parameter space demonstrating the accuracy of the approximation as compared with simulations. We study two variations of this partner-switching model to investigate the evolution, predict stationary states, and compare the total utilities and other qualitative differences between these two variants.",studi model switch strategi prison dilemma game adapt network player pair coevolv player attempt maxim return use node base strategi model player follow one strategi cooper defect time neighbor improv exist pair approxim pa model system use approxim master equat ame explor paramet space demonstr accuraci approxim compar simul studi two variat partner switch model investig evolut predict stationari state compar total util qualit differ two variant,"['Hsuan-Wei Lee', 'Nishant Malik', 'Peter J. Mucha']","['cs.SI', 'cs.GT', '91']",False,False,True,False,False,True
689,2017-03-28T14:08:34Z,2017-02-16T03:39:07Z,http://arxiv.org/abs/1702.04849v1,http://arxiv.org/pdf/1702.04849v1,Theoretical and Practical Advances on Smoothing for Extensive-Form Games,theoret practic advanc smooth extens form game,"Sparse iterative methods, in particular first-order methods, are known to be among the most effective in solving large-scale two-player zero-sum extensive-form games. The convergence rates of these methods depend heavily on the properties of the distance-generating function that they are based on. We investigate the acceleration of first-order methods for solving extensive-form games through better design of the dilated entropy function---a class of distance-generating functions related to the domains associated with the extensive-form games. By introducing a new weighting scheme for the dilated entropy function, we develop the first distance-generating function for the strategy spaces of sequential games that has no dependence on the branching factor of the player. This result improves the convergence rate of several first-order methods by a factor of $\Omega(b^dd)$, where $b$ is the branching factor of the player, and $d$ is the depth of the game tree.   Thus far, counterfactual regret minimization methods have been faster in practice, and more popular, than first-order methods despite their theoretically inferior convergence rates. Using our new weighting scheme and practical tuning we show that, for the first time, the excessive gap technique can be made faster than the fastest counterfactual regret minimization algorithm, CFR+, in practice.",spars iter method particular first order method known among effect solv larg scale two player zero sum extens form game converg rate method depend heavili properti distanc generat function base investig acceler first order method solv extens form game better design dilat entropi function class distanc generat function relat domain associ extens form game introduc new weight scheme dilat entropi function develop first distanc generat function strategi space sequenti game depend branch factor player result improv converg rate sever first order method factor omega dd branch factor player depth game tree thus far counterfactu regret minim method faster practic popular first order method despit theoret inferior converg rate use new weight scheme practic tune show first time excess gap techniqu made faster fastest counterfactu regret minim algorithm cfr practic,"['Christian Kroer', 'Kevin Waugh', 'Fatma Kilinc-Karzan', 'Tuomas Sandholm']","['cs.GT', 'cs.AI']",False,False,True,False,False,True
690,2017-03-28T14:08:38Z,2017-02-16T17:04:36Z,http://arxiv.org/abs/1702.04254v2,http://arxiv.org/pdf/1702.04254v2,"A ""Quantal Regret"" Method for Structural Econometrics in Repeated Games",quantal regret method structur econometr repeat game,"We suggest a general method for inferring players' values from their actions in repeated games. The method extends and improves upon the recent suggestion of (Nekipelov et al., EC 2015) and is based on the assumption that players are more likely to exhibit sequences of actions that have lower regret.   We evaluate this ""quantal regret"" method on two different datasets from experiments of repeated games with controlled player values: those of (Selten and Chmura, AER 2008) on a variety of two-player 2x2 games and our own experiment on ad-auctions (Noti et al., WWW 2014). We find that the quantal regret method is consistently and significantly more precise than either ""classic"" econometric methods that are based on Nash equilibria, or the ""min-regret"" method of (Nekipelov et al., EC 2015).",suggest general method infer player valu action repeat game method extend improv upon recent suggest nekipelov et al ec base assumpt player like exhibit sequenc action lower regret evalu quantal regret method two differ dataset experi repeat game control player valu selten chmura aer varieti two player game experi ad auction noti et al www find quantal regret method consist signific precis either classic econometr method base nash equilibria min regret method nekipelov et al ec,"['Noam Nisan', 'Gali Noti']",['cs.GT'],False,False,True,False,False,True
691,2017-03-28T14:08:38Z,2017-02-14T10:13:15Z,http://arxiv.org/abs/1702.04138v1,http://arxiv.org/pdf/1702.04138v1,Agent Failures in All-Pay Auctions,agent failur pay auction,"All-pay auctions, a common mechanism for various human and agent interactions, suffers, like many other mechanisms, from the possibility of players' failure to participate in the auction. We model such failures, and fully characterize equilibrium for this class of games, we present a symmetric equilibrium and show that under some conditions the equilibrium is unique. We reveal various properties of the equilibrium, such as the lack of influence of the most-likely-to-participate player on the behavior of the other players. We perform this analysis with two scenarios: the sum-profit model, where the auctioneer obtains the sum of all submitted bids, and the max-profit model of crowdsourcing contests, where the auctioneer can only use the best submissions and thus obtains only the winning bid.   Furthermore, we examine various methods of influencing the probability of participation such as the effects of misreporting one's own probability of participating, and how influencing another player's participation chances changes the player's strategy.",pay auction common mechan various human agent interact suffer like mani mechan possibl player failur particip auction model failur fulli character equilibrium class game present symmetr equilibrium show condit equilibrium uniqu reveal various properti equilibrium lack influenc like particip player behavior player perform analysi two scenario sum profit model auction obtain sum submit bid max profit model crowdsourc contest auction onli use best submiss thus obtain onli win bid furthermor examin various method influenc probabl particip effect misreport one probabl particip influenc anoth player particip chanc chang player strategi,"['Yoad Lewenberg', 'Omer Lev', 'Yoram Bachrach', 'Jeffrey S. Rosenschein']","['cs.GT', 'cs.MA']",False,False,True,False,False,False
692,2017-03-28T14:08:38Z,2017-02-13T20:37:18Z,http://arxiv.org/abs/1702.03978v1,http://arxiv.org/pdf/1702.03978v1,Multicast Capacity Through Perfect Domination,multicast capac perfect domin,"The capacity of wireless networks is a classic and important topic of study. Informally, the capacity of a network is simply the total amount of information which it can transfer. In the context of models of wireless radio networks, this has usually meant the total number of point-to-point messages which can be sent or received in one time step. This definition has seen intensive study in recent years, particularly with respect to more accurate models of radio networks such as the SINR model. This paper is motivated by an obvious fact: radio antennae are (at least traditionally) omnidirectional, and hence point-to-point connections are not necessarily the best definition of capacity. To fix this, we introduce a new definition of capacity as the maximum number of messages which can be received in one round, and show that this is related to a new optimization problem we call the Maximum Perfect Dominated Set (MaxPDS) problem. Using this relationship we give tight upper and lower bounds for approximating the capacity. We also analyze this notion of capacity under game-theoretic constraints, giving tight bounds on both the Price of Anarchy and the Price of Stability.",capac wireless network classic import topic studi inform capac network simpli total amount inform transfer context model wireless radio network usual meant total number point point messag sent receiv one time step definit seen intens studi recent year particular respect accur model radio network sinr model paper motiv obvious fact radio antenna least tradit omnidirect henc point point connect necessarili best definit capac fix introduc new definit capac maximum number messag receiv one round show relat new optim problem call maximum perfect domin set maxpd problem use relationship give tight upper lower bound approxim capac also analyz notion capac game theoret constraint give tight bound price anarchi price stabil,"['Michael Dinitz', 'Naomi Ephraim']",['cs.GT'],False,False,True,False,False,False
693,2017-03-28T14:08:38Z,2017-02-13T04:29:14Z,http://arxiv.org/abs/1702.03627v1,http://arxiv.org/pdf/1702.03627v1,Mechanism Design in Social Networks,mechan design social network,"This paper studies an auction design problem for a seller to sell a commodity in a social network, where each individual (the seller or a buyer) can only communicate with her neighbors. The challenge to the seller is to design a mechanism to incentivize the buyers, who are aware of the auction, to further propagate the information to their neighbors so that more buyers will participate in the auction and hence, the seller will be able to make a higher revenue. We propose a novel auction mechanism, called information diffusion mechanism (IDM), which incentivizes the buyers to not only truthfully report their valuations on the commodity to the seller, but also further propagate the auction information to all their neighbors. In comparison, the direct extension of the well-known Vickrey-Clarke-Groves (VCG) mechanism in social networks can also incentivize the information diffusion, but it will decrease the seller's revenue or even lead to a deficit sometimes. The formalization of the problem has not yet been addressed in the literature of mechanism design and our solution is very significant in the presence of large-scale online social networks.",paper studi auction design problem seller sell commod social network individu seller buyer onli communic neighbor challeng seller design mechan incentiv buyer awar auction propag inform neighbor buyer particip auction henc seller abl make higher revenu propos novel auction mechan call inform diffus mechan idm incentiv buyer onli truth report valuat commod seller also propag auction inform neighbor comparison direct extens well known vickrey clark grove vcg mechan social network also incentiv inform diffus decreas seller revenu even lead deficit sometim formal problem yet address literatur mechan design solut veri signific presenc larg scale onlin social network,"['Bin Li', 'Dong Hao', 'Dengji Zhao', 'Tao Zhou']",['cs.GT'],False,False,True,False,False,False
696,2017-03-28T14:08:38Z,2017-02-11T00:29:30Z,http://arxiv.org/abs/1702.04240v1,http://arxiv.org/pdf/1702.04240v1,Prospect Theory for Enhanced Cyber-Physical Security of Drone Delivery   Systems: A Network Interdiction Game,prospect theori enhanc cyber physic secur drone deliveri system network interdict game,"The use of unmanned aerial vehicles (UAVs) as delivery systems of online goods is rapidly becoming a global norm, as corroborated by Amazon's ""Prime Air"" and Google's ""Project Wing"" projects. However, the real-world deployment of such drone delivery systems faces many cyber-physical security challenges. In this paper, a novel mathematical framework for analyzing and enhancing the security of drone delivery systems is introduced. In this regard, a zero-sum network interdiction game is formulated between a vendor, operating a drone delivery system, and a malicious attacker. In this game, the vendor seeks to find the optimal path that its UAV should follow, to deliver a purchase from the vendor's warehouse to a customer location, to minimize the delivery time. Meanwhile, an attacker seeks to choose an optimal location to interdict the potential paths of the UAVs, so as to inflict cyber or physical damage to it, thus, maximizing its delivery time. First, the Nash equilibrium point of this game is characterized. Then, to capture the subjective behavior of both the vendor and attacker, new notions from prospect theory are incorporated into the game. These notions allow capturing the vendor's and attacker's i) subjective perception of attack success probabilities, and ii) their disparate subjective valuations of the achieved delivery times relative to a certain target delivery time. Simulation results have shown that the subjective decision making of the vendor and attacker leads to adopting risky path selection strategies which inflict delays to the delivery, thus, yielding unexpected delivery times which surpass the target delivery time set by the vendor.",use unman aerial vehicl uav deliveri system onlin good rapid becom global norm corrobor amazon prime air googl project wing project howev real world deploy drone deliveri system face mani cyber physic secur challeng paper novel mathemat framework analyz enhanc secur drone deliveri system introduc regard zero sum network interdict game formul vendor oper drone deliveri system malici attack game vendor seek find optim path uav follow deliv purchas vendor warehous custom locat minim deliveri time meanwhil attack seek choos optim locat interdict potenti path uav inflict cyber physic damag thus maxim deliveri time first nash equilibrium point game character captur subject behavior vendor attack new notion prospect theori incorpor game notion allow captur vendor attack subject percept attack success probabl ii dispar subject valuat achiev deliveri time relat certain target deliveri time simul result shown subject decis make vendor attack lead adopt riski path select strategi inflict delay deliveri thus yield unexpect deliveri time surpass target deliveri time set vendor,"['Anibal Sanjab', 'Walid Saad', 'Tamer Başar']","['cs.GT', 'cs.IT', 'math.IT']",False,False,True,False,False,False
698,2017-03-28T14:08:38Z,2017-02-09T23:58:38Z,http://arxiv.org/abs/1703.00899v1,http://arxiv.org/pdf/1703.00899v1,"Addendum to ""A Market Framework for Eliciting Private Data""",addendum market framework elicit privat data,"In Waggoner et al. [NIPS 2015], we proposed an elaboration on prediction markets that preserves differential privacy of transactions. This addendum gives a simpler, prediction-market focused construction and proofs. It also shows how to recover forms of a bounded worst-case loss guarantee by introducing a transaction fee.",waggon et al nip propos elabor predict market preserv differenti privaci transact addendum give simpler predict market focus construct proof also show recov form bound worst case loss guarante introduc transact fee,"['Bo Waggoner', 'Rafael Frongillo', 'Jacob Abernethy']",['cs.GT'],False,False,True,False,False,False
707,2017-03-28T14:10:09Z,2017-03-20T17:12:45Z,http://arxiv.org/abs/1703.06853v1,http://arxiv.org/pdf/1703.06853v1,Modulus consensus in discrete-time signed networks and properties of   special recurrent inequalities,modulus consensus discret time sign network properti special recurr inequ,"Recently the dynamics of signed networks, where the ties among the agents can be both positive (attractive) or negative (repulsive) have attracted substantial attention of the research community. Examples of such networks are models of opinion dynamics over signed graphs, recently introduced by Altafini (2012,2013) and extended to discrete-time case by Meng et al. (2014). It has been shown that under mild connectivity assumptions these protocols provide the convergence of opinions in absolute value, whereas their signs may differ. This ""modulus consensus"" may correspond to the polarization of the opinions (or bipartite consensus, including the usual consensus as a special case), or their convergence to zero. In this paper, we demonstrate that the phenomenon of modulus consensus in the discrete-time Altafini model is a manifestation of a more general and profound fact, regarding the solutions of a special recurrent inequality. Although such a recurrent inequality does not provide the uniqueness of a solution, it can be shown that, under some natural assumptions, each of its bounded solutions has a limit and, moreover, converges to consensus. A similar property has previously been established for special continuous-time differential inequalities (Proskurnikov, Cao, 2016). Besides analysis of signed networks, we link the consensus properties of recurrent inequalities to the convergence analysis of distributed optimization algorithms and the problems of Schur stability of substochastic matrices.",recent dynam sign network tie among agent posit attract negat repuls attract substanti attent research communiti exampl network model opinion dynam sign graph recent introduc altafini extend discret time case meng et al shown mild connect assumpt protocol provid converg opinion absolut valu wherea sign may differ modulus consensus may correspond polar opinion bipartit consensus includ usual consensus special case converg zero paper demonstr phenomenon modulus consensus discret time altafini model manifest general profound fact regard solut special recurr inequ although recurr inequ doe provid uniqu solut shown natur assumpt bound solut limit moreov converg consensus similar properti previous establish special continu time differenti inequ proskurnikov cao besid analysi sign network link consensus properti recurr inequ converg analysi distribut optim algorithm problem schur stabil substochast matric,"['Anton V. Proskurnikov', 'Ming Cao']","['cs.SY', 'cs.MA', 'math.OC', 'nlin.AO']",False,False,True,False,False,False
716,2017-03-28T14:10:13Z,2017-03-15T02:59:10Z,http://arxiv.org/abs/1703.04901v1,http://arxiv.org/pdf/1703.04901v1,On the Analysis of the DeGroot-Friedkin Model with Dynamic Relative   Interaction Matrices,analysi degroot friedkin model dynam relat interact matric,"This paper analyses the DeGroot-Friedkin model for evolution of the individuals' social powers in a social network when the network topology varies dynamically (described by dynamic relative interaction matrices). The DeGroot-Friedkin model describes how individual social power (self-appraisal, self-weight) evolves as a network of individuals discuss a sequence of issues. We seek to study dynamically changing relative interactions because interactions may change depending on the issue being discussed. In order to explore the problem in detail, two different cases of issue-dependent network topologies are studied. First, if the topology varies between issues in a periodic manner, it is shown that the individuals' self-appraisals admit a periodic solution. Second, if the topology changes arbitrarily, under the assumption that each relative interaction matrix is doubly stochastic and irreducible, the individuals' self-appraisals asymptotically converge to a unique non-trivial equilibrium.",paper analys degroot friedkin model evolut individu social power social network network topolog vari dynam describ dynam relat interact matric degroot friedkin model describ individu social power self apprais self weight evolv network individu discuss sequenc issu seek studi dynam chang relat interact becaus interact may chang depend issu discuss order explor problem detail two differ case issu depend network topolog studi first topolog vari issu period manner shown individu self apprais admit period solut second topolog chang arbitrarili assumpt relat interact matrix doubli stochast irreduc individu self apprais asymptot converg uniqu non trivial equilibrium,"['Mengbin Ye', 'Ji Liu', 'Brian David Outram Anderson', 'Changbin Yu', 'Tamer Başar']","['cs.SI', 'cs.MA', 'cs.SY', 'physics.soc-ph']",False,False,True,False,False,True
720,2017-03-28T14:10:17Z,2017-03-08T03:31:16Z,http://arxiv.org/abs/1703.02685v1,http://arxiv.org/pdf/1703.02685v1,New results on multi-agent system consensus: A graph signal processing   perspective,new result multi agent system consensus graph signal process perspect,"This paper revisits the problem of multi-agent consensus from a graph signal processing perspective. By defining the graph filter from the consensus protocol, we establish the direct relation between average consensus of multi-agent systems and filtering of graph signals. This relation not only provides new insights of the average consensus, it also turns out to be a powerful tool to design effective consensus protocols for uncertain networks, which is difficult to deal with by existing time-domain methods. In this paper, we consider two cases, one is uncertain networks modeled by an estimated Laplacian matrix and a fixed eigenvalue bound, the other is connected graphs with unknown topology. The consensus protocols are designed for both cases based on the protocol filter. Several numerical examples are given to demonstrate the effectiveness of our methods.",paper revisit problem multi agent consensus graph signal process perspect defin graph filter consensus protocol establish direct relat averag consensus multi agent system filter graph signal relat onli provid new insight averag consensus also turn power tool design effect consensus protocol uncertain network difficult deal exist time domain method paper consid two case one uncertain network model estim laplacian matrix fix eigenvalu bound connect graph unknown topolog consensus protocol design case base protocol filter sever numer exampl given demonstr effect method,"['Jing-Wen Yi', 'Li Chai']","['cs.SY', 'cs.MA']",False,False,True,False,False,False
721,2017-03-28T14:10:17Z,2017-03-07T14:33:00Z,http://arxiv.org/abs/1703.02399v1,http://arxiv.org/pdf/1703.02399v1,On time and consistency in multi-level agent-based simulations,time consist multi level agent base simul,"The integration of multiple viewpoints became an increasingly popular approach to deal with agent-based simulations. Despite their disparities, recent approaches successfully manage to run such multi-level simulations. Yet, are they doing it appropriately?   This paper tries to answer that question, with an analysis based on a generic model of the temporal dynamics of multi-level simulations. This generic model is then used to build an orthogonal approach to multi-level simulation called SIMILAR. In this approach, most time-related issues are explicitly modeled, owing to an implementation-oriented approach based on the influence/reaction principle.",integr multipl viewpoint becam increas popular approach deal agent base simul despit dispar recent approach success manag run multi level simul yet appropri paper tri answer question analysi base generic model tempor dynam multi level simul generic model use build orthogon approach multi level simul call similar approach time relat issu explicit model owe implement orient approach base influenc reaction principl,"['Gildas Morvan', 'Yoann Kubera']",['cs.MA'],False,False,True,False,False,True
723,2017-03-28T14:10:17Z,2017-03-07T03:16:22Z,http://arxiv.org/abs/1703.02196v1,http://arxiv.org/abs/1703.02196v1,Cooperative Epistemic Multi-Agent Planning for Implicit Coordination,cooper epistem multi agent plan implicit coordin,"Epistemic planning can be used for decision making in multi-agent situations with distributed knowledge and capabilities. Recently, Dynamic Epistemic Logic (DEL) has been shown to provide a very natural and expressive framework for epistemic planning. We extend the DEL-based epistemic planning framework to include perspective shifts, allowing us to define new notions of sequential and conditional planning with implicit coordination. With these, it is possible to solve planning tasks with joint goals in a decentralized manner without the agents having to negotiate about and commit to a joint policy at plan time. First we define the central planning notions and sketch the implementation of a planning system built on those notions. Afterwards we provide some case studies in order to evaluate the planner empirically and to show that the concept is useful for multi-agent systems in practice.",epistem plan use decis make multi agent situat distribut knowledg capabl recent dynam epistem logic del shown provid veri natur express framework epistem plan extend del base epistem plan framework includ perspect shift allow us defin new notion sequenti condit plan implicit coordin possibl solv plan task joint goal decentr manner without agent negoti commit joint polici plan time first defin central plan notion sketch implement plan system built notion afterward provid case studi order evalu planner empir show concept use multi agent system practic,"['Thorsten Engesser', 'Thomas Bolander', 'Robert Mattmüller', 'Bernhard Nebel']","['cs.AI', 'cs.LO', 'cs.MA']",False,False,True,False,False,True
725,2017-03-28T14:10:17Z,2017-03-06T15:44:10Z,http://arxiv.org/abs/1703.01931v1,http://arxiv.org/pdf/1703.01931v1,Context-Based Concurrent Experience Sharing in Multiagent Systems,context base concurr experi share multiag system,"One of the key challenges for multi-agent learning is scalability. In this paper, we introduce a technique for speeding up multi-agent learning by exploiting concurrent and incremental experience sharing. This solution adaptively identifies opportunities to transfer experiences between agents and allows for the rapid acquisition of appropriate policies in large-scale, stochastic, homogeneous multi-agent systems. We introduce an online, distributed, supervisor-directed transfer technique for constructing high-level characterizations of an agent's dynamic learning environment---called contexts---which are used to identify groups of agents operating under approximately similar dynamics within a short temporal window. A set of supervisory agents computes contextual information for groups of subordinate agents, thereby identifying candidates for experience sharing. Our method uses a tiered architecture to propagate, with low communication overhead, state, action, and reward data amongst the members of each dynamically-identified information-sharing group. We applied this method to a large-scale distributed task allocation problem with hundreds of information-sharing agents operating in an unknown, non-stationary environment. We demonstrate that our approach results in significant performance gains, that it is robust to noise-corrupted or suboptimal context features, and that communication costs scale linearly with the supervisor-to-subordinate ratio.",one key challeng multi agent learn scalabl paper introduc techniqu speed multi agent learn exploit concurr increment experi share solut adapt identifi opportun transfer experi agent allow rapid acquisit appropri polici larg scale stochast homogen multi agent system introduc onlin distribut supervisor direct transfer techniqu construct high level character agent dynam learn environ call context use identifi group agent oper approxim similar dynam within short tempor window set supervisori agent comput contextu inform group subordin agent therebi identifi candid experi share method use tier architectur propag low communic overhead state action reward data amongst member dynam identifi inform share group appli method larg scale distribut task alloc problem hundr inform share agent oper unknown non stationari environ demonstr approach result signific perform gain robust nois corrupt suboptim context featur communic cost scale linear supervisor subordin ratio,"['Dan Garant', 'Bruno da Silva', 'Victor Lesser', 'Chongjie Zhang']",['cs.MA'],False,False,True,False,False,True
726,2017-03-28T14:10:17Z,2017-03-01T14:42:20Z,http://arxiv.org/abs/1703.00320v1,http://arxiv.org/pdf/1703.00320v1,Investigating the Characteristics of One-Sided Matching Mechanisms Under   Various Preferences and Risk Attitudes,investig characterist one side match mechan various prefer risk attitud,"One-sided matching mechanisms are fundamental for assigning a set of indivisible objects to a set of self-interested agents when monetary transfers are not allowed. Two widely-studied randomized mechanisms in multiagent settings are the Random Serial Dictatorship (RSD) and the Probabilistic Serial Rule (PS). Both mechanisms require only that agents specify ordinal preferences and have a number of desirable economic and computational properties. However, the induced outcomes of the mechanisms are often incomparable and thus there are challenges when it comes to deciding which mechanism to adopt in practice. In this paper, we first consider the space of general ordinal preferences and provide empirical results on the (in)comparability of RSD and PS. We analyze their respective economic properties under general and lexicographic preferences. We then instantiate utility functions with the goal of gaining insights on the manipulability, efficiency, and envyfreeness of the mechanisms under different risk-attitude models. Our results hold under various preference distribution models, which further confirm the broad use of RSD in most practical applications.",one side match mechan fundament assign set indivis object set self interest agent monetari transfer allow two wide studi random mechan multiag set random serial dictatorship rsd probabilist serial rule ps mechan requir onli agent specifi ordin prefer number desir econom comput properti howev induc outcom mechan often incompar thus challeng come decid mechan adopt practic paper first consid space general ordin prefer provid empir result compar rsd ps analyz respect econom properti general lexicograph prefer instanti util function goal gain insight manipul effici envyfre mechan differ risk attitud model result hold various prefer distribut model confirm broad use rsd practic applic,"['Hadi Hosseini', 'Kate Larson', 'Robin Cohen']","['cs.GT', 'cs.AI', 'cs.MA', 'I.2.11; J.4']",False,False,True,False,False,True
730,2017-03-28T14:10:21Z,2017-02-27T21:03:26Z,http://arxiv.org/abs/1702.08529v1,http://arxiv.org/pdf/1702.08529v1,Multi-agent systems and decentralized artificial superintelligence,multi agent system decentr artifici superintellig,"Multi-agents systems communication is a technology, which provides a way for multiple interacting intelligent agents to communicate with each other and with environment. Multiple-agent systems are used to solve problems that are difficult for solving by individual agent. Multiple-agent communication technologies can be used for management and organization of computing fog and act as a global, distributed operating system. In present publication we suggest technology, which combines decentralized P2P BOINC general-purpose computing tasks distribution, multiple-agents communication protocol and smart-contract based rewards, powered by Ethereum blockchain. Such system can be used as distributed P2P computing power market, protected from any central authority. Such decentralized market can further be updated to system, which learns the most efficient way for software-hardware combinations usage and optimization. Once system learns to optimize software-hardware efficiency it can be updated to general-purpose distributed intelligence, which acts as combination of single-purpose AI.",multi agent system communic technolog provid way multipl interact intellig agent communic environ multipl agent system use solv problem difficult solv individu agent multipl agent communic technolog use manag organ comput fog act global distribut oper system present public suggest technolog combin decentr pp boinc general purpos comput task distribut multipl agent communic protocol smart contract base reward power ethereum blockchain system use distribut pp comput power market protect ani central author decentr market updat system learn effici way softwar hardwar combin usag optim onc system learn optim softwar hardwar effici updat general purpos distribut intellig act combin singl purpos ai,"['S. Ponomarev', 'A. E. Voronkov']",['cs.MA'],False,False,True,False,False,True
734,2017-03-28T14:10:21Z,2017-02-21T03:41:49Z,http://arxiv.org/abs/1702.06253v1,http://arxiv.org/pdf/1702.06253v1,Player Skill Decomposition in Multiplayer Online Battle Arenas,player skill decomposit multiplay onlin battl arena,"Successful analysis of player skills in video games has important impacts on the process of enhancing player experience without undermining their continuous skill development. Moreover, player skill analysis becomes more intriguing in team-based video games because such form of study can help discover useful factors in effective team formation. In this paper, we consider the problem of skill decomposition in MOBA (MultiPlayer Online Battle Arena) games, with the goal to understand what player skill factors are essential for the outcome of a game match. To understand the construct of MOBA player skills, we utilize various skill-based predictive models to decompose player skills into interpretative parts, the impact of which are assessed in statistical terms. We apply this analysis approach on two widely known MOBAs, namely League of Legends (LoL) and Defense of the Ancients 2 (DOTA2). The finding is that base skills of in-game avatars, base skills of players, and players' champion-specific skills are three prominent skill components influencing LoL's match outcomes, while those of DOTA2 are mainly impacted by in-game avatars' base skills but not much by the other two.",success analysi player skill video game import impact process enhanc player experi without undermin continu skill develop moreov player skill analysi becom intrigu team base video game becaus form studi help discov use factor effect team format paper consid problem skill decomposit moba multiplay onlin battl arena game goal understand player skill factor essenti outcom game match understand construct moba player skill util various skill base predict model decompos player skill interpret part impact assess statist term appli analysi approach two wide known moba name leagu legend lol defens ancient dota find base skill game avatar base skill player player champion specif skill three promin skill compon influenc lol match outcom dota main impact game avatar base skill much two,"['Zhengxing Chen', 'Yizhou Sun', 'Magy Seif El-nasr', 'Truong-Huy D. Nguyen']","['cs.SI', 'cs.HC', 'cs.MA']",False,False,True,False,False,False
737,2017-03-28T14:10:21Z,2017-02-17T20:39:38Z,http://arxiv.org/abs/1702.05515v1,http://arxiv.org/pdf/1702.05515v1,Overview: Generalizations of Multi-Agent Path Finding to Real-World   Scenarios,overview general multi agent path find real world scenario,"Multi-agent path finding (MAPF) is well-studied in artificial intelligence, robotics, theoretical computer science and operations research. We discuss issues that arise when generalizing MAPF methods to real-world scenarios and four research directions that address them. We emphasize the importance of addressing these issues as opposed to developing faster methods for the standard formulation of the MAPF problem.",multi agent path find mapf well studi artifici intellig robot theoret comput scienc oper research discuss issu aris general mapf method real world scenario four research direct address emphas import address issu oppos develop faster method standard formul mapf problem,"['Hang Ma', 'Sven Koenig', 'Nora Ayanian', 'Liron Cohen', 'Wolfgang Hoenig', 'T. K. Satish Kumar', 'Tansel Uras', 'Hong Xu', 'Craig Tovey', 'Guni Sharon']","['cs.AI', 'cs.MA', 'cs.RO']",False,False,True,False,False,False
738,2017-03-28T14:10:21Z,2017-02-25T09:23:38Z,http://arxiv.org/abs/1702.05355v2,http://arxiv.org/pdf/1702.05355v2,How Much Does Users' Psychology Matter in Engineering Mean-Field-Type   Games,much doe user psycholog matter engin mean field type game,"Until now mean-field-type game theory was not focused on cognitively-plausible models of choices in humans, animals, machines, robots, software-defined and mobile devices strategic interactions. This work presents some effects of users' psychology in mean-field-type games. In addition to the traditional ""material"" payoff modelling, psychological patterns are introduced in order to better capture and understand behaviors that are observed in engineering practice or in experimental settings. The psychological payoff value depends upon choices, mean-field states, mean-field actions, empathy and beliefs. It is shown that the affective empathy enforces mean-field equilibrium payoff equity and improves fairness between the players. It establishes equilibrium systems for such interactive decision-making problems. Basic empathy concepts are illustrated in several important problems in engineering including resource sharing, packet collision minimization, energy markets, and forwarding in Device-to-Device communications. The work conducts also an experiment with 47 people who have to decide whether to cooperate or not. The basic Interpersonal Reactivity Index of empathy metrics were used to measure the empathy distribution of each participant. Android app called Empathizer is developed to analyze systematically the data obtained from the participants. The experimental results reveal that the dominated strategies of the classical game theory are not dominated any more when users' psychology is involved, and a significant level of cooperation is observed among the users who are positively partially empathetic.",mean field type game theori focus cognit plausibl model choic human anim machin robot softwar defin mobil devic strateg interact work present effect user psycholog mean field type game addit tradit materi payoff model psycholog pattern introduc order better captur understand behavior observ engin practic experiment set psycholog payoff valu depend upon choic mean field state mean field action empathi belief shown affect empathi enforc mean field equilibrium payoff equiti improv fair player establish equilibrium system interact decis make problem basic empathi concept illustr sever import problem engin includ resourc share packet collis minim energi market forward devic devic communic work conduct also experi peopl decid whether cooper basic interperson reactiv index empathi metric use measur empathi distribut particip android app call empath develop analyz systemat data obtain particip experiment result reveal domin strategi classic game theori domin ani user psycholog involv signific level cooper observ among user posit partial empathet,"['Giulia Rossi', 'Alain Tcheukam', 'Hamidou Tembine']","['cs.GT', 'cs.MA']",False,False,True,False,False,False
740,2017-03-28T14:10:25Z,2017-02-14T10:13:15Z,http://arxiv.org/abs/1702.04138v1,http://arxiv.org/pdf/1702.04138v1,Agent Failures in All-Pay Auctions,agent failur pay auction,"All-pay auctions, a common mechanism for various human and agent interactions, suffers, like many other mechanisms, from the possibility of players' failure to participate in the auction. We model such failures, and fully characterize equilibrium for this class of games, we present a symmetric equilibrium and show that under some conditions the equilibrium is unique. We reveal various properties of the equilibrium, such as the lack of influence of the most-likely-to-participate player on the behavior of the other players. We perform this analysis with two scenarios: the sum-profit model, where the auctioneer obtains the sum of all submitted bids, and the max-profit model of crowdsourcing contests, where the auctioneer can only use the best submissions and thus obtains only the winning bid.   Furthermore, we examine various methods of influencing the probability of participation such as the effects of misreporting one's own probability of participating, and how influencing another player's participation chances changes the player's strategy.",pay auction common mechan various human agent interact suffer like mani mechan possibl player failur particip auction model failur fulli character equilibrium class game present symmetr equilibrium show condit equilibrium uniqu reveal various properti equilibrium lack influenc like particip player behavior player perform analysi two scenario sum profit model auction obtain sum submit bid max profit model crowdsourc contest auction onli use best submiss thus obtain onli win bid furthermor examin various method influenc probabl particip effect misreport one probabl particip influenc anoth player particip chanc chang player strategi,"['Yoad Lewenberg', 'Omer Lev', 'Yoram Bachrach', 'Jeffrey S. Rosenschein']","['cs.GT', 'cs.MA']",False,False,True,False,False,False
741,2017-03-28T14:10:25Z,2017-02-13T02:50:55Z,http://arxiv.org/abs/1702.03614v1,http://arxiv.org/pdf/1702.03614v1,Multitask diffusion adaptation over networks with common latent   representations,multitask diffus adapt network common latent represent,"Online learning with streaming data in a distributed and collaborative manner can be useful in a wide range of applications. This topic has been receiving considerable attention in recent years with emphasis on both single-task and multitask scenarios. In single-task adaptation, agents cooperate to track an objective of common interest, while in multitask adaptation agents track multiple objectives simultaneously. Regularization is one useful technique to promote and exploit similarity among tasks in the latter scenario. This work examines an alternative way to model relations among tasks by assuming that they all share a common latent feature representation. As a result, a new multitask learning formulation is presented and algorithms are developed for its solution in a distributed online manner. We present a unified framework to analyze the mean-square-error performance of the adaptive strategies, and conduct simulations to illustrate the theoretical findings and potential applications.",onlin learn stream data distribut collabor manner use wide rang applic topic receiv consider attent recent year emphasi singl task multitask scenario singl task adapt agent cooper track object common interest multitask adapt agent track multipl object simultan regular one use techniqu promot exploit similar among task latter scenario work examin altern way model relat among task assum share common latent featur represent result new multitask learn formul present algorithm develop solut distribut onlin manner present unifi framework analyz mean squar error perform adapt strategi conduct simul illustr theoret find potenti applic,"['Jie Chen', 'Cédric Richard', 'Ali H. Sayed']","['cs.MA', 'stat.ML']",False,False,True,False,False,True
742,2017-03-28T14:10:25Z,2017-02-12T04:53:25Z,http://arxiv.org/abs/1702.03488v1,http://arxiv.org/pdf/1702.03488v1,Octopus: A Framework for Cost-Quality-Time Optimization in Crowdsourcing,octopus framework cost qualiti time optim crowdsourc,"Managing micro-tasks on crowdsourcing marketplaces involves balancing conflicting objectives -- the quality of work, total cost incurred and time to completion. Previous agents have focused on cost-quality, or cost-time tradeoffs, limiting their real-world applicability. As a step towards this goal we present Octopus, the first AI agent that jointly manages all three objectives in tandem. Octopus is based on a computationally tractable, multi-agent formulation consisting of three components; one that sets the price per ballot to adjust the rate of completion of tasks, another that optimizes each task for quality and a third that performs task selection. We demonstrate that Octopus outperforms existing state-of-the-art approaches in simulation and experiments with real data, demonstrating its superior performance. We also deploy Octopus on Amazon Mechanical Turk to establish its ability to manage tasks in a real-world, dynamic setting.",manag micro task crowdsourc marketplac involv balanc conflict object qualiti work total cost incur time complet previous agent focus cost qualiti cost time tradeoff limit real world applic step toward goal present octopus first ai agent joint manag three object tandem octopus base comput tractabl multi agent formul consist three compon one set price per ballot adjust rate complet task anoth optim task qualiti third perform task select demonstr octopus outperform exist state art approach simul experi real data demonstr superior perform also deploy octopus amazon mechan turk establish abil manag task real world dynam set,"['Karan Goel', 'Shreya Rajpal', 'Mausam']","['cs.AI', 'cs.HC', 'cs.MA']",False,False,True,False,False,False
745,2017-03-28T14:10:25Z,2017-03-24T11:29:52Z,http://arxiv.org/abs/1702.03226v2,http://arxiv.org/pdf/1702.03226v2,An applied spatial agent-based model of administrative boundaries using   SEAL,appli spatial agent base model administr boundari use seal,"This paper extends and adapts an existing abstract model into an empirical metropolitan region in Brazil. The model - named SEAL: a Spatial Economic Agent-based Lab - comprehends a framework to enable public policy ex-ante analysis. The aim of the model is to use official data and municipalities spatial boundaries to allow for policy experimentation. The current version considers three markets: housing, labor and goods. Families' members age, consume, join the labor market and trade houses. A single consumption tax is collected by municipalities that invest back into quality of life improvements. We test whether a single metropolitan government - which is an aggregation of municipalities - would be in the best interest of its citizens. Preliminary results for 20 simulation runs indicate that it may be the case. Future developments include improving performance to enable running of higher percentage of the population and a number of runs that make the model more robust.",paper extend adapt exist abstract model empir metropolitan region brazil model name seal spatial econom agent base lab comprehend framework enabl public polici ex ant analysi aim model use offici data municip spatial boundari allow polici experiment current version consid three market hous labor good famili member age consum join labor market trade hous singl consumpt tax collect municip invest back qualiti life improv test whether singl metropolitan govern aggreg municip would best interest citizen preliminari result simul run indic may case futur develop includ improv perform enabl run higher percentag popul number run make model robust,"['Bernardo Alves Furtado', 'Isaque Daniel Eberhardt Rocha']","['cs.MA', 'q-fin.EC']",False,False,True,False,False,False
747,2017-03-28T14:10:25Z,2017-02-09T01:35:31Z,http://arxiv.org/abs/1702.04299v1,http://arxiv.org/pdf/1702.04299v1,Cyclic Dominance in the Spatial Coevolutionary Optional Prisoner's   Dilemma Game,cyclic domin spatial coevolutionari option prison dilemma game,"This paper studies scenarios of cyclic dominance in a coevolutionary spatial model in which game strategies and links between agents adaptively evolve over time. The Optional Prisoner's Dilemma (OPD) game is employed. The OPD is an extended version of the traditional Prisoner's Dilemma where players have a third option to abstain from playing the game. We adopt an agent-based simulation approach and use Monte Carlo methods to perform the OPD with coevolutionary rules. The necessary conditions to break the scenarios of cyclic dominance are also investigated. This work highlights that cyclic dominance is essential in the sustenance of biodiversity. Moreover, we also discuss the importance of a spatial coevolutionary model in maintaining cyclic dominance in adverse conditions.",paper studi scenario cyclic domin coevolutionari spatial model game strategi link agent adapt evolv time option prison dilemma opd game employ opd extend version tradit prison dilemma player third option abstain play game adopt agent base simul approach use mont carlo method perform opd coevolutionari rule necessari condit break scenario cyclic domin also investig work highlight cyclic domin essenti susten biodivers moreov also discuss import spatial coevolutionari model maintain cyclic domin advers condit,"['Marcos Cardinot', 'Josephine Griffith', ""Colm O'Riordan""]","['cs.GT', 'cs.MA', 'math.DS', 'physics.soc-ph']",False,False,True,False,False,True
749,2017-03-28T14:10:25Z,2017-02-08T17:49:31Z,http://arxiv.org/abs/1702.02541v1,http://arxiv.org/pdf/1702.02541v1,Modelling community formation driven by the status of individual in a   society,model communiti format driven status individu societi,"In human societies, people's willingness to compete and strive for better social status as well as being envious of those perceived in some way superior lead to social structures that are intrinsically hierarchical. Here we propose an agent-based, network model to mimic the ranking behaviour of individuals and its possible repercussions in human society. The main ingredient of the model is the assumption that the relevant feature of social interactions is each individual's keenness to maximise his or her status relative to others. The social networks produced by the model are homophilous and assortative, as frequently observed in human communities and most of the network properties seem quite independent of its size. However, it is seen that for small number of agents the resulting network consists of disjoint weakly connected communities while being highly assortative and homophilic. On the other hand larger networks turn out to be more cohesive with larger communities but less homophilic. We find that the reason for these changes is that larger network size allows agents to use new strategies for maximizing their social status allowing for more diverse links between them.",human societi peopl willing compet strive better social status well envious perceiv way superior lead social structur intrins hierarch propos agent base network model mimic rank behaviour individu possibl repercuss human societi main ingredi model assumpt relev featur social interact individu keen maximis status relat social network produc model homophil assort frequent observ human communiti network properti seem quit independ size howev seen small number agent result network consist disjoint weak connect communiti high assort homophil hand larger network turn cohes larger communiti less homophil find reason chang larger network size allow agent use new strategi maxim social status allow divers link,"['Jan E. Snellman', 'Gerardo Iñiguez', 'Tzipe Govezensky', 'Rafael A. Barrio', 'Kimmo K. Kaski']","['cs.MA', 'cs.SI', 'physics.soc-ph', 'I.2.11']",False,False,True,False,False,True
750,2017-03-28T14:10:29Z,2017-03-26T15:37:10Z,http://arxiv.org/abs/1702.00785v2,http://arxiv.org/pdf/1702.00785v2,Evaluation of Automated Vehicles Encountering Pedestrians at   Unsignalized Crossings,evalu autom vehicl encount pedestrian unsign cross,"Interactions between vehicles and pedestrians have always been a major problem in traffic safety. Experienced human drivers are able to analyze the environment and choose driving strategies that will help them avoid crashes. What is not yet clear, however, is how automated vehicles will interact with pedestrians. This paper proposes a new method for evaluating the safety and feasibility of the driving strategy of automated vehicles when encountering unsignalized crossings. MobilEye sensors installed on buses in Ann Arbor, Michigan, collected data on 2,973 valid crossing events. A stochastic interaction model was then created using a multivariate Gaussian mixture model. This model allowed us to simulate the movements of pedestrians reacting to an oncoming vehicle when approaching unsignalized crossings, and to evaluate the passing strategies of automated vehicles. A simulation was then conducted to demonstrate the evaluation procedure.",interact vehicl pedestrian alway major problem traffic safeti experienc human driver abl analyz environ choos drive strategi help avoid crash yet clear howev autom vehicl interact pedestrian paper propos new method evalu safeti feasibl drive strategi autom vehicl encount unsign cross mobiley sensor instal buse ann arbor michigan collect data valid cross event stochast interact model creat use multivari gaussian mixtur model model allow us simul movement pedestrian react oncom vehicl approach unsign cross evalu pass strategi autom vehicl simul conduct demonstr evalu procedur,"['Baiming Chen', 'Ding Zhao', 'Huei Peng']","['cs.MA', 'cs.RO', 'cs.SY']",False,False,True,False,False,False
751,2017-03-28T14:10:29Z,2017-02-01T14:45:48Z,http://arxiv.org/abs/1702.00290v1,http://arxiv.org/pdf/1702.00290v1,Attacking the V: On the Resiliency of Adaptive-Horizon MPC,attack resili adapt horizon mpc,"We introduce the concept of a V-formation game between a controller and an attacker, where controller's goal is to maneuver the plant (a simple model of flocking dynamics) into a V-formation, and the goal of the attacker is to prevent the controller from doing so. Controllers in V-formation games utilize a new formulation of model-predictive control we call Adaptive-Horizon MPC (AMPC), giving them extraordinary power: we prove that under certain controllability assumptions, an AMPC controller is able to attain V-formation with probability 1.   We define several classes of attackers, including those that in one move can remove R birds from the flock, or introduce random displacement into flock dynamics. We consider both naive attackers, whose strategies are purely probabilistic, and AMPC-enabled attackers, putting them on par strategically with the controllers. While an AMPC-enabled controller is expected to win every game with probability 1, in practice, it is resource-constrained: its maximum prediction horizon and the maximum number of game execution steps are fixed. Under these conditions, an attacker has a much better chance of winning a V-formation game.   Our extensive performance evaluation of V-formation games uses statistical model checking to estimate the probability an attacker can thwart the controller. Our results show that for the bird-removal game with R = 1, the controller almost always wins (restores the flock to a V-formation). For R = 2, the game outcome critically depends on which two birds are removed. For the displacement game, our results again demonstrate that an intelligent attacker, i.e. one that uses AMPC in this case, significantly outperforms its naive counterpart that randomly executes its attack.",introduc concept format game control attack control goal maneuv plant simpl model flock dynam format goal attack prevent control control format game util new formul model predict control call adapt horizon mpc ampc give extraordinari power prove certain control assumpt ampc control abl attain format probabl defin sever class attack includ one move remov bird flock introduc random displac flock dynam consid naiv attack whose strategi pure probabilist ampc enabl attack put par strateg control ampc enabl control expect win everi game probabl practic resourc constrain maximum predict horizon maximum number game execut step fix condit attack much better chanc win format game extens perform evalu format game use statist model check estim probabl attack thwart control result show bird remov game control almost alway win restor flock format game outcom critic depend two bird remov displac game result demonstr intellig attack one use ampc case signific outperform naiv counterpart random execut attack,"['Scott A. Smolka', 'Ashish Tiwari', 'Lukas Esterle', 'Anna Lukina', 'Junxing Yang', 'Radu Grosu']","['cs.SY', 'cs.MA']",False,False,True,False,False,True
752,2017-03-28T14:10:29Z,2017-01-31T16:16:41Z,http://arxiv.org/abs/1701.09112v1,http://arxiv.org/pdf/1701.09112v1,Socio-Affective Agents as Models of Human Behaviour in the Networked   Prisoner's Dilemma,socio affect agent model human behaviour network prison dilemma,"Affect Control Theory (ACT) is a powerful and general sociological model of human affective interaction. ACT provides an empirically derived mathematical model of culturally shared sentiments as heuristic guides for human decision making. BayesACT, a variant on classical ACT, combines affective reasoning with cognitive (denotative or logical) reasoning as is traditionally found in AI. Bayes\-ACT allows for the creation of agents that are both emotionally guided and goal-directed. In this work, we simulate BayesACT agents in the Iterated Networked Prisoner's Dilemma (INPD), and we show four out of five known properties of human play in INPD are replicated by these socio-affective agents. In particular, we show how the observed human behaviours of network structure invariance, anti-correlation of cooperation and reward, and player type stratification are all clearly emergent properties of the networked BayesACT agents. We further show that decision hyteresis (Moody Conditional Cooperation) is replicated by BayesACT agents in over $2/3$ of the cases we have considered. In contrast, previously used imitation-based agents are only able to replicate one of the five properties. We discuss the implications of these findings in the development of human-agent societies.",affect control theori act power general sociolog model human affect interact act provid empir deriv mathemat model cultur share sentiment heurist guid human decis make bayesact variant classic act combin affect reason cognit denot logic reason tradit found ai bay act allow creation agent emot guid goal direct work simul bayesact agent iter network prison dilemma inpd show four five known properti human play inpd replic socio affect agent particular show observ human behaviour network structur invari anti correl cooper reward player type stratif clear emerg properti network bayesact agent show decis hyteresi moodi condit cooper replic bayesact agent case consid contrast previous use imit base agent onli abl replic one five properti discuss implic find develop human agent societi,"['Joshua D. A. Jung', 'Jesse Hoey']",['cs.MA'],False,False,True,False,False,True
753,2017-03-28T14:10:29Z,2017-01-27T17:35:56Z,http://arxiv.org/abs/1701.08125v1,http://arxiv.org/pdf/1701.08125v1,Organic Computing in the Spotlight,organ comput spotlight,"Organic Computing is an initiative in the field of systems engineering that proposed to make use of concepts such as self-adaptation and self-organisation to increase the robustness of technical systems. Based on the observation that traditional design and operation concepts reach their limits, transferring more autonomy to the systems themselves should result in a reduction of complexity for users, administrators, and developers. However, there seems to be a need for an updated definition of the term ""Organic Computing"", of desired properties of technical, organic systems, and the objectives of the Organic Computing initiative. With this article, we will address these points.",organ comput initi field system engin propos make use concept self adapt self organis increas robust technic system base observ tradit design oper concept reach limit transfer autonomi system themselv result reduct complex user administr develop howev seem need updat definit term organ comput desir properti technic organ system object organ comput initi articl address point,"['Sven Tomforde', 'Bernhard Sick', 'Christian Müller-Schloer']","['cs.MA', 'cs.AI', '68T05', 'I.2.8, I.2.11']",False,False,True,False,False,True
754,2017-03-28T14:10:29Z,2017-01-27T14:15:12Z,http://arxiv.org/abs/1701.08058v1,http://arxiv.org/pdf/1701.08058v1,Optimal Communication Strategies in Networked Cyber-Physical Systems   with Adversarial Elements,optim communic strategi network cyber physic system adversari element,"This paper studies optimal communication and coordination strategies in cyber-physical systems for both defender and attacker within a game-theoretic framework. We model the communication network of a cyber-physical system as a sensor network which involves one single Gaussian source observed by many sensors, subject to additive independent Gaussian observation noises. The sensors communicate with the estimator over a coherent Gaussian multiple access channel. The aim of the receiver is to reconstruct the underlying source with minimum mean squared error. The scenario of interest here is one where some of the sensors are captured by the attacker and they act as the adversary (jammer): they strive to maximize distortion. The receiver (estimator) knows the captured sensors but still cannot simply ignore them due to the multiple access channel, i.e., the outputs of all sensors are summed to generate the estimator input. We show that the ability of transmitter sensors to secretly agree on a random event, that is ""coordination"", plays a key role in the analysis...",paper studi optim communic coordin strategi cyber physic system defend attack within game theoret framework model communic network cyber physic system sensor network involv one singl gaussian sourc observ mani sensor subject addit independ gaussian observ nois sensor communic estim coher gaussian multipl access channel aim receiv reconstruct sourc minimum mean squar error scenario interest one sensor captur attack act adversari jammer strive maxim distort receiv estim know captur sensor still cannot simpli ignor due multipl access channel output sensor sum generat estim input show abil transmitt sensor secret agre random event coordin play key role analysi,"['Emrah Akyol', 'Kenneth Rose', 'Tamer Basar', 'Cedric Langbort']","['cs.GT', 'cs.CR', 'cs.IT', 'cs.MA', 'math.IT']",False,False,True,False,False,False
757,2017-03-28T14:10:29Z,2017-01-25T14:23:25Z,http://arxiv.org/abs/1701.07331v1,http://arxiv.org/pdf/1701.07331v1,Identifying Key Cyber-Physical Terrain (Extended Version),identifi key cyber physic terrain extend version,"The high mobility of Army tactical networks, combined with their close proximity to hostile actors, elevates the risks associated with short-range network attacks. The connectivity model for such short range connections under active operations is extremely fluid, and highly dependent upon the physical space within which the element is operating, as well as the patterns of movement within that space. To handle these dependencies, we introduce the notion of ""key cyber-physical terrain"": locations within an area of operations that allow for effective control over the spread of proximity-dependent malware in a mobile tactical network, even as the elements of that network are in constant motion with an unpredictable pattern of node-to-node connectivity. We provide an analysis of movement models and approximation strategies for finding such critical nodes, and demonstrate via simulation that we can identify such key cyber-physical terrain quickly and effectively.",high mobil armi tactic network combin close proxim hostil actor elev risk associ short rang network attack connect model short rang connect activ oper extrem fluid high depend upon physic space within element oper well pattern movement within space handl depend introduc notion key cyber physic terrain locat within area oper allow effect control spread proxim depend malwar mobil tactic network even element network constant motion unpredict pattern node node connect provid analysi movement model approxim strategi find critic node demonstr via simul identifi key cyber physic terrain quick effect,"['Brian Thompson', 'Richard Harang']","['cs.CR', 'cs.MA']",False,False,True,False,False,False
759,2017-03-28T14:10:29Z,2017-03-06T20:14:55Z,http://arxiv.org/abs/1701.06307v2,http://arxiv.org/abs/1701.06307v2,A Tutorial on Modeling and Analysis of Dynamic Social Networks. Part I,tutori model analysi dynam social network part,"In recent years, we have observed a significant trend towards filling the gap between social network analysis and control. This trend was enabled by the introduction of new mathematical models describing dynamics of social groups, the advancement in complex networks theory and multi-agent systems, and the development of modern computational tools for big data analysis. The aim of this tutorial is to highlight a novel chapter of control theory, dealing with applications to social systems, to the attention of the broad research community. This paper is the first part of the tutorial, and it is focused on the most classical models of social dynamics and on their relations to the recent achievements in multi-agent systems.",recent year observ signific trend toward fill gap social network analysi control trend enabl introduct new mathemat model describ dynam social group advanc complex network theori multi agent system develop modern comput tool big data analysi aim tutori highlight novel chapter control theori deal applic social system attent broad research communiti paper first part tutori focus classic model social dynam relat recent achiev multi agent system,"['Anton V. Proskurnikov', 'Roberto Tempo']","['cs.SY', 'cs.MA', 'cs.SI', 'nlin.AO', 'physics.soc-ph']",False,False,True,False,False,False
763,2017-03-28T14:10:34Z,2017-01-05T12:02:34Z,http://arxiv.org/abs/1701.01289v1,http://arxiv.org/pdf/1701.01289v1,Applying DCOP to User Association Problem in Heterogeneous Networks with   Markov Chain Based Algorithm,appli dcop user associ problem heterogen network markov chain base algorithm,"Multi-agent systems (MAS) is able to characterize the behavior of individual agent and the interaction between agents. Thus, it motivates us to leverage the distributed constraint optimization problem (DCOP), a framework of modeling MAS, to solve the user association problem in heterogeneous networks (HetNets). Two issues we have to consider when we take DCOP into the application of HetNet including: (i) How to set up an effective model by DCOP taking account of the negtive impact of the increment of users on the modeling process (ii) Which kind of algorithms is more suitable to balance the time consumption and the quality of soltuion. Aiming to overcome these issues, we firstly come up with an ECAV-$\eta$ (Each Connection As Variable) model in which a parameter $\eta$ with an adequate assignment ($\eta=3$ in this paper) is able to control the scale of the model. After that, a Markov chain (MC) based algorithm is proposed on the basis of log-sum-exp function. Experimental results show that the solution obtained by DCOP framework is better than the one obtained by the Max-SINR algorithm. Comparing with the Lagrange dual decomposition based method (LDD), the solution performance has been improved since there is no need to transform original problem into a satisfied one. In addition, it is also apparent that the DCOP based method has better robustness than LDD when the number of users increases but the available resource at base stations are limited.",multi agent system mas abl character behavior individu agent interact agent thus motiv us leverag distribut constraint optim problem dcop framework model mas solv user associ problem heterogen network hetnet two issu consid take dcop applic hetnet includ set effect model dcop take account negtiv impact increment user model process ii kind algorithm suitabl balanc time consumpt qualiti soltuion aim overcom issu first come ecav eta connect variabl model paramet eta adequ assign eta paper abl control scale model markov chain mc base algorithm propos basi log sum exp function experiment result show solut obtain dcop framework better one obtain max sinr algorithm compar lagrang dual decomposit base method ldd solut perform improv sinc need transform origin problem satisfi one addit also appar dcop base method better robust ldd number user increas avail resourc base station limit,"['Peibo Duan', 'Guoqiang Mao', 'Changsheng Zhang', 'Bin Zhang']",['cs.MA'],False,False,True,False,False,True
764,2017-03-28T14:10:34Z,2017-01-05T05:44:25Z,http://arxiv.org/abs/1701.01216v1,http://arxiv.org/abs/1701.01216v1,Crowdsourcing with Tullock contests: A new perspective,crowdsourc tullock contest new perspect,"Incentive mechanisms for crowdsourcing have been extensively studied under the framework of all-pay auctions. Along a distinct line, this paper proposes to use Tullock contests as an alternative tool to design incentive mechanisms for crowdsourcing. We are inspired by the conduciveness of Tullock contests to attracting user entry (yet not necessarily a higher revenue) in other domains. In this paper, we explore a new dimension in optimal Tullock contest design, by superseding the contest prize---which is fixed in conventional Tullock contests---with a prize function that is dependent on the (unknown) winner's contribution, in order to maximize the crowdsourcer's utility. We show that this approach leads to attractive practical advantages: (a) it is well-suited for rapid prototyping in fully distributed web agents and smartphone apps; (b) it overcomes the disincentive to participate caused by players' antagonism to an increasing number of rivals. Furthermore, we optimize conventional, fixed-prize Tullock contests to construct the most superior benchmark to compare against our mechanism. Through extensive evaluations, we show that our mechanism significantly outperforms the optimal benchmark, by over three folds on the crowdsourcer's utility cum profit and up to nine folds on the players' social welfare.",incent mechan crowdsourc extens studi framework pay auction along distinct line paper propos use tullock contest altern tool design incent mechan crowdsourc inspir conduc tullock contest attract user entri yet necessarili higher revenu domain paper explor new dimens optim tullock contest design supersed contest prize fix convent tullock contest prize function depend unknown winner contribut order maxim crowdsourc util show approach lead attract practic advantag well suit rapid prototyp fulli distribut web agent smartphon app overcom disincent particip caus player antagon increas number rival furthermor optim convent fix prize tullock contest construct superior benchmark compar mechan extens evalu show mechan signific outperform optim benchmark three fold crowdsourc util cum profit nine fold player social welfar,"['T. Luo', 'S. S. Kanhere', 'H-P. Tan', 'F. Wu', 'H. Wu']","['cs.GT', 'cs.HC', 'cs.MA', 'cs.NI']",False,False,True,False,False,False
765,2017-03-28T14:10:34Z,2016-12-30T09:31:17Z,http://arxiv.org/abs/1612.09433v1,http://arxiv.org/abs/1612.09433v1,Curiosity-Aware Bargaining,curios awar bargain,"Opponent modeling consists in modeling the strategy or preferences of an agent thanks to the data it provides. In the context of automated negotiation and with machine learning, it can result in an advantage so overwhelming that it may restrain some casual agents to be part of the bargaining process. We qualify as ""curious"" an agent driven by the desire of negotiating in order to collect information and improve its opponent model. However, neither curiosity-based rational-ity nor curiosity-robust protocol have been studied in automatic negotiation. In this paper, we rely on mechanism design to propose three extensions of the standard bargaining protocol that limit information leak. Those extensions are supported by an enhanced rationality model, that considers the exchanged information. Also, they are theoretically analyzed and experimentally evaluated.",oppon model consist model strategi prefer agent thank data provid context autom negoti machin learn result advantag overwhelm may restrain casual agent part bargain process qualifi curious agent driven desir negoti order collect inform improv oppon model howev neither curios base ration iti curios robust protocol studi automat negoti paper reli mechan design propos three extens standard bargain protocol limit inform leak extens support enhanc ration model consid exchang inform also theoret analyz experiment evalu,"['Cédric Buron', 'Sylvain Ductor', 'Zahia Guessoum']","['cs.AI', 'cs.MA']",False,False,True,False,False,False
767,2017-03-28T14:10:34Z,2016-12-27T09:37:56Z,http://arxiv.org/abs/1612.08552v1,http://arxiv.org/pdf/1612.08552v1,A Hybrid Network/Grid Model of Urban Morphogenesis and Optimization,hybrid network grid model urban morphogenesi optim,"We describe a hybrid agent-based model and simulation of urban morphogenesis. It consists of a cellular automata grid coupled to a dynamic network topology. The inherently heterogeneous properties of urban structure and function are taken into account in the dynamics of the system. We propose various layout and performance measures to categorize and explore the generated configurations. An economic evaluation metric was also designed using the sensitivity of segregation models to spatial configuration. Our model is applied to a real-world case, offering a means to optimize the distribution of activities in a zoning context.",describ hybrid agent base model simul urban morphogenesi consist cellular automata grid coupl dynam network topolog inher heterogen properti urban structur function taken account dynam system propos various layout perform measur categor explor generat configur econom evalu metric also design use sensit segreg model spatial configur model appli real world case offer mean optim distribut activ zone context,"['Juste Raimbault', 'Arnaud Banos', 'René Doursat']","['cs.MA', 'physics.soc-ph']",False,False,True,False,False,False
768,2017-03-28T14:10:34Z,2016-12-26T09:41:56Z,http://arxiv.org/abs/1612.08351v1,http://arxiv.org/pdf/1612.08351v1,"Network, Popularity and Social Cohesion: A Game-Theoretic Approach",network popular social cohes game theoret approach,"In studies of social dynamics, cohesion refers to a group's tendency to stay in unity, which -- as argued in sociometry -- arises from the network topology of interpersonal ties between members of the group. We follow this idea and propose a game-based model of cohesion that not only relies on the social network, but also reflects individuals' social needs. In particular, our model is a type of cooperative games where players may gain popularity by strategically forming groups. A group is socially cohesive if the grand coalition is core stable. We study social cohesion in some special types of graphs and draw a link between social cohesion and the classical notion of structural cohesion. We then focus on the problem of deciding whether a given social network is socially cohesive and show that this problem is CoNP-complete. Nevertheless, we give two efficient heuristics for coalition structures where players enjoy high popularity and experimentally evaluate their performances.",studi social dynam cohes refer group tendenc stay uniti argu sociometri aris network topolog interperson tie member group follow idea propos game base model cohes onli reli social network also reflect individu social need particular model type cooper game player may gain popular strateg form group group social cohes grand coalit core stabl studi social cohes special type graph draw link social cohes classic notion structur cohes focus problem decid whether given social network social cohes show problem conp complet nevertheless give two effici heurist coalit structur player enjoy high popular experiment evalu perform,"['Jiamou Liu', 'Ziheng Wei']","['cs.SI', 'cs.CC', 'cs.GT', 'cs.MA', '91D30, 91Cxx, 91A40, 91A12, 68T42, 68Q15', 'I.2.11; J.4; F.1.3']",False,False,True,False,False,True
770,2017-03-28T14:10:38Z,2017-01-19T16:03:42Z,http://arxiv.org/abs/1612.08048v2,http://arxiv.org/pdf/1612.08048v2,Liquid Democracy: An Analysis in Binary Aggregation and Diffusion,liquid democraci analysi binari aggreg diffus,"The paper proposes an analysis of liquid democracy (or, delegable proxy voting) from the perspective of binary aggregation and of binary diffusion models. We show how liquid democracy on binary issues can be embedded into the framework of binary aggregation with abstentions, enabling the transfer of known results about the latter---such as impossibility theorems---to the former. This embedding also sheds light on the relation between delegation cycles in liquid democracy and the probability of collective abstentions, as well as the issue of individual rationality in a delegable proxy voting setting. We then show how liquid democracy on binary issues can be modeled and analyzed also as a specific process of dynamics of binary opinions on networks. These processes---called Boolean DeGroot processes---are a special case of the DeGroot stochastic model of opinion diffusion. We establish the convergence conditions of such processes and show they provide some novel insights on how the effects of delegation cycles and individual rationality could be mitigated within liquid democracy.   The study is a first attempt to provide theoretical foundations to the delgable proxy features of the liquid democracy voting system. Our analysis suggests recommendations on how the system may be modified to make it more resilient with respect to the handling of delegation cycles and of inconsistent majorities.",paper propos analysi liquid democraci deleg proxi vote perspect binari aggreg binari diffus model show liquid democraci binari issu embed framework binari aggreg abstent enabl transfer known result latter imposs theorem former embed also shed light relat deleg cycl liquid democraci probabl collect abstent well issu individu ration deleg proxi vote set show liquid democraci binari issu model analyz also specif process dynam binari opinion network process call boolean degroot process special case degroot stochast model opinion diffus establish converg condit process show provid novel insight effect deleg cycl individu ration could mitig within liquid democraci studi first attempt provid theoret foundat delgabl proxi featur liquid democraci vote system analysi suggest recommend system may modifi make resili respect handl deleg cycl inconsist major,"['Zoé Christoff', 'Davide Grossi']","['cs.MA', 'cs.AI', 'cs.SI']",False,False,True,False,False,False
772,2017-03-28T14:10:38Z,2016-12-21T11:16:50Z,http://arxiv.org/abs/1612.07059v1,http://arxiv.org/pdf/1612.07059v1,ARES: Adaptive Receding-Horizon Synthesis of Optimal Plans,adapt reced horizon synthesi optim plan,"We introduce ARES, an efficient approximation algorithm for generating optimal plans (action sequences) that take an initial state of a Markov Decision Process (MDP) to a state whose cost is below a specified (convergence) threshold. ARES uses Particle Swarm Optimization, with adaptive sizing for both the receding horizon and the particle swarm. Inspired by Importance Splitting, the length of the horizon and the number of particles are chosen such that at least one particle reaches a next-level state, that is, a state where the cost decreases by a required delta from the previous-level state. The level relation on states and the plans constructed by ARES implicitly define a Lyapunov function and an optimal policy, respectively, both of which could be explicitly generated by applying ARES to all states of the MDP, up to some topological equivalence relation. We also assess the effectiveness of ARES by statistically evaluating its rate of success in generating optimal plans. The ARES algorithm resulted from our desire to clarify if flying in V-formation is a flocking policy that optimizes energy conservation, clear view, and velocity alignment. That is, we were interested to see if one could find optimal plans that bring a flock from an arbitrary initial state to a state exhibiting a single connected V-formation. For flocks with 7 birds, ARES is able to generate a plan that leads to a V-formation in 95% of the 8,000 random initial configurations within 63 seconds, on average. ARES can also be easily customized into a model-predictive controller (MPC) with an adaptive receding horizon and statistical guarantees of convergence. To the best of our knowledge, our adaptive-sizing approach is the first to provide convergence guarantees in receding-horizon techniques.",introduc effici approxim algorithm generat optim plan action sequenc take initi state markov decis process mdp state whose cost specifi converg threshold use particl swarm optim adapt size reced horizon particl swarm inspir import split length horizon number particl chosen least one particl reach next level state state cost decreas requir delta previous level state level relat state plan construct implicit defin lyapunov function optim polici respect could explicit generat appli state mdp topolog equival relat also assess effect statist evalu rate success generat optim plan algorithm result desir clarifi fli format flock polici optim energi conserv clear view veloc align interest see one could find optim plan bring flock arbitrari initi state state exhibit singl connect format flock bird abl generat plan lead format random initi configur within second averag also easili custom model predict control mpc adapt reced horizon statist guarante converg best knowledg adapt size approach first provid converg guarante reced horizon techniqu,"['Anna Lukina', 'Lukas Esterle', 'Christian Hirsch', 'Ezio Bartocci', 'Junxing Yang', 'Ashish Tiwari', 'Scott A. Smolka', 'Radu Grosu']","['cs.AI', 'cs.MA', 'cs.SY']",False,False,True,False,False,True
773,2017-03-28T14:10:38Z,2017-02-20T17:54:11Z,http://arxiv.org/abs/1612.06340v2,http://arxiv.org/pdf/1612.06340v2,Computing Human-Understandable Strategies,comput human understand strategi,"Algorithms for equilibrium computation generally make no attempt to ensure that the computed strategies are understandable by humans. For instance the strategies for the strongest poker agents are represented as massive binary files. In many situations, we would like to compute strategies that can actually be implemented by humans, who may have computational limitations and may only be able to remember a small number of features or components of the strategies that have been computed. We study poker games where private information distributions can be arbitrary. We create a large training set of game instances and solutions, by randomly selecting the information probabilities, and present algorithms that learn from the training instances in order to perform well in games with unseen information distributions. We are able to conclude several new fundamental rules about poker strategy that can be easily implemented by humans.",algorithm equilibrium comput general make attempt ensur comput strategi understand human instanc strategi strongest poker agent repres massiv binari file mani situat would like comput strategi actual implement human may comput limit may onli abl rememb small number featur compon strategi comput studi poker game privat inform distribut arbitrari creat larg train set game instanc solut random select inform probabl present algorithm learn train instanc order perform well game unseen inform distribut abl conclud sever new fundament rule poker strategi easili implement human,"['Sam Ganzfried', 'Farzana Yusuf']","['cs.GT', 'cs.AI', 'cs.LG', 'cs.MA']",False,False,True,False,False,True
777,2017-03-28T14:10:38Z,2016-12-14T07:15:18Z,http://arxiv.org/abs/1612.04512v1,http://arxiv.org/pdf/1612.04512v1,Agent-based Model for Spot and Balancing Electricity Markets,agent base model spot balanc electr market,"We present a simple, yet realistic, agent-based model of an electricity market. The proposed model combines the spot and balancing markets with a resolution of one minute, which enables a more accurate depiction of the physical properties of the power grid. As a test, we compare the results obtained from our simulation to data from Nord Pool.",present simpl yet realist agent base model electr market propos model combin spot balanc market resolut one minut enabl accur depict physic properti power grid test compar result obtain simul data nord pool,"['Florian Kühnlenz', 'Pedro H. J. Nardelli']","['cs.MA', 'q-fin.GN']",False,False,True,False,False,False
779,2017-03-28T14:10:38Z,2016-12-13T17:51:59Z,http://arxiv.org/abs/1612.04299v1,http://arxiv.org/pdf/1612.04299v1,Algorithms for Graph-Constrained Coalition Formation in the Real World,algorithm graph constrain coalit format real world,"Coalition formation typically involves the coming together of multiple, heterogeneous, agents to achieve both their individual and collective goals. In this paper, we focus on a special case of coalition formation known as Graph-Constrained Coalition Formation (GCCF) whereby a network connecting the agents constrains the formation of coalitions. We focus on this type of problem given that in many real-world applications, agents may be connected by a communication network or only trust certain peers in their social network. We propose a novel representation of this problem based on the concept of edge contraction, which allows us to model the search space induced by the GCCF problem as a rooted tree. Then, we propose an anytime solution algorithm (CFSS), which is particularly efficient when applied to a general class of characteristic functions called $m+a$ functions. Moreover, we show how CFSS can be efficiently parallelised to solve GCCF using a non-redundant partition of the search space. We benchmark CFSS on both synthetic and realistic scenarios, using a real-world dataset consisting of the energy consumption of a large number of households in the UK. Our results show that, in the best case, the serial version of CFSS is 4 orders of magnitude faster than the state of the art, while the parallel version is 9.44 times faster than the serial version on a 12-core machine. Moreover, CFSS is the first approach to provide anytime approximate solutions with quality guarantees for very large systems of agents (i.e., with more than 2700 agents).",coalit format typic involv come togeth multipl heterogen agent achiev individu collect goal paper focus special case coalit format known graph constrain coalit format gccf wherebi network connect agent constrain format coalit focus type problem given mani real world applic agent may connect communic network onli trust certain peer social network propos novel represent problem base concept edg contract allow us model search space induc gccf problem root tree propos anytim solut algorithm cfss particular effici appli general class characterist function call function moreov show cfss effici parallelis solv gccf use non redund partit search space benchmark cfss synthet realist scenario use real world dataset consist energi consumpt larg number household uk result show best case serial version cfss order magnitud faster state art parallel version time faster serial version core machin moreov cfss first approach provid anytim approxim solut qualiti guarante veri larg system agent agent,"['Filippo Bistaffa', 'Alessandro Farinelli', 'Jesús Cerquides', 'Juan A. Rodríguez-Aguilar', 'Sarvapali D. Ramchurn']","['cs.MA', 'cs.AI', 'I.2']",False,False,True,False,False,True
780,2017-03-28T14:10:43Z,2016-12-11T16:22:27Z,http://arxiv.org/abs/1612.03433v1,http://arxiv.org/abs/1612.03433v1,A Model of Multi-Agent Consensus for Vague and Uncertain Beliefs,model multi agent consensus vagu uncertain belief,"Consensus formation is investigated for multi-agent systems in which agents' beliefs are both vague and uncertain. Vagueness is represented by a third truth state meaning \emph{borderline}. This is combined with a probabilistic model of uncertainty. A belief combination operator is then proposed which exploits borderline truth values to enable agents with conflicting beliefs to reach a compromise. A number of simulation experiments are carried out in which agents apply this operator in pairwise interactions, under the bounded confidence restriction that the two agents' beliefs must be sufficiently consistent with each other before agreement can be reached. As well as studying the consensus operator in isolation we also investigate scenarios in which agents are influenced either directly or indirectly by the state of the world. For the former we conduct simulations which combine consensus formation with belief updating based on evidence. For the latter we investigate the effect of assuming that the closer an agent's beliefs are to the truth the more visible they are in the consensus building process. In all cases applying the consensus operators results in the population converging to a single shared belief which is both crisp and certain. Furthermore, simulations which combine consensus formation with evidential updating converge faster to a shared opinion which is closer to the actual state of the world than those in which beliefs are only changed as a result of directly receiving new evidence. Finally, if agent interactions are guided by belief quality measured as similarity to the true state of the world, then applying the consensus operator alone results in the population converging to a high quality shared belief.",consensus format investig multi agent system agent belief vagu uncertain vagu repres third truth state mean emph borderlin combin probabilist model uncertainti belief combin oper propos exploit borderlin truth valu enabl agent conflict belief reach compromis number simul experi carri agent appli oper pairwis interact bound confid restrict two agent belief must suffici consist befor agreement reach well studi consensus oper isol also investig scenario agent influenc either direct indirect state world former conduct simul combin consensus format belief updat base evid latter investig effect assum closer agent belief truth visibl consensus build process case appli consensus oper result popul converg singl share belief crisp certain furthermor simul combin consensus format evidenti updat converg faster share opinion closer actual state world belief onli chang result direct receiv new evid final agent interact guid belief qualiti measur similar true state world appli consensus oper alon result popul converg high qualiti share belief,"['Michael Crosscombe', 'Jonathan Lawry']","['cs.MA', 'cs.AI']",False,False,True,False,False,True
782,2017-03-28T14:10:43Z,2016-12-06T23:09:42Z,http://arxiv.org/abs/1612.02067v1,http://arxiv.org/pdf/1612.02067v1,Collaborative Visual Area Coverage,collabor visual area coverag,"This article examines the problem of visual area coverage by a network of Mobile Aerial Agents (MAAs). Each MAA is assumed to be equipped with a downwards facing camera with a conical field of view which covers all points within a circle on the ground. The diameter of that circle is proportional to the altitude of the MAA, whereas the quality of the covered area decreases with the altitude. A distributed control law that maximizes a joint coverage-quality criterion by adjusting the MAAs' spatial coordinates is developed. The effectiveness of the proposed control scheme is evaluated through simulation studies.",articl examin problem visual area coverag network mobil aerial agent maa maa assum equip downward face camera conic field view cover point within circl ground diamet circl proport altitud maa wherea qualiti cover area decreas altitud distribut control law maxim joint coverag qualiti criterion adjust maa spatial coordin develop effect propos control scheme evalu simul studi,"['Sotiris Papatheodorou', 'Anthony Tzes', 'Yiannis Stergiopoulos']","['cs.SY', 'cs.MA']",False,False,True,False,False,False
785,2017-03-28T14:10:43Z,2016-12-05T17:09:34Z,http://arxiv.org/abs/1612.01434v1,http://arxiv.org/pdf/1612.01434v1,Proportional Rankings,proport rank,"In this paper we extend the principle of proportional representation to rankings. We consider the setting where alternatives need to be ranked based on approval preferences. In this setting, proportional representation requires that cohesive groups of voters are represented proportionally in each initial segment of the ranking. Proportional rankings are desirable in situations where initial segments of different lengths may be relevant, e.g., hiring decisions (if it is unclear how many positions are to be filled), the presentation of competing proposals on a liquid democracy platform (if it is unclear how many proposals participants are taking into consideration), or recommender systems (if a ranking has to accommodate different user types). We study the proportional representation provided by several ranking methods and prove theoretical guarantees. Furthermore, we experimentally evaluate these methods and present preliminary evidence as to which methods are most suitable for producing proportional rankings.",paper extend principl proport represent rank consid set altern need rank base approv prefer set proport represent requir cohes group voter repres proport initi segment rank proport rank desir situat initi segment differ length may relev hire decis unclear mani posit fill present compet propos liquid democraci platform unclear mani propos particip take consider recommend system rank accommod differ user type studi proport represent provid sever rank method prove theoret guarante furthermor experiment evalu method present preliminari evid method suitabl produc proport rank,"['Piotr Skowron', 'Martin Lackner', 'Markus Brill', 'Dominik Peters', 'Edith Elkind']","['cs.GT', 'cs.AI', 'cs.MA']",False,False,True,False,False,True
786,2017-03-28T14:10:43Z,2016-12-05T06:56:19Z,http://arxiv.org/abs/1612.01260v1,http://arxiv.org/pdf/1612.01260v1,Real-time Collision Handling in Railway Network:An Agent-based Approach,real time collis handl railway network agent base approach,"Advancement in intelligent transportation systems with complex operations requires autonomous planning and management to avoid collisions in day-to-day traffic. As failure and/or inadequacy in traffic safety system are life-critical, such collisions must be detected and resolved in an efficient way to manage continuously rising traffic. In this paper, we address different types of collision scenarios along with their early detection and resolution techniques in a complex railway system. In order to handle collisions dynamically in distributed manner, a novel agent based solution approach is proposed using the idea of max-sum algorithm, where each agent (train agent, station agent, and junction agent) communicates and cooperates with others to generate a good feasible solution that keeps the system in a safe state, i.e., collision free. We implement the proposed mechanism in Java Agent DEvelopment Framework (JADE). The results are evaluated with exhaustive experiments and compared with different existing collision handling methods to show the efficiency of our proposed approach.",advanc intellig transport system complex oper requir autonom plan manag avoid collis day day traffic failur inadequaci traffic safeti system life critic collis must detect resolv effici way manag continu rise traffic paper address differ type collis scenario along earli detect resolut techniqu complex railway system order handl collis dynam distribut manner novel agent base solut approach propos use idea max sum algorithm agent train agent station agent junction agent communic cooper generat good feasibl solut keep system safe state collis free implement propos mechan java agent develop framework jade result evalu exhaust experi compar differ exist collis handl method show effici propos approach,"['Poulami Dalapati', 'Abhijeet Padhy', 'Bhawana Mishra', 'Animesh Dutta', 'Swapan Bhattacharya']",['cs.MA'],False,False,True,False,False,True
788,2017-03-28T14:10:43Z,2016-12-03T00:36:50Z,http://arxiv.org/abs/1612.00903v1,http://arxiv.org/pdf/1612.00903v1,Expander Graph and Communication-Efficient Decentralized Optimization,expand graph communic effici decentr optim,"In this paper, we discuss how to design the graph topology to reduce the communication complexity of certain algorithms for decentralized optimization. Our goal is to minimize the total communication needed to achieve a prescribed accuracy. We discover that the so-called expander graphs are near-optimal choices. We propose three approaches to construct expander graphs for different numbers of nodes and node degrees. Our numerical results show that the performance of decentralized optimization is significantly better on expander graphs than other regular graphs.",paper discuss design graph topolog reduc communic complex certain algorithm decentr optim goal minim total communic need achiev prescrib accuraci discov call expand graph near optim choic propos three approach construct expand graph differ number node node degre numer result show perform decentr optim signific better expand graph regular graph,"['Yat-Tin Chow', 'Wei Shi', 'Tianyu Wu', 'Wotao Yin']","['math.OC', 'cs.DC', 'cs.MA']",False,False,True,False,False,True
791,2017-03-28T14:10:47Z,2016-11-30T14:04:32Z,http://arxiv.org/abs/1611.10154v1,http://arxiv.org/pdf/1611.10154v1,A Majoritarian Representative Voting System,majoritarian repres vote system,"We present an alternative voting system that aims at bridging the gap between proportional representative systems and majoritarian, single winner election systems. The system lets people vote for multiple parties, but then assigns each ballot to a single party. This opens a whole range of possible systems, all representative. We show theoretically that this space is convex. Then among the possible parliaments we present an algorithm to produce the most majoritarian result. We then test the system and compare the results with a pure proportional and a majoritarian voting system showing how the results are comparable with the majoritarian system. Then we simulate the system and show how it tends to produce parties of exponentially decreasing size with always a first, major party. Finally we describe how the system can be used in a context of a parliament made up of two separate houses.",present altern vote system aim bridg gap proport repres system majoritarian singl winner elect system system let peopl vote multipl parti assign ballot singl parti open whole rang possibl system repres show theoret space convex among possibl parliament present algorithm produc majoritarian result test system compar result pure proport majoritarian vote system show result compar majoritarian system simul system show tend produc parti exponenti decreas size alway first major parti final describ system use context parliament made two separ hous,"['Pietro Speroni di Fenizio', 'Daniele A. Gewurz']","['cs.GT', 'cs.MA', 'math.CO']",False,False,True,False,False,True
793,2017-03-28T14:10:47Z,2016-11-30T03:31:34Z,http://arxiv.org/abs/1611.09987v1,http://arxiv.org/abs/1611.09987v1,Digraphs with Distinguishable Dynamics under the Multi-Agent Agreement   Protocol,digraph distinguish dynam multi agent agreement protocol,"In this work, the ability to distinguish digraphs from the output response of some observing agents in a multi-agent network under the agreement protocol has been studied. Given a fixed observation point, it is desired to find sufficient graphical conditions under which the failure of a set of edges in the network information flow digraph is distinguishable from another set. When the latter is empty, this corresponds to the detectability of the former link set given the response of the observing agent. In developing the results, a powerful extension of the all-minors matrix tree theorem in algebraic graph theory is proved which relates the minors of the transformed Laplacian of a directed graph to the number and length of the shortest paths between its vertices. The results reveal an intricate relationship between the ability to distinguish the responses of a healthy and a faulty multi-agent network and the inter-nodal paths in their information flow digraphs. The results have direct implications for the operation and design of multi-agent systems subject to multiple link losses. Simulations and examples are presented to illustrate the analytic findings.",work abil distinguish digraph output respons observ agent multi agent network agreement protocol studi given fix observ point desir find suffici graphic condit failur set edg network inform flow digraph distinguish anoth set latter empti correspond detect former link set given respons observ agent develop result power extens minor matrix tree theorem algebra graph theori prove relat minor transform laplacian direct graph number length shortest path vertic result reveal intric relationship abil distinguish respons healthi faulti multi agent network inter nodal path inform flow digraph result direct implic oper design multi agent system subject multipl link loss simul exampl present illustr analyt find,"['M. Amin Rahimian', 'Amir Ajorlou', 'Amir G. Aghdam']","['cs.SY', 'cs.MA', 'cs.RO', 'math.DS']",False,False,True,False,False,False
796,2017-03-28T14:10:47Z,2016-11-26T09:47:07Z,http://arxiv.org/abs/1611.08691v1,http://arxiv.org/pdf/1611.08691v1,Multiwinner Approval Rules as Apportionment Methods,multiwinn approv rule apportion method,"We establish a link between multiwinner elections and apportionment problems by showing how approval-based multiwinner election rules can be interpreted as methods of apportionment. We consider several multiwinner rules and observe that they induce apportionment methods that are well-established in the literature on proportional representation. For instance, we show that Proportional Approval Voting induces the D'Hondt method and that Monroe's rule induces the largest reminder method. We also consider properties of apportionment methods and exhibit multiwinner rules that induce apportionment methods satisfying these properties.",establish link multiwinn elect apportion problem show approv base multiwinn elect rule interpret method apportion consid sever multiwinn rule observ induc apportion method well establish literatur proport represent instanc show proport approv vote induc hondt method monro rule induc largest remind method also consid properti apportion method exhibit multiwinn rule induc apportion method satisfi properti,"['Markus Brill', 'Jean-François Laslier', 'Piotr Skowron']","['cs.GT', 'cs.AI', 'cs.MA']",False,False,True,False,False,False
798,2017-03-28T14:10:47Z,2016-11-22T18:44:08Z,http://arxiv.org/abs/1611.07454v1,http://arxiv.org/pdf/1611.07454v1,An Agent-Based Model of Message Propagation in the Facebook Electronic   Social Network,agent base model messag propag facebook electron social network,"A large scale agent-based model of common Facebook users was designed to develop an understanding of the underlying mechanism of information diffusion within online social networks at a micro-level analysis. The agent-based model network structure is based on a sample from Facebook. Using an erased configuration model and the idea of common neighbours, a new correction procedure was investigated to overcome the problem of missing graph edges to construct a representative sample of the Facebook network graph. The model parameters are based on assumptions and general activity patterns (such as posting rate, time spent on Facebook etc.) taken from general data on Facebook. Using the agent-based model, the impact of post length, post score and publisher's friend count on the spread of wall posts in several scenarios was analyzed. Findings indicated that post content has the highest impact on the success of post propagation. However, amusing and absorbing but lengthy posts (e.g. a funny video) do not spread as well as short but unremarkable ones (e.g. an interesting photo). In contrast to product adoption and disease spread propagation models, the absence of a similar ""epidemic"" threshold in Facebook post diffusion is observed.",larg scale agent base model common facebook user design develop understand mechan inform diffus within onlin social network micro level analysi agent base model network structur base sampl facebook use eras configur model idea common neighbour new correct procedur investig overcom problem miss graph edg construct repres sampl facebook network graph model paramet base assumpt general activ pattern post rate time spent facebook etc taken general data facebook use agent base model impact post length post score publish friend count spread wall post sever scenario analyz find indic post content highest impact success post propag howev amus absorb lengthi post funni video spread well short unremark one interest photo contrast product adopt diseas spread propag model absenc similar epidem threshold facebook post diffus observ,"['Hamid Reza Nasrinpour', 'Marcia R. Friesen', 'Robert D.', 'McLeod']","['cs.SI', 'cs.MA']",False,False,True,False,False,True
801,2017-03-28T14:10:52Z,2017-03-24T20:59:52Z,http://arxiv.org/abs/1703.08596v1,http://arxiv.org/pdf/1703.08596v1,The Inner Structure of Time-Dependent Signals,inner structur time depend signal,"This paper shows how a time series of measurements of an evolving system can be processed to create an inner time series that is unaffected by any instantaneous invertible, possibly nonlinear transformation of the measurements. An inner time series contains information that does not depend on the nature of the sensors, which the observer chose to monitor the system. Instead, it encodes information that is intrinsic to the evolution of the observed system. Because of its sensor-independence, an inner time series may produce fewer false negatives when it is used to detect events in the presence of sensor drift. Furthermore, if the observed physical system is comprised of non-interacting subsystems, its inner time series is separable; i.e., it consists of a collection of time series, each one being the inner time series of an isolated subsystem. Because of this property, an inner time series can be used to detect a specific behavior of one of the independent subsystems without using blind source separation to disentangle that subsystem from the others. The method is illustrated by applying it to: 1) an analytic example; 2) the audio waveform of one speaker; 3) video images from a moving camera; 4) mixtures of audio waveforms of two speakers.",paper show time seri measur evolv system process creat inner time seri unaffect ani instantan invert possibl nonlinear transform measur inner time seri contain inform doe depend natur sensor observ chose monitor system instead encod inform intrins evolut observ system becaus sensor independ inner time seri may produc fewer fals negat use detect event presenc sensor drift furthermor observ physic system compris non interact subsystem inner time seri separ consist collect time seri one inner time seri isol subsystem becaus properti inner time seri use detect specif behavior one independ subsystem without use blind sourc separ disentangl subsystem method illustr appli analyt exampl audio waveform one speaker video imag move camera mixtur audio waveform two speaker,['David N. Levin'],"['stat.ME', 'cs.SD', 'math.ST', 'stat.TH']",False,False,True,False,False,False
831,2017-03-28T14:11:05Z,2017-03-21T01:42:20Z,http://arxiv.org/abs/1702.06724v3,http://arxiv.org/pdf/1702.06724v3,A new cosine series antialiasing function and its application to   aliasing-free glottal source models for speech and singing synthesis,new cosin seri antialias function applic alias free glottal sourc model speech sing synthesi,We formulated and implemented a procedure to generate aliasing-free excitation source signals. It uses a new antialiasing filter in the continuous time domain followed by an IIR digital filter for response equalization. We introduced a cosine-series-based general design procedure for the new antialiasing function. We applied this new procedure to implement the antialiased Fujisaki-Ljungqvist model. We also applied it to revise our previous implementation of the antialiased Fant-Liljencrants model. A combination of these signals and a lattice implementation of the time varying vocal tract model provides a reliable and flexible basis to test fo extractors and source aperiodicity analysis methods. MATLAB implementations of these antialiased excitation source models are available as part of our open source tools for speech science.,formul implement procedur generat alias free excit sourc signal use new antialias filter continu time domain follow iir digit filter respons equal introduc cosin seri base general design procedur new antialias function appli new procedur implement antialias fujisaki ljungqvist model also appli revis previous implement antialias fant liljencr model combin signal lattic implement time vari vocal tract model provid reliabl flexibl basi test fo extractor sourc aperiod analysi method matlab implement antialias excit sourc model avail part open sourc tool speech scienc,"['Hideki Kawahara', 'Ken-Ichi Sakakibara', 'Hideki Banno', 'Masanori Morise', 'Tomoki Toda', 'Toshio Irino']",['cs.SD'],False,False,True,False,False,False
840,2017-03-28T14:11:09Z,2017-01-31T19:21:41Z,http://arxiv.org/abs/1702.00025v1,http://arxiv.org/pdf/1702.00025v1,An Experimental Analysis of the Entanglement Problem in   Neural-Network-based Music Transcription Systems,experiment analysi entangl problem neural network base music transcript system,"Several recent polyphonic music transcription systems have utilized deep neural networks to achieve state of the art results on various benchmark datasets, pushing the envelope on framewise and note-level performance measures. Unfortunately we can observe a sort of glass ceiling effect. To investigate this effect, we provide a detailed analysis of the particular kinds of errors that state of the art deep neural transcription systems make, when trained and tested on a piano transcription task. We are ultimately forced to draw a rather disheartening conclusion: the networks seem to learn combinations of notes, and have a hard time generalizing to unseen combinations of notes. Furthermore, we speculate on various means to alleviate this situation.",sever recent polyphon music transcript system util deep neural network achiev state art result various benchmark dataset push envelop framewis note level perform measur unfortun observ sort glass ceil effect investig effect provid detail analysi particular kind error state art deep neural transcript system make train test piano transcript task ultim forc draw rather dishearten conclus network seem learn combin note hard time general unseen combin note furthermor specul various mean allevi situat,"['Rainer Kelz', 'Gerhard Widmer']",['cs.SD'],False,False,True,False,False,False
850,2017-03-28T14:11:13Z,2017-01-09T15:10:38Z,http://arxiv.org/abs/1701.03834v1,http://arxiv.org/pdf/1701.03834v1,On Higher Order Positive Differential Energy Operator,higher order posit differenti energi oper,"The higher order differential energy operator (DEO), denoted via $\Upsilon_k(x)$, is an extension to the second order famous Teager-Kaiser operator. The DEO helps measuring the higher order gauge of energy of a signal which is useful for AM-FM demodulation. However, the energy criterion defined by the DEO is not compliant with the presumption of positivity of energy. In this paper we introduce a higher order operator called Positive Differential Energy Operator (PDEO). This operator which can be obtained using alternative recursive relations, resolves the energy sign problem. The simulations demonstrate that the proposed operator can outperform DEOs in terms of Average Signal to Error Ratio (ASER) in AM/FM demodulation.",higher order differenti energi oper deo denot via upsilon extens second order famous teager kaiser oper deo help measur higher order gaug energi signal use fm demodul howev energi criterion defin deo compliant presumpt posit energi paper introduc higher order oper call posit differenti energi oper pdeo oper obtain use altern recurs relat resolv energi sign problem simul demonstr propos oper outperform deo term averag signal error ratio aser fm demodul,"['Amirhossein Javaheri', 'Mohammad Bagher Shamsollahi']",['cs.SD'],False,False,True,False,False,False
877,2017-03-28T14:11:21Z,2017-01-19T15:33:53Z,http://arxiv.org/abs/1612.04056v2,http://arxiv.org/pdf/1612.04056v2,Joint Bayesian Gaussian discriminant analysis for speaker verification,joint bayesian gaussian discrimin analysi speaker verif,"State-of-the-art i-vector based speaker verification relies on variants of Probabilistic Linear Discriminant Analysis (PLDA) for discriminant analysis. We are mainly motivated by the recent work of the joint Bayesian (JB) method, which is originally proposed for discriminant analysis in face verification. We apply JB to speaker verification and make three contributions beyond the original JB. 1) In contrast to the EM iterations with approximated statistics in the original JB, the EM iterations with exact statistics are employed and give better performance. 2) We propose to do simultaneous diagonalization (SD) of the within-class and between-class covariance matrices to achieve efficient testing, which has broader application scope than the SVD-based efficient testing method in the original JB. 3) We scrutinize similarities and differences between various Gaussian PLDAs and JB, complementing the previous analysis of comparing JB only with Prince-Elder PLDA. Extensive experiments are conducted on NIST SRE10 core condition 5, empirically validating the superiority of JB with faster convergence rate and 9-13% EER reduction compared with state-of-the-art PLDA.",state art vector base speaker verif reli variant probabilist linear discrimin analysi plda discrimin analysi main motiv recent work joint bayesian jb method origin propos discrimin analysi face verif appli jb speaker verif make three contribut beyond origin jb contrast em iter approxim statist origin jb em iter exact statist employ give better perform propos simultan diagon sd within class class covari matric achiev effici test broader applic scope svd base effici test method origin jb scrutin similar differ various gaussian pldas jb complement previous analysi compar jb onli princ elder plda extens experi conduct nist sre core condit empir valid superior jb faster converg rate eer reduct compar state art plda,"['Yiyan Wang', 'Haotian Xu', 'Zhijian Ou']","['cs.SD', 'cs.LG']",False,False,True,False,False,False
880,2017-03-28T14:11:25Z,2016-12-12T00:13:35Z,http://arxiv.org/abs/1612.03505v1,http://arxiv.org/pdf/1612.03505v1,Convolutional Neural Networks for Passive Monitoring of a Shallow Water   Environment using a Single Sensor,convolut neural network passiv monitor shallow water environ use singl sensor,"A cost effective approach to remote monitoring of protected areas such as marine reserves and restricted naval waters is to use passive sonar to detect, classify, localize, and track marine vessel activity (including small boats and autonomous underwater vehicles). Cepstral analysis of underwater acoustic data enables the time delay between the direct path arrival and the first multipath arrival to be measured, which in turn enables estimation of the instantaneous range of the source (a small boat). However, this conventional method is limited to ranges where the Lloyd's mirror effect (interference pattern formed between the direct and first multipath arrivals) is discernible. This paper proposes the use of convolutional neural networks (CNNs) for the joint detection and ranging of broadband acoustic noise sources such as marine vessels in conjunction with a data augmentation approach for improving network performance in varied signal-to-noise ratio (SNR) situations. Performance is compared with a conventional passive sonar ranging method for monitoring marine vessel activity using real data from a single hydrophone mounted above the sea floor. It is shown that CNNs operating on cepstrum data are able to detect the presence and estimate the range of transiting vessels at greater distances than the conventional method.",cost effect approach remot monitor protect area marin reserv restrict naval water use passiv sonar detect classifi local track marin vessel activ includ small boat autonom underwat vehicl cepstral analysi underwat acoust data enabl time delay direct path arriv first multipath arriv measur turn enabl estim instantan rang sourc small boat howev convent method limit rang lloyd mirror effect interfer pattern form direct first multipath arriv discern paper propos use convolut neural network cnns joint detect rang broadband acoust nois sourc marin vessel conjunct data augment approach improv network perform vari signal nois ratio snr situat perform compar convent passiv sonar rang method monitor marin vessel activ use real data singl hydrophon mount abov sea floor shown cnns oper cepstrum data abl detect presenc estim rang transit vessel greater distanc convent method,"['Eric L. Ferguson', 'Rishi Ramakrishnan', 'Stefan B. Williams', 'Craig T. Jin']",['cs.SD'],False,False,True,False,True,False
887,2017-03-28T14:11:25Z,2016-12-03T19:17:29Z,http://arxiv.org/abs/1612.01010v1,http://arxiv.org/pdf/1612.01010v1,DeepBach: a Steerable Model for Bach chorales generation,deepbach steerabl model bach choral generat,"The composition of polyphonic chorale music in the style of J.S Bach has represented a major challenge in automatic music composition over the last decades. The art of Bach chorales composition involves combining four-part harmony with characteristic rhythmic patterns and typical melodic movements to produce musical phrases which begin, evolve and end (cadences) in a harmonious way. To our knowledge, no model so far was able to solve all these problems simultaneously using an agnostic machine-learning approach. This paper introduces DeepBach, a statistical model aimed at modeling polyphonic music and specifically four parts, hymn-like pieces. We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach. We evaluate how indistinguishable our generated chorales are from existing Bach chorales with a listening test. The results corroborate our claim. A key strength of DeepBach is that it is agnostic and flexible. Users can constrain the generation by imposing some notes, rhythms or cadences in the generated score. This allows users to reharmonize user-defined melodies. DeepBach's generation is fast, making it usable for interactive music composition applications. Several generation examples are provided and discussed from a musical point of view.",composit polyphon choral music style bach repres major challeng automat music composit last decad art bach choral composit involv combin four part harmoni characterist rhythmic pattern typic melod movement produc music phrase begin evolv end cadenc harmoni way knowledg model far abl solv problem simultan use agnost machin learn approach paper introduc deepbach statist model aim model polyphon music specif four part hymn like piec claim train choral harmon johann sebastian bach model capabl generat high convinc choral style bach evalu indistinguish generat choral exist bach choral listen test result corrobor claim key strength deepbach agnost flexibl user constrain generat impos note rhythm cadenc generat score allow user reharmon user defin melodi deepbach generat fast make usabl interact music composit applic sever generat exampl provid discuss music point view,"['Gaëtan Hadjeres', 'François Pachet']","['cs.AI', 'cs.SD']",False,False,True,False,False,False
890,2017-03-28T14:11:30Z,2016-12-01T08:25:45Z,http://arxiv.org/abs/1612.00171v1,http://arxiv.org/pdf/1612.00171v1,A Non Linear Multifractal Study to Illustrate the Evolution of Tagore   Songs Over a Century,non linear multifract studi illustr evolut tagor song centuri,The works of Rabindranath Tagore have been sung by various artistes over generations spanning over almost 100 years. there are few songs which were popular in the early years and have been able to retain their popularity over the years while some others have faded away. In this study we look to find cues for the singing style of these songs which have kept them alive for all these years. For this we took 3 min clip of four Tagore songs which have been sung by five generation of artistes over 100 years and analyze them with the help of latest nonlinear techniques Multifractal Detrended Fluctuation Analysis (MFDFA). The multifractal spectral width is a manifestation of the inherent complexity of the signal and may prove to be an important parameter to identify the singing style of particular generation of singers and how this style varies over different generations. The results are discussed in detail.,work rabindranath tagor sung various artist generat span almost year song popular earli year abl retain popular year fade away studi look find cue sing style song kept aliv year took min clip four tagor song sung five generat artist year analyz help latest nonlinear techniqu multifract detrend fluctuat analysi mfdfa multifract spectral width manifest inher complex signal may prove import paramet identifi sing style particular generat singer style vari differ generat result discuss detail,"['Shankha Sanyal', 'Archi Banerjee', 'Tarit Guhathakurata', 'Ranjan Sengupta', 'Dipak Ghosh']","['cs.SD', 'nlin.CD']",False,False,True,False,False,True
892,2017-03-28T14:11:30Z,2016-11-29T17:19:45Z,http://arxiv.org/abs/1611.09733v1,http://arxiv.org/abs/1611.09733v1,Getting Closer to the Essence of Music: The Con Espressione Manifesto,get closer essenc music con espression manifesto,"This text offers a personal and very subjective view on the current situation of Music Information Research (MIR). Motivated by the desire to build systems with a somewhat deeper understanding of music than the ones we currently have, I try to sketch a number of challenges for the next decade of MIR research, grouped around six simple truths about music that are probably generally agreed on, but often ignored in everyday research.",text offer person veri subject view current situat music inform research mir motiv desir build system somewhat deeper understand music one current tri sketch number challeng next decad mir research group around six simpl truth music probabl general agre often ignor everyday research,['Gerhard Widmer'],['cs.SD'],False,False,True,False,False,True
895,2017-03-28T14:11:30Z,2016-11-29T04:16:44Z,http://arxiv.org/abs/1611.09482v1,http://arxiv.org/pdf/1611.09482v1,Fast Wavenet Generation Algorithm,fast wavenet generat algorithm,"This paper presents an efficient implementation of the Wavenet generation process called Fast Wavenet. Compared to a naive implementation that has complexity O(2^L) (L denotes the number of layers in the network), our proposed approach removes redundant convolution operations by caching previous calculations, thereby reducing the complexity to O(L) time. Timing experiments show significant advantages of our fast implementation over a naive one. While this method is presented for Wavenet, the same scheme can be applied anytime one wants to perform autoregressive generation or online prediction using a model with dilated convolution layers. The code for our method is publicly available.",paper present effici implement wavenet generat process call fast wavenet compar naiv implement complex denot number layer network propos approach remov redund convolut oper cach previous calcul therebi reduc complex time time experi show signific advantag fast implement naiv one method present wavenet scheme appli anytim one want perform autoregress generat onlin predict use model dilat convolut layer code method public avail,"['Tom Le Paine', 'Pooya Khorrami', 'Shiyu Chang', 'Yang Zhang', 'Prajit Ramachandran', 'Mark A. Hasegawa-Johnson', 'Thomas S. Huang']","['cs.SD', 'cs.DS', 'cs.LG']",False,False,True,False,False,False
902,2017-03-28T14:02:27Z,2017-03-27T10:14:54Z,http://arxiv.org/abs/1703.08994v1,http://arxiv.org/pdf/1703.08994v1,Value of Information: Sensitivity Analysis and Research Design in   Bayesian Evidence Synthesis,valu inform sensit analysi research design bayesian evid synthesi,"Suppose we have a Bayesian model which combines evidence from several different sources. We want to know which model parameters most affect the estimate or decision from the model, or which of the parameter uncertainties drive the decision uncertainty. Furthermore we want to prioritise what further data should be collected. These questions can be addressed by Value of Information (VoI) analysis, in which we estimate expected reductions in loss from learning specific parameters or collecting data of a given design. We describe the theory and practice of VoI for Bayesian evidence synthesis, using and extending ideas from health economics, computer modelling and Bayesian design. The methods are general to a range of decision problems including point estimation and choices between discrete actions. We apply them to a model for estimating prevalence of HIV infection, combining indirect information from several surveys, registers and expert beliefs. This analysis shows which parameters contribute most of the uncertainty about each prevalence estimate, and provides the expected improvements in precision from collecting specific amounts of additional data.",suppos bayesian model combin evid sever differ sourc want know model paramet affect estim decis model paramet uncertainti drive decis uncertainti furthermor want prioritis data collect question address valu inform voi analysi estim expect reduct loss learn specif paramet collect data given design describ theori practic voi bayesian evid synthesi use extend idea health econom comput model bayesian design method general rang decis problem includ point estim choic discret action appli model estim preval hiv infect combin indirect inform sever survey regist expert belief analysi show paramet contribut uncertainti preval estim provid expect improv precis collect specif amount addit data,"['Christopher Jackson', 'Anne Presanis', 'Stefano Conti', 'Daniela De Angelis']",['stat.AP'],False,False,True,False,False,True
908,2017-03-28T14:02:27Z,2017-03-24T16:08:21Z,http://arxiv.org/abs/1703.08487v1,http://arxiv.org/pdf/1703.08487v1,Multiscale Granger causality,multiscal granger causal,"In the study of complex physical and biological systems represented by multivariate stochastic processes, an issue of great relevance is the description of the system dynamics spanning multiple temporal scales. While methods to assess the dynamic complexity of individual processes at different time scales are well-established, the multiscale evaluation of directed interactions between processes is complicated by theoretical and practical issues such as filtering and downsampling. Here we extend the very popular measure of Granger causality (GC), a prominent tool for assessing directed lagged interactions between joint processes, to quantify information transfer across multiple time scales. We show that the multiscale processing of a vector autoregressive (AR) process introduces a moving average (MA) component, and describe how to represent the resulting ARMA process using state space (SS) models and to combine the SS model parameters for computing exact GC values at arbitrarily large time scales. We exploit the theoretical formulation to identify peculiar features of multiscale GC in basic AR processes, and demonstrate with numerical simulations the much larger estimation accuracy of the SS approach compared with pure AR modeling of filtered and downsampled data. The improved computational reliability is exploited to disclose meaningful multiscale patterns of information transfer between global temperature and carbon dioxide concentration time series, both in paleoclimate and in recent years.",studi complex physic biolog system repres multivari stochast process issu great relev descript system dynam span multipl tempor scale method assess dynam complex individu process differ time scale well establish multiscal evalu direct interact process complic theoret practic issu filter downsampl extend veri popular measur granger causal gc promin tool assess direct lag interact joint process quantifi inform transfer across multipl time scale show multiscal process vector autoregress ar process introduc move averag compon describ repres result arma process use state space ss model combin ss model paramet comput exact gc valu arbitrarili larg time scale exploit theoret formul identifi peculiar featur multiscal gc basic ar process demonstr numer simul much larger estim accuraci ss approach compar pure ar model filter downsampl data improv comput reliabl exploit disclos meaning multiscal pattern inform transfer global temperatur carbon dioxid concentr time seri paleoclim recent year,"['Luca Faes', 'Giandomenico Nollo', 'Sebastiano Stramaglia', 'Daniele Marinazzo']","['stat.ME', 'math.ST', 'stat.AP', 'stat.TH']",False,False,True,False,False,False
912,2017-03-28T14:02:31Z,2017-03-23T13:53:06Z,http://arxiv.org/abs/1703.08071v1,http://arxiv.org/pdf/1703.08071v1,Quantifying and suppressing ranking bias in a large citation network,quantifi suppress rank bias larg citat network,"It is widely recognized that citation counts for papers from different fields cannot be directly compared because different scientific fields adopt different citation practices. Citation counts are also strongly biased by paper age since older papers had more time to attract citations. Various procedures aim at suppressing these biases and give rise to new normalized indicators, such as the relative citation count. We use a large citation dataset from Microsoft Academic Graph and a new statistical framework based on the Mahalanobis distance to show that the rankings by well known indicators, including the relative citation count and Google's PageRank score, are significantly biased by paper field and age. We propose a general normalization procedure motivated by the $z$-score which produces much less biased rankings when applied to citation count and PageRank score.",wide recogn citat count paper differ field cannot direct compar becaus differ scientif field adopt differ citat practic citat count also strong bias paper age sinc older paper time attract citat various procedur aim suppress bias give rise new normal indic relat citat count use larg citat dataset microsoft academ graph new statist framework base mahalanobi distanc show rank well known indic includ relat citat count googl pagerank score signific bias paper field age propos general normal procedur motiv score produc much less bias rank appli citat count pagerank score,"['Giacomo Vaccario', 'Matus Medo', 'Nicolas Wider', 'Manuel Sebastian Mariani']","['physics.soc-ph', 'cs.DL', 'cs.IR', 'physics.data-an', 'stat.AP']",False,False,True,False,False,True
913,2017-03-28T14:02:31Z,2017-03-21T19:53:59Z,http://arxiv.org/abs/1703.07408v1,http://arxiv.org/pdf/1703.07408v1,Maximum a posteriori estimation through simulated annealing for binary   asteroid orbit determination,maximum posteriori estim simul anneal binari asteroid orbit determin,"This paper considers a new method for the binary asteroid orbit determination problem. The method is based on the Bayesian approach with a global optimisation algorithm. The orbital parameters to be determined are modelled through a posteriori, including a priori and likelihood terms. The first term constrains the parameters search space. It allows to introduce knowledge about orbit, if such information is available, but at the same time it does not require a good initial estimation of parameters. The second term is based on given observations, besides it allows us to use and to compare different observational error models. Ones the a posteriori model is build the estimator of the orbital parameters is computed using a global optimisation procedure: the simulated annealing algorithm. The new method was implemented for simulated and real observations, having received successful result, and also verified for ephemeris prediction capability. The new approach can prove useful in case of small numbers of observations and/or in case of non-Gaussian observational errors, when the classical least-squares method can not be applied.",paper consid new method binari asteroid orbit determin problem method base bayesian approach global optimis algorithm orbit paramet determin model posteriori includ priori likelihood term first term constrain paramet search space allow introduc knowledg orbit inform avail time doe requir good initi estim paramet second term base given observ besid allow us use compar differ observ error model one posteriori model build estim orbit paramet comput use global optimis procedur simul anneal algorithm new method implement simul real observ receiv success result also verifi ephemeri predict capabl new approach prove use case small number observ case non gaussian observ error classic least squar method appli,"['Irina D. Kovalenko', 'Radu S. Stoica', 'Nikolay V. Emelyanov']","['astro-ph.IM', 'astro-ph.EP', 'stat.AP']",False,False,True,False,False,True
914,2017-03-28T14:02:31Z,2017-03-21T18:00:02Z,http://arxiv.org/abs/1703.07355v1,http://arxiv.org/abs/1703.07355v1,An Army of Me: Sockpuppets in Online Discussion Communities,armi sockpuppet onlin discuss communiti,"In online discussion communities, users can interact and share information and opinions on a wide variety of topics. However, some users may create multiple identities, or sockpuppets, and engage in undesired behavior by deceiving others or manipulating discussions. In this work, we study sockpuppetry across nine discussion communities, and show that sockpuppets differ from ordinary users in terms of their posting behavior, linguistic traits, as well as social network structure. Sockpuppets tend to start fewer discussions, write shorter posts, use more personal pronouns such as ""I"", and have more clustered ego-networks. Further, pairs of sockpuppets controlled by the same individual are more likely to interact on the same discussion at the same time than pairs of ordinary users. Our analysis suggests a taxonomy of deceptive behavior in discussion communities. Pairs of sockpuppets can vary in their deceptiveness, i.e., whether they pretend to be different users, or their supportiveness, i.e., if they support arguments of other sockpuppets controlled by the same user. We apply these findings to a series of prediction tasks, notably, to identify whether a pair of accounts belongs to the same underlying user or not. Altogether, this work presents a data-driven view of deception in online discussion communities and paves the way towards the automatic detection of sockpuppets.",onlin discuss communiti user interact share inform opinion wide varieti topic howev user may creat multipl ident sockpuppet engag undesir behavior deceiv manipul discuss work studi sockpuppetri across nine discuss communiti show sockpuppet differ ordinari user term post behavior linguist trait well social network structur sockpuppet tend start fewer discuss write shorter post use person pronoun cluster ego network pair sockpuppet control individu like interact discuss time pair ordinari user analysi suggest taxonomi decept behavior discuss communiti pair sockpuppet vari decept whether pretend differ user support support argument sockpuppet control user appli find seri predict task notabl identifi whether pair account belong user altogeth work present data driven view decept onlin discuss communiti pave way toward automat detect sockpuppet,"['Srijan Kumar', 'Justin Cheng', 'Jure Leskovec', 'V. S. Subrahmanian']","['cs.SI', 'cs.CY', 'physics.soc-ph', 'stat.AP', 'stat.ML']",False,False,True,False,False,False
916,2017-03-28T14:02:31Z,2017-03-21T15:02:45Z,http://arxiv.org/abs/1703.07256v1,http://arxiv.org/pdf/1703.07256v1,Statistical Topology and the Random Interstellar Medium,statist topolog random interstellar medium,"Current astrophysical models of the interstellar medium assume that small scale variation and noise can be modelled as Gaussian random fields or simple transformations thereof, such as lognormal. We use topological methods to investigate this assumption for three regions of the southern sky. We consider Gaussian random fields on two-dimensional lattices and investigate the expected distribution of topological structures quantified through Betti numbers. We demonstrate that there are circumstances where differences in topology can identify differences in distributions when conventional marginal or correlation analyses may not. We propose a non-parametric method for comparing two fields based on the counts of topological features and the geometry of the associated persistence diagrams. When we apply the methods to the astrophysical data, we find strong evidence against a Gaussian random field model for each of the three regions of the interstellar medium that we consider. Further, we show that there are topological differences at a local scale between these different regions.",current astrophys model interstellar medium assum small scale variat nois model gaussian random field simpl transform thereof lognorm use topolog method investig assumpt three region southern sky consid gaussian random field two dimension lattic investig expect distribut topolog structur quantifi betti number demonstr circumst differ topolog identifi differ distribut convent margin correl analys may propos non parametr method compar two field base count topolog featur geometri associ persist diagram appli method astrophys data find strong evid gaussian random field model three region interstellar medium consid show topolog differ local scale differ region,"['Robin Henderson', 'Irina Makarenko', 'Paul Bushby', 'Andrew Fletcher', 'Anvar Shukurov']","['stat.AP', 'astro-ph.GA']",False,False,True,False,False,True
918,2017-03-28T14:02:31Z,2017-03-21T02:45:23Z,http://arxiv.org/abs/1703.07030v1,http://arxiv.org/pdf/1703.07030v1,An Investigation of Three-point Shooting through an Analysis of NBA   Player Tracking Data,investig three point shoot analysi nba player track data,"I address the difficult challenge of measuring the relative influence of competing basketball game strategies, and I apply my analysis to plays resulting in three-point shots. I use a glut of SportVU player tracking data from over 600 NBA games to derive custom position-based features that capture tangible game strategies from game-play data, such as teamwork, player matchups, and on-ball defender distances. Then, I demonstrate statistical methods for measuring the relative importance of any given basketball strategy. In doing so, I highlight the high importance of teamwork based strategies in affecting three-point shot success. By coupling SportVU data with an advanced variable importance algorithm I am able to extract meaningful results that would have been impossible to achieve even 3 years ago. Further, I demonstrate how player-tracking based features can be used to measure the three- point shooting propensity of players, and I show how this measurement can identify effective shooters that are either highly-utilized or under-utilized. Altogether, my findings provide a substantial body of work for influencing basketball strategy, and for measuring the effectiveness of basketball players.",address difficult challeng measur relat influenc compet basketbal game strategi appli analysi play result three point shot use glut sportvu player track data nba game deriv custom posit base featur captur tangibl game strategi game play data teamwork player matchup ball defend distanc demonstr statist method measur relat import ani given basketbal strategi highlight high import teamwork base strategi affect three point shot success coupl sportvu data advanc variabl import algorithm abl extract meaning result would imposs achiev even year ago demonstr player track base featur use measur three point shoot propens player show measur identifi effect shooter either high util util altogeth find provid substanti bodi work influenc basketbal strategi measur effect basketbal player,['Bradley A. Sliz'],['stat.AP'],False,False,True,False,False,False
922,2017-03-28T14:02:35Z,2017-03-20T15:36:13Z,http://arxiv.org/abs/1703.06804v1,http://arxiv.org/pdf/1703.06804v1,A continuous spatio-temporal approach to estimate climate change,continu spatio tempor approach estim climat chang,"We introduce a method for decomposition of trend, cycle and seasonal components in spatio-temporal models and apply it to investigate the existence of climate changes in temperature and rainfall series. The method incorporates critical features in the analysis of climatic problems - the importance of spatial heterogeneity, information from a large number of weather stations, and the presence of missing data. The spatial component is based on continuous projections of spatial covariance functions, allowing modeling the complex patterns of dependence observed in climatic data.   We apply this method to study climate changes in the Northeast region of Brazil, characterized by a great wealth of climates and large amplitudes of temperatures and rainfall. The results show the presence of a tendency for temperature increases, indicating changes in the climatic patterns in this region.",introduc method decomposit trend cycl season compon spatio tempor model appli investig exist climat chang temperatur rainfal seri method incorpor critic featur analysi climat problem import spatial heterogen inform larg number weather station presenc miss data spatial compon base continu project spatial covari function allow model complex pattern depend observ climat data appli method studi climat chang northeast region brazil character great wealth climat larg amplitud temperatur rainfal result show presenc tendenc temperatur increas indic chang climat pattern region,['Marcio Poletti Laurini'],"['stat.AP', '62']",False,False,True,False,False,False
923,2017-03-28T14:02:35Z,2017-03-20T12:52:20Z,http://arxiv.org/abs/1703.06719v1,http://arxiv.org/pdf/1703.06719v1,Analysing the sensitivity of pollen based land-cover maps to different   auxiliary variables,analys sensit pollen base land cover map differ auxiliari variabl,"Realistic maps of past land cover are needed to investigate prehistoric environmental changes and anthropogenic impacts. However, observation based reconstructions of past land cover are rare. Recently Pirzamanbein et al. (2015, arXiv:1511.06417) developed a statistical method that produces spatially complete reconstructions of past land cover from pollen assemblage. These reconstructions incorporate a number of auxiliary datasets raising questions regarding both the method's sensitivity to the choice of auxiliary data and the unaffected transmission of observational data.   Here the sensitivity of the method is examined by performing spatial reconstructions for Europe for three time periods (1900 CE, 1725 CE and 4000 BCE), based on irregularly distributed pollen based land cover, available for ca $25\%$ of the area, and different auxiliary datasets. The auxiliary datasets considered include the most commonly utilized sources of the past land-cover data --- estimates produced by a dynamic vegetation (DVM) and anthropogenic land-cover change (ALCC) models --- and modern elevation. Five different auxiliary datasets were considered, including different climate data driving the DVM and different ALCC models. The resulting reconstructions were evaluated using deviance information criteria and cross validation for all the time periods. For the recent time period, 1900 CE, the different land-cover reconstructions were also compared against a present day forest map.   The tests confirm that the developed statistical model provides a robust spatial interpolation tool with low sensitivity to differences in auxiliary data and high capacity to un-distortedly transmit the information provided by sparse pollen based observations. Further, usage of auxiliary data with high spatial detail improves the model performance for the areas with complex topography or where observational data is missing.",realist map past land cover need investig prehistor environment chang anthropogen impact howev observ base reconstruct past land cover rare recent pirzamanbein et al arxiv develop statist method produc spatial complet reconstruct past land cover pollen assemblag reconstruct incorpor number auxiliari dataset rais question regard method sensit choic auxiliari data unaffect transmiss observ data sensit method examin perform spatial reconstruct europ three time period ce ce bce base irregular distribut pollen base land cover avail ca area differ auxiliari dataset auxiliari dataset consid includ common util sourc past land cover data estim produc dynam veget dvm anthropogen land cover chang alcc model modern elev five differ auxiliari dataset consid includ differ climat data drive dvm differ alcc model result reconstruct evalu use devianc inform criteria cross valid time period recent time period ce differ land cover reconstruct also compar present day forest map test confirm develop statist model provid robust spatial interpol tool low sensit differ auxiliari data high capac un distort transmit inform provid spars pollen base observ usag auxiliari data high spatial detail improv model perform area complex topographi observ data miss,"['Behnaz Pirzamanbein', 'Anneli Poska', 'Johan Lindström']",['stat.AP'],False,False,True,False,False,False
924,2017-03-28T14:02:35Z,2017-03-21T17:19:46Z,http://arxiv.org/abs/1703.06670v2,http://arxiv.org/pdf/1703.06670v2,The Same Analysis Approach: Practical protection against the pitfalls of   novel neuroimaging analysis methods,analysi approach practic protect pitfal novel neuroimag analysi method,"Standard neuroimaging data analysis based on traditional principles of experimental design, modelling, and statistical inference is increasingly complemented by novel analysis methods, driven e.g. by machine learning methods. While these novel approaches provide new insights into neuroimaging data, they often have unexpected properties, generating a growing literature on possible pitfalls. We propose to meet this challenge by adopting a habit of systematic testing of experimental design, analysis procedures, and statistical inference. Specifically, we suggest to apply the analysis method used for experimental data also to aspects of the experimental design, simulated confounds, simulated null data, and control data. We stress the importance of keeping the analysis method the same in main and test analyses, because only this way possible confounds and unexpected properties can be reliably detected and avoided. We describe and discuss this Same Analysis Approach in detail, and demonstrate it in two worked examples using multivariate decoding. With these examples, we reveal two previously unknown sources of error: A mismatch between counterbalancing and cross-validation which leads to systematic below-chance accuracies, and linear decoding of a nonlinear effect, a difference in variance.",standard neuroimag data analysi base tradit principl experiment design model statist infer increas complement novel analysi method driven machin learn method novel approach provid new insight neuroimag data often unexpect properti generat grow literatur possibl pitfal propos meet challeng adopt habit systemat test experiment design analysi procedur statist infer specif suggest appli analysi method use experiment data also aspect experiment design simul confound simul null data control data stress import keep analysi method main test analys becaus onli way possibl confound unexpect properti reliabl detect avoid describ discuss analysi approach detail demonstr two work exampl use multivari decod exampl reveal two previous unknown sourc error mismatch counterbalanc cross valid lead systemat chanc accuraci linear decod nonlinear effect differ varianc,"['Kai Görgen', 'Martin N. Hebart', 'Carsten Allefeld', 'John-Dylan Haynes']","['q-bio.NC', 'stat.AP']",False,False,True,False,False,False
925,2017-03-28T14:02:35Z,2017-03-20T09:41:34Z,http://arxiv.org/abs/1703.06645v1,http://arxiv.org/pdf/1703.06645v1,A Preferential Attachment Paradox: How does Preferential Attachment   Combine with Growth to Produce Networks with Log-normal In-degree   Distributions?,preferenti attach paradox doe preferenti attach combin growth produc network log normal degre distribut,"Every network scientist knows that preferential attachment combines with growth to produce networks with power-law in-degree distributions. So how, then, is it possible for the network of American Physical Society journal collection citations to enjoy a log-normal citation distribution when it was found to have grown in accordance with preferential attachment? This anomalous result, which we exalt as the preferential attachment paradox, has remained unexplained since the physicist Sidney Redner first made light of it over a decade ago. In this paper we propose a resolution to the paradox. The source of the mischief, we contend, lies in Redner having relied on a measurement procedure bereft of the accuracy required to distinguish preferential attachment from another form of attachment that is consistent with a log-normal in-degree distribution. There was a high-accuracy measurement procedure in general use at the time, but it could not have been used to shed light on the paradox, due to the presence of a systematic error inducing design flaw. But in recent years the design flaw had been recognised and corrected. Here we show that the bringing of the newly corrected measurement procedure to bare on the data leads to a resolution of the paradox with important ramifications for the working network scientist.",everi network scientist know preferenti attach combin growth produc network power law degre distribut possibl network american physic societi journal collect citat enjoy log normal citat distribut found grown accord preferenti attach anomal result exalt preferenti attach paradox remain unexplain sinc physicist sidney redner first made light decad ago paper propos resolut paradox sourc mischief contend lie redner reli measur procedur bereft accuraci requir distinguish preferenti attach anoth form attach consist log normal degre distribut high accuraci measur procedur general use time could use shed light paradox due presenc systemat error induc design flaw recent year design flaw recognis correct show bring newli correct measur procedur bare data lead resolut paradox import ramif work network scientist,"['Paul Sheridan', 'Taku Onodera']","['cs.DL', 'cond-mat.stat-mech', 'physics.data-an', 'stat.AP']",False,False,True,False,False,False
926,2017-03-28T14:02:35Z,2017-03-20T05:18:45Z,http://arxiv.org/abs/1703.06603v1,http://arxiv.org/pdf/1703.06603v1,A New Class of Discrete-time Stochastic Volatility Model with Correlated   Errors,new class discret time stochast volatil model correl error,"In an efficient stock market, the returns and their time-dependent volatility are often jointly modeled by stochastic volatility models (SVMs). Over the last few decades several SVMs have been proposed to adequately capture the defining features of the relationship between the return and its volatility. Among one of the earliest SVM, Taylor (1982) proposed a hierarchical model, where the current return is a function of the current latent volatility, which is further modeled as an auto-regressive process. In an attempt to make the SVMs more appropriate for complex realistic market behavior, a leverage parameter was introduced in the Taylor SVM, which however led to the violation of the efficient market hypothesis (EMH, a necessary mean-zero condition for the return distribution that prevents arbitrage possibilities). Subsequently, a host of alternative SVMs had been developed and are currently in use. In this paper, we propose mean-corrections for several generalizations of Taylor SVM that capture the complex market behavior as well as satisfy EMH. We also establish a few theoretical results to characterize the key desirable features of these models, and present comparison with other popular competitors. Furthermore, four real-life examples (Oil price, CITI bank stock price, Euro-USD rate, and S&P 500 index returns) have been used to demonstrate the performance of this new class of SVMs.",effici stock market return time depend volatil often joint model stochast volatil model svms last decad sever svms propos adequ captur defin featur relationship return volatil among one earliest svm taylor propos hierarch model current return function current latent volatil model auto regress process attempt make svms appropri complex realist market behavior leverag paramet introduc taylor svm howev led violat effici market hypothesi emh necessari mean zero condit return distribut prevent arbitrag possibl subsequ host altern svms develop current use paper propos mean correct sever general taylor svm captur complex market behavior well satisfi emh also establish theoret result character key desir featur model present comparison popular competitor furthermor four real life exampl oil price citi bank stock price euro usd rate index return use demonstr perform new class svms,"['Sujay Mukhoti', 'Pritam Ranjan']","['stat.AP', 'q-fin.ST']",False,False,True,False,False,False
927,2017-03-28T14:02:35Z,2017-03-20T05:18:30Z,http://arxiv.org/abs/1703.06602v1,http://arxiv.org/pdf/1703.06602v1,Dual Lasso Selector,dual lasso selector,"We consider the problem of model selection and estimation in sparse high dimensional linear regression models with strongly correlated variables. First, we study the theoretical properties of the dual Lasso solution, and we show that joint consideration of the Lasso primal and its dual solutions are useful for selecting correlated active variables. Second, we argue that correlations among active predictors are not problematic, and we derive a new weaker condition on the design matrix, called Pseudo Irrepresentable Condition (PIC). Third, we present a new variable selection procedure, Dual Lasso Selector, and we prove that the PIC is a necessary and sufficient condition for consistent variable selection for the proposed method. Finally, by combining the dual Lasso selector further with the Ridge estimation even better prediction performance is achieved. We call the combination (DLSelect+Ridge), it can be viewed as a new combined approach for inference in high-dimensional regression models with correlated variables. We illustrate DLSelect+Ridge method and compare it with popular existing methods in terms of variable selection, prediction accuracy, estimation accuracy and computation speed by considering various simulated and real data examples.",consid problem model select estim spars high dimension linear regress model strong correl variabl first studi theoret properti dual lasso solut show joint consider lasso primal dual solut use select correl activ variabl second argu correl among activ predictor problemat deriv new weaker condit design matrix call pseudo irrepresent condit pic third present new variabl select procedur dual lasso selector prove pic necessari suffici condit consist variabl select propos method final combin dual lasso selector ridg estim even better predict perform achiev call combin dlselect ridg view new combin approach infer high dimension regress model correl variabl illustr dlselect ridg method compar popular exist method term variabl select predict accuraci estim accuraci comput speed consid various simul real data exampl,['Niharika Gauraha'],['stat.AP'],False,False,True,False,False,False
929,2017-03-28T14:02:35Z,2017-03-19T00:53:48Z,http://arxiv.org/abs/1703.06375v1,http://arxiv.org/pdf/1703.06375v1,An Initial Study on Load Forecasting Considering Economic Factors,initi studi load forecast consid econom factor,"This paper proposes a new objective function and quantile regression (QR) algorithm for load forecasting (LF). In LF, the positive forecasting errors often have different economic impact from the negative forecasting errors. Considering this difference, a new objective function is proposed to put different prices on the positive and negative forecasting errors. QR is used to find the optimal solution of the proposed objective function. Using normalized net energy load of New England network, the proposed method is compared with a time series method, the artificial neural network method, and the support vector machine method. The simulation results show that the proposed method is more effective in reducing the economic cost of the LF errors than the other three methods.",paper propos new object function quantil regress qr algorithm load forecast lf lf posit forecast error often differ econom impact negat forecast error consid differ new object function propos put differ price posit negat forecast error qr use find optim solut propos object function use normal net energi load new england network propos method compar time seri method artifici neural network method support vector machin method simul result show propos method effect reduc econom cost lf error three method,"['Hossein Sangrody', 'Ning Zhou']",['stat.AP'],False,False,True,False,False,False
932,2017-03-28T14:02:39Z,2017-03-16T16:00:00Z,http://arxiv.org/abs/1703.05687v1,http://arxiv.org/pdf/1703.05687v1,Gaussian process regression for forecasting battery state of health,gaussian process regress forecast batteri state health,"Accurately predicting the future capacity and remaining useful life of batteries is necessary to ensure reliable system operation and to minimise maintenance costs. The complex nature of battery degradation has meant that mechanistic modelling of capacity fade has thus far remained intractable; however, with the advent of cloud-connected devices, data from cells in various applications is becoming increasingly available, and the feasibility of data-driven methods for battery prognostics is increasing. Here we propose Gaussian process (GP) regression for forecasting battery state of health, and highlight various advantages of GPs over other data-driven and mechanistic approaches. GPs are a type of Bayesian non-parametric method, and hence can model complex systems whilst handling uncertainty in a principled manner. Prior information can be exploited by GPs in a variety of ways: explicit mean functions can be used if the functional form of the underlying degradation model is available, and multiple-output GPs can effectively exploit correlations between data from different cells. We demonstrate the predictive capability of GPs for short-term and long-term (remaining useful life) forecasting on a selection of capacity vs. cycle datasets from lithium-ion cells.",accur predict futur capac remain use life batteri necessari ensur reliabl system oper minimis mainten cost complex natur batteri degrad meant mechanist model capac fade thus far remain intract howev advent cloud connect devic data cell various applic becom increas avail feasibl data driven method batteri prognost increas propos gaussian process gp regress forecast batteri state health highlight various advantag gps data driven mechanist approach gps type bayesian non parametr method henc model complex system whilst handl uncertainti principl manner prior inform exploit gps varieti way explicit mean function use function form degrad model avail multipl output gps effect exploit correl data differ cell demonstr predict capabl gps short term long term remain use life forecast select capac vs cycl dataset lithium ion cell,"['Robert R. Richardson', 'Michael A. Osborne', 'David A. Howey']","['stat.AP', 'stat.ML', '62P30', 'J.2; G.3']",False,False,True,False,False,False
933,2017-03-28T14:02:39Z,2017-03-16T10:06:45Z,http://arxiv.org/abs/1703.05545v1,http://arxiv.org/pdf/1703.05545v1,The nature and origin of heavy tails in retweet activity,natur origin heavi tail retweet activ,"Modern social media platforms facilitate the rapid spread of information online. Modelling phenomena such as social contagion and information diffusion are contingent upon a detailed understanding of the information-sharing processes. In Twitter, an important aspect of this occurs with retweets, where users rebroadcast the tweets of other users. To improve our understanding of how these distributions arise, we analyse the distribution of retweet times. We show that a power law with exponential cutoff provides a better fit than the power laws previously suggested. We explain this fit through the burstiness of human behaviour and the priorities individuals place on different tasks.",modern social media platform facilit rapid spread inform onlin model phenomena social contagion inform diffus conting upon detail understand inform share process twitter import aspect occur retweet user rebroadcast tweet user improv understand distribut aris analys distribut retweet time show power law exponenti cutoff provid better fit power law previous suggest explain fit bursti human behaviour prioriti individu place differ task,"['Peter Mathews', 'Lewis Mitchell', 'Giang T. Nguyen', 'Nigel G. Bean']","['physics.soc-ph', 'cs.SI', 'stat.AP']",False,False,True,False,False,True
934,2017-03-28T14:02:39Z,2017-03-16T09:37:08Z,http://arxiv.org/abs/1703.05532v1,http://arxiv.org/pdf/1703.05532v1,Clustering of Gamma-Ray bursts through kernel principal component   analysis,cluster gamma ray burst kernel princip compon analysi,"We consider the problem related to clustering of gamma-ray bursts (from ""BATSE"" catalogue) through kernel principal component analysis in which our proposed kernel outperforms results of other competent kernels in terms of clustering accuracy and we obtain three physically interpretable groups of gamma-ray bursts. The effectivity of the suggested kernel in combination with kernel principal component analysis in revealing natural clusters in noisy and nonlinear data while reducing the dimension of the data is also explored in two simulated data sets.",consid problem relat cluster gamma ray burst bats catalogu kernel princip compon analysi propos kernel outperform result compet kernel term cluster accuraci obtain three physic interpret group gamma ray burst effect suggest kernel combin kernel princip compon analysi reveal natur cluster noisi nonlinear data reduc dimens data also explor two simul data set,"['Soumita Modak', 'Asis Kumar Chattopadhyay', 'Tanuka Chattopadhyay']","['stat.AP', 'astro-ph.HE', 'astro-ph.IM', '62P35']",False,False,True,False,False,False
938,2017-03-28T14:02:39Z,2017-03-15T14:22:09Z,http://arxiv.org/abs/1703.05172v1,http://arxiv.org/pdf/1703.05172v1,Bayesian adaptive bandit-based designs using the Gittins index for   multi-armed trials with normally distributed endpoints,bayesian adapt bandit base design use gittin index multi arm trial normal distribut endpoint,"Adaptive designs for multi-armed clinical trials have become increasingly popular recently in many areas of medical research because of their potential to shorten development times and to increase patient response. However, developing response-adaptive trial designs that offer patient benefit while ensuring the resulting trial avoids bias and provides a statistically rigorous comparison of the different treatments included is highly challenging. In this paper, the theory of Multi-Armed Bandit Problems is used to define a family of near optimal adaptive designs in the context of a clinical trial with a normally distributed endpoint with known variance. Through simulation studies based on an ongoing trial as a motivation we report the operating characteristics (type I error, power, bias) and patient benefit of these approaches and compare them to traditional and existing alternative designs. These results are then compared to those recently published in the context of Bernoulli endpoints. Many limitations and advantages are similar in both cases but there are also important differences, specially with respect to type I error control. This paper proposes a simulation-based testing procedure to correct for the observed type I error inflation that bandit-based and adaptive rules can induce. Results presented extend recent work by considering a normally distributed endpoint, a very common case in clinical practice yet mostly ignored in the response-adaptive theoretical literature, and illustrate the potential advantages of using these methods in a rare disease context. We also recommend a suitable modified implementation of the bandit-based adaptive designs for the case of common diseases.",adapt design multi arm clinic trial becom increas popular recent mani area medic research becaus potenti shorten develop time increas patient respons howev develop respons adapt trial design offer patient benefit ensur result trial avoid bias provid statist rigor comparison differ treatment includ high challeng paper theori multi arm bandit problem use defin famili near optim adapt design context clinic trial normal distribut endpoint known varianc simul studi base ongo trial motiv report oper characterist type error power bias patient benefit approach compar tradit exist altern design result compar recent publish context bernoulli endpoint mani limit advantag similar case also import differ special respect type error control paper propos simul base test procedur correct observ type error inflat bandit base adapt rule induc result present extend recent work consid normal distribut endpoint veri common case clinic practic yet ignor respons adapt theoret literatur illustr potenti advantag use method rare diseas context also recommend suitabl modifi implement bandit base adapt design case common diseas,"['Adam Smith', 'Sofia S. Villar']",['stat.AP'],False,False,True,False,False,True
940,2017-03-28T14:02:43Z,2017-03-15T06:42:41Z,http://arxiv.org/abs/1703.04961v1,http://arxiv.org/pdf/1703.04961v1,Predicting with limited data - Increasing the accuracy in VIS-NIR   diffuse reflectance spectroscopy by SMOTE,predict limit data increas accuraci vis nir diffus reflect spectroscopi smote,"Diffuse reflectance spectroscopy is a powerful technique to predict soil properties. It can be used in situ to provide data inexpensively and rapidly compared to the standard laboratory measurements. Because most spectral data bases contain air-dried samples scanned in the laboratory, field spectra acquired in situ are either absent or rare in calibration data sets. However, when models are calibrated on air-dried spectra, prediction using field spectra are often inaccurate. We propose a framework to calibrate partial least squares models when field spectra are rare using synthetic minority oversampling technique (SMOTE). We calibrated a model to predict soil organic carbon content using air-dried spectra spiked with synthetic field spectra. The root mean-squared error of prediction decreased from 6.18 to 2.12 mg g$^{-1}$ and $R^2$ increased from $-$0.53 to 0.82 compared to the model calibrated on air-dried spectra only.",diffus reflect spectroscopi power techniqu predict soil properti use situ provid data inexpens rapid compar standard laboratori measur becaus spectral data base contain air dri sampl scan laboratori field spectra acquir situ either absent rare calibr data set howev model calibr air dri spectra predict use field spectra often inaccur propos framework calibr partial least squar model field spectra rare use synthet minor oversampl techniqu smote calibr model predict soil organ carbon content use air dri spectra spike synthet field spectra root mean squar error predict decreas mg increas compar model calibr air dri spectra onli,"['Christina Bogner', 'Anna Kühnel', 'Bernd Huwe']",['stat.AP'],False,False,True,False,False,False
941,2017-03-28T14:02:43Z,2017-03-15T06:36:58Z,http://arxiv.org/abs/1703.04957v1,http://arxiv.org/pdf/1703.04957v1,An algorithm for removing sensitive information: application to   race-independent recidivism prediction,algorithm remov sensit inform applic race independ recidiv predict,"Predictive modeling is increasingly being employed to assist human decision-makers. One purported advantage of replacing or augmenting human judgment with computer models in high stakes settings-- such as sentencing, hiring, policing, college admissions, and parole decisions-- is the perceived ""neutrality"" of computers. It is argued that because computer models do not hold personal prejudice, the predictions they produce will be equally free from prejudice. There is growing recognition that employing algorithms does not remove the potential for bias, and can even amplify it if the training data were generated by a process that is itself biased. In this paper, we provide a probabilistic notion of algorithmic bias. We propose a method to eliminate bias from predictive models by removing all information regarding protected variables from the data to which the models will ultimately be trained. Unlike previous work in this area, our framework is general enough to accommodate data on any measurement scale. Motivated by models currently in use in the criminal justice system that inform decisions on pre-trial release and parole, we apply our proposed method to a dataset on the criminal histories of individuals at the time of sentencing to produce ""race-neutral"" predictions of re-arrest. In the process, we demonstrate that a common approach to creating ""race-neutral"" models-- omitting race as a covariate-- still results in racially disparate predictions. We then demonstrate that the application of our proposed method to these data removes racial disparities from predictions with minimal impact on predictive accuracy.",predict model increas employ assist human decis maker one purport advantag replac augment human judgment comput model high stake set sentenc hire polic colleg admiss parol decis perceiv neutral comput argu becaus comput model hold person prejudic predict produc equal free prejudic grow recognit employ algorithm doe remov potenti bias even amplifi train data generat process bias paper provid probabilist notion algorithm bias propos method elimin bias predict model remov inform regard protect variabl data model ultim train unlik previous work area framework general enough accommod data ani measur scale motiv model current use crimin justic system inform decis pre trial releas parol appli propos method dataset crimin histori individu time sentenc produc race neutral predict arrest process demonstr common approach creat race neutral model omit race covari still result racial dispar predict demonstr applic propos method data remov racial dispar predict minim impact predict accuraci,"['James E. Johndrow', 'Kristian Lum']",['stat.AP'],False,False,True,False,False,False
942,2017-03-28T14:02:43Z,2017-03-14T23:04:14Z,http://arxiv.org/abs/1703.04812v1,http://arxiv.org/pdf/1703.04812v1,An alternative representation of the negative binomial-Lindley   distribution. New results and applications,altern represent negat binomi lindley distribut new result applic,"In this paper we present an alternative representation of the Negative Binomial--Lindley distribution recently proposed by Zamani and Ismail (2010) which shows some advantages over the latter model. This new formulation provides a tractable model with attractive properties which makes it suitable for application not only in insurance settings but also in other fields where overdispersion is observed. Basic properties of the new distribution are studied. A recurrence for the probabilities of the new distribution and an integral equation for the probability density function of the compound version, when the claim severities are absolutely continuous, are derived. Estimation methods are discussed and a numerical application is given.",paper present altern represent negat binomi lindley distribut recent propos zamani ismail show advantag latter model new formul provid tractabl model attract properti make suitabl applic onli insur set also field overdispers observ basic properti new distribut studi recurr probabl new distribut integr equat probabl densiti function compound version claim sever absolut continu deriv estim method discuss numer applic given,"['Emilio Gomez-Deniz', 'Enrique Calderin-Ojeda']",['stat.AP'],False,False,True,False,False,True
945,2017-03-28T14:02:43Z,2017-03-13T11:31:42Z,http://arxiv.org/abs/1703.04341v1,http://arxiv.org/pdf/1703.04341v1,Response adaptive designs for binary responses: how to offer patient   benefit while being robust to time trends?,respons adapt design binari respons offer patient benefit robust time trend,"Response-adaptive randomisation (RAR) can considerably improve the chances of a successful treatment outcome for patients in a clinical trial by skewing the allocation probability towards better performing treatments as data accumulates. There is considerable interest in using RAR designs in drug development for rare diseases, where traditional designs are not feasible or ethically objectionable. In this paper we discuss and address a major criticism of RAR: the undesirable type I error inflation due to unknown time trends in the trial. Time trends can appear because of changes in the characteristics of recruited patients - so-called ""patient drift"". Patient drift is a realistic concern for clinical trials in rare diseases because these typically recruit patients over a very long period of time. We compute by simulations how large the type I error inflation is as a function of the time trend magnitude in order to determine in which contexts a potentially costly correction is actually necessary. We then assess the ability of different correction methods to preserve type I error in this context and their performance in terms of other operating characteristics, including patient benefit and power. We make recommendations of which correction methods are most suitable in the rare disease context for several RAR rules, differentiating between the two-armed and the multi-armed case. We further propose a RAR design for multi-armed clinical trials, which is computationally cheap and robust to several time trends considered.",respons adapt randomis rar consider improv chanc success treatment outcom patient clinic trial skew alloc probabl toward better perform treatment data accumul consider interest use rar design drug develop rare diseas tradit design feasibl ethic objection paper discuss address major critic rar undesir type error inflat due unknown time trend trial time trend appear becaus chang characterist recruit patient call patient drift patient drift realist concern clinic trial rare diseas becaus typic recruit patient veri long period time comput simul larg type error inflat function time trend magnitud order determin context potenti cost correct actual necessari assess abil differ correct method preserv type error context perform term oper characterist includ patient benefit power make recommend correct method suitabl rare diseas context sever rar rule differenti two arm multi arm case propos rar design multi arm clinic trial comput cheap robust sever time trend consid,"['Sofia S. Villar', 'Jack Bowden', 'James Wason']",['stat.AP'],False,False,True,False,False,False
946,2017-03-28T14:02:43Z,2017-03-13T10:14:20Z,http://arxiv.org/abs/1703.04312v1,http://arxiv.org/pdf/1703.04312v1,Assessing Potential Wind Energy Resources in Saudi Arabia with a Skew-t   Distribution,assess potenti wind energi resourc saudi arabia skew distribut,"Facing increasing domestic energy consumption from population growth and industrialization, Saudi Arabia is aiming to reduce its reliance on fossil fuels and to broaden its energy mix by expanding investment in renewable energy sources, including wind energy. A preliminary task in the development of wind energy infrastructure is the assessment of wind energy potential, a key aspect of which is the characterization of its spatio-temporal behavior. In this study we examine the impact of internal climate variability on seasonal wind power density fluctuations using 30 simulations from the Large Ensemble Project (LENS) developed at the National Center for Atmospheric Research. Furthermore, a spatio-temporal model for daily wind speed is proposed with neighbor-based cross-temporal dependence, and a multivariate skew-t distribution to capture the spatial patterns of higher order moments. The model can be used to generate synthetic time series over the entire spatial domain that adequately reproduces the internal variability of the LENS dataset.",face increas domest energi consumpt popul growth industri saudi arabia aim reduc relianc fossil fuel broaden energi mix expand invest renew energi sourc includ wind energi preliminari task develop wind energi infrastructur assess wind energi potenti key aspect character spatio tempor behavior studi examin impact intern climat variabl season wind power densiti fluctuat use simul larg ensembl project len develop nation center atmospher research furthermor spatio tempor model daili wind speed propos neighbor base cross tempor depend multivari skew distribut captur spatial pattern higher order moment model use generat synthet time seri entir spatial domain adequ reproduc intern variabl len dataset,"['Felipe Tagle', 'Stefano Castruccio', 'Paola Crippa', 'Marc G. Genton']",['stat.AP'],False,False,True,False,False,True
948,2017-03-28T14:02:44Z,2017-03-12T02:07:37Z,http://arxiv.org/abs/1703.04056v1,http://arxiv.org/pdf/1703.04056v1,Quantifying the strength of structural connectivity underlying   functional brain networks,quantifi strength structur connect function brain network,"In recent years, there has been strong interest in neuroscience studies to investigate brain organization through networks of brain regions that demonstrate strong functional connectivity (FC). These networks are extracted from observed fMRI using data-driven analytic methods such as independent component analysis (ICA). A notable limitation of these FC methods is that they do not provide any information on the underlying structural connectivity (SC), which is believed to serve as the basis for interregional interactions in brain activity. We propose a new statistical measure of the strength of SC (sSC) underlying FC networks obtained from data-driven methods. The sSC measure is developed using information from diffusion tensor imaging (DTI) data, and can be applied to compare the strength of SC across different FC networks. Furthermore, we propose a reliability index for data-driven FC networks to measure the reproducibility of the networks through re-sampling the observed data. To perform statistical inference such as hypothesis testing on the sSC, we develop a formal variance estimator of sSC based a spatial semivariogram model with a novel distance metric. We demonstrate the performance of the sSC measure and its estimation and inference methods with simulation studies. For real data analysis, we apply our methods to a multimodal imaging study with resting-state fMRI and DTI data from 20 healthy controls and 20 subjects with major depressive disorder. Results show that well-known resting state networks all demonstrate higher SC within the network as compared to the average structural connections across the brain. We also found that sSC is positively associated with the reliability index, indicating that the FC networks that have stronger underlying SC are more reproducible across samples.",recent year strong interest neurosci studi investig brain organ network brain region demonstr strong function connect fc network extract observ fmri use data driven analyt method independ compon analysi ica notabl limit fc method provid ani inform structur connect sc believ serv basi interregion interact brain activ propos new statist measur strength sc ssc fc network obtain data driven method ssc measur develop use inform diffus tensor imag dti data appli compar strength sc across differ fc network furthermor propos reliabl index data driven fc network measur reproduc network sampl observ data perform statist infer hypothesi test ssc develop formal varianc estim ssc base spatial semivariogram model novel distanc metric demonstr perform ssc measur estim infer method simul studi real data analysi appli method multimod imag studi rest state fmri dti data healthi control subject major depress disord result show well known rest state network demonstr higher sc within network compar averag structur connect across brain also found ssc posit associ reliabl index indic fc network stronger sc reproduc across sampl,"['Phebe Brenne Kemmer', 'F. DuBois Bowman', 'Helen Mayberg', 'Ying Guo']","['stat.AP', 'q-bio.NC']",False,False,True,False,False,False
954,2017-03-28T14:02:47Z,2017-03-09T10:19:53Z,http://arxiv.org/abs/1703.03213v1,http://arxiv.org/pdf/1703.03213v1,"Kernel intensity estimation, bootstrapping and bandwidth selection for   inhomogeneous point processes depending on spatial covariates",kernel intens estim bootstrap bandwidth select inhomogen point process depend spatial covari,"In the point process context, kernel intensity estimation has been mainly restricted to exploratory analysis due to its lack of consistency. However the use of covariates has allow to design consistent alternatives under some restrictive assumptions. In this paper we focus our attention on de\-fi\-ning an appropriate framework to derive a consistent kernel intensity estimator using covariates, as well as a consistent smooth bootstrap procedure. For spatial point processes with covariates there is no specific bandwidth selector, hence, we define two new data-driven procedures specifically designed for this scenario: a rule-of-thumb and a plug-in bandwidth based on the bootstrap method previously introduced. A simulation study is accomplished to understand the behaviour of these procedures in finite samples. Finally, we apply the techniques to a real set of data made up of wildfires in Canada during June 2015, using meteorological information as covariates.",point process context kernel intens estim main restrict exploratori analysi due lack consist howev use covari allow design consist altern restrict assumpt paper focus attent de fi ning appropri framework deriv consist kernel intens estim use covari well consist smooth bootstrap procedur spatial point process covari specif bandwidth selector henc defin two new data driven procedur specif design scenario rule thumb plug bandwidth base bootstrap method previous introduc simul studi accomplish understand behaviour procedur finit sampl final appli techniqu real set data made wildfir canada dure june use meteorolog inform covari,"['M. I. Borrajo', 'W. González-Manteiga', 'M. D. Martínez-Miranda']","['stat.ME', 'stat.AP', '62G05, 62G09, 62H11, 60G55, 60-08']",False,False,True,False,False,False
955,2017-03-28T14:02:47Z,2017-03-08T15:23:00Z,http://arxiv.org/abs/1703.02870v1,http://arxiv.org/abs/1703.02870v1,Statistical Inference in Political Networks Research,statist infer polit network research,"Researchers interested in statistically modeling network data have a well-established and quickly growing set of approaches from which to choose. Several of these methods have been regularly applied in research on political networks, while others have yet to permeate the field. Here, we review the most prominent methods of inferential network analysis---both for cross-sectionally and longitudinally observed networks including (temporal) exponential random graph models, latent space models, the quadratic assignment procedure, and stochastic actor oriented models. For each method, we summarize its analytic form, identify prominent published applications in political science and discuss computational considerations. We conclude with a set of guidelines for selecting a method for a given application.",research interest statist model network data well establish quick grow set approach choos sever method regular appli research polit network yet permeat field review promin method inferenti network analysi cross section longitudin observ network includ tempor exponenti random graph model latent space model quadrat assign procedur stochast actor orient model method summar analyt form identifi promin publish applic polit scienc discuss comput consider conclud set guidelin select method given applic,"['Bruce A. Desmarais', 'Skyler J. Cranmer']","['stat.AP', 'cs.SI', 'physics.soc-ph']",False,False,True,False,False,False
957,2017-03-28T14:02:47Z,2017-03-07T18:14:54Z,http://arxiv.org/abs/1703.02502v1,http://arxiv.org/pdf/1703.02502v1,Clustering Methods for Electricity Consumers: An Empirical Study in   Hvaler-Norway,cluster method electr consum empir studi hvaler norway,"The development of Smart Grid in Norway in specific and Europe/US in general will shortly lead to the availability of massive amount of fine-grained spatio-temporal consumption data from domestic households. This enables the application of data mining techniques for traditional problems in power system. Clustering customers into appropriate groups is extremely useful for operators or retailers to address each group differently through dedicated tariffs or customer-tailored services. Currently, the task is done based on demographic data collected through questionnaire, which is error-prone. In this paper, we used three different clustering techniques (together with their variants) to automatically segment electricity consumers based on their consumption patterns. We also proposed a good way to extract consumption patterns for each consumer. The grouping results were assessed using four common internal validity indexes. We found that the combination of Self Organizing Map (SOM) and k-means algorithms produce the most insightful and useful grouping. We also discovered that grouping quality cannot be measured effectively by automatic indicators, which goes against common suggestions in literature.",develop smart grid norway specif europ us general short lead avail massiv amount fine grain spatio tempor consumpt data domest household enabl applic data mine techniqu tradit problem power system cluster custom appropri group extrem use oper retail address group differ dedic tariff custom tailor servic current task done base demograph data collect questionnair error prone paper use three differ cluster techniqu togeth variant automat segment electr consum base consumpt pattern also propos good way extract consumpt pattern consum group result assess use four common intern valid index found combin self organ map som mean algorithm produc insight use group also discov group qualiti cannot measur effect automat indic goe common suggest literatur,"['The-Hien Dang-Ha', 'Roland Olsson', 'Hao Wang']",['stat.AP'],False,False,True,False,False,False
959,2017-03-28T14:02:47Z,2017-03-07T11:15:14Z,http://arxiv.org/abs/1703.02329v1,http://arxiv.org/pdf/1703.02329v1,Time and media-use of Italian Generation Y: dimensions of leisure   preferences,time media use italian generat dimens leisur prefer,"Time spent in leisure is not a minor research question as it is acknowledged as a key aspect of one's quality of life. The primary aim of this article is to qualify time and Internet use of Italian Generation Y beyond media hype and assumptions. To this aim, we apply a multidimensional extension of Item Response Theory models to the Italian ""Multipurpose survey on households: aspects of daily life"" to ascertain the relevant dimensions of Generation Y time-use. We show that the use of technology is neither the first nor the foremost time-use activity of Italian Generation Y, who still prefers to use its time to socialise and have fun with friends in a non media-medalled manner.",time spent leisur minor research question acknowledg key aspect one qualiti life primari aim articl qualifi time internet use italian generat beyond media hype assumpt aim appli multidimension extens item respons theori model italian multipurpos survey household aspect daili life ascertain relev dimens generat time use show use technolog neither first foremost time use activ italian generat still prefer use time socialis fun friend non media medal manner,"['Michela Gnaldi', 'Simone Del Sarto']","['stat.AP', 'cs.CY']",False,False,True,False,False,False
960,2017-03-28T14:02:51Z,2017-03-14T18:52:48Z,http://arxiv.org/abs/1703.02236v2,http://arxiv.org/pdf/1703.02236v2,Propensity score prediction for electronic healthcare databases using   Super Learner and High-dimensional Propensity Score Methods,propens score predict electron healthcar databas use super learner high dimension propens score method,"The optimal learner for prediction modeling varies depending on the underlying data-generating distribution. Super Learner (SL) is a generic ensemble learning algorithm that uses cross-validation to select among a ""library"" of candidate prediction models. The SL is not restricted to a single prediction model, but uses the strengths of a variety of learning algorithms to adapt to different databases. While the SL has been shown to perform well in a number of settings, it has not been thoroughly evaluated in large electronic healthcare databases that are common in pharmacoepidemiology and comparative effectiveness research. In this study, we applied and evaluated the performance of the SL in its ability to predict treatment assignment using three electronic healthcare databases. We considered a library of algorithms that consisted of both nonparametric and parametric models. We also considered a novel strategy for prediction modeling that combines the SL with the high-dimensional propensity score (hdPS) variable selection algorithm. Predictive performance was assessed using three metrics: the negative log-likelihood, area under the curve (AUC), and time complexity. Results showed that the best individual algorithm, in terms of predictive performance, varied across datasets. The SL was able to adapt to the given dataset and optimize predictive performance relative to any individual learner. Combining the SL with the hdPS was the most consistent prediction method and may be promising for PS estimation and prediction modeling in electronic healthcare databases.",optim learner predict model vari depend data generat distribut super learner sl generic ensembl learn algorithm use cross valid select among librari candid predict model sl restrict singl predict model use strength varieti learn algorithm adapt differ databas sl shown perform well number set thorough evalu larg electron healthcar databas common pharmacoepidemiolog compar effect research studi appli evalu perform sl abil predict treatment assign use three electron healthcar databas consid librari algorithm consist nonparametr parametr model also consid novel strategi predict model combin sl high dimension propens score hdps variabl select algorithm predict perform assess use three metric negat log likelihood area curv auc time complex result show best individu algorithm term predict perform vari across dataset sl abl adapt given dataset optim predict perform relat ani individu learner combin sl hdps consist predict method may promis ps estim predict model electron healthcar databas,"['Cheng Ju', 'Mary Combs', 'Samuel D Lendle', 'Jessica M Franklin', 'Richard Wyss', 'Sebastian Schneeweiss', 'Mark J. van der Laan']","['stat.AP', 'stat.ML']",False,False,True,False,False,False
962,2017-03-28T14:02:51Z,2017-03-06T19:33:24Z,http://arxiv.org/abs/1703.02078v1,http://arxiv.org/pdf/1703.02078v1,Cross-screening in observational studies that test many hypotheses,cross screen observ studi test mani hypothes,"We discuss observational studies that test many causal hypotheses, either hypotheses about many outcomes or many treatments. To be credible an observational study that tests many causal hypotheses must demonstrate that its conclusions are neither artifacts of multiple testing nor of small biases from nonrandom treatment assignment. In a sense that needs to be defined carefully, hidden within a sensitivity analysis for nonrandom assignment is an enormous correction for multiple testing: in the absence of bias, it is extremely improbable that multiple testing alone would create an association insensitive to moderate biases. We propose a new strategy called ""cross-screening"", different from but motivated by recent work of Bogomolov and Heller on replicability. Cross-screening splits the data in half at random, uses the first half to plan a study carried out on the second half, then uses the second half to plan a study carried out on the first half, and reports the more favorable conclusions of the two studies correcting using the Bonferroni inequality for having done two studies. If the two studies happen to concur, then they achieve Bogomolov-Heller replicability; however, importantly, replicability is not required for strong control of the family-wise error rate, and either study alone suffices for firm conclusions. In randomized studies with a few hypotheses, cross-split screening is not an attractive method when compared with conventional methods of multiplicity control, but it can become attractive when hundreds or thousands of hypotheses are subjected to sensitivity analyses in an observational study. We illustrate the technique by comparing 46 biomarkers in individuals who consume large quantities of fish versus little or no fish.",discuss observ studi test mani causal hypothes either hypothes mani outcom mani treatment credibl observ studi test mani causal hypothes must demonstr conclus neither artifact multipl test small bias nonrandom treatment assign sens need defin care hidden within sensit analysi nonrandom assign enorm correct multipl test absenc bias extrem improb multipl test alon would creat associ insensit moder bias propos new strategi call cross screen differ motiv recent work bogomolov heller replic cross screen split data half random use first half plan studi carri second half use second half plan studi carri first half report favor conclus two studi correct use bonferroni inequ done two studi two studi happen concur achiev bogomolov heller replic howev import replic requir strong control famili wise error rate either studi alon suffic firm conclus random studi hypothes cross split screen attract method compar convent method multipl control becom attract hundr thousand hypothes subject sensit analys observ studi illustr techniqu compar biomark individu consum larg quantiti fish versus littl fish,"['Qingyuan Zhao', 'Dylan S. Small', 'Paul R. Rosenbaum']","['stat.ME', 'stat.AP']",False,False,True,False,False,False
963,2017-03-28T14:02:51Z,2017-03-06T15:58:26Z,http://arxiv.org/abs/1703.01937v1,http://arxiv.org/pdf/1703.01937v1,Reputation Dynamics in a Market for Illicit Drugs,reput dynam market illicit drug,"We analyze reputation dynamics in an online market for illicit drugs using a novel dataset of prices and ratings. The market is a black market, and so contracts cannot be enforced. We study the role that reputation plays in alleviating adverse selection in this market. We document the following stylized facts: (i) There is a positive relationship between the price and the rating of a seller. This effect is increasing in the number of reviews left for a seller. A mature highly-rated seller charges a 20% higher price than a mature low-rated seller. (ii) Sellers with more reviews charge higher prices regardless of rating. (iii) Low-rated sellers are more likely to exit the market and make fewer sales. We show that these stylized facts are explained by a dynamic model of adverse selection, ratings, and exit, in which buyers form rational inferences about the quality of a seller jointly from his rating and number of sales. Sellers who receive low ratings initially charge the same price as highly-rated sellers since early reviews are less informative about quality. Bad sellers exit rather than face lower prices in the future. We provide conditions under which our model admits a unique equilibrium. We estimate the model, and use the result to compute the returns to reputation in the market. We find that the market would have collapsed due to adverse selection in the absence of a rating system.",analyz reput dynam onlin market illicit drug use novel dataset price rate market black market contract cannot enforc studi role reput play allevi advers select market document follow styliz fact posit relationship price rate seller effect increas number review left seller matur high rate seller charg higher price matur low rate seller ii seller review charg higher price regardless rate iii low rate seller like exit market make fewer sale show styliz fact explain dynam model advers select rate exit buyer form ration infer qualiti seller joint rate number sale seller receiv low rate initi charg price high rate seller sinc earli review less inform qualiti bad seller exit rather face lower price futur provid condit model admit uniqu equilibrium estim model use result comput return reput market find market would collaps due advers select absenc rate system,"['Nick Janetos', 'Jan Tilly']",['stat.AP'],False,False,True,False,False,False
968,2017-03-28T14:02:51Z,2017-03-03T06:28:36Z,http://arxiv.org/abs/1703.01051v1,http://arxiv.org/pdf/1703.01051v1,Interval Estimation of the Unknown Exponential Parameter Based on Time   Truncated Data,interv estim unknown exponenti paramet base time truncat data,In this paper we consider the statistical inference of the unknown parameter of an exponential distribution based on the time truncated data. The time truncated data occurs quite often in the reliability analysis for type-I or hybrid censoring cases. All the results available today are based on the conditional argument that at least one failure occurs during the experiment. In this paper we provide some inferential results based on the unconditional argument. We extend the results for some two-parameter distributions also.,paper consid statist infer unknown paramet exponenti distribut base time truncat data time truncat data occur quit often reliabl analysi type hybrid censor case result avail today base condit argument least one failur occur dure experi paper provid inferenti result base uncondit argument extend result two paramet distribut also,"['Arnab Koley', 'Debasis Kundu']","['stat.AP', '62F10, 62F03, 62H12']",False,False,True,False,False,False
969,2017-03-28T14:02:51Z,2017-03-03T05:54:09Z,http://arxiv.org/abs/1703.01044v1,http://arxiv.org/pdf/1703.01044v1,On Generalized Progressive Hybrid Censoring in presence of competing   risks,general progress hybrid censor presenc compet risk,"The progressive Type-II hybrid censoring scheme introduced by Kundu and Joarder (\textit{Computational Statistics and Data Analysis}, 2509-2528, 2006), has received some attention in the last few years. One major drawback of this censoring scheme is that very few observations (even no observation at all) may be observed at the end of the experiment. To overcome this problem, Cho, Sun and Lee (\textit{Statistical Methodology}, 23, 18-34, 2015) recently introduced generalized progressive censoring which ensures to get a pre specified number of failures. In this paper we analyze generalized progressive censored data in presence of competing risks. For brevity we have considered only two competing causes of failures, and it is assumed that the lifetime of the competing causes follow one parameter exponential distributions with different scale parameters. We obtain the maximum likelihood estimators of the unknown parameters and also provide their exact distributions. Based on the exact distributions of the maximum likelihood estimators exact confidence intervals can be obtained. Asymptotic and bootstrap confidence intervals are also provided for comparison purposes. We further consider the Bayesian analysis of the unknown parameters under a very flexible Beta-Gamma prior. We provide the Bayes estimates and the associated credible intervals of the unknown parameters based on the above priors. We present extensive simulation results to see the effectiveness of the proposed method and finally one real data set is analyzed for illustrative purpose.",progress type ii hybrid censor scheme introduc kundu joarder textit comput statist data analysi receiv attent last year one major drawback censor scheme veri observ even observ may observ end experi overcom problem cho sun lee textit statist methodolog recent introduc general progress censor ensur get pre specifi number failur paper analyz general progress censor data presenc compet risk breviti consid onli two compet caus failur assum lifetim compet caus follow one paramet exponenti distribut differ scale paramet obtain maximum likelihood estim unknown paramet also provid exact distribut base exact distribut maximum likelihood estim exact confid interv obtain asymptot bootstrap confid interv also provid comparison purpos consid bayesian analysi unknown paramet veri flexibl beta gamma prior provid bay estim associ credibl interv unknown paramet base abov prior present extens simul result see effect propos method final one real data set analyz illustr purpos,"['Arnab Koley', 'Debasis Kundu']","['stat.AP', '62F10, 62F03, 62H12']",False,False,True,False,False,False
970,2017-03-28T14:02:55Z,2017-03-26T20:16:27Z,http://arxiv.org/abs/1703.01002v2,http://arxiv.org/pdf/1703.01002v2,Statistical Properties of the Risk-Transfer Formula in the Affordable   Care Act,statist properti risk transfer formula afford care act,"The Affordable Care Act, signed into United States law in 2010, led to the formation of competitive insurance plans which provide universal health-insurance coverage without regard to pre-existing medical conditions. To assist insurers during a transitional period, the Act introduced a risk-transfer formula which requires insurance plans with healthier enrollees to transfer funds to plans with sicker enrollees, thereby dissuading plans from favoring healthier enrollees. In this paper, we treat the risk-transfer amounts as random variables and derive some of their statistical properties by analyzing their means and variances. The results in this paper lead to an explanation for the empirical phenomena, observed by the American Academy of Actuaries, that risk-transfer amounts were more variable and can be extremely large for insurers with smaller market shares. Our results provide conditions, as functions of the market shares of insurance plans, under which those phenomena hold.",afford care act sign unit state law led format competit insur plan provid univers health insur coverag without regard pre exist medic condit assist insur dure transit period act introduc risk transfer formula requir insur plan healthier enrolle transfer fund plan sicker enrolle therebi dissuad plan favor healthier enrolle paper treat risk transfer amount random variabl deriv statist properti analyz mean varianc result paper lead explan empir phenomena observ american academi actuari risk transfer amount variabl extrem larg insur smaller market share result provid condit function market share insur plan phenomena hold,"['Michelle Li', 'Donald Richards']","['stat.AP', 'Primary: 62P05, Secondary: 60E05']",False,False,True,False,False,False
974,2017-03-28T14:02:55Z,2017-03-01T17:42:47Z,http://arxiv.org/abs/1703.00409v1,http://arxiv.org/pdf/1703.00409v1,Sequence of purchases in credit card data reveal life styles in urban   populations,sequenc purchas credit card data reveal life style urban popul,"From our most basic consumption to secondary needs, our spending habits reflect our life styles. Yet, in computational social sciences there is an open question about the existence of ubiquitous trends in spending habits by various groups at urban scale. Limited information collected by expenditure surveys have not proven conclusive in this regard. This is because, the frequency of purchases by type is highly uneven and follows a Zipf-like distribution. In this work, we apply text compression techniques to the purchase codes of credit card data to detect the significant sequences of transactions of each user. Five groups of consumers emerge when grouped by their similarity based on these sequences. Remarkably, individuals in each consumer group are also similar in age, total expenditure, gender, and the diversity of their social and mobility networks extracted by their mobile phone records. By properly deconstructing transaction data with Zipf-like distributions, we find that it can give us insights on collective behavior.",basic consumpt secondari need spend habit reflect life style yet comput social scienc open question exist ubiquit trend spend habit various group urban scale limit inform collect expenditur survey proven conclus regard becaus frequenc purchas type high uneven follow zipf like distribut work appli text compress techniqu purchas code credit card data detect signific sequenc transact user five group consum emerg group similar base sequenc remark individu consum group also similar age total expenditur gender divers social mobil network extract mobil phone record proper deconstruct transact data zipf like distribut find give us insight collect behavior,"['Riccardo Di Clemente', 'Miguel Luengo-Oroz', 'Matias Travizano', 'Bapu Vaitla', 'Marta C. Gonzalez']","['physics.soc-ph', 'cs.IT', 'cs.SI', 'math.IT', 'stat.AP']",False,False,True,False,False,False
975,2017-03-28T14:02:55Z,2017-03-01T07:00:01Z,http://arxiv.org/abs/1703.00154v1,http://arxiv.org/pdf/1703.00154v1,Inertial Odometry on Handheld Smartphones,inerti odometri handheld smartphon,"Building a complete inertial navigation system using the limited quality data provided by current smartphones has been regarded challenging, if not impossible. We present a probabilistic approach for orientation and use-case free inertial odometry, which is based on double-integrating rotated accelerations. Our approach uses a probabilistic approach in fusing the noisy sensor data and learning the model parameters online. It is able to track the phone position, velocity, and pose in real-time and in a computationally lightweight fashion. The information fusion is completed with altitude correction from barometric pressure readings (if available), zero-velocity updates (if the phone remains stationary), and pseudo-updates limiting the momentary speed. We demonstrate our approach using a standard iPad and iPhone in several indoor dead-reckoning applications and in a measurement tool setup.",build complet inerti navig system use limit qualiti data provid current smartphon regard challeng imposs present probabilist approach orient use case free inerti odometri base doubl integr rotat acceler approach use probabilist approach fuse noisi sensor data learn model paramet onlin abl track phone posit veloc pose real time comput lightweight fashion inform fusion complet altitud correct barometr pressur read avail zero veloc updat phone remain stationari pseudo updat limit momentari speed demonstr approach use standard ipad iphon sever indoor dead reckon applic measur tool setup,"['Arno Solin', 'Santiago Cortes', 'Esa Rahtu', 'Juho Kannala']","['cs.CV', 'stat.AP']",False,False,True,False,False,False
976,2017-03-28T14:02:55Z,2017-02-28T21:12:37Z,http://arxiv.org/abs/1703.00056v1,http://arxiv.org/pdf/1703.00056v1,Fair prediction with disparate impact: A study of bias in recidivism   prediction instruments,fair predict dispar impact studi bias recidiv predict instrument,"Recidivism prediction instruments (RPI's) provide decision makers with an assessment of the likelihood that a criminal defendant will reoffend at a future point in time. While such instruments are gaining increasing popularity across the country, their use is attracting tremendous controversy. Much of the controversy concerns potential discriminatory bias in the risk assessments that are produced. This paper discusses several fairness criteria that have recently been applied to assess the fairness of recidivism prediction instruments. We demonstrate that the criteria cannot all be simultaneously satisfied when recidivism prevalence differs across groups. We then show how disparate impact can arise when a recidivism prediction instrument fails to satisfy the criterion of error rate balance.",recidiv predict instrument rpi provid decis maker assess likelihood crimin defend reoffend futur point time instrument gain increas popular across countri use attract tremend controversi much controversi concern potenti discriminatori bias risk assess produc paper discuss sever fair criteria recent appli assess fair recidiv predict instrument demonstr criteria cannot simultan satisfi recidiv preval differ across group show dispar impact aris recidiv predict instrument fail satisfi criterion error rate balanc,['Alexandra Chouldechova'],"['stat.AP', 'cs.CY', 'stat.ML']",False,False,True,False,False,False
978,2017-03-28T14:02:55Z,2017-02-27T22:22:03Z,http://arxiv.org/abs/1702.08560v1,http://arxiv.org/pdf/1702.08560v1,"Estimating the reproductive number, total outbreak size, and reporting   rates for Zika epidemics in South and Central America",estim reproduct number total outbreak size report rate zika epidem south central america,"As South and Central American countries prepare for increased birth defects from Zika virus outbreaks and plan for mitigation strategies to minimize ongoing and future outbreaks, understanding important characteristics of Zika outbreaks and how they vary across regions is a challenging and important problem. We developed a mathematical model for the 2015 Zika virus outbreak dynamics in Colombia, El Salvador, and Suriname. We fit the model to publicly available data provided by the Pan American Health Organization, using Approximate Bayesian Computation to estimate parameter distributions and provide uncertainty quantification. An important model input is the at-risk susceptible population, which can vary with a number of factors including climate, elevation, population density, and socio-economic status. We informed this initial condition using the highest historically reported dengue incidence modified by the probable dengue reporting rates in the chosen countries. The model indicated that a country-level analysis was not appropriate for Colombia. We then estimated the basic reproduction number, or the expected number of new human infections arising from a single infected human, to range between 4 and 6 for El Salvador and Suriname with a median of 4.3 and 5.3, respectively. We estimated the reporting rate to be around 16% in El Salvador and 18% in Suriname with estimated total outbreak sizes of 73,395 and 21,647 people, respectively. The uncertainty in parameter estimates highlights a need for research and data collection that will better constrain parameter ranges.",south central american countri prepar increas birth defect zika virus outbreak plan mitig strategi minim ongo futur outbreak understand import characterist zika outbreak vari across region challeng import problem develop mathemat model zika virus outbreak dynam colombia el salvador surinam fit model public avail data provid pan american health organ use approxim bayesian comput estim paramet distribut provid uncertainti quantif import model input risk suscept popul vari number factor includ climat elev popul densiti socio econom status inform initi condit use highest histor report dengu incid modifi probabl dengu report rate chosen countri model indic countri level analysi appropri colombia estim basic reproduct number expect number new human infect aris singl infect human rang el salvador surinam median respect estim report rate around el salvador surinam estim total outbreak size peopl respect uncertainti paramet estim highlight need research data collect better constrain paramet rang,"['Deborah P. Shutt', 'Carrie A. Manore', 'Stephen Pankavich', 'Aaron T. Porter', 'Sara Y. Del Valle']","['q-bio.PE', 'q-bio.QM', 'stat.AP', '92D30']",False,False,True,False,False,True
979,2017-03-28T14:02:55Z,2017-02-27T18:38:28Z,http://arxiv.org/abs/1703.01237v1,http://arxiv.org/pdf/1703.01237v1,How real is the random censorship model in medical studies?,real random censorship model medic studi,"In survival analysis the random censorship model refers to censoring and survival times being independent of each other. It is one of the fundamental assumptions in the theory of survival analysis. We explain the reason for it being so ubiquitous, and we investigate its presence in medical studies. We differentiate two types of censoring in medical studies (dropout and administrative), and we explain their importance in examining the existence of the random censorship model. We show that in order to presume the random censorship model it is not enough to have a design study which conforms to it, but that one needs to provide evidence for its presence in the results. Blindly presuming the random censorship model might lead to the Kaplan-Meier estimator producing biased results, which might have serious consequences when estimating survival in medical studies.",surviv analysi random censorship model refer censor surviv time independ one fundament assumpt theori surviv analysi explain reason ubiquit investig presenc medic studi differenti two type censor medic studi dropout administr explain import examin exist random censorship model show order presum random censorship model enough design studi conform one need provid evid presenc result blind presum random censorship model might lead kaplan meier estim produc bias result might serious consequ estim surviv medic studi,['Damjan Krstajic'],"['stat.AP', 'math.ST', 'stat.TH']",False,False,True,False,False,False
982,2017-03-28T14:03:00Z,2017-02-27T04:17:36Z,http://arxiv.org/abs/1702.08140v1,http://arxiv.org/pdf/1702.08140v1,A mixture model approach to infer land-use influence on point referenced   water quality,mixtur model approach infer land use influenc point referenc water qualiti,"The assessment of water quality across space and time is of considerable interest for both agricultural and public health reasons. The standard method to assess the water quality of a catchment, or a group of catchments, usually involves collecting point measurements of water quality and other additional information such as the date and time of measurements, rainfall amounts, the land-use and soil-type of the catchment and the elevation. Some of this auxiliary information will be point data, measured at the exact location, whereas other such as land-use will be areal data often in a compositional format. Two problems arise if analysts try to incorporate this information into a statistical model in order to predict (for example) the influence of land-use on water quality. First is the spatial change of support problem that arises when using areal data to predict outcomes at point locations. Secondly, the physical process driving water quality is not compositional, rather it is the observation process that provides compositional data. In this paper we present an approach that accounts for these two issues by using a latent variable to identify the land-use that most likely influences water quality. This latent variable is used in a spatial mixture model to help estimate the influence of land-use on water quality. We demonstrate the potential of this approach with data from a water quality research study in the Mount Lofty range, in South Australia.",assess water qualiti across space time consider interest agricultur public health reason standard method assess water qualiti catchment group catchment usual involv collect point measur water qualiti addit inform date time measur rainfal amount land use soil type catchment elev auxiliari inform point data measur exact locat wherea land use areal data often composit format two problem aris analyst tri incorpor inform statist model order predict exampl influenc land use water qualiti first spatial chang support problem aris use areal data predict outcom point locat second physic process drive water qualiti composit rather observ process provid composit data paper present approach account two issu use latent variabl identifi land use like influenc water qualiti latent variabl use spatial mixtur model help estim influenc land use water qualiti demonstr potenti approach data water qualiti research studi mount lofti rang south australia,"['Adrien Ickowicz', 'Jessica H. Ford', 'Keith R. Hayes']","['stat.AP', 'stat.CO']",False,False,True,False,False,False
986,2017-03-28T14:03:00Z,2017-03-04T00:20:43Z,http://arxiv.org/abs/1702.07909v2,http://arxiv.org/pdf/1702.07909v2,Analysis of Urban Vibrancy and Safety in Philadelphia,analysi urban vibranc safeti philadelphia,"Statistical analyses of urban environments have been recently improved through publicly available high resolution data and mapping technologies that have adopted across industries. These technologies allow us to create metrics to empirically investigate urban design principles of the past half-century. Philadelphia is an interesting case study for this work, with its rapid urban development and population increase in the last decade. We focus on features of what urban planners call ""vibrancy"": measures of positive, healthy activity or energy in an area. Historically, vibrancy has been very challenging to measure empirically. We explore the association between safety (violent and non-violent crime) and features of local neighborhood vibrancy such as population, economic measures and land use zoning. Despite rhetoric about the negative effects of population density in the 1960s and 70s, we find very little association between crime and population density. Measures based on land use zoning are not an adequate description of local vibrancy and so we construct a database and set of measures of business activity in each neighborhood. We employ several matching analyses within census block groups to explore the relationship between neighborhood vibrancy and safety at a higher resolution. We find that neighborhoods with more vacancy have higher crime but within neighborhoods, crimes tend not to be located near vacant properties. We also find that more crimes occur near business locations but businesses that are active (open) for longer periods are associated with fewer crimes.",statist analys urban environ recent improv public avail high resolut data map technolog adopt across industri technolog allow us creat metric empir investig urban design principl past half centuri philadelphia interest case studi work rapid urban develop popul increas last decad focus featur urban planner call vibranc measur posit healthi activ energi area histor vibranc veri challeng measur empir explor associ safeti violent non violent crime featur local neighborhood vibranc popul econom measur land use zone despit rhetor negat effect popul densiti find veri littl associ crime popul densiti measur base land use zone adequ descript local vibranc construct databas set measur busi activ neighborhood employ sever match analys within census block group explor relationship neighborhood vibranc safeti higher resolut find neighborhood vacanc higher crime within neighborhood crime tend locat near vacant properti also find crime occur near busi locat busi activ open longer period associ fewer crime,"['Colman Humphrey', 'Shane T. Jensen', 'Dylan Small', 'Rachel Thurston']",['stat.AP'],False,False,True,False,False,True
989,2017-03-28T14:03:00Z,2017-02-23T23:25:32Z,http://arxiv.org/abs/1702.07422v1,http://arxiv.org/pdf/1702.07422v1,sourceR: Classification and Source Attribution of Infectious Agents   among Heterogeneous Populations,sourcer classif sourc attribut infecti agent among heterogen popul,"Zoonotic diseases are a major cause of morbidity, and productivity losses in both humans and animal populations. Identifying the source of food-borne zoonoses (e.g. an animal reservoir or food product) is crucial for the identification and prioritisation of food safety interventions. For many zoonotic diseases it is difficult to attribute human cases to sources of infection because there is little epidemiological information on the cases. However, microbial strain typing allows zoonotic pathogens to be categorised, and the relative frequencies of the strain types among the sources and in human cases allows inference on the likely source of each infection. We introduce sourceR, an R package for quantitative source attribution, aimed at food-borne diseases. It implements a fully joint Bayesian model using strain-typed surveillance data from both human cases and source samples, capable of identifying important sources of infection. The model measures the force of infection from each source, allowing for varying survivability, pathogenicity and virulence of pathogen strains, and varying abilities of the sources to act as vehicles of infection. A Bayesian non-parametric (Dirichlet process) approach is used to cluster pathogen strain types by epidemiological behaviour, avoiding model overfitting and allowing detection of strain types associated with potentially high 'virulence'.   sourceR is demonstrated using Campylobacter jejuni isolate data collected in New Zealand between 2005 and 2008. It enables straightforward attribution of cases of zoonotic infection to putative sources of infection by epidemiologists and public health decision makers. As sourceR develops, we intend it to become an important and flexible resource for food-borne disease attribution studies.",zoonot diseas major caus morbid product loss human anim popul identifi sourc food born zoonos anim reservoir food product crucial identif prioritis food safeti intervent mani zoonot diseas difficult attribut human case sourc infect becaus littl epidemiolog inform case howev microbi strain type allow zoonot pathogen categoris relat frequenc strain type among sourc human case allow infer like sourc infect introduc sourcer packag quantit sourc attribut aim food born diseas implement fulli joint bayesian model use strain type surveil data human case sourc sampl capabl identifi import sourc infect model measur forc infect sourc allow vari surviv pathogen virul pathogen strain vari abil sourc act vehicl infect bayesian non parametr dirichlet process approach use cluster pathogen strain type epidemiolog behaviour avoid model overfit allow detect strain type associ potenti high virul sourcer demonstr use campylobact jejuni isol data collect new zealand enabl straightforward attribut case zoonot infect putat sourc infect epidemiologist public health decis maker sourcer develop intend becom import flexibl resourc food born diseas attribut studi,"['Poppy Miller', 'Jonathan Marshall', 'Nigel French', 'Chris Jewell']",['stat.AP'],False,False,True,False,False,False
992,2017-03-28T14:03:04Z,2017-02-22T21:09:19Z,http://arxiv.org/abs/1702.07007v1,http://arxiv.org/pdf/1702.07007v1,Detecting causal associations in large nonlinear time series datasets,detect causal associ larg nonlinear time seri dataset,"Detecting causal associations in time series datasets is a key challenge for novel insights into complex dynamical systems such as the Earth system or the human brain. Interactions in high-dimensional dynamical systems often involve time-delays, nonlinearity, and strong autocorrelations. These present major challenges for causal discovery techniques such as Granger causality leading to low detection power, biases, and unreliable hypothesis tests. Here we introduce a reliable and fast method that outperforms current approaches in detection power and scales up to high-dimensional datasets. It overcomes detection biases, especially when strong autocorrelations are present, and allows ranking associations in large-scale analyses by their causal strength. We provide mathematical proofs, evaluate our method in extensive numerical experiments, and illustrate its capabilities in a large-scale analysis of the global surface-pressure system where we unravel spurious associations and find several potentially causal links that are difficult to detect with standard methods. The broadly applicable method promises to discover novel causal insights also in many other fields of science.",detect causal associ time seri dataset key challeng novel insight complex dynam system earth system human brain interact high dimension dynam system often involv time delay nonlinear strong autocorrel present major challeng causal discoveri techniqu granger causal lead low detect power bias unreli hypothesi test introduc reliabl fast method outperform current approach detect power scale high dimension dataset overcom detect bias especi strong autocorrel present allow rank associ larg scale analys causal strength provid mathemat proof evalu method extens numer experi illustr capabl larg scale analysi global surfac pressur system unravel spurious associ find sever potenti causal link difficult detect standard method broad applic method promis discov novel causal insight also mani field scienc,"['Jakob Runge', 'Dino Sejdinovic', 'Seth Flaxman']","['stat.ME', 'physics.ao-ph', 'stat.AP']",False,False,True,False,False,False
993,2017-03-28T14:03:04Z,2017-02-22T20:18:36Z,http://arxiv.org/abs/1702.06993v1,http://arxiv.org/pdf/1702.06993v1,Generalized Pareto Processes and Liquidity,general pareto process liquid,"Motivated by the modeling of liquidity risk in fund management in a dynamic setting, we propose and investigate a class of time series models with generalized Pareto marginals: the autoregressive generalized Pareto process (ARGP), a modified ARGP (MARGP) and a thresholded ARGP (TARGP). These models are able to capture key data features apparent in fund liquidity data and reflect the underlying phenomena via easily interpreted, low-dimensional model parameters. We establish stationarity and ergodicity, provide a link to the class of shot-noise processes, and determine the associated interarrival distributions for exceedances. Moreover, we provide estimators for all relevant model parameters and establish consistency and asymptotic normality for all estimators (except the threshold parameter, which as usual must be dealt with separately). Finally, we illustrate our approach using real-world fund redemption data, and we discuss the goodness-of-fit of the estimated models.",motiv model liquid risk fund manag dynam set propos investig class time seri model general pareto margin autoregress general pareto process argp modifi argp margp threshold argp targp model abl captur key data featur appar fund liquid data reflect phenomena via easili interpret low dimension model paramet establish stationar ergod provid link class shot nois process determin associ interarriv distribut exceed moreov provid estim relev model paramet establish consist asymptot normal estim except threshold paramet usual must dealt separ final illustr approach use real world fund redempt data discuss good fit estim model,"['Sascha Desmettre', 'Johan de Kock', 'Peter Ruckdeschel', 'Frank Thomas Seifried']","['stat.AP', '60G70, 62P05']",False,False,True,False,False,False
994,2017-03-28T14:03:04Z,2017-02-22T03:10:41Z,http://arxiv.org/abs/1702.06661v1,http://arxiv.org/pdf/1702.06661v1,Social Learning and Diffusion of Pervasive Goods: An Empirical Study of   an African App Store,social learn diffus pervas good empir studi african app store,"In this study, the authors develop a structural model that combines a macro diffusion model with a micro choice model to control for the effect of social influence on the mobile app choices of customers over app stores. Social influence refers to the density of adopters within the proximity of other customers. Using a large data set from an African app store and Bayesian estimation methods, the authors quantify the effect of social influence and investigate the impact of ignoring this process in estimating customer choices. The findings show that customer choices in the app store are explained better by offline than online density of adopters and that ignoring social influence in estimations results in biased estimates. Furthermore, the findings show that the mobile app adoption process is similar to adoption of music CDs, among all other classic economy goods. A counterfactual analysis shows that the app store can increase its revenue by 13.6% through a viral marketing policy (e.g., a sharing with friends and family button).",studi author develop structur model combin macro diffus model micro choic model control effect social influenc mobil app choic custom app store social influenc refer densiti adopt within proxim custom use larg data set african app store bayesian estim method author quantifi effect social influenc investig impact ignor process estim custom choic find show custom choic app store explain better offlin onlin densiti adopt ignor social influenc estim result bias estim furthermor find show mobil app adopt process similar adopt music cds among classic economi good counterfactu analysi show app store increas revenu viral market polici share friend famili button,"['Meisam Hejazi Nia', 'Brian T. Ratchford', 'Norris Bruce']","['stat.ML', 'cs.SI', 'stat.AP']",False,False,True,False,False,False
995,2017-03-28T14:03:04Z,2017-02-22T02:21:02Z,http://arxiv.org/abs/1702.06650v1,http://arxiv.org/pdf/1702.06650v1,Reducing the uncertainty in the forest volume-to-biomass relationship   built from limited field plots,reduc uncertainti forest volum biomass relationship built limit field plot,"The method of biomass estimation based on a volume-to-biomass relationship has been applied in estimating forest biomass conventionally through the mean volume (m3 ha-1). However, few studies have been reported concerning the verification of the volume-biomass equations regressed using field data. The possible bias may result from the volume measurements and extrapolations from sample plots to stands or a unit area. This paper addresses (i) how to verify the volume-biomass equations, and (ii) how to reduce the bias while building these equations. This paper presents an applicable method for verifying the field data using reasonable wood densities, restricting the error in field data processing based on limited field plots, and achieving a better understanding of the uncertainty in building those equations. The verified and improved volume-biomass equations are more reliable and will help to estimate forest carbon sequestration and carbon balance at any large scale.",method biomass estim base volum biomass relationship appli estim forest biomass convent mean volum ha howev studi report concern verif volum biomass equat regress use field data possibl bias may result volum measur extrapol sampl plot stand unit area paper address verifi volum biomass equat ii reduc bias build equat paper present applic method verifi field data use reason wood densiti restrict error field data process base limit field plot achiev better understand uncertainti build equat verifi improv volum biomass equat reliabl help estim forest carbon sequestr carbon balanc ani larg scale,"['Caixia Liu', 'Xiaolu Zhou', 'Xiangdong Lei', 'Huabing Huang', 'Changhui Peng', 'Xiaoyi Wang', 'Jianfeng Sun', 'Carl Zhou']","['stat.AP', 'q-bio.QM']",False,False,True,False,False,False
997,2017-03-28T14:03:04Z,2017-02-21T01:38:28Z,http://arxiv.org/abs/1703.00398v1,http://arxiv.org/pdf/1703.00398v1,Analysis on Cohort Effects in view of Differential Geometry and its   Applications,analysi cohort effect view differenti geometri applic,"This paper analyzes birth cohort effects and develops an approach which is based on differential geometry to identify and measure cohort effects in mortality data sets. The measurement is quantitative and provides a potential method to compare cohort effects among different countries or groups. Data sets of four countries (e.g. U.k., U.S., Canada and Japan) are taken as examples to explain our approach and applications of the measurement to a modified Lee-Carter model are analyzed. In fact, this paper is an upgrade version of our paper arXiv:1504.00327. There is a new section which gives applications of our approach based on the Lee-Carter and APC models.",paper analyz birth cohort effect develop approach base differenti geometri identifi measur cohort effect mortal data set measur quantit provid potenti method compar cohort effect among differ countri group data set four countri canada japan taken exampl explain approach applic measur modifi lee carter model analyz fact paper upgrad version paper arxiv new section give applic approach base lee carter apc model,"['Ning Zhang', 'Liang Zhao']","['stat.AP', '91G80, 91B30']",False,False,True,False,False,False
1003,2017-03-28T14:03:09Z,2017-03-26T22:49:31Z,http://arxiv.org/abs/1703.08882v1,http://arxiv.org/pdf/1703.08882v1,A Mixture of Matrix Variate Skew-t Distributions,mixtur matrix variat skew distribut,"Clustering is the process of finding underlying group structures in data. Although model-based clustering is firmly established in the multivariate case, there is relative paucity for matrix variate distributions, and there are even fewer examples using matrix variate skew distributions. In this paper, we look at parameter estimation for a finite mixture of matrix variate skew-t distributions in the context of model-based clustering. Simulated data is used for illustrative purposes.",cluster process find group structur data although model base cluster firm establish multivari case relat pauciti matrix variat distribut even fewer exampl use matrix variat skew distribut paper look paramet estim finit mixtur matrix variat skew distribut context model base cluster simul data use illustr purpos,"['Michael P. B. Gallaugher', 'Paul D. McNicholas']","['stat.ME', 'stat.CO']",False,False,True,False,False,True
1009,2017-03-28T14:03:09Z,2017-03-21T03:27:22Z,http://arxiv.org/abs/1703.07039v1,http://arxiv.org/pdf/1703.07039v1,A Simple Online Parameter Estimation Technique with Asymptotic   Guarantees,simpl onlin paramet estim techniqu asymptot guarante,"In many modern settings, data are acquired iteratively over time, rather than all at once. Such settings are known as online, as opposed to offline or batch. We introduce a simple technique for online parameter estimation, which can operate in low memory settings, settings where data are correlated, and only requires a single inspection of the available data at each time period. We show that the estimators---constructed via the technique---are asymptotically normal under generous assumptions, and present a technique for the online computation of the covariance matrices for such estimators. A set of numerical studies demonstrates that our estimators can be as efficient as their offline counterparts, and that our technique generates estimates and confidence intervals that match their offline counterparts in various parameter estimation settings.",mani modern set data acquir iter time rather onc set known onlin oppos offlin batch introduc simpl techniqu onlin paramet estim oper low memori set set data correl onli requir singl inspect avail data time period show estim construct via techniqu asymptot normal generous assumpt present techniqu onlin comput covari matric estim set numer studi demonstr estim effici offlin counterpart techniqu generat estim confid interv match offlin counterpart various paramet estim set,['Hien D Nguyen'],['stat.CO'],False,False,True,False,False,False
1011,2017-03-28T14:03:13Z,2017-03-18T22:12:08Z,http://arxiv.org/abs/1703.06359v1,http://arxiv.org/pdf/1703.06359v1,Fully symmetric kernel quadrature,fulli symmetr kernel quadratur,"Kernel quadratures and other kernel-based approximation methods typically suffer from prohibitive cubic time and quadratic space complexity in the number of function evaluations. The problem arises because a system of linear equations needs to be solved. In this article we show that the weights of a kernel quadrature rule can be computed efficiently and exactly for up to tens of millions of nodes if the kernel, integration domain, and measure are fully symmetric and the node set is a union of fully symmetric sets. This is based on the observations that in such a setting there are only as many distinct weights as there are fully symmetric sets and that these weights can be solved from a linear system of equations constructed out of row sums of certain submatrices of the full kernel matrix. We present several numerical examples that show feasibility, both for a large number of nodes and in high dimensions, of the developed fully symmetric kernel quadrature rules. Most prominent of the fully symmetric kernel quadrature rules we propose are those that use sparse grids.",kernel quadratur kernel base approxim method typic suffer prohibit cubic time quadrat space complex number function evalu problem aris becaus system linear equat need solv articl show weight kernel quadratur rule comput effici exact ten million node kernel integr domain measur fulli symmetr node set union fulli symmetr set base observ set onli mani distinct weight fulli symmetr set weight solv linear system equat construct row sum certain submatric full kernel matrix present sever numer exampl show feasibl larg number node high dimens develop fulli symmetr kernel quadratur rule promin fulli symmetr kernel quadratur rule propos use spars grid,"['Toni Karvonen', 'Simo Särkkä']","['math.NA', 'cs.NA', 'stat.CO']",False,False,True,False,False,True
1016,2017-03-28T14:03:13Z,2017-03-16T14:09:50Z,http://arxiv.org/abs/1703.06826v1,http://arxiv.org/pdf/1703.06826v1,RatingScaleReduction package: stepwise rating scale item reduction   without predictability loss,ratingscalereduct packag stepwis rate scale item reduct without predict loss,"This study presents an innovative method for reducing the number of rating scale items without predictability loss. The ""area under the re- ceiver operator curve method"" (AUC ROC) is used to implement in the RatingScaleReduction package posted on CRAN. Several cases have been used to illustrate how the stepwise method has reduced the number of rating scale items (variables).",studi present innov method reduc number rate scale item without predict loss area ceiver oper curv method auc roc use implement ratingscalereduct packag post cran sever case use illustr stepwis method reduc number rate scale item variabl,"['Waldemar W. Koczkodaj', 'Alicja Wolny-Dominiak']","['stat.CO', '94A50, 62C25, 62C99, 62P10']",False,False,True,False,False,False
1022,2017-03-28T14:03:17Z,2017-03-18T09:09:43Z,http://arxiv.org/abs/1703.04467v2,http://arxiv.org/pdf/1703.04467v2,spmoran: An R package for Moran's eigenvector-based spatial regression   analysis,spmoran packag moran eigenvector base spatial regress analysi,"The objective of this study is illustrating how to use ""spmoran,"" which is an R package for Moran's eigenvector-based spatial regression analysis. spmoran estimates regression models in the presence of spatial dependence, including eigenvector spatial filtering (ESF) and random effects ESF (RE-ESF) models. These models are allowed to have spatially varying coefficients to capture spatial heterogeneity. These ESF and RE-ESF models are suitable to estimate and infer regression coefficients with/without spatial variation. spmoran implements these models in a computationally efficient manner. For the illustration, this study applies ESF and RE-ESF models for a land price analysis.",object studi illustr use spmoran packag moran eigenvector base spatial regress analysi spmoran estim regress model presenc spatial depend includ eigenvector spatial filter esf random effect esf esf model model allow spatial vari coeffici captur spatial heterogen esf esf model suitabl estim infer regress coeffici without spatial variat spmoran implement model comput effici manner illustr studi appli esf esf model land price analysi,['Daisuke Murakami'],"['stat.OT', 'stat.CO']",False,False,True,False,False,False
1024,2017-03-28T14:03:17Z,2017-03-11T20:07:06Z,http://arxiv.org/abs/1703.04025v1,http://arxiv.org/pdf/1703.04025v1,Learning Large-Scale Bayesian Networks with the sparsebn Package,learn larg scale bayesian network sparsebn packag,"Learning graphical models from data is an important problem with wide applications, ranging from genomics to the social sciences. Nowadays datasets typically have upwards of thousands---sometimes tens or hundreds of thousands---of variables and far fewer samples. To meet this challenge, we develop a new R package called sparsebn for learning the structure of large, sparse graphical models with a focus on Bayesian networks. While there are many existing packages for this task within the R ecosystem, this package focuses on the unique setting of learning large networks from high-dimensional data, possibly with interventions. As such, the methods provided place a premium on scalability and consistency in a high-dimensional setting. Furthermore, in the presence of interventions, the methods implemented here achieve the goal of learning a causal network from data. The sparsebn package is open-source and available on CRAN.",learn graphic model data import problem wide applic rang genom social scienc nowaday dataset typic upward thousand sometim ten hundr thousand variabl far fewer sampl meet challeng develop new packag call sparsebn learn structur larg spars graphic model focus bayesian network mani exist packag task within ecosystem packag focus uniqu set learn larg network high dimension data possibl intervent method provid place premium scalabl consist high dimension set furthermor presenc intervent method implement achiev goal learn causal network data sparsebn packag open sourc avail cran,"['Bryon Aragam', 'Jiaying Gu', 'Qing Zhou']","['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']",False,False,True,False,False,False
1025,2017-03-28T14:03:17Z,2017-03-10T13:43:06Z,http://arxiv.org/abs/1703.03680v1,http://arxiv.org/pdf/1703.03680v1,Strong convergence rates of probabilistic integrators for ordinary   differential equations,strong converg rate probabilist integr ordinari differenti equat,"Probabilistic integration of a continuous dynamical system is a way of systematically introducing model error, at scales no larger than errors inroduced by standard numerical discretisation, in order to enable thorough exploration of possible responses of the system to inputs. It is thus a potentially useful approach in a number of applications such as forward uncertainty quantification, inverse problems, and data assimilation. We extend the convergence analysis of probabilistic integrators for deterministic ordinary differential equations, as proposed by Conrad et al. (Stat. Comput., 2016), to establish mean-square convergence in the uniform norm on discrete- or continuous-time solutions under relaxed regularity assumptions on the driving vector fields and their induced flows. Specifically, we show that randomised high-order integrators for globally Lipschitz flows and randomised Euler integrators for dissipative vector fields with polynomially-bounded local Lipschitz constants all have the same mean-square convergence rate as their deterministic counterparts, provided that the variance of the integration noise is not of higher order than the corresponding deterministic integrator.",probabilist integr continu dynam system way systemat introduc model error scale larger error inroduc standard numer discretis order enabl thorough explor possibl respons system input thus potenti use approach number applic forward uncertainti quantif invers problem data assimil extend converg analysi probabilist integr determinist ordinari differenti equat propos conrad et al stat comput establish mean squar converg uniform norm discret continu time solut relax regular assumpt drive vector field induc flow specif show randomis high order integr global lipschitz flow randomis euler integr dissip vector field polynomi bound local lipschitz constant mean squar converg rate determinist counterpart provid varianc integr nois higher order correspond determinist integr,"['H. C. Lie', 'A. M. Stuart', 'T. J. Sullivan']","['math.NA', 'math.PR', 'math.ST', 'stat.CO', 'stat.TH', '65L20, 65C99, 37H10, 68W20']",False,False,True,False,False,False
1026,2017-03-28T14:03:17Z,2017-03-09T21:57:00Z,http://arxiv.org/abs/1703.03475v1,http://arxiv.org/pdf/1703.03475v1,Auxiliary Variables for Bayesian Inference in Multi-Class Queueing   Networks,auxiliari variabl bayesian infer multi class queue network,"Queue networks describe complex stochastic systems of both theoretical and practical interest. They provide the means to assess alterations, diagnose poor performance and evaluate robustness across sets of interconnected resources. In the present paper, we focus on the underlying continuous-time Markov chains induced by these networks, and we present a flexible method for drawing parameter inference in multi-class Markovian cases with switching and different service disciplines. The approach is directed towards the inferential problem with missing data and introduces a slice sampling technique with mappings to the measurable space of task transitions between service stations. The method deals with time and tractability issues, can handle prior system knowledge and overcomes common restrictions on service rates across existing inferential frameworks. Finally, the proposed algorithm is validated on synthetic data and applied to a real data set, obtained from a service delivery tasking tool implemented in two university hospitals.",queue network describ complex stochast system theoret practic interest provid mean assess alter diagnos poor perform evalu robust across set interconnect resourc present paper focus continu time markov chain induc network present flexibl method draw paramet infer multi class markovian case switch differ servic disciplin approach direct toward inferenti problem miss data introduc slice sampl techniqu map measur space task transit servic station method deal time tractabl issu handl prior system knowledg overcom common restrict servic rate across exist inferenti framework final propos algorithm valid synthet data appli real data set obtain servic deliveri task tool implement two univers hospit,"['Iker Perez', 'David Hodge', 'Theodore Kypraios']",['stat.CO'],False,False,True,False,False,False
1036,2017-03-28T14:03:21Z,2017-03-07T07:44:52Z,http://arxiv.org/abs/1703.02251v1,http://arxiv.org/pdf/1703.02251v1,The Maximum Likelihood Degree of Toric Varieties,maximum likelihood degre toric varieti,"We study the maximum likelihood degree (ML degree) of toric varieties, known as discrete exponential models in statistics. By introducing scaling coefficients to the monomial parameterization of the toric variety, one can change the ML degree. We show that the ML degree is equal to the degree of the toric variety for generic scalings, while it drops if and only if the scaling vector is in the locus of the principal $A$-determinant. We also illustrate how to compute the ML estimate of a toric variety numerically via homotopy continuation from a scaled toric variety with low ML degree. Throughout, we include examples motivated by algebraic geometry and statistics. We compute the ML degree of rational normal scrolls and a large class of Veronese-type varieties. In addition, we investigate the ML degree of scaled Segre varieties, hierarchical loglinear models, and graphical models.",studi maximum likelihood degre ml degre toric varieti known discret exponenti model statist introduc scale coeffici monomi parameter toric varieti one chang ml degre show ml degre equal degre toric varieti generic scale drop onli scale vector locus princip determin also illustr comput ml estim toric varieti numer via homotopi continu scale toric varieti low ml degre throughout includ exampl motiv algebra geometri statist comput ml degre ration normal scroll larg class verones type varieti addit investig ml degre scale segr varieti hierarch loglinear model graphic model,"['Carlos Améndola', 'Nathan Bliss', 'Isaac Burke', 'Courtney R. Gibbons', 'Martin Helmer', 'Serkan Hoşten', 'Evan D. Nash', 'Jose Israel Rodriguez', 'Daniel Smolkin']","['math.AG', 'math.ST', 'stat.CO', 'stat.TH', '14Q15, 14M25, 13P15, 62F10']",False,False,True,False,False,True
1037,2017-03-28T14:03:21Z,2017-03-07T06:40:44Z,http://arxiv.org/abs/1703.02237v1,http://arxiv.org/pdf/1703.02237v1,Scalable Collaborative Targeted Learning for High-Dimensional Data,scalabl collabor target learn high dimension data,"Robust inference of a low-dimensional parameter in a large semi-parametric model relies on external estimators of infinite-dimensional features of the distribution of the data. Typically, only one of the latter is optimized for the sake of constructing a well behaved estimator of the low-dimensional parameter of interest. Optimizing more than one of them for the sake of achieving a better bias-variance trade-off in the estimation of the parameter of interest is the core idea driving the general template of the collaborative targeted minimum loss-based estimation (C-TMLE) procedure. The original implementation/instantiation of the C-TMLE template can be presented as a greedy forward stepwise C-TMLE algorithm. It does not scale well when the number $p$ of covariates increases drastically. This motivates the introduction of a novel instantiation of the C-TMLE template where the covariates are pre-ordered. Its time complexity is $\mathcal{O}(p)$ as opposed to the original $\mathcal{O}(p^2)$, a remarkable gain. We propose two pre-ordering strategies and suggest a rule of thumb to develop other meaningful strategies. Because it is usually unclear a priori which pre-ordering strategy to choose, we also introduce another implementation/instantiation called SL-C-TMLE algorithm that enables the data-driven choice of the better pre-ordering strategy given the problem at hand. Its time complexity is $\mathcal{O}(p)$ as well. The computational burden and relative performance of these algorithms were compared in simulation studies involving fully synthetic data or partially synthetic data based on a real world large electronic health database; and in analyses of three real, large electronic health databases. In all analyses involving electronic health databases, the greedy C-TMLE algorithm is unacceptably slow. Simulation studies indicate our scalable C-TMLE and SL-C-TMLE algorithms work well.",robust infer low dimension paramet larg semi parametr model reli extern estim infinit dimension featur distribut data typic onli one latter optim sake construct well behav estim low dimension paramet interest optim one sake achiev better bias varianc trade estim paramet interest core idea drive general templat collabor target minimum loss base estim tmle procedur origin implement instanti tmle templat present greedi forward stepwis tmle algorithm doe scale well number covari increas drastic motiv introduct novel instanti tmle templat covari pre order time complex mathcal oppos origin mathcal remark gain propos two pre order strategi suggest rule thumb develop meaning strategi becaus usual unclear priori pre order strategi choos also introduc anoth implement instanti call sl tmle algorithm enabl data driven choic better pre order strategi given problem hand time complex mathcal well comput burden relat perform algorithm compar simul studi involv fulli synthet data partial synthet data base real world larg electron health databas analys three real larg electron health databas analys involv electron health databas greedi tmle algorithm unaccept slow simul studi indic scalabl tmle sl tmle algorithm work well,"['Cheng Ju', 'Susan Gruber', 'Samuel D. Lendle', 'Antoine Chambaz', 'Jessica M. Franklin', 'Richard Wyss', 'Sebastian Schneeweiss', 'Mark J. van der Laan']","['stat.CO', 'stat.ME']",False,False,True,False,False,False
1038,2017-03-28T14:03:21Z,2017-03-25T17:47:33Z,http://arxiv.org/abs/1703.02177v2,http://arxiv.org/pdf/1703.02177v2,Mixtures of Generalized Hyperbolic Distributions and Mixtures of Skew-t   Distributions for Model-Based Clustering with Incomplete Data,mixtur general hyperbol distribut mixtur skew distribut model base cluster incomplet data,"Robust clustering from incomplete data is an important topic because, in many practical situations, real data sets are heavy-tailed, asymmetric, and/or have arbitrary patterns of missing observations. Flexible methods and algorithms for model-based clustering are presented via mixture of the generalized hyperbolic distributions and its limiting case, the mixture of multivariate skew-t distributions. An analytically feasible EM algorithm is formulated for parameter estimation and imputation of missing values for mixture models employing missing at random mechanisms. The proposed methodologies are investigated through a simulation study with varying proportions of synthetic missing values and illustrated using a real dataset. Comparisons are made with those obtained from the traditional mixture of generalized hyperbolic distribution counterparts by filling in the missing data using the mean imputation method.",robust cluster incomplet data import topic becaus mani practic situat real data set heavi tail asymmetr arbitrari pattern miss observ flexibl method algorithm model base cluster present via mixtur general hyperbol distribut limit case mixtur multivari skew distribut analyt feasibl em algorithm formul paramet estim imput miss valu mixtur model employ miss random mechan propos methodolog investig simul studi vari proport synthet miss valu illustr use real dataset comparison made obtain tradit mixtur general hyperbol distribut counterpart fill miss data use mean imput method,"['Yuhong Wei', 'Paul D. McNicholas']","['stat.ME', 'stat.CO']",False,False,True,False,False,False
1039,2017-03-28T14:03:21Z,2017-03-06T23:48:50Z,http://arxiv.org/abs/1703.02151v1,http://arxiv.org/pdf/1703.02151v1,Computationally Efficient Simulation of Queues: The R Package   queuecomputer,comput effici simul queue packag queuecomput,"Large networks of queueing systems model important real-world systems such as MapReduce clusters, web-servers, hospitals, call-centers and airport passenger terminals.To model such systems accurately we must infer queueing parameters from data. Unfortunately, for many queueing networks there is no clear way to proceed with parameter inference from data. Approximate Bayesian computation could offer a straight-forward way to infer parameters for such networks if we could simulate data quickly enough.   We present a computationally efficient method for simulating from a very general set of queueing networks with the R package queuecomputer. Remarkable speedups of more than 2 orders of magnitude are observed relative to the popular DES packages simmer and simpy. We replicate output from these packages to validate the package.   The package is modular and integrates well with the popular R package dplyr. Complex queueing networks with tandem, parallel and fork/join topologies can easily be built with these two packages together. We show how to use this package with two examples: a call-centre and an airport terminal.",larg network queue system model import real world system mapreduc cluster web server hospit call center airport passeng termin model system accur must infer queue paramet data unfortun mani queue network clear way proceed paramet infer data approxim bayesian comput could offer straight forward way infer paramet network could simul data quick enough present comput effici method simul veri general set queue network packag queuecomput remark speedup order magnitud observ relat popular des packag simmer simpi replic output packag valid packag packag modular integr well popular packag dplyr complex queue network tandem parallel fork join topolog easili built two packag togeth show use packag two exampl call centr airport termin,"['Anthony Ebert', 'Paul Wu', 'Kerrie Mengersen', 'Fabrizio Ruggeri']","['stat.CO', 'math.OC']",False,False,True,False,False,False
1040,2017-03-28T14:03:25Z,2017-03-06T19:44:45Z,http://arxiv.org/abs/1703.02081v1,http://arxiv.org/pdf/1703.02081v1,Estimation and prediction in sparse and unbalanced tables,estim predict spars unbalanc tabl,"We consider the problem where we have a multi-way table of means, indexed by several factors, where each factor can have a large number of levels. The entry in each cell is the mean of some response, averaged over the observations falling into that cell. Some cells may be very sparsely populated, and in extreme cases, not populated at all. We might still like to estimate an expected response in such cells. We propose here a novel hierarchical ANOVA (HANOVA) representation for such data. Sparse cells will lean more on the lower-order interaction model for the data. These in turn could have components that are poorly represented in the data, in which case they rely on yet lower-order models. Our approach leads to a simple hierarchical algorithm, requiring repeated calculations of sub-table means of modified counts. The algorithm has shown superiority over the unshrinked methods in both simulations and real data sets.",consid problem multi way tabl mean index sever factor factor larg number level entri cell mean respons averag observ fall cell cell may veri spars popul extrem case popul might still like estim expect respons cell propos novel hierarch anova hanova represent data spars cell lean lower order interact model data turn could compon poor repres data case reli yet lower order model approach lead simpl hierarch algorithm requir repeat calcul sub tabl mean modifi count algorithm shown superior unshrink method simul real data set,"['Qingyuan Zhao', 'Trevor Hastie', 'Daryl Pregibon']",['stat.CO'],False,False,True,False,False,False
1045,2017-03-28T14:03:25Z,2017-03-03T10:44:47Z,http://arxiv.org/abs/1703.01106v1,http://arxiv.org/pdf/1703.01106v1,Differentially Private Bayesian Learning on Distributed Data,differenti privat bayesian learn distribut data,"Many applications of machine learning, for example in health care, would benefit from methods that can guarantee privacy of data subjects. Differential privacy (DP) has become established as a standard for protecting learning results, but the proposed algorithms require a single trusted party to have access to the entire data, which is a clear weakness. We consider DP Bayesian learning in a distributed setting, where each party only holds a single sample or a few samples of the data. We propose a novel method for DP learning in this distributed setting, based on a secure multi-party sum function for aggregating summaries from the data holders. Each data holder adds their share of Gaussian noise to make the total computation differentially private using the Gaussian mechanism. We prove that the system can be made secure against a desired number of colluding data owners and robust against faulting data owners. The method builds on an asymptotically optimal and practically efficient DP Bayesian inference with rapidly diminishing extra cost.",mani applic machin learn exampl health care would benefit method guarante privaci data subject differenti privaci dp becom establish standard protect learn result propos algorithm requir singl trust parti access entir data clear weak consid dp bayesian learn distribut set parti onli hold singl sampl sampl data propos novel method dp learn distribut set base secur multi parti sum function aggreg summari data holder data holder add share gaussian nois make total comput differenti privat use gaussian mechan prove system made secur desir number collud data owner robust fault data owner method build asymptot optim practic effici dp bayesian infer rapid diminish extra cost,"['Mikko Heikkilä', 'Yusuke Okimoto', 'Samuel Kaski', 'Kana Shimizu', 'Antti Honkela']","['stat.ML', 'cs.CR', 'cs.LG', 'stat.CO']",False,False,True,False,False,False
1055,2017-03-28T14:03:30Z,2017-02-27T12:03:01Z,http://arxiv.org/abs/1702.08248v1,http://arxiv.org/pdf/1702.08248v1,Scalable and Distributed Clustering via Lightweight Coresets,scalabl distribut cluster via lightweight coreset,"Coresets are compact representations of data sets such that models trained on a coreset are provably competitive with models trained on the full data set. As such, they have been successfully used to scale up clustering models to massive data sets. While existing approaches generally only allow for multiplicative approximation errors, we propose a novel notion of coresets called lightweight coresets that allows for both multiplicative and additive errors. We provide a single algorithm to construct light-weight coresets for k-Means clustering, Bregman clustering and maximum likelihood estimation of Gaussian mixture models. The algorithm is substantially faster than existing constructions, embarrassingly parallel and resulting coresets are smaller. In an extensive experimental evaluation, we demonstrate that the proposed method outperforms existing coreset constructions.",coreset compact represent data set model train coreset provabl competit model train full data set success use scale cluster model massiv data set exist approach general onli allow multipl approxim error propos novel notion coreset call lightweight coreset allow multipl addit error provid singl algorithm construct light weight coreset mean cluster bregman cluster maximum likelihood estim gaussian mixtur model algorithm substanti faster exist construct embarrass parallel result coreset smaller extens experiment evalu demonstr propos method outperform exist coreset construct,"['Olivier Bachem', 'Mario Lucic', 'Andreas Krause']","['stat.ML', 'cs.DC', 'cs.DS', 'cs.LG', 'stat.CO']",False,False,True,False,False,False
1058,2017-03-28T14:03:30Z,2017-02-27T04:17:36Z,http://arxiv.org/abs/1702.08140v1,http://arxiv.org/pdf/1702.08140v1,A mixture model approach to infer land-use influence on point referenced   water quality,mixtur model approach infer land use influenc point referenc water qualiti,"The assessment of water quality across space and time is of considerable interest for both agricultural and public health reasons. The standard method to assess the water quality of a catchment, or a group of catchments, usually involves collecting point measurements of water quality and other additional information such as the date and time of measurements, rainfall amounts, the land-use and soil-type of the catchment and the elevation. Some of this auxiliary information will be point data, measured at the exact location, whereas other such as land-use will be areal data often in a compositional format. Two problems arise if analysts try to incorporate this information into a statistical model in order to predict (for example) the influence of land-use on water quality. First is the spatial change of support problem that arises when using areal data to predict outcomes at point locations. Secondly, the physical process driving water quality is not compositional, rather it is the observation process that provides compositional data. In this paper we present an approach that accounts for these two issues by using a latent variable to identify the land-use that most likely influences water quality. This latent variable is used in a spatial mixture model to help estimate the influence of land-use on water quality. We demonstrate the potential of this approach with data from a water quality research study in the Mount Lofty range, in South Australia.",assess water qualiti across space time consider interest agricultur public health reason standard method assess water qualiti catchment group catchment usual involv collect point measur water qualiti addit inform date time measur rainfal amount land use soil type catchment elev auxiliari inform point data measur exact locat wherea land use areal data often composit format two problem aris analyst tri incorpor inform statist model order predict exampl influenc land use water qualiti first spatial chang support problem aris use areal data predict outcom point locat second physic process drive water qualiti composit rather observ process provid composit data paper present approach account two issu use latent variabl identifi land use like influenc water qualiti latent variabl use spatial mixtur model help estim influenc land use water qualiti demonstr potenti approach data water qualiti research studi mount lofti rang south australia,"['Adrien Ickowicz', 'Jessica H. Ford', 'Keith R. Hayes']","['stat.AP', 'stat.CO']",False,False,True,False,False,False
1060,2017-03-28T14:03:34Z,2017-02-26T17:50:02Z,http://arxiv.org/abs/1702.08446v1,http://arxiv.org/pdf/1702.08446v1,Monte Carlo on manifolds: sampling densities and integrating functions,mont carlo manifold sampl densiti integr function,"We describe and analyze some Monte Carlo methods for manifolds in Euclidean space defined by equality and inequality constraints. First, we give an MCMC sampler for probability distributions defined by un-normalized densities on such manifolds. The sampler uses a specific orthogonal projection to the surface that requires only information about the tangent space to the manifold, obtainable from first derivatives of the constraint functions, hence avoiding the need for curvature information or second derivatives. Second, we use the sampler to develop a multi-stage algorithm to compute integrals over such manifolds. We provide single-run error estimates that avoid the need for multiple independent runs. Computational experiments on various test problems show that the algorithms and error estimates work in practice. The method is applied to compute the entropies of different sticky hard sphere systems. These predict the temperature or interaction energy at which loops of hard sticky spheres become preferable to chains.",describ analyz mont carlo method manifold euclidean space defin equal inequ constraint first give mcmc sampler probabl distribut defin un normal densiti manifold sampler use specif orthogon project surfac requir onli inform tangent space manifold obtain first deriv constraint function henc avoid need curvatur inform second deriv second use sampler develop multi stage algorithm comput integr manifold provid singl run error estim avoid need multipl independ run comput experi various test problem show algorithm error estim work practic method appli comput entropi differ sticki hard sphere system predict temperatur interact energi loop hard sticki sphere becom prefer chain,"['Emilio Zappa', 'Miranda Holmes-Cerfon', 'Jonathan Goodman']","['math.NA', 'cond-mat.stat-mech', 'stat.CO']",False,False,True,False,False,False
1064,2017-03-28T14:03:34Z,2017-02-24T17:01:59Z,http://arxiv.org/abs/1702.07662v1,http://arxiv.org/pdf/1702.07662v1,A Network Epidemic Model for Online Community Commissioning Data,network epidem model onlin communiti commiss data,"Statistical models for network epidemics usually assume a Bernoulli random graph, in which any two nodes have the same probability of being connected. This assumption provides computational simplicity but does not describe real-life networks well. We propose an epidemic model based on the preferential attachment model, which adds nodes sequentially by simple rules to generate a network. A simulation study based on the subsequent Markov Chain Monte Carlo algorithm reveals an identifiability issue with the model parameters, so an alternative parameterisation is suggested. Finally, the model is applied to a set of online commissioning data.",statist model network epidem usual assum bernoulli random graph ani two node probabl connect assumpt provid comput simplic doe describ real life network well propos epidem model base preferenti attach model add node sequenti simpl rule generat network simul studi base subsequ markov chain mont carlo algorithm reveal identifi issu model paramet altern parameteris suggest final model appli set onlin commiss data,"['Clement Lee', 'Andrew Garbett', 'Darren J. Wilkinson']","['stat.CO', 'cs.SI', 'stat.ME']",False,False,True,False,False,False
1066,2017-03-28T14:03:34Z,2017-02-23T04:40:41Z,http://arxiv.org/abs/1702.07094v1,http://arxiv.org/pdf/1702.07094v1,BigVAR: Tools for Modeling Sparse High-Dimensional Multivariate Time   Series,bigvar tool model spars high dimension multivari time seri,"The R package BigVAR allows for the simultaneous estimation of high-dimensional time series by applying structured penalties to the conventional vector autoregression (VAR) and vector autoregression with exogenous variables (VARX) frameworks. Our methods can be utilized in many forecasting applications that make use of time-dependent data such as macroeconomics, finance, and internet traffic. Our package extends solution algorithms from the machine learning and signal processing literatures to a time dependent setting: selecting the regularization parameter by sequential cross validation and provides substantial improvements in forecasting performance over conventional methods. We offer a user-friendly interface that utilizes R's s4 object class structure which makes our methodology easily accessible to practicioners.   In this paper, we present an overview of our notation, the models that comprise BigVAR, and the functionality of our package with a detailed example using publicly available macroeconomic data. In addition, we present a simulation study comparing the performance of several procedures that refit the support selected by a BigVAR procedure according to several variants of least squares and conclude that refitting generally degrades forecast performance.",packag bigvar allow simultan estim high dimension time seri appli structur penalti convent vector autoregress var vector autoregress exogen variabl varx framework method util mani forecast applic make use time depend data macroeconom financ internet traffic packag extend solut algorithm machin learn signal process literatur time depend set select regular paramet sequenti cross valid provid substanti improv forecast perform convent method offer user friend interfac util object class structur make methodolog easili access practicion paper present overview notat model compris bigvar function packag detail exampl use public avail macroeconom data addit present simul studi compar perform sever procedur refit support select bigvar procedur accord sever variant least squar conclud refit general degrad forecast perform,"['William Nicholson', 'David Matteson', 'Jacob Bien']",['stat.CO'],False,False,True,False,False,True
1068,2017-03-28T14:03:34Z,2017-02-23T19:01:53Z,http://arxiv.org/abs/1702.06488v2,http://arxiv.org/pdf/1702.06488v2,Distributed Estimation of Principal Eigenspaces,distribut estim princip eigenspac,"Principal component analysis (PCA) is fundamental to statistical machine learning. It extracts latent principal factors that contribute to the most variation of the data. When data are stored across multiple machines, however, communication cost can prohibit the computation of PCA in a central location and distributed algorithms for PCA are thus needed. This paper proposes and studies a distributed PCA algorithm: each node machine computes the top $K$ eigenvectors and transmits them to the central server; the central server then aggregates the information from all the node machines and conducts a PCA based on the aggregated information. We investigate the bias and variance for the resulting distributed estimator of the top $K$ eigenvectors. In particular, we show that for distributions with symmetric innovation, the distributed PCA is ""unbiased"". We derive the rate of convergence for distributed PCA estimators, which depends explicitly on the effective rank of covariance, eigen-gap, and the number of machines. We show that when the number of machines is not unreasonably large, the distributed PCA performs as well as the whole sample PCA, even without full access of whole data. The theoretical results are verified by an extensive simulation study. We also extend our analysis to the heterogeneous case where the population covariance matrices are different across local machines but share similar top eigen-structures.",princip compon analysi pca fundament statist machin learn extract latent princip factor contribut variat data data store across multipl machin howev communic cost prohibit comput pca central locat distribut algorithm pca thus need paper propos studi distribut pca algorithm node machin comput top eigenvector transmit central server central server aggreg inform node machin conduct pca base aggreg inform investig bias varianc result distribut estim top eigenvector particular show distribut symmetr innov distribut pca unbias deriv rate converg distribut pca estim depend explicit effect rank covari eigen gap number machin show number machin unreason larg distribut pca perform well whole sampl pca even without full access whole data theoret result verifi extens simul studi also extend analysi heterogen case popul covari matric differ across local machin share similar top eigen structur,"['Jianqing Fan', 'Dong Wang', 'Kaizheng Wang', 'Ziwei Zhu']","['stat.CO', 'math.ST', 'stat.TH']",False,False,True,False,False,False
1071,2017-03-28T14:03:38Z,2017-02-18T00:04:25Z,http://arxiv.org/abs/1702.05546v1,http://arxiv.org/pdf/1702.05546v1,A Sequential Scheme for Large Scale Bayesian Multiple Testing,sequenti scheme larg scale bayesian multipl test,"The problem of large scale multiple testing arises in many contexts, including testing for pairwise interaction among large numbers of neurons. With advances in technologies, it has become common to record from hundreds of neurons simultaneously, and this number is growing quickly, so that the number of pairwise tests can be very large. It is important to control the rate at which false positives occur. In addition, there is sometimes information that affects the probability of a positive result for any given pair. In the case of neurons, they are more likely to have correlated activity when they are close together, and when they respond similarly to various stimuli. Recently a method was developed to control false positives when covariate information, such as distances between pairs of neurons, is available. This method, however, relies on computationally-intensive Markov Chain Monte Carlo (MCMC). Here we develop an alternative, based on Sequential Monte Carlo, which scales well with the size of the dataset. This scheme considers data items sequentially, with relevant probabilities being updated at each step. Simulation experiments demonstrate that the proposed algorithm delivers results as accurately as the previous MCMC method with only a single pass through the data. We illustrate the method by using it to analyze neural recordings from extrastriate cortex in a macaque monkey. The scripts that implement the proposed algorithm with a synthetic dataset are available online at: https://github.com/robinlau1981/SMC_Multi_Testing.",problem larg scale multipl test aris mani context includ test pairwis interact among larg number neuron advanc technolog becom common record hundr neuron simultan number grow quick number pairwis test veri larg import control rate fals posit occur addit sometim inform affect probabl posit result ani given pair case neuron like correl activ close togeth respond similar various stimuli recent method develop control fals posit covari inform distanc pair neuron avail method howev reli comput intens markov chain mont carlo mcmc develop altern base sequenti mont carlo scale well size dataset scheme consid data item sequenti relev probabl updat step simul experi demonstr propos algorithm deliv result accur previous mcmc method onli singl pass data illustr method use analyz neural record extrastri cortex macaqu monkey script implement propos algorithm synthet dataset avail onlin https github com robinlau smc multi test,"['Bin Liu', 'Giuseppe Vinci', 'Adam C. Snyder', 'Matthew A. Smith', 'Robert E. Kass']",['stat.CO'],False,False,True,False,False,False
1074,2017-03-28T14:03:38Z,2017-02-15T11:52:14Z,http://arxiv.org/abs/1702.04561v1,http://arxiv.org/pdf/1702.04561v1,Probing for sparse and fast variable selection with model-based boosting,probe spars fast variabl select model base boost,"We present a new variable selection method based on model-based gradient boosting and randomly permuted variables. Model-based boosting is a tool to fit a statistical model while performing variable selection at the same time. A drawback of the fitting lies in the need of multiple model fits on slightly altered data (e.g. cross-validation or bootstrap) to find the optimal number of boosting iterations and prevent overfitting. In our proposed approach, we augment the data set with randomly permuted versions of the true variables, so called shadow variables, and stop the step-wise fitting as soon as such a variable would be added to the model. This allows variable selection in a single fit of the model without requiring further parameter tuning. We show that our probing approach can compete with state-of-the-art selection methods like stability selection in a high-dimensional classification benchmark and apply it on gene expression data for the estimation of riboflavin production of Bacillus subtilis.",present new variabl select method base model base gradient boost random permut variabl model base boost tool fit statist model perform variabl select time drawback fit lie need multipl model fit slight alter data cross valid bootstrap find optim number boost iter prevent overfit propos approach augment data set random permut version true variabl call shadow variabl stop step wise fit soon variabl would ad model allow variabl select singl fit model without requir paramet tune show probe approach compet state art select method like stabil select high dimension classif benchmark appli gene express data estim riboflavin product bacillus subtili,"['Janek Thomas', 'Tobias Hepp', 'Andreas Mayr', 'Bernd Bischl']","['stat.ML', 'stat.CO']",False,False,True,False,False,True
1082,2017-03-28T14:03:42Z,2017-02-09T00:11:27Z,http://arxiv.org/abs/1702.02658v1,http://arxiv.org/pdf/1702.02658v1,Estimating the number of clusters using cross-validation,estim number cluster use cross valid,"Many clustering methods, including k-means, require the user to specify the number of clusters as an input parameter. A variety of methods have been devised to choose the number of clusters automatically, but they often rely on strong modeling assumptions. This paper proposes a data-driven approach to estimate the number of clusters based on a novel form of cross-validation. The proposed method differs from ordinary cross-validation, because clustering is fundamentally an unsupervised learning problem. Simulation and real data analysis results show that the proposed method outperforms existing methods, especially in high-dimensional settings with heterogeneous or heavy-tailed noise. In a yeast cell cycle dataset, the proposed method finds a parsimonious clustering with interpretable gene groupings.",mani cluster method includ mean requir user specifi number cluster input paramet varieti method devis choos number cluster automat often reli strong model assumpt paper propos data driven approach estim number cluster base novel form cross valid propos method differ ordinari cross valid becaus cluster fundament unsupervis learn problem simul real data analysi result show propos method outperform exist method especi high dimension set heterogen heavi tail nois yeast cell cycl dataset propos method find parsimoni cluster interpret gene group,"['Wei Fu', 'Patrick O. Perry']","['stat.ME', 'stat.CO']",False,False,True,False,False,False
1084,2017-03-28T14:03:42Z,2017-03-22T14:35:39Z,http://arxiv.org/abs/1702.01418v2,http://arxiv.org/pdf/1702.01418v2,Choosing the number of groups in a latent stochastic block model for   dynamic networks,choos number group latent stochast block model dynam network,"Latent stochastic block models are flexible statistical models that are widely used in social network analysis. In recent years, efforts have been made to extend these models to temporal dynamic networks, whereby the connections between nodes are observed at a number of different times. In this paper we extend the original stochastic block model by using a Markovian property to describe the evolution of nodes' cluster memberships over time. We recast the problem of clustering the nodes of the network into a model-based context, and show that the integrated completed likelihood can be evaluated analytically for a number of likelihood models. Then, we propose a scalable greedy algorithm to maximise this quantity, thereby estimating both the optimal partition and the ideal number of groups in a single inferential framework. Finally we propose applications of our methodology to both real and artificial datasets.",latent stochast block model flexibl statist model wide use social network analysi recent year effort made extend model tempor dynam network wherebi connect node observ number differ time paper extend origin stochast block model use markovian properti describ evolut node cluster membership time recast problem cluster node network model base context show integr complet likelihood evalu analyt number likelihood model propos scalabl greedi algorithm maximis quantiti therebi estim optim partit ideal number group singl inferenti framework final propos applic methodolog real artifici dataset,"['Riccardo Rastelli', 'Pierre Latouche', 'Nial Friel']","['stat.ME', 'stat.CO']",False,False,True,False,False,False
1085,2017-03-28T14:03:42Z,2017-02-05T04:55:14Z,http://arxiv.org/abs/1702.01373v1,http://arxiv.org/pdf/1702.01373v1,Exact heat kernel on a hypersphere and its applications in kernel SVM,exact heat kernel hyperspher applic kernel svm,"Many contemporary statistical learning methods assume a Euclidean feature space, however, the ""curse of dimensionality"" associated with high feature dimensions is particularly severe for the Euclidean distance. This paper presents a method for defining similarity based on hyperspherical geometry and shows that it often improves the performance of support vector machine compared to other competing similarity measures. Specifically, the idea of using heat diffusion on a hypersphere to measure similarity has been proposed and tested by \citet{Lafferty:2015uy}, demonstrating promising results based on an approximate heat kernel, however, the exact hyperspherical heat kernel hitherto remains unknown. In this paper, we derive an exact form of the heat kernel on a unit hypersphere in terms of a uniformly and absolutely convergent series in high-dimensional angular momentum eigenmodes. Being a natural measure of similarity between sample points dwelling on a hypersphere, the exact kernel often shows superior performance in kernel SVM classifications applied to text mining, tumor somatic mutation imputation, and stock market analysis. The improvement in classification accuracy compared with kernels based on Euclidean geometry may arise from ameliorating the curse of dimensionality on compact manifolds.",mani contemporari statist learn method assum euclidean featur space howev curs dimension associ high featur dimens particular sever euclidean distanc paper present method defin similar base hyperspher geometri show often improv perform support vector machin compar compet similar measur specif idea use heat diffus hyperspher measur similar propos test citet lafferti uy demonstr promis result base approxim heat kernel howev exact hyperspher heat kernel hitherto remain unknown paper deriv exact form heat kernel unit hyperspher term uniform absolut converg seri high dimension angular momentum eigenmod natur measur similar sampl point dwell hyperspher exact kernel often show superior perform kernel svm classif appli text mine tumor somat mutat imput stock market analysi improv classif accuraci compar kernel base euclidean geometri may aris amelior curs dimension compact manifold,"['Chenchao Zhao', 'Jun S. Song']","['stat.ML', 'q-bio.QM', 'stat.CO']",False,False,True,False,False,False
1090,2017-03-28T14:03:46Z,2017-02-01T19:44:14Z,http://arxiv.org/abs/1702.00434v1,http://arxiv.org/pdf/1702.00434v1,Applying Nearest Neighbor Gaussian Processes to Massive Spatial Data   Sets: Forest Canopy Height Prediction Across Tanana Valley Alaska,appli nearest neighbor gaussian process massiv spatial data set forest canopi height predict across tanana valley alaska,"Light detection and ranging (LiDAR) data provide critical information on the three-dimensional structure of forests. However, collecting wall-to-wall LiDAR data at regional and global scales is cost prohibitive. As a result, studies employing LiDAR data from airborne platforms typically collect data via strip sampling; leaving large swaths of the forest domain unmeasured by the instrument. Frameworks to accommodate incomplete coverage information from LiDAR instruments are essential to advance our understanding of forest structure and begin effectively monitoring forest resource dynamics over time. Here, we define and assess several spatial regression models capable of delivering complete coverage forest canopy height prediction maps with associated uncertainty estimates using sparsely sampled LiDAR data. Despite the sparsity of the LiDAR data considered, the number of observations is large, e.g., n=5x10^6. Computational hurdles associated with developing the desired data products is overcome by using highly scalable hierarchical Nearest Neighbor Gaussian Process (NNGP) models. We outline new Markov chain Monte Carlo (MCMC) algorithms that provide improved convergence and run time over existing algorithms. We also propose a MCMC free hybrid implementation of NNGP. We assess the computational and inferential benefits of these alternate NNGP specifications using simulated data sets and LiDAR data collected over the US Forest Service Tanana Inventory Unit (TIU) in a remote portion of Interior Alaska. The resulting data product is the first statistically robust map of forest canopy for the TIU.",light detect rang lidar data provid critic inform three dimension structur forest howev collect wall wall lidar data region global scale cost prohibit result studi employ lidar data airborn platform typic collect data via strip sampl leav larg swath forest domain unmeasur instrument framework accommod incomplet coverag inform lidar instrument essenti advanc understand forest structur begin effect monitor forest resourc dynam time defin assess sever spatial regress model capabl deliv complet coverag forest canopi height predict map associ uncertainti estim use spars sampl lidar data despit sparsiti lidar data consid number observ larg comput hurdl associ develop desir data product overcom use high scalabl hierarch nearest neighbor gaussian process nngp model outlin new markov chain mont carlo mcmc algorithm provid improv converg run time exist algorithm also propos mcmc free hybrid implement nngp assess comput inferenti benefit altern nngp specif use simul data set lidar data collect us forest servic tanana inventori unit tiu remot portion interior alaska result data product first statist robust map forest canopi tiu,"['Andrew O. Finley', 'Abhirup Datta', 'Bruce C. Cook', 'Douglas C. Morton', 'Hans E. Andersen', 'Sudipto Banerjee']","['stat.CO', 'stat.AP']",False,False,True,False,False,False
1094,2017-03-28T14:03:46Z,2017-01-28T16:31:25Z,http://arxiv.org/abs/1701.08299v1,http://arxiv.org/pdf/1701.08299v1,Computing the aggregate loss distribution based on numerical inversion   of the compound empirical characteristic function of frequency and severity,comput aggreg loss distribut base numer invers compound empir characterist function frequenc sever,"A non-parametric method for evaluation of the aggregate loss distribution (ALD) by combining and numerically inverting the empirical characteristic functions (CFs) is presented and illustrated. This approach to evaluate ALD is based on purely non-parametric considerations, i.e., based on the empirical CFs of frequency and severity of the claims in the actuarial risk applications. This approach can be, however, naturally generalized to a more complex semi-parametric modeling approach, e.g., by incorporating the generalized Pareto distribution fit of the severity distribution heavy tails, and/or by considering the weighted mixture of the parametric CFs (used to model the expert knowledge) and the empirical CFs (used to incorporate the knowledge based on the historical data - internal and/or external). Here we present a simple and yet efficient method and algorithms for numerical inversion of the CF, suitable for evaluation of the ALDs and the associated measures of interest important for applications, as, e.g., the value at risk (VaR). The presented approach is based on combination of the Gil-Pelaez inversion formulae for deriving the probability distribution (PDF and CDF) from the compound (empirical) CF and the trapezoidal rule used for numerical integration. The applicability of the suggested approach is illustrated by analysis of a well know insurance dataset, the Danish fire loss data.",non parametr method evalu aggreg loss distribut ald combin numer invert empir characterist function cfs present illustr approach evalu ald base pure non parametr consider base empir cfs frequenc sever claim actuari risk applic approach howev natur general complex semi parametr model approach incorpor general pareto distribut fit sever distribut heavi tail consid weight mixtur parametr cfs use model expert knowledg empir cfs use incorpor knowledg base histor data intern extern present simpl yet effici method algorithm numer invers cf suitabl evalu ald associ measur interest import applic valu risk var present approach base combin gil pelaez invers formula deriv probabl distribut pdf cdf compound empir cf trapezoid rule use numer integr applic suggest approach illustr analysi well know insur dataset danish fire loss data,"['Viktor Witkovsky', 'Gejza Wimmer', 'Tomas Duby']","['stat.CO', 'q-fin.RM', 'stat.AP', '91B30, 62G32']",False,False,True,False,False,False
1098,2017-03-28T14:03:46Z,2017-01-25T21:43:03Z,http://arxiv.org/abs/1701.07496v1,http://arxiv.org/pdf/1701.07496v1,Phylogenetic Factor Analysis,phylogenet factor analysi,"Phylogenetic comparative methods explore the relationships between quantitative traits adjusting for shared evolutionary history. This adjustment often occurs through a Brownian diffusion process along the branches of the phylogeny that generates model residuals or the traits themselves. For high-dimensional traits, inferring all pair-wise correlations within the multivariate diffusion is limiting. To circumvent this problem, we propose phylogenetic factor analysis (PFA) that assumes a small unknown number of independent evolutionary factors arise along the phylogeny and these factors generate clusters of dependent traits. Set in a Bayesian framework, PFA provides measures of uncertainty on the factor number and groupings, combines both continuous and discrete traits, integrates over missing measurements and incorporates phylogenetic uncertainty with the help of molecular sequences. We develop Gibbs samplers based on dynamic programming to estimate the PFA posterior distribution, over three-fold faster than for multivariate diffusion and a further order-of-magnitude more efficiently in the presence of latent traits. We further propose a novel marginal likelihood estimator for previously impractical models with discrete data and find that PFA also provides a better fit than multivariate diffusion in evolutionary questions in columbine flower development, placental reproduction transitions and triggerfish fin morphometry.",phylogenet compar method explor relationship quantit trait adjust share evolutionari histori adjust often occur brownian diffus process along branch phylogeni generat model residu trait themselv high dimension trait infer pair wise correl within multivari diffus limit circumv problem propos phylogenet factor analysi pfa assum small unknown number independ evolutionari factor aris along phylogeni factor generat cluster depend trait set bayesian framework pfa provid measur uncertainti factor number group combin continu discret trait integr miss measur incorpor phylogenet uncertainti help molecular sequenc develop gibb sampler base dynam program estim pfa posterior distribut three fold faster multivari diffus order magnitud effici presenc latent trait propos novel margin likelihood estim previous impract model discret data find pfa also provid better fit multivari diffus evolutionari question columbin flower develop placent reproduct transit triggerfish fin morphometri,"['Max R. Tolkoff', 'Michael L. Alfaro', 'Guy Baele', 'Philippe Lemey', 'Marc A. Suchard']","['stat.ME', 'stat.AP', 'stat.CO']",False,False,True,False,False,False
1100,2017-03-28T14:03:51Z,2017-03-27T14:52:09Z,http://arxiv.org/abs/1703.09124v1,http://arxiv.org/pdf/1703.09124v1,Multi-sensor Transmission Management for Remote State Estimation under   Coordination,multi sensor transmiss manag remot state estim coordin,"This paper considers the remote state estimation in a cyber-physical system (CPS) using multiple sensors. The measurements of each sensor are transmitted to a remote estimator over a shared channel, where simultaneous transmissions from other sensors are regarded as interference signals. In such a competitive environment, each sensor needs to choose its transmission power for sending data packets taking into account of other sensors' behavior. To model this interactive decision-making process among the sensors, we introduce a multi-player non-cooperative game framework. To overcome the inefficiency arising from the Nash equilibrium (NE) solution, we propose a correlation policy, along with the notion of correlation equilibrium (CE). An analytical comparison of the game value between the NE and the CE is provided, with/without the power expenditure constraints for each sensor. Also, numerical simulations demonstrate the comparison results.",paper consid remot state estim cyber physic system cps use multipl sensor measur sensor transmit remot estim share channel simultan transmiss sensor regard interfer signal competit environ sensor need choos transmiss power send data packet take account sensor behavior model interact decis make process among sensor introduc multi player non cooper game framework overcom ineffici aris nash equilibrium ne solut propos correl polici along notion correl equilibrium ce analyt comparison game valu ne ce provid without power expenditur constraint sensor also numer simul demonstr comparison result,"['Kemi Ding', 'Yuzhe Li', 'Subhrakanti Dey', 'Ling Shi']","['stat.ME', 'cs.IT', 'math.IT']",False,False,True,False,False,True
1104,2017-03-28T14:03:51Z,2017-03-26T22:49:31Z,http://arxiv.org/abs/1703.08882v1,http://arxiv.org/pdf/1703.08882v1,A Mixture of Matrix Variate Skew-t Distributions,mixtur matrix variat skew distribut,"Clustering is the process of finding underlying group structures in data. Although model-based clustering is firmly established in the multivariate case, there is relative paucity for matrix variate distributions, and there are even fewer examples using matrix variate skew distributions. In this paper, we look at parameter estimation for a finite mixture of matrix variate skew-t distributions in the context of model-based clustering. Simulated data is used for illustrative purposes.",cluster process find group structur data although model base cluster firm establish multivari case relat pauciti matrix variat distribut even fewer exampl use matrix variat skew distribut paper look paramet estim finit mixtur matrix variat skew distribut context model base cluster simul data use illustr purpos,"['Michael P. B. Gallaugher', 'Paul D. McNicholas']","['stat.ME', 'stat.CO']",False,False,True,False,False,True
1109,2017-03-28T14:03:51Z,2017-03-24T20:59:52Z,http://arxiv.org/abs/1703.08596v1,http://arxiv.org/pdf/1703.08596v1,The Inner Structure of Time-Dependent Signals,inner structur time depend signal,"This paper shows how a time series of measurements of an evolving system can be processed to create an inner time series that is unaffected by any instantaneous invertible, possibly nonlinear transformation of the measurements. An inner time series contains information that does not depend on the nature of the sensors, which the observer chose to monitor the system. Instead, it encodes information that is intrinsic to the evolution of the observed system. Because of its sensor-independence, an inner time series may produce fewer false negatives when it is used to detect events in the presence of sensor drift. Furthermore, if the observed physical system is comprised of non-interacting subsystems, its inner time series is separable; i.e., it consists of a collection of time series, each one being the inner time series of an isolated subsystem. Because of this property, an inner time series can be used to detect a specific behavior of one of the independent subsystems without using blind source separation to disentangle that subsystem from the others. The method is illustrated by applying it to: 1) an analytic example; 2) the audio waveform of one speaker; 3) video images from a moving camera; 4) mixtures of audio waveforms of two speakers.",paper show time seri measur evolv system process creat inner time seri unaffect ani instantan invert possibl nonlinear transform measur inner time seri contain inform doe depend natur sensor observ chose monitor system instead encod inform intrins evolut observ system becaus sensor independ inner time seri may produc fewer fals negat use detect event presenc sensor drift furthermor observ physic system compris non interact subsystem inner time seri separ consist collect time seri one inner time seri isol subsystem becaus properti inner time seri use detect specif behavior one independ subsystem without use blind sourc separ disentangl subsystem method illustr appli analyt exampl audio waveform one speaker video imag move camera mixtur audio waveform two speaker,['David N. Levin'],"['stat.ME', 'cs.SD', 'math.ST', 'stat.TH']",False,False,True,False,False,False
1112,2017-03-28T14:03:55Z,2017-03-24T16:08:21Z,http://arxiv.org/abs/1703.08487v1,http://arxiv.org/pdf/1703.08487v1,Multiscale Granger causality,multiscal granger causal,"In the study of complex physical and biological systems represented by multivariate stochastic processes, an issue of great relevance is the description of the system dynamics spanning multiple temporal scales. While methods to assess the dynamic complexity of individual processes at different time scales are well-established, the multiscale evaluation of directed interactions between processes is complicated by theoretical and practical issues such as filtering and downsampling. Here we extend the very popular measure of Granger causality (GC), a prominent tool for assessing directed lagged interactions between joint processes, to quantify information transfer across multiple time scales. We show that the multiscale processing of a vector autoregressive (AR) process introduces a moving average (MA) component, and describe how to represent the resulting ARMA process using state space (SS) models and to combine the SS model parameters for computing exact GC values at arbitrarily large time scales. We exploit the theoretical formulation to identify peculiar features of multiscale GC in basic AR processes, and demonstrate with numerical simulations the much larger estimation accuracy of the SS approach compared with pure AR modeling of filtered and downsampled data. The improved computational reliability is exploited to disclose meaningful multiscale patterns of information transfer between global temperature and carbon dioxide concentration time series, both in paleoclimate and in recent years.",studi complex physic biolog system repres multivari stochast process issu great relev descript system dynam span multipl tempor scale method assess dynam complex individu process differ time scale well establish multiscal evalu direct interact process complic theoret practic issu filter downsampl extend veri popular measur granger causal gc promin tool assess direct lag interact joint process quantifi inform transfer across multipl time scale show multiscal process vector autoregress ar process introduc move averag compon describ repres result arma process use state space ss model combin ss model paramet comput exact gc valu arbitrarili larg time scale exploit theoret formul identifi peculiar featur multiscal gc basic ar process demonstr numer simul much larger estim accuraci ss approach compar pure ar model filter downsampl data improv comput reliabl exploit disclos meaning multiscal pattern inform transfer global temperatur carbon dioxid concentr time seri paleoclim recent year,"['Luca Faes', 'Giandomenico Nollo', 'Sebastiano Stramaglia', 'Daniele Marinazzo']","['stat.ME', 'math.ST', 'stat.AP', 'stat.TH']",False,False,True,False,False,False
1113,2017-03-28T14:03:55Z,2017-03-23T18:57:39Z,http://arxiv.org/abs/1703.08202v1,http://arxiv.org/pdf/1703.08202v1,A recursive point process model for infectious diseases,recurs point process model infecti diseas,"We introduce a new type of point process model to describe the incidence of contagious diseases. The model is a variant of the Hawkes self-exciting process and exhibits similar clustering but without the restriction that the component describing the contagion must remain static over time. Instead, our proposed model prescribes that the degree of contagion (or productivity) changes as a function of the conditional intensity; of particular interest is the special case where the productivity is inversely proportional to the conditional intensity. The model incorporates the premise that when the disease occurs at very low frequency in the population, such as in the primary stages of an outbreak, then anyone with the disease is likely to have a high rate of transmission to others, whereas when the disease is prevalent in the population, then the transmission rate is lower due to human mitigation actions and prevention measures and a relatively high percentage of previous exposure in the total population. The model is said to be recursive, in the sense that the conditional intensity at any particular time depends on the productivity associated with previous points, and this productivity in turn depends on the conditional intensity at those points. Some basic properties of the model are derived, estimation and simulation are discussed, and the recursive model is shown to fit well to historic data on measles in Los Angeles, California, a relevant example given the 2017 outbreak of this disease in the same region.",introduc new type point process model describ incid contagi diseas model variant hawk self excit process exhibit similar cluster without restrict compon describ contagion must remain static time instead propos model prescrib degre contagion product chang function condit intens particular interest special case product invers proport condit intens model incorpor premis diseas occur veri low frequenc popul primari stage outbreak anyon diseas like high rate transmiss wherea diseas preval popul transmiss rate lower due human mitig action prevent measur relat high percentag previous exposur total popul model said recurs sens condit intens ani particular time depend product associ previous point product turn depend condit intens point basic properti model deriv estim simul discuss recurs model shown fit well histor data measl los angel california relev exampl given outbreak diseas region,"['Frederic Schoenberg', 'Marc Hoffmann', 'Ryan Harrigan']",['stat.ME'],False,False,True,False,False,False
1114,2017-03-28T14:03:55Z,2017-03-23T14:47:29Z,http://arxiv.org/abs/1703.08090v1,http://arxiv.org/pdf/1703.08090v1,"Flexible multi-state models for interval-censored data: specification,   estimation, and an application to ageing research",flexibl multi state model interv censor data specif estim applic age research,"Continuous-time multi-state survival models can be used to describe health-related processes over time. In the presence of interval-censored times for transitions between the living states, the likelihood is constructed using transition probabilities. Models can be specified using parametric or semi-parametric shapes for the hazards. Semi-parametric hazards can be fitted using $P$-splines and penalised maximum likelihood estimation. This paper presents a method to estimate flexible multi-state models which allows for parametric and semi-parametric hazard specifications. The estimation is based on a scoring algorithm. The method is illustrated with data from the English Longitudinal Study of Ageing.",continu time multi state surviv model use describ health relat process time presenc interv censor time transit live state likelihood construct use transit probabl model specifi use parametr semi parametr shape hazard semi parametr hazard fit use spline penalis maximum likelihood estim paper present method estim flexibl multi state model allow parametr semi parametr hazard specif estim base score algorithm method illustr data english longitudin studi age,"['Robson J. M. Machado', 'Ardo van den Hout']",['stat.ME'],False,False,True,False,False,False
1117,2017-03-28T14:03:55Z,2017-03-23T01:30:17Z,http://arxiv.org/abs/1703.07904v1,http://arxiv.org/pdf/1703.07904v1,Cross-Validation with Confidence,cross valid confid,"Cross-validation is one of the most popular model selection methods in statistics and machine learning. Despite its wide applicability, traditional cross-validation methods tend to select overfitting models, unless the ratio between the training and testing sample sizes is much smaller than conventional choices. We argue that such an overfitting tendency of cross-validation is due to the ignorance of the uncertainty in the testing sample. Starting from this observation, we develop a new, statistically principled inference tool based on cross-validation that takes into account the uncertainty in the testing sample. This new method outputs a small set of highly competitive candidate models containing the best one with guaranteed probability. As a consequence, our method can achieve consistent variable selection in a classical linear regression setting, for which existing cross-validation methods require unconventional split ratios. We demonstrate the performance of the proposed method in several simulated and real data examples.",cross valid one popular model select method statist machin learn despit wide applic tradit cross valid method tend select overfit model unless ratio train test sampl size much smaller convent choic argu overfit tendenc cross valid due ignor uncertainti test sampl start observ develop new statist principl infer tool base cross valid take account uncertainti test sampl new method output small set high competit candid model contain best one guarante probabl consequ method achiev consist variabl select classic linear regress set exist cross valid method requir unconvent split ratio demonstr perform propos method sever simul real data exampl,['Jing Lei'],"['stat.ME', 'stat.ML']",False,False,True,False,False,True
1120,2017-03-28T14:03:59Z,2017-03-22T16:56:48Z,http://arxiv.org/abs/1703.07747v1,http://arxiv.org/pdf/1703.07747v1,MIMIX: a Bayesian Mixed-Effects Model for Microbiome Data from Designed   Experiments,mimix bayesian mix effect model microbiom data design experi,"Recent advances in bioinformatics have made high-throughput microbiome data widely available, and new statistical tools are required to maximize the information gained from these data. For example, analysis of high-dimensional microbiome data from designed experiments remains an open area in microbiome research. Contemporary analyses work on metrics that summarize collective properties of the microbiome, but such reductions preclude inference on the fine-scale effects of environmental stimuli on individual microbial taxa. Other approaches model the proportions or counts of individual taxa as response variables in mixed models, but these methods fail to account for complex correlation patterns among microbial communities. In this paper, we propose a novel Bayesian mixed-effects model that exploits cross-taxa correlations within the microbiome, a model we call MIMIX (MIcrobiome MIXed model). MIMIX offers global tests for treatment effects, local tests and estimation of treatment effects on individual taxa, quantification of the relative contribution from heterogeneous sources to microbiome variability, and identification of latent ecological subcommunities in the microbiome. MIMIX is tailored to large microbiome experiments using a combination of Bayesian factor analysis to efficiently represent dependence between taxa and Bayesian variable selection methods to achieve sparsity. We demonstrate the model using a simulation experiment and on a 2x2 factorial experiment of the effects of nutrient supplement and herbivore exclusion on the foliar fungal microbiome of $\textit{Andropogon gerardii}$, a perennial bunchgrass, as part of the global Nutrient Network research initiative.",recent advanc bioinformat made high throughput microbiom data wide avail new statist tool requir maxim inform gain data exampl analysi high dimension microbiom data design experi remain open area microbiom research contemporari analys work metric summar collect properti microbiom reduct preclud infer fine scale effect environment stimuli individu microbi taxa approach model proport count individu taxa respons variabl mix model method fail account complex correl pattern among microbi communiti paper propos novel bayesian mix effect model exploit cross taxa correl within microbiom model call mimix microbiom mix model mimix offer global test treatment effect local test estim treatment effect individu taxa quantif relat contribut heterogen sourc microbiom variabl identif latent ecolog subcommun microbiom mimix tailor larg microbiom experi use combin bayesian factor analysi effici repres depend taxa bayesian variabl select method achiev sparsiti demonstr model use simul experi factori experi effect nutrient supplement herbivor exclus foliar fungal microbiom textit andropogon gerardii perenni bunchgrass part global nutrient network research initi,"['Neal S. Grantham', 'Brian J. Reich', 'Elizabeth T. Borer', 'Kevin Gross']",['stat.ME'],False,False,True,False,False,False
1123,2017-03-28T14:03:59Z,2017-03-21T14:41:52Z,http://arxiv.org/abs/1703.07246v1,http://arxiv.org/pdf/1703.07246v1,Sufficient Dimension Reduction via Random-Partitions for Large-p-Small-n   Problem,suffici dimens reduct via random partit larg small problem,"Sufficient dimension reduction (SDR) is continuing an active research field nowadays for high dimensional data. It aims to estimate the central subspace (CS) without making distributional assumption. To overcome the large-$p$-small-$n$ problem we propose a new approach for SDR. Our method combines the following ideas for high dimensional data analysis: (1) Randomly partition the covariates into subsets and use distance correlation (DC) to construct a sketch of envelope subspace with low dimension. (2) Obtain a sketch of the CS by applying conventional SDR method within the constructed envelope subspace. (3) Repeat the above two steps for a few times and integrate these multiple sketches to form the final estimate of the CS. We name the proposed SDR procedure ""integrated random-partition SDR (iRP-SDR)"". Comparing with existing methods, iRP-SDR is less affected by the selection of tuning parameters. Moreover, the estimation procedure of iRP-SDR does not involve the determination of the structural dimension until at the last stage, which makes the method more robust in a high-dimensional setting. Asymptotic properties of iRP-SDR are also established. The advantageous performance of the proposed method is demonstrated via simulation studies and the EEG data analysis.",suffici dimens reduct sdr continu activ research field nowaday high dimension data aim estim central subspac cs without make distribut assumpt overcom larg small problem propos new approach sdr method combin follow idea high dimension data analysi random partit covari subset use distanc correl dc construct sketch envelop subspac low dimens obtain sketch cs appli convent sdr method within construct envelop subspac repeat abov two step time integr multipl sketch form final estim cs name propos sdr procedur integr random partit sdr irp sdr compar exist method irp sdr less affect select tune paramet moreov estim procedur irp sdr doe involv determin structur dimens last stage make method robust high dimension set asymptot properti irp sdr also establish advantag perform propos method demonstr via simul studi eeg data analysi,"['Hung Hung', 'Su-Yun Huang']",['stat.ME'],False,False,True,False,False,True
1124,2017-03-28T14:03:59Z,2017-03-21T13:02:35Z,http://arxiv.org/abs/1703.07198v1,http://arxiv.org/pdf/1703.07198v1,Overcoming model simplifications when quantifying predictive uncertainty,overcom model simplif quantifi predict uncertainti,"It is generally accepted that all models are wrong -- the difficulty is determining which are useful. Here, a useful model is considered as one that is capable of combining data and expert knowledge, through an inversion or calibration process, to adequately characterize the uncertainty in predictions of interest. This paper derives conditions that specify which simplified models are useful and how they should be calibrated. To start, the notion of an optimal simplification is defined. This relates the model simplifications to the nature of the data and predictions, and determines when a standard probabilistic calibration scheme is capable of accurately characterizing uncertainty. Furthermore, two additional conditions are defined for suboptimal models that determine when the simplifications can be safely ignored. The first allows a suboptimally simplified model to be used in a way that replicates the performance of an optimal model. This is achieved through the judicial selection of a prior term for the calibration process that explicitly includes the nature of the data, predictions and modelling simplifications. The second considers the dependency structure between the predictions and the available data to gain insights into when the simplifications can be overcome by using the right calibration data. Furthermore, the derived conditions are related to the commonly used calibration schemes based on Tikhonov and subspace regularization. To allow concrete insights to be obtained, the analysis is performed under a linear expansion of the model equations and where the predictive uncertainty is characterized via second order moments only.",general accept model wrong difficulti determin use use model consid one capabl combin data expert knowledg invers calibr process adequ character uncertainti predict interest paper deriv condit specifi simplifi model use calibr start notion optim simplif defin relat model simplif natur data predict determin standard probabilist calibr scheme capabl accur character uncertainti furthermor two addit condit defin suboptim model determin simplif safe ignor first allow suboptim simplifi model use way replic perform optim model achiev judici select prior term calibr process explicit includ natur data predict model simplif second consid depend structur predict avail data gain insight simplif overcom use right calibr data furthermor deriv condit relat common use calibr scheme base tikhonov subspac regular allow concret insight obtain analysi perform linear expans model equat predict uncertainti character via second order moment onli,"['George M. Mathews', 'John Vial']","['stat.ML', 'math.PR', 'physics.comp-ph', 'physics.geo-ph', 'stat.ME', '62F15, 62C10, 68U05, 93E12, 93B11, 62P12']",False,False,True,False,False,False
1126,2017-03-28T14:03:59Z,2017-03-21T00:11:45Z,http://arxiv.org/abs/1703.07009v1,http://arxiv.org/pdf/1703.07009v1,New reconstruction and data processing methods for regression and   interpolation analysis of multidimensional big data,new reconstruct data process method regress interpol analysi multidimension big data,"The problems of computational data processing involving regression, interpolation, reconstruction and imputation for multidimensional big datasets are becoming more important these days, because of the availability of data and their widely spread usage in business, technological, scientific and other applications. The existing methods often have limitations, which either do not allow, or make it difficult to accomplish many data processing tasks. The problems usually relate to algorithm accuracy, applicability, performance (computational and algorithmic), demands for computational resources, both in terms of power and memory, and difficulty working with high dimensions. Here, we propose a new concept and introduce two methods, which use local area predictors (input data) for finding outcomes. One method uses the gradient based approach, while the second one employs an introduced family of smooth approximating functions. The new methods are free from many drawbacks of existing approaches. They are practical, have very wide range of applicability, provide high accuracy, excellent computational performance, fit for parallel computing, and very well suited for processing high dimension big data. The methods also provide multidimensional outcome, when needed. We present numerical examples of up to one hundred dimensions, and report in detail performance characteristics and various properties of new methods.",problem comput data process involv regress interpol reconstruct imput multidimension big dataset becom import day becaus avail data wide spread usag busi technolog scientif applic exist method often limit either allow make difficult accomplish mani data process task problem usual relat algorithm accuraci applic perform comput algorithm demand comput resourc term power memori difficulti work high dimens propos new concept introduc two method use local area predictor input data find outcom one method use gradient base approach second one employ introduc famili smooth approxim function new method free mani drawback exist approach practic veri wide rang applic provid high accuraci excel comput perform fit parallel comput veri well suit process high dimens big data method also provid multidimension outcom need present numer exampl one hundr dimens report detail perform characterist various properti new method,"['Yuri K. Shestopaloff', 'Alexander Y. Shestopaloff']",['stat.ME'],False,False,True,False,False,False
1131,2017-03-28T14:04:03Z,2017-03-20T02:01:02Z,http://arxiv.org/abs/1703.06558v1,http://arxiv.org/pdf/1703.06558v1,Using maximum entry-wise deviation to test the goodness-of-fit for   stochastic block models,use maximum entri wise deviat test good fit stochast block model,"The stochastic block model is widely used for detecting community structures in network data. How to test the goodness-of-fit of the model is one of the fundamental problems and has gained growing interests in recent years. In this paper, we propose a new goodness-of-fit test based on the maximum entry of the centered and re-scaled observed adjacency matrix for the stochastic block model in which the number of communities can be allowed to grow linearly with the number of nodes ignoring a logarithm factor. We demonstrate that its asymptotic null distribution is the Gumbel distribution. Our results can also be extended to the degree corrected block model. Numerical studies indicate the proposed method works well.",stochast block model wide use detect communiti structur network data test good fit model one fundament problem gain grow interest recent year paper propos new good fit test base maximum entri center scale observ adjac matrix stochast block model number communiti allow grow linear number node ignor logarithm factor demonstr asymptot null distribut gumbel distribut result also extend degre correct block model numer studi indic propos method work well,"['Jianwei Hu', 'Hong Qin', 'Ting Yan', 'Ji Zhu']",['stat.ME'],False,False,True,False,False,False
1134,2017-03-28T14:04:03Z,2017-03-19T01:29:12Z,http://arxiv.org/abs/1703.06379v1,http://arxiv.org/pdf/1703.06379v1,Penalized pairwise pseudo likelihood for variable selection with   nonignorable missing data,penal pairwis pseudo likelihood variabl select nonignor miss data,"The regularization approach for variable selection was well developed for a completely observed data set in the past two decades. In the presence of missing values, this approach needs to be tailored to different missing data mechanisms. In this paper, we focus on a flexible and generally applicable missing data mechanism, which contains both ignorable and nonignorable missing data mechanism assumptions. We show how the regularization approach for variable selection can be adapted to the situation under this missing data mechanism. The computational and theoretical properties for variable selection consistency are established. The proposed method is further illustrated by comprehensive simulation studies and real data analyses, for both low and high dimensional settings.",regular approach variabl select well develop complet observ data set past two decad presenc miss valu approach need tailor differ miss data mechan paper focus flexibl general applic miss data mechan contain ignor nonignor miss data mechan assumpt show regular approach variabl select adapt situat miss data mechan comput theoret properti variabl select consist establish propos method illustr comprehens simul studi real data analys low high dimension set,"['Jiwei Zhao', 'Yang Yang', 'Yang Ning']",['stat.ME'],False,False,True,False,False,True
1135,2017-03-28T14:04:03Z,2017-03-18T19:00:23Z,http://arxiv.org/abs/1703.06336v1,http://arxiv.org/pdf/1703.06336v1,Analysis of error control in large scale two-stage multiple hypothesis   testing,analysi error control larg scale two stage multipl hypothesi test,"When dealing with the problem of simultaneously testing a large number of null hypotheses, a natural testing strategy is to first reduce the number of tested hypotheses by some selection (screening or filtering) process, and then to simultaneously test the selected hypotheses. The main advantage of this strategy is to greatly reduce the severe effect of high dimensions. However, the first screening or selection stage must be properly accounted for in order to maintain some type of error control. In this paper, we will introduce a selection rule based on a selection statistic that is independent of the test statistic when the tested hypothesis is true. Combining this selection rule and the conventional Bonferroni procedure, we can develop a powerful and valid two-stage procedure. The introduced procedure has several nice properties: (i) it completely removes the selection effect; (ii) it reduces the multiplicity effect; (iii) it does not ""waste"" data while carrying out both selection and testing. Asymptotic power analysis and simulation studies illustrate that this proposed method can provide higher power compared to usual multiple testing methods while controlling the Type 1 error rate. Optimal selection thresholds are also derived based on our asymptotic analysis.",deal problem simultan test larg number null hypothes natur test strategi first reduc number test hypothes select screen filter process simultan test select hypothes main advantag strategi great reduc sever effect high dimens howev first screen select stage must proper account order maintain type error control paper introduc select rule base select statist independ test statist test hypothesi true combin select rule convent bonferroni procedur develop power valid two stage procedur introduc procedur sever nice properti complet remov select effect ii reduc multipl effect iii doe wast data carri select test asymptot power analysi simul studi illustr propos method provid higher power compar usual multipl test method control type error rate optim select threshold also deriv base asymptot analysi,"['Wenge Guo', 'Joseph P. Romano']","['stat.ME', 'math.ST', 'stat.TH', '62J15']",False,False,True,False,False,False
1144,2017-03-28T14:04:07Z,2017-03-17T14:34:01Z,http://arxiv.org/abs/1703.06031v1,http://arxiv.org/pdf/1703.06031v1,Modeling spatial processes with unknown extremal dependence class,model spatial process unknown extrem depend class,"Many environmental processes exhibit weakening spatial dependence as events become more extreme. Well-known limiting models, such as max-stable or generalized Pareto processes, cannot capture this, which can lead to a preference for models that exhibit a property known as asymptotic independence. However, weakening dependence does not automatically imply asymptotic independence, and whether the process is truly asymptotically (in)dependent is usually far from clear. The distinction is key as it can have a large impact upon extrapolation, i.e., the estimated probabilities of events more extreme than those observed. In this work, we present a single spatial model that is able to capture both dependence classes in a parsimonious manner, and with a smooth transition between the two cases. The model covers a wide range of possibilities from asymptotic independence through to complete dependence, and permits weakening dependence of extremes even under asymptotic dependence. Censored likelihood-based inference for the implied copula is feasible in moderate dimensions due to closed-form margins. The model is applied to oceanographic datasets with ambiguous true limiting dependence structure.",mani environment process exhibit weaken spatial depend event becom extrem well known limit model max stabl general pareto process cannot captur lead prefer model exhibit properti known asymptot independ howev weaken depend doe automat impli asymptot independ whether process truli asymptot depend usual far clear distinct key larg impact upon extrapol estim probabl event extrem observ work present singl spatial model abl captur depend class parsimoni manner smooth transit two case model cover wide rang possibl asymptot independ complet depend permit weaken depend extrem even asymptot depend censor likelihood base infer impli copula feasibl moder dimens due close form margin model appli oceanograph dataset ambigu true limit depend structur,"['Raphaël G. Huser', 'Jennifer L. Wadsworth']","['stat.ME', '62G32, 62M30']",False,False,True,False,False,False
1145,2017-03-28T14:04:07Z,2017-03-17T13:19:23Z,http://arxiv.org/abs/1703.06001v1,http://arxiv.org/pdf/1703.06001v1,The use of spatial information in entropy measures,use spatial inform entropi measur,"The concept of entropy, firstly introduced in information theory, rapidly became popular in many applied sciences via Shannon's formula to measure the degree of heterogeneity among observations. A rather recent research field aims at accounting for space in entropy measures, as a generalization when the spatial location of occurrences ought to be accounted for. The main limit of these developments is that all indices are computed conditional on a chosen distance. This work follows and extends the route for including spatial components in entropy measures. Starting from the probabilistic properties of Shannon's entropy for categorical variables, it investigates the characteristics of the quantities known as residual entropy and mutual information, when space is included as a second dimension. This way, the proposal of entropy measures based on univariate distributions is extended to the consideration of bivariate distributions, in a setting where the probabilistic meaning of all components is well defined. As a direct consequence, a spatial entropy measure satisfying the additivity property is obtained, as global residual entropy is a sum of partial entropies based on different distance classes. Moreover, the quantity known as mutual information measures the information brought by the inclusion of space, and also has the property of additivity. A thorough comparative study illustrates the superiority of the proposed indices.",concept entropi first introduc inform theori rapid becam popular mani appli scienc via shannon formula measur degre heterogen among observ rather recent research field aim account space entropi measur general spatial locat occurr ought account main limit develop indic comput condit chosen distanc work follow extend rout includ spatial compon entropi measur start probabilist properti shannon entropi categor variabl investig characterist quantiti known residu entropi mutual inform space includ second dimens way propos entropi measur base univari distribut extend consider bivari distribut set probabilist mean compon well defin direct consequ spatial entropi measur satisfi addit properti obtain global residu entropi sum partial entropi base differ distanc class moreov quantiti known mutual inform measur inform brought inclus space also properti addit thorough compar studi illustr superior propos indic,"['Linda Altieri', 'Daniela Cocchi', 'Giulia Roli']",['stat.ME'],False,False,True,False,False,False
1148,2017-03-28T14:04:07Z,2017-03-16T19:04:22Z,http://arxiv.org/abs/1703.05794v1,http://arxiv.org/pdf/1703.05794v1,Incorporating Covariates into Integrated Factor Analysis of Multi-View   Data,incorpor covari integr factor analysi multi view data,"In modern biomedical research, it is ubiquitous to have multiple data sets measured on the same set of samples from different views (i.e., multi-view data). For example, in genetic studies, multiple genomic data sets at different molecular levels or from different cell types are measured for a common set of individuals to investigate genetic regulation. Integration and reduction of multi-view data have the potential to leverage information in different data sets, and to reduce the magnitude and complexity of data for further statistical analysis and interpretation. In this paper, we develop a novel statistical model, called supervised integrated factor analysis (SIFA), for integrative dimension reduction of multi-view data while incorporating auxiliary covariates. The model decomposes data into joint and individual factors, capturing the joint variation across multiple data sets and the individual variation specific to each set respectively. Moreover, both joint and individual factors are partially informed by auxiliary covariates via nonparametric models. We devise a computationally efficient Expectation-Maximization (EM) algorithm to fit the model under some identifiability conditions. We apply the method to the Genotype-Tissue Expression (GTEx) data, and provide new insights into the variation decomposition of gene expression in multiple tissues. Extensive simulation studies and an additional application to a pediatric growth study demonstrate the advantage of the proposed method over competing methods.",modern biomed research ubiquit multipl data set measur set sampl differ view multi view data exampl genet studi multipl genom data set differ molecular level differ cell type measur common set individu investig genet regul integr reduct multi view data potenti leverag inform differ data set reduc magnitud complex data statist analysi interpret paper develop novel statist model call supervis integr factor analysi sifa integr dimens reduct multi view data incorpor auxiliari covari model decompos data joint individu factor captur joint variat across multipl data set individu variat specif set respect moreov joint individu factor partial inform auxiliari covari via nonparametr model devis comput effici expect maxim em algorithm fit model identifi condit appli method genotyp tissu express gtex data provid new insight variat decomposit gene express multipl tissu extens simul studi addit applic pediatr growth studi demonstr advantag propos method compet method,"['Gen Li', 'Sungkyu Jung']",['stat.ME'],False,False,True,False,False,False
1154,2017-03-28T14:04:12Z,2017-03-15T13:51:52Z,http://arxiv.org/abs/1703.05157v1,http://arxiv.org/pdf/1703.05157v1,One-Sided Cross-Validation for Nonsmooth Density Functions,one side cross valid nonsmooth densiti function,One-sided cross-validation (OSCV) is a bandwidth selection method initially introduced by Hart and Yi (1998) in the context of smooth regression functions. Mart\'{\i}nez-Miranda et al. (2009) developed a version of OSCV for smooth density functions. This article extends the method for nonsmooth densities. It also introduces the fully robust OSCV modification that produces consistent OSCV bandwidths for both smooth and nonsmooth cases. Practical implementations of the OSCV method for smooth and nonsmooth densities are discussed. One of the considered cross-validation kernels has potential for improving the OSCV method's implementation in the regression context.,one side cross valid oscv bandwidth select method initi introduc hart yi context smooth regress function mart nez miranda et al develop version oscv smooth densiti function articl extend method nonsmooth densiti also introduc fulli robust oscv modif produc consist oscv bandwidth smooth nonsmooth case practic implement oscv method smooth nonsmooth densiti discuss one consid cross valid kernel potenti improv oscv method implement regress context,['Olga Y. Savchuk'],"['stat.ME', '62G07']",False,False,True,False,False,True
1157,2017-03-28T14:04:12Z,2017-03-15T06:34:38Z,http://arxiv.org/abs/1703.04956v1,http://arxiv.org/pdf/1703.04956v1,A Short Note on Almost Sure Convergence of Bayes Factors in the General   Set-Up,short note almost sure converg bay factor general set,"In this article we derive the almost sure convergence theory of Bayes factor in the general set-up that includes even dependent data and misspecified models, as a simple application of a result of Shalizi (2009) to a well-known identity satisfied by the Bayes factor.",articl deriv almost sure converg theori bay factor general set includ even depend data misspecifi model simpl applic result shalizi well known ident satisfi bay factor,"['Debashis Chatterjee', 'Trisha Maitra', 'Sourabh Bhattacharya']","['math.ST', 'stat.ME', 'stat.TH']",False,False,True,False,False,False
1159,2017-03-28T14:04:12Z,2017-03-15T02:08:07Z,http://arxiv.org/abs/1703.04882v1,http://arxiv.org/pdf/1703.04882v1,Element analysis: a wavelet-based method for analyzing time-localized   events in noisy time series,element analysi wavelet base method analyz time local event noisi time seri,"A method is derived for the quantitative analysis of signals that are composed of superpositions of isolated, time-localized ""events"". Here these events are taken to be well represented as rescaled and phase-rotated versions of generalized Morse wavelets, a broad family of continuous analytic functions. Analyzing a signal composed of replicates of such a function using another Morse wavelet allows one to directly estimate the properties of events from the values of the wavelet transform at its own maxima. The distribution of events in general power-law noise is determined in order to establish significance based on an expected false detection rate. Finally, an expression for an event's ""region of influence"" within the wavelet transform permits the formation of a criterion for rejecting spurious maxima due to numerical artifacts or other unsuitable events. Signals can then be reconstructed based on a small number of isolated points on the time/scale plane. This method, termed element analysis, is applied to the identification of long-lived eddy structures in ocean currents as observed by along-track measurements of sea surface elevation from satellite altimetry",method deriv quantit analysi signal compos superposit isol time local event event taken well repres rescal phase rotat version general mors wavelet broad famili continu analyt function analyz signal compos replic function use anoth mors wavelet allow one direct estim properti event valu wavelet transform maxima distribut event general power law nois determin order establish signific base expect fals detect rate final express event region influenc within wavelet transform permit format criterion reject spurious maxima due numer artifact unsuit event signal reconstruct base small number isol point time scale plane method term element analysi appli identif long live eddi structur ocean current observ along track measur sea surfac elev satellit altimetri,['J. M. Lilly'],['stat.ME'],False,False,True,False,False,False
1163,2017-03-28T14:04:16Z,2017-03-12T18:29:03Z,http://arxiv.org/abs/1703.04157v1,http://arxiv.org/pdf/1703.04157v1,Using Aggregated Relational Data to feasibly identify network structure   without network data,use aggreg relat data feasibl identifi network structur without network data,"Social and economic network data can be useful for both researchers and policymakers, but can often be impractical to collect. We propose collecting Aggregated Relational Data (ARD) using questions that are simple and easy to add to any survey. These question are of the form ""how many of your friends in the village have trait k?""   We show that by collecting ARD on even a small share of the population, researchers can recover the likely distribution of statistics from the underlying network. We provide three empirical examples. We first apply the technique to the 75 village networks in Karnataka, India, where Banerjee et al. (2016b) collected near-complete network data. We show that with ARD alone on even a 29% sample, we can accurately estimate both node-level features (such as eigenvector centrality, clustering) and network-level features (such as the maximum eigenvalue, average path length). To further demonstrate the power of the approach, we apply our technique to two settings analyzed previously by the authors. We show ARD could have been used to predict how to assign monitors to savers to increase savings in rural villages (Breza and Chandrasekhar, 2016). ARD would have led to the same conclusions the authors arrived at when they used expensive near-complete network data. We then provide an example where survey ARD was collected, along with some partial network data, and demonstrate that the same conclusions would have been drawn using only the ARD data, and that with the ARD, the researchers could more generally measure the impact of microfinance exposure on social capital in urban slums (Banerjee et al., 2016a).",social econom network data use research policymak often impract collect propos collect aggreg relat data ard use question simpl easi add ani survey question form mani friend villag trait show collect ard even small share popul research recov like distribut statist network provid three empir exampl first appli techniqu villag network karnataka india banerje et al collect near complet network data show ard alon even sampl accur estim node level featur eigenvector central cluster network level featur maximum eigenvalu averag path length demonstr power approach appli techniqu two set analyz previous author show ard could use predict assign monitor saver increas save rural villag breza chandrasekhar ard would led conclus author arriv use expens near complet network data provid exampl survey ard collect along partial network data demonstr conclus would drawn use onli ard data ard research could general measur impact microfin exposur social capit urban slum banerje et al,"['Emily Breza', 'Arun G. Chandrasekhar', 'Tyler H. McCormick', 'Mengjie Pan']",['stat.ME'],False,False,True,False,False,False
1164,2017-03-28T14:04:16Z,2017-03-11T20:07:06Z,http://arxiv.org/abs/1703.04025v1,http://arxiv.org/pdf/1703.04025v1,Learning Large-Scale Bayesian Networks with the sparsebn Package,learn larg scale bayesian network sparsebn packag,"Learning graphical models from data is an important problem with wide applications, ranging from genomics to the social sciences. Nowadays datasets typically have upwards of thousands---sometimes tens or hundreds of thousands---of variables and far fewer samples. To meet this challenge, we develop a new R package called sparsebn for learning the structure of large, sparse graphical models with a focus on Bayesian networks. While there are many existing packages for this task within the R ecosystem, this package focuses on the unique setting of learning large networks from high-dimensional data, possibly with interventions. As such, the methods provided place a premium on scalability and consistency in a high-dimensional setting. Furthermore, in the presence of interventions, the methods implemented here achieve the goal of learning a causal network from data. The sparsebn package is open-source and available on CRAN.",learn graphic model data import problem wide applic rang genom social scienc nowaday dataset typic upward thousand sometim ten hundr thousand variabl far fewer sampl meet challeng develop new packag call sparsebn learn structur larg spars graphic model focus bayesian network mani exist packag task within ecosystem packag focus uniqu set learn larg network high dimension data possibl intervent method provid place premium scalabl consist high dimension set furthermor presenc intervent method implement achiev goal learn causal network data sparsebn packag open sourc avail cran,"['Bryon Aragam', 'Jiaying Gu', 'Qing Zhou']","['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']",False,False,True,False,False,False
1166,2017-03-28T14:04:16Z,2017-03-09T15:56:58Z,http://arxiv.org/abs/1703.03312v1,http://arxiv.org/pdf/1703.03312v1,Split Sample Empirical Likelihood,split sampl empir likelihood,"We propose a new approach that combines multiple non-parametric likelihood-type components to build a data-driven approximation of the true likelihood function. Our approach is built on empirical likelihood, a non-parametric approximation of the likelihood function. We show the asymptotic behaviors of our approach are identical to those seen in empirical likelihood. We demonstrate that our method performs comparably to empirical likelihood while significantly decreasing computational time.",propos new approach combin multipl non parametr likelihood type compon build data driven approxim true likelihood function approach built empir likelihood non parametr approxim likelihood function show asymptot behavior approach ident seen empir likelihood demonstr method perform compar empir likelihood signific decreas comput time,"['Adam Jaeger', 'Nicole Lazar']",['stat.ME'],False,False,True,False,False,True
1167,2017-03-28T14:04:16Z,2017-03-09T10:19:53Z,http://arxiv.org/abs/1703.03213v1,http://arxiv.org/pdf/1703.03213v1,"Kernel intensity estimation, bootstrapping and bandwidth selection for   inhomogeneous point processes depending on spatial covariates",kernel intens estim bootstrap bandwidth select inhomogen point process depend spatial covari,"In the point process context, kernel intensity estimation has been mainly restricted to exploratory analysis due to its lack of consistency. However the use of covariates has allow to design consistent alternatives under some restrictive assumptions. In this paper we focus our attention on de\-fi\-ning an appropriate framework to derive a consistent kernel intensity estimator using covariates, as well as a consistent smooth bootstrap procedure. For spatial point processes with covariates there is no specific bandwidth selector, hence, we define two new data-driven procedures specifically designed for this scenario: a rule-of-thumb and a plug-in bandwidth based on the bootstrap method previously introduced. A simulation study is accomplished to understand the behaviour of these procedures in finite samples. Finally, we apply the techniques to a real set of data made up of wildfires in Canada during June 2015, using meteorological information as covariates.",point process context kernel intens estim main restrict exploratori analysi due lack consist howev use covari allow design consist altern restrict assumpt paper focus attent de fi ning appropri framework deriv consist kernel intens estim use covari well consist smooth bootstrap procedur spatial point process covari specif bandwidth selector henc defin two new data driven procedur specif design scenario rule thumb plug bandwidth base bootstrap method previous introduc simul studi accomplish understand behaviour procedur finit sampl final appli techniqu real set data made wildfir canada dure june use meteorolog inform covari,"['M. I. Borrajo', 'W. González-Manteiga', 'M. D. Martínez-Miranda']","['stat.ME', 'stat.AP', '62G05, 62G09, 62H11, 60G55, 60-08']",False,False,True,False,False,False
1170,2017-03-28T14:04:20Z,2017-03-09T01:19:39Z,http://arxiv.org/abs/1703.03095v1,http://arxiv.org/pdf/1703.03095v1,Fitting the Linear Preferential Attachment Model,fit linear preferenti attach model,"Preferential attachment is an appealing mechanism for modeling power-law behavior of the degree distributions in directed social networks. In this paper, we consider methods for fitting a 5-parameter linear preferential model to network data under two data scenarios. In the case where full history of the network formation is given, we derive the maximum likelihood estimator of the parameters and show that it is strongly consistent and asymptotically normal. In the case where only a single-time snapshot of the network is available, we propose an estimation method which combines method of moments with an approximation to the likelihood. The resulting estimator is also strongly consistent and performs quite well compared to the MLE estimator. We illustrate both estimation procedures through simulated data, and explore the usage of this model in a real data example. At the end of the paper, we also present a semi-parametric method to model heavy-tailed features of the degree distributions of the network using ideas from extreme value theory.",preferenti attach appeal mechan model power law behavior degre distribut direct social network paper consid method fit paramet linear preferenti model network data two data scenario case full histori network format given deriv maximum likelihood estim paramet show strong consist asymptot normal case onli singl time snapshot network avail propos estim method combin method moment approxim likelihood result estim also strong consist perform quit well compar mle estim illustr estim procedur simul data explor usag model real data exampl end paper also present semi parametr method model heavi tail featur degre distribut network use idea extrem valu theori,"['Phyllis Wan', 'Tiandong Wang', 'Richard A. Davis', 'Sidney I. Resnick']","['stat.ME', '05C80, 90B15, 62F12']",False,False,True,False,False,False
1171,2017-03-28T14:04:20Z,2017-03-08T21:40:57Z,http://arxiv.org/abs/1703.03043v1,http://arxiv.org/pdf/1703.03043v1,Bootstrap with Clustering in Two or More Dimensions,bootstrap cluster two dimens,"We propose a bootstrap procedure for data that may exhibit clustering in two or more dimensions. We use insights from the theory of generalized U-statistics to analyze the large-sample properties of statistics that are sample averages from the observations pooled across clusters. The asymptotic distribution of these statistics may be non-standard if there is no clustering in means. We show that the proposed bootstrap procedure is (a) point-wise consistent for any fixed data-generating process (DGP), (b) uniformly consistent if we exclude the case of clustering without clustering in means, and (c) provides refinements for any DGP such that the limiting distribution is Gaussian.",propos bootstrap procedur data may exhibit cluster two dimens use insight theori general statist analyz larg sampl properti statist sampl averag observ pool across cluster asymptot distribut statist may non standard cluster mean show propos bootstrap procedur point wise consist ani fix data generat process dgp uniform consist exclud case cluster without cluster mean provid refin ani dgp limit distribut gaussian,['Konrad Menzel'],"['stat.ME', 'stat.OT']",False,False,True,False,False,False
1172,2017-03-28T14:04:20Z,2017-03-08T20:32:20Z,http://arxiv.org/abs/1703.03023v1,http://arxiv.org/pdf/1703.03023v1,"Elicitation, measuring bias, checking for prior-data conflict and   inference with a Dirichlet prior",elicit measur bias check prior data conflict infer dirichlet prior,Methods are developed for eliciting a Dirichlet prior based upon bounds on the individual probabilities that hold with virtual certainty. This approach to selecting a prior is applied to a contingency table problem where it is demonstrated how to assess the bias in the prior as well as how to check for prior-data conflict. It is shown that the assessment of a hypothesis via relative belief can easily take into account what it means for the falsity of the hypothesis to correspond to a difference of practical importance and provide evidence in favor of a hypothesis.,method develop elicit dirichlet prior base upon bound individu probabl hold virtual certainti approach select prior appli conting tabl problem demonstr assess bias prior well check prior data conflict shown assess hypothesi via relat belief easili take account mean falsiti hypothesi correspond differ practic import provid evid favor hypothesi,"['Michael Evans', 'Irwin Guttman', 'Peiying Li']","['stat.ME', '62F15']",False,False,True,False,False,True
1173,2017-03-28T14:04:20Z,2017-03-17T20:10:01Z,http://arxiv.org/abs/1703.03022v2,http://arxiv.org/pdf/1703.03022v2,A New Capture-Recapture Model in Dual-record System,new captur recaptur model dual record system,"Population size estimation based on two sample capture-recapture type experiment is an interesting problem in various fields including epidemiology, ecology, population studies, etc. Lincoln-Petersen estimate is popularly used under the assumption that capture and recapture status of each individual is independent. However, in many real life scenarios, there is some inherent dependency between capture and recapture attempts which is not well-studied in the literature for two sample capture-recapture method. In this article, we propose a novel model that successfully incorporates the possible causal dependency and provide corresponding estimation methodologies for the associated model parameters. Simulation results show superiority of the performance of the proposed method over existing competitors. The method is illustrated through the analysis of real data sets.",popul size estim base two sampl captur recaptur type experi interest problem various field includ epidemiolog ecolog popul studi etc lincoln petersen estim popular use assumpt captur recaptur status individu independ howev mani real life scenario inher depend captur recaptur attempt well studi literatur two sampl captur recaptur method articl propos novel model success incorpor possibl causal depend provid correspond estim methodolog associ model paramet simul result show superior perform propos method exist competitor method illustr analysi real data set,"['Kiranmoy Chatterjee', 'Prajamitra Bhuyan']","['stat.ME', '62F10']",False,False,True,False,False,False
1176,2017-03-28T14:04:20Z,2017-03-08T06:22:56Z,http://arxiv.org/abs/1703.02724v1,http://arxiv.org/pdf/1703.02724v1,Guaranteed Tensor PCA with Optimality in Statistics and Computation,guarante tensor pca optim statist comput,"Tensors, or high-order arrays, attract much attention in recent research. In this paper, we propose a general framework for tensor principal component analysis (tensor PCA), which focuses on the methodology and theory for extracting the hidden low-rank structure from the high-dimensional tensor data. A unified solution is provided for tensor PCA with considerations in both statistical limits and computational costs. The problem exhibits three different phases according to the signal-noise-ratio (SNR). In particular, with strong SNR, we propose a fast spectral power iteration method that achieves the minimax optimal rate of convergence in estimation; with weak SNR, the information-theoretical lower bound shows that it is impossible to have consistent estimation in general; with moderate SNR, we show that the non-convex maximum likelihood estimation provides optimal solution, but with NP-hard computational cost; moreover, under the hardness hypothesis of hypergraphic planted clique detection, there are no polynomial-time algorithms performing consistently in general. Simulation studies show that the proposed spectral power iteration method have good performance under a variety of settings.",tensor high order array attract much attent recent research paper propos general framework tensor princip compon analysi tensor pca focus methodolog theori extract hidden low rank structur high dimension tensor data unifi solut provid tensor pca consider statist limit comput cost problem exhibit three differ phase accord signal nois ratio snr particular strong snr propos fast spectral power iter method achiev minimax optim rate converg estim weak snr inform theoret lower bound show imposs consist estim general moder snr show non convex maximum likelihood estim provid optim solut np hard comput cost moreov hard hypothesi hypergraph plant cliqu detect polynomi time algorithm perform consist general simul studi show propos spectral power iter method good perform varieti set,"['Anru Zhang', 'Dong Xia']","['math.ST', 'cs.LG', 'stat.ME', 'stat.ML', 'stat.TH']",False,False,True,False,False,True
1179,2017-03-28T14:04:20Z,2017-03-07T16:38:11Z,http://arxiv.org/abs/1703.02462v1,http://arxiv.org/pdf/1703.02462v1,Convex and non-convex regularization methods for spatial point processes   intensity estimation,convex non convex regular method spatial point process intens estim,"This paper deals with feature selection procedures for spatial point processes intensity estimation. We consider regularized versions of estimating equations based on Campbell theorem derived from two classical functions: Poisson likelihood and logistic regression likelihood. We provide general conditions on the spatial point processes and on penalty functions which ensure consistency, sparsity and asymptotic normality. We discuss the numerical implementation and assess finite sample properties in a simulation study. Finally, an application to tropical forestry datasets illustrates the use of the proposed methods.",paper deal featur select procedur spatial point process intens estim consid regular version estim equat base campbel theorem deriv two classic function poisson likelihood logist regress likelihood provid general condit spatial point process penalti function ensur consist sparsiti asymptot normal discuss numer implement assess finit sampl properti simul studi final applic tropic forestri dataset illustr use propos method,"['Achmad Choiruddin', 'Jean-François Coeurjolly', 'Frédérique Letué']","['stat.ME', 'math.ST', 'stat.TH']",False,False,True,False,False,False
1182,2017-03-28T14:04:24Z,2017-03-07T06:40:44Z,http://arxiv.org/abs/1703.02237v1,http://arxiv.org/pdf/1703.02237v1,Scalable Collaborative Targeted Learning for High-Dimensional Data,scalabl collabor target learn high dimension data,"Robust inference of a low-dimensional parameter in a large semi-parametric model relies on external estimators of infinite-dimensional features of the distribution of the data. Typically, only one of the latter is optimized for the sake of constructing a well behaved estimator of the low-dimensional parameter of interest. Optimizing more than one of them for the sake of achieving a better bias-variance trade-off in the estimation of the parameter of interest is the core idea driving the general template of the collaborative targeted minimum loss-based estimation (C-TMLE) procedure. The original implementation/instantiation of the C-TMLE template can be presented as a greedy forward stepwise C-TMLE algorithm. It does not scale well when the number $p$ of covariates increases drastically. This motivates the introduction of a novel instantiation of the C-TMLE template where the covariates are pre-ordered. Its time complexity is $\mathcal{O}(p)$ as opposed to the original $\mathcal{O}(p^2)$, a remarkable gain. We propose two pre-ordering strategies and suggest a rule of thumb to develop other meaningful strategies. Because it is usually unclear a priori which pre-ordering strategy to choose, we also introduce another implementation/instantiation called SL-C-TMLE algorithm that enables the data-driven choice of the better pre-ordering strategy given the problem at hand. Its time complexity is $\mathcal{O}(p)$ as well. The computational burden and relative performance of these algorithms were compared in simulation studies involving fully synthetic data or partially synthetic data based on a real world large electronic health database; and in analyses of three real, large electronic health databases. In all analyses involving electronic health databases, the greedy C-TMLE algorithm is unacceptably slow. Simulation studies indicate our scalable C-TMLE and SL-C-TMLE algorithms work well.",robust infer low dimension paramet larg semi parametr model reli extern estim infinit dimension featur distribut data typic onli one latter optim sake construct well behav estim low dimension paramet interest optim one sake achiev better bias varianc trade estim paramet interest core idea drive general templat collabor target minimum loss base estim tmle procedur origin implement instanti tmle templat present greedi forward stepwis tmle algorithm doe scale well number covari increas drastic motiv introduct novel instanti tmle templat covari pre order time complex mathcal oppos origin mathcal remark gain propos two pre order strategi suggest rule thumb develop meaning strategi becaus usual unclear priori pre order strategi choos also introduc anoth implement instanti call sl tmle algorithm enabl data driven choic better pre order strategi given problem hand time complex mathcal well comput burden relat perform algorithm compar simul studi involv fulli synthet data partial synthet data base real world larg electron health databas analys three real larg electron health databas analys involv electron health databas greedi tmle algorithm unaccept slow simul studi indic scalabl tmle sl tmle algorithm work well,"['Cheng Ju', 'Susan Gruber', 'Samuel D. Lendle', 'Antoine Chambaz', 'Jessica M. Franklin', 'Richard Wyss', 'Sebastian Schneeweiss', 'Mark J. van der Laan']","['stat.CO', 'stat.ME']",False,False,True,False,False,False
1183,2017-03-28T14:04:24Z,2017-03-25T17:47:33Z,http://arxiv.org/abs/1703.02177v2,http://arxiv.org/pdf/1703.02177v2,Mixtures of Generalized Hyperbolic Distributions and Mixtures of Skew-t   Distributions for Model-Based Clustering with Incomplete Data,mixtur general hyperbol distribut mixtur skew distribut model base cluster incomplet data,"Robust clustering from incomplete data is an important topic because, in many practical situations, real data sets are heavy-tailed, asymmetric, and/or have arbitrary patterns of missing observations. Flexible methods and algorithms for model-based clustering are presented via mixture of the generalized hyperbolic distributions and its limiting case, the mixture of multivariate skew-t distributions. An analytically feasible EM algorithm is formulated for parameter estimation and imputation of missing values for mixture models employing missing at random mechanisms. The proposed methodologies are investigated through a simulation study with varying proportions of synthetic missing values and illustrated using a real dataset. Comparisons are made with those obtained from the traditional mixture of generalized hyperbolic distribution counterparts by filling in the missing data using the mean imputation method.",robust cluster incomplet data import topic becaus mani practic situat real data set heavi tail asymmetr arbitrari pattern miss observ flexibl method algorithm model base cluster present via mixtur general hyperbol distribut limit case mixtur multivari skew distribut analyt feasibl em algorithm formul paramet estim imput miss valu mixtur model employ miss random mechan propos methodolog investig simul studi vari proport synthet miss valu illustr use real dataset comparison made obtain tradit mixtur general hyperbol distribut counterpart fill miss data use mean imput method,"['Yuhong Wei', 'Paul D. McNicholas']","['stat.ME', 'stat.CO']",False,False,True,False,False,False
1186,2017-03-28T14:04:24Z,2017-03-06T19:33:24Z,http://arxiv.org/abs/1703.02078v1,http://arxiv.org/pdf/1703.02078v1,Cross-screening in observational studies that test many hypotheses,cross screen observ studi test mani hypothes,"We discuss observational studies that test many causal hypotheses, either hypotheses about many outcomes or many treatments. To be credible an observational study that tests many causal hypotheses must demonstrate that its conclusions are neither artifacts of multiple testing nor of small biases from nonrandom treatment assignment. In a sense that needs to be defined carefully, hidden within a sensitivity analysis for nonrandom assignment is an enormous correction for multiple testing: in the absence of bias, it is extremely improbable that multiple testing alone would create an association insensitive to moderate biases. We propose a new strategy called ""cross-screening"", different from but motivated by recent work of Bogomolov and Heller on replicability. Cross-screening splits the data in half at random, uses the first half to plan a study carried out on the second half, then uses the second half to plan a study carried out on the first half, and reports the more favorable conclusions of the two studies correcting using the Bonferroni inequality for having done two studies. If the two studies happen to concur, then they achieve Bogomolov-Heller replicability; however, importantly, replicability is not required for strong control of the family-wise error rate, and either study alone suffices for firm conclusions. In randomized studies with a few hypotheses, cross-split screening is not an attractive method when compared with conventional methods of multiplicity control, but it can become attractive when hundreds or thousands of hypotheses are subjected to sensitivity analyses in an observational study. We illustrate the technique by comparing 46 biomarkers in individuals who consume large quantities of fish versus little or no fish.",discuss observ studi test mani causal hypothes either hypothes mani outcom mani treatment credibl observ studi test mani causal hypothes must demonstr conclus neither artifact multipl test small bias nonrandom treatment assign sens need defin care hidden within sensit analysi nonrandom assign enorm correct multipl test absenc bias extrem improb multipl test alon would creat associ insensit moder bias propos new strategi call cross screen differ motiv recent work bogomolov heller replic cross screen split data half random use first half plan studi carri second half use second half plan studi carri first half report favor conclus two studi correct use bonferroni inequ done two studi two studi happen concur achiev bogomolov heller replic howev import replic requir strong control famili wise error rate either studi alon suffic firm conclus random studi hypothes cross split screen attract method compar convent method multipl control becom attract hundr thousand hypothes subject sensit analys observ studi illustr techniqu compar biomark individu consum larg quantiti fish versus littl fish,"['Qingyuan Zhao', 'Dylan S. Small', 'Paul R. Rosenbaum']","['stat.ME', 'stat.AP']",False,False,True,False,False,False
1187,2017-03-28T14:04:24Z,2017-03-06T13:41:33Z,http://arxiv.org/abs/1703.01866v1,http://arxiv.org/pdf/1703.01866v1,Weighted empirical likelihood for quantile regression with nonignorable   missing covariates,weight empir likelihood quantil regress nonignor miss covari,"In this paper, we propose an empirical likelihood-based weighted (ELW) estimator of regression parameter in quantile regression model with nonignorable missing covariates. The proposed ELW estimator is computationally simple and more efficient than the CCA estimator. Simulation results show that the ELW method works remarkably well in finite samples. A real data example is used to illustrate the proposed ELW method.",paper propos empir likelihood base weight elw estim regress paramet quantil regress model nonignor miss covari propos elw estim comput simpl effici cca estim simul result show elw method work remark well finit sampl real data exampl use illustr propos elw method,"['Xiaohui Yuan', 'He Si']",['stat.ME'],False,False,True,False,False,False
1190,2017-03-28T14:04:28Z,2017-03-06T00:14:36Z,http://arxiv.org/abs/1703.01692v1,http://arxiv.org/pdf/1703.01692v1,Detecting Spatial Patterns of Disease in Large Collections of Electronic   Medical Records Using Neighbor-Based Bootstrapping (NB2),detect spatial pattern diseas larg collect electron medic record use neighbor base bootstrap nb,"We introduce a method called neighbor-based bootstrapping (NB2) that can be used to quantify the geospatial variation of a variable. We applied this method to an analysis of the incidence rates of disease from electronic medical record data (ICD-9 codes) for approximately 100 million individuals in the US over a period of 8 years. We considered the incidence rate of disease in each county and its geospatially contiguous neighbors and rank ordered diseases in terms of their degree of geospatial variation as quantified by the NB2 method.   We show that this method yields results in good agreement with established methods for detecting spatial autocorrelation (Moran's I method and kriging). Moreover, the NB2 method can be tuned to identify both large area and small area geospatial variations. This method also applies more generally in any parameter space that can be partitioned to consist of regions and their neighbors.",introduc method call neighbor base bootstrap nb use quantifi geospati variat variabl appli method analysi incid rate diseas electron medic record data icd code approxim million individu us period year consid incid rate diseas counti geospati contigu neighbor rank order diseas term degre geospati variat quantifi nb method show method yield result good agreement establish method detect spatial autocorrel moran method krige moreov nb method tune identifi larg area small area geospati variat method also appli general ani paramet space partit consist region neighbor,"['Maria T Patterson', 'Robert L Grossman']",['stat.ME'],False,False,True,False,False,False
1194,2017-03-28T14:04:28Z,2017-03-04T00:16:54Z,http://arxiv.org/abs/1703.01364v1,http://arxiv.org/pdf/1703.01364v1,A Matrix Variate Skew-t Distribution,matrix variat skew distribut,"Although there is ample work in the literature dealing with skewness in the multivariate setting, there is a relative paucity of work in the matrix variate paradigm. Such work is, for example, useful for modelling three-way data. A matrix variate skew-t distribution is derived based on a mean-variance matrix normal mixture. An expectation-conditional maximization algorithm is developed for parameter estimation. Simulated data are used for illustration.",although ampl work literatur deal skew multivari set relat pauciti work matrix variat paradigm work exampl use model three way data matrix variat skew distribut deriv base mean varianc matrix normal mixtur expect condit maxim algorithm develop paramet estim simul data use illustr,"['Michael P. B. Gallaugher', 'Paul D. McNicholas']","['stat.ME', 'math.ST', 'stat.TH']",False,False,True,False,False,False
1198,2017-03-28T14:04:28Z,2017-03-02T18:20:18Z,http://arxiv.org/abs/1703.00884v1,http://arxiv.org/pdf/1703.00884v1,A Dichotomy for Sampling Barrier-Crossing Events of Random Walks with   Regularly Varying Tails,dichotomi sampl barrier cross event random walk regular vari tail,"We study how to sample paths of a random walk up to the first time it crosses a fixed barrier, in the setting where the step sizes are iid with negative mean and have a regularly varying right tail. We introduce a desirable property for a change of measure to be suitable for exact simulation. We study whether the change of measure of Blanchet and Glynn (2008) satisfies this property and show that it does so if and only if the tail index $\alpha$ of the right tail lies in the interval $(1, \, 3/2)$.",studi sampl path random walk first time cross fix barrier set step size iid negat mean regular vari right tail introduc desir properti chang measur suitabl exact simul studi whether chang measur blanchet glynn satisfi properti show doe onli tail index alpha right tail lie interv,"['Ton Dieker', 'Guido Lagos']","['math.PR', 'stat.ME', '68U20, 60G50, 68W40']",False,False,True,False,False,True
1200,2017-03-28T14:01:45Z,2017-03-27T17:50:53Z,http://arxiv.org/abs/1703.09207v1,http://arxiv.org/pdf/1703.09207v1,Fairness in Criminal Justice Risk Assessments: The State of the Art,fair crimin justic risk assess state art,"Objectives: Discussions of fairness in criminal justice risk assessments typically lack conceptual precision. Rhetoric too often substitutes for careful analysis. In this paper, we seek to clarify the tradeoffs between different kinds of fairness and between fairness and accuracy.   Methods: We draw on the existing literatures in criminology, computer science and statistics to provide an integrated examination of fairness and accuracy in criminal justice risk assessments. We also provide an empirical illustration using data from arraignments.   Results: We show that there are at least six kinds of fairness, some of which are incompatible with one another and with accuracy.   Conclusions: Except in trivial cases, it is impossible to maximize accuracy and fairness at the same time, and impossible simultaneously to satisfy all kinds of fairness. In practice, a major complication is different base rates across different legally protected groups. There is a need to consider challenging tradeoffs.",object discuss fair crimin justic risk assess typic lack conceptu precis rhetor often substitut care analysi paper seek clarifi tradeoff differ kind fair fair accuraci method draw exist literatur criminolog comput scienc statist provid integr examin fair accuraci crimin justic risk assess also provid empir illustr use data arraign result show least six kind fair incompat one anoth accuraci conclus except trivial case imposs maxim accuraci fair time imposs simultan satisfi kind fair practic major complic differ base rate across differ legal protect group need consid challeng tradeoff,"['Richard Berk', 'Hoda Heidari', 'Shahin Jabbari', 'Michael Kearns', 'Aaron Roth']",['stat.ML'],False,False,True,False,False,False
1204,2017-03-28T14:01:45Z,2017-03-27T14:38:15Z,http://arxiv.org/abs/1703.09112v1,http://arxiv.org/pdf/1703.09112v1,Sparse Multi-Output Gaussian Processes for Medical Time Series   Prediction,spars multi output gaussian process medic time seri predict,"In real-time monitoring of hospital patients, high-quality inference of patients' health status using all information available from clinical covariates and lab tests are essential to enable successful medical interventions and improve patient outcomes. In this work, we develop and explore a Bayesian nonparametric model based on Gaussian process (GP) regression for hospital patient monitoring. Our method, MedGP, incorporates 24 clinical and lab covariates and supports a rich reference data set from which the relationships between these observed covariates may be inferred and exploited for high-quality inference of patient state over time. To do this, we develop a highly structured sparse GP kernel to enable tractable computation over tens of thousands of time points while estimating correlations among clinical covariates, patients, and periodicity in high-dimensional time series measurements of physiological signals. We apply MedGP to data from hundreds of thousands of patients treated at the Hospital of the University of Pennsylvania. MedGP has a number of benefits over current methods, including (i) not requiring an alignment of the time series data, (ii) quantifying confidence intervals in the predictions, (iii) exploiting a vast and rich database of patients, and (iv) providing interpretable relationships among clinical covariates. We evaluate and compare results from MedGP on the task of online state prediction for three different patient subgroups.",real time monitor hospit patient high qualiti infer patient health status use inform avail clinic covari lab test essenti enabl success medic intervent improv patient outcom work develop explor bayesian nonparametr model base gaussian process gp regress hospit patient monitor method medgp incorpor clinic lab covari support rich refer data set relationship observ covari may infer exploit high qualiti infer patient state time develop high structur spars gp kernel enabl tractabl comput ten thousand time point estim correl among clinic covari patient period high dimension time seri measur physiolog signal appli medgp data hundr thousand patient treat hospit univers pennsylvania medgp number benefit current method includ requir align time seri data ii quantifi confid interv predict iii exploit vast rich databas patient iv provid interpret relationship among clinic covari evalu compar result medgp task onlin state predict three differ patient subgroup,"['Li-Fang Cheng', 'Gregory Darnell', 'Corey Chivers', 'Michael E Draugelis', 'Kai Li', 'Barbara E Engelhardt']",['stat.ML'],False,False,True,False,False,False
1207,2017-03-28T14:01:45Z,2017-03-27T05:41:03Z,http://arxiv.org/abs/1703.08937v1,http://arxiv.org/pdf/1703.08937v1,A Scale Free Algorithm for Stochastic Bandits with Bounded Kurtosis,scale free algorithm stochast bandit bound kurtosi,"Existing strategies for finite-armed stochastic bandits mostly depend on a parameter of scale that must be known in advance. Sometimes this is in the form of a bound on the payoffs, or the knowledge of a variance or subgaussian parameter. The notable exceptions are the analysis of Gaussian bandits with unknown mean and variance by Cowan and Katehakis [2015] and of uniform distributions with unknown support [Cowan and Katehakis, 2015]. The results derived in these specialised cases are generalised here to the non-parametric setup, where the learner knows only a bound on the kurtosis of the noise, which is a scale free measure of the extremity of outliers.",exist strategi finit arm stochast bandit depend paramet scale must known advanc sometim form bound payoff knowledg varianc subgaussian paramet notabl except analysi gaussian bandit unknown mean varianc cowan katehaki uniform distribut unknown support cowan katehaki result deriv specialis case generalis non parametr setup learner know onli bound kurtosi nois scale free measur extrem outlier,['Tor Lattimore'],['stat.ML'],False,False,True,False,False,True
1210,2017-03-28T14:01:49Z,2017-03-26T05:53:39Z,http://arxiv.org/abs/1703.08772v1,http://arxiv.org/pdf/1703.08772v1,Multivariate Regression with Gross Errors on Manifold-valued Data,multivari regress gross error manifold valu data,"We consider the topic of multivariate regression on manifold-valued output, that is, for a multivariate observation, its output response lies on a manifold. Moreover, we propose a new regression model to deal with the presence of grossly corrupted manifold-valued responses, a bottleneck issue commonly encountered in practical scenarios. Our model first takes a correction step on the grossly corrupted responses via geodesic curves on the manifold, and then performs multivariate linear regression on the corrected data. This results in a nonconvex and nonsmooth optimization problem on manifolds. To this end, we propose a dedicated approach named PALMR, by utilizing and extending the proximal alternating linearized minimization techniques. Theoretically, we investigate its convergence property, where it is shown to converge to a critical point under mild conditions. Empirically, we test our model on both synthetic and real diffusion tensor imaging data, and show that our model outperforms other multivariate regression models when manifold-valued responses contain gross errors, and is effective in identifying gross errors.",consid topic multivari regress manifold valu output multivari observ output respons lie manifold moreov propos new regress model deal presenc grossli corrupt manifold valu respons bottleneck issu common encount practic scenario model first take correct step grossli corrupt respons via geodes curv manifold perform multivari linear regress correct data result nonconvex nonsmooth optim problem manifold end propos dedic approach name palmr util extend proxim altern linear minim techniqu theoret investig converg properti shown converg critic point mild condit empir test model synthet real diffus tensor imag data show model outperform multivari regress model manifold valu respons contain gross error effect identifi gross error,"['Xiaowei Zhang', 'Xudong Shi', 'Yu Sun', 'Li Cheng']","['stat.ML', 'cs.CV', 'math.OC']",False,False,True,False,False,False
1219,2017-03-28T14:01:49Z,2017-03-24T13:29:52Z,http://arxiv.org/abs/1703.08403v1,http://arxiv.org/pdf/1703.08403v1,Asymmetric Learning Vector Quantization for Efficient Nearest Neighbor   Classification in Dynamic Time Warping Spaces,asymmetr learn vector quantize effici nearest neighbor classif dynam time warp space,"The nearest neighbor method together with the dynamic time warping (DTW) distance is one of the most popular approaches in time series classification. This method suffers from high storage and computation requirements for large training sets. As a solution to both drawbacks, this article extends learning vector quantization (LVQ) from Euclidean spaces to DTW spaces. The proposed LVQ scheme uses asymmetric weighted averaging as update rule. Empirical results exhibited superior performance of asymmetric generalized LVQ (GLVQ) over other state-of-the-art prototype generation methods for nearest neighbor classification.",nearest neighbor method togeth dynam time warp dtw distanc one popular approach time seri classif method suffer high storag comput requir larg train set solut drawback articl extend learn vector quantize lvq euclidean space dtw space propos lvq scheme use asymmetr weight averag updat rule empir result exhibit superior perform asymmetr general lvq glvq state art prototyp generat method nearest neighbor classif,"['Brijnesh Jain', 'David Schultz']","['cs.LG', 'stat.ML']",False,False,True,False,False,False
1221,2017-03-28T14:01:53Z,2017-03-24T02:31:23Z,http://arxiv.org/abs/1703.08267v1,http://arxiv.org/abs/1703.08267v1,A Nonconvex Splitting Method for Symmetric Nonnegative Matrix   Factorization: Convergence Analysis and Optimality,nonconvex split method symmetr nonneg matrix factor converg analysi optim,"Symmetric nonnegative matrix factorization (SymNMF) has important applications in data analytics problems such as document clustering, community detection and image segmentation. In this paper, we propose a novel nonconvex variable splitting method for solving SymNMF. The proposed algorithm is guaranteed to converge to the set of Karush-Kuhn-Tucker (KKT) points of the nonconvex SymNMF problem. Furthermore, it achieves a global sublinear convergence rate. We also show that the algorithm can be efficiently implemented in parallel. Further, sufficient conditions are provided which guarantee the global and local optimality of the obtained solutions. Extensive numerical results performed on both synthetic and real data sets suggest that the proposed algorithm converges quickly to a local minimum solution.",symmetr nonneg matrix factor symnmf import applic data analyt problem document cluster communiti detect imag segment paper propos novel nonconvex variabl split method solv symnmf propos algorithm guarante converg set karush kuhn tucker kkt point nonconvex symnmf problem furthermor achiev global sublinear converg rate also show algorithm effici implement parallel suffici condit provid guarante global local optim obtain solut extens numer result perform synthet real data set suggest propos algorithm converg quick local minimum solut,"['Songtao Lu', 'Mingyi Hong', 'Zhengdao Wang']","['math.OC', 'stat.ML']",False,False,True,False,False,True
1222,2017-03-28T14:01:53Z,2017-03-23T23:27:12Z,http://arxiv.org/abs/1703.08251v1,http://arxiv.org/pdf/1703.08251v1,The Dependence of Machine Learning on Electronic Medical Record Quality,depend machin learn electron medic record qualiti,"There is growing interest in applying machine learning methods to Electronic Medical Records (EMR). Across different institutions, however, EMR quality can vary widely. This work investigated the impact of this disparity on the performance of three advanced machine learning algorithms: logistic regression, multilayer perceptron, and recurrent neural network. The EMR disparity was emulated using different permutations of the EMR collected at Children's Hospital Los Angeles (CHLA) Pediatric Intensive Care Unit (PICU) and Cardiothoracic Intensive Care Unit (CTICU). The algorithms were trained using patients from the PICU to predict in-ICU mortality for patients in a held out set of PICU and CTICU patients. The disparate patient populations between the PICU and CTICU provide an estimate of generalization errors across different ICUs. We quantified and evaluated the generalization of these algorithms on varying EMR size, input types, and fidelity of data.",grow interest appli machin learn method electron medic record emr across differ institut howev emr qualiti vari wide work investig impact dispar perform three advanc machin learn algorithm logist regress multilay perceptron recurr neural network emr dispar emul use differ permut emr collect children hospit los angel chla pediatr intens care unit picu cardiothorac intens care unit cticu algorithm train use patient picu predict icu mortal patient held set picu cticu patient dispar patient popul picu cticu provid estim general error across differ icus quantifi evalu general algorithm vari emr size input type fidel data,"['Long Ho', 'David Ledbetter', 'Melissa Aczon', 'Randall Wetzel']",['stat.ML'],False,False,True,False,False,True
1223,2017-03-28T14:01:53Z,2017-03-23T15:35:33Z,http://arxiv.org/abs/1703.08110v1,http://arxiv.org/pdf/1703.08110v1,Training Mixture Models at Scale via Coresets,train mixtur model scale via coreset,"How can we train a statistical mixture model on a massive data set? In this paper, we show how to construct coresets for mixtures of Gaussians and natural generalizations. A coreset is a weighted subset of the data, which guarantees that models fitting the coreset also provide a good fit for the original data set. We show that, perhaps surprisingly, Gaussian mixtures admit coresets of size polynomial in dimension and the number of mixture components, while being independent of the data set size. Hence, one can compute a $(1+ \varepsilon)$-approximation for the optimal model on a significantly smaller data set. More importantly, such coresets can be efficiently constructed both in distributed and streaming settings. Our results rely on a novel reduction of statistical estimation to problems in computational geometry and new complexity results for mixtures of Gaussians. As a by-product of our analysis, we prove that the pseudo-dimension of arbitrary mixtures of Gaussians is polynomial in the ambient dimension. Empirical evaluation on several real-world datasets suggest that our coreset-based approach enables significant reduction in training-time with negligible approximation error.",train statist mixtur model massiv data set paper show construct coreset mixtur gaussian natur general coreset weight subset data guarante model fit coreset also provid good fit origin data set show perhap surpris gaussian mixtur admit coreset size polynomi dimens number mixtur compon independ data set size henc one comput varepsilon approxim optim model signific smaller data set import coreset effici construct distribut stream set result reli novel reduct statist estim problem comput geometri new complex result mixtur gaussian product analysi prove pseudo dimens arbitrari mixtur gaussian polynomi ambient dimens empir evalu sever real world dataset suggest coreset base approach enabl signific reduct train time neglig approxim error,"['Mario Lucic', 'Matthew Faulkner', 'Andreas Krause', 'Dan Feldman']",['stat.ML'],False,False,True,False,False,False
1231,2017-03-28T14:01:57Z,2017-03-23T03:17:14Z,http://arxiv.org/abs/1703.07915v1,http://arxiv.org/pdf/1703.07915v1,Perspective: Energy Landscapes for Machine Learning,perspect energi landscap machin learn,"Machine learning techniques are being increasingly used as flexible non-linear fitting and prediction tools in the physical sciences. Fitting functions that exhibit multiple solutions as local minima can be analysed in terms of the corresponding machine learning landscape. Methods to explore and visualise molecular potential energy landscapes can be applied to these machine learning landscapes to gain new insight into the solution space involved in training and the nature of the corresponding predictions. In particular, we can define quantities analogous to molecular structure, thermodynamics, and kinetics, and relate these emergent properties to the structure of the underlying landscape. This Perspective aims to describe these analogies with examples from recent applications, and suggest avenues for new interdisciplinary research.",machin learn techniqu increas use flexibl non linear fit predict tool physic scienc fit function exhibit multipl solut local minima analys term correspond machin learn landscap method explor visualis molecular potenti energi landscap appli machin learn landscap gain new insight solut space involv train natur correspond predict particular defin quantiti analog molecular structur thermodynam kinet relat emerg properti structur landscap perspect aim describ analog exampl recent applic suggest avenu new interdisciplinari research,"['Andrew J. Ballard', 'Ritankar Das', 'Stefano Martiniani', 'Dhagash Mehta', 'Levent Sagun', 'Jacob D. Stevenson', 'David J. Wales']","['stat.ML', 'cond-mat.dis-nn', 'cs.LG']",False,False,True,False,False,True
1232,2017-03-28T14:01:57Z,2017-03-23T02:40:36Z,http://arxiv.org/abs/1703.07909v1,http://arxiv.org/pdf/1703.07909v1,Data Driven Exploratory Attacks on Black Box Classifiers in Adversarial   Domains,data driven exploratori attack black box classifi adversari domain,"While modern day web applications aim to create impact at the civilization level, they have become vulnerable to adversarial activity, where the next cyber-attack can take any shape and can originate from anywhere. The increasing scale and sophistication of attacks, has prompted the need for a data driven solution, with machine learning forming the core of many cybersecurity systems. Machine learning was not designed with security in mind, and the essential assumption of stationarity, requiring that the training and testing data follow similar distributions, is violated in an adversarial domain. In this paper, an adversary's view point of a classification based system, is presented. Based on a formal adversarial model, the Seed-Explore-Exploit framework is presented, for simulating the generation of data driven and reverse engineering attacks on classifiers. Experimental evaluation, on 10 real world datasets and using the Google Cloud Prediction Platform, demonstrates the innate vulnerability of classifiers and the ease with which evasion can be carried out, without any explicit information about the classifier type, the training data or the application domain. The proposed framework, algorithms and empirical evaluation, serve as a white hat analysis of the vulnerabilities, and aim to foster the development of secure machine learning frameworks.",modern day web applic aim creat impact civil level becom vulner adversari activ next cyber attack take ani shape origin anywher increas scale sophist attack prompt need data driven solut machin learn form core mani cybersecur system machin learn design secur mind essenti assumpt stationar requir train test data follow similar distribut violat adversari domain paper adversari view point classif base system present base formal adversari model seed explor exploit framework present simul generat data driven revers engin attack classifi experiment evalu real world dataset use googl cloud predict platform demonstr innat vulner classifi eas evas carri without ani explicit inform classifi type train data applic domain propos framework algorithm empir evalu serv white hat analysi vulner aim foster develop secur machin learn framework,"['Tegjyot Singh Sethi', 'Mehmed Kantardzic']","['stat.ML', 'cs.CR', 'cs.LG']",False,False,True,False,False,True
1233,2017-03-28T14:01:57Z,2017-03-23T01:30:17Z,http://arxiv.org/abs/1703.07904v1,http://arxiv.org/pdf/1703.07904v1,Cross-Validation with Confidence,cross valid confid,"Cross-validation is one of the most popular model selection methods in statistics and machine learning. Despite its wide applicability, traditional cross-validation methods tend to select overfitting models, unless the ratio between the training and testing sample sizes is much smaller than conventional choices. We argue that such an overfitting tendency of cross-validation is due to the ignorance of the uncertainty in the testing sample. Starting from this observation, we develop a new, statistically principled inference tool based on cross-validation that takes into account the uncertainty in the testing sample. This new method outputs a small set of highly competitive candidate models containing the best one with guaranteed probability. As a consequence, our method can achieve consistent variable selection in a classical linear regression setting, for which existing cross-validation methods require unconventional split ratios. We demonstrate the performance of the proposed method in several simulated and real data examples.",cross valid one popular model select method statist machin learn despit wide applic tradit cross valid method tend select overfit model unless ratio train test sampl size much smaller convent choic argu overfit tendenc cross valid due ignor uncertainti test sampl start observ develop new statist principl infer tool base cross valid take account uncertainti test sampl new method output small set high competit candid model contain best one guarante probabl consequ method achiev consist variabl select classic linear regress set exist cross valid method requir unconvent split ratio demonstr perform propos method sever simul real data exampl,['Jing Lei'],"['stat.ME', 'stat.ML']",False,False,True,False,False,True
1236,2017-03-28T14:01:57Z,2017-03-22T17:53:27Z,http://arxiv.org/abs/1703.07771v1,http://arxiv.org/pdf/1703.07771v1,Multitask Learning and Benchmarking with Clinical Time Series Data,multitask learn benchmark clinic time seri data,"Health care is one of the most exciting frontiers in data mining and machine learning. Successful adoption of electronic health records (EHRs) created an explosion in digital clinical data available for analysis, but progress in machine learning for healthcare research has been difficult to measure because of the absence of publicly available benchmark data sets. To address this problem, we propose four clinical prediction benchmarks using data derived from the publicly available Medical Information Mart for Intensive Care (MIMIC-III) database. These tasks cover a range of clinical problems including modeling risk of mortality, forecasting length of stay, detecting physiologic decline, and phenotype classification. We formulate a heterogeneous multitask problem where the goal is to jointly learn multiple clinically relevant prediction tasks based on the same time series data. To address this problem, we propose a novel recurrent neural network (RNN) architecture that leverages the correlations between the various tasks to learn a better predictive model. We validate the proposed neural architecture on this benchmark, and demonstrate that it outperforms strong baselines, including single task RNNs.",health care one excit frontier data mine machin learn success adopt electron health record ehr creat explos digit clinic data avail analysi progress machin learn healthcar research difficult measur becaus absenc public avail benchmark data set address problem propos four clinic predict benchmark use data deriv public avail medic inform mart intens care mimic iii databas task cover rang clinic problem includ model risk mortal forecast length stay detect physiolog declin phenotyp classif formul heterogen multitask problem goal joint learn multipl clinic relev predict task base time seri data address problem propos novel recurr neural network rnn architectur leverag correl various task learn better predict model valid propos neural architectur benchmark demonstr outperform strong baselin includ singl task rnns,"['Hrayr Harutyunyan', 'Hrant Khachatrian', 'David C. Kale', 'Aram Galstyan']","['stat.ML', 'cs.LG']",False,False,True,False,False,False
1241,2017-03-28T14:02:02Z,2017-03-22T12:50:15Z,http://arxiv.org/abs/1703.07625v1,http://arxiv.org/pdf/1703.07625v1,Clustering for Different Scales of Measurement - the Gap-Ratio Weighted   K-means Algorithm,cluster differ scale measur gap ratio weight mean algorithm,"This paper describes a method for clustering data that are spread out over large regions and which dimensions are on different scales of measurement. Such an algorithm was developed to implement a robotics application consisting in sorting and storing objects in an unsupervised way. The toy dataset used to validate such application consists of Lego bricks of different shapes and colors. The uncontrolled lighting conditions together with the use of RGB color features, respectively involve data with a large spread and different levels of measurement between data dimensions. To overcome the combination of these two characteristics in the data, we have developed a new weighted K-means algorithm, called gap-ratio K-means, which consists in weighting each dimension of the feature space before running the K-means algorithm. The weight associated with a feature is proportional to the ratio of the biggest gap between two consecutive data points, and the average of all the other gaps. This method is compared with two other variants of K-means on the Lego bricks clustering problem as well as two other common classification datasets.",paper describ method cluster data spread larg region dimens differ scale measur algorithm develop implement robot applic consist sort store object unsupervis way toy dataset use valid applic consist lego brick differ shape color uncontrol light condit togeth use rgb color featur respect involv data larg spread differ level measur data dimens overcom combin two characterist data develop new weight mean algorithm call gap ratio mean consist weight dimens featur space befor run mean algorithm weight associ featur proport ratio biggest gap two consecut data point averag gap method compar two variant mean lego brick cluster problem well two common classif dataset,"['Joris Guérin', 'Olivier Gibaru', 'Stéphane Thiery', 'Eric Nyiri']","['cs.LG', 'cs.DS', 'stat.ML']",False,False,True,False,False,False
1243,2017-03-28T14:02:02Z,2017-03-22T11:48:11Z,http://arxiv.org/abs/1703.07607v1,http://arxiv.org/pdf/1703.07607v1,A probabilistic approach to emission-line galaxy classification,probabilist approach emiss line galaxi classif,"This work employs a Gaussian mixture model (GMM) to jointly analyse two traditional emission-line classification schemes of galaxy ionization sources: the Baldwin-Phillips-Terlevich (BPT) and W$_{H\alpha}$ vs. [NII]/H$\alpha$ (WHAN) diagrams, using spectroscopic data from the Sloan Digital Sky Survey Data Release 7 and SEAGal/STARLIGHT datasets. We apply a GMM to empirically define classes of galaxies in a three-dimensional space spanned by the log [OIII]/H\beta, log [NII]/H\alpha, and log EW(H{\alpha}) optical parameters. The best-fit GMM based on several statistical criteria consists of four Gaussian components (GCs), which are capable to explain up to 97 per cent of the data variance. Using elements of information theory, we compare each GC to their respective astronomical counterpart. GC1 and GC4 are associated with star-forming galaxies, suggesting the need to define a new starburst subgroup. GC2 is associated with BPT's Active Galaxy Nuclei (AGN) class and WHAN's weak AGN class. GC3 is associated with BPT's composite class and WHAN's strong AGN class. Conversely, there is no statistical evidence -- based on GMMs -- for the existence of a Seyfert/LINER dichotomy in our sample. We demonstrate the potential of our methodology to recover/unravel different objects inside the wilderness of astronomical datasets, without lacking the ability to convey physically interpretable results; hence being a precious tool for maximum exploitation of the ever-increasing astronomical surveys. The probabilistic classifications from the GMM analysis are publicly available within the COINtoolbox (https://cointoolbox.github.io/GMM_Catalogue/)",work employ gaussian mixtur model gmm joint analys two tradit emiss line classif scheme galaxi ionize sourc baldwin phillip terlevich bpt alpha vs nii alpha whan diagram use spectroscop data sloan digit sky survey data releas seagal starlight dataset appli gmm empir defin class galaxi three dimension space span log oiii beta log nii alpha log ew alpha optic paramet best fit gmm base sever statist criteria consist four gaussian compon gcs capabl explain per cent data varianc use element inform theori compar gc respect astronom counterpart gc gc associ star form galaxi suggest need defin new starburst subgroup gc associ bpt activ galaxi nuclei agn class whan weak agn class gc associ bpt composit class whan strong agn class convers statist evid base gmms exist seyfert liner dichotomi sampl demonstr potenti methodolog recov unravel differ object insid wilder astronom dataset without lack abil convey physic interpret result henc precious tool maximum exploit ever increas astronom survey probabilist classif gmm analysi public avail within cointoolbox https cointoolbox github io gmm catalogu,"['R. S. de Souza', 'M. L. L. Dantas', 'M. V. Costa-Duarte', 'E. D. Feigelson', 'M. Killedar', 'P. -Y. Lablanche', 'R. Vilalta', 'A. Krone-Martins', 'R. Beck', 'F. Gieseke']","['astro-ph.GA', 'astro-ph.IM', 'stat.ML']",False,False,True,False,False,False
1248,2017-03-28T14:02:02Z,2017-03-21T18:00:02Z,http://arxiv.org/abs/1703.07355v1,http://arxiv.org/abs/1703.07355v1,An Army of Me: Sockpuppets in Online Discussion Communities,armi sockpuppet onlin discuss communiti,"In online discussion communities, users can interact and share information and opinions on a wide variety of topics. However, some users may create multiple identities, or sockpuppets, and engage in undesired behavior by deceiving others or manipulating discussions. In this work, we study sockpuppetry across nine discussion communities, and show that sockpuppets differ from ordinary users in terms of their posting behavior, linguistic traits, as well as social network structure. Sockpuppets tend to start fewer discussions, write shorter posts, use more personal pronouns such as ""I"", and have more clustered ego-networks. Further, pairs of sockpuppets controlled by the same individual are more likely to interact on the same discussion at the same time than pairs of ordinary users. Our analysis suggests a taxonomy of deceptive behavior in discussion communities. Pairs of sockpuppets can vary in their deceptiveness, i.e., whether they pretend to be different users, or their supportiveness, i.e., if they support arguments of other sockpuppets controlled by the same user. We apply these findings to a series of prediction tasks, notably, to identify whether a pair of accounts belongs to the same underlying user or not. Altogether, this work presents a data-driven view of deception in online discussion communities and paves the way towards the automatic detection of sockpuppets.",onlin discuss communiti user interact share inform opinion wide varieti topic howev user may creat multipl ident sockpuppet engag undesir behavior deceiv manipul discuss work studi sockpuppetri across nine discuss communiti show sockpuppet differ ordinari user term post behavior linguist trait well social network structur sockpuppet tend start fewer discuss write shorter post use person pronoun cluster ego network pair sockpuppet control individu like interact discuss time pair ordinari user analysi suggest taxonomi decept behavior discuss communiti pair sockpuppet vari decept whether pretend differ user support support argument sockpuppet control user appli find seri predict task notabl identifi whether pair account belong user altogeth work present data driven view decept onlin discuss communiti pave way toward automat detect sockpuppet,"['Srijan Kumar', 'Justin Cheng', 'Jure Leskovec', 'V. S. Subrahmanian']","['cs.SI', 'cs.CY', 'physics.soc-ph', 'stat.AP', 'stat.ML']",False,False,True,False,False,False
1253,2017-03-28T14:02:06Z,2017-03-21T13:02:35Z,http://arxiv.org/abs/1703.07198v1,http://arxiv.org/pdf/1703.07198v1,Overcoming model simplifications when quantifying predictive uncertainty,overcom model simplif quantifi predict uncertainti,"It is generally accepted that all models are wrong -- the difficulty is determining which are useful. Here, a useful model is considered as one that is capable of combining data and expert knowledge, through an inversion or calibration process, to adequately characterize the uncertainty in predictions of interest. This paper derives conditions that specify which simplified models are useful and how they should be calibrated. To start, the notion of an optimal simplification is defined. This relates the model simplifications to the nature of the data and predictions, and determines when a standard probabilistic calibration scheme is capable of accurately characterizing uncertainty. Furthermore, two additional conditions are defined for suboptimal models that determine when the simplifications can be safely ignored. The first allows a suboptimally simplified model to be used in a way that replicates the performance of an optimal model. This is achieved through the judicial selection of a prior term for the calibration process that explicitly includes the nature of the data, predictions and modelling simplifications. The second considers the dependency structure between the predictions and the available data to gain insights into when the simplifications can be overcome by using the right calibration data. Furthermore, the derived conditions are related to the commonly used calibration schemes based on Tikhonov and subspace regularization. To allow concrete insights to be obtained, the analysis is performed under a linear expansion of the model equations and where the predictive uncertainty is characterized via second order moments only.",general accept model wrong difficulti determin use use model consid one capabl combin data expert knowledg invers calibr process adequ character uncertainti predict interest paper deriv condit specifi simplifi model use calibr start notion optim simplif defin relat model simplif natur data predict determin standard probabilist calibr scheme capabl accur character uncertainti furthermor two addit condit defin suboptim model determin simplif safe ignor first allow suboptim simplifi model use way replic perform optim model achiev judici select prior term calibr process explicit includ natur data predict model simplif second consid depend structur predict avail data gain insight simplif overcom use right calibr data furthermor deriv condit relat common use calibr scheme base tikhonov subspac regular allow concret insight obtain analysi perform linear expans model equat predict uncertainti character via second order moment onli,"['George M. Mathews', 'John Vial']","['stat.ML', 'math.PR', 'physics.comp-ph', 'physics.geo-ph', 'stat.ME', '62F15, 62C10, 68U05, 93E12, 93B11, 62P12']",False,False,True,False,False,False
1256,2017-03-28T14:02:06Z,2017-03-21T05:08:33Z,http://arxiv.org/abs/1703.07056v1,http://arxiv.org/pdf/1703.07056v1,Stochastic Primal Dual Coordinate Method with Non-Uniform Sampling Based   on Optimality Violations,stochast primal dual coordin method non uniform sampl base optim violat,"We study primal-dual type stochastic optimization algorithms with non-uniform sampling. Our main theoretical contribution in this paper is to present a convergence analysis of Stochastic Primal Dual Coordinate (SPDC) Method with arbitrary sampling. Based on this theoretical framework, we propose Optimality Violation-based Sampling SPDC (ovsSPDC), a non-uniform sampling method based on Optimality Violation. We also propose two efficient heuristic variants of ovsSPDC called ovsSDPC+ and ovsSDPC++. Through intensive numerical experiments, we demonstrate that the proposed method and its variants are faster than other state-of-the-art primal-dual type stochastic optimization methods.",studi primal dual type stochast optim algorithm non uniform sampl main theoret contribut paper present converg analysi stochast primal dual coordin spdc method arbitrari sampl base theoret framework propos optim violat base sampl spdc ovsspdc non uniform sampl method base optim violat also propos two effici heurist variant ovsspdc call ovssdpc ovssdpc intens numer experi demonstr propos method variant faster state art primal dual type stochast optim method,"['Atsushi Shibagaki', 'Ichiro Takeuchi']","['stat.ML', 'cs.LG', 'math.OC']",False,False,True,False,False,True
1272,2017-03-28T14:02:14Z,2017-03-19T23:28:39Z,http://arxiv.org/abs/1703.06537v1,http://arxiv.org/pdf/1703.06537v1,A Controlled Set-Up Experiment to Establish Personalized Baselines for   Real-Life Emotion Recognition,control set experi establish person baselin real life emot recognit,"We design, conduct and present the results of a highly personalized baseline emotion recognition experiment, which aims to set reliable ground-truth estimates for the subject's emotional state for real-life prediction under similar conditions using a small number of physiological sensors. We also propose an adaptive stimuli-selection mechanism that would use the user's feedback as guide for future stimuli selection in the controlled-setup experiment and generate optimal ground-truth personalized sessions systematically. Initial results are very promising (85% accuracy) and variable importance analysis shows that only a few features, which are easy-to-implement in portable devices, would suffice to predict the subject's emotional state.",design conduct present result high person baselin emot recognit experi aim set reliabl ground truth estim subject emot state real life predict similar condit use small number physiolog sensor also propos adapt stimuli select mechan would use user feedback guid futur stimuli select control setup experi generat optim ground truth person session systemat initi result veri promis accuraci variabl import analysi show onli featur easi implement portabl devic would suffic predict subject emot state,"['Varvara Kollia', 'Noureddine Tayebi']","['stat.ML', 'cs.HC']",False,False,True,False,False,False
1273,2017-03-28T14:02:14Z,2017-03-19T22:23:01Z,http://arxiv.org/abs/1703.06528v1,http://arxiv.org/pdf/1703.06528v1,Universal Consistency and Robustness of Localized Support Vector   Machines,univers consist robust local support vector machin,"The massive amount of available data potentially used to discover patters in machine learning is a challenge for kernel based algorithms with respect to runtime and storage capacities. Local approaches might help to relieve these issues. From a statistical point of view local approaches allow additionally to deal with different structures in the data in different ways. This paper analyses properties of localized kernel based, non-parametric statistical machine learning methods, in particular of support vector machines (SVMs) and methods close to them. We will show there that locally learnt kernel methods are universal consistent. Furthermore, we give an upper bound for the maxbias in order to show statistical robustness of the proposed method.",massiv amount avail data potenti use discov patter machin learn challeng kernel base algorithm respect runtim storag capac local approach might help reliev issu statist point view local approach allow addit deal differ structur data differ way paper analys properti local kernel base non parametr statist machin learn method particular support vector machin svms method close show local learnt kernel method univers consist furthermor give upper bound maxbia order show statist robust propos method,['Florian Dumpert'],"['stat.ML', '62G08, 62G20, 62G35']",False,False,True,False,False,False
1274,2017-03-28T14:02:14Z,2017-03-19T21:06:51Z,http://arxiv.org/abs/1703.06513v1,http://arxiv.org/pdf/1703.06513v1,Bernoulli Rank-$1$ Bandits for Click Feedback,bernoulli rank bandit click feedback,"The probability that a user will click a search result depends both on its relevance and its position on the results page. The position based model explains this behavior by ascribing to every item an attraction probability, and to every position an examination probability. To be clicked, a result must be both attractive and examined. The probabilities of an item-position pair being clicked thus form the entries of a rank-$1$ matrix. We propose the learning problem of a Bernoulli rank-$1$ bandit where at each step, the learning agent chooses a pair of row and column arms, and receives the product of their Bernoulli-distributed values as a reward. This is a special case of the stochastic rank-$1$ bandit problem considered in recent work that proposed an elimination based algorithm Rank1Elim, and showed that Rank1Elim's regret scales linearly with the number of rows and columns on ""benign"" instances. These are the instances where the minimum of the average row and column rewards $\mu$ is bounded away from zero. The issue with Rank1Elim is that it fails to be competitive with straightforward bandit strategies as $\mu \rightarrow 0$. In this paper we propose Rank1ElimKL which simply replaces the (crude) confidence intervals of Rank1Elim with confidence intervals based on Kullback-Leibler (KL) divergences, and with the help of a novel result concerning the scaling of KL divergences we prove that with this change, our algorithm will be competitive no matter the value of $\mu$. Experiments with synthetic data confirm that on benign instances the performance of Rank1ElimKL is significantly better than that of even Rank1Elim, while experiments with models derived from real data confirm that the improvements are significant across the board, regardless of whether the data is benign or not.",probabl user click search result depend relev posit result page posit base model explain behavior ascrib everi item attract probabl everi posit examin probabl click result must attract examin probabl item posit pair click thus form entri rank matrix propos learn problem bernoulli rank bandit step learn agent choos pair row column arm receiv product bernoulli distribut valu reward special case stochast rank bandit problem consid recent work propos elimin base algorithm rankelim show rankelim regret scale linear number row column benign instanc instanc minimum averag row column reward mu bound away zero issu rankelim fail competit straightforward bandit strategi mu rightarrow paper propos rankelimkl simpli replac crude confid interv rankelim confid interv base kullback leibler kl diverg help novel result concern scale kl diverg prove chang algorithm competit matter valu mu experi synthet data confirm benign instanc perform rankelimkl signific better even rankelim experi model deriv real data confirm improv signific across board regardless whether data benign,"['Sumeet Katariya', 'Branislav Kveton', 'Csaba Szepesvári', 'Claire Vernade', 'Zheng Wen']","['cs.LG', 'stat.ML']",False,False,True,False,False,True
1278,2017-03-28T14:02:14Z,2017-03-18T08:38:51Z,http://arxiv.org/abs/1703.06272v1,http://arxiv.org/pdf/1703.06272v1,An Automated Auto-encoder Correlation-based Health-Monitoring and   Prognostic Method for Machine Bearings,autom auto encod correl base health monitor prognost method machin bear,"This paper studies an intelligent ultimate technique for health-monitoring and prognostic of common rotary machine components, particularly bearings. During a run-to-failure experiment, rich unsupervised features from vibration sensory data are extracted by a trained sparse auto-encoder. Then, the correlation of the extracted attributes of the initial samples (presumably healthy at the beginning of the test) with the succeeding samples is calculated and passed through a moving-average filter. The normalized output is named auto-encoder correlation-based (AEC) rate which stands for an informative attribute of the system depicting its health status and precisely identifying the degradation starting point. We show that AEC technique well-generalizes in several run-to-failure tests. AEC collects rich unsupervised features form the vibration data fully autonomous. We demonstrate the superiority of the AEC over many other state-of-the-art approaches for the health monitoring and prognostic of machine bearings.",paper studi intellig ultim techniqu health monitor prognost common rotari machin compon particular bear dure run failur experi rich unsupervis featur vibrat sensori data extract train spars auto encod correl extract attribut initi sampl presum healthi begin test succeed sampl calcul pass move averag filter normal output name auto encod correl base aec rate stand inform attribut system depict health status precis identifi degrad start point show aec techniqu well general sever run failur test aec collect rich unsupervis featur form vibrat data fulli autonom demonstr superior aec mani state art approach health monitor prognost machin bear,"['Ramin M. Hasani', 'Guodong Wang', 'Radu Grosu']","['cs.LG', 'cs.NE', 'stat.ML']",False,False,True,False,False,False
1279,2017-03-28T14:02:14Z,2017-03-25T13:19:14Z,http://arxiv.org/abs/1703.06270v3,http://arxiv.org/pdf/1703.06270v3,SIM-CE: An Advanced Simulink Platform for Studying the Brain of   Caenorhabditis elegans,sim ce advanc simulink platform studi brain caenorhabd elegan,"We introduce SIM-CE, an advanced, user-friendly modeling and simulation environment in Simulink for performing multi-scale behavioral analysis of the nervous system of Caenorhabditis elegans (C. elegans). SIM-CE contains an implementation of the mathematical models of C. elegans's neurons and synapses, in Simulink, which can be easily extended and particularized by the user. The Simulink model is able to capture both complex dynamics of ion channels and additional biophysical detail such as intracellular calcium concentration. We demonstrate the performance of SIM-CE by carrying out neuronal, synaptic and neural-circuit-level behavioral simulations. Such environment enables the user to capture unknown properties of the neural circuits, test hypotheses and determine the origin of many behavioral plasticities exhibited by the worm.",introduc sim ce advanc user friend model simul environ simulink perform multi scale behavior analysi nervous system caenorhabd elegan elegan sim ce contain implement mathemat model elegan neuron synaps simulink easili extend particular user simulink model abl captur complex dynam ion channel addit biophys detail intracellular calcium concentr demonstr perform sim ce carri neuron synapt neural circuit level behavior simul environ enabl user captur unknown properti neural circuit test hypothes determin origin mani behavior plastic exhibit worm,"['Ramin M. Hasani', 'Victoria Beneder', 'Magdalena Fuchs', 'David Lung', 'Radu Grosu']","['q-bio.NC', 'cs.NE', 'q-bio.QM', 'stat.ML']",False,False,True,False,False,False
1281,2017-03-28T14:02:18Z,2017-03-18T00:59:40Z,http://arxiv.org/abs/1703.06229v1,http://arxiv.org/pdf/1703.06229v1,Curriculum Dropout,curriculum dropout,"Dropout is a very effective way of regularizing neural networks. Stochastically ""dropping out"" units with a certain probability discourages over-specific co-adaptations of feature detectors, preventing overfitting and improving network generalization. Besides, Dropout can be interpreted as an approximate model aggregation technique, where an exponential number of smaller networks are averaged in order to get a more powerful ensemble. In this paper, we show that using a fixed dropout probability during training is a suboptimal choice. We thus propose a time scheduling for the probability of retaining neurons in the network. This induces an adaptive regularization scheme that smoothly increases the difficulty of the optimization problem. This idea of ""starting easy"" and adaptively increasing the difficulty of the learning problem has its roots in curriculum learning and allows one to train better models. Indeed, we prove that our optimization strategy implements a very general curriculum scheme, by gradually adding noise to both the input and intermediate feature representations within the network architecture. Experiments on seven image classification datasets and different network architectures show that our method, named Curriculum Dropout, frequently yields to better generalization and, at worst, performs just as well as the standard Dropout method.",dropout veri effect way regular neural network stochast drop unit certain probabl discourag specif co adapt featur detector prevent overfit improv network general besid dropout interpret approxim model aggreg techniqu exponenti number smaller network averag order get power ensembl paper show use fix dropout probabl dure train suboptim choic thus propos time schedul probabl retain neuron network induc adapt regular scheme smooth increas difficulti optim problem idea start easi adapt increas difficulti learn problem root curriculum learn allow one train better model inde prove optim strategi implement veri general curriculum scheme gradual ad nois input intermedi featur represent within network architectur experi seven imag classif dataset differ network architectur show method name curriculum dropout frequent yield better general worst perform well standard dropout method,"['Pietro Morerio', 'Jacopo Cavazza', 'Riccardo Volpi', 'Rene Vidal', 'Vittorio Murino']","['cs.NE', 'cs.LG', 'stat.ML']",False,False,True,False,False,True
1284,2017-03-28T14:02:18Z,2017-03-17T19:24:09Z,http://arxiv.org/abs/1703.06177v1,http://arxiv.org/pdf/1703.06177v1,On Consistency of Graph-based Semi-supervised Learning,consist graph base semi supervis learn,"Graph-based semi-supervised learning is one of the most popular methods in machine learning. Some of its theoretical properties such as bounds for the generalization error and the convergence of the graph Laplacian regularizer have been studied in computer science and statistics literatures. However, a fundamental statistical property, the consistency of the estimator from this method has not been proved. In this article, we study the consistency problem under a non-parametric framework. We prove the consistency of graph-based learning in the case that the estimated scores are enforced to be equal to the observed responses for the labeled data. The sample sizes of both labeled and unlabeled data are allowed to grow in this result. When the estimated scores are not required to be equal to the observed responses, a tuning parameter is used to balance the loss function and the graph Laplacian regularizer. We give a counterexample demonstrating that the estimator for this case can be inconsistent. The theoretical findings are supported by numerical studies.",graph base semi supervis learn one popular method machin learn theoret properti bound general error converg graph laplacian regular studi comput scienc statist literatur howev fundament statist properti consist estim method prove articl studi consist problem non parametr framework prove consist graph base learn case estim score enforc equal observ respons label data sampl size label unlabel data allow grow result estim score requir equal observ respons tune paramet use balanc loss function graph laplacian regular give counterexampl demonstr estim case inconsist theoret find support numer studi,"['Chengan Du', 'Yunpeng Zhao']",['stat.ML'],False,False,True,False,False,False
1287,2017-03-28T14:02:18Z,2017-03-17T17:09:14Z,http://arxiv.org/abs/1703.06103v1,http://arxiv.org/pdf/1703.06103v1,Modeling Relational Data with Graph Convolutional Networks,model relat data graph convolut network,"Knowledge bases play a crucial role in many applications, for example question answering and information retrieval. Despite the great effort invested in creating and maintaining them, even the largest representatives (e.g., Yago, DBPedia or Wikidata) are highly incomplete. We introduce relational graph convolutional networks (R-GCNs) and apply them to two standard knowledge base completion tasks: link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing attributes of entities). R-GCNs are a generalization of graph convolutional networks, a recent class of neural networks operating on graphs, and are developed specifically to deal with highly multi-relational data, characteristic of realistic knowledge bases. Our methods achieve competitive results on standard benchmarks for both tasks.",knowledg base play crucial role mani applic exampl question answer inform retriev despit great effort invest creat maintain even largest repres yago dbpedia wikidata high incomplet introduc relat graph convolut network gcns appli two standard knowledg base complet task link predict recoveri miss fact subject predic object tripl entiti classif recoveri miss attribut entiti gcns general graph convolut network recent class neural network oper graph develop specif deal high multi relat data characterist realist knowledg base method achiev competit result standard benchmark task,"['Michael Schlichtkrull', 'Thomas N. Kipf', 'Peter Bloem', 'Rianne van den Berg', 'Ivan Titov', 'Max Welling']","['stat.ML', 'cs.AI', 'cs.DB', 'cs.LG']",False,False,True,False,False,False
1291,2017-03-28T14:02:22Z,2017-03-16T22:37:55Z,http://arxiv.org/abs/1703.05841v1,http://arxiv.org/pdf/1703.05841v1,Adaptivity to Noise Parameters in Nonparametric Active Learning,adapt nois paramet nonparametr activ learn,"This work addresses various open questions in the theory of active learning for nonparametric classification. Our contributions are both statistical and algorithmic: -We establish new minimax-rates for active learning under common \textit{noise conditions}. These rates display interesting transitions -- due to the interaction between noise \textit{smoothness and margin} -- not present in the passive setting. Some such transitions were previously conjectured, but remained unconfirmed. -We present a generic algorithmic strategy for adaptivity to unknown noise smoothness and margin; our strategy achieves optimal rates in many general situations; furthermore, unlike in previous work, we avoid the need for \textit{adaptive confidence sets}, resulting in strictly milder distributional requirements.",work address various open question theori activ learn nonparametr classif contribut statist algorithm establish new minimax rate activ learn common textit nois condit rate display interest transit due interact nois textit smooth margin present passiv set transit previous conjectur remain unconfirm present generic algorithm strategi adapt unknown nois smooth margin strategi achiev optim rate mani general situat furthermor unlik previous work avoid need textit adapt confid set result strict milder distribut requir,"['Andrea Locatelli', 'Alexandra Carpentier', 'Samory Kpotufe']",['stat.ML'],False,False,True,False,False,False
1294,2017-03-28T14:02:22Z,2017-03-16T16:00:00Z,http://arxiv.org/abs/1703.05687v1,http://arxiv.org/pdf/1703.05687v1,Gaussian process regression for forecasting battery state of health,gaussian process regress forecast batteri state health,"Accurately predicting the future capacity and remaining useful life of batteries is necessary to ensure reliable system operation and to minimise maintenance costs. The complex nature of battery degradation has meant that mechanistic modelling of capacity fade has thus far remained intractable; however, with the advent of cloud-connected devices, data from cells in various applications is becoming increasingly available, and the feasibility of data-driven methods for battery prognostics is increasing. Here we propose Gaussian process (GP) regression for forecasting battery state of health, and highlight various advantages of GPs over other data-driven and mechanistic approaches. GPs are a type of Bayesian non-parametric method, and hence can model complex systems whilst handling uncertainty in a principled manner. Prior information can be exploited by GPs in a variety of ways: explicit mean functions can be used if the functional form of the underlying degradation model is available, and multiple-output GPs can effectively exploit correlations between data from different cells. We demonstrate the predictive capability of GPs for short-term and long-term (remaining useful life) forecasting on a selection of capacity vs. cycle datasets from lithium-ion cells.",accur predict futur capac remain use life batteri necessari ensur reliabl system oper minimis mainten cost complex natur batteri degrad meant mechanist model capac fade thus far remain intract howev advent cloud connect devic data cell various applic becom increas avail feasibl data driven method batteri prognost increas propos gaussian process gp regress forecast batteri state health highlight various advantag gps data driven mechanist approach gps type bayesian non parametr method henc model complex system whilst handl uncertainti principl manner prior inform exploit gps varieti way explicit mean function use function form degrad model avail multipl output gps effect exploit correl data differ cell demonstr predict capabl gps short term long term remain use life forecast select capac vs cycl dataset lithium ion cell,"['Robert R. Richardson', 'Michael A. Osborne', 'David A. Howey']","['stat.AP', 'stat.ML', '62P30', 'J.2; G.3']",False,False,True,False,False,False
1301,2017-03-28T14:04:33Z,2017-03-25T04:44:47Z,http://arxiv.org/abs/1703.08648v1,http://arxiv.org/pdf/1703.08648v1,The new concepts of measurement error's regularities and effect   characteristics,new concept measur error regular effect characterist,"In several literatures, the authors give a kind of thinking of measurement theory system based on error non-classification philosophy, which completely overthrows the existing measurement concepts system of precision, trueness and accuracy. In this paper, aiming at the issues of error's regularities and effect characteristics, the authors will do a thematic explanation, and prove that the error's regularities actually come from different cognitive perspectives, is also unable to be used for classifying errors, and that the error's effect characteristics actually depend on artificial condition rules of repeated measurement, and is still unable to be used for classifying errors. Thus, from the perspectives of error's regularities and effect characteristics, the existing error classification philosophy is still a mistake, an uncertainty concept system, which must be interpreted by the error non-classification philosophy, naturally become the only way out of measurement theory.",sever literatur author give kind think measur theori system base error non classif philosophi complet overthrow exist measur concept system precis trueness accuraci paper aim issu error regular effect characterist author themat explan prove error regular actual come differ cognit perspect also unabl use classifi error error effect characterist actual depend artifici condit rule repeat measur still unabl use classifi error thus perspect error regular effect characterist exist error classif philosophi still mistak uncertainti concept system must interpret error non classif philosophi natur becom onli way measur theori,"['Xiaoming Ye', 'Haibo Liu', 'Mo Ling', 'Xuebin Xiao']","['math.ST', 'stat.TH']",False,False,True,False,False,False
1302,2017-03-28T14:04:33Z,2017-03-24T20:59:52Z,http://arxiv.org/abs/1703.08596v1,http://arxiv.org/pdf/1703.08596v1,The Inner Structure of Time-Dependent Signals,inner structur time depend signal,"This paper shows how a time series of measurements of an evolving system can be processed to create an inner time series that is unaffected by any instantaneous invertible, possibly nonlinear transformation of the measurements. An inner time series contains information that does not depend on the nature of the sensors, which the observer chose to monitor the system. Instead, it encodes information that is intrinsic to the evolution of the observed system. Because of its sensor-independence, an inner time series may produce fewer false negatives when it is used to detect events in the presence of sensor drift. Furthermore, if the observed physical system is comprised of non-interacting subsystems, its inner time series is separable; i.e., it consists of a collection of time series, each one being the inner time series of an isolated subsystem. Because of this property, an inner time series can be used to detect a specific behavior of one of the independent subsystems without using blind source separation to disentangle that subsystem from the others. The method is illustrated by applying it to: 1) an analytic example; 2) the audio waveform of one speaker; 3) video images from a moving camera; 4) mixtures of audio waveforms of two speakers.",paper show time seri measur evolv system process creat inner time seri unaffect ani instantan invert possibl nonlinear transform measur inner time seri contain inform doe depend natur sensor observ chose monitor system instead encod inform intrins evolut observ system becaus sensor independ inner time seri may produc fewer fals negat use detect event presenc sensor drift furthermor observ physic system compris non interact subsystem inner time seri separ consist collect time seri one inner time seri isol subsystem becaus properti inner time seri use detect specif behavior one independ subsystem without use blind sourc separ disentangl subsystem method illustr appli analyt exampl audio waveform one speaker video imag move camera mixtur audio waveform two speaker,['David N. Levin'],"['stat.ME', 'cs.SD', 'math.ST', 'stat.TH']",False,False,True,False,False,False
1304,2017-03-28T14:04:33Z,2017-03-24T16:08:21Z,http://arxiv.org/abs/1703.08487v1,http://arxiv.org/pdf/1703.08487v1,Multiscale Granger causality,multiscal granger causal,"In the study of complex physical and biological systems represented by multivariate stochastic processes, an issue of great relevance is the description of the system dynamics spanning multiple temporal scales. While methods to assess the dynamic complexity of individual processes at different time scales are well-established, the multiscale evaluation of directed interactions between processes is complicated by theoretical and practical issues such as filtering and downsampling. Here we extend the very popular measure of Granger causality (GC), a prominent tool for assessing directed lagged interactions between joint processes, to quantify information transfer across multiple time scales. We show that the multiscale processing of a vector autoregressive (AR) process introduces a moving average (MA) component, and describe how to represent the resulting ARMA process using state space (SS) models and to combine the SS model parameters for computing exact GC values at arbitrarily large time scales. We exploit the theoretical formulation to identify peculiar features of multiscale GC in basic AR processes, and demonstrate with numerical simulations the much larger estimation accuracy of the SS approach compared with pure AR modeling of filtered and downsampled data. The improved computational reliability is exploited to disclose meaningful multiscale patterns of information transfer between global temperature and carbon dioxide concentration time series, both in paleoclimate and in recent years.",studi complex physic biolog system repres multivari stochast process issu great relev descript system dynam span multipl tempor scale method assess dynam complex individu process differ time scale well establish multiscal evalu direct interact process complic theoret practic issu filter downsampl extend veri popular measur granger causal gc promin tool assess direct lag interact joint process quantifi inform transfer across multipl time scale show multiscal process vector autoregress ar process introduc move averag compon describ repres result arma process use state space ss model combin ss model paramet comput exact gc valu arbitrarili larg time scale exploit theoret formul identifi peculiar featur multiscal gc basic ar process demonstr numer simul much larger estim accuraci ss approach compar pure ar model filter downsampl data improv comput reliabl exploit disclos meaning multiscal pattern inform transfer global temperatur carbon dioxid concentr time seri paleoclim recent year,"['Luca Faes', 'Giandomenico Nollo', 'Sebastiano Stramaglia', 'Daniele Marinazzo']","['stat.ME', 'math.ST', 'stat.AP', 'stat.TH']",False,False,True,False,False,False
1305,2017-03-28T14:04:33Z,2017-03-24T11:16:37Z,http://arxiv.org/abs/1703.08358v1,http://arxiv.org/pdf/1703.08358v1,Nonparametric Bayesian analysis for support boundary recovery,nonparametr bayesian analysi support boundari recoveri,"Given a sample of a Poisson point process with intensity $\lambda_f(x,y) = n \mathbf{1}(f(x) \leq y),$ we study recovery of the boundary function $f$ from a nonparametric Bayes perspective. Because of the irregularity of this model, the analysis is non-standard. We derive contraction rates with respect to the $L^1$-norm for several classes of priors, including Gaussian priors, priors based on (truncated) random series, compound Poisson processes, and subordinators. We also investigate the limiting shape of the posterior distribution and derive a nonparametric version of the Bernstein-von Mises theorem for a specific class of priors on a function space with increasing parameter dimension. We show that the marginal posterior of the functional $\vartheta =\int f$ does some automatic bias correction and contracts with a faster rate than the MLE. In this case, $1-\alpha$-credible sets are also asymptotic $1-\alpha$ confidence intervals. It is also shown that the frequentist coverage of credible sets is lost under model misspecification.",given sampl poisson point process intens lambda mathbf leq studi recoveri boundari function nonparametr bay perspect becaus irregular model analysi non standard deriv contract rate respect norm sever class prior includ gaussian prior prior base truncat random seri compound poisson process subordin also investig limit shape posterior distribut deriv nonparametr version bernstein von mise theorem specif class prior function space increas paramet dimens show margin posterior function vartheta int doe automat bias correct contract faster rate mle case alpha credibl set also asymptot alpha confid interv also shown frequentist coverag credibl set lost model misspecif,"['Markus Reiss', 'Johannes Schmidt-Hieber']","['math.ST', 'stat.TH', '62G05, 62G08']",False,False,True,False,False,True
1309,2017-03-28T14:04:33Z,2017-03-22T18:32:47Z,http://arxiv.org/abs/1703.07809v1,http://arxiv.org/pdf/1703.07809v1,Empirical Risk Minimization as Parameter Choice Rule for General Linear   Regularization Methods,empir risk minim paramet choic rule general linear regular method,"We consider the statistical inverse problem to recover $f$ from noisy measurements $Y = Tf + \sigma \xi$ where $\xi$ is Gaussian white noise and $T$ a compact operator between Hilbert spaces. Considering general reconstruction methods of the form $\hat f_\alpha = q_\alpha \left(T^*T\right)T^*Y$ with an ordered filter $q_\alpha$, we investigate the choice of the regularization parameter $\alpha$ by minimizing an unbiased estimate of the predictive risk $\mathbb E\left[\Vert Tf - T\hat f_\alpha\Vert^2\right]$. The corresponding parameter $\alpha_{\mathrm{pred}}$ and its usage are well-known in the literature, but oracle inequalities and optimality results in this general setting are unknown. We prove a (generalized) oracle inequality, which relates the direct risk $\mathbb E\left[\Vert f - \hat f_{\alpha_{\mathrm{pred}}}\Vert^2\right]$ with the oracle prediction risk $\inf_{\alpha>0}\mathbb E\left[\Vert Tf - T\hat f_{\alpha}\Vert^2\right]$. From this oracle inequality we are then able to conclude that the investigated parameter choice rule is of optimal order.   Finally we also present numerical simulations, which support the order optimality of the method and the quality of the parameter choice in finite sample situations.",consid statist invers problem recov noisi measur tf sigma xi xi gaussian white nois compact oper hilbert space consid general reconstruct method form hat alpha alpha left right order filter alpha investig choic regular paramet alpha minim unbias estim predict risk mathbb left vert tf hat alpha vert right correspond paramet alpha mathrm pred usag well known literatur oracl inequ optim result general set unknown prove general oracl inequ relat direct risk mathbb left vert hat alpha mathrm pred vert right oracl predict risk inf alpha mathbb left vert tf hat alpha vert right oracl inequ abl conclud investig paramet choic rule optim order final also present numer simul support order optim method qualiti paramet choic finit sampl situat,"['Housen Li', 'Frank Werner']","['math.NA', 'math.ST', 'stat.TH', 'Primary 62G05, Secondary 62G20, 65J22, 65J20']",False,False,True,False,False,True
1313,2017-03-28T14:04:37Z,2017-03-21T06:24:47Z,http://arxiv.org/abs/1703.07072v1,http://arxiv.org/pdf/1703.07072v1,Bayesian Nonparametric Inference for M/G/1 Queueing Systems,bayesian nonparametr infer queue system,"In this work, nonparametric statistical inference is provided for the continuous-time M/G/1 queueing model from a Bayesian point of view. The inference is based on observations of the inter-arrival and service times. Beside other characteristics of the system, particular interest is in the waiting time distribution which is not accessible in closed form. Thus, we use an indirect statistical approach by exploiting the Pollaczek-Khinchine transform formula for the Laplace transform of the waiting time distribution. Due to this, an estimator is defined and its frequentist validation in terms of posterior consistency and posterior normality is studied. It will turn out that we can hereby make inference for the observables separately and compose the results subsequently by suitable techniques.",work nonparametr statist infer provid continu time queue model bayesian point view infer base observ inter arriv servic time besid characterist system particular interest wait time distribut access close form thus use indirect statist approach exploit pollaczek khinchin transform formula laplac transform wait time distribut due estim defin frequentist valid term posterior consist posterior normal studi turn herebi make infer observ separ compos result subsequ suitabl techniqu,"['Cornelia Wichelhaus', 'Moritz von Rohrscheidt']","['math.ST', 'stat.TH']",False,False,True,False,False,True
1317,2017-03-28T14:04:37Z,2017-03-18T23:22:23Z,http://arxiv.org/abs/1703.06367v1,http://arxiv.org/pdf/1703.06367v1,Optimal Learning from Multiple Information Sources,optim learn multipl inform sourc,"Decision-makers often learn by acquiring information from distinct sources that possibly provide complementary information. We consider a decision-maker who sequentially samples from a finite set of Gaussian signals, and wants to predict a persistent multi-dimensional state at an unknown final period. What signal should he choose to observe in each period? Related problems about optimal experimentation and dynamic learning tend to have solutions that can only be approximated or implicitly characterized. In contrast, we find that in our problem, the dynamically optimal path of signal acquisitions generically: (1) eventually coincides at every period with the myopic path of signal acquisitions, and (2) eventually achieves ""total optimality,"" so that at every large period, the decision-maker will not want to revise his previous signal acquisitions, even if given this opportunity. In special classes of environments that we describe, these properties attain not only eventually, but from period 1. Finally, we characterize the asymptotic frequency with which each signal is chosen, and how this depends on primitives of the informational environment.",decis maker often learn acquir inform distinct sourc possibl provid complementari inform consid decis maker sequenti sampl finit set gaussian signal want predict persist multi dimension state unknown final period signal choos observ period relat problem optim experiment dynam learn tend solut onli approxim implicit character contrast find problem dynam optim path signal acquisit generic eventu coincid everi period myopic path signal acquisit eventu achiev total optim everi larg period decis maker want revis previous signal acquisit even given opportun special class environ describ properti attain onli eventu period final character asymptot frequenc signal chosen depend primit inform environ,"['Annie Liang', 'Xiaosheng Mu', 'Vasilis Syrgkanis']","['cs.GT', 'cs.LG', 'math.ST', 'stat.TH']",False,False,True,False,False,False
1318,2017-03-28T14:04:37Z,2017-03-18T19:00:23Z,http://arxiv.org/abs/1703.06336v1,http://arxiv.org/pdf/1703.06336v1,Analysis of error control in large scale two-stage multiple hypothesis   testing,analysi error control larg scale two stage multipl hypothesi test,"When dealing with the problem of simultaneously testing a large number of null hypotheses, a natural testing strategy is to first reduce the number of tested hypotheses by some selection (screening or filtering) process, and then to simultaneously test the selected hypotheses. The main advantage of this strategy is to greatly reduce the severe effect of high dimensions. However, the first screening or selection stage must be properly accounted for in order to maintain some type of error control. In this paper, we will introduce a selection rule based on a selection statistic that is independent of the test statistic when the tested hypothesis is true. Combining this selection rule and the conventional Bonferroni procedure, we can develop a powerful and valid two-stage procedure. The introduced procedure has several nice properties: (i) it completely removes the selection effect; (ii) it reduces the multiplicity effect; (iii) it does not ""waste"" data while carrying out both selection and testing. Asymptotic power analysis and simulation studies illustrate that this proposed method can provide higher power compared to usual multiple testing methods while controlling the Type 1 error rate. Optimal selection thresholds are also derived based on our asymptotic analysis.",deal problem simultan test larg number null hypothes natur test strategi first reduc number test hypothes select screen filter process simultan test select hypothes main advantag strategi great reduc sever effect high dimens howev first screen select stage must proper account order maintain type error control paper introduc select rule base select statist independ test statist test hypothesi true combin select rule convent bonferroni procedur develop power valid two stage procedur introduc procedur sever nice properti complet remov select effect ii reduc multipl effect iii doe wast data carri select test asymptot power analysi simul studi illustr propos method provid higher power compar usual multipl test method control type error rate optim select threshold also deriv base asymptot analysi,"['Wenge Guo', 'Joseph P. Romano']","['stat.ME', 'math.ST', 'stat.TH', '62J15']",False,False,True,False,False,False
1320,2017-03-28T14:04:42Z,2017-03-16T21:04:10Z,http://arxiv.org/abs/1703.05757v1,http://arxiv.org/pdf/1703.05757v1,The Beta Flexible Weibull Distribution,beta flexibl weibul distribut,"We introduce in this paper a new generalization of the flexible Weibull distribution with four parameters. This model based on the Beta generalized (BG) distribution, Eugene et al. \cite{Eugeneetal2002}, they first using the BG distribution for generating new generalizations. This new model is called the beta flexible Weibull BFW distribution. Some statistical properties such as the mode, the $r$th moment, skewness and kurtosis are derived. The moment generating function and the order statistics are obtained. Moreover, the estimations of the parameters are given by maximum likelihood method and the Fisher's information matrix is derived. Finally, we study the advantage of the BFW distribution by an application using real data set.",introduc paper new general flexibl weibul distribut four paramet model base beta general bg distribut eugen et al cite eugeneet first use bg distribut generat new general new model call beta flexibl weibul bfw distribut statist properti mode th moment skew kurtosi deriv moment generat function order statist obtain moreov estim paramet given maximum likelihood method fisher inform matrix deriv final studi advantag bfw distribut applic use real data set,"['Beih S. El-Desouky', 'Abdelfattah Mustafa', 'Shamsan AL-Garash']","['math.ST', 'stat.TH', '60E05, 62N02, 62N03']",False,False,True,False,False,False
1321,2017-03-28T14:04:42Z,2017-03-16T13:48:45Z,http://arxiv.org/abs/1703.05619v1,http://arxiv.org/pdf/1703.05619v1,Nonparametric intensity estimation from indirect point process   observations under unknown error distribution,nonparametr intens estim indirect point process observ unknown error distribut,"We consider the nonparametric estimation of the intensity function of a Poisson point process in a circular model from indirect observations $N_1,\ldots,N_n$. These observations emerge from hidden point process realizations with the target intensity through contamination with additive error. Under the assumption that the error distribution is unknown and only available by means of an additional sample $Y_1,\ldots,Y_m$ we derive minimax rates of convergence with respect to the sample sizes $n$ and $m$ under abstract smoothness conditions and propose an orthonormal series estimator which attains the optimal rate of convergence. The performance of the estimator depends on the correct specification of a dimension parameter whose optimal choice relies on smoothness characteristics of both the intensity and the error density. Since a priori knowledge of such characteristics is a too strong assumption, we propose a data-driven choice of the dimension parameter based on model selection and show that the adaptive estimator either attains the minimax optimal rate or is suboptimal only by a logarithmic factor.",consid nonparametr estim intens function poisson point process circular model indirect observ ldot observ emerg hidden point process realize target intens contamin addit error assumpt error distribut unknown onli avail mean addit sampl ldot deriv minimax rate converg respect sampl size abstract smooth condit propos orthonorm seri estim attain optim rate converg perform estim depend correct specif dimens paramet whose optim choic reli smooth characterist intens error densiti sinc priori knowledg characterist strong assumpt propos data driven choic dimens paramet base model select show adapt estim either attain minimax optim rate suboptim onli logarithm factor,['Martin Kroll'],"['math.ST', 'stat.TH', '62G05, 60G55']",False,False,True,False,False,True
1323,2017-03-28T14:04:42Z,2017-03-15T06:34:38Z,http://arxiv.org/abs/1703.04956v1,http://arxiv.org/pdf/1703.04956v1,A Short Note on Almost Sure Convergence of Bayes Factors in the General   Set-Up,short note almost sure converg bay factor general set,"In this article we derive the almost sure convergence theory of Bayes factor in the general set-up that includes even dependent data and misspecified models, as a simple application of a result of Shalizi (2009) to a well-known identity satisfied by the Bayes factor.",articl deriv almost sure converg theori bay factor general set includ even depend data misspecifi model simpl applic result shalizi well known ident satisfi bay factor,"['Debashis Chatterjee', 'Trisha Maitra', 'Sourabh Bhattacharya']","['math.ST', 'stat.ME', 'stat.TH']",False,False,True,False,False,False
1324,2017-03-28T14:04:42Z,2017-03-15T06:26:43Z,http://arxiv.org/abs/1703.04955v1,http://arxiv.org/pdf/1703.04955v1,Theoretical Limits of Record Linkage and Microclustering,theoret limit record linkag microclust,"There has been substantial recent interest in record linkage, attempting to group the records pertaining to the same entities from a large database lacking unique identifiers. This can be viewed as a type of ""microclustering,"" with few observations per cluster and a very large number of clusters. A variety of methods have been proposed, but there is a lack of literature providing theoretical guarantees on performance. We show that the problem is fundamentally hard from a theoretical perspective, and even in idealized cases, accurate entity resolution is effectively impossible when the number of entities is small relative to the number of records and/or the separation among records from different entities is not extremely large. To characterize the fundamental difficulty, we focus on entity resolution based on multivariate Gaussian mixture models, but our conclusions apply broadly and are supported by simulation studies inspired by human rights applications. These results suggest conservatism in interpretation of the results of record linkage, support collection of additional data to more accurately disambiguate the entities, and motivate a focus on coarser inference. For example, results from a simulation study suggest that sometimes one may obtain accurate results for population size estimation even when fine scale entity resolution is inaccurate.",substanti recent interest record linkag attempt group record pertain entiti larg databas lack uniqu identifi view type microclust observ per cluster veri larg number cluster varieti method propos lack literatur provid theoret guarante perform show problem fundament hard theoret perspect even ideal case accur entiti resolut effect imposs number entiti small relat number record separ among record differ entiti extrem larg character fundament difficulti focus entiti resolut base multivari gaussian mixtur model conclus appli broad support simul studi inspir human right applic result suggest conservat interpret result record linkag support collect addit data accur disambigu entiti motiv focus coarser infer exampl result simul studi suggest sometim one may obtain accur result popul size estim even fine scale entiti resolut inaccur,"['James E. Johndrow', 'Kristian Lum', 'David B. Dunson']","['math.ST', 'stat.TH']",False,False,True,False,False,True
1326,2017-03-28T14:04:42Z,2017-03-14T22:49:50Z,http://arxiv.org/abs/1703.04799v1,http://arxiv.org/pdf/1703.04799v1,Multi-parameter One-Sided Monitoring Test,multi paramet one side monitor test,"Multi-parameter one-sided hypothesis test problems arise naturally in many applications. We are particularly interested in effective tests for monitoring multiple quality indices in forestry products. Our search reveals that there are many effective statistical methods in the literature for normal data, and that they can easily be adapted for non-normal data. We find that the beautiful likelihood ratio test is unsatisfactory, because in order to control the size, it must cope with the least favorable distributions at the cost of power. In this paper, we find a novel way to slightly ease the size control, obtaining a much more powerful test. Simulation confirms that the new test retains good control of the type I error and is markedly more powerful than the likelihood ratio test as well as many competitors based on normal data. The new method performs well in the context of monitoring multiple quality indices.",multi paramet one side hypothesi test problem aris natur mani applic particular interest effect test monitor multipl qualiti indic forestri product search reveal mani effect statist method literatur normal data easili adapt non normal data find beauti likelihood ratio test unsatisfactori becaus order control size must cope least favor distribut cost power paper find novel way slight eas size control obtain much power test simul confirm new test retain good control type error mark power likelihood ratio test well mani competitor base normal data new method perform well context monitor multipl qualiti indic,"['Guangyu Zhu', 'Jiahua Chen']","['math.ST', 'stat.TH']",False,False,True,False,False,True
1332,2017-03-28T14:04:46Z,2017-03-13T10:34:44Z,http://arxiv.org/abs/1703.04320v1,http://arxiv.org/pdf/1703.04320v1,Fourier analysis of serial dependence measures,fourier analysi serial depend measur,"Classical spectral analysis is based on the discrete Fourier transform of the auto-covariances. In this paper we investigate the asymptotic properties of new frequency domain methods where the auto-covariances in the spectral density are replaced by alternative dependence measures which can be estimated by U-statistics. An interesting example is given by Kendall{'}s $\tau$ , for which the limiting variance exhibits a surprising behavior.",classic spectral analysi base discret fourier transform auto covari paper investig asymptot properti new frequenc domain method auto covari spectral densiti replac altern depend measur estim statist interest exampl given kendal tau limit varianc exhibit surpris behavior,"['Ria van Hecke', 'Stanislav Volgushev', 'Holger Dette']","['math.ST', 'stat.TH']",False,False,True,False,False,False
1335,2017-03-28T14:04:46Z,2017-03-10T13:43:06Z,http://arxiv.org/abs/1703.03680v1,http://arxiv.org/pdf/1703.03680v1,Strong convergence rates of probabilistic integrators for ordinary   differential equations,strong converg rate probabilist integr ordinari differenti equat,"Probabilistic integration of a continuous dynamical system is a way of systematically introducing model error, at scales no larger than errors inroduced by standard numerical discretisation, in order to enable thorough exploration of possible responses of the system to inputs. It is thus a potentially useful approach in a number of applications such as forward uncertainty quantification, inverse problems, and data assimilation. We extend the convergence analysis of probabilistic integrators for deterministic ordinary differential equations, as proposed by Conrad et al. (Stat. Comput., 2016), to establish mean-square convergence in the uniform norm on discrete- or continuous-time solutions under relaxed regularity assumptions on the driving vector fields and their induced flows. Specifically, we show that randomised high-order integrators for globally Lipschitz flows and randomised Euler integrators for dissipative vector fields with polynomially-bounded local Lipschitz constants all have the same mean-square convergence rate as their deterministic counterparts, provided that the variance of the integration noise is not of higher order than the corresponding deterministic integrator.",probabilist integr continu dynam system way systemat introduc model error scale larger error inroduc standard numer discretis order enabl thorough explor possibl respons system input thus potenti use approach number applic forward uncertainti quantif invers problem data assimil extend converg analysi probabilist integr determinist ordinari differenti equat propos conrad et al stat comput establish mean squar converg uniform norm discret continu time solut relax regular assumpt drive vector field induc flow specif show randomis high order integr global lipschitz flow randomis euler integr dissip vector field polynomi bound local lipschitz constant mean squar converg rate determinist counterpart provid varianc integr nois higher order correspond determinist integr,"['H. C. Lie', 'A. M. Stuart', 'T. J. Sullivan']","['math.NA', 'math.PR', 'math.ST', 'stat.CO', 'stat.TH', '65L20, 65C99, 37H10, 68W20']",False,False,True,False,False,False
1338,2017-03-28T14:04:46Z,2017-03-09T17:18:21Z,http://arxiv.org/abs/1703.03353v1,http://arxiv.org/pdf/1703.03353v1,A Note on Bayesian Model Selection for Discrete Data Using Proper   Scoring Rules,note bayesian model select discret data use proper score rule,"We consider the problem of choosing between parametric models for a discrete observable, taking a Bayesian approach in which the within-model prior distributions are allowed to be improper. In order to avoid the ambiguity in the marginal likelihood function in such a case, we apply a homogeneous scoring rule. For the particular case of distinguishing between Poisson and Negative Binomial models, we conduct simulations that indicate that, applied prequentially, the method will consistently select the true model.",consid problem choos parametr model discret observ take bayesian approach within model prior distribut allow improp order avoid ambigu margin likelihood function case appli homogen score rule particular case distinguish poisson negat binomi model conduct simul indic appli prequenti method consist select true model,"['A. Philip Dawid', 'Monica Musio', 'Silvia Columbu']","['math.ST', 'stat.TH', 'Primary 62C99, secondary 62F15, 62A99']",False,False,True,False,False,True
1343,2017-03-28T14:04:50Z,2017-03-08T21:03:22Z,http://arxiv.org/abs/1703.03031v1,http://arxiv.org/pdf/1703.03031v1,Statistical Inference on Panel Data Models: A Kernel Ridge Regression   Method,statist infer panel data model kernel ridg regress method,"We propose statistical inferential procedures for panel data models with interactive fixed effects in a kernel ridge regression framework.Compared with traditional sieve methods, our method is automatic in the sense that it does not require the choice of basis functions and truncation parameters.Model complexity is controlled by a continuous regularization parameter which can be automatically selected by generalized cross validation. Based on empirical processes theory and functional analysis tools, we derive joint asymptotic distributions for the estimators in the heterogeneous setting. These joint asymptotic results are then used to construct confidence intervals for the regression means and prediction intervals for the future observations, both being the first provably valid intervals in literature. Marginal asymptotic normality of the functional estimators in homogeneous setting is also obtained. Simulation and real data analysis demonstrate the advantages of our method.",propos statist inferenti procedur panel data model interact fix effect kernel ridg regress framework compar tradit siev method method automat sens doe requir choic basi function truncat paramet model complex control continu regular paramet automat select general cross valid base empir process theori function analysi tool deriv joint asymptot distribut estim heterogen set joint asymptot result use construct confid interv regress mean predict interv futur observ first provabl valid interv literatur margin asymptot normal function estim homogen set also obtain simul real data analysi demonstr advantag method,"['Shunan Zhao', 'Ruiqi Liu', 'Zuofeng Shang']","['math.ST', 'stat.TH']",False,False,True,False,False,False
1347,2017-03-28T14:04:50Z,2017-03-08T06:22:56Z,http://arxiv.org/abs/1703.02724v1,http://arxiv.org/pdf/1703.02724v1,Guaranteed Tensor PCA with Optimality in Statistics and Computation,guarante tensor pca optim statist comput,"Tensors, or high-order arrays, attract much attention in recent research. In this paper, we propose a general framework for tensor principal component analysis (tensor PCA), which focuses on the methodology and theory for extracting the hidden low-rank structure from the high-dimensional tensor data. A unified solution is provided for tensor PCA with considerations in both statistical limits and computational costs. The problem exhibits three different phases according to the signal-noise-ratio (SNR). In particular, with strong SNR, we propose a fast spectral power iteration method that achieves the minimax optimal rate of convergence in estimation; with weak SNR, the information-theoretical lower bound shows that it is impossible to have consistent estimation in general; with moderate SNR, we show that the non-convex maximum likelihood estimation provides optimal solution, but with NP-hard computational cost; moreover, under the hardness hypothesis of hypergraphic planted clique detection, there are no polynomial-time algorithms performing consistently in general. Simulation studies show that the proposed spectral power iteration method have good performance under a variety of settings.",tensor high order array attract much attent recent research paper propos general framework tensor princip compon analysi tensor pca focus methodolog theori extract hidden low rank structur high dimension tensor data unifi solut provid tensor pca consider statist limit comput cost problem exhibit three differ phase accord signal nois ratio snr particular strong snr propos fast spectral power iter method achiev minimax optim rate converg estim weak snr inform theoret lower bound show imposs consist estim general moder snr show non convex maximum likelihood estim provid optim solut np hard comput cost moreov hard hypothesi hypergraph plant cliqu detect polynomi time algorithm perform consist general simul studi show propos spectral power iter method good perform varieti set,"['Anru Zhang', 'Dong Xia']","['math.ST', 'cs.LG', 'stat.ME', 'stat.ML', 'stat.TH']",False,False,True,False,False,True
1351,2017-03-28T14:04:54Z,2017-03-07T16:38:11Z,http://arxiv.org/abs/1703.02462v1,http://arxiv.org/pdf/1703.02462v1,Convex and non-convex regularization methods for spatial point processes   intensity estimation,convex non convex regular method spatial point process intens estim,"This paper deals with feature selection procedures for spatial point processes intensity estimation. We consider regularized versions of estimating equations based on Campbell theorem derived from two classical functions: Poisson likelihood and logistic regression likelihood. We provide general conditions on the spatial point processes and on penalty functions which ensure consistency, sparsity and asymptotic normality. We discuss the numerical implementation and assess finite sample properties in a simulation study. Finally, an application to tropical forestry datasets illustrates the use of the proposed methods.",paper deal featur select procedur spatial point process intens estim consid regular version estim equat base campbel theorem deriv two classic function poisson likelihood logist regress likelihood provid general condit spatial point process penalti function ensur consist sparsiti asymptot normal discuss numer implement assess finit sampl properti simul studi final applic tropic forestri dataset illustr use propos method,"['Achmad Choiruddin', 'Jean-François Coeurjolly', 'Frédérique Letué']","['stat.ME', 'math.ST', 'stat.TH']",False,False,True,False,False,False
1353,2017-03-28T14:04:54Z,2017-03-13T15:44:35Z,http://arxiv.org/abs/1703.02307v2,http://arxiv.org/pdf/1703.02307v2,Post hoc inference via joint family-wise error rate control,post hoc infer via joint famili wise error rate control,"We introduce a general methodology for post hoc inference in a large-scale multiple testing framework. The approach is called "" user-agnostic "" in the sense that the statistical guarantee on the number of correct rejections holds for any set of candidate items selected by the user (after having seen the data). This task is investigated by defining a suitable criterion, named the joint-family-wise-error rate (JER for short). We propose several procedures for controlling the JER, with a special focus on incorporating dependencies while adapting to the unknown quantity of signal (via a step-down approach). We show that our proposed setting incorporates as particular cases a version of the higher criticism as well as the closed testing based approach of Goeman and Solari (2011). Our theoretical statements are supported by numerical experiments.",introduc general methodolog post hoc infer larg scale multipl test framework approach call user agnost sens statist guarante number correct reject hold ani set candid item select user seen data task investig defin suitabl criterion name joint famili wise error rate jer short propos sever procedur control jer special focus incorpor depend adapt unknown quantiti signal via step approach show propos set incorpor particular case version higher critic well close test base approach goeman solari theoret statement support numer experi,"['Gilles Blanchard', 'Pierre Neuvial', 'Etienne Roquain']","['math.ST', 'stat.TH']",False,False,True,False,False,True
1354,2017-03-28T14:04:54Z,2017-03-07T07:44:52Z,http://arxiv.org/abs/1703.02251v1,http://arxiv.org/pdf/1703.02251v1,The Maximum Likelihood Degree of Toric Varieties,maximum likelihood degre toric varieti,"We study the maximum likelihood degree (ML degree) of toric varieties, known as discrete exponential models in statistics. By introducing scaling coefficients to the monomial parameterization of the toric variety, one can change the ML degree. We show that the ML degree is equal to the degree of the toric variety for generic scalings, while it drops if and only if the scaling vector is in the locus of the principal $A$-determinant. We also illustrate how to compute the ML estimate of a toric variety numerically via homotopy continuation from a scaled toric variety with low ML degree. Throughout, we include examples motivated by algebraic geometry and statistics. We compute the ML degree of rational normal scrolls and a large class of Veronese-type varieties. In addition, we investigate the ML degree of scaled Segre varieties, hierarchical loglinear models, and graphical models.",studi maximum likelihood degre ml degre toric varieti known discret exponenti model statist introduc scale coeffici monomi parameter toric varieti one chang ml degre show ml degre equal degre toric varieti generic scale drop onli scale vector locus princip determin also illustr comput ml estim toric varieti numer via homotopi continu scale toric varieti low ml degre throughout includ exampl motiv algebra geometri statist comput ml degre ration normal scroll larg class verones type varieti addit investig ml degre scale segr varieti hierarch loglinear model graphic model,"['Carlos Améndola', 'Nathan Bliss', 'Isaac Burke', 'Courtney R. Gibbons', 'Martin Helmer', 'Serkan Hoşten', 'Evan D. Nash', 'Jose Israel Rodriguez', 'Daniel Smolkin']","['math.AG', 'math.ST', 'stat.CO', 'stat.TH', '14Q15, 14M25, 13P15, 62F10']",False,False,True,False,False,True
1357,2017-03-28T14:04:54Z,2017-03-06T04:31:36Z,http://arxiv.org/abs/1703.01721v1,http://arxiv.org/pdf/1703.01721v1,The Bennett-Orlicz norm,bennett orlicz norm,"Lederer and van de Geer (2013) introduced a new Orlicz norm, the Bernstein-Orlicz norm, which is connected to Bernstein type inequalities. Here we introduce another Orlicz norm, the Bennett-Orlicz norm, which is connected to Bennett type inequalities. The new Bennett-Orlicz norm yields inequalities for expectations of maxima which are potentially somewhat tighter than those resulting from the Bernstein-Orlicz norm when they are both applicable. We discuss cross connections between these norms, exponential inequalities of the Bernstein, Bennett, and Prokhorov types, and make comparisons with results of Talagrand (1989, 1994), and Boucheron, Lugosi, and Massart (2013).",leder van de geer introduc new orlicz norm bernstein orlicz norm connect bernstein type inequ introduc anoth orlicz norm bennett orlicz norm connect bennett type inequ new bennett orlicz norm yield inequ expect maxima potenti somewhat tighter result bernstein orlicz norm applic discuss cross connect norm exponenti inequ bernstein bennett prokhorov type make comparison result talagrand boucheron lugosi massart,['Jon A. Wellner'],"['math.ST', 'stat.TH', '60E15, 62E17, 62H10, 46E30']",False,False,True,False,False,True
1358,2017-03-28T14:04:54Z,2017-03-05T20:09:44Z,http://arxiv.org/abs/1703.01658v1,http://arxiv.org/pdf/1703.01658v1,The wrapping hull and a unified framework for estimating the volume of a   body,wrap hull unifi framework estim volum bodi,"This paper develops a unified framework for estimating the volume of a set in $\mathbb{R}^d$ based on observations of points uniformly distributed over the set. The framework applies to all classes of sets satisfying one simple axiom: a class is assumed to be intersection stable. No further hypotheses on the boundary of the set are imposed; in particular, the convex sets and the so-called weakly-convex sets are covered by the framework. The approach rests upon a homogeneous Poisson point process model. We introduce the so-called wrapping hull, a generalization of the convex hull, and prove that it is a sufficient and complete statistic. The proposed estimator of the volume is simply the volume of the wrapping hull scaled with an appropriate factor. It is shown to be consistent for all classes of sets satisfying the axiom and mimics an unbiased estimator with uniformly minimal variance. The construction and proofs hinge upon an interplay between probabilistic and geometric arguments. The tractability of the framework is numerically confirmed in a variety of examples.",paper develop unifi framework estim volum set mathbb base observ point uniform distribut set framework appli class set satisfi one simpl axiom class assum intersect stabl hypothes boundari set impos particular convex set call weak convex set cover framework approach rest upon homogen poisson point process model introduc call wrap hull general convex hull prove suffici complet statist propos estim volum simpli volum wrap hull scale appropri factor shown consist class set satisfi axiom mimic unbias estim uniform minim varianc construct proof hing upon interplay probabilist geometr argument tractabl framework numer confirm varieti exampl,['Nicolai Baldin'],"['math.ST', 'stat.TH', '60G55, 62G05, 62M30']",False,False,True,False,False,False
1359,2017-03-28T14:04:54Z,2017-03-05T19:37:52Z,http://arxiv.org/abs/1703.01654v1,http://arxiv.org/pdf/1703.01654v1,"À propos des tentatives visant à construire un estimateur   ""universel"" à partir de données indépendantes",propo des tentat visant construir un estimateur universel partir de donn es ind pendant,"This paper is based on our personal notes for the short course we gave on January 5, 2017 at Institut Henri Poincar\'e, after an invitation of the SFdS. Our purpose is to give an overview of the method of $\rho$-estimation and of the optimality and robustness properties of the estimators built according to this procedure. This method can be viewed as the sequel of a long series of researches which were devoted to the construction of estimators with good properties in various statistical frameworks. We shall emphasize the connection between the $\rho$-estimators and the previous ones, in particular the maximum likelihood estimator, and we shall show, via some typical examples, that the $\rho$-estimators perform better from various points of view.   ------   Cet article est fond\'e sur les notes du mini-cours que nous avons donn\'e le 5 janvier 2017 \`a l'Institut Henri Poincar\'e \`a l'occasion d'une journ\'ee organis\'ee par la SFdS et consacr\'ee \`a la Statistique Math\'ematique. Il vise \`a donner un aper\c{c}u de la m\'ethode de $\rho$-estimation ainsi que des propri\'et\'es d'optimalit\'e et de robustesse des estimateurs construits selon cette proc\'edure. Cette m\'ethode s'inscrit dans une longue lign\'ee de recherches dont l'objectif a \'et\'e de produire des estimateurs poss\'edant de bonnes propri\'et\'es pour un ensemble de cadres statistiques aussi vaste que possible. Nous mettrons en lumi\`ere les liens forts qui existent entre les $\rho$-estimateurs et ces pr\'ed\'ecesseurs, notamment les estimateurs du maximum de vraisemblance, mais montrerons \'egalement, au travers d'exemples choisis, que les $\rho$-estimateurs les surpassent sur bien des aspects.",paper base person note short cours gave januari institut henri poincar invit sfds purpos give overview method rho estim optim robust properti estim built accord procedur method view sequel long seri research devot construct estim good properti various statist framework shall emphas connect rho estim previous one particular maximum likelihood estim shall show via typic exampl rho estim perform better various point view cet articl est fond sur les note du mini cour que nous avon donn le janvier institut henri poincar occas une journ ee organi ee par la sfds et consacr ee la statistiqu math ematiqu il vise donner un aper de la ethod de rho estim ainsi que des propri et es optimalit et de robustess des estimateur construit selon cett proc edur cett ethod inscrit dan une longu lign ee de recherch dont objectif et de produir des estimateur poss edant de bonn propri et es pour un ensembl de cadr statistiqu aussi vast que possibl nous mettron en lumi ere les lien fort qui exist entr les rho estimateur et ces pr ed ecesseur notam les estimateur du maximum de vraisembl mai montreron egal au traver exempl choisi que les rho estimateur les surpass sur bien des aspect,"['Yannick Baraud', 'Lucien Birgé']","['math.ST', 'stat.TH', '62G05']",False,False,True,False,False,True
1364,2017-03-28T14:04:58Z,2017-03-04T00:16:54Z,http://arxiv.org/abs/1703.01364v1,http://arxiv.org/pdf/1703.01364v1,A Matrix Variate Skew-t Distribution,matrix variat skew distribut,"Although there is ample work in the literature dealing with skewness in the multivariate setting, there is a relative paucity of work in the matrix variate paradigm. Such work is, for example, useful for modelling three-way data. A matrix variate skew-t distribution is derived based on a mean-variance matrix normal mixture. An expectation-conditional maximization algorithm is developed for parameter estimation. Simulated data are used for illustration.",although ampl work literatur deal skew multivari set relat pauciti work matrix variat paradigm work exampl use model three way data matrix variat skew distribut deriv base mean varianc matrix normal mixtur expect condit maxim algorithm develop paramet estim simul data use illustr,"['Michael P. B. Gallaugher', 'Paul D. McNicholas']","['stat.ME', 'math.ST', 'stat.TH']",False,False,True,False,False,False
1378,2017-03-28T14:05:02Z,2017-03-06T15:19:31Z,http://arxiv.org/abs/1702.08900v2,http://arxiv.org/pdf/1702.08900v2,Asymptotic Exponentiality of the First Exit Time of the Shiryaev-Roberts   Diffusion with Constant Positive Drift,asymptot exponenti first exit time shiryaev robert diffus constant posit drift,"We consider the first exit time of a Shiryaev-Roberts diffusion with constant positive drift from the interval $[0,A]$ where $A>0$. We show that the moment generating function (Laplace transform) of a suitably standardized version of the first exit time converges to that of the unit-mean exponential distribution as $A\to+\infty$. The proof is explicit in that the moment generating function of the first exit time is first expressed analytically and in a closed form, and then the desired limit as $A\to+\infty$ is evaluated directly. The result is of importance in the area of quickest change-point detection, and its discrete-time counterpart has been previously established - although in a different manner - by Pollak and Tartakovsky (2009).",consid first exit time shiryaev robert diffus constant posit drift interv show moment generat function laplac transform suitabl standard version first exit time converg unit mean exponenti distribut infti proof explicit moment generat function first exit time first express analyt close form desir limit infti evalu direct result import area quickest chang point detect discret time counterpart previous establish although differ manner pollak tartakovski,['Aleksey S. Polunchenko'],"['stat.ME', 'math.ST', 'stat.TH', '62L10, 60G40, 60J60']",False,False,True,False,False,False
1383,2017-03-28T14:05:06Z,2017-02-27T18:38:28Z,http://arxiv.org/abs/1703.01237v1,http://arxiv.org/pdf/1703.01237v1,How real is the random censorship model in medical studies?,real random censorship model medic studi,"In survival analysis the random censorship model refers to censoring and survival times being independent of each other. It is one of the fundamental assumptions in the theory of survival analysis. We explain the reason for it being so ubiquitous, and we investigate its presence in medical studies. We differentiate two types of censoring in medical studies (dropout and administrative), and we explain their importance in examining the existence of the random censorship model. We show that in order to presume the random censorship model it is not enough to have a design study which conforms to it, but that one needs to provide evidence for its presence in the results. Blindly presuming the random censorship model might lead to the Kaplan-Meier estimator producing biased results, which might have serious consequences when estimating survival in medical studies.",surviv analysi random censorship model refer censor surviv time independ one fundament assumpt theori surviv analysi explain reason ubiquit investig presenc medic studi differenti two type censor medic studi dropout administr explain import examin exist random censorship model show order presum random censorship model enough design studi conform one need provid evid presenc result blind presum random censorship model might lead kaplan meier estim produc bias result might serious consequ estim surviv medic studi,['Damjan Krstajic'],"['stat.AP', 'math.ST', 'stat.TH']",False,False,True,False,False,False
1384,2017-03-28T14:05:06Z,2017-02-27T10:01:36Z,http://arxiv.org/abs/1702.08211v1,http://arxiv.org/pdf/1702.08211v1,"Online Nonparametric Learning, Chaining, and the Role of Partial   Feedback",onlin nonparametr learn chain role partial feedback,"We investigate contextual online learning with nonparametric (Lipschitz) comparison classes under different assumptions on losses and feedback information. For full information feedback and Lipschitz losses, we characterize the minimax regret up to log factors by proving an upper bound matching a previously known lower bound. In a partial feedback model motivated by second-price auctions, we prove upper bounds for Lipschitz and semi-Lipschitz losses that improve on the known bounds for standard bandit feedback. Our analysis combines novel results for contextual second-price auctions with a novel algorithmic approach based on chaining. When the context space is Euclidean, our chaining approach is efficient and delivers an even better regret bound.",investig contextu onlin learn nonparametr lipschitz comparison class differ assumpt loss feedback inform full inform feedback lipschitz loss character minimax regret log factor prove upper bound match previous known lower bound partial feedback model motiv second price auction prove upper bound lipschitz semi lipschitz loss improv known bound standard bandit feedback analysi combin novel result contextu second price auction novel algorithm approach base chain context space euclidean chain approach effici deliv even better regret bound,"['Nicolò Cesa-Bianchi', 'Pierre Gaillard', 'Claudio Gentile', 'Sébastien Gerchinovitz']","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",False,False,True,False,False,True
1390,2017-03-28T14:05:11Z,2017-02-24T02:09:04Z,http://arxiv.org/abs/1702.07448v1,http://arxiv.org/pdf/1702.07448v1,Optimal Bayesian Minimax Rates for Unconstrained Large Covariance   Matrices,optim bayesian minimax rate unconstrain larg covari matric,"We obtain the optimal Bayesian minimax rate for the unconstrained large covariance matrix of multivariate normal sample with mean zero, when both the sample size, n, and the dimension, p, of the covariance matrix tend to infinity. Traditionally the posterior convergence rate is used to compare the frequentist asymptotic performance of priors, but defining the optimality with it is elusive. We propose a new decision theoretic framework for prior selection and define Bayesian minimax rate. Under the proposed framework, we obtain the optimal Bayesian minimax rate for the spectral norm for all rates of p. We also considered Frobenius norm, Bregman divergence and squared log-determinant loss and obtain the optimal Bayesian minimax rate under certain rate conditions on p. A simulation study is conducted to support the theoretical results.",obtain optim bayesian minimax rate unconstrain larg covari matrix multivari normal sampl mean zero sampl size dimens covari matrix tend infin tradit posterior converg rate use compar frequentist asymptot perform prior defin optim elus propos new decis theoret framework prior select defin bayesian minimax rate propos framework obtain optim bayesian minimax rate spectral norm rate also consid frobenius norm bregman diverg squar log determin loss obtain optim bayesian minimax rate certain rate condit simul studi conduct support theoret result,"['Kyoungjae Lee', 'Jaeyong Lee']","['math.ST', 'stat.TH']",False,False,True,False,False,True
1397,2017-03-28T14:05:11Z,2017-02-23T19:01:53Z,http://arxiv.org/abs/1702.06488v2,http://arxiv.org/pdf/1702.06488v2,Distributed Estimation of Principal Eigenspaces,distribut estim princip eigenspac,"Principal component analysis (PCA) is fundamental to statistical machine learning. It extracts latent principal factors that contribute to the most variation of the data. When data are stored across multiple machines, however, communication cost can prohibit the computation of PCA in a central location and distributed algorithms for PCA are thus needed. This paper proposes and studies a distributed PCA algorithm: each node machine computes the top $K$ eigenvectors and transmits them to the central server; the central server then aggregates the information from all the node machines and conducts a PCA based on the aggregated information. We investigate the bias and variance for the resulting distributed estimator of the top $K$ eigenvectors. In particular, we show that for distributions with symmetric innovation, the distributed PCA is ""unbiased"". We derive the rate of convergence for distributed PCA estimators, which depends explicitly on the effective rank of covariance, eigen-gap, and the number of machines. We show that when the number of machines is not unreasonably large, the distributed PCA performs as well as the whole sample PCA, even without full access of whole data. The theoretical results are verified by an extensive simulation study. We also extend our analysis to the heterogeneous case where the population covariance matrices are different across local machines but share similar top eigen-structures.",princip compon analysi pca fundament statist machin learn extract latent princip factor contribut variat data data store across multipl machin howev communic cost prohibit comput pca central locat distribut algorithm pca thus need paper propos studi distribut pca algorithm node machin comput top eigenvector transmit central server central server aggreg inform node machin conduct pca base aggreg inform investig bias varianc result distribut estim top eigenvector particular show distribut symmetr innov distribut pca unbias deriv rate converg distribut pca estim depend explicit effect rank covari eigen gap number machin show number machin unreason larg distribut pca perform well whole sampl pca even without full access whole data theoret result verifi extens simul studi also extend analysi heterogen case popul covari matric differ across local machin share similar top eigen structur,"['Jianqing Fan', 'Dong Wang', 'Kaizheng Wang', 'Ziwei Zhu']","['stat.CO', 'math.ST', 'stat.TH']",False,False,True,False,False,False
1398,2017-03-28T14:05:11Z,2017-02-20T16:40:45Z,http://arxiv.org/abs/1702.06055v1,http://arxiv.org/pdf/1702.06055v1,Performance of information criteria used for model selection of Hawkes   process models of financial data,perform inform criteria use model select hawk process model financi data,"We test three common information criteria (IC) for selecting the order of a Hawkes process with an intensity kernel that can be expressed as a mixture of exponential terms. These processes find application in high-frequency financial data modelling. The information criteria are Akaike's information criterion (AIC), the Bayesian information criterion (BIC) and the Hannan-Quinn criterion (HQ). Since we work with simulated data, we are able to measure the performance of model selection by the success rate of the IC in selecting the model that was used to generate the data. In particular, we are interested in the relation between correct model selection and underlying sample size. The analysis includes realistic sample sizes and parameter sets from recent literature where parameters were estimated using empirical financial intra-day data. We compare our results to theoretical predictions and similar empirical findings on the asymptotic distribution of model selection for consistent and inconsistent IC.",test three common inform criteria ic select order hawk process intens kernel express mixtur exponenti term process find applic high frequenc financi data model inform criteria akaik inform criterion aic bayesian inform criterion bic hannan quinn criterion hq sinc work simul data abl measur perform model select success rate ic select model use generat data particular interest relat correct model select sampl size analysi includ realist sampl size paramet set recent literatur paramet estim use empir financi intra day data compar result theoret predict similar empir find asymptot distribut model select consist inconsist ic,"['J. M. Chen', 'A. G. Hawkes', 'E. Scalas', 'M. Trinh']","['q-fin.ST', 'math.ST', 'stat.TH', '60G55']",False,False,True,False,False,False
