,プログラム実行日時,論文更新日時,論文リンク,PDFリンク,元論文タイトル,論文タイトル,元サマリ,サマリ,著者,事前付与ジャンル,ニューラルネットワーク,自然言語処理,マーケティング,画像解析,音声解析,強化学習
16,2017-03-28T14:05:20Z,2017-03-23T12:32:10Z,http://arxiv.org/abs/1703.08041v1,http://arxiv.org/pdf/1703.08041v1,Resolving the Complexity of Some Fundamental Problems in Computational   Social Choice,resolv complex fundament problem comput social choic,This thesis is in the area called computational social choice which is an intersection area of algorithms and social choice theory.,thesi area call comput social choic intersect area algorithm social choic theori,['Palash Dey'],"['cs.DS', 'cs.AI', 'cs.MA']",False,False,False,False,True,False
65,2017-03-28T14:05:41Z,2017-03-15T21:20:44Z,http://arxiv.org/abs/1703.05390v1,http://arxiv.org/pdf/1703.05390v1,Convolutional Recurrent Neural Networks for Small-Footprint Keyword   Spotting,convolut recurr neural network small footprint keyword spot,"Keyword spotting (KWS) constitutes a major component of human-technology interfaces. Maximizing the detection accuracy at a low false alarm (FA) rate, while minimizing the footprint size, latency and complexity are the goals for KWS. Towards achieving them, we study Convolutional Recurrent Neural Networks (CRNNs). Inspired by large-scale state-of-the-art speech recognition systems, we combine the strengths of convolutional layers and recurrent layers to exploit local structure and long-range context. We analyze the effect of architecture parameters, and propose training strategies to improve performance. With only ~230k parameters, our CRNN model yields acceptably low latency, and achieves 97.71% accuracy at 0.5 FA/hour for 5 dB signal-to-noise ratio.",keyword spot kws constitut major compon human technolog interfac maxim detect accuraci low fals alarm fa rate minim footprint size latenc complex goal kws toward achiev studi convolut recurr neural network crnns inspir larg scale state art speech recognit system combin strength convolut layer recurr layer exploit local structur long rang context analyz effect architectur paramet propos train strategi improv perform onli paramet crnn model yield accept low latenc achiev accuraci fa hour db signal nois ratio,"['Sercan O. Arik', 'Markus Kliegl', 'Rewon Child', 'Joel Hestness', 'Andrew Gibiansky', 'Chris Fougner', 'Ryan Prenger', 'Adam Coates']","['cs.CL', 'cs.AI', 'cs.LG']",False,False,False,False,True,False
177,2017-03-28T14:07:09Z,2017-02-15T19:39:58Z,http://arxiv.org/abs/1702.04748v1,http://arxiv.org/pdf/1702.04748v1,An Improved Dictatorship Test with Perfect Completeness,improv dictatorship test perfect complet,"A Boolean function $f:\{0,1\}^n\rightarrow \{0,1\}$ is called a dictator if it depends on exactly one variable i.e $f(x_1, x_2, \ldots, x_n) = x_i$ for some $i\in [n]$. In this work, we study a $k$-query dictatorship test. Dictatorship tests are central in proving many hardness results for constraint satisfaction problems.   The dictatorship test is said to have {\em perfect completeness} if it accepts any dictator function. The {\em soundness} of a test is the maximum probability with which it accepts any function far from a dictator. Our main result is a $k$-query dictatorship test with perfect completeness and soundness $ \frac{2k + 1}{2^k}$, where $k$ is of the form $2^t -1$ for any integer $t > 2$. This improves upon the result of \cite{TY15} which gave a dictatorship test with soundness $ \frac{2k + 3}{2^k}$.",boolean function rightarrow call dictat depend exact one variabl ldot work studi queri dictatorship test dictatorship test central prove mani hard result constraint satisfact problem dictatorship test said em perfect complet accept ani dictat function em sound test maximum probabl accept ani function far dictat main result queri dictatorship test perfect complet sound frac form ani integ improv upon result cite ty gave dictatorship test sound frac,"['Amey Bhangale', 'Subhash Khot', 'Devanathan Thiruvenkatachari']",['cs.CC'],False,False,False,False,True,False
180,2017-03-28T14:07:13Z,2017-02-14T18:21:28Z,http://arxiv.org/abs/1702.04322v1,http://arxiv.org/pdf/1702.04322v1,Parameterized Algorithms for Recognizing Monopolar and 2-Subcolorable   Graphs,parameter algorithm recogn monopolar subcolor graph,"A graph $G$ is a $(\Pi_A,\Pi_B)$-graph if $V(G)$ can be bipartitioned into $A$ and $B$ such that $G[A]$ satisfies property $\Pi_A$ and $G[B]$ satisfies property $\Pi_B$. The $(\Pi_{A},\Pi_{B})$-Recognition problem is to recognize whether a given graph is a $(\Pi_A,\Pi_B)$-graph. There are many $(\Pi_{A},\Pi_{B})$-Recognition problems, including the recognition problems for bipartite, split, and unipolar graphs. We present efficient algorithms for many cases of $(\Pi_A,\Pi_B)$-Recognition based on a technique which we dub inductive recognition. In particular, we give fixed-parameter algorithms for two NP-hard $(\Pi_{A},\Pi_{B})$-Recognition problems, Monopolar Recognition and 2-Subcoloring. We complement our algorithmic results with several hardness results for $(\Pi_{A},\Pi_{B})$-Recognition.",graph pi pi graph bipartit satisfi properti pi satisfi properti pi pi pi recognit problem recogn whether given graph pi pi graph mani pi pi recognit problem includ recognit problem bipartit split unipolar graph present effici algorithm mani case pi pi recognit base techniqu dub induct recognit particular give fix paramet algorithm two np hard pi pi recognit problem monopolar recognit subcolor complement algorithm result sever hard result pi pi recognit,"['Iyad Kanj', 'Christian Komusiewicz', 'Manuel Sorge', 'Erik Jan van Leeuwen']","['cs.CC', 'cs.DS']",False,False,False,False,True,True
266,2017-03-28T14:07:41Z,2017-02-02T22:20:25Z,http://arxiv.org/abs/1702.00849v1,http://arxiv.org/pdf/1702.00849v1,On the union complexity of families of axis-parallel rectangles with a   low packing number,union complex famili axi parallel rectangl low pack number,"Let R be a family of n axis-parallel rectangles with packing number p-1, meaning that among any p of the rectangles, there are two with a non-empty intersection. We show that the union complexity of R is at most O(n+p^2), and that the (<=k)-level complexity of R is at most O(kn+k^2p^2). Both upper bounds are tight.",let famili axi parallel rectangl pack number mean among ani rectangl two non empti intersect show union complex level complex kn upper bound tight,"['Chaya Keller', 'Shakhar Smorodinsky']","['math.CO', 'cs.CG', '52C45, 52C15']",False,False,False,False,True,False
312,2017-03-28T14:06:02Z,2017-03-24T15:40:19Z,http://arxiv.org/abs/1703.08471v1,http://arxiv.org/pdf/1703.08471v1,Batch-normalized joint training for DNN-based distant speech recognition,batch normal joint train dnn base distant speech recognit,"Improving distant speech recognition is a crucial step towards flexible human-machine interfaces. Current technology, however, still exhibits a lack of robustness, especially when adverse acoustic conditions are met. Despite the significant progress made in the last years on both speech enhancement and speech recognition, one potential limitation of state-of-the-art technology lies in composing modules that are not well matched because they are not trained jointly. To address this concern, a promising approach consists in concatenating a speech enhancement and a speech recognition deep neural network and to jointly update their parameters as if they were within a single bigger network. Unfortunately, joint training can be difficult because the output distribution of the speech enhancement system may change substantially during the optimization procedure. The speech recognition module would have to deal with an input distribution that is non-stationary and unnormalized. To mitigate this issue, we propose a joint training approach based on a fully batch-normalized architecture. Experiments, conducted using different datasets, tasks and acoustic conditions, revealed that the proposed framework significantly overtakes other competitive solutions, especially in challenging environments.",improv distant speech recognit crucial step toward flexibl human machin interfac current technolog howev still exhibit lack robust especi advers acoust condit met despit signific progress made last year speech enhanc speech recognit one potenti limit state art technolog lie compos modul well match becaus train joint address concern promis approach consist concaten speech enhanc speech recognit deep neural network joint updat paramet within singl bigger network unfortun joint train difficult becaus output distribut speech enhanc system may chang substanti dure optim procedur speech recognit modul would deal input distribut non stationari unnorm mitig issu propos joint train approach base fulli batch normal architectur experi conduct use differ dataset task acoust condit reveal propos framework signific overtak competit solut especi challeng environ,"['Mirco Ravanelli', 'Philemon Brakel', 'Maurizio Omologo', 'Yoshua Bengio']","['cs.CL', 'cs.LG']",False,False,False,False,True,False
324,2017-03-28T14:06:07Z,2017-03-23T13:48:45Z,http://arxiv.org/abs/1703.08068v1,http://arxiv.org/pdf/1703.08068v1,Sequential Recurrent Neural Networks for Language Modeling,sequenti recurr neural network languag model,"Feedforward Neural Network (FNN)-based language models estimate the probability of the next word based on the history of the last N words, whereas Recurrent Neural Networks (RNN) perform the same task based only on the last word and some context information that cycles in the network. This paper presents a novel approach, which bridges the gap between these two categories of networks. In particular, we propose an architecture which takes advantage of the explicit, sequential enumeration of the word history in FNN structure while enhancing each word representation at the projection layer through recurrent context information that evolves in the network. The context integration is performed using an additional word-dependent weight matrix that is also learned during the training. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art feedforward as well as recurrent neural network architectures.",feedforward neural network fnn base languag model estim probabl next word base histori last word wherea recurr neural network rnn perform task base onli last word context inform cycl network paper present novel approach bridg gap two categori network particular propos architectur take advantag explicit sequenti enumer word histori fnn structur enhanc word represent project layer recurr context inform evolv network context integr perform use addit word depend weight matrix also learn dure train extens experi conduct penn treebank ptb larg text compress benchmark ltcb corpus show signific reduct perplex compar state art feedforward well recurr neural network architectur,"['Youssef Oualil', 'Clayton Greenberg', 'Mittul Singh', 'Dietrich Klakow']",['cs.CL'],False,False,False,False,True,False
325,2017-03-28T14:06:07Z,2017-03-23T13:00:14Z,http://arxiv.org/abs/1703.08052v1,http://arxiv.org/pdf/1703.08052v1,Dynamic Bernoulli Embeddings for Language Evolution,dynam bernoulli embed languag evolut,"Word embeddings are a powerful approach for unsupervised analysis of language. Recently, Rudolph et al. (2016) developed exponential family embeddings, which cast word embeddings in a probabilistic framework. Here, we develop dynamic embeddings, building on exponential family embeddings to capture how the meanings of words change over time. We use dynamic embeddings to analyze three large collections of historical texts: the U.S. Senate speeches from 1858 to 2009, the history of computer science ACM abstracts from 1951 to 2014, and machine learning papers on the Arxiv from 2007 to 2015. We find dynamic embeddings provide better fits than classical embeddings and capture interesting patterns about how language changes.",word embed power approach unsupervis analysi languag recent rudolph et al develop exponenti famili embed cast word embed probabilist framework develop dynam embed build exponenti famili embed captur mean word chang time use dynam embed analyz three larg collect histor text senat speech histori comput scienc acm abstract machin learn paper arxiv find dynam embed provid better fit classic embed captur interest pattern languag chang,"['Maja Rudolph', 'David Blei']","['stat.ML', 'cs.CL']",False,False,False,False,True,False
326,2017-03-28T14:06:07Z,2017-03-23T11:02:47Z,http://arxiv.org/abs/1703.08002v1,http://arxiv.org/pdf/1703.08002v1,A network of deep neural networks for distant speech recognition,network deep neural network distant speech recognit,"Despite the remarkable progress recently made in distant speech recognition, state-of-the-art technology still suffers from a lack of robustness, especially when adverse acoustic conditions characterized by non-stationary noises and reverberation are met. A prominent limitation of current systems lies in the lack of matching and communication between the various technologies involved in the distant speech recognition process. The speech enhancement and speech recognition modules are, for instance, often trained independently. Moreover, the speech enhancement normally helps the speech recognizer, but the output of the latter is not commonly used, in turn, to improve the speech enhancement. To address both concerns, we propose a novel architecture based on a network of deep neural networks, where all the components are jointly trained and better cooperate with each other thanks to a full communication scheme between them. Experiments, conducted using different datasets, tasks and acoustic conditions, revealed that the proposed framework can overtake other competitive solutions, including recent joint training approaches.",despit remark progress recent made distant speech recognit state art technolog still suffer lack robust especi advers acoust condit character non stationari nois reverber met promin limit current system lie lack match communic various technolog involv distant speech recognit process speech enhanc speech recognit modul instanc often train independ moreov speech enhanc normal help speech recogn output latter common use turn improv speech enhanc address concern propos novel architectur base network deep neural network compon joint train better cooper thank full communic scheme experi conduct use differ dataset task acoust condit reveal propos framework overtak competit solut includ recent joint train approach,"['Mirco Ravanelli', 'Philemon Brakel', 'Maurizio Omologo', 'Yoshua Bengio']","['cs.CL', 'cs.LG']",False,False,False,False,True,False
328,2017-03-28T14:06:07Z,2017-03-22T17:17:16Z,http://arxiv.org/abs/1703.07754v1,http://arxiv.org/pdf/1703.07754v1,Direct Acoustics-to-Word Models for English Conversational Speech   Recognition,direct acoust word model english convers speech recognit,"Recent work on end-to-end automatic speech recognition (ASR) has shown that the connectionist temporal classification (CTC) loss can be used to convert acoustics to phone or character sequences. Such systems are used with a dictionary and separately-trained Language Model (LM) to produce word sequences. However, they are not truly end-to-end in the sense of mapping acoustics directly to words without an intermediate phone representation. In this paper, we present the first results employing direct acoustics-to-word CTC models on two well-known public benchmark tasks: Switchboard and CallHome. These models do not require an LM or even a decoder at run-time and hence recognize speech with minimal complexity. However, due to the large number of word output units, CTC word models require orders of magnitude more data to train reliably compared to traditional systems. We present some techniques to mitigate this issue. Our CTC word model achieves a word error rate of 13.0%/18.8% on the Hub5-2000 Switchboard/CallHome test sets without any LM or decoder compared with 9.6%/16.0% for phone-based CTC with a 4-gram LM. We also present rescoring results on CTC word model lattices to quantify the performance benefits of a LM, and contrast the performance of word and phone CTC models.",recent work end end automat speech recognit asr shown connectionist tempor classif ctc loss use convert acoust phone charact sequenc system use dictionari separ train languag model lm produc word sequenc howev truli end end sens map acoust direct word without intermedi phone represent paper present first result employ direct acoust word ctc model two well known public benchmark task switchboard callhom model requir lm even decod run time henc recogn speech minim complex howev due larg number word output unit ctc word model requir order magnitud data train reliabl compar tradit system present techniqu mitig issu ctc word model achiev word error rate hub switchboard callhom test set without ani lm decod compar phone base ctc gram lm also present rescor result ctc word model lattic quantifi perform benefit lm contrast perform word phone ctc model,"['Kartik Audhkhasi', 'Bhuvana Ramabhadran', 'George Saon', 'Michael Picheny', 'David Nahamoo']","['cs.CL', 'cs.NE', 'stat.ML']",False,False,False,False,True,False
331,2017-03-28T14:06:12Z,2017-03-22T00:37:33Z,http://arxiv.org/abs/1703.07476v1,http://arxiv.org/pdf/1703.07476v1,Topic Identification for Speech without ASR,topic identif speech without asr,"Modern topic identification (topic ID) systems for speech use automatic speech recognition (ASR) to produce speech transcripts, and perform supervised classification on such ASR outputs. However, under resource-limited conditions, the manually transcribed speech required to develop standard ASR systems can be severely limited or unavailable. In this paper, we investigate alternative unsupervised solutions to obtaining tokenizations of speech in terms of a vocabulary of automatically discovered word-like or phoneme-like units, without depending on the supervised training of ASR systems. Moreover, using automatic phoneme-like tokenizations, we demonstrate that a convolutional neural network based framework for learning spoken document representations provides competitive performance compared to a standard bag-of-words representation, as evidenced by comprehensive topic ID evaluations on both single-label and multi-label classification tasks.",modern topic identif topic id system speech use automat speech recognit asr produc speech transcript perform supervis classif asr output howev resourc limit condit manual transcrib speech requir develop standard asr system sever limit unavail paper investig altern unsupervis solut obtain token speech term vocabulari automat discov word like phonem like unit without depend supervis train asr system moreov use automat phonem like token demonstr convolut neural network base framework learn spoken document represent provid competit perform compar standard bag word represent evidenc comprehens topic id evalu singl label multi label classif task,"['Chunxi Liu', 'Jan Trmal', 'Matthew Wiesner', 'Craig Harman', 'Sanjeev Khudanpur']",['cs.CL'],False,True,False,False,True,False
333,2017-03-28T14:06:12Z,2017-03-21T08:24:50Z,http://arxiv.org/abs/1703.07090v1,http://arxiv.org/pdf/1703.07090v1,Deep LSTM for Large Vocabulary Continuous Speech Recognition,deep lstm larg vocabulari continu speech recognit,"Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs, are effective network for sequential task like speech recognition. Deeper LSTM models perform well on large vocabulary continuous speech recognition, because of their impressive learning ability. However, it is more difficult to train a deeper network. We introduce a training framework with layer-wise training and exponential moving average methods for deeper LSTM models. It is a competitive framework that LSTM models of more than 7 layers are successfully trained on Shenma voice search data in Mandarin and they outperform the deep LSTM models trained by conventional approach. Moreover, in order for online streaming speech recognition applications, the shallow model with low real time factor is distilled from the very deep model. The recognition accuracy have little loss in the distillation process. Therefore, the model trained with the proposed training framework reduces relative 14\% character error rate, compared to original model which has the similar real-time capability. Furthermore, the novel transfer learning strategy with segmental Minimum Bayes-Risk is also introduced in the framework. The strategy makes it possible that training with only a small part of dataset could outperform full dataset training from the beginning.",recurr neural network rnns especi long short term memori lstm rnns effect network sequenti task like speech recognit deeper lstm model perform well larg vocabulari continu speech recognit becaus impress learn abil howev difficult train deeper network introduc train framework layer wise train exponenti move averag method deeper lstm model competit framework lstm model layer success train shenma voic search data mandarin outperform deep lstm model train convent approach moreov order onlin stream speech recognit applic shallow model low real time factor distil veri deep model recognit accuraci littl loss distil process therefor model train propos train framework reduc relat charact error rate compar origin model similar real time capabl furthermor novel transfer learn strategi segment minimum bay risk also introduc framework strategi make possibl train onli small part dataset could outperform full dataset train begin,"['Xu Tian', 'Jun Zhang', 'Zejun Ma', 'Yi He', 'Juan Wei', 'Peihao Wu', 'Wenchang Situ', 'Shuai Li', 'Yang Zhang']",['cs.CL'],False,False,False,False,True,False
344,2017-03-28T14:06:16Z,2017-03-17T07:53:03Z,http://arxiv.org/abs/1703.05916v1,http://arxiv.org/pdf/1703.05916v1,Construction of a Japanese Word Similarity Dataset,construct japanes word similar dataset,"An evaluation of distributed word representation is generally conducted using a word similarity task and/or a word analogy task. There are many datasets readily available for these tasks in English. However, evaluating distributed representation in languages that do not have such resources (e.g., Japanese) is difficult. Therefore, as a first step toward evaluating distributed representations in Japanese, we constructed a Japanese word similarity dataset. To the best of our knowledge, our dataset is the first resource that can be used to evaluate distributed representations in Japanese. Moreover, our dataset contains various parts of speech and includes rare words in addition to common words.",evalu distribut word represent general conduct use word similar task word analog task mani dataset readili avail task english howev evalu distribut represent languag resourc japanes difficult therefor first step toward evalu distribut represent japanes construct japanes word similar dataset best knowledg dataset first resourc use evalu distribut represent japanes moreov dataset contain various part speech includ rare word addit common word,"['Yuya Sakaizawa', 'Mamoru Komachi']",['cs.CL'],False,False,False,False,True,False
346,2017-03-28T14:06:16Z,2017-03-17T03:38:48Z,http://arxiv.org/abs/1703.05880v1,http://arxiv.org/pdf/1703.05880v1,Empirical Evaluation of Parallel Training Algorithms on Acoustic   Modeling,empir evalu parallel train algorithm acoust model,"Deep learning models (DLMs) are state-of-the-art techniques in speech recognition. However, training good DLMs can be time consuming especially for production-size models and corpora. Although several parallel training algorithms have been proposed to improve training efficiency, there is no clear guidance on which one to choose for the task in hand due to lack of systematic and fair comparison among them. In this paper we aim at filling this gap by comparing four popular parallel training algorithms in speech recognition, namely asynchronous stochastic gradient descent (ASGD), blockwise model-update filtering (BMUF), bulk synchronous parallel (BSP) and elastic averaging stochastic gradient descent (EASGD), on 1000-hour LibriSpeech corpora using feed-forward deep neural networks (DNNs) and convolutional, long short-term memory, DNNs (CLDNNs). Based on our experiments, we recommend using BMUF as the top choice to train acoustic models since it is most stable, scales well with number of GPUs, can achieve reproducible results, and in many cases even outperforms single-GPU SGD. ASGD can be used as a substitute in some cases.",deep learn model dlms state art techniqu speech recognit howev train good dlms time consum especi product size model corpora although sever parallel train algorithm propos improv train effici clear guidanc one choos task hand due lack systemat fair comparison among paper aim fill gap compar four popular parallel train algorithm speech recognit name asynchron stochast gradient descent asgd blockwis model updat filter bmuf bulk synchron parallel bsp elast averag stochast gradient descent easgd hour librispeech corpora use feed forward deep neural network dnns convolut long short term memori dnns cldnns base experi recommend use bmuf top choic train acoust model sinc stabl scale well number gpus achiev reproduc result mani case even outperform singl gpu sgd asgd use substitut case,"['Wenpeng Li', 'BinBin Zhang', 'Lei Xie', 'Dong Yu']","['cs.CL', 'cs.LG', 'cs.SD']",False,False,False,False,True,False
352,2017-03-28T14:06:20Z,2017-03-15T21:20:44Z,http://arxiv.org/abs/1703.05390v1,http://arxiv.org/pdf/1703.05390v1,Convolutional Recurrent Neural Networks for Small-Footprint Keyword   Spotting,convolut recurr neural network small footprint keyword spot,"Keyword spotting (KWS) constitutes a major component of human-technology interfaces. Maximizing the detection accuracy at a low false alarm (FA) rate, while minimizing the footprint size, latency and complexity are the goals for KWS. Towards achieving them, we study Convolutional Recurrent Neural Networks (CRNNs). Inspired by large-scale state-of-the-art speech recognition systems, we combine the strengths of convolutional layers and recurrent layers to exploit local structure and long-range context. We analyze the effect of architecture parameters, and propose training strategies to improve performance. With only ~230k parameters, our CRNN model yields acceptably low latency, and achieves 97.71% accuracy at 0.5 FA/hour for 5 dB signal-to-noise ratio.",keyword spot kws constitut major compon human technolog interfac maxim detect accuraci low fals alarm fa rate minim footprint size latenc complex goal kws toward achiev studi convolut recurr neural network crnns inspir larg scale state art speech recognit system combin strength convolut layer recurr layer exploit local structur long rang context analyz effect architectur paramet propos train strategi improv perform onli paramet crnn model yield accept low latenc achiev accuraci fa hour db signal nois ratio,"['Sercan O. Arik', 'Markus Kliegl', 'Rewon Child', 'Joel Hestness', 'Andrew Gibiansky', 'Chris Fougner', 'Ryan Prenger', 'Adam Coates']","['cs.CL', 'cs.AI', 'cs.LG']",False,False,False,False,True,False
354,2017-03-28T14:06:20Z,2017-03-16T08:57:29Z,http://arxiv.org/abs/1703.05123v2,http://arxiv.org/pdf/1703.05123v2,Character-based Neural Embeddings for Tweet Clustering,charact base neural embed tweet cluster,In this paper we show how the performance of tweet clustering can be improved by leveraging character-based neural networks. The proposed approach overcomes the limitations related to the vocabulary explosion in the word-based models and allows for the seamless processing of the multilingual content. Our evaluation results and code are available on-line at https://github.com/vendi12/tweet2vec_clustering,paper show perform tweet cluster improv leverag charact base neural network propos approach overcom limit relat vocabulari explos word base model allow seamless process multilingu content evalu result code avail line https github com vendi tweetvec cluster,"['Svitlana Vakulenko', 'Lyndon Nixon', 'Mihai Lupu']","['cs.IR', 'cs.CL']",False,True,False,False,True,False
355,2017-03-28T14:06:20Z,2017-03-15T12:32:34Z,http://arxiv.org/abs/1703.05122v1,http://arxiv.org/pdf/1703.05122v1,Is this word borrowed? An automatic approach to quantify the likeliness   of borrowing in social media,word borrow automat approach quantifi likeli borrow social media,"Code-mixing or code-switching are the effortless phenomena of natural switching between two or more languages in a single conversation. Use of a foreign word in a language; however, does not necessarily mean that the speaker is code-switching because often languages borrow lexical items from other languages. If a word is borrowed, it becomes a part of the lexicon of a language; whereas, during code-switching, the speaker is aware that the conversation involves foreign words or phrases. Identifying whether a foreign word used by a bilingual speaker is due to borrowing or code-switching is a fundamental importance to theories of multilingualism, and an essential prerequisite towards the development of language and speech technologies for multilingual communities. In this paper, we present a series of novel computational methods to identify the borrowed likeliness of a word, based on the social media signals. We first propose context based clustering method to sample a set of candidate words from the social media data.Next, we propose three novel and similar metrics based on the usage of these words by the users in different tweets; these metrics were used to score and rank the candidate words indicating their borrowed likeliness. We compare these rankings with a ground truth ranking constructed through a human judgment experiment. The Spearman's rank correlation between the two rankings (nearly 0.62 for all the three metric variants) is more than double the value (0.26) of the most competitive existing baseline reported in the literature. Some other striking observations are, (i) the correlation is higher for the ground truth data elicited from the younger participants (age less than 30) than that from the older participants, and (ii )those participants who use mixed-language for tweeting the least, provide the best signals of borrowing.",code mix code switch effortless phenomena natur switch two languag singl convers use foreign word languag howev doe necessarili mean speaker code switch becaus often languag borrow lexic item languag word borrow becom part lexicon languag wherea dure code switch speaker awar convers involv foreign word phrase identifi whether foreign word use bilingu speaker due borrow code switch fundament import theori multilingu essenti prerequisit toward develop languag speech technolog multilingu communiti paper present seri novel comput method identifi borrow likeli word base social media signal first propos context base cluster method sampl set candid word social media data next propos three novel similar metric base usag word user differ tweet metric use score rank candid word indic borrow likeli compar rank ground truth rank construct human judgment experi spearman rank correl two rank near three metric variant doubl valu competit exist baselin report literatur strike observ correl higher ground truth data elicit younger particip age less older particip ii particip use mix languag tweet least provid best signal borrow,"['Jasabanta Patro', 'Bidisha Samanta', 'Saurabh Singh', 'Prithwish Mukherjee', 'Monojit Choudhury', 'Animesh Mukherjee']",['cs.CL'],False,False,False,False,True,False
364,2017-03-28T14:06:24Z,2017-03-14T22:28:51Z,http://arxiv.org/abs/1703.04783v1,http://arxiv.org/pdf/1703.04783v1,Multichannel End-to-end Speech Recognition,multichannel end end speech recognit,"The field of speech recognition is in the midst of a paradigm shift: end-to-end neural networks are challenging the dominance of hidden Markov models as a core technology. Using an attention mechanism in a recurrent encoder-decoder architecture solves the dynamic time alignment problem, allowing joint end-to-end training of the acoustic and language modeling components. In this paper we extend the end-to-end framework to encompass microphone array signal processing for noise suppression and speech enhancement within the acoustic encoding network. This allows the beamforming components to be optimized jointly within the recognition architecture to improve the end-to-end speech recognition objective. Experiments on the noisy speech benchmarks (CHiME-4 and AMI) show that our multichannel end-to-end system outperformed the attention-based baseline with input from a conventional adaptive beamformer.",field speech recognit midst paradigm shift end end neural network challeng domin hidden markov model core technolog use attent mechan recurr encod decod architectur solv dynam time align problem allow joint end end train acoust languag model compon paper extend end end framework encompass microphon array signal process nois suppress speech enhanc within acoust encod network allow beamform compon optim joint within recognit architectur improv end end speech recognit object experi noisi speech benchmark chime ami show multichannel end end system outperform attent base baselin input convent adapt beamform,"['Tsubasa Ochiai', 'Shinji Watanabe', 'Takaaki Hori', 'John R. Hershey']","['cs.SD', 'cs.CL']",False,False,False,False,True,False
366,2017-03-28T14:06:24Z,2017-03-21T12:56:11Z,http://arxiv.org/abs/1703.04650v2,http://arxiv.org/pdf/1703.04650v2,Joint Learning of Correlated Sequence Labelling Tasks Using   Bidirectional Recurrent Neural Networks,joint learn correl sequenc label task use bidirect recurr neural network,"The stream of words produced by Automatic Speech Recognition (ASR) systems is devoid of any punctuations and formatting. Most natural language processing applications usually expect segmented and well-formatted texts as input, which is not available in ASR output. This paper proposes a novel technique of jointly modelling multiple correlated tasks such as punctuation and capitalization using bidirectional recurrent neural networks, which leads to improved performance for each of these tasks. This method can be extended for joint modelling of any other correlated multiple sequence labelling tasks.",stream word produc automat speech recognit asr system devoid ani punctuat format natur languag process applic usual expect segment well format text input avail asr output paper propos novel techniqu joint model multipl correl task punctuat capit use bidirect recurr neural network lead improv perform task method extend joint model ani correl multipl sequenc label task,"['Vardaan Pahuja', 'Anirban Laha', 'Shachar Mirkin', 'Vikas Raykar', 'Lili Kotlerman', 'Guy Lev']",['cs.CL'],False,False,False,False,True,False
380,2017-03-28T14:06:31Z,2017-03-11T18:20:13Z,http://arxiv.org/abs/1703.04009v1,http://arxiv.org/pdf/1703.04009v1,Automated Hate Speech Detection and the Problem of Offensive Language,autom hate speech detect problem offens languag,"A key challenge for automatic hate-speech detection on social media is the separation of hate speech from other instances of offensive language. Lexical detection methods tend to have low precision because they classify all messages containing particular terms as hate speech and previous work using supervised learning has failed to distinguish between the two categories. We used a crowd-sourced hate speech lexicon to collect tweets containing hate speech keywords. We use crowd-sourcing to label a sample of these tweets into three categories: those containing hate speech, only offensive language, and those with neither. We train a multi-class classifier to distinguish between these different categories. Close analysis of the predictions and the errors shows when we can reliably separate hate speech from other offensive language and when this differentiation is more difficult. We find that racist and homophobic tweets are more likely to be classified as hate speech but that sexist tweets are generally classified as offensive. Tweets without explicit hate keywords are also more difficult to classify.",key challeng automat hate speech detect social media separ hate speech instanc offens languag lexic detect method tend low precis becaus classifi messag contain particular term hate speech previous work use supervis learn fail distinguish two categori use crowd sourc hate speech lexicon collect tweet contain hate speech keyword use crowd sourc label sampl tweet three categori contain hate speech onli offens languag neither train multi class classifi distinguish differ categori close analysi predict error show reliabl separ hate speech offens languag differenti difficult find racist homophob tweet like classifi hate speech sexist tweet general classifi offens tweet without explicit hate keyword also difficult classifi,"['Thomas Davidson', 'Dana Warmsley', 'Michael Macy', 'Ingmar Weber']",['cs.CL'],False,False,False,False,True,False
395,2017-03-28T14:06:36Z,2017-03-10T08:11:22Z,http://arxiv.org/abs/1703.03200v2,http://arxiv.org/pdf/1703.03200v2,Turkish PoS Tagging by Reducing Sparsity with Morpheme Tags in Small   Datasets,turkish pos tag reduc sparsiti morphem tag small dataset,Sparsity is one of the major problems in natural language processing. The problem becomes even more severe in agglutinating languages that are highly prone to be inflected. We deal with sparsity in Turkish by adopting morphological features for part-of-speech tagging. We learn inflectional and derivational morpheme tags in Turkish by using conditional random fields (CRF) and we employ the morpheme tags in part-of-speech (PoS) tagging by using hidden Markov models (HMMs) to mitigate sparsity. Results show that using morpheme tags in PoS tagging helps alleviate the sparsity in emission probabilities. Our model outperforms other hidden Markov model based PoS tagging models for small training datasets in Turkish. We obtain an accuracy of 94.1% in morpheme tagging and 89.2% in PoS tagging on a 5K training dataset.,sparsiti one major problem natur languag process problem becom even sever agglutin languag high prone inflect deal sparsiti turkish adopt morpholog featur part speech tag learn inflect deriv morphem tag turkish use condit random field crf employ morphem tag part speech pos tag use hidden markov model hmms mitig sparsiti result show use morphem tag pos tag help allevi sparsiti emiss probabl model outperform hidden markov model base pos tag model small train dataset turkish obtain accuraci morphem tag pos tag train dataset,"['Burcu Can', 'Ahmet Üstün', 'Murathan Kurfalı']",['cs.CL'],False,False,False,False,True,False
458,2017-03-28T14:09:05Z,2017-03-23T14:46:46Z,http://arxiv.org/abs/1703.08089v1,http://arxiv.org/pdf/1703.08089v1,A Bag-of-Words Equivalent Recurrent Neural Network for Action   Recognition,bag word equival recurr neural network action recognit,"The traditional bag-of-words approach has found a wide range of applications in computer vision. The standard pipeline consists of a generation of a visual vocabulary, a quantization of the features into histograms of visual words, and a classification step for which usually a support vector machine in combination with a non-linear kernel is used. Given large amounts of data, however, the model suffers from a lack of discriminative power. This applies particularly for action recognition, where the vast amount of video features needs to be subsampled for unsupervised visual vocabulary generation. Moreover, the kernel computation can be very expensive on large datasets. In this work, we propose a recurrent neural network that is equivalent to the traditional bag-of-words approach but enables for the application of discriminative training. The model further allows to incorporate the kernel computation into the neural network directly, solving the complexity issue and allowing to represent the complete classification system within a single network. We evaluate our method on four recent action recognition benchmarks and show that the conventional model as well as sparse coding methods are outperformed.",tradit bag word approach found wide rang applic comput vision standard pipelin consist generat visual vocabulari quantize featur histogram visual word classif step usual support vector machin combin non linear kernel use given larg amount data howev model suffer lack discrimin power appli particular action recognit vast amount video featur need subsampl unsupervis visual vocabulari generat moreov kernel comput veri expens larg dataset work propos recurr neural network equival tradit bag word approach enabl applic discrimin train model allow incorpor kernel comput neural network direct solv complex issu allow repres complet classif system within singl network evalu method four recent action recognit benchmark show convent model well spars code method outperform,"['Alexander Richard', 'Juergen Gall']",['cs.CV'],False,False,False,False,True,False
483,2017-03-28T14:09:17Z,2017-03-22T13:35:49Z,http://arxiv.org/abs/1703.07645v1,http://arxiv.org/pdf/1703.07645v1,Neural Ctrl-F: Segmentation-free Query-by-String Word Spotting in   Handwritten Manuscript Collections,neural ctrl segment free queri string word spot handwritten manuscript collect,"In this paper, we approach the problem of segmentation-free query-by-string word spotting for handwritten documents. In other words, we use methods inspired from computer vision and machine learning to search for words in large collections of digitized manuscripts. In particular, we are interested in historical handwritten texts, which are often far more challenging than modern printed documents. This task is important, as it provides people with a way to quickly find what they are looking for in large collections that are tedious and difficult to read manually. To this end, we introduce an end-to-end trainable model based on deep neural networks that we call Ctrl-F-Net. Given a full manuscript page, the model simultaneously generates region proposals, and embeds these into a distributed word embedding space, where searches are performed. We evaluate the model on common benchmarks for handwritten word spotting, outperforming the previous state-of-the-art segmentation-free approaches by a large margin, and in some cases even segmentation-based approaches. One interesting real-life application of our approach is to help historians to find and count specific words in court records that are related to women's sustenance activities and division of labor. We provide promising preliminary experiments that validate our method on this task.",paper approach problem segment free queri string word spot handwritten document word use method inspir comput vision machin learn search word larg collect digit manuscript particular interest histor handwritten text often far challeng modern print document task import provid peopl way quick find look larg collect tedious difficult read manual end introduc end end trainabl model base deep neural network call ctrl net given full manuscript page model simultan generat region propos emb distribut word embed space search perform evalu model common benchmark handwritten word spot outperform previous state art segment free approach larg margin case even segment base approach one interest real life applic approach help historian find count specif word court record relat women susten activ divis labor provid promis preliminari experi valid method task,"['Tomas Wilkinson', 'Jonas Lindström', 'Anders Brun']",['cs.CV'],False,False,False,False,True,False
506,2017-03-28T14:09:26Z,2017-03-25T07:27:32Z,http://arxiv.org/abs/1703.08658v1,http://arxiv.org/pdf/1703.08658v1,Maximizing the area of intersection of rectangles,maxim area intersect rectangl,"This paper attacks the following problem. We are given a large number $N$ of rectangles in the plane, each with horizontal and vertical sides, and also a number $r<N$. The given list of $N$ rectangles may contain duplicates. The problem is to find $r$ of these rectangles, such that, if they are discarded, then the intersection of the remaining $(N-r)$ rectangles has an intersection with as large an area as possible. We will find an upper bound, depending only on $N$ and $r$, and not on the particular data presented, for the number of steps needed to run the algorithm on (a mathematical model of) a computer. In fact our algorithm is able to determine, for each $s\le r$, $s$ rectangles from the given list of $N$ rectangles, such that the remaining $(N-s)$ rectangles have as large an area as possible, and this takes hardly any more time than taking care only of the case $s=r$. Our algorithm extends to $d$-dimensional rectangles. Our method is to exhaustively examine all possible intersections---this is much faster than it sounds, because we do not need to examine all $\binom Ns$ subsets in order to find all possible intersection rectangles. For an extreme example, suppose the rectangles are nested, for example concentric squares of distinct sizes, then the only intersections examined are the smallest $s+1$ rectangles.",paper attack follow problem given larg number rectangl plane horizont vertic side also number given list rectangl may contain duplic problem find rectangl discard intersect remain rectangl intersect larg area possibl find upper bound depend onli particular data present number step need run algorithm mathemat model comput fact algorithm abl determin le rectangl given list rectangl remain rectangl larg area possibl take hard ani time take care onli case algorithm extend dimension rectangl method exhaust examin possibl intersect much faster sound becaus need examin binom ns subset order find possibl intersect rectangl extrem exampl suppos rectangl nest exampl concentr squar distinct size onli intersect examin smallest rectangl,"['David B. A. Epstein', 'Mike Paterson']","['cs.DS', 'F.2.2']",False,False,False,False,True,False
512,2017-03-28T14:09:30Z,2017-03-23T12:32:10Z,http://arxiv.org/abs/1703.08041v1,http://arxiv.org/pdf/1703.08041v1,Resolving the Complexity of Some Fundamental Problems in Computational   Social Choice,resolv complex fundament problem comput social choic,This thesis is in the area called computational social choice which is an intersection area of algorithms and social choice theory.,thesi area call comput social choic intersect area algorithm social choic theori,['Palash Dey'],"['cs.DS', 'cs.AI', 'cs.MA']",False,False,False,False,True,False
519,2017-03-28T14:09:30Z,2017-03-21T17:53:50Z,http://arxiv.org/abs/1703.07340v1,http://arxiv.org/pdf/1703.07340v1,Construction of Directed 2K Graphs,construct direct graph,"We study the problem of constructing synthetic graphs that resemble real-world directed graphs in terms of their degree correlations. We define the problem of directed 2K construction (D2K) that takes as input the directed degree sequence (DDS) and a joint degree and attribute matrix (JDAM) so as to capture degree correlation specifically in directed graphs. We provide necessary and sufficient conditions to decide whether a target D2K is realizable, and we design an efficient algorithm that creates realizations with that target D2K. We evaluate our algorithm in creating synthetic graphs that target real-world directed graphs (such as Twitter) and we show that it brings significant benefits compared to state-of-the-art approaches.",studi problem construct synthet graph resembl real world direct graph term degre correl defin problem direct construct dk take input direct degre sequenc dds joint degre attribut matrix jdam captur degre correl specif direct graph provid necessari suffici condit decid whether target dk realiz design effici algorithm creat realize target dk evalu algorithm creat synthet graph target real world direct graph twitter show bring signific benefit compar state art approach,"['Bálint Tillman', 'Athina Markopoulou', 'Carter T. Butts', 'Minas Gjoka']","['cs.SI', 'cs.DS']",False,False,False,False,True,False
529,2017-03-28T14:09:34Z,2017-03-17T17:31:01Z,http://arxiv.org/abs/1703.06733v1,http://arxiv.org/pdf/1703.06733v1,Discovering Relaxed Sound Workflow Nets using Integer Linear Programming,discov relax sound workflow net use integ linear program,"Process mining is concerned with the analysis, understanding and improvement of business processes. Process discovery, i.e. discovering a process model based on an event log, is considered the most challenging process mining task. State-of-the-art process discovery algorithms only discover local control-flow patterns and are unable to discover complex, non-local patterns. Region theory based techniques, i.e. an established class of process discovery techniques, do allow for discovering such patterns. However, applying region theory directly results in complex, over-fitting models, which is less desirable. Moreover, region theory does not cope with guarantees provided by state-of-the-art process discovery algorithms, both w.r.t. structural and behavioural properties of the discovered process models. In this paper we present an ILP-based process discovery approach, based on region theory, that guarantees to discover relaxed sound workflow nets. Moreover, we devise a filtering algorithm, based on the internal working of the ILP-formulation, that is able to cope with the presence of infrequent behaviour. We have extensively evaluated the technique using different event logs with different levels of exceptional behaviour. Our experiments show that the presented approach allow us to leverage the inherent shortcomings of existing region-based approaches. The techniques presented are implemented and readily available in the HybridILPMiner package in the open-source process mining tool-kits ProM and RapidProM.",process mine concern analysi understand improv busi process process discoveri discov process model base event log consid challeng process mine task state art process discoveri algorithm onli discov local control flow pattern unabl discov complex non local pattern region theori base techniqu establish class process discoveri techniqu allow discov pattern howev appli region theori direct result complex fit model less desir moreov region theori doe cope guarante provid state art process discoveri algorithm structur behaviour properti discov process model paper present ilp base process discoveri approach base region theori guarante discov relax sound workflow net moreov devis filter algorithm base intern work ilp formul abl cope presenc infrequ behaviour extens evalu techniqu use differ event log differ level except behaviour experi show present approach allow us leverag inher shortcom exist region base approach techniqu present implement readili avail hybridilpmin packag open sourc process mine tool kit prom rapidprom,"['S. J. van Zelst', 'B. F. van Dongen', 'W. M. P. van der Aalst', 'H. M. W. Verbeek']",['cs.DS'],False,False,False,False,True,False
547,2017-03-28T14:09:43Z,2017-03-15T11:57:53Z,http://arxiv.org/abs/1703.05097v1,http://arxiv.org/pdf/1703.05097v1,A cubic-time algorithm for computing the trinet distance between level-1   networks,cubic time algorithm comput trinet distanc level network,"In evolutionary biology, phylogenetic networks are constructed to represent the evolution of species in which reticulate events are thought to have occurred, such as recombination and hybridization. It is therefore useful to have efficiently computable metrics with which to systematically compare such networks. Through developing an optimal algorithm to enumerate all trinets displayed by a level-1 network (a type of network that is slightly more general than an evolutionary tree), here we propose a cubic-time algorithm to compute the trinet distance between two level-1 networks. Employing simulations, we also present a comparison between the trinet metric and the so-called Robinson-Foulds phylogenetic network metric restricted to level-1 networks. The algorithms described in this paper have been implemented in JAVA and are freely available at https://www.uea.ac.uk/computing/TriLoNet.",evolutionari biolog phylogenet network construct repres evolut speci reticul event thought occur recombin hybrid therefor use effici comput metric systemat compar network develop optim algorithm enumer trinet display level network type network slight general evolutionari tree propos cubic time algorithm comput trinet distanc two level network employ simul also present comparison trinet metric call robinson fould phylogenet network metric restrict level network algorithm describ paper implement java freeli avail https www uea ac uk comput trilonet,"['Vincent Moulton', 'James Oldman', 'Taoyang Wu']","['q-bio.PE', 'cs.DM', 'cs.DS']",False,False,True,False,True,False
703,2017-03-28T14:10:09Z,2017-03-23T12:32:10Z,http://arxiv.org/abs/1703.08041v1,http://arxiv.org/pdf/1703.08041v1,Resolving the Complexity of Some Fundamental Problems in Computational   Social Choice,resolv complex fundament problem comput social choic,This thesis is in the area called computational social choice which is an intersection area of algorithms and social choice theory.,thesi area call comput social choic intersect area algorithm social choic theori,['Palash Dey'],"['cs.DS', 'cs.AI', 'cs.MA']",False,False,False,False,True,False
803,2017-03-28T14:10:52Z,2017-03-23T11:45:10Z,http://arxiv.org/abs/1703.08019v1,http://arxiv.org/pdf/1703.08019v1,Single Channel Audio Source Separation using Convolutional Denoising   Autoencoders,singl channel audio sourc separ use convolut denois autoencod,"Deep learning techniques have been used recently to tackle the audio source separation problem. In this work, we propose to use deep convolution denoising auto-encoders (CDAEs) for monaural audio source separation. We use as many CDAEs as the number of sources to be separated from the mixed signal. Each CDAE is trained to separate one source and treats the other sources as background noise. The main idea is to allow each CDAE to learn suitable time-frequency filters and features to its corresponding source. Our experimental results show that CDAEs perform source separation slightly better than the deep feedforward neural networks (FNNs) even with a much less number of parameters than FNNs.",deep learn techniqu use recent tackl audio sourc separ problem work propos use deep convolut denois auto encod cdae monaur audio sourc separ use mani cdae number sourc separ mix signal cdae train separ one sourc treat sourc background nois main idea allow cdae learn suitabl time frequenc filter featur correspond sourc experiment result show cdae perform sourc separ slight better deep feedforward neural network fnns even much less number paramet fnns,"['Emad M. Grais', 'Mark D. Plumbley']",['cs.SD'],False,False,False,False,True,False
805,2017-03-28T14:10:52Z,2017-03-21T12:35:21Z,http://arxiv.org/abs/1703.07172v1,http://arxiv.org/pdf/1703.07172v1,Multi-Objective Learning and Mask-Based Post-Processing for Deep Neural   Network Based Speech Enhancement,multi object learn mask base post process deep neural network base speech enhanc,"We propose a multi-objective framework to learn both secondary targets not directly related to the intended task of speech enhancement (SE) and the primary target of the clean log-power spectra (LPS) features to be used directly for constructing the enhanced speech signals. In deep neural network (DNN) based SE we introduce an auxiliary structure to learn secondary continuous features, such as mel-frequency cepstral coefficients (MFCCs), and categorical information, such as the ideal binary mask (IBM), and integrate it into the original DNN architecture for joint optimization of all the parameters. This joint estimation scheme imposes additional constraints not available in the direct prediction of LPS, and potentially improves the learning of the primary target. Furthermore, the learned secondary information as a byproduct can be used for other purposes, e.g., the IBM-based post-processing in this work. A series of experiments show that joint LPS and MFCC learning improves the SE performance, and IBM-based post-processing further enhances listening quality of the reconstructed speech.",propos multi object framework learn secondari target direct relat intend task speech enhanc se primari target clean log power spectra lps featur use direct construct enhanc speech signal deep neural network dnn base se introduc auxiliari structur learn secondari continu featur mel frequenc cepstral coeffici mfccs categor inform ideal binari mask ibm integr origin dnn architectur joint optim paramet joint estim scheme impos addit constraint avail direct predict lps potenti improv learn primari target furthermor learn secondari inform byproduct use purpos ibm base post process work seri experi show joint lps mfcc learn improv se perform ibm base post process enhanc listen qualiti reconstruct speech,"['Yong Xu', 'Jun Du', 'Zhen Huang', 'Li-Rong Dai', 'Chin-Hui Lee']",['cs.SD'],False,False,False,False,True,False
806,2017-03-28T14:10:52Z,2017-03-21T06:09:32Z,http://arxiv.org/abs/1703.07065v1,http://arxiv.org/pdf/1703.07065v1,Adaptive Multi-Class Audio Classification in Noisy In-Vehicle   Environment,adapt multi class audio classif noisi vehicl environ,"With ever-increasing number of car-mounted electric devices and their complexity, audio classification is increasingly important for the automotive industry as a fundamental tool for human-device interactions. Existing approaches for audio classification, however, fall short as the unique and dynamic audio characteristics of in-vehicle environments are not appropriately taken into account. In this paper, we develop an audio classification system that classifies an audio stream into music, speech, speech+music, and noise, adaptably depending on driving environments including highway, local road, crowded city, and stopped vehicle. More than 420 minutes of audio data including various genres of music, speech, speech+music, and noise are collected from diverse driving environments. The results demonstrate that the proposed approach improves the average classification accuracy up to 166%, and 64% for speech, and speech+music, respectively, compared with a non-adaptive approach in our experimental settings.",ever increas number car mount electr devic complex audio classif increas import automot industri fundament tool human devic interact exist approach audio classif howev fall short uniqu dynam audio characterist vehicl environ appropri taken account paper develop audio classif system classifi audio stream music speech speech music nois adapt depend drive environ includ highway local road crowd citi stop vehicl minut audio data includ various genr music speech speech music nois collect divers drive environ result demonstr propos approach improv averag classif accuraci speech speech music respect compar non adapt approach experiment set,"['Myounggyu Won', 'Haitham Alsaadan', 'Yongsoon Eun']",['cs.SD'],False,False,False,False,True,False
807,2017-03-28T14:10:52Z,2017-03-20T18:11:47Z,http://arxiv.org/abs/1703.06902v1,http://arxiv.org/pdf/1703.06902v1,A Comparison of deep learning methods for environmental sound,comparison deep learn method environment sound,"Environmental sound detection is a challenging application of machine learning because of the noisy nature of the signal, and the small amount of (labeled) data that is typically available. This work thus presents a comparison of several state-of-the-art Deep Learning models on the IEEE challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) 2016 challenge task and data, classifying sounds into one of fifteen common indoor and outdoor acoustic scenes, such as bus, cafe, car, city center, forest path, library, train, etc. In total, 13 hours of stereo audio recordings are available, making this one of the largest datasets available. We perform experiments on six sets of features, including standard Mel-frequency cepstral coefficients (MFCC), Binaural MFCC, log Mel-spectrum and two different large- scale temporal pooling features extracted using OpenSMILE. On these features, we apply five models: Gaussian Mixture Model (GMM), Deep Neural Network (DNN), Recurrent Neural Network (RNN), Convolutional Deep Neural Net- work (CNN) and i-vector. Using the late-fusion approach, we improve the performance of the baseline 72.5% by 15.6% in 4-fold Cross Validation (CV) avg. accuracy and 11% in test accuracy, which matches the best result of the DCASE 2016 challenge. With large feature sets, deep neural network models out- perform traditional methods and achieve the best performance among all the studied methods. Consistent with other work, the best performing single model is the non-temporal DNN model, which we take as evidence that sounds in the DCASE challenge do not exhibit strong temporal dynamics.",environment sound detect challeng applic machin learn becaus noisi natur signal small amount label data typic avail work thus present comparison sever state art deep learn model ieee challeng detect classif acoust scene event dcase challeng task data classifi sound one fifteen common indoor outdoor acoust scene bus cafe car citi center forest path librari train etc total hour stereo audio record avail make one largest dataset avail perform experi six set featur includ standard mel frequenc cepstral coeffici mfcc binaur mfcc log mel spectrum two differ larg scale tempor pool featur extract use opensmil featur appli five model gaussian mixtur model gmm deep neural network dnn recurr neural network rnn convolut deep neural net work cnn vector use late fusion approach improv perform baselin fold cross valid cv avg accuraci test accuraci match best result dcase challeng larg featur set deep neural network model perform tradit method achiev best perform among studi method consist work best perform singl model non tempor dnn model take evid sound dcase challeng exhibit strong tempor dynam,"['Juncheng Li', 'Wei Dai', 'Florian Metze', 'Shuhui Qu', 'Samarjit Das']","['cs.SD', 'cs.LG', '14J60 (Primary)']",False,False,False,False,True,False
810,2017-03-28T14:10:56Z,2017-03-20T12:00:04Z,http://arxiv.org/abs/1703.06697v1,http://arxiv.org/pdf/1703.06697v1,Timbre Analysis of Music Audio Signals with Convolutional Neural   Networks,timbr analysi music audio signal convolut neural network,"The focus of this work is to study how to efficiently tailor Convolutional Neural Networks (CNNs) towards learning timbre representations from log-mel magnitude spectrograms. We first review the trends when designing CNN architectures. Through this literature overview we discuss which are the crucial points to consider for efficiently learning timbre representations using CNNs. From this discussion we propose a design strategy meant to capture the relevant time-frequency contexts for learning timbre, which permits using domain knowledge for designing architectures. In addition, one of our main goals is to design efficient CNN architectures - what reduces the risk of these models to over-fit, since CNNs' number of parameters is minimized. Several architectures based on the design principles we propose are successfully assessed for different research tasks related to timbre: singing voice phoneme classification, musical instrument recognition and music auto-tagging.",focus work studi effici tailor convolut neural network cnns toward learn timbr represent log mel magnitud spectrogram first review trend design cnn architectur literatur overview discuss crucial point consid effici learn timbr represent use cnns discuss propos design strategi meant captur relev time frequenc context learn timbr permit use domain knowledg design architectur addit one main goal design effici cnn architectur reduc risk model fit sinc cnns number paramet minim sever architectur base design principl propos success assess differ research task relat timbr sing voic phonem classif music instrument recognit music auto tag,"['Jordi Pons', 'Olga Slizovskaia', 'Rong Gong', 'Emilia Gómez', 'Xavier Serra']",['cs.SD'],False,False,False,False,True,False
812,2017-03-28T14:10:56Z,2017-03-18T10:59:03Z,http://arxiv.org/abs/1703.06284v1,http://arxiv.org/pdf/1703.06284v1,Multi-talker Speech Separation and Tracing with Permutation Invariant   Training of Deep Recurrent Neural Networks,multi talker speech separ trace permut invari train deep recurr neural network,"Despite the significant progress made in the recent years in dictating single-talker speech, the progress made in speaker independent multi-talker mixed speech separation and tracing, often referred to as the cocktail-party problem, has been less impressive. In this paper we propose a novel technique for attacking this problem. The core of our technique is permutation invariant training (PIT), which aims at minimizing the source stream reconstruction error no matter how labels are ordered. This is achieved by aligning labels to the output streams automatically during the training time. This strategy effectively solves the label permutation problem observed in deep learning based techniques for speech separation. More interestingly, our approach can integrate speaker tracing in the PIT framework so that separation and tracing can be carried out in one step and trained end-to-end. This is achieved using recurrent neural networks (RNNs) by forcing separated frames belonging to the same speaker to be aligned to the same output layer during training. Furthermore, the computational cost introduced by PIT is very small compared to the RNN computation during training and is zero during separation. We evaluated PIT on the WSJ0 and Danish two- and three-talker mixed-speech separation tasks and found that it compares favorably to non-negative matrix factorization (NMF), computational auditory scene analysis (CASA), deep clustering (DPCL) and deep attractor network (DANet), and generalizes well over unseen speakers and languages.",despit signific progress made recent year dictat singl talker speech progress made speaker independ multi talker mix speech separ trace often refer cocktail parti problem less impress paper propos novel techniqu attack problem core techniqu permut invari train pit aim minim sourc stream reconstruct error matter label order achiev align label output stream automat dure train time strategi effect solv label permut problem observ deep learn base techniqu speech separ interest approach integr speaker trace pit framework separ trace carri one step train end end achiev use recurr neural network rnns forc separ frame belong speaker align output layer dure train furthermor comput cost introduc pit veri small compar rnn comput dure train zero dure separ evalu pit wsj danish two three talker mix speech separ task found compar favor non negat matrix factor nmf comput auditori scene analysi casa deep cluster dpcl deep attractor network danet general well unseen speaker languag,"['Morten Kolbæk', 'Dong Yu', 'Zheng-Hua Tan', 'Jesper Jensen']","['cs.SD', 'cs.LG']",False,False,False,False,True,False
814,2017-03-28T14:10:56Z,2017-03-17T03:38:48Z,http://arxiv.org/abs/1703.05880v1,http://arxiv.org/pdf/1703.05880v1,Empirical Evaluation of Parallel Training Algorithms on Acoustic   Modeling,empir evalu parallel train algorithm acoust model,"Deep learning models (DLMs) are state-of-the-art techniques in speech recognition. However, training good DLMs can be time consuming especially for production-size models and corpora. Although several parallel training algorithms have been proposed to improve training efficiency, there is no clear guidance on which one to choose for the task in hand due to lack of systematic and fair comparison among them. In this paper we aim at filling this gap by comparing four popular parallel training algorithms in speech recognition, namely asynchronous stochastic gradient descent (ASGD), blockwise model-update filtering (BMUF), bulk synchronous parallel (BSP) and elastic averaging stochastic gradient descent (EASGD), on 1000-hour LibriSpeech corpora using feed-forward deep neural networks (DNNs) and convolutional, long short-term memory, DNNs (CLDNNs). Based on our experiments, we recommend using BMUF as the top choice to train acoustic models since it is most stable, scales well with number of GPUs, can achieve reproducible results, and in many cases even outperforms single-GPU SGD. ASGD can be used as a substitute in some cases.",deep learn model dlms state art techniqu speech recognit howev train good dlms time consum especi product size model corpora although sever parallel train algorithm propos improv train effici clear guidanc one choos task hand due lack systemat fair comparison among paper aim fill gap compar four popular parallel train algorithm speech recognit name asynchron stochast gradient descent asgd blockwis model updat filter bmuf bulk synchron parallel bsp elast averag stochast gradient descent easgd hour librispeech corpora use feed forward deep neural network dnns convolut long short term memori dnns cldnns base experi recommend use bmuf top choic train acoust model sinc stabl scale well number gpus achiev reproduc result mani case even outperform singl gpu sgd asgd use substitut case,"['Wenpeng Li', 'BinBin Zhang', 'Lei Xie', 'Dong Yu']","['cs.CL', 'cs.LG', 'cs.SD']",False,False,False,False,True,False
815,2017-03-28T14:10:56Z,2017-03-15T18:41:37Z,http://arxiv.org/abs/1703.05344v1,http://arxiv.org/pdf/1703.05344v1,Deducing the severity of psychiatric symptoms from the human voice,deduc sever psychiatr symptom human voic,"Psychiatric illnesses are often associated with multiple symptoms, whose severity must be graded for accurate diagnosis and treatment. This grading is usually done by trained clinicians based on human observations and judgments made within doctor-patient sessions. Current research provides sufficient reason to expect that the human voice may carry biomarkers or signatures of many, if not all, these symptoms. Based on this conjecture, we explore the possibility of objectively and automatically grading the symptoms of psychiatric illnesses with reference to various standard psychiatric rating scales. Using acoustic data from several clinician-patient interviews within hospital settings, we use non-parametric models to learn and predict the relations between symptom-ratings and voice. In the process, we show that different articulatory-phonetic units of speech are able to capture the effects of different symptoms differently, and use this to establish a plausible methodology that could be employed for automatically grading psychiatric symptoms for clinical purposes.",psychiatr ill often associ multipl symptom whose sever must grade accur diagnosi treatment grade usual done train clinician base human observ judgment made within doctor patient session current research provid suffici reason expect human voic may carri biomark signatur mani symptom base conjectur explor possibl object automat grade symptom psychiatr ill refer various standard psychiatr rate scale use acoust data sever clinician patient interview within hospit set use non parametr model learn predict relat symptom rate voic process show differ articulatori phonet unit speech abl captur effect differ symptom differ use establish plausibl methodolog could employ automat grade psychiatr symptom clinic purpos,"['Rita Singh', 'Justin Baker', 'Luciana Pennant', 'Louis-Philippe Morency']","['cs.SD', 'q-bio.NC']",False,False,False,False,True,False
816,2017-03-28T14:10:56Z,2017-03-15T08:33:38Z,http://arxiv.org/abs/1703.05003v1,http://arxiv.org/pdf/1703.05003v1,On the Importance of Super-Gaussian Speech Priors for Pre-Trained Speech   Enhancement,import super gaussian speech prior pre train speech enhanc,"For enhancing noisy signals, pre-trained single-channel speech enhancement schemes exploit prior knowledge about the shape of typical speech structures. This knowledge is obtained from training data for which methods from machine learning are used, e.g., Mixtures of Gaussians, nonnegative matrix factorization, and deep neural networks. If only speech envelopes are employed as prior speech knowledge, e.g., to meet requirements in terms of computational complexity and memory consumption, Wiener-like enhancement filters will not be able to reduce noise components between speech spectral harmonics. In this paper, we highlight the role of clean speech estimators that employ super-Gaussian speech priors in particular for pre- trained approaches when spectral envelope models are used. In the 2000s, such estimators have been considered by many researchers for improving non-trained enhancement schemes. However, while the benefit of super-Gaussian clean speech estimators in non-trained enhancement schemes is limited, we point out that these estimators make a much larger difference for enhancement schemes that employ pre-trained envelope models. We show that for such pre-trained enhancements schemes super- Gaussian estimators allow for a suppression of annoying residual noises which are not reduced using Gaussian filters such as the Wiener filter. As a consequence, considerable improvements in terms of Perceptual Evaluation of Speech Quality and segmental signal-to-noise ratios are achieved.",enhanc noisi signal pre train singl channel speech enhanc scheme exploit prior knowledg shape typic speech structur knowledg obtain train data method machin learn use mixtur gaussian nonneg matrix factor deep neural network onli speech envelop employ prior speech knowledg meet requir term comput complex memori consumpt wiener like enhanc filter abl reduc nois compon speech spectral harmon paper highlight role clean speech estim employ super gaussian speech prior particular pre train approach spectral envelop model use estim consid mani research improv non train enhanc scheme howev benefit super gaussian clean speech estim non train enhanc scheme limit point estim make much larger differ enhanc scheme employ pre train envelop model show pre train enhanc scheme super gaussian estim allow suppress annoy residu nois reduc use gaussian filter wiener filter consequ consider improv term perceptu evalu speech qualiti segment signal nois ratio achiev,"['Robert Rehr', 'Timo Gerkmann']",['cs.SD'],False,False,False,False,True,False
817,2017-03-28T14:10:56Z,2017-03-14T22:28:51Z,http://arxiv.org/abs/1703.04783v1,http://arxiv.org/pdf/1703.04783v1,Multichannel End-to-end Speech Recognition,multichannel end end speech recognit,"The field of speech recognition is in the midst of a paradigm shift: end-to-end neural networks are challenging the dominance of hidden Markov models as a core technology. Using an attention mechanism in a recurrent encoder-decoder architecture solves the dynamic time alignment problem, allowing joint end-to-end training of the acoustic and language modeling components. In this paper we extend the end-to-end framework to encompass microphone array signal processing for noise suppression and speech enhancement within the acoustic encoding network. This allows the beamforming components to be optimized jointly within the recognition architecture to improve the end-to-end speech recognition objective. Experiments on the noisy speech benchmarks (CHiME-4 and AMI) show that our multichannel end-to-end system outperformed the attention-based baseline with input from a conventional adaptive beamformer.",field speech recognit midst paradigm shift end end neural network challeng domin hidden markov model core technolog use attent mechan recurr encod decod architectur solv dynam time align problem allow joint end end train acoust languag model compon paper extend end end framework encompass microphon array signal process nois suppress speech enhanc within acoust encod network allow beamform compon optim joint within recognit architectur improv end end speech recognit object experi noisi speech benchmark chime ami show multichannel end end system outperform attent base baselin input convent adapt beamform,"['Tsubasa Ochiai', 'Shinji Watanabe', 'Takaaki Hori', 'John R. Hershey']","['cs.SD', 'cs.CL']",False,False,False,False,True,False
820,2017-03-28T14:11:00Z,2017-03-07T10:36:30Z,http://arxiv.org/abs/1703.02317v1,http://arxiv.org/pdf/1703.02317v1,Convolutional Recurrent Neural Networks for Bird Audio Detection,convolut recurr neural network bird audio detect,"Bird sounds possess distinctive spectral structure which may exhibit small shifts in spectrum depending on the bird species and environmental conditions. In this paper, we propose using convolutional recurrent neural networks on the task of automated bird audio detection in real-life environments. In the proposed method, convolutional layers extract high dimensional, local frequency shift invariant features, while recurrent layers capture longer term dependencies between the features extracted from short time frames. This method achieves 88.5% Area Under ROC Curve (AUC) score on the unseen evaluation data and obtains the second place in the Bird Audio Detection challenge.",bird sound possess distinct spectral structur may exhibit small shift spectrum depend bird speci environment condit paper propos use convolut recurr neural network task autom bird audio detect real life environ propos method convolut layer extract high dimension local frequenc shift invari featur recurr layer captur longer term depend featur extract short time frame method achiev area roc curv auc score unseen evalu data obtain second place bird audio detect challeng,"['EmreÇakır', 'Sharath Adavanne', 'Giambattista Parascandolo', 'Konstantinos Drossos', 'Tuomas Virtanen']","['cs.SD', 'cs.LG', 'stat.ML']",False,False,False,False,True,False
821,2017-03-28T14:11:00Z,2017-03-19T09:51:36Z,http://arxiv.org/abs/1703.02205v2,http://arxiv.org/pdf/1703.02205v2,Raw Waveform-based Speech Enhancement by Fully Convolutional Networks,raw waveform base speech enhanc fulli convolut network,"This study proposes a fully convolutional network (FCN) model for raw waveform-based speech enhancement. The proposed system performs speech enhancement in an end-to-end (i.e., waveform-in and waveform-out) manner, which dif-fers from most existing denoising methods that process the magnitude spectrum (e.g., log power spectrum (LPS)) only. Because the fully connected layers, which are involved in deep neural networks (DNN) and convolutional neural networks (CNN), may not accurately characterize the local information of speech signals, particularly with high frequency components, we employed fully convolutional layers to model the waveform. More specifically, FCN consists of only convolutional layers and thus the local temporal structures of speech signals can be efficiently and effectively preserved with relatively few weights. Experimental results show that DNN- and CNN-based models have limited capability to restore high frequency components of waveforms, thus leading to decreased intelligibility of enhanced speech. By contrast, the proposed FCN model can not only effectively recover the waveforms but also outperform the LPS-based DNN baseline in terms of short-time objective intelligibility (STOI) and perceptual evaluation of speech quality (PESQ). In addition, the number of model parameters in FCN is approximately only 0.2% compared with that in both DNN and CNN.",studi propos fulli convolut network fcn model raw waveform base speech enhanc propos system perform speech enhanc end end waveform waveform manner dif fer exist denois method process magnitud spectrum log power spectrum lps onli becaus fulli connect layer involv deep neural network dnn convolut neural network cnn may accur character local inform speech signal particular high frequenc compon employ fulli convolut layer model waveform specif fcn consist onli convolut layer thus local tempor structur speech signal effici effect preserv relat weight experiment result show dnn cnn base model limit capabl restor high frequenc compon waveform thus lead decreas intellig enhanc speech contrast propos fcn model onli effect recov waveform also outperform lps base dnn baselin term short time object intellig stoi perceptu evalu speech qualiti pesq addit number model paramet fcn approxim onli compar dnn cnn,"['Szu-Wei Fu', 'Yu Tsao', 'Xugang Lu', 'Hisashi Kawai']","['stat.ML', 'cs.LG', 'cs.SD']",False,False,False,False,True,False
826,2017-03-28T14:11:00Z,2017-02-28T11:00:19Z,http://arxiv.org/abs/1703.00384v1,http://arxiv.org/pdf/1703.00384v1,Nonlinear Volterra model of a loudspeaker behavior based on Laser   Doppler Vibrometry,nonlinear volterra model loudspeak behavior base laser doppler vibrometri,"We demonstrate the capabilities of nonlinear Volterra models to simulate the behavior of an audio system and compare them to linear filters. In this paper a nonlinear model of an audio system based on Volterra series is presented and Normalized Least Mean Square algorithm is used to determine the Volterra series to third order. Training data for the models were collected measuring a physical speaker using a laser interferometer. We explore several training signals and filter's parameters. Results indicate a decrease in Mean Squared Error compared to the linear model with a dependency on the particular test signal, the order and the parameters of the model.",demonstr capabl nonlinear volterra model simul behavior audio system compar linear filter paper nonlinear model audio system base volterra seri present normal least mean squar algorithm use determin volterra seri third order train data model collect measur physic speaker use laser interferomet explor sever train signal filter paramet result indic decreas mean squar error compar linear model depend particular test signal order paramet model,"['Alessandro Loriga', 'Parvin Moyassari', 'Daniele Bernardini', 'Gregorio Landi', 'Francesca Venturini', 'Elisabeth Dumont']","['cs.SD', 'H.5.5; I.2.6; I.6.3; J.5']",False,False,False,False,True,False
827,2017-03-28T14:11:00Z,2017-03-07T23:09:23Z,http://arxiv.org/abs/1702.07825v2,http://arxiv.org/pdf/1702.07825v2,Deep Voice: Real-time Neural Text-to-Speech,deep voic real time neural text speech,"We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.",present deep voic product qualiti text speech system construct entir deep neural network deep voic lay groundwork truli end end neural speech synthesi system compris five major build block segment model locat phonem boundari graphem phonem convers model phonem durat predict model fundament frequenc predict model audio synthesi model segment model propos novel way perform phonem boundari detect deep neural network use connectionist tempor classif ctc loss audio synthesi model implement variant wavenet requir fewer paramet train faster origin use neural network compon system simpler flexibl tradit text speech system compon requir labori featur engin extens domain expertis final show infer system perform faster real time describ optim wavenet infer kernel cpu gpu achiev speedup exist implement,"['Sercan O. Arik', 'Mike Chrzanowski', 'Adam Coates', 'Gregory Diamos', 'Andrew Gibiansky', 'Yongguo Kang', 'Xian Li', 'John Miller', 'Andrew Ng', 'Jonathan Raiman', 'Shubho Sengupta', 'Mohammad Shoeybi']","['cs.CL', 'cs.LG', 'cs.NE', 'cs.SD']",False,False,False,False,True,False
828,2017-03-28T14:11:00Z,2017-02-24T22:27:29Z,http://arxiv.org/abs/1702.07787v1,http://arxiv.org/pdf/1702.07787v1,Convolutional Gated Recurrent Neural Network Incorporating Spatial   Features for Audio Tagging,convolut gate recurr neural network incorpor spatial featur audio tag,"Environmental audio tagging is a newly proposed task to predict the presence or absence of a specific audio event in a chunk. Deep neural network (DNN) based methods have been successfully adopted for predicting the audio tags in the domestic audio scene. In this paper, we propose to use a convolutional neural network (CNN) to extract robust features from mel-filter banks (MFBs), spectrograms or even raw waveforms for audio tagging. Gated recurrent unit (GRU) based recurrent neural networks (RNNs) are then cascaded to model the long-term temporal structure of the audio signal. To complement the input information, an auxiliary CNN is designed to learn on the spatial features of stereo recordings. We evaluate our proposed methods on Task 4 (audio tagging) of the Detection and Classification of Acoustic Scenes and Events 2016 (DCASE 2016) challenge. Compared with our recent DNN-based method, the proposed structure can reduce the equal error rate (EER) from 0.13 to 0.11 on the development set. The spatial features can further reduce the EER to 0.10. The performance of the end-to-end learning on raw waveforms is also comparable. Finally, on the evaluation set, we get the state-of-the-art performance with 0.12 EER while the performance of the best existing system is 0.15 EER.",environment audio tag newli propos task predict presenc absenc specif audio event chunk deep neural network dnn base method success adopt predict audio tag domest audio scene paper propos use convolut neural network cnn extract robust featur mel filter bank mfbs spectrogram even raw waveform audio tag gate recurr unit gru base recurr neural network rnns cascad model long term tempor structur audio signal complement input inform auxiliari cnn design learn spatial featur stereo record evalu propos method task audio tag detect classif acoust scene event dcase challeng compar recent dnn base method propos structur reduc equal error rate eer develop set spatial featur reduc eer perform end end learn raw waveform also compar final evalu set get state art perform eer perform best exist system eer,"['Yong Xu', 'Qiuqiang Kong', 'Qiang Huang', 'Wenwu Wang', 'Mark D. Plumbley']","['cs.SD', 'cs.LG', 'cs.NE']",False,False,False,False,True,False
829,2017-03-28T14:11:00Z,2017-02-24T17:23:01Z,http://arxiv.org/abs/1702.07713v1,http://arxiv.org/pdf/1702.07713v1,Multichannel Linear Prediction for Blind Reverberant Audio Source   Separation,multichannel linear predict blind reverber audio sourc separ,"A class of methods based on multichannel linear prediction (MCLP) can achieve effective blind dereverberation of a source, when the source is observed with a microphone array. We propose an inventive use of MCLP as a pre-processing step for blind source separation with a microphone array. We show theoretically that, under certain assumptions, such pre-processing reduces the original blind reverberant source separation problem to a non-reverberant one, which in turn can be effectively tackled using existing methods. We demonstrate our claims using real recordings obtained with an eight-microphone circular array in reverberant environments.",class method base multichannel linear predict mclp achiev effect blind dereverber sourc sourc observ microphon array propos invent use mclp pre process step blind sourc separ microphon array show theoret certain assumpt pre process reduc origin blind reverber sourc separ problem non reverber one turn effect tackl use exist method demonstr claim use real record obtain eight microphon circular array reverber environ,"['İlker Bayram', 'Savaşkan Bulek']","['cs.SD', 'cs.CE']",False,False,False,False,True,False
830,2017-03-28T14:11:05Z,2017-02-23T02:31:03Z,http://arxiv.org/abs/1702.07071v1,http://arxiv.org/pdf/1702.07071v1,"Pronunciation recognition of English phonemes /\textipa{@}/, /æ/,   /\textipa{A}:/ and /\textipa{2}/ using Formants and Mel Frequency Cepstral   Coefficients",pronunci recognit english phonem textipa textipa textipa use formant mel frequenc cepstral coeffici,"The Vocal Joystick Vowel Corpus, by Washington University, was used to study monophthongs pronounced by native English speakers. The objective of this study was to quantitatively measure the extent at which speech recognition methods can distinguish between similar sounding vowels. In particular, the phonemes /\textipa{@}/, /{\ae}/, /\textipa{A}:/ and /\textipa{2}/ were analysed. 748 sound files from the corpus were used and subjected to Linear Predictive Coding (LPC) to compute their formants, and to Mel Frequency Cepstral Coefficients (MFCC) algorithm, to compute the cepstral coefficients. A Decision Tree Classifier was used to build a predictive model that learnt the patterns of the two first formants measured in the data set, as well as the patterns of the 13 cepstral coefficients. An accuracy of 70\% was achieved using formants for the mentioned phonemes. For the MFCC analysis an accuracy of 52 \% was achieved and an accuracy of 71\% when /\textipa{@}/ was ignored. The results obtained show that the studied algorithms are far from mimicking the ability of distinguishing subtle differences in sounds like human hearing does.",vocal joystick vowel corpus washington univers use studi monophthong pronounc nativ english speaker object studi quantit measur extent speech recognit method distinguish similar sound vowel particular phonem textipa ae textipa textipa analys sound file corpus use subject linear predict code lpc comput formant mel frequenc cepstral coeffici mfcc algorithm comput cepstral coeffici decis tree classifi use build predict model learnt pattern two first formant measur data set well pattern cepstral coeffici accuraci achiev use formant mention phonem mfcc analysi accuraci achiev accuraci textipa ignor result obtain show studi algorithm far mimick abil distinguish subtl differ sound like human hear doe,"['Keith Y. Patarroyo', 'Vladimir Vargas-Calderón']","['cs.CL', 'cs.SD']",False,False,False,False,True,False
832,2017-03-28T14:11:05Z,2017-02-21T07:37:59Z,http://arxiv.org/abs/1702.06286v1,http://arxiv.org/pdf/1702.06286v1,Convolutional Recurrent Neural Networks for Polyphonic Sound Event   Detection,convolut recurr neural network polyphon sound event detect,"Sound events often occur in unstructured environments where they exhibit wide variations in their frequency content and temporal structure. Convolutional neural networks (CNN) are able to extract higher level features that are invariant to local spectral and temporal variations. Recurrent neural networks (RNNs) are powerful in learning the longer term temporal context in the audio signals. CNNs and RNNs as classifiers have recently shown improved performances over established methods in various sound recognition tasks. We combine these two approaches in a Convolutional Recurrent Neural Network (CRNN) and apply it on a polyphonic sound event detection task. We compare the performance of the proposed CRNN method with CNN, RNN, and other established methods, and observe a considerable improvement for four different datasets consisting of everyday sound events.",sound event often occur unstructur environ exhibit wide variat frequenc content tempor structur convolut neural network cnn abl extract higher level featur invari local spectral tempor variat recurr neural network rnns power learn longer term tempor context audio signal cnns rnns classifi recent shown improv perform establish method various sound recognit task combin two approach convolut recurr neural network crnn appli polyphon sound event detect task compar perform propos crnn method cnn rnn establish method observ consider improv four differ dataset consist everyday sound event,"['Emre Çakır', 'Giambattista Parascandolo', 'Toni Heittola', 'Heikki Huttunen', 'Tuomas Virtanen']","['cs.LG', 'cs.SD']",False,False,False,False,True,False
833,2017-03-28T14:11:05Z,2017-02-13T14:44:17Z,http://arxiv.org/abs/1702.03791v1,http://arxiv.org/pdf/1702.03791v1,DNN Filter Bank Cepstral Coefficients for Spoofing Detection,dnn filter bank cepstral coeffici spoof detect,"With the development of speech synthesis techniques, automatic speaker verification systems face the serious challenge of spoofing attack. In order to improve the reliability of speaker verification systems, we develop a new filter bank based cepstral feature, deep neural network filter bank cepstral coefficients (DNN-FBCC), to distinguish between natural and spoofed speech. The deep neural network filter bank is automatically generated by training a filter bank neural network (FBNN) using natural and synthetic speech. By adding restrictions on the training rules, the learned weight matrix of FBNN is band-limited and sorted by frequency, similar to the normal filter bank. Unlike the manually designed filter bank, the learned filter bank has different filter shapes in different channels, which can capture the differences between natural and synthetic speech more effectively. The experimental results on the ASVspoof {2015} database show that the Gaussian mixture model maximum-likelihood (GMM-ML) classifier trained by the new feature performs better than the state-of-the-art linear frequency cepstral coefficients (LFCC) based classifier, especially on detecting unknown attacks.",develop speech synthesi techniqu automat speaker verif system face serious challeng spoof attack order improv reliabl speaker verif system develop new filter bank base cepstral featur deep neural network filter bank cepstral coeffici dnn fbcc distinguish natur spoof speech deep neural network filter bank automat generat train filter bank neural network fbnn use natur synthet speech ad restrict train rule learn weight matrix fbnn band limit sort frequenc similar normal filter bank unlik manual design filter bank learn filter bank differ filter shape differ channel captur differ natur synthet speech effect experiment result asvspoof databas show gaussian mixtur model maximum likelihood gmm ml classifi train new featur perform better state art linear frequenc cepstral coeffici lfcc base classifi especi detect unknown attack,"['Hong Yu', 'Zheng-Hua Tan', 'Zhanyu Ma', 'Jun Guo']","['cs.SD', 'cs.CR', 'cs.LG']",False,False,False,False,True,False
834,2017-03-28T14:11:05Z,2017-02-08T04:59:00Z,http://arxiv.org/abs/1702.02289v1,http://arxiv.org/pdf/1702.02289v1,Neural Network Based Speaker Classification and Verification Systems   with Enhanced Features,neural network base speaker classif verif system enhanc featur,"This work presents a novel framework based on feed-forward neural network for text-independent speaker classification and verification, two related systems of speaker recognition. With optimized features and model training, it achieves 100% classification rate in classification and less than 6% Equal Error Rate (ERR), using merely about 1 second and 5 seconds of data respectively. Features with stricter Voice Active Detection (VAD) than the regular one for speech recognition ensure extracting stronger voiced portion for speaker recognition, speaker-level mean and variance normalization helps to eliminate the discrepancy between samples from the same speaker. Both are proven to improve the system performance. In building the neural network speaker classifier, the network structure parameters are optimized with grid search and dynamically reduced regularization parameters are used to avoid training terminated in local minimum. It enables the training goes further with lower cost. In speaker verification, performance is improved with prediction score normalization, which rewards the speaker identity indices with distinct peaks and penalizes the weak ones with high scores but more competitors, and speaker-specific thresholding, which significantly reduces ERR in the ROC curve. TIMIT corpus with 8K sampling rate is used here. First 200 male speakers are used to train and test the classification performance. The testing files of them are used as in-domain registered speakers, while data from the remaining 126 male speakers are used as out-of-domain speakers, i.e. imposters in speaker verification.",work present novel framework base feed forward neural network text independ speaker classif verif two relat system speaker recognit optim featur model train achiev classif rate classif less equal error rate err use mere second second data respect featur stricter voic activ detect vad regular one speech recognit ensur extract stronger voic portion speaker recognit speaker level mean varianc normal help elimin discrep sampl speaker proven improv system perform build neural network speaker classifi network structur paramet optim grid search dynam reduc regular paramet use avoid train termin local minimum enabl train goe lower cost speaker verif perform improv predict score normal reward speaker ident indic distinct peak penal weak one high score competitor speaker specif threshold signific reduc err roc curv timit corpus sampl rate use first male speaker use train test classif perform test file use domain regist speaker data remain male speaker use domain speaker impost speaker verif,"['Zhenhao Ge', 'Ananth N. Iyer', 'Srinath Cheluvaraja', 'Ram Sundaram', 'Aravind Ganapathiraju']",['cs.SD'],False,False,False,False,True,False
835,2017-03-28T14:11:05Z,2017-02-08T04:37:40Z,http://arxiv.org/abs/1702.02285v1,http://arxiv.org/pdf/1702.02285v1,Speaker Change Detection Using Features through A Neural Network Speaker   Classifier,speaker chang detect use featur neural network speaker classifi,"The mechanism proposed here is for real-time speaker change detection in conversations, which firstly trains a neural network text-independent speaker classifier using in-domain speaker data. Through the network, features of conversational speech from out-of-domain speakers are then converted into likelihood vectors, i.e. similarity scores comparing to the in-domain speakers. These transformed features demonstrate very distinctive patterns, which facilitates differentiating speakers and enable speaker change detection with some straight-forward distance metrics. The speaker classifier and the speaker change detector are trained/tested using speech of the first 200 (in-domain) and the remaining 126 (out-of-domain) male speakers in TIMIT respectively. For the speaker classification, 100% accuracy at a 200 speaker size is achieved on any testing file, given the speech duration is at least 0.97 seconds. For the speaker change detection using speaker classification outputs, performance based on 0.5, 1, and 2 seconds of inspection intervals were evaluated in terms of error rate and F1 score, using synthesized data by concatenating speech from various speakers. It captures close to 97% of the changes by comparing the current second of speech with the previous second, which is very competitive among literature using other methods.",mechan propos real time speaker chang detect convers first train neural network text independ speaker classifi use domain speaker data network featur convers speech domain speaker convert likelihood vector similar score compar domain speaker transform featur demonstr veri distinct pattern facilit differenti speaker enabl speaker chang detect straight forward distanc metric speaker classifi speaker chang detector train test use speech first domain remain domain male speaker timit respect speaker classif accuraci speaker size achiev ani test file given speech durat least second speaker chang detect use speaker classif output perform base second inspect interv evalu term error rate score use synthes data concaten speech various speaker captur close chang compar current second speech previous second veri competit among literatur use method,"['Zhenhao Ge', 'Ananth N. Iyer', 'Srinath Cheluvaraja', 'Aravind Ganapathiraju']",['cs.SD'],False,False,False,False,True,False
836,2017-03-28T14:11:05Z,2017-02-07T18:41:31Z,http://arxiv.org/abs/1702.02130v1,http://arxiv.org/pdf/1702.02130v1,On the Importance of Temporal Context in Proximity Kernels: A Vocal   Separation Case Study,import tempor context proxim kernel vocal separ case studi,"Musical source separation methods exploit source-specific spectral characteristics to facilitate the decomposition process. Kernel Additive Modelling (KAM) models a source applying robust statistics to time-frequency bins as specified by a source-specific kernel, a function defining similarity between bins. Kernels in existing approaches are typically defined using metrics between single time frames. In the presence of noise and other sound sources information from a single-frame, however, turns out to be unreliable and often incorrect frames are selected as similar. In this paper, we incorporate a temporal context into the kernel to provide additional information stabilizing the similarity search. Evaluated in the context of vocal separation, our simple extension led to a considerable improvement in separation quality compared to previous kernels.",music sourc separ method exploit sourc specif spectral characterist facilit decomposit process kernel addit model kam model sourc appli robust statist time frequenc bin specifi sourc specif kernel function defin similar bin kernel exist approach typic defin use metric singl time frame presenc nois sound sourc inform singl frame howev turn unreli often incorrect frame select similar paper incorpor tempor context kernel provid addit inform stabil similar search evalu context vocal separ simpl extens led consider improv separ qualiti compar previous kernel,"['Delia Fano Yela', 'Sebastian Ewert', 'Derry FitzGerald', 'Mark Sandler']","['cs.SD', 'H.5.5']",False,False,False,False,True,False
837,2017-03-28T14:11:05Z,2017-02-07T13:19:08Z,http://arxiv.org/abs/1702.01999v1,http://arxiv.org/pdf/1702.01999v1,Identification of Voice Utterance with Aging Factor Using the Method of   MFCC Multichannel,identif voic utter age factor use method mfcc multichannel,"This research was conducted to develop a method to identify voice utterance. For voice utterance that encounters change caused by aging factor, with the interval of 10 to 25 years. The change of voice utterance influenced by aging factor might be extracted by MFCC (Mel Frequency Cepstrum Coefficient). However, the level of the compatibility of the feature may be dropped down to 55%. While the ones which do not encounter it may reach 95%. To improve the compatibility of the changing voice feature influenced by aging factor, then the method of the more specific feature extraction is developed: which is by separating the voice into several channels, suggested as MFCC multichannel, consisting of multichannel 5 filterbank (M5FB), multichannel 2 filterbank (M2FB) and multichannel 1 filterbank (M1FB). The result of the test shows that for model M5FB and M2FB have the highest score in the level of compatibility with 85% and 82% with 25 years interval. While model M5FB gets the highest score of 86% for 10 years time interval.",research conduct develop method identifi voic utter voic utter encount chang caus age factor interv year chang voic utter influenc age factor might extract mfcc mel frequenc cepstrum coeffici howev level compat featur may drop one encount may reach improv compat chang voic featur influenc age factor method specif featur extract develop separ voic sever channel suggest mfcc multichannel consist multichannel filterbank mfb multichannel filterbank mfb multichannel filterbank mfb result test show model mfb mfb highest score level compat year interv model mfb get highest score year time interv,"['Roy Rudolf Huizen', 'Jazi Eko Istiyanto', 'Agfianto Eko Putra']",['cs.SD'],False,False,False,False,True,False
842,2017-03-28T14:11:09Z,2017-01-27T12:38:47Z,http://arxiv.org/abs/1701.08156v1,http://arxiv.org/pdf/1701.08156v1,A Comprehensive Survey on Bengali Phoneme Recognition,comprehens survey bengali phonem recognit,Hidden Markov model based various phoneme recognition methods for Bengali language is reviewed. Automatic phoneme recognition for Bengali language using multilayer neural network is reviewed. Usefulness of multilayer neural network over single layer neural network is discussed. Bangla phonetic feature table construction and enhancement for Bengali speech recognition is also discussed. Comparison among these methods is discussed.,hidden markov model base various phonem recognit method bengali languag review automat phonem recognit bengali languag use multilay neural network review use multilay neural network singl layer neural network discuss bangla phonet featur tabl construct enhanc bengali speech recognit also discuss comparison among method discuss,"['Sadia Tasnim Swarna', 'Shamim Ehsan', 'Md. Saiful Islam', 'Marium E Jannat']","['cs.SD', 'cs.CL']",False,False,False,False,True,False
843,2017-03-28T14:11:09Z,2017-01-27T19:21:10Z,http://arxiv.org/abs/1701.07138v3,http://arxiv.org/pdf/1701.07138v3,Learning Mid-Level Auditory Codes from Natural Sound Statistics,learn mid level auditori code natur sound statist,"Interaction with the world requires an organism to transform sensory signals into representations in which behaviorally meaningful properties of the environment are made explicit. These representations are derived through cascades of neuronal processing stages in which neurons at each stage recode the output of preceding stages. Explanations of sensory coding may thus involve understanding how low-level patterns are combined into more complex structures. Although models exist in the visual domain to explain how mid-level features such as junctions and curves might be derived from oriented filters in early visual cortex, little is known about analogous grouping principles for mid-level auditory representations. We propose a hierarchical generative model of natural sounds that learns combinations of spectrotemporal features from natural stimulus statistics. In the first layer the model forms a sparse convolutional code of spectrograms using a dictionary of learned spectrotemporal kernels. To generalize from specific kernel activation patterns, the second layer encodes patterns of time-varying magnitude of multiple first layer coefficients. Because second-layer features are sensitive to combinations of spectrotemporal features, the representation they support encodes more complex acoustic patterns than the first layer. When trained on corpora of speech and environmental sounds, some second-layer units learned to group spectrotemporal features that occur together in natural sounds. Others instantiate opponency between dissimilar sets of spectrotemporal features. Such groupings might be instantiated by neurons in the auditory cortex, providing a hypothesis for mid-level neuronal computation.",interact world requir organ transform sensori signal represent behavior meaning properti environ made explicit represent deriv cascad neuron process stage neuron stage recod output preced stage explan sensori code may thus involv understand low level pattern combin complex structur although model exist visual domain explain mid level featur junction curv might deriv orient filter earli visual cortex littl known analog group principl mid level auditori represent propos hierarch generat model natur sound learn combin spectrotempor featur natur stimulus statist first layer model form spars convolut code spectrogram use dictionari learn spectrotempor kernel general specif kernel activ pattern second layer encod pattern time vari magnitud multipl first layer coeffici becaus second layer featur sensit combin spectrotempor featur represent support encod complex acoust pattern first layer train corpora speech environment sound second layer unit learn group spectrotempor featur occur togeth natur sound instanti oppon dissimilar set spectrotempor featur group might instanti neuron auditori cortex provid hypothesi mid level neuron comput,"['Wiktor Młynarski', 'Josh H. McDermott']","['q-bio.NC', 'cs.SD']",False,False,False,False,True,False
844,2017-03-28T14:11:09Z,2017-01-23T11:18:06Z,http://arxiv.org/abs/1702.02092v1,http://arxiv.org/pdf/1702.02092v1,Characterisation of speech diversity using self-organising maps,characteris speech divers use self organis map,"We report investigations into speaker classification of larger quantities of unlabelled speech data using small sets of manually phonemically annotated speech. The Kohonen speech typewriter is a semi-supervised method comprised of self-organising maps (SOMs) that achieves low phoneme error rates. A SOM is a 2D array of cells that learn vector representations of the data based on neighbourhoods. In this paper, we report a method to evaluate pronunciation using multilevel SOMs with /hVd/ single syllable utterances for the study of vowels, for Australian pronunciation.",report investig speaker classif larger quantiti unlabel speech data use small set manual phonem annot speech kohonen speech typewrit semi supervis method compris self organis map som achiev low phonem error rate som array cell learn vector represent data base neighbourhood paper report method evalu pronunci use multilevel som hvd singl syllabl utter studi vowel australian pronunci,"['Tom A. F. Anderson', 'David M. W. Powers']","['cs.CL', 'cs.NE', 'cs.SD']",False,False,False,False,True,False
845,2017-03-28T14:11:09Z,2017-01-24T16:25:15Z,http://arxiv.org/abs/1701.06078v2,http://arxiv.org/pdf/1701.06078v2,Lyrics-to-Audio Alignment by Unsupervised Discovery of Repetitive   Patterns in Vowel Acoustics,lyric audio align unsupervis discoveri repetit pattern vowel acoust,"Most of the previous approaches to lyrics-to-audio alignment used a pre-developed automatic speech recognition (ASR) system that innately suffered from several difficulties to adapt the speech model to individual singers. A significant aspect missing in previous works is the self-learnability of repetitive vowel patterns in the singing voice, where the vowel part used is more consistent than the consonant part. Based on this, our system first learns a discriminative subspace of vowel sequences, based on weighted symmetric non-negative matrix factorization (WS-NMF), by taking the self-similarity of a standard acoustic feature as an input. Then, we make use of canonical time warping (CTW), derived from a recent computer vision technique, to find an optimal spatiotemporal transformation between the text and the acoustic sequences. Experiments with Korean and English data sets showed that deploying this method after a pre-developed, unsupervised, singing source separation achieved more promising results than other state-of-the-art unsupervised approaches and an existing ASR-based system.",previous approach lyric audio align use pre develop automat speech recognit asr system innat suffer sever difficulti adapt speech model individu singer signific aspect miss previous work self learnabl repetit vowel pattern sing voic vowel part use consist conson part base system first learn discrimin subspac vowel sequenc base weight symmetr non negat matrix factor ws nmf take self similar standard acoust featur input make use canon time warp ctw deriv recent comput vision techniqu find optim spatiotempor transform text acoust sequenc experi korean english data set show deploy method pre develop unsupervis sing sourc separ achiev promis result state art unsupervis approach exist asr base system,"['Sungkyun Chang', 'Kyogu Lee']","['cs.SD', 'cs.LG']",False,False,False,False,True,False
846,2017-03-28T14:11:09Z,2017-01-20T12:48:02Z,http://arxiv.org/abs/1701.05779v1,http://arxiv.org/pdf/1701.05779v1,Empirical Study of Drone Sound Detection in Real-Life Environment with   Deep Neural Networks,empir studi drone sound detect real life environ deep neural network,"This work aims to investigate the use of deep neural network to detect commercial hobby drones in real-life environments by analyzing their sound data. The purpose of work is to contribute to a system for detecting drones used for malicious purposes, such as for terrorism. Specifically, we present a method capable of detecting the presence of commercial hobby drones as a binary classification problem based on sound event detection. We recorded the sound produced by a few popular commercial hobby drones, and then augmented this data with diverse environmental sound data to remedy the scarcity of drone sound data in diverse environments. We investigated the effectiveness of state-of-the-art event sound classification methods, i.e., a Gaussian Mixture Model (GMM), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), for drone sound detection. Our empirical results, which were obtained with a testing dataset collected on an urban street, confirmed the effectiveness of these models for operating in a real environment. In summary, our RNN models showed the best detection performance with an F-Score of 0.8009 with 240 ms of input audio with a short processing time, indicating their applicability to real-time detection systems.",work aim investig use deep neural network detect commerci hobbi drone real life environ analyz sound data purpos work contribut system detect drone use malici purpos terror specif present method capabl detect presenc commerci hobbi drone binari classif problem base sound event detect record sound produc popular commerci hobbi drone augment data divers environment sound data remedi scarciti drone sound data divers environ investig effect state art event sound classif method gaussian mixtur model gmm convolut neural network cnn recurr neural network rnn drone sound detect empir result obtain test dataset collect urban street confirm effect model oper real environ summari rnn model show best detect perform score ms input audio short process time indic applic real time detect system,"['Sungho Jeon', 'Jong-Woo Shin', 'Young-Jun Lee', 'Woong-Hee Kim', 'YoungHyoun Kwon', 'Hae-Yong Yang']","['cs.SD', 'cs.LG']",False,False,False,False,True,False
848,2017-03-28T14:11:09Z,2017-01-12T01:02:22Z,http://arxiv.org/abs/1701.03198v1,http://arxiv.org/pdf/1701.03198v1,Unsupervised Latent Behavior Manifold Learning from Acoustic Features:   audio2behavior,unsupervis latent behavior manifold learn acoust featur audiobehavior,"Behavioral annotation using signal processing and machine learning is highly dependent on training data and manual annotations of behavioral labels. Previous studies have shown that speech information encodes significant behavioral information and be used in a variety of automated behavior recognition tasks. However, extracting behavior information from speech is still a difficult task due to the sparseness of training data coupled with the complex, high-dimensionality of speech, and the complex and multiple information streams it encodes. In this work we exploit the slow varying properties of human behavior. We hypothesize that nearby segments of speech share the same behavioral context and hence share a similar underlying representation in a latent space. Specifically, we propose a Deep Neural Network (DNN) model to connect behavioral context and derive the behavioral manifold in an unsupervised manner. We evaluate the proposed manifold in the couples therapy domain and also provide examples from publicly available data (e.g. stand-up comedy). We further investigate training within the couples' therapy domain and from movie data. The results are extremely encouraging and promise improved behavioral quantification in an unsupervised manner and warrants further investigation in a range of applications.",behavior annot use signal process machin learn high depend train data manual annot behavior label previous studi shown speech inform encod signific behavior inform use varieti autom behavior recognit task howev extract behavior inform speech still difficult task due spars train data coupl complex high dimension speech complex multipl inform stream encod work exploit slow vari properti human behavior hypothes nearbi segment speech share behavior context henc share similar represent latent space specif propos deep neural network dnn model connect behavior context deriv behavior manifold unsupervis manner evalu propos manifold coupl therapi domain also provid exampl public avail data stand comedi investig train within coupl therapi domain movi data result extrem encourag promis improv behavior quantif unsupervis manner warrant investig rang applic,"['Haoqi Li', 'Brian Baucom', 'Panayiotis Georgiou']","['cs.LG', 'cs.SD']",False,False,False,False,True,False
849,2017-03-28T14:11:09Z,2017-03-15T00:23:45Z,http://arxiv.org/abs/1701.03360v2,http://arxiv.org/pdf/1701.03360v2,Residual LSTM: Design of a Deep Recurrent Architecture for Distant   Speech Recognition,residu lstm design deep recurr architectur distant speech recognit,"In this paper, a novel architecture for a deep recurrent neural network, residual LSTM is introduced. A plain LSTM has an internal memory cell that can learn long term dependencies of sequential data. It also provides a temporal shortcut path to avoid vanishing or exploding gradients in the temporal domain. The residual LSTM provides an additional spatial shortcut path from lower layers for efficient training of deep networks with multiple LSTM layers. Compared with the previous work, highway LSTM, residual LSTM separates a spatial shortcut path with temporal one by using output layers, which can help to avoid a conflict between spatial and temporal-domain gradient flows. Furthermore, residual LSTM reuses the output projection matrix and the output gate of LSTM to control the spatial information flow instead of additional gate networks, which effectively reduces more than 10% of network parameters. An experiment for distant speech recognition on the AMI SDM corpus shows that 10-layer plain and highway LSTM networks presented 13.7% and 6.2% increase in WER over 3-layer aselines, respectively. On the contrary, 10-layer residual LSTM networks provided the lowest WER 41.0%, which corresponds to 3.3% and 2.8% WER reduction over plain and highway LSTM networks, respectively.",paper novel architectur deep recurr neural network residu lstm introduc plain lstm intern memori cell learn long term depend sequenti data also provid tempor shortcut path avoid vanish explod gradient tempor domain residu lstm provid addit spatial shortcut path lower layer effici train deep network multipl lstm layer compar previous work highway lstm residu lstm separ spatial shortcut path tempor one use output layer help avoid conflict spatial tempor domain gradient flow furthermor residu lstm reus output project matrix output gate lstm control spatial inform flow instead addit gate network effect reduc network paramet experi distant speech recognit ami sdm corpus show layer plain highway lstm network present increas wer layer aselin respect contrari layer residu lstm network provid lowest wer correspond wer reduct plain highway lstm network respect,"['Jaeyoung Kim', 'Mostafa El-Khamy', 'Jungwon Lee']","['cs.LG', 'cs.AI', 'cs.SD']",False,False,False,False,True,False
851,2017-03-28T14:11:13Z,2017-01-04T04:07:11Z,http://arxiv.org/abs/1701.00599v2,http://arxiv.org/pdf/1701.00599v2,AENet: Learning Deep Audio Features for Video Analysis,aenet learn deep audio featur video analysi,"We propose a new deep network for audio event recognition, called AENet. In contrast to speech, sounds coming from audio events may be produced by a wide variety of sources. Furthermore, distinguishing them often requires analyzing an extended time period due to the lack of clear sub-word units that are present in speech. In order to incorporate this long-time frequency structure of audio events, we introduce a convolutional neural network (CNN) operating on a large temporal input. In contrast to previous works this allows us to train an audio event detection system end-to-end. The combination of our network architecture and a novel data augmentation outperforms previous methods for audio event detection by 16%. Furthermore, we perform transfer learning and show that our model learnt generic audio features, similar to the way CNNs learn generic features on vision tasks. In video analysis, combining visual features and traditional audio features such as MFCC typically only leads to marginal improvements. Instead, combining visual features with our AENet features, which can be computed efficiently on a GPU, leads to significant performance improvements on action recognition and video highlight detection. In video highlight detection, our audio features improve the performance by more than 8% over visual features alone.",propos new deep network audio event recognit call aenet contrast speech sound come audio event may produc wide varieti sourc furthermor distinguish often requir analyz extend time period due lack clear sub word unit present speech order incorpor long time frequenc structur audio event introduc convolut neural network cnn oper larg tempor input contrast previous work allow us train audio event detect system end end combin network architectur novel data augment outperform previous method audio event detect furthermor perform transfer learn show model learnt generic audio featur similar way cnns learn generic featur vision task video analysi combin visual featur tradit audio featur mfcc typic onli lead margin improv instead combin visual featur aenet featur comput effici gpu lead signific perform improv action recognit video highlight detect video highlight detect audio featur improv perform visual featur alon,"['Naoya Takahashi', 'Michael Gygli', 'Luc Van Gool']","['cs.MM', 'cs.CV', 'cs.SD']",False,False,False,False,True,False
852,2017-03-28T14:11:13Z,2017-01-09T17:35:17Z,http://arxiv.org/abs/1701.00495v2,http://arxiv.org/pdf/1701.00495v2,Vid2speech: Speech Reconstruction from Silent Video,vidspeech speech reconstruct silent video,"Speechreading is a notoriously difficult task for humans to perform. In this paper we present an end-to-end model based on a convolutional neural network (CNN) for generating an intelligible acoustic speech signal from silent video frames of a speaking person. The proposed CNN generates sound features for each frame based on its neighboring frames. Waveforms are then synthesized from the learned speech features to produce intelligible speech. We show that by leveraging the automatic feature learning capabilities of a CNN, we can obtain state-of-the-art word intelligibility on the GRID dataset, and show promising results for learning out-of-vocabulary (OOV) words.",speechread notori difficult task human perform paper present end end model base convolut neural network cnn generat intellig acoust speech signal silent video frame speak person propos cnn generat sound featur frame base neighbor frame waveform synthes learn speech featur produc intellig speech show leverag automat featur learn capabl cnn obtain state art word intellig grid dataset show promis result learn vocabulari oov word,"['Ariel Ephrat', 'Shmuel Peleg']","['cs.CV', 'cs.SD']",False,False,False,False,True,False
853,2017-03-28T14:11:13Z,2016-12-30T08:46:05Z,http://arxiv.org/abs/1612.09150v2,http://arxiv.org/pdf/1612.09150v2,Phase-incorporating Speech Enhancement Based on Complex-valued Gaussian   Process Latent Variable Model,phase incorpor speech enhanc base complex valu gaussian process latent variabl model,"Traditional speech enhancement techniques modify the magnitude of a speech in time-frequency domain, and use the phase of a noisy speech to resynthesize a time domain speech. This work proposes a complex-valued Gaussian process latent variable model (CGPLVM) to enhance directly the complex-valued noisy spectrum, modifying not only the magnitude but also the phase. The main idea that underlies the developed method is the modeling of short-time Fourier transform (STFT) coefficients across the time frames of a speech as a proper complex Gaussian process (GP) with noise added. The proposed method is based on projecting the spectrum into a low-dimensional subspace. The likelihood criterion is used to optimize the hyperparameters of the model. Experiments were carried out on the CHTTL database, which contains the digits zero to nine in Mandarin. Several standard measures are used to demonstrate that the proposed method outperforms baseline methods.",tradit speech enhanc techniqu modifi magnitud speech time frequenc domain use phase noisi speech resynthes time domain speech work propos complex valu gaussian process latent variabl model cgplvm enhanc direct complex valu noisi spectrum modifi onli magnitud also phase main idea develop method model short time fourier transform stft coeffici across time frame speech proper complex gaussian process gp nois ad propos method base project spectrum low dimension subspac likelihood criterion use optim hyperparamet model experi carri chttl databas contain digit zero nine mandarin sever standard measur use demonstr propos method outperform baselin method,"['Sih-Huei Chen', 'Yuan-Shan Lee', 'Jia-Ching Wang']",['cs.SD'],False,False,False,False,True,False
854,2017-03-28T14:11:13Z,2016-12-30T09:26:26Z,http://arxiv.org/abs/1612.09089v2,http://arxiv.org/pdf/1612.09089v2,What Makes Audio Event Detection Harder than Classification?,make audio event detect harder classif,"There is a common observation that audio event classification is easier to deal with than detection. So far, this observation has been accepted as a fact and we lack a careful analysis. In this paper, we reason the rationale behind this fact and, more importantly, leverage them to benefit the audio event detection task. We present an improved detection pipeline in which a verification step is appended to augment a detection system. This step employs a high-quality event classifier to postprocess the benign event hypotheses outputted by the detection system and reject false alarms. To demonstrate the effectiveness of the proposed pipeline, we implement and pair up different event detectors based on the most common detection schemes and various event classifiers, ranging from the standard bag-of-words model to the state-of-the-art bank-of-regressors one. Experimental results on the ITC-Irst dataset show significant improvements to detection performance. More importantly, these improvements are consistent for all detector-classifier combinations.",common observ audio event classif easier deal detect far observ accept fact lack care analysi paper reason rational behind fact import leverag benefit audio event detect task present improv detect pipelin verif step append augment detect system step employ high qualiti event classifi postprocess benign event hypothes output detect system reject fals alarm demonstr effect propos pipelin implement pair differ event detector base common detect scheme various event classifi rang standard bag word model state art bank regressor one experiment result itc irst dataset show signific improv detect perform import improv consist detector classifi combin,"['Huy Phan', 'Philipp Koch', 'Marco Maass', 'Radoslaw Mazur', 'Ian McLoughlin', 'Alfred Mertins']",['cs.SD'],False,False,False,False,True,False
855,2017-03-28T14:11:13Z,2016-12-27T20:27:24Z,http://arxiv.org/abs/1612.08727v1,http://arxiv.org/pdf/1612.08727v1,"Creating A Musical Performance Dataset for Multimodal Music Analysis:   Challenges, Insights, and Applications",creat music perform dataset multimod music analysi challeng insight applic,"We introduce a dataset for facilitating audio-visual analysis of musical performances. The dataset comprises a number of simple multi-instrument musical pieces assembled from coordinated but separately recorded performances of individual tracks. For each piece, we provide the musical score in MIDI format, the audio recordings of the individual tracks, the audio and video recording of the assembled mixture, and ground-truth annotation files including frame-level and note-level transcriptions. We anticipate that the dataset will be useful for developing and evaluating multi-modal techniques for music source separation, transcription, score following, and performance analysis. We describe our methodology for the creation of this dataset, particularly highlighting our approaches for addressing the challenges involved in maintaining synchronization and naturalness. We briefly discuss the research questions that can be investigated with this dataset.",introduc dataset facilit audio visual analysi music perform dataset compris number simpl multi instrument music piec assembl coordin separ record perform individu track piec provid music score midi format audio record individu track audio video record assembl mixtur ground truth annot file includ frame level note level transcript anticip dataset use develop evalu multi modal techniqu music sourc separ transcript score follow perform analysi describ methodolog creation dataset particular highlight approach address challeng involv maintain synchron natur briefli discuss research question investig dataset,"['Bochen Li', 'Xinzhao Liu', 'Karthik Dinesh', 'Zhiyao Duan', 'Gaurav Sharma']","['cs.MM', 'cs.SD']",False,False,False,False,True,False
857,2017-03-28T14:11:13Z,2016-12-22T10:14:59Z,http://arxiv.org/abs/1612.07523v1,http://arxiv.org/pdf/1612.07523v1,Robustness of Voice Conversion Techniques Under Mismatched Conditions,robust voic convers techniqu mismatch condit,"Most of the existing studies on voice conversion (VC) are conducted in acoustically matched conditions between source and target signal. However, the robustness of VC methods in presence of mismatch remains unknown. In this paper, we report a comparative analysis of different VC techniques under mismatched conditions. The extensive experiments with five different VC techniques on CMU ARCTIC corpus suggest that performance of VC methods substantially degrades in noisy conditions. We have found that bilinear frequency warping with amplitude scaling (BLFWAS) outperforms other methods in most of the noisy conditions. We further explore the suitability of different speech enhancement techniques for robust conversion. The objective evaluation results indicate that spectral subtraction and log minimum mean square error (logMMSE) based speech enhancement techniques can be used to improve the performance in specific noisy conditions.",exist studi voic convers vc conduct acoust match condit sourc target signal howev robust vc method presenc mismatch remain unknown paper report compar analysi differ vc techniqu mismatch condit extens experi five differ vc techniqu cmu arctic corpus suggest perform vc method substanti degrad noisi condit found bilinear frequenc warp amplitud scale blfwas outperform method noisi condit explor suitabl differ speech enhanc techniqu robust convers object evalu result indic spectral subtract log minimum mean squar error logmms base speech enhanc techniqu use improv perform specif noisi condit,"['Monisankha Pal', 'Dipjyoti Paul', 'Md Sahidullah', 'Goutam Saha']","['cs.SD', 'cs.LG', 'stat.ML']",False,False,False,False,True,False
858,2017-03-28T14:11:13Z,2016-12-21T00:02:08Z,http://arxiv.org/abs/1612.07608v1,http://arxiv.org/pdf/1612.07608v1,EchoWear: Smartwatch Technology for Voice and Speech Treatments of   Patients with Parkinson's Disease,echowear smartwatch technolog voic speech treatment patient parkinson diseas,"About 90 percent of people with Parkinson's disease (PD) experience decreased functional communication due to the presence of voice and speech disorders associated with dysarthria that can be characterized by monotony of pitch (or fundamental frequency), reduced loudness, irregular rate of speech, imprecise consonants, and changes in voice quality. Speech-language pathologists (SLPs) work with patients with PD to improve speech intelligibility using various intensive in-clinic speech treatments. SLPs also prescribe home exercises to enhance generalization of speech strategies outside of the treatment room. Even though speech therapies are found to be highly effective in improving vocal loudness and speech quality, patients with PD find it difficult to follow the prescribed exercise regimes outside the clinic and to continue exercises once the treatment is completed. SLPs need techniques to monitor compliance and accuracy of their patients exercises at home and in ecologically valid communication situations. We have designed EchoWear, a smartwatch-based system, to remotely monitor speech and voice exercises as prescribed by SLPs. We conducted a study of 6 individuals; three with PD and three healthy controls. To assess the performance of EchoWear technology compared with high quality audio equipment obtained in a speech laboratory. Our preliminary analysis shows promising outcomes for using EchoWear in speech therapies for people with PD.   Keywords: Dysarthria; knowledge-based speech processing; Parkinson's disease; smartwatch; speech therapy; wearable system.",percent peopl parkinson diseas pd experi decreas function communic due presenc voic speech disord associ dysarthria character monotoni pitch fundament frequenc reduc loud irregular rate speech imprecis conson chang voic qualiti speech languag pathologist slps work patient pd improv speech intellig use various intens clinic speech treatment slps also prescrib home exercis enhanc general speech strategi outsid treatment room even though speech therapi found high effect improv vocal loud speech qualiti patient pd find difficult follow prescrib exercis regim outsid clinic continu exercis onc treatment complet slps need techniqu monitor complianc accuraci patient exercis home ecolog valid communic situat design echowear smartwatch base system remot monitor speech voic exercis prescrib slps conduct studi individu three pd three healthi control assess perform echowear technolog compar high qualiti audio equip obtain speech laboratori preliminari analysi show promis outcom use echowear speech therapi peopl pd keyword dysarthria knowledg base speech process parkinson diseas smartwatch speech therapi wearabl system,"['Harishchandra Dubey', 'Jon C. Goldberg', 'Mohammadreza Abtahi', 'Leslie Mahler', 'Kunal Mankodiya']","['cs.CY', 'cs.SD']",False,False,False,False,True,False
859,2017-03-28T14:11:13Z,2016-12-20T13:04:33Z,http://arxiv.org/abs/1612.06642v1,http://arxiv.org/pdf/1612.06642v1,Efficient Target Activity Detection based on Recurrent Neural Networks,effici target activ detect base recurr neural network,"This paper addresses the problem of Target Activity Detection (TAD) for binaural listening devices. TAD denotes the problem of robustly detecting the activity of a target speaker in a harsh acoustic environment, which comprises interfering speakers and noise (cocktail party scenario). In previous work, it has been shown that employing a Feed-forward Neural Network (FNN) for detecting the target speaker activity is a promising approach to combine the advantage of different TAD features (used as network inputs). In this contribution, we exploit a larger context window for TAD and compare the performance of FNNs and Recurrent Neural Networks (RNNs) with an explicit focus on small network topologies as desirable for embedded acoustic signal processing systems. More specifically, the investigations include a comparison between three different types of RNNs, namely plain RNNs, Long Short-Term Memories, and Gated Recurrent Units. The results indicate that all versions of RNNs outperform FNNs for the task of TAD.",paper address problem target activ detect tad binaur listen devic tad denot problem robust detect activ target speaker harsh acoust environ compris interf speaker nois cocktail parti scenario previous work shown employ feed forward neural network fnn detect target speaker activ promis approach combin advantag differ tad featur use network input contribut exploit larger context window tad compar perform fnns recurr neural network rnns explicit focus small network topolog desir embed acoust signal process system specif investig includ comparison three differ type rnns name plain rnns long short term memori gate recurr unit result indic version rnns outperform fnns task tad,"['Daniel Gerber', 'Stefan Meier', 'Walter Kellermann']",['cs.SD'],False,False,False,False,True,False
860,2017-03-28T14:11:17Z,2017-03-09T08:47:38Z,http://arxiv.org/abs/1612.06151v3,http://arxiv.org/pdf/1612.06151v3,HRTF-based two-dimensional robust least-squares frequency-invariant   beamformer design for robot audition,hrtf base two dimension robust least squar frequenc invari beamform design robot audit,"In this work, we propose a two-dimensional Head-Related Transfer Function (HRTF)-based robust beamformer design for robot audition, which allows for explicit control of the beamformer response for the entire three-dimensional sound field surrounding a humanoid robot. We evaluate the proposed method by means of both signal-independent and signal-dependent measures in a robot audition scenario. Our results confirm the effectiveness of the proposed two-dimensional HRTF-based beamformer design, compared to our previously published one-dimensional HRTF-based beamformer design, which was carried out for a fixed elevation angle only.",work propos two dimension head relat transfer function hrtf base robust beamform design robot audit allow explicit control beamform respons entir three dimension sound field surround humanoid robot evalu propos method mean signal independ signal depend measur robot audit scenario result confirm effect propos two dimension hrtf base beamform design compar previous publish one dimension hrtf base beamform design carri fix elev angl onli,"['Hendrik Barfuss', 'Michael Buerger', 'Jasper Podschus', 'Walter Kellermann']",['cs.SD'],False,False,False,False,True,False
861,2017-03-28T14:11:17Z,2016-12-16T14:40:43Z,http://arxiv.org/abs/1612.05489v1,http://arxiv.org/pdf/1612.05489v1,On-bird Sound Recordings: Automatic Acoustic Recognition of Activities   and Contexts,bird sound record automat acoust recognit activ context,"We introduce a novel approach to studying animal behaviour and the context in which it occurs, through the use of microphone backpacks carried on the backs of individual free-flying birds. These sensors are increasingly used by animal behaviour researchers to study individual vocalisations of freely behaving animals, even in the field. However such devices may record more than an animals vocal behaviour, and have the potential to be used for investigating specific activities (movement) and context (background) within which vocalisations occur. To facilitate this approach, we investigate the automatic annotation of such recordings through two different sound scene analysis paradigms: a scene-classification method using feature learning, and an event-detection method using probabilistic latent component analysis (PLCA). We analyse recordings made with Eurasian jackdaws (Corvus monedula) in both captive and field settings. Results are comparable with the state of the art in sound scene analysis; we find that the current recognition quality level enables scalable automatic annotation of audio logger data, given partial annotation, but also find that individual differences between animals and/or their backpacks limit the generalisation from one individual to another. we consider the interrelation of 'scenes' and 'events' in this particular task, and issues of temporal resolution.",introduc novel approach studi anim behaviour context occur use microphon backpack carri back individu free fli bird sensor increas use anim behaviour research studi individu vocalis freeli behav anim even field howev devic may record anim vocal behaviour potenti use investig specif activ movement context background within vocalis occur facilit approach investig automat annot record two differ sound scene analysi paradigm scene classif method use featur learn event detect method use probabilist latent compon analysi plca analys record made eurasian jackdaw corvus monedula captiv field set result compar state art sound scene analysi find current recognit qualiti level enabl scalabl automat annot audio logger data given partial annot also find individu differ anim backpack limit generalis one individu anoth consid interrel scene event particular task issu tempor resolut,"['Dan Stowell', 'Emmanouil Benetos', 'Lisa F. Gill']",['cs.SD'],False,False,False,False,True,False
863,2017-03-28T14:11:17Z,2016-12-16T05:09:14Z,http://arxiv.org/abs/1612.05369v1,http://arxiv.org/pdf/1612.05369v1,Neural networks based EEG-Speech Models,neural network base eeg speech model,"In this paper, we describe three neural network (NN) based EEG-Speech (NES) models that map the unspoken EEG signals to the corresponding phonemes. Instead of using conventional feature extraction techniques, the proposed NES models rely on graphic learning to project both EEG and speech signals into deep representation feature spaces. This NN based linear projection helps to realize multimodal data fusion (i.e., EEG and acoustic signals). It is convenient to construct the mapping between unspoken EEG signals and phonemes. Specifically, among three NES models, two augmented models (i.e., IANES-B and IANES-G) include spoken EEG signals as either bias or gate information to strengthen the feature learning and translation of unspoken EEG signals. A combined unsupervised and supervised training is implemented stepwise to learn the mapping for all three NES models. To enhance the computational performance, three way factored NN training technique is applied to IANES-G model. Unlike many existing methods, our augmented NES models incorporate spoken-EEG signals that can efficiently suppress the artifacts in unspoken-EEG signals. Experimental results reveal that all three proposed NES models outperform the baseline SVM method, whereas IANES-G demonstrates the best performance on speech recovery and classification task comparatively.",paper describ three neural network nn base eeg speech nes model map unspoken eeg signal correspond phonem instead use convent featur extract techniqu propos nes model reli graphic learn project eeg speech signal deep represent featur space nn base linear project help realiz multimod data fusion eeg acoust signal conveni construct map unspoken eeg signal phonem specif among three nes model two augment model ian ian includ spoken eeg signal either bias gate inform strengthen featur learn translat unspoken eeg signal combin unsupervis supervis train implement stepwis learn map three nes model enhanc comput perform three way factor nn train techniqu appli ian model unlik mani exist method augment nes model incorpor spoken eeg signal effici suppress artifact unspoken eeg signal experiment result reveal three propos nes model outperform baselin svm method wherea ian demonstr best perform speech recoveri classif task compar,"['Pengfei Sun', 'Jun Qin']","['cs.SD', 'cs.LG']",False,False,False,False,True,False
872,2017-03-28T14:11:21Z,2016-12-15T04:22:39Z,http://arxiv.org/abs/1612.04919v1,http://arxiv.org/pdf/1612.04919v1,Combination of Linear Prediction and Phase Decomposition for Glottal   Source Analysis on Voiced Speech,combin linear predict phase decomposit glottal sourc analysi voic speech,"Some glottal analysis approaches based upon linear prediction or complex cepstrum approaches have been proved to be effective to estimate glottal source from real speech utterances. We propose a new approach employing both an all-pole odd-order linear prediction to provide a coarse estimation and phase decomposition based causality/anti-causality separation to generate further refinements. The obtained measures show that this method improved performance in terms of reducing source-filter separation in estimation of glottal flow pulses (GFP). No glottal model fitting is required by this method, thus it has wide and flexible adaptation to retain fidelity of speakers's vocal features with computationally affordable resource. The method is evaluated on real speech utterances to validate it.",glottal analysi approach base upon linear predict complex cepstrum approach prove effect estim glottal sourc real speech utter propos new approach employ pole odd order linear predict provid coars estim phase decomposit base causal anti causal separ generat refin obtain measur show method improv perform term reduc sourc filter separ estim glottal flow puls gfp glottal model fit requir method thus wide flexibl adapt retain fidel speaker vocal featur comput afford resourc method evalu real speech utter valid,"['Yiqiao Chen', 'John N. Gowdy']",['cs.SD'],False,False,False,False,True,False
873,2017-03-28T14:11:21Z,2016-12-14T17:40:02Z,http://arxiv.org/abs/1612.04744v1,http://arxiv.org/pdf/1612.04744v1,Incorporating Language Level Information into Acoustic Models,incorpor languag level inform acoust model,"This paper proposed a class of novel Deep Recurrent Neural Networks which can incorporate language-level information into acoustic models. For simplicity, we named these networks Recurrent Deep Language Networks (RDLNs). Multiple variants of RDLNs were considered, including two kinds of context information, two methods to process the context, and two methods to incorporate the language-level information. RDLNs provided possible methods to fine-tune the whole Automatic Speech Recognition (ASR) system in the acoustic modeling process.",paper propos class novel deep recurr neural network incorpor languag level inform acoust model simplic name network recurr deep languag network rdlns multipl variant rdlns consid includ two kind context inform two method process context two method incorpor languag level inform rdlns provid possibl method fine tune whole automat speech recognit asr system acoust model process,"['Peidong Wang', 'Deliang Wang']","['cs.CL', 'cs.LG', 'cs.SD']",False,False,False,False,True,False
875,2017-03-28T14:11:21Z,2016-12-14T15:40:44Z,http://arxiv.org/abs/1612.06287v1,http://arxiv.org/pdf/1612.06287v1,VAST : The Virtual Acoustic Space Traveler Dataset,vast virtual acoust space travel dataset,"This paper introduces a new paradigm for sound source lo-calization referred to as virtual acoustic space traveling (VAST) and presents a first dataset designed for this purpose. Existing sound source localization methods are either based on an approximate physical model (physics-driven) or on a specific-purpose calibration set (data-driven). With VAST, the idea is to learn a mapping from audio features to desired audio properties using a massive dataset of simulated room impulse responses. This virtual dataset is designed to be maximally representative of the potential audio scenes that the considered system may be evolving in, while remaining reasonably compact. We show that virtually-learned mappings on this dataset generalize to real data, overcoming some intrinsic limitations of traditional binaural sound localization methods based on time differences of arrival.",paper introduc new paradigm sound sourc lo calize refer virtual acoust space travel vast present first dataset design purpos exist sound sourc local method either base approxim physic model physic driven specif purpos calibr set data driven vast idea learn map audio featur desir audio properti use massiv dataset simul room impuls respons virtual dataset design maxim repres potenti audio scene consid system may evolv remain reason compact show virtual learn map dataset general real data overcom intrins limit tradit binaur sound local method base time differ arriv,"['Clément Gaultier', 'Saurabh Kataria', 'Antoine Deleforge']","['cs.SD', 'cs.LG']",False,False,False,False,True,False
876,2017-03-28T14:11:21Z,2016-12-14T15:07:51Z,http://arxiv.org/abs/1612.04675v1,http://arxiv.org/pdf/1612.04675v1,Recurrent Deep Stacking Networks for Speech Recognition,recurr deep stack network speech recognit,"This paper presented our work on applying Recurrent Deep Stacking Networks (RDSNs) to Robust Automatic Speech Recognition (ASR) tasks. In the paper, we also proposed a more efficient yet comparable substitute to RDSN, Bi- Pass Stacking Network (BPSN). The main idea of these two models is to add phoneme-level information into acoustic models, transforming an acoustic model to the combination of an acoustic model and a phoneme-level N-gram model. Experiments showed that RDSN and BPsn can substantially improve the performances over conventional DNNs.",paper present work appli recurr deep stack network rdsns robust automat speech recognit asr task paper also propos effici yet compar substitut rdsn bi pass stack network bpsn main idea two model add phonem level inform acoust model transform acoust model combin acoust model phonem level gram model experi show rdsn bpsn substanti improv perform convent dnns,"['Peidong Wang', 'Zhongqiu Wang', 'Deliang Wang']","['cs.CL', 'cs.SD']",False,False,False,False,True,False
878,2017-03-28T14:11:21Z,2016-12-16T14:25:44Z,http://arxiv.org/abs/1612.04028v2,http://arxiv.org/abs/1612.04028v2,Adaptive DCTNet for Audio Signal Classification,adapt dctnet audio signal classif,"In this paper, we investigate DCTNet for audio signal classification. Its output feature is related to Cohen's class of time-frequency distributions. We introduce the use of adaptive DCTNet (A-DCTNet) for audio signals feature extraction. The A-DCTNet applies the idea of constant-Q transform, with its center frequencies of filterbanks geometrically spaced. The A-DCTNet is adaptive to different acoustic scales, and it can better capture low frequency acoustic information that is sensitive to human audio perception than features such as Mel-frequency spectral coefficients (MFSC). We use features extracted by the A-DCTNet as input for classifiers. Experimental results show that the A-DCTNet and Recurrent Neural Networks (RNN) achieve state-of-the-art performance in bird song classification rate, and improve artist identification accuracy in music data. They demonstrate A-DCTNet's applicability to signal processing problems.",paper investig dctnet audio signal classif output featur relat cohen class time frequenc distribut introduc use adapt dctnet dctnet audio signal featur extract dctnet appli idea constant transform center frequenc filterbank geometr space dctnet adapt differ acoust scale better captur low frequenc acoust inform sensit human audio percept featur mel frequenc spectral coeffici mfsc use featur extract dctnet input classifi experiment result show dctnet recurr neural network rnn achiev state art perform bird song classif rate improv artist identif accuraci music data demonstr dctnet applic signal process problem,"['Yin Xian', 'Yunchen Pu', 'Zhe Gan', 'Liang Lu', 'Andrew Thompson']",['cs.SD'],False,False,False,False,True,False
880,2017-03-28T14:11:25Z,2016-12-12T00:13:35Z,http://arxiv.org/abs/1612.03505v1,http://arxiv.org/pdf/1612.03505v1,Convolutional Neural Networks for Passive Monitoring of a Shallow Water   Environment using a Single Sensor,convolut neural network passiv monitor shallow water environ use singl sensor,"A cost effective approach to remote monitoring of protected areas such as marine reserves and restricted naval waters is to use passive sonar to detect, classify, localize, and track marine vessel activity (including small boats and autonomous underwater vehicles). Cepstral analysis of underwater acoustic data enables the time delay between the direct path arrival and the first multipath arrival to be measured, which in turn enables estimation of the instantaneous range of the source (a small boat). However, this conventional method is limited to ranges where the Lloyd's mirror effect (interference pattern formed between the direct and first multipath arrivals) is discernible. This paper proposes the use of convolutional neural networks (CNNs) for the joint detection and ranging of broadband acoustic noise sources such as marine vessels in conjunction with a data augmentation approach for improving network performance in varied signal-to-noise ratio (SNR) situations. Performance is compared with a conventional passive sonar ranging method for monitoring marine vessel activity using real data from a single hydrophone mounted above the sea floor. It is shown that CNNs operating on cepstrum data are able to detect the presence and estimate the range of transiting vessels at greater distances than the conventional method.",cost effect approach remot monitor protect area marin reserv restrict naval water use passiv sonar detect classifi local track marin vessel activ includ small boat autonom underwat vehicl cepstral analysi underwat acoust data enabl time delay direct path arriv first multipath arriv measur turn enabl estim instantan rang sourc small boat howev convent method limit rang lloyd mirror effect interfer pattern form direct first multipath arriv discern paper propos use convolut neural network cnns joint detect rang broadband acoust nois sourc marin vessel conjunct data augment approach improv network perform vari signal nois ratio snr situat perform compar convent passiv sonar rang method monitor marin vessel activ use real data singl hydrophon mount abov sea floor shown cnns oper cepstrum data abl detect presenc estim rang transit vessel greater distanc convent method,"['Eric L. Ferguson', 'Rishi Ramakrishnan', 'Stefan B. Williams', 'Craig T. Jin']",['cs.SD'],False,False,True,False,True,False
883,2017-03-28T14:11:25Z,2016-12-06T18:37:30Z,http://arxiv.org/abs/1612.01943v1,http://arxiv.org/pdf/1612.01943v1,Segmental Convolutional Neural Networks for Detection of Cardiac   Abnormality With Noisy Heart Sound Recordings,segment convolut neural network detect cardiac abnorm noisi heart sound record,"Heart diseases constitute a global health burden, and the problem is exacerbated by the error-prone nature of listening to and interpreting heart sounds. This motivates the development of automated classification to screen for abnormal heart sounds. Existing machine learning-based systems achieve accurate classification of heart sound recordings but rely on expert features that have not been thoroughly evaluated on noisy recordings. Here we propose a segmental convolutional neural network architecture that achieves automatic feature learning from noisy heart sound recordings. Our experiments show that our best model, trained on noisy recording segments acquired with an existing hidden semi-markov model-based approach, attains a classification accuracy of 87.5% on the 2016 PhysioNet/CinC Challenge dataset, compared to the 84.6% accuracy of the state-of-the-art statistical classifier trained and evaluated on the same dataset. Our results indicate the potential of using neural network-based methods to increase the accuracy of automated classification of heart sound recordings for improved screening of heart diseases.",heart diseas constitut global health burden problem exacerb error prone natur listen interpret heart sound motiv develop autom classif screen abnorm heart sound exist machin learn base system achiev accur classif heart sound record reli expert featur thorough evalu noisi record propos segment convolut neural network architectur achiev automat featur learn noisi heart sound record experi show best model train noisi record segment acquir exist hidden semi markov model base approach attain classif accuraci physionet cinc challeng dataset compar accuraci state art statist classifi train evalu dataset result indic potenti use neural network base method increas accuraci autom classif heart sound record improv screen heart diseas,"['Yuhao Zhang', 'Sandeep Ayyar', 'Long-Huei Chen', 'Ethan J. Li']","['cs.SD', 'cs.LG', 'stat.ML']",False,False,False,False,True,False
884,2017-03-28T14:11:25Z,2016-12-06T14:58:59Z,http://arxiv.org/abs/1612.01840v1,http://arxiv.org/pdf/1612.01840v1,FMA: A Dataset For Music Analysis,fma dataset music analysi,"We present a new music dataset that can be used for several music analysis tasks. Our major goal is to go beyond the existing limitations of available music datasets, which are either the small size of datasets with raw audio tracks, the availability and legality of the music data, or the lack of meta-data for artists analysis or song ratings for recommender systems. Existing datasets such as GTZAN, TagATune, and Million Song suffer from the previous limitations. It is however essential to establish such benchmark datasets to advance the field of music analysis, like the ImageNet dataset which made possible the large success of deep learning techniques in computer vision. In this paper, we introduce the Free Music Archive (FMA) which contains 77,643 songs and 68 genres spanning 26.9 days of song listening and meta-data including artist name, song title, music genre, and track counts. For research purposes, we define two additional datasets from the original one: a small genre-balanced dataset of 4,000 song data and 10 genres compassing 33.3 hours of raw audio and a medium genre-unbalanced dataset of 14,511 data and 20 genres offering 5.1 days of track listening, both datasets come with meta-data and Echonest audio features. For all datasets, we provide a train-test splitting for future algorithms' comparisons.",present new music dataset use sever music analysi task major goal go beyond exist limit avail music dataset either small size dataset raw audio track avail legal music data lack meta data artist analysi song rate recommend system exist dataset gtzan tagatun million song suffer previous limit howev essenti establish benchmark dataset advanc field music analysi like imagenet dataset made possibl larg success deep learn techniqu comput vision paper introduc free music archiv fma contain song genr span day song listen meta data includ artist name song titl music genr track count research purpos defin two addit dataset origin one small genr balanc dataset song data genr compass hour raw audio medium genr unbalanc dataset data genr offer day track listen dataset come meta data echonest audio featur dataset provid train test split futur algorithm comparison,"['Kirell Benzi', 'Michaël Defferrard', 'Pierre Vandergheynst', 'Xavier Bresson']","['cs.SD', 'cs.IR']",False,False,False,False,True,False
891,2017-03-28T14:11:30Z,2016-11-29T20:26:00Z,http://arxiv.org/abs/1611.09827v1,http://arxiv.org/pdf/1611.09827v1,Learning Features of Music from Scratch,learn featur music scratch,"We introduce a new large-scale music dataset, MusicNet, to serve as a source of supervision and evaluation of machine learning methods for music research. MusicNet consists of hundreds of freely-licensed classical music recordings by 10 composers, written for 11 instruments, together with instrument/note annotations resulting in over 1 million temporal labels on 34 hours of chamber music performances under various studio and microphone conditions.   We define a multi-label classification task to predict notes in musical recordings, along with an evaluation protocol. We benchmark several machine learning architectures for this task: i) learning from ""hand-crafted"" spectrogram features; ii) end-to-end learning with a neural net; iii) end-to-end learning with a convolutional neural net. We show that several end-to-end learning proposals outperform approaches based on learning from hand-crafted audio features.",introduc new larg scale music dataset musicnet serv sourc supervis evalu machin learn method music research musicnet consist hundr freeli licens classic music record compos written instrument togeth instrument note annot result million tempor label hour chamber music perform various studio microphon condit defin multi label classif task predict note music record along evalu protocol benchmark sever machin learn architectur task learn hand craft spectrogram featur ii end end learn neural net iii end end learn convolut neural net show sever end end learn propos outperform approach base learn hand craft audio featur,"['John Thickstun', 'Zaid Harchaoui', 'Sham Kakade']","['stat.ML', 'cs.LG', 'cs.SD']",False,False,False,False,True,False
893,2017-03-28T14:11:30Z,2016-11-29T08:46:26Z,http://arxiv.org/abs/1611.09526v1,http://arxiv.org/pdf/1611.09526v1,Learning Filter Banks Using Deep Learning For Acoustic Signals,learn filter bank use deep learn acoust signal,"Designing appropriate features for acoustic event recognition tasks is an active field of research. Expressive features should both improve the performance of the tasks and also be interpret-able. Currently, heuristically designed features based on the domain knowledge requires tremendous effort in hand-crafting, while features extracted through deep network are difficult for human to interpret. In this work, we explore the experience guided learning method for designing acoustic features. This is a novel hybrid approach combining both domain knowledge and purely data driven feature designing. Based on the procedure of log Mel-filter banks, we design a filter bank learning layer. We concatenate this layer with a convolutional neural network (CNN) model. After training the network, the weight of the filter bank learning layer is extracted to facilitate the design of acoustic features. We smooth the trained weight of the learning layer and re-initialize it in filter bank learning layer as audio feature extractor. For the environmental sound recognition task based on the Urban- sound8K dataset, the experience guided learning leads to a 2% accuracy improvement compared with the fixed feature extractors (the log Mel-filter bank). The shape of the new filter banks are visualized and explained to prove the effectiveness of the feature design process.",design appropri featur acoust event recognit task activ field research express featur improv perform task also interpret abl current heurist design featur base domain knowledg requir tremend effort hand craft featur extract deep network difficult human interpret work explor experi guid learn method design acoust featur novel hybrid approach combin domain knowledg pure data driven featur design base procedur log mel filter bank design filter bank learn layer concaten layer convolut neural network cnn model train network weight filter bank learn layer extract facilit design acoust featur smooth train weight learn layer initi filter bank learn layer audio featur extractor environment sound recognit task base urban soundk dataset experi guid learn lead accuraci improv compar fix featur extractor log mel filter bank shape new filter bank visual explain prove effect featur design process,"['Shuhui Qu', 'Juncheng Li', 'Wei Dai', 'Samarjit Das']","['cs.SD', 'cs.AI']",False,False,False,False,True,False
894,2017-03-28T14:11:30Z,2016-11-29T08:33:48Z,http://arxiv.org/abs/1611.09524v1,http://arxiv.org/pdf/1611.09524v1,Understanding Audio Pattern Using Convolutional Neural Network From Raw   Waveforms,understand audio pattern use convolut neural network raw waveform,"One key step in audio signal processing is to transform the raw signal into representations that are efficient for encoding the original information. Traditionally, people transform the audio into spectral representations, as a function of frequency, amplitude and phase transformation. In this work, we take a purely data-driven approach to understand the temporal dynamics of audio at the raw signal level. We maximize the information extracted from the raw signal through a deep convolutional neural network (CNN) model. Our CNN model is trained on the urbansound8k dataset. We discover that salient audio patterns embedded in the raw waveforms can be efficiently extracted through a combination of nonlinear filters learned by the CNN model.",one key step audio signal process transform raw signal represent effici encod origin inform tradit peopl transform audio spectral represent function frequenc amplitud phase transform work take pure data driven approach understand tempor dynam audio raw signal level maxim inform extract raw signal deep convolut neural network cnn model cnn model train urbansoundk dataset discov salient audio pattern embed raw waveform effici extract combin nonlinear filter learn cnn model,"['Shuhui Qu', 'Juncheng Li', 'Wei Dai', 'Samarjit Das']",['cs.SD'],False,False,False,False,True,False
896,2017-03-28T14:11:30Z,2016-11-27T22:47:23Z,http://arxiv.org/abs/1611.08930v1,http://arxiv.org/pdf/1611.08930v1,Deep attractor network for single-microphone speaker separation,deep attractor network singl microphon speaker separ,"Despite the overwhelming success of deep learning in various speech processing tasks, the problem of separating simultaneous speakers in a mixture remains challenging. Two major difficulties in such systems are the arbitrary source permutation and unknown number of sources in the mixture. We propose a novel deep learning framework for single channel speech separation by creating attractor points in high dimensional embedding space of the acoustic signals which pull together the time-frequency bins corresponding to each source. Attractor points in this study are created by finding the centroids of the sources in the embedding space, which are subsequently used to determine the similarity of each bin in the mixture to each source. The network is then trained to minimize the reconstruction error of each source by optimizing the embeddings. The proposed model is different from prior works in that it implements an end-to-end training, and it does not depend on the number of sources in the mixture. Two strategies are explored in the test time, K-means and fixed attractor points, where the latter requires no post-processing and can be implemented in real-time. We evaluated our system on Wall Street Journal dataset and show 5.49\% improvement over the previous state-of-the-art methods.",despit overwhelm success deep learn various speech process task problem separ simultan speaker mixtur remain challeng two major difficulti system arbitrari sourc permut unknown number sourc mixtur propos novel deep learn framework singl channel speech separ creat attractor point high dimension embed space acoust signal pull togeth time frequenc bin correspond sourc attractor point studi creat find centroid sourc embed space subsequ use determin similar bin mixtur sourc network train minim reconstruct error sourc optim embed propos model differ prior work implement end end train doe depend number sourc mixtur two strategi explor test time mean fix attractor point latter requir post process implement real time evalu system wall street journal dataset show improv previous state art method,"['Zhuo Chen', 'Yi Luo', 'Nima Mesgarani']","['cs.SD', 'cs.LG']",False,False,False,False,True,False
897,2017-03-28T14:11:30Z,2016-11-27T22:20:51Z,http://arxiv.org/abs/1612.01928v1,http://arxiv.org/pdf/1612.01928v1,Invariant Representations for Noisy Speech Recognition,invari represent noisi speech recognit,"Modern automatic speech recognition (ASR) systems need to be robust under acoustic variability arising from environmental, speaker, channel, and recording conditions. Ensuring such robustness to variability is a challenge in modern day neural network-based ASR systems, especially when all types of variability are not seen during training. We attempt to address this problem by encouraging the neural network acoustic model to learn invariant feature representations. We use ideas from recent research on image generation using Generative Adversarial Networks and domain adaptation ideas extending adversarial gradient-based training. A recent work from Ganin et al. proposes to use adversarial training for image domain adaptation by using an intermediate representation from the main target classification network to deteriorate the domain classifier performance through a separate neural network. Our work focuses on investigating neural architectures which produce representations invariant to noise conditions for ASR. We evaluate the proposed architecture on the Aurora-4 task, a popular benchmark for noise robust ASR. We show that our method generalizes better than the standard multi-condition training especially when only a few noise categories are seen during training.",modern automat speech recognit asr system need robust acoust variabl aris environment speaker channel record condit ensur robust variabl challeng modern day neural network base asr system especi type variabl seen dure train attempt address problem encourag neural network acoust model learn invari featur represent use idea recent research imag generat use generat adversari network domain adapt idea extend adversari gradient base train recent work ganin et al propos use adversari train imag domain adapt use intermedi represent main target classif network deterior domain classifi perform separ neural network work focus investig neural architectur produc represent invari nois condit asr evalu propos architectur aurora task popular benchmark nois robust asr show method general better standard multi condit train especi onli nois categori seen dure train,"['Dmitriy Serdyuk', 'Kartik Audhkhasi', 'Philémon Brakel', 'Bhuvana Ramabhadran', 'Samuel Thomas', 'Yoshua Bengio']","['cs.CL', 'cs.CV', 'cs.LG', 'cs.SD', 'stat.ML']",False,False,False,False,True,False
899,2017-03-28T14:11:30Z,2017-01-22T22:28:47Z,http://arxiv.org/abs/1611.08749v2,http://arxiv.org/pdf/1611.08749v2,Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on   Animal calls and Speech,fast chirplet transform enhanc cnn machin listen valid anim call speech,"The scattering framework offers an optimal hierarchical convolutional decomposition according to its kernels. Convolutional Neural Net (CNN) can be seen as an optimal kernel decomposition, nevertheless it requires large amount of training data to learn its kernels. We propose a trade-off between these two approaches: a Chirplet kernel as an efficient Q constant bioacoustic representation to pretrain CNN. First we motivate Chirplet bioinspired auditory representation. Second we give the first algorithm (and code) of a Fast Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of FCT on large environmental data base: months of Orca recordings, and 1000 Birds species from the LifeClef challenge. Fourth, we validate FCT on the vowels subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN when it pretrains low level layers: it reduces training duration by -28\% for birds classification, and by -26% for vowels classification. Scores are also enhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average Precision on birds, and +2.3\% of vowel accuracy against raw audio CNN. We conclude on perspectives on tonotopic FCT deep machine listening, and inter-species bioacoustic transfer learning to generalise the representation of animal communication systems.",scatter framework offer optim hierarch convolut decomposit accord kernel convolut neural net cnn seen optim kernel decomposit nevertheless requir larg amount train data learn kernel propos trade two approach chirplet kernel effici constant bioacoust represent pretrain cnn first motiv chirplet bioinspir auditori represent second give first algorithm code fast chirplet transform fct third demonstr comput effici fct larg environment data base month orca record bird speci lifeclef challeng fourth valid fct vowel subset speech timit dataset result show fct acceler cnn pretrain low level layer reduc train durat bird classif vowel classif score also enhanc fct pretrain relat gain mean averag precis bird vowel accuraci raw audio cnn conclud perspect tonotop fct deep machin listen inter speci bioacoust transfer learn generalis represent anim communic system,"['Herve Glotin', 'Julien Ricard', 'Randall Balestriero']",['cs.SD'],False,False,False,False,True,False
928,2017-03-28T14:02:35Z,2017-03-19T01:14:47Z,http://arxiv.org/abs/1703.06378v1,http://arxiv.org/pdf/1703.06378v1,Probabilistic Models for Daily Peak Loads at Distribution Feeders,probabilist model daili peak load distribut feeder,"Load forecasting at distribution networks is more challenging than load forecasting at transmission networks because its load pattern is more stochastic and unpredictable. To plan sufficient resources and estimate DER hosting capacity, it is invaluable for a distribution network planner to get the probabilistic distribution of daily peak-load under a feeder over long term. In this paper, we model the probabilistic distribution functions of daily peak-load under a feeder using power law distributions, which is tested by improved Kolmogorov Smirnov test enhanced by the Monte Carlo simulation approach. In addition, the uncertainty of the modeling is quantified using the bootstrap method. The methodology of parameter estimation of the probabilistic model and the hypothesis test is elaborated in detail. In the case studies, it is shown using measurement data sets that the daily peak loads under several feeders follow the power law distribution by applying the proposed testing methods.",load forecast distribut network challeng load forecast transmiss network becaus load pattern stochast unpredict plan suffici resourc estim der host capac invalu distribut network planner get probabilist distribut daili peak load feeder long term paper model probabilist distribut function daili peak load feeder use power law distribut test improv kolmogorov smirnov test enhanc mont carlo simul approach addit uncertainti model quantifi use bootstrap method methodolog paramet estim probabilist model hypothesi test elabor detail case studi shown use measur data set daili peak load sever feeder follow power law distribut appli propos test method,"['Hossein Sangrody', 'Ning Zhou', 'Xingye Qiao']",['stat.AP'],False,False,False,False,True,False
1149,2017-03-28T14:04:07Z,2017-03-16T18:14:59Z,http://arxiv.org/abs/1703.05782v1,http://arxiv.org/pdf/1703.05782v1,Distributed Multi-Speaker Voice Activity Detection for Wireless Acoustic   Sensor Networks,distribut multi speaker voic activ detect wireless acoust sensor network,"A distributed multi-speaker voice activity detection (DM-VAD) method for wireless acoustic sensor networks (WASNs) is proposed. DM-VAD is required in many signal processing applications, e.g. distributed speech enhancement based on multi-channel Wiener filtering, but is non-existent up to date. The proposed method neither requires a fusion center nor prior knowledge about the node positions, microphone array orientations or the number of observed sources. It consists of two steps: (i) distributed source-specific energy signal unmixing (ii) energy signal based voice activity detection. Existing computationally efficient methods to extract source-specific energy signals from the mixed observations, e.g., multiplicative non-negative independent component analysis (MNICA) quickly loose performance with an increasing number of sources, and require a fusion center. To overcome these limitations, we introduce a distributed energy signal unmixing method based on a source-specific node clustering method to locate the nodes around each source. To determine the number of sources that are observed in the WASN, a source enumeration method that uses a Lasso penalized Poisson generalized linear model is developed. Each identified cluster estimates the energy signal of a single (dominant) source by applying a two-component MNICA. The VAD problem is transformed into a clustering task, by extracting features from the energy signals and applying K-means type clustering algorithms. All steps of the proposed method are evaluated using numerical experiments. A VAD accuracy of $> 85 \%$ is achieved for a challenging scenario where 20 nodes observe 7 sources in a simulated reverberant rectangular room.",distribut multi speaker voic activ detect dm vad method wireless acoust sensor network propos dm vad requir mani signal process applic distribut speech enhanc base multi channel wiener filter non exist date propos method neither requir fusion center prior knowledg node posit microphon array orient number observ sourc consist two step distribut sourc specif energi signal unmix ii energi signal base voic activ detect exist comput effici method extract sourc specif energi signal mix observ multipl non negat independ compon analysi mnica quick loos perform increas number sourc requir fusion center overcom limit introduc distribut energi signal unmix method base sourc specif node cluster method locat node around sourc determin number sourc observ sourc enumer method use lasso penal poisson general linear model develop identifi cluster estim energi signal singl domin sourc appli two compon mnica vad problem transform cluster task extract featur energi signal appli mean type cluster algorithm step propos method evalu use numer experi vad accuraci achiev challeng scenario node observ sourc simul reverber rectangular room,"['Mohamad Hasan Bahari', 'L. Khadidja Hamaidi', 'Michael Muma', 'Jorge Plata-Chaves', 'Marc Moonen', 'Abdelhak M. Zoubir', 'Alexander Bertrand']",['stat.ME'],False,False,False,False,True,False
1226,2017-03-28T14:01:53Z,2017-03-23T13:00:14Z,http://arxiv.org/abs/1703.08052v1,http://arxiv.org/pdf/1703.08052v1,Dynamic Bernoulli Embeddings for Language Evolution,dynam bernoulli embed languag evolut,"Word embeddings are a powerful approach for unsupervised analysis of language. Recently, Rudolph et al. (2016) developed exponential family embeddings, which cast word embeddings in a probabilistic framework. Here, we develop dynamic embeddings, building on exponential family embeddings to capture how the meanings of words change over time. We use dynamic embeddings to analyze three large collections of historical texts: the U.S. Senate speeches from 1858 to 2009, the history of computer science ACM abstracts from 1951 to 2014, and machine learning papers on the Arxiv from 2007 to 2015. We find dynamic embeddings provide better fits than classical embeddings and capture interesting patterns about how language changes.",word embed power approach unsupervis analysi languag recent rudolph et al develop exponenti famili embed cast word embed probabilist framework develop dynam embed build exponenti famili embed captur mean word chang time use dynam embed analyz three larg collect histor text senat speech histori comput scienc acm abstract machin learn paper arxiv find dynam embed provid better fit classic embed captur interest pattern languag chang,"['Maja Rudolph', 'David Blei']","['stat.ML', 'cs.CL']",False,False,False,False,True,False
1238,2017-03-28T14:01:57Z,2017-03-22T17:17:16Z,http://arxiv.org/abs/1703.07754v1,http://arxiv.org/pdf/1703.07754v1,Direct Acoustics-to-Word Models for English Conversational Speech   Recognition,direct acoust word model english convers speech recognit,"Recent work on end-to-end automatic speech recognition (ASR) has shown that the connectionist temporal classification (CTC) loss can be used to convert acoustics to phone or character sequences. Such systems are used with a dictionary and separately-trained Language Model (LM) to produce word sequences. However, they are not truly end-to-end in the sense of mapping acoustics directly to words without an intermediate phone representation. In this paper, we present the first results employing direct acoustics-to-word CTC models on two well-known public benchmark tasks: Switchboard and CallHome. These models do not require an LM or even a decoder at run-time and hence recognize speech with minimal complexity. However, due to the large number of word output units, CTC word models require orders of magnitude more data to train reliably compared to traditional systems. We present some techniques to mitigate this issue. Our CTC word model achieves a word error rate of 13.0%/18.8% on the Hub5-2000 Switchboard/CallHome test sets without any LM or decoder compared with 9.6%/16.0% for phone-based CTC with a 4-gram LM. We also present rescoring results on CTC word model lattices to quantify the performance benefits of a LM, and contrast the performance of word and phone CTC models.",recent work end end automat speech recognit asr shown connectionist tempor classif ctc loss use convert acoust phone charact sequenc system use dictionari separ train languag model lm produc word sequenc howev truli end end sens map acoust direct word without intermedi phone represent paper present first result employ direct acoust word ctc model two well known public benchmark task switchboard callhom model requir lm even decod run time henc recogn speech minim complex howev due larg number word output unit ctc word model requir order magnitud data train reliabl compar tradit system present techniqu mitig issu ctc word model achiev word error rate hub switchboard callhom test set without ani lm decod compar phone base ctc gram lm also present rescor result ctc word model lattic quantifi perform benefit lm contrast perform word phone ctc model,"['Kartik Audhkhasi', 'Bhuvana Ramabhadran', 'George Saon', 'Michael Picheny', 'David Nahamoo']","['cs.CL', 'cs.NE', 'stat.ML']",False,False,False,False,True,False
1245,2017-03-28T14:02:02Z,2017-03-22T03:26:32Z,http://arxiv.org/abs/1703.07506v1,http://arxiv.org/abs/1703.07506v1,LogitBoost autoregressive networks,logitboost autoregress network,"Multivariate binary distributions can be decomposed into products of univariate conditional distributions. Recently popular approaches have modeled these conditionals through neural networks with sophisticated weight-sharing structures. It is shown that state-of-the-art performance on several standard benchmark datasets can actually be achieved by training separate probability estimators for each dimension. In that case, model training can be trivially parallelized over data dimensions. On the other hand, complexity control has to be performed for each learned conditional distribution. Three possible methods are considered and experimentally compared. The estimator that is employed for each conditional is LogitBoost. Similarities and differences between the proposed approach and autoregressive models based on neural networks are discussed in detail.",multivari binari distribut decompos product univari condit distribut recent popular approach model condit neural network sophist weight share structur shown state art perform sever standard benchmark dataset actual achiev train separ probabl estim dimens case model train trivial parallel data dimens hand complex control perform learn condit distribut three possibl method consid experiment compar estim employ condit logitboost similar differ propos approach autoregress model base neural network discuss detail,['Marc Goessling'],"['stat.ML', 'cs.LG']",False,False,False,False,True,False
1296,2017-03-28T14:02:22Z,2017-03-16T09:52:48Z,http://arxiv.org/abs/1703.05537v1,http://arxiv.org/pdf/1703.05537v1,Shift Aggregate Extract Networks,shift aggreg extract network,"We introduce an architecture based on deep hierarchical decompositions to learn effective representations of large graphs. Our framework extends classic R-decompositions used in kernel methods, enabling nested ""part-of-part"" relations. Unlike recursive neural networks, which unroll a template on input graphs directly, we unroll a neural network template over the decomposition hierarchy, allowing us to deal with the high degree variability that typically characterize social network graphs. Deep hierarchical decompositions are also amenable to domain compression, a technique that reduces both space and time complexity by exploiting symmetries. We show empirically that our approach is competitive with current state-of-the-art graph classification methods, particularly when dealing with social network datasets.",introduc architectur base deep hierarch decomposit learn effect represent larg graph framework extend classic decomposit use kernel method enabl nest part part relat unlik recurs neural network unrol templat input graph direct unrol neural network templat decomposit hierarchi allow us deal high degre variabl typic character social network graph deep hierarch decomposit also amen domain compress techniqu reduc space time complex exploit symmetri show empir approach competit current state art graph classif method particular deal social network dataset,"['Francesco Orsini', 'Daniele Baracchi', 'Paolo Frasconi']","['cs.LG', 'stat.ML']",False,False,False,False,True,False
1330,2017-03-28T14:04:46Z,2017-03-13T17:54:07Z,http://arxiv.org/abs/1703.04517v1,http://arxiv.org/pdf/1703.04517v1,Variable selection in discriminant analysis for mixed variables and   several groups,variabl select discrimin analysi mix variabl sever group,"We propose a method for variable selection in discriminant analysis with mixed categorical and continuous variables. This method is based on a criterion that permits to reduce the variable selection problem to a problem of estimating suitable permutation and dimensionality. Then, estimators for these parameters are proposed and the resulting method for selecting variables is shown to be consistent. A simulation study that permits to study several poperties of the proposed approach and to compare it with an existing method is given.",propos method variabl select discrimin analysi mix categor continu variabl method base criterion permit reduc variabl select problem problem estim suitabl permut dimension estim paramet propos result method select variabl shown consist simul studi permit studi sever poperti propos approach compar exist method given,"['Alban Mbina Mbina', 'Guy Martial Nkiet', 'Fulgence Eyi Obiang']","['math.ST', 'math.PR', 'stat.TH']",False,False,False,False,True,False
