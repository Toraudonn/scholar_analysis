2017-03-28T14:10:52Z|2017-03-27T16:48:03Z|http://arxiv.org/abs/1703.09179v1|http://arxiv.org/pdf/1703.09179v1|Transfer learning for music classification and regression tasks|transfer learn music classif regress task|In this paper, we present a transfer learning approach for music classification and regression tasks. We propose to use a pretrained convnet feature, a concatenated feature vector using activations of feature maps of multiple layers in a trained convolutional network. We show that how this convnet feature can serve as a general-purpose music representation. In the experiment, a convnet is trained for music tagging and then transferred for many music-related classification and regression tasks as well as an audio-related classification task. In experiments, the convnet feature outperforms the baseline MFCC feature in all tasks and many reported approaches of aggregating MFCCs and low- and high-level music features.|paper present transfer learn approach music classif regress task propos use pretrain convnet featur concaten featur vector use activ featur map multipl layer train convolut network show convnet featur serv general purpos music represent experi convnet train music tag transfer mani music relat classif regress task well audio relat classif task experi convnet featur outperform baselin mfcc featur task mani report approach aggreg mfccs low high level music featur|['Keunwoo Choi', 'Gy√∂rgy Fazekas', 'Mark Sandler', 'Kyunghyun Cho']|['cs.CV', 'cs.AI', 'cs.MM', 'cs.SD']
2017-03-28T14:10:52Z|2017-03-24T20:59:52Z|http://arxiv.org/abs/1703.08596v1|http://arxiv.org/pdf/1703.08596v1|The Inner Structure of Time-Dependent Signals|inner structur time depend signal|This paper shows how a time series of measurements of an evolving system can be processed to create an inner time series that is unaffected by any instantaneous invertible, possibly nonlinear transformation of the measurements. An inner time series contains information that does not depend on the nature of the sensors, which the observer chose to monitor the system. Instead, it encodes information that is intrinsic to the evolution of the observed system. Because of its sensor-independence, an inner time series may produce fewer false negatives when it is used to detect events in the presence of sensor drift. Furthermore, if the observed physical system is comprised of non-interacting subsystems, its inner time series is separable; i.e., it consists of a collection of time series, each one being the inner time series of an isolated subsystem. Because of this property, an inner time series can be used to detect a specific behavior of one of the independent subsystems without using blind source separation to disentangle that subsystem from the others. The method is illustrated by applying it to: 1) an analytic example; 2) the audio waveform of one speaker; 3) video images from a moving camera; 4) mixtures of audio waveforms of two speakers.|paper show time seri measur evolv system process creat inner time seri unaffect ani instantan invert possibl nonlinear transform measur inner time seri contain inform doe depend natur sensor observ chose monitor system instead encod inform intrins evolut observ system becaus sensor independ inner time seri may produc fewer fals negat use detect event presenc sensor drift furthermor observ physic system compris non interact subsystem inner time seri separ consist collect time seri one inner time seri isol subsystem becaus properti inner time seri use detect specif behavior one independ subsystem without use blind sourc separ disentangl subsystem method illustr appli analyt exampl audio waveform one speaker video imag move camera mixtur audio waveform two speaker|['David N. Levin']|['stat.ME', 'cs.SD', 'math.ST', 'stat.TH']
2017-03-28T14:10:52Z|2017-03-23T17:07:14Z|http://arxiv.org/abs/1703.08144v1|http://arxiv.org/pdf/1703.08144v1|Note Value Recognition for Rhythm Transcription Using a Markov Random   Field Model for Musical Scores and Performances of Piano Music|note valu recognit rhythm transcript use markov random field model music score perform piano music|This paper presents a statistical method for music transcription that can estimate score times of note onsets and offsets from polyphonic MIDI performance signals. Because performed note durations can deviate largely from score-indicated values, previous methods had the problem of not being able to accurately estimate offset score times (or note values) and thus could only output incomplete musical scores. Based on observations that the pitch context and onset score times are influential on the configuration of note values, we construct a context-tree model that provides prior distributions of note values using these features and combine it with a performance model in the framework of Markov random fields. Evaluation results showed that our method reduces the average error rate by around 40 percent compared to existing/simple methods. We also confirmed that, in our model, the score model plays a more important role than the performance model, and it automatically captures the voice structure by unsupervised learning.|paper present statist method music transcript estim score time note onset offset polyphon midi perform signal becaus perform note durat deviat larg score indic valu previous method problem abl accur estim offset score time note valu thus could onli output incomplet music score base observ pitch context onset score time influenti configur note valu construct context tree model provid prior distribut note valu use featur combin perform model framework markov random field evalu result show method reduc averag error rate around percent compar exist simpl method also confirm model score model play import role perform model automat captur voic structur unsupervis learn|['Eita Nakamura', 'Kazuyoshi Yoshii', 'Simon Dixon']|['cs.AI', 'cs.SD']
2017-03-28T14:10:52Z|2017-03-23T11:45:10Z|http://arxiv.org/abs/1703.08019v1|http://arxiv.org/pdf/1703.08019v1|Single Channel Audio Source Separation using Convolutional Denoising   Autoencoders|singl channel audio sourc separ use convolut denois autoencod|Deep learning techniques have been used recently to tackle the audio source separation problem. In this work, we propose to use deep convolution denoising auto-encoders (CDAEs) for monaural audio source separation. We use as many CDAEs as the number of sources to be separated from the mixed signal. Each CDAE is trained to separate one source and treats the other sources as background noise. The main idea is to allow each CDAE to learn suitable time-frequency filters and features to its corresponding source. Our experimental results show that CDAEs perform source separation slightly better than the deep feedforward neural networks (FNNs) even with a much less number of parameters than FNNs.|deep learn techniqu use recent tackl audio sourc separ problem work propos use deep convolut denois auto encod cdae monaur audio sourc separ use mani cdae number sourc separ mix signal cdae train separ one sourc treat sourc background nois main idea allow cdae learn suitabl time frequenc filter featur correspond sourc experiment result show cdae perform sourc separ slight better deep feedforward neural network fnns even much less number paramet fnns|['Emad M. Grais', 'Mark D. Plumbley']|['cs.SD']
2017-03-28T14:10:52Z|2017-03-22T10:08:51Z|http://arxiv.org/abs/1703.07588v1|http://arxiv.org/pdf/1703.07588v1|Gate Activation Signal Analysis for Gated Recurrent Neural Networks and   Its Correlation with Phoneme Boundaries|gate activ signal analysi gate recurr neural network correl phonem boundari|In this paper we analyze the gate activation signals inside the gated recurrent neural networks, and find the temporal structure of such signals is highly correlated with the phoneme boundaries. This correlation is further verified by a set of experiments for phoneme segmentation, in which better results compared to standard approaches were obtained.|paper analyz gate activ signal insid gate recurr neural network find tempor structur signal high correl phonem boundari correl verifi set experi phonem segment better result compar standard approach obtain|['Yu-Hsuan Wang', 'Cheng-Tao Chung', 'Hung-yi Lee']|['cs.SD', 'cs.CL', 'cs.LG']
2017-03-28T14:10:52Z|2017-03-21T12:35:21Z|http://arxiv.org/abs/1703.07172v1|http://arxiv.org/pdf/1703.07172v1|Multi-Objective Learning and Mask-Based Post-Processing for Deep Neural   Network Based Speech Enhancement|multi object learn mask base post process deep neural network base speech enhanc|We propose a multi-objective framework to learn both secondary targets not directly related to the intended task of speech enhancement (SE) and the primary target of the clean log-power spectra (LPS) features to be used directly for constructing the enhanced speech signals. In deep neural network (DNN) based SE we introduce an auxiliary structure to learn secondary continuous features, such as mel-frequency cepstral coefficients (MFCCs), and categorical information, such as the ideal binary mask (IBM), and integrate it into the original DNN architecture for joint optimization of all the parameters. This joint estimation scheme imposes additional constraints not available in the direct prediction of LPS, and potentially improves the learning of the primary target. Furthermore, the learned secondary information as a byproduct can be used for other purposes, e.g., the IBM-based post-processing in this work. A series of experiments show that joint LPS and MFCC learning improves the SE performance, and IBM-based post-processing further enhances listening quality of the reconstructed speech.|propos multi object framework learn secondari target direct relat intend task speech enhanc se primari target clean log power spectra lps featur use direct construct enhanc speech signal deep neural network dnn base se introduc auxiliari structur learn secondari continu featur mel frequenc cepstral coeffici mfccs categor inform ideal binari mask ibm integr origin dnn architectur joint optim paramet joint estim scheme impos addit constraint avail direct predict lps potenti improv learn primari target furthermor learn secondari inform byproduct use purpos ibm base post process work seri experi show joint lps mfcc learn improv se perform ibm base post process enhanc listen qualiti reconstruct speech|['Yong Xu', 'Jun Du', 'Zhen Huang', 'Li-Rong Dai', 'Chin-Hui Lee']|['cs.SD']
2017-03-28T14:10:52Z|2017-03-21T06:09:32Z|http://arxiv.org/abs/1703.07065v1|http://arxiv.org/pdf/1703.07065v1|Adaptive Multi-Class Audio Classification in Noisy In-Vehicle   Environment|adapt multi class audio classif noisi vehicl environ|With ever-increasing number of car-mounted electric devices and their complexity, audio classification is increasingly important for the automotive industry as a fundamental tool for human-device interactions. Existing approaches for audio classification, however, fall short as the unique and dynamic audio characteristics of in-vehicle environments are not appropriately taken into account. In this paper, we develop an audio classification system that classifies an audio stream into music, speech, speech+music, and noise, adaptably depending on driving environments including highway, local road, crowded city, and stopped vehicle. More than 420 minutes of audio data including various genres of music, speech, speech+music, and noise are collected from diverse driving environments. The results demonstrate that the proposed approach improves the average classification accuracy up to 166%, and 64% for speech, and speech+music, respectively, compared with a non-adaptive approach in our experimental settings.|ever increas number car mount electr devic complex audio classif increas import automot industri fundament tool human devic interact exist approach audio classif howev fall short uniqu dynam audio characterist vehicl environ appropri taken account paper develop audio classif system classifi audio stream music speech speech music nois adapt depend drive environ includ highway local road crowd citi stop vehicl minut audio data includ various genr music speech speech music nois collect divers drive environ result demonstr propos approach improv averag classif accuraci speech speech music respect compar non adapt approach experiment set|['Myounggyu Won', 'Haitham Alsaadan', 'Yongsoon Eun']|['cs.SD']
2017-03-28T14:10:52Z|2017-03-20T18:11:47Z|http://arxiv.org/abs/1703.06902v1|http://arxiv.org/pdf/1703.06902v1|A Comparison of deep learning methods for environmental sound|comparison deep learn method environment sound|Environmental sound detection is a challenging application of machine learning because of the noisy nature of the signal, and the small amount of (labeled) data that is typically available. This work thus presents a comparison of several state-of-the-art Deep Learning models on the IEEE challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) 2016 challenge task and data, classifying sounds into one of fifteen common indoor and outdoor acoustic scenes, such as bus, cafe, car, city center, forest path, library, train, etc. In total, 13 hours of stereo audio recordings are available, making this one of the largest datasets available. We perform experiments on six sets of features, including standard Mel-frequency cepstral coefficients (MFCC), Binaural MFCC, log Mel-spectrum and two different large- scale temporal pooling features extracted using OpenSMILE. On these features, we apply five models: Gaussian Mixture Model (GMM), Deep Neural Network (DNN), Recurrent Neural Network (RNN), Convolutional Deep Neural Net- work (CNN) and i-vector. Using the late-fusion approach, we improve the performance of the baseline 72.5% by 15.6% in 4-fold Cross Validation (CV) avg. accuracy and 11% in test accuracy, which matches the best result of the DCASE 2016 challenge. With large feature sets, deep neural network models out- perform traditional methods and achieve the best performance among all the studied methods. Consistent with other work, the best performing single model is the non-temporal DNN model, which we take as evidence that sounds in the DCASE challenge do not exhibit strong temporal dynamics.|environment sound detect challeng applic machin learn becaus noisi natur signal small amount label data typic avail work thus present comparison sever state art deep learn model ieee challeng detect classif acoust scene event dcase challeng task data classifi sound one fifteen common indoor outdoor acoust scene bus cafe car citi center forest path librari train etc total hour stereo audio record avail make one largest dataset avail perform experi six set featur includ standard mel frequenc cepstral coeffici mfcc binaur mfcc log mel spectrum two differ larg scale tempor pool featur extract use opensmil featur appli five model gaussian mixtur model gmm deep neural network dnn recurr neural network rnn convolut deep neural net work cnn vector use late fusion approach improv perform baselin fold cross valid cv avg accuraci test accuraci match best result dcase challeng larg featur set deep neural network model perform tradit method achiev best perform among studi method consist work best perform singl model non tempor dnn model take evid sound dcase challeng exhibit strong tempor dynam|['Juncheng Li', 'Wei Dai', 'Florian Metze', 'Shuhui Qu', 'Samarjit Das']|['cs.SD', 'cs.LG', '14J60 (Primary)']
2017-03-28T14:10:52Z|2017-03-22T07:44:55Z|http://arxiv.org/abs/1703.06891v2|http://arxiv.org/pdf/1703.06891v2|Dance Dance Convolution|danc danc convolut|Dance Dance Revolution (DDR) is a popular rhythm-based video game. Players perform steps on a dance platform in synchronization with music as directed by on-screen step charts. While many step charts are available in standardized packs, users may grow tired of existing charts, or wish to dance to a song for which no chart exists. We introduce the task of learning to choreograph. Given a raw audio track, the goal is to produce a new step chart. This task decomposes naturally into two subtasks: deciding when to place steps and deciding which steps to select. For the step placement task, we combine recurrent and convolutional neural networks to ingest spectrograms of low-level audio features to predict steps, conditioned on chart difficulty. For step selection, we present a conditional LSTM generative model that substantially outperforms n-gram and fixed-window approaches.|danc danc revolut ddr popular rhythm base video game player perform step danc platform synchron music direct screen step chart mani step chart avail standard pack user may grow tire exist chart wish danc song chart exist introduc task learn choreograph given raw audio track goal produc new step chart task decompos natur two subtask decid place step decid step select step placement task combin recurr convolut neural network ingest spectrogram low level audio featur predict step condit chart difficulti step select present condit lstm generat model substanti outperform gram fix window approach|['Chris Donahue', 'Zachary C. Lipton', 'Julian McAuley']|['cs.LG', 'cs.MM', 'cs.NE', 'cs.SD', 'stat.ML']
2017-03-28T14:10:52Z|2017-03-20T15:58:02Z|http://arxiv.org/abs/1703.06812v1|http://arxiv.org/pdf/1703.06812v1|Simple empirical algorithm to obtain signal envelope in three steps|simpl empir algorithm obtain signal envelop three step|Signal amplitude envelope allows to obtain information on the signal features for different applications. It is commonly agreed that the envelope is a signal that varies slowly and it should pass the prominent peaks of the data smoothly. It has been widely used in sound analysis and also in different variables of physiological data for animal and human studies. In order to get signal envelope, a simple algorithm is proposed based on peak detection and it was implemented with python libraries. This method can be used for different applications in sound or in general in time series analysis for signals of different origin or frequency content. As well, some aspects on the parameter selection are discussed here to adapt the same method for different applications and some traditional methods are also revisited.|signal amplitud envelop allow obtain inform signal featur differ applic common agre envelop signal vari slowli pass promin peak data smooth wide use sound analysi also differ variabl physiolog data anim human studi order get signal envelop simpl algorithm propos base peak detect implement python librari method use differ applic sound general time seri analysi signal differ origin frequenc content well aspect paramet select discuss adapt method differ applic tradit method also revisit|['Cecilia Jarne']|['cs.SD']
2017-03-28T14:10:56Z|2017-03-20T12:00:04Z|http://arxiv.org/abs/1703.06697v1|http://arxiv.org/pdf/1703.06697v1|Timbre Analysis of Music Audio Signals with Convolutional Neural   Networks|timbr analysi music audio signal convolut neural network|The focus of this work is to study how to efficiently tailor Convolutional Neural Networks (CNNs) towards learning timbre representations from log-mel magnitude spectrograms. We first review the trends when designing CNN architectures. Through this literature overview we discuss which are the crucial points to consider for efficiently learning timbre representations using CNNs. From this discussion we propose a design strategy meant to capture the relevant time-frequency contexts for learning timbre, which permits using domain knowledge for designing architectures. In addition, one of our main goals is to design efficient CNN architectures - what reduces the risk of these models to over-fit, since CNNs' number of parameters is minimized. Several architectures based on the design principles we propose are successfully assessed for different research tasks related to timbre: singing voice phoneme classification, musical instrument recognition and music auto-tagging.|focus work studi effici tailor convolut neural network cnns toward learn timbr represent log mel magnitud spectrogram first review trend design cnn architectur literatur overview discuss crucial point consid effici learn timbr represent use cnns discuss propos design strategi meant captur relev time frequenc context learn timbr permit use domain knowledg design architectur addit one main goal design effici cnn architectur reduc risk model fit sinc cnns number paramet minim sever architectur base design principl propos success assess differ research task relat timbr sing voic phonem classif music instrument recognit music auto tag|['Jordi Pons', 'Olga Slizovskaia', 'Rong Gong', 'Emilia G√≥mez', 'Xavier Serra']|['cs.SD']
2017-03-28T14:10:56Z|2017-03-19T19:10:27Z|http://arxiv.org/abs/1703.06491v1|http://arxiv.org/pdf/1703.06491v1|Gestalt Phenomenon in Music? A Neurocognitive Physics Study with EEG|gestalt phenomenon music neurocognit physic studi eeg|The term gestalt has been widely used in the field of psychology which defined the perception of human mind to group any object not in part but as a unified whole. Music in general is polytonic i.e. a combination of a number of pure tones (frequencies) mixed together in a manner that sounds harmonius. The study of human brain response due to different frequency groups of acoustic signal can give us an excellent insight regarding the neural and functional architecture of brain functions. In this work we have tried to analyze the effect of different frequency bands of music on the various frequency rhythms of human brain obtained from EEG data of 5 participants. Four (4) widely popular Rabindrasangeet clips were subjected to Wavelet Transform method for extracting five resonant frequency bands from the original music signal. These resonant frequency bands were presented to the subjects as auditory stimulus and EEG signals recorded simultaneously in 19 different locations of the brain. The recorded EEG signals were noise cleaned and subjected to Multifractal Detrended Fluctuation Analysis (MFDFA) technique on the alpha, theta and gamma frequency range. Thus, we obtained the complexity values (in the form of multifractal spectral width) in alpha, theta and gamma EEG rhythms corresponding to different frequency bands of music. We obtain frequency specific arousal based response in different lobes of brain as well as in specific EEG bands corresponding to musical stimuli. This revelation can be of immense importance when it comes to the field of cognitive music therapy.|term gestalt wide use field psycholog defin percept human mind group ani object part unifi whole music general polyton combin number pure tone frequenc mix togeth manner sound harmonius studi human brain respons due differ frequenc group acoust signal give us excel insight regard neural function architectur brain function work tri analyz effect differ frequenc band music various frequenc rhythm human brain obtain eeg data particip four wide popular rabindrasangeet clip subject wavelet transform method extract five reson frequenc band origin music signal reson frequenc band present subject auditori stimulus eeg signal record simultan differ locat brain record eeg signal nois clean subject multifract detrend fluctuat analysi mfdfa techniqu alpha theta gamma frequenc rang thus obtain complex valu form multifract spectral width alpha theta gamma eeg rhythm correspond differ frequenc band music obtain frequenc specif arous base respons differ lobe brain well specif eeg band correspond music stimuli revel immens import come field cognit music therapi|['Shankha Sanyal', 'Archi Banerjee', 'Souparno Roy', 'Sourya Sengupta', 'Sayan Biswas', 'Sayan Nag', 'Ranjan Sengupta', 'Dipak Ghosh']|['cs.SD', 'q-bio.NC']
2017-03-28T14:10:56Z|2017-03-18T10:59:03Z|http://arxiv.org/abs/1703.06284v1|http://arxiv.org/pdf/1703.06284v1|Multi-talker Speech Separation and Tracing with Permutation Invariant   Training of Deep Recurrent Neural Networks|multi talker speech separ trace permut invari train deep recurr neural network|Despite the significant progress made in the recent years in dictating single-talker speech, the progress made in speaker independent multi-talker mixed speech separation and tracing, often referred to as the cocktail-party problem, has been less impressive. In this paper we propose a novel technique for attacking this problem. The core of our technique is permutation invariant training (PIT), which aims at minimizing the source stream reconstruction error no matter how labels are ordered. This is achieved by aligning labels to the output streams automatically during the training time. This strategy effectively solves the label permutation problem observed in deep learning based techniques for speech separation. More interestingly, our approach can integrate speaker tracing in the PIT framework so that separation and tracing can be carried out in one step and trained end-to-end. This is achieved using recurrent neural networks (RNNs) by forcing separated frames belonging to the same speaker to be aligned to the same output layer during training. Furthermore, the computational cost introduced by PIT is very small compared to the RNN computation during training and is zero during separation. We evaluated PIT on the WSJ0 and Danish two- and three-talker mixed-speech separation tasks and found that it compares favorably to non-negative matrix factorization (NMF), computational auditory scene analysis (CASA), deep clustering (DPCL) and deep attractor network (DANet), and generalizes well over unseen speakers and languages.|despit signific progress made recent year dictat singl talker speech progress made speaker independ multi talker mix speech separ trace often refer cocktail parti problem less impress paper propos novel techniqu attack problem core techniqu permut invari train pit aim minim sourc stream reconstruct error matter label order achiev align label output stream automat dure train time strategi effect solv label permut problem observ deep learn base techniqu speech separ interest approach integr speaker trace pit framework separ trace carri one step train end end achiev use recurr neural network rnns forc separ frame belong speaker align output layer dure train furthermor comput cost introduc pit veri small compar rnn comput dure train zero dure separ evalu pit wsj danish two three talker mix speech separ task found compar favor non negat matrix factor nmf comput auditori scene analysi casa deep cluster dpcl deep attractor network danet general well unseen speaker languag|['Morten Kolb√¶k', 'Dong Yu', 'Zheng-Hua Tan', 'Jesper Jensen']|['cs.SD', 'cs.LG']
2017-03-28T14:10:56Z|2017-03-17T15:31:58Z|http://arxiv.org/abs/1703.06052v1|http://arxiv.org/pdf/1703.06052v1|Attention and Localization based on a Deep Convolutional Recurrent Model   for Weakly Supervised Audio Tagging|attent local base deep convolut recurr model weak supervis audio tag|Audio tagging aims to perform multi-label classification on audio chunks and it is a newly proposed task in the Detection and Classification of Acoustic Scenes and Events 2016 (DCASE 2016) challenge. This task encourages research efforts to better analyze and understand the content of the huge amounts of audio data on the web. The difficulty in audio tagging is that it only has a chunk-level label without a frame-level label. This paper presents a weakly supervised method to not only predict the tags but also indicate the temporal locations of the occurred acoustic events. The attention scheme is found to be effective in identifying the important frames while ignoring the unrelated frames. The proposed framework is a deep convolutional recurrent model with two auxiliary modules: an attention module and a localization module. The proposed algorithm was evaluated on the Task 4 of DCASE 2016 challenge. State-of-the-art performance was achieved on the evaluation set with equal error rate (EER) reduced from 0.13 to 0.11, compared with the convolutional recurrent baseline system.|audio tag aim perform multi label classif audio chunk newli propos task detect classif acoust scene event dcase challeng task encourag research effort better analyz understand content huge amount audio data web difficulti audio tag onli chunk level label without frame level label paper present weak supervis method onli predict tag also indic tempor locat occur acoust event attent scheme found effect identifi import frame ignor unrel frame propos framework deep convolut recurr model two auxiliari modul attent modul local modul propos algorithm evalu task dcase challeng state art perform achiev evalu set equal error rate eer reduc compar convolut recurr baselin system|['Yong Xu', 'Qiuqiang Kong', 'Qiang Huang', 'Wenwu Wang', 'Mark D. Plumbley']|['cs.SD']
2017-03-28T14:10:56Z|2017-03-17T03:38:48Z|http://arxiv.org/abs/1703.05880v1|http://arxiv.org/pdf/1703.05880v1|Empirical Evaluation of Parallel Training Algorithms on Acoustic   Modeling|empir evalu parallel train algorithm acoust model|Deep learning models (DLMs) are state-of-the-art techniques in speech recognition. However, training good DLMs can be time consuming especially for production-size models and corpora. Although several parallel training algorithms have been proposed to improve training efficiency, there is no clear guidance on which one to choose for the task in hand due to lack of systematic and fair comparison among them. In this paper we aim at filling this gap by comparing four popular parallel training algorithms in speech recognition, namely asynchronous stochastic gradient descent (ASGD), blockwise model-update filtering (BMUF), bulk synchronous parallel (BSP) and elastic averaging stochastic gradient descent (EASGD), on 1000-hour LibriSpeech corpora using feed-forward deep neural networks (DNNs) and convolutional, long short-term memory, DNNs (CLDNNs). Based on our experiments, we recommend using BMUF as the top choice to train acoustic models since it is most stable, scales well with number of GPUs, can achieve reproducible results, and in many cases even outperforms single-GPU SGD. ASGD can be used as a substitute in some cases.|deep learn model dlms state art techniqu speech recognit howev train good dlms time consum especi product size model corpora although sever parallel train algorithm propos improv train effici clear guidanc one choos task hand due lack systemat fair comparison among paper aim fill gap compar four popular parallel train algorithm speech recognit name asynchron stochast gradient descent asgd blockwis model updat filter bmuf bulk synchron parallel bsp elast averag stochast gradient descent easgd hour librispeech corpora use feed forward deep neural network dnns convolut long short term memori dnns cldnns base experi recommend use bmuf top choic train acoust model sinc stabl scale well number gpus achiev reproduc result mani case even outperform singl gpu sgd asgd use substitut case|['Wenpeng Li', 'BinBin Zhang', 'Lei Xie', 'Dong Yu']|['cs.CL', 'cs.LG', 'cs.SD']
2017-03-28T14:10:56Z|2017-03-15T18:41:37Z|http://arxiv.org/abs/1703.05344v1|http://arxiv.org/pdf/1703.05344v1|Deducing the severity of psychiatric symptoms from the human voice|deduc sever psychiatr symptom human voic|Psychiatric illnesses are often associated with multiple symptoms, whose severity must be graded for accurate diagnosis and treatment. This grading is usually done by trained clinicians based on human observations and judgments made within doctor-patient sessions. Current research provides sufficient reason to expect that the human voice may carry biomarkers or signatures of many, if not all, these symptoms. Based on this conjecture, we explore the possibility of objectively and automatically grading the symptoms of psychiatric illnesses with reference to various standard psychiatric rating scales. Using acoustic data from several clinician-patient interviews within hospital settings, we use non-parametric models to learn and predict the relations between symptom-ratings and voice. In the process, we show that different articulatory-phonetic units of speech are able to capture the effects of different symptoms differently, and use this to establish a plausible methodology that could be employed for automatically grading psychiatric symptoms for clinical purposes.|psychiatr ill often associ multipl symptom whose sever must grade accur diagnosi treatment grade usual done train clinician base human observ judgment made within doctor patient session current research provid suffici reason expect human voic may carri biomark signatur mani symptom base conjectur explor possibl object automat grade symptom psychiatr ill refer various standard psychiatr rate scale use acoust data sever clinician patient interview within hospit set use non parametr model learn predict relat symptom rate voic process show differ articulatori phonet unit speech abl captur effect differ symptom differ use establish plausibl methodolog could employ automat grade psychiatr symptom clinic purpos|['Rita Singh', 'Justin Baker', 'Luciana Pennant', 'Louis-Philippe Morency']|['cs.SD', 'q-bio.NC']
2017-03-28T14:10:56Z|2017-03-15T08:33:38Z|http://arxiv.org/abs/1703.05003v1|http://arxiv.org/pdf/1703.05003v1|On the Importance of Super-Gaussian Speech Priors for Pre-Trained Speech   Enhancement|import super gaussian speech prior pre train speech enhanc|For enhancing noisy signals, pre-trained single-channel speech enhancement schemes exploit prior knowledge about the shape of typical speech structures. This knowledge is obtained from training data for which methods from machine learning are used, e.g., Mixtures of Gaussians, nonnegative matrix factorization, and deep neural networks. If only speech envelopes are employed as prior speech knowledge, e.g., to meet requirements in terms of computational complexity and memory consumption, Wiener-like enhancement filters will not be able to reduce noise components between speech spectral harmonics. In this paper, we highlight the role of clean speech estimators that employ super-Gaussian speech priors in particular for pre- trained approaches when spectral envelope models are used. In the 2000s, such estimators have been considered by many researchers for improving non-trained enhancement schemes. However, while the benefit of super-Gaussian clean speech estimators in non-trained enhancement schemes is limited, we point out that these estimators make a much larger difference for enhancement schemes that employ pre-trained envelope models. We show that for such pre-trained enhancements schemes super- Gaussian estimators allow for a suppression of annoying residual noises which are not reduced using Gaussian filters such as the Wiener filter. As a consequence, considerable improvements in terms of Perceptual Evaluation of Speech Quality and segmental signal-to-noise ratios are achieved.|enhanc noisi signal pre train singl channel speech enhanc scheme exploit prior knowledg shape typic speech structur knowledg obtain train data method machin learn use mixtur gaussian nonneg matrix factor deep neural network onli speech envelop employ prior speech knowledg meet requir term comput complex memori consumpt wiener like enhanc filter abl reduc nois compon speech spectral harmon paper highlight role clean speech estim employ super gaussian speech prior particular pre train approach spectral envelop model use estim consid mani research improv non train enhanc scheme howev benefit super gaussian clean speech estim non train enhanc scheme limit point estim make much larger differ enhanc scheme employ pre train envelop model show pre train enhanc scheme super gaussian estim allow suppress annoy residu nois reduc use gaussian filter wiener filter consequ consider improv term perceptu evalu speech qualiti segment signal nois ratio achiev|['Robert Rehr', 'Timo Gerkmann']|['cs.SD']
2017-03-28T14:10:56Z|2017-03-14T22:28:51Z|http://arxiv.org/abs/1703.04783v1|http://arxiv.org/pdf/1703.04783v1|Multichannel End-to-end Speech Recognition|multichannel end end speech recognit|The field of speech recognition is in the midst of a paradigm shift: end-to-end neural networks are challenging the dominance of hidden Markov models as a core technology. Using an attention mechanism in a recurrent encoder-decoder architecture solves the dynamic time alignment problem, allowing joint end-to-end training of the acoustic and language modeling components. In this paper we extend the end-to-end framework to encompass microphone array signal processing for noise suppression and speech enhancement within the acoustic encoding network. This allows the beamforming components to be optimized jointly within the recognition architecture to improve the end-to-end speech recognition objective. Experiments on the noisy speech benchmarks (CHiME-4 and AMI) show that our multichannel end-to-end system outperformed the attention-based baseline with input from a conventional adaptive beamformer.|field speech recognit midst paradigm shift end end neural network challeng domin hidden markov model core technolog use attent mechan recurr encod decod architectur solv dynam time align problem allow joint end end train acoust languag model compon paper extend end end framework encompass microphon array signal process nois suppress speech enhanc within acoust encod network allow beamform compon optim joint within recognit architectur improv end end speech recognit object experi noisi speech benchmark chime ami show multichannel end end system outperform attent base baselin input convent adapt beamform|['Tsubasa Ochiai', 'Shinji Watanabe', 'Takaaki Hori', 'John R. Hershey']|['cs.SD', 'cs.CL']
2017-03-28T14:10:56Z|2017-03-14T22:17:49Z|http://arxiv.org/abs/1703.04770v1|http://arxiv.org/pdf/1703.04770v1|Audio Scene Classification with Deep Recurrent Neural Networks|audio scene classif deep recurr neural network|We introduce in this work an efficient approach for audio scene classification using deep recurrent neural networks. A scene audio signal is firstly transformed into a sequence of high-level label tree embedding feature vectors. The vector sequence is then divided into multiple subsequences on which a deep GRU-based recurrent neural network is trained for sequence-to-label classification. The global predicted label for the entire sequence is finally obtained via aggregation of subsequence classification outputs. We will show that our approach obtain an F1-score of 97.7% on the LITIS Rouen dataset, which is the largest dataset publicly available for the task. Compared to the best previously reported result on the dataset, our approach is able to reduce the relative classification error by 35.3%.|introduc work effici approach audio scene classif use deep recurr neural network scene audio signal first transform sequenc high level label tree embed featur vector vector sequenc divid multipl subsequ deep gru base recurr neural network train sequenc label classif global predict label entir sequenc final obtain via aggreg subsequ classif output show approach obtain score liti rouen dataset largest dataset public avail task compar best previous report result dataset approach abl reduc relat classif error|['Huy Phan', 'Philipp Koch', 'Fabrice Katzberg', 'Marco Maass', 'Radoslaw Mazur', 'Alfred Mertins']|['cs.SD', 'cs.LG']
2017-03-28T14:10:56Z|2017-03-07T10:36:50Z|http://arxiv.org/abs/1703.02318v1|http://arxiv.org/pdf/1703.02318v1|Linear and Circular Microphone Array for Remote Surveillance: Simulated   Performance Analysis|linear circular microphon array remot surveil simul perform analysi|Acoustic beamforming with a microphone array represents an adequate technology for remote acoustic surveillance, as the system has no mechanical parts and it has moderate size. However, in order to accomplish real implementation, several challenges need to be addressed, such as array geometry, microphone characteristics, and the digital beamforming algorithms. This paper presents a simulated analysis on the effect of the array geometry in the beamforming response. Two geometries are considered, namely, the linear and the circular geometry. The analysis is performed with computer simulations to mimic reality. The future steps comprise the construction of the physical microphone array, and the software implementation on a multichannel digital signal processing (DSP) system.|acoust beamform microphon array repres adequ technolog remot acoust surveil system mechan part moder size howev order accomplish real implement sever challeng need address array geometri microphon characterist digit beamform algorithm paper present simul analysi effect array geometri beamform respons two geometri consid name linear circular geometri analysi perform comput simul mimic realiti futur step compris construct physic microphon array softwar implement multichannel digit signal process dsp system|['Abdulla AlShehhi', 'M. Luai Hammadih', 'M. Sami Zitouni', 'Saif AlKindi', 'Nazar Ali', 'Luis Weruaga']|['cs.SD']
2017-03-28T14:11:00Z|2017-03-07T10:36:30Z|http://arxiv.org/abs/1703.02317v1|http://arxiv.org/pdf/1703.02317v1|Convolutional Recurrent Neural Networks for Bird Audio Detection|convolut recurr neural network bird audio detect|Bird sounds possess distinctive spectral structure which may exhibit small shifts in spectrum depending on the bird species and environmental conditions. In this paper, we propose using convolutional recurrent neural networks on the task of automated bird audio detection in real-life environments. In the proposed method, convolutional layers extract high dimensional, local frequency shift invariant features, while recurrent layers capture longer term dependencies between the features extracted from short time frames. This method achieves 88.5% Area Under ROC Curve (AUC) score on the unseen evaluation data and obtains the second place in the Bird Audio Detection challenge.|bird sound possess distinct spectral structur may exhibit small shift spectrum depend bird speci environment condit paper propos use convolut recurr neural network task autom bird audio detect real life environ propos method convolut layer extract high dimension local frequenc shift invari featur recurr layer captur longer term depend featur extract short time frame method achiev area roc curv auc score unseen evalu data obtain second place bird audio detect challeng|['Emre√áakƒ±r', 'Sharath Adavanne', 'Giambattista Parascandolo', 'Konstantinos Drossos', 'Tuomas Virtanen']|['cs.SD', 'cs.LG', 'stat.ML']
2017-03-28T14:11:00Z|2017-03-19T09:51:36Z|http://arxiv.org/abs/1703.02205v2|http://arxiv.org/pdf/1703.02205v2|Raw Waveform-based Speech Enhancement by Fully Convolutional Networks|raw waveform base speech enhanc fulli convolut network|This study proposes a fully convolutional network (FCN) model for raw waveform-based speech enhancement. The proposed system performs speech enhancement in an end-to-end (i.e., waveform-in and waveform-out) manner, which dif-fers from most existing denoising methods that process the magnitude spectrum (e.g., log power spectrum (LPS)) only. Because the fully connected layers, which are involved in deep neural networks (DNN) and convolutional neural networks (CNN), may not accurately characterize the local information of speech signals, particularly with high frequency components, we employed fully convolutional layers to model the waveform. More specifically, FCN consists of only convolutional layers and thus the local temporal structures of speech signals can be efficiently and effectively preserved with relatively few weights. Experimental results show that DNN- and CNN-based models have limited capability to restore high frequency components of waveforms, thus leading to decreased intelligibility of enhanced speech. By contrast, the proposed FCN model can not only effectively recover the waveforms but also outperform the LPS-based DNN baseline in terms of short-time objective intelligibility (STOI) and perceptual evaluation of speech quality (PESQ). In addition, the number of model parameters in FCN is approximately only 0.2% compared with that in both DNN and CNN.|studi propos fulli convolut network fcn model raw waveform base speech enhanc propos system perform speech enhanc end end waveform waveform manner dif fer exist denois method process magnitud spectrum log power spectrum lps onli becaus fulli connect layer involv deep neural network dnn convolut neural network cnn may accur character local inform speech signal particular high frequenc compon employ fulli convolut layer model waveform specif fcn consist onli convolut layer thus local tempor structur speech signal effici effect preserv relat weight experiment result show dnn cnn base model limit capabl restor high frequenc compon waveform thus lead decreas intellig enhanc speech contrast propos fcn model onli effect recov waveform also outperform lps base dnn baselin term short time object intellig stoi perceptu evalu speech qualiti pesq addit number model paramet fcn approxim onli compar dnn cnn|['Szu-Wei Fu', 'Yu Tsao', 'Xugang Lu', 'Hisashi Kawai']|['stat.ML', 'cs.LG', 'cs.SD']
2017-03-28T14:11:00Z|2017-03-06T09:57:25Z|http://arxiv.org/abs/1703.01793v1|http://arxiv.org/pdf/1703.01793v1|Multi-Level and Multi-Scale Feature Aggregation Using Pre-trained   Convolutional Neural Networks for Music Auto-tagging|multi level multi scale featur aggreg use pre train convolut neural network music auto tag|Music auto-tagging is often handled in a similar manner to image classification by regarding the 2D audio spectrogram as image data. However, music auto-tagging is distinguished from image classification in that the tags are highly diverse and have different levels of abstractions. Considering this issue, we propose a convolutional neural networks (CNN)-based architecture that embraces multi-level and multi-scaled features. The architecture is trained in three steps. First, we conduct supervised feature learning to capture local audio features using a set of CNNs with different input sizes. Second, we extract audio features from each layer of the pre-trained convolutional networks separately and aggregate them altogether given a long audio clip. Finally, we put them into fully-connected networks and make final predictions of the tags. Our experiments show that using the combination of multi-level and multi-scale features is highly effective in music auto-tagging and the proposed method outperforms previous state-of-the-arts on the Magnatagatune dataset and the million song dataset. We further show that the proposed architecture is useful in transfer learning.|music auto tag often handl similar manner imag classif regard audio spectrogram imag data howev music auto tag distinguish imag classif tag high divers differ level abstract consid issu propos convolut neural network cnn base architectur embrac multi level multi scale featur architectur train three step first conduct supervis featur learn captur local audio featur use set cnns differ input size second extract audio featur layer pre train convolut network separ aggreg altogeth given long audio clip final put fulli connect network make final predict tag experi show use combin multi level multi scale featur high effect music auto tag propos method outperform previous state art magnatagatun dataset million song dataset show propos architectur use transfer learn|['Jongpil Lee', 'Juhan Nam']|['cs.NE', 'cs.LG', 'cs.MM', 'cs.SD']
2017-03-28T14:11:00Z|2017-03-06T09:49:48Z|http://arxiv.org/abs/1703.01789v1|http://arxiv.org/pdf/1703.01789v1|Sample-level Deep Convolutional Neural Networks for Music Auto-tagging   Using Raw Waveforms|sampl level deep convolut neural network music auto tag use raw waveform|Recently, the end-to-end approach that learns hierarchical representations from raw data using deep convolutional neural networks has been successfully explored in the image, text and speech domains. This approach was applied to musical signals as well but has been not fully explored yet. To this end, we propose sample-level deep convolutional neural networks which learn representations from very small grains of waveforms (e.g. 2 or 3 samples) beyond typical frame-level input representations. This allows the networks to hierarchically learn filters that are sensitive to log-scaled frequency, such as mel-frequency spectrogram that is widely used in music classification systems. It also helps learning high-level abstraction of music by increasing the depth of layers. We show how deep architectures with sample-level filters improve the accuracy in music auto-tagging and they provide results that are com- parable to previous state-of-the-art performances for the Magnatagatune dataset and Million song dataset. In addition, we visualize filters learned in a sample-level DCNN in each layer to identify hierarchically learned features.|recent end end approach learn hierarch represent raw data use deep convolut neural network success explor imag text speech domain approach appli music signal well fulli explor yet end propos sampl level deep convolut neural network learn represent veri small grain waveform sampl beyond typic frame level input represent allow network hierarch learn filter sensit log scale frequenc mel frequenc spectrogram wide use music classif system also help learn high level abstract music increas depth layer show deep architectur sampl level filter improv accuraci music auto tag provid result com parabl previous state art perform magnatagatun dataset million song dataset addit visual filter learn sampl level dcnn layer identifi hierarch learn featur|['Jongpil Lee', 'Jiyoung Park', 'Keunhyoung Luke Kim', 'Juhan Nam']|['cs.SD', 'cs.LG', 'cs.MM', 'cs.NE']
2017-03-28T14:11:00Z|2017-03-06T04:30:12Z|http://arxiv.org/abs/1703.01720v1|http://arxiv.org/pdf/1703.01720v1|Sound-Word2Vec: Learning Word Representations Grounded in Sounds|sound wordvec learn word represent ground sound|Sound and vision are the primary modalities that influence how we perceive the world around us. Thus, it is crucial to incorporate information from these modalities into language to help machines interact better with humans. While existing works have explored incorporating visual cues into language embeddings, the task of learning word representations that respect auditory grounding remains under-explored. In this work, we propose a new embedding scheme, sound-word2vec that learns language embeddings by grounding them in sound -- for example, two seemingly unrelated concepts, leaves and paper are closer in our embedding space as they produce similar rustling sounds. We demonstrate that the proposed embeddings perform better than language-only word representations, on two purely textual tasks that require reasoning about aural cues -- sound retrieval and foley-sound discovery. Finally, we analyze nearest neighbors to highlight the unique dependencies captured by sound-w2v as compared to language-only embeddings.|sound vision primari modal influenc perceiv world around us thus crucial incorpor inform modal languag help machin interact better human exist work explor incorpor visual cue languag embed task learn word represent respect auditori ground remain explor work propos new embed scheme sound wordvec learn languag embed ground sound exampl two seem unrel concept leav paper closer embed space produc similar rustl sound demonstr propos embed perform better languag onli word represent two pure textual task requir reason aural cue sound retriev foley sound discoveri final analyz nearest neighbor highlight uniqu depend captur sound wv compar languag onli embed|['Ashwin K Vijayakumar', 'Ramakrishna Vedantam', 'Devi Parikh']|['cs.CL', 'cs.AI', 'cs.SD']
2017-03-28T14:11:00Z|2017-02-28T18:26:44Z|http://arxiv.org/abs/1703.00009v1|http://arxiv.org/pdf/1703.00009v1|Nonlinear Model and its Inverse of an Audio System|nonlinear model invers audio system|This computer science master thesis aims at modelling the nonlinearities of a loudspeaker. A piecewise linear approximation is initially explored and then we present a nonlinear Volterra model to simulate the behavior of the system. The general theory of continuous and discrete Volterra series is summarised. A Normalized Least Mean Square algorithm is used to determine the Volterra series to third order. We also present as inverted system which is trained with the same algorithm. Training data for the models were collected measuring a physical speaker using a laser interferometer. Results indicate a decrease in Mean Squared Error compared to the linear model with a dependency on the particular test signal, the order and the parameters of the model.|comput scienc master thesi aim model nonlinear loudspeak piecewis linear approxim initi explor present nonlinear volterra model simul behavior system general theori continu discret volterra seri summaris normal least mean squar algorithm use determin volterra seri third order also present invert system train algorithm train data model collect measur physic speaker use laser interferomet result indic decreas mean squar error compar linear model depend particular test signal order paramet model|['Alessandro Loriga']|['cs.SD']
2017-03-28T14:11:00Z|2017-02-28T11:00:19Z|http://arxiv.org/abs/1703.00384v1|http://arxiv.org/pdf/1703.00384v1|Nonlinear Volterra model of a loudspeaker behavior based on Laser   Doppler Vibrometry|nonlinear volterra model loudspeak behavior base laser doppler vibrometri|We demonstrate the capabilities of nonlinear Volterra models to simulate the behavior of an audio system and compare them to linear filters. In this paper a nonlinear model of an audio system based on Volterra series is presented and Normalized Least Mean Square algorithm is used to determine the Volterra series to third order. Training data for the models were collected measuring a physical speaker using a laser interferometer. We explore several training signals and filter's parameters. Results indicate a decrease in Mean Squared Error compared to the linear model with a dependency on the particular test signal, the order and the parameters of the model.|demonstr capabl nonlinear volterra model simul behavior audio system compar linear filter paper nonlinear model audio system base volterra seri present normal least mean squar algorithm use determin volterra seri third order train data model collect measur physic speaker use laser interferomet explor sever train signal filter paramet result indic decreas mean squar error compar linear model depend particular test signal order paramet model|['Alessandro Loriga', 'Parvin Moyassari', 'Daniele Bernardini', 'Gregorio Landi', 'Francesca Venturini', 'Elisabeth Dumont']|['cs.SD', 'H.5.5; I.2.6; I.6.3; J.5']
2017-03-28T14:11:00Z|2017-03-07T23:09:23Z|http://arxiv.org/abs/1702.07825v2|http://arxiv.org/pdf/1702.07825v2|Deep Voice: Real-time Neural Text-to-Speech|deep voic real time neural text speech|We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.|present deep voic product qualiti text speech system construct entir deep neural network deep voic lay groundwork truli end end neural speech synthesi system compris five major build block segment model locat phonem boundari graphem phonem convers model phonem durat predict model fundament frequenc predict model audio synthesi model segment model propos novel way perform phonem boundari detect deep neural network use connectionist tempor classif ctc loss audio synthesi model implement variant wavenet requir fewer paramet train faster origin use neural network compon system simpler flexibl tradit text speech system compon requir labori featur engin extens domain expertis final show infer system perform faster real time describ optim wavenet infer kernel cpu gpu achiev speedup exist implement|['Sercan O. Arik', 'Mike Chrzanowski', 'Adam Coates', 'Gregory Diamos', 'Andrew Gibiansky', 'Yongguo Kang', 'Xian Li', 'John Miller', 'Andrew Ng', 'Jonathan Raiman', 'Shubho Sengupta', 'Mohammad Shoeybi']|['cs.CL', 'cs.LG', 'cs.NE', 'cs.SD']
2017-03-28T14:11:00Z|2017-02-24T22:27:29Z|http://arxiv.org/abs/1702.07787v1|http://arxiv.org/pdf/1702.07787v1|Convolutional Gated Recurrent Neural Network Incorporating Spatial   Features for Audio Tagging|convolut gate recurr neural network incorpor spatial featur audio tag|Environmental audio tagging is a newly proposed task to predict the presence or absence of a specific audio event in a chunk. Deep neural network (DNN) based methods have been successfully adopted for predicting the audio tags in the domestic audio scene. In this paper, we propose to use a convolutional neural network (CNN) to extract robust features from mel-filter banks (MFBs), spectrograms or even raw waveforms for audio tagging. Gated recurrent unit (GRU) based recurrent neural networks (RNNs) are then cascaded to model the long-term temporal structure of the audio signal. To complement the input information, an auxiliary CNN is designed to learn on the spatial features of stereo recordings. We evaluate our proposed methods on Task 4 (audio tagging) of the Detection and Classification of Acoustic Scenes and Events 2016 (DCASE 2016) challenge. Compared with our recent DNN-based method, the proposed structure can reduce the equal error rate (EER) from 0.13 to 0.11 on the development set. The spatial features can further reduce the EER to 0.10. The performance of the end-to-end learning on raw waveforms is also comparable. Finally, on the evaluation set, we get the state-of-the-art performance with 0.12 EER while the performance of the best existing system is 0.15 EER.|environment audio tag newli propos task predict presenc absenc specif audio event chunk deep neural network dnn base method success adopt predict audio tag domest audio scene paper propos use convolut neural network cnn extract robust featur mel filter bank mfbs spectrogram even raw waveform audio tag gate recurr unit gru base recurr neural network rnns cascad model long term tempor structur audio signal complement input inform auxiliari cnn design learn spatial featur stereo record evalu propos method task audio tag detect classif acoust scene event dcase challeng compar recent dnn base method propos structur reduc equal error rate eer develop set spatial featur reduc eer perform end end learn raw waveform also compar final evalu set get state art perform eer perform best exist system eer|['Yong Xu', 'Qiuqiang Kong', 'Qiang Huang', 'Wenwu Wang', 'Mark D. Plumbley']|['cs.SD', 'cs.LG', 'cs.NE']
2017-03-28T14:11:00Z|2017-02-24T17:23:01Z|http://arxiv.org/abs/1702.07713v1|http://arxiv.org/pdf/1702.07713v1|Multichannel Linear Prediction for Blind Reverberant Audio Source   Separation|multichannel linear predict blind reverber audio sourc separ|A class of methods based on multichannel linear prediction (MCLP) can achieve effective blind dereverberation of a source, when the source is observed with a microphone array. We propose an inventive use of MCLP as a pre-processing step for blind source separation with a microphone array. We show theoretically that, under certain assumptions, such pre-processing reduces the original blind reverberant source separation problem to a non-reverberant one, which in turn can be effectively tackled using existing methods. We demonstrate our claims using real recordings obtained with an eight-microphone circular array in reverberant environments.|class method base multichannel linear predict mclp achiev effect blind dereverber sourc sourc observ microphon array propos invent use mclp pre process step blind sourc separ microphon array show theoret certain assumpt pre process reduc origin blind reverber sourc separ problem non reverber one turn effect tackl use exist method demonstr claim use real record obtain eight microphon circular array reverber environ|['ƒ∞lker Bayram', 'Sava≈ükan Bulek']|['cs.SD', 'cs.CE']
2017-03-28T14:11:05Z|2017-02-23T02:31:03Z|http://arxiv.org/abs/1702.07071v1|http://arxiv.org/pdf/1702.07071v1|Pronunciation recognition of English phonemes /\textipa{@}/, /√¶/,   /\textipa{A}:/ and /\textipa{2}/ using Formants and Mel Frequency Cepstral   Coefficients|pronunci recognit english phonem textipa textipa textipa use formant mel frequenc cepstral coeffici|The Vocal Joystick Vowel Corpus, by Washington University, was used to study monophthongs pronounced by native English speakers. The objective of this study was to quantitatively measure the extent at which speech recognition methods can distinguish between similar sounding vowels. In particular, the phonemes /\textipa{@}/, /{\ae}/, /\textipa{A}:/ and /\textipa{2}/ were analysed. 748 sound files from the corpus were used and subjected to Linear Predictive Coding (LPC) to compute their formants, and to Mel Frequency Cepstral Coefficients (MFCC) algorithm, to compute the cepstral coefficients. A Decision Tree Classifier was used to build a predictive model that learnt the patterns of the two first formants measured in the data set, as well as the patterns of the 13 cepstral coefficients. An accuracy of 70\% was achieved using formants for the mentioned phonemes. For the MFCC analysis an accuracy of 52 \% was achieved and an accuracy of 71\% when /\textipa{@}/ was ignored. The results obtained show that the studied algorithms are far from mimicking the ability of distinguishing subtle differences in sounds like human hearing does.|vocal joystick vowel corpus washington univers use studi monophthong pronounc nativ english speaker object studi quantit measur extent speech recognit method distinguish similar sound vowel particular phonem textipa ae textipa textipa analys sound file corpus use subject linear predict code lpc comput formant mel frequenc cepstral coeffici mfcc algorithm comput cepstral coeffici decis tree classifi use build predict model learnt pattern two first formant measur data set well pattern cepstral coeffici accuraci achiev use formant mention phonem mfcc analysi accuraci achiev accuraci textipa ignor result obtain show studi algorithm far mimick abil distinguish subtl differ sound like human hear doe|['Keith Y. Patarroyo', 'Vladimir Vargas-Calder√≥n']|['cs.CL', 'cs.SD']
2017-03-28T14:11:05Z|2017-03-21T01:42:20Z|http://arxiv.org/abs/1702.06724v3|http://arxiv.org/pdf/1702.06724v3|A new cosine series antialiasing function and its application to   aliasing-free glottal source models for speech and singing synthesis|new cosin seri antialias function applic alias free glottal sourc model speech sing synthesi|We formulated and implemented a procedure to generate aliasing-free excitation source signals. It uses a new antialiasing filter in the continuous time domain followed by an IIR digital filter for response equalization. We introduced a cosine-series-based general design procedure for the new antialiasing function. We applied this new procedure to implement the antialiased Fujisaki-Ljungqvist model. We also applied it to revise our previous implementation of the antialiased Fant-Liljencrants model. A combination of these signals and a lattice implementation of the time varying vocal tract model provides a reliable and flexible basis to test fo extractors and source aperiodicity analysis methods. MATLAB implementations of these antialiased excitation source models are available as part of our open source tools for speech science.|formul implement procedur generat alias free excit sourc signal use new antialias filter continu time domain follow iir digit filter respons equal introduc cosin seri base general design procedur new antialias function appli new procedur implement antialias fujisaki ljungqvist model also appli revis previous implement antialias fant liljencr model combin signal lattic implement time vari vocal tract model provid reliabl flexibl basi test fo extractor sourc aperiod analysi method matlab implement antialias excit sourc model avail part open sourc tool speech scienc|['Hideki Kawahara', 'Ken-Ichi Sakakibara', 'Hideki Banno', 'Masanori Morise', 'Tomoki Toda', 'Toshio Irino']|['cs.SD']
2017-03-28T14:11:05Z|2017-02-21T07:37:59Z|http://arxiv.org/abs/1702.06286v1|http://arxiv.org/pdf/1702.06286v1|Convolutional Recurrent Neural Networks for Polyphonic Sound Event   Detection|convolut recurr neural network polyphon sound event detect|Sound events often occur in unstructured environments where they exhibit wide variations in their frequency content and temporal structure. Convolutional neural networks (CNN) are able to extract higher level features that are invariant to local spectral and temporal variations. Recurrent neural networks (RNNs) are powerful in learning the longer term temporal context in the audio signals. CNNs and RNNs as classifiers have recently shown improved performances over established methods in various sound recognition tasks. We combine these two approaches in a Convolutional Recurrent Neural Network (CRNN) and apply it on a polyphonic sound event detection task. We compare the performance of the proposed CRNN method with CNN, RNN, and other established methods, and observe a considerable improvement for four different datasets consisting of everyday sound events.|sound event often occur unstructur environ exhibit wide variat frequenc content tempor structur convolut neural network cnn abl extract higher level featur invari local spectral tempor variat recurr neural network rnns power learn longer term tempor context audio signal cnns rnns classifi recent shown improv perform establish method various sound recognit task combin two approach convolut recurr neural network crnn appli polyphon sound event detect task compar perform propos crnn method cnn rnn establish method observ consider improv four differ dataset consist everyday sound event|['Emre √áakƒ±r', 'Giambattista Parascandolo', 'Toni Heittola', 'Heikki Huttunen', 'Tuomas Virtanen']|['cs.LG', 'cs.SD']
2017-03-28T14:11:05Z|2017-02-13T14:44:17Z|http://arxiv.org/abs/1702.03791v1|http://arxiv.org/pdf/1702.03791v1|DNN Filter Bank Cepstral Coefficients for Spoofing Detection|dnn filter bank cepstral coeffici spoof detect|With the development of speech synthesis techniques, automatic speaker verification systems face the serious challenge of spoofing attack. In order to improve the reliability of speaker verification systems, we develop a new filter bank based cepstral feature, deep neural network filter bank cepstral coefficients (DNN-FBCC), to distinguish between natural and spoofed speech. The deep neural network filter bank is automatically generated by training a filter bank neural network (FBNN) using natural and synthetic speech. By adding restrictions on the training rules, the learned weight matrix of FBNN is band-limited and sorted by frequency, similar to the normal filter bank. Unlike the manually designed filter bank, the learned filter bank has different filter shapes in different channels, which can capture the differences between natural and synthetic speech more effectively. The experimental results on the ASVspoof {2015} database show that the Gaussian mixture model maximum-likelihood (GMM-ML) classifier trained by the new feature performs better than the state-of-the-art linear frequency cepstral coefficients (LFCC) based classifier, especially on detecting unknown attacks.|develop speech synthesi techniqu automat speaker verif system face serious challeng spoof attack order improv reliabl speaker verif system develop new filter bank base cepstral featur deep neural network filter bank cepstral coeffici dnn fbcc distinguish natur spoof speech deep neural network filter bank automat generat train filter bank neural network fbnn use natur synthet speech ad restrict train rule learn weight matrix fbnn band limit sort frequenc similar normal filter bank unlik manual design filter bank learn filter bank differ filter shape differ channel captur differ natur synthet speech effect experiment result asvspoof databas show gaussian mixtur model maximum likelihood gmm ml classifi train new featur perform better state art linear frequenc cepstral coeffici lfcc base classifi especi detect unknown attack|['Hong Yu', 'Zheng-Hua Tan', 'Zhanyu Ma', 'Jun Guo']|['cs.SD', 'cs.CR', 'cs.LG']
2017-03-28T14:11:05Z|2017-02-08T04:59:00Z|http://arxiv.org/abs/1702.02289v1|http://arxiv.org/pdf/1702.02289v1|Neural Network Based Speaker Classification and Verification Systems   with Enhanced Features|neural network base speaker classif verif system enhanc featur|This work presents a novel framework based on feed-forward neural network for text-independent speaker classification and verification, two related systems of speaker recognition. With optimized features and model training, it achieves 100% classification rate in classification and less than 6% Equal Error Rate (ERR), using merely about 1 second and 5 seconds of data respectively. Features with stricter Voice Active Detection (VAD) than the regular one for speech recognition ensure extracting stronger voiced portion for speaker recognition, speaker-level mean and variance normalization helps to eliminate the discrepancy between samples from the same speaker. Both are proven to improve the system performance. In building the neural network speaker classifier, the network structure parameters are optimized with grid search and dynamically reduced regularization parameters are used to avoid training terminated in local minimum. It enables the training goes further with lower cost. In speaker verification, performance is improved with prediction score normalization, which rewards the speaker identity indices with distinct peaks and penalizes the weak ones with high scores but more competitors, and speaker-specific thresholding, which significantly reduces ERR in the ROC curve. TIMIT corpus with 8K sampling rate is used here. First 200 male speakers are used to train and test the classification performance. The testing files of them are used as in-domain registered speakers, while data from the remaining 126 male speakers are used as out-of-domain speakers, i.e. imposters in speaker verification.|work present novel framework base feed forward neural network text independ speaker classif verif two relat system speaker recognit optim featur model train achiev classif rate classif less equal error rate err use mere second second data respect featur stricter voic activ detect vad regular one speech recognit ensur extract stronger voic portion speaker recognit speaker level mean varianc normal help elimin discrep sampl speaker proven improv system perform build neural network speaker classifi network structur paramet optim grid search dynam reduc regular paramet use avoid train termin local minimum enabl train goe lower cost speaker verif perform improv predict score normal reward speaker ident indic distinct peak penal weak one high score competitor speaker specif threshold signific reduc err roc curv timit corpus sampl rate use first male speaker use train test classif perform test file use domain regist speaker data remain male speaker use domain speaker impost speaker verif|['Zhenhao Ge', 'Ananth N. Iyer', 'Srinath Cheluvaraja', 'Ram Sundaram', 'Aravind Ganapathiraju']|['cs.SD']
2017-03-28T14:11:05Z|2017-02-08T04:37:40Z|http://arxiv.org/abs/1702.02285v1|http://arxiv.org/pdf/1702.02285v1|Speaker Change Detection Using Features through A Neural Network Speaker   Classifier|speaker chang detect use featur neural network speaker classifi|The mechanism proposed here is for real-time speaker change detection in conversations, which firstly trains a neural network text-independent speaker classifier using in-domain speaker data. Through the network, features of conversational speech from out-of-domain speakers are then converted into likelihood vectors, i.e. similarity scores comparing to the in-domain speakers. These transformed features demonstrate very distinctive patterns, which facilitates differentiating speakers and enable speaker change detection with some straight-forward distance metrics. The speaker classifier and the speaker change detector are trained/tested using speech of the first 200 (in-domain) and the remaining 126 (out-of-domain) male speakers in TIMIT respectively. For the speaker classification, 100% accuracy at a 200 speaker size is achieved on any testing file, given the speech duration is at least 0.97 seconds. For the speaker change detection using speaker classification outputs, performance based on 0.5, 1, and 2 seconds of inspection intervals were evaluated in terms of error rate and F1 score, using synthesized data by concatenating speech from various speakers. It captures close to 97% of the changes by comparing the current second of speech with the previous second, which is very competitive among literature using other methods.|mechan propos real time speaker chang detect convers first train neural network text independ speaker classifi use domain speaker data network featur convers speech domain speaker convert likelihood vector similar score compar domain speaker transform featur demonstr veri distinct pattern facilit differenti speaker enabl speaker chang detect straight forward distanc metric speaker classifi speaker chang detector train test use speech first domain remain domain male speaker timit respect speaker classif accuraci speaker size achiev ani test file given speech durat least second speaker chang detect use speaker classif output perform base second inspect interv evalu term error rate score use synthes data concaten speech various speaker captur close chang compar current second speech previous second veri competit among literatur use method|['Zhenhao Ge', 'Ananth N. Iyer', 'Srinath Cheluvaraja', 'Aravind Ganapathiraju']|['cs.SD']
2017-03-28T14:11:05Z|2017-02-07T18:41:31Z|http://arxiv.org/abs/1702.02130v1|http://arxiv.org/pdf/1702.02130v1|On the Importance of Temporal Context in Proximity Kernels: A Vocal   Separation Case Study|import tempor context proxim kernel vocal separ case studi|Musical source separation methods exploit source-specific spectral characteristics to facilitate the decomposition process. Kernel Additive Modelling (KAM) models a source applying robust statistics to time-frequency bins as specified by a source-specific kernel, a function defining similarity between bins. Kernels in existing approaches are typically defined using metrics between single time frames. In the presence of noise and other sound sources information from a single-frame, however, turns out to be unreliable and often incorrect frames are selected as similar. In this paper, we incorporate a temporal context into the kernel to provide additional information stabilizing the similarity search. Evaluated in the context of vocal separation, our simple extension led to a considerable improvement in separation quality compared to previous kernels.|music sourc separ method exploit sourc specif spectral characterist facilit decomposit process kernel addit model kam model sourc appli robust statist time frequenc bin specifi sourc specif kernel function defin similar bin kernel exist approach typic defin use metric singl time frame presenc nois sound sourc inform singl frame howev turn unreli often incorrect frame select similar paper incorpor tempor context kernel provid addit inform stabil similar search evalu context vocal separ simpl extens led consider improv separ qualiti compar previous kernel|['Delia Fano Yela', 'Sebastian Ewert', 'Derry FitzGerald', 'Mark Sandler']|['cs.SD', 'H.5.5']
2017-03-28T14:11:05Z|2017-02-07T13:19:08Z|http://arxiv.org/abs/1702.01999v1|http://arxiv.org/pdf/1702.01999v1|Identification of Voice Utterance with Aging Factor Using the Method of   MFCC Multichannel|identif voic utter age factor use method mfcc multichannel|This research was conducted to develop a method to identify voice utterance. For voice utterance that encounters change caused by aging factor, with the interval of 10 to 25 years. The change of voice utterance influenced by aging factor might be extracted by MFCC (Mel Frequency Cepstrum Coefficient). However, the level of the compatibility of the feature may be dropped down to 55%. While the ones which do not encounter it may reach 95%. To improve the compatibility of the changing voice feature influenced by aging factor, then the method of the more specific feature extraction is developed: which is by separating the voice into several channels, suggested as MFCC multichannel, consisting of multichannel 5 filterbank (M5FB), multichannel 2 filterbank (M2FB) and multichannel 1 filterbank (M1FB). The result of the test shows that for model M5FB and M2FB have the highest score in the level of compatibility with 85% and 82% with 25 years interval. While model M5FB gets the highest score of 86% for 10 years time interval.|research conduct develop method identifi voic utter voic utter encount chang caus age factor interv year chang voic utter influenc age factor might extract mfcc mel frequenc cepstrum coeffici howev level compat featur may drop one encount may reach improv compat chang voic featur influenc age factor method specif featur extract develop separ voic sever channel suggest mfcc multichannel consist multichannel filterbank mfb multichannel filterbank mfb multichannel filterbank mfb result test show model mfb mfb highest score level compat year interv model mfb get highest score year time interv|['Roy Rudolf Huizen', 'Jazi Eko Istiyanto', 'Agfianto Eko Putra']|['cs.SD']
2017-03-28T14:11:05Z|2017-02-06T03:37:28Z|http://arxiv.org/abs/1702.00956v2|http://arxiv.org/pdf/1702.00956v2|KU-ISPL Speaker Recognition Systems under Language mismatch condition   for NIST 2016 Speaker Recognition Evaluation|ku ispl speaker recognit system languag mismatch condit nist speaker recognit evalu|Korea University Intelligent Signal Processing Lab. (KU-ISPL) developed speaker recognition system for SRE16 fixed training condition. Data for evaluation trials are collected from outside North America, spoken in Tagalog and Cantonese while training data only is spoken English. Thus, main issue for SRE16 is compensating the discrepancy between different languages. As development dataset which is spoken in Cebuano and Mandarin, we could prepare the evaluation trials through preliminary experiments to compensate the language mismatched condition. Our team developed 4 different approaches to extract i-vectors and applied state-of-the-art techniques as backend. To compensate language mismatch, we investigated and endeavored unique method such as unsupervised language clustering, inter language variability compensation and gender/language dependent score normalization.|korea univers intellig signal process lab ku ispl develop speaker recognit system sre fix train condit data evalu trial collect outsid north america spoken tagalog cantones train data onli spoken english thus main issu sre compens discrep differ languag develop dataset spoken cebuano mandarin could prepar evalu trial preliminari experi compens languag mismatch condit team develop differ approach extract vector appli state art techniqu backend compens languag mismatch investig endeavor uniqu method unsupervis languag cluster inter languag variabl compens gender languag depend score normal|['Suwon Shon', 'Hanseok Ko']|['cs.SD', 'cs.CL']
2017-03-28T14:11:05Z|2017-02-01T09:44:44Z|http://arxiv.org/abs/1702.00178v1|http://arxiv.org/pdf/1702.00178v1|On the Futility of Learning Complex Frame-Level Language Models for   Chord Recognition|futil learn complex frame level languag model chord recognit|Chord recognition systems use temporal models to post-process frame-wise chord preditions from acoustic models. Traditionally, first-order models such as Hidden Markov Models were used for this task, with recent works suggesting to apply Recurrent Neural Networks instead. Due to their ability to learn longer-term dependencies, these models are supposed to learn and to apply musical knowledge, instead of just smoothing the output of the acoustic model. In this paper, we argue that learning complex temporal models at the level of audio frames is futile on principle, and that non-Markovian models do not perform better than their first-order counterparts. We support our argument through three experiments on the McGill Billboard dataset. The first two show 1) that when learning complex temporal models at the frame level, improvements in chord sequence modelling are marginal; and 2) that these improvements do not translate when applied within a full chord recognition system. The third, still rather preliminary experiment gives first indications that the use of complex sequential models for chord prediction at higher temporal levels might be more promising.|chord recognit system use tempor model post process frame wise chord predit acoust model tradit first order model hidden markov model use task recent work suggest appli recurr neural network instead due abil learn longer term depend model suppos learn appli music knowledg instead smooth output acoust model paper argu learn complex tempor model level audio frame futil principl non markovian model perform better first order counterpart support argument three experi mcgill billboard dataset first two show learn complex tempor model frame level improv chord sequenc model margin improv translat appli within full chord recognit system third still rather preliminari experi give first indic use complex sequenti model chord predict higher tempor level might promis|['Filip Korzeniowski', 'Gerhard Widmer']|['cs.SD', 'cs.LG']
2017-03-28T14:11:09Z|2017-01-31T19:21:41Z|http://arxiv.org/abs/1702.00025v1|http://arxiv.org/pdf/1702.00025v1|An Experimental Analysis of the Entanglement Problem in   Neural-Network-based Music Transcription Systems|experiment analysi entangl problem neural network base music transcript system|Several recent polyphonic music transcription systems have utilized deep neural networks to achieve state of the art results on various benchmark datasets, pushing the envelope on framewise and note-level performance measures. Unfortunately we can observe a sort of glass ceiling effect. To investigate this effect, we provide a detailed analysis of the particular kinds of errors that state of the art deep neural transcription systems make, when trained and tested on a piano transcription task. We are ultimately forced to draw a rather disheartening conclusion: the networks seem to learn combinations of notes, and have a hard time generalizing to unseen combinations of notes. Furthermore, we speculate on various means to alleviate this situation.|sever recent polyphon music transcript system util deep neural network achiev state art result various benchmark dataset push envelop framewis note level perform measur unfortun observ sort glass ceil effect investig effect provid detail analysi particular kind error state art deep neural transcript system make train test piano transcript task ultim forc draw rather dishearten conclus network seem learn combin note hard time general unseen combin note furthermor specul various mean allevi situat|['Rainer Kelz', 'Gerhard Widmer']|['cs.SD']
2017-03-28T14:11:09Z|2017-01-29T01:25:57Z|http://arxiv.org/abs/1701.08343v1|http://arxiv.org/pdf/1701.08343v1|Rhythm Transcription of Polyphonic Piano Music Based on Merged-Output   HMM for Multiple Voices|rhythm transcript polyphon piano music base merg output hmm multipl voic|In a recent conference paper, we have reported a rhythm transcription method based on a merged-output hidden Markov model (HMM) that explicitly describes the multiple-voice structure of polyphonic music. This model solves a major problem of conventional methods that could not properly describe the nature of multiple voices as in polyrhythmic scores or in the phenomenon of loose synchrony between voices. In this paper we present a complete description of the proposed model and develop an inference technique, which is valid for any merged-output HMMs for which output probabilities depend on past events. We also examine the influence of the architecture and parameters of the method in terms of accuracies of rhythm transcription and voice separation and perform comparative evaluations with six other algorithms. Using MIDI recordings of classical piano pieces, we found that the proposed model outperformed other methods by more than 12 points in the accuracy for polyrhythmic performances and performed almost as good as the best one for non-polyrhythmic performances. This reveals the state-of-the-art methods of rhythm transcription for the first time in the literature. Publicly available source codes are also provided for future comparisons.|recent confer paper report rhythm transcript method base merg output hidden markov model hmm explicit describ multipl voic structur polyphon music model solv major problem convent method could proper describ natur multipl voic polyrhythm score phenomenon loos synchroni voic paper present complet descript propos model develop infer techniqu valid ani merg output hmms output probabl depend past event also examin influenc architectur paramet method term accuraci rhythm transcript voic separ perform compar evalu six algorithm use midi record classic piano piec found propos model outperform method point accuraci polyrhythm perform perform almost good best one non polyrhythm perform reveal state art method rhythm transcript first time literatur public avail sourc code also provid futur comparison|['Eita Nakamura', 'Kazuyoshi Yoshii', 'Shigeki Sagayama']|['cs.AI', 'cs.SD']
2017-03-28T14:11:09Z|2017-01-27T12:38:47Z|http://arxiv.org/abs/1701.08156v1|http://arxiv.org/pdf/1701.08156v1|A Comprehensive Survey on Bengali Phoneme Recognition|comprehens survey bengali phonem recognit|Hidden Markov model based various phoneme recognition methods for Bengali language is reviewed. Automatic phoneme recognition for Bengali language using multilayer neural network is reviewed. Usefulness of multilayer neural network over single layer neural network is discussed. Bangla phonetic feature table construction and enhancement for Bengali speech recognition is also discussed. Comparison among these methods is discussed.|hidden markov model base various phonem recognit method bengali languag review automat phonem recognit bengali languag use multilay neural network review use multilay neural network singl layer neural network discuss bangla phonet featur tabl construct enhanc bengali speech recognit also discuss comparison among method discuss|['Sadia Tasnim Swarna', 'Shamim Ehsan', 'Md. Saiful Islam', 'Marium E Jannat']|['cs.SD', 'cs.CL']
2017-03-28T14:11:09Z|2017-01-27T19:21:10Z|http://arxiv.org/abs/1701.07138v3|http://arxiv.org/pdf/1701.07138v3|Learning Mid-Level Auditory Codes from Natural Sound Statistics|learn mid level auditori code natur sound statist|Interaction with the world requires an organism to transform sensory signals into representations in which behaviorally meaningful properties of the environment are made explicit. These representations are derived through cascades of neuronal processing stages in which neurons at each stage recode the output of preceding stages. Explanations of sensory coding may thus involve understanding how low-level patterns are combined into more complex structures. Although models exist in the visual domain to explain how mid-level features such as junctions and curves might be derived from oriented filters in early visual cortex, little is known about analogous grouping principles for mid-level auditory representations. We propose a hierarchical generative model of natural sounds that learns combinations of spectrotemporal features from natural stimulus statistics. In the first layer the model forms a sparse convolutional code of spectrograms using a dictionary of learned spectrotemporal kernels. To generalize from specific kernel activation patterns, the second layer encodes patterns of time-varying magnitude of multiple first layer coefficients. Because second-layer features are sensitive to combinations of spectrotemporal features, the representation they support encodes more complex acoustic patterns than the first layer. When trained on corpora of speech and environmental sounds, some second-layer units learned to group spectrotemporal features that occur together in natural sounds. Others instantiate opponency between dissimilar sets of spectrotemporal features. Such groupings might be instantiated by neurons in the auditory cortex, providing a hypothesis for mid-level neuronal computation.|interact world requir organ transform sensori signal represent behavior meaning properti environ made explicit represent deriv cascad neuron process stage neuron stage recod output preced stage explan sensori code may thus involv understand low level pattern combin complex structur although model exist visual domain explain mid level featur junction curv might deriv orient filter earli visual cortex littl known analog group principl mid level auditori represent propos hierarch generat model natur sound learn combin spectrotempor featur natur stimulus statist first layer model form spars convolut code spectrogram use dictionari learn spectrotempor kernel general specif kernel activ pattern second layer encod pattern time vari magnitud multipl first layer coeffici becaus second layer featur sensit combin spectrotempor featur represent support encod complex acoust pattern first layer train corpora speech environment sound second layer unit learn group spectrotempor featur occur togeth natur sound instanti oppon dissimilar set spectrotempor featur group might instanti neuron auditori cortex provid hypothesi mid level neuron comput|['Wiktor M≈Çynarski', 'Josh H. McDermott']|['q-bio.NC', 'cs.SD']
2017-03-28T14:11:09Z|2017-01-23T11:18:06Z|http://arxiv.org/abs/1702.02092v1|http://arxiv.org/pdf/1702.02092v1|Characterisation of speech diversity using self-organising maps|characteris speech divers use self organis map|We report investigations into speaker classification of larger quantities of unlabelled speech data using small sets of manually phonemically annotated speech. The Kohonen speech typewriter is a semi-supervised method comprised of self-organising maps (SOMs) that achieves low phoneme error rates. A SOM is a 2D array of cells that learn vector representations of the data based on neighbourhoods. In this paper, we report a method to evaluate pronunciation using multilevel SOMs with /hVd/ single syllable utterances for the study of vowels, for Australian pronunciation.|report investig speaker classif larger quantiti unlabel speech data use small set manual phonem annot speech kohonen speech typewrit semi supervis method compris self organis map som achiev low phonem error rate som array cell learn vector represent data base neighbourhood paper report method evalu pronunci use multilevel som hvd singl syllabl utter studi vowel australian pronunci|['Tom A. F. Anderson', 'David M. W. Powers']|['cs.CL', 'cs.NE', 'cs.SD']
2017-03-28T14:11:09Z|2017-01-24T16:25:15Z|http://arxiv.org/abs/1701.06078v2|http://arxiv.org/pdf/1701.06078v2|Lyrics-to-Audio Alignment by Unsupervised Discovery of Repetitive   Patterns in Vowel Acoustics|lyric audio align unsupervis discoveri repetit pattern vowel acoust|Most of the previous approaches to lyrics-to-audio alignment used a pre-developed automatic speech recognition (ASR) system that innately suffered from several difficulties to adapt the speech model to individual singers. A significant aspect missing in previous works is the self-learnability of repetitive vowel patterns in the singing voice, where the vowel part used is more consistent than the consonant part. Based on this, our system first learns a discriminative subspace of vowel sequences, based on weighted symmetric non-negative matrix factorization (WS-NMF), by taking the self-similarity of a standard acoustic feature as an input. Then, we make use of canonical time warping (CTW), derived from a recent computer vision technique, to find an optimal spatiotemporal transformation between the text and the acoustic sequences. Experiments with Korean and English data sets showed that deploying this method after a pre-developed, unsupervised, singing source separation achieved more promising results than other state-of-the-art unsupervised approaches and an existing ASR-based system.|previous approach lyric audio align use pre develop automat speech recognit asr system innat suffer sever difficulti adapt speech model individu singer signific aspect miss previous work self learnabl repetit vowel pattern sing voic vowel part use consist conson part base system first learn discrimin subspac vowel sequenc base weight symmetr non negat matrix factor ws nmf take self similar standard acoust featur input make use canon time warp ctw deriv recent comput vision techniqu find optim spatiotempor transform text acoust sequenc experi korean english data set show deploy method pre develop unsupervis sing sourc separ achiev promis result state art unsupervis approach exist asr base system|['Sungkyun Chang', 'Kyogu Lee']|['cs.SD', 'cs.LG']
2017-03-28T14:11:09Z|2017-01-20T12:48:02Z|http://arxiv.org/abs/1701.05779v1|http://arxiv.org/pdf/1701.05779v1|Empirical Study of Drone Sound Detection in Real-Life Environment with   Deep Neural Networks|empir studi drone sound detect real life environ deep neural network|This work aims to investigate the use of deep neural network to detect commercial hobby drones in real-life environments by analyzing their sound data. The purpose of work is to contribute to a system for detecting drones used for malicious purposes, such as for terrorism. Specifically, we present a method capable of detecting the presence of commercial hobby drones as a binary classification problem based on sound event detection. We recorded the sound produced by a few popular commercial hobby drones, and then augmented this data with diverse environmental sound data to remedy the scarcity of drone sound data in diverse environments. We investigated the effectiveness of state-of-the-art event sound classification methods, i.e., a Gaussian Mixture Model (GMM), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), for drone sound detection. Our empirical results, which were obtained with a testing dataset collected on an urban street, confirmed the effectiveness of these models for operating in a real environment. In summary, our RNN models showed the best detection performance with an F-Score of 0.8009 with 240 ms of input audio with a short processing time, indicating their applicability to real-time detection systems.|work aim investig use deep neural network detect commerci hobbi drone real life environ analyz sound data purpos work contribut system detect drone use malici purpos terror specif present method capabl detect presenc commerci hobbi drone binari classif problem base sound event detect record sound produc popular commerci hobbi drone augment data divers environment sound data remedi scarciti drone sound data divers environ investig effect state art event sound classif method gaussian mixtur model gmm convolut neural network cnn recurr neural network rnn drone sound detect empir result obtain test dataset collect urban street confirm effect model oper real environ summari rnn model show best detect perform score ms input audio short process time indic applic real time detect system|['Sungho Jeon', 'Jong-Woo Shin', 'Young-Jun Lee', 'Woong-Hee Kim', 'YoungHyoun Kwon', 'Hae-Yong Yang']|['cs.SD', 'cs.LG']
2017-03-28T14:11:09Z|2017-01-12T09:26:22Z|http://arxiv.org/abs/1701.03274v1|http://arxiv.org/pdf/1701.03274v1|Investigating the role of musical genre in human perception of music   stretching resistance|investig role music genr human percept music stretch resist|To stretch a music piece to a given length is a common demand in people's daily lives, e.g., in audio-video synchronization and animation production. However, it is not always guaranteed that the stretched music piece is acceptable for general audience since music stretching suffers from people's perceptual artefacts. Over-stretching a music piece will make it uncomfortable for human psychoacoustic hearing. The research on music stretching resistance attempts to estimate the maximum stretchability of music pieces to further avoid over-stretch. It has been observed that musical genres can significantly improve the accuracy of automatic estimation of music stretching resistance, but how musical genres are related to music stretching resistance has never been explained or studied in detail in the literature. In this paper, the characteristics of music stretching resistance are compared across different musical genres. It is found that music stretching resistance has strong intra-genre cohesiveness and inter-genre discrepancies in the experiments. Moreover, the ambiguity and the symmetry of music stretching resistance are also observed in the experimental analysis. These findings lead to a new measurement on the similarity between different musical genres based on their music stretching resistance. In addition, the analysis of variance (ANOVA) also supports the findings in this paper by verifying the significance of musical genre in shaping music stretching resistance.|stretch music piec given length common demand peopl daili live audio video synchron anim product howev alway guarante stretch music piec accept general audienc sinc music stretch suffer peopl perceptu artefact stretch music piec make uncomfort human psychoacoust hear research music stretch resist attempt estim maximum stretchabl music piec avoid stretch observ music genr signific improv accuraci automat estim music stretch resist music genr relat music stretch resist never explain studi detail literatur paper characterist music stretch resist compar across differ music genr found music stretch resist strong intra genr cohes inter genr discrep experi moreov ambigu symmetri music stretch resist also observ experiment analysi find lead new measur similar differ music genr base music stretch resist addit analysi varianc anova also support find paper verifi signific music genr shape music stretch resist|['Jun Chen', 'Chaokun Wang']|['cs.MM', 'cs.SD']
2017-03-28T14:11:09Z|2017-01-12T01:02:22Z|http://arxiv.org/abs/1701.03198v1|http://arxiv.org/pdf/1701.03198v1|Unsupervised Latent Behavior Manifold Learning from Acoustic Features:   audio2behavior|unsupervis latent behavior manifold learn acoust featur audiobehavior|Behavioral annotation using signal processing and machine learning is highly dependent on training data and manual annotations of behavioral labels. Previous studies have shown that speech information encodes significant behavioral information and be used in a variety of automated behavior recognition tasks. However, extracting behavior information from speech is still a difficult task due to the sparseness of training data coupled with the complex, high-dimensionality of speech, and the complex and multiple information streams it encodes. In this work we exploit the slow varying properties of human behavior. We hypothesize that nearby segments of speech share the same behavioral context and hence share a similar underlying representation in a latent space. Specifically, we propose a Deep Neural Network (DNN) model to connect behavioral context and derive the behavioral manifold in an unsupervised manner. We evaluate the proposed manifold in the couples therapy domain and also provide examples from publicly available data (e.g. stand-up comedy). We further investigate training within the couples' therapy domain and from movie data. The results are extremely encouraging and promise improved behavioral quantification in an unsupervised manner and warrants further investigation in a range of applications.|behavior annot use signal process machin learn high depend train data manual annot behavior label previous studi shown speech inform encod signific behavior inform use varieti autom behavior recognit task howev extract behavior inform speech still difficult task due spars train data coupl complex high dimension speech complex multipl inform stream encod work exploit slow vari properti human behavior hypothes nearbi segment speech share behavior context henc share similar represent latent space specif propos deep neural network dnn model connect behavior context deriv behavior manifold unsupervis manner evalu propos manifold coupl therapi domain also provid exampl public avail data stand comedi investig train within coupl therapi domain movi data result extrem encourag promis improv behavior quantif unsupervis manner warrant investig rang applic|['Haoqi Li', 'Brian Baucom', 'Panayiotis Georgiou']|['cs.LG', 'cs.SD']
2017-03-28T14:11:09Z|2017-03-15T00:23:45Z|http://arxiv.org/abs/1701.03360v2|http://arxiv.org/pdf/1701.03360v2|Residual LSTM: Design of a Deep Recurrent Architecture for Distant   Speech Recognition|residu lstm design deep recurr architectur distant speech recognit|In this paper, a novel architecture for a deep recurrent neural network, residual LSTM is introduced. A plain LSTM has an internal memory cell that can learn long term dependencies of sequential data. It also provides a temporal shortcut path to avoid vanishing or exploding gradients in the temporal domain. The residual LSTM provides an additional spatial shortcut path from lower layers for efficient training of deep networks with multiple LSTM layers. Compared with the previous work, highway LSTM, residual LSTM separates a spatial shortcut path with temporal one by using output layers, which can help to avoid a conflict between spatial and temporal-domain gradient flows. Furthermore, residual LSTM reuses the output projection matrix and the output gate of LSTM to control the spatial information flow instead of additional gate networks, which effectively reduces more than 10% of network parameters. An experiment for distant speech recognition on the AMI SDM corpus shows that 10-layer plain and highway LSTM networks presented 13.7% and 6.2% increase in WER over 3-layer aselines, respectively. On the contrary, 10-layer residual LSTM networks provided the lowest WER 41.0%, which corresponds to 3.3% and 2.8% WER reduction over plain and highway LSTM networks, respectively.|paper novel architectur deep recurr neural network residu lstm introduc plain lstm intern memori cell learn long term depend sequenti data also provid tempor shortcut path avoid vanish explod gradient tempor domain residu lstm provid addit spatial shortcut path lower layer effici train deep network multipl lstm layer compar previous work highway lstm residu lstm separ spatial shortcut path tempor one use output layer help avoid conflict spatial tempor domain gradient flow furthermor residu lstm reus output project matrix output gate lstm control spatial inform flow instead addit gate network effect reduc network paramet experi distant speech recognit ami sdm corpus show layer plain highway lstm network present increas wer layer aselin respect contrari layer residu lstm network provid lowest wer correspond wer reduct plain highway lstm network respect|['Jaeyoung Kim', 'Mostafa El-Khamy', 'Jungwon Lee']|['cs.LG', 'cs.AI', 'cs.SD']
2017-03-28T14:11:13Z|2017-01-09T15:10:38Z|http://arxiv.org/abs/1701.03834v1|http://arxiv.org/pdf/1701.03834v1|On Higher Order Positive Differential Energy Operator|higher order posit differenti energi oper|The higher order differential energy operator (DEO), denoted via $\Upsilon_k(x)$, is an extension to the second order famous Teager-Kaiser operator. The DEO helps measuring the higher order gauge of energy of a signal which is useful for AM-FM demodulation. However, the energy criterion defined by the DEO is not compliant with the presumption of positivity of energy. In this paper we introduce a higher order operator called Positive Differential Energy Operator (PDEO). This operator which can be obtained using alternative recursive relations, resolves the energy sign problem. The simulations demonstrate that the proposed operator can outperform DEOs in terms of Average Signal to Error Ratio (ASER) in AM/FM demodulation.|higher order differenti energi oper deo denot via upsilon extens second order famous teager kaiser oper deo help measur higher order gaug energi signal use fm demodul howev energi criterion defin deo compliant presumpt posit energi paper introduc higher order oper call posit differenti energi oper pdeo oper obtain use altern recurs relat resolv energi sign problem simul demonstr propos oper outperform deo term averag signal error ratio aser fm demodul|['Amirhossein Javaheri', 'Mohammad Bagher Shamsollahi']|['cs.SD']
2017-03-28T14:11:13Z|2017-01-04T04:07:11Z|http://arxiv.org/abs/1701.00599v2|http://arxiv.org/pdf/1701.00599v2|AENet: Learning Deep Audio Features for Video Analysis|aenet learn deep audio featur video analysi|We propose a new deep network for audio event recognition, called AENet. In contrast to speech, sounds coming from audio events may be produced by a wide variety of sources. Furthermore, distinguishing them often requires analyzing an extended time period due to the lack of clear sub-word units that are present in speech. In order to incorporate this long-time frequency structure of audio events, we introduce a convolutional neural network (CNN) operating on a large temporal input. In contrast to previous works this allows us to train an audio event detection system end-to-end. The combination of our network architecture and a novel data augmentation outperforms previous methods for audio event detection by 16%. Furthermore, we perform transfer learning and show that our model learnt generic audio features, similar to the way CNNs learn generic features on vision tasks. In video analysis, combining visual features and traditional audio features such as MFCC typically only leads to marginal improvements. Instead, combining visual features with our AENet features, which can be computed efficiently on a GPU, leads to significant performance improvements on action recognition and video highlight detection. In video highlight detection, our audio features improve the performance by more than 8% over visual features alone.|propos new deep network audio event recognit call aenet contrast speech sound come audio event may produc wide varieti sourc furthermor distinguish often requir analyz extend time period due lack clear sub word unit present speech order incorpor long time frequenc structur audio event introduc convolut neural network cnn oper larg tempor input contrast previous work allow us train audio event detect system end end combin network architectur novel data augment outperform previous method audio event detect furthermor perform transfer learn show model learnt generic audio featur similar way cnns learn generic featur vision task video analysi combin visual featur tradit audio featur mfcc typic onli lead margin improv instead combin visual featur aenet featur comput effici gpu lead signific perform improv action recognit video highlight detect video highlight detect audio featur improv perform visual featur alon|['Naoya Takahashi', 'Michael Gygli', 'Luc Van Gool']|['cs.MM', 'cs.CV', 'cs.SD']
2017-03-28T14:11:13Z|2017-01-09T17:35:17Z|http://arxiv.org/abs/1701.00495v2|http://arxiv.org/pdf/1701.00495v2|Vid2speech: Speech Reconstruction from Silent Video|vidspeech speech reconstruct silent video|Speechreading is a notoriously difficult task for humans to perform. In this paper we present an end-to-end model based on a convolutional neural network (CNN) for generating an intelligible acoustic speech signal from silent video frames of a speaking person. The proposed CNN generates sound features for each frame based on its neighboring frames. Waveforms are then synthesized from the learned speech features to produce intelligible speech. We show that by leveraging the automatic feature learning capabilities of a CNN, we can obtain state-of-the-art word intelligibility on the GRID dataset, and show promising results for learning out-of-vocabulary (OOV) words.|speechread notori difficult task human perform paper present end end model base convolut neural network cnn generat intellig acoust speech signal silent video frame speak person propos cnn generat sound featur frame base neighbor frame waveform synthes learn speech featur produc intellig speech show leverag automat featur learn capabl cnn obtain state art word intellig grid dataset show promis result learn vocabulari oov word|['Ariel Ephrat', 'Shmuel Peleg']|['cs.CV', 'cs.SD']
2017-03-28T14:11:13Z|2016-12-30T08:46:05Z|http://arxiv.org/abs/1612.09150v2|http://arxiv.org/pdf/1612.09150v2|Phase-incorporating Speech Enhancement Based on Complex-valued Gaussian   Process Latent Variable Model|phase incorpor speech enhanc base complex valu gaussian process latent variabl model|Traditional speech enhancement techniques modify the magnitude of a speech in time-frequency domain, and use the phase of a noisy speech to resynthesize a time domain speech. This work proposes a complex-valued Gaussian process latent variable model (CGPLVM) to enhance directly the complex-valued noisy spectrum, modifying not only the magnitude but also the phase. The main idea that underlies the developed method is the modeling of short-time Fourier transform (STFT) coefficients across the time frames of a speech as a proper complex Gaussian process (GP) with noise added. The proposed method is based on projecting the spectrum into a low-dimensional subspace. The likelihood criterion is used to optimize the hyperparameters of the model. Experiments were carried out on the CHTTL database, which contains the digits zero to nine in Mandarin. Several standard measures are used to demonstrate that the proposed method outperforms baseline methods.|tradit speech enhanc techniqu modifi magnitud speech time frequenc domain use phase noisi speech resynthes time domain speech work propos complex valu gaussian process latent variabl model cgplvm enhanc direct complex valu noisi spectrum modifi onli magnitud also phase main idea develop method model short time fourier transform stft coeffici across time frame speech proper complex gaussian process gp nois ad propos method base project spectrum low dimension subspac likelihood criterion use optim hyperparamet model experi carri chttl databas contain digit zero nine mandarin sever standard measur use demonstr propos method outperform baselin method|['Sih-Huei Chen', 'Yuan-Shan Lee', 'Jia-Ching Wang']|['cs.SD']
2017-03-28T14:11:13Z|2016-12-30T09:26:26Z|http://arxiv.org/abs/1612.09089v2|http://arxiv.org/pdf/1612.09089v2|What Makes Audio Event Detection Harder than Classification?|make audio event detect harder classif|There is a common observation that audio event classification is easier to deal with than detection. So far, this observation has been accepted as a fact and we lack a careful analysis. In this paper, we reason the rationale behind this fact and, more importantly, leverage them to benefit the audio event detection task. We present an improved detection pipeline in which a verification step is appended to augment a detection system. This step employs a high-quality event classifier to postprocess the benign event hypotheses outputted by the detection system and reject false alarms. To demonstrate the effectiveness of the proposed pipeline, we implement and pair up different event detectors based on the most common detection schemes and various event classifiers, ranging from the standard bag-of-words model to the state-of-the-art bank-of-regressors one. Experimental results on the ITC-Irst dataset show significant improvements to detection performance. More importantly, these improvements are consistent for all detector-classifier combinations.|common observ audio event classif easier deal detect far observ accept fact lack care analysi paper reason rational behind fact import leverag benefit audio event detect task present improv detect pipelin verif step append augment detect system step employ high qualiti event classifi postprocess benign event hypothes output detect system reject fals alarm demonstr effect propos pipelin implement pair differ event detector base common detect scheme various event classifi rang standard bag word model state art bank regressor one experiment result itc irst dataset show signific improv detect perform import improv consist detector classifi combin|['Huy Phan', 'Philipp Koch', 'Marco Maass', 'Radoslaw Mazur', 'Ian McLoughlin', 'Alfred Mertins']|['cs.SD']
2017-03-28T14:11:13Z|2016-12-27T20:27:24Z|http://arxiv.org/abs/1612.08727v1|http://arxiv.org/pdf/1612.08727v1|Creating A Musical Performance Dataset for Multimodal Music Analysis:   Challenges, Insights, and Applications|creat music perform dataset multimod music analysi challeng insight applic|We introduce a dataset for facilitating audio-visual analysis of musical performances. The dataset comprises a number of simple multi-instrument musical pieces assembled from coordinated but separately recorded performances of individual tracks. For each piece, we provide the musical score in MIDI format, the audio recordings of the individual tracks, the audio and video recording of the assembled mixture, and ground-truth annotation files including frame-level and note-level transcriptions. We anticipate that the dataset will be useful for developing and evaluating multi-modal techniques for music source separation, transcription, score following, and performance analysis. We describe our methodology for the creation of this dataset, particularly highlighting our approaches for addressing the challenges involved in maintaining synchronization and naturalness. We briefly discuss the research questions that can be investigated with this dataset.|introduc dataset facilit audio visual analysi music perform dataset compris number simpl multi instrument music piec assembl coordin separ record perform individu track piec provid music score midi format audio record individu track audio video record assembl mixtur ground truth annot file includ frame level note level transcript anticip dataset use develop evalu multi modal techniqu music sourc separ transcript score follow perform analysi describ methodolog creation dataset particular highlight approach address challeng involv maintain synchron natur briefli discuss research question investig dataset|['Bochen Li', 'Xinzhao Liu', 'Karthik Dinesh', 'Zhiyao Duan', 'Gaurav Sharma']|['cs.MM', 'cs.SD']
2017-03-28T14:11:13Z|2017-02-11T20:04:46Z|http://arxiv.org/abs/1612.07837v2|http://arxiv.org/pdf/1612.07837v2|SampleRNN: An Unconditional End-to-End Neural Audio Generation Model|samplernn uncondit end end neural audio generat model|In this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time. We show that our model, which profits from combining memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans, on three datasets of different nature. Human evaluation on the generated samples indicate that our model is preferred over competing models. We also show how each component of the model contributes to the exhibited performance.|paper propos novel model uncondit audio generat base generat one audio sampl time show model profit combin memori less modul name autoregress multilay perceptron state recurr neural network hierarch structur abl captur sourc variat tempor sequenc veri long time span three dataset differ natur human evalu generat sampl indic model prefer compet model also show compon model contribut exhibit perform|['Soroush Mehri', 'Kundan Kumar', 'Ishaan Gulrajani', 'Rithesh Kumar', 'Shubham Jain', 'Jose Sotelo', 'Aaron Courville', 'Yoshua Bengio']|['cs.SD', 'cs.AI']
2017-03-28T14:11:13Z|2016-12-22T10:14:59Z|http://arxiv.org/abs/1612.07523v1|http://arxiv.org/pdf/1612.07523v1|Robustness of Voice Conversion Techniques Under Mismatched Conditions|robust voic convers techniqu mismatch condit|Most of the existing studies on voice conversion (VC) are conducted in acoustically matched conditions between source and target signal. However, the robustness of VC methods in presence of mismatch remains unknown. In this paper, we report a comparative analysis of different VC techniques under mismatched conditions. The extensive experiments with five different VC techniques on CMU ARCTIC corpus suggest that performance of VC methods substantially degrades in noisy conditions. We have found that bilinear frequency warping with amplitude scaling (BLFWAS) outperforms other methods in most of the noisy conditions. We further explore the suitability of different speech enhancement techniques for robust conversion. The objective evaluation results indicate that spectral subtraction and log minimum mean square error (logMMSE) based speech enhancement techniques can be used to improve the performance in specific noisy conditions.|exist studi voic convers vc conduct acoust match condit sourc target signal howev robust vc method presenc mismatch remain unknown paper report compar analysi differ vc techniqu mismatch condit extens experi five differ vc techniqu cmu arctic corpus suggest perform vc method substanti degrad noisi condit found bilinear frequenc warp amplitud scale blfwas outperform method noisi condit explor suitabl differ speech enhanc techniqu robust convers object evalu result indic spectral subtract log minimum mean squar error logmms base speech enhanc techniqu use improv perform specif noisi condit|['Monisankha Pal', 'Dipjyoti Paul', 'Md Sahidullah', 'Goutam Saha']|['cs.SD', 'cs.LG', 'stat.ML']
2017-03-28T14:11:13Z|2016-12-21T00:02:08Z|http://arxiv.org/abs/1612.07608v1|http://arxiv.org/pdf/1612.07608v1|EchoWear: Smartwatch Technology for Voice and Speech Treatments of   Patients with Parkinson's Disease|echowear smartwatch technolog voic speech treatment patient parkinson diseas|About 90 percent of people with Parkinson's disease (PD) experience decreased functional communication due to the presence of voice and speech disorders associated with dysarthria that can be characterized by monotony of pitch (or fundamental frequency), reduced loudness, irregular rate of speech, imprecise consonants, and changes in voice quality. Speech-language pathologists (SLPs) work with patients with PD to improve speech intelligibility using various intensive in-clinic speech treatments. SLPs also prescribe home exercises to enhance generalization of speech strategies outside of the treatment room. Even though speech therapies are found to be highly effective in improving vocal loudness and speech quality, patients with PD find it difficult to follow the prescribed exercise regimes outside the clinic and to continue exercises once the treatment is completed. SLPs need techniques to monitor compliance and accuracy of their patients exercises at home and in ecologically valid communication situations. We have designed EchoWear, a smartwatch-based system, to remotely monitor speech and voice exercises as prescribed by SLPs. We conducted a study of 6 individuals; three with PD and three healthy controls. To assess the performance of EchoWear technology compared with high quality audio equipment obtained in a speech laboratory. Our preliminary analysis shows promising outcomes for using EchoWear in speech therapies for people with PD.   Keywords: Dysarthria; knowledge-based speech processing; Parkinson's disease; smartwatch; speech therapy; wearable system.|percent peopl parkinson diseas pd experi decreas function communic due presenc voic speech disord associ dysarthria character monotoni pitch fundament frequenc reduc loud irregular rate speech imprecis conson chang voic qualiti speech languag pathologist slps work patient pd improv speech intellig use various intens clinic speech treatment slps also prescrib home exercis enhanc general speech strategi outsid treatment room even though speech therapi found high effect improv vocal loud speech qualiti patient pd find difficult follow prescrib exercis regim outsid clinic continu exercis onc treatment complet slps need techniqu monitor complianc accuraci patient exercis home ecolog valid communic situat design echowear smartwatch base system remot monitor speech voic exercis prescrib slps conduct studi individu three pd three healthi control assess perform echowear technolog compar high qualiti audio equip obtain speech laboratori preliminari analysi show promis outcom use echowear speech therapi peopl pd keyword dysarthria knowledg base speech process parkinson diseas smartwatch speech therapi wearabl system|['Harishchandra Dubey', 'Jon C. Goldberg', 'Mohammadreza Abtahi', 'Leslie Mahler', 'Kunal Mankodiya']|['cs.CY', 'cs.SD']
2017-03-28T14:11:13Z|2016-12-20T13:04:33Z|http://arxiv.org/abs/1612.06642v1|http://arxiv.org/pdf/1612.06642v1|Efficient Target Activity Detection based on Recurrent Neural Networks|effici target activ detect base recurr neural network|This paper addresses the problem of Target Activity Detection (TAD) for binaural listening devices. TAD denotes the problem of robustly detecting the activity of a target speaker in a harsh acoustic environment, which comprises interfering speakers and noise (cocktail party scenario). In previous work, it has been shown that employing a Feed-forward Neural Network (FNN) for detecting the target speaker activity is a promising approach to combine the advantage of different TAD features (used as network inputs). In this contribution, we exploit a larger context window for TAD and compare the performance of FNNs and Recurrent Neural Networks (RNNs) with an explicit focus on small network topologies as desirable for embedded acoustic signal processing systems. More specifically, the investigations include a comparison between three different types of RNNs, namely plain RNNs, Long Short-Term Memories, and Gated Recurrent Units. The results indicate that all versions of RNNs outperform FNNs for the task of TAD.|paper address problem target activ detect tad binaur listen devic tad denot problem robust detect activ target speaker harsh acoust environ compris interf speaker nois cocktail parti scenario previous work shown employ feed forward neural network fnn detect target speaker activ promis approach combin advantag differ tad featur use network input contribut exploit larger context window tad compar perform fnns recurr neural network rnns explicit focus small network topolog desir embed acoust signal process system specif investig includ comparison three differ type rnns name plain rnns long short term memori gate recurr unit result indic version rnns outperform fnns task tad|['Daniel Gerber', 'Stefan Meier', 'Walter Kellermann']|['cs.SD']
2017-03-28T14:11:17Z|2017-03-09T08:47:38Z|http://arxiv.org/abs/1612.06151v3|http://arxiv.org/pdf/1612.06151v3|HRTF-based two-dimensional robust least-squares frequency-invariant   beamformer design for robot audition|hrtf base two dimension robust least squar frequenc invari beamform design robot audit|In this work, we propose a two-dimensional Head-Related Transfer Function (HRTF)-based robust beamformer design for robot audition, which allows for explicit control of the beamformer response for the entire three-dimensional sound field surrounding a humanoid robot. We evaluate the proposed method by means of both signal-independent and signal-dependent measures in a robot audition scenario. Our results confirm the effectiveness of the proposed two-dimensional HRTF-based beamformer design, compared to our previously published one-dimensional HRTF-based beamformer design, which was carried out for a fixed elevation angle only.|work propos two dimension head relat transfer function hrtf base robust beamform design robot audit allow explicit control beamform respons entir three dimension sound field surround humanoid robot evalu propos method mean signal independ signal depend measur robot audit scenario result confirm effect propos two dimension hrtf base beamform design compar previous publish one dimension hrtf base beamform design carri fix elev angl onli|['Hendrik Barfuss', 'Michael Buerger', 'Jasper Podschus', 'Walter Kellermann']|['cs.SD']
2017-03-28T14:11:17Z|2016-12-16T14:40:43Z|http://arxiv.org/abs/1612.05489v1|http://arxiv.org/pdf/1612.05489v1|On-bird Sound Recordings: Automatic Acoustic Recognition of Activities   and Contexts|bird sound record automat acoust recognit activ context|We introduce a novel approach to studying animal behaviour and the context in which it occurs, through the use of microphone backpacks carried on the backs of individual free-flying birds. These sensors are increasingly used by animal behaviour researchers to study individual vocalisations of freely behaving animals, even in the field. However such devices may record more than an animals vocal behaviour, and have the potential to be used for investigating specific activities (movement) and context (background) within which vocalisations occur. To facilitate this approach, we investigate the automatic annotation of such recordings through two different sound scene analysis paradigms: a scene-classification method using feature learning, and an event-detection method using probabilistic latent component analysis (PLCA). We analyse recordings made with Eurasian jackdaws (Corvus monedula) in both captive and field settings. Results are comparable with the state of the art in sound scene analysis; we find that the current recognition quality level enables scalable automatic annotation of audio logger data, given partial annotation, but also find that individual differences between animals and/or their backpacks limit the generalisation from one individual to another. we consider the interrelation of 'scenes' and 'events' in this particular task, and issues of temporal resolution.|introduc novel approach studi anim behaviour context occur use microphon backpack carri back individu free fli bird sensor increas use anim behaviour research studi individu vocalis freeli behav anim even field howev devic may record anim vocal behaviour potenti use investig specif activ movement context background within vocalis occur facilit approach investig automat annot record two differ sound scene analysi paradigm scene classif method use featur learn event detect method use probabilist latent compon analysi plca analys record made eurasian jackdaw corvus monedula captiv field set result compar state art sound scene analysi find current recognit qualiti level enabl scalabl automat annot audio logger data given partial annot also find individu differ anim backpack limit generalis one individu anoth consid interrel scene event particular task issu tempor resolut|['Dan Stowell', 'Emmanouil Benetos', 'Lisa F. Gill']|['cs.SD']
2017-03-28T14:11:17Z|2016-12-16T11:05:52Z|http://arxiv.org/abs/1612.05432v1|http://arxiv.org/pdf/1612.05432v1|Basis-Function Modeling of Loudness Variations in Ensemble Performance|basi function model loud variat ensembl perform|This paper describes a computational model of loudness variations in expressive ensemble performance. The model predicts and explains the continuous variation of loudness as a function of information extracted automatically from the written score. Although such models have been proposed for expressive performance in solo instruments, this is (to the best of our knowledge) the first attempt to define a model for expressive performance in ensembles. To that end, we extend an existing model that was designed to model expressive piano performances, and describe the additional steps necessary for the model to deal with scores of arbitrary instrumentation, including orchestral scores. We test both linear and non-linear variants of the extended model n a data set of audio recordings of symphonic music, in a leave-one-out setting. The experiments reveal that the most successful model variant is a recurrent, non-linear model. Even if the accuracy of the predicted loudness varies from one recording to another, in several cases the model explains well over 50% of the variance in loudness.|paper describ comput model loud variat express ensembl perform model predict explain continu variat loud function inform extract automat written score although model propos express perform solo instrument best knowledg first attempt defin model express perform ensembl end extend exist model design model express piano perform describ addit step necessari model deal score arbitrari instrument includ orchestr score test linear non linear variant extend model data set audio record symphon music leav one set experi reveal success model variant recurr non linear model even accuraci predict loud vari one record anoth sever case model explain well varianc loud|['Thassilo Gadermaier', 'Maarten Grachten', 'Carlos Eduardo Cancino Chac√≥n']|['cs.SD']
2017-03-28T14:11:17Z|2016-12-16T05:09:14Z|http://arxiv.org/abs/1612.05369v1|http://arxiv.org/pdf/1612.05369v1|Neural networks based EEG-Speech Models|neural network base eeg speech model|In this paper, we describe three neural network (NN) based EEG-Speech (NES) models that map the unspoken EEG signals to the corresponding phonemes. Instead of using conventional feature extraction techniques, the proposed NES models rely on graphic learning to project both EEG and speech signals into deep representation feature spaces. This NN based linear projection helps to realize multimodal data fusion (i.e., EEG and acoustic signals). It is convenient to construct the mapping between unspoken EEG signals and phonemes. Specifically, among three NES models, two augmented models (i.e., IANES-B and IANES-G) include spoken EEG signals as either bias or gate information to strengthen the feature learning and translation of unspoken EEG signals. A combined unsupervised and supervised training is implemented stepwise to learn the mapping for all three NES models. To enhance the computational performance, three way factored NN training technique is applied to IANES-G model. Unlike many existing methods, our augmented NES models incorporate spoken-EEG signals that can efficiently suppress the artifacts in unspoken-EEG signals. Experimental results reveal that all three proposed NES models outperform the baseline SVM method, whereas IANES-G demonstrates the best performance on speech recovery and classification task comparatively.|paper describ three neural network nn base eeg speech nes model map unspoken eeg signal correspond phonem instead use convent featur extract techniqu propos nes model reli graphic learn project eeg speech signal deep represent featur space nn base linear project help realiz multimod data fusion eeg acoust signal conveni construct map unspoken eeg signal phonem specif among three nes model two augment model ian ian includ spoken eeg signal either bias gate inform strengthen featur learn translat unspoken eeg signal combin unsupervis supervis train implement stepwis learn map three nes model enhanc comput perform three way factor nn train techniqu appli ian model unlik mani exist method augment nes model incorpor spoken eeg signal effici suppress artifact unspoken eeg signal experiment result reveal three propos nes model outperform baselin svm method wherea ian demonstr best perform speech recoveri classif task compar|['Pengfei Sun', 'Jun Qin']|['cs.SD', 'cs.LG']
2017-03-28T14:11:17Z|2016-12-15T19:43:54Z|http://arxiv.org/abs/1612.05156v1|http://arxiv.org/pdf/1612.05156v1|A Phase Vocoder based on Nonstationary Gabor Frames|phase vocod base nonstationari gabor frame|We propose a new algorithm for time stretching music signals based on the theory of nonstationary Gabor frames. The algorithm extends the techniques of the classical phase vocoder by incorporating adaptive time-frequency representations and adaptive phase locking. Applying a preliminary onset detection algorithm, the obtained time-frequency representation implies good time resolution for the onsets and good frequency resolution for the sinusoidal components.   The phase estimates are done only at peak channels using quadratic interpolation and the remaining phases are then locked to the values of the peaks in an adaptive manner. In contrast to previous attempts we let the number of frequency channels vary over time in order to obtain a low redundancy of the corresponding transform. We show that with a redundancy comparable to that of the phase vocoder we can greatly reduce artefacts such as phasiness and transient smearing. The algorithm is tested on both synthetic and real world signals and compared with state of the art algorithms in a reproducible manner.|propos new algorithm time stretch music signal base theori nonstationari gabor frame algorithm extend techniqu classic phase vocod incorpor adapt time frequenc represent adapt phase lock appli preliminari onset detect algorithm obtain time frequenc represent impli good time resolut onset good frequenc resolut sinusoid compon phase estim done onli peak channel use quadrat interpol remain phase lock valu peak adapt manner contrast previous attempt let number frequenc channel vari time order obtain low redund correspond transform show redund compar phase vocod great reduc artefact phasi transient smear algorithm test synthet real world signal compar state art algorithm reproduc manner|['Emil Solsb√¶k Ottosen', 'Monika D√∂rfler']|['cs.SD']
2017-03-28T14:11:17Z|2016-12-15T17:59:05Z|http://arxiv.org/abs/1612.05168v1|http://arxiv.org/pdf/1612.05168v1|LIA system description for NIST SRE 2016|lia system descript nist sre|This paper describes the LIA speaker recognition system developed for the Speaker Recognition Evaluation (SRE) campaign. Eight sub-systems are developed, all based on a state-of-the-art approach: i-vector/PLDA which represents the mainstream technique in text-independent speaker recognition. These sub-systems differ: on the acoustic feature extraction front-end (MFCC, PLP), at the i-vector extraction stage (UBM, DNN or two-feats posteriors) and finally on the data-shifting (IDVC, mean-shifting). The submitted system is a fusion at the score-level of these eight sub-systems.|paper describ lia speaker recognit system develop speaker recognit evalu sre campaign eight sub system develop base state art approach vector plda repres mainstream techniqu text independ speaker recognit sub system differ acoust featur extract front end mfcc plp vector extract stage ubm dnn two feat posterior final data shift idvc mean shift submit system fusion score level eight sub system|['Mickael Rouvier', 'Pierre-Michel Bousquet', 'Moez Ajili', 'Waad Ben Kheder', 'Driss Matrouf', 'Jean-Fran√ßois Bonastre']|['cs.SD']
2017-03-28T14:11:17Z|2016-12-15T17:32:11Z|http://arxiv.org/abs/1612.05153v1|http://arxiv.org/pdf/1612.05153v1|On the Potential of Simple Framewise Approaches to Piano Transcription|potenti simpl framewis approach piano transcript|In an attempt at exploring the limitations of simple approaches to the task of piano transcription (as usually defined in MIR), we conduct an in-depth analysis of neural network-based framewise transcription. We systematically compare different popular input representations for transcription systems to determine the ones most suitable for use with neural networks. Exploiting recent advances in training techniques and new regularizers, and taking into account hyper-parameter tuning, we show that it is possible, by simple bottom-up frame-wise processing, to obtain a piano transcriber that outperforms the current published state of the art on the publicly available MAPS dataset -- without any complex post-processing steps. Thus, we propose this simple approach as a new baseline for this dataset, for future transcription research to build on and improve.|attempt explor limit simpl approach task piano transcript usual defin mir conduct depth analysi neural network base framewis transcript systemat compar differ popular input represent transcript system determin one suitabl use neural network exploit recent advanc train techniqu new regular take account hyper paramet tune show possibl simpl bottom frame wise process obtain piano transcrib outperform current publish state art public avail map dataset without ani complex post process step thus propos simpl approach new baselin dataset futur transcript research build improv|['Rainer Kelz', 'Matthias Dorfer', 'Filip Korzeniowski', 'Sebastian B√∂ck', 'Andreas Arzt', 'Gerhard Widmer']|['cs.SD', 'cs.LG']
2017-03-28T14:11:17Z|2016-12-15T14:32:20Z|http://arxiv.org/abs/1612.05082v1|http://arxiv.org/abs/1612.05082v1|A Fully Convolutional Deep Auditory Model for Musical Chord Recognition|fulli convolut deep auditori model music chord recognit|Chord recognition systems depend on robust feature extraction pipelines. While these pipelines are traditionally hand-crafted, recent advances in end-to-end machine learning have begun to inspire researchers to explore data-driven methods for such tasks. In this paper, we present a chord recognition system that uses a fully convolutional deep auditory model for feature extraction. The extracted features are processed by a Conditional Random Field that decodes the final chord sequence. Both processing stages are trained automatically and do not require expert knowledge for optimising parameters. We show that the learned auditory system extracts musically interpretable features, and that the proposed chord recognition system achieves results on par or better than state-of-the-art algorithms.|chord recognit system depend robust featur extract pipelin pipelin tradit hand craft recent advanc end end machin learn begun inspir research explor data driven method task paper present chord recognit system use fulli convolut deep auditori model featur extract extract featur process condit random field decod final chord sequenc process stage train automat requir expert knowledg optimis paramet show learn auditori system extract music interpret featur propos chord recognit system achiev result par better state art algorithm|['Filip Korzeniowski', 'Gerhard Widmer']|['cs.LG', 'cs.SD']
2017-03-28T14:11:17Z|2016-12-15T14:16:56Z|http://arxiv.org/abs/1612.05076v1|http://arxiv.org/pdf/1612.05076v1|Live Score Following on Sheet Music Images|live score follow sheet music imag|In this demo we show a novel approach to score following. Instead of relying on some symbolic representation, we are using a multi-modal convolutional neural network to match the incoming audio stream directly to sheet music images. This approach is in an early stage and should be seen as proof of concept. Nonetheless, the audience will have the opportunity to test our implementation themselves via 3 simple piano pieces.|demo show novel approach score follow instead reli symbol represent use multi modal convolut neural network match incom audio stream direct sheet music imag approach earli stage seen proof concept nonetheless audienc opportun test implement themselv via simpl piano piec|['Matthias Dorfer', 'Andreas Arzt', 'Sebastian B√∂ck', 'Amaury Durand', 'Gerhard Widmer']|['cs.SD']
2017-03-28T14:11:17Z|2016-12-15T14:07:51Z|http://arxiv.org/abs/1612.05070v1|http://arxiv.org/pdf/1612.05070v1|Towards End-to-End Audio-Sheet-Music Retrieval|toward end end audio sheet music retriev|This paper demonstrates the feasibility of learning to retrieve short snippets of sheet music (images) when given a short query excerpt of music (audio) -- and vice versa --, without any symbolic representation of music or scores. This would be highly useful in many content-based musical retrieval scenarios. Our approach is based on Deep Canonical Correlation Analysis (DCCA) and learns correlated latent spaces allowing for cross-modality retrieval in both directions. Initial experiments with relatively simple monophonic music show promising results.|paper demonstr feasibl learn retriev short snippet sheet music imag given short queri excerpt music audio vice versa without ani symbol represent music score would high use mani content base music retriev scenario approach base deep canon correl analysi dcca learn correl latent space allow cross modal retriev direct initi experi relat simpl monophon music show promis result|['Matthias Dorfer', 'Andreas Arzt', 'Gerhard Widmer']|['cs.SD', 'cs.IR', 'cs.LG']
2017-03-28T14:11:21Z|2016-12-15T14:01:50Z|http://arxiv.org/abs/1612.05065v1|http://arxiv.org/pdf/1612.05065v1|Feature Learning for Chord Recognition: The Deep Chroma Extractor|featur learn chord recognit deep chroma extractor|We explore frame-level audio feature learning for chord recognition using artificial neural networks. We present the argument that chroma vectors potentially hold enough information to model harmonic content of audio for chord recognition, but that standard chroma extractors compute too noisy features. This leads us to propose a learned chroma feature extractor based on artificial neural networks. It is trained to compute chroma features that encode harmonic information important for chord recognition, while being robust to irrelevant interferences. We achieve this by feeding the network an audio spectrum with context instead of a single frame as input. This way, the network can learn to selectively compensate noise and resolve harmonic ambiguities.   We compare the resulting features to hand-crafted ones by using a simple linear frame-wise classifier for chord recognition on various data sets. The results show that the learned feature extractor produces superior chroma vectors for chord recognition.|explor frame level audio featur learn chord recognit use artifici neural network present argument chroma vector potenti hold enough inform model harmon content audio chord recognit standard chroma extractor comput noisi featur lead us propos learn chroma featur extractor base artifici neural network train comput chroma featur encod harmon inform import chord recognit robust irrelev interfer achiev feed network audio spectrum context instead singl frame input way network learn select compens nois resolv harmon ambigu compar result featur hand craft one use simpl linear frame wise classifi chord recognit various data set result show learn featur extractor produc superior chroma vector chord recognit|['Filip Korzeniowski', 'Gerhard Widmer']|['cs.SD', 'cs.LG']
2017-03-28T14:11:21Z|2016-12-15T05:06:40Z|http://arxiv.org/abs/1612.04928v1|http://arxiv.org/pdf/1612.04928v1|Music Generation with Deep Learning|music generat deep learn|The use of deep learning to solve problems in literary arts has been a recent trend that has gained a lot of attention and automated generation of music has been an active area. This project deals with the generation of music using raw audio files in the frequency domain relying on various LSTM architectures. Fully connected and convolutional layers are used along with LSTM's to capture rich features in the frequency domain and increase the quality of music generated. The work is focused on unconstrained music generation and uses no information about musical structure(notes or chords) to aid learning.The music generated from various architectures are compared using blind fold tests. Using the raw audio to train models is the direction to tapping the enormous amount of mp3 files that exist over the internet without requiring the manual effort to make structured MIDI files. Moreover, not all audio files can be represented with MIDI files making the study of these models an interesting prospect to the future of such models.|use deep learn solv problem literari art recent trend gain lot attent autom generat music activ area project deal generat music use raw audio file frequenc domain reli various lstm architectur fulli connect convolut layer use along lstm captur rich featur frequenc domain increas qualiti music generat work focus unconstrain music generat use inform music structur note chord aid learn music generat various architectur compar use blind fold test use raw audio train model direct tap enorm amount mp file exist internet without requir manual effort make structur midi file moreov audio file repres midi file make studi model interest prospect futur model|['Vasanth Kalingeri', 'Srikanth Grandhe']|['cs.SD']
2017-03-28T14:11:21Z|2016-12-15T04:22:39Z|http://arxiv.org/abs/1612.04919v1|http://arxiv.org/pdf/1612.04919v1|Combination of Linear Prediction and Phase Decomposition for Glottal   Source Analysis on Voiced Speech|combin linear predict phase decomposit glottal sourc analysi voic speech|Some glottal analysis approaches based upon linear prediction or complex cepstrum approaches have been proved to be effective to estimate glottal source from real speech utterances. We propose a new approach employing both an all-pole odd-order linear prediction to provide a coarse estimation and phase decomposition based causality/anti-causality separation to generate further refinements. The obtained measures show that this method improved performance in terms of reducing source-filter separation in estimation of glottal flow pulses (GFP). No glottal model fitting is required by this method, thus it has wide and flexible adaptation to retain fidelity of speakers's vocal features with computationally affordable resource. The method is evaluated on real speech utterances to validate it.|glottal analysi approach base upon linear predict complex cepstrum approach prove effect estim glottal sourc real speech utter propos new approach employ pole odd order linear predict provid coars estim phase decomposit base causal anti causal separ generat refin obtain measur show method improv perform term reduc sourc filter separ estim glottal flow puls gfp glottal model fit requir method thus wide flexibl adapt retain fidel speaker vocal featur comput afford resourc method evalu real speech utter valid|['Yiqiao Chen', 'John N. Gowdy']|['cs.SD']
2017-03-28T14:11:21Z|2016-12-14T17:40:02Z|http://arxiv.org/abs/1612.04744v1|http://arxiv.org/pdf/1612.04744v1|Incorporating Language Level Information into Acoustic Models|incorpor languag level inform acoust model|This paper proposed a class of novel Deep Recurrent Neural Networks which can incorporate language-level information into acoustic models. For simplicity, we named these networks Recurrent Deep Language Networks (RDLNs). Multiple variants of RDLNs were considered, including two kinds of context information, two methods to process the context, and two methods to incorporate the language-level information. RDLNs provided possible methods to fine-tune the whole Automatic Speech Recognition (ASR) system in the acoustic modeling process.|paper propos class novel deep recurr neural network incorpor languag level inform acoust model simplic name network recurr deep languag network rdlns multipl variant rdlns consid includ two kind context inform two method process context two method incorpor languag level inform rdlns provid possibl method fine tune whole automat speech recognit asr system acoust model process|['Peidong Wang', 'Deliang Wang']|['cs.CL', 'cs.LG', 'cs.SD']
2017-03-28T14:11:21Z|2016-12-15T01:48:52Z|http://arxiv.org/abs/1612.04742v2|http://arxiv.org/pdf/1612.04742v2|Imposing higher-level Structure in Polyphonic Music Generation using   Convolutional Restricted Boltzmann Machines and Constraints|impos higher level structur polyphon music generat use convolut restrict boltzmann machin constraint|"We introduce a method for imposing higher-level structure on generated, polyphonic music. A Convolutional Restricted Boltzmann Machine (C-RBM) as a generative model is combined with gradient descent constraint optimization to provide further control over the generation process. Among other things, this allows for the use of a ""template"" piece, from which some structural properties can be extracted, and transferred as constraints to newly generated material. The sampling process is guided with Simulated Annealing in order to avoid local optima, and find solutions that both satisfy the constraints, and are relatively stable with respect to the C-RBM. Results show that with this approach it is possible to control the higher level self-similarity structure, the meter, as well as tonal properties of the resulting musical piece while preserving its local musical coherence."|introduc method impos higher level structur generat polyphon music convolut restrict boltzmann machin rbm generat model combin gradient descent constraint optim provid control generat process among thing allow use templat piec structur properti extract transfer constraint newli generat materi sampl process guid simul anneal order avoid local optima find solut satisfi constraint relat stabl respect rbm result show approach possibl control higher level self similar structur meter well tonal properti result music piec preserv local music coher|['Stefan Lattner', 'Maarten Grachten', 'Gerhard Widmer']|['cs.SD', 'cs.AI', 'cs.NE']
2017-03-28T14:11:21Z|2016-12-14T15:40:44Z|http://arxiv.org/abs/1612.06287v1|http://arxiv.org/pdf/1612.06287v1|VAST : The Virtual Acoustic Space Traveler Dataset|vast virtual acoust space travel dataset|This paper introduces a new paradigm for sound source lo-calization referred to as virtual acoustic space traveling (VAST) and presents a first dataset designed for this purpose. Existing sound source localization methods are either based on an approximate physical model (physics-driven) or on a specific-purpose calibration set (data-driven). With VAST, the idea is to learn a mapping from audio features to desired audio properties using a massive dataset of simulated room impulse responses. This virtual dataset is designed to be maximally representative of the potential audio scenes that the considered system may be evolving in, while remaining reasonably compact. We show that virtually-learned mappings on this dataset generalize to real data, overcoming some intrinsic limitations of traditional binaural sound localization methods based on time differences of arrival.|paper introduc new paradigm sound sourc lo calize refer virtual acoust space travel vast present first dataset design purpos exist sound sourc local method either base approxim physic model physic driven specif purpos calibr set data driven vast idea learn map audio featur desir audio properti use massiv dataset simul room impuls respons virtual dataset design maxim repres potenti audio scene consid system may evolv remain reason compact show virtual learn map dataset general real data overcom intrins limit tradit binaur sound local method base time differ arriv|['Cl√©ment Gaultier', 'Saurabh Kataria', 'Antoine Deleforge']|['cs.SD', 'cs.LG']
2017-03-28T14:11:21Z|2016-12-14T15:07:51Z|http://arxiv.org/abs/1612.04675v1|http://arxiv.org/pdf/1612.04675v1|Recurrent Deep Stacking Networks for Speech Recognition|recurr deep stack network speech recognit|This paper presented our work on applying Recurrent Deep Stacking Networks (RDSNs) to Robust Automatic Speech Recognition (ASR) tasks. In the paper, we also proposed a more efficient yet comparable substitute to RDSN, Bi- Pass Stacking Network (BPSN). The main idea of these two models is to add phoneme-level information into acoustic models, transforming an acoustic model to the combination of an acoustic model and a phoneme-level N-gram model. Experiments showed that RDSN and BPsn can substantially improve the performances over conventional DNNs.|paper present work appli recurr deep stack network rdsns robust automat speech recognit asr task paper also propos effici yet compar substitut rdsn bi pass stack network bpsn main idea two model add phonem level inform acoust model transform acoust model combin acoust model phonem level gram model experi show rdsn bpsn substanti improv perform convent dnns|['Peidong Wang', 'Zhongqiu Wang', 'Deliang Wang']|['cs.CL', 'cs.SD']
2017-03-28T14:11:21Z|2017-01-19T15:33:53Z|http://arxiv.org/abs/1612.04056v2|http://arxiv.org/pdf/1612.04056v2|Joint Bayesian Gaussian discriminant analysis for speaker verification|joint bayesian gaussian discrimin analysi speaker verif|State-of-the-art i-vector based speaker verification relies on variants of Probabilistic Linear Discriminant Analysis (PLDA) for discriminant analysis. We are mainly motivated by the recent work of the joint Bayesian (JB) method, which is originally proposed for discriminant analysis in face verification. We apply JB to speaker verification and make three contributions beyond the original JB. 1) In contrast to the EM iterations with approximated statistics in the original JB, the EM iterations with exact statistics are employed and give better performance. 2) We propose to do simultaneous diagonalization (SD) of the within-class and between-class covariance matrices to achieve efficient testing, which has broader application scope than the SVD-based efficient testing method in the original JB. 3) We scrutinize similarities and differences between various Gaussian PLDAs and JB, complementing the previous analysis of comparing JB only with Prince-Elder PLDA. Extensive experiments are conducted on NIST SRE10 core condition 5, empirically validating the superiority of JB with faster convergence rate and 9-13% EER reduction compared with state-of-the-art PLDA.|state art vector base speaker verif reli variant probabilist linear discrimin analysi plda discrimin analysi main motiv recent work joint bayesian jb method origin propos discrimin analysi face verif appli jb speaker verif make three contribut beyond origin jb contrast em iter approxim statist origin jb em iter exact statist employ give better perform propos simultan diagon sd within class class covari matric achiev effici test broader applic scope svd base effici test method origin jb scrutin similar differ various gaussian pldas jb complement previous analysi compar jb onli princ elder plda extens experi conduct nist sre core condit empir valid superior jb faster converg rate eer reduct compar state art plda|['Yiyan Wang', 'Haotian Xu', 'Zhijian Ou']|['cs.SD', 'cs.LG']
2017-03-28T14:11:21Z|2016-12-16T14:25:44Z|http://arxiv.org/abs/1612.04028v2|http://arxiv.org/abs/1612.04028v2|Adaptive DCTNet for Audio Signal Classification|adapt dctnet audio signal classif|In this paper, we investigate DCTNet for audio signal classification. Its output feature is related to Cohen's class of time-frequency distributions. We introduce the use of adaptive DCTNet (A-DCTNet) for audio signals feature extraction. The A-DCTNet applies the idea of constant-Q transform, with its center frequencies of filterbanks geometrically spaced. The A-DCTNet is adaptive to different acoustic scales, and it can better capture low frequency acoustic information that is sensitive to human audio perception than features such as Mel-frequency spectral coefficients (MFSC). We use features extracted by the A-DCTNet as input for classifiers. Experimental results show that the A-DCTNet and Recurrent Neural Networks (RNN) achieve state-of-the-art performance in bird song classification rate, and improve artist identification accuracy in music data. They demonstrate A-DCTNet's applicability to signal processing problems.|paper investig dctnet audio signal classif output featur relat cohen class time frequenc distribut introduc use adapt dctnet dctnet audio signal featur extract dctnet appli idea constant transform center frequenc filterbank geometr space dctnet adapt differ acoust scale better captur low frequenc acoust inform sensit human audio percept featur mel frequenc spectral coeffici mfsc use featur extract dctnet input classifi experiment result show dctnet recurr neural network rnn achiev state art perform bird song classif rate improv artist identif accuraci music data demonstr dctnet applic signal process problem|['Yin Xian', 'Yunchen Pu', 'Zhe Gan', 'Liang Lu', 'Andrew Thompson']|['cs.SD']
2017-03-28T14:11:21Z|2016-12-12T17:06:19Z|http://arxiv.org/abs/1612.03789v1|http://arxiv.org/pdf/1612.03789v1|A Unit Selection Methodology for Music Generation Using Deep Neural   Networks|unit select methodolog music generat use deep neural network|Several methods exist for a computer to generate music based on data including Markov chains, recurrent neural networks, recombinancy, and grammars. We explore the use of unit selection and concatenation as a means of generating music using a procedure based on ranking, where, we consider a unit to be a variable length number of measures of music. We first examine whether a unit selection method, that is restricted to a finite size unit library, can be sufficient for encompassing a wide spectrum of music. We do this by developing a deep autoencoder that encodes a musical input and reconstructs the input by selecting from the library. We then describe a generative model that combines a deep structured semantic model (DSSM) with an LSTM to predict the next unit, where units consist of four, two, and one measures of music. We evaluate the generative model using objective metrics including mean rank and accuracy and with a subjective listening test in which expert musicians are asked to complete a forced-choiced ranking task. We compare our model to a note-level generative baseline that consists of a stacked LSTM trained to predict forward by one note.|sever method exist comput generat music base data includ markov chain recurr neural network recombin grammar explor use unit select concaten mean generat music use procedur base rank consid unit variabl length number measur music first examin whether unit select method restrict finit size unit librari suffici encompass wide spectrum music develop deep autoencod encod music input reconstruct input select librari describ generat model combin deep structur semant model dssm lstm predict next unit unit consist four two one measur music evalu generat model use object metric includ mean rank accuraci subject listen test expert musician ask complet forc choic rank task compar model note level generat baselin consist stack lstm train predict forward one note|['Mason Bretan', 'Gil Weinberg', 'Larry Heck']|['cs.SD', 'cs.AI', 'cs.LG']
2017-03-28T14:11:25Z|2016-12-12T00:13:35Z|http://arxiv.org/abs/1612.03505v1|http://arxiv.org/pdf/1612.03505v1|Convolutional Neural Networks for Passive Monitoring of a Shallow Water   Environment using a Single Sensor|convolut neural network passiv monitor shallow water environ use singl sensor|A cost effective approach to remote monitoring of protected areas such as marine reserves and restricted naval waters is to use passive sonar to detect, classify, localize, and track marine vessel activity (including small boats and autonomous underwater vehicles). Cepstral analysis of underwater acoustic data enables the time delay between the direct path arrival and the first multipath arrival to be measured, which in turn enables estimation of the instantaneous range of the source (a small boat). However, this conventional method is limited to ranges where the Lloyd's mirror effect (interference pattern formed between the direct and first multipath arrivals) is discernible. This paper proposes the use of convolutional neural networks (CNNs) for the joint detection and ranging of broadband acoustic noise sources such as marine vessels in conjunction with a data augmentation approach for improving network performance in varied signal-to-noise ratio (SNR) situations. Performance is compared with a conventional passive sonar ranging method for monitoring marine vessel activity using real data from a single hydrophone mounted above the sea floor. It is shown that CNNs operating on cepstrum data are able to detect the presence and estimate the range of transiting vessels at greater distances than the conventional method.|cost effect approach remot monitor protect area marin reserv restrict naval water use passiv sonar detect classifi local track marin vessel activ includ small boat autonom underwat vehicl cepstral analysi underwat acoust data enabl time delay direct path arriv first multipath arriv measur turn enabl estim instantan rang sourc small boat howev convent method limit rang lloyd mirror effect interfer pattern form direct first multipath arriv discern paper propos use convolut neural network cnns joint detect rang broadband acoust nois sourc marin vessel conjunct data augment approach improv network perform vari signal nois ratio snr situat perform compar convent passiv sonar rang method monitor marin vessel activ use real data singl hydrophon mount abov sea floor shown cnns oper cepstrum data abl detect presenc estim rang transit vessel greater distanc convent method|['Eric L. Ferguson', 'Rishi Ramakrishnan', 'Stefan B. Williams', 'Craig T. Jin']|['cs.SD']
2017-03-28T14:11:25Z|2016-12-28T14:47:27Z|http://arxiv.org/abs/1612.02350v2|http://arxiv.org/pdf/1612.02350v2|An Information-theoretic Approach to Machine-oriented Music   Summarization|inform theoret approach machin orient music summar|Applying generic media-agnostic summarization to music allows for higher efficiency in automatic processing, storage, and communication of datasets while also alleviating copyright issues. This process has already been proven useful in the context of music genre classification. In this paper, we generalize conclusions from previous work by evaluating the impact of generic summarization in music from a probabilistic perspective and agnostic relative to certain tasks. We estimate Gaussian distributions for original and summarized songs and compute their relative entropy to measure how much information is lost in the summarization process. Based on this observation, we further propose a simple yet expressive summarization method that objectively outperforms previous methods and is better suited to avoid copyright issues. We present results suggesting that relative entropy is a good predictor of summarization performance in the context of tasks relying on a bag-of-features model.|appli generic media agnost summar music allow higher effici automat process storag communic dataset also allevi copyright issu process alreadi proven use context music genr classif paper general conclus previous work evalu impact generic summar music probabilist perspect agnost relat certain task estim gaussian distribut origin summar song comput relat entropi measur much inform lost summar process base observ propos simpl yet express summar method object outperform previous method better suit avoid copyright issu present result suggest relat entropi good predictor summar perform context task reli bag featur model|['Francisco Raposo', 'David Martins de Matos', 'Ricardo Ribeiro']|['cs.IR', 'cs.LG', 'cs.SD', 'H.5.5']
2017-03-28T14:11:25Z|2016-12-13T11:14:30Z|http://arxiv.org/abs/1612.02198v2|http://arxiv.org/pdf/1612.02198v2|Towards computer-assisted understanding of dynamics in symphonic music|toward comput assist understand dynam symphon music|Many people enjoy classical symphonic music. Its diverse instrumentation makes for a rich listening experience. This diversity adds to the conductor's expressive freedom to shape the sound according to their imagination. As a result, the same piece may sound quite differently from one conductor to another. Differences in interpretation may be noticeable subjectively to listeners, but they are sometimes hard to pinpoint, presumably because of the acoustic complexity of the sound. We describe a computational model that interprets dynamics---expressive loudness variations in performances---in terms of the musical score, highlighting differences between performances of the same piece. We demonstrate experimentally that the model has predictive power, and give examples of conductor ideosyncrasies found by using the model as an explanatory tool. Although the present model is still in active development, it may pave the road for a consumer-oriented companion to interactive classical music understanding.|mani peopl enjoy classic symphon music divers instrument make rich listen experi divers add conductor express freedom shape sound accord imagin result piec may sound quit differ one conductor anoth differ interpret may notic subject listen sometim hard pinpoint presum becaus acoust complex sound describ comput model interpret dynam express loud variat perform term music score highlight differ perform piec demonstr experiment model predict power give exampl conductor ideosyncrasi found use model explanatori tool although present model still activ develop may pave road consum orient companion interact classic music understand|['Maarten Grachten', 'Carlos Eduardo Cancino-Chac√≥n', 'Thassilo Gadermaier', 'Gerhard Widmer']|['cs.SD', 'cs.MM']
2017-03-28T14:11:25Z|2016-12-06T18:37:30Z|http://arxiv.org/abs/1612.01943v1|http://arxiv.org/pdf/1612.01943v1|Segmental Convolutional Neural Networks for Detection of Cardiac   Abnormality With Noisy Heart Sound Recordings|segment convolut neural network detect cardiac abnorm noisi heart sound record|Heart diseases constitute a global health burden, and the problem is exacerbated by the error-prone nature of listening to and interpreting heart sounds. This motivates the development of automated classification to screen for abnormal heart sounds. Existing machine learning-based systems achieve accurate classification of heart sound recordings but rely on expert features that have not been thoroughly evaluated on noisy recordings. Here we propose a segmental convolutional neural network architecture that achieves automatic feature learning from noisy heart sound recordings. Our experiments show that our best model, trained on noisy recording segments acquired with an existing hidden semi-markov model-based approach, attains a classification accuracy of 87.5% on the 2016 PhysioNet/CinC Challenge dataset, compared to the 84.6% accuracy of the state-of-the-art statistical classifier trained and evaluated on the same dataset. Our results indicate the potential of using neural network-based methods to increase the accuracy of automated classification of heart sound recordings for improved screening of heart diseases.|heart diseas constitut global health burden problem exacerb error prone natur listen interpret heart sound motiv develop autom classif screen abnorm heart sound exist machin learn base system achiev accur classif heart sound record reli expert featur thorough evalu noisi record propos segment convolut neural network architectur achiev automat featur learn noisi heart sound record experi show best model train noisi record segment acquir exist hidden semi markov model base approach attain classif accuraci physionet cinc challeng dataset compar accuraci state art statist classifi train evalu dataset result indic potenti use neural network base method increas accuraci autom classif heart sound record improv screen heart diseas|['Yuhao Zhang', 'Sandeep Ayyar', 'Long-Huei Chen', 'Ethan J. Li']|['cs.SD', 'cs.LG', 'stat.ML']
2017-03-28T14:11:25Z|2016-12-06T14:58:59Z|http://arxiv.org/abs/1612.01840v1|http://arxiv.org/pdf/1612.01840v1|FMA: A Dataset For Music Analysis|fma dataset music analysi|We present a new music dataset that can be used for several music analysis tasks. Our major goal is to go beyond the existing limitations of available music datasets, which are either the small size of datasets with raw audio tracks, the availability and legality of the music data, or the lack of meta-data for artists analysis or song ratings for recommender systems. Existing datasets such as GTZAN, TagATune, and Million Song suffer from the previous limitations. It is however essential to establish such benchmark datasets to advance the field of music analysis, like the ImageNet dataset which made possible the large success of deep learning techniques in computer vision. In this paper, we introduce the Free Music Archive (FMA) which contains 77,643 songs and 68 genres spanning 26.9 days of song listening and meta-data including artist name, song title, music genre, and track counts. For research purposes, we define two additional datasets from the original one: a small genre-balanced dataset of 4,000 song data and 10 genres compassing 33.3 hours of raw audio and a medium genre-unbalanced dataset of 14,511 data and 20 genres offering 5.1 days of track listening, both datasets come with meta-data and Echonest audio features. For all datasets, we provide a train-test splitting for future algorithms' comparisons.|present new music dataset use sever music analysi task major goal go beyond exist limit avail music dataset either small size dataset raw audio track avail legal music data lack meta data artist analysi song rate recommend system exist dataset gtzan tagatun million song suffer previous limit howev essenti establish benchmark dataset advanc field music analysi like imagenet dataset made possibl larg success deep learn techniqu comput vision paper introduc free music archiv fma contain song genr span day song listen meta data includ artist name song titl music genr track count research purpos defin two addit dataset origin one small genr balanc dataset song data genr compass hour raw audio medium genr unbalanc dataset data genr offer day track listen dataset come meta data echonest audio featur dataset provid train test split futur algorithm comparison|['Kirell Benzi', 'Micha√´l Defferrard', 'Pierre Vandergheynst', 'Xavier Bresson']|['cs.SD', 'cs.IR']
2017-03-28T14:11:25Z|2017-01-31T17:33:34Z|http://arxiv.org/abs/1612.01860v4|http://arxiv.org/pdf/1612.01860v4|An algorithm to assign musical prime commas to every prime number and   construct a universal and compact free Just Intonation musical notation|algorithm assign music prime comma everi prime number construct univers compact free inton music notat|Musical frequencies in Just Intonation are comprised of rational numbers. The structure of rational numbers is determined by prime factorisations. Just Intonation frequencies can be split into two components. The larger component uses only integer powers of the first two primes, 2 and 3. The smaller component decomposes into a series of microtonal adjustments, one for each prime number 5 and above present in the original frequency. The larger 3-limit component can be notated using scientific pitch notation modified to use Pythagorean tuning. The microtonal adjustments can be notated using rational commas which are built up from prime commas. This gives a notation system for the whole of free-JI, called Rational Comma Notation. RCN is compact since all microtonal adjustments can be represented by a single notational unit based on a rational number. RCN has different versions depending on the choice of algorithm to assign a prime comma to each prime number. Two existing algorithms SAG and KG are found in the literature. A novel algorithm DR is developed based on discussion of mathematical and musical criteria for algorithm design. Results for DR are presented for primes below 1400. Some observations are made about these results and their applications, including shorthand notation and pitch class lattices. Results for DR are compared with those for SAG and KG. Translation is possible between any two free-JI notations and any two versions of RCN since they all represent the same underlying set of rational numbers.|music frequenc inton compris ration number structur ration number determin prime factoris inton frequenc split two compon larger compon use onli integ power first two prime smaller compon decompos seri microton adjust one prime number abov present origin frequenc larger limit compon notat use scientif pitch notat modifi use pythagorean tune microton adjust notat use ration comma built prime comma give notat system whole free ji call ration comma notat rcn compact sinc microton adjust repres singl notat unit base ration number rcn differ version depend choic algorithm assign prime comma prime number two exist algorithm sag kg found literatur novel algorithm dr develop base discuss mathemat music criteria algorithm design result dr present prime observ made result applic includ shorthand notat pitch class lattic result dr compar sag kg translat possibl ani two free ji notat ani two version rcn sinc repres set ration number|['David Ryan']|['cs.SD']
2017-03-28T14:11:25Z|2016-12-04T03:36:51Z|http://arxiv.org/abs/1612.01058v1|http://arxiv.org/pdf/1612.01058v1|Algorithmic Songwriting with ALYSIA|algorithm songwrit alysia|This paper introduces ALYSIA: Automated LYrical SongwrIting Application. ALYSIA is based on a machine learning model using Random Forests, and we discuss its success at pitch and rhythm prediction. Next, we show how ALYSIA was used to create original pop songs that were subsequently recorded and produced. Finally, we discuss our vision for the future of Automated Songwriting for both co-creative and autonomous systems.|paper introduc alysia autom lyric songwrit applic alysia base machin learn model use random forest discuss success pitch rhythm predict next show alysia use creat origin pop song subsequ record produc final discuss vision futur autom songwrit co creativ autonom system|['Margareta Ackerman', 'David Loker']|['cs.AI', 'cs.LG', 'cs.MM', 'cs.SD']
2017-03-28T14:11:25Z|2016-12-03T19:17:29Z|http://arxiv.org/abs/1612.01010v1|http://arxiv.org/pdf/1612.01010v1|DeepBach: a Steerable Model for Bach chorales generation|deepbach steerabl model bach choral generat|The composition of polyphonic chorale music in the style of J.S Bach has represented a major challenge in automatic music composition over the last decades. The art of Bach chorales composition involves combining four-part harmony with characteristic rhythmic patterns and typical melodic movements to produce musical phrases which begin, evolve and end (cadences) in a harmonious way. To our knowledge, no model so far was able to solve all these problems simultaneously using an agnostic machine-learning approach. This paper introduces DeepBach, a statistical model aimed at modeling polyphonic music and specifically four parts, hymn-like pieces. We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach. We evaluate how indistinguishable our generated chorales are from existing Bach chorales with a listening test. The results corroborate our claim. A key strength of DeepBach is that it is agnostic and flexible. Users can constrain the generation by imposing some notes, rhythms or cadences in the generated score. This allows users to reharmonize user-defined melodies. DeepBach's generation is fast, making it usable for interactive music composition applications. Several generation examples are provided and discussed from a musical point of view.|composit polyphon choral music style bach repres major challeng automat music composit last decad art bach choral composit involv combin four part harmoni characterist rhythmic pattern typic melod movement produc music phrase begin evolv end cadenc harmoni way knowledg model far abl solv problem simultan use agnost machin learn approach paper introduc deepbach statist model aim model polyphon music specif four part hymn like piec claim train choral harmon johann sebastian bach model capabl generat high convinc choral style bach evalu indistinguish generat choral exist bach choral listen test result corrobor claim key strength deepbach agnost flexibl user constrain generat impos note rhythm cadenc generat score allow user reharmon user defin melodi deepbach generat fast make usabl interact music composit applic sever generat exampl provid discuss music point view|['Ga√´tan Hadjeres', 'Fran√ßois Pachet']|['cs.AI', 'cs.SD']
2017-03-28T14:11:25Z|2016-12-02T22:02:04Z|http://arxiv.org/abs/1612.00876v1|http://arxiv.org/pdf/1612.00876v1|FRIDA: FRI-Based DOA Estimation for Arbitrary Array Layouts|frida fri base doa estim arbitrari array layout|In this paper we present FRIDA---an algorithm for estimating directions of arrival of multiple wideband sound sources. FRIDA combines multi-band information coherently and achieves state-of-the-art resolution at extremely low signal-to-noise ratios. It works for arbitrary array layouts, but unlike the various steered response power and subspace methods, it does not require a grid search. FRIDA leverages recent advances in sampling signals with a finite rate of innovation. It is based on the insight that for any array layout, the entries of the spatial covariance matrix can be linearly transformed into a uniformly sampled sum of sinusoids.|paper present frida algorithm estim direct arriv multipl wideband sound sourc frida combin multi band inform coher achiev state art resolut extrem low signal nois ratio work arbitrari array layout unlik various steer respons power subspac method doe requir grid search frida leverag recent advanc sampl signal finit rate innov base insight ani array layout entri spatial covari matrix linear transform uniform sampl sum sinusoid|['Hanjie Pan', 'Robin Scheibler', 'Eric Bezzam', 'Ivan Dokmanic', 'Martin Vetterli']|['cs.SD']
2017-03-28T14:11:25Z|2016-12-01T08:31:23Z|http://arxiv.org/abs/1612.00172v1|http://arxiv.org/pdf/1612.00172v1|A Non Linear Approach towards Automated Emotion Analysis in Hindustani   Music|non linear approach toward autom emot analysi hindustani music|In North Indian Classical Music, raga forms the basic structure over which individual improvisations is performed by an artist based on his/her creativity. The Alap is the opening section of a typical Hindustani Music (HM) performance, where the raga is introduced and the paths of its development are revealed using all the notes used in that particular raga and allowed transitions between them with proper distribution over time. In India, corresponding to each raga, several emotional flavors are listed, namely erotic love, pathetic, devotional, comic, horrific, repugnant, heroic, fantastic, furious, peaceful. The detection of emotional cues from Hindustani Classical music is a demanding task due to the inherent ambiguity present in the different ragas, which makes it difficult to identify any particular emotion from a certain raga. In this study we took the help of a high resolution mathematical microscope (MFDFA or Multifractal Detrended Fluctuation Analysis) to procure information about the inherent complexities and time series fluctuations that constitute an acoustic signal. With the help of this technique, 3 min alap portion of six conventional ragas of Hindustani classical music namely, Darbari Kanada, Yaman, Mian ki Malhar, Durga, Jay Jayanti and Hamswadhani played in three different musical instruments were analyzed. The results are discussed in detail.|north indian classic music raga form basic structur individu improvis perform artist base creativ alap open section typic hindustani music hm perform raga introduc path develop reveal use note use particular raga allow transit proper distribut time india correspond raga sever emot flavor list name erot love pathet devot comic horrif repugn heroic fantast furious peac detect emot cue hindustani classic music demand task due inher ambigu present differ raga make difficult identifi ani particular emot certain raga studi took help high resolut mathemat microscop mfdfa multifract detrend fluctuat analysi procur inform inher complex time seri fluctuat constitut acoust signal help techniqu min alap portion six convent raga hindustani classic music name darbari kanada yaman mian ki malhar durga jay jayanti hamswadhani play three differ music instrument analyz result discuss detail|['Shankha Sanyal', 'Archi Banerjee', 'Tarit Guhathakurata', 'Ranjan Sengupta', 'Dipak Ghosh']|['cs.SD', 'nlin.CD']
2017-03-28T14:11:30Z|2016-12-01T08:25:45Z|http://arxiv.org/abs/1612.00171v1|http://arxiv.org/pdf/1612.00171v1|A Non Linear Multifractal Study to Illustrate the Evolution of Tagore   Songs Over a Century|non linear multifract studi illustr evolut tagor song centuri|The works of Rabindranath Tagore have been sung by various artistes over generations spanning over almost 100 years. there are few songs which were popular in the early years and have been able to retain their popularity over the years while some others have faded away. In this study we look to find cues for the singing style of these songs which have kept them alive for all these years. For this we took 3 min clip of four Tagore songs which have been sung by five generation of artistes over 100 years and analyze them with the help of latest nonlinear techniques Multifractal Detrended Fluctuation Analysis (MFDFA). The multifractal spectral width is a manifestation of the inherent complexity of the signal and may prove to be an important parameter to identify the singing style of particular generation of singers and how this style varies over different generations. The results are discussed in detail.|work rabindranath tagor sung various artist generat span almost year song popular earli year abl retain popular year fade away studi look find cue sing style song kept aliv year took min clip four tagor song sung five generat artist year analyz help latest nonlinear techniqu multifract detrend fluctuat analysi mfdfa multifract spectral width manifest inher complex signal may prove import paramet identifi sing style particular generat singer style vari differ generat result discuss detail|['Shankha Sanyal', 'Archi Banerjee', 'Tarit Guhathakurata', 'Ranjan Sengupta', 'Dipak Ghosh']|['cs.SD', 'nlin.CD']
2017-03-28T14:11:30Z|2016-11-29T20:26:00Z|http://arxiv.org/abs/1611.09827v1|http://arxiv.org/pdf/1611.09827v1|Learning Features of Music from Scratch|learn featur music scratch|"We introduce a new large-scale music dataset, MusicNet, to serve as a source of supervision and evaluation of machine learning methods for music research. MusicNet consists of hundreds of freely-licensed classical music recordings by 10 composers, written for 11 instruments, together with instrument/note annotations resulting in over 1 million temporal labels on 34 hours of chamber music performances under various studio and microphone conditions.   We define a multi-label classification task to predict notes in musical recordings, along with an evaluation protocol. We benchmark several machine learning architectures for this task: i) learning from ""hand-crafted"" spectrogram features; ii) end-to-end learning with a neural net; iii) end-to-end learning with a convolutional neural net. We show that several end-to-end learning proposals outperform approaches based on learning from hand-crafted audio features."|introduc new larg scale music dataset musicnet serv sourc supervis evalu machin learn method music research musicnet consist hundr freeli licens classic music record compos written instrument togeth instrument note annot result million tempor label hour chamber music perform various studio microphon condit defin multi label classif task predict note music record along evalu protocol benchmark sever machin learn architectur task learn hand craft spectrogram featur ii end end learn neural net iii end end learn convolut neural net show sever end end learn propos outperform approach base learn hand craft audio featur|['John Thickstun', 'Zaid Harchaoui', 'Sham Kakade']|['stat.ML', 'cs.LG', 'cs.SD']
2017-03-28T14:11:30Z|2016-11-29T17:19:45Z|http://arxiv.org/abs/1611.09733v1|http://arxiv.org/abs/1611.09733v1|Getting Closer to the Essence of Music: The Con Espressione Manifesto|get closer essenc music con espression manifesto|This text offers a personal and very subjective view on the current situation of Music Information Research (MIR). Motivated by the desire to build systems with a somewhat deeper understanding of music than the ones we currently have, I try to sketch a number of challenges for the next decade of MIR research, grouped around six simple truths about music that are probably generally agreed on, but often ignored in everyday research.|text offer person veri subject view current situat music inform research mir motiv desir build system somewhat deeper understand music one current tri sketch number challeng next decad mir research group around six simpl truth music probabl general agre often ignor everyday research|['Gerhard Widmer']|['cs.SD']
2017-03-28T14:11:30Z|2016-11-29T08:46:26Z|http://arxiv.org/abs/1611.09526v1|http://arxiv.org/pdf/1611.09526v1|Learning Filter Banks Using Deep Learning For Acoustic Signals|learn filter bank use deep learn acoust signal|Designing appropriate features for acoustic event recognition tasks is an active field of research. Expressive features should both improve the performance of the tasks and also be interpret-able. Currently, heuristically designed features based on the domain knowledge requires tremendous effort in hand-crafting, while features extracted through deep network are difficult for human to interpret. In this work, we explore the experience guided learning method for designing acoustic features. This is a novel hybrid approach combining both domain knowledge and purely data driven feature designing. Based on the procedure of log Mel-filter banks, we design a filter bank learning layer. We concatenate this layer with a convolutional neural network (CNN) model. After training the network, the weight of the filter bank learning layer is extracted to facilitate the design of acoustic features. We smooth the trained weight of the learning layer and re-initialize it in filter bank learning layer as audio feature extractor. For the environmental sound recognition task based on the Urban- sound8K dataset, the experience guided learning leads to a 2% accuracy improvement compared with the fixed feature extractors (the log Mel-filter bank). The shape of the new filter banks are visualized and explained to prove the effectiveness of the feature design process.|design appropri featur acoust event recognit task activ field research express featur improv perform task also interpret abl current heurist design featur base domain knowledg requir tremend effort hand craft featur extract deep network difficult human interpret work explor experi guid learn method design acoust featur novel hybrid approach combin domain knowledg pure data driven featur design base procedur log mel filter bank design filter bank learn layer concaten layer convolut neural network cnn model train network weight filter bank learn layer extract facilit design acoust featur smooth train weight learn layer initi filter bank learn layer audio featur extractor environment sound recognit task base urban soundk dataset experi guid learn lead accuraci improv compar fix featur extractor log mel filter bank shape new filter bank visual explain prove effect featur design process|['Shuhui Qu', 'Juncheng Li', 'Wei Dai', 'Samarjit Das']|['cs.SD', 'cs.AI']
2017-03-28T14:11:30Z|2016-11-29T08:33:48Z|http://arxiv.org/abs/1611.09524v1|http://arxiv.org/pdf/1611.09524v1|Understanding Audio Pattern Using Convolutional Neural Network From Raw   Waveforms|understand audio pattern use convolut neural network raw waveform|One key step in audio signal processing is to transform the raw signal into representations that are efficient for encoding the original information. Traditionally, people transform the audio into spectral representations, as a function of frequency, amplitude and phase transformation. In this work, we take a purely data-driven approach to understand the temporal dynamics of audio at the raw signal level. We maximize the information extracted from the raw signal through a deep convolutional neural network (CNN) model. Our CNN model is trained on the urbansound8k dataset. We discover that salient audio patterns embedded in the raw waveforms can be efficiently extracted through a combination of nonlinear filters learned by the CNN model.|one key step audio signal process transform raw signal represent effici encod origin inform tradit peopl transform audio spectral represent function frequenc amplitud phase transform work take pure data driven approach understand tempor dynam audio raw signal level maxim inform extract raw signal deep convolut neural network cnn model cnn model train urbansoundk dataset discov salient audio pattern embed raw waveform effici extract combin nonlinear filter learn cnn model|['Shuhui Qu', 'Juncheng Li', 'Wei Dai', 'Samarjit Das']|['cs.SD']
2017-03-28T14:11:30Z|2016-11-29T04:16:44Z|http://arxiv.org/abs/1611.09482v1|http://arxiv.org/pdf/1611.09482v1|Fast Wavenet Generation Algorithm|fast wavenet generat algorithm|This paper presents an efficient implementation of the Wavenet generation process called Fast Wavenet. Compared to a naive implementation that has complexity O(2^L) (L denotes the number of layers in the network), our proposed approach removes redundant convolution operations by caching previous calculations, thereby reducing the complexity to O(L) time. Timing experiments show significant advantages of our fast implementation over a naive one. While this method is presented for Wavenet, the same scheme can be applied anytime one wants to perform autoregressive generation or online prediction using a model with dilated convolution layers. The code for our method is publicly available.|paper present effici implement wavenet generat process call fast wavenet compar naiv implement complex denot number layer network propos approach remov redund convolut oper cach previous calcul therebi reduc complex time time experi show signific advantag fast implement naiv one method present wavenet scheme appli anytim one want perform autoregress generat onlin predict use model dilat convolut layer code method public avail|['Tom Le Paine', 'Pooya Khorrami', 'Shiyu Chang', 'Yang Zhang', 'Prajit Ramachandran', 'Mark A. Hasegawa-Johnson', 'Thomas S. Huang']|['cs.SD', 'cs.DS', 'cs.LG']
2017-03-28T14:11:30Z|2016-11-27T22:47:23Z|http://arxiv.org/abs/1611.08930v1|http://arxiv.org/pdf/1611.08930v1|Deep attractor network for single-microphone speaker separation|deep attractor network singl microphon speaker separ|Despite the overwhelming success of deep learning in various speech processing tasks, the problem of separating simultaneous speakers in a mixture remains challenging. Two major difficulties in such systems are the arbitrary source permutation and unknown number of sources in the mixture. We propose a novel deep learning framework for single channel speech separation by creating attractor points in high dimensional embedding space of the acoustic signals which pull together the time-frequency bins corresponding to each source. Attractor points in this study are created by finding the centroids of the sources in the embedding space, which are subsequently used to determine the similarity of each bin in the mixture to each source. The network is then trained to minimize the reconstruction error of each source by optimizing the embeddings. The proposed model is different from prior works in that it implements an end-to-end training, and it does not depend on the number of sources in the mixture. Two strategies are explored in the test time, K-means and fixed attractor points, where the latter requires no post-processing and can be implemented in real-time. We evaluated our system on Wall Street Journal dataset and show 5.49\% improvement over the previous state-of-the-art methods.|despit overwhelm success deep learn various speech process task problem separ simultan speaker mixtur remain challeng two major difficulti system arbitrari sourc permut unknown number sourc mixtur propos novel deep learn framework singl channel speech separ creat attractor point high dimension embed space acoust signal pull togeth time frequenc bin correspond sourc attractor point studi creat find centroid sourc embed space subsequ use determin similar bin mixtur sourc network train minim reconstruct error sourc optim embed propos model differ prior work implement end end train doe depend number sourc mixtur two strategi explor test time mean fix attractor point latter requir post process implement real time evalu system wall street journal dataset show improv previous state art method|['Zhuo Chen', 'Yi Luo', 'Nima Mesgarani']|['cs.SD', 'cs.LG']
2017-03-28T14:11:30Z|2016-11-27T22:20:51Z|http://arxiv.org/abs/1612.01928v1|http://arxiv.org/pdf/1612.01928v1|Invariant Representations for Noisy Speech Recognition|invari represent noisi speech recognit|Modern automatic speech recognition (ASR) systems need to be robust under acoustic variability arising from environmental, speaker, channel, and recording conditions. Ensuring such robustness to variability is a challenge in modern day neural network-based ASR systems, especially when all types of variability are not seen during training. We attempt to address this problem by encouraging the neural network acoustic model to learn invariant feature representations. We use ideas from recent research on image generation using Generative Adversarial Networks and domain adaptation ideas extending adversarial gradient-based training. A recent work from Ganin et al. proposes to use adversarial training for image domain adaptation by using an intermediate representation from the main target classification network to deteriorate the domain classifier performance through a separate neural network. Our work focuses on investigating neural architectures which produce representations invariant to noise conditions for ASR. We evaluate the proposed architecture on the Aurora-4 task, a popular benchmark for noise robust ASR. We show that our method generalizes better than the standard multi-condition training especially when only a few noise categories are seen during training.|modern automat speech recognit asr system need robust acoust variabl aris environment speaker channel record condit ensur robust variabl challeng modern day neural network base asr system especi type variabl seen dure train attempt address problem encourag neural network acoust model learn invari featur represent use idea recent research imag generat use generat adversari network domain adapt idea extend adversari gradient base train recent work ganin et al propos use adversari train imag domain adapt use intermedi represent main target classif network deterior domain classifi perform separ neural network work focus investig neural architectur produc represent invari nois condit asr evalu propos architectur aurora task popular benchmark nois robust asr show method general better standard multi condit train especi onli nois categori seen dure train|['Dmitriy Serdyuk', 'Kartik Audhkhasi', 'Phil√©mon Brakel', 'Bhuvana Ramabhadran', 'Samuel Thomas', 'Yoshua Bengio']|['cs.CL', 'cs.CV', 'cs.LG', 'cs.SD', 'stat.ML']
2017-03-28T14:11:30Z|2016-11-27T20:29:53Z|http://arxiv.org/abs/1611.08905v1|http://arxiv.org/pdf/1611.08905v1|SISO and SIMO Accompaniment Cancellation for Live Solo Recordings Based   on Short-Time ERB-Band Wiener Filtering and Spectral Subtraction|siso simo accompani cancel live solo record base short time erb band wiener filter spectral subtract|Research in collaborative music learning is subject to unresolved problems demanding new technological solutions. One such problem poses the suppression of the accompaniment in a live recording of a performance during practice, which can be for the purposes of self-assessment or further machine-aided analysis. Being able to separate a solo from the accompaniment allows to create learning agents that may act as personal tutors and help the apprentice improve his or her technique. First, we start from the classical adaptive noise cancelling approach, and adjust it to the problem at hand. In a second step, we compare some adaptive and Wiener filtering approaches and assess their performances on the task. Our findings underpin that adaptive filtering is inapt of dealing with music signals and that Wiener filtering in the short-time Fourier transform domain is a much more effective approach. In addition, it is very cheap if carried out in the frequency bands of auditory filters. A double-output extension based on maximal-ratio combining is also proposed.|research collabor music learn subject unresolv problem demand new technolog solut one problem pose suppress accompani live record perform dure practic purpos self assess machin aid analysi abl separ solo accompani allow creat learn agent may act person tutor help apprentic improv techniqu first start classic adapt nois cancel approach adjust problem hand second step compar adapt wiener filter approach assess perform task find underpin adapt filter inapt deal music signal wiener filter short time fourier transform domain much effect approach addit veri cheap carri frequenc band auditori filter doubl output extens base maxim ratio combin also propos|['Stanislaw Gorlow', 'Mathieu Ramona', 'Fran√ßois Pachet']|['cs.SD']
2017-03-28T14:11:30Z|2017-01-22T22:28:47Z|http://arxiv.org/abs/1611.08749v2|http://arxiv.org/pdf/1611.08749v2|Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on   Animal calls and Speech|fast chirplet transform enhanc cnn machin listen valid anim call speech|The scattering framework offers an optimal hierarchical convolutional decomposition according to its kernels. Convolutional Neural Net (CNN) can be seen as an optimal kernel decomposition, nevertheless it requires large amount of training data to learn its kernels. We propose a trade-off between these two approaches: a Chirplet kernel as an efficient Q constant bioacoustic representation to pretrain CNN. First we motivate Chirplet bioinspired auditory representation. Second we give the first algorithm (and code) of a Fast Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of FCT on large environmental data base: months of Orca recordings, and 1000 Birds species from the LifeClef challenge. Fourth, we validate FCT on the vowels subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN when it pretrains low level layers: it reduces training duration by -28\% for birds classification, and by -26% for vowels classification. Scores are also enhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average Precision on birds, and +2.3\% of vowel accuracy against raw audio CNN. We conclude on perspectives on tonotopic FCT deep machine listening, and inter-species bioacoustic transfer learning to generalise the representation of animal communication systems.|scatter framework offer optim hierarch convolut decomposit accord kernel convolut neural net cnn seen optim kernel decomposit nevertheless requir larg amount train data learn kernel propos trade two approach chirplet kernel effici constant bioacoust represent pretrain cnn first motiv chirplet bioinspir auditori represent second give first algorithm code fast chirplet transform fct third demonstr comput effici fct larg environment data base month orca record bird speci lifeclef challeng fourth valid fct vowel subset speech timit dataset result show fct acceler cnn pretrain low level layer reduc train durat bird classif vowel classif score also enhanc fct pretrain relat gain mean averag precis bird vowel accuraci raw audio cnn conclud perspect tonotop fct deep machin listen inter speci bioacoust transfer learn generalis represent anim communic system|['Herve Glotin', 'Julien Ricard', 'Randall Balestriero']|['cs.SD']
