2017-03-28T14:01:45Z|2017-03-27T17:50:53Z|http://arxiv.org/abs/1703.09207v1|http://arxiv.org/pdf/1703.09207v1|Fairness in Criminal Justice Risk Assessments: The State of the Art|fair crimin justic risk assess state art|Objectives: Discussions of fairness in criminal justice risk assessments typically lack conceptual precision. Rhetoric too often substitutes for careful analysis. In this paper, we seek to clarify the tradeoffs between different kinds of fairness and between fairness and accuracy.   Methods: We draw on the existing literatures in criminology, computer science and statistics to provide an integrated examination of fairness and accuracy in criminal justice risk assessments. We also provide an empirical illustration using data from arraignments.   Results: We show that there are at least six kinds of fairness, some of which are incompatible with one another and with accuracy.   Conclusions: Except in trivial cases, it is impossible to maximize accuracy and fairness at the same time, and impossible simultaneously to satisfy all kinds of fairness. In practice, a major complication is different base rates across different legally protected groups. There is a need to consider challenging tradeoffs.|object discuss fair crimin justic risk assess typic lack conceptu precis rhetor often substitut care analysi paper seek clarifi tradeoff differ kind fair fair accuraci method draw exist literatur criminolog comput scienc statist provid integr examin fair accuraci crimin justic risk assess also provid empir illustr use data arraign result show least six kind fair incompat one anoth accuraci conclus except trivial case imposs maxim accuraci fair time imposs simultan satisfi kind fair practic major complic differ base rate across differ legal protect group need consid challeng tradeoff|['Richard Berk', 'Hoda Heidari', 'Shahin Jabbari', 'Michael Kearns', 'Aaron Roth']|['stat.ML']
2017-03-28T14:01:45Z|2017-03-27T17:45:07Z|http://arxiv.org/abs/1703.09202v1|http://arxiv.org/pdf/1703.09202v1|Biologically inspired protection of deep networks from adversarial   attacks|biolog inspir protect deep network adversari attack|Inspired by biophysical principles underlying nonlinear dendritic computation in neural circuits, we develop a scheme to train deep neural networks to make them robust to adversarial attacks. Our scheme generates highly nonlinear, saturated neural networks that achieve state of the art performance on gradient based adversarial examples on MNIST, despite never being exposed to adversarially chosen examples during training. Moreover, these networks exhibit unprecedented robustness to targeted, iterative schemes for generating adversarial examples, including second-order methods. We further identify principles governing how these networks achieve their robustness, drawing on methods from information geometry. We find these networks progressively create highly flat and compressed internal representations that are sensitive to very few input dimensions, while still solving the task. Moreover, they employ highly kurtotic weight distributions, also found in the brain, and we demonstrate how such kurtosis can protect even linear classifiers from adversarial attack.|inspir biophys principl nonlinear dendrit comput neural circuit develop scheme train deep neural network make robust adversari attack scheme generat high nonlinear satur neural network achiev state art perform gradient base adversari exampl mnist despit never expos adversari chosen exampl dure train moreov network exhibit unpreced robust target iter scheme generat adversari exampl includ second order method identifi principl govern network achiev robust draw method inform geometri find network progress creat high flat compress intern represent sensit veri input dimens still solv task moreov employ high kurtot weight distribut also found brain demonstr kurtosi protect even linear classifi adversari attack|['Aran Nayebi', 'Surya Ganguli']|['stat.ML', 'cs.LG', 'q-bio.NC']
2017-03-28T14:01:45Z|2017-03-27T17:25:02Z|http://arxiv.org/abs/1703.09194v1|http://arxiv.org/pdf/1703.09194v1|Sticking the Landing: An Asymptotically Zero-Variance Gradient Estimator   for Variational Inference|stick land asymptot zero varianc gradient estim variat infer|We propose a simple and general variant of the standard reparameterized gradient estimator for the variational evidence lower bound. Specifically, we remove a part of the total derivative with respect to the variational parameters that corresponds to the score function. Removing this term produces an unbiased gradient estimator whose variance approaches zero as the approximate posterior approaches the exact posterior. We analyze the behavior of this gradient estimator theoretically and empirically, and generalize it to more complex variational distributions such as mixtures and importance-weighted posteriors.|propos simpl general variant standard reparameter gradient estim variat evid lower bound specif remov part total deriv respect variat paramet correspond score function remov term produc unbias gradient estim whose varianc approach zero approxim posterior approach exact posterior analyz behavior gradient estim theoret empir general complex variat distribut mixtur import weight posterior|['Geoffrey Roeder', 'Yuhuai Wu', 'David Duvenaud']|['stat.ML', 'cs.LG']
2017-03-28T14:01:45Z|2017-03-27T16:16:35Z|http://arxiv.org/abs/1703.09165v1|http://arxiv.org/pdf/1703.09165v1|PWLS-ULTRA: An Efficient Clustering and Learning-Based Approach for   Low-Dose 3D CT Image Reconstruction|pwls ultra effici cluster learn base approach low dose ct imag reconstruct|The development of computed tomography (CT) image reconstruction methods that significantly reduce patient radiation exposure while maintaining high image quality is an important area of research in low-dose CT (LDCT) imaging. We propose a new penalized weighted least squares (PWLS) reconstruction method that exploits regularization based on an efficient Union of Learned TRAnsforms (PWLS-ULTRA). The union of square transforms is pre-learned from numerous image patches extracted from a dataset of CT images or volumes. The proposed PWLS-based cost function is optimized by alternating between an image update step, and a sparse coding and clustering step. The CT image update step is accelerated by a relaxed linearized augmented Lagrangian method with ordered-subsets that reduces the number of forward and backward projections. Simulations with 2D and 3D axial CT scans of the XCAT phantom and 3D helical chest scans show that for low-dose levels, the proposed method significantly improves the quality of reconstructed images compared to PWLS reconstruction with a nonadaptive edge-preserving regularizer (PWLS-EP). PWLS with regularization based on a union of learned transforms leads to better image reconstructions than using a single learned square transform or a learned overcomplete synthesis dictionary. We also incorporate patch-based weights in PWLS-ULTRA that enhance image quality and help improve image resolution uniformity.|develop comput tomographi ct imag reconstruct method signific reduc patient radiat exposur maintain high imag qualiti import area research low dose ct ldct imag propos new penal weight least squar pwls reconstruct method exploit regular base effici union learn transform pwls ultra union squar transform pre learn numer imag patch extract dataset ct imag volum propos pwls base cost function optim altern imag updat step spars code cluster step ct imag updat step acceler relax linear augment lagrangian method order subset reduc number forward backward project simul axial ct scan xcat phantom helic chest scan show low dose level propos method signific improv qualiti reconstruct imag compar pwls reconstruct nonadapt edg preserv regular pwls ep pwls regular base union learn transform lead better imag reconstruct use singl learn squar transform learn overcomplet synthesi dictionari also incorpor patch base weight pwls ultra enhanc imag qualiti help improv imag resolut uniform|['Xuehang Zheng', 'Saiprasad Ravishankar', 'Yong Long', 'Jeffrey A. Fessler']|['stat.ML']
2017-03-28T14:01:45Z|2017-03-27T14:38:15Z|http://arxiv.org/abs/1703.09112v1|http://arxiv.org/pdf/1703.09112v1|Sparse Multi-Output Gaussian Processes for Medical Time Series   Prediction|spars multi output gaussian process medic time seri predict|In real-time monitoring of hospital patients, high-quality inference of patients' health status using all information available from clinical covariates and lab tests are essential to enable successful medical interventions and improve patient outcomes. In this work, we develop and explore a Bayesian nonparametric model based on Gaussian process (GP) regression for hospital patient monitoring. Our method, MedGP, incorporates 24 clinical and lab covariates and supports a rich reference data set from which the relationships between these observed covariates may be inferred and exploited for high-quality inference of patient state over time. To do this, we develop a highly structured sparse GP kernel to enable tractable computation over tens of thousands of time points while estimating correlations among clinical covariates, patients, and periodicity in high-dimensional time series measurements of physiological signals. We apply MedGP to data from hundreds of thousands of patients treated at the Hospital of the University of Pennsylvania. MedGP has a number of benefits over current methods, including (i) not requiring an alignment of the time series data, (ii) quantifying confidence intervals in the predictions, (iii) exploiting a vast and rich database of patients, and (iv) providing interpretable relationships among clinical covariates. We evaluate and compare results from MedGP on the task of online state prediction for three different patient subgroups.|real time monitor hospit patient high qualiti infer patient health status use inform avail clinic covari lab test essenti enabl success medic intervent improv patient outcom work develop explor bayesian nonparametr model base gaussian process gp regress hospit patient monitor method medgp incorpor clinic lab covari support rich refer data set relationship observ covari may infer exploit high qualiti infer patient state time develop high structur spars gp kernel enabl tractabl comput ten thousand time point estim correl among clinic covari patient period high dimension time seri measur physiolog signal appli medgp data hundr thousand patient treat hospit univers pennsylvania medgp number benefit current method includ requir align time seri data ii quantifi confid interv predict iii exploit vast rich databas patient iv provid interpret relationship among clinic covari evalu compar result medgp task onlin state predict three differ patient subgroup|['Li-Fang Cheng', 'Gregory Darnell', 'Corey Chivers', 'Michael E Draugelis', 'Kai Li', 'Barbara E Engelhardt']|['stat.ML']
2017-03-28T14:01:45Z|2017-03-27T10:03:27Z|http://arxiv.org/abs/1703.08991v1|http://arxiv.org/pdf/1703.08991v1|Multilabel Classification with R Package mlr|multilabel classif packag mlr|We implemented several multilabel classification algorithms in the machine learning package mlr. The implemented methods are binary relevance, classifier chains, nested stacking, dependent binary relevance and stacking, which can be used with any base learner that is accessible in mlr. Moreover, there is access to the multilabel classification versions of randomForestSRC and rFerns. All these methods can be easily compared by different implemented multilabel performance measures and resampling methods in the standardized mlr framework. In a benchmark experiment with several multilabel datasets, the performance of the different methods is evaluated.|implement sever multilabel classif algorithm machin learn packag mlr implement method binari relev classifi chain nest stack depend binari relev stack use ani base learner access mlr moreov access multilabel classif version randomforestsrc rfern method easili compar differ implement multilabel perform measur resampl method standard mlr framework benchmark experi sever multilabel dataset perform differ method evalu|['Philipp Probst', 'Quay Au', 'Giuseppe Casalicchio', 'Clemens Stachl', 'Bernd Bischl']|['stat.ML']
2017-03-28T14:01:45Z|2017-03-27T08:45:57Z|http://arxiv.org/abs/1703.08972v1|http://arxiv.org/pdf/1703.08972v1|Thompson Sampling for Linear-Quadratic Control Problems|thompson sampl linear quadrat control problem|We consider the exploration-exploitation tradeoff in linear quadratic (LQ) control problems, where the state dynamics is linear and the cost function is quadratic in states and controls. We analyze the regret of Thompson sampling (TS) (a.k.a. posterior-sampling for reinforcement learning) in the frequentist setting, i.e., when the parameters characterizing the LQ dynamics are fixed. Despite the empirical and theoretical success in a wide range of problems from multi-armed bandit to linear bandit, we show that when studying the frequentist regret TS in control problems, we need to trade-off the frequency of sampling optimistic parameters and the frequency of switches in the control policy. This results in an overall regret of $O(T^{2/3})$, which is significantly worse than the regret $O(\sqrt{T})$ achieved by the optimism-in-face-of-uncertainty algorithm in LQ control problems.|consid explor exploit tradeoff linear quadrat lq control problem state dynam linear cost function quadrat state control analyz regret thompson sampl ts posterior sampl reinforc learn frequentist set paramet character lq dynam fix despit empir theoret success wide rang problem multi arm bandit linear bandit show studi frequentist regret ts control problem need trade frequenc sampl optimist paramet frequenc switch control polici result overal regret signific wors regret sqrt achiev optim face uncertainti algorithm lq control problem|['Marc Abeille', 'Alessandro Lazaric']|['stat.ML']
2017-03-28T14:01:45Z|2017-03-27T05:41:03Z|http://arxiv.org/abs/1703.08937v1|http://arxiv.org/pdf/1703.08937v1|A Scale Free Algorithm for Stochastic Bandits with Bounded Kurtosis|scale free algorithm stochast bandit bound kurtosi|Existing strategies for finite-armed stochastic bandits mostly depend on a parameter of scale that must be known in advance. Sometimes this is in the form of a bound on the payoffs, or the knowledge of a variance or subgaussian parameter. The notable exceptions are the analysis of Gaussian bandits with unknown mean and variance by Cowan and Katehakis [2015] and of uniform distributions with unknown support [Cowan and Katehakis, 2015]. The results derived in these specialised cases are generalised here to the non-parametric setup, where the learner knows only a bound on the kurtosis of the noise, which is a scale free measure of the extremity of outliers.|exist strategi finit arm stochast bandit depend paramet scale must known advanc sometim form bound payoff knowledg varianc subgaussian paramet notabl except analysi gaussian bandit unknown mean varianc cowan katehaki uniform distribut unknown support cowan katehaki result deriv specialis case generalis non parametr setup learner know onli bound kurtosi nois scale free measur extrem outlier|['Tor Lattimore']|['stat.ML']
2017-03-28T14:01:45Z|2017-03-26T16:01:28Z|http://arxiv.org/abs/1703.08831v1|http://arxiv.org/pdf/1703.08831v1|Token-based Function Computation with Memory|token base function comput memori|"In distributed function computation, each node has an initial value and the goal is to compute a function of these values in a distributed manner. In this paper, we propose a novel token-based approach to compute a wide class of target functions to which we refer as ""Token-based function Computation with Memory"" (TCM) algorithm. In this approach, node values are attached to tokens and travel across the network. Each pair of travelling tokens would coalesce when they meet, forming a token with a new value as a function of the original token values. In contrast to the Coalescing Random Walk (CRW) algorithm, where token movement is governed by random walk, meeting of tokens in our scheme is accelerated by adopting a novel chasing mechanism. We proved that, compared to the CRW algorithm, the TCM algorithm results in a reduction of time complexity by a factor of at least $\sqrt{n/\log(n)}$ in Erd\""os-Renyi and complete graphs, and by a factor of $\log(n)/\log(\log(n))$ in torus networks. Simulation results show that there is at least a constant factor improvement in the message complexity of TCM algorithm in all considered topologies. Robustness of the CRW and TCM algorithms in the presence of node failure is analyzed. We show that their robustness can be improved by running multiple instances of the algorithms in parallel."|distribut function comput node initi valu goal comput function valu distribut manner paper propos novel token base approach comput wide class target function refer token base function comput memori tcm algorithm approach node valu attach token travel across network pair travel token would coalesc meet form token new valu function origin token valu contrast coalesc random walk crw algorithm token movement govern random walk meet token scheme acceler adopt novel chase mechan prove compar crw algorithm tcm algorithm result reduct time complex factor least sqrt log erd os renyi complet graph factor log log log torus network simul result show least constant factor improv messag complex tcm algorithm consid topolog robust crw tcm algorithm presenc node failur analyz show robust improv run multipl instanc algorithm parallel|['Saber Salehkaleybar', 'S. Jamaloddin Golestani']|['cs.DC', 'stat.ML']
2017-03-28T14:01:45Z|2017-03-26T13:29:25Z|http://arxiv.org/abs/1703.08816v1|http://arxiv.org/pdf/1703.08816v1|Uncertainty Quantification in the Classification of High Dimensional   Data|uncertainti quantif classif high dimension data|Classification of high dimensional data finds wide-ranging applications. In many of these applications equipping the resulting classification with a measure of uncertainty may be as important as the classification itself. In this paper we introduce, develop algorithms for, and investigate the properties of, a variety of Bayesian models for the task of binary classification; via the posterior distribution on the classification labels, these methods automatically give measures of uncertainty. The methods are all based around the graph formulation of semi-supervised learning.   We provide a unified framework which brings together a variety of methods which have been introduced in different communities within the mathematical sciences. We study probit classification, generalize the level-set method for Bayesian inverse problems to the classification setting, and generalize the Ginzburg-Landau optimization-based classifier to a Bayesian setting; we also show that the probit and level set approaches are natural relaxations of the harmonic function approach.   We introduce efficient numerical methods, suited to large data-sets, for both MCMC-based sampling as well as gradient-based MAP estimation. Through numerical experiments we study classification accuracy and uncertainty quantification for our models; these experiments showcase a suite of datasets commonly used to evaluate graph-based semi-supervised learning algorithms.|classif high dimension data find wide rang applic mani applic equip result classif measur uncertainti may import classif paper introduc develop algorithm investig properti varieti bayesian model task binari classif via posterior distribut classif label method automat give measur uncertainti method base around graph formul semi supervis learn provid unifi framework bring togeth varieti method introduc differ communiti within mathemat scienc studi probit classif general level set method bayesian invers problem classif set general ginzburg landau optim base classifi bayesian set also show probit level set approach natur relax harmon function approach introduc effici numer method suit larg data set mcmc base sampl well gradient base map estim numer experi studi classif accuraci uncertainti quantif model experi showcas suit dataset common use evalu graph base semi supervis learn algorithm|['Andrea L. Bertozzi', 'Xiyang Luo', 'Andrew M. Stuart', 'Konstantinos C. Zygalakis']|['cs.LG', 'stat.ML']
2017-03-28T14:01:49Z|2017-03-26T05:53:39Z|http://arxiv.org/abs/1703.08772v1|http://arxiv.org/pdf/1703.08772v1|Multivariate Regression with Gross Errors on Manifold-valued Data|multivari regress gross error manifold valu data|We consider the topic of multivariate regression on manifold-valued output, that is, for a multivariate observation, its output response lies on a manifold. Moreover, we propose a new regression model to deal with the presence of grossly corrupted manifold-valued responses, a bottleneck issue commonly encountered in practical scenarios. Our model first takes a correction step on the grossly corrupted responses via geodesic curves on the manifold, and then performs multivariate linear regression on the corrected data. This results in a nonconvex and nonsmooth optimization problem on manifolds. To this end, we propose a dedicated approach named PALMR, by utilizing and extending the proximal alternating linearized minimization techniques. Theoretically, we investigate its convergence property, where it is shown to converge to a critical point under mild conditions. Empirically, we test our model on both synthetic and real diffusion tensor imaging data, and show that our model outperforms other multivariate regression models when manifold-valued responses contain gross errors, and is effective in identifying gross errors.|consid topic multivari regress manifold valu output multivari observ output respons lie manifold moreov propos new regress model deal presenc grossli corrupt manifold valu respons bottleneck issu common encount practic scenario model first take correct step grossli corrupt respons via geodes curv manifold perform multivari linear regress correct data result nonconvex nonsmooth optim problem manifold end propos dedic approach name palmr util extend proxim altern linear minim techniqu theoret investig converg properti shown converg critic point mild condit empir test model synthet real diffus tensor imag data show model outperform multivari regress model manifold valu respons contain gross error effect identifi gross error|['Xiaowei Zhang', 'Xudong Shi', 'Yu Sun', 'Li Cheng']|['stat.ML', 'cs.CV', 'math.OC']
2017-03-28T14:01:49Z|2017-03-25T20:06:10Z|http://arxiv.org/abs/1703.08737v1|http://arxiv.org/pdf/1703.08737v1|Learning to Predict: A Fast Re-constructive Method to Generate   Multimodal Embeddings|learn predict fast construct method generat multimod embed|"Integrating visual and linguistic information into a single multimodal representation is an unsolved problem with wide-reaching applications to both natural language processing and computer vision. In this paper, we present a simple method to build multimodal representations by learning a language-to-vision mapping and using its output to build multimodal embeddings. In this sense, our method provides a cognitively plausible way of building representations, consistent with the inherently re-constructive and associative nature of human memory. Using seven benchmark concept similarity tests we show that the mapped vectors not only implicitly encode multimodal information, but also outperform strong unimodal baselines and state-of-the-art multimodal methods, thus exhibiting more ""human-like"" judgments---particularly in zero-shot settings."|integr visual linguist inform singl multimod represent unsolv problem wide reach applic natur languag process comput vision paper present simpl method build multimod represent learn languag vision map use output build multimod embed sens method provid cognit plausibl way build represent consist inher construct associ natur human memori use seven benchmark concept similar test show map vector onli implicit encod multimod inform also outperform strong unimod baselin state art multimod method thus exhibit human like judgment particular zero shot set|['Guillem Collell', 'Teddy Zhang', 'Marie-Francine Moens']|['stat.ML']
2017-03-28T14:01:49Z|2017-03-25T18:45:55Z|http://arxiv.org/abs/1703.08729v1|http://arxiv.org/pdf/1703.08729v1|Solving SDPs for synchronization and MaxCut problems via the   Grothendieck inequality|solv sdps synchron maxcut problem via grothendieck inequ|A number of statistical estimation problems can be addressed by semidefinite programs (SDP). While SDPs are solvable in polynomial time using interior point methods, in practice generic SDP solvers do not scale well to high-dimensional problems. In order to cope with this problem, Burer and Monteiro proposed a non-convex rank-constrained formulation, which has good performance in practice but is still poorly understood theoretically.   In this paper we study the rank-constrained version of SDPs arising in MaxCut and in synchronization problems. We establish a Grothendieck-type inequality that proves that all the local maxima and dangerous saddle points are within a small multiplicative gap from the global maximum. We use this structural information to prove that SDPs can be solved within a known accuracy, by applying the Riemannian trust-region method to this non-convex problem, while constraining the rank to be of order one. For the MaxCut problem, our inequality implies that any local maximizer of the rank-constrained SDP provides a $ (1 - 1/(k-1)) \times 0.878$ approximation of the MaxCut, when the rank is fixed to $k$.   We then apply our results to data matrices generated according to the Gaussian ${\mathbb Z}_2$ synchronization problem, and the two-groups stochastic block model with large bounded degree. We prove that the error achieved by local maximizers undergoes a phase transition at the same threshold as for information-theoretically optimal methods.|number statist estim problem address semidefinit program sdp sdps solvabl polynomi time use interior point method practic generic sdp solver scale well high dimension problem order cope problem burer monteiro propos non convex rank constrain formul good perform practic still poor understood theoret paper studi rank constrain version sdps aris maxcut synchron problem establish grothendieck type inequ prove local maxima danger saddl point within small multipl gap global maximum use structur inform prove sdps solv within known accuraci appli riemannian trust region method non convex problem constrain rank order one maxcut problem inequ impli ani local maxim rank constrain sdp provid time approxim maxcut rank fix appli result data matric generat accord gaussian mathbb synchron problem two group stochast block model larg bound degre prove error achiev local maxim undergo phase transit threshold inform theoret optim method|['Song Mei', 'Theodor Misiakiewicz', 'Andrea Montanari', 'Roberto I. Oliveira']|['math.OC', 'stat.ML']
2017-03-28T14:01:49Z|2017-03-25T16:49:03Z|http://arxiv.org/abs/1703.08710v1|http://arxiv.org/pdf/1703.08710v1|Count-ception: Counting by Fully Convolutional Redundant Counting|count ception count fulli convolut redund count|Counting objects in digital images is a process that should be replaced by machines. This tedious task is time consuming and prone to errors due to fatigue of human annotators. The goal is to have a system that takes as input an image and returns a count of the objects inside and justification for the prediction in the form of object localization. We repose a problem, originally posed by Lempitsky and Zisserman, to instead predict a count map which contains redundant counts based on the receptive field of a smaller regression network. The regression network predicts a count of the objects that exist inside this frame. By processing the image in a fully convolutional way each pixel is going to be accounted for some number of times, the number of windows which include it, which is the size of each window, (i.e., 32x32 = 1024). To recover the true count take the average over the redundant predictions. Our contribution is redundant counting instead of predicting a density map in order to average over errors. We also propose a novel deep neural network architecture adapted from the Inception family of networks called the Count-ception network. Together our approach results in a 20% gain over the state of the art method by Xie, Noble, and Zisserman in 2016.|count object digit imag process replac machin tedious task time consum prone error due fatigu human annot goal system take input imag return count object insid justif predict form object local repos problem origin pose lempitski zisserman instead predict count map contain redund count base recept field smaller regress network regress network predict count object exist insid frame process imag fulli convolut way pixel go account number time number window includ size window recov true count take averag redund predict contribut redund count instead predict densiti map order averag error also propos novel deep neural network architectur adapt incept famili network call count ception network togeth approach result gain state art method xie nobl zisserman|['Joseph Paul Cohen', 'Henry Z. Lo', 'Yoshua Bengio']|['cs.CV', 'cs.LG', 'stat.ML']
2017-03-28T14:01:49Z|2017-03-25T15:37:09Z|http://arxiv.org/abs/1703.08705v1|http://arxiv.org/pdf/1703.08705v1|Comparing Rule-Based and Deep Learning Models for Patient Phenotyping|compar rule base deep learn model patient phenotyp|Objective: We investigate whether deep learning techniques for natural language processing (NLP) can be used efficiently for patient phenotyping. Patient phenotyping is a classification task for determining whether a patient has a medical condition, and is a crucial part of secondary analysis of healthcare data. We assess the performance of deep learning algorithms and compare them with classical NLP approaches.   Materials and Methods: We compare convolutional neural networks (CNNs), n-gram models, and approaches based on cTAKES that extract pre-defined medical concepts from clinical notes and use them to predict patient phenotypes. The performance is tested on 10 different phenotyping tasks using 1,610 discharge summaries extracted from the MIMIC-III database.   Results: CNNs outperform other phenotyping algorithms in all 10 tasks. The average F1-score of our model is 76 (PPV of 83, and sensitivity of 71) with our model having an F1-score up to 37 points higher than alternative approaches. We additionally assess the interpretability of our model by presenting a method that extracts the most salient phrases for a particular prediction.   Conclusion: We show that NLP methods based on deep learning improve the performance of patient phenotyping. Our CNN-based algorithm automatically learns the phrases associated with each patient phenotype. As such, it reduces the annotation complexity for clinical domain experts, who are normally required to develop task-specific annotation rules and identify relevant phrases. Our method performs well in terms of both performance and interpretability, which indicates that deep learning is an effective approach to patient phenotyping based on clinicians' notes.|object investig whether deep learn techniqu natur languag process nlp use effici patient phenotyp patient phenotyp classif task determin whether patient medic condit crucial part secondari analysi healthcar data assess perform deep learn algorithm compar classic nlp approach materi method compar convolut neural network cnns gram model approach base ctake extract pre defin medic concept clinic note use predict patient phenotyp perform test differ phenotyp task use discharg summari extract mimic iii databas result cnns outperform phenotyp algorithm task averag score model ppv sensit model score point higher altern approach addit assess interpret model present method extract salient phrase particular predict conclus show nlp method base deep learn improv perform patient phenotyp cnn base algorithm automat learn phrase associ patient phenotyp reduc annot complex clinic domain expert normal requir develop task specif annot rule identifi relev phrase method perform well term perform interpret indic deep learn effect approach patient phenotyp base clinician note|['Sebastian Gehrmann', 'Franck Dernoncourt', 'Yeran Li', 'Eric T. Carlson', 'Joy T. Wu', 'Jonathan Welt', 'John Foote Jr.', 'Edward T. Moseley', 'David W. Grant', 'Patrick D. Tyler', 'Leo Anthony Celi']|['cs.CL', 'cs.AI', 'cs.NE', 'stat.ML']
2017-03-28T14:01:49Z|2017-03-24T22:54:17Z|http://arxiv.org/abs/1703.08619v1|http://arxiv.org/pdf/1703.08619v1|Binarsity: a penalization for one-hot encoded features|binars penal one hot encod featur|This paper deals with the problem of large-scale linear supervised learning in settings where a large number of continuous features are available. We propose to combine the well-known trick of one-hot encoding of continuous features with a new penalization called binarsity. In each group of binary features coming from the one-hot encoding of a single raw continuous feature, this penalization uses total-variation regularization together with an extra linear constraint to avoid collinearity within groups. Non-asymptotic oracle inequalities for generalized linear models are proposed, and numerical experiments illustrate the good performances of our approach on several datasets. It is also noteworthy that our method has a numerical complexity comparable to standard $\ell_1$ penalization.|paper deal problem larg scale linear supervis learn set larg number continu featur avail propos combin well known trick one hot encod continu featur new penal call binars group binari featur come one hot encod singl raw continu featur penal use total variat regular togeth extra linear constraint avoid collinear within group non asymptot oracl inequ general linear model propos numer experi illustr good perform approach sever dataset also noteworthi method numer complex compar standard ell penal|['Mokhtar Z. Alaya', 'Simon Bussy', 'Stéphane Gaïffas', 'Agathe Guilloux']|['stat.ML']
2017-03-28T14:01:49Z|2017-03-24T19:45:24Z|http://arxiv.org/abs/1703.08581v1|http://arxiv.org/pdf/1703.08581v1|Sequence-to-Sequence Models Can Directly Transcribe Foreign Speech|sequenc sequenc model direct transcrib foreign speech|We present a recurrent encoder-decoder deep neural network architecture that directly translates speech in one language into text in another. The model does not explicitly transcribe the speech into text in the source language, nor does it require supervision from the ground truth source language transcription during training. We apply a slightly modified sequence-to-sequence with attention architecture that has previously been used for speech recognition and show that it can be repurposed for this more complex task, illustrating the power of attention-based models. A single model trained end-to-end obtains state-of-the-art performance on the Fisher Callhome Spanish-English speech translation task, outperforming a cascade of independently trained sequence-to-sequence speech recognition and machine translation models by 1.8 BLEU points on the Fisher test set. In addition, we find that making use of the training data in both languages by multi-task training sequence-to-sequence speech translation and recognition models with a shared encoder network can improve performance by a further 1.4 BLEU points.|present recurr encod decod deep neural network architectur direct translat speech one languag text anoth model doe explicit transcrib speech text sourc languag doe requir supervis ground truth sourc languag transcript dure train appli slight modifi sequenc sequenc attent architectur previous use speech recognit show repurpos complex task illustr power attent base model singl model train end end obtain state art perform fisher callhom spanish english speech translat task outperform cascad independ train sequenc sequenc speech recognit machin translat model bleu point fisher test set addit find make use train data languag multi task train sequenc sequenc speech translat recognit model share encod network improv perform bleu point|['Ron J. Weiss', 'Jan Chorowski', 'Navdeep Jaitly', 'Yonghui Wu', 'Zhifeng Chen']|['cs.CL', 'cs.LG', 'stat.ML']
2017-03-28T14:01:49Z|2017-03-24T17:17:45Z|http://arxiv.org/abs/1703.08520v1|http://arxiv.org/pdf/1703.08520v1|Rejection-free Ensemble MCMC with applications to Factorial Hidden   Markov Models|reject free ensembl mcmc applic factori hidden markov model|"Bayesian inference for complex models is challenging due to the need to explore high-dimensional spaces and multimodality and standard Monte Carlo samplers can have difficulties effectively exploring the posterior. We introduce a general purpose rejection-free ensemble Markov Chain Monte Carlo (MCMC) technique to improve on existing poorly mixing samplers. This is achieved by combining parallel tempering and an auxiliary variable move to exchange information between the chains. We demonstrate this ensemble MCMC scheme on Bayesian inference in Factorial Hidden Markov Models. This high-dimensional inference problem is difficult due to the exponentially sized latent variable space. Existing sampling approaches mix slowly and can get trapped in local modes. We show that the performance of these samplers is improved by our rejection-free ensemble technique and that the method is attractive and ""easy-to-use"" since no parameter tuning is required."|bayesian infer complex model challeng due need explor high dimension space multimod standard mont carlo sampler difficulti effect explor posterior introduc general purpos reject free ensembl markov chain mont carlo mcmc techniqu improv exist poor mix sampler achiev combin parallel temper auxiliari variabl move exchang inform chain demonstr ensembl mcmc scheme bayesian infer factori hidden markov model high dimension infer problem difficult due exponenti size latent variabl space exist sampl approach mix slowli get trap local mode show perform sampler improv reject free ensembl techniqu method attract easi use sinc paramet tune requir|['Kaspar Märtens', 'Michalis K Titsias', 'Christopher Yau']|['stat.CO', 'stat.ME', 'stat.ML']
2017-03-28T14:01:49Z|2017-03-24T14:49:58Z|http://arxiv.org/abs/1703.08544v1|http://arxiv.org/pdf/1703.08544v1|D.TRUMP: Data-mining Textual Responses to Uncover Misconception Patterns|trump data mine textual respons uncov misconcept pattern|An important, yet largely unstudied, problem in student data analysis is to detect misconceptions from students' responses to open-response questions. Misconception detection enables instructors to deliver more targeted feedback on the misconceptions exhibited by many students in their class, thus improving the quality of instruction. In this paper, we propose D.TRUMP, a new natural language processing-based framework to detect the common misconceptions among students' textual responses to short-answer questions. We propose a probabilistic model for students' textual responses involving misconceptions and experimentally validate it on a real-world student-response dataset. Experimental results show that D.TRUMP excels at classifying whether a response exhibits one or more misconceptions. More importantly, it can also automatically detect the common misconceptions exhibited across responses from multiple students to multiple questions; this property is especially important at large scale, since instructors will no longer need to manually specify all possible misconceptions that students might exhibit.|import yet larg unstudi problem student data analysi detect misconcept student respons open respons question misconcept detect enabl instructor deliv target feedback misconcept exhibit mani student class thus improv qualiti instruct paper propos trump new natur languag process base framework detect common misconcept among student textual respons short answer question propos probabilist model student textual respons involv misconcept experiment valid real world student respons dataset experiment result show trump excel classifi whether respons exhibit one misconcept import also automat detect common misconcept exhibit across respons multipl student multipl question properti especi import larg scale sinc instructor longer need manual specifi possibl misconcept student might exhibit|['Joshua J. Michalenko', 'Andrew S. Lan', 'Richard G. Baraniuk']|['stat.ML', 'cs.CL']
2017-03-28T14:01:49Z|2017-03-24T13:29:52Z|http://arxiv.org/abs/1703.08403v1|http://arxiv.org/pdf/1703.08403v1|Asymmetric Learning Vector Quantization for Efficient Nearest Neighbor   Classification in Dynamic Time Warping Spaces|asymmetr learn vector quantize effici nearest neighbor classif dynam time warp space|The nearest neighbor method together with the dynamic time warping (DTW) distance is one of the most popular approaches in time series classification. This method suffers from high storage and computation requirements for large training sets. As a solution to both drawbacks, this article extends learning vector quantization (LVQ) from Euclidean spaces to DTW spaces. The proposed LVQ scheme uses asymmetric weighted averaging as update rule. Empirical results exhibited superior performance of asymmetric generalized LVQ (GLVQ) over other state-of-the-art prototype generation methods for nearest neighbor classification.|nearest neighbor method togeth dynam time warp dtw distanc one popular approach time seri classif method suffer high storag comput requir larg train set solut drawback articl extend learn vector quantize lvq euclidean space dtw space propos lvq scheme use asymmetr weight averag updat rule empir result exhibit superior perform asymmetr general lvq glvq state art prototyp generat method nearest neighbor classif|['Brijnesh Jain', 'David Schultz']|['cs.LG', 'stat.ML']
2017-03-28T14:01:53Z|2017-03-24T12:07:34Z|http://arxiv.org/abs/1703.08383v1|http://arxiv.org/pdf/1703.08383v1|Smart Augmentation - Learning an Optimal Data Augmentation Strategy|smart augment learn optim data augment strategi|A recurring problem faced when training neural networks is that there is typically not enough data to maximize the generalization capability of deep neural networks(DNN). There are many techniques to address this, including data augmentation, dropout, and transfer learning. In this paper, we introduce an additional method which we call Smart Augmentation and we show how to use it to increase the accuracy and reduce overfitting on a target network. Smart Augmentation works by creating a network that learns how to generate augmented data during the training process of a target network in a way that reduces that networks loss. This allows us to learn augmentations that minimize the error of that network.   Smart Augmentation has shown the potential to increase accuracy by demonstrably significant measures on all datasets tested. In addition, it has shown potential to achieve similar or improved performance levels with significantly smaller network sizes in a number of tested cases.|recur problem face train neural network typic enough data maxim general capabl deep neural network dnn mani techniqu address includ data augment dropout transfer learn paper introduc addit method call smart augment show use increas accuraci reduc overfit target network smart augment work creat network learn generat augment data dure train process target network way reduc network loss allow us learn augment minim error network smart augment shown potenti increas accuraci demonstr signific measur dataset test addit shown potenti achiev similar improv perform level signific smaller network size number test case|['Joseph Lemley', 'Shabab Bazrafkan', 'Peter Corcoran']|['cs.AI', 'cs.LG', 'stat.ML']
2017-03-28T14:01:53Z|2017-03-24T02:31:23Z|http://arxiv.org/abs/1703.08267v1|http://arxiv.org/abs/1703.08267v1|A Nonconvex Splitting Method for Symmetric Nonnegative Matrix   Factorization: Convergence Analysis and Optimality|nonconvex split method symmetr nonneg matrix factor converg analysi optim|Symmetric nonnegative matrix factorization (SymNMF) has important applications in data analytics problems such as document clustering, community detection and image segmentation. In this paper, we propose a novel nonconvex variable splitting method for solving SymNMF. The proposed algorithm is guaranteed to converge to the set of Karush-Kuhn-Tucker (KKT) points of the nonconvex SymNMF problem. Furthermore, it achieves a global sublinear convergence rate. We also show that the algorithm can be efficiently implemented in parallel. Further, sufficient conditions are provided which guarantee the global and local optimality of the obtained solutions. Extensive numerical results performed on both synthetic and real data sets suggest that the proposed algorithm converges quickly to a local minimum solution.|symmetr nonneg matrix factor symnmf import applic data analyt problem document cluster communiti detect imag segment paper propos novel nonconvex variabl split method solv symnmf propos algorithm guarante converg set karush kuhn tucker kkt point nonconvex symnmf problem furthermor achiev global sublinear converg rate also show algorithm effici implement parallel suffici condit provid guarante global local optim obtain solut extens numer result perform synthet real data set suggest propos algorithm converg quick local minimum solut|['Songtao Lu', 'Mingyi Hong', 'Zhengdao Wang']|['math.OC', 'stat.ML']
2017-03-28T14:01:53Z|2017-03-23T23:27:12Z|http://arxiv.org/abs/1703.08251v1|http://arxiv.org/pdf/1703.08251v1|The Dependence of Machine Learning on Electronic Medical Record Quality|depend machin learn electron medic record qualiti|There is growing interest in applying machine learning methods to Electronic Medical Records (EMR). Across different institutions, however, EMR quality can vary widely. This work investigated the impact of this disparity on the performance of three advanced machine learning algorithms: logistic regression, multilayer perceptron, and recurrent neural network. The EMR disparity was emulated using different permutations of the EMR collected at Children's Hospital Los Angeles (CHLA) Pediatric Intensive Care Unit (PICU) and Cardiothoracic Intensive Care Unit (CTICU). The algorithms were trained using patients from the PICU to predict in-ICU mortality for patients in a held out set of PICU and CTICU patients. The disparate patient populations between the PICU and CTICU provide an estimate of generalization errors across different ICUs. We quantified and evaluated the generalization of these algorithms on varying EMR size, input types, and fidelity of data.|grow interest appli machin learn method electron medic record emr across differ institut howev emr qualiti vari wide work investig impact dispar perform three advanc machin learn algorithm logist regress multilay perceptron recurr neural network emr dispar emul use differ permut emr collect children hospit los angel chla pediatr intens care unit picu cardiothorac intens care unit cticu algorithm train use patient picu predict icu mortal patient held set picu cticu patient dispar patient popul picu cticu provid estim general error across differ icus quantifi evalu general algorithm vari emr size input type fidel data|['Long Ho', 'David Ledbetter', 'Melissa Aczon', 'Randall Wetzel']|['stat.ML']
2017-03-28T14:01:53Z|2017-03-23T15:35:33Z|http://arxiv.org/abs/1703.08110v1|http://arxiv.org/pdf/1703.08110v1|Training Mixture Models at Scale via Coresets|train mixtur model scale via coreset|How can we train a statistical mixture model on a massive data set? In this paper, we show how to construct coresets for mixtures of Gaussians and natural generalizations. A coreset is a weighted subset of the data, which guarantees that models fitting the coreset also provide a good fit for the original data set. We show that, perhaps surprisingly, Gaussian mixtures admit coresets of size polynomial in dimension and the number of mixture components, while being independent of the data set size. Hence, one can compute a $(1+ \varepsilon)$-approximation for the optimal model on a significantly smaller data set. More importantly, such coresets can be efficiently constructed both in distributed and streaming settings. Our results rely on a novel reduction of statistical estimation to problems in computational geometry and new complexity results for mixtures of Gaussians. As a by-product of our analysis, we prove that the pseudo-dimension of arbitrary mixtures of Gaussians is polynomial in the ambient dimension. Empirical evaluation on several real-world datasets suggest that our coreset-based approach enables significant reduction in training-time with negligible approximation error.|train statist mixtur model massiv data set paper show construct coreset mixtur gaussian natur general coreset weight subset data guarante model fit coreset also provid good fit origin data set show perhap surpris gaussian mixtur admit coreset size polynomi dimens number mixtur compon independ data set size henc one comput varepsilon approxim optim model signific smaller data set import coreset effici construct distribut stream set result reli novel reduct statist estim problem comput geometri new complex result mixtur gaussian product analysi prove pseudo dimens arbitrari mixtur gaussian polynomi ambient dimens empir evalu sever real world dataset suggest coreset base approach enabl signific reduct train time neglig approxim error|['Mario Lucic', 'Matthew Faulkner', 'Andreas Krause', 'Dan Feldman']|['stat.ML']
2017-03-28T14:01:53Z|2017-03-23T14:29:29Z|http://arxiv.org/abs/1703.08085v1|http://arxiv.org/pdf/1703.08085v1|Unifying Framework for Crowd-sourcing via Graphon Estimation|unifi framework crowd sourc via graphon estim|We consider the question of inferring true answers associated with tasks based on potentially noisy answers obtained through a micro-task crowd-sourcing platform such as Amazon Mechanical Turk. We propose a generic, non-parametric model for this setting: for a given task $i$, $1\leq i \leq T$, the response of worker $j$, $1\leq j\leq W$ for this task is correct with probability $F_{ij}$, where matrix $F = [F_{ij}]_{i\leq T, j\leq W}$ may satisfy one of a collection of regularity conditions including low rank, which can express the popular Dawid-Skene model; piecewise constant, which occurs when there is finitely many worker and task types; monotonic under permutation, when there is some ordering of worker skills and task difficulties; or Lipschitz with respect to an associated latent non-parametric function. This model, contains most, if not all, of the previously proposed models to the best of our knowledge.   We show that the question of estimating the true answers to tasks can be reduced to solving the Graphon estimation problem, for which there has been much recent progress. By leveraging these techniques, we provide a crowdsourcing inference algorithm along with theoretical bounds on the fraction of incorrectly estimated tasks. Subsequently, we have a solution for inferring the true answers for tasks using noisy answers collected from crowd-sourcing platform under a significantly larger class of models. Concretely, we establish that if the $(i,j)$th element of $F$, $F_{ij}$, is equal to a Lipschitz continuous function over latent features associated with the task $i$ and worker $j$ for all $i, j$, then all task answers can be inferred correctly with high probability by soliciting $\tilde{O}(\ln(T)^{3/2})$ responses per task even without any knowledge of the Lipschitz function, task and worker features, or the matrix $F$.|consid question infer true answer associ task base potenti noisi answer obtain micro task crowd sourc platform amazon mechan turk propos generic non parametr model set given task leq leq respons worker leq leq task correct probabl ij matrix ij leq leq may satisfi one collect regular condit includ low rank express popular dawid skene model piecewis constant occur finit mani worker task type monoton permut order worker skill task difficulti lipschitz respect associ latent non parametr function model contain previous propos model best knowledg show question estim true answer task reduc solv graphon estim problem much recent progress leverag techniqu provid crowdsourc infer algorithm along theoret bound fraction incorrect estim task subsequ solut infer true answer task use noisi answer collect crowd sourc platform signific larger class model concret establish th element ij equal lipschitz continu function latent featur associ task worker task answer infer correct high probabl solicit tild ln respons per task even without ani knowledg lipschitz function task worker featur matrix|['Christina E. Lee', 'Devavrat Shah']|['stat.ML']
2017-03-28T14:01:53Z|2017-03-23T13:41:47Z|http://arxiv.org/abs/1703.08065v1|http://arxiv.org/pdf/1703.08065v1|Robustness of Maximum Correntropy Estimation Against Large Outliers|robust maximum correntropi estim larg outlier|The maximum correntropy criterion (MCC) has recently been successfully applied in robust regression, classification and adaptive filtering, where the correntropy is maximized instead of minimizing the well-known mean square error (MSE) to improve the robustness with respect to outliers (or impulsive noises). Considerable efforts have been devoted to develop various robust adaptive algorithms under MCC, but so far little insight has been gained as to how the optimal solution will be affected by outliers. In this work, we study this problem in the context of parameter estimation for a simple linear errors-in-variables (EIV) model where all variables are scalar. Under certain conditions, we derive an upper bound on the absolute value of the estimation error and show that the optimal solution under MCC can be very close to the true value of the unknown parameter even with outliers (whose values can be arbitrarily large) in both input and output variables. An illustrative example is presented to verify and clarify the theory.|maximum correntropi criterion mcc recent success appli robust regress classif adapt filter correntropi maxim instead minim well known mean squar error mse improv robust respect outlier impuls nois consider effort devot develop various robust adapt algorithm mcc far littl insight gain optim solut affect outlier work studi problem context paramet estim simpl linear error variabl eiv model variabl scalar certain condit deriv upper bound absolut valu estim error show optim solut mcc veri close true valu unknown paramet even outlier whose valu arbitrarili larg input output variabl illustr exampl present verifi clarifi theori|['Badong Chen', 'Lei Xing', 'Haiquan Zhao', 'Bin Xu', 'Jose C. Principe']|['stat.ML']
2017-03-28T14:01:53Z|2017-03-23T13:00:14Z|http://arxiv.org/abs/1703.08052v1|http://arxiv.org/pdf/1703.08052v1|Dynamic Bernoulli Embeddings for Language Evolution|dynam bernoulli embed languag evolut|Word embeddings are a powerful approach for unsupervised analysis of language. Recently, Rudolph et al. (2016) developed exponential family embeddings, which cast word embeddings in a probabilistic framework. Here, we develop dynamic embeddings, building on exponential family embeddings to capture how the meanings of words change over time. We use dynamic embeddings to analyze three large collections of historical texts: the U.S. Senate speeches from 1858 to 2009, the history of computer science ACM abstracts from 1951 to 2014, and machine learning papers on the Arxiv from 2007 to 2015. We find dynamic embeddings provide better fits than classical embeddings and capture interesting patterns about how language changes.|word embed power approach unsupervis analysi languag recent rudolph et al develop exponenti famili embed cast word embed probabilist framework develop dynam embed build exponenti famili embed captur mean word chang time use dynam embed analyz three larg collect histor text senat speech histori comput scienc acm abstract machin learn paper arxiv find dynam embed provid better fit classic embed captur interest pattern languag chang|['Maja Rudolph', 'David Blei']|['stat.ML', 'cs.CL']
2017-03-28T14:01:53Z|2017-03-23T12:17:00Z|http://arxiv.org/abs/1703.08031v1|http://arxiv.org/pdf/1703.08031v1|Distribution of Gaussian Process Arc Lengths|distribut gaussian process arc length|We present the first treatment of the arc length of the Gaussian Process (GP) with more than a single output dimension. GPs are commonly used for tasks such as trajectory modelling, where path length is a crucial quantity of interest. Previously, only paths in one dimension have been considered, with no theoretical consideration of higher dimensional problems. We fill the gap in the existing literature by deriving the moments of the arc length for a stationary GP with multiple output dimensions. A new method is used to derive the mean of a one-dimensional GP over a finite interval, by considering the distribution of the arc length integrand. This technique is used to derive an approximate distribution over the arc length of a vector valued GP in $\mathbb{R}^n$ by moment matching the distribution. Numerical simulations confirm our theoretical derivations.|present first treatment arc length gaussian process gp singl output dimens gps common use task trajectori model path length crucial quantiti interest previous onli path one dimens consid theoret consider higher dimension problem fill gap exist literatur deriv moment arc length stationari gp multipl output dimens new method use deriv mean one dimension gp finit interv consid distribut arc length integrand techniqu use deriv approxim distribut arc length vector valu gp mathbb moment match distribut numer simul confirm theoret deriv|['Justin D. Bewsher', 'Alessandra Tosi', 'Michael A. Osborne', 'Stephen J. Roberts']|['stat.ML']
2017-03-28T14:01:53Z|2017-03-23T07:13:28Z|http://arxiv.org/abs/1703.07948v1|http://arxiv.org/pdf/1703.07948v1|Fast Stochastic Variance Reduced Gradient Method with Momentum   Acceleration for Machine Learning|fast stochast varianc reduc gradient method momentum acceler machin learn|Recently, research on accelerated stochastic gradient descent methods (e.g., SVRG) has made exciting progress (e.g., linear convergence for strongly convex problems). However, the best-known methods (e.g., Katyusha) requires at least two auxiliary variables and two momentum parameters. In this paper, we propose a fast stochastic variance reduction gradient (FSVRG) method, in which we design a novel update rule with the Nesterov's momentum and incorporate the technique of growing epoch size. FSVRG has only one auxiliary variable and one momentum weight, and thus it is much simpler and has much lower per-iteration complexity. We prove that FSVRG achieves linear convergence for strongly convex problems and the optimal $\mathcal{O}(1/T^2)$ convergence rate for non-strongly convex problems, where $T$ is the number of outer-iterations. We also extend FSVRG to directly solve the problems with non-smooth component functions, such as SVM. Finally, we empirically study the performance of FSVRG for solving various machine learning problems such as logistic regression, ridge regression, Lasso and SVM. Our results show that FSVRG outperforms the state-of-the-art stochastic methods, including Katyusha.|recent research acceler stochast gradient descent method svrg made excit progress linear converg strong convex problem howev best known method katyusha requir least two auxiliari variabl two momentum paramet paper propos fast stochast varianc reduct gradient fsvrg method design novel updat rule nesterov momentum incorpor techniqu grow epoch size fsvrg onli one auxiliari variabl one momentum weight thus much simpler much lower per iter complex prove fsvrg achiev linear converg strong convex problem optim mathcal converg rate non strong convex problem number outer iter also extend fsvrg direct solv problem non smooth compon function svm final empir studi perform fsvrg solv various machin learn problem logist regress ridg regress lasso svm result show fsvrg outperform state art stochast method includ katyusha|['Fanhua Shang', 'Yuanyuan Liu', 'James Cheng', 'Jiacheng Zhuo']|['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']
2017-03-28T14:01:53Z|2017-03-23T05:23:34Z|http://arxiv.org/abs/1703.07940v1|http://arxiv.org/pdf/1703.07940v1|Unsupervised Basis Function Adaptation for Reinforcement Learning|unsupervis basi function adapt reinforc learn|When using reinforcement learning (RL) algorithms to evaluate a policy it is common, given a large state space, to introduce some form of approximation architecture for the value function (VF). The exact form of this architecture can have a significant effect on the accuracy of the VF estimate, however, and determining a suitable approximation architecture can often be a highly complex task. Consequently there is a large amount of interest in the potential for allowing RL algorithms to adaptively generate (i.e. to learn) approximation architectures.   We investigate a method of adapting approximation architectures which uses feedback regarding the frequency with which an agent has visited certain states to guide which areas of the state space to approximate with greater detail. We introduce an algorithm based upon this idea which adapts a state aggregation approximation architecture on-line.   Assuming $S$ states, we demonstrate theoretically that - provided the following relatively non-restrictive assumptions are satisfied: (a) the number of cells $X$ in the state aggregation architecture is of order $\sqrt{S}\ln{S}\log_2{S}$ or greater, (b) the policy and transition function are close to deterministic, and (c) the prior for the transition function is uniformly distributed - our algorithm can guarantee, assuming we use an appropriate scoring function to measure VF error, error which is arbitrarily close to zero as $S$ becomes large. It is able to do this despite having only $O(X\log_2{S})$ space complexity (and negligible time complexity). We conclude by generating a set of empirical results which support the theoretical results.|use reinforc learn rl algorithm evalu polici common given larg state space introduc form approxim architectur valu function vf exact form architectur signific effect accuraci vf estim howev determin suitabl approxim architectur often high complex task consequ larg amount interest potenti allow rl algorithm adapt generat learn approxim architectur investig method adapt approxim architectur use feedback regard frequenc agent visit certain state guid area state space approxim greater detail introduc algorithm base upon idea adapt state aggreg approxim architectur line assum state demonstr theoret provid follow relat non restrict assumpt satisfi number cell state aggreg architectur order sqrt ln log greater polici transit function close determinist prior transit function uniform distribut algorithm guarante assum use appropri score function measur vf error error arbitrarili close zero becom larg abl despit onli log space complex neglig time complex conclud generat set empir result support theoret result|['Edward Barker', 'Charl Ras']|['cs.LG', 'cs.AI', 'stat.ML']
2017-03-28T14:01:57Z|2017-03-23T04:25:48Z|http://arxiv.org/abs/1703.07928v1|http://arxiv.org/pdf/1703.07928v1|Guided Perturbations: Self Corrective Behavior in Convolutional Neural   Networks|guid perturb self correct behavior convolut neural network|Convolutional Neural Networks have been a subject of great importance over the past decade and great strides have been made in their utility for producing state of the art performance in many computer vision problems. However, the behavior of deep networks is yet to be fully understood and is still an active area of research. In this work, we present an intriguing behavior: pre-trained CNNs can be made to improve their predictions by structurally perturbing the input. We observe that these perturbations - referred as Guided Perturbations - enable a trained network to improve its prediction performance without any learning or change in network weights. We perform various ablative experiments to understand how these perturbations affect the local context and feature representations. Furthermore, we demonstrate that this idea can improve performance of several existing approaches on semantic segmentation and scene labeling tasks on the PASCAL VOC dataset and supervised classification tasks on MNIST and CIFAR10 datasets.|convolut neural network subject great import past decad great stride made util produc state art perform mani comput vision problem howev behavior deep network yet fulli understood still activ area research work present intrigu behavior pre train cnns made improv predict structur perturb input observ perturb refer guid perturb enabl train network improv predict perform without ani learn chang network weight perform various ablat experi understand perturb affect local context featur represent furthermor demonstr idea improv perform sever exist approach semant segment scene label task pascal voc dataset supervis classif task mnist cifar dataset|['Swami Sankaranarayanan', 'Arpit Jain', 'Ser Nam Lim']|['cs.CV', 'cs.AI', 'stat.ML']
2017-03-28T14:01:57Z|2017-03-23T03:17:14Z|http://arxiv.org/abs/1703.07915v1|http://arxiv.org/pdf/1703.07915v1|Perspective: Energy Landscapes for Machine Learning|perspect energi landscap machin learn|Machine learning techniques are being increasingly used as flexible non-linear fitting and prediction tools in the physical sciences. Fitting functions that exhibit multiple solutions as local minima can be analysed in terms of the corresponding machine learning landscape. Methods to explore and visualise molecular potential energy landscapes can be applied to these machine learning landscapes to gain new insight into the solution space involved in training and the nature of the corresponding predictions. In particular, we can define quantities analogous to molecular structure, thermodynamics, and kinetics, and relate these emergent properties to the structure of the underlying landscape. This Perspective aims to describe these analogies with examples from recent applications, and suggest avenues for new interdisciplinary research.|machin learn techniqu increas use flexibl non linear fit predict tool physic scienc fit function exhibit multipl solut local minima analys term correspond machin learn landscap method explor visualis molecular potenti energi landscap appli machin learn landscap gain new insight solut space involv train natur correspond predict particular defin quantiti analog molecular structur thermodynam kinet relat emerg properti structur landscap perspect aim describ analog exampl recent applic suggest avenu new interdisciplinari research|['Andrew J. Ballard', 'Ritankar Das', 'Stefano Martiniani', 'Dhagash Mehta', 'Levent Sagun', 'Jacob D. Stevenson', 'David J. Wales']|['stat.ML', 'cond-mat.dis-nn', 'cs.LG']
2017-03-28T14:01:57Z|2017-03-23T02:40:36Z|http://arxiv.org/abs/1703.07909v1|http://arxiv.org/pdf/1703.07909v1|Data Driven Exploratory Attacks on Black Box Classifiers in Adversarial   Domains|data driven exploratori attack black box classifi adversari domain|While modern day web applications aim to create impact at the civilization level, they have become vulnerable to adversarial activity, where the next cyber-attack can take any shape and can originate from anywhere. The increasing scale and sophistication of attacks, has prompted the need for a data driven solution, with machine learning forming the core of many cybersecurity systems. Machine learning was not designed with security in mind, and the essential assumption of stationarity, requiring that the training and testing data follow similar distributions, is violated in an adversarial domain. In this paper, an adversary's view point of a classification based system, is presented. Based on a formal adversarial model, the Seed-Explore-Exploit framework is presented, for simulating the generation of data driven and reverse engineering attacks on classifiers. Experimental evaluation, on 10 real world datasets and using the Google Cloud Prediction Platform, demonstrates the innate vulnerability of classifiers and the ease with which evasion can be carried out, without any explicit information about the classifier type, the training data or the application domain. The proposed framework, algorithms and empirical evaluation, serve as a white hat analysis of the vulnerabilities, and aim to foster the development of secure machine learning frameworks.|modern day web applic aim creat impact civil level becom vulner adversari activ next cyber attack take ani shape origin anywher increas scale sophist attack prompt need data driven solut machin learn form core mani cybersecur system machin learn design secur mind essenti assumpt stationar requir train test data follow similar distribut violat adversari domain paper adversari view point classif base system present base formal adversari model seed explor exploit framework present simul generat data driven revers engin attack classifi experiment evalu real world dataset use googl cloud predict platform demonstr innat vulner classifi eas evas carri without ani explicit inform classifi type train data applic domain propos framework algorithm empir evalu serv white hat analysi vulner aim foster develop secur machin learn framework|['Tegjyot Singh Sethi', 'Mehmed Kantardzic']|['stat.ML', 'cs.CR', 'cs.LG']
2017-03-28T14:01:57Z|2017-03-23T01:30:17Z|http://arxiv.org/abs/1703.07904v1|http://arxiv.org/pdf/1703.07904v1|Cross-Validation with Confidence|cross valid confid|Cross-validation is one of the most popular model selection methods in statistics and machine learning. Despite its wide applicability, traditional cross-validation methods tend to select overfitting models, unless the ratio between the training and testing sample sizes is much smaller than conventional choices. We argue that such an overfitting tendency of cross-validation is due to the ignorance of the uncertainty in the testing sample. Starting from this observation, we develop a new, statistically principled inference tool based on cross-validation that takes into account the uncertainty in the testing sample. This new method outputs a small set of highly competitive candidate models containing the best one with guaranteed probability. As a consequence, our method can achieve consistent variable selection in a classical linear regression setting, for which existing cross-validation methods require unconventional split ratios. We demonstrate the performance of the proposed method in several simulated and real data examples.|cross valid one popular model select method statist machin learn despit wide applic tradit cross valid method tend select overfit model unless ratio train test sampl size much smaller convent choic argu overfit tendenc cross valid due ignor uncertainti test sampl start observ develop new statist principl infer tool base cross valid take account uncertainti test sampl new method output small set high competit candid model contain best one guarante probabl consequ method achiev consist variabl select classic linear regress set exist cross valid method requir unconvent split ratio demonstr perform propos method sever simul real data exampl|['Jing Lei']|['stat.ME', 'stat.ML']
2017-03-28T14:01:57Z|2017-03-22T23:35:51Z|http://arxiv.org/abs/1703.07886v1|http://arxiv.org/pdf/1703.07886v1|Robust Kronecker-Decomposable Component Analysis for Low Rank Modeling|robust kroneck decompos compon analysi low rank model|Dictionary learning and component analysis are part of one of the most well-studied and active research fields, at the intersection of signal and image processing, computer vision, and statistical machine learning. In dictionary learning, the current methods of choice are arguably K-SVD and its variants, which learn a dictionary (i.e., a decomposition) for sparse coding via Singular Value Decomposition. In robust component analysis, leading methods derive from Principal Component Pursuit (PCP), which recovers a low-rank matrix from sparse corruptions of unknown magnitude and support. While K-SVD is sensitive to the presence of noise and outliers in the training set, PCP does not provide a dictionary that respects the structure of the data (e.g., images), and requires expensive SVD computations when solved by convex relaxation. In this paper, we introduce a new robust decomposition of images by combining ideas from sparse dictionary learning and PCP. We propose a novel Kronecker-decomposable component analysis which is robust to gross corruption, can be used for low-rank modeling, and leverages separability to solve significantly smaller problems. We design an efficient learning algorithm by drawing links with a restricted form of tensor factorization. The effectiveness of the proposed approach is demonstrated on real-world applications, namely background subtraction and image denoising, by performing a thorough comparison with the current state of the art.|dictionari learn compon analysi part one well studi activ research field intersect signal imag process comput vision statist machin learn dictionari learn current method choic arguabl svd variant learn dictionari decomposit spars code via singular valu decomposit robust compon analysi lead method deriv princip compon pursuit pcp recov low rank matrix spars corrupt unknown magnitud support svd sensit presenc nois outlier train set pcp doe provid dictionari respect structur data imag requir expens svd comput solv convex relax paper introduc new robust decomposit imag combin idea spars dictionari learn pcp propos novel kroneck decompos compon analysi robust gross corrupt use low rank model leverag separ solv signific smaller problem design effici learn algorithm draw link restrict form tensor factor effect propos approach demonstr real world applic name background subtract imag denois perform thorough comparison current state art|['Mehdi Bahri', 'Yannis Panagakis', 'Stefanos Zafeiriou']|['stat.ML', 'cs.CV']
2017-03-28T14:01:57Z|2017-03-22T19:33:54Z|http://arxiv.org/abs/1703.07830v1|http://arxiv.org/abs/1703.07830v1|Randomized Kernel Methods for Least-Squares Support Vector Machines|random kernel method least squar support vector machin|The least-squares support vector machine is a frequently used kernel method for non-linear regression and classification tasks. Here we discuss several approximation algorithms for the least-squares support vector machine classifier. The proposed methods are based on randomized block kernel matrices, and we show that they provide good accuracy and reliable scaling for multi-class classification problems with relatively large data sets. Also, we present several numerical experiments that illustrate the practical applicability of the proposed methods.|least squar support vector machin frequent use kernel method non linear regress classif task discuss sever approxim algorithm least squar support vector machin classifi propos method base random block kernel matric show provid good accuraci reliabl scale multi class classif problem relat larg data set also present sever numer experi illustr practic applic propos method|['M. Andrecut']|['cs.LG', 'physics.data-an', 'stat.ML']
2017-03-28T14:01:57Z|2017-03-22T17:53:27Z|http://arxiv.org/abs/1703.07771v1|http://arxiv.org/pdf/1703.07771v1|Multitask Learning and Benchmarking with Clinical Time Series Data|multitask learn benchmark clinic time seri data|Health care is one of the most exciting frontiers in data mining and machine learning. Successful adoption of electronic health records (EHRs) created an explosion in digital clinical data available for analysis, but progress in machine learning for healthcare research has been difficult to measure because of the absence of publicly available benchmark data sets. To address this problem, we propose four clinical prediction benchmarks using data derived from the publicly available Medical Information Mart for Intensive Care (MIMIC-III) database. These tasks cover a range of clinical problems including modeling risk of mortality, forecasting length of stay, detecting physiologic decline, and phenotype classification. We formulate a heterogeneous multitask problem where the goal is to jointly learn multiple clinically relevant prediction tasks based on the same time series data. To address this problem, we propose a novel recurrent neural network (RNN) architecture that leverages the correlations between the various tasks to learn a better predictive model. We validate the proposed neural architecture on this benchmark, and demonstrate that it outperforms strong baselines, including single task RNNs.|health care one excit frontier data mine machin learn success adopt electron health record ehr creat explos digit clinic data avail analysi progress machin learn healthcar research difficult measur becaus absenc public avail benchmark data set address problem propos four clinic predict benchmark use data deriv public avail medic inform mart intens care mimic iii databas task cover rang clinic problem includ model risk mortal forecast length stay detect physiolog declin phenotyp classif formul heterogen multitask problem goal joint learn multipl clinic relev predict task base time seri data address problem propos novel recurr neural network rnn architectur leverag correl various task learn better predict model valid propos neural architectur benchmark demonstr outperform strong baselin includ singl task rnns|['Hrayr Harutyunyan', 'Hrant Khachatrian', 'David C. Kale', 'Aram Galstyan']|['stat.ML', 'cs.LG']
2017-03-28T14:01:57Z|2017-03-22T17:27:57Z|http://arxiv.org/abs/1703.07758v1|http://arxiv.org/pdf/1703.07758v1|S-Concave Distributions: Towards Broader Distributions for   Noise-Tolerant and Sample-Efficient Learning Algorithms|concav distribut toward broader distribut nois toler sampl effici learn algorithm|We provide new results concerning noise-tolerant and sample-efficient learning algorithms under $s$-concave distributions over $\mathbb{R}^n$ for $-\frac{1}{2n+3}\le s\le 0$. The new class of $s$-concave distributions is a broad and natural generalization of log-concavity, and includes many important additional distributions, e.g., the Pareto distribution and $t$-distribution. This class has been studied in the context of efficient sampling, integration, and optimization, but much remains unknown concerning the geometry of this class of distributions and their applications in the context of learning.   The challenge is that unlike the commonly used distributions in learning (uniform or more generally log-concave distributions), this broader class is not closed under the marginalization operator and many such distributions are fat-tailed. In this work, we introduce new convex geometry tools to study the properties of s-concave distributions and use these properties to provide bounds on quantities of interest to learning including the probability of disagreement between two halfspaces, disagreement outside a band, and disagreement coefficient. We use these results to significantly generalize prior results for margin-based active learning, disagreement-based active learning, and passively learning of intersections of halfspaces.   Our analysis of geometric properties of s-concave distributions might be of independent interest to optimization more broadly.|provid new result concern nois toler sampl effici learn algorithm concav distribut mathbb frac le le new class concav distribut broad natur general log concav includ mani import addit distribut pareto distribut distribut class studi context effici sampl integr optim much remain unknown concern geometri class distribut applic context learn challeng unlik common use distribut learn uniform general log concav distribut broader class close margin oper mani distribut fat tail work introduc new convex geometri tool studi properti concav distribut use properti provid bound quantiti interest learn includ probabl disagr two halfspac disagr outsid band disagr coeffici use result signific general prior result margin base activ learn disagr base activ learn passiv learn intersect halfspac analysi geometr properti concav distribut might independ interest optim broad|['Maria-Florina Balcan', 'Hongyang Zhang']|['stat.ML', 'cs.AI', 'cs.LG']
2017-03-28T14:01:57Z|2017-03-22T17:17:16Z|http://arxiv.org/abs/1703.07754v1|http://arxiv.org/pdf/1703.07754v1|Direct Acoustics-to-Word Models for English Conversational Speech   Recognition|direct acoust word model english convers speech recognit|Recent work on end-to-end automatic speech recognition (ASR) has shown that the connectionist temporal classification (CTC) loss can be used to convert acoustics to phone or character sequences. Such systems are used with a dictionary and separately-trained Language Model (LM) to produce word sequences. However, they are not truly end-to-end in the sense of mapping acoustics directly to words without an intermediate phone representation. In this paper, we present the first results employing direct acoustics-to-word CTC models on two well-known public benchmark tasks: Switchboard and CallHome. These models do not require an LM or even a decoder at run-time and hence recognize speech with minimal complexity. However, due to the large number of word output units, CTC word models require orders of magnitude more data to train reliably compared to traditional systems. We present some techniques to mitigate this issue. Our CTC word model achieves a word error rate of 13.0%/18.8% on the Hub5-2000 Switchboard/CallHome test sets without any LM or decoder compared with 9.6%/16.0% for phone-based CTC with a 4-gram LM. We also present rescoring results on CTC word model lattices to quantify the performance benefits of a LM, and contrast the performance of word and phone CTC models.|recent work end end automat speech recognit asr shown connectionist tempor classif ctc loss use convert acoust phone charact sequenc system use dictionari separ train languag model lm produc word sequenc howev truli end end sens map acoust direct word without intermedi phone represent paper present first result employ direct acoust word ctc model two well known public benchmark task switchboard callhom model requir lm even decod run time henc recogn speech minim complex howev due larg number word output unit ctc word model requir order magnitud data train reliabl compar tradit system present techniqu mitig issu ctc word model achiev word error rate hub switchboard callhom test set without ani lm decod compar phone base ctc gram lm also present rescor result ctc word model lattic quantifi perform benefit lm contrast perform word phone ctc model|['Kartik Audhkhasi', 'Bhuvana Ramabhadran', 'George Saon', 'Michael Picheny', 'David Nahamoo']|['cs.CL', 'cs.NE', 'stat.ML']
2017-03-28T14:01:57Z|2017-03-22T15:34:23Z|http://arxiv.org/abs/1703.07710v1|http://arxiv.org/pdf/1703.07710v1|UBEV - A More Practical Algorithm for Episodic RL with Near-Optimal PAC   and Regret Guarantees|ubev practic algorithm episod rl near optim pac regret guarante|We present UBEV, a simple and efficient reinforcement learning algorithm for fixed-horizon episodic Markov decision processes. The main contribution is a proof that UBEV enjoys a sample-complexity bound that holds for all accuracy levels simultaneously with high probability, and matches the lower bound except for logarithmic terms and one factor of the horizon. A consequence of the fact that our sample-complexity bound holds for all accuracy levels is that the new algorithm achieves a sub-linear regret of O(sqrt(SAT)), which is the first time the dependence on the size of the state space has provably appeared inside the square root. A brief empirical evaluation shows that UBEV is practically superior to existing algorithms with known sample-complexity guarantees.|present ubev simpl effici reinforc learn algorithm fix horizon episod markov decis process main contribut proof ubev enjoy sampl complex bound hold accuraci level simultan high probabl match lower bound except logarithm term one factor horizon consequ fact sampl complex bound hold accuraci level new algorithm achiev sub linear regret sqrt sat first time depend size state space provabl appear insid squar root brief empir evalu show ubev practic superior exist algorithm known sampl complex guarante|['Christoph Dann', 'Tor Lattimore', 'Emma Brunskill']|['cs.LG', 'cs.AI', 'stat.ML']
2017-03-28T14:02:02Z|2017-03-22T15:11:55Z|http://arxiv.org/abs/1703.07698v1|http://arxiv.org/pdf/1703.07698v1|Characterization of Deterministic and Probabilistic Sampling Patterns   for Finite Completability of Low Tensor-Train Rank Tensor|character determinist probabilist sampl pattern finit complet low tensor train rank tensor|In this paper, we analyze the fundamental conditions for low-rank tensor completion given the separation or tensor-train (TT) rank, i.e., ranks of unfoldings. We exploit the algebraic structure of the TT decomposition to obtain the deterministic necessary and sufficient conditions on the locations of the samples to ensure finite completability. Specifically, we propose an algebraic geometric analysis on the TT manifold that can incorporate the whole rank vector simultaneously in contrast to the existing approach based on the Grassmannian manifold that can only incorporate one rank component. Our proposed technique characterizes the algebraic independence of a set of polynomials defined based on the sampling pattern and the TT decomposition, which is instrumental to obtaining the deterministic condition on the sampling pattern for finite completability. In addition, based on the proposed analysis, assuming that the entries of the tensor are sampled independently with probability $p$, we derive a lower bound on the sampling probability $p$, or equivalently, the number of sampled entries that ensures finite completability with high probability. Moreover, we also provide the deterministic and probabilistic conditions for unique completability.|paper analyz fundament condit low rank tensor complet given separ tensor train tt rank rank unfold exploit algebra structur tt decomposit obtain determinist necessari suffici condit locat sampl ensur finit complet specif propos algebra geometr analysi tt manifold incorpor whole rank vector simultan contrast exist approach base grassmannian manifold onli incorpor one rank compon propos techniqu character algebra independ set polynomi defin base sampl pattern tt decomposit instrument obtain determinist condit sampl pattern finit complet addit base propos analysi assum entri tensor sampl independ probabl deriv lower bound sampl probabl equival number sampl entri ensur finit complet high probabl moreov also provid determinist probabilist condit uniqu complet|['Morteza Ashraphijuo', 'Xiaodong Wang']|['cs.LG', 'cs.IT', 'math.AG', 'math.IT', 'stat.ML']
2017-03-28T14:02:02Z|2017-03-22T12:50:15Z|http://arxiv.org/abs/1703.07625v1|http://arxiv.org/pdf/1703.07625v1|Clustering for Different Scales of Measurement - the Gap-Ratio Weighted   K-means Algorithm|cluster differ scale measur gap ratio weight mean algorithm|This paper describes a method for clustering data that are spread out over large regions and which dimensions are on different scales of measurement. Such an algorithm was developed to implement a robotics application consisting in sorting and storing objects in an unsupervised way. The toy dataset used to validate such application consists of Lego bricks of different shapes and colors. The uncontrolled lighting conditions together with the use of RGB color features, respectively involve data with a large spread and different levels of measurement between data dimensions. To overcome the combination of these two characteristics in the data, we have developed a new weighted K-means algorithm, called gap-ratio K-means, which consists in weighting each dimension of the feature space before running the K-means algorithm. The weight associated with a feature is proportional to the ratio of the biggest gap between two consecutive data points, and the average of all the other gaps. This method is compared with two other variants of K-means on the Lego bricks clustering problem as well as two other common classification datasets.|paper describ method cluster data spread larg region dimens differ scale measur algorithm develop implement robot applic consist sort store object unsupervis way toy dataset use valid applic consist lego brick differ shape color uncontrol light condit togeth use rgb color featur respect involv data larg spread differ level measur data dimens overcom combin two characterist data develop new weight mean algorithm call gap ratio mean consist weight dimens featur space befor run mean algorithm weight associ featur proport ratio biggest gap two consecut data point averag gap method compar two variant mean lego brick cluster problem well two common classif dataset|['Joris Guérin', 'Olivier Gibaru', 'Stéphane Thiery', 'Eric Nyiri']|['cs.LG', 'cs.DS', 'stat.ML']
2017-03-28T14:02:02Z|2017-03-22T11:53:53Z|http://arxiv.org/abs/1703.07608v1|http://arxiv.org/pdf/1703.07608v1|Deep Exploration via Randomized Value Functions|deep explor via random valu function|We study the use of randomized value functions to guide deep exploration in reinforcement learning. This offers an elegant means for synthesizing statistically and computationally efficient exploration with common practical approaches to value function learning. We present several reinforcement learning algorithms that leverage randomized value functions and demonstrate their efficacy through computational studies. We also prove a regret bound that establishes statistical efficiency with a tabular representation.|studi use random valu function guid deep explor reinforc learn offer eleg mean synthes statist comput effici explor common practic approach valu function learn present sever reinforc learn algorithm leverag random valu function demonstr efficaci comput studi also prove regret bound establish statist effici tabular represent|['Ian Osband', 'Daniel Russo', 'Zheng Wen', 'Benjamin Van Roy']|['stat.ML', 'cs.AI', 'cs.LG']
2017-03-28T14:02:02Z|2017-03-22T11:48:11Z|http://arxiv.org/abs/1703.07607v1|http://arxiv.org/pdf/1703.07607v1|A probabilistic approach to emission-line galaxy classification|probabilist approach emiss line galaxi classif|This work employs a Gaussian mixture model (GMM) to jointly analyse two traditional emission-line classification schemes of galaxy ionization sources: the Baldwin-Phillips-Terlevich (BPT) and W$_{H\alpha}$ vs. [NII]/H$\alpha$ (WHAN) diagrams, using spectroscopic data from the Sloan Digital Sky Survey Data Release 7 and SEAGal/STARLIGHT datasets. We apply a GMM to empirically define classes of galaxies in a three-dimensional space spanned by the log [OIII]/H\beta, log [NII]/H\alpha, and log EW(H{\alpha}) optical parameters. The best-fit GMM based on several statistical criteria consists of four Gaussian components (GCs), which are capable to explain up to 97 per cent of the data variance. Using elements of information theory, we compare each GC to their respective astronomical counterpart. GC1 and GC4 are associated with star-forming galaxies, suggesting the need to define a new starburst subgroup. GC2 is associated with BPT's Active Galaxy Nuclei (AGN) class and WHAN's weak AGN class. GC3 is associated with BPT's composite class and WHAN's strong AGN class. Conversely, there is no statistical evidence -- based on GMMs -- for the existence of a Seyfert/LINER dichotomy in our sample. We demonstrate the potential of our methodology to recover/unravel different objects inside the wilderness of astronomical datasets, without lacking the ability to convey physically interpretable results; hence being a precious tool for maximum exploitation of the ever-increasing astronomical surveys. The probabilistic classifications from the GMM analysis are publicly available within the COINtoolbox (https://cointoolbox.github.io/GMM_Catalogue/)|work employ gaussian mixtur model gmm joint analys two tradit emiss line classif scheme galaxi ionize sourc baldwin phillip terlevich bpt alpha vs nii alpha whan diagram use spectroscop data sloan digit sky survey data releas seagal starlight dataset appli gmm empir defin class galaxi three dimension space span log oiii beta log nii alpha log ew alpha optic paramet best fit gmm base sever statist criteria consist four gaussian compon gcs capabl explain per cent data varianc use element inform theori compar gc respect astronom counterpart gc gc associ star form galaxi suggest need defin new starburst subgroup gc associ bpt activ galaxi nuclei agn class whan weak agn class gc associ bpt composit class whan strong agn class convers statist evid base gmms exist seyfert liner dichotomi sampl demonstr potenti methodolog recov unravel differ object insid wilder astronom dataset without lack abil convey physic interpret result henc precious tool maximum exploit ever increas astronom survey probabilist classif gmm analysi public avail within cointoolbox https cointoolbox github io gmm catalogu|['R. S. de Souza', 'M. L. L. Dantas', 'M. V. Costa-Duarte', 'E. D. Feigelson', 'M. Killedar', 'P. -Y. Lablanche', 'R. Vilalta', 'A. Krone-Martins', 'R. Beck', 'F. Gieseke']|['astro-ph.GA', 'astro-ph.IM', 'stat.ML']
2017-03-28T14:02:02Z|2017-03-22T10:40:40Z|http://arxiv.org/abs/1703.07596v1|http://arxiv.org/pdf/1703.07596v1|Testing and Learning on Distributions with Symmetric Noise Invariance|test learn distribut symmetr nois invari|Kernel embeddings of distributions and the Maximum Mean Discrepancy (MMD), the resulting distance between distributions, are useful tools for fully nonparametric two-sample testing and learning on distributions. However, it is rarely that all possible differences between samples are of interest -- discovered differences can be due to different types of measurement noise, data collection artefacts or other irrelevant sources of variability. We propose distances between distributions which encode invariance to additive symmetric noise, aimed at testing whether the assumed true underlying processes differ. Moreover, we construct invariant features of distributions, leading to learning algorithms robust to the impairment of the input distributions with symmetric additive noise. Such features lend themselves to a straightforward neural network implementation and can thus also be learned given a supervised signal.|kernel embed distribut maximum mean discrep mmd result distanc distribut use tool fulli nonparametr two sampl test learn distribut howev rare possibl differ sampl interest discov differ due differ type measur nois data collect artefact irrelev sourc variabl propos distanc distribut encod invari addit symmetr nois aim test whether assum true process differ moreov construct invari featur distribut lead learn algorithm robust impair input distribut symmetr addit nois featur lend themselv straightforward neural network implement thus also learn given supervis signal|['Ho Chung Leon Law', 'Christopher Yau', 'Dino Sejdinovic']|['stat.ML']
2017-03-28T14:02:02Z|2017-03-22T03:26:32Z|http://arxiv.org/abs/1703.07506v1|http://arxiv.org/abs/1703.07506v1|LogitBoost autoregressive networks|logitboost autoregress network|Multivariate binary distributions can be decomposed into products of univariate conditional distributions. Recently popular approaches have modeled these conditionals through neural networks with sophisticated weight-sharing structures. It is shown that state-of-the-art performance on several standard benchmark datasets can actually be achieved by training separate probability estimators for each dimension. In that case, model training can be trivially parallelized over data dimensions. On the other hand, complexity control has to be performed for each learned conditional distribution. Three possible methods are considered and experimentally compared. The estimator that is employed for each conditional is LogitBoost. Similarities and differences between the proposed approach and autoregressive models based on neural networks are discussed in detail.|multivari binari distribut decompos product univari condit distribut recent popular approach model condit neural network sophist weight share structur shown state art perform sever standard benchmark dataset actual achiev train separ probabl estim dimens case model train trivial parallel data dimens hand complex control perform learn condit distribut three possibl method consid experiment compar estim employ condit logitboost similar differ propos approach autoregress model base neural network discuss detail|['Marc Goessling']|['stat.ML', 'cs.LG']
2017-03-28T14:02:02Z|2017-03-21T23:56:51Z|http://arxiv.org/abs/1703.07473v1|http://arxiv.org/pdf/1703.07473v1|Episode-Based Active Learning with Bayesian Neural Networks|episod base activ learn bayesian neural network|We investigate different strategies for active learning with Bayesian deep neural networks. We focus our analysis on scenarios where new, unlabeled data is obtained episodically, such as commonly encountered in mobile robotics applications. An evaluation of different strategies for acquisition, updating, and final training on the CIFAR-10 dataset shows that incremental network updates with final training on the accumulated acquisition set are essential for best performance, while limiting the amount of required human labeling labor.|investig differ strategi activ learn bayesian deep neural network focus analysi scenario new unlabel data obtain episod common encount mobil robot applic evalu differ strategi acquisit updat final train cifar dataset show increment network updat final train accumul acquisit set essenti best perform limit amount requir human label labor|['Feras Dayoub', 'Niko Sünderhauf', 'Peter Corke']|['cs.CV', 'cs.LG', 'stat.ML']
2017-03-28T14:02:02Z|2017-03-21T18:05:31Z|http://arxiv.org/abs/1703.07370v1|http://arxiv.org/pdf/1703.07370v1|REBAR: Low-variance, unbiased gradient estimates for discrete latent   variable models|rebar low varianc unbias gradient estim discret latent variabl model|Learning in models with discrete latent variables is challenging due to high variance gradient estimators. Generally, approaches have relied on control variates to reduce the variance of the REINFORCE estimator. Recent work (Jang et al. 2016, Maddison et al. 2016) has taken a different approach, introducing a continuous relaxation of discrete variables to produce low-variance, but biased, gradient estimates. In this work, we combine the two approaches through a novel control variate that produces low-variance, unbiased gradient estimates. We present encouraging preliminary results on a toy problem and on learning sigmoid belief networks.|learn model discret latent variabl challeng due high varianc gradient estim general approach reli control variat reduc varianc reinforc estim recent work jang et al maddison et al taken differ approach introduc continu relax discret variabl produc low varianc bias gradient estim work combin two approach novel control variat produc low varianc unbias gradient estim present encourag preliminari result toy problem learn sigmoid belief network|['George Tucker', 'Andriy Mnih', 'Chris J. Maddison', 'Jascha Sohl-Dickstein']|['cs.LG', 'stat.ML']
2017-03-28T14:02:02Z|2017-03-21T18:00:02Z|http://arxiv.org/abs/1703.07355v1|http://arxiv.org/abs/1703.07355v1|An Army of Me: Sockpuppets in Online Discussion Communities|armi sockpuppet onlin discuss communiti|"In online discussion communities, users can interact and share information and opinions on a wide variety of topics. However, some users may create multiple identities, or sockpuppets, and engage in undesired behavior by deceiving others or manipulating discussions. In this work, we study sockpuppetry across nine discussion communities, and show that sockpuppets differ from ordinary users in terms of their posting behavior, linguistic traits, as well as social network structure. Sockpuppets tend to start fewer discussions, write shorter posts, use more personal pronouns such as ""I"", and have more clustered ego-networks. Further, pairs of sockpuppets controlled by the same individual are more likely to interact on the same discussion at the same time than pairs of ordinary users. Our analysis suggests a taxonomy of deceptive behavior in discussion communities. Pairs of sockpuppets can vary in their deceptiveness, i.e., whether they pretend to be different users, or their supportiveness, i.e., if they support arguments of other sockpuppets controlled by the same user. We apply these findings to a series of prediction tasks, notably, to identify whether a pair of accounts belongs to the same underlying user or not. Altogether, this work presents a data-driven view of deception in online discussion communities and paves the way towards the automatic detection of sockpuppets."|onlin discuss communiti user interact share inform opinion wide varieti topic howev user may creat multipl ident sockpuppet engag undesir behavior deceiv manipul discuss work studi sockpuppetri across nine discuss communiti show sockpuppet differ ordinari user term post behavior linguist trait well social network structur sockpuppet tend start fewer discuss write shorter post use person pronoun cluster ego network pair sockpuppet control individu like interact discuss time pair ordinari user analysi suggest taxonomi decept behavior discuss communiti pair sockpuppet vari decept whether pretend differ user support support argument sockpuppet control user appli find seri predict task notabl identifi whether pair account belong user altogeth work present data driven view decept onlin discuss communiti pave way toward automat detect sockpuppet|['Srijan Kumar', 'Justin Cheng', 'Jure Leskovec', 'V. S. Subrahmanian']|['cs.SI', 'cs.CY', 'physics.soc-ph', 'stat.AP', 'stat.ML']
2017-03-28T14:02:02Z|2017-03-21T17:58:03Z|http://arxiv.org/abs/1703.07345v1|http://arxiv.org/pdf/1703.07345v1|On The Projection Operator to A Three-view Cardinality Constrained Set|project oper three view cardin constrain set|The cardinality constraint is an intrinsic way to restrict the solution structure in many domains, for example, sparse learning, feature selection, and compressed sensing. To solve a cardinality constrained problem, the key challenge is to solve the projection onto the cardinality constraint set, which is NP-hard in general when there exist multiple overlapped cardiaiality constraints. In this paper, we consider the scenario where overlapped cardinality constraints satisfy a Three-view Cardinality Structure (TVCS), which reflects the natural restriction in many applications, such as identification of gene regulatory networks and task-worker assignment problem. We cast the projection onto the TVCS set into a linear programming, and prove that its solution can be obtained by finding an integer solution to such linear programming. We further prove that such integer solution can be found with the complexity proportional to the problem scale. We finally use synthetic experiments and two interesting applications in bioinformatics and crowdsourcing to validate the proposed TVCS model and method.|cardin constraint intrins way restrict solut structur mani domain exampl spars learn featur select compress sens solv cardin constrain problem key challeng solv project onto cardin constraint set np hard general exist multipl overlap cardiaial constraint paper consid scenario overlap cardin constraint satisfi three view cardin structur tvcs reflect natur restrict mani applic identif gene regulatori network task worker assign problem cast project onto tvcs set linear program prove solut obtain find integ solut linear program prove integ solut found complex proport problem scale final use synthet experi two interest applic bioinformat crowdsourc valid propos tvcs model method|['Haichuan Yang', 'Shupeng Gui', 'Chuyang Ke', 'Daniel Stefankovic', 'Ryohei Fujimaki', 'Ji Liu']|['cs.LG', 'stat.ML']
2017-03-28T14:02:06Z|2017-03-21T16:39:28Z|http://arxiv.org/abs/1703.07305v1|http://arxiv.org/abs/1703.07305v1|Targeting Bayes factors with direct-path non-equilibrium thermodynamic   integration|target bay factor direct path non equilibrium thermodynam integr|Thermodynamic integration (TI) for computing marginal likelihoods is based on an inverse annealing path from the prior to the posterior distribution. In many cases, the resulting estimator suffers from high variability, which particularly stems from the prior regime. When comparing complex models with differences in a comparatively small number of parameters, intrinsic errors from sampling fluctuations may outweigh the differences in the log marginal likelihood estimates. In the present article, we propose a thermodynamic integration scheme that directly targets the log Bayes factor. The method is based on a modified annealing path between the posterior distributions of the two models compared, which systematically avoids the high variance prior regime. We combine this scheme with the concept of non-equilibrium TI to minimise discretisation errors from numerical integration. Results obtained on Bayesian regression models applied to standard benchmark data, and a complex hierarchical model applied to biopathway inference, demonstrate a significant reduction in estimator variance over state-of-the-art TI methods.|thermodynam integr ti comput margin likelihood base invers anneal path prior posterior distribut mani case result estim suffer high variabl particular stem prior regim compar complex model differ compar small number paramet intrins error sampl fluctuat may outweigh differ log margin likelihood estim present articl propos thermodynam integr scheme direct target log bay factor method base modifi anneal path posterior distribut two model compar systemat avoid high varianc prior regim combin scheme concept non equilibrium ti minimis discretis error numer integr result obtain bayesian regress model appli standard benchmark data complex hierarch model appli biopathway infer demonstr signific reduct estim varianc state art ti method|['Marco Grzegorczyk', 'Andrej Aderhold', 'Dirk Husmeier']|['stat.ME', 'stat.ML']
2017-03-28T14:02:06Z|2017-03-21T15:42:38Z|http://arxiv.org/abs/1703.07285v1|http://arxiv.org/pdf/1703.07285v1|From safe screening rules to working sets for faster Lasso-type solvers|safe screen rule work set faster lasso type solver|Convex sparsity-promoting regularizations are ubiquitous in modern statistical learning. By construction, they yield solutions with few non-zero coefficients, which correspond to saturated constraints in the dual optimization formulation. Working set (WS) strategies are generic optimization techniques that consist in solving simpler problems that only consider a subset of constraints, whose indices form the WS. Working set methods therefore involve two nested iterations: the outer loop corresponds to the definition of the WS and the inner loop calls a solver for the subproblems. For the Lasso estimator a WS is a set of features, while for a Group Lasso it refers to a set of groups. In practice, WS are generally small in this context so the associated feature Gram matrix can fit in memory. Here we show that the Gauss-Southwell rule (a greedy strategy for block coordinate descent techniques) leads to fast solvers in this case. Combined with a working set strategy based on an aggressive use of so-called Gap Safe screening rules, we propose a solver achieving state-of-the-art performance on sparse learning problems. Results are presented on Lasso and multi-task Lasso estimators.|convex sparsiti promot regular ubiquit modern statist learn construct yield solut non zero coeffici correspond satur constraint dual optim formul work set ws strategi generic optim techniqu consist solv simpler problem onli consid subset constraint whose indic form ws work set method therefor involv two nest iter outer loop correspond definit ws inner loop call solver subproblem lasso estim ws set featur group lasso refer set group practic ws general small context associ featur gram matrix fit memori show gauss southwel rule greedi strategi block coordin descent techniqu lead fast solver case combin work set strategi base aggress use call gap safe screen rule propos solver achiev state art perform spars learn problem result present lasso multi task lasso estim|['Mathurin Massias', 'Alexandre Gramfort', 'Joseph Salmon']|['stat.ML', 'cs.LG', 'math.OC', 'stat.CO']
2017-03-28T14:02:06Z|2017-03-22T17:08:40Z|http://arxiv.org/abs/1703.07255v2|http://arxiv.org/pdf/1703.07255v2|ZM-Net: Real-time Zero-shot Image Manipulation Network|zm net real time zero shot imag manipul network|Many problems in image processing and computer vision (e.g. colorization, style transfer) can be posed as 'manipulating' an input image into a corresponding output image given a user-specified guiding signal. A holy-grail solution towards generic image manipulation should be able to efficiently alter an input image with any personalized signals (even signals unseen during training), such as diverse paintings and arbitrary descriptive attributes. However, existing methods are either inefficient to simultaneously process multiple signals (let alone generalize to unseen signals), or unable to handle signals from other modalities. In this paper, we make the first attempt to address the zero-shot image manipulation task. We cast this problem as manipulating an input image according to a parametric model whose key parameters can be conditionally generated from any guiding signal (even unseen ones). To this end, we propose the Zero-shot Manipulation Net (ZM-Net), a fully-differentiable architecture that jointly optimizes an image-transformation network (TNet) and a parameter network (PNet). The PNet learns to generate key transformation parameters for the TNet given any guiding signal while the TNet performs fast zero-shot image manipulation according to both signal-dependent parameters from the PNet and signal-invariant parameters from the TNet itself. Extensive experiments show that our ZM-Net can perform high-quality image manipulation conditioned on different forms of guiding signals (e.g. style images and attributes) in real-time (tens of milliseconds per image) even for unseen signals. Moreover, a large-scale style dataset with over 20,000 style images is also constructed to promote further research.|mani problem imag process comput vision color style transfer pose manipul input imag correspond output imag given user specifi guid signal holi grail solut toward generic imag manipul abl effici alter input imag ani person signal even signal unseen dure train divers paint arbitrari descript attribut howev exist method either ineffici simultan process multipl signal let alon general unseen signal unabl handl signal modal paper make first attempt address zero shot imag manipul task cast problem manipul input imag accord parametr model whose key paramet condit generat ani guid signal even unseen one end propos zero shot manipul net zm net fulli differenti architectur joint optim imag transform network tnet paramet network pnet pnet learn generat key transform paramet tnet given ani guid signal tnet perform fast zero shot imag manipul accord signal depend paramet pnet signal invari paramet tnet extens experi show zm net perform high qualiti imag manipul condit differ form guid signal style imag attribut real time ten millisecond per imag even unseen signal moreov larg scale style dataset style imag also construct promot research|['Hao Wang', 'Xiaodan Liang', 'Hao Zhang', 'Dit-Yan Yeung', 'Eric P. Xing']|['cs.CV', 'cs.AI', 'cs.GR', 'cs.LG', 'stat.ML']
2017-03-28T14:02:06Z|2017-03-21T13:02:35Z|http://arxiv.org/abs/1703.07198v1|http://arxiv.org/pdf/1703.07198v1|Overcoming model simplifications when quantifying predictive uncertainty|overcom model simplif quantifi predict uncertainti|It is generally accepted that all models are wrong -- the difficulty is determining which are useful. Here, a useful model is considered as one that is capable of combining data and expert knowledge, through an inversion or calibration process, to adequately characterize the uncertainty in predictions of interest. This paper derives conditions that specify which simplified models are useful and how they should be calibrated. To start, the notion of an optimal simplification is defined. This relates the model simplifications to the nature of the data and predictions, and determines when a standard probabilistic calibration scheme is capable of accurately characterizing uncertainty. Furthermore, two additional conditions are defined for suboptimal models that determine when the simplifications can be safely ignored. The first allows a suboptimally simplified model to be used in a way that replicates the performance of an optimal model. This is achieved through the judicial selection of a prior term for the calibration process that explicitly includes the nature of the data, predictions and modelling simplifications. The second considers the dependency structure between the predictions and the available data to gain insights into when the simplifications can be overcome by using the right calibration data. Furthermore, the derived conditions are related to the commonly used calibration schemes based on Tikhonov and subspace regularization. To allow concrete insights to be obtained, the analysis is performed under a linear expansion of the model equations and where the predictive uncertainty is characterized via second order moments only.|general accept model wrong difficulti determin use use model consid one capabl combin data expert knowledg invers calibr process adequ character uncertainti predict interest paper deriv condit specifi simplifi model use calibr start notion optim simplif defin relat model simplif natur data predict determin standard probabilist calibr scheme capabl accur character uncertainti furthermor two addit condit defin suboptim model determin simplif safe ignor first allow suboptim simplifi model use way replic perform optim model achiev judici select prior term calibr process explicit includ natur data predict model simplif second consid depend structur predict avail data gain insight simplif overcom use right calibr data furthermor deriv condit relat common use calibr scheme base tikhonov subspac regular allow concret insight obtain analysi perform linear expans model equat predict uncertainti character via second order moment onli|['George M. Mathews', 'John Vial']|['stat.ML', 'math.PR', 'physics.comp-ph', 'physics.geo-ph', 'stat.ME', '62F15, 62C10, 68U05, 93E12, 93B11, 62P12']
2017-03-28T14:02:06Z|2017-03-21T12:33:19Z|http://arxiv.org/abs/1703.07169v1|http://arxiv.org/pdf/1703.07169v1|A Deterministic Global Optimization Method for Variational Inference|determinist global optim method variat infer|Variational inference methods for latent variable statistical models have gained popularity because they are relatively fast, can handle large data sets, and have deterministic convergence guarantees. However, in practice it is unclear whether the fixed point identified by the variational inference algorithm is a local or a global optimum. Here, we propose a method for constructing iterative optimization algorithms for variational inference problems that are guaranteed to converge to the $\epsilon$-global variational lower bound on the log-likelihood. We derive inference algorithms for two variational approximations to a standard Bayesian Gaussian mixture model (BGMM). We present a minimal data set for empirically testing convergence and show that a variational inference algorithm frequently converges to a local optimum while our algorithm always converges to the globally optimal variational lower bound. We characterize the loss incurred by choosing a non-optimal variational approximation distribution suggesting that selection of the approximating variational distribution deserves as much attention as the selection of the original statistical model for a given data set.|variat infer method latent variabl statist model gain popular becaus relat fast handl larg data set determinist converg guarante howev practic unclear whether fix point identifi variat infer algorithm local global optimum propos method construct iter optim algorithm variat infer problem guarante converg epsilon global variat lower bound log likelihood deriv infer algorithm two variat approxim standard bayesian gaussian mixtur model bgmm present minim data set empir test converg show variat infer algorithm frequent converg local optimum algorithm alway converg global optim variat lower bound character loss incur choos non optim variat approxim distribut suggest select approxim variat distribut deserv much attent select origin statist model given data set|['Hachem Saddiki', 'Andrew C. Trapp', 'Patrick Flaherty']|['stat.ME', 'stat.ML']
2017-03-28T14:02:06Z|2017-03-21T10:34:59Z|http://arxiv.org/abs/1703.07131v1|http://arxiv.org/pdf/1703.07131v1|Knowledge distillation using unlabeled mismatched images|knowledg distil use unlabel mismatch imag|Current approaches for Knowledge Distillation (KD) either directly use training data or sample from the training data distribution. In this paper, we demonstrate effectiveness of 'mismatched' unlabeled stimulus to perform KD for image classification networks. For illustration, we consider scenarios where this is a complete absence of training data, or mismatched stimulus has to be used for augmenting a small amount of training data. We demonstrate that stimulus complexity is a key factor for distillation's good performance. Our examples include use of various datasets for stimulating MNIST and CIFAR teachers.|current approach knowledg distil kd either direct use train data sampl train data distribut paper demonstr effect mismatch unlabel stimulus perform kd imag classif network illustr consid scenario complet absenc train data mismatch stimulus use augment small amount train data demonstr stimulus complex key factor distil good perform exampl includ use various dataset stimul mnist cifar teacher|['Mandar Kulkarni', 'Kalpesh Patil', 'Shirish Karande']|['cs.CV', 'cs.LG', 'stat.ML']
2017-03-28T14:02:06Z|2017-03-21T05:08:33Z|http://arxiv.org/abs/1703.07056v1|http://arxiv.org/pdf/1703.07056v1|Stochastic Primal Dual Coordinate Method with Non-Uniform Sampling Based   on Optimality Violations|stochast primal dual coordin method non uniform sampl base optim violat|We study primal-dual type stochastic optimization algorithms with non-uniform sampling. Our main theoretical contribution in this paper is to present a convergence analysis of Stochastic Primal Dual Coordinate (SPDC) Method with arbitrary sampling. Based on this theoretical framework, we propose Optimality Violation-based Sampling SPDC (ovsSPDC), a non-uniform sampling method based on Optimality Violation. We also propose two efficient heuristic variants of ovsSPDC called ovsSDPC+ and ovsSDPC++. Through intensive numerical experiments, we demonstrate that the proposed method and its variants are faster than other state-of-the-art primal-dual type stochastic optimization methods.|studi primal dual type stochast optim algorithm non uniform sampl main theoret contribut paper present converg analysi stochast primal dual coordin spdc method arbitrari sampl base theoret framework propos optim violat base sampl spdc ovsspdc non uniform sampl method base optim violat also propos two effici heurist variant ovsspdc call ovssdpc ovssdpc intens numer experi demonstr propos method variant faster state art primal dual type stochast optim method|['Atsushi Shibagaki', 'Ichiro Takeuchi']|['stat.ML', 'cs.LG', 'math.OC']
2017-03-28T14:02:06Z|2017-03-21T04:11:13Z|http://arxiv.org/abs/1703.07047v1|http://arxiv.org/pdf/1703.07047v1|High-Resolution Breast Cancer Screening with Multi-View Deep   Convolutional Neural Networks|high resolut breast cancer screen multi view deep convolut neural network|Recent advances in deep learning for object recognition in natural images has prompted a surge of interest in applying a similar set of techniques to medical images. Most of the initial attempts largely focused on replacing the input to such a deep convolutional neural network from a natural image to a medical image. This, however, does not take into consideration the fundamental differences between these two types of data. More specifically, detection or recognition of an anomaly in medical images depends significantly on fine details, unlike object recognition in natural images where coarser, more global structures matter more. This difference makes it inadequate to use the existing deep convolutional neural networks architectures, which were developed for natural images, because they rely on heavily downsampling an image to a much lower resolution to reduce the memory requirements. This hides details necessary to make accurate predictions for medical images. Furthermore, a single exam in medical imaging often comes with a set of different views which must be seamlessly fused in order to reach a correct conclusion. In our work, we propose to use a multi-view deep convolutional neural network that handles a set of more than one high-resolution medical image. We evaluate this network on large-scale mammography-based breast cancer screening (BI-RADS prediction) using 103 thousand images. We focus on investigating the impact of training set sizes and image sizes on the prediction accuracy. Our results highlight that performance clearly increases with the size of training set, and that the best performance can only be achieved using the images in the original resolution. This suggests the future direction of medical imaging research using deep neural networks is to utilize as much data as possible with the least amount of potentially harmful preprocessing.|recent advanc deep learn object recognit natur imag prompt surg interest appli similar set techniqu medic imag initi attempt larg focus replac input deep convolut neural network natur imag medic imag howev doe take consider fundament differ two type data specif detect recognit anomali medic imag depend signific fine detail unlik object recognit natur imag coarser global structur matter differ make inadequ use exist deep convolut neural network architectur develop natur imag becaus reli heavili downsampl imag much lower resolut reduc memori requir hide detail necessari make accur predict medic imag furthermor singl exam medic imag often come set differ view must seamless fuse order reach correct conclus work propos use multi view deep convolut neural network handl set one high resolut medic imag evalu network larg scale mammographi base breast cancer screen bi rad predict use thousand imag focus investig impact train set size imag size predict accuraci result highlight perform clear increas size train set best perform onli achiev use imag origin resolut suggest futur direct medic imag research use deep neural network util much data possibl least amount potenti harm preprocess|['Krzysztof J. Geras', 'Stacey Wolfson', 'S. Gene Kim', 'Linda Moy', 'Kyunghyun Cho']|['cs.CV', 'cs.LG', 'stat.ML']
2017-03-28T14:02:06Z|2017-03-21T02:08:05Z|http://arxiv.org/abs/1703.07027v1|http://arxiv.org/pdf/1703.07027v1|Nonparametric Variational Auto-encoders for Hierarchical Representation   Learning|nonparametr variat auto encod hierarch represent learn|The recently developed variational autoencoders (VAEs) have proved to be an effective confluence of the rich representational power of neural networks with Bayesian methods. However, most work on VAEs use a rather simple prior over the latent variables such as standard normal distribution, thereby restricting its applications to relatively simple phenomena. In this work, we propose hierarchical nonparametric variational autoencoders, which combines tree-structured Bayesian nonparametric priors with VAEs, to enable infinite flexibility of the latent representation space. Both the neural parameters and Bayesian priors are learned jointly using tailored variational inference. The resulting model induces a hierarchical structure of latent semantic concepts underlying the data corpus, and infers accurate representations of data instances. We apply our model in video representation learning. Our method is able to discover highly interpretable activity hierarchies, and obtain improved clustering accuracy and generalization capacity based on the learned rich representations.|recent develop variat autoencod vae prove effect confluenc rich represent power neural network bayesian method howev work vae use rather simpl prior latent variabl standard normal distribut therebi restrict applic relat simpl phenomena work propos hierarch nonparametr variat autoencod combin tree structur bayesian nonparametr prior vae enabl infinit flexibl latent represent space neural paramet bayesian prior learn joint use tailor variat infer result model induc hierarch structur latent semant concept data corpus infer accur represent data instanc appli model video represent learn method abl discov high interpret activ hierarchi obtain improv cluster accuraci general capac base learn rich represent|['Prasoon Goyal', 'Zhiting Hu', 'Xiaodan Liang', 'Chenyu Wang', 'Eric Xing']|['cs.LG', 'stat.ML']
2017-03-28T14:02:06Z|2017-03-21T02:04:30Z|http://arxiv.org/abs/1703.07026v1|http://arxiv.org/pdf/1703.07026v1|Cross-modal Deep Metric Learning with Multi-task Regularization|cross modal deep metric learn multi task regular|DNN-based cross-modal retrieval has become a research hotspot, by which users can search results across various modalities like image and text. However, existing methods mainly focus on the pairwise correlation and reconstruction error of labeled data. They ignore the semantically similar and dissimilar constraints between different modalities, and cannot take advantage of unlabeled data. This paper proposes Cross-modal Deep Metric Learning with Multi-task Regularization (CDMLMR), which integrates quadruplet ranking loss and semi-supervised contrastive loss for modeling cross-modal semantic similarity in a unified multi-task learning architecture. The quadruplet ranking loss can model the semantically similar and dissimilar constraints to preserve cross-modal relative similarity ranking information. The semi-supervised contrastive loss is able to maximize the semantic similarity on both labeled and unlabeled data. Compared to the existing methods, CDMLMR exploits not only the similarity ranking information but also unlabeled cross-modal data, and thus boosts cross-modal retrieval accuracy.|dnn base cross modal retriev becom research hotspot user search result across various modal like imag text howev exist method main focus pairwis correl reconstruct error label data ignor semant similar dissimilar constraint differ modal cannot take advantag unlabel data paper propos cross modal deep metric learn multi task regular cdmlmr integr quadruplet rank loss semi supervis contrast loss model cross modal semant similar unifi multi task learn architectur quadruplet rank loss model semant similar dissimilar constraint preserv cross modal relat similar rank inform semi supervis contrast loss abl maxim semant similar label unlabel data compar exist method cdmlmr exploit onli similar rank inform also unlabel cross modal data thus boost cross modal retriev accuraci|['Xin Huang', 'Yuxin Peng']|['cs.LG', 'cs.CV', 'stat.ML']
2017-03-28T14:02:09Z|2017-03-20T22:32:36Z|http://arxiv.org/abs/1703.06990v1|http://arxiv.org/pdf/1703.06990v1|Metalearning for Feature Selection|metalearn featur select|"A general formulation of optimization problems in which various candidate solutions may use different feature-sets is presented, encompassing supervised classification, automated program learning and other cases. A novel characterization of the concept of a ""good quality feature"" for such an optimization problem is provided; and a proposal regarding the integration of quality based feature selection into metalearning is suggested, wherein the quality of a feature for a problem is estimated using knowledge about related features in the context of related problems. Results are presented regarding extensive testing of this ""feature metalearning"" approach on supervised text classification problems; it is demonstrated that, in this context, feature metalearning can provide significant and sometimes dramatic speedup over standard feature selection heuristics."|general formul optim problem various candid solut may use differ featur set present encompass supervis classif autom program learn case novel character concept good qualiti featur optim problem provid propos regard integr qualiti base featur select metalearn suggest wherein qualiti featur problem estim use knowledg relat featur context relat problem result present regard extens test featur metalearn approach supervis text classif problem demonstr context featur metalearn provid signific sometim dramat speedup standard featur select heurist|['Ben Goertzel', 'Nil Geisweiller', 'Chris Poulin']|['cs.LG', 'stat.ML']
2017-03-28T14:02:09Z|2017-03-20T21:29:18Z|http://arxiv.org/abs/1703.06975v1|http://arxiv.org/pdf/1703.06975v1|Learning to Generate Samples from Noise through Infusion Training|learn generat sampl nois infus train|In this work, we investigate a novel training procedure to learn a generative model as the transition operator of a Markov chain, such that, when applied repeatedly on an unstructured random noise sample, it will denoise it into a sample that matches the target distribution from the training set. The novel training procedure to learn this progressive denoising operation involves sampling from a slightly different chain than the model chain used for generation in the absence of a denoising target. In the training chain we infuse information from the training target example that we would like the chains to reach with a high probability. The thus learned transition operator is able to produce quality and varied samples in a small number of steps. Experiments show competitive results compared to the samples generated with a basic Generative Adversarial Net|work investig novel train procedur learn generat model transit oper markov chain appli repeat unstructur random nois sampl denois sampl match target distribut train set novel train procedur learn progress denois oper involv sampl slight differ chain model chain use generat absenc denois target train chain infus inform train target exampl would like chain reach high probabl thus learn transit oper abl produc qualiti vari sampl small number step experi show competit result compar sampl generat basic generat adversari net|['Florian Bordes', 'Sina Honari', 'Pascal Vincent']|['stat.ML', 'cs.LG']
2017-03-28T14:02:09Z|2017-03-20T19:26:00Z|http://arxiv.org/abs/1703.06934v1|http://arxiv.org/pdf/1703.06934v1|Ensemble representation learning: an analysis of fitness and survival   for wrapper-based genetic programming methods|ensembl represent learn analysi fit surviv wrapper base genet program method|Recently we proposed a general, ensemble-based feature engineering wrapper (FEW) that was paired with a number of machine learning methods to solve regression problems. Here, we adapt FEW for supervised classification and perform a thorough analysis of fitness and survival methods within this framework. Our tests demonstrate that two fitness metrics, one introduced as an adaptation of the silhouette score, outperform the more commonly used Fisher criterion. We analyze survival methods and demonstrate that $\epsilon$-lexicase survival works best across our test problems, followed by random survival which outperforms both tournament and deterministic crowding. We conduct hyper-parameter optimization for several classification methods using a large set of problems to benchmark the ability of FEW to improve data representations. The results show that FEW can improve the best classifier performance on several problems. We show that FEW generates readable and meaningful features for a biomedical problem with different ML pairings.|recent propos general ensembl base featur engin wrapper pair number machin learn method solv regress problem adapt supervis classif perform thorough analysi fit surviv method within framework test demonstr two fit metric one introduc adapt silhouett score outperform common use fisher criterion analyz surviv method demonstr epsilon lexicas surviv work best across test problem follow random surviv outperform tournament determinist crowd conduct hyper paramet optim sever classif method use larg set problem benchmark abil improv data represent result show improv best classifi perform sever problem show generat readabl meaning featur biomed problem differ ml pair|['William La Cava', 'Jason H. Moore']|['cs.NE', 'cs.LG', 'stat.ML']
2017-03-28T14:02:09Z|2017-03-22T07:44:55Z|http://arxiv.org/abs/1703.06891v2|http://arxiv.org/pdf/1703.06891v2|Dance Dance Convolution|danc danc convolut|Dance Dance Revolution (DDR) is a popular rhythm-based video game. Players perform steps on a dance platform in synchronization with music as directed by on-screen step charts. While many step charts are available in standardized packs, users may grow tired of existing charts, or wish to dance to a song for which no chart exists. We introduce the task of learning to choreograph. Given a raw audio track, the goal is to produce a new step chart. This task decomposes naturally into two subtasks: deciding when to place steps and deciding which steps to select. For the step placement task, we combine recurrent and convolutional neural networks to ingest spectrograms of low-level audio features to predict steps, conditioned on chart difficulty. For step selection, we present a conditional LSTM generative model that substantially outperforms n-gram and fixed-window approaches.|danc danc revolut ddr popular rhythm base video game player perform step danc platform synchron music direct screen step chart mani step chart avail standard pack user may grow tire exist chart wish danc song chart exist introduc task learn choreograph given raw audio track goal produc new step chart task decompos natur two subtask decid place step decid step select step placement task combin recurr convolut neural network ingest spectrogram low level audio featur predict step condit chart difficulti step select present condit lstm generat model substanti outperform gram fix window approach|['Chris Donahue', 'Zachary C. Lipton', 'Julian McAuley']|['cs.LG', 'cs.MM', 'cs.NE', 'cs.SD', 'stat.ML']
2017-03-28T14:02:09Z|2017-03-20T17:21:19Z|http://arxiv.org/abs/1703.06857v1|http://arxiv.org/pdf/1703.06857v1|Deep Neural Networks Do Not Recognize Negative Images|deep neural network recogn negat imag|"Deep Neural Networks (DNNs) have achieved remarkable performance on a variety of pattern-recognition tasks, particularly visual classification problems, where new algorithms reported to achieve or even surpass the human performance. In this paper, we test the state-of-the-art DNNs with negative images and show that the accuracy drops to the level of random classification. This leads us to the conjecture that the DNNs, which are merely trained on raw data, do not recognize the semantics of the objects, but rather memorize the inputs. We suggest that negative images can be thought as ""semantic adversarial examples"", which we define as transformed inputs that semantically represent the same objects, but the model does not classify them correctly."|deep neural network dnns achiev remark perform varieti pattern recognit task particular visual classif problem new algorithm report achiev even surpass human perform paper test state art dnns negat imag show accuraci drop level random classif lead us conjectur dnns mere train raw data recogn semant object rather memor input suggest negat imag thought semant adversari exampl defin transform input semant repres object model doe classifi correct|['Hossein Hosseini', 'Radha Poovendran']|['cs.CV', 'cs.LG', 'stat.ML']
2017-03-28T14:02:09Z|2017-03-20T17:18:57Z|http://arxiv.org/abs/1703.06856v1|http://arxiv.org/pdf/1703.06856v1|Counterfactual Fairness|counterfactu fair|Machine learning has matured to the point to where it is now being considered to automate decisions in loan lending, employee hiring, and predictive policing. In many of these scenarios however, previous decisions have been made that are unfairly biased against certain subpopulations (e.g., those of a particular race, gender, or sexual orientation). Because this past data is often biased, machine learning predictors must account for this to avoid perpetuating discriminatory practices (or incidentally making new ones). In this paper, we develop a framework for modeling fairness in any dataset using tools from counterfactual inference. We propose a definition called counterfactual fairness that captures the intuition that a decision is fair towards an individual if it gives the same predictions in (a) the observed world and (b) a world where the individual had always belonged to a different demographic group, other background causes of the outcome being equal. We demonstrate our framework on two real-world problems: fair prediction of law school success, and fair modeling of an individual's criminality in policing data.|machin learn matur point consid autom decis loan lend employe hire predict polic mani scenario howev previous decis made unfair bias certain subpopul particular race gender sexual orient becaus past data often bias machin learn predictor must account avoid perpetu discriminatori practic incident make new one paper develop framework model fair ani dataset use tool counterfactu infer propos definit call counterfactu fair captur intuit decis fair toward individu give predict observ world world individu alway belong differ demograph group background caus outcom equal demonstr framework two real world problem fair predict law school success fair model individu crimin polic data|['Matt J. Kusner', 'Joshua R. Loftus', 'Chris Russell', 'Ricardo Silva']|['stat.ML', 'cs.CY', 'cs.LG']
2017-03-28T14:02:09Z|2017-03-20T15:43:10Z|http://arxiv.org/abs/1703.06807v1|http://arxiv.org/pdf/1703.06807v1|Variance Reduced Stochastic Gradient Descent with Sufficient Decrease|varianc reduc stochast gradient descent suffici decreas|The sufficient decrease technique has been widely used in deterministic optimization, even for non-convex optimization problems, such as line-search techniques. Motivated by those successes, we propose a novel sufficient decrease framework for a class of variance reduced stochastic gradient descent (VR-SGD) methods such as SVRG and SAGA. In order to make sufficient decrease for stochastic optimization, we design a new sufficient decrease criterion. We then introduce a coefficient \theta to satisfy the sufficient decrease property, which takes the decisions to shrink, expand or move in the opposite direction (i.e., \theta x for the variable x), and give two specific update rules for Lasso and ridge regression. Moreover, we analyze the convergence properties of our algorithms for strongly convex problems, which show that both of our algorithms attain linear convergence rates. We also provide the convergence guarantees of both of our algorithms for non-strongly convex problems. Our experimental results further verify that our algorithms achieve better performance than their counterparts.|suffici decreas techniqu wide use determinist optim even non convex optim problem line search techniqu motiv success propos novel suffici decreas framework class varianc reduc stochast gradient descent vr sgd method svrg saga order make suffici decreas stochast optim design new suffici decreas criterion introduc coeffici theta satisfi suffici decreas properti take decis shrink expand move opposit direct theta variabl give two specif updat rule lasso ridg regress moreov analyz converg properti algorithm strong convex problem show algorithm attain linear converg rate also provid converg guarante algorithm non strong convex problem experiment result verifi algorithm achiev better perform counterpart|['Fanhua Shang', 'Yuanyuan Liu', 'James Cheng', 'Kelvin Kai Wing Ng', 'Yuichi Yoshida']|['cs.LG', 'math.OC', 'stat.ML']
2017-03-28T14:02:09Z|2017-03-20T14:42:27Z|http://arxiv.org/abs/1703.06777v1|http://arxiv.org/pdf/1703.06777v1|On the Use of Default Parameter Settings in the Empirical Evaluation of   Classification Algorithms|use default paramet set empir evalu classif algorithm|We demonstrate that, for a range of state-of-the-art machine learning algorithms, the differences in generalisation performance obtained using default parameter settings and using parameters tuned via cross-validation can be similar in magnitude to the differences in performance observed between state-of-the-art and uncompetitive learning systems. This means that fair and rigorous evaluation of new learning algorithms requires performance comparison against benchmark methods with best-practice model selection procedures, rather than using default parameter settings. We investigate the sensitivity of three key machine learning algorithms (support vector machine, random forest and rotation forest) to their default parameter settings, and provide guidance on determining sensible default parameter values for implementations of these algorithms. We also conduct an experimental comparison of these three algorithms on 121 classification problems and find that, perhaps surprisingly, rotation forest is significantly more accurate on average than both random forest and a support vector machine.|demonstr rang state art machin learn algorithm differ generalis perform obtain use default paramet set use paramet tune via cross valid similar magnitud differ perform observ state art uncompetit learn system mean fair rigor evalu new learn algorithm requir perform comparison benchmark method best practic model select procedur rather use default paramet set investig sensit three key machin learn algorithm support vector machin random forest rotat forest default paramet set provid guidanc determin sensibl default paramet valu implement algorithm also conduct experiment comparison three algorithm classif problem find perhap surpris rotat forest signific accur averag random forest support vector machin|['Anthony Bagnall', 'Gavin C. Cawley']|['cs.LG', 'stat.ML']
2017-03-28T14:02:09Z|2017-03-20T14:02:11Z|http://arxiv.org/abs/1703.06749v1|http://arxiv.org/pdf/1703.06749v1|Efficient variational Bayesian neural network ensembles for outlier   detection|effici variat bayesian neural network ensembl outlier detect|In this work we perform outlier detection using ensembles of neural networks obtained by variational approximation of the posterior in a Bayesian neural network setting. The variational parameters are obtained by sampling from the true posterior by gradient descent. We show our outlier detection results are better than those obtained using other efficient ensembling methods.|work perform outlier detect use ensembl neural network obtain variat approxim posterior bayesian neural network set variat paramet obtain sampl true posterior gradient descent show outlier detect result better obtain use effici ensembl method|['Nick Pawlowski', 'Miguel Jaques', 'Ben Glocker']|['stat.ML', 'cs.LG']
2017-03-28T14:02:09Z|2017-03-20T12:09:53Z|http://arxiv.org/abs/1703.06700v1|http://arxiv.org/pdf/1703.06700v1|Independence clustering (without a matrix)|independ cluster without matrix|The independence clustering problem is considered in the following formulation: given a set $S$ of random variables, it is required to find the finest partitioning $\{U_1,\dots,U_k\}$ of $S$ into clusters such that the clusters $U_1,\dots,U_k$ are mutually independent. Since mutual independence is the target, pairwise similarity measurements are of no use, and thus traditional clustering algorithms are inapplicable. The distribution of the random variables in $S$ is, in general, unknown, but a sample is available. Thus, the problem is cast in terms of time series. Two forms of sampling are considered: i.i.d.\ and stationary time series, with the main emphasis being on the latter, more general, case. A consistent, computationally tractable algorithm for each of the settings is proposed, and a number of open directions for further research are outlined.|independ cluster problem consid follow formul given set random variabl requir find finest partit dot cluster cluster dot mutual independ sinc mutual independ target pairwis similar measur use thus tradit cluster algorithm inapplic distribut random variabl general unknown sampl avail thus problem cast term time seri two form sampl consid stationari time seri main emphasi latter general case consist comput tractabl algorithm set propos number open direct research outlin|['Daniil Ryabko']|['cs.LG', 'cs.IT', 'math.IT', 'stat.ML']
2017-03-28T14:02:14Z|2017-03-20T11:44:00Z|http://arxiv.org/abs/1703.06692v1|http://arxiv.org/pdf/1703.06692v1|QMDP-Net: Deep Learning for Planning under Partial Observability|qmdp net deep learn plan partial observ|This paper introduces QMDP-net, a neural network architecture for planning under partial observability. The QMDP-net combines the strengths of model-free learning and model-based planning. It is a recurrent policy network, but it represents a policy by connecting a model with a planning algorithm that solves the model, thus embedding the solution structure of planning in the network architecture. The QMDP-net is fully differentiable and allows end-to-end training. We train a QMDP-net over a set of different environments so that it can generalize over new ones. In preliminary experiments, QMDP-net showed strong performance on several robotic tasks in simulation. Interestingly, it also sometimes outperformed the QMDP algorithm, which generated the data for learning, because of QMDP-net's robustness resulting from end-to-end learning.|paper introduc qmdp net neural network architectur plan partial observ qmdp net combin strength model free learn model base plan recurr polici network repres polici connect model plan algorithm solv model thus embed solut structur plan network architectur qmdp net fulli differenti allow end end train train qmdp net set differ environ general new one preliminari experi qmdp net show strong perform sever robot task simul interest also sometim outperform qmdp algorithm generat data learn becaus qmdp net robust result end end learn|['Peter Karkus', 'David Hsu', 'Wee Sun Lee']|['cs.AI', 'cs.LG', 'cs.NE', 'stat.ML']
2017-03-28T14:02:14Z|2017-03-20T11:26:04Z|http://arxiv.org/abs/1703.06686v1|http://arxiv.org/pdf/1703.06686v1|Copula Index for Detecting Dependence and Monotonicity between   Stochastic Signals|copula index detect depend monoton stochast signal|This paper introduces a nonparametric copula-based approach for detecting the strength and monotonicity of linear and nonlinear statistical dependence between bivariate continuous, discrete or hybrid random variables and stochastic signals, termed CIM. We show that CIM satisfies the data processing inequality and is consequently a self-equitable metric. Simulation results using synthetic datasets reveal that the CIM compares favorably to other state-of-the-art statistical dependency metrics, including the Maximal Information Coefficient (MIC), Randomized Dependency Coefficient (RDC), distance Correlation (dCor), Copula correlation (Ccor), and Copula Statistic (CoS) in both statistical power and sample size requirements. Simulations using real world data highlight the importance of understanding the monotonicity of the dependence structure.|paper introduc nonparametr copula base approach detect strength monoton linear nonlinear statist depend bivari continu discret hybrid random variabl stochast signal term cim show cim satisfi data process inequ consequ self equit metric simul result use synthet dataset reveal cim compar favor state art statist depend metric includ maxim inform coeffici mic random depend coeffici rdc distanc correl dcor copula correl ccor copula statist cos statist power sampl size requir simul use real world data highlight import understand monoton depend structur|['Kiran Karra', 'Lamine Mili']|['stat.ML', 'q-bio.QM']
2017-03-28T14:02:14Z|2017-03-19T23:28:39Z|http://arxiv.org/abs/1703.06537v1|http://arxiv.org/pdf/1703.06537v1|A Controlled Set-Up Experiment to Establish Personalized Baselines for   Real-Life Emotion Recognition|control set experi establish person baselin real life emot recognit|We design, conduct and present the results of a highly personalized baseline emotion recognition experiment, which aims to set reliable ground-truth estimates for the subject's emotional state for real-life prediction under similar conditions using a small number of physiological sensors. We also propose an adaptive stimuli-selection mechanism that would use the user's feedback as guide for future stimuli selection in the controlled-setup experiment and generate optimal ground-truth personalized sessions systematically. Initial results are very promising (85% accuracy) and variable importance analysis shows that only a few features, which are easy-to-implement in portable devices, would suffice to predict the subject's emotional state.|design conduct present result high person baselin emot recognit experi aim set reliabl ground truth estim subject emot state real life predict similar condit use small number physiolog sensor also propos adapt stimuli select mechan would use user feedback guid futur stimuli select control setup experi generat optim ground truth person session systemat initi result veri promis accuraci variabl import analysi show onli featur easi implement portabl devic would suffic predict subject emot state|['Varvara Kollia', 'Noureddine Tayebi']|['stat.ML', 'cs.HC']
2017-03-28T14:02:14Z|2017-03-19T22:23:01Z|http://arxiv.org/abs/1703.06528v1|http://arxiv.org/pdf/1703.06528v1|Universal Consistency and Robustness of Localized Support Vector   Machines|univers consist robust local support vector machin|The massive amount of available data potentially used to discover patters in machine learning is a challenge for kernel based algorithms with respect to runtime and storage capacities. Local approaches might help to relieve these issues. From a statistical point of view local approaches allow additionally to deal with different structures in the data in different ways. This paper analyses properties of localized kernel based, non-parametric statistical machine learning methods, in particular of support vector machines (SVMs) and methods close to them. We will show there that locally learnt kernel methods are universal consistent. Furthermore, we give an upper bound for the maxbias in order to show statistical robustness of the proposed method.|massiv amount avail data potenti use discov patter machin learn challeng kernel base algorithm respect runtim storag capac local approach might help reliev issu statist point view local approach allow addit deal differ structur data differ way paper analys properti local kernel base non parametr statist machin learn method particular support vector machin svms method close show local learnt kernel method univers consist furthermor give upper bound maxbia order show statist robust propos method|['Florian Dumpert']|['stat.ML', '62G08, 62G20, 62G35']
2017-03-28T14:02:14Z|2017-03-19T21:06:51Z|http://arxiv.org/abs/1703.06513v1|http://arxiv.org/pdf/1703.06513v1|Bernoulli Rank-$1$ Bandits for Click Feedback|bernoulli rank bandit click feedback|"The probability that a user will click a search result depends both on its relevance and its position on the results page. The position based model explains this behavior by ascribing to every item an attraction probability, and to every position an examination probability. To be clicked, a result must be both attractive and examined. The probabilities of an item-position pair being clicked thus form the entries of a rank-$1$ matrix. We propose the learning problem of a Bernoulli rank-$1$ bandit where at each step, the learning agent chooses a pair of row and column arms, and receives the product of their Bernoulli-distributed values as a reward. This is a special case of the stochastic rank-$1$ bandit problem considered in recent work that proposed an elimination based algorithm Rank1Elim, and showed that Rank1Elim's regret scales linearly with the number of rows and columns on ""benign"" instances. These are the instances where the minimum of the average row and column rewards $\mu$ is bounded away from zero. The issue with Rank1Elim is that it fails to be competitive with straightforward bandit strategies as $\mu \rightarrow 0$. In this paper we propose Rank1ElimKL which simply replaces the (crude) confidence intervals of Rank1Elim with confidence intervals based on Kullback-Leibler (KL) divergences, and with the help of a novel result concerning the scaling of KL divergences we prove that with this change, our algorithm will be competitive no matter the value of $\mu$. Experiments with synthetic data confirm that on benign instances the performance of Rank1ElimKL is significantly better than that of even Rank1Elim, while experiments with models derived from real data confirm that the improvements are significant across the board, regardless of whether the data is benign or not."|probabl user click search result depend relev posit result page posit base model explain behavior ascrib everi item attract probabl everi posit examin probabl click result must attract examin probabl item posit pair click thus form entri rank matrix propos learn problem bernoulli rank bandit step learn agent choos pair row column arm receiv product bernoulli distribut valu reward special case stochast rank bandit problem consid recent work propos elimin base algorithm rankelim show rankelim regret scale linear number row column benign instanc instanc minimum averag row column reward mu bound away zero issu rankelim fail competit straightforward bandit strategi mu rightarrow paper propos rankelimkl simpli replac crude confid interv rankelim confid interv base kullback leibler kl diverg help novel result concern scale kl diverg prove chang algorithm competit matter valu mu experi synthet data confirm benign instanc perform rankelimkl signific better even rankelim experi model deriv real data confirm improv signific across board regardless whether data benign|['Sumeet Katariya', 'Branislav Kveton', 'Csaba Szepesvári', 'Claire Vernade', 'Zheng Wen']|['cs.LG', 'stat.ML']
2017-03-28T14:02:14Z|2017-03-19T17:45:29Z|http://arxiv.org/abs/1703.06476v1|http://arxiv.org/pdf/1703.06476v1|Practical Coreset Constructions for Machine Learning|practic coreset construct machin learn|We investigate coresets - succinct, small summaries of large data sets - so that solutions found on the summary are provably competitive with solution found on the full data set. We provide an overview over the state-of-the-art in coreset construction for machine learning. In Section 2, we present both the intuition behind and a theoretically sound framework to construct coresets for general problems and apply it to $k$-means clustering. In Section 3 we summarize existing coreset construction algorithms for a variety of machine learning problems such as maximum likelihood estimation of mixture models, Bayesian non-parametric models, principal component analysis, regression and general empirical risk minimization.|investig coreset succinct small summari larg data set solut found summari provabl competit solut found full data set provid overview state art coreset construct machin learn section present intuit behind theoret sound framework construct coreset general problem appli mean cluster section summar exist coreset construct algorithm varieti machin learn problem maximum likelihood estim mixtur model bayesian non parametr model princip compon analysi regress general empir risk minim|['Olivier Bachem', 'Mario Lucic', 'Andreas Krause']|['stat.ML']
2017-03-28T14:02:14Z|2017-03-18T18:12:17Z|http://arxiv.org/abs/1703.06327v1|http://arxiv.org/pdf/1703.06327v1|Spectrum Estimation from a Few Entries|spectrum estim entri|Singular values of a data in a matrix form provide insights on the structure of the data, the effective dimensionality, and the choice of hyper-parameters on higher-level data analysis tools. However, in many practical applications such as collaborative filtering and network analysis, we only get a partial observation. Under such scenarios, we consider the fundamental problem of recovering spectral properties of the underlying matrix from a sampling of its entries. We are particularly interested in directly recovering the spectrum, which is the set of singular values, and also in sample-efficient approaches for recovering a spectral sum function, which is an aggregate sum of the same function applied to each of the singular values. We propose first estimating the Schatten $k$-norms of a matrix, and then applying Chebyshev approximation to the spectral sum function or applying moment matching in Wasserstein distance to recover the singular values. The main technical challenge is in accurately estimating the Schatten norms from a sampling of a matrix. We introduce a novel unbiased estimator based on counting small structures in a graph and provide guarantees that match its empirical performance. Our theoretical analysis shows that Schatten norms can be recovered accurately from strictly smaller number of samples compared to what is needed to recover the underlying low-rank matrix. Numerical experiments suggest that we significantly improve upon a competing approach of using matrix completion methods.|singular valu data matrix form provid insight structur data effect dimension choic hyper paramet higher level data analysi tool howev mani practic applic collabor filter network analysi onli get partial observ scenario consid fundament problem recov spectral properti matrix sampl entri particular interest direct recov spectrum set singular valu also sampl effici approach recov spectral sum function aggreg sum function appli singular valu propos first estim schatten norm matrix appli chebyshev approxim spectral sum function appli moment match wasserstein distanc recov singular valu main technic challeng accur estim schatten norm sampl matrix introduc novel unbias estim base count small structur graph provid guarante match empir perform theoret analysi show schatten norm recov accur strict smaller number sampl compar need recov low rank matrix numer experi suggest signific improv upon compet approach use matrix complet method|['Ashish Khetan', 'Sewoong Oh']|['stat.ML', 'cs.DS', 'cs.LG', 'cs.NA']
2017-03-28T14:02:14Z|2017-03-18T17:49:42Z|http://arxiv.org/abs/1703.06324v1|http://arxiv.org/pdf/1703.06324v1|Deep Tensor Encoding|deep tensor encod|Learning an encoding of feature vectors in terms of an over-complete dictionary or a probabilistic information geometric (Fisher vectors) construct is wide-spread in statistical signal processing and computer vision. In content based information retrieval using deep-learning classifiers, such encodings are learnt on the flattened last layer, without adherence to the multi-linear structure of the underlying feature tensor. We illustrate a variety of feature encodings incl. sparse dictionary coding and Fisher vectors along with proposing that a structured tensor factorization scheme enables us to perform retrieval that is at par, in terms of average precision, with Fisher vector encoded image signatures. In short, we illustrate how structural constraints increase retrieval fidelity.|learn encod featur vector term complet dictionari probabilist inform geometr fisher vector construct wide spread statist signal process comput vision content base inform retriev use deep learn classifi encod learnt flatten last layer without adher multi linear structur featur tensor illustr varieti featur encod incl spars dictionari code fisher vector along propos structur tensor factor scheme enabl us perform retriev par term averag precis fisher vector encod imag signatur short illustr structur constraint increas retriev fidel|['B Sengupta', 'E Vasquez', 'Y Qian']|['cs.IR', 'cs.LG', 'stat.ML']
2017-03-28T14:02:14Z|2017-03-18T08:38:51Z|http://arxiv.org/abs/1703.06272v1|http://arxiv.org/pdf/1703.06272v1|An Automated Auto-encoder Correlation-based Health-Monitoring and   Prognostic Method for Machine Bearings|autom auto encod correl base health monitor prognost method machin bear|This paper studies an intelligent ultimate technique for health-monitoring and prognostic of common rotary machine components, particularly bearings. During a run-to-failure experiment, rich unsupervised features from vibration sensory data are extracted by a trained sparse auto-encoder. Then, the correlation of the extracted attributes of the initial samples (presumably healthy at the beginning of the test) with the succeeding samples is calculated and passed through a moving-average filter. The normalized output is named auto-encoder correlation-based (AEC) rate which stands for an informative attribute of the system depicting its health status and precisely identifying the degradation starting point. We show that AEC technique well-generalizes in several run-to-failure tests. AEC collects rich unsupervised features form the vibration data fully autonomous. We demonstrate the superiority of the AEC over many other state-of-the-art approaches for the health monitoring and prognostic of machine bearings.|paper studi intellig ultim techniqu health monitor prognost common rotari machin compon particular bear dure run failur experi rich unsupervis featur vibrat sensori data extract train spars auto encod correl extract attribut initi sampl presum healthi begin test succeed sampl calcul pass move averag filter normal output name auto encod correl base aec rate stand inform attribut system depict health status precis identifi degrad start point show aec techniqu well general sever run failur test aec collect rich unsupervis featur form vibrat data fulli autonom demonstr superior aec mani state art approach health monitor prognost machin bear|['Ramin M. Hasani', 'Guodong Wang', 'Radu Grosu']|['cs.LG', 'cs.NE', 'stat.ML']
2017-03-28T14:02:14Z|2017-03-25T13:19:14Z|http://arxiv.org/abs/1703.06270v3|http://arxiv.org/pdf/1703.06270v3|SIM-CE: An Advanced Simulink Platform for Studying the Brain of   Caenorhabditis elegans|sim ce advanc simulink platform studi brain caenorhabd elegan|We introduce SIM-CE, an advanced, user-friendly modeling and simulation environment in Simulink for performing multi-scale behavioral analysis of the nervous system of Caenorhabditis elegans (C. elegans). SIM-CE contains an implementation of the mathematical models of C. elegans's neurons and synapses, in Simulink, which can be easily extended and particularized by the user. The Simulink model is able to capture both complex dynamics of ion channels and additional biophysical detail such as intracellular calcium concentration. We demonstrate the performance of SIM-CE by carrying out neuronal, synaptic and neural-circuit-level behavioral simulations. Such environment enables the user to capture unknown properties of the neural circuits, test hypotheses and determine the origin of many behavioral plasticities exhibited by the worm.|introduc sim ce advanc user friend model simul environ simulink perform multi scale behavior analysi nervous system caenorhabd elegan elegan sim ce contain implement mathemat model elegan neuron synaps simulink easili extend particular user simulink model abl captur complex dynam ion channel addit biophys detail intracellular calcium concentr demonstr perform sim ce carri neuron synapt neural circuit level behavior simul environ enabl user captur unknown properti neural circuit test hypothes determin origin mani behavior plastic exhibit worm|['Ramin M. Hasani', 'Victoria Beneder', 'Magdalena Fuchs', 'David Lung', 'Radu Grosu']|['q-bio.NC', 'cs.NE', 'q-bio.QM', 'stat.ML']
2017-03-28T14:02:18Z|2017-03-18T03:28:40Z|http://arxiv.org/abs/1703.06240v1|http://arxiv.org/pdf/1703.06240v1|Multi-fidelity Bayesian Optimisation with Continuous Approximations|multi fidel bayesian optimis continu approxim|Bandit methods for black-box optimisation, such as Bayesian optimisation, are used in a variety of applications including hyper-parameter tuning and experiment design. Recently, \emph{multi-fidelity} methods have garnered considerable attention since function evaluations have become increasingly expensive in such applications. Multi-fidelity methods use cheap approximations to the function of interest to speed up the overall optimisation process. However, most multi-fidelity methods assume only a finite number of approximations. In many practical applications however, a continuous spectrum of approximations might be available. For instance, when tuning an expensive neural network, one might choose to approximate the cross validation performance using less data $N$ and/or few training iterations $T$. Here, the approximations are best viewed as arising out of a continuous two dimensional space $(N,T)$. In this work, we develop a Bayesian optimisation method, BOCA, for this setting. We characterise its theoretical properties and show that it achieves better regret than than strategies which ignore the approximations. BOCA outperforms several other baselines in synthetic and real experiments.|bandit method black box optimis bayesian optimis use varieti applic includ hyper paramet tune experi design recent emph multi fidel method garner consider attent sinc function evalu becom increas expens applic multi fidel method use cheap approxim function interest speed overal optimis process howev multi fidel method assum onli finit number approxim mani practic applic howev continu spectrum approxim might avail instanc tune expens neural network one might choos approxim cross valid perform use less data train iter approxim best view aris continu two dimension space work develop bayesian optimis method boca set characteris theoret properti show achiev better regret strategi ignor approxim boca outperform sever baselin synthet real experi|['Kirthevasan Kandasamy', 'Gautam Dasarathy', 'Jeff Schneider', 'Barnabas Poczos']|['stat.ML']
2017-03-28T14:02:18Z|2017-03-18T00:59:40Z|http://arxiv.org/abs/1703.06229v1|http://arxiv.org/pdf/1703.06229v1|Curriculum Dropout|curriculum dropout|"Dropout is a very effective way of regularizing neural networks. Stochastically ""dropping out"" units with a certain probability discourages over-specific co-adaptations of feature detectors, preventing overfitting and improving network generalization. Besides, Dropout can be interpreted as an approximate model aggregation technique, where an exponential number of smaller networks are averaged in order to get a more powerful ensemble. In this paper, we show that using a fixed dropout probability during training is a suboptimal choice. We thus propose a time scheduling for the probability of retaining neurons in the network. This induces an adaptive regularization scheme that smoothly increases the difficulty of the optimization problem. This idea of ""starting easy"" and adaptively increasing the difficulty of the learning problem has its roots in curriculum learning and allows one to train better models. Indeed, we prove that our optimization strategy implements a very general curriculum scheme, by gradually adding noise to both the input and intermediate feature representations within the network architecture. Experiments on seven image classification datasets and different network architectures show that our method, named Curriculum Dropout, frequently yields to better generalization and, at worst, performs just as well as the standard Dropout method."|dropout veri effect way regular neural network stochast drop unit certain probabl discourag specif co adapt featur detector prevent overfit improv network general besid dropout interpret approxim model aggreg techniqu exponenti number smaller network averag order get power ensembl paper show use fix dropout probabl dure train suboptim choic thus propos time schedul probabl retain neuron network induc adapt regular scheme smooth increas difficulti optim problem idea start easi adapt increas difficulti learn problem root curriculum learn allow one train better model inde prove optim strategi implement veri general curriculum scheme gradual ad nois input intermedi featur represent within network architectur experi seven imag classif dataset differ network architectur show method name curriculum dropout frequent yield better general worst perform well standard dropout method|['Pietro Morerio', 'Jacopo Cavazza', 'Riccardo Volpi', 'Rene Vidal', 'Vittorio Murino']|['cs.NE', 'cs.LG', 'stat.ML']
2017-03-28T14:02:18Z|2017-03-18T00:08:59Z|http://arxiv.org/abs/1703.06222v1|http://arxiv.org/pdf/1703.06222v1|A Unified Treatment of Multiple Testing with Prior Knowledge|unifi treatment multipl test prior knowledg|A significant literature has arisen to study ways to employing prior knowledge to improve power and precision of multiple testing procedures. Some common forms of prior knowledge may include (a) a priori beliefs about which hypotheses are null, modeled by non-uniform prior weights; (b) differing importances of hypotheses, modeled by differing penalties for false discoveries; (c) partitions of the hypotheses into known groups, indicating (dis)similarity of hypotheses; and (d) knowledge of independence, positive dependence or arbitrary dependence between hypotheses or groups, allowing for more aggressive or conservative procedures. We present a general framework for global null testing and false discovery rate (FDR) control that allows the scientist to incorporate all four types of prior knowledge (a)-(d) simultaneously. We unify a number of existing procedures, generalize the conditions under which they are known to work, and simplify their proofs of FDR control under independence, positive and arbitrary dependence. We also present an algorithmic framework that strictly generalizes and unifies the classic algorithms of Benjamini and Hochberg [3] and Simes [25], algorithms that guard against unknown dependence [7, 9], algorithms that employ prior weights [17, 15], algorithms that use penalty weights [4], algorithms that incorporate null-proportion adaptivity [26, 27], and algorithms that make use of multiple arbitrary partitions into groups [1]. Unlike this previous work, we can simultaneously incorporate all of the four types of prior knowledge, combined with all of the three forms of dependence.|signific literatur arisen studi way employ prior knowledg improv power precis multipl test procedur common form prior knowledg may includ priori belief hypothes null model non uniform prior weight differ import hypothes model differ penalti fals discoveri partit hypothes known group indic dis similar hypothes knowledg independ posit depend arbitrari depend hypothes group allow aggress conserv procedur present general framework global null test fals discoveri rate fdr control allow scientist incorpor four type prior knowledg simultan unifi number exist procedur general condit known work simplifi proof fdr control independ posit arbitrari depend also present algorithm framework strict general unifi classic algorithm benjamini hochberg sime algorithm guard unknown depend algorithm employ prior weight algorithm use penalti weight algorithm incorpor null proport adapt algorithm make use multipl arbitrari partit group unlik previous work simultan incorpor four type prior knowledg combin three form depend|['Aaditya Ramdas', 'Rina Foygel Barber', 'Martin J. Wainwright', 'Michael I. Jordan']|['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']
2017-03-28T14:02:18Z|2017-03-17T23:52:14Z|http://arxiv.org/abs/1703.06217v1|http://arxiv.org/pdf/1703.06217v1|Deciding How to Decide: Dynamic Routing in Artificial Neural Networks|decid decid dynam rout artifici neural network|We propose and systematically evaluate three strategies for training dynamically-routed artificial neural networks: graphs of learned transformations through which different input signals may take different paths. Though some approaches have advantages over others, the resulting networks are often qualitatively similar. We find that, in dynamically-routed networks trained to classify images, layers and branches become specialized to process distinct categories of images. Additionally, given a fixed computational budget, dynamically-routed networks tend to perform better than comparable statically-routed networks.|propos systemat evalu three strategi train dynam rout artifici neural network graph learn transform differ input signal may take differ path though approach advantag result network often qualit similar find dynam rout network train classifi imag layer branch becom special process distinct categori imag addit given fix comput budget dynam rout network tend perform better compar static rout network|['Mason McGill', 'Pietro Perona']|['stat.ML', 'cs.CV', 'cs.LG', 'cs.NE']
2017-03-28T14:02:18Z|2017-03-17T19:24:09Z|http://arxiv.org/abs/1703.06177v1|http://arxiv.org/pdf/1703.06177v1|On Consistency of Graph-based Semi-supervised Learning|consist graph base semi supervis learn|Graph-based semi-supervised learning is one of the most popular methods in machine learning. Some of its theoretical properties such as bounds for the generalization error and the convergence of the graph Laplacian regularizer have been studied in computer science and statistics literatures. However, a fundamental statistical property, the consistency of the estimator from this method has not been proved. In this article, we study the consistency problem under a non-parametric framework. We prove the consistency of graph-based learning in the case that the estimated scores are enforced to be equal to the observed responses for the labeled data. The sample sizes of both labeled and unlabeled data are allowed to grow in this result. When the estimated scores are not required to be equal to the observed responses, a tuning parameter is used to balance the loss function and the graph Laplacian regularizer. We give a counterexample demonstrating that the estimator for this case can be inconsistent. The theoretical findings are supported by numerical studies.|graph base semi supervis learn one popular method machin learn theoret properti bound general error converg graph laplacian regular studi comput scienc statist literatur howev fundament statist properti consist estim method prove articl studi consist problem non parametr framework prove consist graph base learn case estim score enforc equal observ respons label data sampl size label unlabel data allow grow result estim score requir equal observ respons tune paramet use balanc loss function graph laplacian regular give counterexampl demonstr estim case inconsist theoret find support numer studi|['Chengan Du', 'Yunpeng Zhao']|['stat.ML']
2017-03-28T14:02:18Z|2017-03-17T17:50:44Z|http://arxiv.org/abs/1703.06131v1|http://arxiv.org/pdf/1703.06131v1|Inference via low-dimensional couplings|infer via low dimension coupl|"Integration against an intractable probability measure is among the fundamental challenges of statistical inference, particularly in the Bayesian setting. A principled approach to this problem seeks a deterministic coupling of the measure of interest with a tractable ""reference"" measure (e.g., a standard Gaussian). This coupling is induced by a transport map, and enables direct simulation from the desired measure simply by evaluating the transport map at samples from the reference. Yet characterizing such a map---e.g., representing and evaluating it---grows challenging in high dimensions. The central contribution of this paper is to establish a link between the Markov properties of the target measure and the existence of certain low-dimensional couplings, induced by transport maps that are sparse or decomposable. Our analysis not only facilitates the construction of couplings in high-dimensional settings, but also suggests new inference methodologies. For instance, in the context of nonlinear and non-Gaussian state space models, we describe new online and single-pass variational algorithms that characterize the full posterior distribution of the sequential inference problem using operations only slightly more complex than regular filtering."|integr intract probabl measur among fundament challeng statist infer particular bayesian set principl approach problem seek determinist coupl measur interest tractabl refer measur standard gaussian coupl induc transport map enabl direct simul desir measur simpli evalu transport map sampl refer yet character map repres evalu grow challeng high dimens central contribut paper establish link markov properti target measur exist certain low dimension coupl induc transport map spars decompos analysi onli facilit construct coupl high dimension set also suggest new infer methodolog instanc context nonlinear non gaussian state space model describ new onlin singl pass variat algorithm character full posterior distribut sequenti infer problem use oper onli slight complex regular filter|['Alessio Spantini', 'Daniele Bigoni', 'Youssef Marzouk']|['stat.ME', 'stat.CO', 'stat.ML']
2017-03-28T14:02:18Z|2017-03-17T17:09:15Z|http://arxiv.org/abs/1703.06104v1|http://arxiv.org/pdf/1703.06104v1|Nonconvex One-bit Single-label Multi-label Learning|nonconvex one bit singl label multi label learn|We study an extreme scenario in multi-label learning where each training instance is endowed with a single one-bit label out of multiple labels. We formulate this problem as a non-trivial special case of one-bit rank-one matrix sensing and develop an efficient non-convex algorithm based on alternating power iteration. The proposed algorithm is able to recover the underlying low-rank matrix model with linear convergence. For a rank-$k$ model with $d_1$ features and $d_2$ classes, the proposed algorithm achieves $O(\epsilon)$ recovery error after retrieving $O(k^{1.5}d_1 d_2/\epsilon)$ one-bit labels within $O(kd)$ memory. Our bound is nearly optimal in the order of $O(1/\epsilon)$. This significantly improves the state-of-the-art sampling complexity of one-bit multi-label learning. We perform experiments to verify our theory and evaluate the performance of the proposed algorithm.|studi extrem scenario multi label learn train instanc endow singl one bit label multipl label formul problem non trivial special case one bit rank one matrix sens develop effici non convex algorithm base altern power iter propos algorithm abl recov low rank matrix model linear converg rank model featur class propos algorithm achiev epsilon recoveri error retriev epsilon one bit label within kd memori bound near optim order epsilon signific improv state art sampl complex one bit multi label learn perform experi verifi theori evalu perform propos algorithm|['Shuang Qiu', 'Tingjin Luo', 'Jieping Ye', 'Ming Lin']|['stat.ML', 'cs.LG']
2017-03-28T14:02:18Z|2017-03-17T17:09:14Z|http://arxiv.org/abs/1703.06103v1|http://arxiv.org/pdf/1703.06103v1|Modeling Relational Data with Graph Convolutional Networks|model relat data graph convolut network|Knowledge bases play a crucial role in many applications, for example question answering and information retrieval. Despite the great effort invested in creating and maintaining them, even the largest representatives (e.g., Yago, DBPedia or Wikidata) are highly incomplete. We introduce relational graph convolutional networks (R-GCNs) and apply them to two standard knowledge base completion tasks: link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing attributes of entities). R-GCNs are a generalization of graph convolutional networks, a recent class of neural networks operating on graphs, and are developed specifically to deal with highly multi-relational data, characteristic of realistic knowledge bases. Our methods achieve competitive results on standard benchmarks for both tasks.|knowledg base play crucial role mani applic exampl question answer inform retriev despit great effort invest creat maintain even largest repres yago dbpedia wikidata high incomplet introduc relat graph convolut network gcns appli two standard knowledg base complet task link predict recoveri miss fact subject predic object tripl entiti classif recoveri miss attribut entiti gcns general graph convolut network recent class neural network oper graph develop specif deal high multi relat data characterist realist knowledg base method achiev competit result standard benchmark task|['Michael Schlichtkrull', 'Thomas N. Kipf', 'Peter Bloem', 'Rianne van den Berg', 'Ivan Titov', 'Max Welling']|['stat.ML', 'cs.AI', 'cs.DB', 'cs.LG']
2017-03-28T14:02:18Z|2017-03-17T16:08:23Z|http://arxiv.org/abs/1703.06065v1|http://arxiv.org/pdf/1703.06065v1|Block CUR : Decomposing Large Distributed Matrices|block cur decompos larg distribut matric|A common problem in large-scale data analysis is to approximate a matrix using a combination of specifically sampled rows and columns, known as CUR decomposition. Unfortunately, in many real-world environments, the ability to sample specific individual rows or columns of the matrix is limited by either system constraints or cost. In this paper, we consider matrix approximation by sampling predefined blocks of columns (or rows) from the matrix. This regime is commonly found when data is distributed across multiple nodes in a compute cluster, where such blocks correspond to columns (or rows) of the matrix stored on the same node, which can be retrieved with much less overhead than retrieving individual columns stored across different nodes. We propose a novel algorithm for sampling useful column blocks and provide guarantees for the quality of the approximation. We demonstrate the practical utility of this algorithm for computing the block CUR decomposition of large matrices in a distributed setting using Apache Spark. Using our proposed block CUR algorithms, we can achieve a significant speed-up compared to a regular CUR decomposition with the same quality of approximation.|common problem larg scale data analysi approxim matrix use combin specif sampl row column known cur decomposit unfortun mani real world environ abil sampl specif individu row column matrix limit either system constraint cost paper consid matrix approxim sampl predefin block column row matrix regim common found data distribut across multipl node comput cluster block correspond column row matrix store node retriev much less overhead retriev individu column store across differ node propos novel algorithm sampl use column block provid guarante qualiti approxim demonstr practic util algorithm comput block cur decomposit larg matric distribut set use apach spark use propos block cur algorithm achiev signific speed compar regular cur decomposit qualiti approxim|['Urvashi Oswal', 'Swayambhoo Jain', 'Kevin S. Xu', 'Brian Eriksson']|['stat.ML', 'cs.DC', 'cs.DS', 'cs.LG']
2017-03-28T14:02:18Z|2017-03-17T14:59:17Z|http://arxiv.org/abs/1703.06043v1|http://arxiv.org/pdf/1703.06043v1|Pattern representation and recognition with accelerated analog   neuromorphic systems|pattern represent recognit acceler analog neuromorph system|Despite being originally inspired by the central nervous system, artificial neural networks have diverged from their biological archetypes as they have been remodeled to fit particular tasks. In this paper, we review several possibilites to reverse map these architectures to biologically more realistic spiking networks with the aim of emulating them on fast, low-power neuromorphic hardware. Since many of these devices employ analog components, which cannot be perfectly controlled, finding ways to compensate for the resulting effects represents a key challenge. Here, we discuss three different strategies to address this problem: the addition of auxiliary network components for stabilizing activity, the utilization of inherently robust architectures and a training method for hardware-emulated networks that functions without perfect knowledge of the system's dynamics and parameters. For all three scenarios, we corroborate our theoretical considerations with experimental results on accelerated analog neuromorphic platforms.|despit origin inspir central nervous system artifici neural network diverg biolog archetyp remodel fit particular task paper review sever possibilit revers map architectur biolog realist spike network aim emul fast low power neuromorph hardwar sinc mani devic employ analog compon cannot perfect control find way compens result effect repres key challeng discuss three differ strategi address problem addit auxiliari network compon stabil activ util inher robust architectur train method hardwar emul network function without perfect knowledg system dynam paramet three scenario corrobor theoret consider experiment result acceler analog neuromorph platform|['Mihai A. Petrovici', 'Sebastian Schmitt', 'Johann Klähn', 'David Stöckel', 'Anna Schroeder', 'Guillaume Bellec', 'Johannes Bill', 'Oliver Breitwieser', 'Ilja Bytschok', 'Andreas Grübl', 'Maurice Güttler', 'Andreas Hartel', 'Stephan Hartmann', 'Dan Husmann', 'Kai Husmann', 'Sebastian Jeltsch', 'Vitali Karasenko', 'Mitja Kleider', 'Christoph Koke', 'Alexander Kononov', 'Christian Mauch', 'Paul Müller', 'Johannes Partzsch', 'Thomas Pfeil', 'Stefan Schiefer', 'Stefan Scholze', 'Anand Subramoney', 'Vasilis Thanasoulis', 'Bernhard Vogginger', 'Robert Legenstein', 'Wolfgang Maass', 'René Schüffny', 'Christian Mayr', 'Johannes Schemmel', 'Karlheinz Meier']|['q-bio.NC', 'cs.NE', 'stat.ML']
2017-03-28T14:02:22Z|2017-03-16T23:33:24Z|http://arxiv.org/abs/1703.05849v1|http://arxiv.org/pdf/1703.05849v1|Causal Inference through the Method of Direct Estimation|causal infer method direct estim|The intersection of causal inference and machine learning is a rapidly advancing field. We propose a new approach, the method of direct estimation, that draws on both traditions in order to obtain nonparametric estimates of treatment effects. The approach focuses on estimating the effect of fluctuations in a treatment variable on an outcome. A tensor-spline implementation enables rich interactions between functional bases allowing for the approach to capture treatment/covariate interactions. We show how new innovations in Bayesian sparse modeling readily handle the proposed framework, and then document its performance in simulation and applied examples. Furthermore we show how the method of direct estimation can easily extend to structural estimators commonly used in a variety of disciplines, like instrumental variables, mediation analysis, and sequential g-estimation.|intersect causal infer machin learn rapid advanc field propos new approach method direct estim draw tradit order obtain nonparametr estim treatment effect approach focus estim effect fluctuat treatment variabl outcom tensor spline implement enabl rich interact function base allow approach captur treatment covari interact show new innov bayesian spars model readili handl propos framework document perform simul appli exampl furthermor show method direct estim easili extend structur estim common use varieti disciplin like instrument variabl mediat analysi sequenti estim|['Marc Ratkovic', 'Dustin Tingley']|['stat.ML', 'stat.ME', '62G08, 46N30, 62P20, 62P25']
2017-03-28T14:02:22Z|2017-03-16T22:37:55Z|http://arxiv.org/abs/1703.05841v1|http://arxiv.org/pdf/1703.05841v1|Adaptivity to Noise Parameters in Nonparametric Active Learning|adapt nois paramet nonparametr activ learn|This work addresses various open questions in the theory of active learning for nonparametric classification. Our contributions are both statistical and algorithmic: -We establish new minimax-rates for active learning under common \textit{noise conditions}. These rates display interesting transitions -- due to the interaction between noise \textit{smoothness and margin} -- not present in the passive setting. Some such transitions were previously conjectured, but remained unconfirmed. -We present a generic algorithmic strategy for adaptivity to unknown noise smoothness and margin; our strategy achieves optimal rates in many general situations; furthermore, unlike in previous work, we avoid the need for \textit{adaptive confidence sets}, resulting in strictly milder distributional requirements.|work address various open question theori activ learn nonparametr classif contribut statist algorithm establish new minimax rate activ learn common textit nois condit rate display interest transit due interact nois textit smooth margin present passiv set transit previous conjectur remain unconfirm present generic algorithm strategi adapt unknown nois smooth margin strategi achiev optim rate mani general situat furthermor unlik previous work avoid need textit adapt confid set result strict milder distribut requir|['Andrea Locatelli', 'Alexandra Carpentier', 'Samory Kpotufe']|['stat.ML']
2017-03-28T14:02:22Z|2017-03-24T20:28:16Z|http://arxiv.org/abs/1703.05840v2|http://arxiv.org/pdf/1703.05840v2|Conditional Accelerated Lazy Stochastic Gradient Descent|condit acceler lazi stochast gradient descent|In this work we introduce a conditional accelerated lazy stochastic gradient descent algorithm with optimal number of calls to a stochastic first-order oracle and convergence rate $O\left(\frac{1}{\varepsilon^2}\right)$ improving over the projection-free, Online Frank-Wolfe based stochastic gradient descent of Hazan and Kale [2012] with convergence rate $O\left(\frac{1}{\varepsilon^4}\right)$.|work introduc condit acceler lazi stochast gradient descent algorithm optim number call stochast first order oracl converg rate left frac varepsilon right improv project free onlin frank wolf base stochast gradient descent hazan kale converg rate left frac varepsilon right|['Guanghui Lan', 'Sebastian Pokutta', 'Yi Zhou', 'Daniel Zink']|['cs.LG', 'stat.ML', '90C25', 'G.1.6']
2017-03-28T14:02:22Z|2017-03-16T18:25:21Z|http://arxiv.org/abs/1703.05785v1|http://arxiv.org/pdf/1703.05785v1|Low-rank and Sparse NMF for Joint Endmembers' Number Estimation and   Blind Unmixing of Hyperspectral Images|low rank spars nmf joint endmemb number estim blind unmix hyperspectr imag|Estimation of the number of endmembers existing in a scene constitutes a critical task in the hyperspectral unmixing process. The accuracy of this estimate plays a crucial role in subsequent unsupervised unmixing steps i.e., the derivation of the spectral signatures of the endmembers (endmembers' extraction) and the estimation of the abundance fractions of the pixels. A common practice amply followed in literature is to treat endmembers' number estimation and unmixing, independently as two separate tasks, providing the outcome of the former as input to the latter. In this paper, we go beyond this computationally demanding strategy. More precisely, we set forth a multiple constrained optimization framework, which encapsulates endmembers' number estimation and unsupervised unmixing in a single task. This is attained by suitably formulating the problem via a low-rank and sparse nonnegative matrix factorization rationale, where low-rankness is promoted with the use of a sophisticated $\ell_2/\ell_1$ norm penalty term. An alternating proximal algorithm is then proposed for minimizing the emerging cost function. The results obtained by simulated and real data experiments verify the effectiveness of the proposed approach.|estim number endmemb exist scene constitut critic task hyperspectr unmix process accuraci estim play crucial role subsequ unsupervis unmix step deriv spectral signatur endmemb endmemb extract estim abund fraction pixel common practic ampli follow literatur treat endmemb number estim unmix independ two separ task provid outcom former input latter paper go beyond comput demand strategi precis set forth multipl constrain optim framework encapsul endmemb number estim unsupervis unmix singl task attain suitabl formul problem via low rank spars nonneg matrix factor rational low rank promot use sophist ell ell norm penalti term altern proxim algorithm propos minim emerg cost function result obtain simul real data experi verifi effect propos approach|['Paris V. Giampouras', 'Athanasios A. Rontogiannis', 'Konstantinos D. Koutroumbas']|['cs.CV', 'stat.ML']
2017-03-28T14:02:22Z|2017-03-16T16:00:00Z|http://arxiv.org/abs/1703.05687v1|http://arxiv.org/pdf/1703.05687v1|Gaussian process regression for forecasting battery state of health|gaussian process regress forecast batteri state health|Accurately predicting the future capacity and remaining useful life of batteries is necessary to ensure reliable system operation and to minimise maintenance costs. The complex nature of battery degradation has meant that mechanistic modelling of capacity fade has thus far remained intractable; however, with the advent of cloud-connected devices, data from cells in various applications is becoming increasingly available, and the feasibility of data-driven methods for battery prognostics is increasing. Here we propose Gaussian process (GP) regression for forecasting battery state of health, and highlight various advantages of GPs over other data-driven and mechanistic approaches. GPs are a type of Bayesian non-parametric method, and hence can model complex systems whilst handling uncertainty in a principled manner. Prior information can be exploited by GPs in a variety of ways: explicit mean functions can be used if the functional form of the underlying degradation model is available, and multiple-output GPs can effectively exploit correlations between data from different cells. We demonstrate the predictive capability of GPs for short-term and long-term (remaining useful life) forecasting on a selection of capacity vs. cycle datasets from lithium-ion cells.|accur predict futur capac remain use life batteri necessari ensur reliabl system oper minimis mainten cost complex natur batteri degrad meant mechanist model capac fade thus far remain intract howev advent cloud connect devic data cell various applic becom increas avail feasibl data driven method batteri prognost increas propos gaussian process gp regress forecast batteri state health highlight various advantag gps data driven mechanist approach gps type bayesian non parametr method henc model complex system whilst handl uncertainti principl manner prior inform exploit gps varieti way explicit mean function use function form degrad model avail multipl output gps effect exploit correl data differ cell demonstr predict capabl gps short term long term remain use life forecast select capac vs cycl dataset lithium ion cell|['Robert R. Richardson', 'Michael A. Osborne', 'David A. Howey']|['stat.AP', 'stat.ML', '62P30', 'J.2; G.3']
2017-03-28T14:02:22Z|2017-03-16T15:14:48Z|http://arxiv.org/abs/1703.05667v1|http://arxiv.org/pdf/1703.05667v1|End-to-End Learning for Structured Prediction Energy Networks|end end learn structur predict energi network|Structured Prediction Energy Networks (Belanger and McCallum, 2016) (SPENs) are a simple, yet expressive family of structured prediction models. An energy function over candidate structured outputs is given by a deep network, and predictions are formed by gradient-based optimization. Unfortunately, we have struggled to apply the structured SVM (SSVM) learning method of Belanger and McCallum, 2016 to applications with more complex structure than multi-label classification. In general, SSVMs are unreliable whenever exact energy minimization is intractable. In response, we present end-to-end learning for SPENs, where the energy function is discriminatively trained by back-propagating through gradient-based prediction. This paper presents a collection of methods necessary to apply the technique to problems with complex structure. For example, we avoid vanishing gradients when learning SPENs for convex relaxations of discrete prediction problems and explicitly train models such that energy minimization converges quickly in practice. Using end-to-end learning, we demonstrate the power of SPENs on 7-Scenes depth image denoising and CoNLL-2005 semantic role labeling tasks. In both, we outperform competitive baselines that employ more simplistic energy functions, but perform exact energy minimization. In particular, for denoising we achieve 40 PSNR, outperforming the previous state-of-the-art of 36.|structur predict energi network belang mccallum spen simpl yet express famili structur predict model energi function candid structur output given deep network predict form gradient base optim unfortun struggl appli structur svm ssvm learn method belang mccallum applic complex structur multi label classif general ssvms unreli whenev exact energi minim intract respons present end end learn spen energi function discrimin train back propag gradient base predict paper present collect method necessari appli techniqu problem complex structur exampl avoid vanish gradient learn spen convex relax discret predict problem explicit train model energi minim converg quick practic use end end learn demonstr power spen scene depth imag denois conll semant role label task outperform competit baselin employ simplist energi function perform exact energi minim particular denois achiev psnr outperform previous state art|['David Belanger', 'Bishan Yang', 'Andrew McCallum']|['stat.ML', 'cs.LG']
2017-03-28T14:02:22Z|2017-03-16T09:52:48Z|http://arxiv.org/abs/1703.05537v1|http://arxiv.org/pdf/1703.05537v1|Shift Aggregate Extract Networks|shift aggreg extract network|"We introduce an architecture based on deep hierarchical decompositions to learn effective representations of large graphs. Our framework extends classic R-decompositions used in kernel methods, enabling nested ""part-of-part"" relations. Unlike recursive neural networks, which unroll a template on input graphs directly, we unroll a neural network template over the decomposition hierarchy, allowing us to deal with the high degree variability that typically characterize social network graphs. Deep hierarchical decompositions are also amenable to domain compression, a technique that reduces both space and time complexity by exploiting symmetries. We show empirically that our approach is competitive with current state-of-the-art graph classification methods, particularly when dealing with social network datasets."|introduc architectur base deep hierarch decomposit learn effect represent larg graph framework extend classic decomposit use kernel method enabl nest part part relat unlik recurs neural network unrol templat input graph direct unrol neural network templat decomposit hierarchi allow us deal high degre variabl typic character social network graph deep hierarch decomposit also amen domain compress techniqu reduc space time complex exploit symmetri show empir approach competit current state art graph classif method particular deal social network dataset|['Francesco Orsini', 'Daniele Baracchi', 'Paolo Frasconi']|['cs.LG', 'stat.ML']
2017-03-28T14:02:22Z|2017-03-16T01:37:25Z|http://arxiv.org/abs/1703.05452v1|http://arxiv.org/pdf/1703.05452v1|Efficient Online Learning for Optimizing Value of Information: Theory   and Application to Interactive Troubleshooting|effici onlin learn optim valu inform theori applic interact troubleshoot|We consider the optimal value of information (VoI) problem, where the goal is to sequentially select a set of tests with a minimal cost, so that one can efficiently make the best decision based on the observed outcomes. Existing algorithms are either heuristics with no guarantees, or scale poorly (with exponential run time in terms of the number of available tests). Moreover, these methods assume a known distribution over the test outcomes, which is often not the case in practice. We propose an efficient sampling-based online learning framework to address the above issues. First, assuming the distribution over hypotheses is known, we propose a dynamic hypothesis enumeration strategy, which allows efficient information gathering with strong theoretical guarantees. We show that with sufficient amount of samples, one can identify a near-optimal decision with high probability. Second, when the parameters of the hypotheses distribution are unknown, we propose an algorithm which learns the parameters progressively via posterior sampling in an online fashion. We further establish a rigorous bound on the expected regret. We demonstrate the effectiveness of our approach on a real-world interactive troubleshooting application and show that one can efficiently make high-quality decisions with low cost.|consid optim valu inform voi problem goal sequenti select set test minim cost one effici make best decis base observ outcom exist algorithm either heurist guarante scale poor exponenti run time term number avail test moreov method assum known distribut test outcom often case practic propos effici sampl base onlin learn framework address abov issu first assum distribut hypothes known propos dynam hypothesi enumer strategi allow effici inform gather strong theoret guarante show suffici amount sampl one identifi near optim decis high probabl second paramet hypothes distribut unknown propos algorithm learn paramet progress via posterior sampl onlin fashion establish rigor bound expect regret demonstr effect approach real world interact troubleshoot applic show one effici make high qualiti decis low cost|['Yuxin Chen', 'Jean-Michel Renders', 'Morteza Haghir Chehreghani', 'Andreas Krause']|['cs.AI', 'cs.LG', 'stat.ML']
2017-03-28T14:02:22Z|2017-03-16T01:31:33Z|http://arxiv.org/abs/1703.05449v1|http://arxiv.org/pdf/1703.05449v1|Minimax Regret Bounds for Reinforcement Learning|minimax regret bound reinforc learn|"We consider the problem of efficient exploration in finite horizon MDPs.We show that an optimistic modification to model-based value iteration, can achieve a regret bound $\tilde{O}( \sqrt{HSAT} + H^2S^2A+H\sqrt{T})$ where $H$ is the time horizon, $S$ the number of states, $A$ the number of actions and $T$ the time elapsed. This result improves over the best previous known bound $\tilde{O}(HS \sqrt{AT})$ achieved by the UCRL2 algorithm.The key significance of our new results is that when $T\geq H^3S^3A$ and $SA\geq H$, it leads to a regret of $\tilde{O}(\sqrt{HSAT})$ that matches the established lower bounds of $\Omega(\sqrt{HSAT})$ up to a logarithmic factor. Our analysis contain two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in $S$), and we use ""exploration bonuses"" based on Bernstein's inequality, together with using a recursive -Bellman-type- Law of Total Variance (to improve scaling in $H$)."|consid problem effici explor finit horizon mdps show optimist modif model base valu iter achiev regret bound tild sqrt hsat sqrt time horizon number state number action time elaps result improv best previous known bound tild hs sqrt achiev ucrl algorithm key signific new result geq sa geq lead regret tild sqrt hsat match establish lower bound omega sqrt hsat logarithm factor analysi contain two key insight use care applic concentr inequ optim valu function whole rather transit probabl improv scale use explor bonus base bernstein inequ togeth use recurs bellman type law total varianc improv scale|['Mohammad Gheshlaghi Azar', 'Ian Osband', 'Rémi Munos']|['stat.ML', 'cs.AI', 'cs.LG']
2017-03-28T14:02:22Z|2017-03-15T23:58:19Z|http://arxiv.org/abs/1703.05430v1|http://arxiv.org/pdf/1703.05430v1|Cost-complexity pruning of random forests|cost complex prune random forest|Random forests perform bootstrap-aggregation by sampling the training samples with replacement. This enables the evaluation of out-of-bag error which serves as a internal cross-validation mechanism. Our motivation lies in using the unsampled training samples to improve each decision tree in the ensemble. We study the effect of using the out-of-bag samples to improve the generalization error first of the decision trees and second the random forest by post-pruning. A preliminary empirical study on four UCI repository datasets show consistent decrease in the size of the forests without considerable loss in accuracy.|random forest perform bootstrap aggreg sampl train sampl replac enabl evalu bag error serv intern cross valid mechan motiv lie use unsampl train sampl improv decis tree ensembl studi effect use bag sampl improv general error first decis tree second random forest post prune preliminari empir studi four uci repositori dataset show consist decreas size forest without consider loss accuraci|['Kiran Bangalore Ravi', 'Jean Serra']|['stat.ML', 'cs.LG']
