2017-03-28T14:04:33Z|2017-03-26T16:37:52Z|http://arxiv.org/abs/1703.08843v1|http://arxiv.org/pdf/1703.08843v1|Testing independence with high-dimensional correlated samples|test independ high dimension correl sampl|"Testing independence among a number of (ultra) high-dimensional random samples is a fundamental and challenging problem. By arranging $n$ identically distributed $p$-dimensional random vectors into a $p \times n$ data matrix, we investigate the problem of testing independence among columns under the matrix-variate normal modeling of data. We propose a computationally simple and tuning-free test statistic, characterize its limiting null distribution, analyze the statistical power and prove its minimax optimality. As an important by-product of the test statistic, a ratio-consistent estimator for the quadratic functional of a covariance matrix from correlated samples is developed. We further study the effect of correlation among samples to an important high-dimensional inference problem --- large-scale multiple testing of Pearson's correlation coefficients. Indeed, blindly using classical inference results based on the assumed independence of samples will lead to many false discoveries, which suggests the need for conducting independence testing before applying existing methods. To address the challenge arising from correlation among samples, we propose a ""sandwich estimator"" of Pearson's correlation coefficient by de-correlating the samples. Based on this approach, the resulting multiple testing procedure asymptotically controls the overall false discovery rate at the nominal level while maintaining good statistical power. Both simulated and real data experiments are carried out to demonstrate the advantages of the proposed methods."|test independ among number ultra high dimension random sampl fundament challeng problem arrang ident distribut dimension random vector time data matrix investig problem test independ among column matrix variat normal model data propos comput simpl tune free test statist character limit null distribut analyz statist power prove minimax optim import product test statist ratio consist estim quadrat function covari matrix correl sampl develop studi effect correl among sampl import high dimension infer problem larg scale multipl test pearson correl coeffici inde blind use classic infer result base assum independ sampl lead mani fals discoveri suggest need conduct independ test befor appli exist method address challeng aris correl among sampl propos sandwich estim pearson correl coeffici de correl sampl base approach result multipl test procedur asymptot control overal fals discoveri rate nomin level maintain good statist power simul real data experi carri demonstr advantag propos method|['Xi Chen', 'Weidong Liu']|['math.ST', 'stat.TH']
2017-03-28T14:04:33Z|2017-03-25T04:44:47Z|http://arxiv.org/abs/1703.08648v1|http://arxiv.org/pdf/1703.08648v1|The new concepts of measurement error's regularities and effect   characteristics|new concept measur error regular effect characterist|In several literatures, the authors give a kind of thinking of measurement theory system based on error non-classification philosophy, which completely overthrows the existing measurement concepts system of precision, trueness and accuracy. In this paper, aiming at the issues of error's regularities and effect characteristics, the authors will do a thematic explanation, and prove that the error's regularities actually come from different cognitive perspectives, is also unable to be used for classifying errors, and that the error's effect characteristics actually depend on artificial condition rules of repeated measurement, and is still unable to be used for classifying errors. Thus, from the perspectives of error's regularities and effect characteristics, the existing error classification philosophy is still a mistake, an uncertainty concept system, which must be interpreted by the error non-classification philosophy, naturally become the only way out of measurement theory.|sever literatur author give kind think measur theori system base error non classif philosophi complet overthrow exist measur concept system precis trueness accuraci paper aim issu error regular effect characterist author themat explan prove error regular actual come differ cognit perspect also unabl use classifi error error effect characterist actual depend artifici condit rule repeat measur still unabl use classifi error thus perspect error regular effect characterist exist error classif philosophi still mistak uncertainti concept system must interpret error non classif philosophi natur becom onli way measur theori|['Xiaoming Ye', 'Haibo Liu', 'Mo Ling', 'Xuebin Xiao']|['math.ST', 'stat.TH']
2017-03-28T14:04:33Z|2017-03-24T20:59:52Z|http://arxiv.org/abs/1703.08596v1|http://arxiv.org/pdf/1703.08596v1|The Inner Structure of Time-Dependent Signals|inner structur time depend signal|This paper shows how a time series of measurements of an evolving system can be processed to create an inner time series that is unaffected by any instantaneous invertible, possibly nonlinear transformation of the measurements. An inner time series contains information that does not depend on the nature of the sensors, which the observer chose to monitor the system. Instead, it encodes information that is intrinsic to the evolution of the observed system. Because of its sensor-independence, an inner time series may produce fewer false negatives when it is used to detect events in the presence of sensor drift. Furthermore, if the observed physical system is comprised of non-interacting subsystems, its inner time series is separable; i.e., it consists of a collection of time series, each one being the inner time series of an isolated subsystem. Because of this property, an inner time series can be used to detect a specific behavior of one of the independent subsystems without using blind source separation to disentangle that subsystem from the others. The method is illustrated by applying it to: 1) an analytic example; 2) the audio waveform of one speaker; 3) video images from a moving camera; 4) mixtures of audio waveforms of two speakers.|paper show time seri measur evolv system process creat inner time seri unaffect ani instantan invert possibl nonlinear transform measur inner time seri contain inform doe depend natur sensor observ chose monitor system instead encod inform intrins evolut observ system becaus sensor independ inner time seri may produc fewer fals negat use detect event presenc sensor drift furthermor observ physic system compris non interact subsystem inner time seri separ consist collect time seri one inner time seri isol subsystem becaus properti inner time seri use detect specif behavior one independ subsystem without use blind sourc separ disentangl subsystem method illustr appli analyt exampl audio waveform one speaker video imag move camera mixtur audio waveform two speaker|['David N. Levin']|['stat.ME', 'cs.SD', 'math.ST', 'stat.TH']
2017-03-28T14:04:33Z|2017-03-24T18:45:06Z|http://arxiv.org/abs/1703.08570v1|http://arxiv.org/pdf/1703.08570v1|Stochastic Methods for Composite Optimization Problems|stochast method composit optim problem|We consider minimization of stochastic functionals that are compositions of a (potentially) non-smooth convex function $h$ and smooth function $c$. We develop two stochastic methods---a stochastic prox-linear algorithm and a stochastic (generalized) sub-gradient procedure---and prove that, under mild technical conditions, each converges to first-order stationary points of the stochastic objective. We provide experiments further investigating our methods on non-smooth phase retrieval problems, the experiments indicate the practical effectiveness of the procedures.|consid minim stochast function composit potenti non smooth convex function smooth function develop two stochast method stochast prox linear algorithm stochast general sub gradient procedur prove mild technic condit converg first order stationari point stochast object provid experi investig method non smooth phase retriev problem experi indic practic effect procedur|['John Duchi', 'Feng Ruan']|['math.OC', 'math.ST', 'stat.TH']
2017-03-28T14:04:33Z|2017-03-24T16:08:21Z|http://arxiv.org/abs/1703.08487v1|http://arxiv.org/pdf/1703.08487v1|Multiscale Granger causality|multiscal granger causal|In the study of complex physical and biological systems represented by multivariate stochastic processes, an issue of great relevance is the description of the system dynamics spanning multiple temporal scales. While methods to assess the dynamic complexity of individual processes at different time scales are well-established, the multiscale evaluation of directed interactions between processes is complicated by theoretical and practical issues such as filtering and downsampling. Here we extend the very popular measure of Granger causality (GC), a prominent tool for assessing directed lagged interactions between joint processes, to quantify information transfer across multiple time scales. We show that the multiscale processing of a vector autoregressive (AR) process introduces a moving average (MA) component, and describe how to represent the resulting ARMA process using state space (SS) models and to combine the SS model parameters for computing exact GC values at arbitrarily large time scales. We exploit the theoretical formulation to identify peculiar features of multiscale GC in basic AR processes, and demonstrate with numerical simulations the much larger estimation accuracy of the SS approach compared with pure AR modeling of filtered and downsampled data. The improved computational reliability is exploited to disclose meaningful multiscale patterns of information transfer between global temperature and carbon dioxide concentration time series, both in paleoclimate and in recent years.|studi complex physic biolog system repres multivari stochast process issu great relev descript system dynam span multipl tempor scale method assess dynam complex individu process differ time scale well establish multiscal evalu direct interact process complic theoret practic issu filter downsampl extend veri popular measur granger causal gc promin tool assess direct lag interact joint process quantifi inform transfer across multipl time scale show multiscal process vector autoregress ar process introduc move averag compon describ repres result arma process use state space ss model combin ss model paramet comput exact gc valu arbitrarili larg time scale exploit theoret formul identifi peculiar featur multiscal gc basic ar process demonstr numer simul much larger estim accuraci ss approach compar pure ar model filter downsampl data improv comput reliabl exploit disclos meaning multiscal pattern inform transfer global temperatur carbon dioxid concentr time seri paleoclim recent year|['Luca Faes', 'Giandomenico Nollo', 'Sebastiano Stramaglia', 'Daniele Marinazzo']|['stat.ME', 'math.ST', 'stat.AP', 'stat.TH']
2017-03-28T14:04:33Z|2017-03-24T11:16:37Z|http://arxiv.org/abs/1703.08358v1|http://arxiv.org/pdf/1703.08358v1|Nonparametric Bayesian analysis for support boundary recovery|nonparametr bayesian analysi support boundari recoveri|Given a sample of a Poisson point process with intensity $\lambda_f(x,y) = n \mathbf{1}(f(x) \leq y),$ we study recovery of the boundary function $f$ from a nonparametric Bayes perspective. Because of the irregularity of this model, the analysis is non-standard. We derive contraction rates with respect to the $L^1$-norm for several classes of priors, including Gaussian priors, priors based on (truncated) random series, compound Poisson processes, and subordinators. We also investigate the limiting shape of the posterior distribution and derive a nonparametric version of the Bernstein-von Mises theorem for a specific class of priors on a function space with increasing parameter dimension. We show that the marginal posterior of the functional $\vartheta =\int f$ does some automatic bias correction and contracts with a faster rate than the MLE. In this case, $1-\alpha$-credible sets are also asymptotic $1-\alpha$ confidence intervals. It is also shown that the frequentist coverage of credible sets is lost under model misspecification.|given sampl poisson point process intens lambda mathbf leq studi recoveri boundari function nonparametr bay perspect becaus irregular model analysi non standard deriv contract rate respect norm sever class prior includ gaussian prior prior base truncat random seri compound poisson process subordin also investig limit shape posterior distribut deriv nonparametr version bernstein von mise theorem specif class prior function space increas paramet dimens show margin posterior function vartheta int doe automat bias correct contract faster rate mle case alpha credibl set also asymptot alpha confid interv also shown frequentist coverag credibl set lost model misspecif|['Markus Reiss', 'Johannes Schmidt-Hieber']|['math.ST', 'stat.TH', '62G05, 62G08']
2017-03-28T14:04:33Z|2017-03-24T04:51:03Z|http://arxiv.org/abs/1703.08285v1|http://arxiv.org/pdf/1703.08285v1|The Multi-Armed Bandit Problem: An Efficient Non-Parametric Solution|multi arm bandit problem effici non parametr solut|Lai and Robbins (1985) and Lai (1987) provided efficient parametric solutions to the multi-armed bandit problem, showing that arm allocation via upper confidence bounds (UCB) achieves minimum regret. These bounds are constructed from the Kullback-Leibler information of the reward distributions, estimated from within a specified parametric family. In recent years there has been renewed interest in the multi-armed bandit problem due to new applications in machine learning algorithms and data analytics. Non-parametric arm allocation procedures like $\epsilon$-greedy and Boltzmann exploration were studied, and modified versions of the UCB procedure were also analyzed under a non-parametric setting. However unlike UCB these non-parametric procedures are not efficient under a parametric setting. In this paper we propose a subsample comparison procedure that is non-parametric, but still efficient under parametric settings.|lai robbin lai provid effici parametr solut multi arm bandit problem show arm alloc via upper confid bound ucb achiev minimum regret bound construct kullback leibler inform reward distribut estim within specifi parametr famili recent year renew interest multi arm bandit problem due new applic machin learn algorithm data analyt non parametr arm alloc procedur like epsilon greedi boltzmann explor studi modifi version ucb procedur also analyz non parametr set howev unlik ucb non parametr procedur effici parametr set paper propos subsampl comparison procedur non parametr still effici parametr set|['Hock Peng Chan']|['math.ST', 'stat.TH']
2017-03-28T14:04:33Z|2017-03-23T18:01:39Z|http://arxiv.org/abs/1703.08190v1|http://arxiv.org/pdf/1703.08190v1|MSE estimates for multitaper spectral estimation and off-grid   compressive sensing|mse estim multitap spectral estim grid compress sens|We obtain estimates for the Mean Squared Error (MSE) for the multitaper spectral estimator and certain compressive acquisition methods for multi-band signals. We confirm a fact discovered by Thomson [Spectrum estimation and harmonic analysis, Proc. IEEE, 1982]: assuming bandwidth $W$ and $N$ time domain observations, the average of the square of the first $K=2NW$ Slepian functions approaches, as $K$ grows, an ideal band-pass kernel for the interval $[-W,W]$. We provide an analytic proof of this fact and measure the corresponding rate of convergence in the $L^{1}$ norm. This validates a heuristic approximation used to control the MSE of the multitaper estimator. The estimates have also consequences for the method of compressive acquisition of multi-band signals introduced by Davenport and Wakin, giving MSE approximation bounds for the dictionary formed by modulation of the critical number of prolates.|obtain estim mean squar error mse multitap spectral estim certain compress acquisit method multi band signal confirm fact discov thomson spectrum estim harmon analysi proc ieee assum bandwidth time domain observ averag squar first nw slepian function approach grow ideal band pass kernel interv provid analyt proof fact measur correspond rate converg norm valid heurist approxim use control mse multitap estim estim also consequ method compress acquisit multi band signal introduc davenport wakin give mse approxim bound dictionari form modul critic number prolat|['Luís Daniel Abreu', 'José Luis Romero']|['cs.IT', 'math.IT', 'math.ST', 'stat.TH']
2017-03-28T14:04:33Z|2017-03-22T22:59:20Z|http://arxiv.org/abs/1703.07879v1|http://arxiv.org/pdf/1703.07879v1|How to avoid the curse of dimensionality: scalability of particle   filters with and without importance weights|avoid curs dimension scalabl particl filter without import weight|Particle filters are a popular and flexible class of numerical algorithms to solve a large class of nonlinear filtering problems. However, standard particle filters with importance weights have been shown to require a sample size that increases exponentially with the dimension D of the state space in order to achieve a certain performance, which precludes their use in very high-dimensional filtering problems. Here, we focus on the dynamic aspect of this curse of dimensionality (COD) in continuous time filtering, which is caused by the degeneracy of importance weights over time. We show that the degeneracy occurs on a time-scale that decreases with increasing D. In order to soften the effects of weight degeneracy, most particle filters use particle resampling and improved proposal functions for the particle motion. We explain why neither of the two can prevent the COD in general. In order to address this fundamental problem, we investigate an existing filtering algorithm based on optimal feedback control that sidesteps the use of importance weights. We use numerical experiments to show that this Feedback Particle Filter (FPF) by Yang et al. (2013) does not exhibit a COD.|particl filter popular flexibl class numer algorithm solv larg class nonlinear filter problem howev standard particl filter import weight shown requir sampl size increas exponenti dimens state space order achiev certain perform preclud use veri high dimension filter problem focus dynam aspect curs dimension cod continu time filter caus degeneraci import weight time show degeneraci occur time scale decreas increas order soften effect weight degeneraci particl filter use particl resampl improv propos function particl motion explain whi neither two prevent cod general order address fundament problem investig exist filter algorithm base optim feedback control sidestep use import weight use numer experi show feedback particl filter fpf yang et al doe exhibit cod|['Simone Carlo Surace', 'Anna Kutschireiter', 'Jean-Pascal Pfister']|['math.OC', 'math.PR', 'math.ST', 'stat.ME', 'stat.TH']
2017-03-28T14:04:33Z|2017-03-22T18:32:47Z|http://arxiv.org/abs/1703.07809v1|http://arxiv.org/pdf/1703.07809v1|Empirical Risk Minimization as Parameter Choice Rule for General Linear   Regularization Methods|empir risk minim paramet choic rule general linear regular method|We consider the statistical inverse problem to recover $f$ from noisy measurements $Y = Tf + \sigma \xi$ where $\xi$ is Gaussian white noise and $T$ a compact operator between Hilbert spaces. Considering general reconstruction methods of the form $\hat f_\alpha = q_\alpha \left(T^*T\right)T^*Y$ with an ordered filter $q_\alpha$, we investigate the choice of the regularization parameter $\alpha$ by minimizing an unbiased estimate of the predictive risk $\mathbb E\left[\Vert Tf - T\hat f_\alpha\Vert^2\right]$. The corresponding parameter $\alpha_{\mathrm{pred}}$ and its usage are well-known in the literature, but oracle inequalities and optimality results in this general setting are unknown. We prove a (generalized) oracle inequality, which relates the direct risk $\mathbb E\left[\Vert f - \hat f_{\alpha_{\mathrm{pred}}}\Vert^2\right]$ with the oracle prediction risk $\inf_{\alpha>0}\mathbb E\left[\Vert Tf - T\hat f_{\alpha}\Vert^2\right]$. From this oracle inequality we are then able to conclude that the investigated parameter choice rule is of optimal order.   Finally we also present numerical simulations, which support the order optimality of the method and the quality of the parameter choice in finite sample situations.|consid statist invers problem recov noisi measur tf sigma xi xi gaussian white nois compact oper hilbert space consid general reconstruct method form hat alpha alpha left right order filter alpha investig choic regular paramet alpha minim unbias estim predict risk mathbb left vert tf hat alpha vert right correspond paramet alpha mathrm pred usag well known literatur oracl inequ optim result general set unknown prove general oracl inequ relat direct risk mathbb left vert hat alpha mathrm pred vert right oracl predict risk inf alpha mathbb left vert tf hat alpha vert right oracl inequ abl conclud investig paramet choic rule optim order final also present numer simul support order optim method qualiti paramet choic finit sampl situat|['Housen Li', 'Frank Werner']|['math.NA', 'math.ST', 'stat.TH', 'Primary 62G05, Secondary 62G20, 65J22, 65J20']
2017-03-28T14:04:37Z|2017-03-21T16:36:30Z|http://arxiv.org/abs/1703.07303v1|http://arxiv.org/pdf/1703.07303v1|Sequential Detection of Three-Dimensional Signals under Dependent Noise|sequenti detect three dimension signal depend nois|We study detection methods for multivariable signals under dependent noise. The main focus is on three-dimensional signals, i.e. on signals in the space-time domain. Examples for such signals are multifaceted. They include geographic and climatic data as well as image data, that are observed over a fixed time horizon. We assume that the signal is observed as a finite block of noisy samples whereby we are interested in detecting changes from a given reference signal. Our detector statistic is based on a sequential partial sum process, related to classical signal decomposition and reconstruction approaches applied to the sampled signal. We show that this detector process converges weakly under the no change null hypothesis that the signal coincides with the reference signal, provided that the spatial-temporal partial sum process associated to the random field of the noise terms disturbing the sampled signal con- verges to a Brownian motion. More generally, we also establish the limiting distribution under a wide class of local alternatives that allows for smooth as well as discontinuous changes. Our results also cover extensions to the case that the reference signal is unknown. We conclude with an extensive simulation study of the detection algorithm.|studi detect method multivari signal depend nois main focus three dimension signal signal space time domain exampl signal multifacet includ geograph climat data well imag data observ fix time horizon assum signal observ finit block noisi sampl wherebi interest detect chang given refer signal detector statist base sequenti partial sum process relat classic signal decomposit reconstruct approach appli sampl signal show detector process converg weak chang null hypothesi signal coincid refer signal provid spatial tempor partial sum process associ random field nois term disturb sampl signal con verg brownian motion general also establish limit distribut wide class local altern allow smooth well discontinu chang result also cover extens case refer signal unknown conclud extens simul studi detect algorithm|['Annabel Prause', 'Ansgar Steland']|['math.PR', 'math.ST', 'stat.TH', '60F17, 60F05, 60G35, 62L10, 62G08']
2017-03-28T14:04:37Z|2017-03-21T15:38:51Z|http://arxiv.org/abs/1703.07281v1|http://arxiv.org/pdf/1703.07281v1|Convergence rates in the central limit theorem for weighted sums of   Bernoulli random fields|converg rate central limit theorem weight sum bernoulli random field|We prove moment inequalities for a class of functionals of i.i.d. random fields. We then derive rates in the central limit theorem for weighted sums of such randoms fields via an approximation by $m$-dependent random fields.|prove moment inequ class function random field deriv rate central limit theorem weight sum random field via approxim depend random field|['Davide Giraudo']|['math.ST', 'math.PR', 'stat.TH']
2017-03-28T14:04:37Z|2017-03-21T14:20:44Z|http://arxiv.org/abs/1703.07233v1|http://arxiv.org/pdf/1703.07233v1|Gibbs Reference Prior for Robust Gaussian Process Emulation|gibb refer prior robust gaussian process emul|We propose an objective prior distribution on correlation kernel parameters for Simple Kriging models in the spirit of reference priors. Because it is proper and defined through its conditional densities, it and its associated posterior distribution lend themselves well to Gibbs sampling, thus making the full-Bayesian procedure tractable. Numerical examples show it has near-optimal frequentist performance in terms of prediction interval coverage|propos object prior distribut correl kernel paramet simpl krige model spirit refer prior becaus proper defin condit densiti associ posterior distribut lend themselv well gibb sampl thus make full bayesian procedur tractabl numer exampl show near optim frequentist perform term predict interv coverag|['Joseph Muré']|['math.ST', 'stat.TH', '62F15 (Primary) 62M30, 60G15 (Secondary)']
2017-03-28T14:04:37Z|2017-03-21T06:24:47Z|http://arxiv.org/abs/1703.07072v1|http://arxiv.org/pdf/1703.07072v1|Bayesian Nonparametric Inference for M/G/1 Queueing Systems|bayesian nonparametr infer queue system|In this work, nonparametric statistical inference is provided for the continuous-time M/G/1 queueing model from a Bayesian point of view. The inference is based on observations of the inter-arrival and service times. Beside other characteristics of the system, particular interest is in the waiting time distribution which is not accessible in closed form. Thus, we use an indirect statistical approach by exploiting the Pollaczek-Khinchine transform formula for the Laplace transform of the waiting time distribution. Due to this, an estimator is defined and its frequentist validation in terms of posterior consistency and posterior normality is studied. It will turn out that we can hereby make inference for the observables separately and compose the results subsequently by suitable techniques.|work nonparametr statist infer provid continu time queue model bayesian point view infer base observ inter arriv servic time besid characterist system particular interest wait time distribut access close form thus use indirect statist approach exploit pollaczek khinchin transform formula laplac transform wait time distribut due estim defin frequentist valid term posterior consist posterior normal studi turn herebi make infer observ separ compos result subsequ suitabl techniqu|['Cornelia Wichelhaus', 'Moritz von Rohrscheidt']|['math.ST', 'stat.TH']
2017-03-28T14:04:37Z|2017-03-21T03:58:47Z|http://arxiv.org/abs/1703.07044v1|http://arxiv.org/pdf/1703.07044v1|The Minimum Distance Estimation with Multiple Integral in Panel Data|minimum distanc estim multipl integr panel data|This paper studies the minimum distance estimation problem for panel data model. We propose the minimum distance estimators of regression parameters of the panel data model and investigate their asymptotic distributions. This paper contains two main contributions. First, the domain of application of the minimum distance estimation method is extended to the panel data model. Second, the proposed estimators are more efficient than other existing ones. Simulation studies compare performance of the proposed estimators with performance of others and demonstrate some superiority of our estimators.|paper studi minimum distanc estim problem panel data model propos minimum distanc estim regress paramet panel data model investig asymptot distribut paper contain two main contribut first domain applic minimum distanc estim method extend panel data model second propos estim effici exist one simul studi compar perform propos estim perform demonstr superior estim|['Jiwoong Kim']|['math.ST', 'stat.TH']
2017-03-28T14:04:37Z|2017-03-20T15:55:05Z|http://arxiv.org/abs/1703.06810v1|http://arxiv.org/pdf/1703.06810v1|The geometry of hypothesis testing over convex cones: Generalized   likelihood tests and minimax radii|geometri hypothesi test convex cone general likelihood test minimax radii|We consider a compound testing problem within the Gaussian sequence model in which the null and alternative are specified by a pair of closed, convex cones. Such cone testing problem arise in various applications, including detection of treatment effects, trend detection in econometrics, signal detection in radar processing, and shape-constrained inference in non-parametric statistics. We provide a sharp characterization of the GLRT testing radius up to a universal multiplicative constant in terms of the geometric structure of the underlying convex cones. When applied to concrete examples, this result reveals some interesting phenomena that do not arise in the analogous problems of estimation under convex constraints. In particular, in contrast to estimation error, the testing error no longer depends purely on the problem complexity via a volume-based measure (such as metric entropy or Gaussian complexity), other geometric properties of the cones also play an important role. To address the issue of optimality, we prove information-theoretic lower bounds for minimax testing radius again in terms of geometric quantities. Our general theorems are illustrated by examples including the cases of monotone and orthant cones, and involve some results of independent interest.|consid compound test problem within gaussian sequenc model null altern specifi pair close convex cone cone test problem aris various applic includ detect treatment effect trend detect econometr signal detect radar process shape constrain infer non parametr statist provid sharp character glrt test radius univers multipl constant term geometr structur convex cone appli concret exampl result reveal interest phenomena aris analog problem estim convex constraint particular contrast estim error test error longer depend pure problem complex via volum base measur metric entropi gaussian complex geometr properti cone also play import role address issu optim prove inform theoret lower bound minimax test radius term geometr quantiti general theorem illustr exampl includ case monoton orthant cone involv result independ interest|['Yuting Wei', 'Martin J. Wainwright', 'Adityanand Guntuboyina']|['math.ST', 'cs.IT', 'math.IT', 'stat.TH']
2017-03-28T14:04:37Z|2017-03-20T05:37:48Z|http://arxiv.org/abs/1703.06610v1|http://arxiv.org/pdf/1703.06610v1|Asymptotic Performance of PCA for High-Dimensional Heteroscedastic Data|asymptot perform pca high dimension heteroscedast data|Principal Component Analysis (PCA) is a classical method for reducing the dimensionality of data by projecting them onto a subspace that captures most of their variation. Effective use of PCA in modern applications requires understanding its performance for data that are both high-dimensional (i.e., with dimension comparable to or larger than the number of samples) and heteroscedastic (i.e., with noise that has non uniform variance across samples such as outliers). This paper analyzes the statistical performance of PCA in this setting, that is, for high-dimensional data drawn from a low-dimensional subspace and degraded by heteroscedastic noise. We provide simple expressions for the asymptotic PCA recovery of the underlying subspace, subspace amplitudes and subspace coefficients; the expressions enable both easy and efficient calculation and reasoning about the performance of PCA. We exploit the structure of these expressions to show that asymptotic recovery for a fixed average noise variance is maximized when the noise variances are equal (i.e., when the noise is in fact homoscedastic). Hence, while average noise variance is often a practically convenient measure for the overall quality of data, it gives an overly optimistic estimate of the performance of PCA for heteroscedastic data.|princip compon analysi pca classic method reduc dimension data project onto subspac captur variat effect use pca modern applic requir understand perform data high dimension dimens compar larger number sampl heteroscedast nois non uniform varianc across sampl outlier paper analyz statist perform pca set high dimension data drawn low dimension subspac degrad heteroscedast nois provid simpl express asymptot pca recoveri subspac subspac amplitud subspac coeffici express enabl easi effici calcul reason perform pca exploit structur express show asymptot recoveri fix averag nois varianc maxim nois varianc equal nois fact homoscedast henc averag nois varianc often practic conveni measur overal qualiti data give optimist estim perform pca heteroscedast data|['David Hong', 'Laura Balzano', 'Jeffrey A. Fessler']|['math.ST', 'stat.TH']
2017-03-28T14:04:37Z|2017-03-18T23:22:23Z|http://arxiv.org/abs/1703.06367v1|http://arxiv.org/pdf/1703.06367v1|Optimal Learning from Multiple Information Sources|optim learn multipl inform sourc|"Decision-makers often learn by acquiring information from distinct sources that possibly provide complementary information. We consider a decision-maker who sequentially samples from a finite set of Gaussian signals, and wants to predict a persistent multi-dimensional state at an unknown final period. What signal should he choose to observe in each period? Related problems about optimal experimentation and dynamic learning tend to have solutions that can only be approximated or implicitly characterized. In contrast, we find that in our problem, the dynamically optimal path of signal acquisitions generically: (1) eventually coincides at every period with the myopic path of signal acquisitions, and (2) eventually achieves ""total optimality,"" so that at every large period, the decision-maker will not want to revise his previous signal acquisitions, even if given this opportunity. In special classes of environments that we describe, these properties attain not only eventually, but from period 1. Finally, we characterize the asymptotic frequency with which each signal is chosen, and how this depends on primitives of the informational environment."|decis maker often learn acquir inform distinct sourc possibl provid complementari inform consid decis maker sequenti sampl finit set gaussian signal want predict persist multi dimension state unknown final period signal choos observ period relat problem optim experiment dynam learn tend solut onli approxim implicit character contrast find problem dynam optim path signal acquisit generic eventu coincid everi period myopic path signal acquisit eventu achiev total optim everi larg period decis maker want revis previous signal acquisit even given opportun special class environ describ properti attain onli eventu period final character asymptot frequenc signal chosen depend primit inform environ|['Annie Liang', 'Xiaosheng Mu', 'Vasilis Syrgkanis']|['cs.GT', 'cs.LG', 'math.ST', 'stat.TH']
2017-03-28T14:04:37Z|2017-03-18T19:00:23Z|http://arxiv.org/abs/1703.06336v1|http://arxiv.org/pdf/1703.06336v1|Analysis of error control in large scale two-stage multiple hypothesis   testing|analysi error control larg scale two stage multipl hypothesi test|"When dealing with the problem of simultaneously testing a large number of null hypotheses, a natural testing strategy is to first reduce the number of tested hypotheses by some selection (screening or filtering) process, and then to simultaneously test the selected hypotheses. The main advantage of this strategy is to greatly reduce the severe effect of high dimensions. However, the first screening or selection stage must be properly accounted for in order to maintain some type of error control. In this paper, we will introduce a selection rule based on a selection statistic that is independent of the test statistic when the tested hypothesis is true. Combining this selection rule and the conventional Bonferroni procedure, we can develop a powerful and valid two-stage procedure. The introduced procedure has several nice properties: (i) it completely removes the selection effect; (ii) it reduces the multiplicity effect; (iii) it does not ""waste"" data while carrying out both selection and testing. Asymptotic power analysis and simulation studies illustrate that this proposed method can provide higher power compared to usual multiple testing methods while controlling the Type 1 error rate. Optimal selection thresholds are also derived based on our asymptotic analysis."|deal problem simultan test larg number null hypothes natur test strategi first reduc number test hypothes select screen filter process simultan test select hypothes main advantag strategi great reduc sever effect high dimens howev first screen select stage must proper account order maintain type error control paper introduc select rule base select statist independ test statist test hypothesi true combin select rule convent bonferroni procedur develop power valid two stage procedur introduc procedur sever nice properti complet remov select effect ii reduc multipl effect iii doe wast data carri select test asymptot power analysi simul studi illustr propos method provid higher power compar usual multipl test method control type error rate optim select threshold also deriv base asymptot analysi|['Wenge Guo', 'Joseph P. Romano']|['stat.ME', 'math.ST', 'stat.TH', '62J15']
2017-03-28T14:04:37Z|2017-03-18T00:08:59Z|http://arxiv.org/abs/1703.06222v1|http://arxiv.org/pdf/1703.06222v1|A Unified Treatment of Multiple Testing with Prior Knowledge|unifi treatment multipl test prior knowledg|A significant literature has arisen to study ways to employing prior knowledge to improve power and precision of multiple testing procedures. Some common forms of prior knowledge may include (a) a priori beliefs about which hypotheses are null, modeled by non-uniform prior weights; (b) differing importances of hypotheses, modeled by differing penalties for false discoveries; (c) partitions of the hypotheses into known groups, indicating (dis)similarity of hypotheses; and (d) knowledge of independence, positive dependence or arbitrary dependence between hypotheses or groups, allowing for more aggressive or conservative procedures. We present a general framework for global null testing and false discovery rate (FDR) control that allows the scientist to incorporate all four types of prior knowledge (a)-(d) simultaneously. We unify a number of existing procedures, generalize the conditions under which they are known to work, and simplify their proofs of FDR control under independence, positive and arbitrary dependence. We also present an algorithmic framework that strictly generalizes and unifies the classic algorithms of Benjamini and Hochberg [3] and Simes [25], algorithms that guard against unknown dependence [7, 9], algorithms that employ prior weights [17, 15], algorithms that use penalty weights [4], algorithms that incorporate null-proportion adaptivity [26, 27], and algorithms that make use of multiple arbitrary partitions into groups [1]. Unlike this previous work, we can simultaneously incorporate all of the four types of prior knowledge, combined with all of the three forms of dependence.|signific literatur arisen studi way employ prior knowledg improv power precis multipl test procedur common form prior knowledg may includ priori belief hypothes null model non uniform prior weight differ import hypothes model differ penalti fals discoveri partit hypothes known group indic dis similar hypothes knowledg independ posit depend arbitrari depend hypothes group allow aggress conserv procedur present general framework global null test fals discoveri rate fdr control allow scientist incorpor four type prior knowledg simultan unifi number exist procedur general condit known work simplifi proof fdr control independ posit arbitrari depend also present algorithm framework strict general unifi classic algorithm benjamini hochberg sime algorithm guard unknown depend algorithm employ prior weight algorithm use penalti weight algorithm incorpor null proport adapt algorithm make use multipl arbitrari partit group unlik previous work simultan incorpor four type prior knowledg combin three form depend|['Aaditya Ramdas', 'Rina Foygel Barber', 'Martin J. Wainwright', 'Michael I. Jordan']|['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']
2017-03-28T14:04:42Z|2017-03-16T21:04:10Z|http://arxiv.org/abs/1703.05757v1|http://arxiv.org/pdf/1703.05757v1|The Beta Flexible Weibull Distribution|beta flexibl weibul distribut|We introduce in this paper a new generalization of the flexible Weibull distribution with four parameters. This model based on the Beta generalized (BG) distribution, Eugene et al. \cite{Eugeneetal2002}, they first using the BG distribution for generating new generalizations. This new model is called the beta flexible Weibull BFW distribution. Some statistical properties such as the mode, the $r$th moment, skewness and kurtosis are derived. The moment generating function and the order statistics are obtained. Moreover, the estimations of the parameters are given by maximum likelihood method and the Fisher's information matrix is derived. Finally, we study the advantage of the BFW distribution by an application using real data set.|introduc paper new general flexibl weibul distribut four paramet model base beta general bg distribut eugen et al cite eugeneet first use bg distribut generat new general new model call beta flexibl weibul bfw distribut statist properti mode th moment skew kurtosi deriv moment generat function order statist obtain moreov estim paramet given maximum likelihood method fisher inform matrix deriv final studi advantag bfw distribut applic use real data set|['Beih S. El-Desouky', 'Abdelfattah Mustafa', 'Shamsan AL-Garash']|['math.ST', 'stat.TH', '60E05, 62N02, 62N03']
2017-03-28T14:04:42Z|2017-03-16T13:48:45Z|http://arxiv.org/abs/1703.05619v1|http://arxiv.org/pdf/1703.05619v1|Nonparametric intensity estimation from indirect point process   observations under unknown error distribution|nonparametr intens estim indirect point process observ unknown error distribut|We consider the nonparametric estimation of the intensity function of a Poisson point process in a circular model from indirect observations $N_1,\ldots,N_n$. These observations emerge from hidden point process realizations with the target intensity through contamination with additive error. Under the assumption that the error distribution is unknown and only available by means of an additional sample $Y_1,\ldots,Y_m$ we derive minimax rates of convergence with respect to the sample sizes $n$ and $m$ under abstract smoothness conditions and propose an orthonormal series estimator which attains the optimal rate of convergence. The performance of the estimator depends on the correct specification of a dimension parameter whose optimal choice relies on smoothness characteristics of both the intensity and the error density. Since a priori knowledge of such characteristics is a too strong assumption, we propose a data-driven choice of the dimension parameter based on model selection and show that the adaptive estimator either attains the minimax optimal rate or is suboptimal only by a logarithmic factor.|consid nonparametr estim intens function poisson point process circular model indirect observ ldot observ emerg hidden point process realize target intens contamin addit error assumpt error distribut unknown onli avail mean addit sampl ldot deriv minimax rate converg respect sampl size abstract smooth condit propos orthonorm seri estim attain optim rate converg perform estim depend correct specif dimens paramet whose optim choic reli smooth characterist intens error densiti sinc priori knowledg characterist strong assumpt propos data driven choic dimens paramet base model select show adapt estim either attain minimax optim rate suboptim onli logarithm factor|['Martin Kroll']|['math.ST', 'stat.TH', '62G05, 60G55']
2017-03-28T14:04:42Z|2017-03-15T12:04:34Z|http://arxiv.org/abs/1703.05101v1|http://arxiv.org/pdf/1703.05101v1|Optimal graphon estimation in cut distance|optim graphon estim cut distanc|Consider the twin problems of estimating the connection probability matrix of an inhomogeneous random graph and the graphon of a W-random graph. We establish the minimax estimation rates with respect to the cut metric for classes of block constant matrices and step function graphons. Surprisingly, our results imply that, from the minimax point of view, the raw data, that is, the adjacency matrix of the observed graph, is already optimal and more involved procedures cannot improve the convergence rates for this metric. This phenomenon contrasts with optimal rates of convergence with respect to other classical distances for graphons such as the l 1 or l 2 metrics.|consid twin problem estim connect probabl matrix inhomogen random graph graphon random graph establish minimax estim rate respect cut metric class block constant matric step function graphon surpris result impli minimax point view raw data adjac matrix observ graph alreadi optim involv procedur cannot improv converg rate metric phenomenon contrast optim rate converg respect classic distanc graphon metric|['Olga Klopp', 'Nicolas Verzelen']|['math.ST', 'stat.TH']
2017-03-28T14:04:42Z|2017-03-15T06:34:38Z|http://arxiv.org/abs/1703.04956v1|http://arxiv.org/pdf/1703.04956v1|A Short Note on Almost Sure Convergence of Bayes Factors in the General   Set-Up|short note almost sure converg bay factor general set|In this article we derive the almost sure convergence theory of Bayes factor in the general set-up that includes even dependent data and misspecified models, as a simple application of a result of Shalizi (2009) to a well-known identity satisfied by the Bayes factor.|articl deriv almost sure converg theori bay factor general set includ even depend data misspecifi model simpl applic result shalizi well known ident satisfi bay factor|['Debashis Chatterjee', 'Trisha Maitra', 'Sourabh Bhattacharya']|['math.ST', 'stat.ME', 'stat.TH']
2017-03-28T14:04:42Z|2017-03-15T06:26:43Z|http://arxiv.org/abs/1703.04955v1|http://arxiv.org/pdf/1703.04955v1|Theoretical Limits of Record Linkage and Microclustering|theoret limit record linkag microclust|"There has been substantial recent interest in record linkage, attempting to group the records pertaining to the same entities from a large database lacking unique identifiers. This can be viewed as a type of ""microclustering,"" with few observations per cluster and a very large number of clusters. A variety of methods have been proposed, but there is a lack of literature providing theoretical guarantees on performance. We show that the problem is fundamentally hard from a theoretical perspective, and even in idealized cases, accurate entity resolution is effectively impossible when the number of entities is small relative to the number of records and/or the separation among records from different entities is not extremely large. To characterize the fundamental difficulty, we focus on entity resolution based on multivariate Gaussian mixture models, but our conclusions apply broadly and are supported by simulation studies inspired by human rights applications. These results suggest conservatism in interpretation of the results of record linkage, support collection of additional data to more accurately disambiguate the entities, and motivate a focus on coarser inference. For example, results from a simulation study suggest that sometimes one may obtain accurate results for population size estimation even when fine scale entity resolution is inaccurate."|substanti recent interest record linkag attempt group record pertain entiti larg databas lack uniqu identifi view type microclust observ per cluster veri larg number cluster varieti method propos lack literatur provid theoret guarante perform show problem fundament hard theoret perspect even ideal case accur entiti resolut effect imposs number entiti small relat number record separ among record differ entiti extrem larg character fundament difficulti focus entiti resolut base multivari gaussian mixtur model conclus appli broad support simul studi inspir human right applic result suggest conservat interpret result record linkag support collect addit data accur disambigu entiti motiv focus coarser infer exampl result simul studi suggest sometim one may obtain accur result popul size estim even fine scale entiti resolut inaccur|['James E. Johndrow', 'Kristian Lum', 'David B. Dunson']|['math.ST', 'stat.TH']
2017-03-28T14:04:42Z|2017-03-15T02:25:31Z|http://arxiv.org/abs/1703.04886v1|http://arxiv.org/pdf/1703.04886v1|Towards Optimal Sparse Inverse Covariance Selection through Non-Convex   Optimization|toward optim spars invers covari select non convex optim|We study the problem of reconstructing the graph of a sparse Gaussian Graphical Model from independent observations, which is equivalent to finding non-zero elements of an inverse covariance matrix. For a model of size $p$ and maximum degree $d$, information theoretic lower bounds established in prior works require that the number of samples needed for recovering the graph perfectly is at least $d \log p/\kappa^2$, where $\kappa$ is the minimum normalized non-zero entry of the inverse covariance matrix. Existing algorithms require additional assumptions to guarantee perfect graph reconstruction, and consequently, their sample complexity is dependent on parameters that are not present in the information theoretic lower bound. We propose an estimator, called SLICE, that consists of a cardinality constrained least-squares regression followed by a thresholding procedure. Without any additional assumptions we show that SLICE attains a sample complexity of $\frac{64}{\kappa^4}d \log p$, which differs from the lower bound by only a factor proportional to $1/\kappa^2$ and depends only on parameters present in the lower bound.|studi problem reconstruct graph spars gaussian graphic model independ observ equival find non zero element invers covari matrix model size maximum degre inform theoret lower bound establish prior work requir number sampl need recov graph perfect least log kappa kappa minimum normal non zero entri invers covari matrix exist algorithm requir addit assumpt guarante perfect graph reconstruct consequ sampl complex depend paramet present inform theoret lower bound propos estim call slice consist cardin constrain least squar regress follow threshold procedur without ani addit assumpt show slice attain sampl complex frac kappa log differ lower bound onli factor proport kappa depend onli paramet present lower bound|['Sidhant Misra', 'Marc Vuffray', 'Andrey Y. Lokhov', 'Michael Chertkov']|['cs.LG', 'cs.IT', 'math.IT', 'math.ST', 'stat.TH']
2017-03-28T14:04:42Z|2017-03-14T22:49:50Z|http://arxiv.org/abs/1703.04799v1|http://arxiv.org/pdf/1703.04799v1|Multi-parameter One-Sided Monitoring Test|multi paramet one side monitor test|Multi-parameter one-sided hypothesis test problems arise naturally in many applications. We are particularly interested in effective tests for monitoring multiple quality indices in forestry products. Our search reveals that there are many effective statistical methods in the literature for normal data, and that they can easily be adapted for non-normal data. We find that the beautiful likelihood ratio test is unsatisfactory, because in order to control the size, it must cope with the least favorable distributions at the cost of power. In this paper, we find a novel way to slightly ease the size control, obtaining a much more powerful test. Simulation confirms that the new test retains good control of the type I error and is markedly more powerful than the likelihood ratio test as well as many competitors based on normal data. The new method performs well in the context of monitoring multiple quality indices.|multi paramet one side hypothesi test problem aris natur mani applic particular interest effect test monitor multipl qualiti indic forestri product search reveal mani effect statist method literatur normal data easili adapt non normal data find beauti likelihood ratio test unsatisfactori becaus order control size must cope least favor distribut cost power paper find novel way slight eas size control obtain much power test simul confirm new test retain good control type error mark power likelihood ratio test well mani competitor base normal data new method perform well context monitor multipl qualiti indic|['Guangyu Zhu', 'Jiahua Chen']|['math.ST', 'stat.TH']
2017-03-28T14:04:42Z|2017-03-14T22:36:09Z|http://arxiv.org/abs/1703.04790v1|http://arxiv.org/pdf/1703.04790v1|Robust Power System Dynamic State Estimator with Non-Gaussian   Measurement Noise: Part I--Theory|robust power system dynam state estim non gaussian measur nois part theori|This paper develops the theoretical framework and the equations of a new robust Generalized Maximum-likelihood-type Unscented Kalman Filter (GM-UKF) that is able to suppress observation and innovation outliers while filtering out non-Gaussian measurement noise. Because the errors of the real and reactive power measurements calculated using Phasor Measurement Units (PMUs) follow long-tailed probability distributions, the conventional UKF provides strongly biased state estimates since it relies on the weighted least squares estimator. By contrast, the state estimates and residuals of our GM-UKF are proved to be roughly Gaussian, allowing the sigma points to reliably approximate the mean and the covariance matrices of the predicted and corrected state vectors. To develop our GM-UKF, we first derive a batch-mode regression form by processing the predictions and observations simultaneously, where the statistical linearization approach is used. We show that the set of equations so derived are equivalent to those of the unscented transformation. Then, a robust GM-estimator that minimizes a convex Huber cost function while using weights calculated via Projection Statistics (PS's) is proposed. The PS's are applied to a two-dimensional matrix that consists of serially correlated predicted state and innovation vectors to detect observation and innovation outliers. These outliers are suppressed by the GM-estimator using the iteratively reweighted least squares algorithm. Finally, the asymptotic error covariance matrix of the GM-UKF state estimates is derived from the total influence function. In the companion paper, extensive simulation results will be shown to verify the effectiveness and robustness of the proposed method.|paper develop theoret framework equat new robust general maximum likelihood type unscent kalman filter gm ukf abl suppress observ innov outlier filter non gaussian measur nois becaus error real reactiv power measur calcul use phasor measur unit pmus follow long tail probabl distribut convent ukf provid strong bias state estim sinc reli weight least squar estim contrast state estim residu gm ukf prove rough gaussian allow sigma point reliabl approxim mean covari matric predict correct state vector develop gm ukf first deriv batch mode regress form process predict observ simultan statist linear approach use show set equat deriv equival unscent transform robust gm estim minim convex huber cost function use weight calcul via project statist ps propos ps appli two dimension matrix consist serial correl predict state innov vector detect observ innov outlier outlier suppress gm estim use iter reweight least squar algorithm final asymptot error covari matrix gm ukf state estim deriv total influenc function companion paper extens simul result shown verifi effect robust propos method|['Junbo Zhao', 'Lamine Mili']|['math.ST', 'stat.TH']
2017-03-28T14:04:42Z|2017-03-14T20:19:08Z|http://arxiv.org/abs/1703.04697v1|http://arxiv.org/pdf/1703.04697v1|On the benefits of output sparsity for multi-label classification|benefit output sparsiti multi label classif|The multi-label classification framework, where each observation can be associated with a set of labels, has generated a tremendous amount of attention over recent years. The modern multi-label problems are typically large-scale in terms of number of observations, features and labels, and the amount of labels can even be comparable with the amount of observations. In this context, different remedies have been proposed to overcome the curse of dimensionality. In this work, we aim at exploiting the output sparsity by introducing a new loss, called the sparse weighted Hamming loss. This proposed loss can be seen as a weighted version of classical ones, where active and inactive labels are weighted separately. Leveraging the influence of sparsity in the loss function, we provide improved generalization bounds for the empirical risk minimizer, a suitable property for large-scale problems. For this new loss, we derive rates of convergence linear in the underlying output-sparsity rather than linear in the number of labels. In practice, minimizing the associated risk can be performed efficiently by using convex surrogates and modern convex optimization algorithms. We provide experiments on various real-world datasets demonstrating the pertinence of our approach when compared to non-weighted techniques.|multi label classif framework observ associ set label generat tremend amount attent recent year modern multi label problem typic larg scale term number observ featur label amount label even compar amount observ context differ remedi propos overcom curs dimension work aim exploit output sparsiti introduc new loss call spars weight ham loss propos loss seen weight version classic one activ inact label weight separ leverag influenc sparsiti loss function provid improv general bound empir risk minim suitabl properti larg scale problem new loss deriv rate converg linear output sparsiti rather linear number label practic minim associ risk perform effici use convex surrog modern convex optim algorithm provid experi various real world dataset demonstr pertin approach compar non weight techniqu|['Evgenii Chzhen', 'Christophe Denis', 'Mohamed Hebiri', 'Joseph Salmon']|['math.ST', 'cs.LG', 'stat.ML', 'stat.TH']
2017-03-28T14:04:42Z|2017-03-16T01:23:09Z|http://arxiv.org/abs/1703.04661v2|http://arxiv.org/pdf/1703.04661v2|A Noninformative Prior on a Space of Functions|noninform prior space function|In a given problem, the Bayesian statistical paradigm requires the specification of a prior distribution that quantifies relevant information, about the unknowns of main interest, external to the data. In cases where little such information is available, the problem under study may possess an invariance under a transformation group that encodes a lack of information, leading to a unique prior. Previous successful examples of this idea have included location-scale invariance under linear transformation, multiplicative invariance of the rate at which events in a counting process are observed, and the derivation of the Haldane prior for a Bernoulli success probability. In this paper we show that this method can be extended in two ways: (1) to yield families of approximately invariant priors, and (2) to the infinite-dimensional setting, yielding families of priors on spaces of distribution functions. Our results can be used to describe conditions under which a particular Dirichlet Process posterior arises from an optimal Bayesian analysis, in the sense that invariances in the prior and likelihood lead to one and only one posterior distribution.|given problem bayesian statist paradigm requir specif prior distribut quantifi relev inform unknown main interest extern data case littl inform avail problem studi may possess invari transform group encod lack inform lead uniqu prior previous success exampl idea includ locat scale invari linear transform multipl invari rate event count process observ deriv haldan prior bernoulli success probabl paper show method extend two way yield famili approxim invari prior infinit dimension set yield famili prior space distribut function result use describ condit particular dirichlet process posterior aris optim bayesian analysi sens invari prior likelihood lead one onli one posterior distribut|['Alexander Terenin', 'David Draper']|['math.ST', 'stat.TH']
2017-03-28T14:04:46Z|2017-03-13T17:54:07Z|http://arxiv.org/abs/1703.04517v1|http://arxiv.org/pdf/1703.04517v1|Variable selection in discriminant analysis for mixed variables and   several groups|variabl select discrimin analysi mix variabl sever group|We propose a method for variable selection in discriminant analysis with mixed categorical and continuous variables. This method is based on a criterion that permits to reduce the variable selection problem to a problem of estimating suitable permutation and dimensionality. Then, estimators for these parameters are proposed and the resulting method for selecting variables is shown to be consistent. A simulation study that permits to study several poperties of the proposed approach and to compare it with an existing method is given.|propos method variabl select discrimin analysi mix categor continu variabl method base criterion permit reduc variabl select problem problem estim suitabl permut dimension estim paramet propos result method select variabl shown consist simul studi permit studi sever poperti propos approach compar exist method given|['Alban Mbina Mbina', 'Guy Martial Nkiet', 'Fulgence Eyi Obiang']|['math.ST', 'math.PR', 'stat.TH']
2017-03-28T14:04:46Z|2017-03-13T14:36:48Z|http://arxiv.org/abs/1703.04419v1|http://arxiv.org/pdf/1703.04419v1|Iterated failure rate monotonicity and ordering relations within Gamma   and Weibull distributions|iter failur rate monoton order relat within gamma weibul distribut|Stochastic ordering of distributions of random variables may be defined by the relative convexity of the tail functions. This has been extended to higher order stochastic orderings, by iteratively reassigning tail-weights. The actual verification of those stochastic orderings is not simple, as this depends on inverting distribution functions for which there may be no explicit expression. The iterative definition of distributions, of course, contributes to make that verification even harder. We have a look at the stochastic ordering, introducing a method that allows for explicit usage, applying it to the Gamma and Weibull distributions, giving a complete description of the order relations within each of those families.|stochast order distribut random variabl may defin relat convex tail function extend higher order stochast order iter reassign tail weight actual verif stochast order simpl depend invert distribut function may explicit express iter definit distribut cours contribut make verif even harder look stochast order introduc method allow explicit usag appli gamma weibul distribut give complet descript order relat within famili|['Idir Arab', 'Paulo Eduardo Oliveira']|['math.ST', 'stat.TH', '60E15, 26A51']
2017-03-28T14:04:46Z|2017-03-13T10:34:44Z|http://arxiv.org/abs/1703.04320v1|http://arxiv.org/pdf/1703.04320v1|Fourier analysis of serial dependence measures|fourier analysi serial depend measur|Classical spectral analysis is based on the discrete Fourier transform of the auto-covariances. In this paper we investigate the asymptotic properties of new frequency domain methods where the auto-covariances in the spectral density are replaced by alternative dependence measures which can be estimated by U-statistics. An interesting example is given by Kendall{'}s $\tau$ , for which the limiting variance exhibits a surprising behavior.|classic spectral analysi base discret fourier transform auto covari paper investig asymptot properti new frequenc domain method auto covari spectral densiti replac altern depend measur estim statist interest exampl given kendal tau limit varianc exhibit surpris behavior|['Ria van Hecke', 'Stanislav Volgushev', 'Holger Dette']|['math.ST', 'stat.TH']
2017-03-28T14:04:46Z|2017-03-12T02:16:11Z|http://arxiv.org/abs/1703.04058v1|http://arxiv.org/pdf/1703.04058v1|Think globally, fit locally under the Manifold Setup: Asymptotic   Analysis of Locally Linear Embedding|think global fit local manifold setup asymptot analysi local linear embed|Since its introduction in 2000, the locally linear embedding (LLE) has been widely applied in data science. We provide an asymptotical analysis of the LLE under the manifold setup. We show that for the general manifold, asymptotically we may not obtain the Laplace-Beltrami operator, and the result may depend on the non-uniform sampling, unless a correct regularization is chosen. We also derive the corresponding kernel function, which indicates that the LLE is not a Markov process. A comparison with the other commonly applied nonlinear algorithms, particularly the diffusion map, is provided, and its relationship with the locally linear regression is also discussed.|sinc introduct local linear embed lle wide appli data scienc provid asymptot analysi lle manifold setup show general manifold asymptot may obtain laplac beltrami oper result may depend non uniform sampl unless correct regular chosen also deriv correspond kernel function indic lle markov process comparison common appli nonlinear algorithm particular diffus map provid relationship local linear regress also discuss|['Hau-Tieng Wu', 'Nan Wu']|['math.ST', 'stat.TH', '62-07']
2017-03-28T14:04:46Z|2017-03-11T12:39:33Z|http://arxiv.org/abs/1703.03965v1|http://arxiv.org/pdf/1703.03965v1|Sparse Poisson Regression with Penalized Weighted Score Function|spars poisson regress penal weight score function|We proposed a new penalized method in this paper to solve sparse Poisson Regression problems. Being different from $\ell_1$ penalized log-likelihood estimation, our new method can be viewed as penalized weighted score function method. We show that under mild conditions, our estimator is $\ell_1$ consistent and the tuning parameter can be pre-specified, which shares the same good property of the square-root Lasso.|propos new penal method paper solv spars poisson regress problem differ ell penal log likelihood estim new method view penal weight score function method show mild condit estim ell consist tune paramet pre specifi share good properti squar root lasso|['Jinzhu Jia', 'Fang Xie', 'Lihu Xu']|['math.ST', 'stat.TH']
2017-03-28T14:04:46Z|2017-03-10T13:43:06Z|http://arxiv.org/abs/1703.03680v1|http://arxiv.org/pdf/1703.03680v1|Strong convergence rates of probabilistic integrators for ordinary   differential equations|strong converg rate probabilist integr ordinari differenti equat|Probabilistic integration of a continuous dynamical system is a way of systematically introducing model error, at scales no larger than errors inroduced by standard numerical discretisation, in order to enable thorough exploration of possible responses of the system to inputs. It is thus a potentially useful approach in a number of applications such as forward uncertainty quantification, inverse problems, and data assimilation. We extend the convergence analysis of probabilistic integrators for deterministic ordinary differential equations, as proposed by Conrad et al. (Stat. Comput., 2016), to establish mean-square convergence in the uniform norm on discrete- or continuous-time solutions under relaxed regularity assumptions on the driving vector fields and their induced flows. Specifically, we show that randomised high-order integrators for globally Lipschitz flows and randomised Euler integrators for dissipative vector fields with polynomially-bounded local Lipschitz constants all have the same mean-square convergence rate as their deterministic counterparts, provided that the variance of the integration noise is not of higher order than the corresponding deterministic integrator.|probabilist integr continu dynam system way systemat introduc model error scale larger error inroduc standard numer discretis order enabl thorough explor possibl respons system input thus potenti use approach number applic forward uncertainti quantif invers problem data assimil extend converg analysi probabilist integr determinist ordinari differenti equat propos conrad et al stat comput establish mean squar converg uniform norm discret continu time solut relax regular assumpt drive vector field induc flow specif show randomis high order integr global lipschitz flow randomis euler integr dissip vector field polynomi bound local lipschitz constant mean squar converg rate determinist counterpart provid varianc integr nois higher order correspond determinist integr|['H. C. Lie', 'A. M. Stuart', 'T. J. Sullivan']|['math.NA', 'math.PR', 'math.ST', 'stat.CO', 'stat.TH', '65L20, 65C99, 37H10, 68W20']
2017-03-28T14:04:46Z|2017-03-10T12:42:32Z|http://arxiv.org/abs/1703.03658v1|http://arxiv.org/pdf/1703.03658v1|Construction of Non-asymptotic Confidence Sets in 2-Wasserstein Space|construct non asymptot confid set wasserstein space|In this paper, we consider a probabilistic setting where the probability measures are considered to be random objects. We propose a procedure of construction non-asymptotic confidence sets for empirical barycenters in 2-Wasserstein space and develop the idea further to construction of a non-parametric two-sample test that is then applied to the detection of structural breaks in data with complex geometry. Both procedures mainly rely on the idea of multiplier bootstrap (Spokoiny and Zhilova (2015), Chernozhukov et al. (2014)). The main focus lies on probability measures that have commuting covariance matrices and belong to the same scatter-location family: we proof the validity of a bootstrap procedure that allows to compute confidence sets and critical values for a Wasserstein-based two-sample test.|paper consid probabilist set probabl measur consid random object propos procedur construct non asymptot confid set empir barycent wasserstein space develop idea construct non parametr two sampl test appli detect structur break data complex geometri procedur main reli idea multipli bootstrap spokoini zhilova chernozhukov et al main focus lie probabl measur commut covari matric belong scatter locat famili proof valid bootstrap procedur allow comput confid set critic valu wasserstein base two sampl test|['Johannes Ebert', 'Vladimir Spokoiny', 'Alexandra Suvorikova']|['math.ST', 'stat.TH']
2017-03-28T14:04:46Z|2017-03-09T19:00:01Z|http://arxiv.org/abs/1703.03412v1|http://arxiv.org/pdf/1703.03412v1|Uniform estimation of a class of random graph functionals|uniform estim class random graph function|We consider estimation of certain functionals of random graphs. The random graph is generated by a stochastic block model (SBM). The number of classes is fixed or grows with the number of vertices. Minimax lower and upper bounds of estimation along specific submodels are derived. The results are nonasymptotic and imply that uniform estimation of a single connectivity parameter is much slower than the expected asymptotic pointwise rate. Specifically, the uniform quadratic rate does not scale as the number of edges, but only as the number of vertices. The lower bounds are local around any possible SBM. An analogous result is derived for functionals of a class of smooth graphons.|consid estim certain function random graph random graph generat stochast block model sbm number class fix grow number vertic minimax lower upper bound estim along specif submodel deriv result nonasymptot impli uniform estim singl connect paramet much slower expect asymptot pointwis rate specif uniform quadrat rate doe scale number edg onli number vertic lower bound local around ani possibl sbm analog result deriv function class smooth graphon|['Ismaël Castillo', 'Peter Orbanz']|['math.ST', 'stat.TH']
2017-03-28T14:04:46Z|2017-03-09T17:18:21Z|http://arxiv.org/abs/1703.03353v1|http://arxiv.org/pdf/1703.03353v1|A Note on Bayesian Model Selection for Discrete Data Using Proper   Scoring Rules|note bayesian model select discret data use proper score rule|We consider the problem of choosing between parametric models for a discrete observable, taking a Bayesian approach in which the within-model prior distributions are allowed to be improper. In order to avoid the ambiguity in the marginal likelihood function in such a case, we apply a homogeneous scoring rule. For the particular case of distinguishing between Poisson and Negative Binomial models, we conduct simulations that indicate that, applied prequentially, the method will consistently select the true model.|consid problem choos parametr model discret observ take bayesian approach within model prior distribut allow improp order avoid ambigu margin likelihood function case appli homogen score rule particular case distinguish poisson negat binomi model conduct simul indic appli prequenti method consist select true model|['A. Philip Dawid', 'Monica Musio', 'Silvia Columbu']|['math.ST', 'stat.TH', 'Primary 62C99, secondary 62F15, 62A99']
2017-03-28T14:04:46Z|2017-03-10T16:16:31Z|http://arxiv.org/abs/1703.03282v2|http://arxiv.org/pdf/1703.03282v2|Confidence intervals in high-dimensional regression based on regularized   pseudoinverses|confid interv high dimension regress base regular pseudoinvers|In modern data sets, the number of available variables can greatly exceed the number of observations. In this paper we show how valid confidence intervals can be constructed by approximating the inverse covariance matrix by a scaled Moore-Penrose pseudoinverse, and using the lasso to perform a bias correction. In addition, we propose random least squares, a new regularization technique which yields narrower confidence intervals with the same theoretical validity. Random least squares estimates the inverse covariance matrix using multiple low-dimensional random projections of the data. This is shown to be equivalent to a generalized form of ridge regularization. The methods are illustrated in Monte Carlo experiments and an empirical example using quarterly data from the FRED-QD database, where gross domestic product is explained by a large number of macroeconomic and financial indicators.|modern data set number avail variabl great exceed number observ paper show valid confid interv construct approxim invers covari matrix scale moor penros pseudoinvers use lasso perform bias correct addit propos random least squar new regular techniqu yield narrow confid interv theoret valid random least squar estim invers covari matrix use multipl low dimension random project data shown equival general form ridg regular method illustr mont carlo experi empir exampl use quarter data fred qd databas gross domest product explain larg number macroeconom financi indic|['Tom Boot', 'Didier Nibbering']|['math.ST', 'stat.TH']
2017-03-28T14:04:50Z|2017-03-09T11:42:34Z|http://arxiv.org/abs/1703.03237v1|http://arxiv.org/pdf/1703.03237v1|Fractional compound Poisson processes with multiple internal states|fraction compound poisson process multipl intern state|For the particles undergoing the anomalous diffusion with different waiting time distributions for different internal states, we derive the Fokker-Planck and Feymann-Kac equations, respectively, describing positions of the particles and functional distributions of the trajectories of particles; in particular, the equations governing the functional distribution of internal states are also obtained. The dynamics of the stochastic processes are analyzed and the applications, calculating the distribution of the first passage time and the distribution of the fraction of the occupation time, of the equations are given.|particl undergo anomal diffus differ wait time distribut differ intern state deriv fokker planck feymann kac equat respect describ posit particl function distribut trajectori particl particular equat govern function distribut intern state also obtain dynam stochast process analyz applic calcul distribut first passag time distribut fraction occup time equat given|['Pengbo Xu', 'Weihua Deng']|['math.ST', 'cond-mat.stat-mech', 'stat.TH']
2017-03-28T14:04:50Z|2017-03-09T07:40:53Z|http://arxiv.org/abs/1703.03167v1|http://arxiv.org/pdf/1703.03167v1|Cross-validation|cross valid|This text is a survey on cross-validation. We define all classical cross-validation procedures, and we study their properties for two different goals: estimating the risk of a given estimator, and selecting the best estimator among a given family. For the risk estimation problem, we compute the bias (which can also be corrected) and the variance of cross-validation methods. For estimator selection, we first provide a first-order analysis (based on expectations). Then, we explain how to take into account second-order terms (from variance computations, and by taking into account the usefulness of overpenalization). This allows, in the end, to provide some guidelines for choosing the best cross-validation method for a given learning problem.|text survey cross valid defin classic cross valid procedur studi properti two differ goal estim risk given estim select best estim among given famili risk estim problem comput bias also correct varianc cross valid method estim select first provid first order analysi base expect explain take account second order term varianc comput take account use overpen allow end provid guidelin choos best cross valid method given learn problem|['Sylvain Arlot']|['math.ST', 'stat.ML', 'stat.TH']
2017-03-28T14:04:50Z|2017-03-09T07:27:33Z|http://arxiv.org/abs/1703.03165v1|http://arxiv.org/pdf/1703.03165v1|Perturbation Bootstrap in Adaptive Lasso|perturb bootstrap adapt lasso|The Adaptive LASSO (ALASSO) was proposed by Zou [J. Amer. Statist. Assoc. 101 (2006) 1418-1429] as a modification of the LASSO for the purpose of simultaneous variable selection and estimation of the parameters in a linear regression model. Zou (2006) established that the ALASSO estimator is variable-selection consistent as well as asymptotically Normal in the indices corresponding to the nonzero regression coefficients in certain fixed-dimensional settings. In an influential paper, Minnier, Tian and Cai [J. Amer. Statist. Assoc. 106 (2011) 1371-1382] proposed a perturbation bootstrap method and established its distributional consistency for the ALASSO estimator in the fixed-dimensional setting. In this paper, however, we show that this (naive) perturbation bootstrap fails to achieve second order correctness in approximating the distribution of the ALASSO estimator. We propose a modification to the perturbation bootstrap objective function and show that a suitably studentized version of our modified perturbation bootstrap ALASSO estimator achieves second-order correctness even when the dimension of the model is allowed to grow to infinity with the sample size. As a consequence, inferences based on the modified perturbation bootstrap will be more accurate than the inferences based on the oracle Normal approximation. We give simulation studies demonstrating good finite-sample properties of our modified perturbation bootstrap method as well as an illustration of our method on a real data set.|adapt lasso alasso propos zou amer statist assoc modif lasso purpos simultan variabl select estim paramet linear regress model zou establish alasso estim variabl select consist well asymptot normal indic correspond nonzero regress coeffici certain fix dimension set influenti paper minnier tian cai amer statist assoc propos perturb bootstrap method establish distribut consist alasso estim fix dimension set paper howev show naiv perturb bootstrap fail achiev second order correct approxim distribut alasso estim propos modif perturb bootstrap object function show suitabl student version modifi perturb bootstrap alasso estim achiev second order correct even dimens model allow grow infin sampl size consequ infer base modifi perturb bootstrap accur infer base oracl normal approxim give simul studi demonstr good finit sampl properti modifi perturb bootstrap method well illustr method real data set|['Debraj Das', 'Karl Gregory', 'S. N. Lahiri']|['stat.ME', 'math.ST', 'stat.TH']
2017-03-28T14:04:50Z|2017-03-08T21:03:22Z|http://arxiv.org/abs/1703.03031v1|http://arxiv.org/pdf/1703.03031v1|Statistical Inference on Panel Data Models: A Kernel Ridge Regression   Method|statist infer panel data model kernel ridg regress method|We propose statistical inferential procedures for panel data models with interactive fixed effects in a kernel ridge regression framework.Compared with traditional sieve methods, our method is automatic in the sense that it does not require the choice of basis functions and truncation parameters.Model complexity is controlled by a continuous regularization parameter which can be automatically selected by generalized cross validation. Based on empirical processes theory and functional analysis tools, we derive joint asymptotic distributions for the estimators in the heterogeneous setting. These joint asymptotic results are then used to construct confidence intervals for the regression means and prediction intervals for the future observations, both being the first provably valid intervals in literature. Marginal asymptotic normality of the functional estimators in homogeneous setting is also obtained. Simulation and real data analysis demonstrate the advantages of our method.|propos statist inferenti procedur panel data model interact fix effect kernel ridg regress framework compar tradit siev method method automat sens doe requir choic basi function truncat paramet model complex control continu regular paramet automat select general cross valid base empir process theori function analysi tool deriv joint asymptot distribut estim heterogen set joint asymptot result use construct confid interv regress mean predict interv futur observ first provabl valid interv literatur margin asymptot normal function estim homogen set also obtain simul real data analysi demonstr advantag method|['Shunan Zhao', 'Ruiqi Liu', 'Zuofeng Shang']|['math.ST', 'stat.TH']
2017-03-28T14:04:50Z|2017-03-09T16:57:54Z|http://arxiv.org/abs/1703.02907v2|http://arxiv.org/pdf/1703.02907v2|Improved bounds for Square-Root Lasso and Square-Root Slope|improv bound squar root lasso squar root slope|We show that two estimators, the Square-Root Lasso and the Square-Root Slope can achieve the exact optimal minimax prediction rate, which is $(s/n) \log(p/s)$ in the setting of the sparse high-dimensional linear regression. Here, $n$ is the sample size, $p$ is the dimension and $s$ is the sparsity parameter. We also prove optimality for the estimation error in the $l_q$-norm, with $q \in [1,2]$ for the Square-Root Lasso, and in the $l_2$ and sorted $l_1$ norms for the Square-Root Slope. Both estimators are adaptive to the unknown variance of the noise. The Square-Root Slope is also adaptive to the sparsity $s$ of the true parameter. Moreover, in both cases, the chosen tuning parameters are independent of the confidence level under which the rate is valid, and we obtain improved concentration properties as in [Bellec, Lecu\'e and Tsybakov, 2016] where the case of known variance is treated. Our results are non-asymptotic.|show two estim squar root lasso squar root slope achiev exact optim minimax predict rate log set spars high dimension linear regress sampl size dimens sparsiti paramet also prove optim estim error norm squar root lasso sort norm squar root slope estim adapt unknown varianc nois squar root slope also adapt sparsiti true paramet moreov case chosen tune paramet independ confid level rate valid obtain improv concentr properti bellec lecu tsybakov case known varianc treat result non asymptot|['Alexis Derumigny']|['math.ST', 'stat.TH', '62G08 (Primary), 62C20, 62G05 (Secondary)']
2017-03-28T14:04:50Z|2017-03-08T13:47:17Z|http://arxiv.org/abs/1703.02834v1|http://arxiv.org/pdf/1703.02834v1|Exact Dimensionality Selection for Bayesian PCA|exact dimension select bayesian pca|We present a Bayesian model selection approach to estimate the intrinsic dimensionality of a high-dimensional dataset. To this end, we introduce a novel formulation of the probabilisitic principal component analysis model based on a normal-gamma prior distribution. In this context, we exhibit a closed-form expression of the marginal likelihood which allows to infer an optimal number of components. We also propose a heuristic based on the expected shape of the marginal likelihood curve in order to choose the hyperparameters. In non-asymptotic frameworks, we show on simulated data that this exact dimensionality selection approach is competitive with both Bayesian and frequentist state-of-the-art methods.|present bayesian model select approach estim intrins dimension high dimension dataset end introduc novel formul probabilisit princip compon analysi model base normal gamma prior distribut context exhibit close form express margin likelihood allow infer optim number compon also propos heurist base expect shape margin likelihood curv order choos hyperparamet non asymptot framework show simul data exact dimension select approach competit bayesian frequentist state art method|['Charles Bouveyron', 'Pierre Latouche', 'Pierre-Alexandre Mattei']|['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']
2017-03-28T14:04:50Z|2017-03-08T07:41:13Z|http://arxiv.org/abs/1703.02736v1|http://arxiv.org/pdf/1703.02736v1|Profile Estimation for Partial Functional Partially Linear Single-Index   Model|profil estim partial function partial linear singl index model|This paper studies a \textit{partial functional partially linear single-index model} that consists of a functional linear component as well as a linear single-index component. This model generalizes many well-known existing models and is suitable for more complicated data structures. However, its estimation inherits the difficulties and complexities from both components and makes it a challenging problem, which calls for new methodology. We propose a novel profile B-spline method to estimate the parameters by approximating the unknown nonparametric link function in the single-index component part with B-spline, while the linear slope function in the functional component part is estimated by the functional principal component basis. The consistency and asymptotic normality of the parametric estimators are derived, and the global convergence of the proposed estimator of the linear slope function is also established. More excitingly, the latter convergence is optimal in the minimax sense. A two-stage procedure is implemented to estimate the nonparametric link function, and the resulting estimator possesses the optimal global rate of convergence. Furthermore, the convergence rate of the mean squared prediction error for a predictor is also obtained. Empirical properties of the proposed procedures are studied through Monte Carlo simulations. A real data example is also analyzed to illustrate the power and flexibility of the proposed methodology.|paper studi textit partial function partial linear singl index model consist function linear compon well linear singl index compon model general mani well known exist model suitabl complic data structur howev estim inherit difficulti complex compon make challeng problem call new methodolog propos novel profil spline method estim paramet approxim unknown nonparametr link function singl index compon part spline linear slope function function compon part estim function princip compon basi consist asymptot normal parametr estim deriv global converg propos estim linear slope function also establish excit latter converg optim minimax sens two stage procedur implement estim nonparametr link function result estim possess optim global rate converg furthermor converg rate mean squar predict error predictor also obtain empir properti propos procedur studi mont carlo simul real data exampl also analyz illustr power flexibl propos methodolog|['Qingguo Tang', 'Linglong Kong', 'David Ruppert', 'Rohana J. Karunamuni']|['math.ST', 'stat.ME', 'stat.TH']
2017-03-28T14:04:50Z|2017-03-08T06:22:56Z|http://arxiv.org/abs/1703.02724v1|http://arxiv.org/pdf/1703.02724v1|Guaranteed Tensor PCA with Optimality in Statistics and Computation|guarante tensor pca optim statist comput|Tensors, or high-order arrays, attract much attention in recent research. In this paper, we propose a general framework for tensor principal component analysis (tensor PCA), which focuses on the methodology and theory for extracting the hidden low-rank structure from the high-dimensional tensor data. A unified solution is provided for tensor PCA with considerations in both statistical limits and computational costs. The problem exhibits three different phases according to the signal-noise-ratio (SNR). In particular, with strong SNR, we propose a fast spectral power iteration method that achieves the minimax optimal rate of convergence in estimation; with weak SNR, the information-theoretical lower bound shows that it is impossible to have consistent estimation in general; with moderate SNR, we show that the non-convex maximum likelihood estimation provides optimal solution, but with NP-hard computational cost; moreover, under the hardness hypothesis of hypergraphic planted clique detection, there are no polynomial-time algorithms performing consistently in general. Simulation studies show that the proposed spectral power iteration method have good performance under a variety of settings.|tensor high order array attract much attent recent research paper propos general framework tensor princip compon analysi tensor pca focus methodolog theori extract hidden low rank structur high dimension tensor data unifi solut provid tensor pca consider statist limit comput cost problem exhibit three differ phase accord signal nois ratio snr particular strong snr propos fast spectral power iter method achiev minimax optim rate converg estim weak snr inform theoret lower bound show imposs consist estim general moder snr show non convex maximum likelihood estim provid optim solut np hard comput cost moreov hard hypothesi hypergraph plant cliqu detect polynomi time algorithm perform consist general simul studi show propos spectral power iter method good perform varieti set|['Anru Zhang', 'Dong Xia']|['math.ST', 'cs.LG', 'stat.ME', 'stat.ML', 'stat.TH']
2017-03-28T14:04:50Z|2017-03-08T06:16:28Z|http://arxiv.org/abs/1703.02720v1|http://arxiv.org/pdf/1703.02720v1|Model Selection for Explosive Models|model select explos model|This paper examines the limit properties of information criteria (such as AIC, BIC, HQIC) for distinguishing between the unit root model and the various kinds of explosive models. The explosive models include the local-to-unit-root model, the mildly explosive model and the regular explosive model. Initial conditions with different order of magnitude are considered. Both the OLS estimator and the indirect inference estimator are studied. It is found that BIC and HQIC, but not AIC, consistently select the unit root model when data come from the unit root model. When data come from the local-to-unit-root model, both BIC and HQIC select the wrong model with probability approaching 1 while AIC has a positive probability of selecting the right model in the limit. When data come from the regular explosive model or from the mildly explosive model in the form of $1+n^{\alpha }/n$ with $\alpha \in (0,1)$, all three information criteria consistently select the true model. Indirect inference estimation can increase or decrease the probability for information criteria to select the right model asymptotically relative to OLS, depending on the information criteria and the true model. Simulation results confirm our asymptotic results in finite sample.|paper examin limit properti inform criteria aic bic hqic distinguish unit root model various kind explos model explos model includ local unit root model mild explos model regular explos model initi condit differ order magnitud consid ol estim indirect infer estim studi found bic hqic aic consist select unit root model data come unit root model data come local unit root model bic hqic select wrong model probabl approach aic posit probabl select right model limit data come regular explos model mild explos model form alpha alpha three inform criteria consist select true model indirect infer estim increas decreas probabl inform criteria select right model asymptot relat ol depend inform criteria true model simul result confirm asymptot result finit sampl|['Yubo Tao', 'Jun Yu']|['math.ST', 'stat.TH', 'G.3']
2017-03-28T14:04:50Z|2017-03-08T03:07:37Z|http://arxiv.org/abs/1703.02679v1|http://arxiv.org/pdf/1703.02679v1|Performance Bounds for Graphical Record Linkage|perform bound graphic record linkag|Record linkage involves merging records in large, noisy databases to remove duplicate entities. It has become an important area because of its widespread occurrence in bibliometrics, public health, official statistics production, political science, and beyond. Traditional linkage methods directly linking records to one another are computationally infeasible as the number of records grows. As a result, it is increasingly common for researchers to treat record linkage as a clustering task, in which each latent entity is associated with one or more noisy database records. We critically assess performance bounds using the Kullback-Leibler (KL) divergence under a Bayesian record linkage framework, making connections to Kolchin partition models. We provide an upper bound using the KL divergence and a lower bound on the minimum probability of misclassifying a latent entity. We give insights for when our bounds hold using simulated data and provide practical user guidance.|record linkag involv merg record larg noisi databas remov duplic entiti becom import area becaus widespread occurr bibliometr public health offici statist product polit scienc beyond tradit linkag method direct link record one anoth comput infeas number record grow result increas common research treat record linkag cluster task latent entiti associ one noisi databas record critic assess perform bound use kullback leibler kl diverg bayesian record linkag framework make connect kolchin partit model provid upper bound use kl diverg lower bound minimum probabl misclassifi latent entiti give insight bound hold use simul data provid practic user guidanc|['Rebecca C. Steorts', 'Matt Barnes', 'Willie Neiswanger']|['math.ST', 'cs.IT', 'math.IT', 'stat.ME', 'stat.ML', 'stat.TH']
2017-03-28T14:04:54Z|2017-03-07T22:18:35Z|http://arxiv.org/abs/1703.02625v1|http://arxiv.org/pdf/1703.02625v1|On Sampling from Massive Graph Streams|sampl massiv graph stream|We propose Graph Priority Sampling (GPS), a new paradigm for order-based reservoir sampling from massive streams of graph edges. GPS provides a general way to weight edge sampling according to auxiliary and/or size variables so as to accomplish various estimation goals of graph properties. In the context of subgraph counting, we show how edge sampling weights can be chosen so as to minimize the estimation variance of counts of specified sets of subgraphs. In distinction with many prior graph sampling schemes, GPS separates the functions of edge sampling and subgraph estimation. We propose two estimation frameworks: (1) Post-Stream estimation, to allow GPS to construct a reference sample of edges to support retrospective graph queries, and (2) In-Stream estimation, to allow GPS to obtain lower variance estimates by incrementally updating the subgraph count estimates during stream processing. Unbiasedness of subgraph estimators is established through a new Martingale formulation of graph stream order sampling, which shows that subgraph estimators, written as a product of constituent edge estimators are unbiased, even when computed at different points in the stream. The separation of estimation and sampling enables significant resource savings relative to previous work. We illustrate our framework with applications to triangle and wedge counting. We perform a large-scale experimental study on real-world graphs from various domains and types. GPS achieves high accuracy with less than 1% error for triangle and wedge counting, while storing a small fraction of the graph with average update times of a few microseconds per edge. Notably, for a large Twitter graph with more than 260M edges, GPS accurately estimates triangle counts with less than 1% error, while storing only 40K edges.|propos graph prioriti sampl gps new paradigm order base reservoir sampl massiv stream graph edg gps provid general way weight edg sampl accord auxiliari size variabl accomplish various estim goal graph properti context subgraph count show edg sampl weight chosen minim estim varianc count specifi set subgraph distinct mani prior graph sampl scheme gps separ function edg sampl subgraph estim propos two estim framework post stream estim allow gps construct refer sampl edg support retrospect graph queri stream estim allow gps obtain lower varianc estim increment updat subgraph count estim dure stream process unbiased subgraph estim establish new martingal formul graph stream order sampl show subgraph estim written product constitu edg estim unbias even comput differ point stream separ estim sampl enabl signific resourc save relat previous work illustr framework applic triangl wedg count perform larg scale experiment studi real world graph various domain type gps achiev high accuraci less error triangl wedg count store small fraction graph averag updat time microsecond per edg notabl larg twitter graph edg gps accur estim triangl count less error store onli edg|['Nesreen K. Ahmed', 'Nick Duffield', 'Theodore Willke', 'Ryan A. Rossi']|['cs.SI', 'cs.DS', 'cs.IR', 'math.ST', 'stat.TH']
2017-03-28T14:04:54Z|2017-03-07T16:38:11Z|http://arxiv.org/abs/1703.02462v1|http://arxiv.org/pdf/1703.02462v1|Convex and non-convex regularization methods for spatial point processes   intensity estimation|convex non convex regular method spatial point process intens estim|This paper deals with feature selection procedures for spatial point processes intensity estimation. We consider regularized versions of estimating equations based on Campbell theorem derived from two classical functions: Poisson likelihood and logistic regression likelihood. We provide general conditions on the spatial point processes and on penalty functions which ensure consistency, sparsity and asymptotic normality. We discuss the numerical implementation and assess finite sample properties in a simulation study. Finally, an application to tropical forestry datasets illustrates the use of the proposed methods.|paper deal featur select procedur spatial point process intens estim consid regular version estim equat base campbel theorem deriv two classic function poisson likelihood logist regress likelihood provid general condit spatial point process penalti function ensur consist sparsiti asymptot normal discuss numer implement assess finit sampl properti simul studi final applic tropic forestri dataset illustr use propos method|['Achmad Choiruddin', 'Jean-François Coeurjolly', 'Frédérique Letué']|['stat.ME', 'math.ST', 'stat.TH']
2017-03-28T14:04:54Z|2017-03-07T13:44:56Z|http://arxiv.org/abs/1703.02376v1|http://arxiv.org/pdf/1703.02376v1|On conditional least squares estimation for affine diffusions based on   continuous time observations|condit least squar estim affin diffus base continu time observ|We study asymptotic properties of conditional least squares estimators for the drift parameters of two-factor affine diffusions based on continuous time observations. We distinguish three cases: subcritical, critical and supercritical. For all the drift parameters, in the subcritical and supercritical cases, asymptotic normality and asymptotic mixed normality is proved, while in the critical case, non-standard asymptotic behavior is described.|studi asymptot properti condit least squar estim drift paramet two factor affin diffus base continu time observ distinguish three case subcrit critic supercrit drift paramet subcrit supercrit case asymptot normal asymptot mix normal prove critic case non standard asymptot behavior describ|['Beáta Bolyog', 'Gyula Pap']|['math.PR', 'math.ST', 'stat.TH', '60J60, 62F12']
2017-03-28T14:04:54Z|2017-03-13T15:44:35Z|http://arxiv.org/abs/1703.02307v2|http://arxiv.org/pdf/1703.02307v2|Post hoc inference via joint family-wise error rate control|post hoc infer via joint famili wise error rate control|"We introduce a general methodology for post hoc inference in a large-scale multiple testing framework. The approach is called "" user-agnostic "" in the sense that the statistical guarantee on the number of correct rejections holds for any set of candidate items selected by the user (after having seen the data). This task is investigated by defining a suitable criterion, named the joint-family-wise-error rate (JER for short). We propose several procedures for controlling the JER, with a special focus on incorporating dependencies while adapting to the unknown quantity of signal (via a step-down approach). We show that our proposed setting incorporates as particular cases a version of the higher criticism as well as the closed testing based approach of Goeman and Solari (2011). Our theoretical statements are supported by numerical experiments."|introduc general methodolog post hoc infer larg scale multipl test framework approach call user agnost sens statist guarante number correct reject hold ani set candid item select user seen data task investig defin suitabl criterion name joint famili wise error rate jer short propos sever procedur control jer special focus incorpor depend adapt unknown quantiti signal via step approach show propos set incorpor particular case version higher critic well close test base approach goeman solari theoret statement support numer experi|['Gilles Blanchard', 'Pierre Neuvial', 'Etienne Roquain']|['math.ST', 'stat.TH']
2017-03-28T14:04:54Z|2017-03-07T07:44:52Z|http://arxiv.org/abs/1703.02251v1|http://arxiv.org/pdf/1703.02251v1|The Maximum Likelihood Degree of Toric Varieties|maximum likelihood degre toric varieti|We study the maximum likelihood degree (ML degree) of toric varieties, known as discrete exponential models in statistics. By introducing scaling coefficients to the monomial parameterization of the toric variety, one can change the ML degree. We show that the ML degree is equal to the degree of the toric variety for generic scalings, while it drops if and only if the scaling vector is in the locus of the principal $A$-determinant. We also illustrate how to compute the ML estimate of a toric variety numerically via homotopy continuation from a scaled toric variety with low ML degree. Throughout, we include examples motivated by algebraic geometry and statistics. We compute the ML degree of rational normal scrolls and a large class of Veronese-type varieties. In addition, we investigate the ML degree of scaled Segre varieties, hierarchical loglinear models, and graphical models.|studi maximum likelihood degre ml degre toric varieti known discret exponenti model statist introduc scale coeffici monomi parameter toric varieti one chang ml degre show ml degre equal degre toric varieti generic scale drop onli scale vector locus princip determin also illustr comput ml estim toric varieti numer via homotopi continu scale toric varieti low ml degre throughout includ exampl motiv algebra geometri statist comput ml degre ration normal scroll larg class verones type varieti addit investig ml degre scale segr varieti hierarch loglinear model graphic model|['Carlos Améndola', 'Nathan Bliss', 'Isaac Burke', 'Courtney R. Gibbons', 'Martin Helmer', 'Serkan Hoşten', 'Evan D. Nash', 'Jose Israel Rodriguez', 'Daniel Smolkin']|['math.AG', 'math.ST', 'stat.CO', 'stat.TH', '14Q15, 14M25, 13P15, 62F10']
2017-03-28T14:04:54Z|2017-03-06T15:03:55Z|http://arxiv.org/abs/1703.01913v1|http://arxiv.org/pdf/1703.01913v1|Near-Optimal Closeness Testing of Discrete Histogram Distributions|near optim close test discret histogram distribut|We investigate the problem of testing the equivalence between two discrete histograms. A {\em $k$-histogram} over $[n]$ is a probability distribution that is piecewise constant over some set of $k$ intervals over $[n]$. Histograms have been extensively studied in computer science and statistics. Given a set of samples from two $k$-histogram distributions $p, q$ over $[n]$, we want to distinguish (with high probability) between the cases that $p = q$ and $\ p-q\ _1 \geq \epsilon$. The main contribution of this paper is a new algorithm for this testing problem and a nearly matching information-theoretic lower bound. Specifically, the sample complexity of our algorithm matches our lower bound up to a logarithmic factor, improving on previous work by polynomial factors in the relevant parameters. Our algorithmic approach applies in a more general setting and yields improved sample upper bounds for testing closeness of other structured distributions as well.|investig problem test equival two discret histogram em histogram probabl distribut piecewis constant set interv histogram extens studi comput scienc statist given set sampl two histogram distribut want distinguish high probabl case geq epsilon main contribut paper new algorithm test problem near match inform theoret lower bound specif sampl complex algorithm match lower bound logarithm factor improv previous work polynomi factor relev paramet algorithm approach appli general set yield improv sampl upper bound test close structur distribut well|['Ilias Diakonikolas', 'Daniel M. Kane', 'Vladimir Nikishkin']|['cs.DS', 'cs.IT', 'cs.LG', 'math.IT', 'math.ST', 'stat.TH']
2017-03-28T14:04:54Z|2017-03-06T09:25:09Z|http://arxiv.org/abs/1703.01777v1|http://arxiv.org/pdf/1703.01777v1|D-optimal design for multivariate polynomial regression via the   Christoffel function and semidefinite relaxations|optim design multivari polynomi regress via christoffel function semidefinit relax|We present a new approach to the design of D-optimal experiments with multivariate polynomial regressions on compact semi-algebraic design spaces. We apply the moment-sum-of-squares hierarchy of semidefinite programming problems to solve numerically and approximately the optimal design problem. The geometry of the design is recovered with semidefinite programming duality theory and the Christoffel polynomial.|present new approach design optim experi multivari polynomi regress compact semi algebra design space appli moment sum squar hierarchi semidefinit program problem solv numer approxim optim design problem geometri design recov semidefinit program dualiti theori christoffel polynomi|['Yohann De Castro', 'F Gamboa', 'D Henrion', 'R Hess', 'J. -B Lasserre']|['math.ST', 'math.OC', 'stat.TH']
2017-03-28T14:04:54Z|2017-03-06T04:31:36Z|http://arxiv.org/abs/1703.01721v1|http://arxiv.org/pdf/1703.01721v1|The Bennett-Orlicz norm|bennett orlicz norm|Lederer and van de Geer (2013) introduced a new Orlicz norm, the Bernstein-Orlicz norm, which is connected to Bernstein type inequalities. Here we introduce another Orlicz norm, the Bennett-Orlicz norm, which is connected to Bennett type inequalities. The new Bennett-Orlicz norm yields inequalities for expectations of maxima which are potentially somewhat tighter than those resulting from the Bernstein-Orlicz norm when they are both applicable. We discuss cross connections between these norms, exponential inequalities of the Bernstein, Bennett, and Prokhorov types, and make comparisons with results of Talagrand (1989, 1994), and Boucheron, Lugosi, and Massart (2013).|leder van de geer introduc new orlicz norm bernstein orlicz norm connect bernstein type inequ introduc anoth orlicz norm bennett orlicz norm connect bennett type inequ new bennett orlicz norm yield inequ expect maxima potenti somewhat tighter result bernstein orlicz norm applic discuss cross connect norm exponenti inequ bernstein bennett prokhorov type make comparison result talagrand boucheron lugosi massart|['Jon A. Wellner']|['math.ST', 'stat.TH', '60E15, 62E17, 62H10, 46E30']
2017-03-28T14:04:54Z|2017-03-05T20:09:44Z|http://arxiv.org/abs/1703.01658v1|http://arxiv.org/pdf/1703.01658v1|The wrapping hull and a unified framework for estimating the volume of a   body|wrap hull unifi framework estim volum bodi|This paper develops a unified framework for estimating the volume of a set in $\mathbb{R}^d$ based on observations of points uniformly distributed over the set. The framework applies to all classes of sets satisfying one simple axiom: a class is assumed to be intersection stable. No further hypotheses on the boundary of the set are imposed; in particular, the convex sets and the so-called weakly-convex sets are covered by the framework. The approach rests upon a homogeneous Poisson point process model. We introduce the so-called wrapping hull, a generalization of the convex hull, and prove that it is a sufficient and complete statistic. The proposed estimator of the volume is simply the volume of the wrapping hull scaled with an appropriate factor. It is shown to be consistent for all classes of sets satisfying the axiom and mimics an unbiased estimator with uniformly minimal variance. The construction and proofs hinge upon an interplay between probabilistic and geometric arguments. The tractability of the framework is numerically confirmed in a variety of examples.|paper develop unifi framework estim volum set mathbb base observ point uniform distribut set framework appli class set satisfi one simpl axiom class assum intersect stabl hypothes boundari set impos particular convex set call weak convex set cover framework approach rest upon homogen poisson point process model introduc call wrap hull general convex hull prove suffici complet statist propos estim volum simpli volum wrap hull scale appropri factor shown consist class set satisfi axiom mimic unbias estim uniform minim varianc construct proof hing upon interplay probabilist geometr argument tractabl framework numer confirm varieti exampl|['Nicolai Baldin']|['math.ST', 'stat.TH', '60G55, 62G05, 62M30']
2017-03-28T14:04:54Z|2017-03-05T19:37:52Z|http://arxiv.org/abs/1703.01654v1|http://arxiv.org/pdf/1703.01654v1|"À propos des tentatives visant à construire un estimateur   ""universel"" à partir de données indépendantes"|propo des tentat visant construir un estimateur universel partir de donn es ind pendant|This paper is based on our personal notes for the short course we gave on January 5, 2017 at Institut Henri Poincar\'e, after an invitation of the SFdS. Our purpose is to give an overview of the method of $\rho$-estimation and of the optimality and robustness properties of the estimators built according to this procedure. This method can be viewed as the sequel of a long series of researches which were devoted to the construction of estimators with good properties in various statistical frameworks. We shall emphasize the connection between the $\rho$-estimators and the previous ones, in particular the maximum likelihood estimator, and we shall show, via some typical examples, that the $\rho$-estimators perform better from various points of view.   ------   Cet article est fond\'e sur les notes du mini-cours que nous avons donn\'e le 5 janvier 2017 \`a l'Institut Henri Poincar\'e \`a l'occasion d'une journ\'ee organis\'ee par la SFdS et consacr\'ee \`a la Statistique Math\'ematique. Il vise \`a donner un aper\c{c}u de la m\'ethode de $\rho$-estimation ainsi que des propri\'et\'es d'optimalit\'e et de robustesse des estimateurs construits selon cette proc\'edure. Cette m\'ethode s'inscrit dans une longue lign\'ee de recherches dont l'objectif a \'et\'e de produire des estimateurs poss\'edant de bonnes propri\'et\'es pour un ensemble de cadres statistiques aussi vaste que possible. Nous mettrons en lumi\`ere les liens forts qui existent entre les $\rho$-estimateurs et ces pr\'ed\'ecesseurs, notamment les estimateurs du maximum de vraisemblance, mais montrerons \'egalement, au travers d'exemples choisis, que les $\rho$-estimateurs les surpassent sur bien des aspects.|paper base person note short cours gave januari institut henri poincar invit sfds purpos give overview method rho estim optim robust properti estim built accord procedur method view sequel long seri research devot construct estim good properti various statist framework shall emphas connect rho estim previous one particular maximum likelihood estim shall show via typic exampl rho estim perform better various point view cet articl est fond sur les note du mini cour que nous avon donn le janvier institut henri poincar occas une journ ee organi ee par la sfds et consacr ee la statistiqu math ematiqu il vise donner un aper de la ethod de rho estim ainsi que des propri et es optimalit et de robustess des estimateur construit selon cett proc edur cett ethod inscrit dan une longu lign ee de recherch dont objectif et de produir des estimateur poss edant de bonn propri et es pour un ensembl de cadr statistiqu aussi vast que possibl nous mettron en lumi ere les lien fort qui exist entr les rho estimateur et ces pr ed ecesseur notam les estimateur du maximum de vraisembl mai montreron egal au traver exempl choisi que les rho estimateur les surpass sur bien des aspect|['Yannick Baraud', 'Lucien Birgé']|['math.ST', 'stat.TH', '62G05']
2017-03-28T14:04:58Z|2017-03-04T22:15:27Z|http://arxiv.org/abs/1703.01527v1|http://arxiv.org/pdf/1703.01527v1|Power Allocation for Full-Duplex Relay Selection in Underlay Cognitive   Radio Networks: Coherent versus Non-Coherent Scenarios|power alloc full duplex relay select underlay cognit radio network coher versus non coher scenario|This paper investigates power control and relay selection in Full Duplex Cognitive Relay Networks (FDCRNs), where the secondary-user (SU) relays can simultaneously receive data from the SU source and forward them to the SU destination. We study both non-coherent and coherent scenarios. In the non-coherent case, the SU relay forwards the signal from the SU source without regulating the phase; while in the coherent scenario, the SU relay regulates the phase when forwarding the signal to minimize the interference at the primary-user (PU) receiver. We consider the problem of maximizing the transmission rate from the SU source to the SU destination subject to the interference constraint at the PU receiver and power constraints at both the SU source and SU relay. We then develop a mathematical model to analyze the data rate performance of the FDCRN considering the self-interference effects at the FD relay. We develop low-complexity and high-performance joint power control and relay selection algorithms. Extensive numerical results are presented to illustrate the impacts of power level parameters and the self-interference cancellation quality on the rate performance. Moreover, we demonstrate the significant gain of phase regulation at the SU relay.|paper investig power control relay select full duplex cognit relay network fdcrns secondari user su relay simultan receiv data su sourc forward su destin studi non coher coher scenario non coher case su relay forward signal su sourc without regul phase coher scenario su relay regul phase forward signal minim interfer primari user pu receiv consid problem maxim transmiss rate su sourc su destin subject interfer constraint pu receiv power constraint su sourc su relay develop mathemat model analyz data rate perform fdcrn consid self interfer effect fd relay develop low complex high perform joint power control relay select algorithm extens numer result present illustr impact power level paramet self interfer cancel qualiti rate perform moreov demonstr signific gain phase regul su relay|['Le Thanh Tan', 'Lei Ying', 'Daniel W. Bliss']|['cs.IT', 'cs.NI', 'math.IT', 'math.ST', 'stat.TH']
2017-03-28T14:04:58Z|2017-03-04T21:48:41Z|http://arxiv.org/abs/1703.01525v1|http://arxiv.org/pdf/1703.01525v1|Power Control and Relay Selection in Full-Duplex Cognitive Relay   Networks: Coherent versus Non-coherent Scenarios|power control relay select full duplex cognit relay network coher versus non coher scenario|This paper investigates power control and relay selection in Full Duplex Cognitive Relay Networks (FDCRNs), where the secondary-user (SU) relays can simultaneously receive and forward the signal from the SU source. We study both non-coherent and coherent scenarios. In the non-coherent case, the SU relay forwards the signal from the SU source without regulating the phase, while in the coherent scenario, the SU relay regulates the phase when forwarding the signal to minimize the interference at the primary-user (PU) receiver. We consider the problem of maximizing the transmission rate from the SU source to the SU destination subject to the interference constraint at the PU receiver and power constraints at both the SU source and SU relay. We develop low-complexity and high-performance joint power control and relay selection algorithms. The superior performance of the proposed algorithms are confirmed using extensive numerical evaluation. In particular, we demonstrate the significant gain of phase regulation at the SU relay (i.e., the gain of the coherent mechanism over the noncoherent mechanism).|paper investig power control relay select full duplex cognit relay network fdcrns secondari user su relay simultan receiv forward signal su sourc studi non coher coher scenario non coher case su relay forward signal su sourc without regul phase coher scenario su relay regul phase forward signal minim interfer primari user pu receiv consid problem maxim transmiss rate su sourc su destin subject interfer constraint pu receiv power constraint su sourc su relay develop low complex high perform joint power control relay select algorithm superior perform propos algorithm confirm use extens numer evalu particular demonstr signific gain phase regul su relay gain coher mechan noncoher mechan|['Le Thanh Tan', 'Lei Ying', 'Daniel W. Bliss']|['cs.IT', 'cs.NI', 'math.IT', 'math.ST', 'stat.TH']
2017-03-28T14:04:58Z|2017-03-04T15:13:41Z|http://arxiv.org/abs/1703.01474v1|http://arxiv.org/pdf/1703.01474v1|Sharp bounds for population recovery|sharp bound popul recoveri|The population recovery problem is a basic problem in noisy unsupervised learning that has attracted significant research attention in recent years [WY12,DRWY12, MS13, BIMP13, LZ15,DST16]. A number of different variants of this problem have been studied, often under assumptions on the unknown distribution (such as that it has restricted support size). In this work we study the sample complexity and algorithmic complexity of the most general version of the problem, under both bit-flip noise and erasure noise model. We give essentially matching upper and lower sample complexity bounds for both noise models, and efficient algorithms matching these sample complexity bounds up to polynomial factors.|popul recoveri problem basic problem noisi unsupervis learn attract signific research attent recent year wy drwi ms bimp lz dst number differ variant problem studi often assumpt unknown distribut restrict support size work studi sampl complex algorithm complex general version problem bit flip nois erasur nois model give essenti match upper lower sampl complex bound nois model effici algorithm match sampl complex bound polynomi factor|"['Anindya De', ""Ryan O'Donnell"", 'Rocco Servedio']"|['cs.DS', 'cs.LG', 'math.ST', 'stat.TH']
2017-03-28T14:04:58Z|2017-03-04T09:12:42Z|http://arxiv.org/abs/1703.01421v1|http://arxiv.org/pdf/1703.01421v1|$l_0$-estimation of piecewise-constant signals on graphs|estim piecewis constant signal graph|We study recovery of piecewise-constant signals over arbitrary graphs by the estimator minimizing an $l_0$-edge-penalized objective. Although exact minimization of this objective may be computationally intractable, we show that the same statistical risk guarantees are achieved by the alpha-expansion algorithm which approximately minimizes this objective in polynomial time. We establish that for graphs with small average vertex degree, these guarantees are rate-optimal in a minimax sense over classes of edge-sparse signals. For application to spatially inhomogeneous graphs, we propose minimization of an edge-weighted variant of this objective where each edge is weighted by its effective resistance or another measure of its contribution to the graph's connectivity. We establish minimax optimality of the resulting estimators over corresponding edge-weighted sparsity classes. We show theoretically that these risk guarantees are not always achieved by the estimator minimizing the $l_1$/total-variation relaxation, and empirically that the $l_0$-based estimates are more accurate in high signal-to-noise settings.|studi recoveri piecewis constant signal arbitrari graph estim minim edg penal object although exact minim object may comput intract show statist risk guarante achiev alpha expans algorithm approxim minim object polynomi time establish graph small averag vertex degre guarante rate optim minimax sens class edg spars signal applic spatial inhomogen graph propos minim edg weight variant object edg weight effect resist anoth measur contribut graph connect establish minimax optim result estim correspond edg weight sparsiti class show theoret risk guarante alway achiev estim minim total variat relax empir base estim accur high signal nois set|['Zhou Fan', 'Leying Guan']|['stat.ME', 'math.ST', 'stat.CO', 'stat.TH']
2017-03-28T14:04:58Z|2017-03-04T00:16:54Z|http://arxiv.org/abs/1703.01364v1|http://arxiv.org/pdf/1703.01364v1|A Matrix Variate Skew-t Distribution|matrix variat skew distribut|Although there is ample work in the literature dealing with skewness in the multivariate setting, there is a relative paucity of work in the matrix variate paradigm. Such work is, for example, useful for modelling three-way data. A matrix variate skew-t distribution is derived based on a mean-variance matrix normal mixture. An expectation-conditional maximization algorithm is developed for parameter estimation. Simulated data are used for illustration.|although ampl work literatur deal skew multivari set relat pauciti work matrix variat paradigm work exampl use model three way data matrix variat skew distribut deriv base mean varianc matrix normal mixtur expect condit maxim algorithm develop paramet estim simul data use illustr|['Michael P. B. Gallaugher', 'Paul D. McNicholas']|['stat.ME', 'math.ST', 'stat.TH']
2017-03-28T14:04:58Z|2017-03-14T22:44:25Z|http://arxiv.org/abs/1703.01332v2|http://arxiv.org/pdf/1703.01332v2|Optimistic lower bounds for convex regularized least-squares|optimist lower bound convex regular least squar|Minimax lower bounds are pessimistic in nature: for any given estimator, minimax lower bounds yield the existence of a worst-case target vector $\beta^*_{worst}$ for which the prediction error of the given estimator is bounded from below. However, minimax lower bounds shed no light on the prediction error of the given estimator for target vectors different than $\beta^*_{worst}$. A characterization of the prediction error of any convex regularized least-squares is given. This characterization provide both a lower bound and an upper bound on the prediction error. This produces lower bounds that are applicable for any target vector and not only for a single, worst-case $\beta^*_{worst}$. Finally, these lower and upper bounds on the prediction error are applied to the Lasso is sparse linear regression. We obtain a lower bound involving the compatibility constant for any tuning parameter, matching upper and lower bounds for the universal choice of the tuning parameter, and a lower bound for the Lasso with small tuning parameter.|minimax lower bound pessimist natur ani given estim minimax lower bound yield exist worst case target vector beta worst predict error given estim bound howev minimax lower bound shed light predict error given estim target vector differ beta worst character predict error ani convex regular least squar given character provid lower bound upper bound predict error produc lower bound applic ani target vector onli singl worst case beta worst final lower upper bound predict error appli lasso spars linear regress obtain lower bound involv compat constant ani tune paramet match upper lower bound univers choic tune paramet lower bound lasso small tune paramet|['Pierre C Bellec']|['math.ST', 'stat.TH']
2017-03-28T14:04:58Z|2017-03-03T20:14:59Z|http://arxiv.org/abs/1703.01326v1|http://arxiv.org/pdf/1703.01326v1|Prediction based on the Kennedy-O'Hagan calibration model: asymptotic   consistency and other properties|predict base kennedi hagan calibr model asymptot consist properti|Kennedy and O'Hagan (2001) propose a model for calibrating some unknown parameters in a computer model and estimating the discrepancy between the computer output and physical response. This model is known to have certain identifiability issues. Tuo and Wu (2016) show that there are examples for which the Kennedy-O'Hagan method renders unreasonable results in calibration. In spite of its unstable performance in calibration, the Kennedy-O'Hagan approach has a more robust behavior in predicting the physical response. In this work, we present some theoretical analysis to show the consistency of predictor based on their calibration model in the context of radial basis functions.|kennedi hagan propos model calibr unknown paramet comput model estim discrep comput output physic respons model known certain identifi issu tuo wu show exampl kennedi hagan method render unreason result calibr spite unstabl perform calibr kennedi hagan approach robust behavior predict physic respons work present theoret analysi show consist predictor base calibr model context radial basi function|['Rui Tuo', 'C. F. Jeff Wu']|['math.ST', 'stat.TH']
2017-03-28T14:04:58Z|2017-03-07T08:29:02Z|http://arxiv.org/abs/1703.01232v2|http://arxiv.org/pdf/1703.01232v2|Inconsistency of Template Estimation with the Fr{é}chet mean in   Quotient Space|inconsist templat estim fr chet mean quotient space|We tackle the problem of template estimation when data have been randomly transformed under an isometric group action in the presence of noise. In order to estimate the template, one often minimizes the variance when the influence of the transformations have been removed (computation of the Fr{\'e}chet mean in quotient space). The consistency bias is defined as the distance (possibly zero) between the orbit of the template and the orbit of one element which minimizes the variance. In this article we establish an asymptotic behavior of the consistency bias with respect to the noise level. This behavior is linear with respect to the noise level. As a result the inconsistency is unavoidable as soon as the noise is large enough. In practice, the template estimation with a finite sample is often done with an algorithm called max-max. We show the convergence of this algorithm to an empirical Karcher mean. Finally, our numerical experiments show that the bias observed in practice cannot be attributed to the small sample size or to a convergence problem but is indeed due to the previously studied inconsistency.|tackl problem templat estim data random transform isometr group action presenc nois order estim templat one often minim varianc influenc transform remov comput fr chet mean quotient space consist bias defin distanc possibl zero orbit templat orbit one element minim varianc articl establish asymptot behavior consist bias respect nois level behavior linear respect nois level result inconsist unavoid soon nois larg enough practic templat estim finit sampl often done algorithm call max max show converg algorithm empir karcher mean final numer experi show bias observ practic cannot attribut small sampl size converg problem inde due previous studi inconsist|['Loïc Devilliers', 'Xavier Pennec', 'Stéphanie Allassonnière']|['math.ST', 'stat.TH']
2017-03-28T14:04:58Z|2017-03-02T19:05:00Z|http://arxiv.org/abs/1703.00918v1|http://arxiv.org/pdf/1703.00918v1|A note on conditional covariance matrices for elliptical distributions|note condit covari matric ellipt distribut|In this short note we provide an analytical formula for the conditional covariance matrices of the elliptically distributed random vectors, when the conditioning is based on the values of any linear combination of the marginal random variables. We show that one could introduce the univariate invariant depending solely on the conditioning set, which greatly simplifies the calculations. As an application, we show that one could define uniquely defined quantile-based sets on which conditional covariance matrices must be equal to each other if only the vector is multivariate normal. The similar results are obtained for conditional correlation matrices of the general elliptic case.|short note provid analyt formula condit covari matric ellipt distribut random vector condit base valu ani linear combin margin random variabl show one could introduc univari invari depend sole condit set great simplifi calcul applic show one could defin uniqu defin quantil base set condit covari matric must equal onli vector multivari normal similar result obtain condit correl matric general ellipt case|['Piotr Jaworski', 'Marcin Pitera']|['math.PR', 'math.ST', 'q-fin.RM', 'stat.TH', '62H05, 60E05']
2017-03-28T14:04:58Z|2017-03-02T17:48:18Z|http://arxiv.org/abs/1703.00871v1|http://arxiv.org/pdf/1703.00871v1|Bootstrap confidence sets for spectral projectors of sample covariance|bootstrap confid set spectral projector sampl covari|Let $X_{1},\ldots,X_{n}$ be i.i.d. sample in $\mathbb{R}^{p}$ with zero mean and the covariance matrix $\mathbf{\Sigma}$. The problem of recovering the projector onto an eigenspace of $\mathbf{\Sigma}$ from these observations naturally arises in many applications. Recent technique from [Koltchinskii, Lounici, 2015] helps to study the asymptotic distribution of the distance in the Frobenius norm $\  \mathbf{P}_r - \widehat{\mathbf{P}}_r \ _{2}$ between the true projector $\mathbf{P}_r$ on the subspace of the $r$-th eigenvalue and its empirical counterpart $\widehat{\mathbf{P}}_r$ in terms of the effective rank of $\mathbf{\Sigma}$. This paper offers a bootstrap procedure for building sharp confidence sets for the true projector $\mathbf{P}_r$ from the given data. This procedure does not rely on the asymptotic distribution of $\  \mathbf{P}_r - \widehat{\mathbf{P}}_r \ _{2}$ and its moments. It could be applied for small or moderate sample size $n$ and large dimension $p$. The main result states the validity of the proposed procedure for finite samples with an explicit error bound for the error of bootstrap approximation. This bound involves some new sharp results on Gaussian comparison and Gaussian anti-concentration in high-dimensional spaces. Numeric results confirm a good performance of the method in realistic examples.|let ldot sampl mathbb zero mean covari matrix mathbf sigma problem recov projector onto eigenspac mathbf sigma observ natur aris mani applic recent techniqu koltchinskii lounici help studi asymptot distribut distanc frobenius norm mathbf widehat mathbf true projector mathbf subspac th eigenvalu empir counterpart widehat mathbf term effect rank mathbf sigma paper offer bootstrap procedur build sharp confid set true projector mathbf given data procedur doe reli asymptot distribut mathbf widehat mathbf moment could appli small moder sampl size larg dimens main result state valid propos procedur finit sampl explicit error bound error bootstrap approxim bound involv new sharp result gaussian comparison gaussian anti concentr high dimension space numer result confirm good perform method realist exampl|['Alexey Naumov', 'Vladimir Spokoiny', 'Vladimir Ulyanov']|['math.ST', 'math.PR', 'stat.TH']
2017-03-28T14:05:02Z|2017-03-02T07:31:13Z|http://arxiv.org/abs/1703.00647v1|http://arxiv.org/pdf/1703.00647v1|Inference for Multiple Change-points in Linear and Non-linear Time   Series Models|infer multipl chang point linear non linear time seri model|In this paper we develop a generalized likelihood ratio scan method (GLRSM) for multiple change-points inference in piecewise stationary time series, which estimates the number and positions of change-points and provides a confidence interval for each change-point. The computational complexity of using GLRSM for multiple change-points detection is as low as $O(n(\log n)^3)$ for a series of length $n$. Consistency of the estimated numbers and positions of the change-points is established. Extensive simulation studies are provided to demonstrate the effectiveness of the proposed methodology under different scenarios.|paper develop general likelihood ratio scan method glrsm multipl chang point infer piecewis stationari time seri estim number posit chang point provid confid interv chang point comput complex use glrsm multipl chang point detect low log seri length consist estim number posit chang point establish extens simul studi provid demonstr effect propos methodolog differ scenario|['Wai Leong Ng', 'Shenyi Pan', 'Chun Yip Yau']|['math.ST', 'stat.TH']
2017-03-28T14:05:02Z|2017-03-01T23:08:33Z|http://arxiv.org/abs/1703.00542v1|http://arxiv.org/pdf/1703.00542v1|A note on the approximate admissibility of regularized estimators in the   Gaussian sequence model|note approxim admiss regular estim gaussian sequenc model|"We study the problem of estimating an unknown vector $\theta$ from an observation $X$ drawn according to the normal distribution with mean $\theta$ and identity covariance matrix under the knowledge that $\theta$ belongs to a known closed convex set $\Theta$. In this general setting, Chatterjee (2014) proved that the natural constrained least squares estimator is ""approximately admissible"" for every $\Theta$. We extend this result by proving that the same property holds for all convex penalized estimators as well. Moreover, we simplify and shorten the original proof considerably. We also provide explicit upper and lower bounds for the universal constant underlying the notion of approximate admissibility."|studi problem estim unknown vector theta observ drawn accord normal distribut mean theta ident covari matrix knowledg theta belong known close convex set theta general set chatterje prove natur constrain least squar estim approxim admiss everi theta extend result prove properti hold convex penal estim well moreov simplifi shorten origin proof consider also provid explicit upper lower bound univers constant notion approxim admiss|['Xi Chen', 'Adityanand Guntuboyina', 'Yuchen Zhang']|['math.ST', 'stat.TH']
2017-03-28T14:05:02Z|2017-03-01T22:53:13Z|http://arxiv.org/abs/1703.00539v1|http://arxiv.org/pdf/1703.00539v1|Learning Determinantal Point Processes with Moments and Cycles|learn determinant point process moment cycl|Determinantal Point Processes (DPPs) are a family of probabilistic models that have a repulsive behavior, and lend themselves naturally to many tasks in machine learning where returning a diverse set of objects is important. While there are fast algorithms for sampling, marginalization and conditioning, much less is known about learning the parameters of a DPP. Our contribution is twofold: (i) we establish the optimal sample complexity achievable in this problem and show that it is governed by a natural parameter, which we call the \emph{cycle sparsity}; (ii) we propose a provably fast combinatorial algorithm that implements the method of moments efficiently and achieves optimal sample complexity. Finally, we give experimental results that confirm our theoretical findings.|determinant point process dpps famili probabilist model repuls behavior lend themselv natur mani task machin learn return divers set object import fast algorithm sampl margin condit much less known learn paramet dpp contribut twofold establish optim sampl complex achiev problem show govern natur paramet call emph cycl sparsiti ii propos provabl fast combinatori algorithm implement method moment effici achiev optim sampl complex final give experiment result confirm theoret find|['John Urschel', 'Victor-Emmanuel Brunel', 'Ankur Moitra', 'Philippe Rigollet']|['math.ST', 'stat.TH', '62M30, 60G55, 62C20, 05C38']
2017-03-28T14:05:02Z|2017-03-01T19:25:14Z|http://arxiv.org/abs/1703.00471v1|http://arxiv.org/pdf/1703.00471v1|Multidimensional Sampling of Isotropically Bandlimited Signals|multidimension sampl isotrop bandlimit signal|A new lower bound on the average reconstruction error variance of multidimensional sampling and reconstruction is presented. It applies to sampling on arbitrary lattices in arbitrary dimensions, assuming a stochastic process with constant, isotropically bandlimited spectrum and reconstruction by the best linear interpolator. The lower bound is exact for any lattice at sufficiently high and low sampling rates. The two threshold rates where the error variance deviates from the lower bound gives two optimality criteria for sampling lattices. It is proved that at low rates, near the first threshold, the optimal lattice is the dual of the best sphere-covering lattice, which for the first time establishes a rigorous relation between optimal sampling and optimal sphere covering. A previously known result is confirmed at high rates, near the second threshold, namely, that the optimal lattice is the dual of the best sphere-packing lattice. Numerical results quantify the performance of various lattices for sampling and support the theoretical optimality criteria.|new lower bound averag reconstruct error varianc multidimension sampl reconstruct present appli sampl arbitrari lattic arbitrari dimens assum stochast process constant isotrop bandlimit spectrum reconstruct best linear interpol lower bound exact ani lattic suffici high low sampl rate two threshold rate error varianc deviat lower bound give two optim criteria sampl lattic prove low rate near first threshold optim lattic dual best sphere cover lattic first time establish rigor relat optim sampl optim sphere cover previous known result confirm high rate near second threshold name optim lattic dual best sphere pack lattic numer result quantifi perform various lattic sampl support theoret optim criteria|['Erik Agrell', 'Balázs Csébfalvi']|['cs.IT', 'math.IT', 'math.ST', 'stat.TH']
2017-03-28T14:05:02Z|2017-03-01T19:08:00Z|http://arxiv.org/abs/1703.00469v1|http://arxiv.org/pdf/1703.00469v1|Confidence Bands for Coefficients in High Dimensional Linear Models with   Error-in-variables|confid band coeffici high dimension linear model error variabl|We study high-dimensional linear models with error-in-variables. Such models are motivated by various applications in econometrics, finance and genetics. These models are challenging because of the need to account for measurement errors to avoid non-vanishing biases in addition to handle the high dimensionality of the parameters. A recent growing literature has proposed various estimators that achieve good rates of convergence. Our main contribution complements this literature with the construction of simultaneous confidence regions for the parameters of interest in such high-dimensional linear models with error-in-variables.   These confidence regions are based on the construction of moment conditions that have an additional orthogonal property with respect to nuisance parameters. We provide a construction that requires us to estimate an additional high-dimensional linear model with error-in-variables for each component of interest. We use a multiplier bootstrap to compute critical values for simultaneous confidence intervals for a subset $S$ of the components. We show its validity despite of possible model selection mistakes, and allowing for the cardinality of $S$ to be larger than the sample size.   We apply and discuss the implications of our results to two examples and conduct Monte Carlo simulations to illustrate the performance of the proposed procedure.|studi high dimension linear model error variabl model motiv various applic econometr financ genet model challeng becaus need account measur error avoid non vanish bias addit handl high dimension paramet recent grow literatur propos various estim achiev good rate converg main contribut complement literatur construct simultan confid region paramet interest high dimension linear model error variabl confid region base construct moment condit addit orthogon properti respect nuisanc paramet provid construct requir us estim addit high dimension linear model error variabl compon interest use multipli bootstrap comput critic valu simultan confid interv subset compon show valid despit possibl model select mistak allow cardin larger sampl size appli discuss implic result two exampl conduct mont carlo simul illustr perform propos procedur|['Alexandre Belloni', 'Victor Chernozhukov', 'Abhishek Kaul']|['math.ST', 'stat.TH']
2017-03-28T14:05:02Z|2017-03-01T15:49:03Z|http://arxiv.org/abs/1703.00353v1|http://arxiv.org/pdf/1703.00353v1|Matrix product moments in normal variables|matrix product moment normal variabl|Let ${\cal X }=XX^{\prime}$ be a random matrix associated with a centered $r$-column centered Gaussian vector $X$ with a covariance matrix $P$. In this article we compute expectations of matrix-products of the form $\prod_{1\leq i\leq n}({\cal X } P^{v_i})$ for any $n\geq 1$ and any multi-index parameters $v_i\in\mathbb{N}$. We derive closed form formulae and a simple sequential algorithm to compute these matrices w.r.t. the parameter $n$. The second part of the article is dedicated to a non commutative binomial formula for the central matrix-moments $\mathbb{E}\left(\left[{\cal X }-P\right]^n\right)$. The matrix product moments discussed in this study are expressed in terms of polynomial formulae w.r.t. the powers of the covariance matrix, with coefficients depending on the trace of these matrices. We also derive a series of estimates w.r.t. the Loewner order on quadratic forms. For instance we shall prove the rather crude estimate $\mathbb{E}\left(\left[{\cal X }-P\right]^n\right)\leq \mathbb{E}\left({\cal X }^n-P^n\right)$, for any $n\geq 1$|let cal xx prime random matrix associ center column center gaussian vector covari matrix articl comput expect matrix product form prod leq leq cal ani geq ani multi index paramet mathbb deriv close form formula simpl sequenti algorithm comput matric paramet second part articl dedic non commut binomi formula central matrix moment mathbb left left cal right right matrix product moment discuss studi express term polynomi formula power covari matrix coeffici depend trace matric also deriv seri estim loewner order quadrat form instanc shall prove rather crude estim mathbb left left cal right right leq mathbb left cal right ani geq|['Pierre Del Moral', 'Adrian N. Bishop']|['math.ST', 'stat.TH', '15B52, 60B20, 46L53, 05A10']
2017-03-28T14:05:02Z|2017-03-01T15:03:04Z|http://arxiv.org/abs/1703.00329v1|http://arxiv.org/pdf/1703.00329v1|Convergence rate of a simulated annealing algorithm with noisy   observations|converg rate simul anneal algorithm noisi observ|In this paper we propose a modified version of the simulated annealing algorithm for solving a stochastic global optimization problem. More precisely, we address the problem of finding a global minimizer of a function with noisy evaluations. We provide a rate of convergence and its optimized parametrization to ensure a minimal number of evaluations for a given accuracy and a confidence level close to 1. This work is completed with a set of numerical experimentations and assesses the practical performance both on benchmark test cases and on real world examples.|paper propos modifi version simul anneal algorithm solv stochast global optim problem precis address problem find global minim function noisi evalu provid rate converg optim parametr ensur minim number evalu given accuraci confid level close work complet set numer experiment assess practic perform benchmark test case real world exampl|['Clément Bouttier', 'Ioana Gavra']|['stat.ML', 'math.OC', 'math.ST', 'stat.TH']
2017-03-28T14:05:02Z|2017-03-01T07:57:26Z|http://arxiv.org/abs/1703.00167v1|http://arxiv.org/pdf/1703.00167v1|Adaptive estimation of the sparsity in the Gaussian vector model|adapt estim sparsiti gaussian vector model|Consider the Gaussian vector model with mean value {\theta}. We study the twin problems of estimating the number  {\theta} _0 of non-zero components of {\theta} and testing whether  {\theta} _0 is smaller than some value. For testing, we establish the minimax separation distances for this model and introduce a minimax adaptive test. Extensions to the case of unknown variance are also discussed. Rewriting the estimation of  {\theta} _0 as a multiple testing problem of all hypotheses { {\theta} _0 <= q}, we both derive a new way of assessing the optimality of a sparsity estimator and we exhibit such an optimal procedure. This general approach provides a roadmap for estimating the complexity of the signal in various statistical models.|consid gaussian vector model mean valu theta studi twin problem estim number theta non zero compon theta test whether theta smaller valu test establish minimax separ distanc model introduc minimax adapt test extens case unknown varianc also discuss rewrit estim theta multipl test problem hypothes theta deriv new way assess optim sparsiti estim exhibit optim procedur general approach provid roadmap estim complex signal various statist model|['Alexandra Carpentier', 'Nicolas Verzelen']|['math.ST', 'stat.TH', '62C20, 62G10, 62B10']
2017-03-28T14:05:02Z|2017-03-06T15:19:31Z|http://arxiv.org/abs/1702.08900v2|http://arxiv.org/pdf/1702.08900v2|Asymptotic Exponentiality of the First Exit Time of the Shiryaev-Roberts   Diffusion with Constant Positive Drift|asymptot exponenti first exit time shiryaev robert diffus constant posit drift|We consider the first exit time of a Shiryaev-Roberts diffusion with constant positive drift from the interval $[0,A]$ where $A>0$. We show that the moment generating function (Laplace transform) of a suitably standardized version of the first exit time converges to that of the unit-mean exponential distribution as $A\to+\infty$. The proof is explicit in that the moment generating function of the first exit time is first expressed analytically and in a closed form, and then the desired limit as $A\to+\infty$ is evaluated directly. The result is of importance in the area of quickest change-point detection, and its discrete-time counterpart has been previously established - although in a different manner - by Pollak and Tartakovsky (2009).|consid first exit time shiryaev robert diffus constant posit drift interv show moment generat function laplac transform suitabl standard version first exit time converg unit mean exponenti distribut infti proof explicit moment generat function first exit time first express analyt close form desir limit infti evalu direct result import area quickest chang point detect discret time counterpart previous establish although differ manner pollak tartakovski|['Aleksey S. Polunchenko']|['stat.ME', 'math.ST', 'stat.TH', '62L10, 60G40, 60J60']
2017-03-28T14:05:02Z|2017-02-28T18:28:07Z|http://arxiv.org/abs/1702.08895v1|http://arxiv.org/pdf/1702.08895v1|Minimax density estimation for growing dimension|minimax densiti estim grow dimens|This paper presents minimax rates for density estimation when the data dimension $d$ is allowed to grow with the number of observations $n$ rather than remaining fixed as in previous analyses. We prove a non-asymptotic lower bound which gives the worst-case rate over standard classes of smooth densities, and we show that kernel density estimators achieve this rate. We also give oracle choices for the bandwidth and derive the fastest rate $d$ can grow with $n$ to maintain estimation consistency.|paper present minimax rate densiti estim data dimens allow grow number observ rather remain fix previous analys prove non asymptot lower bound give worst case rate standard class smooth densiti show kernel densiti estim achiev rate also give oracl choic bandwidth deriv fastest rate grow maintain estim consist|['Daniel J. McDonald']|['math.ST', 'stat.TH']
2017-03-28T14:05:06Z|2017-02-28T13:50:56Z|http://arxiv.org/abs/1702.08787v1|http://arxiv.org/pdf/1702.08787v1|Compound Poisson approximation to estimate the Lévy density|compound poisson approxim estim vy densiti|"We construct an estimator of the L\'evy density, with respect to the Lebesgue measure, of a pure jump L\'evy process from high frequency observations: we observe one trajectory of the L\'evy process over [0, T] at the sampling rate $\Delta$, where $\Delta$ $\rightarrow$ 0 as T $\rightarrow$ $\infty$. The main novelty of our result is that we directly estimate the L\'evy density in cases where the process may present infinite activity. Moreover, we study the risk of the estimator with respect to L\_p loss functions, 1 $\le$ p \textless{} $\infty$, whereas existing results only focus on p $\in$ {2, $\infty$}. The main idea behind the estimation procedure that we propose is to use that ""every infinitely divisible distribution is the limit of a sequence of compound Poisson distributions"" (see e.g. Corollary 8.8 in Sato (1999)) and to take advantage of the fact that it is well known how to estimate the L\'evy density of a compound Poisson process in the high frequency setting. We consider linear wavelet estimators and the performance of our procedure is studied in term of L\_p loss functions, p $\ge$ 1, over Besov balls. The results are illustrated on several examples."|construct estim evi densiti respect lebesgu measur pure jump evi process high frequenc observ observ one trajectori evi process sampl rate delta delta rightarrow rightarrow infti main novelti result direct estim evi densiti case process may present infinit activ moreov studi risk estim respect loss function le textless infti wherea exist result onli focus infti main idea behind estim procedur propos use everi infinit divis distribut limit sequenc compound poisson distribut see corollari sato take advantag fact well known estim evi densiti compound poisson process high frequenc set consid linear wavelet estim perform procedur studi term loss function ge besov ball result illustr sever exampl|['Céline Duval', 'Ester Mariucci']|['math.PR', 'math.ST', 'stat.TH']
2017-03-28T14:05:06Z|2017-02-28T02:54:56Z|http://arxiv.org/abs/1702.08615v1|http://arxiv.org/pdf/1702.08615v1|Bridging Finite and Super Population Causal Inference|bridg finit super popul causal infer|There are two general views in causal analysis of experimental data: the super population view that the units are an independent sample from some hypothetical infinite populations, and the finite population view that the potential outcomes of the experimental units are fixed and the randomness comes solely from the physical randomization of the treatment assignment. These two views differs conceptually and mathematically, resulting in different sampling variances of the usual difference-in-means estimator of the average causal effect. Practically, however, these two views result in identical variance estimators. By recalling a variance decomposition and exploiting a completeness-type argument, we establish a connection between these two views in completely randomized experiments. This alternative formulation could serve as a template for bridging finite and super population causal inference in other scenarios.|two general view causal analysi experiment data super popul view unit independ sampl hypothet infinit popul finit popul view potenti outcom experiment unit fix random come sole physic random treatment assign two view differ conceptu mathemat result differ sampl varianc usual differ mean estim averag causal effect practic howev two view result ident varianc estim recal varianc decomposit exploit complet type argument establish connect two view complet random experi altern formul could serv templat bridg finit super popul causal infer scenario|['Peng Ding', 'Xinran Li', 'Luke W. Miratrix']|['math.ST', 'stat.TH']
2017-03-28T14:05:06Z|2017-02-27T21:52:17Z|http://arxiv.org/abs/1702.08546v1|http://arxiv.org/pdf/1702.08546v1|Optimal rates of estimation for multi-reference alignment|optim rate estim multi refer align|This paper describes optimal rates of adaptive estimation of a vector in the multi-reference alignment model, a problem with important applications in fields such as signal processing, image processing, and computer vision, among others. We describe how this model can be viewed as a multivariate Gaussian mixture model under the constraint that the centers belong to the orbit of a group. This enables us to derive matching upper and lower bounds that feature an interesting dependence on the signal-to-noise ratio of the model. Both upper and lower bounds are articulated around a tight local control of Kullback-Leibler divergences that showcases the central role of moment tensors in this problem.|paper describ optim rate adapt estim vector multi refer align model problem import applic field signal process imag process comput vision among describ model view multivari gaussian mixtur model constraint center belong orbit group enabl us deriv match upper lower bound featur interest depend signal nois ratio model upper lower bound articul around tight local control kullback leibler diverg showcas central role moment tensor problem|['Afonso Bandeira', 'Philippe Rigollet', 'Jonathan Weed']|['math.ST', 'stat.TH']
2017-03-28T14:05:06Z|2017-02-27T18:38:28Z|http://arxiv.org/abs/1703.01237v1|http://arxiv.org/pdf/1703.01237v1|How real is the random censorship model in medical studies?|real random censorship model medic studi|In survival analysis the random censorship model refers to censoring and survival times being independent of each other. It is one of the fundamental assumptions in the theory of survival analysis. We explain the reason for it being so ubiquitous, and we investigate its presence in medical studies. We differentiate two types of censoring in medical studies (dropout and administrative), and we explain their importance in examining the existence of the random censorship model. We show that in order to presume the random censorship model it is not enough to have a design study which conforms to it, but that one needs to provide evidence for its presence in the results. Blindly presuming the random censorship model might lead to the Kaplan-Meier estimator producing biased results, which might have serious consequences when estimating survival in medical studies.|surviv analysi random censorship model refer censor surviv time independ one fundament assumpt theori surviv analysi explain reason ubiquit investig presenc medic studi differenti two type censor medic studi dropout administr explain import examin exist random censorship model show order presum random censorship model enough design studi conform one need provid evid presenc result blind presum random censorship model might lead kaplan meier estim produc bias result might serious consequ estim surviv medic studi|['Damjan Krstajic']|['stat.AP', 'math.ST', 'stat.TH']
2017-03-28T14:05:06Z|2017-02-27T10:01:36Z|http://arxiv.org/abs/1702.08211v1|http://arxiv.org/pdf/1702.08211v1|Online Nonparametric Learning, Chaining, and the Role of Partial   Feedback|onlin nonparametr learn chain role partial feedback|We investigate contextual online learning with nonparametric (Lipschitz) comparison classes under different assumptions on losses and feedback information. For full information feedback and Lipschitz losses, we characterize the minimax regret up to log factors by proving an upper bound matching a previously known lower bound. In a partial feedback model motivated by second-price auctions, we prove upper bounds for Lipschitz and semi-Lipschitz losses that improve on the known bounds for standard bandit feedback. Our analysis combines novel results for contextual second-price auctions with a novel algorithmic approach based on chaining. When the context space is Euclidean, our chaining approach is efficient and delivers an even better regret bound.|investig contextu onlin learn nonparametr lipschitz comparison class differ assumpt loss feedback inform full inform feedback lipschitz loss character minimax regret log factor prove upper bound match previous known lower bound partial feedback model motiv second price auction prove upper bound lipschitz semi lipschitz loss improv known bound standard bandit feedback analysi combin novel result contextu second price auction novel algorithm approach base chain context space euclidean chain approach effici deliv even better regret bound|['Nicolò Cesa-Bianchi', 'Pierre Gaillard', 'Claudio Gentile', 'Sébastien Gerchinovitz']|['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']
2017-03-28T14:05:06Z|2017-02-26T22:59:02Z|http://arxiv.org/abs/1702.08109v1|http://arxiv.org/pdf/1702.08109v1|Constrained Maximum Likelihood Estimators for Densities|constrain maximum likelihood estim densiti|We put forward a framework for nonparametric density estimation in situations where the sample is supplemented by information and assumptions about shape, support, continuity, slope, location of modes, density values, etc. These supplements are incorporated as constraints that in conjunction with a maximum likelihood criterion lead to constrained infinite-dimensional optimization problems that we formulate over spaces of semicontinuous functions. These spaces, when equipped with an appropriate metric, offer a series of advantages including simple conditions for existence of estimators and their limits and, in particular, guarantee the convergence of modes of densities. Relying on the approximation theory---epi-convergence---for optimization problems, we provide general conditions under which estimators subject to essentially arbitrary constraints are consistent and illustrate the framework with a number of examples that span classical and novel shape constraints.|put forward framework nonparametr densiti estim situat sampl supplement inform assumpt shape support continu slope locat mode densiti valu etc supplement incorpor constraint conjunct maximum likelihood criterion lead constrain infinit dimension optim problem formul space semicontinu function space equip appropri metric offer seri advantag includ simpl condit exist estim limit particular guarante converg mode densiti reli approxim theori epi converg optim problem provid general condit estim subject essenti arbitrari constraint consist illustr framework number exampl span classic novel shape constraint|['Johannes O. Royset', 'Roger J-B Wets']|['math.ST', 'stat.TH']
2017-03-28T14:05:06Z|2017-02-25T14:53:53Z|http://arxiv.org/abs/1702.07899v1|http://arxiv.org/pdf/1702.07899v1|Are there needles in a moving haystack? Adaptive sensing for detection   of dynamically evolving signals|needl move haystack adapt sens detect dynam evolv signal|In this paper we investigate the problem of detecting dynamically evolving signals. We model the signal as an $n$ dimensional vector that is either zero or has $s$ non-zero components. At each time step $t\in \mathbb{N}$ the non-zero components change their location independently with probability $p$. The statistical problem is to decide whether the signal is a zero vector or in fact it has non-zero components. This decision is based on $m$ noisy observations of individual signal components collected at times $t=1,\ldots,m$. We consider two different sensing paradigms, namely adaptive and non-adaptive sensing. For non-adaptive sensing the choice of components to measure has to be decided before the data collection process started, while for adaptive sensing one can adjust the sensing process based on observations collected earlier. We characterize the difficulty of this detection problem in both sensing paradigms in terms of the aforementioned parameters, with special interest to the speed of change of the active components. In addition we provide an adaptive sensing algorithm for this problem and contrast its performance to that of non-adaptive detection algorithms.|paper investig problem detect dynam evolv signal model signal dimension vector either zero non zero compon time step mathbb non zero compon chang locat independ probabl statist problem decid whether signal zero vector fact non zero compon decis base noisi observ individu signal compon collect time ldot consid two differ sens paradigm name adapt non adapt sens non adapt sens choic compon measur decid befor data collect process start adapt sens one adjust sens process base observ collect earlier character difficulti detect problem sens paradigm term aforement paramet special interest speed chang activ compon addit provid adapt sens algorithm problem contrast perform non adapt detect algorithm|['Rui M. Castro', 'Ervin Tánczos']|['math.ST', 'stat.TH']
2017-03-28T14:05:06Z|2017-02-24T23:43:06Z|http://arxiv.org/abs/1702.07803v1|http://arxiv.org/pdf/1702.07803v1|Nonparanormal Information Estimation|nonparanorm inform estim|We study the problem of using i.i.d. samples from an unknown multivariate probability distribution $p$ to estimate the mutual information of $p$. This problem has recently received attention in two settings: (1) where $p$ is assumed to be Gaussian and (2) where $p$ is assumed only to lie in a large nonparametric smoothness class. Estimators proposed for the Gaussian case converge in high dimensions when the Gaussian assumption holds, but are brittle, failing dramatically when $p$ is not Gaussian. Estimators proposed for the nonparametric case fail to converge with realistic sample sizes except in very low dimensions. As a result, there is a lack of robust mutual information estimators for many realistic data. To address this, we propose estimators for mutual information when $p$ is assumed to be a nonparanormal (a.k.a., Gaussian copula) model, a semiparametric compromise between Gaussian and nonparametric extremes. Using theoretical bounds and experiments, we show these estimators strike a practical balance between robustness and scaling with dimensionality.|studi problem use sampl unknown multivari probabl distribut estim mutual inform problem recent receiv attent two set assum gaussian assum onli lie larg nonparametr smooth class estim propos gaussian case converg high dimens gaussian assumpt hold brittl fail dramat gaussian estim propos nonparametr case fail converg realist sampl size except veri low dimens result lack robust mutual inform estim mani realist data address propos estim mutual inform assum nonparanorm gaussian copula model semiparametr compromis gaussian nonparametr extrem use theoret bound experi show estim strike practic balanc robust scale dimension|['Shashank Singh', 'Barnabás Pøczos']|['math.ST', 'cs.IT', 'math.IT', 'stat.ML', 'stat.TH']
2017-03-28T14:05:06Z|2017-02-24T23:31:04Z|http://arxiv.org/abs/1702.07801v1|http://arxiv.org/pdf/1702.07801v1|Consistent structure estimation of exponential-family random graph   models with additional structure|consist structur estim exponenti famili random graph model addit structur|We consider the challenging problem of statistical inference for exponential-family random graph models given one observation of a random graph with complex dependence (e.g., transitivity). To facilitate statistical inference, we endow random graphs with additional structure. The basic idea is that random graphs are composed of subgraphs with complex dependence. We have shown elsewhere that when the composition of random graphs is known, $M$-estimators of canonical and curved exponential families with complex dependence are consistent. In practice, the composition is known in some applications, but is unknown in others. If the composition is unknown, the first and foremost question is whether it can be recovered. The main consistency results of the paper show that it is possible to do so as long as exponential families satisfy weak dependence and smoothness conditions. These results confirm that exponential-family random graph models with additional structure constitute a promising direction of statistical network analysis.|consid challeng problem statist infer exponenti famili random graph model given one observ random graph complex depend transit facilit statist infer endow random graph addit structur basic idea random graph compos subgraph complex depend shown elsewher composit random graph known estim canon curv exponenti famili complex depend consist practic composit known applic unknown composit unknown first foremost question whether recov main consist result paper show possibl long exponenti famili satisfi weak depend smooth condit result confirm exponenti famili random graph model addit structur constitut promis direct statist network analysi|['Michael Schweinberger']|['math.ST', 'stat.TH']
2017-03-28T14:05:06Z|2017-03-01T00:17:04Z|http://arxiv.org/abs/1702.07795v2|http://arxiv.org/pdf/1702.07795v2|A Study of the Allan Variance for Constant-Mean Non-Stationary Processes|studi allan varianc constant mean non stationari process|The Allan Variance (AV) is a widely used quantity in areas focusing on error measurement as well as in the general analysis of variance for autocorrelated processes in domains such as engineering and, more specifically, metrology. The form of this quantity is widely used to detect noise patterns and indications of stability within signals. However, the properties of this quantity are not known for commonly occurring processes whose covariance structure is non-stationary and, in these cases, an erroneous interpretation of the AV could lead to misleading conclusions. This paper generalizes the theoretical form of the AV to some non-stationary processes while at the same time being valid also for weakly stationary processes. Some simulation examples show how this new form can help to understand the processes for which the AV is able to distinguish these from the stationary cases and hence allow for a better interpretation of this quantity in applied cases.|allan varianc av wide use quantiti area focus error measur well general analysi varianc autocorrel process domain engin specif metrolog form quantiti wide use detect nois pattern indic stabil within signal howev properti quantiti known common occur process whose covari structur non stationari case erron interpret av could lead mislead conclus paper general theoret form av non stationari process time valid also weak stationari process simul exampl show new form help understand process av abl distinguish stationari case henc allow better interpret quantiti appli case|['Haotian Xu', 'Stéphane Guerrier', 'Roberto Molinari', 'Yuming Zhang']|['math.ST', 'stat.TH']
2017-03-28T14:05:11Z|2017-02-24T02:09:04Z|http://arxiv.org/abs/1702.07448v1|http://arxiv.org/pdf/1702.07448v1|Optimal Bayesian Minimax Rates for Unconstrained Large Covariance   Matrices|optim bayesian minimax rate unconstrain larg covari matric|We obtain the optimal Bayesian minimax rate for the unconstrained large covariance matrix of multivariate normal sample with mean zero, when both the sample size, n, and the dimension, p, of the covariance matrix tend to infinity. Traditionally the posterior convergence rate is used to compare the frequentist asymptotic performance of priors, but defining the optimality with it is elusive. We propose a new decision theoretic framework for prior selection and define Bayesian minimax rate. Under the proposed framework, we obtain the optimal Bayesian minimax rate for the spectral norm for all rates of p. We also considered Frobenius norm, Bregman divergence and squared log-determinant loss and obtain the optimal Bayesian minimax rate under certain rate conditions on p. A simulation study is conducted to support the theoretical results.|obtain optim bayesian minimax rate unconstrain larg covari matrix multivari normal sampl mean zero sampl size dimens covari matrix tend infin tradit posterior converg rate use compar frequentist asymptot perform prior defin optim elus propos new decis theoret framework prior select defin bayesian minimax rate propos framework obtain optim bayesian minimax rate spectral norm rate also consid frobenius norm bregman diverg squar log determin loss obtain optim bayesian minimax rate certain rate condit simul studi conduct support theoret result|['Kyoungjae Lee', 'Jaeyong Lee']|['math.ST', 'stat.TH']
2017-03-28T14:05:11Z|2017-02-23T13:49:57Z|http://arxiv.org/abs/1702.07211v1|http://arxiv.org/pdf/1702.07211v1|A minimax and asymptotically optimal algorithm for stochastic bandits|minimax asymptot optim algorithm stochast bandit|We propose the kl-UCB ++ algorithm for regret minimization in stochastic bandit models with exponential families of distributions. We prove that it is simultaneously asymptotically optimal (in the sense of Lai and Robbins' lower bound) and minimax optimal. This is the first algorithm proved to enjoy these two properties at the same time. This work thus merges two different lines of research, with simple proofs involving no complexity overhead.|propos kl ucb algorithm regret minim stochast bandit model exponenti famili distribut prove simultan asymptot optim sens lai robbin lower bound minimax optim first algorithm prove enjoy two properti time work thus merg two differ line research simpl proof involv complex overhead|['Pierre Ménard', 'Aurélien Garivier']|['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']
2017-03-28T14:05:11Z|2017-02-23T07:22:32Z|http://arxiv.org/abs/1702.07118v1|http://arxiv.org/pdf/1702.07118v1|Warped metrics for location-scale models|warp metric locat scale model|This paper argues that a class of Riemannian metrics, called warped metrics, plays a fundamental role in statistical problems involving location-scale models. The paper reports three new results : i) the Rao-Fisher metric of any location-scale model is a warped metric, provided that this model satisfies a natural invariance condition, ii) the analytic expression of the sectional curvature of this metric, iii) the exact analytic solution of the geodesic equation of this metric. The paper applies these new results to several examples of interest, where it shows that warped metrics turn location-scale models into complete Riemannian manifolds of negative sectional curvature. This is a very suitable situation for developing algorithms which solve problems of classification and on-line estimation. Thus, by revealing the connection between warped metrics and location-scale models, the present paper paves the way to the introduction of new efficient statistical algorithms.|paper argu class riemannian metric call warp metric play fundament role statist problem involv locat scale model paper report three new result rao fisher metric ani locat scale model warp metric provid model satisfi natur invari condit ii analyt express section curvatur metric iii exact analyt solut geodes equat metric paper appli new result sever exampl interest show warp metric turn locat scale model complet riemannian manifold negat section curvatur veri suitabl situat develop algorithm solv problem classif line estim thus reveal connect warp metric locat scale model present paper pave way introduct new effici statist algorithm|['Salem Said', 'Yannick Berthoumieu']|['math.ST', 'math.DG', 'stat.TH']
2017-03-28T14:05:11Z|2017-02-23T03:31:22Z|http://arxiv.org/abs/1702.07082v1|http://arxiv.org/pdf/1702.07082v1|Distributions and Statistical Power of Optimal Signal-Detection Methods   In Finite Cases|distribut statist power optim signal detect method finit case|In big data analysis for detecting rare and weak signals among $n$ features, some grouping-test methods such as Higher Criticism test (HC), Berk-Jones test (B-J), and $\phi$-divergence test share the similar asymptotical optimality when $n \rightarrow \infty$. However, in practical data analysis $n$ is frequently small and moderately large at most. In order to properly apply these optimal tests and wisely choose them for practical studies, it is important to know how to get the p-values and statistical power of them. To address this problem in an even broader context, this paper provides analytical solutions for a general family of goodness-of-fit (GOF) tests, which covers these optimal tests. For any given i.i.d. and continuous distributions of the input test statistics of the $n$ features, both p-value and statistical power of such a GOF test can be calculated. By calculation we compared the finite-sample performances of asymptotically optimal tests under the normal mixture alternative. Results show that HC is the best choice when signals are rare, while B-J is more robust over various signal patterns. In the application to a real genome-wide association study, results illustrate that the p-value calculation works well, and the optimal tests have potentials for detecting novel disease genes with weak genetic effects. The calculations have been implemented in an R package SetTest and published on the CRAN.|big data analysi detect rare weak signal among featur group test method higher critic test hc berk jone test phi diverg test share similar asymptot optim rightarrow infti howev practic data analysi frequent small moder larg order proper appli optim test wise choos practic studi import know get valu statist power address problem even broader context paper provid analyt solut general famili good fit gof test cover optim test ani given continu distribut input test statist featur valu statist power gof test calcul calcul compar finit sampl perform asymptot optim test normal mixtur altern result show hc best choic signal rare robust various signal pattern applic real genom wide associ studi result illustr valu calcul work well optim test potenti detect novel diseas gene weak genet effect calcul implement packag settest publish cran|['Hong Zhang', 'Jiashun Jin', 'Zheyang Wu']|['math.ST', 'stat.TH']
2017-03-28T14:05:11Z|2017-02-26T20:06:00Z|http://arxiv.org/abs/1702.07027v2|http://arxiv.org/pdf/1702.07027v2|Nonparametric Inference via Bootstrapping the Debiased Estimator|nonparametr infer via bootstrap debias estim|In this paper, we propose to construct confidence bands by bootstrapping the debiased kernel density estimator (for density estimation) and the debiased local polynomial regression estimator (for regression analysis). The idea of using a debiased estimator was first introduced in Calonico et al. (2015), where they construct a confidence interval of the density function (and regression function) at a given point by explicitly estimating stochastic variations. We extend their ideas and propose a bootstrap approach for constructing confidence bands that is uniform for every point in the support. We prove that the resulting bootstrap confidence band is asymptotically valid and is compatible with most tuning parameter selection approaches, such as the rule of thumb and cross-validation. We further generalize our method to confidence sets of density level sets and inverse regression problems. Simulation studies confirm the validity of the proposed confidence bands/sets.|paper propos construct confid band bootstrap debias kernel densiti estim densiti estim debias local polynomi regress estim regress analysi idea use debias estim first introduc calonico et al construct confid interv densiti function regress function given point explicit estim stochast variat extend idea propos bootstrap approach construct confid band uniform everi point support prove result bootstrap confid band asymptot valid compat tune paramet select approach rule thumb cross valid general method confid set densiti level set invers regress problem simul studi confirm valid propos confid band set|['Yen-Chi Chen']|['stat.ME', 'math.ST', 'stat.TH', 'Primary 62G15, secondary 62G09, 62G07, 62G08']
2017-03-28T14:05:11Z|2017-02-22T19:25:29Z|http://arxiv.org/abs/1702.06975v1|http://arxiv.org/pdf/1702.06975v1|High dimensional deformed rectangle matrices with applications in matrix   denoising|high dimension deform rectangl matric applic matrix denois|We consider the recovery of a low rank $M \times N$ matrix $S$ from its noisy observation $\tilde{S}$ in two different regimes. Under the assumption that $M$ is comparable to $N$, we propose two optimal estimators for $S$. Our analysis rely on the local behavior of the large dimensional rectangle matrices with finite rank perturbation. We also derive the convergent limits and rates for the singular values and vectors of such matrices.|consid recoveri low rank time matrix noisi observ tild two differ regim assumpt compar propos two optim estim analysi reli local behavior larg dimension rectangl matric finit rank perturb also deriv converg limit rate singular valu vector matric|['Xiucai Ding']|['math.ST', 'stat.TH']
2017-03-28T14:05:11Z|2017-02-22T19:22:55Z|http://arxiv.org/abs/1702.06972v1|http://arxiv.org/pdf/1702.06972v1|Approximations of the Restless Bandit Problem|approxim restless bandit problem|The multi-armed restless bandit problem is studied in the case where the pay-offs are not necessarily independent over time nor across the arms. Even though this version of the problem provides a more realistic model for most real-world applications, it cannot be optimally solved in practice since it is known to be PSPACE-hard. The objective of this paper is to characterize special sub-classes of the problem where good approximate solutions can be found using tractable approaches. Specifically, it is shown that in the case where the joint distribution over the arms is $\varphi$-mixing, and under some conditions on the $\varphi$-mixing coefficients, a modified version of UCB can prove optimal. On the other hand, it is shown that when the pay-off distributions are strongly dependent, simple switching strategies may be devised which leverage the strong inter-dependencies. To this end, an example is provided using Gaussian Processes. The techniques developed in this paper apply, more generally, to the problem of online sampling under dependence.|multi arm restless bandit problem studi case pay necessarili independ time across arm even though version problem provid realist model real world applic cannot optim solv practic sinc known pspace hard object paper character special sub class problem good approxim solut found use tractabl approach specif shown case joint distribut arm varphi mix condit varphi mix coeffici modifi version ucb prove optim hand shown pay distribut strong depend simpl switch strategi may devis leverag strong inter depend end exampl provid use gaussian process techniqu develop paper appli general problem onlin sampl depend|['Steffen Grunewalder', 'Azadeh Khaleghi']|['math.ST', 'cs.LG', 'math.PR', 'stat.ML', 'stat.TH']
2017-03-28T14:05:11Z|2017-02-23T19:01:53Z|http://arxiv.org/abs/1702.06488v2|http://arxiv.org/pdf/1702.06488v2|Distributed Estimation of Principal Eigenspaces|distribut estim princip eigenspac|"Principal component analysis (PCA) is fundamental to statistical machine learning. It extracts latent principal factors that contribute to the most variation of the data. When data are stored across multiple machines, however, communication cost can prohibit the computation of PCA in a central location and distributed algorithms for PCA are thus needed. This paper proposes and studies a distributed PCA algorithm: each node machine computes the top $K$ eigenvectors and transmits them to the central server; the central server then aggregates the information from all the node machines and conducts a PCA based on the aggregated information. We investigate the bias and variance for the resulting distributed estimator of the top $K$ eigenvectors. In particular, we show that for distributions with symmetric innovation, the distributed PCA is ""unbiased"". We derive the rate of convergence for distributed PCA estimators, which depends explicitly on the effective rank of covariance, eigen-gap, and the number of machines. We show that when the number of machines is not unreasonably large, the distributed PCA performs as well as the whole sample PCA, even without full access of whole data. The theoretical results are verified by an extensive simulation study. We also extend our analysis to the heterogeneous case where the population covariance matrices are different across local machines but share similar top eigen-structures."|princip compon analysi pca fundament statist machin learn extract latent princip factor contribut variat data data store across multipl machin howev communic cost prohibit comput pca central locat distribut algorithm pca thus need paper propos studi distribut pca algorithm node machin comput top eigenvector transmit central server central server aggreg inform node machin conduct pca base aggreg inform investig bias varianc result distribut estim top eigenvector particular show distribut symmetr innov distribut pca unbias deriv rate converg distribut pca estim depend explicit effect rank covari eigen gap number machin show number machin unreason larg distribut pca perform well whole sampl pca even without full access whole data theoret result verifi extens simul studi also extend analysi heterogen case popul covari matric differ across local machin share similar top eigen structur|['Jianqing Fan', 'Dong Wang', 'Kaizheng Wang', 'Ziwei Zhu']|['stat.CO', 'math.ST', 'stat.TH']
2017-03-28T14:05:11Z|2017-02-20T16:40:45Z|http://arxiv.org/abs/1702.06055v1|http://arxiv.org/pdf/1702.06055v1|Performance of information criteria used for model selection of Hawkes   process models of financial data|perform inform criteria use model select hawk process model financi data|We test three common information criteria (IC) for selecting the order of a Hawkes process with an intensity kernel that can be expressed as a mixture of exponential terms. These processes find application in high-frequency financial data modelling. The information criteria are Akaike's information criterion (AIC), the Bayesian information criterion (BIC) and the Hannan-Quinn criterion (HQ). Since we work with simulated data, we are able to measure the performance of model selection by the success rate of the IC in selecting the model that was used to generate the data. In particular, we are interested in the relation between correct model selection and underlying sample size. The analysis includes realistic sample sizes and parameter sets from recent literature where parameters were estimated using empirical financial intra-day data. We compare our results to theoretical predictions and similar empirical findings on the asymptotic distribution of model selection for consistent and inconsistent IC.|test three common inform criteria ic select order hawk process intens kernel express mixtur exponenti term process find applic high frequenc financi data model inform criteria akaik inform criterion aic bayesian inform criterion bic hannan quinn criterion hq sinc work simul data abl measur perform model select success rate ic select model use generat data particular interest relat correct model select sampl size analysi includ realist sampl size paramet set recent literatur paramet estim use empir financi intra day data compar result theoret predict similar empir find asymptot distribut model select consist inconsist ic|['J. M. Chen', 'A. G. Hawkes', 'E. Scalas', 'M. Trinh']|['q-fin.ST', 'math.ST', 'stat.TH', '60G55']
2017-03-28T14:05:11Z|2017-02-20T14:34:46Z|http://arxiv.org/abs/1702.05985v1|http://arxiv.org/pdf/1702.05985v1|Fano's inequality for random variables|fano inequ random variabl|We extend Fano's inequality, which controls the average probability of (disjoint) events in terms of the average of some Kullback-Leibler divergences, to work with arbitrary [0,1]-valued random variables. Our simple two-step methodology is general enough to cover the case of an arbitrary (possibly continuously infinite) family of distributions as well as [0,1]-valued random variables not necessarily summing up to 1. Several novel applications are provided, in which the consideration of random variables is particularly handy. The most important applications deal with the problem of Bayesian posterior concentration (minimax or distribution-dependent) rates and with a lower bound on the regret in non-stochastic sequential learning. We also improve in passing some earlier fundamental results: in particular, we provide a simple and enlightening proof of the refined Pinsker's inequality of Ordentlich and Weinberger and derive a sharper Bretagnolle-Huber inequality.|extend fano inequ control averag probabl disjoint event term averag kullback leibler diverg work arbitrari valu random variabl simpl two step methodolog general enough cover case arbitrari possibl continu infinit famili distribut well valu random variabl necessarili sum sever novel applic provid consider random variabl particular handi import applic deal problem bayesian posterior concentr minimax distribut depend rate lower bound regret non stochast sequenti learn also improv pass earlier fundament result particular provid simpl enlighten proof refin pinsker inequ ordentlich weinberg deriv sharper bretagnoll huber inequ|['Sebastien Gerchinovitz', 'Pierre Ménard', 'Gilles Stoltz']|['math.ST', 'cs.IT', 'math.IT', 'stat.TH']
