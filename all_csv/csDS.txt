2017-03-28T14:09:26Z|2017-03-27T13:57:31Z|http://arxiv.org/abs/1703.09083v1|http://arxiv.org/pdf/1703.09083v1|The weighted stable matching problem|weight stabl match problem|We study the stable matching problem in non-bipartite graphs with incomplete but strict preference lists, where the edges have weights and the goal is to compute a stable matching of minimum or maximum weight. This problem is known to be NP-hard in general. Our contribution is two fold: a polyhedral characterization and an approximation algorithm. Previously Chen et al. have shown that the stable matching polytope is integral if and only if the subgraph obtained after running phase one of Irving's algorithm is bipartite. We improve upon this result by showing that there are instances where this subgraph might not be bipartite but one can further eliminate some edges and arrive at a bipartite subgraph. Our elimination procedure ensures that the set of stable matchings remains the same, and thus the stable matching polytope of the final subgraph contains the incidence vectors of all stable matchings of our original graph. This allows us to characterize a larger class of instances for which the weighted stable matching problem is polynomial-time solvable. We also show that our edge elimination procedure is best possible, meaning that if the subgraph we arrive at is not bipartite, then there is no bipartite subgraph that has the same set of stable matchings as the original graph. We complement these results with a $2$-approximation algorithm for the minimum weight stable matching problem for instances where each agent has at most two possible partners in any stable matching. This is the first approximation result for any class of instances with general weights.|studi stabl match problem non bipartit graph incomplet strict prefer list edg weight goal comput stabl match minimum maximum weight problem known np hard general contribut two fold polyhedr character approxim algorithm previous chen et al shown stabl match polytop integr onli subgraph obtain run phase one irv algorithm bipartit improv upon result show instanc subgraph might bipartit one elimin edg arriv bipartit subgraph elimin procedur ensur set stabl match remain thus stabl match polytop final subgraph contain incid vector stabl match origin graph allow us character larger class instanc weight stabl match problem polynomi time solvabl also show edg elimin procedur best possibl mean subgraph arriv bipartit bipartit subgraph set stabl match origin graph complement result approxim algorithm minimum weight stabl match problem instanc agent two possibl partner ani stabl match first approxim result ani class instanc general weight|['Linda Farczadi', 'Natália Guričanová']|['cs.GT', 'cs.DS']
2017-03-28T14:09:26Z|2017-03-27T06:50:30Z|http://arxiv.org/abs/1703.08950v1|http://arxiv.org/pdf/1703.08950v1|Gene tree species tree reconciliation with gene conversion|gene tree speci tree reconcili gene convers|Gene tree/species tree reconciliation is a recent decisive progress in phylo-genetic methods, accounting for the possible differences between gene histories and species histories. Reconciliation consists in explaining these differences by gene-scale events such as duplication, loss, transfer, which translates mathematically into a mapping between gene tree nodes and species tree nodes or branches. Gene conversion is a very frequent biological event, which results in the replacement of a gene by a copy of another from the same species and in the same gene tree. Including this event in reconciliations has never been attempted because this changes as well the solutions as the methods to construct reconciliations. Standard algorithms based on dynamic programming become ineffective. We propose here a novel mathematical framework including gene conversion as an evolutionary event in gene tree/species tree reconciliation. We describe a randomized algorithm giving in polynomial running time a reconciliation minimizing the number of duplications, losses and conversions. We show that the space of reconciliations includes an analog of the Last Common Ancestor reconciliation, but is not limited to it. Our algorithm outputs any optimal reconciliation with non null probability. We argue that this study opens a wide research avenue on including gene conversion in reconciliation, which can be important for biology.|gene tree speci tree reconcili recent decis progress phylo genet method account possibl differ gene histori speci histori reconcili consist explain differ gene scale event duplic loss transfer translat mathemat map gene tree node speci tree node branch gene convers veri frequent biolog event result replac gene copi anoth speci gene tree includ event reconcili never attempt becaus chang well solut method construct reconcili standard algorithm base dynam program becom ineffect propos novel mathemat framework includ gene convers evolutionari event gene tree speci tree reconcili describ random algorithm give polynomi run time reconcili minim number duplic loss convers show space reconcili includ analog last common ancestor reconcili limit algorithm output ani optim reconcili non null probabl argu studi open wide research avenu includ gene convers reconcili import biolog|['Damir Hasic', 'Eric Tannier']|['q-bio.QM', 'cs.DS']
2017-03-28T14:09:26Z|2017-03-27T05:48:36Z|http://arxiv.org/abs/1703.08940v1|http://arxiv.org/pdf/1703.08940v1|Tree Edit Distance Cannot be Computed in Strongly Subcubic Time (unless   APSP can)|tree edit distanc cannot comput strong subcub time unless apsp|The edit distance between two rooted ordered trees with $n$ nodes labeled from an alphabet~$\Sigma$ is the minimum cost of transforming one tree into the other by a sequence of elementary operations consisting of deleting and relabeling existing nodes, as well as inserting new nodes. Tree edit distance is a well known generalization of string edit distance. The fastest known algorithm for tree edit distance runs in cubic $O(n^3)$ time and is based on a similar dynamic programming solution as string edit distance. In this paper we show that a truly subcubic $O(n^{3-\varepsilon})$ time algorithm for tree edit distance is unlikely: For $ \Sigma  = \Omega(n)$, a truly subcubic algorithm for tree edit distance implies a truly subcubic algorithm for the all pairs shortest paths problem. For $ \Sigma  = O(1)$, a truly subcubic algorithm for tree edit distance implies an $O(n^{k-\varepsilon})$ algorithm for finding a maximum weight $k$-clique.   Thus, while in terms of upper bounds string edit distance and tree edit distance are highly related, in terms of lower bounds string edit distance exhibits the hardness of the strong exponential time hypothesis [Backurs, Indyk STOC'15] whereas tree edit distance exhibits the hardness of all pairs shortest paths. Our result provides a matching conditional lower bound for one of the last remaining classic dynamic programming problems.|edit distanc two root order tree node label alphabet sigma minimum cost transform one tree sequenc elementari oper consist delet relabel exist node well insert new node tree edit distanc well known general string edit distanc fastest known algorithm tree edit distanc run cubic time base similar dynam program solut string edit distanc paper show truli subcub varepsilon time algorithm tree edit distanc unlik sigma omega truli subcub algorithm tree edit distanc impli truli subcub algorithm pair shortest path problem sigma truli subcub algorithm tree edit distanc impli varepsilon algorithm find maximum weight cliqu thus term upper bound string edit distanc tree edit distanc high relat term lower bound string edit distanc exhibit hard strong exponenti time hypothesi backur indyk stoc wherea tree edit distanc exhibit hard pair shortest path result provid match condit lower bound one last remain classic dynam program problem|['Karl Bringmann', 'Paweł Gawrychowski', 'Shay Mozes', 'Oren Weimann']|['cs.DS']
2017-03-28T14:09:26Z|2017-03-27T05:09:06Z|http://arxiv.org/abs/1703.08931v1|http://arxiv.org/pdf/1703.08931v1|Palindromic Decompositions with Gaps and Errors|palindrom decomposit gap error|Identifying palindromes in sequences has been an interesting line of research in combinatorics on words and also in computational biology, after the discovery of the relation of palindromes in the DNA sequence with the HIV virus. Efficient algorithms for the factorization of sequences into palindromes and maximal palindromes have been devised in recent years. We extend these studies by allowing gaps in decompositions and errors in palindromes, and also imposing a lower bound to the length of acceptable palindromes.   We first present an algorithm for obtaining a palindromic decomposition of a string of length n with the minimal total gap length in time O(n log n * g) and space O(n g), where g is the number of allowed gaps in the decomposition. We then consider a decomposition of the string in maximal \delta-palindromes (i.e. palindromes with \delta errors under the edit or Hamming distance) and g allowed gaps. We present an algorithm to obtain such a decomposition with the minimal total gap length in time O(n (g + \delta)) and space O(n g).|identifi palindrom sequenc interest line research combinator word also comput biolog discoveri relat palindrom dna sequenc hiv virus effici algorithm factor sequenc palindrom maxim palindrom devis recent year extend studi allow gap decomposit error palindrom also impos lower bound length accept palindrom first present algorithm obtain palindrom decomposit string length minim total gap length time log space number allow gap decomposit consid decomposit string maxim delta palindrom palindrom delta error edit ham distanc allow gap present algorithm obtain decomposit minim total gap length time delta space|['Michał Adamczyk', 'Mai Alzamel', 'Panagiotis Charalampopoulos', 'Costas S. Iliopoulos', 'Jakub Radoszewski']|['cs.DS']
2017-03-28T14:09:26Z|2017-03-26T09:03:07Z|http://arxiv.org/abs/1703.08790v1|http://arxiv.org/pdf/1703.08790v1|Steiner Point Removal --- Distant Terminals Don't (Really) Bother|steiner point remov distant termin realli bother|Given a weighted graph $G=(V,E,w)$ with a set of $k$ terminals $T\subset V$, the Steiner Point Removal problem seeks for a minor of the graph with vertex set $T$, such that the distance between every pair of terminals is preserved within a small multiplicative distortion. Kamma, Krauthgamer and Nguyen (SODA 2014, SICOMP 2015) used a ball-growing algorithm to show that the distortion is at most $\mathcal{O}(\log^5 k)$ for general graphs.   In this paper, we improve the distortion bound to $\mathcal{O}(\log^2 k)$. The improvement is achieved based on a known algorithm that constructs terminal-distance exact-preservation minor with $\mathcal{O}(k^4)$ (which is independent of $ V $) vertices, and also two tail bounds on the sum of independent exponential random variables, which allow us to show that it is unlikely for a non-terminal being contracted to a distant terminal.|given weight graph set termin subset steiner point remov problem seek minor graph vertex set distanc everi pair termin preserv within small multipl distort kamma krauthgam nguyen soda sicomp use ball grow algorithm show distort mathcal log general graph paper improv distort bound mathcal log improv achiev base known algorithm construct termin distanc exact preserv minor mathcal independ vertic also two tail bound sum independ exponenti random variabl allow us show unlik non termin contract distant termin|['Yun Kuen Cheung']|['cs.DS', 'cs.DM', 'math.CO', 'math.PR']
2017-03-28T14:09:26Z|2017-03-25T15:03:49Z|http://arxiv.org/abs/1703.08702v1|http://arxiv.org/pdf/1703.08702v1|Randomized Load Balancing on Networks with Stochastic Inputs|random load balanc network stochast input|Iterative load balancing algorithms for indivisible tokens have been studied intensively in the past. Complementing previous worst-case analyses, we study an average-case scenario where the load inputs are drawn from a fixed probability distribution. For cycles, tori, hypercubes and expanders, we obtain almost matching upper and lower bounds on the discrepancy, the difference between the maximum and the minimum load. Our bounds hold for a variety of probability distributions including the uniform and binomial distribution but also distributions with unbounded range such as the Poisson and geometric distribution. For graphs with slow convergence like cycles and tori, our results demonstrate a substantial difference between the convergence in the worst- and average-case. An important ingredient in our analysis is new upper bound on the t-step transition probability of a general Markov chain, which is derived by invoking the evolving set process.|iter load balanc algorithm indivis token studi intens past complement previous worst case analys studi averag case scenario load input drawn fix probabl distribut cycl tori hypercub expand obtain almost match upper lower bound discrep differ maximum minimum load bound hold varieti probabl distribut includ uniform binomi distribut also distribut unbound rang poisson geometr distribut graph slow converg like cycl tori result demonstr substanti differ converg worst averag case import ingredi analysi new upper bound step transit probabl general markov chain deriv invok evolv set process|['Leran Cai', 'Thomas Sauerwald']|['cs.DC', 'cs.DS', 'G.3']
2017-03-28T14:09:26Z|2017-03-25T07:27:32Z|http://arxiv.org/abs/1703.08658v1|http://arxiv.org/pdf/1703.08658v1|Maximizing the area of intersection of rectangles|maxim area intersect rectangl|This paper attacks the following problem. We are given a large number $N$ of rectangles in the plane, each with horizontal and vertical sides, and also a number $r<N$. The given list of $N$ rectangles may contain duplicates. The problem is to find $r$ of these rectangles, such that, if they are discarded, then the intersection of the remaining $(N-r)$ rectangles has an intersection with as large an area as possible. We will find an upper bound, depending only on $N$ and $r$, and not on the particular data presented, for the number of steps needed to run the algorithm on (a mathematical model of) a computer. In fact our algorithm is able to determine, for each $s\le r$, $s$ rectangles from the given list of $N$ rectangles, such that the remaining $(N-s)$ rectangles have as large an area as possible, and this takes hardly any more time than taking care only of the case $s=r$. Our algorithm extends to $d$-dimensional rectangles. Our method is to exhaustively examine all possible intersections---this is much faster than it sounds, because we do not need to examine all $\binom Ns$ subsets in order to find all possible intersection rectangles. For an extreme example, suppose the rectangles are nested, for example concentric squares of distinct sizes, then the only intersections examined are the smallest $s+1$ rectangles.|paper attack follow problem given larg number rectangl plane horizont vertic side also number given list rectangl may contain duplic problem find rectangl discard intersect remain rectangl intersect larg area possibl find upper bound depend onli particular data present number step need run algorithm mathemat model comput fact algorithm abl determin le rectangl given list rectangl remain rectangl larg area possibl take hard ani time take care onli case algorithm extend dimension rectangl method exhaust examin possibl intersect much faster sound becaus need examin binom ns subset order find possibl intersect rectangl extrem exampl suppos rectangl nest exampl concentr squar distinct size onli intersect examin smallest rectangl|['David B. A. Epstein', 'Mike Paterson']|['cs.DS', 'F.2.2']
2017-03-28T14:09:26Z|2017-03-24T20:21:52Z|http://arxiv.org/abs/1703.08589v1|http://arxiv.org/pdf/1703.08589v1|Polynomial-Time Methods to Solve Unimodular Quadratic Programs With   Performance Guarantees|polynomi time method solv unimodular quadrat program perform guarante|We develop polynomial-time heuristic methods to solve unimodular quadratic programs (UQPs) approximately, which are known to be NP-hard. In the UQP framework, we maximize a quadratic function of a vector of complex variables with unit modulus. Several problems in active sensing and wireless communication applications boil down to UQP. With this motivation, we present three new heuristic methods with polynomial-time complexity to solve the UQP approximately. The first method is called dominant-eigenvector-matching; here the solution is picked that matches the complex arguments of the dominant eigenvector of the Hermitian matrix in the UQP formulation. We also provide a performance guarantee for this method. The second method, a greedy strategy, is shown to provide a performance guarantee of (1-1/e) with respect to the optimal objective value given that the objective function possesses a property called string submodularity. The third heuristic method is called row-swap greedy strategy, which is an extension to the greedy strategy and utilizes certain properties of the UQP to provide a better performance than the greedy strategy at the expense of an increase in computational complexity. We present numerical results to demonstrate the performance of these heuristic methods, and also compare the performance of these methods against a standard heuristic method called semidefinite relaxation.|develop polynomi time heurist method solv unimodular quadrat program uqp approxim known np hard uqp framework maxim quadrat function vector complex variabl unit modulus sever problem activ sens wireless communic applic boil uqp motiv present three new heurist method polynomi time complex solv uqp approxim first method call domin eigenvector match solut pick match complex argument domin eigenvector hermitian matrix uqp formul also provid perform guarante method second method greedi strategi shown provid perform guarante respect optim object valu given object function possess properti call string submodular third heurist method call row swap greedi strategi extens greedi strategi util certain properti uqp provid better perform greedi strategi expens increas comput complex present numer result demonstr perform heurist method also compar perform method standard heurist method call semidefinit relax|['Shankarachary Ragi', 'Edwin K. P. Chong', 'Hans D. Mittelmann']|['math.OC', 'cs.DS']
2017-03-28T14:09:26Z|2017-03-24T17:10:23Z|http://arxiv.org/abs/1703.08511v1|http://arxiv.org/pdf/1703.08511v1|ALLSAT compressed with wildcards. Part 2: All k-models of a BDD|allsat compress wildcard part model bdd|If f is a Boolean function given by a BDD then it is well known how to calculate the number of models (i.e. bitstrings x with f(x)=1). Let  x  be the number of 1's in x. How to calculate the number of k-models x (i.e. having  x =k) is lesser known; we review a nice method due to Knuth. The main topic however is enumeration (=generation) as opposed to counting. Again, that ALL models can be enumerated in polynomial total time, is well known. Apparently new is the fact that also all k-models (for any fixed k) can be enumerated in polynomial total time. Using suitable wildcards this can be achieved in a compressed format.|boolean function given bdd well known calcul number model bitstr let number calcul number model lesser known review nice method due knuth main topic howev enumer generat oppos count model enumer polynomi total time well known appar new fact also model ani fix enumer polynomi total time use suitabl wildcard achiev compress format|['Marcel Wild']|['cs.DS']
2017-03-28T14:09:26Z|2017-03-24T14:44:04Z|http://arxiv.org/abs/1703.08433v1|http://arxiv.org/pdf/1703.08433v1|Metric random matchings with applications|metric random match applic|Let $(\{1,2,\ldots,n\},d)$ be a metric space. We analyze the expected value and the variance of $\sum_{i=1}^{\lfloor n/2\rfloor}\,d({\boldsymbol{\pi}}(2i-1),{\boldsymbol{\pi}}(2i))$ for a uniformly random permutation ${\boldsymbol{\pi}}$ of $\{1,2,\ldots,n\}$, leading to the following results: (I) Consider the problem of finding a point in $\{1,2,\ldots,n\}$ with the minimum sum of distances to all points. We show that this problem has a randomized algorithm that (1) always outputs a $(2+\epsilon)$-approximate solution in expected $O(n/\epsilon^2)$ time and that (2) inherits Indyk's~\cite{Ind99, Ind00} algorithm to output a $(1+\epsilon)$-approximate solution in $O(n/\epsilon^2)$ time with probability $\Omega(1)$, where $\epsilon\in(0,1)$. (II) The average distance in $(\{1,2,\ldots,n\},d)$ can be approximated in $O(n/\epsilon)$ time to within a multiplicative factor in $[\,1/2-\epsilon,1\,]$ with probability $1/2+\Omega(1)$, where $\epsilon>0$. (III) Assume $d$ to be a graph metric. Then the average distance in $(\{1,2,\ldots,n\},d)$ can be approximated in $O(n)$ time to within a multiplicative factor in $[\,1-\epsilon,1+\epsilon\,]$ with probability $1/2+\Omega(1)$, where $\epsilon=\omega(1/n^{1/4})$.|let ldot metric space analyz expect valu varianc sum lfloor rfloor boldsymbol pi boldsymbol pi uniform random permut boldsymbol pi ldot lead follow result consid problem find point ldot minimum sum distanc point show problem random algorithm alway output epsilon approxim solut expect epsilon time inherit indyk cite ind ind algorithm output epsilon approxim solut epsilon time probabl omega epsilon ii averag distanc ldot approxim epsilon time within multipl factor epsilon probabl omega epsilon iii assum graph metric averag distanc ldot approxim time within multipl factor epsilon epsilon probabl omega epsilon omega|['Ching-Lueh Chang']|['cs.DS']
2017-03-28T14:09:30Z|2017-03-24T02:59:51Z|http://arxiv.org/abs/1703.08273v1|http://arxiv.org/pdf/1703.08273v1|An Asymptotically Tighter Bound on Sampling for Frequent Itemsets Mining|asymptot tighter bound sampl frequent itemset mine|In this paper we present a new error bound on sampling algorithms for frequent itemsets mining. We show that the new bound is asymptotically tighter than the state-of-art bounds, i.e., given the chosen samples, for small enough error probability, the new error bound is roughly half of the existing bounds. Based on the new bound, we give a new approximation algorithm, which is much simpler compared to the existing approximation algorithms, but can also guarantee the worst approximation error with precomputed sample size. We also give an algorithm which can approximate the top-$k$ frequent itemsets with high accuracy and efficiency.|paper present new error bound sampl algorithm frequent itemset mine show new bound asymptot tighter state art bound given chosen sampl small enough error probabl new error bound rough half exist bound base new bound give new approxim algorithm much simpler compar exist approxim algorithm also guarante worst approxim error precomput sampl size also give algorithm approxim top frequent itemset high accuraci effici|['Shiyu Ji', 'Kun Wan']|['cs.DS', 'cs.DB']
2017-03-28T14:09:30Z|2017-03-23T16:50:03Z|http://arxiv.org/abs/1703.08139v1|http://arxiv.org/pdf/1703.08139v1|Optimal lower bounds for universal relation, samplers, and finding   duplicates|optim lower bound univers relat sampler find duplic|In the communication problem $\mathbf{UR}$ (universal relation) [KRW95], Alice and Bob respectively receive $x$ and $y$ in $\{0,1\}^n$ with the promise that $x\neq y$. The last player to receive a message must output an index $i$ such that $x_i\neq y_i$. We prove that the randomized one-way communication complexity of this problem in the public coin model is exactly $\Theta(\min\{n, \log(1/\delta)\log^2(\frac{n}{\log(1/\delta)})\})$ bits for failure probability $\delta$. Our lower bound holds even if promised $\mathop{support}(y)\subset \mathop{support}(x)$. As a corollary, we obtain optimal lower bounds for $\ell_p$-sampling in strict turnstile streams for $0\le p < 2$, as well as for the problem of finding duplicates in a stream. Our lower bounds do not need to use large weights, and hold even if it is promised that $x\in\{0,1\}^n$ at all points in the stream.   Our lower bound demonstrates that any algorithm $\mathcal{A}$ solving sampling problems in turnstile streams in low memory can be used to encode subsets of $[n]$ of certain sizes into a number of bits below the information theoretic minimum. Our encoder makes adaptive queries to $\mathcal{A}$ throughout its execution, but done carefully so as to not violate correctness. This is accomplished by injecting random noise into the encoder's interactions with $\mathcal{A}$, which is loosely motivated by techniques in differential privacy. Our correctness analysis involves understanding the ability of $\mathcal{A}$ to correctly answer adaptive queries which have positive but bounded mutual information with $\mathcal{A}$'s internal randomness, and may be of independent interest in the newly emerging area of adaptive data analysis with a theoretical computer science lens.|communic problem mathbf ur univers relat krw alic bob respect receiv promis neq last player receiv messag must output index neq prove random one way communic complex problem public coin model exact theta min log delta log frac log delta bit failur probabl delta lower bound hold even promis mathop support subset mathop support corollari obtain optim lower bound ell sampl strict turnstil stream le well problem find duplic stream lower bound need use larg weight hold even promis point stream lower bound demonstr ani algorithm mathcal solv sampl problem turnstil stream low memori use encod subset certain size number bit inform theoret minimum encod make adapt queri mathcal throughout execut done care violat correct accomplish inject random nois encod interact mathcal loos motiv techniqu differenti privaci correct analysi involv understand abil mathcal correct answer adapt queri posit bound mutual inform mathcal intern random may independ interest newli emerg area adapt data analysi theoret comput scienc len|['Jelani Nelson', 'Jakub Pachocki', 'Zhengyu Wang']|['cs.CC', 'cs.DS']
2017-03-28T14:09:30Z|2017-03-23T12:32:10Z|http://arxiv.org/abs/1703.08041v1|http://arxiv.org/pdf/1703.08041v1|Resolving the Complexity of Some Fundamental Problems in Computational   Social Choice|resolv complex fundament problem comput social choic|This thesis is in the area called computational social choice which is an intersection area of algorithms and social choice theory.|thesi area call comput social choic intersect area algorithm social choic theori|['Palash Dey']|['cs.DS', 'cs.AI', 'cs.MA']
2017-03-28T14:09:30Z|2017-03-23T08:37:54Z|http://arxiv.org/abs/1703.07964v1|http://arxiv.org/abs/1703.07964v1|Minimum Cuts and Shortest Cycles in Directed Planar Graphs via   Noncrossing Shortest Paths|minimum cut shortest cycl direct planar graph via noncross shortest path|Let $G$ be an $n$-node simple directed planar graph with nonnegative edge weights. We study the fundamental problems of computing (1) a global cut of $G$ with minimum weight and (2) a~cycle of $G$ with minimum weight. The best previously known algorithm for the former problem, running in $O(n\log^3 n)$ time, can be obtained from the algorithm of \Lacki, Nussbaum, Sankowski, and Wulff-Nilsen for single-source all-sinks maximum flows. The best previously known result for the latter problem is the $O(n\log^3 n)$-time algorithm of Wulff-Nilsen. By exploiting duality between the two problems in planar graphs, we solve both problems in $O(n\log n\log\log n)$ time via a divide-and-conquer algorithm that finds a shortest non-degenerate cycle. The kernel of our result is an $O(n\log\log n)$-time algorithm for computing noncrossing shortest paths among nodes well ordered on a common face of a directed plane graph, which is extended from the algorithm of Italiano, Nussbaum, Sankowski, and Wulff-Nilsen for an undirected plane graph.|let node simpl direct planar graph nonneg edg weight studi fundament problem comput global cut minimum weight cycl minimum weight best previous known algorithm former problem run log time obtain algorithm lacki nussbaum sankowski wulff nilsen singl sourc sink maximum flow best previous known result latter problem log time algorithm wulff nilsen exploit dualiti two problem planar graph solv problem log log log time via divid conquer algorithm find shortest non degener cycl kernel result log log time algorithm comput noncross shortest path among node well order common face direct plane graph extend algorithm italiano nussbaum sankowski wulff nilsen undirect plane graph|['Hung-Chun Liang', 'Hsueh-I Lu']|['cs.DS', '05C38, 05C10, 05C85, 68P05']
2017-03-28T14:09:30Z|2017-03-22T21:52:05Z|http://arxiv.org/abs/1703.07867v1|http://arxiv.org/pdf/1703.07867v1|Distance-sensitive hashing|distanc sensit hash|"We initiate the study of distance-sensitive hashing, a generalization of locality-sensitive hashing that seeks a family of hash functions such that the probability of two points having the same hash value is a given function of the distance between them. More precisely, given a distance space $(X, \text{dist})$ and a ""collision probability function"" (CPF) $f\colon \mathbb{R}\rightarrow [0,1]$ we seek a distribution over pairs of functions $(h,g)$ such that for every pair of points $x, y \in X$ the collision probability is $\Pr[h(x)=g(y)] = f(\text{dist}(x,y))$. Locality-sensitive hashing is the study of how fast a CPF can decrease as the distance grows. For many spaces $f$ can be made exponentially decreasing even if we restrict attention to the symmetric case where $g=h$. In this paper we study how asymmetry makes it possible to achieve CPFs that are, for example, increasing or unimodal. Our original motivation comes from annulus queries where we are interested in searching for points at distance approximately $r$ from a query point, but we believe that distance-sensitive hashing is of interest beyond this application."|initi studi distanc sensit hash general local sensit hash seek famili hash function probabl two point hash valu given function distanc precis given distanc space text dist collis probabl function cpf colon mathbb rightarrow seek distribut pair function everi pair point collis probabl pr text dist local sensit hash studi fast cpf decreas distanc grow mani space made exponenti decreas even restrict attent symmetr case paper studi asymmetri make possibl achiev cpfs exampl increas unimod origin motiv come annulus queri interest search point distanc approxim queri point believ distanc sensit hash interest beyond applic|['Martin Aumüller', 'Tobias Christiani', 'Rasmus Pagh', 'Francesco Silvestri']|['cs.DS', 'H.3.3']
2017-03-28T14:09:30Z|2017-03-22T16:28:17Z|http://arxiv.org/abs/1703.07734v1|http://arxiv.org/pdf/1703.07734v1|On the Probe Complexity of Local Computation Algorithms|probe complex local comput algorithm|"The Local Computation Algorithms (LCA) model is a computational model aimed at problem instances with huge inputs and output. For graph problems, the input graph is accessed using probes: strong probes (SP) specify a vertex $v$ and receive as a reply a list of $v$'s neighbors, and weak probes (WP) specify a vertex $v$ and a port number $i$ and receive as a reply $v$'s $i^{th}$ neighbor. Given a local query (e.g., ""is a certain vertex in the vertex cover of the input graph?""), an LCA should compute the corresponding local output (e.g., ""yes"" or ""no"") while making only a small number of probes, with the requirement that all local outputs form a single global solution (e.g., a legal vertex cover). We study the probe complexity of LCAs that are required to work on graphs that may have arbitrarily large degrees. In particular, such LCAs are expected to probe the graph a number of times that is significantly smaller than the maximum, average, or even minimum degree.   For weak probes, we focus on the weak coloring problem. Among our results we show a separation between weak 3-coloring and weak 2-coloring for deterministic LCAs: $\log^* n + O(1)$ weak probes suffice for weak 3-coloring, but $\Omega\left(\frac{\log n}{\log\log n}\right)$ weak probes are required for weak 2-coloring.   For strong probes, we consider randomized LCAs for vertex cover and maximal/maximum matching. Our negative results include showing that there are graphs for which finding a \emph{maximal} matching requires $\Omega(\sqrt{n})$ strong probes. On the positive side, we design a randomized LCA that finds a $(1-\epsilon)$ approximation to \emph{maximum} matching in regular graphs, and uses $\frac{1}{\epsilon }^{O\left( \frac{1}{\epsilon ^2}\right)}$ probes, independently of the number of vertices and of their degrees."|local comput algorithm lca model comput model aim problem instanc huge input output graph problem input graph access use probe strong probe sp specifi vertex receiv repli list neighbor weak probe wp specifi vertex port number receiv repli th neighbor given local queri certain vertex vertex cover input graph lca comput correspond local output yes make onli small number probe requir local output form singl global solut legal vertex cover studi probe complex lcas requir work graph may arbitrarili larg degre particular lcas expect probe graph number time signific smaller maximum averag even minimum degre weak probe focus weak color problem among result show separ weak color weak color determinist lcas log weak probe suffic weak color omega left frac log log log right weak probe requir weak color strong probe consid random lcas vertex cover maxim maximum match negat result includ show graph find emph maxim match requir omega sqrt strong probe posit side design random lca find epsilon approxim emph maximum match regular graph use frac epsilon left frac epsilon right probe independ number vertic degre|['Uriel Feige', 'Boaz Patt-Shamir', 'Shai Vardi']|['cs.DS']
2017-03-28T14:09:30Z|2017-03-22T12:50:15Z|http://arxiv.org/abs/1703.07625v1|http://arxiv.org/pdf/1703.07625v1|Clustering for Different Scales of Measurement - the Gap-Ratio Weighted   K-means Algorithm|cluster differ scale measur gap ratio weight mean algorithm|This paper describes a method for clustering data that are spread out over large regions and which dimensions are on different scales of measurement. Such an algorithm was developed to implement a robotics application consisting in sorting and storing objects in an unsupervised way. The toy dataset used to validate such application consists of Lego bricks of different shapes and colors. The uncontrolled lighting conditions together with the use of RGB color features, respectively involve data with a large spread and different levels of measurement between data dimensions. To overcome the combination of these two characteristics in the data, we have developed a new weighted K-means algorithm, called gap-ratio K-means, which consists in weighting each dimension of the feature space before running the K-means algorithm. The weight associated with a feature is proportional to the ratio of the biggest gap between two consecutive data points, and the average of all the other gaps. This method is compared with two other variants of K-means on the Lego bricks clustering problem as well as two other common classification datasets.|paper describ method cluster data spread larg region dimens differ scale measur algorithm develop implement robot applic consist sort store object unsupervis way toy dataset use valid applic consist lego brick differ shape color uncontrol light condit togeth use rgb color featur respect involv data larg spread differ level measur data dimens overcom combin two characterist data develop new weight mean algorithm call gap ratio mean consist weight dimens featur space befor run mean algorithm weight associ featur proport ratio biggest gap two consecut data point averag gap method compar two variant mean lego brick cluster problem well two common classif dataset|['Joris Guérin', 'Olivier Gibaru', 'Stéphane Thiery', 'Eric Nyiri']|['cs.LG', 'cs.DS', 'stat.ML']
2017-03-28T14:09:30Z|2017-03-21T21:05:27Z|http://arxiv.org/abs/1703.07432v1|http://arxiv.org/pdf/1703.07432v1|Efficient PAC Learning from the Crowd|effici pac learn crowd|In recent years crowdsourcing has become the method of choice for gathering labeled training data for learning algorithms. Standard approaches to crowdsourcing view the process of acquiring labeled data separately from the process of learning a classifier from the gathered data. This can give rise to computational and statistical challenges. For example, in most cases there are no known computationally efficient learning algorithms that are robust to the high level of noise that exists in crowdsourced data, and efforts to eliminate noise through voting often require a large number of queries per example.   In this paper, we show how by interleaving the process of labeling and learning, we can attain computational efficiency with much less overhead in the labeling cost. In particular, we consider the realizable setting where there exists a true target function in $\mathcal{F}$ and consider a pool of labelers. When a noticeable fraction of the labelers are perfect, and the rest behave arbitrarily, we show that any $\mathcal{F}$ that can be efficiently learned in the traditional realizable PAC model can be learned in a computationally efficient manner by querying the crowd, despite high amounts of noise in the responses. Moreover, we show that this can be done while each labeler only labels a constant number of examples and the number of labels requested per example, on average, is a constant. When no perfect labelers exist, a related task is to find a set of the labelers which are good but not perfect. We show that we can identify all good labelers, when at least the majority of labelers are good.|recent year crowdsourc becom method choic gather label train data learn algorithm standard approach crowdsourc view process acquir label data separ process learn classifi gather data give rise comput statist challeng exampl case known comput effici learn algorithm robust high level nois exist crowdsourc data effort elimin nois vote often requir larg number queri per exampl paper show interleav process label learn attain comput effici much less overhead label cost particular consid realiz set exist true target function mathcal consid pool label notic fraction label perfect rest behav arbitrarili show ani mathcal effici learn tradit realiz pac model learn comput effici manner queri crowd despit high amount nois respons moreov show done label onli label constant number exampl number label request per exampl averag constant perfect label exist relat task find set label good perfect show identifi good label least major label good|['Pranjal Awasthi', 'Avrim Blum', 'Nika Haghtalab', 'Yishay Mansour']|['cs.LG', 'cs.DS']
2017-03-28T14:09:30Z|2017-03-21T20:28:52Z|http://arxiv.org/abs/1703.07417v1|http://arxiv.org/pdf/1703.07417v1|Approximating k-spanners in the LOCAL model|approxim spanner local model|Graph spanners have been studied extensively, and have many applications in algorithms, distributed systems, and computer networks. For many of these application, we want distributed constructions of spanners, i.e., algorithms which use only local information. Dinitz and Krauthgamer (PODC 2011) provided a distributed approximation algorithm for 2-spanners in the LOCAL model with polylogarithmic running time, but the question of whether a similar algorithm exists for k-spanners with k > 2 remained open. In this paper, we show that a similar algorithm also works for cases where k > 2.|graph spanner studi extens mani applic algorithm distribut system comput network mani applic want distribut construct spanner algorithm use onli local inform dinitz krauthgam podc provid distribut approxim algorithm spanner local model polylogarithm run time question whether similar algorithm exist spanner remain open paper show similar algorithm also work case|['Michael Dinitz', 'Yasamin Nazari']|['cs.DS', 'cs.DC', 'math.CO']
2017-03-28T14:09:30Z|2017-03-21T17:53:50Z|http://arxiv.org/abs/1703.07340v1|http://arxiv.org/pdf/1703.07340v1|Construction of Directed 2K Graphs|construct direct graph|We study the problem of constructing synthetic graphs that resemble real-world directed graphs in terms of their degree correlations. We define the problem of directed 2K construction (D2K) that takes as input the directed degree sequence (DDS) and a joint degree and attribute matrix (JDAM) so as to capture degree correlation specifically in directed graphs. We provide necessary and sufficient conditions to decide whether a target D2K is realizable, and we design an efficient algorithm that creates realizations with that target D2K. We evaluate our algorithm in creating synthetic graphs that target real-world directed graphs (such as Twitter) and we show that it brings significant benefits compared to state-of-the-art approaches.|studi problem construct synthet graph resembl real world direct graph term degre correl defin problem direct construct dk take input direct degre sequenc dds joint degre attribut matrix jdam captur degre correl specif direct graph provid necessari suffici condit decid whether target dk realiz design effici algorithm creat realize target dk evalu algorithm creat synthet graph target real world direct graph twitter show bring signific benefit compar state art approach|['Bálint Tillman', 'Athina Markopoulou', 'Carter T. Butts', 'Minas Gjoka']|['cs.SI', 'cs.DS']
2017-03-28T14:09:34Z|2017-03-21T15:57:42Z|http://arxiv.org/abs/1703.07290v1|http://arxiv.org/pdf/1703.07290v1|Just-in-Time Batch Scheduling Problem with Two-dimensional Bin Packing   Constraints|time batch schedul problem two dimension bin pack constraint|This paper introduces and approximately solves a multi-component problem where small rectangular items are produced from large rectangular bins via guillotine cuts. An item is characterized by its width, height, due date, and earliness and tardiness penalties per unit time. Each item induces a cost that is proportional to its earliness and tardiness. Items cut from the same bin form a batch, whose processing and completion times depend on its assigned items. The items of a batch have the completion time of their bin. The objective is to find a cutting plan that minimizes the weighted sum of earliness and tardiness penalties. We address this problem via a constraint programming based heuristic (CP) and an agent based modelling heuristic (AB). CP is an impact-based search strategy, implemented in the general-purpose solver IBM CP Optimizer. AB is constructive. It builds a solution through repeated negotiations between the set of agents representing the items and the set representing the bins. The agents cooperate to minimize the weighted earliness-tardiness penalties. The computational investigation shows that CP outperforms AB on small-sized instances while the opposite prevails for larger instances.|paper introduc approxim solv multi compon problem small rectangular item produc larg rectangular bin via guillotin cut item character width height due date earli tardi penalti per unit time item induc cost proport earli tardi item cut bin form batch whose process complet time depend assign item item batch complet time bin object find cut plan minim weight sum earli tardi penalti address problem via constraint program base heurist cp agent base model heurist ab cp impact base search strategi implement general purpos solver ibm cp optim ab construct build solut repeat negoti set agent repres item set repres bin agent cooper minim weight earli tardi penalti comput investig show cp outperform ab small size instanc opposit prevail larger instanc|"['S. Polyakovskiy', 'A. Makarowsky', ""R. M'Hallah""]"|['cs.DS']
2017-03-28T14:09:34Z|2017-03-21T14:46:39Z|http://arxiv.org/abs/1703.07247v1|http://arxiv.org/pdf/1703.07247v1|A Note on the Tree Augmentation Problem|note tree augment problem|"In the Tree Augmentation problem we are given a tree $T=(V,F)$ and an additional set $E \subseteq V \times V$ of edges, called ""links"", with positive integer costs $\{c_e:e \in E\}$. The goal is to augment $T$ by a minimum cost set of links $J \subseteq E$ such that $T \cup J$ is $2$-edge-connected. Let $M$ denote the maximum cost of a link. Recently, Adjiashvili introduced a novel LP for the problem and used it to break the natural $2$-approximation barrier for instances when $M$ is a constant. Specifically, his algorithm computes a $1.96418+\epsilon$ approximate solution in time $n^{O(M/\epsilon^2)}$. Using a slightly weaker LP we achieve ratio $\frac{12}{7}+\epsilon$ for arbitrary costs and ratio $1.6+\epsilon$ for unit costs in time $2^{O(M/\epsilon^2)}$."|tree augment problem given tree addit set subseteq time edg call link posit integ cost goal augment minimum cost set link subseteq cup edg connect let denot maximum cost link recent adjiashvili introduc novel lp problem use break natur approxim barrier instanc constant specif algorithm comput epsilon approxim solut time epsilon use slight weaker lp achiev ratio frac epsilon arbitrari cost ratio epsilon unit cost time epsilon|['Zeev Nutov']|['cs.DS']
2017-03-28T14:09:34Z|2017-03-21T14:39:35Z|http://arxiv.org/abs/1703.07244v1|http://arxiv.org/pdf/1703.07244v1|A Hybrid Feasibility Constraints-Guided Search to the Two-Dimensional   Bin Packing Problem with Due Dates|hybrid feasibl constraint guid search two dimension bin pack problem due date|The two-dimensional non-oriented bin packing problem with due dates packs a set of rectangular items, which may be rotated by 90 degrees, into identical rectangular bins. The bins have equal processing times. An item's lateness is the difference between its due date and the completion time of its bin. The problem packs all items without overlap as to minimize maximum lateness Lmax.   The paper proposes a tight lower bound that enhances an existing bound on Lmax for 24.07% of the benchmark instances and matches it in 30.87% cases. In addition, it models the problem using mixed integer programming (MIP), and solves small-sized instances exactly using CPLEX. It approximately solves larger-sized instances using a two-stage heuristic. The first stage constructs an initial solution via a first-fit heuristic that applies an iterative constraint programming (CP)-based neighborhood search. The second stage, which is iterative too, approximately solves a series of assignment low-level MIPs that are guided by feasibility constraints. It then enhances the solution via a high-level random local search. The approximate approach improves existing upper bounds by 27.45% on average, and obtains the optimum for 33.93% of the instances. Overall, the exact and approximate approaches identify the optimum for 39.07% cases.   The proposed approach is applicable to complex problems. It applies CP and MIP sequentially, while exploring their advantages, and hybridizes heuristic search with MIP. It embeds a new lookahead strategy that guards against infeasible search directions and constrains the search to improving directions only; thus, differs from traditional lookahead beam searches.|two dimension non orient bin pack problem due date pack set rectangular item may rotat degre ident rectangular bin bin equal process time item late differ due date complet time bin problem pack item without overlap minim maximum late lmax paper propos tight lower bound enhanc exist bound lmax benchmark instanc match case addit model problem use mix integ program mip solv small size instanc exact use cplex approxim solv larger size instanc use two stage heurist first stage construct initi solut via first fit heurist appli iter constraint program cp base neighborhood search second stage iter approxim solv seri assign low level mip guid feasibl constraint enhanc solut via high level random local search approxim approach improv exist upper bound averag obtain optimum instanc overal exact approxim approach identifi optimum case propos approach applic complex problem appli cp mip sequenti explor advantag hybrid heurist search mip emb new lookahead strategi guard infeas search direct constrain search improv direct onli thus differ tradit lookahead beam search|"['S. Polyakovskiy', ""R. M'Hallah""]"|['cs.DS']
2017-03-28T14:09:34Z|2017-03-21T09:37:16Z|http://arxiv.org/abs/1703.07107v1|http://arxiv.org/pdf/1703.07107v1|On the Interplay between Strong Regularity and Graph Densification|interplay strong regular graph densif|In this paper we analyze the practical implications of Szemer\'edi's regularity lemma in the preservation of metric information contained in large graphs. To this end, we present a heuristic algorithm to find regular partitions. Our experiments show that this method is quite robust to the natural sparsification of proximity graphs. In addition, this robustness can be enforced by graph densification.|paper analyz practic implic szemer edi regular lemma preserv metric inform contain larg graph end present heurist algorithm find regular partit experi show method quit robust natur sparsif proxim graph addit robust enforc graph densif|['Marco Fiorucci', 'Alessandro Torcinovich', 'Manuel Curado', 'Francisco Escolano', 'Marcello Pelillo']|['cs.DS', 'cs.CV']
2017-03-28T14:09:34Z|2017-03-20T11:17:39Z|http://arxiv.org/abs/1703.06680v1|http://arxiv.org/pdf/1703.06680v1|Parallel Sort-Based Matching for Data Distribution Management on   Shared-Memory Multiprocessors|parallel sort base match data distribut manag share memori multiprocessor|In this paper we consider the problem of identifying intersections between two sets of d-dimensional axis-parallel rectangles. This is a common problem that arises in many agent-based simulation studies, and is of central importance in the context of High Level Architecture (HLA), where it is at the core of the Data Distribution Management (DDM) service. Several realizations of the DDM service have been proposed; however, many of them are either inefficient or inherently sequential. These are serious limitations since multicore processors are now ubiquitous, and DDM algorithms -- being CPU-intensive -- could benefit from additional computing power. We propose a parallel version of the Sort-Based Matching algorithm for shared-memory multiprocessors. Sort-Based Matching is one of the most efficient serial algorithms for the DDM problem, but is quite difficult to parallelize due to data dependencies. We describe the algorithm and compute its asymptotic running time; we complete the analysis by assessing its performance and scalability through extensive experiments on two commodity multicore systems based on a dual socket Intel Xeon processor, and a single socket Intel Core i7 processor.|paper consid problem identifi intersect two set dimension axi parallel rectangl common problem aris mani agent base simul studi central import context high level architectur hla core data distribut manag ddm servic sever realize ddm servic propos howev mani either ineffici inher sequenti serious limit sinc multicor processor ubiquit ddm algorithm cpu intens could benefit addit comput power propos parallel version sort base match algorithm share memori multiprocessor sort base match one effici serial algorithm ddm problem quit difficult parallel due data depend describ algorithm comput asymptot run time complet analysi assess perform scalabl extens experi two commod multicor system base dual socket intel xeon processor singl socket intel core processor|"['Moreno Marzolla', ""Gabriele D'Angelo""]"|['cs.DC', 'cs.DS', 'cs.MA']
2017-03-28T14:09:34Z|2017-03-20T09:34:51Z|http://arxiv.org/abs/1703.06644v1|http://arxiv.org/pdf/1703.06644v1|Reoptimization of the Closest Substring Problem under Pattern Length   Modification|reoptim closest substr problem pattern length modif|This study investigates whether reoptimization can help in solving the closest substring problem. We are dealing with the following reoptimization scenario. Suppose, we have an optimal l-length closest substring of a given set of sequences S. How can this information be beneficial in obtaining an (l+k)-length closest substring for S? In this study, we show that the problem is still computationally hard even with k=1. We present greedy approximation algorithms that make use of the given information and prove that it has an additive error that grows as the parameter k increases. Furthermore, we present hard instances for each algorithm to show that the computed approximation ratio is tight. We also show that we can slightly improve the running-time of the existing polynomial-time approximation scheme (PTAS) for the original problem through reoptimization.|studi investig whether reoptim help solv closest substr problem deal follow reoptim scenario suppos optim length closest substr given set sequenc inform benefici obtain length closest substr studi show problem still comput hard even present greedi approxim algorithm make use given inform prove addit error grow paramet increas furthermor present hard instanc algorithm show comput approxim ratio tight also show slight improv run time exist polynomi time approxim scheme ptas origin problem reoptim|['Jhoirene B. Clemente', 'Henry N. Adorna']|['cs.DS', '68W25', 'G.2.1']
2017-03-28T14:09:34Z|2017-03-18T18:12:17Z|http://arxiv.org/abs/1703.06327v1|http://arxiv.org/pdf/1703.06327v1|Spectrum Estimation from a Few Entries|spectrum estim entri|Singular values of a data in a matrix form provide insights on the structure of the data, the effective dimensionality, and the choice of hyper-parameters on higher-level data analysis tools. However, in many practical applications such as collaborative filtering and network analysis, we only get a partial observation. Under such scenarios, we consider the fundamental problem of recovering spectral properties of the underlying matrix from a sampling of its entries. We are particularly interested in directly recovering the spectrum, which is the set of singular values, and also in sample-efficient approaches for recovering a spectral sum function, which is an aggregate sum of the same function applied to each of the singular values. We propose first estimating the Schatten $k$-norms of a matrix, and then applying Chebyshev approximation to the spectral sum function or applying moment matching in Wasserstein distance to recover the singular values. The main technical challenge is in accurately estimating the Schatten norms from a sampling of a matrix. We introduce a novel unbiased estimator based on counting small structures in a graph and provide guarantees that match its empirical performance. Our theoretical analysis shows that Schatten norms can be recovered accurately from strictly smaller number of samples compared to what is needed to recover the underlying low-rank matrix. Numerical experiments suggest that we significantly improve upon a competing approach of using matrix completion methods.|singular valu data matrix form provid insight structur data effect dimension choic hyper paramet higher level data analysi tool howev mani practic applic collabor filter network analysi onli get partial observ scenario consid fundament problem recov spectral properti matrix sampl entri particular interest direct recov spectrum set singular valu also sampl effici approach recov spectral sum function aggreg sum function appli singular valu propos first estim schatten norm matrix appli chebyshev approxim spectral sum function appli moment match wasserstein distanc recov singular valu main technic challeng accur estim schatten norm sampl matrix introduc novel unbias estim base count small structur graph provid guarante match empir perform theoret analysi show schatten norm recov accur strict smaller number sampl compar need recov low rank matrix numer experi suggest signific improv upon compet approach use matrix complet method|['Ashish Khetan', 'Sewoong Oh']|['stat.ML', 'cs.DS', 'cs.LG', 'cs.NA']
2017-03-28T14:09:34Z|2017-03-18T17:21:14Z|http://arxiv.org/abs/1703.06320v1|http://arxiv.org/pdf/1703.06320v1|Hardware-Efficient Schemes of Quaternion Multiplying Units for 2D   Discrete Quaternion Fourier Transform Processors|hardwar effici scheme quaternion multipli unit discret quaternion fourier transform processor|In this paper, we offer and discuss three efficient structural solutions for the hardware-oriented implementation of discrete quaternion Fourier transform basic operations with reduced implementation complexities. The first solution: a scheme for calculating sq product, the second solution: a scheme for calculating qt product, and the third solution: a scheme for calculating sqt product, where s is a so-called i-quaternion, t is an j-quaternion, and q is an usual quaternion. The direct multiplication of two usual quaternions requires 16 real multiplications (or two-operand multipliers in the case of fully parallel hardware implementation) and 12 real additions (or binary adders). At the same time, our solutions allow to design the computation units, which consume only 6 multipliers plus 6 two input adders for implementation of sq or qt basic operations and 9 binary multipliers plus 6 two-input adders and 4 four-input adders for implementation of sqt basic operation.|paper offer discuss three effici structur solut hardwar orient implement discret quaternion fourier transform basic oper reduc implement complex first solut scheme calcul sq product second solut scheme calcul qt product third solut scheme calcul sqt product call quaternion quaternion usual quaternion direct multipl two usual quaternion requir real multipl two operand multipli case fulli parallel hardwar implement real addit binari adder time solut allow design comput unit consum onli multipli plus two input adder implement sq qt basic oper binari multipli plus two input adder four input adder implement sqt basic oper|['Aleksandr Cariow', 'Galina Cariowa', 'Marina Chicheva']|['cs.DS', 'cs.AR', '65T50, 15A04, 15A66, 15A66, 15A69, 03D15, 65Y20, 65Y10', 'F.2.1; I.1.2; C.1.4; C.3']
2017-03-28T14:09:34Z|2017-03-18T00:44:38Z|http://arxiv.org/abs/1703.06227v1|http://arxiv.org/pdf/1703.06227v1|Discriminative Distance-Based Network Indices and the Tiny-World   Property|discrimin distanc base network indic tini world properti|Distance-based indices, including closeness centrality, average path length, eccentricity and average eccentricity, are important tools for network analysis. In these indices, the distance between two vertices is measured by the size of shortest paths between them. However, this measure has shortcomings. A well-studied shortcoming is that extending it to disconnected graphs (and also directed graphs) is controversial. The second shortcoming is that when this measure is used in real-world networks, a huge number of vertices may have exactly the same closeness/eccentricity scores. The third shortcoming is that in many applications, the distance between two vertices not only depends on the size of shortest paths, but also on the number of shortest paths between them. In this paper, we develop a new distance measure between vertices of a graph that yields discriminative distance-based centrality indices. This measure is proportional to the size of shortest paths and inversely proportional to the number of shortest paths. We present algorithms for exact computation of the proposed discriminative indices. We then develop randomized algorithms that precisely estimate average discriminative path length and average discriminative eccentricity and show that they give $(\epsilon,\delta)$-approximations of these indices. Finally, we preform extensive experiments over several real-world networks from different domains and show that compared to the traditional indices, discriminative indices have usually much more discriminability. Our experiments reveal that real-world networks have usually a tiny average discriminative path length, bounded by a constant (e.g., 2). We refer to this property as the tiny-world property.|distanc base indic includ close central averag path length eccentr averag eccentr import tool network analysi indic distanc two vertic measur size shortest path howev measur shortcom well studi shortcom extend disconnect graph also direct graph controversi second shortcom measur use real world network huge number vertic may exact close eccentr score third shortcom mani applic distanc two vertic onli depend size shortest path also number shortest path paper develop new distanc measur vertic graph yield discrimin distanc base central indic measur proport size shortest path invers proport number shortest path present algorithm exact comput propos discrimin indic develop random algorithm precis estim averag discrimin path length averag discrimin eccentr show give epsilon delta approxim indic final preform extens experi sever real world network differ domain show compar tradit indic discrimin indic usual much discrimin experi reveal real world network usual tini averag discrimin path length bound constant refer properti tini world properti|['Mostafa Haghir Chehreghani', 'Albert Bifet', 'Talel Abdessalem']|['cs.DS', 'cs.SI']
2017-03-28T14:09:34Z|2017-03-17T17:31:01Z|http://arxiv.org/abs/1703.06733v1|http://arxiv.org/pdf/1703.06733v1|Discovering Relaxed Sound Workflow Nets using Integer Linear Programming|discov relax sound workflow net use integ linear program|Process mining is concerned with the analysis, understanding and improvement of business processes. Process discovery, i.e. discovering a process model based on an event log, is considered the most challenging process mining task. State-of-the-art process discovery algorithms only discover local control-flow patterns and are unable to discover complex, non-local patterns. Region theory based techniques, i.e. an established class of process discovery techniques, do allow for discovering such patterns. However, applying region theory directly results in complex, over-fitting models, which is less desirable. Moreover, region theory does not cope with guarantees provided by state-of-the-art process discovery algorithms, both w.r.t. structural and behavioural properties of the discovered process models. In this paper we present an ILP-based process discovery approach, based on region theory, that guarantees to discover relaxed sound workflow nets. Moreover, we devise a filtering algorithm, based on the internal working of the ILP-formulation, that is able to cope with the presence of infrequent behaviour. We have extensively evaluated the technique using different event logs with different levels of exceptional behaviour. Our experiments show that the presented approach allow us to leverage the inherent shortcomings of existing region-based approaches. The techniques presented are implemented and readily available in the HybridILPMiner package in the open-source process mining tool-kits ProM and RapidProM.|process mine concern analysi understand improv busi process process discoveri discov process model base event log consid challeng process mine task state art process discoveri algorithm onli discov local control flow pattern unabl discov complex non local pattern region theori base techniqu establish class process discoveri techniqu allow discov pattern howev appli region theori direct result complex fit model less desir moreov region theori doe cope guarante provid state art process discoveri algorithm structur behaviour properti discov process model paper present ilp base process discoveri approach base region theori guarante discov relax sound workflow net moreov devis filter algorithm base intern work ilp formul abl cope presenc infrequ behaviour extens evalu techniqu use differ event log differ level except behaviour experi show present approach allow us leverag inher shortcom exist region base approach techniqu present implement readili avail hybridilpmin packag open sourc process mine tool kit prom rapidprom|['S. J. van Zelst', 'B. F. van Dongen', 'W. M. P. van der Aalst', 'H. M. W. Verbeek']|['cs.DS']
2017-03-28T14:09:38Z|2017-03-17T16:18:31Z|http://arxiv.org/abs/1703.06074v1|http://arxiv.org/pdf/1703.06074v1|Robust Assignments with Vulnerable Nodes|robust assign vulner node|Various real-life planning problems require making upfront decisions before all parameters of the problem have been disclosed. An important special case of such problem especially arises in scheduling and staff rostering problems, where a set of tasks needs to be assigned to an available set of resources (personnel or machines), in a way that each task is assigned to one resource, while no task is allowed to share a resource with another task. In its nominal form, the resulting computational problem reduces to the well-known assignment problem that can be modeled as matching problems on bipartite graphs.   In recent work \cite{adjiashvili_bindewald_michaels_icalp2016}, a new robust model for the assignment problem was introduced that can deal with situations in which certain resources, i.e.\ nodes or edges of the underlying bipartite graph, are vulnerable and may become unavailable after a solution has been chosen. In the original version from \cite{adjiashvili_bindewald_michaels_icalp2016} the resources subject to uncertainty are the edges of the underlying bipartite graph.   In this follow-up work, we complement our previous study by considering nodes as being vulnerable, instead of edges. The goal is now to choose a minimum-cost collection of nodes such that, if any vulnerable node becomes unavailable, the remaining part of the solution still contains sufficient nodes to perform all tasks. From a practical point of view, such type of unavailability is interesting as it is typically caused e.g.\ by an employee's sickness, or machine failure. We present algorithms and hardness of approximation results for several variants of the problem.|various real life plan problem requir make upfront decis befor paramet problem disclos import special case problem especi aris schedul staff roster problem set task need assign avail set resourc personnel machin way task assign one resourc task allow share resourc anoth task nomin form result comput problem reduc well known assign problem model match problem bipartit graph recent work cite adjiashvili bindewald michael icalp new robust model assign problem introduc deal situat certain resourc node edg bipartit graph vulner may becom unavail solut chosen origin version cite adjiashvili bindewald michael icalp resourc subject uncertainti edg bipartit graph follow work complement previous studi consid node vulner instead edg goal choos minimum cost collect node ani vulner node becom unavail remain part solut still contain suffici node perform task practic point view type unavail interest typic caus employe sick machin failur present algorithm hard approxim result sever variant problem|['David Adjiashvili', 'Viktor Bindewald', 'Dennis Michaels']|['cs.DS', 'cs.DM', '90C27', 'I.1.2; G.2.2; G.1.6']
2017-03-28T14:09:38Z|2017-03-17T16:08:23Z|http://arxiv.org/abs/1703.06065v1|http://arxiv.org/pdf/1703.06065v1|Block CUR : Decomposing Large Distributed Matrices|block cur decompos larg distribut matric|A common problem in large-scale data analysis is to approximate a matrix using a combination of specifically sampled rows and columns, known as CUR decomposition. Unfortunately, in many real-world environments, the ability to sample specific individual rows or columns of the matrix is limited by either system constraints or cost. In this paper, we consider matrix approximation by sampling predefined blocks of columns (or rows) from the matrix. This regime is commonly found when data is distributed across multiple nodes in a compute cluster, where such blocks correspond to columns (or rows) of the matrix stored on the same node, which can be retrieved with much less overhead than retrieving individual columns stored across different nodes. We propose a novel algorithm for sampling useful column blocks and provide guarantees for the quality of the approximation. We demonstrate the practical utility of this algorithm for computing the block CUR decomposition of large matrices in a distributed setting using Apache Spark. Using our proposed block CUR algorithms, we can achieve a significant speed-up compared to a regular CUR decomposition with the same quality of approximation.|common problem larg scale data analysi approxim matrix use combin specif sampl row column known cur decomposit unfortun mani real world environ abil sampl specif individu row column matrix limit either system constraint cost paper consid matrix approxim sampl predefin block column row matrix regim common found data distribut across multipl node comput cluster block correspond column row matrix store node retriev much less overhead retriev individu column store across differ node propos novel algorithm sampl use column block provid guarante qualiti approxim demonstr practic util algorithm comput block cur decomposit larg matric distribut set use apach spark use propos block cur algorithm achiev signific speed compar regular cur decomposit qualiti approxim|['Urvashi Oswal', 'Swayambhoo Jain', 'Kevin S. Xu', 'Brian Eriksson']|['stat.ML', 'cs.DC', 'cs.DS', 'cs.LG']
2017-03-28T14:09:38Z|2017-03-17T15:56:50Z|http://arxiv.org/abs/1703.06061v1|http://arxiv.org/pdf/1703.06061v1|Approximation ratio of RePair|approxim ratio repair|In a seminal paper of Charikar et al.~on the smallest grammar problem, the authors derive upper and lower bounds on the approximation ratios for several grammar-based compressors. Here we improve the lower bound for the famous {\sf RePair} algorithm from $\Omega(\sqrt{\log n})$ to $\Omega(\log n/\log\log n)$. The family of words used in our proof is defined over a binary alphabet, while the lower bound from Charikar et al. needs an alphabet of logarithmic size in the length of the provided words.|semin paper charikar et al smallest grammar problem author deriv upper lower bound approxim ratio sever grammar base compressor improv lower bound famous sf repair algorithm omega sqrt log omega log log log famili word use proof defin binari alphabet lower bound charikar et al need alphabet logarithm size length provid word|['Danny Hucke', 'Artur Jez', 'Markus Lohrey']|['cs.DS', 'F.2.2, E.4']
2017-03-28T14:09:38Z|2017-03-21T12:29:29Z|http://arxiv.org/abs/1703.06053v2|http://arxiv.org/pdf/1703.06053v2|Fast Non-Monotone Submodular Maximisation Subject to a Matroid   Constraint|fast non monoton submodular maximis subject matroid constraint|In this work we present the first practical $\left(\frac{1}{e}-\epsilon\right)$-approximation algorithm to maximise a general non-negative submodular function subject to a matroid constraint. Our algorithm is based on combining the decreasing-threshold procedure of Badanidiyuru and Vondrak (SODA 2014) with a smoother version of the measured continuous greedy algorithm of Feldman et al. (FOCS 2011). This enables us to obtain an algorithm that requires $O(\frac{nr^2}{\epsilon^4} \big(\frac{a+b}{a}\big)^2 \log^2({\frac{n}{\epsilon}}))$ value oracle calls, where $n$ is the cardinality of the ground set, $r$ is the matroid rank, and $ b, a \in \mathbb{R}^+$ are the absolute values of the minimum and maximum marginal values that the function $f$ can take i.e.: $ -b \leq f_S(i) \leq a$, for all $i\in E$ and $S\subseteq E$, (here, $E$ is the ground set). The additional value oracle calls with respect to the work of Badanidiyuru and Vondrak come from the greater spread in the sampling of the multilinear extension that the possibility of negative marginal values introduce.|work present first practic left frac epsilon right approxim algorithm maximis general non negat submodular function subject matroid constraint algorithm base combin decreas threshold procedur badanidiyuru vondrak soda smoother version measur continu greedi algorithm feldman et al foc enabl us obtain algorithm requir frac nr epsilon big frac big log frac epsilon valu oracl call cardin ground set matroid rank mathbb absolut valu minimum maximum margin valu function take leq leq subseteq ground set addit valu oracl call respect work badanidiyuru vondrak come greater spread sampl multilinear extens possibl negat margin valu introduc|['Pau Segui-Gasco', 'Hyo-Sang Shin']|['cs.DS']
2017-03-28T14:09:38Z|2017-03-17T15:08:17Z|http://arxiv.org/abs/1703.06048v1|http://arxiv.org/pdf/1703.06048v1|An FPTAS for the Knapsack Problem with Parametric Weights|fptas knapsack problem parametr weight|In this paper, we investigate the parametric weight knapsack problem, in which the item weights are affine functions of the form $w_i(\lambda) = a_i + \lambda \cdot b_i$ for $i \in \{1,\ldots,n\}$ depending on a real-valued parameter $\lambda$. The aim is to provide a solution for all values of the parameter. It is well-known that any exact algorithm for the problem may need to output an exponential number of knapsack solutions. We present the first fully polynomial-time approximation scheme (FPTAS) for the problem that, for any desired precision $\varepsilon \in (0,1)$, computes $(1-\varepsilon)$-approximate solutions for all values of the parameter. Our FPTAS is based on two different approaches and achieves a running time of $\mathcal{O}(n^3/\varepsilon^2 \cdot \min\{ \log^2 P, n^2 \} \cdot \min\{\log M, n \log (n/\varepsilon) / \log(n \log (n/\varepsilon) )\})$ where $P$ is an upper bound on the optimal profit and $M := \max\{W, n \cdot \max\{a_i,b_i: i \in \{1,\ldots,n\}\}\}$ for a knapsack with capacity $W$.|paper investig parametr weight knapsack problem item weight affin function form lambda lambda cdot ldot depend real valu paramet lambda aim provid solut valu paramet well known ani exact algorithm problem may need output exponenti number knapsack solut present first fulli polynomi time approxim scheme fptas problem ani desir precis varepsilon comput varepsilon approxim solut valu paramet fptas base two differ approach achiev run time mathcal varepsilon cdot min log cdot min log log varepsilon log log varepsilon upper bound optim profit max cdot max ldot knapsack capac|['Michael Holzhauser', 'Sven O. Krumke']|['cs.DS', 'cs.CC', 'math.OC']
2017-03-28T14:09:38Z|2017-03-17T14:53:55Z|http://arxiv.org/abs/1703.06040v1|http://arxiv.org/pdf/1703.06040v1|Towards a Topology-Shape-Metrics Framework for Ortho-Radial Drawings|toward topolog shape metric framework ortho radial draw|Ortho-Radial drawings are a generalization of orthogonal drawings to grids that are formed by concentric circles and straight-line spokes emanating from the circles' center. Such drawings have applications in schematic graph layouts, e.g., for metro maps and destination maps.   A plane graph is a planar graph with a fixed planar embedding. We give a combinatorial characterization of the plane graphs that admit a planar ortho-radial drawing without bends. Previously, such a characterization was only known for paths, cycles, and theta graphs, and in the special case of rectangular drawings for cubic graphs, where the contour of each face is required to be a rectangle.   The characterization is expressed in terms of an ortho-radial representation that, similar to Tamassia's orthogonal representations for orthogonal drawings describes such a drawing combinatorially in terms of angles around vertices and bends on the edges. In this sense our characterization can be seen as a first step towards generalizing the Topology-Shape-Metrics framework of Tamassia to ortho-radial drawings.|ortho radial draw general orthogon draw grid form concentr circl straight line spoke eman circl center draw applic schemat graph layout metro map destin map plane graph planar graph fix planar embed give combinatori character plane graph admit planar ortho radial draw without bend previous character onli known path cycl theta graph special case rectangular draw cubic graph contour face requir rectangl character express term ortho radial represent similar tamassia orthogon represent orthogon draw describ draw combinatori term angl around vertic bend edg sens character seen first step toward general topolog shape metric framework tamassia ortho radial draw|['Lukas Barth', 'Benjamin Niedermann', 'Ignaz Rutter', 'Matthias Wolf']|['cs.DM', 'cs.DS']
2017-03-28T14:09:38Z|2017-03-17T12:57:18Z|http://arxiv.org/abs/1703.05997v1|http://arxiv.org/pdf/1703.05997v1|Connection Scan Algorithm|connect scan algorithm|We introduce the Connection Scan Algorithm (CSA) to efficiently answer queries to timetable information systems. The input consists, in the simplest setting, of a source position and a desired target position. The output consist is a sequence of vehicles such as trains or buses that a traveler should take to get from the source to the target. We study several problem variations such as the earliest arrival and profile problems. We present algorithm variants that only optimize the arrival time or additionally optimize the number of transfers in the Pareto sense. An advantage of CSA is that is can easily adjust to changes in the timetable, allowing the easy incorporation of known vehicle delays. We additionally introduce the Minimum Expected Arrival Time (MEAT) problem to handle possible, uncertain, future vehicle delays. We present a solution to the MEAT problem that is based upon CSA. Finally, we extend CSA using the multilevel overlay paradigm to answer complex queries on nation-wide integrated timetables with trains and buses.|introduc connect scan algorithm csa effici answer queri timet inform system input consist simplest set sourc posit desir target posit output consist sequenc vehicl train buse travel take get sourc target studi sever problem variat earliest arriv profil problem present algorithm variant onli optim arriv time addit optim number transfer pareto sens advantag csa easili adjust chang timet allow easi incorpor known vehicl delay addit introduc minimum expect arriv time meat problem handl possibl uncertain futur vehicl delay present solut meat problem base upon csa final extend csa use multilevel overlay paradigm answer complex queri nation wide integr timet train buse|['Julian Dibbelt', 'Thomas Pajor', 'Ben Strasser', 'Dorothea Wagner']|['cs.DS']
2017-03-28T14:09:38Z|2017-03-16T13:10:29Z|http://arxiv.org/abs/1703.05598v1|http://arxiv.org/pdf/1703.05598v1|Linear-Time Algorithm for Maximum-Cardinality Matching on   Cocomparability Graphs|linear time algorithm maximum cardin match cocompar graph|Finding maximum-cardinality matchings in undirected graphs is arguably one of the most central graph problems. For general m-edge and n-vertex graphs, it is well-known to be solvable in $O(m \sqrt{n})$ time. We develop the first linear-time algorithm to find maximum-cardinality matchings on cocomparability graphs, a prominent subclass of perfect graphs that contains interval graphs as well as permutation graphs. Our algorithm is based on the recently discovered Lexicographic Depth First Search (LDFS).|find maximum cardin match undirect graph arguabl one central graph problem general edg vertex graph well known solvabl sqrt time develop first linear time algorithm find maximum cardin match cocompar graph promin subclass perfect graph contain interv graph well permut graph algorithm base recent discov lexicograph depth first search ldfs|['George B. Mertzios', 'André Nichterlein', 'Rolf Niedermeier']|['cs.DS', 'F.2.2']
2017-03-28T14:09:38Z|2017-03-16T11:39:22Z|http://arxiv.org/abs/1703.05568v1|http://arxiv.org/pdf/1703.05568v1|Quantum Spectral Clustering through a Biased Phase Estimation Algorithm|quantum spectral cluster bias phase estim algorithm|In this brief paper, we go through the theoretical steps of the spectral clustering on quantum computers by employing the phase estimation and the amplitude amplification algorithms. To speed-up the amplitude amplification, we introduce a biased version of the phase estimation algorithm. In addition, when the circuit representation of a data matrix of order $N$ is produced through an ancilla based circuit in which the matrix is written as a sum of $L$ number of Householder matrices; we show that the computational complexity of the whole process is bound by $O(c2^mLN)$ number of quantum gates. Here, $m$ represents the number of qubits involved in the phase register of the phase estimation algorithm and $c$ represents the number of trials done to find the best clustering.|brief paper go theoret step spectral cluster quantum comput employ phase estim amplitud amplif algorithm speed amplitud amplif introduc bias version phase estim algorithm addit circuit represent data matrix order produc ancilla base circuit matrix written sum number household matric show comput complex whole process bound mln number quantum gate repres number qubit involv phase regist phase estim algorithm repres number trial done find best cluster|['Ammar Daskin']|['quant-ph', 'cs.DS']
2017-03-28T14:09:38Z|2017-03-16T11:09:25Z|http://arxiv.org/abs/1703.05559v1|http://arxiv.org/pdf/1703.05559v1|Improving TSP tours using dynamic programming over tree decomposition|improv tsp tour use dynam program tree decomposit|Given a traveling salesman problem (TSP) tour $H$ in graph $G$ a $k$-move is an operation which removes $k$ edges from $H$, and adds $k$ edges of $G$ so that a new tour $H'$ is formed. The popular $k$-OPT heuristics for TSP finds a local optimum by starting from an arbitrary tour $H$ and then improving it by a sequence of $k$-moves.   Until 2016, the only known algorithm to find an improving $k$-move for a given tour was the naive solution in time $O(n^k)$. At ICALP'16 de Berg, Buchin, Jansen and Woeginger showed an $O(n^{\lfloor 2/3k \rfloor+1})$-time algorithm.   We show an algorithm which runs in $O(n^{(1/4+\epsilon_k)k})$ time, where $\lim \epsilon_k = 0$. We are able to show that it improves over the state of the art for every $k=5,\ldots,10$. For the most practically relevant case $k=5$ we provide a slightly refined algorithm running in $O(n^{3.4})$ time. We also show that for the $k=4$ case, improving over the $O(n^3)$-time algorithm of de Berg et al. would be a major breakthrough: an $O(n^{3-\epsilon})$-time algorithm for any $\epsilon>0$ would imply an $O(n^{3-\delta})$-time algorithm for the ALL PAIRS SHORTEST PATHS problem, for some $\delta>0$.|given travel salesman problem tsp tour graph move oper remov edg add edg new tour form popular opt heurist tsp find local optimum start arbitrari tour improv sequenc move onli known algorithm find improv move given tour naiv solut time icalp de berg buchin jansen woeging show lfloor rfloor time algorithm show algorithm run epsilon time lim epsilon abl show improv state art everi ldot practic relev case provid slight refin algorithm run time also show case improv time algorithm de berg et al would major breakthrough epsilon time algorithm ani epsilon would impli delta time algorithm pair shortest path problem delta|['Marek Cygan', 'Lukasz Kowalik', 'Arkadiusz Socala']|['cs.DS']
2017-03-28T14:09:43Z|2017-03-16T08:52:21Z|http://arxiv.org/abs/1703.05509v1|http://arxiv.org/pdf/1703.05509v1|VieM v1.00 -- Vienna Mapping and Sparse Quadratic Assignment User Guide|viem vienna map spars quadrat assign user guid|This paper severs as a user guide to the mapping framework VieM (Vienna Mapping and Sparse Quadratic Assignment). We give a rough overview of the techniques used within the framework and describe the user interface as well as the file formats used.|paper sever user guid map framework viem vienna map spars quadrat assign give rough overview techniqu use within framework describ user interfac well file format use|['Christian Schulz', 'Jesper Larsson Träff']|['cs.DC', 'cs.DS', 'math.CO']
2017-03-28T14:09:43Z|2017-03-16T08:10:30Z|http://arxiv.org/abs/1703.05496v1|http://arxiv.org/pdf/1703.05496v1|Data Delivery by Mobile Agents with Energy Constraints over a fixed path|data deliveri mobil agent energi constraint fix path|We consider $k$ mobile agents of limited energy that are initially located at vertices of an edge-weighted graph $G$ and have to collectively deliver data from a source vertex $s$ to a target vertex $t$. The data are to be collected by an agent reaching $s$, who can carry and then hand them over another agent etc., until some agent with the data reaches $t$. The data can be carried only over a fixed $s-t$ path of $G$; each agent has an initial energy budget and each time it passes an edge, it consumes the edge's weights in energy units and stalls if its energy is not anymore sufficient to move. The main result of this paper is a 3-approximation polynomial time algorithm for the data delivery problem over a fixed $s-t$ path in the graph, for identical initial energy budgets and at most one allowed data hand-over per agent.|consid mobil agent limit energi initi locat vertic edg weight graph collect deliv data sourc vertex target vertex data collect agent reach carri hand anoth agent etc agent data reach data carri onli fix path agent initi energi budget time pass edg consum edg weight energi unit stall energi anymor suffici move main result paper approxim polynomi time algorithm data deliveri problem fix path graph ident initi energi budget one allow data hand per agent|['Aristotelis Giannakos', 'Mhand Hifi', 'Gregory Karagiorgos']|['cs.DS']
2017-03-28T14:09:43Z|2017-03-15T23:02:07Z|http://arxiv.org/abs/1703.05418v1|http://arxiv.org/pdf/1703.05418v1|A Local Algorithm for the Sparse Spanning Graph Problem|local algorithm spars span graph problem|Constructing a sparse \emph{spanning subgraph} is a fundamental primitive in graph theory. In this paper, we study this problem in the Centralized Local model, where the goal is to decide whether an edge is part of the spanning subgraph by examining only a small part of the input; yet, answers must be globally consistent and independent of prior queries.   Unfortunately, maximally sparse spanning subgraphs, i.e., spanning trees, cannot be constructed efficiently in this model. Therefore, we settle for a spanning subgraph containing at most $(1+\varepsilon)n$ edges (where $n$ is the number of vertices and $\varepsilon$ is a given approximation/sparsity parameter). We achieve query complexity of $\tilde{O}(poly(\Delta/\varepsilon)n^{2/3})$,\footnote{$\tilde{O}$-notation hides polylogarithmic factors in $n$.} where $\Delta$ is the maximum degree of the input graph. Our algorithm is the first to do so on arbitrary graphs. Moreover, we achieve the additional property that our algorithm outputs a \emph{spanner,} i.e., distances are approximately preserved. With high probability, for each deleted edge there is a path of $O(poly(\Delta/\varepsilon)\log^2 n)$ hops in the output that connects its endpoints.|construct spars emph span subgraph fundament primit graph theori paper studi problem central local model goal decid whether edg part span subgraph examin onli small part input yet answer must global consist independ prior queri unfortun maxim spars span subgraph span tree cannot construct effici model therefor settl span subgraph contain varepsilon edg number vertic varepsilon given approxim sparsiti paramet achiev queri complex tild poli delta varepsilon footnot tild notat hide polylogarithm factor delta maximum degre input graph algorithm first arbitrari graph moreov achiev addit properti algorithm output emph spanner distanc approxim preserv high probabl delet edg path poli delta varepsilon log hop output connect endpoint|['Christoph Lenzen', 'Reut Levi']|['cs.DS']
2017-03-28T14:09:43Z|2017-03-15T15:10:16Z|http://arxiv.org/abs/1703.05199v1|http://arxiv.org/pdf/1703.05199v1|Optimal Unateness Testers for Real-Valued Functions: Adaptivity Helps|optim unat tester real valu function adapt help|We study the problem of testing unateness of functions $f:\{0,1\}^d \to \mathbb{R}.$ We give a $O(\frac{d}{\epsilon} \cdot \log\frac{d}{\epsilon})$-query nonadaptive tester and a $O(\frac{d}{\epsilon})$-query adaptive tester and show that both testers are optimal for a fixed distance parameter $\epsilon$. Previously known unateness testers worked only for Boolean functions, and their query complexity had worse dependence on the dimension both for the adaptive and the nonadaptive case. Moreover, no lower bounds for testing unateness were known. We also generalize our results to obtain optimal unateness testers for functions $f:[n]^d \to \mathbb{R}$.   Our results establish that adaptivity helps with testing unateness of real-valued functions on domains of the form $\{0,1\}^d$ and, more generally, $[n]^d$. This stands in contrast to the situation for monotonicity testing where there is no adaptivity gap for functions $f:[n]^d \to \mathbb{R}$.|studi problem test unat function mathbb give frac epsilon cdot log frac epsilon queri nonadapt tester frac epsilon queri adapt tester show tester optim fix distanc paramet epsilon previous known unat tester work onli boolean function queri complex wors depend dimens adapt nonadapt case moreov lower bound test unat known also general result obtain optim unat tester function mathbb result establish adapt help test unat real valu function domain form general stand contrast situat monoton test adapt gap function mathbb|['Roksana Baleshzar', 'Deeparnab Chakrabarty', 'Ramesh Krishnan S. Pallavoor', 'Sofya Raskhodnikova', 'C. Seshadhri']|['cs.DS', 'cs.DM']
2017-03-28T14:09:43Z|2017-03-15T14:01:21Z|http://arxiv.org/abs/1703.05160v1|http://arxiv.org/pdf/1703.05160v1|A New Unbiased and Efficient Class of LSH-Based Samplers and Estimators   for Partition Function Computation in Log-Linear Models|new unbias effici class lsh base sampler estim partit function comput log linear model|"Log-linear models are arguably the most successful class of graphical models for large-scale applications because of their simplicity and tractability. Learning and inference with these models require calculating the partition function, which is a major bottleneck and intractable for large state spaces. Importance Sampling (IS) and MCMC-based approaches are lucrative. However, the condition of having a ""good"" proposal distribution is often not satisfied in practice.   In this paper, we add a new dimension to efficient estimation via sampling. We propose a new sampling scheme and an unbiased estimator that estimates the partition function accurately in sub-linear time. Our samples are generated in near-constant time using locality sensitive hashing (LSH), and so are correlated and unnormalized. We demonstrate the effectiveness of our proposed approach by comparing the accuracy and speed of estimating the partition function against other state-of-the-art estimation techniques including IS and the efficient variant of Gumbel-Max sampling. With our efficient sampling scheme, we accurately train real-world language models using only 1-2% of computations."|log linear model arguabl success class graphic model larg scale applic becaus simplic tractabl learn infer model requir calcul partit function major bottleneck intract larg state space import sampl mcmc base approach lucrat howev condit good propos distribut often satisfi practic paper add new dimens effici estim via sampl propos new sampl scheme unbias estim estim partit function accur sub linear time sampl generat near constant time use local sensit hash lsh correl unnorm demonstr effect propos approach compar accuraci speed estim partit function state art estim techniqu includ effici variant gumbel max sampl effici sampl scheme accur train real world languag model use onli comput|['Ryan Spring', 'Anshumali Shrivastava']|['stat.ML', 'cs.DB', 'cs.DS', 'cs.LG']
2017-03-28T14:09:43Z|2017-03-15T13:51:23Z|http://arxiv.org/abs/1703.05156v1|http://arxiv.org/pdf/1703.05156v1|Complexity Dichotomies for the Minimum F-Overlay Problem|complex dichotomi minimum overlay problem|For a (possibly infinite) fixed family of graphs F, we say that a graph G overlays F on a hypergraph H if V(H) is equal to V(G) and the subgraph of G induced by every hyperedge of H contains some member of F as a spanning subgraph.While it is easy to see that the complete graph on  V(H)  overlays F on a hypergraph H whenever the problem admits a solution, the Minimum F-Overlay problem asks for such a graph with the minimum number of edges.This problem allows to generalize some natural problems which may arise in practice. For instance, if the family F contains all connected graphs, then Minimum F-Overlay corresponds to the Minimum Connectivity Inference problem (also known as Subset Interconnection Design problem) introduced for the low-resolution reconstruction of macro-molecular assembly in structural biology, or for the design of networks.Our main contribution is a strong dichotomy result regarding the polynomial vs. NP-hard status with respect to the considered family F. Roughly speaking, we show that the easy cases one can think of (e.g. when edgeless graphs of the right sizes are in F, or if F contains only cliques) are the only families giving rise to a polynomial problem: all others are NP-complete.We then investigate the parameterized complexity of the problem and give similar sufficient conditions on F that give rise to W[1]-hard, W[2]-hard or FPT problems when the parameter is the size of the solution.This yields an FPT/W[1]-hard dichotomy for a relaxed problem, where every hyperedge of H must contain some member of F as a (non necessarily spanning) subgraph.|possibl infinit fix famili graph say graph overlay hypergraph equal subgraph induc everi hyperedg contain member span subgraph easi see complet graph overlay hypergraph whenev problem admit solut minimum overlay problem ask graph minimum number edg problem allow general natur problem may aris practic instanc famili contain connect graph minimum overlay correspond minimum connect infer problem also known subset interconnect design problem introduc low resolut reconstruct macro molecular assembl structur biolog design network main contribut strong dichotomi result regard polynomi vs np hard status respect consid famili rough speak show easi case one think edgeless graph right size contain onli cliqu onli famili give rise polynomi problem np complet investig parameter complex problem give similar suffici condit give rise hard hard fpt problem paramet size solut yield fpt hard dichotomi relax problem everi hyperedg must contain member non necessarili span subgraph|['Nathann Cohen', 'Frédéric Havet', 'Dorian Mazauric', 'Ignasi Sau', 'Rémi Watrigant']|['cs.DS', 'cs.CC']
2017-03-28T14:09:43Z|2017-03-15T12:06:53Z|http://arxiv.org/abs/1703.05102v1|http://arxiv.org/pdf/1703.05102v1|Algorithms for outerplanar graph roots and graph roots of pathwidth at   most 2|algorithm outerplanar graph root graph root pathwidth|Deciding whether a given graph has a square root is a classical problem that has been studied extensively both from graph theoretic and from algorithmic perspectives. The problem is NP-complete in general, and consequently substantial effort has been dedicated to deciding whether a given graph has a square root that belongs to a particular graph class. There are both polynomial-time solvable and NP-complete cases, depending on the graph class. We contribute with new results in this direction. Given an arbitrary input graph G, we give polynomial-time algorithms to decide whether G has an outerplanar square root, and whether G has a square root that is of pathwidth at most 2.|decid whether given graph squar root classic problem studi extens graph theoret algorithm perspect problem np complet general consequ substanti effort dedic decid whether given graph squar root belong particular graph class polynomi time solvabl np complet case depend graph class contribut new result direct given arbitrari input graph give polynomi time algorithm decid whether outerplanar squar root whether squar root pathwidth|['Petr A. Golovach', 'Pinar Heggernes', 'Dieter Kratsch', 'Paloma T. Lima', 'Daniel Paulusma']|['cs.DS', 'cs.DM']
2017-03-28T14:09:43Z|2017-03-15T11:57:53Z|http://arxiv.org/abs/1703.05097v1|http://arxiv.org/pdf/1703.05097v1|A cubic-time algorithm for computing the trinet distance between level-1   networks|cubic time algorithm comput trinet distanc level network|In evolutionary biology, phylogenetic networks are constructed to represent the evolution of species in which reticulate events are thought to have occurred, such as recombination and hybridization. It is therefore useful to have efficiently computable metrics with which to systematically compare such networks. Through developing an optimal algorithm to enumerate all trinets displayed by a level-1 network (a type of network that is slightly more general than an evolutionary tree), here we propose a cubic-time algorithm to compute the trinet distance between two level-1 networks. Employing simulations, we also present a comparison between the trinet metric and the so-called Robinson-Foulds phylogenetic network metric restricted to level-1 networks. The algorithms described in this paper have been implemented in JAVA and are freely available at https://www.uea.ac.uk/computing/TriLoNet.|evolutionari biolog phylogenet network construct repres evolut speci reticul event thought occur recombin hybrid therefor use effici comput metric systemat compar network develop optim algorithm enumer trinet display level network type network slight general evolutionari tree propos cubic time algorithm comput trinet distanc two level network employ simul also present comparison trinet metric call robinson fould phylogenet network metric restrict level network algorithm describ paper implement java freeli avail https www uea ac uk comput trilonet|['Vincent Moulton', 'James Oldman', 'Taoyang Wu']|['q-bio.PE', 'cs.DM', 'cs.DS']
2017-03-28T14:09:43Z|2017-03-15T06:21:59Z|http://arxiv.org/abs/1703.04954v1|http://arxiv.org/pdf/1703.04954v1|Faster STR-IC-LCS computation via RLE|faster str ic lcs comput via rle|The constrained LCS problem asks one to find a longest common subsequence of two input strings $A$ and $B$ with some constraints. The STR-IC-LCS problem is a variant of the constrained LCS problem, where the solution must include a given constraint string $C$ as a substring. Given two strings $A$ and $B$ of respective lengths $M$ and $N$, and a constraint string $C$ of length at most $\min\{M, N\}$, the best known algorithm for the STR-IC-LCS problem, proposed by Deorowicz~({\em Inf. Process. Lett.}, 11:423--426, 2012), runs in $O(MN)$ time. In this work, we present an $O(mN + nM)$-time solution to the STR-IC-LCS problem, where $m$ and $n$ denote the sizes of the run-length encodings of $A$ and $B$, respectively. Since $m \leq M$ and $n \leq N$ always hold, our algorithm is always as fast as Deorowicz's algorithm, and is faster when input strings are compressible via RLE.|constrain lcs problem ask one find longest common subsequ two input string constraint str ic lcs problem variant constrain lcs problem solut must includ given constraint string substr given two string respect length constraint string length min best known algorithm str ic lcs problem propos deorowicz em inf process lett run mn time work present mn nm time solut str ic lcs problem denot size run length encod respect sinc leq leq alway hold algorithm alway fast deorowicz algorithm faster input string compress via rle|['Keita Kuboi', 'Yuta Fujishige', 'Shunsuke Inenaga', 'Hideo Bannai', 'Masayuki Takeda']|['cs.DS']
2017-03-28T14:09:43Z|2017-03-14T23:06:33Z|http://arxiv.org/abs/1703.04814v1|http://arxiv.org/pdf/1703.04814v1|Near-Optimal Compression for the Planar Graph Metric|near optim compress planar graph metric|"The Planar Graph Metric Compression Problem is to compactly encode the distances among $k$ nodes in a planar graph of size $n$. Two na\""ive solutions are to store the graph using $O(n)$ bits, or to explicitly store the distance matrix with $O(k^2 \log{n})$ bits. The only lower bounds are from the seminal work of Gavoille, Peleg, Prennes, and Raz [SODA'01], who rule out compressions into a polynomially smaller number of bits, for {\em weighted} planar graphs, but leave a large gap for unweighted planar graphs. For example, when $k=\sqrt{n}$, the upper bound is $O(n)$ and their constructions imply an $\Omega(n^{3/4})$ lower bound. This gap is directly related to other major open questions in labelling schemes, dynamic algorithms, and compact routing.   Our main result is a new compression of the planar graph metric into $\tilde{O}(\min (k^2 , \sqrt{k\cdot n}))$ bits, which is optimal up to log factors. Our data structure breaks an $\Omega(k^2)$ lower bound of Krauthgamer, Nguyen, and Zondiner [SICOMP'14] for compression using minors, and the lower bound of Gavoille et al. for compression of weighted planar graphs. This is an unexpected and decisive proof that weights can make planar graphs inherently more complex. Moreover, we design a new {\em Subset Distance Oracle} for planar graphs with $\tilde O(\sqrt{k\cdot n})$ space, and $\tilde O(n^{3/4})$ query time.   Our work carries strong messages to related fields. In particular, the famous $O(n^{1/2})$ vs. $\Omega(n^{1/3})$ gap for distance labelling schemes in planar graphs {\em cannot} be resolved with the current lower bound techniques."|planar graph metric compress problem compact encod distanc among node planar graph size two na ive solut store graph use bit explicit store distanc matrix log bit onli lower bound semin work gavoill peleg prenn raz soda rule compress polynomi smaller number bit em weight planar graph leav larg gap unweight planar graph exampl sqrt upper bound construct impli omega lower bound gap direct relat major open question label scheme dynam algorithm compact rout main result new compress planar graph metric tild min sqrt cdot bit optim log factor data structur break omega lower bound krauthgam nguyen zondin sicomp compress use minor lower bound gavoill et al compress weight planar graph unexpect decis proof weight make planar graph inher complex moreov design new em subset distanc oracl planar graph tild sqrt cdot space tild queri time work carri strong messag relat field particular famous vs omega gap distanc label scheme planar graph em cannot resolv current lower bound techniqu|['Amir Abboud', 'Pawel Gawrychowski', 'Shay Mozes', 'Oren Weimann']|['cs.DS']
2017-03-28T14:09:47Z|2017-03-14T22:16:53Z|http://arxiv.org/abs/1703.04769v1|http://arxiv.org/pdf/1703.04769v1|The Stochastic Container Relocation Problem|stochast contain reloc problem|"The Container Relocation Problem (CRP) is concerned with finding a sequence of moves of containers that minimizes the number of relocations needed to retrieve all containers, while respecting a given order of retrieval. However, the assumption of knowing the full retrieval order of containers is particularly unrealistic in real operations. This paper studies the stochastic CRP (SCRP), which relaxes this assumption. A new multi-stage stochastic model, called the batch model, is introduced, motivated, and compared with an existing model (the online model). The two main contributions are an optimal algorithm called Pruning-Best-First-Search (PBFS) and a randomized approximate algorithm called PBFS-Approximate with a bounded average error. Both algorithms, applicable in the batch and online models, are based on a new family of lower bounds for which we show some theoretical properties. Moreover, we introduce two new heuristics outperforming the best existing heuristics. Algorithms, bounds and heuristics are tested in an extensive computational section. Finally, based on strong computational evidence, we conjecture the optimality of the ""Leveling"" heuristic in a special ""no information"" case, where at any retrieval stage, any of the remaining containers is equally likely to be retrieved next."|contain reloc problem crp concern find sequenc move contain minim number reloc need retriev contain respect given order retriev howev assumpt know full retriev order contain particular unrealist real oper paper studi stochast crp scrp relax assumpt new multi stage stochast model call batch model introduc motiv compar exist model onlin model two main contribut optim algorithm call prune best first search pbfs random approxim algorithm call pbfs approxim bound averag error algorithm applic batch onlin model base new famili lower bound show theoret properti moreov introduc two new heurist outperform best exist heurist algorithm bound heurist test extens comput section final base strong comput evid conjectur optim level heurist special inform case ani retriev stage ani remain contain equal like retriev next|['Virgile Galle', 'Setareh Borjian Boroujeni', 'Vahideh H. Manshadi', 'Cynthia Barnhart', 'Patrick Jaillet']|['cs.DS']
2017-03-28T14:09:47Z|2017-03-14T18:49:57Z|http://arxiv.org/abs/1703.04664v1|http://arxiv.org/pdf/1703.04664v1|Optimal Densification for Fast and Accurate Minwise Hashing|optim densif fast accur minwis hash|Minwise hashing is a fundamental and one of the most successful hashing algorithm in the literature. Recent advances based on the idea of densification~\cite{Proc:OneHashLSH_ICML14,Proc:Shrivastava_UAI14} have shown that it is possible to compute $k$ minwise hashes, of a vector with $d$ nonzeros, in mere $(d + k)$ computations, a significant improvement over the classical $O(dk)$. These advances have led to an algorithmic improvement in the query complexity of traditional indexing algorithms based on minwise hashing. Unfortunately, the variance of the current densification techniques is unnecessarily high, which leads to significantly poor accuracy compared to vanilla minwise hashing, especially when the data is sparse. In this paper, we provide a novel densification scheme which relies on carefully tailored 2-universal hashes. We show that the proposed scheme is variance-optimal, and without losing the runtime efficiency, it is significantly more accurate than existing densification techniques. As a result, we obtain a significantly efficient hashing scheme which has the same variance and collision probability as minwise hashing. Experimental evaluations on real sparse and high-dimensional datasets validate our claims. We believe that given the significant advantages, our method will replace minwise hashing implementations in practice.|minwis hash fundament one success hash algorithm literatur recent advanc base idea densif cite proc onehashlsh icml proc shrivastava uai shown possibl comput minwis hash vector nonzero mere comput signific improv classic dk advanc led algorithm improv queri complex tradit index algorithm base minwis hash unfortun varianc current densif techniqu unnecessarili high lead signific poor accuraci compar vanilla minwis hash especi data spars paper provid novel densif scheme reli care tailor univers hash show propos scheme varianc optim without lose runtim effici signific accur exist densif techniqu result obtain signific effici hash scheme varianc collis probabl minwis hash experiment evalu real spars high dimension dataset valid claim believ given signific advantag method replac minwis hash implement practic|['Anshumali Shrivastava']|['cs.DS', 'cs.LG']
2017-03-28T14:09:47Z|2017-03-13T16:18:01Z|http://arxiv.org/abs/1703.04466v1|http://arxiv.org/pdf/1703.04466v1|Bicriteria Rectilinear Shortest Paths among Rectilinear Obstacles in the   Plane|bicriteria rectilinear shortest path among rectilinear obstacl plane|Given a rectilinear domain $\mathcal{P}$ of $h$ pairwise-disjoint rectilinear obstacles with a total of $n$ vertices in the plane, we study the problem of computing bicriteria rectilinear shortest paths between two points $s$ and $t$ in $\mathcal{P}$. Three types of bicriteria rectilinear paths are considered: minimum-link shortest paths, shortest minimum-link paths, and minimum-cost paths where the cost of a path is a non-decreasing function of both the number of edges and the length of the path. The one-point and two-point path queries are also considered. Algorithms for these problems have been given previously. Our contributions are threefold. First, we find a critical error in all previous algorithms. Second, we correct the error in a not-so-trivial way. Third, we further improve the algorithms so that they are even faster than the previous (incorrect) algorithms when $h$ is relatively small. For example, for the minimum-link shortest paths, we obtain the following results. Our algorithm computes a minimum-link shortest $s$-$t$ path in $O(n+h\log^{3/2} h)$ time. For the one-point queries, we build a data structure of size $O(n+ h\log h)$ in $O(n+h\log^{3/2} h)$ time for a source point $s$, such that given any query point $t$, a minimum-link shortest $s$-$t$ path can be determined in $O(\log n)$ time. For the two-point queries, with $O(n+h^2\log^2 h)$ time and space preprocessing, a minimum-link shortest $s$-$t$ path can be determined in $O(\log n+\log^2 h)$ time for any two query points $s$ and $t$; alternatively, with $O(n+h^2\cdot \log^{2} h \cdot 4^{\sqrt{\log h}})$ time and $O(n+h^2\cdot \log h \cdot 4^{\sqrt{\log h}})$ space preprocessing, we can answer each two-point query in $O(\log n)$ time.|given rectilinear domain mathcal pairwis disjoint rectilinear obstacl total vertic plane studi problem comput bicriteria rectilinear shortest path two point mathcal three type bicriteria rectilinear path consid minimum link shortest path shortest minimum link path minimum cost path cost path non decreas function number edg length path one point two point path queri also consid algorithm problem given previous contribut threefold first find critic error previous algorithm second correct error trivial way third improv algorithm even faster previous incorrect algorithm relat small exampl minimum link shortest path obtain follow result algorithm comput minimum link shortest path log time one point queri build data structur size log log time sourc point given ani queri point minimum link shortest path determin log time two point queri log time space preprocess minimum link shortest path determin log log time ani two queri point altern cdot log cdot sqrt log time cdot log cdot sqrt log space preprocess answer two point queri log time|['Haitao Wang']|['cs.CG', 'cs.DS']
2017-03-28T14:09:47Z|2017-03-13T13:31:17Z|http://arxiv.org/abs/1703.04381v1|http://arxiv.org/pdf/1703.04381v1|On the Transformation Capability of Feasible Mechanisms for Programmable   Matter|transform capabl feasibl mechan programm matter|In this work, we study theoretical models of \emph{programmable matter} systems. The systems under consideration consist of spherical modules, kept together by magnetic forces and able to perform two minimal mechanical operations (or movements): \emph{rotate} around a neighbor and \emph{slide} over a line. In terms of modeling, there are $n$ nodes arranged in a 2-dimensional grid and forming some initial \emph{shape}. The goal is for the initial shape $A$ to \emph{transform} to some target shape $B$ by a sequence of movements. Most of the paper focuses on \emph{transformability} questions, meaning whether it is in principle feasible to transform a given shape to another. We first consider the case in which only rotation is available to the nodes. Our main result is that deciding whether two given shapes $A$ and $B$ can be transformed to each other, is in $\mathbf{P}$. We then insist on rotation only and impose the restriction that the nodes must maintain global connectivity throughout the transformation. We prove that the corresponding transformability question is in $\mathbf{PSPACE}$ and study the problem of determining the minimum \emph{seeds} that can make feasible, otherwise infeasible transformations. Next we allow both rotations and slidings and prove universality: any two connected shapes $A,B$ of the same order, can be transformed to each other without breaking connectivity. The worst-case number of movements of the generic strategy is $\Omega(n^2)$. We improve this to $O(n)$ parallel time, by a pipelining strategy, and prove optimality of both by matching lower bounds. In the last part of the paper, we turn our attention to distributed transformations. The nodes are now distributed processes able to perform communicate-compute-move rounds. We provide distributed algorithms for a general type of transformations.|work studi theoret model emph programm matter system system consider consist spheric modul kept togeth magnet forc abl perform two minim mechan oper movement emph rotat around neighbor emph slide line term model node arrang dimension grid form initi emph shape goal initi shape emph transform target shape sequenc movement paper focus emph transform question mean whether principl feasibl transform given shape anoth first consid case onli rotat avail node main result decid whether two given shape transform mathbf insist rotat onli impos restrict node must maintain global connect throughout transform prove correspond transform question mathbf pspace studi problem determin minimum emph seed make feasibl otherwis infeas transform next allow rotat slide prove univers ani two connect shape order transform without break connect worst case number movement generic strategi omega improv parallel time pipelin strategi prove optim match lower bound last part paper turn attent distribut transform node distribut process abl perform communic comput move round provid distribut algorithm general type transform|['Othon Michail', 'George Skretas', 'Paul G. Spirakis']|['cs.DS', 'cs.DC', 'cs.RO']
2017-03-28T14:09:47Z|2017-03-13T02:57:50Z|http://arxiv.org/abs/1703.04230v1|http://arxiv.org/pdf/1703.04230v1|Improved approximation algorithms for $k$-connected $m$-dominating set   problems|improv approxim algorithm connect domin set problem|A graph is $k$-connected if it has $k$ internally-disjoint paths between every pair of nodes. A subset $S$ of nodes in a graph $G$ is a $k$-connected set if the subgraph $G[S]$ induced by $S$ is $k$-connected; $S$ is an $m$-dominating set if every $v \in V \setminus S$ has at least $m$ neighbors in $S$. If $S$ is both $k$-connected and $m$-dominating then $S$ is a $k$-connected $m$-dominating set, or $(k,m)$-cds for short. In the $k$-Connected $m$-Dominating Set ($(k,m)$-CDS) problem the goal is to find a minimum weight $(k,m)$-cds in a node-weighted graph. We consider the case $m \geq k$ and obtain the following approximation ratios. For unit disc-graphs we obtain ratio $O(k\ln k)$, improving the previous ratio $O(k^2 \ln k)$. For general graphs we obtain the first non-trivial approximation ratio $O(k^2 \ln n)$.|graph connect intern disjoint path everi pair node subset node graph connect set subgraph induc connect domin set everi setminus least neighbor connect domin connect domin set cds short connect domin set cds problem goal find minimum weight cds node weight graph consid case geq obtain follow approxim ratio unit disc graph obtain ratio ln improv previous ratio ln general graph obtain first non trivial approxim ratio ln|['Zeev Nutov']|['cs.DS']
2017-03-28T14:09:47Z|2017-03-12T17:11:49Z|http://arxiv.org/abs/1703.04143v1|http://arxiv.org/pdf/1703.04143v1|Bernoulli Factories and Black-Box Reductions in Mechanism Design|bernoulli factori black box reduct mechan design|"We provide a polynomial time reduction from Bayesian incentive compatible mechanism design to Bayesian algorithm design for welfare maximization problems. Unlike prior results, our reduction achieves exact incentive compatibility for problems with multi-dimensional and continuous type spaces. The key technical barrier preventing exact incentive compatibility in prior black-box reductions is that repairing violations of incentive constraints requires understanding the distribution of the mechanism's output. Reductions that instead estimate the output distribution by sampling inevitably suffer from sampling error, which typically precludes exact incentive compatibility.   We overcome this barrier by employing and generalizing the computational model in the literature on Bernoulli Factories. In a Bernoulli factory problem, one is given a function mapping the bias of an ""input coin"" to that of an ""output coin"", and the challenge is to efficiently simulate the output coin given sample access to the input coin. We generalize this to the ""expectations from samples"" computational model, in which an instance is specified by a function mapping the expected values of a set of input distributions to a distribution over outcomes. The challenge is to give a polynomial time algorithm that exactly samples from the distribution over outcomes given only sample access to the input distributions. In this model, we give a polynomial time algorithm for the exponential weights: expected values of the input distributions correspond to the weights of alternatives and we wish to select an alternative with probability proportional to an exponential function of its weight. This algorithm is the key ingredient in designing an incentive compatible mechanism for bipartite matching, which can be used to make the approximately incentive compatible reduction of Hartline et al. (2015) exactly incentive compatible."|provid polynomi time reduct bayesian incent compat mechan design bayesian algorithm design welfar maxim problem unlik prior result reduct achiev exact incent compat problem multi dimension continu type space key technic barrier prevent exact incent compat prior black box reduct repair violat incent constraint requir understand distribut mechan output reduct instead estim output distribut sampl inevit suffer sampl error typic preclud exact incent compat overcom barrier employ general comput model literatur bernoulli factori bernoulli factori problem one given function map bias input coin output coin challeng effici simul output coin given sampl access input coin general expect sampl comput model instanc specifi function map expect valu set input distribut distribut outcom challeng give polynomi time algorithm exact sampl distribut outcom given onli sampl access input distribut model give polynomi time algorithm exponenti weight expect valu input distribut correspond weight altern wish select altern probabl proport exponenti function weight algorithm key ingredi design incent compat mechan bipartit match use make approxim incent compat reduct hartlin et al exact incent compat|['Shaddin Dughmi', 'Jason Hartline', 'Robert Kleinberg', 'Rad Niazadeh']|['cs.GT', 'cs.CC', 'cs.DS', 'math.PR']
2017-03-28T14:09:47Z|2017-03-11T23:16:23Z|http://arxiv.org/abs/1703.04040v1|http://arxiv.org/abs/1703.04040v1|Locality-sensitive hashing of curves|local sensit hash curv|We study data structures for storing a set of polygonal curves in ${\rm R}^d$ such that, given a query curve, we can efficiently retrieve similar curves from the set, where similarity is measured using the discrete Fr\'echet distance or the dynamic time warping distance. To this end we devise the first locality-sensitive hashing schemes for these distance measures. A major challenge is posed by the fact that these distance measures internally optimize the alignment between the curves. We give solutions for different types of alignments including constrained and unconstrained versions. For unconstrained alignments, we improve over a result by Indyk from 2002 for short curves. Let $n$ be the number of input curves and let $m$ be the maximum complexity of a curve in the input. In the particular case where $m \leq \frac{\alpha}{4d} \log n$, for some fixed $\alpha>0$, our solutions imply an approximate near-neighbor data structure for the discrete Fr\'echet distance that uses space in $O(n^{1+\alpha}\log n)$ and achieves query time in $O(n^{\alpha}\log^2 n)$ and constant approximation factor. Furthermore, our solutions provide a trade-off between approximation quality and computational performance: for any parameter $k \in [m]$, we can give a data structure that uses space in $O(2^{2k}m^{k-1} n \log n + nm)$, answers queries in $O( 2^{2k} m^{k}\log n)$ time and achieves approximation factor in $O(m/k)$.|studi data structur store set polygon curv rm given queri curv effici retriev similar curv set similar measur use discret fr echet distanc dynam time warp distanc end devis first local sensit hash scheme distanc measur major challeng pose fact distanc measur intern optim align curv give solut differ type align includ constrain unconstrain version unconstrain align improv result indyk short curv let number input curv let maximum complex curv input particular case leq frac alpha log fix alpha solut impli approxim near neighbor data structur discret fr echet distanc use space alpha log achiev queri time alpha log constant approxim factor furthermor solut provid trade approxim qualiti comput perform ani paramet give data structur use space log nm answer queri log time achiev approxim factor|['Anne Driemel', 'Francesco Silvestri']|['cs.CG', 'cs.DS', 'cs.IR', 'F.2.2']
2017-03-28T14:09:47Z|2017-03-11T16:53:04Z|http://arxiv.org/abs/1703.03998v1|http://arxiv.org/pdf/1703.03998v1|The Weighted Matching Approach to Maximum Cardinality Matching|weight match approach maximum cardin match|Several papers have achieved time $O(\sqrt n m)$ for cardinality matching, starting from first principles. This results in a long derivation. We simplify the task by employing well-known concepts for maximum weight matching. We use Edmonds' algorithm to derive the structure of shortest augmenting paths. We extend this to a complete algorithm for maximum cardinality matching in time $O(\sqrt n m)$.|sever paper achiev time sqrt cardin match start first principl result long deriv simplifi task employ well known concept maximum weight match use edmond algorithm deriv structur shortest augment path extend complet algorithm maximum cardin match time sqrt|['Harold N. Gabow']|['cs.DS']
2017-03-28T14:09:47Z|2017-03-11T12:24:19Z|http://arxiv.org/abs/1703.03963v1|http://arxiv.org/pdf/1703.03963v1|On Solving Travelling Salesman Problem with Vertex Requisitions|solv travel salesman problem vertex requisit|We consider the Travelling Salesman Problem with Vertex Requisitions, where for each position of the tour at most two possible vertices are given. It is known that the problem is strongly NP-hard. The proposed algorithm for this problem has less time complexity compared to the previously known one. In particular, almost all feasible instances of the problem are solvable in O(n) time using the new algorithm, where n is the number of vertices. The developed approach also helps in fast enumeration of a neighborhood in the local search and yields an integer programming model with O(n) binary variables for the problem.|consid travel salesman problem vertex requisit posit tour two possibl vertic given known problem strong np hard propos algorithm problem less time complex compar previous known one particular almost feasibl instanc problem solvabl time use new algorithm number vertic develop approach also help fast enumer neighborhood local search yield integ program model binari variabl problem|['Anton Eremeev', 'Yulia Kovalenko']|['cs.DS']
2017-03-28T14:09:47Z|2017-03-11T03:28:30Z|http://arxiv.org/abs/1703.03900v1|http://arxiv.org/pdf/1703.03900v1|Core Maintenance in Dynamic Graphs: A Parallel Approach based on   Matching|core mainten dynam graph parallel approach base match|The core number of a vertex is a basic index depicting cohesiveness of a graph, and has been widely used in large-scale graph analytics. In this paper, we study the update of core numbers of vertices in dynamic graphs with edge insertions/deletions, which is known as the core maintenance problem. Different from previous approaches that just focus on the case of single-edge insertion/deletion and sequentially handle the edges when multiple edges are inserted/deleted, we investigate the parallelism in the core maintenance procedure. Specifically, we show that if the inserted/deleted edges constitute a matching, the core number update with respect to each inserted/deleted edge can be handled in parallel. Based on this key observation, we propose parallel algorithms for core maintenance in both cases of edge insertions and deletions. We conduct extensive experiments to evaluate the efficiency, stability, parallelism and scalability of our algorithms on different types of real-world and synthetic graphs. Comparing with sequential approaches, our algorithms can improve the core maintenance efficiency significantly.|core number vertex basic index depict cohes graph wide use larg scale graph analyt paper studi updat core number vertic dynam graph edg insert delet known core mainten problem differ previous approach focus case singl edg insert delet sequenti handl edg multipl edg insert delet investig parallel core mainten procedur specif show insert delet edg constitut match core number updat respect insert delet edg handl parallel base key observ propos parallel algorithm core mainten case edg insert delet conduct extens experi evalu effici stabil parallel scalabl algorithm differ type real world synthet graph compar sequenti approach algorithm improv core mainten effici signific|['Na Wang', 'Dongxiao Yu', 'Hai Jin', 'Qiang-Sheng Hua', 'Xuanhua Shi', 'Xia Xie']|['cs.DS']
2017-03-28T14:09:51Z|2017-03-11T02:10:17Z|http://arxiv.org/abs/1703.06113v1|http://arxiv.org/pdf/1703.06113v1|Toward an enumeration of unlabeled trees|toward enumer unlabel tree|We present an algorithm that, on input $n$, lists every unlabeled tree of order $n$.|present algorithm input list everi unlabel tree order|['Pedro Recuero']|['cs.DS', 'math.CO', '05C05']
2017-03-28T14:09:51Z|2017-03-10T22:25:56Z|http://arxiv.org/abs/1703.03859v1|http://arxiv.org/abs/1703.03859v1|Markov Chain Lifting and Distributed ADMM|markov chain lift distribut admm|The time to converge to the steady state of a finite Markov chain can be greatly reduced by a lifting operation, which creates a new Markov chain on an expanded state space. For a class of quadratic objectives, we show an analogous behavior where a distributed ADMM algorithm can be seen as a lifting of Gradient Descent algorithm. This provides a deep insight for its faster convergence rate under optimal parameter tuning. We conjecture that this gain is always present, as opposed to the lifting of a Markov chain which sometimes only provides a marginal speedup.|time converg steadi state finit markov chain great reduc lift oper creat new markov chain expand state space class quadrat object show analog behavior distribut admm algorithm seen lift gradient descent algorithm provid deep insight faster converg rate optim paramet tune conjectur gain alway present oppos lift markov chain sometim onli provid margin speedup|['Guilherme França', 'José Bento']|['stat.ML', 'cs.DS', 'cs.IT', 'cs.LG', 'math.IT', 'math.OC']
2017-03-28T14:09:51Z|2017-03-10T21:51:50Z|http://arxiv.org/abs/1703.03849v1|http://arxiv.org/pdf/1703.03849v1|A note on approximate strengths of edges in a hypergraph|note approxim strength edg hypergraph|Let $H=(V,E)$ be an edge-weighted hypergraph of rank $r$. Kogan and Krauthgamer extended Bencz\'{u}r and Karger's random sampling scheme for cut sparsification from graphs to hypergraphs. The sampling requires an algorithm for computing the approximate strengths of edges. In this note we extend the algorithm for graphs to hypergraphs and describe a near-linear time algorithm to compute approximate strengths of edges; we build on a sparsification result for hypergraphs from our recent work. Combined with prior results we obtain faster algorithms for finding $(1+\epsilon)$-approximate mincuts when the rank of the hypergraph is small.|let edg weight hypergraph rank kogan krauthgam extend bencz karger random sampl scheme cut sparsif graph hypergraph sampl requir algorithm comput approxim strength edg note extend algorithm graph hypergraph describ near linear time algorithm comput approxim strength edg build sparsif result hypergraph recent work combin prior result obtain faster algorithm find epsilon approxim mincut rank hypergraph small|['Chandra Chekuri', 'Chao Xu']|['cs.DS']
2017-03-28T14:09:51Z|2017-03-10T09:58:40Z|http://arxiv.org/abs/1703.03603v1|http://arxiv.org/pdf/1703.03603v1|The Densest Subgraph Problem with a Convex/Concave Size Function|densest subgraph problem convex concav size function|In the densest subgraph problem, given an edge-weighted undirected graph $G=(V,E,w)$, we are asked to find $S\subseteq V$ that maximizes the density, i.e., $w(S)/ S $, where $w(S)$ is the sum of weights of the edges in the subgraph induced by $S$. This problem has often been employed in a wide variety of graph mining applications. However, the problem has a drawback; it may happen that the obtained subset is too large or too small in comparison with the size desired in the application at hand. In this study, we address the size issue of the densest subgraph problem by generalizing the density of $S\subseteq V$. Specifically, we introduce the $f$-density of $S\subseteq V$, which is defined as $w(S)/f( S )$, where $f:\mathbb{Z}_{\geq 0}\rightarrow \mathbb{R}_{\geq 0}$ is a monotonically non-decreasing function. In the $f$-densest subgraph problem ($f$-DS), we aim to find $S\subseteq V$ that maximizes the $f$-density $w(S)/f( S )$. Although $f$-DS does not explicitly specify the size of the output subset of vertices, we can handle the above size issue using a convex/concave size function $f$ appropriately. For $f$-DS with convex function $f$, we propose a nearly-linear-time algorithm with a provable approximation guarantee. On the other hand, for $f$-DS with concave function $f$, we propose an LP-based exact algorithm, a flow-based $O( V ^3)$-time exact algorithm for unweighted graphs, and a nearly-linear-time approximation algorithm.|densest subgraph problem given edg weight undirect graph ask find subseteq maxim densiti sum weight edg subgraph induc problem often employ wide varieti graph mine applic howev problem drawback may happen obtain subset larg small comparison size desir applic hand studi address size issu densest subgraph problem general densiti subseteq specif introduc densiti subseteq defin mathbb geq rightarrow mathbb geq monoton non decreas function densest subgraph problem ds aim find subseteq maxim densiti although ds doe explicit specifi size output subset vertic handl abov size issu use convex concav size function appropri ds convex function propos near linear time algorithm provabl approxim guarante hand ds concav function propos lp base exact algorithm flow base time exact algorithm unweight graph near linear time approxim algorithm|['Yasushi Kawase', 'Atsushi Miyauchi']|['cs.DS', 'cs.DM', 'cs.SI']
2017-03-28T14:09:51Z|2017-03-10T08:35:24Z|http://arxiv.org/abs/1703.03575v1|http://arxiv.org/pdf/1703.03575v1|Crossing the Logarithmic Barrier for Dynamic Boolean Data Structure   Lower Bounds|cross logarithm barrier dynam boolean data structur lower bound|"This paper proves the first super-logarithmic lower bounds on the cell probe complexity of dynamic boolean (a.k.a. decision) data structure problems, a long-standing milestone in data structure lower bounds.   We introduce a new method for proving dynamic cell probe lower bounds and use it to prove a $\tilde{\Omega}(\log^{1.5} n)$ lower bound on the operational time of a wide range of boolean data structure problems, most notably, on the query time of dynamic range counting over $\mathbb{F}_2$ ([Pat07]). Proving an $\omega(\lg n)$ lower bound for this problem was explicitly posed as one of five important open problems in the late Mihai P\v{a}tra\c{s}cu's obituary [Tho13]. This result also implies the first $\omega(\lg n)$ lower bound for the classical 2D range counting problem, one of the most fundamental data structure problems in computational geometry and spatial databases. We derive similar lower bounds for boolean versions of dynamic polynomial evaluation and 2D rectangle stabbing, and for the (non-boolean) problems of range selection and range median.   Our technical centerpiece is a new way of ""weakly"" simulating dynamic data structures using efficient one-way communication protocols with small advantage over random guessing. This simulation involves a surprising excursion to low-degree (Chebychev) polynomials which may be of independent interest, and offers an entirely new algorithmic angle on the ""cell sampling"" method of Panigrahy et al. [PTW10]."|paper prove first super logarithm lower bound cell probe complex dynam boolean decis data structur problem long stand mileston data structur lower bound introduc new method prove dynam cell probe lower bound use prove tild omega log lower bound oper time wide rang boolean data structur problem notabl queri time dynam rang count mathbb pat prove omega lg lower bound problem explicit pose one five import open problem late mihai tra cu obituari tho result also impli first omega lg lower bound classic rang count problem one fundament data structur problem comput geometri spatial databas deriv similar lower bound boolean version dynam polynomi evalu rectangl stab non boolean problem rang select rang median technic centerpiec new way weak simul dynam data structur use effici one way communic protocol small advantag random guess simul involv surpris excurs low degre chebychev polynomi may independ interest offer entir new algorithm angl cell sampl method panigrahi et al ptw|['Kasper Green Larsen', 'Omri Weinstein', 'Huacheng Yu']|['cs.DS', 'cs.CC', 'cs.CG', 'cs.IT', 'math.IT']
2017-03-28T14:09:51Z|2017-03-09T22:47:10Z|http://arxiv.org/abs/1703.03484v1|http://arxiv.org/pdf/1703.03484v1|Combinatorial Auctions with Online XOS Bidders|combinatori auction onlin xos bidder|In combinatorial auctions, a designer must decide how to allocate a set of indivisible items amongst a set of bidders. Each bidder has a valuation function which gives the utility they obtain from any subset of the items. Our focus is specifically on welfare maximization, where the objective is to maximize the sum of valuations that the bidders place on the items that they were allocated (the valuation functions are assumed to be reported truthfully). We analyze an online problem in which the algorithm is not given the set of bidders in advance. Instead, the bidders are revealed sequentially in a uniformly random order, similarly to secretary problems. The algorithm must make an irrevocable decision about which items to allocate to the current bidder before the next one is revealed. When the valuation functions lie in the class $XOS$ (which includes submodular functions), we provide a black box reduction from offline to online optimization. Specifically, given an $\alpha$-approximation algorithm for offline welfare maximization, we show how to create a $(0.199 \alpha)$-approximation algorithm for the online problem. Our algorithm draws on connections to secretary problems; in fact, we show that the online welfare maximization problem itself can be viewed as a particular kind of secretary problem with nonuniform arrival order.|combinatori auction design must decid alloc set indivis item amongst set bidder bidder valuat function give util obtain ani subset item focus specif welfar maxim object maxim sum valuat bidder place item alloc valuat function assum report truth analyz onlin problem algorithm given set bidder advanc instead bidder reveal sequenti uniform random order similar secretari problem algorithm must make irrevoc decis item alloc current bidder befor next one reveal valuat function lie class xos includ submodular function provid black box reduct offlin onlin optim specif given alpha approxim algorithm offlin welfar maxim show creat alpha approxim algorithm onlin problem algorithm draw connect secretari problem fact show onlin welfar maxim problem view particular kind secretari problem nonuniform arriv order|['Shaddin Dughmi', 'Bryan Wilder']|['cs.GT', 'cs.DS']
2017-03-28T14:09:51Z|2017-03-09T15:46:25Z|http://arxiv.org/abs/1703.03304v1|http://arxiv.org/pdf/1703.03304v1|On low rank-width colorings|low rank width color|We introduce the concept of low rank-width colorings, generalising the notion of low tree-depth colorings introduced by Ne\v{s}et\v{r}il and Ossona de Mendez in [Grad and classes with bounded expansion I. Decompositions. EJC, 2008]. We say that a class $\mathcal{C}$ of graphs admits low rank-width colourings if there exist functions $N\colon \mathbb{N}\rightarrow\mathbb{N}$ and $Q\colon \mathbb{N}\rightarrow\mathbb{N}$ such that for all $p\in \mathbb{N}$, every graph $G\in \mathcal{C}$ can be vertex colored with at most $N(p)$ colors such that the union of any $i\leq p$ color classes induces a subgraph of rank-width at most $Q(i)$.   Graph classes admitting low rank-width colorings strictly generalize graph classes admitting low tree-depth colorings and graph classes of bounded rank-width. We prove that for every graph class $\mathcal{C}$ of bounded expansion and every positive integer $r$, the class $\{G^r\colon G\in \mathcal{C}\}$ of $r$th powers of graphs from $\mathcal{C}$, as well as the classes of unit interval graphs and bipartite permutation graphs admit low rank-width colorings. All of these classes have unbounded rank-width and do not admit low tree-depth colorings. We also show that the classes of interval graphs and permutation graphs do not admit low rank-width colorings. As interesting side properties, we prove that every graph class admitting low rank-width colorings has the Erd\H{o}s-Hajnal property and is $\chi$-bounded.|introduc concept low rank width color generalis notion low tree depth color introduc ne et il ossona de mendez grad class bound expans decomposit ejc say class mathcal graph admit low rank width colour exist function colon mathbb rightarrow mathbb colon mathbb rightarrow mathbb mathbb everi graph mathcal vertex color color union ani leq color class induc subgraph rank width graph class admit low rank width color strict general graph class admit low tree depth color graph class bound rank width prove everi graph class mathcal bound expans everi posit integ class colon mathcal th power graph mathcal well class unit interv graph bipartit permut graph admit low rank width color class unbound rank width admit low tree depth color also show class interv graph permut graph admit low rank width color interest side properti prove everi graph class admit low rank width color erd hajnal properti chi bound|['O-joung Kwon', 'Michał Pilipczuk', 'Sebastian Siebertz']|['cs.DS', 'math.CO']
2017-03-28T14:09:51Z|2017-03-09T06:08:01Z|http://arxiv.org/abs/1703.03147v1|http://arxiv.org/pdf/1703.03147v1|Juggling Functions Inside a Database|juggl function insid databas|"We define and study the Functional Aggregate Query (FAQ) problem, which captures common computational tasks across a very wide range of domains including relational databases, logic, matrix and tensor computation, probabilistic graphical models, constraint satisfaction, and signal processing. Simply put, an FAQ is a declarative way of defining a new function from a database of input functions.   We present ""InsideOut"", a dynamic programming algorithm, to evaluate an FAQ. The algorithm rewrites the input query into a set of easier-to-compute FAQ sub-queries. Each sub-query is then evaluated using a worst-case optimal relational join algorithm. The topic of designing algorithms to optimally evaluate the classic multiway join problem has seen exciting developments in the past few years. Our framework tightly connects these new ideas in database theory with a vast number of application areas in a coherent manner, showing potentially that a good database engine can be a general-purpose constraint solver, relational data store, graphical model inference engine, and matrix/tensor computation processor all at once.   The InsideOut algorithm is very simple, as shall be described in this paper. Yet, in spite of solving an extremely general problem, its runtime either is as good as or improves upon the best known algorithm for the applications that FAQ specializes to. These corollaries include computational tasks in graphical model inference, matrix/tensor operations, relational joins, and logic. Better yet, InsideOut can be used within any database engine, because it is basically a principled way of rewriting queries. Indeed, it is already part of the LogicBlox database engine, helping efficiently answer traditional database queries, graphical model inference queries, and train a large class of machine learning models inside the database itself."|defin studi function aggreg queri faq problem captur common comput task across veri wide rang domain includ relat databas logic matrix tensor comput probabilist graphic model constraint satisfact signal process simpli put faq declar way defin new function databas input function present insideout dynam program algorithm evalu faq algorithm rewrit input queri set easier comput faq sub queri sub queri evalu use worst case optim relat join algorithm topic design algorithm optim evalu classic multiway join problem seen excit develop past year framework tight connect new idea databas theori vast number applic area coher manner show potenti good databas engin general purpos constraint solver relat data store graphic model infer engin matrix tensor comput processor onc insideout algorithm veri simpl shall describ paper yet spite solv extrem general problem runtim either good improv upon best known algorithm applic faq special corollari includ comput task graphic model infer matrix tensor oper relat join logic better yet insideout use within ani databas engin becaus basic principl way rewrit queri inde alreadi part logicblox databas engin help effici answer tradit databas queri graphic model infer queri train larg class machin learn model insid databas|['Mahmoud Abo Khamis', 'Hung Q. Ngo', 'Atri Rudra']|['cs.DB', 'cs.DS', 'cs.LO']
2017-03-28T14:09:51Z|2017-03-08T21:50:06Z|http://arxiv.org/abs/1703.03048v1|http://arxiv.org/pdf/1703.03048v1|Quickest Visibility Queries in Polygonal Domains|quickest visibl queri polygon domain|Let $s$ be a point in a polygonal domain $\mathcal{P}$ of $h-1$ holes and $n$ vertices. We consider a quickest visibility query problem. Given a query point $q$ in $\mathcal{P}$, the goal is to find a shortest path in $\mathcal{P}$ to move from $s$ to see $q$ as quickly as possible. Previously, Arkin et al. (SoCG 2015) built a data structure of size $O(n^22^{\alpha(n)}\log n)$ that can answer each query in $O(K\log^2 n)$ time, where $\alpha(n)$ is the inverse Ackermann function and $K$ is the size of the visibility polygon of $q$ in $\mathcal{P}$ (and $K$ can be $\Theta(n)$ in the worst case). In this paper, we present a new data structure of size $O(n\log h + h^2)$ that can answer each query in $O(h\log h\log n)$ time. Our result improves the previous work when $h$ is relatively small. In particular, if $h$ is a constant, then our result even matches the best result for the simple polygon case (i.e., $h=1$), which is optimal. As a by-product, we also have a new algorithm for a shortest-path-to-segment query problem. Given a query line segment $\tau$ in $\mathcal{P}$, the query seeks a shortest path from $s$ to all points of $\tau$. Previously, Arkin et al. gave a data structure of size $O(n^22^{\alpha(n)}\log n)$ that can answer each query in $O(\log^2 n)$ time, and another data structure of size $O(n^3\log n)$ with $O(\log n)$ query time. We present a data structure of size $O(n)$ with query time $O(h\log \frac{n}{h})$, which also favors small values of $h$ and is optimal when $h=O(1)$.|let point polygon domain mathcal hole vertic consid quickest visibl queri problem given queri point mathcal goal find shortest path mathcal move see quick possibl previous arkin et al socg built data structur size alpha log answer queri log time alpha invers ackermann function size visibl polygon mathcal theta worst case paper present new data structur size log answer queri log log time result improv previous work relat small particular constant result even match best result simpl polygon case optim product also new algorithm shortest path segment queri problem given queri line segment tau mathcal queri seek shortest path point tau previous arkin et al gave data structur size alpha log answer queri log time anoth data structur size log log queri time present data structur size queri time log frac also favor small valu optim|['Haitao Wang']|['cs.CG', 'cs.DS']
2017-03-28T14:09:51Z|2017-03-08T15:16:11Z|http://arxiv.org/abs/1703.02867v1|http://arxiv.org/pdf/1703.02867v1|Electoral District Design via Constrained Clustering|elector district design via constrain cluster|The paper studies the electoral district design problem where municipalities of a state have to be grouped into districts of nearly equal population while obeying certain politically motivated requirements. We develop a general framework for electoral district design that is based on the close connection of constrained geometric clustering and diagrams. The approach is computationally efficient and flexible enough to pursue various conflicting juridical demands for the shape of the districts. We demonstrate the practicability of our methodology for electoral districting in Germany.|paper studi elector district design problem municip state group district near equal popul obey certain polit motiv requir develop general framework elector district design base close connect constrain geometr cluster diagram approach comput effici flexibl enough pursu various conflict jurid demand shape district demonstr practic methodolog elector district germani|['Andreas Brieden', 'Peter Gritzmann', 'Fabian Klemm']|['cs.DS', 'math.CO', '90C90']
2017-03-28T14:09:55Z|2017-03-08T15:16:05Z|http://arxiv.org/abs/1703.02866v1|http://arxiv.org/pdf/1703.02866v1|The Half-integral Erdös-Pósa Property for Non-null Cycles|half integr erd sa properti non null cycl|"A Group Labeled Graph is a pair $(G,\Lambda)$ where $G$ is an oriented graph and $\Lambda$ is a mapping from the arcs of $G$ to elements of a group. A (not necessarily directed) cycle $C$ is called non-null if for any cyclic ordering of the arcs in $C$, the group element obtained by `adding' the labels on forward arcs and `subtracting' the labels on reverse arcs is not the identity element of the group. Non-null cycles in group labeled graphs generalize several well-known graph structures, including odd cycles.   In this paper, we prove that non-null cycles on Group Labeled Graphs have the half-integral Erd\""os-P\'osa property. That is, there is a function $f:{\mathbb N}\to {\mathbb N}$ such that for any $k\in {\mathbb N}$, any group labeled graph $(G,\Lambda)$ has a set of $k$ non-null cycles such that each vertex of $G$ appears in at most two of these cycles or there is a set of at most $f(k)$ vertices that intersects every non-null cycle. Since it is known that non-null cycles do not have the integeral Erd\""os-P\'osa property in general, a half-integral Erd\""os-P\'osa result is the best one could hope for."|group label graph pair lambda orient graph lambda map arc element group necessarili direct cycl call non null ani cyclic order arc group element obtain ad label forward arc subtract label revers arc ident element group non null cycl group label graph general sever well known graph structur includ odd cycl paper prove non null cycl group label graph half integr erd os osa properti function mathbb mathbb ani mathbb ani group label graph lambda set non null cycl vertex appear two cycl set vertic intersect everi non null cycl sinc known non null cycl integer erd os osa properti general half integr erd os osa result best one could hope|['Daniel Lokshtanov', 'M. S. Ramanujan', 'Saket Saurabh']|['cs.DM', 'cs.DS']
2017-03-28T14:09:55Z|2017-03-08T10:56:03Z|http://arxiv.org/abs/1703.02784v1|http://arxiv.org/pdf/1703.02784v1|$K$-Best Solutions of MSO Problems on Tree-Decomposable Graphs|best solut mso problem tree decompos graph|We show that, for any graph optimization problem in which the feasible solutions can be expressed by a formula in monadic second-order logic describing sets of vertices or edges and in which the goal is to minimize the sum of the weights in the selected sets, we can find the $k$ best solutions for $n$-vertex graphs of bounded treewidth in time $\mathcal O(n+k\log n)$. In particular, this applies to the problem of finding the $k$ shortest simple paths between given vertices in directed graphs of bounded treewidth, giving an exponential speedup in the per-path cost over previous algorithms.|show ani graph optim problem feasibl solut express formula monad second order logic describ set vertic edg goal minim sum weight select set find best solut vertex graph bound treewidth time mathcal log particular appli problem find shortest simpl path given vertic direct graph bound treewidth give exponenti speedup per path cost previous algorithm|['David Eppstein', 'Denis Kurz']|['cs.DS', 'G.2.2']
2017-03-28T14:09:55Z|2017-03-08T04:18:58Z|http://arxiv.org/abs/1703.02693v1|http://arxiv.org/pdf/1703.02693v1|Stream Aggregation Through Order Sampling|stream aggreg order sampl|This paper introduces a new single-pass reservoir weighted-sampling stream aggregation algorithm, Priority Sample and Hold. PrSH combines aspects of the well-known Sample and Hold algorithm with Priority Sampling. In particular, it achieves a reduced computational cost for rate adaptation in a fixed cache by using a single persistent random variable across the lifetime of each key in the cache. The basic approach can be supplemented with a Sample and Hold pre-sampling stage with a sampling rate adaptation controlled by PrSH. We prove that PrSH provides unbiased estimates of the true aggregates. We analyze the computational complexity of PrSH and its variants, and provide a detailed evaluation of its accuracy on synthetic and trace data. Weighted relative error is reduced by 40% to 65% at sampling rates of 5% to 17%, relative to Adaptive Sample and Hold; there is also substantial improvement for rank queries.|paper introduc new singl pass reservoir weight sampl stream aggreg algorithm prioriti sampl hold prsh combin aspect well known sampl hold algorithm prioriti sampl particular achiev reduc comput cost rate adapt fix cach use singl persist random variabl across lifetim key cach basic approach supplement sampl hold pre sampl stage sampl rate adapt control prsh prove prsh provid unbias estim true aggreg analyz comput complex prsh variant provid detail evalu accuraci synthet trace data weight relat error reduc sampl rate relat adapt sampl hold also substanti improv rank queri|['Nick Duffield', 'Yunhong Xu', 'Liangzhen Xia', 'Nesreen Ahmed', 'Minlan Yu']|['cs.DS']
2017-03-28T14:09:55Z|2017-03-08T03:56:27Z|http://arxiv.org/abs/1703.02690v1|http://arxiv.org/pdf/1703.02690v1|Leveraging Sparsity for Efficient Submodular Data Summarization|leverag sparsiti effici submodular data summar|The facility location problem is widely used for summarizing large datasets and has additional applications in sensor placement, image retrieval, and clustering. One difficulty of this problem is that submodular optimization algorithms require the calculation of pairwise benefits for all items in the dataset. This is infeasible for large problems, so recent work proposed to only calculate nearest neighbor benefits. One limitation is that several strong assumptions were invoked to obtain provable approximation guarantees. In this paper we establish that these extra assumptions are not necessary---solving the sparsified problem will be almost optimal under the standard assumptions of the problem. We then analyze a different method of sparsification that is a better model for methods such as Locality Sensitive Hashing to accelerate the nearest neighbor computations and extend the use of the problem to a broader family of similarities. We validate our approach by demonstrating that it rapidly generates interpretable summaries.|facil locat problem wide use summar larg dataset addit applic sensor placement imag retriev cluster one difficulti problem submodular optim algorithm requir calcul pairwis benefit item dataset infeas larg problem recent work propos onli calcul nearest neighbor benefit one limit sever strong assumpt invok obtain provabl approxim guarante paper establish extra assumpt necessari solv sparsifi problem almost optim standard assumpt problem analyz differ method sparsif better model method local sensit hash acceler nearest neighbor comput extend use problem broader famili similar valid approach demonstr rapid generat interpret summari|['Erik M. Lindgren', 'Shanshan Wu', 'Alexandros G. Dimakis']|['stat.ML', 'cs.DS', 'cs.IT', 'cs.LG', 'math.IT']
2017-03-28T14:09:55Z|2017-03-08T03:55:27Z|http://arxiv.org/abs/1703.02689v1|http://arxiv.org/pdf/1703.02689v1|Exact MAP Inference by Avoiding Fractional Vertices|exact map infer avoid fraction vertic|Given a graphical model, one essential problem is MAP inference, that is, finding the most likely configuration of states according to the model. Although this problem is NP-hard, large instances can be solved in practice. A major open question is to explain why this is true. We give a natural condition under which we can provably perform MAP inference in polynomial time. We require that the number of fractional vertices in the LP relaxation exceeding the optimal solution is bounded by a polynomial in the problem size. This resolves an open question by Dimakis, Gohari, and Wainwright. In contrast, for general LP relaxations of integer programs, known techniques can only handle a constant number of fractional vertices whose value exceeds the optimal solution. We experimentally verify this condition and demonstrate how efficient various integer programming methods are at removing fractional solutions.|given graphic model one essenti problem map infer find like configur state accord model although problem np hard larg instanc solv practic major open question explain whi true give natur condit provabl perform map infer polynomi time requir number fraction vertic lp relax exceed optim solut bound polynomi problem size resolv open question dimaki gohari wainwright contrast general lp relax integ program known techniqu onli handl constant number fraction vertic whose valu exceed optim solut experiment verifi condit demonstr effici various integ program method remov fraction solut|['Erik M. Lindgren', 'Alexandros G. Dimakis', 'Adam Klivans']|['stat.ML', 'cs.DS', 'cs.IT', 'cs.LG', 'math.IT']
2017-03-28T14:09:55Z|2017-03-07T22:18:35Z|http://arxiv.org/abs/1703.02625v1|http://arxiv.org/pdf/1703.02625v1|On Sampling from Massive Graph Streams|sampl massiv graph stream|We propose Graph Priority Sampling (GPS), a new paradigm for order-based reservoir sampling from massive streams of graph edges. GPS provides a general way to weight edge sampling according to auxiliary and/or size variables so as to accomplish various estimation goals of graph properties. In the context of subgraph counting, we show how edge sampling weights can be chosen so as to minimize the estimation variance of counts of specified sets of subgraphs. In distinction with many prior graph sampling schemes, GPS separates the functions of edge sampling and subgraph estimation. We propose two estimation frameworks: (1) Post-Stream estimation, to allow GPS to construct a reference sample of edges to support retrospective graph queries, and (2) In-Stream estimation, to allow GPS to obtain lower variance estimates by incrementally updating the subgraph count estimates during stream processing. Unbiasedness of subgraph estimators is established through a new Martingale formulation of graph stream order sampling, which shows that subgraph estimators, written as a product of constituent edge estimators are unbiased, even when computed at different points in the stream. The separation of estimation and sampling enables significant resource savings relative to previous work. We illustrate our framework with applications to triangle and wedge counting. We perform a large-scale experimental study on real-world graphs from various domains and types. GPS achieves high accuracy with less than 1% error for triangle and wedge counting, while storing a small fraction of the graph with average update times of a few microseconds per edge. Notably, for a large Twitter graph with more than 260M edges, GPS accurately estimates triangle counts with less than 1% error, while storing only 40K edges.|propos graph prioriti sampl gps new paradigm order base reservoir sampl massiv stream graph edg gps provid general way weight edg sampl accord auxiliari size variabl accomplish various estim goal graph properti context subgraph count show edg sampl weight chosen minim estim varianc count specifi set subgraph distinct mani prior graph sampl scheme gps separ function edg sampl subgraph estim propos two estim framework post stream estim allow gps construct refer sampl edg support retrospect graph queri stream estim allow gps obtain lower varianc estim increment updat subgraph count estim dure stream process unbiased subgraph estim establish new martingal formul graph stream order sampl show subgraph estim written product constitu edg estim unbias even comput differ point stream separ estim sampl enabl signific resourc save relat previous work illustr framework applic triangl wedg count perform larg scale experiment studi real world graph various domain type gps achiev high accuraci less error triangl wedg count store small fraction graph averag updat time microsecond per edg notabl larg twitter graph edg gps accur estim triangl count less error store onli edg|['Nesreen K. Ahmed', 'Nick Duffield', 'Theodore Willke', 'Ryan A. Rossi']|['cs.SI', 'cs.DS', 'cs.IR', 'math.ST', 'stat.TH']
2017-03-28T14:09:55Z|2017-03-07T17:35:51Z|http://arxiv.org/abs/1703.02485v1|http://arxiv.org/pdf/1703.02485v1|Certifying coloring algorithms for graphs without long induced paths|certifi color algorithm graph without long induc path|Let $P_k$ be a path, $C_k$ a cycle on $k$ vertices, and $K_{k,k}$ a complete bipartite graph with $k$ vertices on each side of the bipartition. We prove that (1) for any integers $k, t>0$ and a graph $H$ there are finitely many subgraph minimal graphs with no induced $P_k$ and $K_{t,t}$ that are not $H$-colorable and (2) for any integer $k>4$ there are finitely many subgraph minimal graphs with no induced $P_k$ that are not $C_{k-2}$-colorable.   The former generalizes the result of Hell and Huang [Complexity of coloring graphs without paths and cycles, Discrete Appl. Math. 216: 211--232 (2017)] and the latter extends a result of Bruce, Hoang, and Sawada [A certifying algorithm for 3-colorability of $P_5$-Free Graphs, ISAAC 2009: 594--604]. Both our results lead to polynomial-time certifying algorithms for the corresponding coloring problems.|let path cycl vertic complet bipartit graph vertic side bipartit prove ani integ graph finit mani subgraph minim graph induc color ani integ finit mani subgraph minim graph induc color former general result hell huang complex color graph without path cycl discret appl math latter extend result bruce hoang sawada certifi algorithm color free graph isaac result lead polynomi time certifi algorithm correspond color problem|['Marcin Kamiński', 'Anna Pstrucha']|['math.CO', 'cs.DS']
2017-03-28T14:09:55Z|2017-03-07T14:48:15Z|http://arxiv.org/abs/1703.02411v1|http://arxiv.org/pdf/1703.02411v1|A Simple Deterministic Distributed MST Algorithm, with Near-Optimal Time   and Message Complexities|simpl determinist distribut mst algorithm near optim time messag complex|Distributed minimum spanning tree (MST) problem is one of the most central and fundamental problems in distributed graph algorithms. Garay et al. \cite{GKP98,KP98} devised an algorithm with running time $O(D + \sqrt{n} \cdot \log^* n)$, where $D$ is the hop-diameter of the input $n$-vertex $m$-edge graph, and with message complexity $O(m + n^{3/2})$. Peleg and Rubinovich \cite{PR99} showed that the running time of the algorithm of \cite{KP98} is essentially tight, and asked if one can achieve near-optimal running time **together with near-optimal message complexity**.   In a recent breakthrough, Pandurangan et al. \cite{PRS16} answered this question in the affirmative, and devised a **randomized** algorithm with time $\tilde{O}(D+ \sqrt{n})$ and message complexity $\tilde{O}(m)$. They asked if such a simultaneous time- and message-optimality can be achieved by a **deterministic** algorithm.   In this paper, building upon the work of \cite{PRS16}, we answer this question in the affirmative, and devise a **deterministic** algorithm that computes MST in time $O((D + \sqrt{n}) \cdot \log n)$, using $O(m \cdot \log n + n \log n \cdot \log^* n)$ messages. The polylogarithmic factors in the time and message complexities of our algorithm are significantly smaller than the respective factors in the result of \cite{PRS16}. Also, our algorithm and its analysis are very **simple** and self-contained, as opposed to rather complicated previous sublinear-time algorithms \cite{GKP98,KP98,E04b,PRS16}.|distribut minimum span tree mst problem one central fundament problem distribut graph algorithm garay et al cite gkp kp devis algorithm run time sqrt cdot log hop diamet input vertex edg graph messag complex peleg rubinovich cite pr show run time algorithm cite kp essenti tight ask one achiev near optim run time togeth near optim messag complex recent breakthrough pandurangan et al cite prs answer question affirm devis random algorithm time tild sqrt messag complex tild ask simultan time messag optim achiev determinist algorithm paper build upon work cite prs answer question affirm devis determinist algorithm comput mst time sqrt cdot log use cdot log log cdot log messag polylogarithm factor time messag complex algorithm signific smaller respect factor result cite prs also algorithm analysi veri simpl self contain oppos rather complic previous sublinear time algorithm cite gkp kp eb prs|['Michael Elkin']|['cs.DS']
2017-03-28T14:09:55Z|2017-03-23T13:11:52Z|http://arxiv.org/abs/1703.02375v2|http://arxiv.org/pdf/1703.02375v2|Graph sketching-based Massive Data Clustering|graph sketch base massiv data cluster|In this paper, we address the problem of recovering arbitrary-shaped data clusters from massive datasets. We present DBMSTClu a new density-based non-parametric method working on a limited number of linear measurements i.e. a sketched version of the similarity graph $G$ between the $N$ objects to cluster. Unlike $k$-means, $k$-medians or $k$-medoids algorithms, it does not fail at distinguishing clusters with particular structures. No input parameter is needed contrarily to DBSCAN or the Spectral Clustering method. DBMSTClu as a graph-based technique relies on the similarity graph $G$ which costs theoretically $O(N^2)$ in memory. However, our algorithm follows the dynamic semi-streaming model by handling $G$ as a stream of edge weight updates and sketches it in one pass over the data into a compact structure requiring $O(N \operatorname{poly} \operatorname{log} (N))$ space. Thanks to the property of the Minimum Spanning Tree (MST) for expressing the underlying structure of a graph, our algorithm successfully detects the right number of non-convex clusters by recovering an approximate MST from the graph sketch of $G$. We provide theoretical guarantees on the quality of the clustering partition and also demonstrate its advantage over the existing state-of-the-art on several datasets.|paper address problem recov arbitrari shape data cluster massiv dataset present dbmstclu new densiti base non parametr method work limit number linear measur sketch version similar graph object cluster unlik mean median medoid algorithm doe fail distinguish cluster particular structur input paramet need contrarili dbscan spectral cluster method dbmstclu graph base techniqu reli similar graph cost theoret memori howev algorithm follow dynam semi stream model handl stream edg weight updat sketch one pass data compact structur requir operatornam poli operatornam log space thank properti minimum span tree mst express structur graph algorithm success detect right number non convex cluster recov approxim mst graph sketch provid theoret guarante qualiti cluster partit also demonstr advantag exist state art sever dataset|['Anne Morvan', 'Krzysztof Choromanski', 'Cédric Gouy-Pailler', 'Jamal Atif']|['cs.LG', 'cs.DS']
2017-03-28T14:09:55Z|2017-03-07T05:43:56Z|http://arxiv.org/abs/1703.02224v1|http://arxiv.org/pdf/1703.02224v1|Space-efficient K-MER algorithm for generalized suffix tree|space effici mer algorithm general suffix tree|Suffix trees have emerged to be very fast for pattern searching yielding O (m) time, where m is the pattern size. Unfortunately their high memory requirements make it impractical to work with huge amounts of data. We present a memory efficient algorithm of a generalized suffix tree which reduces the space size by a factor of 10 when the size of the pattern is known beforehand. Experiments on the chromosomes and Pizza&Chili corpus show significant advantages of our algorithm over standard linear time suffix tree construction in terms of memory usage for pattern searching.|suffix tree emerg veri fast pattern search yield time pattern size unfortun high memori requir make impract work huge amount data present memori effici algorithm general suffix tree reduc space size factor size pattern known beforehand experi chromosom pizza chili corpus show signific advantag algorithm standard linear time suffix tree construct term memori usag pattern search|['Freeson Kaniwa', 'Venu Madhav Kuthadi', 'Otlhapile Dinakenyane', 'Heiko Schroeder']|['cs.DS']
2017-03-28T14:09:59Z|2017-03-06T20:28:23Z|http://arxiv.org/abs/1703.02100v1|http://arxiv.org/pdf/1703.02100v1|Guarantees for Greedy Maximization of Non-submodular Functions with   Applications|guarante greedi maxim non submodular function applic|We investigate the performance of the Greedy algorithm for cardinality constrained maximization of non-submodular nondecreasing set functions. While there are strong theoretical guarantees on the performance of Greedy for maximizing submodular functions, there are few guarantees for non-submodular ones. However, Greedy enjoys strong empirical performance for many important non-submodular functions, e.g., the Bayesian A-optimality objective in experimental design. We prove theoretical guarantees supporting the empirical performance. Our guarantees are characterized by the (generalized) submodularity ratio $\gamma$ and the (generalized) curvature $\alpha$. In particular, we prove that Greedy enjoys a tight approximation guarantee of $\frac{1}{\alpha}(1- e^{-\gamma\alpha})$ for cardinality constrained maximization. In addition, we bound the submodularity ratio and curvature for several important real-world objectives, e.g., the Bayesian A-optimality objective, the determinantal function of a square submatrix and certain linear programs with combinatorial constraints. We experimentally validate our theoretical findings for several real-world applications.|investig perform greedi algorithm cardin constrain maxim non submodular nondecreas set function strong theoret guarante perform greedi maxim submodular function guarante non submodular one howev greedi enjoy strong empir perform mani import non submodular function bayesian optim object experiment design prove theoret guarante support empir perform guarante character general submodular ratio gamma general curvatur alpha particular prove greedi enjoy tight approxim guarante frac alpha gamma alpha cardin constrain maxim addit bound submodular ratio curvatur sever import real world object bayesian optim object determinant function squar submatrix certain linear program combinatori constraint experiment valid theoret find sever real world applic|['Andrew An Bian', 'Joachim M. Buhmann', 'Andreas Krause', 'Sebastian Tschiatschek']|['cs.DM', 'cs.AI', 'cs.DS', 'cs.LG', 'math.OC']
2017-03-28T14:09:59Z|2017-03-06T19:01:03Z|http://arxiv.org/abs/1703.02059v1|http://arxiv.org/pdf/1703.02059v1|Cheshire: An Online Algorithm for Activity Maximization in Social   Networks|cheshir onlin algorithm activ maxim social network|User engagement in social networks depends critically on the number of online actions their users take in the network. Can we design an algorithm that finds when to incentivize users to take actions to maximize the overall activity in a social network? In this paper, we model the number of online actions over time using multidimensional Hawkes processes, derive an alternate representation of these processes based on stochastic differential equations (SDEs) with jumps and, exploiting this alternate representation, address the above question from the perspective of stochastic optimal control of SDEs with jumps. We find that the optimal level of incentivized actions depends linearly on the current level of overall actions. Moreover, the coefficients of this linear relationship can be found by solving a matrix Riccati differential equation, which can be solved efficiently, and a first order differential equation, which has a closed form solution. As a result, we are able to design an efficient online algorithm, Cheshire, to sample the optimal times of the users' incentivized actions. Experiments on both synthetic and real data gathered from Twitter show that our algorithm is able to consistently maximize the number of online actions more effectively than the state of the art.|user engag social network depend critic number onlin action user take network design algorithm find incentiv user take action maxim overal activ social network paper model number onlin action time use multidimension hawk process deriv altern represent process base stochast differenti equat sdes jump exploit altern represent address abov question perspect stochast optim control sdes jump find optim level incentiv action depend linear current level overal action moreov coeffici linear relationship found solv matrix riccati differenti equat solv effici first order differenti equat close form solut result abl design effici onlin algorithm cheshir sampl optim time user incentiv action experi synthet real data gather twitter show algorithm abl consist maxim number onlin action effect state art|['Ali Zarezade', 'Abir De', 'Hamid Rabiee', 'Manuel Gomez Rodriguez']|['stat.ML', 'cs.DS', 'cs.LG', 'cs.SI']
2017-03-28T14:09:59Z|2017-03-07T14:17:51Z|http://arxiv.org/abs/1703.01939v2|http://arxiv.org/pdf/1703.01939v2|Distributed Exact Shortest Paths in Sublinear Time|distribut exact shortest path sublinear time|"The distributed single-source shortest paths problem is one of the most fundamental and central problems in the message-passing distributed computing. Classical Bellman-Ford algorithm solves it in $O(n)$ time, where $n$ is the number of vertices in the input graph $G$. Peleg and Rubinovich (FOCS'99) showed a lower bound of $\tilde{\Omega}(D + \sqrt{n})$ for this problem, where $D$ is the hop-diameter of $G$.   Whether or not this problem can be solved in $o(n)$ time when $D$ is relatively small is a major notorious open question. Despite intensive research \cite{LP13,N14,HKN15,EN16,BKKL16} that yielded near-optimal algorithms for the approximate variant of this problem, no progress was reported for the original problem.   In this paper we answer this question in the affirmative. We devise an algorithm that requires $O((n \log n)^{5/6})$ time, for $D = O(\sqrt{n \log n})$, and $O(D^{1/3} \cdot (n \log n)^{2/3})$ time, for larger $D$. This running time is sublinear in $n$ in almost the entire range of parameters, specifically, for $D = o(n/\log^2 n)$.   We also devise the first algorithm with non-trivial complexity guarantees for computing exact shortest paths in the multipass semi-streaming model of computation.   From the technical viewpoint, our algorithm computes a hopset $G""$ of a skeleton graph $G'$ of $G$ without first computing $G'$ itself. We then conduct a Bellman-Ford exploration in $G' \cup G""$, while computing the required edges of $G'$ on the fly. As a result, our algorithm computes exactly those edges of $G'$ that it really needs, rather than computing approximately the entire $G'$."|distribut singl sourc shortest path problem one fundament central problem messag pass distribut comput classic bellman ford algorithm solv time number vertic input graph peleg rubinovich foc show lower bound tild omega sqrt problem hop diamet whether problem solv time relat small major notori open question despit intens research cite lp hkn en bkkl yield near optim algorithm approxim variant problem progress report origin problem paper answer question affirm devis algorithm requir log time sqrt log cdot log time larger run time sublinear almost entir rang paramet specif log also devis first algorithm non trivial complex guarante comput exact shortest path multipass semi stream model comput technic viewpoint algorithm comput hopset skeleton graph without first comput conduct bellman ford explor cup comput requir edg fli result algorithm comput exact edg realli need rather comput approxim entir|['Michael Elkin']|['cs.DS']
2017-03-28T14:09:59Z|2017-03-06T15:03:55Z|http://arxiv.org/abs/1703.01913v1|http://arxiv.org/pdf/1703.01913v1|Near-Optimal Closeness Testing of Discrete Histogram Distributions|near optim close test discret histogram distribut|We investigate the problem of testing the equivalence between two discrete histograms. A {\em $k$-histogram} over $[n]$ is a probability distribution that is piecewise constant over some set of $k$ intervals over $[n]$. Histograms have been extensively studied in computer science and statistics. Given a set of samples from two $k$-histogram distributions $p, q$ over $[n]$, we want to distinguish (with high probability) between the cases that $p = q$ and $\ p-q\ _1 \geq \epsilon$. The main contribution of this paper is a new algorithm for this testing problem and a nearly matching information-theoretic lower bound. Specifically, the sample complexity of our algorithm matches our lower bound up to a logarithmic factor, improving on previous work by polynomial factors in the relevant parameters. Our algorithmic approach applies in a more general setting and yields improved sample upper bounds for testing closeness of other structured distributions as well.|investig problem test equival two discret histogram em histogram probabl distribut piecewis constant set interv histogram extens studi comput scienc statist given set sampl two histogram distribut want distinguish high probabl case geq epsilon main contribut paper new algorithm test problem near match inform theoret lower bound specif sampl complex algorithm match lower bound logarithm factor improv previous work polynomi factor relev paramet algorithm approach appli general set yield improv sampl upper bound test close structur distribut well|['Ilias Diakonikolas', 'Daniel M. Kane', 'Vladimir Nikishkin']|['cs.DS', 'cs.IT', 'cs.LG', 'math.IT', 'math.ST', 'stat.TH']
2017-03-28T14:09:59Z|2017-03-24T15:42:05Z|http://arxiv.org/abs/1703.01905v2|http://arxiv.org/pdf/1703.01905v2|A randomized, efficient algorithm for 3SAT|random effici algorithm sat|In this paper I present a 3SAT algorithm based on the randomized algorithm of Papadimitriou from 1991, and Schoning from 1991. We also show that this algorithm finds a solution (if it exists) for a 3SAT problem with high probability in polynomial time.|paper present sat algorithm base random algorithm papadimitriou schone also show algorithm find solut exist sat problem high probabl polynomi time|['Cristian Dumitrescu']|['cs.DS']
2017-03-28T14:09:59Z|2017-03-06T12:55:21Z|http://arxiv.org/abs/1703.01847v1|http://arxiv.org/pdf/1703.01847v1|Tight Space-Approximation Tradeoff for the Multi-Pass Streaming Set   Cover Problem|tight space approxim tradeoff multi pass stream set cover problem|We study the classic set cover problem in the streaming model: the sets that comprise the instance are revealed one by one in a stream and the goal is to solve the problem by making one or few passes over the stream while maintaining a sublinear space $o(mn)$ in the input size; here $m$ denotes the number of the sets and $n$ is the universe size. Notice that in this model, we are mainly concerned with the space requirement of the algorithms and hence do not restrict their computation time.   Our main result is a resolution of the space-approximation tradeoff for the streaming set cover problem: we show that any $\alpha$-approximation algorithm for the set cover problem requires $\widetilde{\Omega}(mn^{1/\alpha})$ space, even if it is allowed polylog${(n)}$ passes over the stream, and even if the sets are arriving in a random order in the stream. This space-approximation tradeoff matches the best known bounds achieved by the recent algorithm of Har-Peled et.al. (PODS 2016) that requires only $O(\alpha)$ passes over the stream in an adversarial order, hence settling the space complexity of approximating the set cover problem in data streams in a quite robust manner. Additionally, our approach yields tight lower bounds for the space complexity of $(1- \epsilon)$-approximating the streaming maximum coverage problem studied in several recent works.|studi classic set cover problem stream model set compris instanc reveal one one stream goal solv problem make one pass stream maintain sublinear space mn input size denot number set univers size notic model main concern space requir algorithm henc restrict comput time main result resolut space approxim tradeoff stream set cover problem show ani alpha approxim algorithm set cover problem requir widetild omega mn alpha space even allow polylog pass stream even set arriv random order stream space approxim tradeoff match best known bound achiev recent algorithm har pele et al pod requir onli alpha pass stream adversari order henc settl space complex approxim set cover problem data stream quit robust manner addit approach yield tight lower bound space complex epsilon approxim stream maximum coverag problem studi sever recent work|['Sepehr Assadi']|['cs.DS']
2017-03-28T14:09:59Z|2017-03-06T12:06:58Z|http://arxiv.org/abs/1703.01830v1|http://arxiv.org/pdf/1703.01830v1|Decomposable Submodular Function Minimization: Discrete and Continuous|decompos submodular function minim discret continu|This paper investigates connections between discrete and continuous approaches for decomposable submodular function minimization. We provide improved running time estimates for the state-of-the-art continuous algorithms for the problem using combinatorial arguments. We also provide a systematic experimental comparison of the two types of methods, based on a clear distinction between level-0 and level-1 algorithms.|paper investig connect discret continu approach decompos submodular function minim provid improv run time estim state art continu algorithm problem use combinatori argument also provid systemat experiment comparison two type method base clear distinct level level algorithm|['Alina Ene', 'Huy L. Nguyen', 'László A. Végh']|['cs.LG', 'cs.DS']
2017-03-28T14:09:59Z|2017-03-06T05:14:07Z|http://arxiv.org/abs/1703.01727v1|http://arxiv.org/pdf/1703.01727v1|Frequent Query Matching in Dynamic Data Warehousing|frequent queri match dynam data wareh|With the need for flexible and on-demand decision support, Dynamic Data Warehouses (DDW) provide benefits over traditional data warehouses due to their dynamic characteristics in structuring and access mechanism. A DDW is a data framework that accommodates data source changes easily to allow seamless querying to users. Materialized Views (MV) are proven to be an effective methodology to enhance the process of retrieving data from a DDW as results are pre-computed and stored in it. However, due to the static nature of materialized views, the level of dynamicity that can be provided at the MV access layer is restricted. As a result, the collection of materialized views is not compatible with ever-changing reporting requirements. It is important that the MV collection is consistent with current and upcoming queries. The solution to the above problem must consider the following aspects: (a) MV must be matched against an OLAP query in order to recognize whether the MV can answer the query, (b) enable scalability in the MV collection, an intuitive mechanism to prune it and retrieve closely matching MVs must be incorporated, (c) MV collection must be able to evolve in correspondence to the regularly changing user query patterns. Therefore, the primary objective of this paper is to explore these aspects and provide a well-rounded solution for the MV access layer to remove the mismatch between the MV collection and reporting requirements. Our contribution to solve the problem includes a Query Matching Technique, a Domain Matching Technique and Maintenance of the MV collection. We developed an experimental platform using real data-sets to evaluate the effectiveness in terms of performance and precision of the proposed techniques.|need flexibl demand decis support dynam data warehous ddw provid benefit tradit data warehous due dynam characterist structur access mechan ddw data framework accommod data sourc chang easili allow seamless queri user materi view mv proven effect methodolog enhanc process retriev data ddw result pre comput store howev due static natur materi view level dynam provid mv access layer restrict result collect materi view compat ever chang report requir import mv collect consist current upcom queri solut abov problem must consid follow aspect mv must match olap queri order recogn whether mv answer queri enabl scalabl mv collect intuit mechan prune retriev close match mvs must incorpor mv collect must abl evolv correspond regular chang user queri pattern therefor primari object paper explor aspect provid well round solut mv access layer remov mismatch mv collect report requir contribut solv problem includ queri match techniqu domain match techniqu mainten mv collect develop experiment platform use real data set evalu effect term perform precis propos techniqu|['Charles H. Goonetilleke', 'J. Wenny Rahayu', 'Md. Saiful Islam']|['cs.DB', 'cs.DS']
2017-03-28T14:09:59Z|2017-03-05T23:06:03Z|http://arxiv.org/abs/1703.01686v1|http://arxiv.org/pdf/1703.01686v1|Parameterized complexity of finding a spanning tree with minimum reload   cost diameter|parameter complex find span tree minimum reload cost diamet|We study the minimum diameter spanning tree problem under the reload cost model (DIAMETER-TREE for short) introduced by Wirth and Steffan (2001). In this problem, given an undirected edge-colored graph $G$, reload costs on a path arise at a node where the path uses consecutive edges of different colors. The objective is to find a spanning tree of $G$ of minimum diameter with respect to the reload costs. We initiate a systematic study of the parameterized complexity of the DIAMETER-TREE problem by considering the following parameters: the cost of a solution, and the treewidth and the maximum degree $\Delta$ of the input graph. We prove that DIAMETER-TREE is para-NP-hard for any combination of two of these three parameters, and that it is FPT parameterized by the three of them. We also prove that the problem can be solved in polynomial time on cactus graphs. This result is somehow surprising since we prove DIAMETER-TREE to be NP-hard on graphs of treewidth two, which is best possible as the problem can be trivially solved on forests. When the reload costs satisfy the triangle inequality, Wirth and Steffan (2001) proved that the problem can be solved in polynomial time on graphs with $\Delta = 3$, and Galbiati (2008) proved that it is NP-hard if $\Delta = 4$. Our results show, in particular, that without the requirement of the triangle inequality, the problem is NP-hard if $\Delta = 3$, which is also best possible. Finally, in the case where the reload costs are polynomially bounded by the size of the input graph, we prove that DIAMETER-TREE is in XP and W[1]-hard parameterized by the treewidth plus $\Delta$.|studi minimum diamet span tree problem reload cost model diamet tree short introduc wirth steffan problem given undirect edg color graph reload cost path aris node path use consecut edg differ color object find span tree minimum diamet respect reload cost initi systemat studi parameter complex diamet tree problem consid follow paramet cost solut treewidth maximum degre delta input graph prove diamet tree para np hard ani combin two three paramet fpt parameter three also prove problem solv polynomi time cactus graph result somehow surpris sinc prove diamet tree np hard graph treewidth two best possibl problem trivial solv forest reload cost satisfi triangl inequ wirth steffan prove problem solv polynomi time graph delta galbiati prove np hard delta result show particular without requir triangl inequ problem np hard delta also best possibl final case reload cost polynomi bound size input graph prove diamet tree xp hard parameter treewidth plus delta|['Julien Baste', 'Didem Gözüpek', 'Christophe Paul', 'Ignasi Sau', 'Mordechai Shalom', 'Dimitrios M. Thilikos']|['cs.DS', 'cs.CC', '05C85, 05C10', 'G.2.2; G.2.3']
2017-03-28T14:09:59Z|2017-03-05T18:24:23Z|http://arxiv.org/abs/1703.01640v1|http://arxiv.org/abs/1703.01640v1|Approximation algorithms for TSP with neighborhoods in the plane|approxim algorithm tsp neighborhood plane|In the Euclidean TSP with neighborhoods (TSPN), we are given a collection of n regions (neighborhoods) and we seek a shortest tour that visits each region. As a generalization of the classical Euclidean TSP, TSPN is also NP-hard. In this paper, we present new approximation results for the TSPN, including (1) a constant-factor approximation algorithm for the case of arbitrary connected neighborhoods having comparable diameters; and (2) a PTAS for the important special case of disjoint unit disk neighborhoods (or nearly disjoint, nearly-unit disks). Our methods also yield improved approximation ratios for various special classes of neighborhoods, which have previously been studied. Further, we give a linear-time O(1)-approximation algorithm for the case of neighborhoods that are (infinite) straight lines.|euclidean tsp neighborhood tspn given collect region neighborhood seek shortest tour visit region general classic euclidean tsp tspn also np hard paper present new approxim result tspn includ constant factor approxim algorithm case arbitrari connect neighborhood compar diamet ptas import special case disjoint unit disk neighborhood near disjoint near unit disk method also yield improv approxim ratio various special class neighborhood previous studi give linear time approxim algorithm case neighborhood infinit straight line|['Adrian Dumitrescu', 'Joseph S. B. Mitchell']|['cs.CG', 'cs.DS']
2017-03-28T14:10:04Z|2017-03-05T18:12:06Z|http://arxiv.org/abs/1703.01638v1|http://arxiv.org/pdf/1703.01638v1|Conditional Hardness for Sensitivity Problems|condit hard sensit problem|In recent years it has become popular to study dynamic problems in a sensitivity setting: Instead of allowing for an arbitrary sequence of updates, the sensitivity model only allows to apply batch updates of small size to the original input data. The sensitivity model is particularly appealing since recent strong conditional lower bounds ruled out fast algorithms for many dynamic problems, such as shortest paths, reachability, or subgraph connectivity.   In this paper we prove conditional lower bounds for sensitivity problems. For example, we show that under the Boolean Matrix Multiplication (BMM) conjecture combinatorial algorithms cannot compute the (4/3 - {\epsilon})-approximate diameter of an undirected unweighted dense graph with truly subcubic preprocessing time and truly subquadratic update/query time. This result is surprising since in the static setting it is not clear whether a reduction from BMM to diameter is possible. We further show under the BMM conjecture that many problems, such as reachability or approximate shortest paths, cannot be solved faster than by recomputation from scratch even after only one or two edge insertions. We give more lower bounds under the Strong Exponential Time Hypothesis and the All Pairs Shortest Paths Conjecture. Many of our lower bounds also hold for static oracle data structures where no sensitivity is required. Finally, we give the first algorithm for the (1 + {\epsilon})-approximate radius, diameter, and eccentricity problems in directed or undirected unweighted graphs in case of single edges failures. The algorithm has a truly subcubic running time for graphs with a truly subquadratic number of edges; it is tight w.r.t. the conditional lower bounds we obtain.|recent year becom popular studi dynam problem sensit set instead allow arbitrari sequenc updat sensit model onli allow appli batch updat small size origin input data sensit model particular appeal sinc recent strong condit lower bound rule fast algorithm mani dynam problem shortest path reachabl subgraph connect paper prove condit lower bound sensit problem exampl show boolean matrix multipl bmm conjectur combinatori algorithm cannot comput epsilon approxim diamet undirect unweight dens graph truli subcub preprocess time truli subquadrat updat queri time result surpris sinc static set clear whether reduct bmm diamet possibl show bmm conjectur mani problem reachabl approxim shortest path cannot solv faster recomput scratch even onli one two edg insert give lower bound strong exponenti time hypothesi pair shortest path conjectur mani lower bound also hold static oracl data structur sensit requir final give first algorithm epsilon approxim radius diamet eccentr problem direct undirect unweight graph case singl edg failur algorithm truli subcub run time graph truli subquadrat number edg tight condit lower bound obtain|['Monika Henzinger', 'Andrea Lincoln', 'Stefan Neumann', 'Virginia Vassilevska Williams']|['cs.DS', 'F.2.2']
2017-03-28T14:10:04Z|2017-03-05T17:45:59Z|http://arxiv.org/abs/1703.01634v1|http://arxiv.org/pdf/1703.01634v1|Stochastic Online Scheduling on Unrelated Machines|stochast onlin schedul unrel machin|We derive the first performance guarantees for a combinatorial online algorithm that schedules stochastic, non-preemptive jobs on unrelated machines to minimize the expectation of the total weighted completion time. Prior work on unrelated machine scheduling with stochastic jobs was restricted to the offline case, and required sophisticated linear or convex programming relaxations for the assignment of jobs to machines. Our algorithm is purely combinatorial, and therefore it also works for the online setting. As to the techniques applied, this paper shows how the dual fitting technique can be put to work for stochastic and non-preemptive scheduling problems.|deriv first perform guarante combinatori onlin algorithm schedul stochast non preemptiv job unrel machin minim expect total weight complet time prior work unrel machin schedul stochast job restrict offlin case requir sophist linear convex program relax assign job machin algorithm pure combinatori therefor also work onlin set techniqu appli paper show dual fit techniqu put work stochast non preemptiv schedul problem|['Varun Gupta', 'Benjamin Moseley', 'Marc Uetz', 'Qiaomin Xie']|['cs.DS']
2017-03-28T14:10:04Z|2017-03-05T01:08:29Z|http://arxiv.org/abs/1703.01539v1|http://arxiv.org/pdf/1703.01539v1|Distributed Partial Clustering|distribut partial cluster|Recent years have witnessed an increasing popularity of algorithm design for distributed data, largely due to the fact that massive datasets are often collected and stored in different locations. In the distributed setting communication typically dominates the query processing time. Thus it becomes crucial to design communication efficient algorithms for queries on distributed data. Simultaneously, it has been widely recognized that partial optimizations, where we are allowed to disregard a small part of the data, provide us significantly better solutions. The motivation for disregarded points often arise from noise and other phenomena that are pervasive in large data scenarios.   In this paper we focus on partial clustering problems, $k$-center, $k$-median and $k$-means, in the distributed model, and provide algorithms with communication sublinear of the input size. As a consequence we develop the first algorithms for the partial $k$-median and means objectives that run in subquadratic running time. We also initiate the study of distributed algorithms for clustering uncertain data, where each data point can possibly fall into multiple locations under certain probability distribution.|recent year wit increas popular algorithm design distribut data larg due fact massiv dataset often collect store differ locat distribut set communic typic domin queri process time thus becom crucial design communic effici algorithm queri distribut data simultan wide recogn partial optim allow disregard small part data provid us signific better solut motiv disregard point often aris nois phenomena pervas larg data scenario paper focus partial cluster problem center median mean distribut model provid algorithm communic sublinear input size consequ develop first algorithm partial median mean object run subquadrat run time also initi studi distribut algorithm cluster uncertain data data point possibl fall multipl locat certain probabl distribut|['Sudipto Guha', 'Yi Li', 'Qin Zhang']|['cs.DS']
2017-03-28T14:10:04Z|2017-03-04T22:56:03Z|http://arxiv.org/abs/1703.01532v1|http://arxiv.org/pdf/1703.01532v1|Using Matching to Detect Infeasibility of Some Integer Programs|use match detect infeas integ program|A novel matching based heuristic algorithm designed to detect specially formulated infeasible zero-one IPs is presented. The algorithm input is a set of nested doubly stochastic subsystems and a set E of instance defining variables set at zero level. The algorithm deduces additional variables at zero level until either a constraint is violated (the IP is infeasible), or no more variables can be deduced zero (the IP is undecided). All feasible IPs, and all infeasible IPs not detected infeasible are undecided. We successfully apply the algorithm to a small set of specially formulated infeasible zero-one IP instances of the Hamilton cycle decision problem. We show how to model both the graph and subgraph isomorphism decision problems for input to the algorithm. Increased levels of nested doubly stochastic subsystems can be implemented dynamically. The algorithm is designed for parallel processing, and for inclusion of techniques in addition to matching.|novel match base heurist algorithm design detect special formul infeas zero one ip present algorithm input set nest doubli stochast subsystem set instanc defin variabl set zero level algorithm deduc addit variabl zero level either constraint violat ip infeas variabl deduc zero ip undecid feasibl ip infeas ip detect infeas undecid success appli algorithm small set special formul infeas zero one ip instanc hamilton cycl decis problem show model graph subgraph isomorph decis problem input algorithm increas level nest doubli stochast subsystem implement dynam algorithm design parallel process inclus techniqu addit match|['S. J. Gismondi', 'E. R. Swart']|['cs.DS', 'cs.DM']
2017-03-28T14:10:04Z|2017-03-04T19:08:22Z|http://arxiv.org/abs/1703.01507v1|http://arxiv.org/pdf/1703.01507v1|Machine Learning Friendly Set Version of Johnson-Lindenstrauss Lemma|machin learn friend set version johnson lindenstrauss lemma|In this paper we make a novel use of the Johnson-Lindenstrauss Lemma. The Lemma has an existential form saying that there exists a JL transformation $f$ of the data points into lower dimensional space such that all of them fall into predefined error range $\delta$. We formulate in this paper a theorem stating that we can choose the target dimensionality in a random projection type JL linear transformation in such a way that with probability $1-\epsilon$ all of them fall into predefined error range $\delta$ for any user-predefined failure probability $\epsilon$. This result is important for applications such a data clustering where we want to have a priori dimensionality reducing transformation instead of trying out a (large) number of them, as with traditional Johnson-Lindenstrauss Lemma.|paper make novel use johnson lindenstrauss lemma lemma existenti form say exist jl transform data point lower dimension space fall predefin error rang delta formul paper theorem state choos target dimension random project type jl linear transform way probabl epsilon fall predefin error rang delta ani user predefin failur probabl epsilon result import applic data cluster want priori dimension reduc transform instead tri larg number tradit johnson lindenstrauss lemma|['Mieczysław A. Kłopotek']|['cs.DS', 'cs.LG']
2017-03-28T14:10:04Z|2017-03-04T15:14:06Z|http://arxiv.org/abs/1703.01475v1|http://arxiv.org/pdf/1703.01475v1|4/3 Rectangle Tiling lower bound|rectangl tile lower bound|The problem that we consider is the following: given an $n \times n$ array $A$ of positive numbers, find a tiling using at most $p$ rectangles (which means that each array element must be covered by some rectangle and no two rectangles must overlap) that minimizes the maximum weight of any rectangle (the weight of a rectangle is the sum of elements which are covered by it). We prove that it is NP-hard to approximate this problem to within a factor of \textbf{1$\frac{1}{3}$} (the previous best result was $1\frac{1}{4}$).|problem consid follow given time array posit number find tile use rectangl mean array element must cover rectangl two rectangl must overlap minim maximum weight ani rectangl weight rectangl sum element cover prove np hard approxim problem within factor textbf frac previous best result frac|['Grzegorz Głuch', 'Krzysztof Loryś']|['cs.DS']
2017-03-28T14:10:04Z|2017-03-04T15:13:41Z|http://arxiv.org/abs/1703.01474v1|http://arxiv.org/pdf/1703.01474v1|Sharp bounds for population recovery|sharp bound popul recoveri|The population recovery problem is a basic problem in noisy unsupervised learning that has attracted significant research attention in recent years [WY12,DRWY12, MS13, BIMP13, LZ15,DST16]. A number of different variants of this problem have been studied, often under assumptions on the unknown distribution (such as that it has restricted support size). In this work we study the sample complexity and algorithmic complexity of the most general version of the problem, under both bit-flip noise and erasure noise model. We give essentially matching upper and lower sample complexity bounds for both noise models, and efficient algorithms matching these sample complexity bounds up to polynomial factors.|popul recoveri problem basic problem noisi unsupervis learn attract signific research attent recent year wy drwi ms bimp lz dst number differ variant problem studi often assumpt unknown distribut restrict support size work studi sampl complex algorithm complex general version problem bit flip nois erasur nois model give essenti match upper lower sampl complex bound nois model effici algorithm match sampl complex bound polynomi factor|"['Anindya De', ""Ryan O'Donnell"", 'Rocco Servedio']"|['cs.DS', 'cs.LG', 'math.ST', 'stat.TH']
2017-03-28T14:10:04Z|2017-03-06T09:05:51Z|http://arxiv.org/abs/1703.01166v2|http://arxiv.org/pdf/1703.01166v2|Efficient Network Measurements through Approximated Windows|effici network measur approxim window|Many networking applications require timely access to recent network measurements, which can be captured using a sliding window model. Maintaining such measurements is a challenging task due to the fast line speed and scarcity of fast memory in routers. In this work, we study the efficiency factor that can be gained by approximating the window size. That is, we allow the algorithm to dynamically adjust the window size between $W$ and $W(1+\tau)$ where $\tau$ is a small positive parameter. For example, consider the \emph{basic summing} problem of computing the sum of the last $W$ elements in a stream whose items are integers in $\{0,1\ldots,R\}$, where $R=\text{poly}(W)$. While it is known that $\Omega(W\log{R})$ bits are needed in the exact window model, we show that approximate windows allow an exponential space reduction for constant $\tau$.   Specifically, we present a lower bound of $\Omega(\tau^{-1}\log(RW\tau))$ bits for the basic summing problem. Further, an $(1+\epsilon)$ multiplicative approximation of this problem requires $\Omega(\log({W/\epsilon}+\log\log{R}))$ bits for constant $\tau$. Additionally, for $RW\epsilon$ additive approximations, we show an $\Omega(\tau^{-1}\log\lfloor{1+\tau/\epsilon}\rfloor+\log({W/\epsilon}))$ lower bound~\footnote{ We also provide an optimal bound and algorithm for the $\tau<\epsilon$ case.}. For all three settings, we provide memory optimal algorithms that operate in constant time. Finally, we demonstrate the generality of the approximated window model by applying it to counting the number of distinct flows in a sliding window over a network stream. We present an algorithm that solves this problem while requiring asymptotically less space than previous sliding window methods when $\tau=O(1)$.|mani network applic requir time access recent network measur captur use slide window model maintain measur challeng task due fast line speed scarciti fast memori router work studi effici factor gain approxim window size allow algorithm dynam adjust window size tau tau small posit paramet exampl consid emph basic sum problem comput sum last element stream whose item integ ldot text poli known omega log bit need exact window model show approxim window allow exponenti space reduct constant tau specif present lower bound omega tau log rw tau bit basic sum problem epsilon multipl approxim problem requir omega log epsilon log log bit constant tau addit rw epsilon addit approxim show omega tau log lfloor tau epsilon rfloor log epsilon lower bound footnot also provid optim bound algorithm tau epsilon case three set provid memori optim algorithm oper constant time final demonstr general approxim window model appli count number distinct flow slide window network stream present algorithm solv problem requir asymptot less space previous slide window method tau|['Ran Ben Basat', 'Gil Einziger', 'Roy Friedman']|['cs.DS']
2017-03-28T14:10:04Z|2017-03-03T06:41:02Z|http://arxiv.org/abs/1703.01054v1|http://arxiv.org/abs/1703.01054v1|When Hashes Met Wedges: A Distributed Algorithm for Finding High   Similarity Vectors|hash met wedg distribut algorithm find high similar vector|"Finding similar user pairs is a fundamental task in social networks, with numerous applications in ranking and personalization tasks such as link prediction and tie strength detection. A common manifestation of user similarity is based upon network structure: each user is represented by a vector that represents the user's network connections, where pairwise cosine similarity among these vectors defines user similarity. The predominant task for user similarity applications is to discover all similar pairs that have a pairwise cosine similarity value larger than a given threshold $\tau$. In contrast to previous work where $\tau$ is assumed to be quite close to 1, we focus on recommendation applications where $\tau$ is small, but still meaningful. The all pairs cosine similarity problem is computationally challenging on networks with billions of edges, and especially so for settings with small $\tau$. To the best of our knowledge, there is no practical solution for computing all user pairs with, say $\tau = 0.2$ on large social networks, even using the power of distributed algorithms.   Our work directly addresses this challenge by introducing a new algorithm --- WHIMP --- that solves this problem efficiently in the MapReduce model. The key insight in WHIMP is to combine the ""wedge-sampling"" approach of Cohen-Lewis for approximate matrix multiplication with the SimHash random projection techniques of Charikar. We provide a theoretical analysis of WHIMP, proving that it has near optimal communication costs while maintaining computation cost comparable with the state of the art. We also empirically demonstrate WHIMP's scalability by computing all highly similar pairs on four massive data sets, and show that it accurately finds high similarity pairs. In particular, we note that WHIMP successfully processes the entire Twitter network, which has tens of billions of edges."|find similar user pair fundament task social network numer applic rank person task link predict tie strength detect common manifest user similar base upon network structur user repres vector repres user network connect pairwis cosin similar among vector defin user similar predomin task user similar applic discov similar pair pairwis cosin similar valu larger given threshold tau contrast previous work tau assum quit close focus recommend applic tau small still meaning pair cosin similar problem comput challeng network billion edg especi set small tau best knowledg practic solut comput user pair say tau larg social network even use power distribut algorithm work direct address challeng introduc new algorithm whimp solv problem effici mapreduc model key insight whimp combin wedg sampl approach cohen lewi approxim matrix multipl simhash random project techniqu charikar provid theoret analysi whimp prove near optim communic cost maintain comput cost compar state art also empir demonstr whimp scalabl comput high similar pair four massiv data set show accur find high similar pair particular note whimp success process entir twitter network ten billion edg|['Aneesh Sharma', 'C. Seshadhri', 'Ashish Goel']|['cs.SI', 'cs.DC', 'cs.DS']
2017-03-28T14:10:04Z|2017-03-06T04:41:19Z|http://arxiv.org/abs/1703.01009v2|http://arxiv.org/pdf/1703.01009v2|Optimal Time and Space Construction of Suffix Arrays and LCP Arrays for   Integer Alphabets|optim time space construct suffix array lcp array integ alphabet|Suffix arrays and LCP arrays are one of the most fundamental data structures widely used for various kinds of string processing. Many problems can be solved efficiently by using suffix arrays, or a pair of suffix arrays and LCP arrays. In this paper, we consider two problems for a string of length $N$, the characters of which are represented as integers in $[1, \dots, \sigma]$ for $1 \leq \sigma \leq N$; the string contains $\sigma$ distinct characters, (1) construction of the suffix array and (2) simultaneous construction of both the suffix array and the LCP array. In the word RAM model, we propose algorithms to solve both the problems in $O(N)$ time using $O(1)$ extra words, which are optimal in time and space. Extra words mean the required space except for the space of the input string and output suffix array and LCP array. Our contribution improves the previous most efficient algorithm that runs in $O(N)$ time using $\sigma+O(1)$ extra words for the suffix array construction proposed by [Nong, TOIS 2013], and it improves the previous most efficient solution that runs in $O(N)$ time using $\sigma + O(1)$ extra words for both suffix array and LCP array construction using the combination of [Nong, TOIS 2013] and [Manzini, SWAT 2004].   Another optimal time and space algorithm to construct the suffix array was proposed by [Li et al, arXiv 2016] very recently and independently. Our algorithm is simpler than theirs, and it allows us to solve the second problem in optimal time and space.|suffix array lcp array one fundament data structur wide use various kind string process mani problem solv effici use suffix array pair suffix array lcp array paper consid two problem string length charact repres integ dot sigma leq sigma leq string contain sigma distinct charact construct suffix array simultan construct suffix array lcp array word ram model propos algorithm solv problem time use extra word optim time space extra word mean requir space except space input string output suffix array lcp array contribut improv previous effici algorithm run time use sigma extra word suffix array construct propos nong toi improv previous effici solut run time use sigma extra word suffix array lcp array construct use combin nong toi manzini swat anoth optim time space algorithm construct suffix array propos li et al arxiv veri recent independ algorithm simpler allow us solv second problem optim time space|['Keisuke Goto']|['cs.DS']
