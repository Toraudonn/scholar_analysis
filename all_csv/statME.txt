2017-03-28T14:03:51Z|2017-03-27T14:52:09Z|http://arxiv.org/abs/1703.09124v1|http://arxiv.org/pdf/1703.09124v1|Multi-sensor Transmission Management for Remote State Estimation under   Coordination|multi sensor transmiss manag remot state estim coordin|This paper considers the remote state estimation in a cyber-physical system (CPS) using multiple sensors. The measurements of each sensor are transmitted to a remote estimator over a shared channel, where simultaneous transmissions from other sensors are regarded as interference signals. In such a competitive environment, each sensor needs to choose its transmission power for sending data packets taking into account of other sensors' behavior. To model this interactive decision-making process among the sensors, we introduce a multi-player non-cooperative game framework. To overcome the inefficiency arising from the Nash equilibrium (NE) solution, we propose a correlation policy, along with the notion of correlation equilibrium (CE). An analytical comparison of the game value between the NE and the CE is provided, with/without the power expenditure constraints for each sensor. Also, numerical simulations demonstrate the comparison results.|paper consid remot state estim cyber physic system cps use multipl sensor measur sensor transmit remot estim share channel simultan transmiss sensor regard interfer signal competit environ sensor need choos transmiss power send data packet take account sensor behavior model interact decis make process among sensor introduc multi player non cooper game framework overcom ineffici aris nash equilibrium ne solut propos correl polici along notion correl equilibrium ce analyt comparison game valu ne ce provid without power expenditur constraint sensor also numer simul demonstr comparison result|['Kemi Ding', 'Yuzhe Li', 'Subhrakanti Dey', 'Ling Shi']|['stat.ME', 'cs.IT', 'math.IT']
2017-03-28T14:03:51Z|2017-03-27T13:33:39Z|http://arxiv.org/abs/1703.09062v1|http://arxiv.org/pdf/1703.09062v1|A numerical method for the estimation of time-varying parameter models   in large dimensions|numer method estim time vari paramet model larg dimens|A novel numerical method for the estimation of large time-varying parameter (TVP) models is proposed. The Kalman filter and Kalman smoother estimates of the TVP model are derived within the context of generalised linear least squares and through the use of numerical linear algebra. The method developed is based on numerically stable and computationally efficient strategies. The computational cost is reduced by exploiting the special sparse structure of the TVP model and by utilising previous computations. The proposed method is also extended to solve the downdating problem of removing the effect of some observations from current estimates and also to the rolling window estimation of the TVP model. Experimental results show the effectiveness of the new strategies in high dimensions when a large number of covariates are included in the TVP model.|novel numer method estim larg time vari paramet tvp model propos kalman filter kalman smoother estim tvp model deriv within context generalis linear least squar use numer linear algebra method develop base numer stabl comput effici strategi comput cost reduc exploit special spars structur tvp model utilis previous comput propos method also extend solv downdat problem remov effect observ current estim also roll window estim tvp model experiment result show effect new strategi high dimens larg number covari includ tvp model|['Stella Hadjiantoni', 'Erricos J. Kontoghiorghes']|['stat.ME', 'stat.CO']
2017-03-28T14:03:51Z|2017-03-27T13:33:09Z|http://arxiv.org/abs/1703.09061v1|http://arxiv.org/pdf/1703.09061v1|Bayesian Repulsive Gaussian Mixture Model|bayesian repuls gaussian mixtur model|We develop a general class of Bayesian repulsive Gaussian mixture models that encourage well-separated clusters, aiming at reducing potentially redundant components produced by independent priors for locations (such as the Dirichlet process). The asymptotic results for the posterior distribution of the proposed models are derived, including posterior consistency and posterior contraction rate in the context of nonparametric density estimation. More importantly, we show that, as a measurement of the model complexity, the posterior number of necessary components to fit the data well grows sub-linearly with respect to the sample size asymptotically. In addition, an efficient and easy-to-implement blocked-collapsed Gibbs sampler is developed based on the exchangeable partition distribution and the corresponding urn model. We evaluate the performance and demonstrate the advantages of the proposed model through extensive simulation studies and real data analysis. The R code is available at https://drive.google.com/open?id=0B_zFse0eqxBHMVduVEM2Tk9tZFU.|develop general class bayesian repuls gaussian mixtur model encourag well separ cluster aim reduc potenti redund compon produc independ prior locat dirichlet process asymptot result posterior distribut propos model deriv includ posterior consist posterior contract rate context nonparametr densiti estim import show measur model complex posterior number necessari compon fit data well grow sub linear respect sampl size asymptot addit effici easi implement block collaps gibb sampler develop base exchang partit distribut correspond urn model evalu perform demonstr advantag propos model extens simul studi real data analysi code avail https drive googl com open id zfseeqxbhmvduvemtktzfu|['Fangzheng Xie', 'Yanxun Xu']|['stat.ME']
2017-03-28T14:03:51Z|2017-03-27T10:17:00Z|http://arxiv.org/abs/1703.08995v1|http://arxiv.org/pdf/1703.08995v1|On the Limit Imbalanced Logistic Regression by Binary Predictors|limit imbalanc logist regress binari predictor|"In this work, we introduce a modified (rescaled) likelihood for imbalanced logistic regression. This new approach makes easier the use of exponential priors and the computation of lasso regularization path. Precisely, we study a limiting behavior for which class imbalance is artificially increased by replication of the majority class observations. If some strong overlap conditions are satisfied, the maximum likelihood estimate converges towards a finite value close to the initial one (intercept excluded) as shown by simulations with binary predictors. This solution corresponds to the extremum of a concave function that we refer to as ""rescaled"" likelihood. In this context, the use of exponential priors has a clear interpretation as a shift on the predictor means for the minority class. Thanks to the simple binary structure, some random designs give analytic path estimators for the lasso regularization problem. An effective approximate path algorithm by piecewise logarithmic functions based on matrix inversions is also presented. This work was motivated by its potential application to spontaneous reports databases in a pharmacovigilance context."|work introduc modifi rescal likelihood imbalanc logist regress new approach make easier use exponenti prior comput lasso regular path precis studi limit behavior class imbal artifici increas replic major class observ strong overlap condit satisfi maximum likelihood estim converg toward finit valu close initi one intercept exclud shown simul binari predictor solut correspond extremum concav function refer rescal likelihood context use exponenti prior clear interpret shift predictor mean minor class thank simpl binari structur random design give analyt path estim lasso regular problem effect approxim path algorithm piecewis logarithm function base matrix invers also present work motiv potenti applic spontan report databas pharmacovigil context|['Vincent Runge']|['stat.ME', 'Primary 62J12, 62F12, 62F15, secondary 34E05, 49M29, 62P10']
2017-03-28T14:03:51Z|2017-03-26T22:49:31Z|http://arxiv.org/abs/1703.08882v1|http://arxiv.org/pdf/1703.08882v1|A Mixture of Matrix Variate Skew-t Distributions|mixtur matrix variat skew distribut|Clustering is the process of finding underlying group structures in data. Although model-based clustering is firmly established in the multivariate case, there is relative paucity for matrix variate distributions, and there are even fewer examples using matrix variate skew distributions. In this paper, we look at parameter estimation for a finite mixture of matrix variate skew-t distributions in the context of model-based clustering. Simulated data is used for illustrative purposes.|cluster process find group structur data although model base cluster firm establish multivari case relat pauciti matrix variat distribut even fewer exampl use matrix variat skew distribut paper look paramet estim finit mixtur matrix variat skew distribut context model base cluster simul data use illustr purpos|['Michael P. B. Gallaugher', 'Paul D. McNicholas']|['stat.ME', 'stat.CO']
2017-03-28T14:03:51Z|2017-03-25T20:57:56Z|http://arxiv.org/abs/1703.08741v1|http://arxiv.org/pdf/1703.08741v1|Clustering and Variable Selection in the Presence of Mixed Variable   Types and Missing Data|cluster variabl select presenc mix variabl type miss data|We consider the problem of model-based clustering in the presence of many correlated, mixed continuous and discrete variables, some of which may have missing values. Discrete variables are treated with a latent continuous variable approach and the Dirichlet process is used to construct a mixture model with an unknown number of components. Variable selection is also performed to identify the variables that are most influential for determining cluster membership. The work is motivated by the need to cluster patients thought to potentially have autism spectrum disorder (ASD) on the basis of many cognitive and/or behavioral test scores. There are a modest number of patients (~480) in the data set along with many (~100) test score variables (many of which are discrete valued and/or missing). The goal of the work is to (i) cluster these patients into similar groups to help identify those with similar clinical presentation, and (ii) identify a sparse subset of tests that inform the clusters in order to eliminate unnecessary testing. The proposed approach compares very favorably to other methods via simulation of problems of this type. The results of the ASD analysis suggested three clusters to be most likely, while only four test scores had high (>0.5) posterior probability of being informative. This will result in much more efficient and informative testing. The need to cluster observations on the basis of many correlated, continuous/discrete variables with missing values, is a common problem in the health sciences as well as in many other disciplines.|consid problem model base cluster presenc mani correl mix continu discret variabl may miss valu discret variabl treat latent continu variabl approach dirichlet process use construct mixtur model unknown number compon variabl select also perform identifi variabl influenti determin cluster membership work motiv need cluster patient thought potenti autism spectrum disord asd basi mani cognit behavior test score modest number patient data set along mani test score variabl mani discret valu miss goal work cluster patient similar group help identifi similar clinic present ii identifi spars subset test inform cluster order elimin unnecessari test propos approach compar veri favor method via simul problem type result asd analysi suggest three cluster like onli four test score high posterior probabl inform result much effici inform test need cluster observ basi mani correl continu discret variabl miss valu common problem health scienc well mani disciplin|['Curtis Storlie', 'Scott Myers', 'S Katusic', 'Amy Weaver', 'Robert Voigt', 'Robert Colligan', 'Paul Croarkin', 'Ruth Stoeckel', 'John Port']|['stat.ME']
2017-03-28T14:03:51Z|2017-03-25T17:57:31Z|http://arxiv.org/abs/1703.08723v1|http://arxiv.org/pdf/1703.08723v1|Extending Growth Mixture Models Using Continuous Non-Elliptical   Distributions|extend growth mixtur model use continu non ellipt distribut|Growth mixture models (GMMs) incorporate both conventional random effects growth modeling and latent trajectory classes as in finite mixture modeling; therefore, they offer a way to handle the unobserved heterogeneity between subjects in their development. GMMs with Gaussian random effects dominate the literature. When the data are asymmetric and/or have heavier tails, more than one latent class is required to capture the observed variable distribution. Therefore, a GMM with continuous non-elliptical distributions is proposed to capture skewness and heavier tails in the data set. Specifically, multivariate skew-t distributions and generalized hyperbolic distributions are introduced to extend GMMs. When extending GMMs, four statistical models are considered with differing distributions of measurement errors and random effects. The mathematical development of a GMM with non-elliptical distributions relies on its relationship with the generalized inverse Gaussian distribution. Parameter estimation is outlined within the expectation-maximization framework before the performance of our GMM with non-elliptical distributions is illustrated on simulated and real data.|growth mixtur model gmms incorpor convent random effect growth model latent trajectori class finit mixtur model therefor offer way handl unobserv heterogen subject develop gmms gaussian random effect domin literatur data asymmetr heavier tail one latent class requir captur observ variabl distribut therefor gmm continu non ellipt distribut propos captur skew heavier tail data set specif multivari skew distribut general hyperbol distribut introduc extend gmms extend gmms four statist model consid differ distribut measur error random effect mathemat develop gmm non ellipt distribut reli relationship general invers gaussian distribut paramet estim outlin within expect maxim framework befor perform gmm non ellipt distribut illustr simul real data|['Yuhong Wei', 'Emilie Shireman', 'Paul D. McNicholas', 'Douglas L. Steinley']|['stat.ME', 'stat.AP', 'stat.CO']
2017-03-28T14:03:51Z|2017-03-25T09:11:28Z|http://arxiv.org/abs/1703.08665v1|http://arxiv.org/pdf/1703.08665v1|Full likelihood inference for max-stable data|full likelihood infer max stabl data|We show how to perform full likelihood inference for max-stable multivariate distributions or processes based on a stochastic Expectation-Maximisation algorithm. In contrast to current approaches, such as pairwise likelihoods or the Stephenson--Tawn likelihood, our method combines statistical and computational efficiency in high-dimensions, and it is not subject to bias entailed by lack of convergence of the underlying partition. The good performance of this methodology is demonstrated by simulation based on the logistic model, and it is shown to provide dramatic computational time improvements with respect to a direct computation of the likelihood. Strategies to further reduce the computational burden are also discussed.|show perform full likelihood infer max stabl multivari distribut process base stochast expect maximis algorithm contrast current approach pairwis likelihood stephenson tawn likelihood method combin statist comput effici high dimens subject bias entail lack converg partit good perform methodolog demonstr simul base logist model shown provid dramat comput time improv respect direct comput likelihood strategi reduc comput burden also discuss|['Clément Dombry', 'Marc G. Genton', 'Raphaël Huser', 'Mathieu Ribatet']|['stat.ME']
2017-03-28T14:03:51Z|2017-03-24T23:04:17Z|http://arxiv.org/abs/1703.08620v1|http://arxiv.org/pdf/1703.08620v1|LANOVA Penalization for Unreplicated Data|lanova penal unrepl data|We consider the problem of estimating the entries of an unknown mean matrix or tensor, $\boldsymbol M$, given a single noisy realization, $\boldsymbol Y = \boldsymbol M + \boldsymbol Z$. In the matrix case, we address this problem by decomposing $\boldsymbol M$ into a component that is additive in the rows and columns, i.e. the additive ANOVA decomposition of $\boldsymbol M$, plus a matrix of elementwise effects, $\boldsymbol C$, and assuming that $\boldsymbol C$ may be sparse. Accordingly, we estimate $\boldsymbol M$ by solving a penalized regression problem, applying a lasso penalty for elements of $\boldsymbol C$. We call the corresponding estimate of $\boldsymbol M$ the LANOVA penalized estimate. Although solving this penalized regression problem is straightforward, specifying appropriate values of the penalty parameters is not. Leveraging the posterior mode interpretation of the penalized regression problem, we define and study moment-based empirical Bayes estimators of the penalty parameters. We show that our empirical Bayes estimators are consistent, and examine the behavior of LANOVA penalized estimates under misspecification of the distribution of elements of $\boldsymbol C$. We extend LANOVA penalization to accommodate sparsity of row and column effects and to tensor data. We demonstrate empirical Bayes LANOVA penalization in analyses of several datasets, including a matrix of microarray data, a three-way tensor of fMRI data and a three-way tensor of experimental data.|consid problem estim entri unknown mean matrix tensor boldsymbol given singl noisi realize boldsymbol boldsymbol boldsymbol matrix case address problem decompos boldsymbol compon addit row column addit anova decomposit boldsymbol plus matrix elementwis effect boldsymbol assum boldsymbol may spars accord estim boldsymbol solv penal regress problem appli lasso penalti element boldsymbol call correspond estim boldsymbol lanova penal estim although solv penal regress problem straightforward specifi appropri valu penalti paramet leverag posterior mode interpret penal regress problem defin studi moment base empir bay estim penalti paramet show empir bay estim consist examin behavior lanova penal estim misspecif distribut element boldsymbol extend lanova penal accommod sparsiti row column effect tensor data demonstr empir bay lanova penal analys sever dataset includ matrix microarray data three way tensor fmri data three way tensor experiment data|['Maryclare Griffin', 'Peter Hoff']|['stat.ME', 'stat.AP']
2017-03-28T14:03:51Z|2017-03-24T20:59:52Z|http://arxiv.org/abs/1703.08596v1|http://arxiv.org/pdf/1703.08596v1|The Inner Structure of Time-Dependent Signals|inner structur time depend signal|This paper shows how a time series of measurements of an evolving system can be processed to create an inner time series that is unaffected by any instantaneous invertible, possibly nonlinear transformation of the measurements. An inner time series contains information that does not depend on the nature of the sensors, which the observer chose to monitor the system. Instead, it encodes information that is intrinsic to the evolution of the observed system. Because of its sensor-independence, an inner time series may produce fewer false negatives when it is used to detect events in the presence of sensor drift. Furthermore, if the observed physical system is comprised of non-interacting subsystems, its inner time series is separable; i.e., it consists of a collection of time series, each one being the inner time series of an isolated subsystem. Because of this property, an inner time series can be used to detect a specific behavior of one of the independent subsystems without using blind source separation to disentangle that subsystem from the others. The method is illustrated by applying it to: 1) an analytic example; 2) the audio waveform of one speaker; 3) video images from a moving camera; 4) mixtures of audio waveforms of two speakers.|paper show time seri measur evolv system process creat inner time seri unaffect ani instantan invert possibl nonlinear transform measur inner time seri contain inform doe depend natur sensor observ chose monitor system instead encod inform intrins evolut observ system becaus sensor independ inner time seri may produc fewer fals negat use detect event presenc sensor drift furthermor observ physic system compris non interact subsystem inner time seri separ consist collect time seri one inner time seri isol subsystem becaus properti inner time seri use detect specif behavior one independ subsystem without use blind sourc separ disentangl subsystem method illustr appli analyt exampl audio waveform one speaker video imag move camera mixtur audio waveform two speaker|['David N. Levin']|['stat.ME', 'cs.SD', 'math.ST', 'stat.TH']
2017-03-28T14:03:55Z|2017-03-24T17:17:45Z|http://arxiv.org/abs/1703.08520v1|http://arxiv.org/pdf/1703.08520v1|Rejection-free Ensemble MCMC with applications to Factorial Hidden   Markov Models|reject free ensembl mcmc applic factori hidden markov model|"Bayesian inference for complex models is challenging due to the need to explore high-dimensional spaces and multimodality and standard Monte Carlo samplers can have difficulties effectively exploring the posterior. We introduce a general purpose rejection-free ensemble Markov Chain Monte Carlo (MCMC) technique to improve on existing poorly mixing samplers. This is achieved by combining parallel tempering and an auxiliary variable move to exchange information between the chains. We demonstrate this ensemble MCMC scheme on Bayesian inference in Factorial Hidden Markov Models. This high-dimensional inference problem is difficult due to the exponentially sized latent variable space. Existing sampling approaches mix slowly and can get trapped in local modes. We show that the performance of these samplers is improved by our rejection-free ensemble technique and that the method is attractive and ""easy-to-use"" since no parameter tuning is required."|bayesian infer complex model challeng due need explor high dimension space multimod standard mont carlo sampler difficulti effect explor posterior introduc general purpos reject free ensembl markov chain mont carlo mcmc techniqu improv exist poor mix sampler achiev combin parallel temper auxiliari variabl move exchang inform chain demonstr ensembl mcmc scheme bayesian infer factori hidden markov model high dimension infer problem difficult due exponenti size latent variabl space exist sampl approach mix slowli get trap local mode show perform sampler improv reject free ensembl techniqu method attract easi use sinc paramet tune requir|['Kaspar Märtens', 'Michalis K Titsias', 'Christopher Yau']|['stat.CO', 'stat.ME', 'stat.ML']
2017-03-28T14:03:55Z|2017-03-24T16:11:19Z|http://arxiv.org/abs/1703.08489v1|http://arxiv.org/pdf/1703.08489v1|regsem: Regularized Structural Equation Modeling|regsem regular structur equat model|The regsem package in R, an implementation of regularized structural equation modeling (RegSEM; Jacobucci, Grimm, and McArdle 2016), was recently developed with the goal of incorporating various forms of penalized likelihood estimation in a broad array of structural equations models. The forms of regularization include both the ridge (Hoerl and Kennard 1970) and the least absolute shrinkage and selection operator (lasso; Tibshirani 1996), along with sparser extensions. RegSEM is particularly useful for structural equation models that have a small parameter to sample size ratio, as the addition of penalties can reduce the complexity, thus reducing the bias of the parameter estimates. The paper covers the algorithmic details and an overview of the use of regsem with the application of both factor analysis and latent growth curve models.|regsem packag implement regular structur equat model regsem jacobucci grimm mcardl recent develop goal incorpor various form penal likelihood estim broad array structur equat model form regular includ ridg hoerl kennard least absolut shrinkag select oper lasso tibshirani along sparser extens regsem particular use structur equat model small paramet sampl size ratio addit penalti reduc complex thus reduc bias paramet estim paper cover algorithm detail overview use regsem applic factor analysi latent growth curv model|['Ross Jacobucci']|['stat.ME']
2017-03-28T14:03:55Z|2017-03-24T16:08:21Z|http://arxiv.org/abs/1703.08487v1|http://arxiv.org/pdf/1703.08487v1|Multiscale Granger causality|multiscal granger causal|In the study of complex physical and biological systems represented by multivariate stochastic processes, an issue of great relevance is the description of the system dynamics spanning multiple temporal scales. While methods to assess the dynamic complexity of individual processes at different time scales are well-established, the multiscale evaluation of directed interactions between processes is complicated by theoretical and practical issues such as filtering and downsampling. Here we extend the very popular measure of Granger causality (GC), a prominent tool for assessing directed lagged interactions between joint processes, to quantify information transfer across multiple time scales. We show that the multiscale processing of a vector autoregressive (AR) process introduces a moving average (MA) component, and describe how to represent the resulting ARMA process using state space (SS) models and to combine the SS model parameters for computing exact GC values at arbitrarily large time scales. We exploit the theoretical formulation to identify peculiar features of multiscale GC in basic AR processes, and demonstrate with numerical simulations the much larger estimation accuracy of the SS approach compared with pure AR modeling of filtered and downsampled data. The improved computational reliability is exploited to disclose meaningful multiscale patterns of information transfer between global temperature and carbon dioxide concentration time series, both in paleoclimate and in recent years.|studi complex physic biolog system repres multivari stochast process issu great relev descript system dynam span multipl tempor scale method assess dynam complex individu process differ time scale well establish multiscal evalu direct interact process complic theoret practic issu filter downsampl extend veri popular measur granger causal gc promin tool assess direct lag interact joint process quantifi inform transfer across multipl time scale show multiscal process vector autoregress ar process introduc move averag compon describ repres result arma process use state space ss model combin ss model paramet comput exact gc valu arbitrarili larg time scale exploit theoret formul identifi peculiar featur multiscal gc basic ar process demonstr numer simul much larger estim accuraci ss approach compar pure ar model filter downsampl data improv comput reliabl exploit disclos meaning multiscal pattern inform transfer global temperatur carbon dioxid concentr time seri paleoclim recent year|['Luca Faes', 'Giandomenico Nollo', 'Sebastiano Stramaglia', 'Daniele Marinazzo']|['stat.ME', 'math.ST', 'stat.AP', 'stat.TH']
2017-03-28T14:03:55Z|2017-03-23T18:57:39Z|http://arxiv.org/abs/1703.08202v1|http://arxiv.org/pdf/1703.08202v1|A recursive point process model for infectious diseases|recurs point process model infecti diseas|We introduce a new type of point process model to describe the incidence of contagious diseases. The model is a variant of the Hawkes self-exciting process and exhibits similar clustering but without the restriction that the component describing the contagion must remain static over time. Instead, our proposed model prescribes that the degree of contagion (or productivity) changes as a function of the conditional intensity; of particular interest is the special case where the productivity is inversely proportional to the conditional intensity. The model incorporates the premise that when the disease occurs at very low frequency in the population, such as in the primary stages of an outbreak, then anyone with the disease is likely to have a high rate of transmission to others, whereas when the disease is prevalent in the population, then the transmission rate is lower due to human mitigation actions and prevention measures and a relatively high percentage of previous exposure in the total population. The model is said to be recursive, in the sense that the conditional intensity at any particular time depends on the productivity associated with previous points, and this productivity in turn depends on the conditional intensity at those points. Some basic properties of the model are derived, estimation and simulation are discussed, and the recursive model is shown to fit well to historic data on measles in Los Angeles, California, a relevant example given the 2017 outbreak of this disease in the same region.|introduc new type point process model describ incid contagi diseas model variant hawk self excit process exhibit similar cluster without restrict compon describ contagion must remain static time instead propos model prescrib degre contagion product chang function condit intens particular interest special case product invers proport condit intens model incorpor premis diseas occur veri low frequenc popul primari stage outbreak anyon diseas like high rate transmiss wherea diseas preval popul transmiss rate lower due human mitig action prevent measur relat high percentag previous exposur total popul model said recurs sens condit intens ani particular time depend product associ previous point product turn depend condit intens point basic properti model deriv estim simul discuss recurs model shown fit well histor data measl los angel california relev exampl given outbreak diseas region|['Frederic Schoenberg', 'Marc Hoffmann', 'Ryan Harrigan']|['stat.ME']
2017-03-28T14:03:55Z|2017-03-23T14:47:29Z|http://arxiv.org/abs/1703.08090v1|http://arxiv.org/pdf/1703.08090v1|Flexible multi-state models for interval-censored data: specification,   estimation, and an application to ageing research|flexibl multi state model interv censor data specif estim applic age research|Continuous-time multi-state survival models can be used to describe health-related processes over time. In the presence of interval-censored times for transitions between the living states, the likelihood is constructed using transition probabilities. Models can be specified using parametric or semi-parametric shapes for the hazards. Semi-parametric hazards can be fitted using $P$-splines and penalised maximum likelihood estimation. This paper presents a method to estimate flexible multi-state models which allows for parametric and semi-parametric hazard specifications. The estimation is based on a scoring algorithm. The method is illustrated with data from the English Longitudinal Study of Ageing.|continu time multi state surviv model use describ health relat process time presenc interv censor time transit live state likelihood construct use transit probabl model specifi use parametr semi parametr shape hazard semi parametr hazard fit use spline penalis maximum likelihood estim paper present method estim flexibl multi state model allow parametr semi parametr hazard specif estim base score algorithm method illustr data english longitudin studi age|['Robson J. M. Machado', 'Ardo van den Hout']|['stat.ME']
2017-03-28T14:03:55Z|2017-03-23T12:43:23Z|http://arxiv.org/abs/1703.08045v1|http://arxiv.org/pdf/1703.08045v1|Profiled deviance for the multivariate linear mixed-effects model   fitting|profil devianc multivari linear mix effect model fit|This paper focuses on the multivariate linear mixed-effects model, including all the correlations between the random effects when the marginal residual terms are assumed uncorrelated and homoscedastic with possibly different standard deviations. The random effects covariance matrix is Cholesky factorized to directly estimate the variance components of these random effects. This strategy enables a consistent estimate of the random effects covariance matrix which, generally, has a poor estimate when it is grossly (or directly) estimated, using the estimating methods such as the EM algorithm. By using simulated data sets, we compare the estimates based on the present method with the EM algorithm-based estimates. We provide an illustration by using the real-life data concerning the study of the child's immune against malaria in Benin (West Africa).|paper focus multivari linear mix effect model includ correl random effect margin residu term assum uncorrel homoscedast possibl differ standard deviat random effect covari matrix choleski factor direct estim varianc compon random effect strategi enabl consist estim random effect covari matrix general poor estim grossli direct estim use estim method em algorithm use simul data set compar estim base present method em algorithm base estim provid illustr use real life data concern studi child immun malaria benin west africa|['Eric Adjakossa', 'Grégory Nuel']|['stat.ME']
2017-03-28T14:03:55Z|2017-03-23T09:20:36Z|http://arxiv.org/abs/1703.07975v1|http://arxiv.org/pdf/1703.07975v1|An Adapted Loss Function for Censored Quantile Regression|adapt loss function censor quantil regress|"In this paper, we study a novel approach for the estimation of quantiles when facing potential right censoring of the responses. Contrary to the existing literature on the subject, the adopted strategy of this paper is to tackle censoring at the very level of the loss function usually employed for the computation of quantiles, the so-called ""check"" function. For interpretation purposes, a simple comparison with the latter reveals how censoring is accounted for in the newly proposed loss function. Subsequently, when considering the inclusion of covariates for conditional quantile estimation, by defining a new general loss function, the proposed methodology opens the gate to numerous parametric, semiparametric and nonparametric modelling techniques. In order to illustrate this statement, we consider the well-studied linear regression under the usual assumption of conditional independence between the true response and the censoring variable. For practical minimization of the studied loss function, we also provide a simple algorithmic procedure shown to yield satisfactory results for the proposed estimator with respect to the existing literature in an extensive simulation study. From a more theoretical prospect, consistency of the estimator for linear regression is obtained using very recent results on non-smooth semiparametric estimation equations with an infinite-dimensional nuisance parameter, while numerical examples illustrate the adequateness of a simple bootstrap procedure for inferential purposes. Lastly, an application to a real dataset is used to further illustrate the validity and finite sample performance of the proposed estimator."|paper studi novel approach estim quantil face potenti right censor respons contrari exist literatur subject adopt strategi paper tackl censor veri level loss function usual employ comput quantil call check function interpret purpos simpl comparison latter reveal censor account newli propos loss function subsequ consid inclus covari condit quantil estim defin new general loss function propos methodolog open gate numer parametr semiparametr nonparametr model techniqu order illustr statement consid well studi linear regress usual assumpt condit independ true respons censor variabl practic minim studi loss function also provid simpl algorithm procedur shown yield satisfactori result propos estim respect exist literatur extens simul studi theoret prospect consist estim linear regress obtain use veri recent result non smooth semiparametr estim equat infinit dimension nuisanc paramet numer exampl illustr adequ simpl bootstrap procedur inferenti purpos last applic real dataset use illustr valid finit sampl perform propos estim|['Mickaël De Backer', 'Anouar El Ghouch', 'Ingrid Van Keilegom']|['stat.ME']
2017-03-28T14:03:55Z|2017-03-23T01:30:17Z|http://arxiv.org/abs/1703.07904v1|http://arxiv.org/pdf/1703.07904v1|Cross-Validation with Confidence|cross valid confid|Cross-validation is one of the most popular model selection methods in statistics and machine learning. Despite its wide applicability, traditional cross-validation methods tend to select overfitting models, unless the ratio between the training and testing sample sizes is much smaller than conventional choices. We argue that such an overfitting tendency of cross-validation is due to the ignorance of the uncertainty in the testing sample. Starting from this observation, we develop a new, statistically principled inference tool based on cross-validation that takes into account the uncertainty in the testing sample. This new method outputs a small set of highly competitive candidate models containing the best one with guaranteed probability. As a consequence, our method can achieve consistent variable selection in a classical linear regression setting, for which existing cross-validation methods require unconventional split ratios. We demonstrate the performance of the proposed method in several simulated and real data examples.|cross valid one popular model select method statist machin learn despit wide applic tradit cross valid method tend select overfit model unless ratio train test sampl size much smaller convent choic argu overfit tendenc cross valid due ignor uncertainti test sampl start observ develop new statist principl infer tool base cross valid take account uncertainti test sampl new method output small set high competit candid model contain best one guarante probabl consequ method achiev consist variabl select classic linear regress set exist cross valid method requir unconvent split ratio demonstr perform propos method sever simul real data exampl|['Jing Lei']|['stat.ME', 'stat.ML']
2017-03-28T14:03:55Z|2017-03-22T22:59:20Z|http://arxiv.org/abs/1703.07879v1|http://arxiv.org/pdf/1703.07879v1|How to avoid the curse of dimensionality: scalability of particle   filters with and without importance weights|avoid curs dimension scalabl particl filter without import weight|Particle filters are a popular and flexible class of numerical algorithms to solve a large class of nonlinear filtering problems. However, standard particle filters with importance weights have been shown to require a sample size that increases exponentially with the dimension D of the state space in order to achieve a certain performance, which precludes their use in very high-dimensional filtering problems. Here, we focus on the dynamic aspect of this curse of dimensionality (COD) in continuous time filtering, which is caused by the degeneracy of importance weights over time. We show that the degeneracy occurs on a time-scale that decreases with increasing D. In order to soften the effects of weight degeneracy, most particle filters use particle resampling and improved proposal functions for the particle motion. We explain why neither of the two can prevent the COD in general. In order to address this fundamental problem, we investigate an existing filtering algorithm based on optimal feedback control that sidesteps the use of importance weights. We use numerical experiments to show that this Feedback Particle Filter (FPF) by Yang et al. (2013) does not exhibit a COD.|particl filter popular flexibl class numer algorithm solv larg class nonlinear filter problem howev standard particl filter import weight shown requir sampl size increas exponenti dimens state space order achiev certain perform preclud use veri high dimension filter problem focus dynam aspect curs dimension cod continu time filter caus degeneraci import weight time show degeneraci occur time scale decreas increas order soften effect weight degeneraci particl filter use particl resampl improv propos function particl motion explain whi neither two prevent cod general order address fundament problem investig exist filter algorithm base optim feedback control sidestep use import weight use numer experi show feedback particl filter fpf yang et al doe exhibit cod|['Simone Carlo Surace', 'Anna Kutschireiter', 'Jean-Pascal Pfister']|['math.OC', 'math.PR', 'math.ST', 'stat.ME', 'stat.TH']
2017-03-28T14:03:55Z|2017-03-22T21:13:02Z|http://arxiv.org/abs/1703.07856v1|http://arxiv.org/pdf/1703.07856v1|Testing for the Equality of two Distributions on High Dimensional Object   Spaces|test equal two distribut high dimension object space|Energy statistics are estimators of the energy distance that depend on the distances between observations. The idea behind energy statistics is to consider a statistical potential energy that would parallel Newton's gravitational potential energy. This statistical potential energy is zero if and only if a certain null hypothesis relating two distributions holds true. In Szekely and Rizzo(2004), a nonparametric test for equality of two multivariate distributions was given, based on the Euclidean distance between observations. This test was shown to be effective for high dimensional multivariate data, and was implemented by an appropriate distribution free permutation test. As an extension of Szekely and Rizzo (2013), here we consider the energy distance between to independent random objects X and Y on the object space M, that admits an embedding into an Euclidean space. In the case of a Kendall shape space, we can use its VW-embedding into an Euclidean space of matrices and define the extrinsic distance between two shapes as their VW associated distance. The corresponding energy distance between two distributions of Kendall shapes of k-ads will be called VW-energy distance We test our methodology on, to compare the distributions of Kendall shape of the contour of the midsagittal section of the Corpus Callossum in normal vs ADHD diagnosed individuals. Here we use the VW distance between the shapes of two children CC midsections. Using the CC data coming originally from http://fcon 1000.projects.nitrc.org/indi/adhd200/ it appears that the two Kendall shape distributions are not significantly different.|energi statist estim energi distanc depend distanc observ idea behind energi statist consid statist potenti energi would parallel newton gravit potenti energi statist potenti energi zero onli certain null hypothesi relat two distribut hold true szeke rizzo nonparametr test equal two multivari distribut given base euclidean distanc observ test shown effect high dimension multivari data implement appropri distribut free permut test extens szeke rizzo consid energi distanc independ random object object space admit embed euclidean space case kendal shape space use vw embed euclidean space matric defin extrins distanc two shape vw associ distanc correspond energi distanc two distribut kendal shape ad call vw energi distanc test methodolog compar distribut kendal shape contour midsagitt section corpus callossum normal vs adhd diagnos individu use vw distanc shape two children cc midsect use cc data come origin http fcon project nitrc org indi adhd appear two kendal shape distribut signific differ|['Ruite Guo', 'Vic Patrangenaru']|['stat.ME']
2017-03-28T14:03:59Z|2017-03-22T16:56:48Z|http://arxiv.org/abs/1703.07747v1|http://arxiv.org/pdf/1703.07747v1|MIMIX: a Bayesian Mixed-Effects Model for Microbiome Data from Designed   Experiments|mimix bayesian mix effect model microbiom data design experi|Recent advances in bioinformatics have made high-throughput microbiome data widely available, and new statistical tools are required to maximize the information gained from these data. For example, analysis of high-dimensional microbiome data from designed experiments remains an open area in microbiome research. Contemporary analyses work on metrics that summarize collective properties of the microbiome, but such reductions preclude inference on the fine-scale effects of environmental stimuli on individual microbial taxa. Other approaches model the proportions or counts of individual taxa as response variables in mixed models, but these methods fail to account for complex correlation patterns among microbial communities. In this paper, we propose a novel Bayesian mixed-effects model that exploits cross-taxa correlations within the microbiome, a model we call MIMIX (MIcrobiome MIXed model). MIMIX offers global tests for treatment effects, local tests and estimation of treatment effects on individual taxa, quantification of the relative contribution from heterogeneous sources to microbiome variability, and identification of latent ecological subcommunities in the microbiome. MIMIX is tailored to large microbiome experiments using a combination of Bayesian factor analysis to efficiently represent dependence between taxa and Bayesian variable selection methods to achieve sparsity. We demonstrate the model using a simulation experiment and on a 2x2 factorial experiment of the effects of nutrient supplement and herbivore exclusion on the foliar fungal microbiome of $\textit{Andropogon gerardii}$, a perennial bunchgrass, as part of the global Nutrient Network research initiative.|recent advanc bioinformat made high throughput microbiom data wide avail new statist tool requir maxim inform gain data exampl analysi high dimension microbiom data design experi remain open area microbiom research contemporari analys work metric summar collect properti microbiom reduct preclud infer fine scale effect environment stimuli individu microbi taxa approach model proport count individu taxa respons variabl mix model method fail account complex correl pattern among microbi communiti paper propos novel bayesian mix effect model exploit cross taxa correl within microbiom model call mimix microbiom mix model mimix offer global test treatment effect local test estim treatment effect individu taxa quantif relat contribut heterogen sourc microbiom variabl identif latent ecolog subcommun microbiom mimix tailor larg microbiom experi use combin bayesian factor analysi effici repres depend taxa bayesian variabl select method achiev sparsiti demonstr model use simul experi factori experi effect nutrient supplement herbivor exclus foliar fungal microbiom textit andropogon gerardii perenni bunchgrass part global nutrient network research initi|['Neal S. Grantham', 'Brian J. Reich', 'Elizabeth T. Borer', 'Kevin Gross']|['stat.ME']
2017-03-28T14:03:59Z|2017-03-22T11:20:00Z|http://arxiv.org/abs/1703.07603v1|http://arxiv.org/pdf/1703.07603v1|Effect fusion using model-based clustering|effect fusion use model base cluster|In social and economic studies many of the collected variables are measured on a nominal scale, often with a large number of categories. The definition of categories is usually not unambiguous and different classification schemes using either a finer or a coarser grid are possible. Categorisation has an impact when such a variable is included as covariate in a regression model: a too fine grid will result in imprecise estimates of the corresponding effects, whereas with a too coarse grid important effects will be missed, resulting in biased effect estimates and poor predictive performance.   To achieve automatic grouping of levels with essentially the same effect, we adopt a Bayesian approach and specify the prior on the level effects as a location mixture of spiky normal components. Fusion of level effects is induced by a prior on the mixture weights which encourages empty components. Model-based clustering of the effects during MCMC sampling allows to simultaneously detect categories which have essentially the same effect size and identify variables with no effect at all. The properties of this approach are investigated in simulation studies. Finally, the method is applied to analyse effects of high-dimensional categorical predictors on income in Austria.|social econom studi mani collect variabl measur nomin scale often larg number categori definit categori usual unambigu differ classif scheme use either finer coarser grid possibl categoris impact variabl includ covari regress model fine grid result imprecis estim correspond effect wherea coars grid import effect miss result bias effect estim poor predict perform achiev automat group level essenti effect adopt bayesian approach specifi prior level effect locat mixtur spiki normal compon fusion level effect induc prior mixtur weight encourag empti compon model base cluster effect dure mcmc sampl allow simultan detect categori essenti effect size identifi variabl effect properti approach investig simul studi final method appli analys effect high dimension categor predictor incom austria|['Gertraud Malsiner-Walli', 'Daniela Pauger', 'Helga Wagner']|['stat.ME']
2017-03-28T14:03:59Z|2017-03-21T16:39:28Z|http://arxiv.org/abs/1703.07305v1|http://arxiv.org/abs/1703.07305v1|Targeting Bayes factors with direct-path non-equilibrium thermodynamic   integration|target bay factor direct path non equilibrium thermodynam integr|Thermodynamic integration (TI) for computing marginal likelihoods is based on an inverse annealing path from the prior to the posterior distribution. In many cases, the resulting estimator suffers from high variability, which particularly stems from the prior regime. When comparing complex models with differences in a comparatively small number of parameters, intrinsic errors from sampling fluctuations may outweigh the differences in the log marginal likelihood estimates. In the present article, we propose a thermodynamic integration scheme that directly targets the log Bayes factor. The method is based on a modified annealing path between the posterior distributions of the two models compared, which systematically avoids the high variance prior regime. We combine this scheme with the concept of non-equilibrium TI to minimise discretisation errors from numerical integration. Results obtained on Bayesian regression models applied to standard benchmark data, and a complex hierarchical model applied to biopathway inference, demonstrate a significant reduction in estimator variance over state-of-the-art TI methods.|thermodynam integr ti comput margin likelihood base invers anneal path prior posterior distribut mani case result estim suffer high variabl particular stem prior regim compar complex model differ compar small number paramet intrins error sampl fluctuat may outweigh differ log margin likelihood estim present articl propos thermodynam integr scheme direct target log bay factor method base modifi anneal path posterior distribut two model compar systemat avoid high varianc prior regim combin scheme concept non equilibrium ti minimis discretis error numer integr result obtain bayesian regress model appli standard benchmark data complex hierarch model appli biopathway infer demonstr signific reduct estim varianc state art ti method|['Marco Grzegorczyk', 'Andrej Aderhold', 'Dirk Husmeier']|['stat.ME', 'stat.ML']
2017-03-28T14:03:59Z|2017-03-21T14:41:52Z|http://arxiv.org/abs/1703.07246v1|http://arxiv.org/pdf/1703.07246v1|Sufficient Dimension Reduction via Random-Partitions for Large-p-Small-n   Problem|suffici dimens reduct via random partit larg small problem|"Sufficient dimension reduction (SDR) is continuing an active research field nowadays for high dimensional data. It aims to estimate the central subspace (CS) without making distributional assumption. To overcome the large-$p$-small-$n$ problem we propose a new approach for SDR. Our method combines the following ideas for high dimensional data analysis: (1) Randomly partition the covariates into subsets and use distance correlation (DC) to construct a sketch of envelope subspace with low dimension. (2) Obtain a sketch of the CS by applying conventional SDR method within the constructed envelope subspace. (3) Repeat the above two steps for a few times and integrate these multiple sketches to form the final estimate of the CS. We name the proposed SDR procedure ""integrated random-partition SDR (iRP-SDR)"". Comparing with existing methods, iRP-SDR is less affected by the selection of tuning parameters. Moreover, the estimation procedure of iRP-SDR does not involve the determination of the structural dimension until at the last stage, which makes the method more robust in a high-dimensional setting. Asymptotic properties of iRP-SDR are also established. The advantageous performance of the proposed method is demonstrated via simulation studies and the EEG data analysis."|suffici dimens reduct sdr continu activ research field nowaday high dimension data aim estim central subspac cs without make distribut assumpt overcom larg small problem propos new approach sdr method combin follow idea high dimension data analysi random partit covari subset use distanc correl dc construct sketch envelop subspac low dimens obtain sketch cs appli convent sdr method within construct envelop subspac repeat abov two step time integr multipl sketch form final estim cs name propos sdr procedur integr random partit sdr irp sdr compar exist method irp sdr less affect select tune paramet moreov estim procedur irp sdr doe involv determin structur dimens last stage make method robust high dimension set asymptot properti irp sdr also establish advantag perform propos method demonstr via simul studi eeg data analysi|['Hung Hung', 'Su-Yun Huang']|['stat.ME']
2017-03-28T14:03:59Z|2017-03-21T13:02:35Z|http://arxiv.org/abs/1703.07198v1|http://arxiv.org/pdf/1703.07198v1|Overcoming model simplifications when quantifying predictive uncertainty|overcom model simplif quantifi predict uncertainti|It is generally accepted that all models are wrong -- the difficulty is determining which are useful. Here, a useful model is considered as one that is capable of combining data and expert knowledge, through an inversion or calibration process, to adequately characterize the uncertainty in predictions of interest. This paper derives conditions that specify which simplified models are useful and how they should be calibrated. To start, the notion of an optimal simplification is defined. This relates the model simplifications to the nature of the data and predictions, and determines when a standard probabilistic calibration scheme is capable of accurately characterizing uncertainty. Furthermore, two additional conditions are defined for suboptimal models that determine when the simplifications can be safely ignored. The first allows a suboptimally simplified model to be used in a way that replicates the performance of an optimal model. This is achieved through the judicial selection of a prior term for the calibration process that explicitly includes the nature of the data, predictions and modelling simplifications. The second considers the dependency structure between the predictions and the available data to gain insights into when the simplifications can be overcome by using the right calibration data. Furthermore, the derived conditions are related to the commonly used calibration schemes based on Tikhonov and subspace regularization. To allow concrete insights to be obtained, the analysis is performed under a linear expansion of the model equations and where the predictive uncertainty is characterized via second order moments only.|general accept model wrong difficulti determin use use model consid one capabl combin data expert knowledg invers calibr process adequ character uncertainti predict interest paper deriv condit specifi simplifi model use calibr start notion optim simplif defin relat model simplif natur data predict determin standard probabilist calibr scheme capabl accur character uncertainti furthermor two addit condit defin suboptim model determin simplif safe ignor first allow suboptim simplifi model use way replic perform optim model achiev judici select prior term calibr process explicit includ natur data predict model simplif second consid depend structur predict avail data gain insight simplif overcom use right calibr data furthermor deriv condit relat common use calibr scheme base tikhonov subspac regular allow concret insight obtain analysi perform linear expans model equat predict uncertainti character via second order moment onli|['George M. Mathews', 'John Vial']|['stat.ML', 'math.PR', 'physics.comp-ph', 'physics.geo-ph', 'stat.ME', '62F15, 62C10, 68U05, 93E12, 93B11, 62P12']
2017-03-28T14:03:59Z|2017-03-21T12:33:19Z|http://arxiv.org/abs/1703.07169v1|http://arxiv.org/pdf/1703.07169v1|A Deterministic Global Optimization Method for Variational Inference|determinist global optim method variat infer|Variational inference methods for latent variable statistical models have gained popularity because they are relatively fast, can handle large data sets, and have deterministic convergence guarantees. However, in practice it is unclear whether the fixed point identified by the variational inference algorithm is a local or a global optimum. Here, we propose a method for constructing iterative optimization algorithms for variational inference problems that are guaranteed to converge to the $\epsilon$-global variational lower bound on the log-likelihood. We derive inference algorithms for two variational approximations to a standard Bayesian Gaussian mixture model (BGMM). We present a minimal data set for empirically testing convergence and show that a variational inference algorithm frequently converges to a local optimum while our algorithm always converges to the globally optimal variational lower bound. We characterize the loss incurred by choosing a non-optimal variational approximation distribution suggesting that selection of the approximating variational distribution deserves as much attention as the selection of the original statistical model for a given data set.|variat infer method latent variabl statist model gain popular becaus relat fast handl larg data set determinist converg guarante howev practic unclear whether fix point identifi variat infer algorithm local global optimum propos method construct iter optim algorithm variat infer problem guarante converg epsilon global variat lower bound log likelihood deriv infer algorithm two variat approxim standard bayesian gaussian mixtur model bgmm present minim data set empir test converg show variat infer algorithm frequent converg local optimum algorithm alway converg global optim variat lower bound character loss incur choos non optim variat approxim distribut suggest select approxim variat distribut deserv much attent select origin statist model given data set|['Hachem Saddiki', 'Andrew C. Trapp', 'Patrick Flaherty']|['stat.ME', 'stat.ML']
2017-03-28T14:03:59Z|2017-03-21T00:11:45Z|http://arxiv.org/abs/1703.07009v1|http://arxiv.org/pdf/1703.07009v1|New reconstruction and data processing methods for regression and   interpolation analysis of multidimensional big data|new reconstruct data process method regress interpol analysi multidimension big data|The problems of computational data processing involving regression, interpolation, reconstruction and imputation for multidimensional big datasets are becoming more important these days, because of the availability of data and their widely spread usage in business, technological, scientific and other applications. The existing methods often have limitations, which either do not allow, or make it difficult to accomplish many data processing tasks. The problems usually relate to algorithm accuracy, applicability, performance (computational and algorithmic), demands for computational resources, both in terms of power and memory, and difficulty working with high dimensions. Here, we propose a new concept and introduce two methods, which use local area predictors (input data) for finding outcomes. One method uses the gradient based approach, while the second one employs an introduced family of smooth approximating functions. The new methods are free from many drawbacks of existing approaches. They are practical, have very wide range of applicability, provide high accuracy, excellent computational performance, fit for parallel computing, and very well suited for processing high dimension big data. The methods also provide multidimensional outcome, when needed. We present numerical examples of up to one hundred dimensions, and report in detail performance characteristics and various properties of new methods.|problem comput data process involv regress interpol reconstruct imput multidimension big dataset becom import day becaus avail data wide spread usag busi technolog scientif applic exist method often limit either allow make difficult accomplish mani data process task problem usual relat algorithm accuraci applic perform comput algorithm demand comput resourc term power memori difficulti work high dimens propos new concept introduc two method use local area predictor input data find outcom one method use gradient base approach second one employ introduc famili smooth approxim function new method free mani drawback exist approach practic veri wide rang applic provid high accuraci excel comput perform fit parallel comput veri well suit process high dimens big data method also provid multidimension outcom need present numer exampl one hundr dimens report detail perform characterist various properti new method|['Yuri K. Shestopaloff', 'Alexander Y. Shestopaloff']|['stat.ME']
2017-03-28T14:03:59Z|2017-03-20T21:41:20Z|http://arxiv.org/abs/1703.06978v1|http://arxiv.org/pdf/1703.06978v1|A Conditional Density Estimation Partition Model Using Logistic Gaussian   Processes|condit densiti estim partit model use logist gaussian process|Conditional density estimation (density regression) estimates the distribution of a response variable y conditional on covariates x. Utilizing a partition model framework, a conditional density estimation method is proposed using logistic Gaussian processes. The partition is created using a Voronoi tessellation and is learned from the data using a reversible jump Markov chain Monte Carlo algorithm. The Markov chain Monte Carlo algorithm is made possible through a Laplace approximation on the latent variables of the logistic Gaussian process model. This approximation marginalizes the parameters in each partition element, allowing an efficient search of the posterior distribution of the tessellation. The method has desirable consistency properties. In simulation and applications, the model successfully estimates the partition structure and conditional distribution of y.|condit densiti estim densiti regress estim distribut respons variabl condit covari util partit model framework condit densiti estim method propos use logist gaussian process partit creat use voronoi tessel learn data use revers jump markov chain mont carlo algorithm markov chain mont carlo algorithm made possibl laplac approxim latent variabl logist gaussian process model approxim margin paramet partit element allow effici search posterior distribut tessel method desir consist properti simul applic model success estim partit structur condit distribut|['Richard D. Payne', 'Nilabja Guha', 'Yu Ding', 'Bani K. Mallick']|['stat.ME']
2017-03-28T14:03:59Z|2017-03-20T15:45:44Z|http://arxiv.org/abs/1703.06808v1|http://arxiv.org/pdf/1703.06808v1|Worth Weighting? How to Think About and Use Sample Weights in Survey   Experiments|worth weight think use sampl weight survey experi|"The popularity of online surveys has increased the prominence of sampling weights in claims of representativeness. Yet, much uncertainty remains regarding how these weights should be employed in the analysis of survey experiments: Should they be used or ignored? If they are used, which estimators are preferred? We offer practical advice, rooted in the Neyman-Rubin model, for researchers producing and working with survey experimental data. We examine simple, efficient estimators (Horvitz-Thompson, H\`ajek, ""double-H\`ajek"", and post-stratification) for analyzing these data, along with formulae for biases and variances. We provide simulations that examine these estimators and real examples from experiments administered online through YouGov. We find that for examining the existence of population treatment effects using high-quality, broadly representative samples recruited by top online survey firms, sample quantities, which do not rely on weights, are often sufficient. Sample Average Treatment Effect (SATE) estimates are unlikely to differ substantially from weighted estimates, and they avoid the statistical power loss that accompanies weighting. When precise estimates of Population Average Treatment Effects (PATE) are essential, we analytically show post-stratifying on survey weights and/or covariates highly correlated with the outcome to be a conservative choice."|popular onlin survey increas promin sampl weight claim repres yet much uncertainti remain regard weight employ analysi survey experi use ignor use estim prefer offer practic advic root neyman rubin model research produc work survey experiment data examin simpl effici estim horvitz thompson ajek doubl ajek post stratif analyz data along formula bias varianc provid simul examin estim real exampl experi administ onlin yougov find examin exist popul treatment effect use high qualiti broad repres sampl recruit top onlin survey firm sampl quantiti reli weight often suffici sampl averag treatment effect sate estim unlik differ substanti weight estim avoid statist power loss accompani weight precis estim popul averag treatment effect pate essenti analyt show post stratifi survey weight covari high correl outcom conserv choic|['Luke W. Miratrix', 'Jasjeet S. Sekhon', 'Alexander G. Theodoridis', 'Luis F. Campos']|['stat.ME', 'stat.AP']
2017-03-28T14:03:59Z|2017-03-20T08:51:47Z|http://arxiv.org/abs/1703.06633v1|http://arxiv.org/pdf/1703.06633v1|Variational inference for probabilistic Poisson PCA|variat infer probabilist poisson pca|Many application domains such as ecology or genomics have to deal with multivariate non Gaussian observations. A typical example is the joint observation of the respective abundances of a set of species in a series of sites, aiming to understand the co-variations between these species. The Gaussian setting provides a canonical way to model such dependencies, but does not apply in general. We consider here the multivariate exponential family framework for which we introduce a generic model with multivariate Gaussian latent variables. We show that approximate maximum likelihood inference can be achieved via a variational algorithm for which gradient descent easily applies. We show that this setting enables us to account for covariates and offsets. We then focus on the case of the Poisson-lognormal model in the context of community ecology.|mani applic domain ecolog genom deal multivari non gaussian observ typic exampl joint observ respect abund set speci seri site aim understand co variat speci gaussian set provid canon way model depend doe appli general consid multivari exponenti famili framework introduc generic model multivari gaussian latent variabl show approxim maximum likelihood infer achiev via variat algorithm gradient descent easili appli show set enabl us account covari offset focus case poisson lognorm model context communiti ecolog|['Julien Chiquet', 'Mahendra Mariadassou', 'Stéphane Robin']|['stat.ME']
2017-03-28T14:04:03Z|2017-03-20T02:04:24Z|http://arxiv.org/abs/1703.06559v1|http://arxiv.org/pdf/1703.06559v1|Adaptive p-values after cross-validation|adapt valu cross valid|We describe a way to construct hypothesis tests and confidence intervals after having used the Lasso for feature selection, allowing the regularization parameter to be chosen via an estimate of prediction error. Our estimate of prediction error is a slight variation on cross-validation. Using this variation, we are able to describe an appropriate selection event for choosing a parameter by cross-validation. Adjusting for this selection event, we derive a pivotal quantity that has an asymptotically Unif(0,1) distribution which can be used to test hypotheses or construct intervals. To enhance power, we consider the randomized Lasso with cross-validation. We derive a similar test statistic and develop MCMC sampling scheme to construct valid post-selective confidence intervals empirically. Finally, we demonstrate via simulation that our procedure achieves high-statistical power and FDR control, yielding results comparable to knockoffs (in simulations favorable to knockoffs).|describ way construct hypothesi test confid interv use lasso featur select allow regular paramet chosen via estim predict error estim predict error slight variat cross valid use variat abl describ appropri select event choos paramet cross valid adjust select event deriv pivot quantiti asymptot unif distribut use test hypothes construct interv enhanc power consid random lasso cross valid deriv similar test statist develop mcmc sampl scheme construct valid post select confid interv empir final demonstr via simul procedur achiev high statist power fdr control yield result compar knockoff simul favor knockoff|['Jelena Markovic', 'Lucy Xia', 'Jonathan Taylor']|['stat.ME']
2017-03-28T14:04:03Z|2017-03-20T02:01:02Z|http://arxiv.org/abs/1703.06558v1|http://arxiv.org/pdf/1703.06558v1|Using maximum entry-wise deviation to test the goodness-of-fit for   stochastic block models|use maximum entri wise deviat test good fit stochast block model|The stochastic block model is widely used for detecting community structures in network data. How to test the goodness-of-fit of the model is one of the fundamental problems and has gained growing interests in recent years. In this paper, we propose a new goodness-of-fit test based on the maximum entry of the centered and re-scaled observed adjacency matrix for the stochastic block model in which the number of communities can be allowed to grow linearly with the number of nodes ignoring a logarithm factor. We demonstrate that its asymptotic null distribution is the Gumbel distribution. Our results can also be extended to the degree corrected block model. Numerical studies indicate the proposed method works well.|stochast block model wide use detect communiti structur network data test good fit model one fundament problem gain grow interest recent year paper propos new good fit test base maximum entri center scale observ adjac matrix stochast block model number communiti allow grow linear number node ignor logarithm factor demonstr asymptot null distribut gumbel distribut result also extend degre correct block model numer studi indic propos method work well|['Jianwei Hu', 'Hong Qin', 'Ting Yan', 'Ji Zhu']|['stat.ME']
2017-03-28T14:04:03Z|2017-03-19T10:51:41Z|http://arxiv.org/abs/1703.06419v1|http://arxiv.org/pdf/1703.06419v1|Multivariate Functional Data Visualization and Outlier Detection|multivari function data visual outlier detect|This article proposes a new graphical tool, the magnitude-shape (MS) plot, for visualizing both the magnitude and shape outlyingness of multivariate functional data. The proposed tool builds on the recent notion of functional directional outlyingness, which measures the centrality of functional data by simultaneously considering the level and the direction of their deviation from the central region. The MS-plot intuitively presents not only levels but also directions of magnitude outlyingness on the horizontal axis or plane, and demonstrates shape outlyingness on the vertical axis. A dividing curve or surface is provided to separate non-outlying data from the outliers. Both the simulated data and the practical examples confirm that the MS-plot is superior to existing tools for visualizing centrality and detecting outliers for functional data.|articl propos new graphic tool magnitud shape ms plot visual magnitud shape outlying multivari function data propos tool build recent notion function direct outlying measur central function data simultan consid level direct deviat central region ms plot intuit present onli level also direct magnitud outlying horizont axi plane demonstr shape outlying vertic axi divid curv surfac provid separ non data outlier simul data practic exampl confirm ms plot superior exist tool visual central detect outlier function data|['Wenlin Dai', 'Marc G. Genton']|['stat.ME', 'stat.CO']
2017-03-28T14:04:03Z|2017-03-19T10:36:53Z|http://arxiv.org/abs/1703.06417v1|http://arxiv.org/pdf/1703.06417v1|Spectral analysis of stationary random bivariate signals|spectral analysi stationari random bivari signal|A novel approach towards the spectral analysis of stationary random bivariate signals is proposed. Using the Quaternion Fourier Transform, we introduce a quaternion-valued spectral representation of random bivariate signals seen as complex-valued sequences. This makes possible the definition of a scalar quaternion-valued spectral density for bivariate signals. This spectral density can be meaningfully interpreted in terms of frequency-dependent polarization attributes. A natural decomposition of any random bivariate signal in terms of unpolarized and polarized components is introduced. Nonparametric spectral density estimation is investigated, and we introduce the polarization periodogram of a random bivariate signal. Numerical experiments support our theoretical analysis, illustrating the relevance of the approach on synthetic data.|novel approach toward spectral analysi stationari random bivari signal propos use quaternion fourier transform introduc quaternion valu spectral represent random bivari signal seen complex valu sequenc make possibl definit scalar quaternion valu spectral densiti bivari signal spectral densiti meaning interpret term frequenc depend polar attribut natur decomposit ani random bivari signal term unpolar polar compon introduc nonparametr spectral densiti estim investig introduc polar periodogram random bivari signal numer experi support theoret analysi illustr relev approach synthet data|['Julien Flamant', 'Nicolas Le Bihan', 'Pierre Chainais']|['stat.ME']
2017-03-28T14:04:03Z|2017-03-19T01:29:12Z|http://arxiv.org/abs/1703.06379v1|http://arxiv.org/pdf/1703.06379v1|Penalized pairwise pseudo likelihood for variable selection with   nonignorable missing data|penal pairwis pseudo likelihood variabl select nonignor miss data|The regularization approach for variable selection was well developed for a completely observed data set in the past two decades. In the presence of missing values, this approach needs to be tailored to different missing data mechanisms. In this paper, we focus on a flexible and generally applicable missing data mechanism, which contains both ignorable and nonignorable missing data mechanism assumptions. We show how the regularization approach for variable selection can be adapted to the situation under this missing data mechanism. The computational and theoretical properties for variable selection consistency are established. The proposed method is further illustrated by comprehensive simulation studies and real data analyses, for both low and high dimensional settings.|regular approach variabl select well develop complet observ data set past two decad presenc miss valu approach need tailor differ miss data mechan paper focus flexibl general applic miss data mechan contain ignor nonignor miss data mechan assumpt show regular approach variabl select adapt situat miss data mechan comput theoret properti variabl select consist establish propos method illustr comprehens simul studi real data analys low high dimension set|['Jiwei Zhao', 'Yang Yang', 'Yang Ning']|['stat.ME']
2017-03-28T14:04:03Z|2017-03-18T19:00:23Z|http://arxiv.org/abs/1703.06336v1|http://arxiv.org/pdf/1703.06336v1|Analysis of error control in large scale two-stage multiple hypothesis   testing|analysi error control larg scale two stage multipl hypothesi test|"When dealing with the problem of simultaneously testing a large number of null hypotheses, a natural testing strategy is to first reduce the number of tested hypotheses by some selection (screening or filtering) process, and then to simultaneously test the selected hypotheses. The main advantage of this strategy is to greatly reduce the severe effect of high dimensions. However, the first screening or selection stage must be properly accounted for in order to maintain some type of error control. In this paper, we will introduce a selection rule based on a selection statistic that is independent of the test statistic when the tested hypothesis is true. Combining this selection rule and the conventional Bonferroni procedure, we can develop a powerful and valid two-stage procedure. The introduced procedure has several nice properties: (i) it completely removes the selection effect; (ii) it reduces the multiplicity effect; (iii) it does not ""waste"" data while carrying out both selection and testing. Asymptotic power analysis and simulation studies illustrate that this proposed method can provide higher power compared to usual multiple testing methods while controlling the Type 1 error rate. Optimal selection thresholds are also derived based on our asymptotic analysis."|deal problem simultan test larg number null hypothes natur test strategi first reduc number test hypothes select screen filter process simultan test select hypothes main advantag strategi great reduc sever effect high dimens howev first screen select stage must proper account order maintain type error control paper introduc select rule base select statist independ test statist test hypothesi true combin select rule convent bonferroni procedur develop power valid two stage procedur introduc procedur sever nice properti complet remov select effect ii reduc multipl effect iii doe wast data carri select test asymptot power analysi simul studi illustr propos method provid higher power compar usual multipl test method control type error rate optim select threshold also deriv base asymptot analysi|['Wenge Guo', 'Joseph P. Romano']|['stat.ME', 'math.ST', 'stat.TH', '62J15']
2017-03-28T14:04:03Z|2017-03-18T09:10:51Z|http://arxiv.org/abs/1703.06277v1|http://arxiv.org/pdf/1703.06277v1|Unsupervised Learning of Mixture Regression Models for Longitudinal Data|unsupervis learn mixtur regress model longitudin data|"This paper is concerned with learning of mixture regression models for individuals that are measured repeatedly. The adjective ""unsupervised"" implies that the number of mixing components is unknown and has to be determined, ideally by data driven tools. For this purpose, a novel penalized method is proposed to simultaneously select the number of mixing components and to estimate the mixing proportions and unknown parameters in the models. The proposed method is capable of handling both continuous and discrete responses by only requiring the first two moment conditions of the model distribution. It is shown to be consistent in both selecting the number of components and estimating the mixing proportions and unknown regression parameters. Further, a modified EM algorithm is developed to seamlessly integrate model selection and estimation. Simulation studies are conducted to evaluate the finite sample performance of the proposed procedure. And it is further illustrated via an analysis of a primary biliary cirrhosis data set."|paper concern learn mixtur regress model individu measur repeat adject unsupervis impli number mix compon unknown determin ideal data driven tool purpos novel penal method propos simultan select number mix compon estim mix proport unknown paramet model propos method capabl handl continu discret respons onli requir first two moment condit model distribut shown consist select number compon estim mix proport unknown regress paramet modifi em algorithm develop seamless integr model select estim simul studi conduct evalu finit sampl perform propos procedur illustr via analysi primari biliari cirrhosi data set|['Peirong Xu', 'Heng Peng', 'Tao Huang']|['stat.ME']
2017-03-28T14:04:03Z|2017-03-18T00:37:18Z|http://arxiv.org/abs/1703.06226v1|http://arxiv.org/pdf/1703.06226v1|Identifying the Support of Rectangular Signals in Gaussian Noise|identifi support rectangular signal gaussian nois|We consider the problem of identifying the support of the block signal in a sequence when both the length and the location of the block signal are unknown. The multivariate version of this problem is also considered, in which we try to identify the support of the rectangular signal in the hyper- rectangle. We allow the length of the block signal to grow polynomially with the length of the sequence, which greatly generalizes the previous results in [16]. A statistical boundary above which the identification is possible is presented and an asymptotically optimal and computationally efficient procedure is proposed under Gaussian white noise in both the univariate and multivariate settings. The problem of block signal identification is shown to have the same statistical difficulty as the corresponding problem of detection in both the univariate and multivariate cases, in the sense that whenever we can detect the signal, we can identify the support of the signal. Some generalizations are also considered here: (1) We ex- tend our theory to the case of multiple block signals. (2) We also discuss about the robust identification problem when the noise distribution is un- specified and the block signal identification problem under the exponential family setting.|consid problem identifi support block signal sequenc length locat block signal unknown multivari version problem also consid tri identifi support rectangular signal hyper rectangl allow length block signal grow polynomi length sequenc great general previous result statist boundari abov identif possibl present asymptot optim comput effici procedur propos gaussian white nois univari multivari set problem block signal identif shown statist difficulti correspond problem detect univari multivari case sens whenev detect signal identifi support signal general also consid ex tend theori case multipl block signal also discuss robust identif problem nois distribut un specifi block signal identif problem exponenti famili set|['Jiyao Kou']|['stat.ME']
2017-03-28T14:04:03Z|2017-03-18T00:08:59Z|http://arxiv.org/abs/1703.06222v1|http://arxiv.org/pdf/1703.06222v1|A Unified Treatment of Multiple Testing with Prior Knowledge|unifi treatment multipl test prior knowledg|A significant literature has arisen to study ways to employing prior knowledge to improve power and precision of multiple testing procedures. Some common forms of prior knowledge may include (a) a priori beliefs about which hypotheses are null, modeled by non-uniform prior weights; (b) differing importances of hypotheses, modeled by differing penalties for false discoveries; (c) partitions of the hypotheses into known groups, indicating (dis)similarity of hypotheses; and (d) knowledge of independence, positive dependence or arbitrary dependence between hypotheses or groups, allowing for more aggressive or conservative procedures. We present a general framework for global null testing and false discovery rate (FDR) control that allows the scientist to incorporate all four types of prior knowledge (a)-(d) simultaneously. We unify a number of existing procedures, generalize the conditions under which they are known to work, and simplify their proofs of FDR control under independence, positive and arbitrary dependence. We also present an algorithmic framework that strictly generalizes and unifies the classic algorithms of Benjamini and Hochberg [3] and Simes [25], algorithms that guard against unknown dependence [7, 9], algorithms that employ prior weights [17, 15], algorithms that use penalty weights [4], algorithms that incorporate null-proportion adaptivity [26, 27], and algorithms that make use of multiple arbitrary partitions into groups [1]. Unlike this previous work, we can simultaneously incorporate all of the four types of prior knowledge, combined with all of the three forms of dependence.|signific literatur arisen studi way employ prior knowledg improv power precis multipl test procedur common form prior knowledg may includ priori belief hypothes null model non uniform prior weight differ import hypothes model differ penalti fals discoveri partit hypothes known group indic dis similar hypothes knowledg independ posit depend arbitrari depend hypothes group allow aggress conserv procedur present general framework global null test fals discoveri rate fdr control allow scientist incorpor four type prior knowledg simultan unifi number exist procedur general condit known work simplifi proof fdr control independ posit arbitrari depend also present algorithm framework strict general unifi classic algorithm benjamini hochberg sime algorithm guard unknown depend algorithm employ prior weight algorithm use penalti weight algorithm incorpor null proport adapt algorithm make use multipl arbitrari partit group unlik previous work simultan incorpor four type prior knowledg combin three form depend|['Aaditya Ramdas', 'Rina Foygel Barber', 'Martin J. Wainwright', 'Michael I. Jordan']|['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']
2017-03-28T14:04:03Z|2017-03-17T19:23:10Z|http://arxiv.org/abs/1703.06176v1|http://arxiv.org/pdf/1703.06176v1|Sampling from a pseudo selective posterior using a primal-dual approach|sampl pseudo select posterior use primal dual approach|Adopting the Bayesian methodology of adjusting for selection to provide valid inference in Panigrahi (2016), the current work proposes an approximation to a selective posterior, post randomized queries on data. Such a posterior differs from the usual one as it involves a truncated likelihood prepended with a prior belief on parameters in a Bayesian model. The truncation, imposed by selection, leads to intractability of the selective posterior, thereby posing a technical hurdle in sampling from such a posterior. We derive an optimization problem to approximate the otherwise intractable posterior, the efficiency of a sampler targeting the pseudo selective posterior depends on the computational cost involved in solving the approximating optimization problem. We adopt a primal-dual approach in the current work to obtain a reduced optimization problem that allows for scalable Bayesian inference in both low and high dimensional regimes.|adopt bayesian methodolog adjust select provid valid infer panigrahi current work propos approxim select posterior post random queri data posterior differ usual one involv truncat likelihood prepend prior belief paramet bayesian model truncat impos select lead intract select posterior therebi pose technic hurdl sampl posterior deriv optim problem approxim otherwis intract posterior effici sampler target pseudo select posterior depend comput cost involv solv approxim optim problem adopt primal dual approach current work obtain reduc optim problem allow scalabl bayesian infer low high dimension regim|['Snigdha Panigrahi', 'Jonathan Taylor']|['stat.ME']
2017-03-28T14:04:07Z|2017-03-17T18:31:49Z|http://arxiv.org/abs/1703.06154v1|http://arxiv.org/pdf/1703.06154v1|An MCMC free approach to post-selective inference|mcmc free approach post select infer|The current work proposes a Monte Carlo free alternative to inference post randomized selection algorithms with a convex loss and a convex penalty. The pivots based on the selective law that is truncated to all selected realizations, typically lack closed form expressions in randomized settings. Inference in these settings relies upon standard Monte Carlo sampling techniques, which can be prove to be unstable for parameters far off from the chosen reference distribution. This work offers an approximation to the selective pivot based on a pseudo selective law and consequently, provides valid inference as confidence intervals and the selective MLE; such an approximation takes the form of an optimization problem in  E  dimensions, where  E  is the size of the active set observed from selection. The guarantees of inference are valid coverage of confidence intervals based on inverting the approximate pivot, which have the additional merit of lengths being comparable to the unadjusted intervals with inducted randomization in the selective analysis.|current work propos mont carlo free altern infer post random select algorithm convex loss convex penalti pivot base select law truncat select realize typic lack close form express random set infer set reli upon standard mont carlo sampl techniqu prove unstabl paramet far chosen refer distribut work offer approxim select pivot base pseudo select law consequ provid valid infer confid interv select mle approxim take form optim problem dimens size activ set observ select guarante infer valid coverag confid interv base invert approxim pivot addit merit length compar unadjust interv induct random select analysi|['Snigdha Panigrahi', 'Jelena Markovic', 'Jonathan Taylor']|['stat.ME']
2017-03-28T14:04:07Z|2017-03-17T17:50:44Z|http://arxiv.org/abs/1703.06131v1|http://arxiv.org/pdf/1703.06131v1|Inference via low-dimensional couplings|infer via low dimension coupl|"Integration against an intractable probability measure is among the fundamental challenges of statistical inference, particularly in the Bayesian setting. A principled approach to this problem seeks a deterministic coupling of the measure of interest with a tractable ""reference"" measure (e.g., a standard Gaussian). This coupling is induced by a transport map, and enables direct simulation from the desired measure simply by evaluating the transport map at samples from the reference. Yet characterizing such a map---e.g., representing and evaluating it---grows challenging in high dimensions. The central contribution of this paper is to establish a link between the Markov properties of the target measure and the existence of certain low-dimensional couplings, induced by transport maps that are sparse or decomposable. Our analysis not only facilitates the construction of couplings in high-dimensional settings, but also suggests new inference methodologies. For instance, in the context of nonlinear and non-Gaussian state space models, we describe new online and single-pass variational algorithms that characterize the full posterior distribution of the sequential inference problem using operations only slightly more complex than regular filtering."|integr intract probabl measur among fundament challeng statist infer particular bayesian set principl approach problem seek determinist coupl measur interest tractabl refer measur standard gaussian coupl induc transport map enabl direct simul desir measur simpli evalu transport map sampl refer yet character map repres evalu grow challeng high dimens central contribut paper establish link markov properti target measur exist certain low dimension coupl induc transport map spars decompos analysi onli facilit construct coupl high dimension set also suggest new infer methodolog instanc context nonlinear non gaussian state space model describ new onlin singl pass variat algorithm character full posterior distribut sequenti infer problem use oper onli slight complex regular filter|['Alessio Spantini', 'Daniele Bigoni', 'Youssef Marzouk']|['stat.ME', 'stat.CO', 'stat.ML']
2017-03-28T14:04:07Z|2017-03-17T17:00:53Z|http://arxiv.org/abs/1703.06098v1|http://arxiv.org/pdf/1703.06098v1|Analysis of the Gibbs Sampler for Gaussian hierarchical models via   multigrid decomposition|analysi gibb sampler gaussian hierarch model via multigrid decomposit|We study the convergence properties of the Gibbs Sampler in the context of posterior distributions arising from Bayesian analysis of Gaussian hierarchical models. We consider centred and non-centred parameterizations as well as their hybrids including the full family of partially non-centred parameterizations. We develop a novel methodology based on multi-grid decompositions to derive analytic expressions for the convergence rates of the algorithm for an arbitrary number of layers in the hierarchy, while previous work was typically limited to the two-level case. Our work gives a complete understanding for the three-level symmetric case and this gives rise to approximations for the non-symmetric case. We also give analogous, if less explicit, results for models of arbitrary level. This theory gives rise to simple and easy-to-implement guidelines for the practical implementation of Gibbs samplers on conditionally Gaussian hierarchical models.|studi converg properti gibb sampler context posterior distribut aris bayesian analysi gaussian hierarch model consid centr non centr parameter well hybrid includ full famili partial non centr parameter develop novel methodolog base multi grid decomposit deriv analyt express converg rate algorithm arbitrari number layer hierarchi previous work typic limit two level case work give complet understand three level symmetr case give rise approxim non symmetr case also give analog less explicit result model arbitrari level theori give rise simpl easi implement guidelin practic implement gibb sampler condit gaussian hierarch model|['Giacomo Zanella', 'Gareth Roberts']|['stat.CO', 'math.PR', 'stat.ME', '60J22, 62F15, 65C40, 65C05']
2017-03-28T14:04:07Z|2017-03-17T16:30:08Z|http://arxiv.org/abs/1703.06086v1|http://arxiv.org/pdf/1703.06086v1|Propensity score weighting for causal inference with clustered data|propens score weight causal infer cluster data|Propensity score weighting is a tool for causal inference to adjust for measured confounders in observational studies. In practice, data often present complex structures, such as clustering, which make propensity score modeling and estimation challenging. In addition, for clustered data, there may be unmeasured cluster-specific variables that are related to both the treatment assignment and the outcome. When such unmeasured cluster-specific confounders exist and are omitted in the propensity score model, the subsequent propensity score adjustment may be biased. In this article, we propose a calibration technique for propensity score estimation under the latent ignorable treatment assignment mechanism, i.e., the treatment-outcome relationship is unconfounded given the observed covariates and the latent cluster effects. We then provide a consistent propensity score weighting estimator of the average treatment effect when the propensity score and outcome follow generalized linear mixed effects models. The proposed propensity score weighting estimator is attractive, because it does not require specification of functional forms of the propensity score and outcome models, and therefore is robust to model misspecification. The proposed weighting method can be combined with sampling weights for an integrated solution to handle confounding and sampling designs for causal inference with clustered survey data. In simulation studies, we show that the proposed estimator is superior to other competitors. We estimate the effect of School Body Mass Index Screening on prevalence of overweight and obesity for elementary schools in Pennsylvania.|propens score weight tool causal infer adjust measur confound observ studi practic data often present complex structur cluster make propens score model estim challeng addit cluster data may unmeasur cluster specif variabl relat treatment assign outcom unmeasur cluster specif confound exist omit propens score model subsequ propens score adjust may bias articl propos calibr techniqu propens score estim latent ignor treatment assign mechan treatment outcom relationship unconfound given observ covari latent cluster effect provid consist propens score weight estim averag treatment effect propens score outcom follow general linear mix effect model propos propens score weight estim attract becaus doe requir specif function form propens score outcom model therefor robust model misspecif propos weight method combin sampl weight integr solut handl confound sampl design causal infer cluster survey data simul studi show propos estim superior competitor estim effect school bodi mass index screen preval overweight obes elementari school pennsylvania|['Shu Yang']|['stat.ME']
2017-03-28T14:04:07Z|2017-03-17T14:34:01Z|http://arxiv.org/abs/1703.06031v1|http://arxiv.org/pdf/1703.06031v1|Modeling spatial processes with unknown extremal dependence class|model spatial process unknown extrem depend class|Many environmental processes exhibit weakening spatial dependence as events become more extreme. Well-known limiting models, such as max-stable or generalized Pareto processes, cannot capture this, which can lead to a preference for models that exhibit a property known as asymptotic independence. However, weakening dependence does not automatically imply asymptotic independence, and whether the process is truly asymptotically (in)dependent is usually far from clear. The distinction is key as it can have a large impact upon extrapolation, i.e., the estimated probabilities of events more extreme than those observed. In this work, we present a single spatial model that is able to capture both dependence classes in a parsimonious manner, and with a smooth transition between the two cases. The model covers a wide range of possibilities from asymptotic independence through to complete dependence, and permits weakening dependence of extremes even under asymptotic dependence. Censored likelihood-based inference for the implied copula is feasible in moderate dimensions due to closed-form margins. The model is applied to oceanographic datasets with ambiguous true limiting dependence structure.|mani environment process exhibit weaken spatial depend event becom extrem well known limit model max stabl general pareto process cannot captur lead prefer model exhibit properti known asymptot independ howev weaken depend doe automat impli asymptot independ whether process truli asymptot depend usual far clear distinct key larg impact upon extrapol estim probabl event extrem observ work present singl spatial model abl captur depend class parsimoni manner smooth transit two case model cover wide rang possibl asymptot independ complet depend permit weaken depend extrem even asymptot depend censor likelihood base infer impli copula feasibl moder dimens due close form margin model appli oceanograph dataset ambigu true limit depend structur|['Raphaël G. Huser', 'Jennifer L. Wadsworth']|['stat.ME', '62G32, 62M30']
2017-03-28T14:04:07Z|2017-03-17T13:19:23Z|http://arxiv.org/abs/1703.06001v1|http://arxiv.org/pdf/1703.06001v1|The use of spatial information in entropy measures|use spatial inform entropi measur|The concept of entropy, firstly introduced in information theory, rapidly became popular in many applied sciences via Shannon's formula to measure the degree of heterogeneity among observations. A rather recent research field aims at accounting for space in entropy measures, as a generalization when the spatial location of occurrences ought to be accounted for. The main limit of these developments is that all indices are computed conditional on a chosen distance. This work follows and extends the route for including spatial components in entropy measures. Starting from the probabilistic properties of Shannon's entropy for categorical variables, it investigates the characteristics of the quantities known as residual entropy and mutual information, when space is included as a second dimension. This way, the proposal of entropy measures based on univariate distributions is extended to the consideration of bivariate distributions, in a setting where the probabilistic meaning of all components is well defined. As a direct consequence, a spatial entropy measure satisfying the additivity property is obtained, as global residual entropy is a sum of partial entropies based on different distance classes. Moreover, the quantity known as mutual information measures the information brought by the inclusion of space, and also has the property of additivity. A thorough comparative study illustrates the superiority of the proposed indices.|concept entropi first introduc inform theori rapid becam popular mani appli scienc via shannon formula measur degre heterogen among observ rather recent research field aim account space entropi measur general spatial locat occurr ought account main limit develop indic comput condit chosen distanc work follow extend rout includ spatial compon entropi measur start probabilist properti shannon entropi categor variabl investig characterist quantiti known residu entropi mutual inform space includ second dimens way propos entropi measur base univari distribut extend consider bivari distribut set probabilist mean compon well defin direct consequ spatial entropi measur satisfi addit properti obtain global residu entropi sum partial entropi base differ distanc class moreov quantiti known mutual inform measur inform brought inclus space also properti addit thorough compar studi illustr superior propos indic|['Linda Altieri', 'Daniela Cocchi', 'Giulia Roli']|['stat.ME']
2017-03-28T14:04:07Z|2017-03-17T05:38:07Z|http://arxiv.org/abs/1703.05899v1|http://arxiv.org/pdf/1703.05899v1|Decomposition analysis to identify intervention targets for reducing   disparities|decomposit analysi identifi intervent target reduc dispar|There has been considerable interest in using decomposition methods in epidemiology (mediation analysis) and economics (Oaxaca-Blinder decomposition) to understand how health disparities arise and how they might change upon intervention. It has not been clear when estimates from the Oaxaca-Blinder decomposition can be interpreted causally because its implementation does not explicitly address potential confounding of target variables. While mediation analysis does explicitly adjust for confounders of target variables, it does so in a way that entails equalizing confounders across racial groups, which may not reflect the intended intervention. Revisiting prior analyses in the National Longitudinal Survey of Youth on disparities in wages, unemployment, incarceration, and overall health with test scores, taken as a proxy for educational attainment, as a target intervention, we propose and demonstrate a novel decomposition that controls for confounders of test scores (measures of childhood SES) while leaving their association with race intact. We compare this decomposition with others that use standardization (to equalize childhood SES alone), mediation analysis (to equalize test scores within levels of childhood SES), and one that equalizes both childhood SES and test scores. We also show how these decompositions, including our novel proposals, are equivalent to causal implementations of the Oaxaca-Blinder decomposition.|consider interest use decomposit method epidemiolog mediat analysi econom oaxaca blinder decomposit understand health dispar aris might chang upon intervent clear estim oaxaca blinder decomposit interpret causal becaus implement doe explicit address potenti confound target variabl mediat analysi doe explicit adjust confound target variabl doe way entail equal confound across racial group may reflect intend intervent revisit prior analys nation longitudin survey youth dispar wage unemploy incarcer overal health test score taken proxi educ attain target intervent propos demonstr novel decomposit control confound test score measur childhood ses leav associ race intact compar decomposit use standard equal childhood ses alon mediat analysi equal test score within level childhood ses one equal childhood ses test score also show decomposit includ novel propos equival causal implement oaxaca blinder decomposit|['John W. Jackson', 'Tyler J. VanderWeele']|['stat.ME']
2017-03-28T14:04:07Z|2017-03-16T23:33:24Z|http://arxiv.org/abs/1703.05849v1|http://arxiv.org/pdf/1703.05849v1|Causal Inference through the Method of Direct Estimation|causal infer method direct estim|The intersection of causal inference and machine learning is a rapidly advancing field. We propose a new approach, the method of direct estimation, that draws on both traditions in order to obtain nonparametric estimates of treatment effects. The approach focuses on estimating the effect of fluctuations in a treatment variable on an outcome. A tensor-spline implementation enables rich interactions between functional bases allowing for the approach to capture treatment/covariate interactions. We show how new innovations in Bayesian sparse modeling readily handle the proposed framework, and then document its performance in simulation and applied examples. Furthermore we show how the method of direct estimation can easily extend to structural estimators commonly used in a variety of disciplines, like instrumental variables, mediation analysis, and sequential g-estimation.|intersect causal infer machin learn rapid advanc field propos new approach method direct estim draw tradit order obtain nonparametr estim treatment effect approach focus estim effect fluctuat treatment variabl outcom tensor spline implement enabl rich interact function base allow approach captur treatment covari interact show new innov bayesian spars model readili handl propos framework document perform simul appli exampl furthermor show method direct estim easili extend structur estim common use varieti disciplin like instrument variabl mediat analysi sequenti estim|['Marc Ratkovic', 'Dustin Tingley']|['stat.ML', 'stat.ME', '62G08, 46N30, 62P20, 62P25']
2017-03-28T14:04:07Z|2017-03-16T19:04:22Z|http://arxiv.org/abs/1703.05794v1|http://arxiv.org/pdf/1703.05794v1|Incorporating Covariates into Integrated Factor Analysis of Multi-View   Data|incorpor covari integr factor analysi multi view data|In modern biomedical research, it is ubiquitous to have multiple data sets measured on the same set of samples from different views (i.e., multi-view data). For example, in genetic studies, multiple genomic data sets at different molecular levels or from different cell types are measured for a common set of individuals to investigate genetic regulation. Integration and reduction of multi-view data have the potential to leverage information in different data sets, and to reduce the magnitude and complexity of data for further statistical analysis and interpretation. In this paper, we develop a novel statistical model, called supervised integrated factor analysis (SIFA), for integrative dimension reduction of multi-view data while incorporating auxiliary covariates. The model decomposes data into joint and individual factors, capturing the joint variation across multiple data sets and the individual variation specific to each set respectively. Moreover, both joint and individual factors are partially informed by auxiliary covariates via nonparametric models. We devise a computationally efficient Expectation-Maximization (EM) algorithm to fit the model under some identifiability conditions. We apply the method to the Genotype-Tissue Expression (GTEx) data, and provide new insights into the variation decomposition of gene expression in multiple tissues. Extensive simulation studies and an additional application to a pediatric growth study demonstrate the advantage of the proposed method over competing methods.|modern biomed research ubiquit multipl data set measur set sampl differ view multi view data exampl genet studi multipl genom data set differ molecular level differ cell type measur common set individu investig genet regul integr reduct multi view data potenti leverag inform differ data set reduc magnitud complex data statist analysi interpret paper develop novel statist model call supervis integr factor analysi sifa integr dimens reduct multi view data incorpor auxiliari covari model decompos data joint individu factor captur joint variat across multipl data set individu variat specif set respect moreov joint individu factor partial inform auxiliari covari via nonparametr model devis comput effici expect maxim em algorithm fit model identifi condit appli method genotyp tissu express gtex data provid new insight variat decomposit gene express multipl tissu extens simul studi addit applic pediatr growth studi demonstr advantag propos method compet method|['Gen Li', 'Sungkyu Jung']|['stat.ME']
2017-03-28T14:04:07Z|2017-03-16T18:14:59Z|http://arxiv.org/abs/1703.05782v1|http://arxiv.org/pdf/1703.05782v1|Distributed Multi-Speaker Voice Activity Detection for Wireless Acoustic   Sensor Networks|distribut multi speaker voic activ detect wireless acoust sensor network|A distributed multi-speaker voice activity detection (DM-VAD) method for wireless acoustic sensor networks (WASNs) is proposed. DM-VAD is required in many signal processing applications, e.g. distributed speech enhancement based on multi-channel Wiener filtering, but is non-existent up to date. The proposed method neither requires a fusion center nor prior knowledge about the node positions, microphone array orientations or the number of observed sources. It consists of two steps: (i) distributed source-specific energy signal unmixing (ii) energy signal based voice activity detection. Existing computationally efficient methods to extract source-specific energy signals from the mixed observations, e.g., multiplicative non-negative independent component analysis (MNICA) quickly loose performance with an increasing number of sources, and require a fusion center. To overcome these limitations, we introduce a distributed energy signal unmixing method based on a source-specific node clustering method to locate the nodes around each source. To determine the number of sources that are observed in the WASN, a source enumeration method that uses a Lasso penalized Poisson generalized linear model is developed. Each identified cluster estimates the energy signal of a single (dominant) source by applying a two-component MNICA. The VAD problem is transformed into a clustering task, by extracting features from the energy signals and applying K-means type clustering algorithms. All steps of the proposed method are evaluated using numerical experiments. A VAD accuracy of $> 85 \%$ is achieved for a challenging scenario where 20 nodes observe 7 sources in a simulated reverberant rectangular room.|distribut multi speaker voic activ detect dm vad method wireless acoust sensor network propos dm vad requir mani signal process applic distribut speech enhanc base multi channel wiener filter non exist date propos method neither requir fusion center prior knowledg node posit microphon array orient number observ sourc consist two step distribut sourc specif energi signal unmix ii energi signal base voic activ detect exist comput effici method extract sourc specif energi signal mix observ multipl non negat independ compon analysi mnica quick loos perform increas number sourc requir fusion center overcom limit introduc distribut energi signal unmix method base sourc specif node cluster method locat node around sourc determin number sourc observ sourc enumer method use lasso penal poisson general linear model develop identifi cluster estim energi signal singl domin sourc appli two compon mnica vad problem transform cluster task extract featur energi signal appli mean type cluster algorithm step propos method evalu use numer experi vad accuraci achiev challeng scenario node observ sourc simul reverber rectangular room|['Mohamad Hasan Bahari', 'L. Khadidja Hamaidi', 'Michael Muma', 'Jorge Plata-Chaves', 'Marc Moonen', 'Abdelhak M. Zoubir', 'Alexander Bertrand']|['stat.ME']
2017-03-28T14:04:12Z|2017-03-15T17:03:56Z|http://arxiv.org/abs/1703.05264v1|http://arxiv.org/pdf/1703.05264v1|Smooth Image-on-Scalar Regression for Brain Mapping|smooth imag scalar regress brain map|Brain mapping is an increasingly important tool in neurology and psychiatry researches for the realization of data-driven personalized medicine in the big data era, which learns the statistical links between brain images and subject level features. Taking images as responses, the task raises a lot of challenges due to the high dimensionality of the image with relatively small number of samples, as well as the noisiness of measurements in medical images.   In this paper we propose a novel method {\it Smooth Image-on-scalar Regression} (SIR) for recovering the true association between an image outcome and scalar predictors. The estimator is achieved by minimizing a mean squared error with a total variation (TV) regularization term on the predicted mean image across all subjects. It denoises the images from all subjects and at the same time returns the coefficient maps estimation. We propose an algorithm to solve this optimization problem, which is efficient when combined with recent advances in graph fused lasso solvers. The statistical consistency of the estimator is shown via an oracle inequality.   Simulation results demonstrate that the proposed method outperforms existing methods with separate denoising and regression steps. Especially, SIR shows an evident advantage in recovering signals in small regions. We apply SIR on Alzheimer's Disease Neuroimaging Initiative data and produce interpretable brain maps of the PET image to patient-level features include age, gender, genotype and disease groups.|brain map increas import tool neurolog psychiatri research realize data driven person medicin big data era learn statist link brain imag subject level featur take imag respons task rais lot challeng due high dimension imag relat small number sampl well noisi measur medic imag paper propos novel method smooth imag scalar regress sir recov true associ imag outcom scalar predictor estim achiev minim mean squar error total variat tv regular term predict mean imag across subject denois imag subject time return coeffici map estim propos algorithm solv optim problem effici combin recent advanc graph fuse lasso solver statist consist estim shown via oracl inequ simul result demonstr propos method outperform exist method separ denois regress step especi sir show evid advantag recov signal small region appli sir alzheim diseas neuroimag initi data produc interpret brain map pet imag patient level featur includ age gender genotyp diseas group|['Ying Liu', 'Bowei Yan']|['stat.ME', 'stat.AP']
2017-03-28T14:04:12Z|2017-03-15T15:27:46Z|http://arxiv.org/abs/1703.05208v1|http://arxiv.org/pdf/1703.05208v1|Understanding the Probabilistic Latent Component Analysis Framework|understand probabilist latent compon analysi framework|Probabilistic Component Latent Analysis (PLCA) is a statistical modeling method for feature extraction from non-negative data. It has been fruitfully applied to various research fields of information retrieval. However, the EM-solved optimization problem coming with the parameter estimation of PLCA-based models has never been properly posed and justified. We then propose in this short paper to re-define the theoretical framework of this problem, with the motivation of making it clearer to understand, and more admissible for further developments of PLCA-based computational systems.|probabilist compon latent analysi plca statist model method featur extract non negat data fruit appli various research field inform retriev howev em solv optim problem come paramet estim plca base model never proper pose justifi propos short paper defin theoret framework problem motiv make clearer understand admiss develop plca base comput system|['D. Cazau', 'G. Nuel']|['stat.ME']
2017-03-28T14:04:12Z|2017-03-15T15:19:14Z|http://arxiv.org/abs/1703.05203v1|http://arxiv.org/pdf/1703.05203v1|Growing simplified vine copula trees: improving Dißmann's algorithm|grow simplifi vine copula tree improv di mann algorithm|Vine copulas are pair-copula constructions enabling multivariate dependence modeling in terms of bivariate building blocks. One of the main tasks of fitting a vine copula is the selection of a suitable tree structure. For this the prevalent method is a heuristic called Di{\ss}mann's algorithm. It sequentially constructs the vine's trees by maximizing dependence at each tree level, where dependence is measured in terms of absolute Kendall's $\tau$. However, the algorithm disregards any implications of the tree structure on the simplifying assumption that is usually made for vine copulas to keep inference tractable. We develop two new algorithms that select tree structures focused on producing simplified vine copulas for which the simplifying assumption is violated as little as possible. For this we make use of a recently developed statistical test of the simplifying assumption. In a simulation study we show that our proposed methods outperform the benchmark given by Di{\ss}mann's algorithm by a great margin. Several real data applications emphasize their practical relevance.|vine copula pair copula construct enabl multivari depend model term bivari build block one main task fit vine copula select suitabl tree structur preval method heurist call di ss mann algorithm sequenti construct vine tree maxim depend tree level depend measur term absolut kendal tau howev algorithm disregard ani implic tree structur simplifi assumpt usual made vine copula keep infer tractabl develop two new algorithm select tree structur focus produc simplifi vine copula simplifi assumpt violat littl possibl make use recent develop statist test simplifi assumpt simul studi show propos method outperform benchmark given di ss mann algorithm great margin sever real data applic emphas practic relev|['Daniel Kraus', 'Claudia Czado']|['stat.ME']
2017-03-28T14:04:12Z|2017-03-16T09:36:40Z|http://arxiv.org/abs/1703.05189v2|http://arxiv.org/pdf/1703.05189v2|Student-t Process Quadratures for Filtering of Non-Linear Systems with   Heavy-Tailed Noise|student process quadratur filter non linear system heavi tail nois|The aim of this article is to design a moment transformation for Student- t distributed random variables, which is able to account for the error in the numerically computed mean. We employ Student-t process quadrature, an instance of Bayesian quadrature, which allows us to treat the integral itself as a random variable whose variance provides information about the incurred integration error. Advantage of the Student- t process quadrature over the traditional Gaussian process quadrature, is that the integral variance depends also on the function values, allowing for a more robust modelling of the integration error. The moment transform is applied in nonlinear sigma-point filtering and evaluated on two numerical examples, where it is shown to outperform the state-of-the-art moment transforms.|aim articl design moment transform student distribut random variabl abl account error numer comput mean employ student process quadratur instanc bayesian quadratur allow us treat integr random variabl whose varianc provid inform incur integr error advantag student process quadratur tradit gaussian process quadratur integr varianc depend also function valu allow robust model integr error moment transform appli nonlinear sigma point filter evalu two numer exampl shown outperform state art moment transform|['Jakub Prüher', 'Filip Tronarp', 'Toni Karvonen', 'Simo Särkkä', 'Ondřej Straka']|['stat.ME', 'stat.ML']
2017-03-28T14:04:12Z|2017-03-15T13:51:52Z|http://arxiv.org/abs/1703.05157v1|http://arxiv.org/pdf/1703.05157v1|One-Sided Cross-Validation for Nonsmooth Density Functions|one side cross valid nonsmooth densiti function|One-sided cross-validation (OSCV) is a bandwidth selection method initially introduced by Hart and Yi (1998) in the context of smooth regression functions. Mart\'{\i}nez-Miranda et al. (2009) developed a version of OSCV for smooth density functions. This article extends the method for nonsmooth densities. It also introduces the fully robust OSCV modification that produces consistent OSCV bandwidths for both smooth and nonsmooth cases. Practical implementations of the OSCV method for smooth and nonsmooth densities are discussed. One of the considered cross-validation kernels has potential for improving the OSCV method's implementation in the regression context.|one side cross valid oscv bandwidth select method initi introduc hart yi context smooth regress function mart nez miranda et al develop version oscv smooth densiti function articl extend method nonsmooth densiti also introduc fulli robust oscv modif produc consist oscv bandwidth smooth nonsmooth case practic implement oscv method smooth nonsmooth densiti discuss one consid cross valid kernel potenti improv oscv method implement regress context|['Olga Y. Savchuk']|['stat.ME', '62G07']
2017-03-28T14:04:12Z|2017-03-15T12:18:21Z|http://arxiv.org/abs/1703.05109v1|http://arxiv.org/pdf/1703.05109v1|Quantile Treatment Effects in the Regression Kink Design|quantil treatment effect regress kink design|This paper studies identification, estimation, and inference of quantile treatment effects in the fuzzy regression kink design with a binary treatment variable. We first show the identification of conditional quantile treatment effects given the event of local compliance. We then propose a bootstrap method of uniform inference for the local quantile process. This bootstrap method is fast and is robust against common optimal choices of bandwidth parameters. We provide practical guidelines as well as a formal theory. Simulation studies show accurate coverage probabilities for tests of uniform treatment significance and treatment heterogeneity.|paper studi identif estim infer quantil treatment effect fuzzi regress kink design binari treatment variabl first show identif condit quantil treatment effect given event local complianc propos bootstrap method uniform infer local quantil process bootstrap method fast robust common optim choic bandwidth paramet provid practic guidelin well formal theori simul studi show accur coverag probabl test uniform treatment signific treatment heterogen|['Harold D. Chiang', 'Yuya Sasaki']|['stat.ME']
2017-03-28T14:04:12Z|2017-03-15T08:09:27Z|http://arxiv.org/abs/1703.05312v1|http://arxiv.org/pdf/1703.05312v1|On optimal experimental designs for Sparse Polynomial Chaos Expansions|optim experiment design spars polynomi chao expans|Uncertainty quantification (UQ) has received much attention in the literature in the past decade. In this context, Sparse Polynomial chaos expansions (PCE) have been shown to be among the most promising methods because of their ability to model highly complex models at relatively low computational costs. A least-square minimization technique may be used to determine the coefficients of the sparse PCE by relying on the so called experimental design (ED), i.e. the sample points where the original computational model is evaluated. An efficient sampling strategy is then needed to generate an accurate PCE at low computational cost. This paper is concerned with the problem of identifying an optimal experimental design that maximizes the accuracy of the surrogate model over the whole input space within a given computational budget. A novel sequential adaptive strategy where the ED is enriched sequentially by capitalizing on the sparsity of the underlying metamodel is introduced. A comparative study between several state-of-the-art methods is performed on four numerical models with varying input dimensionality and computational complexity. It is shown that the optimal sequential design based on the S-value criterion yields accurate, stable and computationally efficient PCE.|uncertainti quantif uq receiv much attent literatur past decad context spars polynomi chao expans pce shown among promis method becaus abil model high complex model relat low comput cost least squar minim techniqu may use determin coeffici spars pce reli call experiment design ed sampl point origin comput model evalu effici sampl strategi need generat accur pce low comput cost paper concern problem identifi optim experiment design maxim accuraci surrog model whole input space within given comput budget novel sequenti adapt strategi ed enrich sequenti capit sparsiti metamodel introduc compar studi sever state art method perform four numer model vari input dimension comput complex shown optim sequenti design base valu criterion yield accur stabl comput effici pce|['N. Fajraoui', 'S. Marelli', 'B. Sudret']|['stat.ME']
2017-03-28T14:04:12Z|2017-03-15T06:34:38Z|http://arxiv.org/abs/1703.04956v1|http://arxiv.org/pdf/1703.04956v1|A Short Note on Almost Sure Convergence of Bayes Factors in the General   Set-Up|short note almost sure converg bay factor general set|In this article we derive the almost sure convergence theory of Bayes factor in the general set-up that includes even dependent data and misspecified models, as a simple application of a result of Shalizi (2009) to a well-known identity satisfied by the Bayes factor.|articl deriv almost sure converg theori bay factor general set includ even depend data misspecifi model simpl applic result shalizi well known ident satisfi bay factor|['Debashis Chatterjee', 'Trisha Maitra', 'Sourabh Bhattacharya']|['math.ST', 'stat.ME', 'stat.TH']
2017-03-28T14:04:12Z|2017-03-15T06:17:40Z|http://arxiv.org/abs/1703.04951v1|http://arxiv.org/pdf/1703.04951v1|Robust and sparse estimation methods for high dimensional linear and   logistic regression|robust spars estim method high dimension linear logist regress|Fully robust versions of the elastic net estimator are introduced for linear and logistic regression. The algorithms to compute the estimators are based on the idea of repeatedly applying the non-robust classical estimators to data subsets only. It is shown how outlier-free subsets can be identified efficiently, and how appropriate tuning parameters for the elastic net penalties can be selected. A final reweighting step improves the efficiency of the estimators. Simulation studies compare with non-robust and other competing robust estimators and reveal the superiority of the newly proposed methods. This is also supported by a reasonable computation time and by good performance in real data examples.|fulli robust version elast net estim introduc linear logist regress algorithm comput estim base idea repeat appli non robust classic estim data subset onli shown outlier free subset identifi effici appropri tune paramet elast net penalti select final reweight step improv effici estim simul studi compar non robust compet robust estim reveal superior newli propos method also support reason comput time good perform real data exampl|['Fatma Sevinc Kurnaz', 'Irene Hoffmann', 'Peter Filzmoser']|['stat.ME']
2017-03-28T14:04:12Z|2017-03-15T02:08:07Z|http://arxiv.org/abs/1703.04882v1|http://arxiv.org/pdf/1703.04882v1|Element analysis: a wavelet-based method for analyzing time-localized   events in noisy time series|element analysi wavelet base method analyz time local event noisi time seri|"A method is derived for the quantitative analysis of signals that are composed of superpositions of isolated, time-localized ""events"". Here these events are taken to be well represented as rescaled and phase-rotated versions of generalized Morse wavelets, a broad family of continuous analytic functions. Analyzing a signal composed of replicates of such a function using another Morse wavelet allows one to directly estimate the properties of events from the values of the wavelet transform at its own maxima. The distribution of events in general power-law noise is determined in order to establish significance based on an expected false detection rate. Finally, an expression for an event's ""region of influence"" within the wavelet transform permits the formation of a criterion for rejecting spurious maxima due to numerical artifacts or other unsuitable events. Signals can then be reconstructed based on a small number of isolated points on the time/scale plane. This method, termed element analysis, is applied to the identification of long-lived eddy structures in ocean currents as observed by along-track measurements of sea surface elevation from satellite altimetry"|method deriv quantit analysi signal compos superposit isol time local event event taken well repres rescal phase rotat version general mors wavelet broad famili continu analyt function analyz signal compos replic function use anoth mors wavelet allow one direct estim properti event valu wavelet transform maxima distribut event general power law nois determin order establish signific base expect fals detect rate final express event region influenc within wavelet transform permit format criterion reject spurious maxima due numer artifact unsuit event signal reconstruct base small number isol point time scale plane method term element analysi appli identif long live eddi structur ocean current observ along track measur sea surfac elev satellit altimetri|['J. M. Lilly']|['stat.ME']
2017-03-28T14:04:16Z|2017-03-13T11:19:28Z|http://arxiv.org/abs/1703.04334v1|http://arxiv.org/pdf/1703.04334v1|Probabilistic Matching: Causal Inference under Measurement Errors|probabilist match causal infer measur error|The abundance of data produced daily from large variety of sources has boosted the need of novel approaches on causal inference analysis from observational data. Observational data often contain noisy or missing entries. Moreover, causal inference studies may require unobserved high-level information which needs to be inferred from other observed attributes. In such cases, inaccuracies of the applied inference methods will result in noisy outputs. In this study, we propose a novel approach for causal inference when one or more key variables are noisy. Our method utilizes the knowledge about the uncertainty of the real values of key variables in order to reduce the bias induced by noisy measurements. We evaluate our approach in comparison with existing methods both on simulated and real scenarios and we demonstrate that our method reduces the bias and avoids false causal inference conclusions in most cases.|abund data produc daili larg varieti sourc boost need novel approach causal infer analysi observ data observ data often contain noisi miss entri moreov causal infer studi may requir unobserv high level inform need infer observ attribut case inaccuraci appli infer method result noisi output studi propos novel approach causal infer one key variabl noisi method util knowledg uncertainti real valu key variabl order reduc bias induc noisi measur evalu approach comparison exist method simul real scenario demonstr method reduc bias avoid fals causal infer conclus case|['Fani Tsapeli', 'Peter Tino', 'Mirco Musolesi']|['stat.ME', 'stat.CO', 'stat.ML']
2017-03-28T14:04:16Z|2017-03-13T06:08:51Z|http://arxiv.org/abs/1703.04264v1|http://arxiv.org/pdf/1703.04264v1|Poisson multi-Bernoulli mixture filter: direct derivation and   implementation|poisson multi bernoulli mixtur filter direct deriv implement|We provide a derivation of the Poisson multi-Bernoulli mixture (PMBM) filter for multi-target tracking with the standard point target measurements without using probability generating functionals or functional derivatives. We also establish the connection with the \delta-generalised labelled multi-Bernoulli (\delta-GLMB) filter, showing that a \delta-GLMB density represents a multi-Bernoulli mixture with labelled targets so it can be seen as a special case of PMBM. In addition, we propose an implementation for linear/Gaussian dynamic and measurement models and how to efficiently obtain typical estimators in the literature from the PMBM. The PMBM filter is shown to outperform other filters in the literature in a challenging scenario|provid deriv poisson multi bernoulli mixtur pmbm filter multi target track standard point target measur without use probabl generat function function deriv also establish connect delta generalis label multi bernoulli delta glmb filter show delta glmb densiti repres multi bernoulli mixtur label target seen special case pmbm addit propos implement linear gaussian dynam measur model effici obtain typic estim literatur pmbm pmbm filter shown outperform filter literatur challeng scenario|['Ángel F. García-Fernández', 'Jason L. Williams', 'Karl Granström', 'Lennart Svensson']|['cs.CV', 'stat.ME']
2017-03-28T14:04:16Z|2017-03-12T21:19:29Z|http://arxiv.org/abs/1703.04180v1|http://arxiv.org/pdf/1703.04180v1|MEDL and MEDLA: Methods for Assessment of Scaling by Medians of   Log-Squared Nondecimated Wavelet Coefficients|medl medla method assess scale median log squar nondecim wavelet coeffici|"High-frequency measurements and images acquired from various sources in the real world often possess a degree of self-similarity and inherent regular scaling. When data look like a noise, the scaling exponent may be the only informative feature that summarizes such data. Methods for the assessment of self-similarity by estimating Hurst exponent often involve analysis of rate of decay in a spectrum defined in various multiresolution domains. When this spectrum is calculated using discrete non-decimated wavelet transforms, due to increased autocorrelation in wavelet coefficients, the estimators of $H$ show increased bias compared to the estimators that use traditional orthogonal transforms. At the same time, non-decimated transforms have a number of advantages when employed for calculation of wavelet spectra and estimation of Hurst exponents: the variance of the estimator is smaller, input signals and images could be of arbitrary size, and due to the shift-invariance, the local scaling can be assessed as well. We propose two methods based on robust estimation and resampling that alleviate the effect of increased autocorrelation while maintaining all advantages of non-decimated wavelet transforms. The proposed methods extend the approaches in existing literature where the logarithmic transformation and pairing of wavelet coefficients are used for lowering the bias. In a simulation study we use fractional Brownian motions with a range of theoretical Hurst exponents. For such signals for which ""true"" $H$ is known, we demonstrate bias reduction and overall reduction of the mean-squared error by the two proposed estimators. For fractional Brownian motions, both proposed methods yield estimators of $H$ that are asymptotically normal and unbiased."|high frequenc measur imag acquir various sourc real world often possess degre self similar inher regular scale data look like nois scale expon may onli inform featur summar data method assess self similar estim hurst expon often involv analysi rate decay spectrum defin various multiresolut domain spectrum calcul use discret non decim wavelet transform due increas autocorrel wavelet coeffici estim show increas bias compar estim use tradit orthogon transform time non decim transform number advantag employ calcul wavelet spectra estim hurst expon varianc estim smaller input signal imag could arbitrari size due shift invari local scale assess well propos two method base robust estim resampl allevi effect increas autocorrel maintain advantag non decim wavelet transform propos method extend approach exist literatur logarithm transform pair wavelet coeffici use lower bias simul studi use fraction brownian motion rang theoret hurst expon signal true known demonstr bias reduct overal reduct mean squar error two propos estim fraction brownian motion propos method yield estim asymptot normal unbias|['Minkyoung Kang', 'Brani Vidakovic']|['stat.ME']
2017-03-28T14:04:16Z|2017-03-12T18:29:03Z|http://arxiv.org/abs/1703.04157v1|http://arxiv.org/pdf/1703.04157v1|Using Aggregated Relational Data to feasibly identify network structure   without network data|use aggreg relat data feasibl identifi network structur without network data|"Social and economic network data can be useful for both researchers and policymakers, but can often be impractical to collect. We propose collecting Aggregated Relational Data (ARD) using questions that are simple and easy to add to any survey. These question are of the form ""how many of your friends in the village have trait k?""   We show that by collecting ARD on even a small share of the population, researchers can recover the likely distribution of statistics from the underlying network. We provide three empirical examples. We first apply the technique to the 75 village networks in Karnataka, India, where Banerjee et al. (2016b) collected near-complete network data. We show that with ARD alone on even a 29% sample, we can accurately estimate both node-level features (such as eigenvector centrality, clustering) and network-level features (such as the maximum eigenvalue, average path length). To further demonstrate the power of the approach, we apply our technique to two settings analyzed previously by the authors. We show ARD could have been used to predict how to assign monitors to savers to increase savings in rural villages (Breza and Chandrasekhar, 2016). ARD would have led to the same conclusions the authors arrived at when they used expensive near-complete network data. We then provide an example where survey ARD was collected, along with some partial network data, and demonstrate that the same conclusions would have been drawn using only the ARD data, and that with the ARD, the researchers could more generally measure the impact of microfinance exposure on social capital in urban slums (Banerjee et al., 2016a)."|social econom network data use research policymak often impract collect propos collect aggreg relat data ard use question simpl easi add ani survey question form mani friend villag trait show collect ard even small share popul research recov like distribut statist network provid three empir exampl first appli techniqu villag network karnataka india banerje et al collect near complet network data show ard alon even sampl accur estim node level featur eigenvector central cluster network level featur maximum eigenvalu averag path length demonstr power approach appli techniqu two set analyz previous author show ard could use predict assign monitor saver increas save rural villag breza chandrasekhar ard would led conclus author arriv use expens near complet network data provid exampl survey ard collect along partial network data demonstr conclus would drawn use onli ard data ard research could general measur impact microfin exposur social capit urban slum banerje et al|['Emily Breza', 'Arun G. Chandrasekhar', 'Tyler H. McCormick', 'Mengjie Pan']|['stat.ME']
2017-03-28T14:04:16Z|2017-03-11T20:07:06Z|http://arxiv.org/abs/1703.04025v1|http://arxiv.org/pdf/1703.04025v1|Learning Large-Scale Bayesian Networks with the sparsebn Package|learn larg scale bayesian network sparsebn packag|Learning graphical models from data is an important problem with wide applications, ranging from genomics to the social sciences. Nowadays datasets typically have upwards of thousands---sometimes tens or hundreds of thousands---of variables and far fewer samples. To meet this challenge, we develop a new R package called sparsebn for learning the structure of large, sparse graphical models with a focus on Bayesian networks. While there are many existing packages for this task within the R ecosystem, this package focuses on the unique setting of learning large networks from high-dimensional data, possibly with interventions. As such, the methods provided place a premium on scalability and consistency in a high-dimensional setting. Furthermore, in the presence of interventions, the methods implemented here achieve the goal of learning a causal network from data. The sparsebn package is open-source and available on CRAN.|learn graphic model data import problem wide applic rang genom social scienc nowaday dataset typic upward thousand sometim ten hundr thousand variabl far fewer sampl meet challeng develop new packag call sparsebn learn structur larg spars graphic model focus bayesian network mani exist packag task within ecosystem packag focus uniqu set learn larg network high dimension data possibl intervent method provid place premium scalabl consist high dimension set furthermor presenc intervent method implement achiev goal learn causal network data sparsebn packag open sourc avail cran|['Bryon Aragam', 'Jiaying Gu', 'Qing Zhou']|['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']
2017-03-28T14:04:16Z|2017-03-11T00:54:13Z|http://arxiv.org/abs/1703.03882v1|http://arxiv.org/pdf/1703.03882v1|Generalized Full Matching|general full match|Matching methods are used to make units comparable on observed characteristics. Full matching can be used to derive optimal matches. However, the method has only been defined in the case of two treatment categories, it places unnecessary restrictions on the matched groups, and existing implementations are computationally intractable in large samples. As a result, the method has not been feasible in studies with large samples or complex designs. We introduce a generalization of full matching that inherits its optimality properties but allows the investigator to specify any desired structure of the matched groups over any number of treatment conditions. We also describe a new approximation algorithm to derive generalized full matchings. In the worst case, the maximum within-group dissimilarity produced by the algorithm is no worse than four times the optimal solution, but it typically performs close to on par with existing optimal algorithms when they exist. Despite its performance, the algorithm is fast and uses little memory: it terminates, on average, in linearithmic time using linear space. This enables investigators to derive well-performing matchings within minutes even in complex studies with samples of several million units.|match method use make unit compar observ characterist full match use deriv optim match howev method onli defin case two treatment categori place unnecessari restrict match group exist implement comput intract larg sampl result method feasibl studi larg sampl complex design introduc general full match inherit optim properti allow investig specifi ani desir structur match group ani number treatment condit also describ new approxim algorithm deriv general full match worst case maximum within group dissimilar produc algorithm wors four time optim solut typic perform close par exist optim algorithm exist despit perform algorithm fast use littl memori termin averag linearithm time use linear space enabl investig deriv well perform match within minut even complex studi sampl sever million unit|['Fredrik Sävje', 'Michael J. Higgins', 'Jasjeet S. Sekhon']|['stat.ME']
2017-03-28T14:04:16Z|2017-03-09T15:56:58Z|http://arxiv.org/abs/1703.03312v1|http://arxiv.org/pdf/1703.03312v1|Split Sample Empirical Likelihood|split sampl empir likelihood|We propose a new approach that combines multiple non-parametric likelihood-type components to build a data-driven approximation of the true likelihood function. Our approach is built on empirical likelihood, a non-parametric approximation of the likelihood function. We show the asymptotic behaviors of our approach are identical to those seen in empirical likelihood. We demonstrate that our method performs comparably to empirical likelihood while significantly decreasing computational time.|propos new approach combin multipl non parametr likelihood type compon build data driven approxim true likelihood function approach built empir likelihood non parametr approxim likelihood function show asymptot behavior approach ident seen empir likelihood demonstr method perform compar empir likelihood signific decreas comput time|['Adam Jaeger', 'Nicole Lazar']|['stat.ME']
2017-03-28T14:04:16Z|2017-03-09T10:19:53Z|http://arxiv.org/abs/1703.03213v1|http://arxiv.org/pdf/1703.03213v1|Kernel intensity estimation, bootstrapping and bandwidth selection for   inhomogeneous point processes depending on spatial covariates|kernel intens estim bootstrap bandwidth select inhomogen point process depend spatial covari|In the point process context, kernel intensity estimation has been mainly restricted to exploratory analysis due to its lack of consistency. However the use of covariates has allow to design consistent alternatives under some restrictive assumptions. In this paper we focus our attention on de\-fi\-ning an appropriate framework to derive a consistent kernel intensity estimator using covariates, as well as a consistent smooth bootstrap procedure. For spatial point processes with covariates there is no specific bandwidth selector, hence, we define two new data-driven procedures specifically designed for this scenario: a rule-of-thumb and a plug-in bandwidth based on the bootstrap method previously introduced. A simulation study is accomplished to understand the behaviour of these procedures in finite samples. Finally, we apply the techniques to a real set of data made up of wildfires in Canada during June 2015, using meteorological information as covariates.|point process context kernel intens estim main restrict exploratori analysi due lack consist howev use covari allow design consist altern restrict assumpt paper focus attent de fi ning appropri framework deriv consist kernel intens estim use covari well consist smooth bootstrap procedur spatial point process covari specif bandwidth selector henc defin two new data driven procedur specif design scenario rule thumb plug bandwidth base bootstrap method previous introduc simul studi accomplish understand behaviour procedur finit sampl final appli techniqu real set data made wildfir canada dure june use meteorolog inform covari|['M. I. Borrajo', 'W. González-Manteiga', 'M. D. Martínez-Miranda']|['stat.ME', 'stat.AP', '62G05, 62G09, 62H11, 60G55, 60-08']
2017-03-28T14:04:16Z|2017-03-09T07:27:33Z|http://arxiv.org/abs/1703.03165v1|http://arxiv.org/pdf/1703.03165v1|Perturbation Bootstrap in Adaptive Lasso|perturb bootstrap adapt lasso|The Adaptive LASSO (ALASSO) was proposed by Zou [J. Amer. Statist. Assoc. 101 (2006) 1418-1429] as a modification of the LASSO for the purpose of simultaneous variable selection and estimation of the parameters in a linear regression model. Zou (2006) established that the ALASSO estimator is variable-selection consistent as well as asymptotically Normal in the indices corresponding to the nonzero regression coefficients in certain fixed-dimensional settings. In an influential paper, Minnier, Tian and Cai [J. Amer. Statist. Assoc. 106 (2011) 1371-1382] proposed a perturbation bootstrap method and established its distributional consistency for the ALASSO estimator in the fixed-dimensional setting. In this paper, however, we show that this (naive) perturbation bootstrap fails to achieve second order correctness in approximating the distribution of the ALASSO estimator. We propose a modification to the perturbation bootstrap objective function and show that a suitably studentized version of our modified perturbation bootstrap ALASSO estimator achieves second-order correctness even when the dimension of the model is allowed to grow to infinity with the sample size. As a consequence, inferences based on the modified perturbation bootstrap will be more accurate than the inferences based on the oracle Normal approximation. We give simulation studies demonstrating good finite-sample properties of our modified perturbation bootstrap method as well as an illustration of our method on a real data set.|adapt lasso alasso propos zou amer statist assoc modif lasso purpos simultan variabl select estim paramet linear regress model zou establish alasso estim variabl select consist well asymptot normal indic correspond nonzero regress coeffici certain fix dimension set influenti paper minnier tian cai amer statist assoc propos perturb bootstrap method establish distribut consist alasso estim fix dimension set paper howev show naiv perturb bootstrap fail achiev second order correct approxim distribut alasso estim propos modif perturb bootstrap object function show suitabl student version modifi perturb bootstrap alasso estim achiev second order correct even dimens model allow grow infin sampl size consequ infer base modifi perturb bootstrap accur infer base oracl normal approxim give simul studi demonstr good finit sampl properti modifi perturb bootstrap method well illustr method real data set|['Debraj Das', 'Karl Gregory', 'S. N. Lahiri']|['stat.ME', 'math.ST', 'stat.TH']
2017-03-28T14:04:16Z|2017-03-09T03:49:33Z|http://arxiv.org/abs/1703.03123v1|http://arxiv.org/pdf/1703.03123v1|Calibrated Data Augmentation for Scalable Markov Chain Monte Carlo|calibr data augment scalabl markov chain mont carlo|Data augmentation is a common technique for building tuning-free Markov chain Monte Carlo algorithms. Although these algorithms are very popular, autocorrelations are often high in large samples, leading to poor computational efficiency. This phenomenon has been attributed to a discrepancy between Gibbs step sizes and the rate of posterior concentration. In this article, we propose a family of calibrated data augmentation algorithms, which adjust for this discrepancy by inflating Gibbs step sizes while adjusting for bias. A Metropolis-Hastings step is included to account for the slight discrepancy between the stationary distribution of the resulting sampler and the exact posterior distribution. The approach is applicable to a broad variety of existing data augmentation algorithms, and we focus on three popular models: probit, logistic and Poisson log-linear. Dramatic gains in computational efficiency are shown in applications.|data augment common techniqu build tune free markov chain mont carlo algorithm although algorithm veri popular autocorrel often high larg sampl lead poor comput effici phenomenon attribut discrep gibb step size rate posterior concentr articl propos famili calibr data augment algorithm adjust discrep inflat gibb step size adjust bias metropoli hast step includ account slight discrep stationari distribut result sampler exact posterior distribut approach applic broad varieti exist data augment algorithm focus three popular model probit logist poisson log linear dramat gain comput effici shown applic|['Leo L. Duan', 'James E. Johndrow', 'David B. Dunson']|['stat.ME']
2017-03-28T14:04:20Z|2017-03-09T01:19:39Z|http://arxiv.org/abs/1703.03095v1|http://arxiv.org/pdf/1703.03095v1|Fitting the Linear Preferential Attachment Model|fit linear preferenti attach model|Preferential attachment is an appealing mechanism for modeling power-law behavior of the degree distributions in directed social networks. In this paper, we consider methods for fitting a 5-parameter linear preferential model to network data under two data scenarios. In the case where full history of the network formation is given, we derive the maximum likelihood estimator of the parameters and show that it is strongly consistent and asymptotically normal. In the case where only a single-time snapshot of the network is available, we propose an estimation method which combines method of moments with an approximation to the likelihood. The resulting estimator is also strongly consistent and performs quite well compared to the MLE estimator. We illustrate both estimation procedures through simulated data, and explore the usage of this model in a real data example. At the end of the paper, we also present a semi-parametric method to model heavy-tailed features of the degree distributions of the network using ideas from extreme value theory.|preferenti attach appeal mechan model power law behavior degre distribut direct social network paper consid method fit paramet linear preferenti model network data two data scenario case full histori network format given deriv maximum likelihood estim paramet show strong consist asymptot normal case onli singl time snapshot network avail propos estim method combin method moment approxim likelihood result estim also strong consist perform quit well compar mle estim illustr estim procedur simul data explor usag model real data exampl end paper also present semi parametr method model heavi tail featur degre distribut network use idea extrem valu theori|['Phyllis Wan', 'Tiandong Wang', 'Richard A. Davis', 'Sidney I. Resnick']|['stat.ME', '05C80, 90B15, 62F12']
2017-03-28T14:04:20Z|2017-03-08T21:40:57Z|http://arxiv.org/abs/1703.03043v1|http://arxiv.org/pdf/1703.03043v1|Bootstrap with Clustering in Two or More Dimensions|bootstrap cluster two dimens|We propose a bootstrap procedure for data that may exhibit clustering in two or more dimensions. We use insights from the theory of generalized U-statistics to analyze the large-sample properties of statistics that are sample averages from the observations pooled across clusters. The asymptotic distribution of these statistics may be non-standard if there is no clustering in means. We show that the proposed bootstrap procedure is (a) point-wise consistent for any fixed data-generating process (DGP), (b) uniformly consistent if we exclude the case of clustering without clustering in means, and (c) provides refinements for any DGP such that the limiting distribution is Gaussian.|propos bootstrap procedur data may exhibit cluster two dimens use insight theori general statist analyz larg sampl properti statist sampl averag observ pool across cluster asymptot distribut statist may non standard cluster mean show propos bootstrap procedur point wise consist ani fix data generat process dgp uniform consist exclud case cluster without cluster mean provid refin ani dgp limit distribut gaussian|['Konrad Menzel']|['stat.ME', 'stat.OT']
2017-03-28T14:04:20Z|2017-03-08T20:32:20Z|http://arxiv.org/abs/1703.03023v1|http://arxiv.org/pdf/1703.03023v1|Elicitation, measuring bias, checking for prior-data conflict and   inference with a Dirichlet prior|elicit measur bias check prior data conflict infer dirichlet prior|Methods are developed for eliciting a Dirichlet prior based upon bounds on the individual probabilities that hold with virtual certainty. This approach to selecting a prior is applied to a contingency table problem where it is demonstrated how to assess the bias in the prior as well as how to check for prior-data conflict. It is shown that the assessment of a hypothesis via relative belief can easily take into account what it means for the falsity of the hypothesis to correspond to a difference of practical importance and provide evidence in favor of a hypothesis.|method develop elicit dirichlet prior base upon bound individu probabl hold virtual certainti approach select prior appli conting tabl problem demonstr assess bias prior well check prior data conflict shown assess hypothesi via relat belief easili take account mean falsiti hypothesi correspond differ practic import provid evid favor hypothesi|['Michael Evans', 'Irwin Guttman', 'Peiying Li']|['stat.ME', '62F15']
2017-03-28T14:04:20Z|2017-03-17T20:10:01Z|http://arxiv.org/abs/1703.03022v2|http://arxiv.org/pdf/1703.03022v2|A New Capture-Recapture Model in Dual-record System|new captur recaptur model dual record system|Population size estimation based on two sample capture-recapture type experiment is an interesting problem in various fields including epidemiology, ecology, population studies, etc. Lincoln-Petersen estimate is popularly used under the assumption that capture and recapture status of each individual is independent. However, in many real life scenarios, there is some inherent dependency between capture and recapture attempts which is not well-studied in the literature for two sample capture-recapture method. In this article, we propose a novel model that successfully incorporates the possible causal dependency and provide corresponding estimation methodologies for the associated model parameters. Simulation results show superiority of the performance of the proposed method over existing competitors. The method is illustrated through the analysis of real data sets.|popul size estim base two sampl captur recaptur type experi interest problem various field includ epidemiolog ecolog popul studi etc lincoln petersen estim popular use assumpt captur recaptur status individu independ howev mani real life scenario inher depend captur recaptur attempt well studi literatur two sampl captur recaptur method articl propos novel model success incorpor possibl causal depend provid correspond estim methodolog associ model paramet simul result show superior perform propos method exist competitor method illustr analysi real data set|['Kiranmoy Chatterjee', 'Prajamitra Bhuyan']|['stat.ME', '62F10']
2017-03-28T14:04:20Z|2017-03-08T13:47:17Z|http://arxiv.org/abs/1703.02834v1|http://arxiv.org/pdf/1703.02834v1|Exact Dimensionality Selection for Bayesian PCA|exact dimension select bayesian pca|We present a Bayesian model selection approach to estimate the intrinsic dimensionality of a high-dimensional dataset. To this end, we introduce a novel formulation of the probabilisitic principal component analysis model based on a normal-gamma prior distribution. In this context, we exhibit a closed-form expression of the marginal likelihood which allows to infer an optimal number of components. We also propose a heuristic based on the expected shape of the marginal likelihood curve in order to choose the hyperparameters. In non-asymptotic frameworks, we show on simulated data that this exact dimensionality selection approach is competitive with both Bayesian and frequentist state-of-the-art methods.|present bayesian model select approach estim intrins dimension high dimension dataset end introduc novel formul probabilisit princip compon analysi model base normal gamma prior distribut context exhibit close form express margin likelihood allow infer optim number compon also propos heurist base expect shape margin likelihood curv order choos hyperparamet non asymptot framework show simul data exact dimension select approach competit bayesian frequentist state art method|['Charles Bouveyron', 'Pierre Latouche', 'Pierre-Alexandre Mattei']|['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']
2017-03-28T14:04:20Z|2017-03-08T07:41:13Z|http://arxiv.org/abs/1703.02736v1|http://arxiv.org/pdf/1703.02736v1|Profile Estimation for Partial Functional Partially Linear Single-Index   Model|profil estim partial function partial linear singl index model|This paper studies a \textit{partial functional partially linear single-index model} that consists of a functional linear component as well as a linear single-index component. This model generalizes many well-known existing models and is suitable for more complicated data structures. However, its estimation inherits the difficulties and complexities from both components and makes it a challenging problem, which calls for new methodology. We propose a novel profile B-spline method to estimate the parameters by approximating the unknown nonparametric link function in the single-index component part with B-spline, while the linear slope function in the functional component part is estimated by the functional principal component basis. The consistency and asymptotic normality of the parametric estimators are derived, and the global convergence of the proposed estimator of the linear slope function is also established. More excitingly, the latter convergence is optimal in the minimax sense. A two-stage procedure is implemented to estimate the nonparametric link function, and the resulting estimator possesses the optimal global rate of convergence. Furthermore, the convergence rate of the mean squared prediction error for a predictor is also obtained. Empirical properties of the proposed procedures are studied through Monte Carlo simulations. A real data example is also analyzed to illustrate the power and flexibility of the proposed methodology.|paper studi textit partial function partial linear singl index model consist function linear compon well linear singl index compon model general mani well known exist model suitabl complic data structur howev estim inherit difficulti complex compon make challeng problem call new methodolog propos novel profil spline method estim paramet approxim unknown nonparametr link function singl index compon part spline linear slope function function compon part estim function princip compon basi consist asymptot normal parametr estim deriv global converg propos estim linear slope function also establish excit latter converg optim minimax sens two stage procedur implement estim nonparametr link function result estim possess optim global rate converg furthermor converg rate mean squar predict error predictor also obtain empir properti propos procedur studi mont carlo simul real data exampl also analyz illustr power flexibl propos methodolog|['Qingguo Tang', 'Linglong Kong', 'David Ruppert', 'Rohana J. Karunamuni']|['math.ST', 'stat.ME', 'stat.TH']
2017-03-28T14:04:20Z|2017-03-08T06:22:56Z|http://arxiv.org/abs/1703.02724v1|http://arxiv.org/pdf/1703.02724v1|Guaranteed Tensor PCA with Optimality in Statistics and Computation|guarante tensor pca optim statist comput|Tensors, or high-order arrays, attract much attention in recent research. In this paper, we propose a general framework for tensor principal component analysis (tensor PCA), which focuses on the methodology and theory for extracting the hidden low-rank structure from the high-dimensional tensor data. A unified solution is provided for tensor PCA with considerations in both statistical limits and computational costs. The problem exhibits three different phases according to the signal-noise-ratio (SNR). In particular, with strong SNR, we propose a fast spectral power iteration method that achieves the minimax optimal rate of convergence in estimation; with weak SNR, the information-theoretical lower bound shows that it is impossible to have consistent estimation in general; with moderate SNR, we show that the non-convex maximum likelihood estimation provides optimal solution, but with NP-hard computational cost; moreover, under the hardness hypothesis of hypergraphic planted clique detection, there are no polynomial-time algorithms performing consistently in general. Simulation studies show that the proposed spectral power iteration method have good performance under a variety of settings.|tensor high order array attract much attent recent research paper propos general framework tensor princip compon analysi tensor pca focus methodolog theori extract hidden low rank structur high dimension tensor data unifi solut provid tensor pca consider statist limit comput cost problem exhibit three differ phase accord signal nois ratio snr particular strong snr propos fast spectral power iter method achiev minimax optim rate converg estim weak snr inform theoret lower bound show imposs consist estim general moder snr show non convex maximum likelihood estim provid optim solut np hard comput cost moreov hard hypothesi hypergraph plant cliqu detect polynomi time algorithm perform consist general simul studi show propos spectral power iter method good perform varieti set|['Anru Zhang', 'Dong Xia']|['math.ST', 'cs.LG', 'stat.ME', 'stat.ML', 'stat.TH']
2017-03-28T14:04:20Z|2017-03-08T03:07:37Z|http://arxiv.org/abs/1703.02679v1|http://arxiv.org/pdf/1703.02679v1|Performance Bounds for Graphical Record Linkage|perform bound graphic record linkag|Record linkage involves merging records in large, noisy databases to remove duplicate entities. It has become an important area because of its widespread occurrence in bibliometrics, public health, official statistics production, political science, and beyond. Traditional linkage methods directly linking records to one another are computationally infeasible as the number of records grows. As a result, it is increasingly common for researchers to treat record linkage as a clustering task, in which each latent entity is associated with one or more noisy database records. We critically assess performance bounds using the Kullback-Leibler (KL) divergence under a Bayesian record linkage framework, making connections to Kolchin partition models. We provide an upper bound using the KL divergence and a lower bound on the minimum probability of misclassifying a latent entity. We give insights for when our bounds hold using simulated data and provide practical user guidance.|record linkag involv merg record larg noisi databas remov duplic entiti becom import area becaus widespread occurr bibliometr public health offici statist product polit scienc beyond tradit linkag method direct link record one anoth comput infeas number record grow result increas common research treat record linkag cluster task latent entiti associ one noisi databas record critic assess perform bound use kullback leibler kl diverg bayesian record linkag framework make connect kolchin partit model provid upper bound use kl diverg lower bound minimum probabl misclassifi latent entiti give insight bound hold use simul data provid practic user guidanc|['Rebecca C. Steorts', 'Matt Barnes', 'Willie Neiswanger']|['math.ST', 'cs.IT', 'math.IT', 'stat.ME', 'stat.ML', 'stat.TH']
2017-03-28T14:04:20Z|2017-03-07T16:50:38Z|http://arxiv.org/abs/1703.02468v1|http://arxiv.org/pdf/1703.02468v1|Data-Driven Estimation Of Mutual Information Between Dependent Data|data driven estim mutual inform depend data|We consider the problem of estimating mutual information between dependent data, an important problem in many science and engineering applications. We propose a data-driven, non-parametric estimator of mutual information in this paper. The main novelty of our solution lies in transforming the data to frequency domain to make the problem tractable. We define a novel metric--mutual information in frequency--to detect and quantify the dependence between two random processes across frequency using Cram\'{e}r's spectral representation. Our solution calculates mutual information as a function of frequency to estimate the mutual information between the dependent data over time. We validate its performance on linear and nonlinear models. In addition, mutual information in frequency estimated as a part of our solution can also be used to infer cross-frequency coupling in the data.|consid problem estim mutual inform depend data import problem mani scienc engin applic propos data driven non parametr estim mutual inform paper main novelti solut lie transform data frequenc domain make problem tractabl defin novel metric mutual inform frequenc detect quantifi depend two random process across frequenc use cram spectral represent solut calcul mutual inform function frequenc estim mutual inform depend data time valid perform linear nonlinear model addit mutual inform frequenc estim part solut also use infer cross frequenc coupl data|['Rakesh Malladi', 'Don H Johnson', 'Behnaam Aazhang']|['cs.IT', 'math.IT', 'stat.ME']
2017-03-28T14:04:20Z|2017-03-07T16:38:11Z|http://arxiv.org/abs/1703.02462v1|http://arxiv.org/pdf/1703.02462v1|Convex and non-convex regularization methods for spatial point processes   intensity estimation|convex non convex regular method spatial point process intens estim|This paper deals with feature selection procedures for spatial point processes intensity estimation. We consider regularized versions of estimating equations based on Campbell theorem derived from two classical functions: Poisson likelihood and logistic regression likelihood. We provide general conditions on the spatial point processes and on penalty functions which ensure consistency, sparsity and asymptotic normality. We discuss the numerical implementation and assess finite sample properties in a simulation study. Finally, an application to tropical forestry datasets illustrates the use of the proposed methods.|paper deal featur select procedur spatial point process intens estim consid regular version estim equat base campbel theorem deriv two classic function poisson likelihood logist regress likelihood provid general condit spatial point process penalti function ensur consist sparsiti asymptot normal discuss numer implement assess finit sampl properti simul studi final applic tropic forestri dataset illustr use propos method|['Achmad Choiruddin', 'Jean-François Coeurjolly', 'Frédérique Letué']|['stat.ME', 'math.ST', 'stat.TH']
2017-03-28T14:04:24Z|2017-03-07T15:13:08Z|http://arxiv.org/abs/1703.02428v1|http://arxiv.org/pdf/1703.02428v1|Robust Bayesian Filtering and Smoothing Using Student's t Distribution|robust bayesian filter smooth use student distribut|State estimation in heavy-tailed process and measurement noise is an important challenge that must be addressed in, e.g., tracking scenarios with agile targets and outlier-corrupted measurements. The performance of the Kalman filter (KF) can deteriorate in such applications because of the close relation to the Gaussian distribution. Therefore, this paper describes the use of Student's t distribution to develop robust, scalable, and simple filtering and smoothing algorithms.   After a discussion of Student's t distribution, exact filtering in linear state-space models with t noise is analyzed. Intermediate approximation steps are used to arrive at filtering and smoothing algorithms that closely resemble the KF and the Rauch-Tung-Striebel (RTS) smoother except for a nonlinear measurement-dependent matrix update. The required approximations are discussed and an undesirable behavior of moment matching for t densities is revealed. A favorable approximation based on minimization of the Kullback-Leibler divergence is presented. Because of its relation to the KF, some properties and algorithmic extensions are inherited by the t filter. Instructive simulation examples demonstrate the performance and robustness of the novel algorithms.|state estim heavi tail process measur nois import challeng must address track scenario agil target outlier corrupt measur perform kalman filter kf deterior applic becaus close relat gaussian distribut therefor paper describ use student distribut develop robust scalabl simpl filter smooth algorithm discuss student distribut exact filter linear state space model nois analyz intermedi approxim step use arriv filter smooth algorithm close resembl kf rauch tung striebel rts smoother except nonlinear measur depend matrix updat requir approxim discuss undesir behavior moment match densiti reveal favor approxim base minim kullback leibler diverg present becaus relat kf properti algorithm extens inherit filter instruct simul exampl demonstr perform robust novel algorithm|['Michael Roth', 'Tohid Ardeshiri', 'Emre Özkan', 'Fredrik Gustafsson']|['stat.ME', 'cs.SY', 'stat.CO']
2017-03-28T14:04:24Z|2017-03-07T09:39:00Z|http://arxiv.org/abs/1703.02296v1|http://arxiv.org/pdf/1703.02296v1|Low-rank Interaction Contingency Tables|low rank interact conting tabl|Log-linear models are popular tools to analyze contingency tables, particularly to model row and column effects as well as row-column interactions in two-way tables. In this paper, we introduce a regularized log-linear model designed for denoising and visualizing count data, which can incorporate side information such as row and column features. The estimation is performed through a convex optimization problem where we minimize a negative Poisson log-likelihood penalized by the nuclear norm of the interaction matrix. We derive an upper bound on the Frobenius estimation error, which improves previous rates for Poisson matrix recovery, and an algorithm based on the alternating direction method of multipliers to compute our estimator. To propose a complete methodology to users, we also address automatic selection of the regularization parameter. A Monte Carlo simulation reveals that our estimator is particularly well suited to estimate the rank of the interaction in low signal to noise ratio regimes. We illustrate with two data analyses that the results can be easily interpreted through biplot vizualization. The method is available as an R code.|log linear model popular tool analyz conting tabl particular model row column effect well row column interact two way tabl paper introduc regular log linear model design denois visual count data incorpor side inform row column featur estim perform convex optim problem minim negat poisson log likelihood penal nuclear norm interact matrix deriv upper bound frobenius estim error improv previous rate poisson matrix recoveri algorithm base altern direct method multipli comput estim propos complet methodolog user also address automat select regular paramet mont carlo simul reveal estim particular well suit estim rank interact low signal nois ratio regim illustr two data analys result easili interpret biplot vizual method avail code|['Geneviève Robin', 'Julie Josse', 'Eric Moulines', 'Sylvain Sardy']|['stat.ME']
2017-03-28T14:04:24Z|2017-03-07T06:40:44Z|http://arxiv.org/abs/1703.02237v1|http://arxiv.org/pdf/1703.02237v1|Scalable Collaborative Targeted Learning for High-Dimensional Data|scalabl collabor target learn high dimension data|Robust inference of a low-dimensional parameter in a large semi-parametric model relies on external estimators of infinite-dimensional features of the distribution of the data. Typically, only one of the latter is optimized for the sake of constructing a well behaved estimator of the low-dimensional parameter of interest. Optimizing more than one of them for the sake of achieving a better bias-variance trade-off in the estimation of the parameter of interest is the core idea driving the general template of the collaborative targeted minimum loss-based estimation (C-TMLE) procedure. The original implementation/instantiation of the C-TMLE template can be presented as a greedy forward stepwise C-TMLE algorithm. It does not scale well when the number $p$ of covariates increases drastically. This motivates the introduction of a novel instantiation of the C-TMLE template where the covariates are pre-ordered. Its time complexity is $\mathcal{O}(p)$ as opposed to the original $\mathcal{O}(p^2)$, a remarkable gain. We propose two pre-ordering strategies and suggest a rule of thumb to develop other meaningful strategies. Because it is usually unclear a priori which pre-ordering strategy to choose, we also introduce another implementation/instantiation called SL-C-TMLE algorithm that enables the data-driven choice of the better pre-ordering strategy given the problem at hand. Its time complexity is $\mathcal{O}(p)$ as well. The computational burden and relative performance of these algorithms were compared in simulation studies involving fully synthetic data or partially synthetic data based on a real world large electronic health database; and in analyses of three real, large electronic health databases. In all analyses involving electronic health databases, the greedy C-TMLE algorithm is unacceptably slow. Simulation studies indicate our scalable C-TMLE and SL-C-TMLE algorithms work well.|robust infer low dimension paramet larg semi parametr model reli extern estim infinit dimension featur distribut data typic onli one latter optim sake construct well behav estim low dimension paramet interest optim one sake achiev better bias varianc trade estim paramet interest core idea drive general templat collabor target minimum loss base estim tmle procedur origin implement instanti tmle templat present greedi forward stepwis tmle algorithm doe scale well number covari increas drastic motiv introduct novel instanti tmle templat covari pre order time complex mathcal oppos origin mathcal remark gain propos two pre order strategi suggest rule thumb develop meaning strategi becaus usual unclear priori pre order strategi choos also introduc anoth implement instanti call sl tmle algorithm enabl data driven choic better pre order strategi given problem hand time complex mathcal well comput burden relat perform algorithm compar simul studi involv fulli synthet data partial synthet data base real world larg electron health databas analys three real larg electron health databas analys involv electron health databas greedi tmle algorithm unaccept slow simul studi indic scalabl tmle sl tmle algorithm work well|['Cheng Ju', 'Susan Gruber', 'Samuel D. Lendle', 'Antoine Chambaz', 'Jessica M. Franklin', 'Richard Wyss', 'Sebastian Schneeweiss', 'Mark J. van der Laan']|['stat.CO', 'stat.ME']
2017-03-28T14:04:24Z|2017-03-25T17:47:33Z|http://arxiv.org/abs/1703.02177v2|http://arxiv.org/pdf/1703.02177v2|Mixtures of Generalized Hyperbolic Distributions and Mixtures of Skew-t   Distributions for Model-Based Clustering with Incomplete Data|mixtur general hyperbol distribut mixtur skew distribut model base cluster incomplet data|Robust clustering from incomplete data is an important topic because, in many practical situations, real data sets are heavy-tailed, asymmetric, and/or have arbitrary patterns of missing observations. Flexible methods and algorithms for model-based clustering are presented via mixture of the generalized hyperbolic distributions and its limiting case, the mixture of multivariate skew-t distributions. An analytically feasible EM algorithm is formulated for parameter estimation and imputation of missing values for mixture models employing missing at random mechanisms. The proposed methodologies are investigated through a simulation study with varying proportions of synthetic missing values and illustrated using a real dataset. Comparisons are made with those obtained from the traditional mixture of generalized hyperbolic distribution counterparts by filling in the missing data using the mean imputation method.|robust cluster incomplet data import topic becaus mani practic situat real data set heavi tail asymmetr arbitrari pattern miss observ flexibl method algorithm model base cluster present via mixtur general hyperbol distribut limit case mixtur multivari skew distribut analyt feasibl em algorithm formul paramet estim imput miss valu mixtur model employ miss random mechan propos methodolog investig simul studi vari proport synthet miss valu illustr use real dataset comparison made obtain tradit mixtur general hyperbol distribut counterpart fill miss data use mean imput method|['Yuhong Wei', 'Paul D. McNicholas']|['stat.ME', 'stat.CO']
2017-03-28T14:04:24Z|2017-03-06T21:19:20Z|http://arxiv.org/abs/1703.02113v1|http://arxiv.org/pdf/1703.02113v1|A fresh look at effect aliasing and interactions: some new wine in old   bottles|fresh look effect alias interact new wine old bottl|"Interactions and effect aliasing are among the fundamental concepts in experimental design. In this paper, some new insights and approaches are provided on these subjects. In the literature, the ""de-aliasing"" of aliased effects is deemed to be impossible. We argue that this ""impossibility"" can indeed be resolved by employing a new approach which consists of reparametrization of effects and exploitation of effect non-orthogonality. This approach is successfully applied to three classes of designs: regular and nonregular two-level fractional factorial designs, and three-level fractional factorial designs. For reparametrization, the notion of conditional main effects (cme's) is employed for two-level regular designs, while the linear-quadratic system is used for three-level designs. For nonregular two-level designs, reparametrization is not needed because the partial aliasing of their effects already induces non-orthogonality. The approach can be extended to general observational data by using a new bi-level variable selection technique based on the cme's. A historical recollection is given on how these ideas were discovered."|interact effect alias among fundament concept experiment design paper new insight approach provid subject literatur de alias alias effect deem imposs argu imposs inde resolv employ new approach consist reparametr effect exploit effect non orthogon approach success appli three class design regular nonregular two level fraction factori design three level fraction factori design reparametr notion condit main effect cme employ two level regular design linear quadrat system use three level design nonregular two level design reparametr need becaus partial alias effect alreadi induc non orthogon approach extend general observ data use new bi level variabl select techniqu base cme histor recollect given idea discov|['C. F. Jeff Wu']|['stat.ME']
2017-03-28T14:04:24Z|2017-03-06T21:17:42Z|http://arxiv.org/abs/1703.02112v1|http://arxiv.org/pdf/1703.02112v1|Process convolution approaches for modeling interacting trajectories|process convolut approach model interact trajectori|"Gaussian processes are a fundamental statistical tool used in a wide range of applications. In the spatio-temporal setting, several families of covariance functions exist to accommodate a wide variety of dependence structures arising in different applications. These parametric families can be restrictive and are insufficient in some situations. In contrast, process convolutions represent a flexible, interpretable approach to defining the covariance of a Gaussian process and have modest requirements to ensure validity. We introduce a generalization of the process convolution approach that employs multiple convolutions sequentially to form a ""process convolution chain."" In our proposed multi-stage framework, complex dependencies that arise from a combination of different interacting mechanisms are decomposed into a series of interpretable kernel smoothers. We demonstrate an application of process convolution chains to model killer whale movement, in which the paths taken by multiple individuals are not independent, but reflect dynamic social interactions within the population. Our proposed model for dependent movement provides inference for the latent dynamic social structure in the study population. Additionally, by leveraging the positive dependence among individual paths, we achieve a reduction in uncertainty for the estimated locations of the whales, compared to a model that treats paths as independent."|gaussian process fundament statist tool use wide rang applic spatio tempor set sever famili covari function exist accommod wide varieti depend structur aris differ applic parametr famili restrict insuffici situat contrast process convolut repres flexibl interpret approach defin covari gaussian process modest requir ensur valid introduc general process convolut approach employ multipl convolut sequenti form process convolut chain propos multi stage framework complex depend aris combin differ interact mechan decompos seri interpret kernel smoother demonstr applic process convolut chain model killer whale movement path taken multipl individu independ reflect dynam social interact within popul propos model depend movement provid infer latent dynam social structur studi popul addit leverag posit depend among individu path achiev reduct uncertainti estim locat whale compar model treat path independ|['Henry R. Scharf', 'Mevin B. Hooten', 'Devin S. Johnson', 'John W. Durban']|['stat.ME', 'stat.AP']
2017-03-28T14:04:24Z|2017-03-06T19:33:24Z|http://arxiv.org/abs/1703.02078v1|http://arxiv.org/pdf/1703.02078v1|Cross-screening in observational studies that test many hypotheses|cross screen observ studi test mani hypothes|"We discuss observational studies that test many causal hypotheses, either hypotheses about many outcomes or many treatments. To be credible an observational study that tests many causal hypotheses must demonstrate that its conclusions are neither artifacts of multiple testing nor of small biases from nonrandom treatment assignment. In a sense that needs to be defined carefully, hidden within a sensitivity analysis for nonrandom assignment is an enormous correction for multiple testing: in the absence of bias, it is extremely improbable that multiple testing alone would create an association insensitive to moderate biases. We propose a new strategy called ""cross-screening"", different from but motivated by recent work of Bogomolov and Heller on replicability. Cross-screening splits the data in half at random, uses the first half to plan a study carried out on the second half, then uses the second half to plan a study carried out on the first half, and reports the more favorable conclusions of the two studies correcting using the Bonferroni inequality for having done two studies. If the two studies happen to concur, then they achieve Bogomolov-Heller replicability; however, importantly, replicability is not required for strong control of the family-wise error rate, and either study alone suffices for firm conclusions. In randomized studies with a few hypotheses, cross-split screening is not an attractive method when compared with conventional methods of multiplicity control, but it can become attractive when hundreds or thousands of hypotheses are subjected to sensitivity analyses in an observational study. We illustrate the technique by comparing 46 biomarkers in individuals who consume large quantities of fish versus little or no fish."|discuss observ studi test mani causal hypothes either hypothes mani outcom mani treatment credibl observ studi test mani causal hypothes must demonstr conclus neither artifact multipl test small bias nonrandom treatment assign sens need defin care hidden within sensit analysi nonrandom assign enorm correct multipl test absenc bias extrem improb multipl test alon would creat associ insensit moder bias propos new strategi call cross screen differ motiv recent work bogomolov heller replic cross screen split data half random use first half plan studi carri second half use second half plan studi carri first half report favor conclus two studi correct use bonferroni inequ done two studi two studi happen concur achiev bogomolov heller replic howev import replic requir strong control famili wise error rate either studi alon suffic firm conclus random studi hypothes cross split screen attract method compar convent method multipl control becom attract hundr thousand hypothes subject sensit analys observ studi illustr techniqu compar biomark individu consum larg quantiti fish versus littl fish|['Qingyuan Zhao', 'Dylan S. Small', 'Paul R. Rosenbaum']|['stat.ME', 'stat.AP']
2017-03-28T14:04:24Z|2017-03-06T13:41:33Z|http://arxiv.org/abs/1703.01866v1|http://arxiv.org/pdf/1703.01866v1|Weighted empirical likelihood for quantile regression with nonignorable   missing covariates|weight empir likelihood quantil regress nonignor miss covari|In this paper, we propose an empirical likelihood-based weighted (ELW) estimator of regression parameter in quantile regression model with nonignorable missing covariates. The proposed ELW estimator is computationally simple and more efficient than the CCA estimator. Simulation results show that the ELW method works remarkably well in finite samples. A real data example is used to illustrate the proposed ELW method.|paper propos empir likelihood base weight elw estim regress paramet quantil regress model nonignor miss covari propos elw estim comput simpl effici cca estim simul result show elw method work remark well finit sampl real data exampl use illustr propos elw method|['Xiaohui Yuan', 'He Si']|['stat.ME']
2017-03-28T14:04:24Z|2017-03-06T10:32:54Z|http://arxiv.org/abs/1703.01805v1|http://arxiv.org/pdf/1703.01805v1|Bayesian Estimation of Kendall's tau Using a Latent Normal Approach|bayesian estim kendal tau use latent normal approach|The rank-based association between two variables can be modeled by introducing a latent normal level to ordinal data. We demonstrate how this approach yields Bayesian inference for Kendall's rank correlation coefficient, improving on a recent Bayesian solution from asymptotic properties of the test statistic.|rank base associ two variabl model introduc latent normal level ordin data demonstr approach yield bayesian infer kendal rank correl coeffici improv recent bayesian solut asymptot properti test statist|['Johnny van Doorn', 'Alexander Ly', 'Maarten Marsman', 'Eric-Jan Wagenmakers']|['stat.ME']
2017-03-28T14:04:24Z|2017-03-06T09:24:07Z|http://arxiv.org/abs/1703.01776v1|http://arxiv.org/pdf/1703.01776v1|Online Sequential Monte Carlo smoother for partially observed stochastic   differential equations|onlin sequenti mont carlo smoother partial observ stochast differenti equat|This paper introduces a new algorithm to approximate smoothed additive functionals for partially observed stochastic differential equations. This method relies on a recent procedure which allows to compute such approximations online, i.e. as the observations are received, and with a computational complexity growing linearly with the number of Monte Carlo samples. This online smoother cannot be used directly in the case of partially observed stochastic differential equations since the transition density of the latent data is usually unknown. We prove that a similar algorithm may still be defined for partially observed continuous processes by replacing this unknown quantity by an unbiased estimator obtained for instance using general Poisson estimators. We prove that this estimator is consistent and its performance are illustrated using data from two models.|paper introduc new algorithm approxim smooth addit function partial observ stochast differenti equat method reli recent procedur allow comput approxim onlin observ receiv comput complex grow linear number mont carlo sampl onlin smoother cannot use direct case partial observ stochast differenti equat sinc transit densiti latent data usual unknown prove similar algorithm may still defin partial observ continu process replac unknown quantiti unbias estim obtain instanc use general poisson estim prove estim consist perform illustr use data two model|['Pierre Gloaguen', 'Marie-Pierre Etienne', 'Sylvain Le Corff']|['stat.ME', 'stat.AP']
2017-03-28T14:04:28Z|2017-03-06T00:14:36Z|http://arxiv.org/abs/1703.01692v1|http://arxiv.org/pdf/1703.01692v1|Detecting Spatial Patterns of Disease in Large Collections of Electronic   Medical Records Using Neighbor-Based Bootstrapping (NB2)|detect spatial pattern diseas larg collect electron medic record use neighbor base bootstrap nb|We introduce a method called neighbor-based bootstrapping (NB2) that can be used to quantify the geospatial variation of a variable. We applied this method to an analysis of the incidence rates of disease from electronic medical record data (ICD-9 codes) for approximately 100 million individuals in the US over a period of 8 years. We considered the incidence rate of disease in each county and its geospatially contiguous neighbors and rank ordered diseases in terms of their degree of geospatial variation as quantified by the NB2 method.   We show that this method yields results in good agreement with established methods for detecting spatial autocorrelation (Moran's I method and kriging). Moreover, the NB2 method can be tuned to identify both large area and small area geospatial variations. This method also applies more generally in any parameter space that can be partitioned to consist of regions and their neighbors.|introduc method call neighbor base bootstrap nb use quantifi geospati variat variabl appli method analysi incid rate diseas electron medic record data icd code approxim million individu us period year consid incid rate diseas counti geospati contigu neighbor rank order diseas term degre geospati variat quantifi nb method show method yield result good agreement establish method detect spatial autocorrel moran method krige moreov nb method tune identifi larg area small area geospati variat method also appli general ani paramet space partit consist region neighbor|['Maria T Patterson', 'Robert L Grossman']|['stat.ME']
2017-03-28T14:04:28Z|2017-03-05T21:14:28Z|http://arxiv.org/abs/1703.01665v1|http://arxiv.org/pdf/1703.01665v1|Anisotropic functional Laplace deconvolution|anisotrop function laplac deconvolut|In the present paper we consider the problem of estimating a three-dimensional function $f$ based on observations from its noisy Laplace convolution. Our study is motivated by the analysis of Dynamic Contrast Enhanced (DCE) imaging data. We construct an adaptive wavelet-Laguerre estimator of $f$, derive minimax lower bounds for the $L^2$-risk when $f$ belongs to a three-dimensional Laguerre-Sobolev ball and demonstrate that the wavelet-Laguerre estimator is adaptive and asymptotically near-optimal in a wide range of Laguerre-Sobolev spaces. We carry out a limited simulations study and show that the estimator performs well in a finite sample setting. Finally, we use the technique for the solution of the Laplace deconvolution problem on the basis of DCE Computerized Tomography data.|present paper consid problem estim three dimension function base observ noisi laplac convolut studi motiv analysi dynam contrast enhanc dce imag data construct adapt wavelet laguerr estim deriv minimax lower bound risk belong three dimension laguerr sobolev ball demonstr wavelet laguerr estim adapt asymptot near optim wide rang laguerr sobolev space carri limit simul studi show estim perform well finit sampl set final use techniqu solut laplac deconvolut problem basi dce computer tomographi data|['Rida Benhaddou', 'Marianna Pensky', 'Rasika Rajapakshage']|['stat.ME', 'Primary 62G05, , secondary 62G08, 62P35']
2017-03-28T14:04:28Z|2017-03-04T20:27:02Z|http://arxiv.org/abs/1703.01518v1|http://arxiv.org/pdf/1703.01518v1|Model-Independent Analytic Nonlinear Blind Source Separation|model independ analyt nonlinear blind sourc separ|Consider a time series of measurements of the state of an evolving system, x(t), where x has two or more components. This paper shows how to perform nonlinear blind source separation; i.e., how to determine if these signals are equal to linear or nonlinear mixtures of the state variables of two or more statistically independent subsystems. First, the local distributions of measurement velocities are processed in order to derive vectors at each point in x-space. If the data are separable, each of these vectors must be directed along a subspace of x-space that is traversed by varying the state variable of one subsystem, while all other subsystems are kept constant. Because of this property, these vectors can be used to construct a small set of mappings, which must contain the unmixing function, if it exists. Therefore, nonlinear blind source separation can be performed by examining the separability of the data after it has been transformed by each of these mappings. The method is analytic, constructive, and model-independent. It is illustrated by blindly recovering the separate utterances of two speakers from nonlinear combinations of their audio waveforms.|consid time seri measur state evolv system two compon paper show perform nonlinear blind sourc separ determin signal equal linear nonlinear mixtur state variabl two statist independ subsystem first local distribut measur veloc process order deriv vector point space data separ vector must direct along subspac space travers vari state variabl one subsystem subsystem kept constant becaus properti vector use construct small set map must contain unmix function exist therefor nonlinear blind sourc separ perform examin separ data transform map method analyt construct model independ illustr blind recov separ utter two speaker nonlinear combin audio waveform|['David N. Levin']|['stat.ME']
2017-03-28T14:04:28Z|2017-03-04T09:12:42Z|http://arxiv.org/abs/1703.01421v1|http://arxiv.org/pdf/1703.01421v1|$l_0$-estimation of piecewise-constant signals on graphs|estim piecewis constant signal graph|We study recovery of piecewise-constant signals over arbitrary graphs by the estimator minimizing an $l_0$-edge-penalized objective. Although exact minimization of this objective may be computationally intractable, we show that the same statistical risk guarantees are achieved by the alpha-expansion algorithm which approximately minimizes this objective in polynomial time. We establish that for graphs with small average vertex degree, these guarantees are rate-optimal in a minimax sense over classes of edge-sparse signals. For application to spatially inhomogeneous graphs, we propose minimization of an edge-weighted variant of this objective where each edge is weighted by its effective resistance or another measure of its contribution to the graph's connectivity. We establish minimax optimality of the resulting estimators over corresponding edge-weighted sparsity classes. We show theoretically that these risk guarantees are not always achieved by the estimator minimizing the $l_1$/total-variation relaxation, and empirically that the $l_0$-based estimates are more accurate in high signal-to-noise settings.|studi recoveri piecewis constant signal arbitrari graph estim minim edg penal object although exact minim object may comput intract show statist risk guarante achiev alpha expans algorithm approxim minim object polynomi time establish graph small averag vertex degre guarante rate optim minimax sens class edg spars signal applic spatial inhomogen graph propos minim edg weight variant object edg weight effect resist anoth measur contribut graph connect establish minimax optim result estim correspond edg weight sparsiti class show theoret risk guarante alway achiev estim minim total variat relax empir base estim accur high signal nois set|['Zhou Fan', 'Leying Guan']|['stat.ME', 'math.ST', 'stat.CO', 'stat.TH']
2017-03-28T14:04:28Z|2017-03-04T00:16:54Z|http://arxiv.org/abs/1703.01364v1|http://arxiv.org/pdf/1703.01364v1|A Matrix Variate Skew-t Distribution|matrix variat skew distribut|Although there is ample work in the literature dealing with skewness in the multivariate setting, there is a relative paucity of work in the matrix variate paradigm. Such work is, for example, useful for modelling three-way data. A matrix variate skew-t distribution is derived based on a mean-variance matrix normal mixture. An expectation-conditional maximization algorithm is developed for parameter estimation. Simulated data are used for illustration.|although ampl work literatur deal skew multivari set relat pauciti work matrix variat paradigm work exampl use model three way data matrix variat skew distribut deriv base mean varianc matrix normal mixtur expect condit maxim algorithm develop paramet estim simul data use illustr|['Michael P. B. Gallaugher', 'Paul D. McNicholas']|['stat.ME', 'math.ST', 'stat.TH']
2017-03-28T14:04:28Z|2017-03-03T16:29:21Z|http://arxiv.org/abs/1703.01234v1|http://arxiv.org/pdf/1703.01234v1|A Bayesian computer model analysis of Robust Bayesian analyses|bayesian comput model analysi robust bayesian analys|We harness the power of Bayesian emulation techniques, designed to aid the analysis of complex computer models, to examine the structure of complex Bayesian analyses themselves. These techniques facilitate robust Bayesian analyses and/or sensitivity analyses of complex problems, and hence allow global exploration of the impacts of choices made in both the likelihood and prior specification. We show how previously intractable problems in robustness studies can be overcome using emulation techniques, and how these methods allow other scientists to quickly extract approximations to posterior results corresponding to their own particular subjective specification. The utility and flexibility of our method is demonstrated on a reanalysis of a real application where Bayesian methods were employed to capture beliefs about river flow. We discuss the obvious extensions and directions of future research that such an approach opens up.|har power bayesian emul techniqu design aid analysi complex comput model examin structur complex bayesian analys themselv techniqu facilit robust bayesian analys sensit analys complex problem henc allow global explor impact choic made likelihood prior specif show previous intract problem robust studi overcom use emul techniqu method allow scientist quick extract approxim posterior result correspond particular subject specif util flexibl method demonstr reanalysi real applic bayesian method employ captur belief river flow discuss obvious extens direct futur research approach open|['Ian Vernon', 'John Paul Gosling']|['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']
2017-03-28T14:04:28Z|2017-03-03T10:30:11Z|http://arxiv.org/abs/1703.01102v1|http://arxiv.org/pdf/1703.01102v1|A New Test of Multivariate Nonlinear Causality|new test multivari nonlinear causal|The multivariate nonlinear Granger causality developed by Bai et al. (2010) plays an important role in detecting the dynamic interrelationships between two groups of variables. Following the idea of Hiemstra-Jones (HJ) test proposed by Hiemstra and Jones (1994), they attempt to establish a central limit theorem (CLT) of their test statistic by applying the asymptotical property of multivariate $U$-statistic. However, Bai et al. (2016) revisit the HJ test and find that the test statistic given by HJ is NOT a function of $U$-statistics which implies that the CLT neither proposed by Hiemstra and Jones (1994) nor the one extended by Bai et al. (2010) is valid for statistical inference. In this paper, we re-estimate the probabilities and reestablish the CLT of the new test statistic. Numerical simulation shows that our new estimates are consistent and our new test performs decent size and power.|multivari nonlinear granger causal develop bai et al play import role detect dynam interrelationship two group variabl follow idea hiemstra jone hj test propos hiemstra jone attempt establish central limit theorem clt test statist appli asymptot properti multivari statist howev bai et al revisit hj test find test statist given hj function statist impli clt neither propos hiemstra jone one extend bai et al valid statist infer paper estim probabl reestablish clt new test statist numer simul show new estim consist new test perform decent size power|['Zhidong Bai', 'Yongchang Hui', 'Zhihui Lv', 'Wing-Keung Wong', 'Shurong Zheng', 'Zhenzhen Zhu']|['stat.ME']
2017-03-28T14:04:28Z|2017-03-02T22:17:52Z|http://arxiv.org/abs/1703.00968v1|http://arxiv.org/pdf/1703.00968v1|Bayesian inference for generalized extreme value distribution with   Gaussian copula dependence|bayesian infer general extrem valu distribut gaussian copula depend|Dependent generalized extreme value (dGEV) models have attracted much attention due to the dependency structure that often appears in real datasets. To construct a dGEV model, a natural approach is to assume that some parameters in the model are time-varying. A previous study has shown that a dependent Gumbel process can be naturally incorporated into a GEV model. The model is a nonlinear state space model with a hidden state that follows a Markov process, with its innovation following a Gumbel distribution. Inference may be made for the model using Bayesian methods, sampling the hidden process from a mixture normal distribution, used to approximate the Gumbel distribution. Thus the response follows an approximate GEV model. We propose a new model in which each marginal distribution is an exact GEV distribution. We use a variable transformation to combine the marginal CDF of a Gumbel distribution with the standard normal copula. Then our model is a nonlinear state space model in which the hidden state equation is Gaussian. We analyze this model using Bayesian methods, and sample the elements of the state vector using particle Gibbs with ancestor sampling (PGAS). The PGAS algorithm turns out to be very efficient in solving nonlinear state space models. We also show our model is flexible enough to incorporate seasonality.|depend general extrem valu dgev model attract much attent due depend structur often appear real dataset construct dgev model natur approach assum paramet model time vari previous studi shown depend gumbel process natur incorpor gev model model nonlinear state space model hidden state follow markov process innov follow gumbel distribut infer may made model use bayesian method sampl hidden process mixtur normal distribut use approxim gumbel distribut thus respons follow approxim gev model propos new model margin distribut exact gev distribut use variabl transform combin margin cdf gumbel distribut standard normal copula model nonlinear state space model hidden state equat gaussian analyz model use bayesian method sampl element state vector use particl gibb ancestor sampl pgas pgas algorithm turn veri effici solv nonlinear state space model also show model flexibl enough incorpor season|['Bo Ning', 'Peter Bloomfield']|['stat.ME']
2017-03-28T14:04:28Z|2017-03-02T18:20:18Z|http://arxiv.org/abs/1703.00884v1|http://arxiv.org/pdf/1703.00884v1|A Dichotomy for Sampling Barrier-Crossing Events of Random Walks with   Regularly Varying Tails|dichotomi sampl barrier cross event random walk regular vari tail|We study how to sample paths of a random walk up to the first time it crosses a fixed barrier, in the setting where the step sizes are iid with negative mean and have a regularly varying right tail. We introduce a desirable property for a change of measure to be suitable for exact simulation. We study whether the change of measure of Blanchet and Glynn (2008) satisfies this property and show that it does so if and only if the tail index $\alpha$ of the right tail lies in the interval $(1, \, 3/2)$.|studi sampl path random walk first time cross fix barrier set step size iid negat mean regular vari right tail introduc desir properti chang measur suitabl exact simul studi whether chang measur blanchet glynn satisfi properti show doe onli tail index alpha right tail lie interv|['Ton Dieker', 'Guido Lagos']|['math.PR', 'stat.ME', '68U20, 60G50, 68W40']
2017-03-28T14:04:28Z|2017-03-02T11:48:24Z|http://arxiv.org/abs/1703.00734v1|http://arxiv.org/pdf/1703.00734v1|Distributed Bayesian Matrix Factorization with Minimal Communication|distribut bayesian matrix factor minim communic|Bayesian matrix factorization (BMF) is a powerful tool for producing low-rank representations of matrices, and giving principled predictions of missing values. However, scaling up MCMC samplers to large matrices has proven to be difficult with parallel algorithms that require communication between MCMC iterations. On the other hand, designing communication-free algorithms is challenging due to the inherent unidentifiability of BMF solutions. We propose posterior propagation, an embarrassingly parallel inference procedure, which hierarchically introduces dependencies between data subsets and thus alleviates the unidentifiability problem.|bayesian matrix factor bmf power tool produc low rank represent matric give principl predict miss valu howev scale mcmc sampler larg matric proven difficult parallel algorithm requir communic mcmc iter hand design communic free algorithm challeng due inher unidentifi bmf solut propos posterior propag embarrass parallel infer procedur hierarch introduc depend data subset thus allevi unidentifi problem|['Xiangju Qin', 'Paul Blomstedt', 'Eemeli Leppäaho', 'Pekka Parviainen', 'Samuel Kaski']|['stat.ML', 'cs.DC', 'cs.LG', 'cs.NA', 'stat.ME']
