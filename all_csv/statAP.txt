2017-03-28T14:02:27Z|2017-03-27T15:36:40Z|http://arxiv.org/abs/1703.09147v1|http://arxiv.org/pdf/1703.09147v1|Two-part models with stochastic processes for modelling longitudinal   semicontinuous data: computationally efficient inference and modelling the   overall marginal mean|two part model stochast process model longitudin semicontinu data comput effici infer model overal margin mean|Several researchers have described two-part models with patient-specific stochastic processes for analysing longitudinal semicontinuous data. In theory, such models can offer greater flexibility than the standard two-part model with patient-specific random effects. However, in practice the high dimensional integrations involved in the marginal likelihood (i.e. integrated over the stochastic processes) significantly complicates model fitting. Thus non-standard computationally intensive procedures based on simulating the marginal likelihood have so far only been proposed. In this paper, we describe an efficient method of implementation by demonstrating how the high dimensional integrations involved in the marginal likelihood can be computed efficiently. Specifically, by using a property of the multivariate normal distribution and the standard marginal cumulative distribution function identity, we transform the marginal likelihood so that the high dimensional integrations are contained in the cumulative distribution function of a multivariate normal distribution, which can then be efficiently evaluated. Hence maximum likelihood estimation can be used to obtain parameter estimates and asymptotic standard errors (from the observed information matrix) of model parameters. We describe our proposed efficient implementation procedure for the standard two-part model parameterisation and when it is of interest to directly model the overall marginal mean. The methodology is applied on a psoriatic arthritis data set concerning functional disability.|sever research describ two part model patient specif stochast process analys longitudin semicontinu data theori model offer greater flexibl standard two part model patient specif random effect howev practic high dimension integr involv margin likelihood integr stochast process signific complic model fit thus non standard comput intens procedur base simul margin likelihood far onli propos paper describ effici method implement demonstr high dimension integr involv margin likelihood comput effici specif use properti multivari normal distribut standard margin cumul distribut function ident transform margin likelihood high dimension integr contain cumul distribut function multivari normal distribut effici evalu henc maximum likelihood estim use obtain paramet estim asymptot standard error observ inform matrix model paramet describ propos effici implement procedur standard two part model parameteris interest direct model overal margin mean methodolog appli psoriat arthriti data set concern function disabl|['Sean Yiu', 'Brian Tom']|['stat.AP']
2017-03-28T14:02:27Z|2017-03-27T11:04:00Z|http://arxiv.org/abs/1703.09007v1|http://arxiv.org/pdf/1703.09007v1|Detection of Spatiotemporally Coherent Rainfall Anomalies Using Markov   Random Fields|detect spatiotempor coher rainfal anomali use markov random field|Precipitation is a large-scale, spatio-temporally heterogeneous phenomenon, with frequent anomalies exhibiting unusually high or low values. We use Markov Random Fields (MRFs) to detect spatio-temporally coherent anomalies in gridded annual rainfall data across India from 1901-2005. MRFs are undirected graphical models where each node is associated with a \{location,year\} pair, with edges connecting nodes representing adjacent locations or years. Some nodes represent observations of precipitation, while the rest represent unobserved (\emph{latent}) states that can take one of three values: high/low/normal. The MRF represents a probability distribution over the variables, using \emph{node potential} and \emph{edge potential} functions defined on nodes and edges of the graph. Optimal values of latent state variables are estimated by maximizing the posterior probability of the observations, using Gibbs sampling. Edge potentials enforce spatial and temporal coherence, and node potentials influence threshold for anomalies by affecting the prior probabilities of the states. The model can be tuned to recover anomalies detected by threshold-based methods. The competing influences of spatial and temporal coherence can be adjusted through edge potentials.   We study spatio-temporal properties of rainfall anomalies discovered by this method, using suitable measures. We identify nonstationarities in occurrence of positive and negative anomalies between the first and second halves of the 20th century. We find that between these periods, there has been decrease in rainfall during June-September (JJAS) and an increase during other months. These effects are highlighted prominently in the statistics of anomalies. Properties of anomalies learnt from this approach could present tests of regional-scale rainfall simulations by climate models and statistical simulators.|precipit larg scale spatio tempor heterogen phenomenon frequent anomali exhibit unusu high low valu use markov random field mrfs detect spatio tempor coher anomali grid annual rainfal data across india mrfs undirect graphic model node associ locat year pair edg connect node repres adjac locat year node repres observ precipit rest repres unobserv emph latent state take one three valu high low normal mrf repres probabl distribut variabl use emph node potenti emph edg potenti function defin node edg graph optim valu latent state variabl estim maxim posterior probabl observ use gibb sampl edg potenti enforc spatial tempor coher node potenti influenc threshold anomali affect prior probabl state model tune recov anomali detect threshold base method compet influenc spatial tempor coher adjust edg potenti studi spatio tempor properti rainfal anomali discov method use suitabl measur identifi nonstationar occurr posit negat anomali first second halv th centuri find period decreas rainfal dure june septemb jjas increas dure month effect highlight promin statist anomali properti anomali learnt approach could present test region scale rainfal simul climat model statist simul|['Adway Mitra', 'Ashwin K. Seshadri']|['stat.AP']
2017-03-28T14:02:27Z|2017-03-27T10:14:54Z|http://arxiv.org/abs/1703.08994v1|http://arxiv.org/pdf/1703.08994v1|Value of Information: Sensitivity Analysis and Research Design in   Bayesian Evidence Synthesis|valu inform sensit analysi research design bayesian evid synthesi|Suppose we have a Bayesian model which combines evidence from several different sources. We want to know which model parameters most affect the estimate or decision from the model, or which of the parameter uncertainties drive the decision uncertainty. Furthermore we want to prioritise what further data should be collected. These questions can be addressed by Value of Information (VoI) analysis, in which we estimate expected reductions in loss from learning specific parameters or collecting data of a given design. We describe the theory and practice of VoI for Bayesian evidence synthesis, using and extending ideas from health economics, computer modelling and Bayesian design. The methods are general to a range of decision problems including point estimation and choices between discrete actions. We apply them to a model for estimating prevalence of HIV infection, combining indirect information from several surveys, registers and expert beliefs. This analysis shows which parameters contribute most of the uncertainty about each prevalence estimate, and provides the expected improvements in precision from collecting specific amounts of additional data.|suppos bayesian model combin evid sever differ sourc want know model paramet affect estim decis model paramet uncertainti drive decis uncertainti furthermor want prioritis data collect question address valu inform voi analysi estim expect reduct loss learn specif paramet collect data given design describ theori practic voi bayesian evid synthesi use extend idea health econom comput model bayesian design method general rang decis problem includ point estim choic discret action appli model estim preval hiv infect combin indirect inform sever survey regist expert belief analysi show paramet contribut uncertainti preval estim provid expect improv precis collect specif amount addit data|['Christopher Jackson', 'Anne Presanis', 'Stefano Conti', 'Daniela De Angelis']|['stat.AP']
2017-03-28T14:02:27Z|2017-03-27T07:11:05Z|http://arxiv.org/abs/1703.08954v1|http://arxiv.org/pdf/1703.08954v1|Exact and approximate limit behaviour of the Yule tree's cophenetic   index|exact approxim limit behaviour yule tree cophenet index|In this work we study the limit distribution of an appropriately normalized cophenetic index of the pure birth tree on n contemporary tips. We show that this normalized phylogenetic balance index is a submartingale that converges almost surely and in L2. We link our work with studies on trees without branch lengths and show that in this case the limit distribution is a contraction type distribution, similar to the Quicksort limit distribution. In the continuous branch case we suggest approximations to the limit distribution. We propose heuristic methods of simulating from these distributions and it may be observed that these algorithms result in reasonable tails. Therefore, we postulate using quantiles of the derived distributions for hypothesis testing, whether an observed phylogenetic tree is consistent with the pure birth process. Simulating a sample by the proposed heuristics is rapid while exact simulation (simulating the tree and then calculating the index) is a time-consuming procedure.|work studi limit distribut appropri normal cophenet index pure birth tree contemporari tip show normal phylogenet balanc index submartingal converg almost sure link work studi tree without branch length show case limit distribut contract type distribut similar quicksort limit distribut continu branch case suggest approxim limit distribut propos heurist method simul distribut may observ algorithm result reason tail therefor postul use quantil deriv distribut hypothesi test whether observ phylogenet tree consist pure birth process simul sampl propos heurist rapid exact simul simul tree calcul index time consum procedur|['Krzysztof Bartoszek']|['q-bio.PE', 'math.PR', 'stat.AP', '05C80, 60F15, 60J85, 62M02, 62P10, 92-08, 92B10, 92D15']
2017-03-28T14:02:27Z|2017-03-27T03:54:15Z|http://arxiv.org/abs/1703.08920v1|http://arxiv.org/pdf/1703.08920v1|Modeling high dimensional multichannel brain signals|model high dimension multichannel brain signal|In this paper, our goal is to model functional and effective (directional) connectivity in network of multichannel brain physiological signals (e.g., electroencephalograms, local field potentials). The primary challenges here are twofold: first, there are major statistical and computational difficulties for modeling and analyzing high dimensional multichannel brain signals; second, there is no set of universally-agreed measures for characterizing connectivity. To model multichannel brain signals, our approach is to fit a vector autoregressive (VAR) model with sufficiently high order so that complex lead-lag temporal dynamics between the channels can be accurately characterized. However, such a model contains a large number of parameters. Thus, we will estimate the high dimensional VAR parameter space by our proposed hybrid LASSLE method (LASSO+LSE) which is imposes regularization on the first step (to control for sparsity) and constrained least squares estimation on the second step (to improve bias and mean-squared error of the estimator). Then to characterize connectivity between channels in a brain network, we will use various measures but put an emphasis on partial directed coherence (PDC) in order to capture directional connectivity between channels. PDC is a directed frequency-specific measure that explains the extent to which the present oscillatory activity in a sender channel influences the future oscillatory activity in a specific receiver channel relative all possible receivers in the network. Using the proposed modeling approach, we have achieved some insights on learning in a rat engaged in a non-spatial memory task.|paper goal model function effect direct connect network multichannel brain physiolog signal electroencephalogram local field potenti primari challeng twofold first major statist comput difficulti model analyz high dimension multichannel brain signal second set univers agre measur character connect model multichannel brain signal approach fit vector autoregress var model suffici high order complex lead lag tempor dynam channel accur character howev model contain larg number paramet thus estim high dimension var paramet space propos hybrid lassl method lasso lse impos regular first step control sparsiti constrain least squar estim second step improv bias mean squar error estim character connect channel brain network use various measur put emphasi partial direct coher pdc order captur direct connect channel pdc direct frequenc specif measur explain extent present oscillatori activ sender channel influenc futur oscillatori activ specif receiv channel relat possibl receiv network use propos model approach achiev insight learn rat engag non spatial memori task|['Lechuan Hu', 'Norbert Fortin', 'Hernando Ombao']|['stat.AP']
2017-03-28T14:02:27Z|2017-03-25T17:57:31Z|http://arxiv.org/abs/1703.08723v1|http://arxiv.org/pdf/1703.08723v1|Extending Growth Mixture Models Using Continuous Non-Elliptical   Distributions|extend growth mixtur model use continu non ellipt distribut|Growth mixture models (GMMs) incorporate both conventional random effects growth modeling and latent trajectory classes as in finite mixture modeling; therefore, they offer a way to handle the unobserved heterogeneity between subjects in their development. GMMs with Gaussian random effects dominate the literature. When the data are asymmetric and/or have heavier tails, more than one latent class is required to capture the observed variable distribution. Therefore, a GMM with continuous non-elliptical distributions is proposed to capture skewness and heavier tails in the data set. Specifically, multivariate skew-t distributions and generalized hyperbolic distributions are introduced to extend GMMs. When extending GMMs, four statistical models are considered with differing distributions of measurement errors and random effects. The mathematical development of a GMM with non-elliptical distributions relies on its relationship with the generalized inverse Gaussian distribution. Parameter estimation is outlined within the expectation-maximization framework before the performance of our GMM with non-elliptical distributions is illustrated on simulated and real data.|growth mixtur model gmms incorpor convent random effect growth model latent trajectori class finit mixtur model therefor offer way handl unobserv heterogen subject develop gmms gaussian random effect domin literatur data asymmetr heavier tail one latent class requir captur observ variabl distribut therefor gmm continu non ellipt distribut propos captur skew heavier tail data set specif multivari skew distribut general hyperbol distribut introduc extend gmms extend gmms four statist model consid differ distribut measur error random effect mathemat develop gmm non ellipt distribut reli relationship general invers gaussian distribut paramet estim outlin within expect maxim framework befor perform gmm non ellipt distribut illustr simul real data|['Yuhong Wei', 'Emilie Shireman', 'Paul D. McNicholas', 'Douglas L. Steinley']|['stat.ME', 'stat.AP', 'stat.CO']
2017-03-28T14:02:27Z|2017-03-25T03:44:05Z|http://arxiv.org/abs/1703.08644v1|http://arxiv.org/pdf/1703.08644v1|Exact Spike Train Inference Via $\ell_0$ Optimization|exact spike train infer via ell optim|In recent years, new technologies in neuroscience have made it possible to measure the activities of large numbers of neurons in behaving animals. For each neuron, a fluorescence trace is measured; this can be seen as a first-order approximation of the neuron's activity over time. Determining the exact time at which a neuron spikes on the basis of its fluorescence trace is an important open problem in the field of computational neuroscience.   Recently, a convex optimization problem involving an $\ell_1$ penalty was proposed for this task. In this paper, we slightly modify that recent proposal by replacing the $\ell_1$ penalty with an $\ell_0$ penalty. In stark contrast to the conventional wisdom that $\ell_0$ optimization problems are computationally intractable, we show that the resulting optimization problem can be efficiently solved for the global optimum using an extremely simple and efficient dynamic programming algorithm. Our R-language implementation of the proposed algorithm runs in a few minutes on fluorescence traces of $100,000$ timesteps. Furthermore, our proposal leads to substantially better results than the previous $\ell_1$ proposal, on synthetic data as well as on two calcium imaging data sets.   R-language software for our proposal is now available at https://github.com/jewellsean/LZeroSpikeInference and will be available soon on CRAN in the package LZeroSpikeInference.|recent year new technolog neurosci made possibl measur activ larg number neuron behav anim neuron fluoresc trace measur seen first order approxim neuron activ time determin exact time neuron spike basi fluoresc trace import open problem field comput neurosci recent convex optim problem involv ell penalti propos task paper slight modifi recent propos replac ell penalti ell penalti stark contrast convent wisdom ell optim problem comput intract show result optim problem effici solv global optimum use extrem simpl effici dynam program algorithm languag implement propos algorithm run minut fluoresc trace timestep furthermor propos lead substanti better result previous ell propos synthet data well two calcium imag data set languag softwar propos avail https github com jewellsean lzerospikeinfer avail soon cran packag lzerospikeinfer|['Sean Jewell', 'Daniela Witten']|['stat.AP']
2017-03-28T14:02:27Z|2017-03-24T23:04:17Z|http://arxiv.org/abs/1703.08620v1|http://arxiv.org/pdf/1703.08620v1|LANOVA Penalization for Unreplicated Data|lanova penal unrepl data|We consider the problem of estimating the entries of an unknown mean matrix or tensor, $\boldsymbol M$, given a single noisy realization, $\boldsymbol Y = \boldsymbol M + \boldsymbol Z$. In the matrix case, we address this problem by decomposing $\boldsymbol M$ into a component that is additive in the rows and columns, i.e. the additive ANOVA decomposition of $\boldsymbol M$, plus a matrix of elementwise effects, $\boldsymbol C$, and assuming that $\boldsymbol C$ may be sparse. Accordingly, we estimate $\boldsymbol M$ by solving a penalized regression problem, applying a lasso penalty for elements of $\boldsymbol C$. We call the corresponding estimate of $\boldsymbol M$ the LANOVA penalized estimate. Although solving this penalized regression problem is straightforward, specifying appropriate values of the penalty parameters is not. Leveraging the posterior mode interpretation of the penalized regression problem, we define and study moment-based empirical Bayes estimators of the penalty parameters. We show that our empirical Bayes estimators are consistent, and examine the behavior of LANOVA penalized estimates under misspecification of the distribution of elements of $\boldsymbol C$. We extend LANOVA penalization to accommodate sparsity of row and column effects and to tensor data. We demonstrate empirical Bayes LANOVA penalization in analyses of several datasets, including a matrix of microarray data, a three-way tensor of fMRI data and a three-way tensor of experimental data.|consid problem estim entri unknown mean matrix tensor boldsymbol given singl noisi realize boldsymbol boldsymbol boldsymbol matrix case address problem decompos boldsymbol compon addit row column addit anova decomposit boldsymbol plus matrix elementwis effect boldsymbol assum boldsymbol may spars accord estim boldsymbol solv penal regress problem appli lasso penalti element boldsymbol call correspond estim boldsymbol lanova penal estim although solv penal regress problem straightforward specifi appropri valu penalti paramet leverag posterior mode interpret penal regress problem defin studi moment base empir bay estim penalti paramet show empir bay estim consist examin behavior lanova penal estim misspecif distribut element boldsymbol extend lanova penal accommod sparsiti row column effect tensor data demonstr empir bay lanova penal analys sever dataset includ matrix microarray data three way tensor fmri data three way tensor experiment data|['Maryclare Griffin', 'Peter Hoff']|['stat.ME', 'stat.AP']
2017-03-28T14:02:27Z|2017-03-24T16:08:21Z|http://arxiv.org/abs/1703.08487v1|http://arxiv.org/pdf/1703.08487v1|Multiscale Granger causality|multiscal granger causal|In the study of complex physical and biological systems represented by multivariate stochastic processes, an issue of great relevance is the description of the system dynamics spanning multiple temporal scales. While methods to assess the dynamic complexity of individual processes at different time scales are well-established, the multiscale evaluation of directed interactions between processes is complicated by theoretical and practical issues such as filtering and downsampling. Here we extend the very popular measure of Granger causality (GC), a prominent tool for assessing directed lagged interactions between joint processes, to quantify information transfer across multiple time scales. We show that the multiscale processing of a vector autoregressive (AR) process introduces a moving average (MA) component, and describe how to represent the resulting ARMA process using state space (SS) models and to combine the SS model parameters for computing exact GC values at arbitrarily large time scales. We exploit the theoretical formulation to identify peculiar features of multiscale GC in basic AR processes, and demonstrate with numerical simulations the much larger estimation accuracy of the SS approach compared with pure AR modeling of filtered and downsampled data. The improved computational reliability is exploited to disclose meaningful multiscale patterns of information transfer between global temperature and carbon dioxide concentration time series, both in paleoclimate and in recent years.|studi complex physic biolog system repres multivari stochast process issu great relev descript system dynam span multipl tempor scale method assess dynam complex individu process differ time scale well establish multiscal evalu direct interact process complic theoret practic issu filter downsampl extend veri popular measur granger causal gc promin tool assess direct lag interact joint process quantifi inform transfer across multipl time scale show multiscal process vector autoregress ar process introduc move averag compon describ repres result arma process use state space ss model combin ss model paramet comput exact gc valu arbitrarili larg time scale exploit theoret formul identifi peculiar featur multiscal gc basic ar process demonstr numer simul much larger estim accuraci ss approach compar pure ar model filter downsampl data improv comput reliabl exploit disclos meaning multiscal pattern inform transfer global temperatur carbon dioxid concentr time seri paleoclim recent year|['Luca Faes', 'Giandomenico Nollo', 'Sebastiano Stramaglia', 'Daniele Marinazzo']|['stat.ME', 'math.ST', 'stat.AP', 'stat.TH']
2017-03-28T14:02:27Z|2017-03-24T14:40:45Z|http://arxiv.org/abs/1703.08429v1|http://arxiv.org/pdf/1703.08429v1|Modeling and Estimation for Self-Exciting Spatio-Temporal Models of   Terrorist Activity|model estim self excit spatio tempor model terrorist activ|Spatio-temporal hierarchical modeling is an extremely attractive way to model the spread of crime or terrorism data over a given region, especially when the observations are counts and must be modeled discretely. The spatio-temporal diffusion is placed, as a matter of convenience, in the process model allowing for straightforward estimation of the diffusion parameters through Bayesian techniques. However, this method of modeling does not allow for the existence of self-excitation, or a temporal data model dependency, that has been shown to exist in criminal and terrorism data. In this manuscript we will use existing theories on how violence spreads to create models that allow for both spatio-temporal diffusion in the process model as well as temporal diffusion, or self-excitation, in the data model. We will further demonstrate how Laplace approximations similar to their use in Integrated Nested Laplace Approximation can be used to quickly and accurately conduct inference of self-exciting spatio-temporal models allowing practitioners a new way of fitting and comparing multiple process models. We will illustrate this approach by fitting a self-exciting spatio-temporal model to terrorism data in Iraq and demonstrate how choice of process model leads to differing conclusions on the existence of self-excitation in the data and differing conclusions on how violence is spreading spatio-temporally.|spatio tempor hierarch model extrem attract way model spread crime terror data given region especi observ count must model discret spatio tempor diffus place matter conveni process model allow straightforward estim diffus paramet bayesian techniqu howev method model doe allow exist self excit tempor data model depend shown exist crimin terror data manuscript use exist theori violenc spread creat model allow spatio tempor diffus process model well tempor diffus self excit data model demonstr laplac approxim similar use integr nest laplac approxim use quick accur conduct infer self excit spatio tempor model allow practition new way fit compar multipl process model illustr approach fit self excit spatio tempor model terror data iraq demonstr choic process model lead differ conclus exist self excit data differ conclus violenc spread spatio tempor|['Nicholas J. Clark', 'Philip M. Dixon']|['stat.AP']
2017-03-28T14:02:31Z|2017-03-24T00:05:08Z|http://arxiv.org/abs/1703.08254v1|http://arxiv.org/pdf/1703.08254v1|Improved NN-JPDAF for Joint Multiple Target Tracking and Feature   Extraction|improv nn jpdaf joint multipl target track featur extract|Feature aided tracking can often yield improved tracking performance over the standard multiple target tracking (MTT) algorithms with only kinematic measurements. However, in many applications, the feature signal of the targets consists of sparse Fourier-domain signals. It changes quickly and nonlinearly in the time domain, and the feature measurements are corrupted by missed detections and mis-associations. These two factors make it hard to extract the feature information to be used in MTT. In this paper, we develop a feature-aided nearest neighbour joint probabilistic data association filter (NN-JPDAF) for joint MTT and feature extraction in dense target environments. To estimate the rapidly varying feature signal from incomplete and corrupted measurements, we use the atomic norm constraint to formulate the sparsity of feature signal and use the $\ell_1$-norm to formulate the sparsity of the corruption induced by mis-associations. Based on the sparse representation, the feature signal are estimated by solving a semidefinite program (SDP) which is convex. We also provide an iterative method for solving this SDP via the alternating direction method of multipliers (ADMM) where each iteration involves closed-form computation. With the estimated feature signal, re-filtering is performed to estimate the kinematic states of the targets, where the association makes use of both kinematic and feature information. Simulation results are presented to illustrate the performance of the proposed algorithm in a radar application.|featur aid track often yield improv track perform standard multipl target track mtt algorithm onli kinemat measur howev mani applic featur signal target consist spars fourier domain signal chang quick nonlinear time domain featur measur corrupt miss detect mis associ two factor make hard extract featur inform use mtt paper develop featur aid nearest neighbour joint probabilist data associ filter nn jpdaf joint mtt featur extract dens target environ estim rapid vari featur signal incomplet corrupt measur use atom norm constraint formul sparsiti featur signal use ell norm formul sparsiti corrupt induc mis associ base spars represent featur signal estim solv semidefinit program sdp convex also provid iter method solv sdp via altern direct method multipli admm iter involv close form comput estim featur signal filter perform estim kinemat state target associ make use kinemat featur inform simul result present illustr perform propos algorithm radar applic|['Le Zheng', 'Xiaodong Wang']|['cs.SY', 'stat.AP']
2017-03-28T14:02:31Z|2017-03-23T15:38:17Z|http://arxiv.org/abs/1703.08111v1|http://arxiv.org/pdf/1703.08111v1|Alternating optimization for GxE modelling with weighted genetic and   environmental scores: examples from the MAVAN study|altern optim gxe model weight genet environment score exampl mavan studi|Motivated by the goal of expanding currently existing genotype x environment interaction (GxE) models to simultaneously include multiple genetic variants and environmental exposures in a parsimonious way, we developed a novel method to estimate the parameters in a GxE model, where G is a weighted sum of genetic variants (genetic score) and E is a weighted sum of environments (environmental score). The approach uses alternating optimization to estimate the parameters of the GxE model. This is an iterative process where the genetic score weights, the environmental score weights, and the main model parameters are estimated in turn assuming the other parameters to be constant. This technique can be used to construct relatively complex interaction models that are constrained to a particular structure, and hence contain fewer parameters.   We present the model as a two-way interaction longitudinal mixed model, for which ordinary linear regression is a special case, but it can easily be extended to be compatible with k-way interaction models and generalized linear mixed models. The model is implemented in R (LEGIT package) and using SAS macros (LEGIT_SAS). Here we present examples from the Maternal Adversity, Vulnerability, and Neurodevelopment (MAVAN) study where we improve significantly upon already existing models using alternating optimization. Furthermore, through simulations, we demonstrate the power and validity of this approach even with small sample sizes.|motiv goal expand current exist genotyp environ interact gxe model simultan includ multipl genet variant environment exposur parsimoni way develop novel method estim paramet gxe model weight sum genet variant genet score weight sum environ environment score approach use altern optim estim paramet gxe model iter process genet score weight environment score weight main model paramet estim turn assum paramet constant techniqu use construct relat complex interact model constrain particular structur henc contain fewer paramet present model two way interact longitudin mix model ordinari linear regress special case easili extend compat way interact model general linear mix model model implement legit packag use sas macro legit sas present exampl matern advers vulner neurodevelop mavan studi improv signific upon alreadi exist model use altern optim furthermor simul demonstr power valid approach even small sampl size|['Alexia Jolicoeur-Martineau', 'Ashley Wazana', 'Eszter Székely', 'Meir Steiner', 'Alison S. Fleming', 'James L. Kennedy', 'Michael J. Meaney', 'Celia M. T. Greenwood']|['stat.AP']
2017-03-28T14:02:31Z|2017-03-23T13:53:06Z|http://arxiv.org/abs/1703.08071v1|http://arxiv.org/pdf/1703.08071v1|Quantifying and suppressing ranking bias in a large citation network|quantifi suppress rank bias larg citat network|It is widely recognized that citation counts for papers from different fields cannot be directly compared because different scientific fields adopt different citation practices. Citation counts are also strongly biased by paper age since older papers had more time to attract citations. Various procedures aim at suppressing these biases and give rise to new normalized indicators, such as the relative citation count. We use a large citation dataset from Microsoft Academic Graph and a new statistical framework based on the Mahalanobis distance to show that the rankings by well known indicators, including the relative citation count and Google's PageRank score, are significantly biased by paper field and age. We propose a general normalization procedure motivated by the $z$-score which produces much less biased rankings when applied to citation count and PageRank score.|wide recogn citat count paper differ field cannot direct compar becaus differ scientif field adopt differ citat practic citat count also strong bias paper age sinc older paper time attract citat various procedur aim suppress bias give rise new normal indic relat citat count use larg citat dataset microsoft academ graph new statist framework base mahalanobi distanc show rank well known indic includ relat citat count googl pagerank score signific bias paper field age propos general normal procedur motiv score produc much less bias rank appli citat count pagerank score|['Giacomo Vaccario', 'Matus Medo', 'Nicolas Wider', 'Manuel Sebastian Mariani']|['physics.soc-ph', 'cs.DL', 'cs.IR', 'physics.data-an', 'stat.AP']
2017-03-28T14:02:31Z|2017-03-21T19:53:59Z|http://arxiv.org/abs/1703.07408v1|http://arxiv.org/pdf/1703.07408v1|Maximum a posteriori estimation through simulated annealing for binary   asteroid orbit determination|maximum posteriori estim simul anneal binari asteroid orbit determin|This paper considers a new method for the binary asteroid orbit determination problem. The method is based on the Bayesian approach with a global optimisation algorithm. The orbital parameters to be determined are modelled through a posteriori, including a priori and likelihood terms. The first term constrains the parameters search space. It allows to introduce knowledge about orbit, if such information is available, but at the same time it does not require a good initial estimation of parameters. The second term is based on given observations, besides it allows us to use and to compare different observational error models. Ones the a posteriori model is build the estimator of the orbital parameters is computed using a global optimisation procedure: the simulated annealing algorithm. The new method was implemented for simulated and real observations, having received successful result, and also verified for ephemeris prediction capability. The new approach can prove useful in case of small numbers of observations and/or in case of non-Gaussian observational errors, when the classical least-squares method can not be applied.|paper consid new method binari asteroid orbit determin problem method base bayesian approach global optimis algorithm orbit paramet determin model posteriori includ priori likelihood term first term constrain paramet search space allow introduc knowledg orbit inform avail time doe requir good initi estim paramet second term base given observ besid allow us use compar differ observ error model one posteriori model build estim orbit paramet comput use global optimis procedur simul anneal algorithm new method implement simul real observ receiv success result also verifi ephemeri predict capabl new approach prove use case small number observ case non gaussian observ error classic least squar method appli|['Irina D. Kovalenko', 'Radu S. Stoica', 'Nikolay V. Emelyanov']|['astro-ph.IM', 'astro-ph.EP', 'stat.AP']
2017-03-28T14:02:31Z|2017-03-21T18:00:02Z|http://arxiv.org/abs/1703.07355v1|http://arxiv.org/abs/1703.07355v1|An Army of Me: Sockpuppets in Online Discussion Communities|armi sockpuppet onlin discuss communiti|"In online discussion communities, users can interact and share information and opinions on a wide variety of topics. However, some users may create multiple identities, or sockpuppets, and engage in undesired behavior by deceiving others or manipulating discussions. In this work, we study sockpuppetry across nine discussion communities, and show that sockpuppets differ from ordinary users in terms of their posting behavior, linguistic traits, as well as social network structure. Sockpuppets tend to start fewer discussions, write shorter posts, use more personal pronouns such as ""I"", and have more clustered ego-networks. Further, pairs of sockpuppets controlled by the same individual are more likely to interact on the same discussion at the same time than pairs of ordinary users. Our analysis suggests a taxonomy of deceptive behavior in discussion communities. Pairs of sockpuppets can vary in their deceptiveness, i.e., whether they pretend to be different users, or their supportiveness, i.e., if they support arguments of other sockpuppets controlled by the same user. We apply these findings to a series of prediction tasks, notably, to identify whether a pair of accounts belongs to the same underlying user or not. Altogether, this work presents a data-driven view of deception in online discussion communities and paves the way towards the automatic detection of sockpuppets."|onlin discuss communiti user interact share inform opinion wide varieti topic howev user may creat multipl ident sockpuppet engag undesir behavior deceiv manipul discuss work studi sockpuppetri across nine discuss communiti show sockpuppet differ ordinari user term post behavior linguist trait well social network structur sockpuppet tend start fewer discuss write shorter post use person pronoun cluster ego network pair sockpuppet control individu like interact discuss time pair ordinari user analysi suggest taxonomi decept behavior discuss communiti pair sockpuppet vari decept whether pretend differ user support support argument sockpuppet control user appli find seri predict task notabl identifi whether pair account belong user altogeth work present data driven view decept onlin discuss communiti pave way toward automat detect sockpuppet|['Srijan Kumar', 'Justin Cheng', 'Jure Leskovec', 'V. S. Subrahmanian']|['cs.SI', 'cs.CY', 'physics.soc-ph', 'stat.AP', 'stat.ML']
2017-03-28T14:02:31Z|2017-03-21T16:48:50Z|http://arxiv.org/abs/1703.07309v1|http://arxiv.org/pdf/1703.07309v1|Phytoplankton Hotspot Prediction With an Unsupervised Spatial Community   Model|phytoplankton hotspot predict unsupervis spatial communiti model|Many interesting natural phenomena are sparsely distributed and discrete. Locating the hotspots of such sparsely distributed phenomena is often difficult because their density gradient is likely to be very noisy. We present a novel approach to this search problem, where we model the co-occurrence relations between a robot's observations with a Bayesian nonparametric topic model. This approach makes it possible to produce a robust estimate of the spatial distribution of the target, even in the absence of direct target observations. We apply the proposed approach to the problem of finding the spatial locations of the hotspots of a specific phytoplankton taxon in the ocean. We use classified image data from Imaging FlowCytobot (IFCB), which automatically measures individual microscopic cells and colonies of cells. Given these individual taxon-specific observations, we learn a phytoplankton community model that characterizes the co-occurrence relations between taxa. We present experiments with simulated robot missions drawn from real observation data collected during a research cruise traversing the US Atlantic coast. Our results show that the proposed approach outperforms nearest neighbor and k-means based methods for predicting the spatial distribution of hotspots from in-situ observations.|mani interest natur phenomena spars distribut discret locat hotspot spars distribut phenomena often difficult becaus densiti gradient like veri noisi present novel approach search problem model co occurr relat robot observ bayesian nonparametr topic model approach make possibl produc robust estim spatial distribut target even absenc direct target observ appli propos approach problem find spatial locat hotspot specif phytoplankton taxon ocean use classifi imag data imag flowcytobot ifcb automat measur individu microscop cell coloni cell given individu taxon specif observ learn phytoplankton communiti model character co occurr relat taxa present experi simul robot mission drawn real observ data collect dure research cruis travers us atlant coast result show propos approach outperform nearest neighbor mean base method predict spatial distribut hotspot situ observ|['Arnold Kalmbach', 'Yogesh Girdhar', 'Heidi M. Sosik', 'Gregory Dudek']|['cs.RO', 'stat.AP']
2017-03-28T14:02:31Z|2017-03-21T15:02:45Z|http://arxiv.org/abs/1703.07256v1|http://arxiv.org/pdf/1703.07256v1|Statistical Topology and the Random Interstellar Medium|statist topolog random interstellar medium|Current astrophysical models of the interstellar medium assume that small scale variation and noise can be modelled as Gaussian random fields or simple transformations thereof, such as lognormal. We use topological methods to investigate this assumption for three regions of the southern sky. We consider Gaussian random fields on two-dimensional lattices and investigate the expected distribution of topological structures quantified through Betti numbers. We demonstrate that there are circumstances where differences in topology can identify differences in distributions when conventional marginal or correlation analyses may not. We propose a non-parametric method for comparing two fields based on the counts of topological features and the geometry of the associated persistence diagrams. When we apply the methods to the astrophysical data, we find strong evidence against a Gaussian random field model for each of the three regions of the interstellar medium that we consider. Further, we show that there are topological differences at a local scale between these different regions.|current astrophys model interstellar medium assum small scale variat nois model gaussian random field simpl transform thereof lognorm use topolog method investig assumpt three region southern sky consid gaussian random field two dimension lattic investig expect distribut topolog structur quantifi betti number demonstr circumst differ topolog identifi differ distribut convent margin correl analys may propos non parametr method compar two field base count topolog featur geometri associ persist diagram appli method astrophys data find strong evid gaussian random field model three region interstellar medium consid show topolog differ local scale differ region|['Robin Henderson', 'Irina Makarenko', 'Paul Bushby', 'Andrew Fletcher', 'Anvar Shukurov']|['stat.AP', 'astro-ph.GA']
2017-03-28T14:02:31Z|2017-03-21T10:45:07Z|http://arxiv.org/abs/1703.07137v1|http://arxiv.org/pdf/1703.07137v1|MRI-based Surgical Planning for Lumbar Spinal Stenosis|mri base surgic plan lumbar spinal stenosi|The most common reason for spinal surgery in elderly patients is lumbar spinal stenosis(LSS). For LSS, treatment decisions based on clinical and radiological information as well as personal experience of the surgeon shows large variance. Thus a standardized support system is of high value for a more objective and reproducible decision. In this work, we develop an automated algorithm to localize the stenosis causing the symptoms of the patient in magnetic resonance imaging (MRI). With 22 MRI features of each of five spinal levels of 321 patients, we show it is possible to predict the location of lesion triggering the symptoms. To support this hypothesis, we conduct an automated analysis of labeled and unlabeled MRI scans extracted from 788 patients. We confirm quantitatively the importance of radiological information and provide an algorithmic pipeline for working with raw MRI scans.|common reason spinal surgeri elder patient lumbar spinal stenosi lss lss treatment decis base clinic radiolog inform well person experi surgeon show larg varianc thus standard support system high valu object reproduc decis work develop autom algorithm local stenosi caus symptom patient magnet reson imag mri mri featur five spinal level patient show possibl predict locat lesion trigger symptom support hypothesi conduct autom analysi label unlabel mri scan extract patient confirm quantit import radiolog inform provid algorithm pipelin work raw mri scan|['Gabriele Abbati', 'Stefan Bauer', 'Peter J. Schüffler', 'Jakob Burgstaller', 'Ulrike Held', 'Sebastian Winklhofer', 'Johann Steurer', 'Joachim M. Buhmann']|['stat.AP']
2017-03-28T14:02:31Z|2017-03-21T02:45:23Z|http://arxiv.org/abs/1703.07030v1|http://arxiv.org/pdf/1703.07030v1|An Investigation of Three-point Shooting through an Analysis of NBA   Player Tracking Data|investig three point shoot analysi nba player track data|I address the difficult challenge of measuring the relative influence of competing basketball game strategies, and I apply my analysis to plays resulting in three-point shots. I use a glut of SportVU player tracking data from over 600 NBA games to derive custom position-based features that capture tangible game strategies from game-play data, such as teamwork, player matchups, and on-ball defender distances. Then, I demonstrate statistical methods for measuring the relative importance of any given basketball strategy. In doing so, I highlight the high importance of teamwork based strategies in affecting three-point shot success. By coupling SportVU data with an advanced variable importance algorithm I am able to extract meaningful results that would have been impossible to achieve even 3 years ago. Further, I demonstrate how player-tracking based features can be used to measure the three- point shooting propensity of players, and I show how this measurement can identify effective shooters that are either highly-utilized or under-utilized. Altogether, my findings provide a substantial body of work for influencing basketball strategy, and for measuring the effectiveness of basketball players.|address difficult challeng measur relat influenc compet basketbal game strategi appli analysi play result three point shot use glut sportvu player track data nba game deriv custom posit base featur captur tangibl game strategi game play data teamwork player matchup ball defend distanc demonstr statist method measur relat import ani given basketbal strategi highlight high import teamwork base strategi affect three point shot success coupl sportvu data advanc variabl import algorithm abl extract meaning result would imposs achiev even year ago demonstr player track base featur use measur three point shoot propens player show measur identifi effect shooter either high util util altogeth find provid substanti bodi work influenc basketbal strategi measur effect basketbal player|['Bradley A. Sliz']|['stat.AP']
2017-03-28T14:02:31Z|2017-03-20T20:26:10Z|http://arxiv.org/abs/1703.06957v1|http://arxiv.org/pdf/1703.06957v1|Nuisance parameter based sample size re-estimation incorporating prior   information|nuisanc paramet base sampl size estim incorpor prior inform|Prior information is often incorporated informally when planning a clinical trial. Here, we present an approach on how to incorporate prior information, such as data from historical clinical trials, into the nuisance parameter based sample size re-estimation in a design with an internal pilot study. We focus on trials with continuous endpoints in which the outcome variance is the nuisance parameter. For planning and analyzing the trial frequentist methods are considered. Moreover, the external information on the variance are summarized by the Bayesian meta-analytic-predictive approach. To incorporate external information into the sample size re-estimation, we propose to update the MAP prior based on the results of the internal pilot study and to re-estimate the sample size using a Bayes estimator from the posterior. By means of a simulation study, we compare the operating characteristics such as power and sample size distribution of the proposed procedure with the traditional sample size re-estimation approach which uses the pooled variance estimator. The simulation study shows that, if no prior data conflict is present, incorporating external information into the sample size re-estimation improves the operating characteristics compared to the traditional approach. In the case of a prior data conflict, that is when the variance of the ongoing clinical trial is unequal to the prior location, the performance of the traditional sample size re-estimation procedure is in general superior, even when the prior information is robustified. When considering to include prior information in sample size re-estimation, the potential gains should be balanced against the risks.|prior inform often incorpor inform plan clinic trial present approach incorpor prior inform data histor clinic trial nuisanc paramet base sampl size estim design intern pilot studi focus trial continu endpoint outcom varianc nuisanc paramet plan analyz trial frequentist method consid moreov extern inform varianc summar bayesian meta analyt predict approach incorpor extern inform sampl size estim propos updat map prior base result intern pilot studi estim sampl size use bay estim posterior mean simul studi compar oper characterist power sampl size distribut propos procedur tradit sampl size estim approach use pool varianc estim simul studi show prior data conflict present incorpor extern inform sampl size estim improv oper characterist compar tradit approach case prior data conflict varianc ongo clinic trial unequ prior locat perform tradit sampl size estim procedur general superior even prior inform robustifi consid includ prior inform sampl size estim potenti gain balanc risk|['Tobias Mütze', 'Heinz Schmidli', 'Tim Friede']|['stat.AP']
2017-03-28T14:02:35Z|2017-03-20T19:45:39Z|http://arxiv.org/abs/1703.06946v1|http://arxiv.org/pdf/1703.06946v1|SCALPEL: Extracting Neurons from Calcium Imaging Data|scalpel extract neuron calcium imag data|"In the past few years, new technologies in the field of neuroscience have made it possible to simultaneously image activity in large populations of neurons at cellular resolution in behaving animals. In mid-2016, a huge repository of this so-called ""calcium imaging"" data was made publicly-available. The availability of this large-scale data resource opens the door to a host of scientific questions, for which new statistical methods must be developed.   In this paper, we consider the first step in the analysis of calcium imaging data: namely, identifying the neurons in a calcium imaging video. We propose a dictionary learning approach for this task. First, we perform image segmentation to develop a dictionary containing a huge number of candidate neurons. Next, we refine the dictionary using clustering. Finally, we apply the dictionary in order to select neurons and estimate their corresponding activity over time, using a sparse group lasso optimization problem. We apply our proposal to three calcium imaging data sets.   Our proposed approach is implemented in the R package scalpel, which is available on CRAN."|past year new technolog field neurosci made possibl simultan imag activ larg popul neuron cellular resolut behav anim mid huge repositori call calcium imag data made public avail avail larg scale data resourc open door host scientif question new statist method must develop paper consid first step analysi calcium imag data name identifi neuron calcium imag video propos dictionari learn approach task first perform imag segment develop dictionari contain huge number candid neuron next refin dictionari use cluster final appli dictionari order select neuron estim correspond activ time use spars group lasso optim problem appli propos three calcium imag data set propos approach implement packag scalpel avail cran|['Ashley Petersen', 'Noah Simon', 'Daniela Witten']|['stat.AP', 'q-bio.NC']
2017-03-28T14:02:35Z|2017-03-20T15:45:44Z|http://arxiv.org/abs/1703.06808v1|http://arxiv.org/pdf/1703.06808v1|Worth Weighting? How to Think About and Use Sample Weights in Survey   Experiments|worth weight think use sampl weight survey experi|"The popularity of online surveys has increased the prominence of sampling weights in claims of representativeness. Yet, much uncertainty remains regarding how these weights should be employed in the analysis of survey experiments: Should they be used or ignored? If they are used, which estimators are preferred? We offer practical advice, rooted in the Neyman-Rubin model, for researchers producing and working with survey experimental data. We examine simple, efficient estimators (Horvitz-Thompson, H\`ajek, ""double-H\`ajek"", and post-stratification) for analyzing these data, along with formulae for biases and variances. We provide simulations that examine these estimators and real examples from experiments administered online through YouGov. We find that for examining the existence of population treatment effects using high-quality, broadly representative samples recruited by top online survey firms, sample quantities, which do not rely on weights, are often sufficient. Sample Average Treatment Effect (SATE) estimates are unlikely to differ substantially from weighted estimates, and they avoid the statistical power loss that accompanies weighting. When precise estimates of Population Average Treatment Effects (PATE) are essential, we analytically show post-stratifying on survey weights and/or covariates highly correlated with the outcome to be a conservative choice."|popular onlin survey increas promin sampl weight claim repres yet much uncertainti remain regard weight employ analysi survey experi use ignor use estim prefer offer practic advic root neyman rubin model research produc work survey experiment data examin simpl effici estim horvitz thompson ajek doubl ajek post stratif analyz data along formula bias varianc provid simul examin estim real exampl experi administ onlin yougov find examin exist popul treatment effect use high qualiti broad repres sampl recruit top onlin survey firm sampl quantiti reli weight often suffici sampl averag treatment effect sate estim unlik differ substanti weight estim avoid statist power loss accompani weight precis estim popul averag treatment effect pate essenti analyt show post stratifi survey weight covari high correl outcom conserv choic|['Luke W. Miratrix', 'Jasjeet S. Sekhon', 'Alexander G. Theodoridis', 'Luis F. Campos']|['stat.ME', 'stat.AP']
2017-03-28T14:02:35Z|2017-03-20T15:36:13Z|http://arxiv.org/abs/1703.06804v1|http://arxiv.org/pdf/1703.06804v1|A continuous spatio-temporal approach to estimate climate change|continu spatio tempor approach estim climat chang|We introduce a method for decomposition of trend, cycle and seasonal components in spatio-temporal models and apply it to investigate the existence of climate changes in temperature and rainfall series. The method incorporates critical features in the analysis of climatic problems - the importance of spatial heterogeneity, information from a large number of weather stations, and the presence of missing data. The spatial component is based on continuous projections of spatial covariance functions, allowing modeling the complex patterns of dependence observed in climatic data.   We apply this method to study climate changes in the Northeast region of Brazil, characterized by a great wealth of climates and large amplitudes of temperatures and rainfall. The results show the presence of a tendency for temperature increases, indicating changes in the climatic patterns in this region.|introduc method decomposit trend cycl season compon spatio tempor model appli investig exist climat chang temperatur rainfal seri method incorpor critic featur analysi climat problem import spatial heterogen inform larg number weather station presenc miss data spatial compon base continu project spatial covari function allow model complex pattern depend observ climat data appli method studi climat chang northeast region brazil character great wealth climat larg amplitud temperatur rainfal result show presenc tendenc temperatur increas indic chang climat pattern region|['Marcio Poletti Laurini']|['stat.AP', '62']
2017-03-28T14:02:35Z|2017-03-20T12:52:20Z|http://arxiv.org/abs/1703.06719v1|http://arxiv.org/pdf/1703.06719v1|Analysing the sensitivity of pollen based land-cover maps to different   auxiliary variables|analys sensit pollen base land cover map differ auxiliari variabl|Realistic maps of past land cover are needed to investigate prehistoric environmental changes and anthropogenic impacts. However, observation based reconstructions of past land cover are rare. Recently Pirzamanbein et al. (2015, arXiv:1511.06417) developed a statistical method that produces spatially complete reconstructions of past land cover from pollen assemblage. These reconstructions incorporate a number of auxiliary datasets raising questions regarding both the method's sensitivity to the choice of auxiliary data and the unaffected transmission of observational data.   Here the sensitivity of the method is examined by performing spatial reconstructions for Europe for three time periods (1900 CE, 1725 CE and 4000 BCE), based on irregularly distributed pollen based land cover, available for ca $25\%$ of the area, and different auxiliary datasets. The auxiliary datasets considered include the most commonly utilized sources of the past land-cover data --- estimates produced by a dynamic vegetation (DVM) and anthropogenic land-cover change (ALCC) models --- and modern elevation. Five different auxiliary datasets were considered, including different climate data driving the DVM and different ALCC models. The resulting reconstructions were evaluated using deviance information criteria and cross validation for all the time periods. For the recent time period, 1900 CE, the different land-cover reconstructions were also compared against a present day forest map.   The tests confirm that the developed statistical model provides a robust spatial interpolation tool with low sensitivity to differences in auxiliary data and high capacity to un-distortedly transmit the information provided by sparse pollen based observations. Further, usage of auxiliary data with high spatial detail improves the model performance for the areas with complex topography or where observational data is missing.|realist map past land cover need investig prehistor environment chang anthropogen impact howev observ base reconstruct past land cover rare recent pirzamanbein et al arxiv develop statist method produc spatial complet reconstruct past land cover pollen assemblag reconstruct incorpor number auxiliari dataset rais question regard method sensit choic auxiliari data unaffect transmiss observ data sensit method examin perform spatial reconstruct europ three time period ce ce bce base irregular distribut pollen base land cover avail ca area differ auxiliari dataset auxiliari dataset consid includ common util sourc past land cover data estim produc dynam veget dvm anthropogen land cover chang alcc model modern elev five differ auxiliari dataset consid includ differ climat data drive dvm differ alcc model result reconstruct evalu use devianc inform criteria cross valid time period recent time period ce differ land cover reconstruct also compar present day forest map test confirm develop statist model provid robust spatial interpol tool low sensit differ auxiliari data high capac un distort transmit inform provid spars pollen base observ usag auxiliari data high spatial detail improv model perform area complex topographi observ data miss|['Behnaz Pirzamanbein', 'Anneli Poska', 'Johan Lindström']|['stat.AP']
2017-03-28T14:02:35Z|2017-03-21T17:19:46Z|http://arxiv.org/abs/1703.06670v2|http://arxiv.org/pdf/1703.06670v2|The Same Analysis Approach: Practical protection against the pitfalls of   novel neuroimaging analysis methods|analysi approach practic protect pitfal novel neuroimag analysi method|Standard neuroimaging data analysis based on traditional principles of experimental design, modelling, and statistical inference is increasingly complemented by novel analysis methods, driven e.g. by machine learning methods. While these novel approaches provide new insights into neuroimaging data, they often have unexpected properties, generating a growing literature on possible pitfalls. We propose to meet this challenge by adopting a habit of systematic testing of experimental design, analysis procedures, and statistical inference. Specifically, we suggest to apply the analysis method used for experimental data also to aspects of the experimental design, simulated confounds, simulated null data, and control data. We stress the importance of keeping the analysis method the same in main and test analyses, because only this way possible confounds and unexpected properties can be reliably detected and avoided. We describe and discuss this Same Analysis Approach in detail, and demonstrate it in two worked examples using multivariate decoding. With these examples, we reveal two previously unknown sources of error: A mismatch between counterbalancing and cross-validation which leads to systematic below-chance accuracies, and linear decoding of a nonlinear effect, a difference in variance.|standard neuroimag data analysi base tradit principl experiment design model statist infer increas complement novel analysi method driven machin learn method novel approach provid new insight neuroimag data often unexpect properti generat grow literatur possibl pitfal propos meet challeng adopt habit systemat test experiment design analysi procedur statist infer specif suggest appli analysi method use experiment data also aspect experiment design simul confound simul null data control data stress import keep analysi method main test analys becaus onli way possibl confound unexpect properti reliabl detect avoid describ discuss analysi approach detail demonstr two work exampl use multivari decod exampl reveal two previous unknown sourc error mismatch counterbalanc cross valid lead systemat chanc accuraci linear decod nonlinear effect differ varianc|['Kai Görgen', 'Martin N. Hebart', 'Carsten Allefeld', 'John-Dylan Haynes']|['q-bio.NC', 'stat.AP']
2017-03-28T14:02:35Z|2017-03-20T09:41:34Z|http://arxiv.org/abs/1703.06645v1|http://arxiv.org/pdf/1703.06645v1|A Preferential Attachment Paradox: How does Preferential Attachment   Combine with Growth to Produce Networks with Log-normal In-degree   Distributions?|preferenti attach paradox doe preferenti attach combin growth produc network log normal degre distribut|Every network scientist knows that preferential attachment combines with growth to produce networks with power-law in-degree distributions. So how, then, is it possible for the network of American Physical Society journal collection citations to enjoy a log-normal citation distribution when it was found to have grown in accordance with preferential attachment? This anomalous result, which we exalt as the preferential attachment paradox, has remained unexplained since the physicist Sidney Redner first made light of it over a decade ago. In this paper we propose a resolution to the paradox. The source of the mischief, we contend, lies in Redner having relied on a measurement procedure bereft of the accuracy required to distinguish preferential attachment from another form of attachment that is consistent with a log-normal in-degree distribution. There was a high-accuracy measurement procedure in general use at the time, but it could not have been used to shed light on the paradox, due to the presence of a systematic error inducing design flaw. But in recent years the design flaw had been recognised and corrected. Here we show that the bringing of the newly corrected measurement procedure to bare on the data leads to a resolution of the paradox with important ramifications for the working network scientist.|everi network scientist know preferenti attach combin growth produc network power law degre distribut possibl network american physic societi journal collect citat enjoy log normal citat distribut found grown accord preferenti attach anomal result exalt preferenti attach paradox remain unexplain sinc physicist sidney redner first made light decad ago paper propos resolut paradox sourc mischief contend lie redner reli measur procedur bereft accuraci requir distinguish preferenti attach anoth form attach consist log normal degre distribut high accuraci measur procedur general use time could use shed light paradox due presenc systemat error induc design flaw recent year design flaw recognis correct show bring newli correct measur procedur bare data lead resolut paradox import ramif work network scientist|['Paul Sheridan', 'Taku Onodera']|['cs.DL', 'cond-mat.stat-mech', 'physics.data-an', 'stat.AP']
2017-03-28T14:02:35Z|2017-03-20T05:18:45Z|http://arxiv.org/abs/1703.06603v1|http://arxiv.org/pdf/1703.06603v1|A New Class of Discrete-time Stochastic Volatility Model with Correlated   Errors|new class discret time stochast volatil model correl error|In an efficient stock market, the returns and their time-dependent volatility are often jointly modeled by stochastic volatility models (SVMs). Over the last few decades several SVMs have been proposed to adequately capture the defining features of the relationship between the return and its volatility. Among one of the earliest SVM, Taylor (1982) proposed a hierarchical model, where the current return is a function of the current latent volatility, which is further modeled as an auto-regressive process. In an attempt to make the SVMs more appropriate for complex realistic market behavior, a leverage parameter was introduced in the Taylor SVM, which however led to the violation of the efficient market hypothesis (EMH, a necessary mean-zero condition for the return distribution that prevents arbitrage possibilities). Subsequently, a host of alternative SVMs had been developed and are currently in use. In this paper, we propose mean-corrections for several generalizations of Taylor SVM that capture the complex market behavior as well as satisfy EMH. We also establish a few theoretical results to characterize the key desirable features of these models, and present comparison with other popular competitors. Furthermore, four real-life examples (Oil price, CITI bank stock price, Euro-USD rate, and S&P 500 index returns) have been used to demonstrate the performance of this new class of SVMs.|effici stock market return time depend volatil often joint model stochast volatil model svms last decad sever svms propos adequ captur defin featur relationship return volatil among one earliest svm taylor propos hierarch model current return function current latent volatil model auto regress process attempt make svms appropri complex realist market behavior leverag paramet introduc taylor svm howev led violat effici market hypothesi emh necessari mean zero condit return distribut prevent arbitrag possibl subsequ host altern svms develop current use paper propos mean correct sever general taylor svm captur complex market behavior well satisfi emh also establish theoret result character key desir featur model present comparison popular competitor furthermor four real life exampl oil price citi bank stock price euro usd rate index return use demonstr perform new class svms|['Sujay Mukhoti', 'Pritam Ranjan']|['stat.AP', 'q-fin.ST']
2017-03-28T14:02:35Z|2017-03-20T05:18:30Z|http://arxiv.org/abs/1703.06602v1|http://arxiv.org/pdf/1703.06602v1|Dual Lasso Selector|dual lasso selector|We consider the problem of model selection and estimation in sparse high dimensional linear regression models with strongly correlated variables. First, we study the theoretical properties of the dual Lasso solution, and we show that joint consideration of the Lasso primal and its dual solutions are useful for selecting correlated active variables. Second, we argue that correlations among active predictors are not problematic, and we derive a new weaker condition on the design matrix, called Pseudo Irrepresentable Condition (PIC). Third, we present a new variable selection procedure, Dual Lasso Selector, and we prove that the PIC is a necessary and sufficient condition for consistent variable selection for the proposed method. Finally, by combining the dual Lasso selector further with the Ridge estimation even better prediction performance is achieved. We call the combination (DLSelect+Ridge), it can be viewed as a new combined approach for inference in high-dimensional regression models with correlated variables. We illustrate DLSelect+Ridge method and compare it with popular existing methods in terms of variable selection, prediction accuracy, estimation accuracy and computation speed by considering various simulated and real data examples.|consid problem model select estim spars high dimension linear regress model strong correl variabl first studi theoret properti dual lasso solut show joint consider lasso primal dual solut use select correl activ variabl second argu correl among activ predictor problemat deriv new weaker condit design matrix call pseudo irrepresent condit pic third present new variabl select procedur dual lasso selector prove pic necessari suffici condit consist variabl select propos method final combin dual lasso selector ridg estim even better predict perform achiev call combin dlselect ridg view new combin approach infer high dimension regress model correl variabl illustr dlselect ridg method compar popular exist method term variabl select predict accuraci estim accuraci comput speed consid various simul real data exampl|['Niharika Gauraha']|['stat.AP']
2017-03-28T14:02:35Z|2017-03-19T01:14:47Z|http://arxiv.org/abs/1703.06378v1|http://arxiv.org/pdf/1703.06378v1|Probabilistic Models for Daily Peak Loads at Distribution Feeders|probabilist model daili peak load distribut feeder|Load forecasting at distribution networks is more challenging than load forecasting at transmission networks because its load pattern is more stochastic and unpredictable. To plan sufficient resources and estimate DER hosting capacity, it is invaluable for a distribution network planner to get the probabilistic distribution of daily peak-load under a feeder over long term. In this paper, we model the probabilistic distribution functions of daily peak-load under a feeder using power law distributions, which is tested by improved Kolmogorov Smirnov test enhanced by the Monte Carlo simulation approach. In addition, the uncertainty of the modeling is quantified using the bootstrap method. The methodology of parameter estimation of the probabilistic model and the hypothesis test is elaborated in detail. In the case studies, it is shown using measurement data sets that the daily peak loads under several feeders follow the power law distribution by applying the proposed testing methods.|load forecast distribut network challeng load forecast transmiss network becaus load pattern stochast unpredict plan suffici resourc estim der host capac invalu distribut network planner get probabilist distribut daili peak load feeder long term paper model probabilist distribut function daili peak load feeder use power law distribut test improv kolmogorov smirnov test enhanc mont carlo simul approach addit uncertainti model quantifi use bootstrap method methodolog paramet estim probabilist model hypothesi test elabor detail case studi shown use measur data set daili peak load sever feeder follow power law distribut appli propos test method|['Hossein Sangrody', 'Ning Zhou', 'Xingye Qiao']|['stat.AP']
2017-03-28T14:02:35Z|2017-03-19T00:53:48Z|http://arxiv.org/abs/1703.06375v1|http://arxiv.org/pdf/1703.06375v1|An Initial Study on Load Forecasting Considering Economic Factors|initi studi load forecast consid econom factor|This paper proposes a new objective function and quantile regression (QR) algorithm for load forecasting (LF). In LF, the positive forecasting errors often have different economic impact from the negative forecasting errors. Considering this difference, a new objective function is proposed to put different prices on the positive and negative forecasting errors. QR is used to find the optimal solution of the proposed objective function. Using normalized net energy load of New England network, the proposed method is compared with a time series method, the artificial neural network method, and the support vector machine method. The simulation results show that the proposed method is more effective in reducing the economic cost of the LF errors than the other three methods.|paper propos new object function quantil regress qr algorithm load forecast lf lf posit forecast error often differ econom impact negat forecast error consid differ new object function propos put differ price posit negat forecast error qr use find optim solut propos object function use normal net energi load new england network propos method compar time seri method artifici neural network method support vector machin method simul result show propos method effect reduc econom cost lf error three method|['Hossein Sangrody', 'Ning Zhou']|['stat.AP']
2017-03-28T14:02:39Z|2017-03-21T09:36:42Z|http://arxiv.org/abs/1703.05926v2|http://arxiv.org/pdf/1703.05926v2|Quantifying the causal effect of speed cameras on road traffic accidents   via an approximate Bayesian doubly robust estimator|quantifi causal effect speed camera road traffic accid via approxim bayesian doubli robust estim|This paper develops an approximate Bayesian doubly-robust (DR) estimation method to quantify the causal effect of speed cameras on road traffic accidents. Previous empirical work on this topic, which shows a diverse range of estimated effects, is based largely on outcome regression (OR) models using the Empirical Bayes approach or on simple before and after comparisons. Issues of causality and confounding have received little formal attention. A causal DR approach combines propensity score (PS) and OR models to give an average treatment effect (ATE) estimator that is consistent and asymptotically normal under correct specification of either of the two component models. We develop this approach within a novel approximate Bayesian framework to derive posterior predictive distributions for the ATE of speed cameras on road traffic accidents. Our results for England indicate significant reductions in the number of accidents at speed cameras sites (mean ATE = -30%). Our proposed method offers a promising approach for evaluation of transport safety interventions.|paper develop approxim bayesian doubli robust dr estim method quantifi causal effect speed camera road traffic accid previous empir work topic show divers rang estim effect base larg outcom regress model use empir bay approach simpl befor comparison issu causal confound receiv littl formal attent causal dr approach combin propens score ps model give averag treatment effect ate estim consist asymptot normal correct specif either two compon model develop approach within novel approxim bayesian framework deriv posterior predict distribut ate speed camera road traffic accid result england indic signific reduct number accid speed camera site mean ate propos method offer promis approach evalu transport safeti intervent|['Daniel J Graham', 'Cian Naik', 'Emma J McCoy']|['stat.AP']
2017-03-28T14:02:39Z|2017-03-16T19:11:58Z|http://arxiv.org/abs/1703.05799v1|http://arxiv.org/pdf/1703.05799v1|A new sample-based algorithms to compute the total sensitivity index|new sampl base algorithm comput total sensit index|Variance based sensitivity indices have established themselves as a reference among practitioners of sensitivity analysis of model output. It is not unusual to consider a variance based sensitivity analysis as informative if it produces at least the first order sensitivity indices Sj and the so-called total-effect sensitivity indices STj or Tj for all the uncertain factors of the mathematical model under analysis. Computational economy is critical in sensitivity analysis. It depends mostly upon the number of model evaluations needed to obtain stable values of the estimates. While for the first order indices efficient estimation procedures are available which are independent from the number of factors under analysis, this is less the case for the total sensitivity indices. When estimating the Tj one can either use a sample based approach, whose computational cost depends from the number of factors, or approaches based on meta-modelling / emulators, e.g. based on Gaussian processes. The present work focuses on sample-based estimation procedures for Tj, and tries different avenues to achieve an algorithmic improvement over the designs proposed in the existing best practices. One among the selected procedures appear to lead to a considerable improvement when the mean absolute error is considered.|varianc base sensit indic establish themselv refer among practition sensit analysi model output unusu consid varianc base sensit analysi inform produc least first order sensit indic sj call total effect sensit indic stj tj uncertain factor mathemat model analysi comput economi critic sensit analysi depend upon number model evalu need obtain stabl valu estim first order indic effici estim procedur avail independ number factor analysi less case total sensit indic estim tj one either use sampl base approach whose comput cost depend number factor approach base meta model emul base gaussian process present work focus sampl base estim procedur tj tri differ avenu achiev algorithm improv design propos exist best practic one among select procedur appear lead consider improv mean absolut error consid|['Andrea Saltelli', 'Daniel Albrecht', 'Stefano Tarantola', 'Federico Ferretti']|['stat.AP', '00A71', 'G.3; G.4']
2017-03-28T14:02:39Z|2017-03-16T16:00:00Z|http://arxiv.org/abs/1703.05687v1|http://arxiv.org/pdf/1703.05687v1|Gaussian process regression for forecasting battery state of health|gaussian process regress forecast batteri state health|Accurately predicting the future capacity and remaining useful life of batteries is necessary to ensure reliable system operation and to minimise maintenance costs. The complex nature of battery degradation has meant that mechanistic modelling of capacity fade has thus far remained intractable; however, with the advent of cloud-connected devices, data from cells in various applications is becoming increasingly available, and the feasibility of data-driven methods for battery prognostics is increasing. Here we propose Gaussian process (GP) regression for forecasting battery state of health, and highlight various advantages of GPs over other data-driven and mechanistic approaches. GPs are a type of Bayesian non-parametric method, and hence can model complex systems whilst handling uncertainty in a principled manner. Prior information can be exploited by GPs in a variety of ways: explicit mean functions can be used if the functional form of the underlying degradation model is available, and multiple-output GPs can effectively exploit correlations between data from different cells. We demonstrate the predictive capability of GPs for short-term and long-term (remaining useful life) forecasting on a selection of capacity vs. cycle datasets from lithium-ion cells.|accur predict futur capac remain use life batteri necessari ensur reliabl system oper minimis mainten cost complex natur batteri degrad meant mechanist model capac fade thus far remain intract howev advent cloud connect devic data cell various applic becom increas avail feasibl data driven method batteri prognost increas propos gaussian process gp regress forecast batteri state health highlight various advantag gps data driven mechanist approach gps type bayesian non parametr method henc model complex system whilst handl uncertainti principl manner prior inform exploit gps varieti way explicit mean function use function form degrad model avail multipl output gps effect exploit correl data differ cell demonstr predict capabl gps short term long term remain use life forecast select capac vs cycl dataset lithium ion cell|['Robert R. Richardson', 'Michael A. Osborne', 'David A. Howey']|['stat.AP', 'stat.ML', '62P30', 'J.2; G.3']
2017-03-28T14:02:39Z|2017-03-16T10:06:45Z|http://arxiv.org/abs/1703.05545v1|http://arxiv.org/pdf/1703.05545v1|The nature and origin of heavy tails in retweet activity|natur origin heavi tail retweet activ|Modern social media platforms facilitate the rapid spread of information online. Modelling phenomena such as social contagion and information diffusion are contingent upon a detailed understanding of the information-sharing processes. In Twitter, an important aspect of this occurs with retweets, where users rebroadcast the tweets of other users. To improve our understanding of how these distributions arise, we analyse the distribution of retweet times. We show that a power law with exponential cutoff provides a better fit than the power laws previously suggested. We explain this fit through the burstiness of human behaviour and the priorities individuals place on different tasks.|modern social media platform facilit rapid spread inform onlin model phenomena social contagion inform diffus conting upon detail understand inform share process twitter import aspect occur retweet user rebroadcast tweet user improv understand distribut aris analys distribut retweet time show power law exponenti cutoff provid better fit power law previous suggest explain fit bursti human behaviour prioriti individu place differ task|['Peter Mathews', 'Lewis Mitchell', 'Giang T. Nguyen', 'Nigel G. Bean']|['physics.soc-ph', 'cs.SI', 'stat.AP']
2017-03-28T14:02:39Z|2017-03-16T09:37:08Z|http://arxiv.org/abs/1703.05532v1|http://arxiv.org/pdf/1703.05532v1|Clustering of Gamma-Ray bursts through kernel principal component   analysis|cluster gamma ray burst kernel princip compon analysi|"We consider the problem related to clustering of gamma-ray bursts (from ""BATSE"" catalogue) through kernel principal component analysis in which our proposed kernel outperforms results of other competent kernels in terms of clustering accuracy and we obtain three physically interpretable groups of gamma-ray bursts. The effectivity of the suggested kernel in combination with kernel principal component analysis in revealing natural clusters in noisy and nonlinear data while reducing the dimension of the data is also explored in two simulated data sets."|consid problem relat cluster gamma ray burst bats catalogu kernel princip compon analysi propos kernel outperform result compet kernel term cluster accuraci obtain three physic interpret group gamma ray burst effect suggest kernel combin kernel princip compon analysi reveal natur cluster noisi nonlinear data reduc dimens data also explor two simul data set|['Soumita Modak', 'Asis Kumar Chattopadhyay', 'Tanuka Chattopadhyay']|['stat.AP', 'astro-ph.HE', 'astro-ph.IM', '62P35']
2017-03-28T14:02:39Z|2017-03-16T08:28:11Z|http://arxiv.org/abs/1703.05502v1|http://arxiv.org/pdf/1703.05502v1|Steganographic Generative Adversarial Networks|steganograph generat adversari network|"Steganography is collection of methods to hide secret information (""payload"") within non-secret information (""container""). Its counterpart, Steganalysis, is the practice of determining if a message contains a hidden payload, and recovering it if possible. Presence of hidden payloads is typically detected by a binary classifier. In the present study, we propose a new model for generating image-like containers based on Deep Convolutional Generative Adversarial Networks (DCGAN). This approach allows to generate more setganalysis-secure message embedding using standard steganography algorithms. Experiment results demonstrate that the new model successfully deceives the steganography analyzer, and for this reason, can be used in steganographic applications."|steganographi collect method hide secret inform payload within non secret inform contain counterpart steganalysi practic determin messag contain hidden payload recov possibl presenc hidden payload typic detect binari classifi present studi propos new model generat imag like contain base deep convolut generat adversari network dcgan approach allow generat setganalysi secur messag embed use standard steganographi algorithm experi result demonstr new model success deceiv steganographi analyz reason use steganograph applic|['Denis Volkhonskiy', 'Ivan Nazarov', 'Boris Borisenko', 'Evgeny Burnaev']|['cs.MM', 'cs.CR', 'cs.CV', 'stat.AP']
2017-03-28T14:02:39Z|2017-03-15T18:04:21Z|http://arxiv.org/abs/1703.05339v1|http://arxiv.org/pdf/1703.05339v1|Generalised additive mixed models for dynamic analysis in linguistics: a   practical introduction|generalis addit mix model dynam analysi linguist practic introduct|This is a hands-on introduction to Generalised Additive Mixed Models (GAMMs) in the context of linguistics with a particular focus on dynamic speech analysis (e.g. formant contours, pitch tracks, diachronic change, etc.). The main goal is to explain some of the main ideas underlying GAMMs, and to provide a practical guide to frequentist significance testing using these models. The introduction covers a range of topics including basis functions, the smoothing penalty, random smooths, difference smooths, smooth interactions, model comparison and autocorrelation. It is divided into two parts. The first part looks at what GAMMs are, how they work and why/when we should use them. Although the reader can replicate some of the example analyses in this section, this is not essential. The second part is a tutorial introduction that illustrates the process of fitting and evaluating GAMMs in the R statistical software environment, and the reader is strongly encouraged to work through the examples on their own machine.|hand introduct generalis addit mix model gamm context linguist particular focus dynam speech analysi formant contour pitch track diachron chang etc main goal explain main idea gamm provid practic guid frequentist signific test use model introduct cover rang topic includ basi function smooth penalti random smooth differ smooth smooth interact model comparison autocorrel divid two part first part look gamm work whi use although reader replic exampl analys section essenti second part tutori introduct illustr process fit evalu gamm statist softwar environ reader strong encourag work exampl machin|['Márton Sóskuthy']|['stat.AP']
2017-03-28T14:02:39Z|2017-03-15T17:03:56Z|http://arxiv.org/abs/1703.05264v1|http://arxiv.org/pdf/1703.05264v1|Smooth Image-on-Scalar Regression for Brain Mapping|smooth imag scalar regress brain map|Brain mapping is an increasingly important tool in neurology and psychiatry researches for the realization of data-driven personalized medicine in the big data era, which learns the statistical links between brain images and subject level features. Taking images as responses, the task raises a lot of challenges due to the high dimensionality of the image with relatively small number of samples, as well as the noisiness of measurements in medical images.   In this paper we propose a novel method {\it Smooth Image-on-scalar Regression} (SIR) for recovering the true association between an image outcome and scalar predictors. The estimator is achieved by minimizing a mean squared error with a total variation (TV) regularization term on the predicted mean image across all subjects. It denoises the images from all subjects and at the same time returns the coefficient maps estimation. We propose an algorithm to solve this optimization problem, which is efficient when combined with recent advances in graph fused lasso solvers. The statistical consistency of the estimator is shown via an oracle inequality.   Simulation results demonstrate that the proposed method outperforms existing methods with separate denoising and regression steps. Especially, SIR shows an evident advantage in recovering signals in small regions. We apply SIR on Alzheimer's Disease Neuroimaging Initiative data and produce interpretable brain maps of the PET image to patient-level features include age, gender, genotype and disease groups.|brain map increas import tool neurolog psychiatri research realize data driven person medicin big data era learn statist link brain imag subject level featur take imag respons task rais lot challeng due high dimension imag relat small number sampl well noisi measur medic imag paper propos novel method smooth imag scalar regress sir recov true associ imag outcom scalar predictor estim achiev minim mean squar error total variat tv regular term predict mean imag across subject denois imag subject time return coeffici map estim propos algorithm solv optim problem effici combin recent advanc graph fuse lasso solver statist consist estim shown via oracl inequ simul result demonstr propos method outperform exist method separ denois regress step especi sir show evid advantag recov signal small region appli sir alzheim diseas neuroimag initi data produc interpret brain map pet imag patient level featur includ age gender genotyp diseas group|['Ying Liu', 'Bowei Yan']|['stat.ME', 'stat.AP']
2017-03-28T14:02:39Z|2017-03-15T14:22:09Z|http://arxiv.org/abs/1703.05172v1|http://arxiv.org/pdf/1703.05172v1|Bayesian adaptive bandit-based designs using the Gittins index for   multi-armed trials with normally distributed endpoints|bayesian adapt bandit base design use gittin index multi arm trial normal distribut endpoint|Adaptive designs for multi-armed clinical trials have become increasingly popular recently in many areas of medical research because of their potential to shorten development times and to increase patient response. However, developing response-adaptive trial designs that offer patient benefit while ensuring the resulting trial avoids bias and provides a statistically rigorous comparison of the different treatments included is highly challenging. In this paper, the theory of Multi-Armed Bandit Problems is used to define a family of near optimal adaptive designs in the context of a clinical trial with a normally distributed endpoint with known variance. Through simulation studies based on an ongoing trial as a motivation we report the operating characteristics (type I error, power, bias) and patient benefit of these approaches and compare them to traditional and existing alternative designs. These results are then compared to those recently published in the context of Bernoulli endpoints. Many limitations and advantages are similar in both cases but there are also important differences, specially with respect to type I error control. This paper proposes a simulation-based testing procedure to correct for the observed type I error inflation that bandit-based and adaptive rules can induce. Results presented extend recent work by considering a normally distributed endpoint, a very common case in clinical practice yet mostly ignored in the response-adaptive theoretical literature, and illustrate the potential advantages of using these methods in a rare disease context. We also recommend a suitable modified implementation of the bandit-based adaptive designs for the case of common diseases.|adapt design multi arm clinic trial becom increas popular recent mani area medic research becaus potenti shorten develop time increas patient respons howev develop respons adapt trial design offer patient benefit ensur result trial avoid bias provid statist rigor comparison differ treatment includ high challeng paper theori multi arm bandit problem use defin famili near optim adapt design context clinic trial normal distribut endpoint known varianc simul studi base ongo trial motiv report oper characterist type error power bias patient benefit approach compar tradit exist altern design result compar recent publish context bernoulli endpoint mani limit advantag similar case also import differ special respect type error control paper propos simul base test procedur correct observ type error inflat bandit base adapt rule induc result present extend recent work consid normal distribut endpoint veri common case clinic practic yet ignor respons adapt theoret literatur illustr potenti advantag use method rare diseas context also recommend suitabl modifi implement bandit base adapt design case common diseas|['Adam Smith', 'Sofia S. Villar']|['stat.AP']
2017-03-28T14:02:39Z|2017-03-15T12:07:47Z|http://arxiv.org/abs/1703.05103v1|http://arxiv.org/pdf/1703.05103v1|Do pay-for-performance incentives lead to a better health outcome?|pay perform incent lead better health outcom|Pay-for-performance approaches have been widely adopted in order to drive improvements in the quality of healthcare provision. Previous studies evaluating the impact of these programs are either limited by the number of health outcomes or of medical conditions considered. In this paper, we evaluate the effectiveness of a pay-for-performance program on the basis of five health outcomes and across a wide range of medical conditions. The context of the study is the Lombardy region in Italy, where a rewarding program was introduced in 2012. The policy evaluation is based on a difference-in-differences approach. The model includes multiple dependent outcomes, that allow quantifying the joint effect of the program, and random effects, that account for the heterogeneity of the data at the ward and hospital level. Our results show that the policy had a positive effect on the hospitals' performance in terms of those outcomes that can be more influenced by a managerial activity, namely the number of readmissions, transfers and returns to the surgery room. No significant changes which can be related to the pay-for-performance introduction are observed for the number of voluntary discharges and for mortality. Finally, our study shows evidence that the medical wards have reacted more strongly to the pay-for-performance program than the surgical ones, whereas only limited evidence is found in support of a different policy reaction across different types of hospital ownership.|pay perform approach wide adopt order drive improv qualiti healthcar provis previous studi evalu impact program either limit number health outcom medic condit consid paper evalu effect pay perform program basi five health outcom across wide rang medic condit context studi lombardi region itali reward program introduc polici evalu base differ differ approach model includ multipl depend outcom allow quantifi joint effect program random effect account heterogen data ward hospit level result show polici posit effect hospit perform term outcom influenc manageri activ name number readmiss transfer return surgeri room signific chang relat pay perform introduct observ number voluntari discharg mortal final studi show evid medic ward react strong pay perform program surgic one wherea onli limit evid found support differ polici reaction across differ type hospit ownership|['Alina Peluso', 'Paolo Berta', 'Veronica Vinciotti']|['stat.AP']
2017-03-28T14:02:43Z|2017-03-15T06:42:41Z|http://arxiv.org/abs/1703.04961v1|http://arxiv.org/pdf/1703.04961v1|Predicting with limited data - Increasing the accuracy in VIS-NIR   diffuse reflectance spectroscopy by SMOTE|predict limit data increas accuraci vis nir diffus reflect spectroscopi smote|Diffuse reflectance spectroscopy is a powerful technique to predict soil properties. It can be used in situ to provide data inexpensively and rapidly compared to the standard laboratory measurements. Because most spectral data bases contain air-dried samples scanned in the laboratory, field spectra acquired in situ are either absent or rare in calibration data sets. However, when models are calibrated on air-dried spectra, prediction using field spectra are often inaccurate. We propose a framework to calibrate partial least squares models when field spectra are rare using synthetic minority oversampling technique (SMOTE). We calibrated a model to predict soil organic carbon content using air-dried spectra spiked with synthetic field spectra. The root mean-squared error of prediction decreased from 6.18 to 2.12 mg g$^{-1}$ and $R^2$ increased from $-$0.53 to 0.82 compared to the model calibrated on air-dried spectra only.|diffus reflect spectroscopi power techniqu predict soil properti use situ provid data inexpens rapid compar standard laboratori measur becaus spectral data base contain air dri sampl scan laboratori field spectra acquir situ either absent rare calibr data set howev model calibr air dri spectra predict use field spectra often inaccur propos framework calibr partial least squar model field spectra rare use synthet minor oversampl techniqu smote calibr model predict soil organ carbon content use air dri spectra spike synthet field spectra root mean squar error predict decreas mg increas compar model calibr air dri spectra onli|['Christina Bogner', 'Anna Kühnel', 'Bernd Huwe']|['stat.AP']
2017-03-28T14:02:43Z|2017-03-15T06:36:58Z|http://arxiv.org/abs/1703.04957v1|http://arxiv.org/pdf/1703.04957v1|An algorithm for removing sensitive information: application to   race-independent recidivism prediction|algorithm remov sensit inform applic race independ recidiv predict|"Predictive modeling is increasingly being employed to assist human decision-makers. One purported advantage of replacing or augmenting human judgment with computer models in high stakes settings-- such as sentencing, hiring, policing, college admissions, and parole decisions-- is the perceived ""neutrality"" of computers. It is argued that because computer models do not hold personal prejudice, the predictions they produce will be equally free from prejudice. There is growing recognition that employing algorithms does not remove the potential for bias, and can even amplify it if the training data were generated by a process that is itself biased. In this paper, we provide a probabilistic notion of algorithmic bias. We propose a method to eliminate bias from predictive models by removing all information regarding protected variables from the data to which the models will ultimately be trained. Unlike previous work in this area, our framework is general enough to accommodate data on any measurement scale. Motivated by models currently in use in the criminal justice system that inform decisions on pre-trial release and parole, we apply our proposed method to a dataset on the criminal histories of individuals at the time of sentencing to produce ""race-neutral"" predictions of re-arrest. In the process, we demonstrate that a common approach to creating ""race-neutral"" models-- omitting race as a covariate-- still results in racially disparate predictions. We then demonstrate that the application of our proposed method to these data removes racial disparities from predictions with minimal impact on predictive accuracy."|predict model increas employ assist human decis maker one purport advantag replac augment human judgment comput model high stake set sentenc hire polic colleg admiss parol decis perceiv neutral comput argu becaus comput model hold person prejudic predict produc equal free prejudic grow recognit employ algorithm doe remov potenti bias even amplifi train data generat process bias paper provid probabilist notion algorithm bias propos method elimin bias predict model remov inform regard protect variabl data model ultim train unlik previous work area framework general enough accommod data ani measur scale motiv model current use crimin justic system inform decis pre trial releas parol appli propos method dataset crimin histori individu time sentenc produc race neutral predict arrest process demonstr common approach creat race neutral model omit race covari still result racial dispar predict demonstr applic propos method data remov racial dispar predict minim impact predict accuraci|['James E. Johndrow', 'Kristian Lum']|['stat.AP']
2017-03-28T14:02:43Z|2017-03-14T23:04:14Z|http://arxiv.org/abs/1703.04812v1|http://arxiv.org/pdf/1703.04812v1|An alternative representation of the negative binomial-Lindley   distribution. New results and applications|altern represent negat binomi lindley distribut new result applic|In this paper we present an alternative representation of the Negative Binomial--Lindley distribution recently proposed by Zamani and Ismail (2010) which shows some advantages over the latter model. This new formulation provides a tractable model with attractive properties which makes it suitable for application not only in insurance settings but also in other fields where overdispersion is observed. Basic properties of the new distribution are studied. A recurrence for the probabilities of the new distribution and an integral equation for the probability density function of the compound version, when the claim severities are absolutely continuous, are derived. Estimation methods are discussed and a numerical application is given.|paper present altern represent negat binomi lindley distribut recent propos zamani ismail show advantag latter model new formul provid tractabl model attract properti make suitabl applic onli insur set also field overdispers observ basic properti new distribut studi recurr probabl new distribut integr equat probabl densiti function compound version claim sever absolut continu deriv estim method discuss numer applic given|['Emilio Gomez-Deniz', 'Enrique Calderin-Ojeda']|['stat.AP']
2017-03-28T14:02:43Z|2017-03-14T19:46:32Z|http://arxiv.org/abs/1703.06933v1|http://arxiv.org/pdf/1703.06933v1|Fast Radio Map Construction and Position Estimation via Direct Mapping   for WLAN Indoor Localization System|fast radio map construct posit estim via direct map wlan indoor local system|The main limitation that constrains the fast and comprehensive application of Wireless Local Area Network (WLAN) based indoor localization systems with Received Signal Strength (RSS) positioning algorithms is the building of the fingerprinting radio map, which is time-consuming especially when the indoor environment is large and/or with high frequent changes. Different approaches have been proposed to reduce workload, including fingerprinting deployment and update efforts, but the performance degrades greatly when the workload is reduced below a certain level. In this paper, we propose an indoor localization scenario that applies metric learning and manifold alignment to realize direct mapping localization (DML) using a low resolution radio map with single sample of RSS that reduces the fingerprinting workload by up to 87\%. Compared to previous work. The proposed two localization approaches, DML and $k$ nearest neighbors based on reconstructed radio map (reKNN), were shown to achieve less than 4.3\ m and 3.7\ m mean localization error respectively in a typical office environment with an area of approximately 170\ m$^2$, while the unsupervised localization with perturbation algorithm was shown to achieve 4.7\ m mean localization error with 8 times more workload than the proposed methods. As for the room level localization application, both DML and reKNN can meet the requirement with at most 9\ m of localization error which is enough to tell apart different rooms with over 99\% accuracy.|main limit constrain fast comprehens applic wireless local area network wlan base indoor local system receiv signal strength rss posit algorithm build fingerprint radio map time consum especi indoor environ larg high frequent chang differ approach propos reduc workload includ fingerprint deploy updat effort perform degrad great workload reduc certain level paper propos indoor local scenario appli metric learn manifold align realiz direct map local dml use low resolut radio map singl sampl rss reduc fingerprint workload compar previous work propos two local approach dml nearest neighbor base reconstruct radio map reknn shown achiev less mean local error respect typic offic environ area approxim unsupervis local perturb algorithm shown achiev mean local error time workload propos method room level local applic dml reknn meet requir local error enough tell apart differ room accuraci|['Caifa Zhou', 'Andreas Wieser', 'Xuezhi Tan']|['cs.NI', 'stat.AP']
2017-03-28T14:02:43Z|2017-03-14T18:16:06Z|http://arxiv.org/abs/1703.04642v1|http://arxiv.org/pdf/1703.04642v1|Robust Morphometric Analysis based on Landmarks. Applications|robust morphometr analysi base landmark applic|Procrustes Analysis is a Morphometric method based on Configurations of Landmarks that estimates the superimposition parameters by least-squares; for this reason, the procedure is very sensitive to outliers. In the first part of the paper we robustify this technique to classify individuals from a descriptive point of view. In the literature there are also classical results, based on the normality of the observations, to test whether there are significant differences between individuals. In the second part of the paper we determine a Von Mises plus Saddlepoint approximation for the tail probability of the Procrustes Statistic when the observations come from a model close to the normal. We conclude the paper with some applications using the Geographical Information System QGIS.|procrust analysi morphometr method base configur landmark estim superimposit paramet least squar reason procedur veri sensit outlier first part paper robustifi techniqu classifi individu descript point view literatur also classic result base normal observ test whether signific differ individu second part paper determin von mise plus saddlepoint approxim tail probabl procrust statist observ come model close normal conclud paper applic use geograph inform system qgis|['A. Garcia-Perez', 'M. A. Cabrero-Ortega']|['stat.AP']
2017-03-28T14:02:43Z|2017-03-13T11:31:42Z|http://arxiv.org/abs/1703.04341v1|http://arxiv.org/pdf/1703.04341v1|Response adaptive designs for binary responses: how to offer patient   benefit while being robust to time trends?|respons adapt design binari respons offer patient benefit robust time trend|"Response-adaptive randomisation (RAR) can considerably improve the chances of a successful treatment outcome for patients in a clinical trial by skewing the allocation probability towards better performing treatments as data accumulates. There is considerable interest in using RAR designs in drug development for rare diseases, where traditional designs are not feasible or ethically objectionable. In this paper we discuss and address a major criticism of RAR: the undesirable type I error inflation due to unknown time trends in the trial. Time trends can appear because of changes in the characteristics of recruited patients - so-called ""patient drift"". Patient drift is a realistic concern for clinical trials in rare diseases because these typically recruit patients over a very long period of time. We compute by simulations how large the type I error inflation is as a function of the time trend magnitude in order to determine in which contexts a potentially costly correction is actually necessary. We then assess the ability of different correction methods to preserve type I error in this context and their performance in terms of other operating characteristics, including patient benefit and power. We make recommendations of which correction methods are most suitable in the rare disease context for several RAR rules, differentiating between the two-armed and the multi-armed case. We further propose a RAR design for multi-armed clinical trials, which is computationally cheap and robust to several time trends considered."|respons adapt randomis rar consider improv chanc success treatment outcom patient clinic trial skew alloc probabl toward better perform treatment data accumul consider interest use rar design drug develop rare diseas tradit design feasibl ethic objection paper discuss address major critic rar undesir type error inflat due unknown time trend trial time trend appear becaus chang characterist recruit patient call patient drift patient drift realist concern clinic trial rare diseas becaus typic recruit patient veri long period time comput simul larg type error inflat function time trend magnitud order determin context potenti cost correct actual necessari assess abil differ correct method preserv type error context perform term oper characterist includ patient benefit power make recommend correct method suitabl rare diseas context sever rar rule differenti two arm multi arm case propos rar design multi arm clinic trial comput cheap robust sever time trend consid|['Sofia S. Villar', 'Jack Bowden', 'James Wason']|['stat.AP']
2017-03-28T14:02:43Z|2017-03-13T10:14:20Z|http://arxiv.org/abs/1703.04312v1|http://arxiv.org/pdf/1703.04312v1|Assessing Potential Wind Energy Resources in Saudi Arabia with a Skew-t   Distribution|assess potenti wind energi resourc saudi arabia skew distribut|Facing increasing domestic energy consumption from population growth and industrialization, Saudi Arabia is aiming to reduce its reliance on fossil fuels and to broaden its energy mix by expanding investment in renewable energy sources, including wind energy. A preliminary task in the development of wind energy infrastructure is the assessment of wind energy potential, a key aspect of which is the characterization of its spatio-temporal behavior. In this study we examine the impact of internal climate variability on seasonal wind power density fluctuations using 30 simulations from the Large Ensemble Project (LENS) developed at the National Center for Atmospheric Research. Furthermore, a spatio-temporal model for daily wind speed is proposed with neighbor-based cross-temporal dependence, and a multivariate skew-t distribution to capture the spatial patterns of higher order moments. The model can be used to generate synthetic time series over the entire spatial domain that adequately reproduces the internal variability of the LENS dataset.|face increas domest energi consumpt popul growth industri saudi arabia aim reduc relianc fossil fuel broaden energi mix expand invest renew energi sourc includ wind energi preliminari task develop wind energi infrastructur assess wind energi potenti key aspect character spatio tempor behavior studi examin impact intern climat variabl season wind power densiti fluctuat use simul larg ensembl project len develop nation center atmospher research furthermor spatio tempor model daili wind speed propos neighbor base cross tempor depend multivari skew distribut captur spatial pattern higher order moment model use generat synthet time seri entir spatial domain adequ reproduc intern variabl len dataset|['Felipe Tagle', 'Stefano Castruccio', 'Paola Crippa', 'Marc G. Genton']|['stat.AP']
2017-03-28T14:02:43Z|2017-03-12T08:11:29Z|http://arxiv.org/abs/1703.04081v1|http://arxiv.org/pdf/1703.04081v1|Feature overwriting as a finite mixture process: Evidence from   comprehension data|featur overwrit finit mixtur process evid comprehens data|"The ungrammatical sentence ""The key to the cabinets are on the table"" is known to lead to an illusion of grammaticality. As discussed in the meta-analysis by Jaeger et al., 2017, faster reading times are observed at the verb are in the agreement-attraction sentence above compared to the equally ungrammatical sentence ""The key to the cabinet are on the table"". One explanation for this facilitation effect is the feature percolation account: the plural feature on cabinets percolates up to the head noun key, leading to the illusion. An alternative account is in terms of cue-based retrieval (Lewis & Vasishth, 2005), which assumes that the non-subject noun cabinets is misretrieved due to a partial feature-match when a dependency completion process at the auxiliary initiates a memory access for a subject with plural marking. We present evidence for yet another explanation for the observed facilitation. Because the second sentence has two nouns with identical number, it is possible that these are, in some proportion of trials, more difficult to keep distinct, leading to slower reading times at the verb in the first sentence above; this is the feature overwriting account of Nairne, 1990. We show that the feature overwriting proposal can be implemented as a finite mixture process. We reanalysed ten published data-sets, fitting hierarchical Bayesian mixture models to these data assuming a two-mixture distribution. We show that in nine out of the ten studies, a mixture distribution corresponding to feature overwriting furnishes a superior fit over both the feature percolation and the cue-based retrieval accounts."|ungrammat sentenc key cabinet tabl known lead illus grammat discuss meta analysi jaeger et al faster read time observ verb agreement attract sentenc abov compar equal ungrammat sentenc key cabinet tabl one explan facilit effect featur percol account plural featur cabinet percol head noun key lead illus altern account term cue base retriev lewi vasishth assum non subject noun cabinet misretriev due partial featur match depend complet process auxiliari initi memori access subject plural mark present evid yet anoth explan observ facilit becaus second sentenc two noun ident number possibl proport trial difficult keep distinct lead slower read time verb first sentenc abov featur overwrit account nairn show featur overwrit propos implement finit mixtur process reanalys ten publish data set fit hierarch bayesian mixtur model data assum two mixtur distribut show nine ten studi mixtur distribut correspond featur overwrit furnish superior fit featur percol cue base retriev account|['Shravan Vasishth', 'Lena A. Jaeger', 'Bruno Nicenboim']|['stat.ML', 'cs.CL', 'stat.AP']
2017-03-28T14:02:44Z|2017-03-12T02:07:37Z|http://arxiv.org/abs/1703.04056v1|http://arxiv.org/pdf/1703.04056v1|Quantifying the strength of structural connectivity underlying   functional brain networks|quantifi strength structur connect function brain network|In recent years, there has been strong interest in neuroscience studies to investigate brain organization through networks of brain regions that demonstrate strong functional connectivity (FC). These networks are extracted from observed fMRI using data-driven analytic methods such as independent component analysis (ICA). A notable limitation of these FC methods is that they do not provide any information on the underlying structural connectivity (SC), which is believed to serve as the basis for interregional interactions in brain activity. We propose a new statistical measure of the strength of SC (sSC) underlying FC networks obtained from data-driven methods. The sSC measure is developed using information from diffusion tensor imaging (DTI) data, and can be applied to compare the strength of SC across different FC networks. Furthermore, we propose a reliability index for data-driven FC networks to measure the reproducibility of the networks through re-sampling the observed data. To perform statistical inference such as hypothesis testing on the sSC, we develop a formal variance estimator of sSC based a spatial semivariogram model with a novel distance metric. We demonstrate the performance of the sSC measure and its estimation and inference methods with simulation studies. For real data analysis, we apply our methods to a multimodal imaging study with resting-state fMRI and DTI data from 20 healthy controls and 20 subjects with major depressive disorder. Results show that well-known resting state networks all demonstrate higher SC within the network as compared to the average structural connections across the brain. We also found that sSC is positively associated with the reliability index, indicating that the FC networks that have stronger underlying SC are more reproducible across samples.|recent year strong interest neurosci studi investig brain organ network brain region demonstr strong function connect fc network extract observ fmri use data driven analyt method independ compon analysi ica notabl limit fc method provid ani inform structur connect sc believ serv basi interregion interact brain activ propos new statist measur strength sc ssc fc network obtain data driven method ssc measur develop use inform diffus tensor imag dti data appli compar strength sc across differ fc network furthermor propos reliabl index data driven fc network measur reproduc network sampl observ data perform statist infer hypothesi test ssc develop formal varianc estim ssc base spatial semivariogram model novel distanc metric demonstr perform ssc measur estim infer method simul studi real data analysi appli method multimod imag studi rest state fmri dti data healthi control subject major depress disord result show well known rest state network demonstr higher sc within network compar averag structur connect across brain also found ssc posit associ reliabl index indic fc network stronger sc reproduc across sampl|['Phebe Brenne Kemmer', 'F. DuBois Bowman', 'Helen Mayberg', 'Ying Guo']|['stat.AP', 'q-bio.NC']
2017-03-28T14:02:44Z|2017-03-10T22:46:09Z|http://arxiv.org/abs/1703.03862v1|http://arxiv.org/pdf/1703.03862v1|Joint Embedding of Graphs|joint embed graph|Feature extraction and dimension reduction for networks is critical in a wide variety of domains. Efficiently and accurately learning features for multiple graphs has important applications in statistical inference on graphs. We propose a method to jointly embed multiple undirected graphs. Given a set of graphs, the joint embedding method identifies a linear subspace spanned by rank one symmetric matrices and projects adjacency matrices of graphs into this subspace. The projection coefficients can be treated as features of the graphs. We also propose a random graph model which generalizes classical random graph model and can be used to model multiple graphs. We show through theory and numerical experiments that under the model, the joint embedding method produces estimates of parameters with small errors. Via simulation experiments, we demonstrate that the joint embedding method produces features which lead to state of the art performance in classifying graphs. Applying the joint embedding method to human brain graphs, we find it extract interpretable features that can be used to predict individual composite creativity index.|featur extract dimens reduct network critic wide varieti domain effici accur learn featur multipl graph import applic statist infer graph propos method joint emb multipl undirect graph given set graph joint embed method identifi linear subspac span rank one symmetr matric project adjac matric graph subspac project coeffici treat featur graph also propos random graph model general classic random graph model use model multipl graph show theori numer experi model joint embed method produc estim paramet small error via simul experi demonstr joint embed method produc featur lead state art perform classifi graph appli joint embed method human brain graph find extract interpret featur use predict individu composit creativ index|['Shangsi Wang', 'Joshua T. Vogelstein', 'Carey E. Priebe']|['stat.AP', 'cs.LG', 'stat.ML']
2017-03-28T14:02:47Z|2017-03-10T22:03:17Z|http://arxiv.org/abs/1703.03853v1|http://arxiv.org/pdf/1703.03853v1|PairCloneTree: Reconstruction of Tumor Subclone Phylogeny Based on   Mutation Pairs using Next Generation Sequencing Data|pairclonetre reconstruct tumor subclon phylogeni base mutat pair use next generat sequenc data|We present a latent feature allocation model to reconstruct tumor subclones subject to phylogenetic evolution that mimics tumor evolution. Similar to most current methods, we consider data from next-generation sequencing. Unlike most methods that use information in short reads mapped to single nucleotide variants (SNVs), we consider subclone reconstruction using pairs of two proximal SNVs that can be mapped by the same short reads. As part of the Bayesian inference model, we construct a phylogenetic tree prior. The use of the tree structure in the prior greatly strengthens inference. Only subclones that can be approximated by a phylogenetic tree are assigned non-negligible probability. The proposed Bayesian framework implies posterior distributions on the number of subclones, their genotypes, cellular proportions, and the phylogenetic tree spanned by the inferred subclones. The proposed method is validated against different sets of simulated and real-world data using single and multiple tumor samples. An open source software package is available at http://www.compgenome.org/pairclonetree|present latent featur alloc model reconstruct tumor subclon subject phylogenet evolut mimic tumor evolut similar current method consid data next generat sequenc unlik method use inform short read map singl nucleotid variant snvs consid subclon reconstruct use pair two proxim snvs map short read part bayesian infer model construct phylogenet tree prior use tree structur prior great strengthen infer onli subclon approxim phylogenet tree assign non neglig probabl propos bayesian framework impli posterior distribut number subclon genotyp cellular proport phylogenet tree span infer subclon propos method valid differ set simul real world data use singl multipl tumor sampl open sourc softwar packag avail http www compgenom org pairclonetre|['Tianjian Zhou', 'Subhajit Sengupta', 'Peter Mueller', 'Yuan Ji']|['stat.AP']
2017-03-28T14:02:47Z|2017-03-10T18:41:00Z|http://arxiv.org/abs/1703.03790v1|http://arxiv.org/pdf/1703.03790v1|Summertime, and the livin is easy: Winter and summer pseudoseasonal life   expectancy in the United States|summertim livin easi winter summer pseudoseason life expect unit state|"In temperate climates, mortality is seasonal with a winter-dominant pattern, due in part to pneumonia and influenza. Cardiac causes, which are the leading cause of death in the United States, are also winter-seasonal although it is not clear why. Interactions between circulating respiratory viruses (f.e., influenza) and cardiac conditions have been suggested as a cause of winter-dominant mortality patterns. We propose and implement a way to estimate an upper bound on mortality attributable to winter-dominant viruses like influenza. We calculate 'pseudo-seasonal' life expectancy, dividing the year into two six-month spans, one encompassing winter the other summer. During the summer when the circulation of respiratory viruses is drastically reduced, life expectancy is about one year longer. We also quantify the seasonal mortality difference in terms of seasonal ""equivalent ages"" (defined herein) and proportional hazards. We suggest that even if viruses cause excess winter cardiac mortality, the population-level mortality reduction of a perfect influenza vaccine would be much more modest than is often recognized."|temper climat mortal season winter domin pattern due part pneumonia influenza cardiac caus lead caus death unit state also winter season although clear whi interact circul respiratori virus influenza cardiac condit suggest caus winter domin mortal pattern propos implement way estim upper bound mortal attribut winter domin virus like influenza calcul pseudo season life expect divid year two six month span one encompass winter summer dure summer circul respiratori virus drastic reduc life expect one year longer also quantifi season mortal differ term season equival age defin herein proport hazard suggest even virus caus excess winter cardiac mortal popul level mortal reduct perfect influenza vaccin would much modest often recogn|['Tina Ho', 'Andrew Noymer']|['stat.AP']
2017-03-28T14:02:47Z|2017-03-10T16:35:40Z|http://arxiv.org/abs/1703.03753v1|http://arxiv.org/pdf/1703.03753v1|Latent Gaussian Mixture Models for Nationwide Kidney Transplant Center   Evaluation|latent gaussian mixtur model nationwid kidney transplant center evalu|Five year post-transplant survival rate is an important indicator on quality of care delivered by kidney transplant centers in the United States. To provide a fair assessment of each transplant center, an effect that represents the center-specific care quality, along with patient level risk factors, is often included in the risk adjustment model. In the past, the center effects have been modeled as either fixed effects or Gaussian random effects, with various pros and cons. Our numerical analyses reveal that the distributional assumptions do impact the prediction of center effects especially when the effect is extreme. To bridge the gap between these two approaches, we propose to model the transplant center effect as a latent random variable with a finite Gaussian mixture distribution. Such latent Gaussian mixture models provide a convenient framework to study the heterogeneity among the transplant centers. To overcome the weak identifiability issues, we propose to estimate the latent Gaussian mixture model using a penalized likelihood approach, and develop sequential locally restricted likelihood ratio tests to determine the number of components in the Gaussian mixture distribution. The fitted mixture model provides a convenient means of controlling the false discovery rate when screening for underperforming or outperforming transplant centers. The performance of the methods is verified by simulations and by the analysis of the motivating data example.|five year post transplant surviv rate import indic qualiti care deliv kidney transplant center unit state provid fair assess transplant center effect repres center specif care qualiti along patient level risk factor often includ risk adjust model past center effect model either fix effect gaussian random effect various pros con numer analys reveal distribut assumpt impact predict center effect especi effect extrem bridg gap two approach propos model transplant center effect latent random variabl finit gaussian mixtur distribut latent gaussian mixtur model provid conveni framework studi heterogen among transplant center overcom weak identifi issu propos estim latent gaussian mixtur model use penal likelihood approach develop sequenti local restrict likelihood ratio test determin number compon gaussian mixtur distribut fit mixtur model provid conveni mean control fals discoveri rate screen underperform outperform transplant center perform method verifi simul analysi motiv data exampl|['Lanfeng Pan', 'Yehua Li', 'Kevin He', 'Yanming Li', 'Yi Li']|['stat.AP']
2017-03-28T14:02:47Z|2017-03-09T16:56:27Z|http://arxiv.org/abs/1703.03340v1|http://arxiv.org/pdf/1703.03340v1|Adaptive Non-uniform Compressive Sampling for Time-varying Signals|adapt non uniform compress sampl time vari signal|In this paper, adaptive non-uniform compressive sampling (ANCS) of time-varying signals, which are sparse in a proper basis, is introduced. ANCS employs the measurements of previous time steps to distribute the sensing energy among coefficients more intelligently. To this aim, a Bayesian inference method is proposed that does not require any prior knowledge of importance levels of coefficients or sparsity of the signal. Our numerical simulations show that ANCS is able to achieve the desired non-uniform recovery of the signal. Moreover, if the signal is sparse in canonical basis, ANCS can reduce the number of required measurements significantly.|paper adapt non uniform compress sampl anc time vari signal spars proper basi introduc anc employ measur previous time step distribut sens energi among coeffici intellig aim bayesian infer method propos doe requir ani prior knowledg import level coeffici sparsiti signal numer simul show anc abl achiev desir non uniform recoveri signal moreov signal spars canon basi anc reduc number requir measur signific|['Alireza Zaeemzadeh', 'Mohsen Joneidi', 'Nazanin Rahnavard']|['stat.AP', 'cs.IT', 'math.IT']
2017-03-28T14:02:47Z|2017-03-09T10:19:53Z|http://arxiv.org/abs/1703.03213v1|http://arxiv.org/pdf/1703.03213v1|Kernel intensity estimation, bootstrapping and bandwidth selection for   inhomogeneous point processes depending on spatial covariates|kernel intens estim bootstrap bandwidth select inhomogen point process depend spatial covari|In the point process context, kernel intensity estimation has been mainly restricted to exploratory analysis due to its lack of consistency. However the use of covariates has allow to design consistent alternatives under some restrictive assumptions. In this paper we focus our attention on de\-fi\-ning an appropriate framework to derive a consistent kernel intensity estimator using covariates, as well as a consistent smooth bootstrap procedure. For spatial point processes with covariates there is no specific bandwidth selector, hence, we define two new data-driven procedures specifically designed for this scenario: a rule-of-thumb and a plug-in bandwidth based on the bootstrap method previously introduced. A simulation study is accomplished to understand the behaviour of these procedures in finite samples. Finally, we apply the techniques to a real set of data made up of wildfires in Canada during June 2015, using meteorological information as covariates.|point process context kernel intens estim main restrict exploratori analysi due lack consist howev use covari allow design consist altern restrict assumpt paper focus attent de fi ning appropri framework deriv consist kernel intens estim use covari well consist smooth bootstrap procedur spatial point process covari specif bandwidth selector henc defin two new data driven procedur specif design scenario rule thumb plug bandwidth base bootstrap method previous introduc simul studi accomplish understand behaviour procedur finit sampl final appli techniqu real set data made wildfir canada dure june use meteorolog inform covari|['M. I. Borrajo', 'W. González-Manteiga', 'M. D. Martínez-Miranda']|['stat.ME', 'stat.AP', '62G05, 62G09, 62H11, 60G55, 60-08']
2017-03-28T14:02:47Z|2017-03-08T15:23:00Z|http://arxiv.org/abs/1703.02870v1|http://arxiv.org/abs/1703.02870v1|Statistical Inference in Political Networks Research|statist infer polit network research|Researchers interested in statistically modeling network data have a well-established and quickly growing set of approaches from which to choose. Several of these methods have been regularly applied in research on political networks, while others have yet to permeate the field. Here, we review the most prominent methods of inferential network analysis---both for cross-sectionally and longitudinally observed networks including (temporal) exponential random graph models, latent space models, the quadratic assignment procedure, and stochastic actor oriented models. For each method, we summarize its analytic form, identify prominent published applications in political science and discuss computational considerations. We conclude with a set of guidelines for selecting a method for a given application.|research interest statist model network data well establish quick grow set approach choos sever method regular appli research polit network yet permeat field review promin method inferenti network analysi cross section longitudin observ network includ tempor exponenti random graph model latent space model quadrat assign procedur stochast actor orient model method summar analyt form identifi promin publish applic polit scienc discuss comput consider conclud set guidelin select method given applic|['Bruce A. Desmarais', 'Skyler J. Cranmer']|['stat.AP', 'cs.SI', 'physics.soc-ph']
2017-03-28T14:02:47Z|2017-03-08T00:47:45Z|http://arxiv.org/abs/1703.02650v1|http://arxiv.org/pdf/1703.02650v1|Joint Multichannel Deconvolution and Blind Source Separation|joint multichannel deconvolut blind sourc separ|Blind Source Separation (BSS) is a challenging matrix factorization problem that plays a central role in multichannel imaging science. In a large number of applications, such as astrophysics, current unmixing methods are limited since real-world mixtures are generally affected by extra instrumental effects like blurring. Therefore, BSS has to be solved jointly with a deconvolution problem, which requires tackling a new inverse problem: deconvolution BSS (DBSS). In this article, we introduce an innovative DBSS approach, called DecGMCA, based on sparse signal modeling and an efficient alternative projected least square algorithm. Numerical results demonstrate that the DecGMCA algorithm performs very well on simulations. It further highlights the importance of jointly solving BSS and deconvolution instead of considering these two problems independently. Furthermore, the performance of the proposed DecGMCA algorithm is demonstrated on simulated radio-interferometric data.|blind sourc separ bss challeng matrix factor problem play central role multichannel imag scienc larg number applic astrophys current unmix method limit sinc real world mixtur general affect extra instrument effect like blur therefor bss solv joint deconvolut problem requir tackl new invers problem deconvolut bss dbss articl introduc innov dbss approach call decgmca base spars signal model effici altern project least squar algorithm numer result demonstr decgmca algorithm perform veri well simul highlight import joint solv bss deconvolut instead consid two problem independ furthermor perform propos decgmca algorithm demonstr simul radio interferometr data|['Ming Jiang', 'Jérôme Bobin', 'Jean-Luc Starck']|['stat.AP', 'cs.IT', 'math.IT']
2017-03-28T14:02:47Z|2017-03-07T18:14:54Z|http://arxiv.org/abs/1703.02502v1|http://arxiv.org/pdf/1703.02502v1|Clustering Methods for Electricity Consumers: An Empirical Study in   Hvaler-Norway|cluster method electr consum empir studi hvaler norway|The development of Smart Grid in Norway in specific and Europe/US in general will shortly lead to the availability of massive amount of fine-grained spatio-temporal consumption data from domestic households. This enables the application of data mining techniques for traditional problems in power system. Clustering customers into appropriate groups is extremely useful for operators or retailers to address each group differently through dedicated tariffs or customer-tailored services. Currently, the task is done based on demographic data collected through questionnaire, which is error-prone. In this paper, we used three different clustering techniques (together with their variants) to automatically segment electricity consumers based on their consumption patterns. We also proposed a good way to extract consumption patterns for each consumer. The grouping results were assessed using four common internal validity indexes. We found that the combination of Self Organizing Map (SOM) and k-means algorithms produce the most insightful and useful grouping. We also discovered that grouping quality cannot be measured effectively by automatic indicators, which goes against common suggestions in literature.|develop smart grid norway specif europ us general short lead avail massiv amount fine grain spatio tempor consumpt data domest household enabl applic data mine techniqu tradit problem power system cluster custom appropri group extrem use oper retail address group differ dedic tariff custom tailor servic current task done base demograph data collect questionnair error prone paper use three differ cluster techniqu togeth variant automat segment electr consum base consumpt pattern also propos good way extract consumpt pattern consum group result assess use four common intern valid index found combin self organ map som mean algorithm produc insight use group also discov group qualiti cannot measur effect automat indic goe common suggest literatur|['The-Hien Dang-Ha', 'Roland Olsson', 'Hao Wang']|['stat.AP']
2017-03-28T14:02:47Z|2017-03-07T15:43:22Z|http://arxiv.org/abs/1703.02441v1|http://arxiv.org/pdf/1703.02441v1|Statistical Analysis of the Ricker Model|statist analysi ricker model|The Ricker model was introduced in the context of managing fishing stocks. It is a discrete non-linear iterative model given by $N(t+1)=rN(t)\exp(-N(t))$ where $N(t)$ is the population at time $t$. The model treated in this paper includes a random component $N(t+1)=rN(t)\exp(-N(t)+\varepsilon(t+1))$ and what is observed at time $t$ is a Poisson random variable with parameter $\varphi N(t)$. Such a model has been analysed using `synthetic likelihood' and ABC (Approximate Bayesian Computation). In contrast this paper takes a non-likelihood approach and treats the model in a consistent manner as an approximation. The goal is to specify those parameter values if any which are consistent with the data.|ricker model introduc context manag fish stock discret non linear iter model given rn exp popul time model treat paper includ random compon rn exp varepsilon observ time poisson random variabl paramet varphi model analys use synthet likelihood abc approxim bayesian comput contrast paper take non likelihood approach treat model consist manner approxim goal specifi paramet valu ani consist data|['Laurie Davies']|['stat.AP', '62M99']
2017-03-28T14:02:47Z|2017-03-07T11:15:14Z|http://arxiv.org/abs/1703.02329v1|http://arxiv.org/pdf/1703.02329v1|Time and media-use of Italian Generation Y: dimensions of leisure   preferences|time media use italian generat dimens leisur prefer|"Time spent in leisure is not a minor research question as it is acknowledged as a key aspect of one's quality of life. The primary aim of this article is to qualify time and Internet use of Italian Generation Y beyond media hype and assumptions. To this aim, we apply a multidimensional extension of Item Response Theory models to the Italian ""Multipurpose survey on households: aspects of daily life"" to ascertain the relevant dimensions of Generation Y time-use. We show that the use of technology is neither the first nor the foremost time-use activity of Italian Generation Y, who still prefers to use its time to socialise and have fun with friends in a non media-medalled manner."|time spent leisur minor research question acknowledg key aspect one qualiti life primari aim articl qualifi time internet use italian generat beyond media hype assumpt aim appli multidimension extens item respons theori model italian multipurpos survey household aspect daili life ascertain relev dimens generat time use show use technolog neither first foremost time use activ italian generat still prefer use time socialis fun friend non media medal manner|['Michela Gnaldi', 'Simone Del Sarto']|['stat.AP', 'cs.CY']
2017-03-28T14:02:51Z|2017-03-14T18:52:48Z|http://arxiv.org/abs/1703.02236v2|http://arxiv.org/pdf/1703.02236v2|Propensity score prediction for electronic healthcare databases using   Super Learner and High-dimensional Propensity Score Methods|propens score predict electron healthcar databas use super learner high dimension propens score method|"The optimal learner for prediction modeling varies depending on the underlying data-generating distribution. Super Learner (SL) is a generic ensemble learning algorithm that uses cross-validation to select among a ""library"" of candidate prediction models. The SL is not restricted to a single prediction model, but uses the strengths of a variety of learning algorithms to adapt to different databases. While the SL has been shown to perform well in a number of settings, it has not been thoroughly evaluated in large electronic healthcare databases that are common in pharmacoepidemiology and comparative effectiveness research. In this study, we applied and evaluated the performance of the SL in its ability to predict treatment assignment using three electronic healthcare databases. We considered a library of algorithms that consisted of both nonparametric and parametric models. We also considered a novel strategy for prediction modeling that combines the SL with the high-dimensional propensity score (hdPS) variable selection algorithm. Predictive performance was assessed using three metrics: the negative log-likelihood, area under the curve (AUC), and time complexity. Results showed that the best individual algorithm, in terms of predictive performance, varied across datasets. The SL was able to adapt to the given dataset and optimize predictive performance relative to any individual learner. Combining the SL with the hdPS was the most consistent prediction method and may be promising for PS estimation and prediction modeling in electronic healthcare databases."|optim learner predict model vari depend data generat distribut super learner sl generic ensembl learn algorithm use cross valid select among librari candid predict model sl restrict singl predict model use strength varieti learn algorithm adapt differ databas sl shown perform well number set thorough evalu larg electron healthcar databas common pharmacoepidemiolog compar effect research studi appli evalu perform sl abil predict treatment assign use three electron healthcar databas consid librari algorithm consist nonparametr parametr model also consid novel strategi predict model combin sl high dimension propens score hdps variabl select algorithm predict perform assess use three metric negat log likelihood area curv auc time complex result show best individu algorithm term predict perform vari across dataset sl abl adapt given dataset optim predict perform relat ani individu learner combin sl hdps consist predict method may promis ps estim predict model electron healthcar databas|['Cheng Ju', 'Mary Combs', 'Samuel D Lendle', 'Jessica M Franklin', 'Richard Wyss', 'Sebastian Schneeweiss', 'Mark J. van der Laan']|['stat.AP', 'stat.ML']
2017-03-28T14:02:51Z|2017-03-06T21:17:42Z|http://arxiv.org/abs/1703.02112v1|http://arxiv.org/pdf/1703.02112v1|Process convolution approaches for modeling interacting trajectories|process convolut approach model interact trajectori|"Gaussian processes are a fundamental statistical tool used in a wide range of applications. In the spatio-temporal setting, several families of covariance functions exist to accommodate a wide variety of dependence structures arising in different applications. These parametric families can be restrictive and are insufficient in some situations. In contrast, process convolutions represent a flexible, interpretable approach to defining the covariance of a Gaussian process and have modest requirements to ensure validity. We introduce a generalization of the process convolution approach that employs multiple convolutions sequentially to form a ""process convolution chain."" In our proposed multi-stage framework, complex dependencies that arise from a combination of different interacting mechanisms are decomposed into a series of interpretable kernel smoothers. We demonstrate an application of process convolution chains to model killer whale movement, in which the paths taken by multiple individuals are not independent, but reflect dynamic social interactions within the population. Our proposed model for dependent movement provides inference for the latent dynamic social structure in the study population. Additionally, by leveraging the positive dependence among individual paths, we achieve a reduction in uncertainty for the estimated locations of the whales, compared to a model that treats paths as independent."|gaussian process fundament statist tool use wide rang applic spatio tempor set sever famili covari function exist accommod wide varieti depend structur aris differ applic parametr famili restrict insuffici situat contrast process convolut repres flexibl interpret approach defin covari gaussian process modest requir ensur valid introduc general process convolut approach employ multipl convolut sequenti form process convolut chain propos multi stage framework complex depend aris combin differ interact mechan decompos seri interpret kernel smoother demonstr applic process convolut chain model killer whale movement path taken multipl individu independ reflect dynam social interact within popul propos model depend movement provid infer latent dynam social structur studi popul addit leverag posit depend among individu path achiev reduct uncertainti estim locat whale compar model treat path independ|['Henry R. Scharf', 'Mevin B. Hooten', 'Devin S. Johnson', 'John W. Durban']|['stat.ME', 'stat.AP']
2017-03-28T14:02:51Z|2017-03-06T19:33:24Z|http://arxiv.org/abs/1703.02078v1|http://arxiv.org/pdf/1703.02078v1|Cross-screening in observational studies that test many hypotheses|cross screen observ studi test mani hypothes|"We discuss observational studies that test many causal hypotheses, either hypotheses about many outcomes or many treatments. To be credible an observational study that tests many causal hypotheses must demonstrate that its conclusions are neither artifacts of multiple testing nor of small biases from nonrandom treatment assignment. In a sense that needs to be defined carefully, hidden within a sensitivity analysis for nonrandom assignment is an enormous correction for multiple testing: in the absence of bias, it is extremely improbable that multiple testing alone would create an association insensitive to moderate biases. We propose a new strategy called ""cross-screening"", different from but motivated by recent work of Bogomolov and Heller on replicability. Cross-screening splits the data in half at random, uses the first half to plan a study carried out on the second half, then uses the second half to plan a study carried out on the first half, and reports the more favorable conclusions of the two studies correcting using the Bonferroni inequality for having done two studies. If the two studies happen to concur, then they achieve Bogomolov-Heller replicability; however, importantly, replicability is not required for strong control of the family-wise error rate, and either study alone suffices for firm conclusions. In randomized studies with a few hypotheses, cross-split screening is not an attractive method when compared with conventional methods of multiplicity control, but it can become attractive when hundreds or thousands of hypotheses are subjected to sensitivity analyses in an observational study. We illustrate the technique by comparing 46 biomarkers in individuals who consume large quantities of fish versus little or no fish."|discuss observ studi test mani causal hypothes either hypothes mani outcom mani treatment credibl observ studi test mani causal hypothes must demonstr conclus neither artifact multipl test small bias nonrandom treatment assign sens need defin care hidden within sensit analysi nonrandom assign enorm correct multipl test absenc bias extrem improb multipl test alon would creat associ insensit moder bias propos new strategi call cross screen differ motiv recent work bogomolov heller replic cross screen split data half random use first half plan studi carri second half use second half plan studi carri first half report favor conclus two studi correct use bonferroni inequ done two studi two studi happen concur achiev bogomolov heller replic howev import replic requir strong control famili wise error rate either studi alon suffic firm conclus random studi hypothes cross split screen attract method compar convent method multipl control becom attract hundr thousand hypothes subject sensit analys observ studi illustr techniqu compar biomark individu consum larg quantiti fish versus littl fish|['Qingyuan Zhao', 'Dylan S. Small', 'Paul R. Rosenbaum']|['stat.ME', 'stat.AP']
2017-03-28T14:02:51Z|2017-03-06T15:58:26Z|http://arxiv.org/abs/1703.01937v1|http://arxiv.org/pdf/1703.01937v1|Reputation Dynamics in a Market for Illicit Drugs|reput dynam market illicit drug|We analyze reputation dynamics in an online market for illicit drugs using a novel dataset of prices and ratings. The market is a black market, and so contracts cannot be enforced. We study the role that reputation plays in alleviating adverse selection in this market. We document the following stylized facts: (i) There is a positive relationship between the price and the rating of a seller. This effect is increasing in the number of reviews left for a seller. A mature highly-rated seller charges a 20% higher price than a mature low-rated seller. (ii) Sellers with more reviews charge higher prices regardless of rating. (iii) Low-rated sellers are more likely to exit the market and make fewer sales. We show that these stylized facts are explained by a dynamic model of adverse selection, ratings, and exit, in which buyers form rational inferences about the quality of a seller jointly from his rating and number of sales. Sellers who receive low ratings initially charge the same price as highly-rated sellers since early reviews are less informative about quality. Bad sellers exit rather than face lower prices in the future. We provide conditions under which our model admits a unique equilibrium. We estimate the model, and use the result to compute the returns to reputation in the market. We find that the market would have collapsed due to adverse selection in the absence of a rating system.|analyz reput dynam onlin market illicit drug use novel dataset price rate market black market contract cannot enforc studi role reput play allevi advers select market document follow styliz fact posit relationship price rate seller effect increas number review left seller matur high rate seller charg higher price matur low rate seller ii seller review charg higher price regardless rate iii low rate seller like exit market make fewer sale show styliz fact explain dynam model advers select rate exit buyer form ration infer qualiti seller joint rate number sale seller receiv low rate initi charg price high rate seller sinc earli review less inform qualiti bad seller exit rather face lower price futur provid condit model admit uniqu equilibrium estim model use result comput return reput market find market would collaps due advers select absenc rate system|['Nick Janetos', 'Jan Tilly']|['stat.AP']
2017-03-28T14:02:51Z|2017-03-06T09:24:07Z|http://arxiv.org/abs/1703.01776v1|http://arxiv.org/pdf/1703.01776v1|Online Sequential Monte Carlo smoother for partially observed stochastic   differential equations|onlin sequenti mont carlo smoother partial observ stochast differenti equat|This paper introduces a new algorithm to approximate smoothed additive functionals for partially observed stochastic differential equations. This method relies on a recent procedure which allows to compute such approximations online, i.e. as the observations are received, and with a computational complexity growing linearly with the number of Monte Carlo samples. This online smoother cannot be used directly in the case of partially observed stochastic differential equations since the transition density of the latent data is usually unknown. We prove that a similar algorithm may still be defined for partially observed continuous processes by replacing this unknown quantity by an unbiased estimator obtained for instance using general Poisson estimators. We prove that this estimator is consistent and its performance are illustrated using data from two models.|paper introduc new algorithm approxim smooth addit function partial observ stochast differenti equat method reli recent procedur allow comput approxim onlin observ receiv comput complex grow linear number mont carlo sampl onlin smoother cannot use direct case partial observ stochast differenti equat sinc transit densiti latent data usual unknown prove similar algorithm may still defin partial observ continu process replac unknown quantiti unbias estim obtain instanc use general poisson estim prove estim consist perform illustr use data two model|['Pierre Gloaguen', 'Marie-Pierre Etienne', 'Sylvain Le Corff']|['stat.ME', 'stat.AP']
2017-03-28T14:02:51Z|2017-03-04T21:50:25Z|http://arxiv.org/abs/1703.01526v1|http://arxiv.org/abs/1703.01526v1|High Accuracy Classification of Parkinson's Disease through Shape   Analysis and Surface Fitting in $^{123}$I-Ioflupane SPECT Imaging|high accuraci classif parkinson diseas shape analysi surfac fit ioflupan spect imag|Early and accurate identification of parkinsonian syndromes (PS) involving presynaptic degeneration from non-degenerative variants such as Scans Without Evidence of Dopaminergic Deficit (SWEDD) and tremor disorders, is important for effective patient management as the course, therapy and prognosis differ substantially between the two groups. In this study, we use Single Photon Emission Computed Tomography (SPECT) images from healthy normal, early PD and SWEDD subjects, as obtained from the Parkinson's Progression Markers Initiative (PPMI) database, and process them to compute shape- and surface fitting-based features for the three groups. We use these features to develop and compare various classification models that can discriminate between scans showing dopaminergic deficit, as in PD, from scans without the deficit, as in healthy normal or SWEDD. Along with it, we also compare these features with Striatal Binding Ratio (SBR)-based features, which are well-established and clinically used, by computing a feature importance score using Random forests technique. We observe that the Support Vector Machine (SVM) classifier gave the best performance with an accuracy of 97.29%. These features also showed higher importance than the SBR-based features. We infer from the study that shape analysis and surface fitting are useful and promising methods for extracting discriminatory features that can be used to develop diagnostic models that might have the potential to help clinicians in the diagnostic process.|earli accur identif parkinsonian syndrom ps involv presynapt degener non degen variant scan without evid dopaminerg deficit swedd tremor disord import effect patient manag cours therapi prognosi differ substanti two group studi use singl photon emiss comput tomographi spect imag healthi normal earli pd swedd subject obtain parkinson progress marker initi ppmi databas process comput shape surfac fit base featur three group use featur develop compar various classif model discrimin scan show dopaminerg deficit pd scan without deficit healthi normal swedd along also compar featur striatal bind ratio sbr base featur well establish clinic use comput featur import score use random forest techniqu observ support vector machin svm classifi gave best perform accuraci featur also show higher import sbr base featur infer studi shape analysi surfac fit use promis method extract discriminatori featur use develop diagnost model might potenti help clinician diagnost process|['R. Prashanth', 'Sumantra Dutta Roy', 'Pravat K. Mandal', 'Shantanu Ghosh']|['stat.AP', 'cs.CV', 'physics.data-an', 'stat.CO', 'stat.ML']
2017-03-28T14:02:51Z|2017-03-04T19:07:42Z|http://arxiv.org/abs/1703.01506v1|http://arxiv.org/pdf/1703.01506v1|Accelerating Permutation Testing in Voxel-wise Analysis through Subspace   Tracking: A new plugin for SnPM|acceler permut test voxel wise analysi subspac track new plugin snpm|Permutation testing is a non-parametric method for obtaining the max null distribution used to compute corrected $p$-values to provide strong control of false positives. In neuroimaging, however, the computational burden of running such algorithm can be significant. We find that by viewing the permutation testing procedure as the construction of a very large permutation testing matrix $T$, one can exploit structural properties derived from the data and the test statistics to reduce the runtime under certain conditions. In particular, we see that $T$ has a low-rank plus a low-variance residual. This makes $T$ a good candidate for low-rank matrix completion methods, where only a very small number of entries of $T$ ($~0.35\%$ of all entries in our experiments) have to be computed to obtain good estimate of it. Based on this observation, we developed an algorithm, RapidPT, that is able to efficiently recover the max null distribution commonly obtained through regular permutation testing in neuroimage analysis. We present an extensive experimental validation on four varying sized datasets against two baselines: Statistical NonParametric Mapping (SnPM13) and a standard permutation testing implementation (referred to as NaivePT). We find that RapidPT achieves its best runtime performance on medium sized datasets ($50 \leq n \leq 200$), with speedup gains of 1.5x - 38x (vs. SnPM13) and 20x-1000x (vs. NaivePT). For larger datasets ($n \geq 200$) RapidPT outperforms NaivePT (6x - 200x), and provides substantial speedups over SnPM13 when performing more than 10000 permutations (2x - 15x). The Matlab implementation is available as a standalone toolbox called RapidPT. Our code is also integrated within SnPM13, and is able to leverage multi-core architectures when available.|permut test non parametr method obtain max null distribut use comput correct valu provid strong control fals posit neuroimag howev comput burden run algorithm signific find view permut test procedur construct veri larg permut test matrix one exploit structur properti deriv data test statist reduc runtim certain condit particular see low rank plus low varianc residu make good candid low rank matrix complet method onli veri small number entri entri experi comput obtain good estim base observ develop algorithm rapidpt abl effici recov max null distribut common obtain regular permut test neuroimag analysi present extens experiment valid four vari size dataset two baselin statist nonparametr map snpm standard permut test implement refer naivept find rapidpt achiev best runtim perform medium size dataset leq leq speedup gain vs snpm vs naivept larger dataset geq rapidpt outperform naivept provid substanti speedup snpm perform permut matlab implement avail standalon toolbox call rapidpt code also integr within snpm abl leverag multi core architectur avail|['Felipe Gutierrez-Barragan', 'Vamsi K. Ithapu', 'Chris Hinrichs', 'Camille Maumet', 'Sterling C. Johnson', 'Thomas E. Nichols', 'Vikas Singh', 'the ADNI']|['stat.AP', 'cs.CV', 'stat.ML']
2017-03-28T14:02:51Z|2017-03-03T16:29:21Z|http://arxiv.org/abs/1703.01234v1|http://arxiv.org/pdf/1703.01234v1|A Bayesian computer model analysis of Robust Bayesian analyses|bayesian comput model analysi robust bayesian analys|We harness the power of Bayesian emulation techniques, designed to aid the analysis of complex computer models, to examine the structure of complex Bayesian analyses themselves. These techniques facilitate robust Bayesian analyses and/or sensitivity analyses of complex problems, and hence allow global exploration of the impacts of choices made in both the likelihood and prior specification. We show how previously intractable problems in robustness studies can be overcome using emulation techniques, and how these methods allow other scientists to quickly extract approximations to posterior results corresponding to their own particular subjective specification. The utility and flexibility of our method is demonstrated on a reanalysis of a real application where Bayesian methods were employed to capture beliefs about river flow. We discuss the obvious extensions and directions of future research that such an approach opens up.|har power bayesian emul techniqu design aid analysi complex comput model examin structur complex bayesian analys themselv techniqu facilit robust bayesian analys sensit analys complex problem henc allow global explor impact choic made likelihood prior specif show previous intract problem robust studi overcom use emul techniqu method allow scientist quick extract approxim posterior result correspond particular subject specif util flexibl method demonstr reanalysi real applic bayesian method employ captur belief river flow discuss obvious extens direct futur research approach open|['Ian Vernon', 'John Paul Gosling']|['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']
2017-03-28T14:02:51Z|2017-03-03T06:28:36Z|http://arxiv.org/abs/1703.01051v1|http://arxiv.org/pdf/1703.01051v1|Interval Estimation of the Unknown Exponential Parameter Based on Time   Truncated Data|interv estim unknown exponenti paramet base time truncat data|In this paper we consider the statistical inference of the unknown parameter of an exponential distribution based on the time truncated data. The time truncated data occurs quite often in the reliability analysis for type-I or hybrid censoring cases. All the results available today are based on the conditional argument that at least one failure occurs during the experiment. In this paper we provide some inferential results based on the unconditional argument. We extend the results for some two-parameter distributions also.|paper consid statist infer unknown paramet exponenti distribut base time truncat data time truncat data occur quit often reliabl analysi type hybrid censor case result avail today base condit argument least one failur occur dure experi paper provid inferenti result base uncondit argument extend result two paramet distribut also|['Arnab Koley', 'Debasis Kundu']|['stat.AP', '62F10, 62F03, 62H12']
2017-03-28T14:02:51Z|2017-03-03T05:54:09Z|http://arxiv.org/abs/1703.01044v1|http://arxiv.org/pdf/1703.01044v1|On Generalized Progressive Hybrid Censoring in presence of competing   risks|general progress hybrid censor presenc compet risk|The progressive Type-II hybrid censoring scheme introduced by Kundu and Joarder (\textit{Computational Statistics and Data Analysis}, 2509-2528, 2006), has received some attention in the last few years. One major drawback of this censoring scheme is that very few observations (even no observation at all) may be observed at the end of the experiment. To overcome this problem, Cho, Sun and Lee (\textit{Statistical Methodology}, 23, 18-34, 2015) recently introduced generalized progressive censoring which ensures to get a pre specified number of failures. In this paper we analyze generalized progressive censored data in presence of competing risks. For brevity we have considered only two competing causes of failures, and it is assumed that the lifetime of the competing causes follow one parameter exponential distributions with different scale parameters. We obtain the maximum likelihood estimators of the unknown parameters and also provide their exact distributions. Based on the exact distributions of the maximum likelihood estimators exact confidence intervals can be obtained. Asymptotic and bootstrap confidence intervals are also provided for comparison purposes. We further consider the Bayesian analysis of the unknown parameters under a very flexible Beta-Gamma prior. We provide the Bayes estimates and the associated credible intervals of the unknown parameters based on the above priors. We present extensive simulation results to see the effectiveness of the proposed method and finally one real data set is analyzed for illustrative purpose.|progress type ii hybrid censor scheme introduc kundu joarder textit comput statist data analysi receiv attent last year one major drawback censor scheme veri observ even observ may observ end experi overcom problem cho sun lee textit statist methodolog recent introduc general progress censor ensur get pre specifi number failur paper analyz general progress censor data presenc compet risk breviti consid onli two compet caus failur assum lifetim compet caus follow one paramet exponenti distribut differ scale paramet obtain maximum likelihood estim unknown paramet also provid exact distribut base exact distribut maximum likelihood estim exact confid interv obtain asymptot bootstrap confid interv also provid comparison purpos consid bayesian analysi unknown paramet veri flexibl beta gamma prior provid bay estim associ credibl interv unknown paramet base abov prior present extens simul result see effect propos method final one real data set analyz illustr purpos|['Arnab Koley', 'Debasis Kundu']|['stat.AP', '62F10, 62F03, 62H12']
2017-03-28T14:02:55Z|2017-03-26T20:16:27Z|http://arxiv.org/abs/1703.01002v2|http://arxiv.org/pdf/1703.01002v2|Statistical Properties of the Risk-Transfer Formula in the Affordable   Care Act|statist properti risk transfer formula afford care act|The Affordable Care Act, signed into United States law in 2010, led to the formation of competitive insurance plans which provide universal health-insurance coverage without regard to pre-existing medical conditions. To assist insurers during a transitional period, the Act introduced a risk-transfer formula which requires insurance plans with healthier enrollees to transfer funds to plans with sicker enrollees, thereby dissuading plans from favoring healthier enrollees. In this paper, we treat the risk-transfer amounts as random variables and derive some of their statistical properties by analyzing their means and variances. The results in this paper lead to an explanation for the empirical phenomena, observed by the American Academy of Actuaries, that risk-transfer amounts were more variable and can be extremely large for insurers with smaller market shares. Our results provide conditions, as functions of the market shares of insurance plans, under which those phenomena hold.|afford care act sign unit state law led format competit insur plan provid univers health insur coverag without regard pre exist medic condit assist insur dure transit period act introduc risk transfer formula requir insur plan healthier enrolle transfer fund plan sicker enrolle therebi dissuad plan favor healthier enrolle paper treat risk transfer amount random variabl deriv statist properti analyz mean varianc result paper lead explan empir phenomena observ american academi actuari risk transfer amount variabl extrem larg insur smaller market share result provid condit function market share insur plan phenomena hold|['Michelle Li', 'Donald Richards']|['stat.AP', 'Primary: 62P05, Secondary: 60E05']
2017-03-28T14:02:55Z|2017-03-24T13:54:01Z|http://arxiv.org/abs/1703.01000v2|http://arxiv.org/abs/1703.01000v2|The M33 Synoptic Stellar Survey. II. Mira Variables|synopt stellar survey ii mira variabl|We present the discovery of 1847 Mira candidates in the Local Group galaxy M33 using a novel semi-parametric periodogram technique coupled with a Random Forest classifier. The algorithms were applied to ~2.4x10^5 I-band light curves previously obtained by the M33 Synoptic Stellar Survey. We derive preliminary Period-Luminosity relations at optical, near- & mid-infrared wavelengths and compare them to the corresponding relations in the Large Magellanic Cloud.|present discoveri mira candid local group galaxi use novel semi parametr periodogram techniqu coupl random forest classifi algorithm appli band light curv previous obtain synopt stellar survey deriv preliminari period luminos relat optic near mid infrar wavelength compar correspond relat larg magellan cloud|['Wenlong Yuan', 'Shiyuan He', 'Lucas M. Macri', 'James Long', 'Jianhua Z. Huang']|['astro-ph.SR', 'stat.AP']
2017-03-28T14:02:55Z|2017-03-02T23:03:56Z|http://arxiv.org/abs/1703.00981v1|http://arxiv.org/pdf/1703.00981v1|A Restaurant Process Mixture Model for Connectivity Based Parcellation   of the Cortex|restaur process mixtur model connect base parcel cortex|One of the primary objectives of human brain mapping is the division of the cortical surface into functionally distinct regions, i.e. parcellation. While it is generally agreed that at macro-scale different regions of the cortex have different functions, the exact number and configuration of these regions is not known. Methods for the discovery of these regions are thus important, particularly as the volume of available information grows. Towards this end, we present a parcellation method based on a Bayesian non-parametric mixture model of cortical connectivity.|one primari object human brain map divis cortic surfac function distinct region parcel general agre macro scale differ region cortex differ function exact number configur region known method discoveri region thus import particular volum avail inform grow toward end present parcel method base bayesian non parametr mixtur model cortic connect|['Daniel Moyer', 'Boris A Gutman', 'Neda Jahanshad', 'Paul M. Thompson']|['q-bio.NC', 'cs.CE', 'cs.CV', 'q-bio.QM', 'stat.AP']
2017-03-28T14:02:55Z|2017-03-02T07:57:20Z|http://arxiv.org/abs/1703.00654v1|http://arxiv.org/pdf/1703.00654v1|Nonparametric estimation of galaxy cluster's emissivity and point source   detection in astrophysics with two lasso penalties|nonparametr estim galaxi cluster emiss point sourc detect astrophys two lasso penalti|Astrophysicists are interested in recovering the 3D gas emissivity of a galaxy cluster from a 2D image taken by a telescope. A blurring phenomenon and presence of point sources make this inverse problem even harder to solve. The current state-of-the-art technique is two step: first identify the location of potential point sources, then mask these locations and deproject the data.   We instead model the data as a Poisson generalized linear model (involving blurring, Abel and wavelets operators) regularized by two lasso penalties to induce sparse wavelet representation and sparse point sources. The amount of sparsity is controlled by two quantile universal thresholds. As a result, our method outperforms the existing one.|astrophysicist interest recov gas emiss galaxi cluster imag taken telescop blur phenomenon presenc point sourc make invers problem even harder solv current state art techniqu two step first identifi locat potenti point sourc mask locat deproject data instead model data poisson general linear model involv blur abel wavelet oper regular two lasso penalti induc spars wavelet represent spars point sourc amount sparsiti control two quantil univers threshold result method outperform exist one|['Jairo Diaz Rodriguez', 'Dominique Eckert', 'Hatef Monajemi', 'Stéphane Paltani', 'Sylvain Sardy']|['stat.AP', 'stat.ME']
2017-03-28T14:02:55Z|2017-03-01T17:42:47Z|http://arxiv.org/abs/1703.00409v1|http://arxiv.org/pdf/1703.00409v1|Sequence of purchases in credit card data reveal life styles in urban   populations|sequenc purchas credit card data reveal life style urban popul|From our most basic consumption to secondary needs, our spending habits reflect our life styles. Yet, in computational social sciences there is an open question about the existence of ubiquitous trends in spending habits by various groups at urban scale. Limited information collected by expenditure surveys have not proven conclusive in this regard. This is because, the frequency of purchases by type is highly uneven and follows a Zipf-like distribution. In this work, we apply text compression techniques to the purchase codes of credit card data to detect the significant sequences of transactions of each user. Five groups of consumers emerge when grouped by their similarity based on these sequences. Remarkably, individuals in each consumer group are also similar in age, total expenditure, gender, and the diversity of their social and mobility networks extracted by their mobile phone records. By properly deconstructing transaction data with Zipf-like distributions, we find that it can give us insights on collective behavior.|basic consumpt secondari need spend habit reflect life style yet comput social scienc open question exist ubiquit trend spend habit various group urban scale limit inform collect expenditur survey proven conclus regard becaus frequenc purchas type high uneven follow zipf like distribut work appli text compress techniqu purchas code credit card data detect signific sequenc transact user five group consum emerg group similar base sequenc remark individu consum group also similar age total expenditur gender divers social mobil network extract mobil phone record proper deconstruct transact data zipf like distribut find give us insight collect behavior|['Riccardo Di Clemente', 'Miguel Luengo-Oroz', 'Matias Travizano', 'Bapu Vaitla', 'Marta C. Gonzalez']|['physics.soc-ph', 'cs.IT', 'cs.SI', 'math.IT', 'stat.AP']
2017-03-28T14:02:55Z|2017-03-01T07:00:01Z|http://arxiv.org/abs/1703.00154v1|http://arxiv.org/pdf/1703.00154v1|Inertial Odometry on Handheld Smartphones|inerti odometri handheld smartphon|Building a complete inertial navigation system using the limited quality data provided by current smartphones has been regarded challenging, if not impossible. We present a probabilistic approach for orientation and use-case free inertial odometry, which is based on double-integrating rotated accelerations. Our approach uses a probabilistic approach in fusing the noisy sensor data and learning the model parameters online. It is able to track the phone position, velocity, and pose in real-time and in a computationally lightweight fashion. The information fusion is completed with altitude correction from barometric pressure readings (if available), zero-velocity updates (if the phone remains stationary), and pseudo-updates limiting the momentary speed. We demonstrate our approach using a standard iPad and iPhone in several indoor dead-reckoning applications and in a measurement tool setup.|build complet inerti navig system use limit qualiti data provid current smartphon regard challeng imposs present probabilist approach orient use case free inerti odometri base doubl integr rotat acceler approach use probabilist approach fuse noisi sensor data learn model paramet onlin abl track phone posit veloc pose real time comput lightweight fashion inform fusion complet altitud correct barometr pressur read avail zero veloc updat phone remain stationari pseudo updat limit momentari speed demonstr approach use standard ipad iphon sever indoor dead reckon applic measur tool setup|['Arno Solin', 'Santiago Cortes', 'Esa Rahtu', 'Juho Kannala']|['cs.CV', 'stat.AP']
2017-03-28T14:02:55Z|2017-02-28T21:12:37Z|http://arxiv.org/abs/1703.00056v1|http://arxiv.org/pdf/1703.00056v1|Fair prediction with disparate impact: A study of bias in recidivism   prediction instruments|fair predict dispar impact studi bias recidiv predict instrument|Recidivism prediction instruments (RPI's) provide decision makers with an assessment of the likelihood that a criminal defendant will reoffend at a future point in time. While such instruments are gaining increasing popularity across the country, their use is attracting tremendous controversy. Much of the controversy concerns potential discriminatory bias in the risk assessments that are produced. This paper discusses several fairness criteria that have recently been applied to assess the fairness of recidivism prediction instruments. We demonstrate that the criteria cannot all be simultaneously satisfied when recidivism prevalence differs across groups. We then show how disparate impact can arise when a recidivism prediction instrument fails to satisfy the criterion of error rate balance.|recidiv predict instrument rpi provid decis maker assess likelihood crimin defend reoffend futur point time instrument gain increas popular across countri use attract tremend controversi much controversi concern potenti discriminatori bias risk assess produc paper discuss sever fair criteria recent appli assess fair recidiv predict instrument demonstr criteria cannot simultan satisfi recidiv preval differ across group show dispar impact aris recidiv predict instrument fail satisfi criterion error rate balanc|['Alexandra Chouldechova']|['stat.AP', 'cs.CY', 'stat.ML']
2017-03-28T14:02:55Z|2017-02-28T04:25:44Z|http://arxiv.org/abs/1702.08638v1|http://arxiv.org/pdf/1702.08638v1|Single-lead f-wave extraction using diffusion geometry|singl lead wave extract use diffus geometri|A novel single-lead f-wave extraction algorithm based on the modern diffusion geometry data analysis framework is proposed. The algorithm is essentially an averaged beat subtraction algorithm, where the ventricular activity template is estimated by combining a newly designed metric, the diffusion distance, and the non-local Euclidean median based on the non-linear manifold setup. To validate the algorithm, two simulation schemes are proposed and tested, and state-of-the-art results are reported. The clinical potential is shown in the real Holter signal, and we introduce a new score to evaluate the performance of the algorithm.|novel singl lead wave extract algorithm base modern diffus geometri data analysi framework propos algorithm essenti averag beat subtract algorithm ventricular activ templat estim combin newli design metric diffus distanc non local euclidean median base non linear manifold setup valid algorithm two simul scheme propos test state art result report clinic potenti shown real holter signal introduc new score evalu perform algorithm|['John Malik', 'Neil Reed', 'Chun-Li Wang', 'Hautieng Wu']|['physics.data-an', 'q-bio.QM', 'stat.AP']
2017-03-28T14:02:55Z|2017-02-27T22:22:03Z|http://arxiv.org/abs/1702.08560v1|http://arxiv.org/pdf/1702.08560v1|Estimating the reproductive number, total outbreak size, and reporting   rates for Zika epidemics in South and Central America|estim reproduct number total outbreak size report rate zika epidem south central america|As South and Central American countries prepare for increased birth defects from Zika virus outbreaks and plan for mitigation strategies to minimize ongoing and future outbreaks, understanding important characteristics of Zika outbreaks and how they vary across regions is a challenging and important problem. We developed a mathematical model for the 2015 Zika virus outbreak dynamics in Colombia, El Salvador, and Suriname. We fit the model to publicly available data provided by the Pan American Health Organization, using Approximate Bayesian Computation to estimate parameter distributions and provide uncertainty quantification. An important model input is the at-risk susceptible population, which can vary with a number of factors including climate, elevation, population density, and socio-economic status. We informed this initial condition using the highest historically reported dengue incidence modified by the probable dengue reporting rates in the chosen countries. The model indicated that a country-level analysis was not appropriate for Colombia. We then estimated the basic reproduction number, or the expected number of new human infections arising from a single infected human, to range between 4 and 6 for El Salvador and Suriname with a median of 4.3 and 5.3, respectively. We estimated the reporting rate to be around 16% in El Salvador and 18% in Suriname with estimated total outbreak sizes of 73,395 and 21,647 people, respectively. The uncertainty in parameter estimates highlights a need for research and data collection that will better constrain parameter ranges.|south central american countri prepar increas birth defect zika virus outbreak plan mitig strategi minim ongo futur outbreak understand import characterist zika outbreak vari across region challeng import problem develop mathemat model zika virus outbreak dynam colombia el salvador surinam fit model public avail data provid pan american health organ use approxim bayesian comput estim paramet distribut provid uncertainti quantif import model input risk suscept popul vari number factor includ climat elev popul densiti socio econom status inform initi condit use highest histor report dengu incid modifi probabl dengu report rate chosen countri model indic countri level analysi appropri colombia estim basic reproduct number expect number new human infect aris singl infect human rang el salvador surinam median respect estim report rate around el salvador surinam estim total outbreak size peopl respect uncertainti paramet estim highlight need research data collect better constrain paramet rang|['Deborah P. Shutt', 'Carrie A. Manore', 'Stephen Pankavich', 'Aaron T. Porter', 'Sara Y. Del Valle']|['q-bio.PE', 'q-bio.QM', 'stat.AP', '92D30']
2017-03-28T14:02:55Z|2017-02-27T18:38:28Z|http://arxiv.org/abs/1703.01237v1|http://arxiv.org/pdf/1703.01237v1|How real is the random censorship model in medical studies?|real random censorship model medic studi|In survival analysis the random censorship model refers to censoring and survival times being independent of each other. It is one of the fundamental assumptions in the theory of survival analysis. We explain the reason for it being so ubiquitous, and we investigate its presence in medical studies. We differentiate two types of censoring in medical studies (dropout and administrative), and we explain their importance in examining the existence of the random censorship model. We show that in order to presume the random censorship model it is not enough to have a design study which conforms to it, but that one needs to provide evidence for its presence in the results. Blindly presuming the random censorship model might lead to the Kaplan-Meier estimator producing biased results, which might have serious consequences when estimating survival in medical studies.|surviv analysi random censorship model refer censor surviv time independ one fundament assumpt theori surviv analysi explain reason ubiquit investig presenc medic studi differenti two type censor medic studi dropout administr explain import examin exist random censorship model show order presum random censorship model enough design studi conform one need provid evid presenc result blind presum random censorship model might lead kaplan meier estim produc bias result might serious consequ estim surviv medic studi|['Damjan Krstajic']|['stat.AP', 'math.ST', 'stat.TH']
2017-03-28T14:03:00Z|2017-02-27T13:03:33Z|http://arxiv.org/abs/1702.08262v1|http://arxiv.org/pdf/1702.08262v1|Sequential Discrete Kalman Filter for Real-Time State Estimation in   Power Distribution Systems: Theory and Implementation|sequenti discret kalman filter real time state estim power distribut system theori implement|This paper demonstrates the feasibility of implementing Real-Time State Estimators (RTSEs) for Active Distribution Networks (ADNs) in Field-Programmable Gate Arrays (FPGAs) by presenting an operational prototype. The prototype is based on a Linear State Estimator (LSE) that uses synchrophasor measurements from Phasor Measurement Units (PMUs). The underlying algorithm is the Sequential Discrete Kalman Filter (SDKF), an equivalent formulation of the Discrete Kalman Filter (DKF) for the case of uncorrelated measurement noise. In this regard, this work formally proves the equivalence the SDKF and the DKF, and highlights the suitability of the SDKF for an FPGA implementation by means of a computational complexity analysis. The developed prototype is validated using a case study adapted from the IEEE 34-node distribution test feeder.|paper demonstr feasibl implement real time state estim rtses activ distribut network adn field programm gate array fpgas present oper prototyp prototyp base linear state estim lse use synchrophasor measur phasor measur unit pmus algorithm sequenti discret kalman filter sdkf equival formul discret kalman filter dkf case uncorrel measur nois regard work formal prove equival sdkf dkf highlight suitabl sdkf fpga implement mean comput complex analysi develop prototyp valid use case studi adapt ieee node distribut test feeder|['Andreas Martin Kettner', 'Mario Paolone']|['stat.AP']
2017-03-28T14:03:00Z|2017-02-27T08:33:26Z|http://arxiv.org/abs/1702.08185v1|http://arxiv.org/pdf/1702.08185v1|An update on statistical boosting in biomedicine|updat statist boost biomedicin|Statistical boosting algorithms have triggered a lot of research during the last decade. They combine a powerful machine-learning approach with classical statistical modelling, offering various practical advantages like automated variable selection and implicit regularization of effect estimates. They are extremely flexible, as the underlying base-learners (regression functions defining the type of effect for the explanatory variables) can be combined with any kind of loss function (target function to be optimized, defining the type of regression setting). In this review article, we highlight the most recent methodological developments on statistical boosting regarding variable selection, functional regression and advanced time-to-event modelling. Additionally, we provide a short overview on relevant applications of statistical boosting in biomedicine.|statist boost algorithm trigger lot research dure last decad combin power machin learn approach classic statist model offer various practic advantag like autom variabl select implicit regular effect estim extrem flexibl base learner regress function defin type effect explanatori variabl combin ani kind loss function target function optim defin type regress set review articl highlight recent methodolog develop statist boost regard variabl select function regress advanc time event model addit provid short overview relev applic statist boost biomedicin|['Andreas Mayr', 'Benjamin Hofner', 'Elisabeth Waldmann', 'Tobias Hepp', 'Olaf Gefeller', 'Matthias Schmid']|['stat.AP', 'stat.CO', 'stat.ML']
2017-03-28T14:03:00Z|2017-02-27T04:17:36Z|http://arxiv.org/abs/1702.08140v1|http://arxiv.org/pdf/1702.08140v1|A mixture model approach to infer land-use influence on point referenced   water quality|mixtur model approach infer land use influenc point referenc water qualiti|The assessment of water quality across space and time is of considerable interest for both agricultural and public health reasons. The standard method to assess the water quality of a catchment, or a group of catchments, usually involves collecting point measurements of water quality and other additional information such as the date and time of measurements, rainfall amounts, the land-use and soil-type of the catchment and the elevation. Some of this auxiliary information will be point data, measured at the exact location, whereas other such as land-use will be areal data often in a compositional format. Two problems arise if analysts try to incorporate this information into a statistical model in order to predict (for example) the influence of land-use on water quality. First is the spatial change of support problem that arises when using areal data to predict outcomes at point locations. Secondly, the physical process driving water quality is not compositional, rather it is the observation process that provides compositional data. In this paper we present an approach that accounts for these two issues by using a latent variable to identify the land-use that most likely influences water quality. This latent variable is used in a spatial mixture model to help estimate the influence of land-use on water quality. We demonstrate the potential of this approach with data from a water quality research study in the Mount Lofty range, in South Australia.|assess water qualiti across space time consider interest agricultur public health reason standard method assess water qualiti catchment group catchment usual involv collect point measur water qualiti addit inform date time measur rainfal amount land use soil type catchment elev auxiliari inform point data measur exact locat wherea land use areal data often composit format two problem aris analyst tri incorpor inform statist model order predict exampl influenc land use water qualiti first spatial chang support problem aris use areal data predict outcom point locat second physic process drive water qualiti composit rather observ process provid composit data paper present approach account two issu use latent variabl identifi land use like influenc water qualiti latent variabl use spatial mixtur model help estim influenc land use water qualiti demonstr potenti approach data water qualiti research studi mount lofti rang south australia|['Adrien Ickowicz', 'Jessica H. Ford', 'Keith R. Hayes']|['stat.AP', 'stat.CO']
2017-03-28T14:03:00Z|2017-02-26T21:23:33Z|http://arxiv.org/abs/1702.08088v1|http://arxiv.org/pdf/1702.08088v1|Selection of training populations (and other subset selection problems)   with an accelerated genetic algorithm (STPGA: An R-package for selection of   training populations with a genetic algorithm)|select train popul subset select problem acceler genet algorithm stpga packag select train popul genet algorithm|Optimal subset selection is an important task that has numerous algorithms designed for it and has many application areas. STPGA contains a special genetic algorithm supplemented with a tabu memory property (that keeps track of previously tried solutions and their fitness for a number of iterations), and with a regression of the fitness of the solutions on their coding that is used to form the ideal estimated solution (look ahead property) to search for solutions of generic optimal subset selection problems. I have initially developed the programs for the specific problem of selecting training populations for genomic prediction or association problems, therefore I give discussion of the theory behind optimal design of experiments to explain the default optimization criteria in STPGA, and illustrate the use of the programs in this endeavor. Nevertheless, I have picked a few other areas of application: supervised and unsupervised variable selection based on kernel alignment, supervised variable selection with design criteria, influential observation identification for regression, solving mixed integer quadratic optimization problems, balancing gains and inbreeding in a breeding population. Some of these illustrations pertain new statistical approaches.|optim subset select import task numer algorithm design mani applic area stpga contain special genet algorithm supplement tabu memori properti keep track previous tri solut fit number iter regress fit solut code use form ideal estim solut look ahead properti search solut generic optim subset select problem initi develop program specif problem select train popul genom predict associ problem therefor give discuss theori behind optim design experi explain default optim criteria stpga illustr use program endeavor nevertheless pick area applic supervis unsupervis variabl select base kernel align supervis variabl select design criteria influenti observ identif regress solv mix integ quadrat optim problem balanc gain inbreed breed popul illustr pertain new statist approach|['Deniz Akdemir']|['stat.ME', 'cs.LG', 'q-bio.GN', 'q-bio.QM', 'stat.AP']
2017-03-28T14:03:00Z|2017-02-26T10:41:26Z|http://arxiv.org/abs/1703.01977v1|http://arxiv.org/pdf/1703.01977v1|Linear, Machine Learning and Probabilistic Approaches for Time Series   Analysis|linear machin learn probabilist approach time seri analysi|In this paper we study different approaches for time series modeling. The forecasting approaches using linear models, ARIMA alpgorithm, XGBoost machine learning algorithm are described. Results of different model combinations are shown. For probabilistic modeling the approaches using copulas and Bayesian inference are considered.|paper studi differ approach time seri model forecast approach use linear model arima alpgorithm xgboost machin learn algorithm describ result differ model combin shown probabilist model approach use copula bayesian infer consid|['B. M. Pavlyshenko']|['stat.AP', 'cs.LG', 'stat.ME']
2017-03-28T14:03:00Z|2017-02-26T03:10:41Z|http://arxiv.org/abs/1702.07981v1|http://arxiv.org/pdf/1702.07981v1|BayCount: A Bayesian Decomposition Method for Inferring Tumor   Heterogeneity using RNA-Seq Counts|baycount bayesian decomposit method infer tumor heterogen use rna seq count|Tumor is heterogeneous - a tumor sample usually consists of a set of subclones with distinct transcriptional profiles and potentially different degrees of aggressiveness and responses to drugs. Understanding tumor heterogeneity is therefore critical to precise cancer prognosis and treatment. In this paper, we introduce BayCount, a Bayesian decomposition method to infer tumor heterogeneity with highly over-dispersed RNA sequencing count data. Using negative binomial factor analysis, BayCount takes into account both the between-sample and gene-specific random effects on raw counts of sequencing reads mapped to each gene. For posterior inference, we develop an efficient compound Poisson based blocked Gibbs sampler. Through extensive simulation studies and analysis of The Cancer Genome Atlas lung cancer and kidney cancer RNA sequencing count data, we show that BayCount is able to accurately estimate the number of subclones, the proportions of these subclones in each tumor sample, and the gene expression profiles in each subclone. Our method represents the first effort in characterizing tumor heterogeneity using RNA sequencing count data that simultaneously removes the need of normalizing the counts, achieves statistical robustness, and obtains biologically and clinically meaningful insights.|tumor heterogen tumor sampl usual consist set subclon distinct transcript profil potenti differ degre aggress respons drug understand tumor heterogen therefor critic precis cancer prognosi treatment paper introduc baycount bayesian decomposit method infer tumor heterogen high dispers rna sequenc count data use negat binomi factor analysi baycount take account sampl gene specif random effect raw count sequenc read map gene posterior infer develop effici compound poisson base block gibb sampler extens simul studi analysi cancer genom atlas lung cancer kidney cancer rna sequenc count data show baycount abl accur estim number subclon proport subclon tumor sampl gene express profil subclon method repres first effort character tumor heterogen use rna sequenc count data simultan remov need normal count achiev statist robust obtain biolog clinic meaning insight|['Fangzheng Xie', 'Mingyuan Zhou', 'Yanxun Xu']|['stat.AP']
2017-03-28T14:03:00Z|2017-03-04T00:20:43Z|http://arxiv.org/abs/1702.07909v2|http://arxiv.org/pdf/1702.07909v2|Analysis of Urban Vibrancy and Safety in Philadelphia|analysi urban vibranc safeti philadelphia|"Statistical analyses of urban environments have been recently improved through publicly available high resolution data and mapping technologies that have adopted across industries. These technologies allow us to create metrics to empirically investigate urban design principles of the past half-century. Philadelphia is an interesting case study for this work, with its rapid urban development and population increase in the last decade. We focus on features of what urban planners call ""vibrancy"": measures of positive, healthy activity or energy in an area. Historically, vibrancy has been very challenging to measure empirically. We explore the association between safety (violent and non-violent crime) and features of local neighborhood vibrancy such as population, economic measures and land use zoning. Despite rhetoric about the negative effects of population density in the 1960s and 70s, we find very little association between crime and population density. Measures based on land use zoning are not an adequate description of local vibrancy and so we construct a database and set of measures of business activity in each neighborhood. We employ several matching analyses within census block groups to explore the relationship between neighborhood vibrancy and safety at a higher resolution. We find that neighborhoods with more vacancy have higher crime but within neighborhoods, crimes tend not to be located near vacant properties. We also find that more crimes occur near business locations but businesses that are active (open) for longer periods are associated with fewer crimes."|statist analys urban environ recent improv public avail high resolut data map technolog adopt across industri technolog allow us creat metric empir investig urban design principl past half centuri philadelphia interest case studi work rapid urban develop popul increas last decad focus featur urban planner call vibranc measur posit healthi activ energi area histor vibranc veri challeng measur empir explor associ safeti violent non violent crime featur local neighborhood vibranc popul econom measur land use zone despit rhetor negat effect popul densiti find veri littl associ crime popul densiti measur base land use zone adequ descript local vibranc construct databas set measur busi activ neighborhood employ sever match analys within census block group explor relationship neighborhood vibranc safeti higher resolut find neighborhood vacanc higher crime within neighborhood crime tend locat near vacant properti also find crime occur near busi locat busi activ open longer period associ fewer crime|['Colman Humphrey', 'Shane T. Jensen', 'Dylan Small', 'Rachel Thurston']|['stat.AP']
2017-03-28T14:03:00Z|2017-02-25T10:26:59Z|http://arxiv.org/abs/1702.07869v1|http://arxiv.org/pdf/1702.07869v1|Signal Denoising Using the Minimum-Probability-of-Error Criterion|signal denois use minimum probabl error criterion|We address the problem of signal denoising via transform-domain shrinkage based on a novel $\textit{risk}$ criterion called the minimum probability of error (MPE), which measures the probability that the estimated parameter lies outside an $\epsilon$-neighborhood of the actual value. However, the MPE, similar to the mean-squared error (MSE), depends on the ground-truth parameter, and has to be estimated from the noisy observations. We consider linear shrinkage-based denoising functions, wherein the optimum shrinkage parameter is obtained by minimizing an estimate of the MPE. When the probability of error is integrated over $\epsilon$, it leads to the expected $\ell_1$ distortion. The proposed MPE and $\ell_1$ distortion formulations are applicable to various noise distributions by invoking a Gaussian mixture model approximation. Within the realm of MPE, we also develop an extension of the transform-domain shrinkage by grouping transform coefficients, resulting in $\textit{subband shrinkage}$. The denoising performance obtained within the proposed framework is shown to be better than that obtained using the minimum MSE-based approaches formulated within $\textbf{$\textit {Stein's unbiased risk estimation}$}$ (SURE) framework, especially in the low measurement signal-to-noise ratio (SNR) regime. Performance comparison with three state-of-the-art denoising algorithms, carried out on electrocardiogram signals and two test signals taken from the $\textit{Wavelab}$ toolbox, exhibits that the MPE framework results in consistent SNR gains for input SNRs below $5$ dB.|address problem signal denois via transform domain shrinkag base novel textit risk criterion call minimum probabl error mpe measur probabl estim paramet lie outsid epsilon neighborhood actual valu howev mpe similar mean squar error mse depend ground truth paramet estim noisi observ consid linear shrinkag base denois function wherein optimum shrinkag paramet obtain minim estim mpe probabl error integr epsilon lead expect ell distort propos mpe ell distort formul applic various nois distribut invok gaussian mixtur model approxim within realm mpe also develop extens transform domain shrinkag group transform coeffici result textit subband shrinkag denois perform obtain within propos framework shown better obtain use minimum mse base approach formul within textbf textit stein unbias risk estim sure framework especi low measur signal nois ratio snr regim perform comparison three state art denois algorithm carri electrocardiogram signal two test signal taken textit wavelab toolbox exhibit mpe framework result consist snr gain input snrs db|['Jishnu Sadasivan', 'Subhadip Mukherjee', 'Chandra Sekhar Seelamantula']|['stat.AP']
2017-03-28T14:03:00Z|2017-02-24T05:16:23Z|http://arxiv.org/abs/1702.07465v1|http://arxiv.org/pdf/1702.07465v1|PairClone: A Bayesian Subclone Caller Based on Mutation Pairs|pairclon bayesian subclon caller base mutat pair|Tumor cell populations can be thought of as being composed of homogeneous cell subpopulations, with each subpopulation being characterized by overlapping sets of single nucleotide variants (SNVs). Such subpopulations are known as subclones and are an important target for precision medicine. Reconstructing such subclones from next-generation sequencing (NGS) data is one of the major challenges in precision medicine. We present PairClone as a new tool to implement this reconstruction. The main idea of PairClone is to model short reads mapped to pairs of proximal SNVs. In contrast, most existing methods use only marginal reads for unpaired SNVs. Using Bayesian nonparametric models, we estimate posterior probabilities of the number, genotypes and population frequencies of subclones in one or more tumor sample. We use the categorical Indian buffet process (cIBP) as a prior probability model for subclones that are represented as vectors of categorical matrices that record the corresponding sets of mutation pairs. Performance of PairClone is assessed using simulated and real datasets. An open source software package can be obtained at http://www.compgenome.org/pairclone.|tumor cell popul thought compos homogen cell subpopul subpopul character overlap set singl nucleotid variant snvs subpopul known subclon import target precis medicin reconstruct subclon next generat sequenc ngs data one major challeng precis medicin present pairclon new tool implement reconstruct main idea pairclon model short read map pair proxim snvs contrast exist method use onli margin read unpair snvs use bayesian nonparametr model estim posterior probabl number genotyp popul frequenc subclon one tumor sampl use categor indian buffet process cibp prior probabl model subclon repres vector categor matric record correspond set mutat pair perform pairclon assess use simul real dataset open sourc softwar packag obtain http www compgenom org pairclon|['Tianjian Zhou', 'Peter Mueller', 'Subhajit Sengupta', 'Yuan Ji']|['stat.AP']
2017-03-28T14:03:00Z|2017-02-23T23:25:32Z|http://arxiv.org/abs/1702.07422v1|http://arxiv.org/pdf/1702.07422v1|sourceR: Classification and Source Attribution of Infectious Agents   among Heterogeneous Populations|sourcer classif sourc attribut infecti agent among heterogen popul|Zoonotic diseases are a major cause of morbidity, and productivity losses in both humans and animal populations. Identifying the source of food-borne zoonoses (e.g. an animal reservoir or food product) is crucial for the identification and prioritisation of food safety interventions. For many zoonotic diseases it is difficult to attribute human cases to sources of infection because there is little epidemiological information on the cases. However, microbial strain typing allows zoonotic pathogens to be categorised, and the relative frequencies of the strain types among the sources and in human cases allows inference on the likely source of each infection. We introduce sourceR, an R package for quantitative source attribution, aimed at food-borne diseases. It implements a fully joint Bayesian model using strain-typed surveillance data from both human cases and source samples, capable of identifying important sources of infection. The model measures the force of infection from each source, allowing for varying survivability, pathogenicity and virulence of pathogen strains, and varying abilities of the sources to act as vehicles of infection. A Bayesian non-parametric (Dirichlet process) approach is used to cluster pathogen strain types by epidemiological behaviour, avoiding model overfitting and allowing detection of strain types associated with potentially high 'virulence'.   sourceR is demonstrated using Campylobacter jejuni isolate data collected in New Zealand between 2005 and 2008. It enables straightforward attribution of cases of zoonotic infection to putative sources of infection by epidemiologists and public health decision makers. As sourceR develops, we intend it to become an important and flexible resource for food-borne disease attribution studies.|zoonot diseas major caus morbid product loss human anim popul identifi sourc food born zoonos anim reservoir food product crucial identif prioritis food safeti intervent mani zoonot diseas difficult attribut human case sourc infect becaus littl epidemiolog inform case howev microbi strain type allow zoonot pathogen categoris relat frequenc strain type among sourc human case allow infer like sourc infect introduc sourcer packag quantit sourc attribut aim food born diseas implement fulli joint bayesian model use strain type surveil data human case sourc sampl capabl identifi import sourc infect model measur forc infect sourc allow vari surviv pathogen virul pathogen strain vari abil sourc act vehicl infect bayesian non parametr dirichlet process approach use cluster pathogen strain type epidemiolog behaviour avoid model overfit allow detect strain type associ potenti high virul sourcer demonstr use campylobact jejuni isol data collect new zealand enabl straightforward attribut case zoonot infect putat sourc infect epidemiologist public health decis maker sourcer develop intend becom import flexibl resourc food born diseas attribut studi|['Poppy Miller', 'Jonathan Marshall', 'Nigel French', 'Chris Jewell']|['stat.AP']
2017-03-28T14:03:04Z|2017-02-23T18:24:58Z|http://arxiv.org/abs/1702.07326v1|http://arxiv.org/pdf/1702.07326v1|Time-Series Adaptive Estimation of Vaccination Uptake Using Web Search   Queries|time seri adapt estim vaccin uptak use web search queri|Estimating vaccination uptake is an integral part of ensuring public health. It was recently shown that vaccination uptake can be estimated automatically from web data, instead of slowly collected clinical records or population surveys. All prior work in this area assumes that features of vaccination uptake collected from the web are temporally regular. We present the first ever method to remove this assumption from vaccination uptake estimation: our method dynamically adapts to temporal fluctuations in time series web data used to estimate vaccination uptake. We show our method to outperform the state of the art compared to competitive baselines that use not only web data but also curated clinical data. This performance improvement is more pronounced for vaccines whose uptake has been irregular due to negative media attention (HPV-1 and HPV-2), problems in vaccine supply (DiTeKiPol), and targeted at children of 12 years old (whose vaccination is more irregular compared to younger children).|estim vaccin uptak integr part ensur public health recent shown vaccin uptak estim automat web data instead slowli collect clinic record popul survey prior work area assum featur vaccin uptak collect web tempor regular present first ever method remov assumpt vaccin uptak estim method dynam adapt tempor fluctuat time seri web data use estim vaccin uptak show method outperform state art compar competit baselin use onli web data also curat clinic data perform improv pronounc vaccin whose uptak irregular due negat media attent hpv hpv problem vaccin suppli ditekipol target children year old whose vaccin irregular compar younger children|['Niels Dalum Hansen', 'Kåre Mølbak', 'Ingemar J. Cox', 'Christina Lioma']|['cs.IR', 'q-bio.QM', 'stat.AP']
2017-03-28T14:03:04Z|2017-02-22T21:21:54Z|http://arxiv.org/abs/1702.07009v1|http://arxiv.org/pdf/1702.07009v1|The Impact of Confounder Selection in Propensity Scores for Rare Events   Data - with Applications to Birth Defects|impact confound select propens score rare event data applic birth defect|Our work was motivated by a recent study on birth defects of infants born to pregnant women exposed to a certain medication for treating chronic diseases. Outcomes such as birth defects are rare events in the general population, which often translate to very small numbers of events in the unexposed group. As drug safety studies in pregnancy are typically observational in nature, we control for confounding in this rare events setting using propensity scores (PS). Using our empirical data, we noticed that the estimated odds ratio for birth defects due to exposure varied drastically depending on the specific approach used. The commonly used approaches with PS are matching, stratification, inverse probability weighting (IPW) and regression adjustment. The extremely rare events setting renders the matching or stratification infeasible. In addition, the PS itself may be formed via different approaches to select confounders from a relatively long list of potential confounders. We carried out simulation experiments to compare different combinations of approaches: IPW or regression adjustment, with 1) including all potential confounders without selection, 2) selection based on univariate association between the candidate variable and the outcome, 3) selection based on change in effects (CIE). The simulation showed that IPW without selection leads to extremely large variances in the estimated odds ratio, which help to explain the empirical data analysis results that we had observed. The simulation also showed that IPW with selection based on univariate association with the outcome is preferred over IPW with CIE. Regression adjustment has small variances of the estimated odds ratio regardless of the selection methods used.|work motiv recent studi birth defect infant born pregnant women expos certain medic treat chronic diseas outcom birth defect rare event general popul often translat veri small number event unexpos group drug safeti studi pregnanc typic observ natur control confound rare event set use propens score ps use empir data notic estim odd ratio birth defect due exposur vari drastic depend specif approach use common use approach ps match stratif invers probabl weight ipw regress adjust extrem rare event set render match stratif infeas addit ps may form via differ approach select confound relat long list potenti confound carri simul experi compar differ combin approach ipw regress adjust includ potenti confound without select select base univari associ candid variabl outcom select base chang effect cie simul show ipw without select lead extrem larg varianc estim odd ratio help explain empir data analysi result observ simul also show ipw select base univari associ outcom prefer ipw cie regress adjust small varianc estim odd ratio regardless select method use|['Ronghui Xu', 'Jue Hou', 'Christina D. Chambers']|['stat.AP']
2017-03-28T14:03:04Z|2017-02-22T21:09:19Z|http://arxiv.org/abs/1702.07007v1|http://arxiv.org/pdf/1702.07007v1|Detecting causal associations in large nonlinear time series datasets|detect causal associ larg nonlinear time seri dataset|Detecting causal associations in time series datasets is a key challenge for novel insights into complex dynamical systems such as the Earth system or the human brain. Interactions in high-dimensional dynamical systems often involve time-delays, nonlinearity, and strong autocorrelations. These present major challenges for causal discovery techniques such as Granger causality leading to low detection power, biases, and unreliable hypothesis tests. Here we introduce a reliable and fast method that outperforms current approaches in detection power and scales up to high-dimensional datasets. It overcomes detection biases, especially when strong autocorrelations are present, and allows ranking associations in large-scale analyses by their causal strength. We provide mathematical proofs, evaluate our method in extensive numerical experiments, and illustrate its capabilities in a large-scale analysis of the global surface-pressure system where we unravel spurious associations and find several potentially causal links that are difficult to detect with standard methods. The broadly applicable method promises to discover novel causal insights also in many other fields of science.|detect causal associ time seri dataset key challeng novel insight complex dynam system earth system human brain interact high dimension dynam system often involv time delay nonlinear strong autocorrel present major challeng causal discoveri techniqu granger causal lead low detect power bias unreli hypothesi test introduc reliabl fast method outperform current approach detect power scale high dimension dataset overcom detect bias especi strong autocorrel present allow rank associ larg scale analys causal strength provid mathemat proof evalu method extens numer experi illustr capabl larg scale analysi global surfac pressur system unravel spurious associ find sever potenti causal link difficult detect standard method broad applic method promis discov novel causal insight also mani field scienc|['Jakob Runge', 'Dino Sejdinovic', 'Seth Flaxman']|['stat.ME', 'physics.ao-ph', 'stat.AP']
2017-03-28T14:03:04Z|2017-02-22T20:18:36Z|http://arxiv.org/abs/1702.06993v1|http://arxiv.org/pdf/1702.06993v1|Generalized Pareto Processes and Liquidity|general pareto process liquid|Motivated by the modeling of liquidity risk in fund management in a dynamic setting, we propose and investigate a class of time series models with generalized Pareto marginals: the autoregressive generalized Pareto process (ARGP), a modified ARGP (MARGP) and a thresholded ARGP (TARGP). These models are able to capture key data features apparent in fund liquidity data and reflect the underlying phenomena via easily interpreted, low-dimensional model parameters. We establish stationarity and ergodicity, provide a link to the class of shot-noise processes, and determine the associated interarrival distributions for exceedances. Moreover, we provide estimators for all relevant model parameters and establish consistency and asymptotic normality for all estimators (except the threshold parameter, which as usual must be dealt with separately). Finally, we illustrate our approach using real-world fund redemption data, and we discuss the goodness-of-fit of the estimated models.|motiv model liquid risk fund manag dynam set propos investig class time seri model general pareto margin autoregress general pareto process argp modifi argp margp threshold argp targp model abl captur key data featur appar fund liquid data reflect phenomena via easili interpret low dimension model paramet establish stationar ergod provid link class shot nois process determin associ interarriv distribut exceed moreov provid estim relev model paramet establish consist asymptot normal estim except threshold paramet usual must dealt separ final illustr approach use real world fund redempt data discuss good fit estim model|['Sascha Desmettre', 'Johan de Kock', 'Peter Ruckdeschel', 'Frank Thomas Seifried']|['stat.AP', '60G70, 62P05']
2017-03-28T14:03:04Z|2017-02-22T03:10:41Z|http://arxiv.org/abs/1702.06661v1|http://arxiv.org/pdf/1702.06661v1|Social Learning and Diffusion of Pervasive Goods: An Empirical Study of   an African App Store|social learn diffus pervas good empir studi african app store|In this study, the authors develop a structural model that combines a macro diffusion model with a micro choice model to control for the effect of social influence on the mobile app choices of customers over app stores. Social influence refers to the density of adopters within the proximity of other customers. Using a large data set from an African app store and Bayesian estimation methods, the authors quantify the effect of social influence and investigate the impact of ignoring this process in estimating customer choices. The findings show that customer choices in the app store are explained better by offline than online density of adopters and that ignoring social influence in estimations results in biased estimates. Furthermore, the findings show that the mobile app adoption process is similar to adoption of music CDs, among all other classic economy goods. A counterfactual analysis shows that the app store can increase its revenue by 13.6% through a viral marketing policy (e.g., a sharing with friends and family button).|studi author develop structur model combin macro diffus model micro choic model control effect social influenc mobil app choic custom app store social influenc refer densiti adopt within proxim custom use larg data set african app store bayesian estim method author quantifi effect social influenc investig impact ignor process estim custom choic find show custom choic app store explain better offlin onlin densiti adopt ignor social influenc estim result bias estim furthermor find show mobil app adopt process similar adopt music cds among classic economi good counterfactu analysi show app store increas revenu viral market polici share friend famili button|['Meisam Hejazi Nia', 'Brian T. Ratchford', 'Norris Bruce']|['stat.ML', 'cs.SI', 'stat.AP']
2017-03-28T14:03:04Z|2017-02-22T02:21:02Z|http://arxiv.org/abs/1702.06650v1|http://arxiv.org/pdf/1702.06650v1|Reducing the uncertainty in the forest volume-to-biomass relationship   built from limited field plots|reduc uncertainti forest volum biomass relationship built limit field plot|The method of biomass estimation based on a volume-to-biomass relationship has been applied in estimating forest biomass conventionally through the mean volume (m3 ha-1). However, few studies have been reported concerning the verification of the volume-biomass equations regressed using field data. The possible bias may result from the volume measurements and extrapolations from sample plots to stands or a unit area. This paper addresses (i) how to verify the volume-biomass equations, and (ii) how to reduce the bias while building these equations. This paper presents an applicable method for verifying the field data using reasonable wood densities, restricting the error in field data processing based on limited field plots, and achieving a better understanding of the uncertainty in building those equations. The verified and improved volume-biomass equations are more reliable and will help to estimate forest carbon sequestration and carbon balance at any large scale.|method biomass estim base volum biomass relationship appli estim forest biomass convent mean volum ha howev studi report concern verif volum biomass equat regress use field data possibl bias may result volum measur extrapol sampl plot stand unit area paper address verifi volum biomass equat ii reduc bias build equat paper present applic method verifi field data use reason wood densiti restrict error field data process base limit field plot achiev better understand uncertainti build equat verifi improv volum biomass equat reliabl help estim forest carbon sequestr carbon balanc ani larg scale|['Caixia Liu', 'Xiaolu Zhou', 'Xiangdong Lei', 'Huabing Huang', 'Changhui Peng', 'Xiaoyi Wang', 'Jianfeng Sun', 'Carl Zhou']|['stat.AP', 'q-bio.QM']
2017-03-28T14:03:04Z|2017-02-21T18:32:22Z|http://arxiv.org/abs/1702.06512v1|http://arxiv.org/pdf/1702.06512v1|Semiparametric panel data models using neural networks|semiparametr panel data model use neural network|This paper presents an estimator for semiparametric models that uses a feed-forward neural network to fit the nonparametric component. Unlike many methodologies from the machine learning literature, this approach is suitable for longitudinal/panel data. It provides unbiased estimation of the parametric component of the model, with associated confidence intervals that have near-nominal coverage rates. It is further shown that this model and estimator nests a nonparametric heterogeneous treatment effects model and estimator, which can consistently estimate individualized treatment effects conditional on covariates. Simulations demonstrate (1) efficiency, (2) that parametric estimates are unbiased, and (3) coverage properties of estimated intervals. An application section demonstrates the method by predicting county-level corn yield using daily weather data from the period 1981-2015, along with parametric time trends representing technological change. The method is shown to out-perform linear methods such as OLS and ridge/lasso, as well as random forest. The procedures described in this paper are implemented in the R package panelNNET.|paper present estim semiparametr model use feed forward neural network fit nonparametr compon unlik mani methodolog machin learn literatur approach suitabl longitudin panel data provid unbias estim parametr compon model associ confid interv near nomin coverag rate shown model estim nest nonparametr heterogen treatment effect model estim consist estim individu treatment effect condit covari simul demonstr effici parametr estim unbias coverag properti estim interv applic section demonstr method predict counti level corn yield use daili weather data period along parametr time trend repres technolog chang method shown perform linear method ol ridg lasso well random forest procedur describ paper implement packag panelnnet|['Andrew Crane-Droesch']|['stat.AP']
2017-03-28T14:03:04Z|2017-02-21T01:38:28Z|http://arxiv.org/abs/1703.00398v1|http://arxiv.org/pdf/1703.00398v1|Analysis on Cohort Effects in view of Differential Geometry and its   Applications|analysi cohort effect view differenti geometri applic|This paper analyzes birth cohort effects and develops an approach which is based on differential geometry to identify and measure cohort effects in mortality data sets. The measurement is quantitative and provides a potential method to compare cohort effects among different countries or groups. Data sets of four countries (e.g. U.k., U.S., Canada and Japan) are taken as examples to explain our approach and applications of the measurement to a modified Lee-Carter model are analyzed. In fact, this paper is an upgrade version of our paper arXiv:1504.00327. There is a new section which gives applications of our approach based on the Lee-Carter and APC models.|paper analyz birth cohort effect develop approach base differenti geometri identifi measur cohort effect mortal data set measur quantit provid potenti method compar cohort effect among differ countri group data set four countri canada japan taken exampl explain approach applic measur modifi lee carter model analyz fact paper upgrad version paper arxiv new section give applic approach base lee carter apc model|['Ning Zhang', 'Liang Zhao']|['stat.AP', '91G80, 91B30']
2017-03-28T14:03:04Z|2017-02-19T10:08:16Z|http://arxiv.org/abs/1702.05732v1|http://arxiv.org/pdf/1702.05732v1|Low-dose cryo electron ptychography via non-convex Bayesian optimization|low dose cryo electron ptychographi via non convex bayesian optim|Electron ptychography has seen a recent surge of interest for phase sensitive imaging at atomic or near-atomic resolution. However, applications are so far mainly limited to radiation-hard samples because the required doses are too high for imaging biological samples at high resolution. We propose the use of non-convex, Bayesian optimization to overcome this problem and reduce the dose required for successful reconstruction by two orders of magnitude compared to previous experiments. We suggest to use this method for imaging single biological macromolecules at cryogenic temperatures and demonstrate 2D single-particle reconstructions from simulated data with a resolution of 7.9 \AA$\,$ at a dose of 20 $e^- / \AA^2$. When averaging over only 15 low-dose datasets, a resolution of 4 \AA$\,$ is possible for large macromolecular complexes. With its independence from microscope transfer function, direct recovery of phase contrast and better scaling of signal-to-noise ratio, cryo-electron ptychography may become a promising alternative to Zernike phase-contrast microscopy.|electron ptychographi seen recent surg interest phase sensit imag atom near atom resolut howev applic far main limit radiat hard sampl becaus requir dose high imag biolog sampl high resolut propos use non convex bayesian optim overcom problem reduc dose requir success reconstruct two order magnitud compar previous experi suggest use method imag singl biolog macromolecul cryogen temperatur demonstr singl particl reconstruct simul data resolut aa dose aa averag onli low dose dataset resolut aa possibl larg macromolecular complex independ microscop transfer function direct recoveri phase contrast better scale signal nois ratio cryo electron ptychographi may becom promis altern zernik phase contrast microscopi|['Philipp Michael Pelz', 'Wen Xuan Qiu', 'Robert Bücker', 'Günther Kassier', 'R. J. Dwayne Miller']|['physics.comp-ph', 'math.OC', 'physics.data-an', 'stat.AP']
2017-03-28T14:03:04Z|2017-03-20T19:49:02Z|http://arxiv.org/abs/1702.05698v2|http://arxiv.org/pdf/1702.05698v2|Online Robust Principal Component Analysis with Change Point Detection|onlin robust princip compon analysi chang point detect|Robust PCA methods are typically batch algorithms which requires loading all observations into memory before processing. This makes them inefficient to process big data. In this paper, we develop an efficient online robust principal component methods, namely online moving window robust principal component analysis (OMWRPCA). Unlike existing algorithms, OMWRPCA can successfully track not only slowly changing subspace but also abruptly changed subspace. By embedding hypothesis testing into the algorithm, OMWRPCA can detect change points of the underlying subspaces. Extensive simulation studies demonstrate the superior performance of OMWRPCA compared with other state-of-art approaches. We also apply the algorithm for real-time background subtraction of surveillance video.|robust pca method typic batch algorithm requir load observ memori befor process make ineffici process big data paper develop effici onlin robust princip compon method name onlin move window robust princip compon analysi omwrpca unlik exist algorithm omwrpca success track onli slowli chang subspac also abrupt chang subspac embed hypothesi test algorithm omwrpca detect chang point subspac extens simul studi demonstr superior perform omwrpca compar state art approach also appli algorithm real time background subtract surveil video|['Wei Xiao', 'Xiaolin Huang', 'Jorge Silva', 'Saba Emrani', 'Arin Chaudhuri']|['cs.LG', 'cs.CV', 'stat.AP', 'stat.CO', 'stat.ML']
2017-04-07T11:23:40Z|2017-04-05T15:02:26Z|http://arxiv.org/abs/1704.01469v1|http://arxiv.org/pdf/1704.01469v1|Notes on Creating a Standardized Version of DVARS|note creat standard version dvar|By constructing a sampling distribution for DVARS we can create a standardized version of DVARS that should be more similar across scanners and datasets.|construct sampl distribut dvar creat standard version dvar similar across scanner dataset|['Thomas E. Nichols']|['stat.AP']
2017-04-07T11:23:40Z|2017-04-04T23:47:41Z|http://arxiv.org/abs/1704.01220v1|http://arxiv.org/pdf/1704.01220v1|Perceived Performance of Webpages In the Wild: Insights from Large-scale   Crowdsourcing of Above-the-Fold QoE|perceiv perform webpag wild insight larg scale crowdsourc abov fold qoe|"Clearly, no one likes webpages with poor quality of experience (QoE). Being perceived as slow or fast is a key element in the overall perceived QoE of web applications. While extensive effort has been put into optimizing web applications (both in industry and academia), not a lot of work exists in characterizing what aspects of webpage loading process truly influence human end-user's perception of the ""Speed"" of a page. In this paper we present ""SpeedPerception"", a large-scale web performance crowdsourcing framework focused on understanding the perceived loading performance of above-the-fold (ATF) webpage content. Our end goal is to create free open-source benchmarking datasets to advance the systematic analysis of how humans perceive webpage loading process. In Phase-1 of our ""SpeedPerception"" study using Internet Retailer Top 500 (IR 500) websites (https://github.com/pahammad/speedperception), we found that commonly used navigation metrics such as ""onLoad"" and ""Time To First Byte (TTFB)"" fail (less than 60% match) to represent majority human perception when comparing the speed of two webpages. We present a simple 3-variable-based machine learning model that explains the majority end-user choices better (with $87 \pm 2\%$ accuracy). In addition, our results suggest that the time needed by end-users to evaluate relative perceived speed of webpage is far less than the time of its ""visualComplete"" event."|clear one like webpag poor qualiti experi qoe perceiv slow fast key element overal perceiv qoe web applic extens effort put optim web applic industri academia lot work exist character aspect webpag load process truli influenc human end user percept speed page paper present speedpercept larg scale web perform crowdsourc framework focus understand perceiv load perform abov fold atf webpag content end goal creat free open sourc benchmark dataset advanc systemat analysi human perceiv webpag load process phase speedpercept studi use internet retail top ir websit https github com pahammad speedpercept found common use navig metric onload time first byte ttfb fail less match repres major human percept compar speed two webpag present simpl variabl base machin learn model explain major end user choic better pm accuraci addit result suggest time need end user evalu relat perceiv speed webpag far less time visualcomplet event|['Qingzhu Gao', 'Prasenjit Dey', 'Parvez Ahammad']|['cs.NI', 'cs.HC', 'stat.AP']
2017-04-07T11:23:40Z|2017-04-04T22:34:49Z|http://arxiv.org/abs/1704.01207v1|http://arxiv.org/pdf/1704.01207v1|Bayesian Model Averaging for the X-Chromosome Inactivation Dilemma in   Genetic Association Study|bayesian model averag chromosom inactiv dilemma genet associ studi|X-chromosome is often excluded from the so called `whole-genome' association studies due to its intrinsic difference between males and females. One particular analytical challenge is the unknown status of X-inactivation, where one of the two X-chromosome variants in females may be randomly selected to be silenced. In the absence of biological evidence in favour of one specific model, we consider a Bayesian model averaging framework that offers a principled way to account for the inherent model uncertainty, providing BMA-based posterior density intervals and Bayes factors. We examine the inferential properties of the proposed methods via extensive simulations and an association study of meconium ileus, an intestinal disease occurring in about twenty percent of Cystic Fibrosis patients. Compared with results previously reported assuming the presence of X-inactivation, we show that the proposed Bayesian methods provide more feature-rich quantities that are useful in practice.|chromosom often exclud call whole genom associ studi due intrins differ male femal one particular analyt challeng unknown status inactiv one two chromosom variant femal may random select silenc absenc biolog evid favour one specif model consid bayesian model averag framework offer principl way account inher model uncertainti provid bma base posterior densiti interv bay factor examin inferenti properti propos method via extens simul associ studi meconium ileus intestin diseas occur twenti percent cystic fibrosi patient compar result previous report assum presenc inactiv show propos bayesian method provid featur rich quantiti use practic|['Bo Chen', 'Radu', 'V. Craiu', 'Lei Sun']|['stat.AP']
2017-04-07T11:23:40Z|2017-04-04T11:26:40Z|http://arxiv.org/abs/1704.00959v1|http://arxiv.org/pdf/1704.00959v1|Using clustering of rankings to explain brand preferences with   personality and socio-demographic variables|use cluster rank explain brand prefer person socio demograph variabl|The primary aim of market segmentation is to identify relevant groups of consumers that can be addressed efficiently by marketing or advertising campaigns. This paper addresses the issue whether consumer groups can be identified from background variables that are not brand-related and how much personality vs. socio-demographic variables contribute to the identification of consumer clusters. This is done by clustering aggregated preferences for 25 brands across 5 different product categories, and by relating socio-demographic and personality variables to the clusters using logistic regression and random forests over a range of different numbers of clusters. Results indicate that some personality variables contribute significantly to the identification of consumer groups in one sample. However, these results were not replicated on a second sample that was more heterogeneous in terms of socio-demographic characteristics and not representative of the brands' target audience.|primari aim market segment identifi relev group consum address effici market advertis campaign paper address issu whether consum group identifi background variabl brand relat much person vs socio demograph variabl contribut identif consum cluster done cluster aggreg prefer brand across differ product categori relat socio demograph person variabl cluster use logist regress random forest rang differ number cluster result indic person variabl contribut signific identif consum group one sampl howev result replic second sampl heterogen term socio demograph characterist repres brand target audienc|['Daniel Müllensiefen', 'Christian Hennig', 'Hedie Howells']|['stat.AP', '62H30, 91B08']
2017-04-07T11:23:40Z|2017-04-04T10:59:25Z|http://arxiv.org/abs/1704.00953v1|http://arxiv.org/pdf/1704.00953v1|Stress Testing German Industry Sectors: Results from a Vine Copula Based   Quantile Regression|stress test german industri sector result vine copula base quantil regress|Measuring interdependence between probabilities of default (PDs) in different industry sectors of an economy plays a crucial role in financial stress testing. Thereby, regression approaches may be employed to model the impact of stressed industry sectors as covariates on other response sectors. We identify vine copula based quantile regression as an eligible tool for conducting such stress tests as this method has good robustness properties, takes into account potential nonlinearities of conditional quantile functions and ensures that no quantile crossing effects occur. We illustrate its performance by a data set of sector specific PDs for the German economy. Empirical results are provided for a rough and a fine-grained industry sector classification scheme. Amongst others, we confirm that a stressed automobile industry has a severe impact on the German economy as a whole at different quantile levels whereas e.g., for a stressed financial sector the impact is rather moderate. Moreover, the vine copula based quantile regression approach is benchmarked against both classical linear quantile regression and expectile regression in order to illustrate its methodological effectiveness in the scenarios evaluated.|measur interdepend probabl default pds differ industri sector economi play crucial role financi stress test therebi regress approach may employ model impact stress industri sector covari respons sector identifi vine copula base quantil regress elig tool conduct stress test method good robust properti take account potenti nonlinear condit quantil function ensur quantil cross effect occur illustr perform data set sector specif pds german economi empir result provid rough fine grain industri sector classif scheme amongst confirm stress automobil industri sever impact german economi whole differ quantil level wherea stress financi sector impact rather moder moreov vine copula base quantil regress approach benchmark classic linear quantil regress expectil regress order illustr methodolog effect scenario evalu|['Matthias Fischer', 'Daniel Kraus', 'Marius Pfeuffer']|['stat.AP']
2017-04-07T11:23:40Z|2017-04-03T22:40:48Z|http://arxiv.org/abs/1704.00829v1|http://arxiv.org/pdf/1704.00829v1|Online deforestation detection|onlin deforest detect|Deforestation detection using satellite images can make an important contribution to forest management. Current approaches can be broadly divided into those that compare two images taken at similar periods of the year and those that monitor changes by using multiple images taken during the growing season. The CMFDA algorithm described in Zhu et al. (2012) is an algorithm that builds on the latter category by implementing a year-long, continuous, time-series based approach to monitoring images. This algorithm was developed for 30m resolution, 16-day frequency reflectance data from the Landsat satellite. In this work we adapt the algorithm to 1km, 16-day frequency reflectance data from the modis sensor aboard the Terra satellite. The CMFDA algorithm is composed of two submodels which are fitted on a pixel-by-pixel basis. The first estimates the amount of surface reflectance as a function of the day of the year. The second estimates the occurrence of a deforestation event by comparing the last few predicted and real reflectance values. For this comparison, the reflectance observations for six different bands are first combined into a forest index. Real and predicted values of the forest index are then compared and high absolute differences for consecutive observation dates are flagged as deforestation events. Our adapted algorithm also uses the two model framework. However, since the modis 13A2 dataset used, includes reflectance data for different spectral bands than those included in the Landsat dataset, we cannot construct the forest index. Instead we propose two contrasting approaches: a multivariate and an index approach similar to that of CMFDA.|deforest detect use satellit imag make import contribut forest manag current approach broad divid compar two imag taken similar period year monitor chang use multipl imag taken dure grow season cmfda algorithm describ zhu et al algorithm build latter categori implement year long continu time seri base approach monitor imag algorithm develop resolut day frequenc reflect data landsat satellit work adapt algorithm km day frequenc reflect data modi sensor aboard terra satellit cmfda algorithm compos two submodel fit pixel pixel basi first estim amount surfac reflect function day year second estim occurr deforest event compar last predict real reflect valu comparison reflect observ six differ band first combin forest index real predict valu forest index compar high absolut differ consecut observ date flag deforest event adapt algorithm also use two model framework howev sinc modi dataset use includ reflect data differ spectral band includ landsat dataset cannot construct forest index instead propos two contrast approach multivari index approach similar cmfda|['Emiliano Diaz']|['stat.AP', 'cs.CV']
2017-04-07T11:23:40Z|2017-04-03T21:56:29Z|http://arxiv.org/abs/1704.00823v1|http://arxiv.org/pdf/1704.00823v1|SAFE2: A Hierarchical Model of Pitch Framing|safe hierarch model pitch frame|"Since the advent of high-resolution pitch tracking data (PITCHf/x), many in the sabermetrics community have attempted to quantify a Major League Baseball catcher's ability to ""frame"" a pitch (i.e. increase the chance that a pitch is called as a strike). Especially in the last three years, there has been an explosion of interest in the ""art of pitch framing"" in the popular press as well as signs that teams are considering framing when making roster decisions.   We introduce a Bayesian hierarchical model to estimate each umpire's probability of calling a strike, adjusting for pitch participants, pitch location, and contextual information like the count. Using our model, we can estimate each catcher's effect on an umpire's chance of calling a strike.We are then able to translate these estimated effects into average runs saved across a season. We also introduce a new metric, analogous to Jensen, Shirley, and Wyner's Spatially Aggregate Fielding Evaluation metric, which provides a more honest assessment of the impact of framing."|sinc advent high resolut pitch track data pitchf mani sabermetr communiti attempt quantifi major leagu basebal catcher abil frame pitch increas chanc pitch call strike especi last three year explos interest art pitch frame popular press well sign team consid frame make roster decis introduc bayesian hierarch model estim umpir probabl call strike adjust pitch particip pitch locat contextu inform like count use model estim catcher effect umpir chanc call strike abl translat estim effect averag run save across season also introduc new metric analog jensen shirley wyner spatial aggreg field evalu metric provid honest assess impact frame|['Sameer K. Deshpande', 'Abraham J. Wyner']|['stat.AP']
2017-04-07T11:23:40Z|2017-04-03T16:25:19Z|http://arxiv.org/abs/1704.00665v1|http://arxiv.org/pdf/1704.00665v1|Investigating consumers' store-choice behavior via hierarchical variable   selection|investig consum store choic behavior via hierarch variabl select|This paper is concerned with a store-choice model for investigating consumers' store-choice behavior based on scanner panel data. Our store-choice model enables us to evaluate the effects of the consumer/product attributes not only on the consumer's store choice but also on his/her purchase quantity. Moreover, we adopt a mixed-integer optimization (MIO) approach to selecting the best set of explanatory variables with which to construct a store-choice model. We devise two MIO models for hierarchical variable selection in which the hierarchical structure of product categories is used to enhance the reliability and computational efficiency of the variable selection. We assess the effectiveness of our MIO models through computational experiments on actual scanner panel data. These experiments are focused on the consumer's choice among three types of stores in Japan: convenience stores, drugstores, and grocery supermarkets. The computational results demonstrate that our method has several advantages over the common methods for variable selection, namely, the stepwise method and $L_1$-regularized regression. Furthermore, our analysis reveals that convenience stores tend to be chosen because of accessibility, drugstores are chosen for the purchase of specific products at low prices, and grocery supermarkets are chosen for health food products by women with families.|paper concern store choic model investig consum store choic behavior base scanner panel data store choic model enabl us evalu effect consum product attribut onli consum store choic also purchas quantiti moreov adopt mix integ optim mio approach select best set explanatori variabl construct store choic model devis two mio model hierarch variabl select hierarch structur product categori use enhanc reliabl comput effici variabl select assess effect mio model comput experi actual scanner panel data experi focus consum choic among three type store japan conveni store drugstor groceri supermarket comput result demonstr method sever advantag common method variabl select name stepwis method regular regress furthermor analysi reveal conveni store tend chosen becaus access drugstor chosen purchas specif product low price groceri supermarket chosen health food product women famili|['Toshiki Sato', 'Yuichi Takano', 'Takanobu Nakahara']|['stat.AP']
2017-04-07T11:23:40Z|2017-04-03T13:51:50Z|http://arxiv.org/abs/1704.00588v1|http://arxiv.org/pdf/1704.00588v1|Causality and surrogate variable analysis|causal surrog variabl analysi|Gene expression depends on thousands of factors and we usually only have access to tens or hundreds of observations of gene expression levels meaning we are in a high-dimensional setting. Additionally we don't always observe or care about all the factors. However, many different gene expression levels depend on a set of common factors. By observing the joint variance of the gene expression levels together with the observed primary variables (those we care about) Surrogate Variable Analysis (SVA) seeks to estimate the remaining unobserved factors. The ultimate goal is to assess whether the primary variable (or vector) has a significant effect on the different gene expression levels, but without estimating unobserved factors first the various regression models and hypothesis tests are dependent which complicates significance analysis. In this work we define a class of additive gene expression structural equation models (SEMs) which are convenient for modeling gene expression data and which provides a useful framework to understand the various steps of the SVA methodology. We justify the use of this class from a modeling viewpoint but also from a causality viewpoint by exploring the independence and causality properties of this class and comparing to the biologically driven data assumptions. For this we use some of the theory that has been developed elsewhere on graphical models and causality. We then give a detailed description of the SVA methodology and its implementation in the R package sva referring each step to different parts of the additive gene expression SEM defined previously.|gene express depend thousand factor usual onli access ten hundr observ gene express level mean high dimension set addit alway observ care factor howev mani differ gene express level depend set common factor observ joint varianc gene express level togeth observ primari variabl care surrog variabl analysi sva seek estim remain unobserv factor ultim goal assess whether primari variabl vector signific effect differ gene express level without estim unobserv factor first various regress model hypothesi test depend complic signific analysi work defin class addit gene express structur equat model sem conveni model gene express data provid use framework understand various step sva methodolog justifi use class model viewpoint also causal viewpoint explor independ causal properti class compar biolog driven data assumpt use theori develop elsewher graphic model causal give detail descript sva methodolog implement packag sva refer step differ part addit gene express sem defin previous|['Emiliano Diaz']|['stat.AP']
2017-04-07T11:23:40Z|2017-04-03T13:49:25Z|http://arxiv.org/abs/1704.00587v1|http://arxiv.org/pdf/1704.00587v1|Analysis, detection and correction of misspecified discrete time state   space models|analysi detect correct misspecifi discret time state space model|Misspecifications (i.e. errors on the parameters) of state space models lead to incorrect inference of the hidden states. This paper studies weakly nonlin-ear state space models with additive Gaussian noises and proposes a method for detecting and correcting misspecifications. The latter induce a biased estimator of the hidden state but also happen to induce correlation on innovations and other residues. This property is used to find a well-defined objective function for which an optimisation routine is applied to recover the true parameters of the model. It is argued that this method can consistently estimate the bias on the parameter. We demonstrate the algorithm on various models of increasing complexity.|misspecif error paramet state space model lead incorrect infer hidden state paper studi weak nonlin ear state space model addit gaussian nois propos method detect correct misspecif latter induc bias estim hidden state also happen induc correl innov residu properti use find well defin object function optimis routin appli recov true paramet model argu method consist estim bias paramet demonstr algorithm various model increas complex|['Salima El Kolei', 'Frédéric Patras']|['stat.AP', 'stat.CO']
2017-04-07T11:23:44Z|2017-04-03T13:35:17Z|http://arxiv.org/abs/1704.00575v1|http://arxiv.org/pdf/1704.00575v1|Sparse mean localization by information theory|spars mean local inform theori|Sparse feature selection is necessary when we fit statistical models, we have access to a large group of features, don't know which are relevant, but assume that most are not. Alternatively, when the number of features is larger than the available data the model becomes over parametrized and the sparse feature selection task involves selecting the most informative variables for the model. When the model is a simple location model and the number of relevant features does not grow with the total number of features, sparse feature selection corresponds to sparse mean estimation. We deal with a simplified mean estimation problem consisting of an additive model with gaussian noise and mean that is in a restricted, finite hypothesis space. This restriction simplifies the mean estimation problem into a selection problem of combinatorial nature. Although the hypothesis space is finite, its size is exponential in the dimension of the mean. In limited data settings and when the size of the hypothesis space depends on the amount of data or on the dimension of the data, choosing an approximation set of hypotheses is a desirable approach. Choosing a set of hypotheses instead of a single one implies replacing the bias-variance trade off with a resolution-stability trade off. Generalization capacity provides a resolution selection criterion based on allowing the learning algorithm to communicate the largest amount of information in the data to the learner without error. In this work the theory of approximation set coding and generalization capacity is explored in order to understand this approach. We then apply the generalization capacity criterion to the simplified sparse mean estimation problem and detail an importance sampling algorithm which at once solves the difficulty posed by large hypothesis spaces and the slow convergence of uniform sampling algorithms.|spars featur select necessari fit statist model access larg group featur know relev assum altern number featur larger avail data model becom parametr spars featur select task involv select inform variabl model model simpl locat model number relev featur doe grow total number featur spars featur select correspond spars mean estim deal simplifi mean estim problem consist addit model gaussian nois mean restrict finit hypothesi space restrict simplifi mean estim problem select problem combinatori natur although hypothesi space finit size exponenti dimens mean limit data set size hypothesi space depend amount data dimens data choos approxim set hypothes desir approach choos set hypothes instead singl one impli replac bias varianc trade resolut stabil trade general capac provid resolut select criterion base allow learn algorithm communic largest amount inform data learner without error work theori approxim set code general capac explor order understand approach appli general capac criterion simplifi spars mean estim problem detail import sampl algorithm onc solv difficulti pose larg hypothesi space slow converg uniform sampl algorithm|['Emiliano Diaz']|['stat.AP', 'cs.IT', 'math.IT']
2017-04-07T11:23:44Z|2017-04-03T12:09:19Z|http://arxiv.org/abs/1704.00543v1|http://arxiv.org/pdf/1704.00543v1|Mixture Hidden Markov Models for Sequence Data: The seqHMM Package in R|mixtur hidden markov model sequenc data seqhmm packag|Sequence analysis is being more and more widely used for the analysis of social sequences and other multivariate categorical time series data. However, it is often complex to describe, visualize, and compare large sequence data, especially when there are multiple parallel sequences per subject. Hidden (latent) Markov models (HMMs) are able to detect underlying latent structures and they can be used in various longitudinal settings: to account for measurement error, to detect unobservable states, or to compress information across several types of observations. Extending to mixture hidden Markov models (MHMMs) allows clustering data into homogeneous subsets, with or without external covariates. The seqHMM package in R is designed for the efficient modeling of sequences and other categorical time series data containing one or multiple subjects with one or multiple interdependent sequences using HMMs and MHMMs. Also other restricted variants of the MHMM can be fitted, e.g., latent class models, Markov models, mixture Markov models, or even ordinary multinomial regression models with suitable parameterization of the HMM. Good graphical presentations of data and models are useful during the whole analysis process from the first glimpse at the data to model fitting and presentation of results. The package provides easy options for plotting parallel sequence data, and proposes visualizing HMMs as directed graphs.|sequenc analysi wide use analysi social sequenc multivari categor time seri data howev often complex describ visual compar larg sequenc data especi multipl parallel sequenc per subject hidden latent markov model hmms abl detect latent structur use various longitudin set account measur error detect unobserv state compress inform across sever type observ extend mixtur hidden markov model mhmms allow cluster data homogen subset without extern covari seqhmm packag design effici model sequenc categor time seri data contain one multipl subject one multipl interdepend sequenc use hmms mhmms also restrict variant mhmm fit latent class model markov model mixtur markov model even ordinari multinomi regress model suitabl parameter hmm good graphic present data model use dure whole analysi process first glimps data model fit present result packag provid easi option plot parallel sequenc data propos visual hmms direct graph|['Satu Helske', 'Jouni Helske']|['stat.CO', 'stat.AP']
2017-04-07T11:23:44Z|2017-04-03T10:50:03Z|http://arxiv.org/abs/1704.00522v1|http://arxiv.org/pdf/1704.00522v1|Clustered multi-state models with observation-level random effects,   mover-stayer effects and dynamic covariates: Modelling transition intensities   and sojourn times in a study of psoriatic arthritis|cluster multi state model observ level random effect mover stayer effect dynam covari model transit intens sojourn time studi psoriat arthriti|In psoriatic arthritis, it is important to understand the joint activity (represented by swelling and pain) and damage processes because both are related to severe physical disability. This paper aims to provide a comprehensive investigation in to both processes occurring over time, in particular their relationship, by specifying a joint multi-state model at the individual hand joint-level, which also accounts for many of their important features. As there are multiple hand joints, such an analysis will be based on the use of clustered multi-state models. Here we consider an observation-level random effects structure with dynamic covariates and allow for the possibility that a subpopulation of patients are at minimal risk of damage. Such an analysis is found to provide further understanding of the activity-damage relationship beyond that provided by previous analyses. Consideration is also given to the modelling of mean sojourn times and jump probabilities. In particular, a novel model parameterization which allows easily interpretable covariate effects to act on these quantities is proposed.|psoriat arthriti import understand joint activ repres swell pain damag process becaus relat sever physic disabl paper aim provid comprehens investig process occur time particular relationship specifi joint multi state model individu hand joint level also account mani import featur multipl hand joint analysi base use cluster multi state model consid observ level random effect structur dynam covari allow possibl subpopul patient minim risk damag analysi found provid understand activ damag relationship beyond provid previous analys consider also given model mean sojourn time jump probabl particular novel model parameter allow easili interpret covari effect act quantiti propos|['Sean Yiu', 'Vernon T. Farewell', 'Brian D. M. Tom']|['stat.AP']
2017-04-07T11:23:44Z|2017-04-03T06:05:44Z|http://arxiv.org/abs/1704.00436v1|http://arxiv.org/pdf/1704.00436v1|Multi-frequency sparse Bayesian learning with uncertainty models|multi frequenc spars bayesian learn uncertainti model|Sparse Bayesian learning (SBL) has emerged as a fast and competitive method to perform sparse processing. The SBL algorithm, which is developed using a Bayesian framework, approximately solves a non-convex optimization problem using fixed point updates. It provides comparable performance and is significantly faster than convex optimization techniques used in sparse processing. In this paper we propose signal model which accounts for dictionary mismatch and the presence of spurious peaks at low signal-to-noise ratios. As an extension of SBL, modified fixed point update equation is derived which incorporates the statistics of mismatch and spurious peaks. We also extend the SBL algorithm to process multi-frequency observations. The derived update equations are studied quantitatively using beamforming simulations applied to direction-of-arrival (DoA) estimation. Performance of SBL using single frequency and multi-frequency observations, and in the presence of aliasing is evaluated. Data collected from the SwellEx-96 experiment is used to demonstrate qualitatively the advantages of SBL.|spars bayesian learn sbl emerg fast competit method perform spars process sbl algorithm develop use bayesian framework approxim solv non convex optim problem use fix point updat provid compar perform signific faster convex optim techniqu use spars process paper propos signal model account dictionari mismatch presenc spurious peak low signal nois ratio extens sbl modifi fix point updat equat deriv incorpor statist mismatch spurious peak also extend sbl algorithm process multi frequenc observ deriv updat equat studi quantit use beamform simul appli direct arriv doa estim perform sbl use singl frequenc multi frequenc observ presenc alias evalu data collect swellex experi use demonstr qualit advantag sbl|['Santosh Nannuru', 'Kay L. Gemba', 'Peter Gerstoft', 'William S. Hodgkiss', 'Christoph Mecklenbräuker']|['stat.AP', 'cs.IT', 'math.IT']
2017-04-07T11:23:44Z|2017-04-02T19:12:26Z|http://arxiv.org/abs/1704.00352v1|http://arxiv.org/pdf/1704.00352v1|Simple Measures of Individual Cluster-Membership Certainty for Hard   Partitional Clustering|simpl measur individu cluster membership certainti hard partit cluster|We propose two posterior-probability-like measures of individual cluster-membership certainty which can be applied to a hard partition of the sample such as that obtained from the Partitioning Around Medoids (PAM) algorithm. One measure extends the individual silhouette widths and the other is obtained directly from the pairwise dissimilarities in the sample. Unlike the classic silhouette, however, the measures behave like probabilities and can be used to investigate an individual's tendency to belong to a cluster. Motivated by an application to a clinical database, we evaluate the performance of both measures in individuals with ambiguous cluster membership, using simulated binary datasets that have been partitioned by the PAM algorithm. For comparison, we also present results from soft clustering algorithms such as soft analysis clustering (FANNY) and two model-based clustering methods. Our proposed measures perform comparably to the posterior-probability estimators from either FANNY or the model-based clustering methods.|propos two posterior probabl like measur individu cluster membership certainti appli hard partit sampl obtain partit around medoid pam algorithm one measur extend individu silhouett width obtain direct pairwis dissimilar sampl unlik classic silhouett howev measur behav like probabl use investig individu tendenc belong cluster motiv applic clinic databas evalu perform measur individu ambigu cluster membership use simul binari dataset partit pam algorithm comparison also present result soft cluster algorithm soft analysi cluster fanni two model base cluster method propos measur perform compar posterior probabl estim either fanni model base cluster method|['Dongmeng Liu', 'Jinko Graham']|['stat.AP']
2017-04-07T11:23:44Z|2017-04-02T19:09:39Z|http://arxiv.org/abs/1704.00351v1|http://arxiv.org/pdf/1704.00351v1|Two-exponential models of gene expression patterns for noisy   experimental data|two exponenti model gene express pattern noisi experiment data|Motivation: Spatial pattern formation of the primary anterior-posterior morphogenetic gradient of the transcription factor Bicoid (Bcd) has been studied experimentally and computationally for many years. Bcd specifies positional information for the downstream segmentation genes, affecting the fly body plan. More recently, a number of researchers have focused on the patterning dynamics of the underlying bcd mRNA gradient, which is translated into Bcd protein. New, more accurate techniques for visualizing bcd mRNA need to be combined with quantitative signal extraction techniques to reconstruct the bcd mRNA distribution.   Results: Here, we present a robust technique for quantifying gradients with a two-exponential model. This approach: 1) has natural, biologically relevant parameters; and 2) is invariant to linear transformations of the data which can arise due to variation in experimental conditions (e.g. microscope settings, non-specific background signal). This allows us to quantify bcd mRNA gradient variability from embryo to embryo (important for studying the robustness of developmental regulatory networks); sort out atypical gradients; and classify embryos to developmental stage by quantitative gradient parameters.|motiv spatial pattern format primari anterior posterior morphogenet gradient transcript factor bicoid bcd studi experiment comput mani year bcd specifi posit inform downstream segment gene affect fli bodi plan recent number research focus pattern dynam bcd mrna gradient translat bcd protein new accur techniqu visual bcd mrna need combin quantit signal extract techniqu reconstruct bcd mrna distribut result present robust techniqu quantifi gradient two exponenti model approach natur biolog relev paramet invari linear transform data aris due variat experiment condit microscop set non specif background signal allow us quantifi bcd mrna gradient variabl embryo embryo import studi robust development regulatori network sort atyp gradient classifi embryo development stage quantit gradient paramet|['Theodore Alexandrov', 'Nina Golyandina', 'David Holloway', 'Alex Shlemov', 'Alexander Spirov']|['q-bio.QM', 'stat.AP']
2017-04-07T11:23:44Z|2017-04-02T00:26:20Z|http://arxiv.org/abs/1704.00240v1|http://arxiv.org/pdf/1704.00240v1|Crime Prediction by Data-Driven Green's Function method|crime predict data driven green function method|We present an algorithm for crime prediction based on the near-repeat victimization model solved by a Green's function scheme. The Green's function is generated from spatio-temporal correlations of a density of crime events in a historical dataset. We examine the accuracy of our method by applying it to the open data of burglaries in Chicago and New York City. We find that the cascade of the crimes has a long-time, logarithmic tail, which is consistent with an earlier study on other data of burglaries. The presented method is a powerful tool not only to predict crimes but also to analyze their correlations, because the Green's function can describe how a past crime influences the future events.|present algorithm crime predict base near repeat victim model solv green function scheme green function generat spatio tempor correl densiti crime event histor dataset examin accuraci method appli open data burglari chicago new york citi find cascad crime long time logarithm tail consist earlier studi data burglari present method power tool onli predict crime also analyz correl becaus green function describ past crime influenc futur event|['Mami Kajita', 'Seiji Kajita']|['stat.AP', 'cs.CE', 'physics.soc-ph']
2017-04-07T11:23:44Z|2017-04-01T20:24:52Z|http://arxiv.org/abs/1704.00224v1|http://arxiv.org/pdf/1704.00224v1|A Time-Frequency Domain Approach of Heart Rate Estimation From   Photoplethysmographic (PPG) Signal|time frequenc domain approach heart rate estim photoplethysmograph ppg signal|Objective- Heart rate monitoring using wrist type Photoplethysmographic (PPG) signals is getting popularity because of construction simplicity and low cost of wearable devices. The task becomes very difficult due to the presence of various motion artifacts. The objective is to develop algorithms to reduce the effect of motion artifacts and thus obtain accurate heart rate estimation. Methods- Proposed heart rate estimation scheme utilizes both time and frequency domain analyses. Unlike conventional single stage adaptive filter, multi-stage cascaded adaptive filtering is introduced by using three channel accelerometer data to reduce the effect of motion artifacts. Both recursive least squares (RLS) and least mean squares (LMS) adaptive filters are tested. Moreover, singular spectrum analysis (SSA) is employed to obtain improved spectral peak tracking. The outputs from the filter block and SSA operation are logically combined and used for spectral domain heart rate estimation. Finally, a tracking algorithm is incorporated considering neighbouring estimates. Results- The proposed method provides an average absolute error of 1.16 beat per minute (BPM) with a standard deviation of 1.74 BPM while tested on publicly available database consisting of recordings from 12 subjects during physical activities. Conclusion- It is found that the proposed method provides consistently better heart rate estimation performance in comparison to that recently reported by TROIKA, JOSS and SPECTRAP methods. Significance- The proposed method offers very low estimation error and a smooth heart rate tracking with simple algorithmic approach and thus feasible for implementing in wearable devices to monitor heart rate for fitness and clinical purpose.|object heart rate monitor use wrist type photoplethysmograph ppg signal get popular becaus construct simplic low cost wearabl devic task becom veri difficult due presenc various motion artifact object develop algorithm reduc effect motion artifact thus obtain accur heart rate estim method propos heart rate estim scheme util time frequenc domain analys unlik convent singl stage adapt filter multi stage cascad adapt filter introduc use three channel acceleromet data reduc effect motion artifact recurs least squar rls least mean squar lms adapt filter test moreov singular spectrum analysi ssa employ obtain improv spectral peak track output filter block ssa oper logic combin use spectral domain heart rate estim final track algorithm incorpor consid neighbour estim result propos method provid averag absolut error beat per minut bpm standard deviat bpm test public avail databas consist record subject dure physic activ conclus found propos method provid consist better heart rate estim perform comparison recent report troika joss spectrap method signific propos method offer veri low estim error smooth heart rate track simpl algorithm approach thus feasibl implement wearabl devic monitor heart rate fit clinic purpos|['Mohammad Tariqul Islam', 'Ishman Zabir', 'Sk. Tanvir Ahamed', 'Md. Tahmid Yasar', 'Celia Shahnaz', 'Shaikh Anowarul Fattah']|['stat.AP']
2017-04-07T11:23:44Z|2017-04-01T16:33:39Z|http://arxiv.org/abs/1704.00197v1|http://arxiv.org/pdf/1704.00197v1|iWinrNFL: A Simple and Well-Calibrated In-Game NFL Win Probability Model|iwinrnfl simpl well calibr game nfl win probabl model|"During the last sports season a lot of discussion has been generated for the several, high-profile, ""comebacks"" that were observed in almost all sports. The Cavaliers won the championship after being down 3-1 in the NBA final's series, which was exactly the case for Chicago Cubs and the World Series. The Patriots won the Super Bowl even though they were trailing by 25 points late in the third quarter, while more recently FC Barcelona in the top-16 round of Champions League scored 3 goals during the last 7 minutes of the game (including stoppage time) against PSG to advance in the tournament. This has brought the robustness and accuracy of the various probabilistic prediction models under high scrutiny. In this paper, we discuss some of the issues that have been brought up in the wake of last year's events and we also develop our own in-game win probability model for NFL (iWinrNFL). In particular, we build a simple logistic regression model that utilizes a set of 10 variables to predict the running win probability of the home team. In order to account for non-linearities that exist at the final minutes of the game we enhance the logistic regression model with a radial basis Support Vector Machine model for the last 3 minutes of the regulation. We train our model using detailed play-by-play data from the last 8 NFL seasons obtained through the league's API. Our results indicate that in 74% of the cases iWinrNFL provides an accurate winner projection, while most importantly the probabilities are well-calibrated."|dure last sport season lot discuss generat sever high profil comeback observ almost sport cavali championship nba final seri exact case chicago cub world seri patriot super bowl even though trail point late third quarter recent fc barcelona top round champion leagu score goal dure last minut game includ stoppag time psg advanc tournament brought robust accuraci various probabilist predict model high scrutini paper discuss issu brought wake last year event also develop game win probabl model nfl iwinrnfl particular build simpl logist regress model util set variabl predict run win probabl home team order account non linear exist final minut game enhanc logist regress model radial basi support vector machin model last minut regul train model use detail play play data last nfl season obtain leagu api result indic case iwinrnfl provid accur winner project import probabl well calibr|['Konstantinos Pelechrinis']|['stat.AP']
2017-04-07T11:23:44Z|2017-04-01T14:25:10Z|http://arxiv.org/abs/1704.01932v1|http://arxiv.org/pdf/1704.01932v1|Estimación de la inicial de referencia utilizando simulación|estimaci de la inici de referencia utilizando simulaci|The method proposed by Bernardo and Smith [2000] to approximate reference priors by simulation was analyzed with the objective of improving the procedure in order to obtain consistent estimators and to allow the estimation of asymptotic probability intervals. In this sense, the variance of Bernardo's estimator was derived and was used to construct probability intervals that permitted the expression of the estimation error as a function of sample size. Additionally a variance reduction technique (common random numbers) were explored as a means to obtain more precise estimations with smaller sample sizes. These technique was found to considerably reduce estimation error for some of the examples explored. In other cases the use of the technique resulted in zero estimation error given that the estimator does not depend on the sample.|method propos bernardo smith approxim refer prior simul analyz object improv procedur order obtain consist estim allow estim asymptot probabl interv sens varianc bernardo estim deriv use construct probabl interv permit express estim error function sampl size addit varianc reduct techniqu common random number explor mean obtain precis estim smaller sampl size techniqu found consider reduc estim error exampl explor case use techniqu result zero estim error given estim doe depend sampl|['Emiliano Díaz']|['stat.AP']
2017-04-07T11:23:49Z|2017-03-31T22:29:59Z|http://arxiv.org/abs/1704.00076v1|http://arxiv.org/pdf/1704.00076v1|A multivariate variable selection approach for analyzing LC-MS   metabolomics data|multivari variabl select approach analyz lc ms metabolom data|Omic data are characterized by the presence of strong dependence structures that result either from data acquisition or from some underlying biological processes. In metabolomics, for instance, data resulting from Liquid Chromatography-Mass Spectrometry (LC-MS) -- a technique which gives access to a large coverage of metabolites -- exhibit such patterns. These data sets are typically used to find the metabolites characterizing a phenotype of interest associated with the samples. However, applying some statistical procedures that do not adjust the variable selection step to the dependence pattern may result in a loss of power and the selection of spurious variables. The goal of this paper is to propose a variable selection procedure in the multivariate linear model that accounts for the dependence structure of the multiple outputs which may lead in the LC-MS framework to the selection of more relevant metabolites. We propose a novel Lasso-based approach in the multivariate framework of the general linear model taking into account the dependence structure by using various modelings of the covariance matrix of the residuals. Our numerical experiments show that including the estimation of the covariance matrix of the residuals in the Lasso criterion dramatically improves the variable selection performance. Our approach is also successfully applied to a LC-MS data set made of African copals samples for which it is able to provide a small list of metabolites without altering the phenotype discrimination. Our methodology is implemented in the R package MultiVarSel which is available from the CRAN (Comprehensive R Archive Network).|omic data character presenc strong depend structur result either data acquisit biolog process metabolom instanc data result liquid chromatographi mass spectrometri lc ms techniqu give access larg coverag metabolit exhibit pattern data set typic use find metabolit character phenotyp interest associ sampl howev appli statist procedur adjust variabl select step depend pattern may result loss power select spurious variabl goal paper propos variabl select procedur multivari linear model account depend structur multipl output may lead lc ms framework select relev metabolit propos novel lasso base approach multivari framework general linear model take account depend structur use various model covari matrix residu numer experi show includ estim covari matrix residu lasso criterion dramat improv variabl select perform approach also success appli lc ms data set made african copal sampl abl provid small list metabolit without alter phenotyp discrimin methodolog implement packag multivarsel avail cran comprehens archiv network|['M. Perrot-Dockès', 'C. Lévy-Leduc', 'J. Chiquet', 'L. Sansonnet', 'M. Brégère', 'M. -P. Étienne', 'S. Robin', 'G. Genta-Jouve']|['stat.AP']
2017-04-07T11:23:49Z|2017-03-31T20:03:08Z|http://arxiv.org/abs/1704.00040v1|http://arxiv.org/pdf/1704.00040v1|Robust Student's t based Stochastic Cubature Filter for Nonlinear   Systems with Heavy-tailed Process and Measurement Noises|robust student base stochast cubatur filter nonlinear system heavi tail process measur nois|In this paper, a new robust Student's t based stochastic cubature filter (RSTSCF) is proposed for nonlinear state-space model with heavy-tailed process and measurement noises. The heart of the RSTSCF is a stochastic Student's t spherical radial cubature rule (SSTSRCR), which is derived based on the third-degree unbiased spherical rule and the proposed third-degree unbiased radial rule. The existing stochastic integration rule is a special case of the proposed SSTSRCR when the degrees of freedom parameter tends to infinity. The proposed filter is applied to a manoeuvring bearings-only tracking example, where an agile target is tracked and the bearing is observed in clutter. Simulation results show that the proposed RSTSCF can achieve higher estimation accuracy than the existing Gaussian approximate filter, Gaussian sum filter, Huber-based nonlinear Kalman filter, maximum correntropy criterion based Kalman filter, and robust Student's t based nonlinear filters, and is computationally much more efficient than the existing particle filter.|paper new robust student base stochast cubatur filter rstscf propos nonlinear state space model heavi tail process measur nois heart rstscf stochast student spheric radial cubatur rule sstsrcr deriv base third degre unbias spheric rule propos third degre unbias radial rule exist stochast integr rule special case propos sstsrcr degre freedom paramet tend infin propos filter appli manoeuvr bear onli track exampl agil target track bear observ clutter simul result show propos rstscf achiev higher estim accuraci exist gaussian approxim filter gaussian sum filter huber base nonlinear kalman filter maximum correntropi criterion base kalman filter robust student base nonlinear filter comput much effici exist particl filter|['Yulong Huang', 'Yonggang Zhang']|['stat.AP']
2017-04-07T11:23:49Z|2017-03-31T16:03:40Z|http://arxiv.org/abs/1704.00583v1|http://arxiv.org/pdf/1704.00583v1|A PageRank Model for Player Performance Assessment in Basketball, Soccer   and Hockey|pagerank model player perform assess basketbal soccer hockey|"In the sports of soccer, hockey and basketball the most commonly used statistics for player performance assessment are divided into two categories: offensive statistics and defensive statistics. However, qualitative assessments of playmaking (for example making ""smart"" passes) are difficult to quantify. It would be advantageous to have available a single statistic that can emphasize the flow of a game, rewarding those players who initiate and contribute to successful plays more. In this paper we will examine a model based on Google's PageRank. Other papers have explored ranking teams, coaches, and captains but here we construct ratings and rankings for individual members on both teams that emphasizes initiating and partaking in successful plays and forcing defensive turnovers. For a soccer/hockey/basketball game, our model assigns a node for each of the n players who play in the game and a ""goal node"". Arcs between player nodes indicate sport-specific situations (including passes, turnovers, scoring, fouls, out-of-bounds, play-stoppages, turnovers, missed shots, defensive plays etc.), tailored for each sport. As well, some additional arcs are added in to ensure that the associated matrix is primitive and hence there is a unique PageRank vector. The PageRank vector of the associated matrix is used to rank the players of the game. To illustrate the model, data was taken from nine NBA games played between 2014-2016. Many of the top-ranked players (in the model) in a given game had some of the most impressive traditional stat-lines. However, from the model there were surprises where some players who had impressive stat-lines had lower ranks, and others who had less impressive stat-lines had higher ranks. Overall, the model's ranking and ratings reflect more the flow of the game compared to traditional sports statistics."|sport soccer hockey basketbal common use statist player perform assess divid two categori offens statist defens statist howev qualit assess playmak exampl make smart pass difficult quantifi would advantag avail singl statist emphas flow game reward player initi contribut success play paper examin model base googl pagerank paper explor rank team coach captain construct rate rank individu member team emphas initi partak success play forc defens turnov soccer hockey basketbal game model assign node player play game goal node arc player node indic sport specif situat includ pass turnov score foul bound play stoppag turnov miss shot defens play etc tailor sport well addit arc ad ensur associ matrix primit henc uniqu pagerank vector pagerank vector associ matrix use rank player game illustr model data taken nine nba game play mani top rank player model given game impress tradit stat line howev model surpris player impress stat line lower rank less impress stat line higher rank overal model rank rate reflect flow game compar tradit sport statist|['Shael Brown']|['stat.AP']
2017-04-07T11:23:49Z|2017-03-31T09:13:08Z|http://arxiv.org/abs/1703.10806v1|http://arxiv.org/pdf/1703.10806v1|Probabilistic Mid- and Long-Term Electricity Price Forecasting|probabilist mid long term electr price forecast|The liberalization of electricity markets and the development of renewable energy sources has led to new challenges for decision makers. These challenges are accompanied by an increasing uncertainty about future electricity price movements. The increasing amount of papers, which aim to model and predict electricity prices for a short period of time provided new opportunities for market participants. However, the electricity price literature seem to be very scarce on the issue of medium- to long-term price forecasting, which is mandatory for investment and political decisions. Our paper closes this gap by introducing a new approach to simulate electricity prices with hourly resolution for several months up to three years. Considering the uncertainty of future events we are able to provide probabilistic forecasts which are able to detect probabilities for price spikes even in the long-run. As market we decided to use the EPEX day-ahead electricity market for Germany and Austria. Our model extends the X-Model which mainly utilizes the sale and purchase curve for electricity day-ahead auctions. By applying our procedure we are able to give probabilities for the due to the EEG practical relevant event of six consecutive hours of negative prices. We find that using the supply and demand curve based model in the long-run yields realistic patterns for the time series of electricity prices and leads to promising results considering common error measures.|liber electr market develop renew energi sourc led new challeng decis maker challeng accompani increas uncertainti futur electr price movement increas amount paper aim model predict electr price short period time provid new opportun market particip howev electr price literatur seem veri scarc issu medium long term price forecast mandatori invest polit decis paper close gap introduc new approach simul electr price hour resolut sever month three year consid uncertainti futur event abl provid probabilist forecast abl detect probabl price spike even long run market decid use epex day ahead electr market germani austria model extend model main util sale purchas curv electr day ahead auction appli procedur abl give probabl due eeg practic relev event six consecut hour negat price find use suppli demand curv base model long run yield realist pattern time seri electr price lead promis result consid common error measur|['Florian Ziel', 'Rick Steinert']|['stat.AP', 'q-fin.ST', '62P05, 62P20, 62P12, 91G70, 62J07', 'I.5.1; J.4; J.2; J.1; I.2.6; I.6.3']
2017-04-07T11:23:49Z|2017-03-29T23:19:34Z|http://arxiv.org/abs/1703.10266v1|http://arxiv.org/pdf/1703.10266v1|Bayesian latent time joint mixed effect models for multicohort   longitudinal data|bayesian latent time joint mix effect model multicohort longitudin data|Characterization of long-term disease dynamics, from disease-free to end-stage, is integral to understanding the course of neurodegenerative diseases such as Parkinson's and Alzheimer's; and ultimately, how best to intervene. Natural history studies typically recruit multiple cohorts at different stages of disease and follow them longitudinally for a relatively short period of time. We propose a latent time joint mixed effects model to characterize long-term disease dynamics using this short-term data. Markov chain Monte Carlo methods are proposed for estimation, model selection, and inference. We apply the model to detailed simulation studies and data from the Alzheimer's Disease Neuroimaging Initiative.|character long term diseas dynam diseas free end stage integr understand cours neurodegen diseas parkinson alzheim ultim best interven natur histori studi typic recruit multipl cohort differ stage diseas follow longitudin relat short period time propos latent time joint mix effect model character long term diseas dynam use short term data markov chain mont carlo method propos estim model select infer appli model detail simul studi data alzheim diseas neuroimag initi|['Dan Li', 'Samuel Iddi', 'Wesley K. Thompson', 'Michael C. Donohue']|['stat.AP', 'stat.ME']
2017-04-07T11:23:49Z|2017-03-29T12:40:31Z|http://arxiv.org/abs/1703.10002v1|http://arxiv.org/pdf/1703.10002v1|Spatially-Dependent Multiple Testing Under Model Misspecification, with   Application to Detection of Anthropogenic Influence on Extreme Climate Events|spatial depend multipl test model misspecif applic detect anthropogen influenc extrem climat event|"The Weather Risk Attribution Forecast (WRAF) is a forecasting tool that uses output from global climate models to make simultaneous attribution statements about whether and how greenhouse gas emissions have contributed to extreme weather across the globe. However, in conducting a large number of simultaneous hypothesis tests, the WRAF is prone to identifying false ""discoveries."" A common technique for addressing this multiple testing problem is to adjust the procedure in a way that controls the proportion of true null hypotheses that are incorrectly rejected, or the false discovery rate (FDR). Unfortunately, generic FDR procedures suffer from low power when the hypotheses are dependent, and techniques designed to account for dependence are sensitive to misspecification of the underlying statistical model. In this paper, we develop a Bayesian decision theoretic approach for dependent multiple testing that flexibly controls false discovery and is robust to model misspecification. We illustrate the robustness of our procedure to model error with a simulation study, using a framework that accounts for generic spatial dependence and allows the practitioner to flexibly specify the decision criteria. Finally, we outline the best procedure of those considered for use in the WRAF workflow and apply the procedure to several seasonal forecasts."|weather risk attribut forecast wraf forecast tool use output global climat model make simultan attribut statement whether greenhous gas emiss contribut extrem weather across globe howev conduct larg number simultan hypothesi test wraf prone identifi fals discoveri common techniqu address multipl test problem adjust procedur way control proport true null hypothes incorrect reject fals discoveri rate fdr unfortun generic fdr procedur suffer low power hypothes depend techniqu design account depend sensit misspecif statist model paper develop bayesian decis theoret approach depend multipl test flexibl control fals discoveri robust model misspecif illustr robust procedur model error simul studi use framework account generic spatial depend allow practition flexibl specifi decis criteria final outlin best procedur consid use wraf workflow appli procedur sever season forecast|['Mark D. Risser', 'Christopher J. Paciorek', 'Daithi Stone']|['stat.AP']
2017-04-07T11:23:49Z|2017-03-28T20:45:29Z|http://arxiv.org/abs/1703.09795v1|http://arxiv.org/pdf/1703.09795v1|Reply to Cox et al. and Kessler et al.: Data and code sharing is the way   forward for fMRI|repli cox et al kessler et al data code share way forward fmri|We are glad that our paper has generated intense discussions in the fMRI field, on how to analyze fMRI data and how to correct for multiple comparisons. The goal of the paper was not to disparage any specific fMRI software, but to point out that parametric statistical methods are based on a number of assumptions that are not always valid for fMRI data, and that non-parametric statistical methods are a good alternative. Through AFNIs introduction of non-parametric statistics in the function 3dttest++, the three most common fMRI softwares now all support non-parametric group inference (SPM through the toolbox SnPM, and FSL through the function randomise).|glad paper generat intens discuss fmri field analyz fmri data correct multipl comparison goal paper disparag ani specif fmri softwar point parametr statist method base number assumpt alway valid fmri data non parametr statist method good altern afni introduct non parametr statist function dttest three common fmri softwar support non parametr group infer spm toolbox snpm fsl function randomis|['Anders Eklund', 'Thomas Nichols', 'Hans Knutsson']|['stat.AP']
2017-04-07T11:23:49Z|2017-03-28T18:00:02Z|http://arxiv.org/abs/1703.09710v1|http://arxiv.org/pdf/1703.09710v1|Fast and scalable Gaussian process modeling with applications to   astronomical time series|fast scalabl gaussian process model applic astronom time seri|The growing field of large-scale time domain astronomy requires methods for probabilistic data analysis that are computationally tractable, even with large datasets. Gaussian Processes are a popular class of models used for this purpose but, since the computational cost scales as the cube of the number of data points, their application has been limited to relatively small datasets. In this paper, we present a method for Gaussian Process modeling in one-dimension where the computational requirements scale linearly with the size of the dataset. We demonstrate the method by applying it to simulated and real astronomical time series datasets. These demonstrations are examples of probabilistic inference of stellar rotation periods, asteroseismic oscillation spectra, and transiting planet parameters. The method exploits structure in the problem when the covariance function is expressed as a mixture of complex exponentials, without requiring evenly spaced observations or uniform noise. This form of covariance arises naturally when the process is a mixture of stochastically-driven damped harmonic oscillators - providing a physical motivation for and interpretation of this choice - but we also demonstrate that it is effective in many other cases. We present a mathematical description of the method, the details of the implementation, and a comparison to existing scalable Gaussian Process methods. The method is flexible, fast, and most importantly, interpretable, with a wide range of potential applications within astronomical data analysis and beyond. We provide well-tested and documented open-source implementations of this method in C++, Python, and Julia.|grow field larg scale time domain astronomi requir method probabilist data analysi comput tractabl even larg dataset gaussian process popular class model use purpos sinc comput cost scale cube number data point applic limit relat small dataset paper present method gaussian process model one dimens comput requir scale linear size dataset demonstr method appli simul real astronom time seri dataset demonstr exampl probabilist infer stellar rotat period asteroseism oscil spectra transit planet paramet method exploit structur problem covari function express mixtur complex exponenti without requir even space observ uniform nois form covari aris natur process mixtur stochast driven damp harmon oscil provid physic motiv interpret choic also demonstr effect mani case present mathemat descript method detail implement comparison exist scalabl gaussian process method method flexibl fast import interpret wide rang potenti applic within astronom data analysi beyond provid well test document open sourc implement method python julia|['Daniel Foreman-Mackey', 'Eric Agol', 'Ruth Angus', 'Sivaram Ambikasaran']|['astro-ph.IM', 'astro-ph.EP', 'astro-ph.SR', 'physics.data-an', 'stat.AP']
2017-04-07T11:23:49Z|2017-03-28T18:00:00Z|http://arxiv.org/abs/1703.09701v1|http://arxiv.org/pdf/1703.09701v1|Sampling errors in nested sampling parameter estimation|sampl error nest sampl paramet estim|Sampling errors in nested sampling parameter estimation differ from those in Bayesian evidence calculation, but have been little studied in the literature. This paper provides the first explanation of the two main sources of sampling errors in nested sampling parameter estimation, and presents a new diagrammatic representation for the process. We find no current method can accurately measure the parameter estimation errors of a single nested sampling run, and propose a method for doing so using a new algorithm for dividing nested sampling runs. We empirically verify our conclusions and the accuracy of our new method.|sampl error nest sampl paramet estim differ bayesian evid calcul littl studi literatur paper provid first explan two main sourc sampl error nest sampl paramet estim present new diagrammat represent process find current method accur measur paramet estim error singl nest sampl run propos method use new algorithm divid nest sampl run empir verifi conclus accuraci new method|['Edward Higson', 'Will Handley', 'Mike Hobson', 'Anthony Lasenby']|['stat.ME', 'astro-ph.IM', 'stat.AP']
2017-04-07T11:23:49Z|2017-03-28T09:19:12Z|http://arxiv.org/abs/1703.09472v1|http://arxiv.org/pdf/1703.09472v1|Index of Environmental Awareness in Russia - MIMIC Approaches for   Different Economic Situations|index environment awar russia mimic approach differ econom situat|The paper addresses the issue of environmental awareness in the regions of the Russian Federation. Russia provides an important study area to investigate the relationship between economic development and environmental consciousness in general. This paper introduces an index of environmental awareness, which is derived as a latent variable from various categories of search entries in Yandex, the prominent Russian search engine, during two periods in years 2014 and 2015. These indicators are presumably dependent on certain causes, which are also integrated into the model. The resulting Multiple Indicators-Multiple Causes model allows to estimate the proposed index of environmental awareness and to rank the Russian regions for each period. Comparing the results of 2014 and 2015 is especially interesting, because of the RUR devaluation at the end of 2014. In additional, we answer the question of the existence of an Environmental Kuznets Curve with respect to environmental understanding.|paper address issu environment awar region russian feder russia provid import studi area investig relationship econom develop environment conscious general paper introduc index environment awar deriv latent variabl various categori search entri yandex promin russian search engin dure two period year indic presum depend certain caus also integr model result multipl indic multipl caus model allow estim propos index environment awar rank russian region period compar result especi interest becaus rur devalu end addit answer question exist environment kuznet curv respect environment understand|['Dilya Khakimova', 'Stefanie Lösch', 'Danny Wende', 'Hans Wiesmeth', 'Ostap Okhrin']|['stat.AP']
2017-04-07T11:23:55Z|2017-03-28T07:30:31Z|http://arxiv.org/abs/1703.09430v1|http://arxiv.org/pdf/1703.09430v1|Biased polls and the psychology of voter indecisiveness|bias poll psycholog voter indecis|Accounting for undecided and uncertain voters is a challenging issue for predicting election results from public opinion polls. Undecided voters typify the uncertainty of swing voters in polls but are often ignored or allocated to each candidate in a simplistic manner. Historically this has been adequate because first, the undecided tend to settle on a candidate as the election day draws closer, and second, they are comparatively small enough to assume that the undecided voters do not affect the relative proportions of the decided voters. These assumptions are used by poll authors and meta-poll analysts, but in the presence of high numbers of undecided voters these static rules may bias election predictions. In this paper, we examine the effect of undecided voters in the 2016 US presidential election. This election was unique in that a) there was a relatively high number of undecided voters and b) the major party candidates had high unfavorability ratings. We draw on psychological theories of decision making such as decision field theory and prospect theory to explain the link between candidate unfavorability and voter indecisiveness, and to describe how these factors likely contributed to a systematic bias in polling. We then show that the allocation of undecided voters in the 2016 election biased polls and meta-polls in a manner consistent with these theories. These findings imply that, given the increasing number of undecided voters in recent elections, it will be important to take into account the underlying psychology of voting when making predictions about elections.|account undecid uncertain voter challeng issu predict elect result public opinion poll undecid voter typifi uncertainti swing voter poll often ignor alloc candid simplist manner histor adequ becaus first undecid tend settl candid elect day draw closer second compar small enough assum undecid voter affect relat proport decid voter assumpt use poll author meta poll analyst presenc high number undecid voter static rule may bias elect predict paper examin effect undecid voter us presidenti elect elect uniqu relat high number undecid voter major parti candid high unfavor rate draw psycholog theori decis make decis field theori prospect theori explain link candid unfavor voter indecis describ factor like contribut systemat bias poll show alloc undecid voter elect bias poll meta poll manner consist theori find impli given increas number undecid voter recent elect import take account psycholog vote make predict elect|['Joshua J Bon', 'Timothy Ballard', 'Bernard Baffour']|['stat.AP']
2017-04-07T11:23:55Z|2017-03-27T15:36:40Z|http://arxiv.org/abs/1703.09147v1|http://arxiv.org/pdf/1703.09147v1|Two-part models with stochastic processes for modelling longitudinal   semicontinuous data: computationally efficient inference and modelling the   overall marginal mean|two part model stochast process model longitudin semicontinu data comput effici infer model overal margin mean|Several researchers have described two-part models with patient-specific stochastic processes for analysing longitudinal semicontinuous data. In theory, such models can offer greater flexibility than the standard two-part model with patient-specific random effects. However, in practice the high dimensional integrations involved in the marginal likelihood (i.e. integrated over the stochastic processes) significantly complicates model fitting. Thus non-standard computationally intensive procedures based on simulating the marginal likelihood have so far only been proposed. In this paper, we describe an efficient method of implementation by demonstrating how the high dimensional integrations involved in the marginal likelihood can be computed efficiently. Specifically, by using a property of the multivariate normal distribution and the standard marginal cumulative distribution function identity, we transform the marginal likelihood so that the high dimensional integrations are contained in the cumulative distribution function of a multivariate normal distribution, which can then be efficiently evaluated. Hence maximum likelihood estimation can be used to obtain parameter estimates and asymptotic standard errors (from the observed information matrix) of model parameters. We describe our proposed efficient implementation procedure for the standard two-part model parameterisation and when it is of interest to directly model the overall marginal mean. The methodology is applied on a psoriatic arthritis data set concerning functional disability.|sever research describ two part model patient specif stochast process analys longitudin semicontinu data theori model offer greater flexibl standard two part model patient specif random effect howev practic high dimension integr involv margin likelihood integr stochast process signific complic model fit thus non standard comput intens procedur base simul margin likelihood far onli propos paper describ effici method implement demonstr high dimension integr involv margin likelihood comput effici specif use properti multivari normal distribut standard margin cumul distribut function ident transform margin likelihood high dimension integr contain cumul distribut function multivari normal distribut effici evalu henc maximum likelihood estim use obtain paramet estim asymptot standard error observ inform matrix model paramet describ propos effici implement procedur standard two part model parameteris interest direct model overal margin mean methodolog appli psoriat arthriti data set concern function disabl|['Sean Yiu', 'Brian Tom']|['stat.AP']
2017-04-07T11:23:55Z|2017-03-27T11:04:00Z|http://arxiv.org/abs/1703.09007v1|http://arxiv.org/pdf/1703.09007v1|Detection of Spatiotemporally Coherent Rainfall Anomalies Using Markov   Random Fields|detect spatiotempor coher rainfal anomali use markov random field|Precipitation is a large-scale, spatio-temporally heterogeneous phenomenon, with frequent anomalies exhibiting unusually high or low values. We use Markov Random Fields (MRFs) to detect spatio-temporally coherent anomalies in gridded annual rainfall data across India from 1901-2005. MRFs are undirected graphical models where each node is associated with a \{location,year\} pair, with edges connecting nodes representing adjacent locations or years. Some nodes represent observations of precipitation, while the rest represent unobserved (\emph{latent}) states that can take one of three values: high/low/normal. The MRF represents a probability distribution over the variables, using \emph{node potential} and \emph{edge potential} functions defined on nodes and edges of the graph. Optimal values of latent state variables are estimated by maximizing the posterior probability of the observations, using Gibbs sampling. Edge potentials enforce spatial and temporal coherence, and node potentials influence threshold for anomalies by affecting the prior probabilities of the states. The model can be tuned to recover anomalies detected by threshold-based methods. The competing influences of spatial and temporal coherence can be adjusted through edge potentials.   We study spatio-temporal properties of rainfall anomalies discovered by this method, using suitable measures. We identify nonstationarities in occurrence of positive and negative anomalies between the first and second halves of the 20th century. We find that between these periods, there has been decrease in rainfall during June-September (JJAS) and an increase during other months. These effects are highlighted prominently in the statistics of anomalies. Properties of anomalies learnt from this approach could present tests of regional-scale rainfall simulations by climate models and statistical simulators.|precipit larg scale spatio tempor heterogen phenomenon frequent anomali exhibit unusu high low valu use markov random field mrfs detect spatio tempor coher anomali grid annual rainfal data across india mrfs undirect graphic model node associ locat year pair edg connect node repres adjac locat year node repres observ precipit rest repres unobserv emph latent state take one three valu high low normal mrf repres probabl distribut variabl use emph node potenti emph edg potenti function defin node edg graph optim valu latent state variabl estim maxim posterior probabl observ use gibb sampl edg potenti enforc spatial tempor coher node potenti influenc threshold anomali affect prior probabl state model tune recov anomali detect threshold base method compet influenc spatial tempor coher adjust edg potenti studi spatio tempor properti rainfal anomali discov method use suitabl measur identifi nonstationar occurr posit negat anomali first second halv th centuri find period decreas rainfal dure june septemb jjas increas dure month effect highlight promin statist anomali properti anomali learnt approach could present test region scale rainfal simul climat model statist simul|['Adway Mitra', 'Ashwin K. Seshadri']|['stat.AP']
2017-04-07T11:23:55Z|2017-03-27T10:14:54Z|http://arxiv.org/abs/1703.08994v1|http://arxiv.org/pdf/1703.08994v1|Value of Information: Sensitivity Analysis and Research Design in   Bayesian Evidence Synthesis|valu inform sensit analysi research design bayesian evid synthesi|Suppose we have a Bayesian model which combines evidence from several different sources. We want to know which model parameters most affect the estimate or decision from the model, or which of the parameter uncertainties drive the decision uncertainty. Furthermore we want to prioritise what further data should be collected. These questions can be addressed by Value of Information (VoI) analysis, in which we estimate expected reductions in loss from learning specific parameters or collecting data of a given design. We describe the theory and practice of VoI for Bayesian evidence synthesis, using and extending ideas from health economics, computer modelling and Bayesian design. The methods are general to a range of decision problems including point estimation and choices between discrete actions. We apply them to a model for estimating prevalence of HIV infection, combining indirect information from several surveys, registers and expert beliefs. This analysis shows which parameters contribute most of the uncertainty about each prevalence estimate, and provides the expected improvements in precision from collecting specific amounts of additional data.|suppos bayesian model combin evid sever differ sourc want know model paramet affect estim decis model paramet uncertainti drive decis uncertainti furthermor want prioritis data collect question address valu inform voi analysi estim expect reduct loss learn specif paramet collect data given design describ theori practic voi bayesian evid synthesi use extend idea health econom comput model bayesian design method general rang decis problem includ point estim choic discret action appli model estim preval hiv infect combin indirect inform sever survey regist expert belief analysi show paramet contribut uncertainti preval estim provid expect improv precis collect specif amount addit data|['Christopher Jackson', 'Anne Presanis', 'Stefano Conti', 'Daniela De Angelis']|['stat.AP']
2017-04-07T11:23:55Z|2017-03-27T07:11:05Z|http://arxiv.org/abs/1703.08954v1|http://arxiv.org/pdf/1703.08954v1|Exact and approximate limit behaviour of the Yule tree's cophenetic   index|exact approxim limit behaviour yule tree cophenet index|In this work we study the limit distribution of an appropriately normalized cophenetic index of the pure birth tree on n contemporary tips. We show that this normalized phylogenetic balance index is a submartingale that converges almost surely and in L2. We link our work with studies on trees without branch lengths and show that in this case the limit distribution is a contraction type distribution, similar to the Quicksort limit distribution. In the continuous branch case we suggest approximations to the limit distribution. We propose heuristic methods of simulating from these distributions and it may be observed that these algorithms result in reasonable tails. Therefore, we postulate using quantiles of the derived distributions for hypothesis testing, whether an observed phylogenetic tree is consistent with the pure birth process. Simulating a sample by the proposed heuristics is rapid while exact simulation (simulating the tree and then calculating the index) is a time-consuming procedure.|work studi limit distribut appropri normal cophenet index pure birth tree contemporari tip show normal phylogenet balanc index submartingal converg almost sure link work studi tree without branch length show case limit distribut contract type distribut similar quicksort limit distribut continu branch case suggest approxim limit distribut propos heurist method simul distribut may observ algorithm result reason tail therefor postul use quantil deriv distribut hypothesi test whether observ phylogenet tree consist pure birth process simul sampl propos heurist rapid exact simul simul tree calcul index time consum procedur|['Krzysztof Bartoszek']|['q-bio.PE', 'math.PR', 'stat.AP', '05C80, 60F15, 60J85, 62M02, 62P10, 92-08, 92B10, 92D15']
2017-04-07T11:23:55Z|2017-03-27T03:54:15Z|http://arxiv.org/abs/1703.08920v1|http://arxiv.org/pdf/1703.08920v1|Modeling high dimensional multichannel brain signals|model high dimension multichannel brain signal|In this paper, our goal is to model functional and effective (directional) connectivity in network of multichannel brain physiological signals (e.g., electroencephalograms, local field potentials). The primary challenges here are twofold: first, there are major statistical and computational difficulties for modeling and analyzing high dimensional multichannel brain signals; second, there is no set of universally-agreed measures for characterizing connectivity. To model multichannel brain signals, our approach is to fit a vector autoregressive (VAR) model with sufficiently high order so that complex lead-lag temporal dynamics between the channels can be accurately characterized. However, such a model contains a large number of parameters. Thus, we will estimate the high dimensional VAR parameter space by our proposed hybrid LASSLE method (LASSO+LSE) which is imposes regularization on the first step (to control for sparsity) and constrained least squares estimation on the second step (to improve bias and mean-squared error of the estimator). Then to characterize connectivity between channels in a brain network, we will use various measures but put an emphasis on partial directed coherence (PDC) in order to capture directional connectivity between channels. PDC is a directed frequency-specific measure that explains the extent to which the present oscillatory activity in a sender channel influences the future oscillatory activity in a specific receiver channel relative all possible receivers in the network. Using the proposed modeling approach, we have achieved some insights on learning in a rat engaged in a non-spatial memory task.|paper goal model function effect direct connect network multichannel brain physiolog signal electroencephalogram local field potenti primari challeng twofold first major statist comput difficulti model analyz high dimension multichannel brain signal second set univers agre measur character connect model multichannel brain signal approach fit vector autoregress var model suffici high order complex lead lag tempor dynam channel accur character howev model contain larg number paramet thus estim high dimension var paramet space propos hybrid lassl method lasso lse impos regular first step control sparsiti constrain least squar estim second step improv bias mean squar error estim character connect channel brain network use various measur put emphasi partial direct coher pdc order captur direct connect channel pdc direct frequenc specif measur explain extent present oscillatori activ sender channel influenc futur oscillatori activ specif receiv channel relat possibl receiv network use propos model approach achiev insight learn rat engag non spatial memori task|['Lechuan Hu', 'Norbert Fortin', 'Hernando Ombao']|['stat.AP']
2017-04-07T11:23:55Z|2017-03-25T17:57:31Z|http://arxiv.org/abs/1703.08723v1|http://arxiv.org/pdf/1703.08723v1|Extending Growth Mixture Models Using Continuous Non-Elliptical   Distributions|extend growth mixtur model use continu non ellipt distribut|Growth mixture models (GMMs) incorporate both conventional random effects growth modeling and latent trajectory classes as in finite mixture modeling; therefore, they offer a way to handle the unobserved heterogeneity between subjects in their development. GMMs with Gaussian random effects dominate the literature. When the data are asymmetric and/or have heavier tails, more than one latent class is required to capture the observed variable distribution. Therefore, a GMM with continuous non-elliptical distributions is proposed to capture skewness and heavier tails in the data set. Specifically, multivariate skew-t distributions and generalized hyperbolic distributions are introduced to extend GMMs. When extending GMMs, four statistical models are considered with differing distributions of measurement errors and random effects. The mathematical development of a GMM with non-elliptical distributions relies on its relationship with the generalized inverse Gaussian distribution. Parameter estimation is outlined within the expectation-maximization framework before the performance of our GMM with non-elliptical distributions is illustrated on simulated and real data.|growth mixtur model gmms incorpor convent random effect growth model latent trajectori class finit mixtur model therefor offer way handl unobserv heterogen subject develop gmms gaussian random effect domin literatur data asymmetr heavier tail one latent class requir captur observ variabl distribut therefor gmm continu non ellipt distribut propos captur skew heavier tail data set specif multivari skew distribut general hyperbol distribut introduc extend gmms extend gmms four statist model consid differ distribut measur error random effect mathemat develop gmm non ellipt distribut reli relationship general invers gaussian distribut paramet estim outlin within expect maxim framework befor perform gmm non ellipt distribut illustr simul real data|['Yuhong Wei', 'Emilie Shireman', 'Paul D. McNicholas', 'Douglas L. Steinley']|['stat.ME', 'stat.AP', 'stat.CO']
2017-04-07T11:23:55Z|2017-03-25T03:44:05Z|http://arxiv.org/abs/1703.08644v1|http://arxiv.org/pdf/1703.08644v1|Exact Spike Train Inference Via $\ell_0$ Optimization|exact spike train infer via ell optim|In recent years, new technologies in neuroscience have made it possible to measure the activities of large numbers of neurons in behaving animals. For each neuron, a fluorescence trace is measured; this can be seen as a first-order approximation of the neuron's activity over time. Determining the exact time at which a neuron spikes on the basis of its fluorescence trace is an important open problem in the field of computational neuroscience.   Recently, a convex optimization problem involving an $\ell_1$ penalty was proposed for this task. In this paper, we slightly modify that recent proposal by replacing the $\ell_1$ penalty with an $\ell_0$ penalty. In stark contrast to the conventional wisdom that $\ell_0$ optimization problems are computationally intractable, we show that the resulting optimization problem can be efficiently solved for the global optimum using an extremely simple and efficient dynamic programming algorithm. Our R-language implementation of the proposed algorithm runs in a few minutes on fluorescence traces of $100,000$ timesteps. Furthermore, our proposal leads to substantially better results than the previous $\ell_1$ proposal, on synthetic data as well as on two calcium imaging data sets.   R-language software for our proposal is now available at https://github.com/jewellsean/LZeroSpikeInference and will be available soon on CRAN in the package LZeroSpikeInference.|recent year new technolog neurosci made possibl measur activ larg number neuron behav anim neuron fluoresc trace measur seen first order approxim neuron activ time determin exact time neuron spike basi fluoresc trace import open problem field comput neurosci recent convex optim problem involv ell penalti propos task paper slight modifi recent propos replac ell penalti ell penalti stark contrast convent wisdom ell optim problem comput intract show result optim problem effici solv global optimum use extrem simpl effici dynam program algorithm languag implement propos algorithm run minut fluoresc trace timestep furthermor propos lead substanti better result previous ell propos synthet data well two calcium imag data set languag softwar propos avail https github com jewellsean lzerospikeinfer avail soon cran packag lzerospikeinfer|['Sean Jewell', 'Daniela Witten']|['stat.AP', 'q-bio.NC']
2017-04-07T11:23:55Z|2017-03-24T23:04:17Z|http://arxiv.org/abs/1703.08620v1|http://arxiv.org/pdf/1703.08620v1|LANOVA Penalization for Unreplicated Data|lanova penal unrepl data|We consider the problem of estimating the entries of an unknown mean matrix or tensor, $\boldsymbol M$, given a single noisy realization, $\boldsymbol Y = \boldsymbol M + \boldsymbol Z$. In the matrix case, we address this problem by decomposing $\boldsymbol M$ into a component that is additive in the rows and columns, i.e. the additive ANOVA decomposition of $\boldsymbol M$, plus a matrix of elementwise effects, $\boldsymbol C$, and assuming that $\boldsymbol C$ may be sparse. Accordingly, we estimate $\boldsymbol M$ by solving a penalized regression problem, applying a lasso penalty for elements of $\boldsymbol C$. We call the corresponding estimate of $\boldsymbol M$ the LANOVA penalized estimate. Although solving this penalized regression problem is straightforward, specifying appropriate values of the penalty parameters is not. Leveraging the posterior mode interpretation of the penalized regression problem, we define and study moment-based empirical Bayes estimators of the penalty parameters. We show that our empirical Bayes estimators are consistent, and examine the behavior of LANOVA penalized estimates under misspecification of the distribution of elements of $\boldsymbol C$. We extend LANOVA penalization to accommodate sparsity of row and column effects and to tensor data. We demonstrate empirical Bayes LANOVA penalization in analyses of several datasets, including a matrix of microarray data, a three-way tensor of fMRI data and a three-way tensor of experimental data.|consid problem estim entri unknown mean matrix tensor boldsymbol given singl noisi realize boldsymbol boldsymbol boldsymbol matrix case address problem decompos boldsymbol compon addit row column addit anova decomposit boldsymbol plus matrix elementwis effect boldsymbol assum boldsymbol may spars accord estim boldsymbol solv penal regress problem appli lasso penalti element boldsymbol call correspond estim boldsymbol lanova penal estim although solv penal regress problem straightforward specifi appropri valu penalti paramet leverag posterior mode interpret penal regress problem defin studi moment base empir bay estim penalti paramet show empir bay estim consist examin behavior lanova penal estim misspecif distribut element boldsymbol extend lanova penal accommod sparsiti row column effect tensor data demonstr empir bay lanova penal analys sever dataset includ matrix microarray data three way tensor fmri data three way tensor experiment data|['Maryclare Griffin', 'Peter Hoff']|['stat.ME', 'stat.AP']
2017-04-07T11:23:55Z|2017-03-24T16:08:21Z|http://arxiv.org/abs/1703.08487v1|http://arxiv.org/pdf/1703.08487v1|Multiscale Granger causality|multiscal granger causal|In the study of complex physical and biological systems represented by multivariate stochastic processes, an issue of great relevance is the description of the system dynamics spanning multiple temporal scales. While methods to assess the dynamic complexity of individual processes at different time scales are well-established, the multiscale evaluation of directed interactions between processes is complicated by theoretical and practical issues such as filtering and downsampling. Here we extend the very popular measure of Granger causality (GC), a prominent tool for assessing directed lagged interactions between joint processes, to quantify information transfer across multiple time scales. We show that the multiscale processing of a vector autoregressive (AR) process introduces a moving average (MA) component, and describe how to represent the resulting ARMA process using state space (SS) models and to combine the SS model parameters for computing exact GC values at arbitrarily large time scales. We exploit the theoretical formulation to identify peculiar features of multiscale GC in basic AR processes, and demonstrate with numerical simulations the much larger estimation accuracy of the SS approach compared with pure AR modeling of filtered and downsampled data. The improved computational reliability is exploited to disclose meaningful multiscale patterns of information transfer between global temperature and carbon dioxide concentration time series, both in paleoclimate and in recent years.|studi complex physic biolog system repres multivari stochast process issu great relev descript system dynam span multipl tempor scale method assess dynam complex individu process differ time scale well establish multiscal evalu direct interact process complic theoret practic issu filter downsampl extend veri popular measur granger causal gc promin tool assess direct lag interact joint process quantifi inform transfer across multipl time scale show multiscal process vector autoregress ar process introduc move averag compon describ repres result arma process use state space ss model combin ss model paramet comput exact gc valu arbitrarili larg time scale exploit theoret formul identifi peculiar featur multiscal gc basic ar process demonstr numer simul much larger estim accuraci ss approach compar pure ar model filter downsampl data improv comput reliabl exploit disclos meaning multiscal pattern inform transfer global temperatur carbon dioxid concentr time seri paleoclim recent year|['Luca Faes', 'Giandomenico Nollo', 'Sebastiano Stramaglia', 'Daniele Marinazzo']|['stat.ME', 'math.ST', 'stat.AP', 'stat.TH']
2017-04-07T11:24:00Z|2017-03-24T14:40:45Z|http://arxiv.org/abs/1703.08429v1|http://arxiv.org/pdf/1703.08429v1|Modeling and Estimation for Self-Exciting Spatio-Temporal Models of   Terrorist Activity|model estim self excit spatio tempor model terrorist activ|Spatio-temporal hierarchical modeling is an extremely attractive way to model the spread of crime or terrorism data over a given region, especially when the observations are counts and must be modeled discretely. The spatio-temporal diffusion is placed, as a matter of convenience, in the process model allowing for straightforward estimation of the diffusion parameters through Bayesian techniques. However, this method of modeling does not allow for the existence of self-excitation, or a temporal data model dependency, that has been shown to exist in criminal and terrorism data. In this manuscript we will use existing theories on how violence spreads to create models that allow for both spatio-temporal diffusion in the process model as well as temporal diffusion, or self-excitation, in the data model. We will further demonstrate how Laplace approximations similar to their use in Integrated Nested Laplace Approximation can be used to quickly and accurately conduct inference of self-exciting spatio-temporal models allowing practitioners a new way of fitting and comparing multiple process models. We will illustrate this approach by fitting a self-exciting spatio-temporal model to terrorism data in Iraq and demonstrate how choice of process model leads to differing conclusions on the existence of self-excitation in the data and differing conclusions on how violence is spreading spatio-temporally.|spatio tempor hierarch model extrem attract way model spread crime terror data given region especi observ count must model discret spatio tempor diffus place matter conveni process model allow straightforward estim diffus paramet bayesian techniqu howev method model doe allow exist self excit tempor data model depend shown exist crimin terror data manuscript use exist theori violenc spread creat model allow spatio tempor diffus process model well tempor diffus self excit data model demonstr laplac approxim similar use integr nest laplac approxim use quick accur conduct infer self excit spatio tempor model allow practition new way fit compar multipl process model illustr approach fit self excit spatio tempor model terror data iraq demonstr choic process model lead differ conclus exist self excit data differ conclus violenc spread spatio tempor|['Nicholas J. Clark', 'Philip M. Dixon']|['stat.AP']
2017-04-07T11:24:00Z|2017-03-24T00:05:08Z|http://arxiv.org/abs/1703.08254v1|http://arxiv.org/pdf/1703.08254v1|Improved NN-JPDAF for Joint Multiple Target Tracking and Feature   Extraction|improv nn jpdaf joint multipl target track featur extract|Feature aided tracking can often yield improved tracking performance over the standard multiple target tracking (MTT) algorithms with only kinematic measurements. However, in many applications, the feature signal of the targets consists of sparse Fourier-domain signals. It changes quickly and nonlinearly in the time domain, and the feature measurements are corrupted by missed detections and mis-associations. These two factors make it hard to extract the feature information to be used in MTT. In this paper, we develop a feature-aided nearest neighbour joint probabilistic data association filter (NN-JPDAF) for joint MTT and feature extraction in dense target environments. To estimate the rapidly varying feature signal from incomplete and corrupted measurements, we use the atomic norm constraint to formulate the sparsity of feature signal and use the $\ell_1$-norm to formulate the sparsity of the corruption induced by mis-associations. Based on the sparse representation, the feature signal are estimated by solving a semidefinite program (SDP) which is convex. We also provide an iterative method for solving this SDP via the alternating direction method of multipliers (ADMM) where each iteration involves closed-form computation. With the estimated feature signal, re-filtering is performed to estimate the kinematic states of the targets, where the association makes use of both kinematic and feature information. Simulation results are presented to illustrate the performance of the proposed algorithm in a radar application.|featur aid track often yield improv track perform standard multipl target track mtt algorithm onli kinemat measur howev mani applic featur signal target consist spars fourier domain signal chang quick nonlinear time domain featur measur corrupt miss detect mis associ two factor make hard extract featur inform use mtt paper develop featur aid nearest neighbour joint probabilist data associ filter nn jpdaf joint mtt featur extract dens target environ estim rapid vari featur signal incomplet corrupt measur use atom norm constraint formul sparsiti featur signal use ell norm formul sparsiti corrupt induc mis associ base spars represent featur signal estim solv semidefinit program sdp convex also provid iter method solv sdp via altern direct method multipli admm iter involv close form comput estim featur signal filter perform estim kinemat state target associ make use kinemat featur inform simul result present illustr perform propos algorithm radar applic|['Le Zheng', 'Xiaodong Wang']|['cs.SY', 'stat.AP']
2017-04-07T11:24:00Z|2017-03-23T15:38:17Z|http://arxiv.org/abs/1703.08111v1|http://arxiv.org/pdf/1703.08111v1|Alternating optimization for GxE modelling with weighted genetic and   environmental scores: examples from the MAVAN study|altern optim gxe model weight genet environment score exampl mavan studi|Motivated by the goal of expanding currently existing genotype x environment interaction (GxE) models to simultaneously include multiple genetic variants and environmental exposures in a parsimonious way, we developed a novel method to estimate the parameters in a GxE model, where G is a weighted sum of genetic variants (genetic score) and E is a weighted sum of environments (environmental score). The approach uses alternating optimization to estimate the parameters of the GxE model. This is an iterative process where the genetic score weights, the environmental score weights, and the main model parameters are estimated in turn assuming the other parameters to be constant. This technique can be used to construct relatively complex interaction models that are constrained to a particular structure, and hence contain fewer parameters.   We present the model as a two-way interaction longitudinal mixed model, for which ordinary linear regression is a special case, but it can easily be extended to be compatible with k-way interaction models and generalized linear mixed models. The model is implemented in R (LEGIT package) and using SAS macros (LEGIT_SAS). Here we present examples from the Maternal Adversity, Vulnerability, and Neurodevelopment (MAVAN) study where we improve significantly upon already existing models using alternating optimization. Furthermore, through simulations, we demonstrate the power and validity of this approach even with small sample sizes.|motiv goal expand current exist genotyp environ interact gxe model simultan includ multipl genet variant environment exposur parsimoni way develop novel method estim paramet gxe model weight sum genet variant genet score weight sum environ environment score approach use altern optim estim paramet gxe model iter process genet score weight environment score weight main model paramet estim turn assum paramet constant techniqu use construct relat complex interact model constrain particular structur henc contain fewer paramet present model two way interact longitudin mix model ordinari linear regress special case easili extend compat way interact model general linear mix model model implement legit packag use sas macro legit sas present exampl matern advers vulner neurodevelop mavan studi improv signific upon alreadi exist model use altern optim furthermor simul demonstr power valid approach even small sampl size|['Alexia Jolicoeur-Martineau', 'Ashley Wazana', 'Eszter Székely', 'Meir Steiner', 'Alison S. Fleming', 'James L. Kennedy', 'Michael J. Meaney', 'Celia M. T. Greenwood']|['stat.AP']
2017-04-07T11:24:00Z|2017-03-23T13:53:06Z|http://arxiv.org/abs/1703.08071v1|http://arxiv.org/pdf/1703.08071v1|Quantifying and suppressing ranking bias in a large citation network|quantifi suppress rank bias larg citat network|It is widely recognized that citation counts for papers from different fields cannot be directly compared because different scientific fields adopt different citation practices. Citation counts are also strongly biased by paper age since older papers had more time to attract citations. Various procedures aim at suppressing these biases and give rise to new normalized indicators, such as the relative citation count. We use a large citation dataset from Microsoft Academic Graph and a new statistical framework based on the Mahalanobis distance to show that the rankings by well known indicators, including the relative citation count and Google's PageRank score, are significantly biased by paper field and age. We propose a general normalization procedure motivated by the $z$-score which produces much less biased rankings when applied to citation count and PageRank score.|wide recogn citat count paper differ field cannot direct compar becaus differ scientif field adopt differ citat practic citat count also strong bias paper age sinc older paper time attract citat various procedur aim suppress bias give rise new normal indic relat citat count use larg citat dataset microsoft academ graph new statist framework base mahalanobi distanc show rank well known indic includ relat citat count googl pagerank score signific bias paper field age propos general normal procedur motiv score produc much less bias rank appli citat count pagerank score|['Giacomo Vaccario', 'Matus Medo', 'Nicolas Wider', 'Manuel Sebastian Mariani']|['physics.soc-ph', 'cs.DL', 'cs.IR', 'physics.data-an', 'stat.AP']
2017-04-07T11:24:00Z|2017-03-21T19:53:59Z|http://arxiv.org/abs/1703.07408v1|http://arxiv.org/pdf/1703.07408v1|Maximum a posteriori estimation through simulated annealing for binary   asteroid orbit determination|maximum posteriori estim simul anneal binari asteroid orbit determin|This paper considers a new method for the binary asteroid orbit determination problem. The method is based on the Bayesian approach with a global optimisation algorithm. The orbital parameters to be determined are modelled through a posteriori, including a priori and likelihood terms. The first term constrains the parameters search space. It allows to introduce knowledge about orbit, if such information is available, but at the same time it does not require a good initial estimation of parameters. The second term is based on given observations, besides it allows us to use and to compare different observational error models. Ones the a posteriori model is build the estimator of the orbital parameters is computed using a global optimisation procedure: the simulated annealing algorithm. The new method was implemented for simulated and real observations, having received successful result, and also verified for ephemeris prediction capability. The new approach can prove useful in case of small numbers of observations and/or in case of non-Gaussian observational errors, when the classical least-squares method can not be applied.|paper consid new method binari asteroid orbit determin problem method base bayesian approach global optimis algorithm orbit paramet determin model posteriori includ priori likelihood term first term constrain paramet search space allow introduc knowledg orbit inform avail time doe requir good initi estim paramet second term base given observ besid allow us use compar differ observ error model one posteriori model build estim orbit paramet comput use global optimis procedur simul anneal algorithm new method implement simul real observ receiv success result also verifi ephemeri predict capabl new approach prove use case small number observ case non gaussian observ error classic least squar method appli|['Irina D. Kovalenko', 'Radu S. Stoica', 'Nikolay V. Emelyanov']|['astro-ph.IM', 'astro-ph.EP', 'stat.AP']
2017-04-07T11:24:00Z|2017-03-21T18:00:02Z|http://arxiv.org/abs/1703.07355v1|http://arxiv.org/abs/1703.07355v1|An Army of Me: Sockpuppets in Online Discussion Communities|armi sockpuppet onlin discuss communiti|"In online discussion communities, users can interact and share information and opinions on a wide variety of topics. However, some users may create multiple identities, or sockpuppets, and engage in undesired behavior by deceiving others or manipulating discussions. In this work, we study sockpuppetry across nine discussion communities, and show that sockpuppets differ from ordinary users in terms of their posting behavior, linguistic traits, as well as social network structure. Sockpuppets tend to start fewer discussions, write shorter posts, use more personal pronouns such as ""I"", and have more clustered ego-networks. Further, pairs of sockpuppets controlled by the same individual are more likely to interact on the same discussion at the same time than pairs of ordinary users. Our analysis suggests a taxonomy of deceptive behavior in discussion communities. Pairs of sockpuppets can vary in their deceptiveness, i.e., whether they pretend to be different users, or their supportiveness, i.e., if they support arguments of other sockpuppets controlled by the same user. We apply these findings to a series of prediction tasks, notably, to identify whether a pair of accounts belongs to the same underlying user or not. Altogether, this work presents a data-driven view of deception in online discussion communities and paves the way towards the automatic detection of sockpuppets."|onlin discuss communiti user interact share inform opinion wide varieti topic howev user may creat multipl ident sockpuppet engag undesir behavior deceiv manipul discuss work studi sockpuppetri across nine discuss communiti show sockpuppet differ ordinari user term post behavior linguist trait well social network structur sockpuppet tend start fewer discuss write shorter post use person pronoun cluster ego network pair sockpuppet control individu like interact discuss time pair ordinari user analysi suggest taxonomi decept behavior discuss communiti pair sockpuppet vari decept whether pretend differ user support support argument sockpuppet control user appli find seri predict task notabl identifi whether pair account belong user altogeth work present data driven view decept onlin discuss communiti pave way toward automat detect sockpuppet|['Srijan Kumar', 'Justin Cheng', 'Jure Leskovec', 'V. S. Subrahmanian']|['cs.SI', 'cs.CY', 'physics.soc-ph', 'stat.AP', 'stat.ML']
2017-04-07T11:24:00Z|2017-03-21T16:48:50Z|http://arxiv.org/abs/1703.07309v1|http://arxiv.org/pdf/1703.07309v1|Phytoplankton Hotspot Prediction With an Unsupervised Spatial Community   Model|phytoplankton hotspot predict unsupervis spatial communiti model|Many interesting natural phenomena are sparsely distributed and discrete. Locating the hotspots of such sparsely distributed phenomena is often difficult because their density gradient is likely to be very noisy. We present a novel approach to this search problem, where we model the co-occurrence relations between a robot's observations with a Bayesian nonparametric topic model. This approach makes it possible to produce a robust estimate of the spatial distribution of the target, even in the absence of direct target observations. We apply the proposed approach to the problem of finding the spatial locations of the hotspots of a specific phytoplankton taxon in the ocean. We use classified image data from Imaging FlowCytobot (IFCB), which automatically measures individual microscopic cells and colonies of cells. Given these individual taxon-specific observations, we learn a phytoplankton community model that characterizes the co-occurrence relations between taxa. We present experiments with simulated robot missions drawn from real observation data collected during a research cruise traversing the US Atlantic coast. Our results show that the proposed approach outperforms nearest neighbor and k-means based methods for predicting the spatial distribution of hotspots from in-situ observations.|mani interest natur phenomena spars distribut discret locat hotspot spars distribut phenomena often difficult becaus densiti gradient like veri noisi present novel approach search problem model co occurr relat robot observ bayesian nonparametr topic model approach make possibl produc robust estim spatial distribut target even absenc direct target observ appli propos approach problem find spatial locat hotspot specif phytoplankton taxon ocean use classifi imag data imag flowcytobot ifcb automat measur individu microscop cell coloni cell given individu taxon specif observ learn phytoplankton communiti model character co occurr relat taxa present experi simul robot mission drawn real observ data collect dure research cruis travers us atlant coast result show propos approach outperform nearest neighbor mean base method predict spatial distribut hotspot situ observ|['Arnold Kalmbach', 'Yogesh Girdhar', 'Heidi M. Sosik', 'Gregory Dudek']|['cs.RO', 'stat.AP']
2017-04-07T11:24:00Z|2017-03-21T15:02:45Z|http://arxiv.org/abs/1703.07256v1|http://arxiv.org/pdf/1703.07256v1|Statistical Topology and the Random Interstellar Medium|statist topolog random interstellar medium|Current astrophysical models of the interstellar medium assume that small scale variation and noise can be modelled as Gaussian random fields or simple transformations thereof, such as lognormal. We use topological methods to investigate this assumption for three regions of the southern sky. We consider Gaussian random fields on two-dimensional lattices and investigate the expected distribution of topological structures quantified through Betti numbers. We demonstrate that there are circumstances where differences in topology can identify differences in distributions when conventional marginal or correlation analyses may not. We propose a non-parametric method for comparing two fields based on the counts of topological features and the geometry of the associated persistence diagrams. When we apply the methods to the astrophysical data, we find strong evidence against a Gaussian random field model for each of the three regions of the interstellar medium that we consider. Further, we show that there are topological differences at a local scale between these different regions.|current astrophys model interstellar medium assum small scale variat nois model gaussian random field simpl transform thereof lognorm use topolog method investig assumpt three region southern sky consid gaussian random field two dimension lattic investig expect distribut topolog structur quantifi betti number demonstr circumst differ topolog identifi differ distribut convent margin correl analys may propos non parametr method compar two field base count topolog featur geometri associ persist diagram appli method astrophys data find strong evid gaussian random field model three region interstellar medium consid show topolog differ local scale differ region|['Robin Henderson', 'Irina Makarenko', 'Paul Bushby', 'Andrew Fletcher', 'Anvar Shukurov']|['stat.AP', 'astro-ph.GA']
2017-04-07T11:24:00Z|2017-03-21T10:45:07Z|http://arxiv.org/abs/1703.07137v1|http://arxiv.org/pdf/1703.07137v1|MRI-based Surgical Planning for Lumbar Spinal Stenosis|mri base surgic plan lumbar spinal stenosi|The most common reason for spinal surgery in elderly patients is lumbar spinal stenosis(LSS). For LSS, treatment decisions based on clinical and radiological information as well as personal experience of the surgeon shows large variance. Thus a standardized support system is of high value for a more objective and reproducible decision. In this work, we develop an automated algorithm to localize the stenosis causing the symptoms of the patient in magnetic resonance imaging (MRI). With 22 MRI features of each of five spinal levels of 321 patients, we show it is possible to predict the location of lesion triggering the symptoms. To support this hypothesis, we conduct an automated analysis of labeled and unlabeled MRI scans extracted from 788 patients. We confirm quantitatively the importance of radiological information and provide an algorithmic pipeline for working with raw MRI scans.|common reason spinal surgeri elder patient lumbar spinal stenosi lss lss treatment decis base clinic radiolog inform well person experi surgeon show larg varianc thus standard support system high valu object reproduc decis work develop autom algorithm local stenosi caus symptom patient magnet reson imag mri mri featur five spinal level patient show possibl predict locat lesion trigger symptom support hypothesi conduct autom analysi label unlabel mri scan extract patient confirm quantit import radiolog inform provid algorithm pipelin work raw mri scan|['Gabriele Abbati', 'Stefan Bauer', 'Peter J. Schüffler', 'Jakob Burgstaller', 'Ulrike Held', 'Sebastian Winklhofer', 'Johann Steurer', 'Joachim M. Buhmann']|['stat.AP']
2017-04-07T11:24:00Z|2017-03-21T02:45:23Z|http://arxiv.org/abs/1703.07030v1|http://arxiv.org/pdf/1703.07030v1|An Investigation of Three-point Shooting through an Analysis of NBA   Player Tracking Data|investig three point shoot analysi nba player track data|I address the difficult challenge of measuring the relative influence of competing basketball game strategies, and I apply my analysis to plays resulting in three-point shots. I use a glut of SportVU player tracking data from over 600 NBA games to derive custom position-based features that capture tangible game strategies from game-play data, such as teamwork, player matchups, and on-ball defender distances. Then, I demonstrate statistical methods for measuring the relative importance of any given basketball strategy. In doing so, I highlight the high importance of teamwork based strategies in affecting three-point shot success. By coupling SportVU data with an advanced variable importance algorithm I am able to extract meaningful results that would have been impossible to achieve even 3 years ago. Further, I demonstrate how player-tracking based features can be used to measure the three- point shooting propensity of players, and I show how this measurement can identify effective shooters that are either highly-utilized or under-utilized. Altogether, my findings provide a substantial body of work for influencing basketball strategy, and for measuring the effectiveness of basketball players.|address difficult challeng measur relat influenc compet basketbal game strategi appli analysi play result three point shot use glut sportvu player track data nba game deriv custom posit base featur captur tangibl game strategi game play data teamwork player matchup ball defend distanc demonstr statist method measur relat import ani given basketbal strategi highlight high import teamwork base strategi affect three point shot success coupl sportvu data advanc variabl import algorithm abl extract meaning result would imposs achiev even year ago demonstr player track base featur use measur three point shoot propens player show measur identifi effect shooter either high util util altogeth find provid substanti bodi work influenc basketbal strategi measur effect basketbal player|['Bradley A. Sliz']|['stat.AP']
2017-04-07T11:24:03Z|2017-03-20T20:26:10Z|http://arxiv.org/abs/1703.06957v1|http://arxiv.org/pdf/1703.06957v1|Nuisance parameter based sample size re-estimation incorporating prior   information|nuisanc paramet base sampl size estim incorpor prior inform|Prior information is often incorporated informally when planning a clinical trial. Here, we present an approach on how to incorporate prior information, such as data from historical clinical trials, into the nuisance parameter based sample size re-estimation in a design with an internal pilot study. We focus on trials with continuous endpoints in which the outcome variance is the nuisance parameter. For planning and analyzing the trial frequentist methods are considered. Moreover, the external information on the variance are summarized by the Bayesian meta-analytic-predictive approach. To incorporate external information into the sample size re-estimation, we propose to update the MAP prior based on the results of the internal pilot study and to re-estimate the sample size using a Bayes estimator from the posterior. By means of a simulation study, we compare the operating characteristics such as power and sample size distribution of the proposed procedure with the traditional sample size re-estimation approach which uses the pooled variance estimator. The simulation study shows that, if no prior data conflict is present, incorporating external information into the sample size re-estimation improves the operating characteristics compared to the traditional approach. In the case of a prior data conflict, that is when the variance of the ongoing clinical trial is unequal to the prior location, the performance of the traditional sample size re-estimation procedure is in general superior, even when the prior information is robustified. When considering to include prior information in sample size re-estimation, the potential gains should be balanced against the risks.|prior inform often incorpor inform plan clinic trial present approach incorpor prior inform data histor clinic trial nuisanc paramet base sampl size estim design intern pilot studi focus trial continu endpoint outcom varianc nuisanc paramet plan analyz trial frequentist method consid moreov extern inform varianc summar bayesian meta analyt predict approach incorpor extern inform sampl size estim propos updat map prior base result intern pilot studi estim sampl size use bay estim posterior mean simul studi compar oper characterist power sampl size distribut propos procedur tradit sampl size estim approach use pool varianc estim simul studi show prior data conflict present incorpor extern inform sampl size estim improv oper characterist compar tradit approach case prior data conflict varianc ongo clinic trial unequ prior locat perform tradit sampl size estim procedur general superior even prior inform robustifi consid includ prior inform sampl size estim potenti gain balanc risk|['Tobias Mütze', 'Heinz Schmidli', 'Tim Friede']|['stat.AP']
2017-04-07T11:24:03Z|2017-03-20T19:45:39Z|http://arxiv.org/abs/1703.06946v1|http://arxiv.org/pdf/1703.06946v1|SCALPEL: Extracting Neurons from Calcium Imaging Data|scalpel extract neuron calcium imag data|"In the past few years, new technologies in the field of neuroscience have made it possible to simultaneously image activity in large populations of neurons at cellular resolution in behaving animals. In mid-2016, a huge repository of this so-called ""calcium imaging"" data was made publicly-available. The availability of this large-scale data resource opens the door to a host of scientific questions, for which new statistical methods must be developed.   In this paper, we consider the first step in the analysis of calcium imaging data: namely, identifying the neurons in a calcium imaging video. We propose a dictionary learning approach for this task. First, we perform image segmentation to develop a dictionary containing a huge number of candidate neurons. Next, we refine the dictionary using clustering. Finally, we apply the dictionary in order to select neurons and estimate their corresponding activity over time, using a sparse group lasso optimization problem. We apply our proposal to three calcium imaging data sets.   Our proposed approach is implemented in the R package scalpel, which is available on CRAN."|past year new technolog field neurosci made possibl simultan imag activ larg popul neuron cellular resolut behav anim mid huge repositori call calcium imag data made public avail avail larg scale data resourc open door host scientif question new statist method must develop paper consid first step analysi calcium imag data name identifi neuron calcium imag video propos dictionari learn approach task first perform imag segment develop dictionari contain huge number candid neuron next refin dictionari use cluster final appli dictionari order select neuron estim correspond activ time use spars group lasso optim problem appli propos three calcium imag data set propos approach implement packag scalpel avail cran|['Ashley Petersen', 'Noah Simon', 'Daniela Witten']|['stat.AP', 'q-bio.NC']
2017-04-07T11:24:03Z|2017-03-20T15:45:44Z|http://arxiv.org/abs/1703.06808v1|http://arxiv.org/pdf/1703.06808v1|Worth Weighting? How to Think About and Use Sample Weights in Survey   Experiments|worth weight think use sampl weight survey experi|"The popularity of online surveys has increased the prominence of sampling weights in claims of representativeness. Yet, much uncertainty remains regarding how these weights should be employed in the analysis of survey experiments: Should they be used or ignored? If they are used, which estimators are preferred? We offer practical advice, rooted in the Neyman-Rubin model, for researchers producing and working with survey experimental data. We examine simple, efficient estimators (Horvitz-Thompson, H\`ajek, ""double-H\`ajek"", and post-stratification) for analyzing these data, along with formulae for biases and variances. We provide simulations that examine these estimators and real examples from experiments administered online through YouGov. We find that for examining the existence of population treatment effects using high-quality, broadly representative samples recruited by top online survey firms, sample quantities, which do not rely on weights, are often sufficient. Sample Average Treatment Effect (SATE) estimates are unlikely to differ substantially from weighted estimates, and they avoid the statistical power loss that accompanies weighting. When precise estimates of Population Average Treatment Effects (PATE) are essential, we analytically show post-stratifying on survey weights and/or covariates highly correlated with the outcome to be a conservative choice."|popular onlin survey increas promin sampl weight claim repres yet much uncertainti remain regard weight employ analysi survey experi use ignor use estim prefer offer practic advic root neyman rubin model research produc work survey experiment data examin simpl effici estim horvitz thompson ajek doubl ajek post stratif analyz data along formula bias varianc provid simul examin estim real exampl experi administ onlin yougov find examin exist popul treatment effect use high qualiti broad repres sampl recruit top onlin survey firm sampl quantiti reli weight often suffici sampl averag treatment effect sate estim unlik differ substanti weight estim avoid statist power loss accompani weight precis estim popul averag treatment effect pate essenti analyt show post stratifi survey weight covari high correl outcom conserv choic|['Luke W. Miratrix', 'Jasjeet S. Sekhon', 'Alexander G. Theodoridis', 'Luis F. Campos']|['stat.ME', 'stat.AP']
2017-04-07T11:24:04Z|2017-03-20T15:36:13Z|http://arxiv.org/abs/1703.06804v1|http://arxiv.org/pdf/1703.06804v1|A continuous spatio-temporal approach to estimate climate change|continu spatio tempor approach estim climat chang|We introduce a method for decomposition of trend, cycle and seasonal components in spatio-temporal models and apply it to investigate the existence of climate changes in temperature and rainfall series. The method incorporates critical features in the analysis of climatic problems - the importance of spatial heterogeneity, information from a large number of weather stations, and the presence of missing data. The spatial component is based on continuous projections of spatial covariance functions, allowing modeling the complex patterns of dependence observed in climatic data.   We apply this method to study climate changes in the Northeast region of Brazil, characterized by a great wealth of climates and large amplitudes of temperatures and rainfall. The results show the presence of a tendency for temperature increases, indicating changes in the climatic patterns in this region.|introduc method decomposit trend cycl season compon spatio tempor model appli investig exist climat chang temperatur rainfal seri method incorpor critic featur analysi climat problem import spatial heterogen inform larg number weather station presenc miss data spatial compon base continu project spatial covari function allow model complex pattern depend observ climat data appli method studi climat chang northeast region brazil character great wealth climat larg amplitud temperatur rainfal result show presenc tendenc temperatur increas indic chang climat pattern region|['Marcio Poletti Laurini']|['stat.AP', '62']
2017-04-07T11:24:04Z|2017-03-20T12:52:20Z|http://arxiv.org/abs/1703.06719v1|http://arxiv.org/pdf/1703.06719v1|Analysing the sensitivity of pollen based land-cover maps to different   auxiliary variables|analys sensit pollen base land cover map differ auxiliari variabl|Realistic maps of past land cover are needed to investigate prehistoric environmental changes and anthropogenic impacts. However, observation based reconstructions of past land cover are rare. Recently Pirzamanbein et al. (2015, arXiv:1511.06417) developed a statistical method that produces spatially complete reconstructions of past land cover from pollen assemblage. These reconstructions incorporate a number of auxiliary datasets raising questions regarding both the method's sensitivity to the choice of auxiliary data and the unaffected transmission of observational data.   Here the sensitivity of the method is examined by performing spatial reconstructions for Europe for three time periods (1900 CE, 1725 CE and 4000 BCE), based on irregularly distributed pollen based land cover, available for ca $25\%$ of the area, and different auxiliary datasets. The auxiliary datasets considered include the most commonly utilized sources of the past land-cover data --- estimates produced by a dynamic vegetation (DVM) and anthropogenic land-cover change (ALCC) models --- and modern elevation. Five different auxiliary datasets were considered, including different climate data driving the DVM and different ALCC models. The resulting reconstructions were evaluated using deviance information criteria and cross validation for all the time periods. For the recent time period, 1900 CE, the different land-cover reconstructions were also compared against a present day forest map.   The tests confirm that the developed statistical model provides a robust spatial interpolation tool with low sensitivity to differences in auxiliary data and high capacity to un-distortedly transmit the information provided by sparse pollen based observations. Further, usage of auxiliary data with high spatial detail improves the model performance for the areas with complex topography or where observational data is missing.|realist map past land cover need investig prehistor environment chang anthropogen impact howev observ base reconstruct past land cover rare recent pirzamanbein et al arxiv develop statist method produc spatial complet reconstruct past land cover pollen assemblag reconstruct incorpor number auxiliari dataset rais question regard method sensit choic auxiliari data unaffect transmiss observ data sensit method examin perform spatial reconstruct europ three time period ce ce bce base irregular distribut pollen base land cover avail ca area differ auxiliari dataset auxiliari dataset consid includ common util sourc past land cover data estim produc dynam veget dvm anthropogen land cover chang alcc model modern elev five differ auxiliari dataset consid includ differ climat data drive dvm differ alcc model result reconstruct evalu use devianc inform criteria cross valid time period recent time period ce differ land cover reconstruct also compar present day forest map test confirm develop statist model provid robust spatial interpol tool low sensit differ auxiliari data high capac un distort transmit inform provid spars pollen base observ usag auxiliari data high spatial detail improv model perform area complex topographi observ data miss|['Behnaz Pirzamanbein', 'Anneli Poska', 'Johan Lindström']|['stat.AP']
2017-04-07T11:24:04Z|2017-03-21T17:19:46Z|http://arxiv.org/abs/1703.06670v2|http://arxiv.org/pdf/1703.06670v2|The Same Analysis Approach: Practical protection against the pitfalls of   novel neuroimaging analysis methods|analysi approach practic protect pitfal novel neuroimag analysi method|Standard neuroimaging data analysis based on traditional principles of experimental design, modelling, and statistical inference is increasingly complemented by novel analysis methods, driven e.g. by machine learning methods. While these novel approaches provide new insights into neuroimaging data, they often have unexpected properties, generating a growing literature on possible pitfalls. We propose to meet this challenge by adopting a habit of systematic testing of experimental design, analysis procedures, and statistical inference. Specifically, we suggest to apply the analysis method used for experimental data also to aspects of the experimental design, simulated confounds, simulated null data, and control data. We stress the importance of keeping the analysis method the same in main and test analyses, because only this way possible confounds and unexpected properties can be reliably detected and avoided. We describe and discuss this Same Analysis Approach in detail, and demonstrate it in two worked examples using multivariate decoding. With these examples, we reveal two previously unknown sources of error: A mismatch between counterbalancing and cross-validation which leads to systematic below-chance accuracies, and linear decoding of a nonlinear effect, a difference in variance.|standard neuroimag data analysi base tradit principl experiment design model statist infer increas complement novel analysi method driven machin learn method novel approach provid new insight neuroimag data often unexpect properti generat grow literatur possibl pitfal propos meet challeng adopt habit systemat test experiment design analysi procedur statist infer specif suggest appli analysi method use experiment data also aspect experiment design simul confound simul null data control data stress import keep analysi method main test analys becaus onli way possibl confound unexpect properti reliabl detect avoid describ discuss analysi approach detail demonstr two work exampl use multivari decod exampl reveal two previous unknown sourc error mismatch counterbalanc cross valid lead systemat chanc accuraci linear decod nonlinear effect differ varianc|['Kai Görgen', 'Martin N. Hebart', 'Carsten Allefeld', 'John-Dylan Haynes']|['q-bio.NC', 'stat.AP']
2017-04-07T11:24:04Z|2017-03-20T09:41:34Z|http://arxiv.org/abs/1703.06645v1|http://arxiv.org/pdf/1703.06645v1|A Preferential Attachment Paradox: How does Preferential Attachment   Combine with Growth to Produce Networks with Log-normal In-degree   Distributions?|preferenti attach paradox doe preferenti attach combin growth produc network log normal degre distribut|Every network scientist knows that preferential attachment combines with growth to produce networks with power-law in-degree distributions. So how, then, is it possible for the network of American Physical Society journal collection citations to enjoy a log-normal citation distribution when it was found to have grown in accordance with preferential attachment? This anomalous result, which we exalt as the preferential attachment paradox, has remained unexplained since the physicist Sidney Redner first made light of it over a decade ago. In this paper we propose a resolution to the paradox. The source of the mischief, we contend, lies in Redner having relied on a measurement procedure bereft of the accuracy required to distinguish preferential attachment from another form of attachment that is consistent with a log-normal in-degree distribution. There was a high-accuracy measurement procedure in general use at the time, but it could not have been used to shed light on the paradox, due to the presence of a systematic error inducing design flaw. But in recent years the design flaw had been recognised and corrected. Here we show that the bringing of the newly corrected measurement procedure to bare on the data leads to a resolution of the paradox with important ramifications for the working network scientist.|everi network scientist know preferenti attach combin growth produc network power law degre distribut possibl network american physic societi journal collect citat enjoy log normal citat distribut found grown accord preferenti attach anomal result exalt preferenti attach paradox remain unexplain sinc physicist sidney redner first made light decad ago paper propos resolut paradox sourc mischief contend lie redner reli measur procedur bereft accuraci requir distinguish preferenti attach anoth form attach consist log normal degre distribut high accuraci measur procedur general use time could use shed light paradox due presenc systemat error induc design flaw recent year design flaw recognis correct show bring newli correct measur procedur bare data lead resolut paradox import ramif work network scientist|['Paul Sheridan', 'Taku Onodera']|['cs.DL', 'cond-mat.stat-mech', 'physics.data-an', 'stat.AP']
2017-04-07T11:24:04Z|2017-03-20T05:18:45Z|http://arxiv.org/abs/1703.06603v1|http://arxiv.org/pdf/1703.06603v1|A New Class of Discrete-time Stochastic Volatility Model with Correlated   Errors|new class discret time stochast volatil model correl error|In an efficient stock market, the returns and their time-dependent volatility are often jointly modeled by stochastic volatility models (SVMs). Over the last few decades several SVMs have been proposed to adequately capture the defining features of the relationship between the return and its volatility. Among one of the earliest SVM, Taylor (1982) proposed a hierarchical model, where the current return is a function of the current latent volatility, which is further modeled as an auto-regressive process. In an attempt to make the SVMs more appropriate for complex realistic market behavior, a leverage parameter was introduced in the Taylor SVM, which however led to the violation of the efficient market hypothesis (EMH, a necessary mean-zero condition for the return distribution that prevents arbitrage possibilities). Subsequently, a host of alternative SVMs had been developed and are currently in use. In this paper, we propose mean-corrections for several generalizations of Taylor SVM that capture the complex market behavior as well as satisfy EMH. We also establish a few theoretical results to characterize the key desirable features of these models, and present comparison with other popular competitors. Furthermore, four real-life examples (Oil price, CITI bank stock price, Euro-USD rate, and S&P 500 index returns) have been used to demonstrate the performance of this new class of SVMs.|effici stock market return time depend volatil often joint model stochast volatil model svms last decad sever svms propos adequ captur defin featur relationship return volatil among one earliest svm taylor propos hierarch model current return function current latent volatil model auto regress process attempt make svms appropri complex realist market behavior leverag paramet introduc taylor svm howev led violat effici market hypothesi emh necessari mean zero condit return distribut prevent arbitrag possibl subsequ host altern svms develop current use paper propos mean correct sever general taylor svm captur complex market behavior well satisfi emh also establish theoret result character key desir featur model present comparison popular competitor furthermor four real life exampl oil price citi bank stock price euro usd rate index return use demonstr perform new class svms|['Sujay Mukhoti', 'Pritam Ranjan']|['stat.AP', 'q-fin.ST']
2017-04-07T11:24:04Z|2017-03-20T05:18:30Z|http://arxiv.org/abs/1703.06602v1|http://arxiv.org/pdf/1703.06602v1|Dual Lasso Selector|dual lasso selector|We consider the problem of model selection and estimation in sparse high dimensional linear regression models with strongly correlated variables. First, we study the theoretical properties of the dual Lasso solution, and we show that joint consideration of the Lasso primal and its dual solutions are useful for selecting correlated active variables. Second, we argue that correlations among active predictors are not problematic, and we derive a new weaker condition on the design matrix, called Pseudo Irrepresentable Condition (PIC). Third, we present a new variable selection procedure, Dual Lasso Selector, and we prove that the PIC is a necessary and sufficient condition for consistent variable selection for the proposed method. Finally, by combining the dual Lasso selector further with the Ridge estimation even better prediction performance is achieved. We call the combination (DLSelect+Ridge), it can be viewed as a new combined approach for inference in high-dimensional regression models with correlated variables. We illustrate DLSelect+Ridge method and compare it with popular existing methods in terms of variable selection, prediction accuracy, estimation accuracy and computation speed by considering various simulated and real data examples.|consid problem model select estim spars high dimension linear regress model strong correl variabl first studi theoret properti dual lasso solut show joint consider lasso primal dual solut use select correl activ variabl second argu correl among activ predictor problemat deriv new weaker condit design matrix call pseudo irrepresent condit pic third present new variabl select procedur dual lasso selector prove pic necessari suffici condit consist variabl select propos method final combin dual lasso selector ridg estim even better predict perform achiev call combin dlselect ridg view new combin approach infer high dimension regress model correl variabl illustr dlselect ridg method compar popular exist method term variabl select predict accuraci estim accuraci comput speed consid various simul real data exampl|['Niharika Gauraha']|['stat.AP']
2017-04-07T11:24:04Z|2017-03-19T01:14:47Z|http://arxiv.org/abs/1703.06378v1|http://arxiv.org/pdf/1703.06378v1|Probabilistic Models for Daily Peak Loads at Distribution Feeders|probabilist model daili peak load distribut feeder|Load forecasting at distribution networks is more challenging than load forecasting at transmission networks because its load pattern is more stochastic and unpredictable. To plan sufficient resources and estimate DER hosting capacity, it is invaluable for a distribution network planner to get the probabilistic distribution of daily peak-load under a feeder over long term. In this paper, we model the probabilistic distribution functions of daily peak-load under a feeder using power law distributions, which is tested by improved Kolmogorov Smirnov test enhanced by the Monte Carlo simulation approach. In addition, the uncertainty of the modeling is quantified using the bootstrap method. The methodology of parameter estimation of the probabilistic model and the hypothesis test is elaborated in detail. In the case studies, it is shown using measurement data sets that the daily peak loads under several feeders follow the power law distribution by applying the proposed testing methods.|load forecast distribut network challeng load forecast transmiss network becaus load pattern stochast unpredict plan suffici resourc estim der host capac invalu distribut network planner get probabilist distribut daili peak load feeder long term paper model probabilist distribut function daili peak load feeder use power law distribut test improv kolmogorov smirnov test enhanc mont carlo simul approach addit uncertainti model quantifi use bootstrap method methodolog paramet estim probabilist model hypothesi test elabor detail case studi shown use measur data set daili peak load sever feeder follow power law distribut appli propos test method|['Hossein Sangrody', 'Ning Zhou', 'Xingye Qiao']|['stat.AP']
2017-04-07T11:24:07Z|2017-03-19T00:53:48Z|http://arxiv.org/abs/1703.06375v1|http://arxiv.org/pdf/1703.06375v1|An Initial Study on Load Forecasting Considering Economic Factors|initi studi load forecast consid econom factor|This paper proposes a new objective function and quantile regression (QR) algorithm for load forecasting (LF). In LF, the positive forecasting errors often have different economic impact from the negative forecasting errors. Considering this difference, a new objective function is proposed to put different prices on the positive and negative forecasting errors. QR is used to find the optimal solution of the proposed objective function. Using normalized net energy load of New England network, the proposed method is compared with a time series method, the artificial neural network method, and the support vector machine method. The simulation results show that the proposed method is more effective in reducing the economic cost of the LF errors than the other three methods.|paper propos new object function quantil regress qr algorithm load forecast lf lf posit forecast error often differ econom impact negat forecast error consid differ new object function propos put differ price posit negat forecast error qr use find optim solut propos object function use normal net energi load new england network propos method compar time seri method artifici neural network method support vector machin method simul result show propos method effect reduc econom cost lf error three method|['Hossein Sangrody', 'Ning Zhou']|['stat.AP']
2017-04-07T11:24:07Z|2017-03-21T09:36:42Z|http://arxiv.org/abs/1703.05926v2|http://arxiv.org/pdf/1703.05926v2|Quantifying the causal effect of speed cameras on road traffic accidents   via an approximate Bayesian doubly robust estimator|quantifi causal effect speed camera road traffic accid via approxim bayesian doubli robust estim|This paper develops an approximate Bayesian doubly-robust (DR) estimation method to quantify the causal effect of speed cameras on road traffic accidents. Previous empirical work on this topic, which shows a diverse range of estimated effects, is based largely on outcome regression (OR) models using the Empirical Bayes approach or on simple before and after comparisons. Issues of causality and confounding have received little formal attention. A causal DR approach combines propensity score (PS) and OR models to give an average treatment effect (ATE) estimator that is consistent and asymptotically normal under correct specification of either of the two component models. We develop this approach within a novel approximate Bayesian framework to derive posterior predictive distributions for the ATE of speed cameras on road traffic accidents. Our results for England indicate significant reductions in the number of accidents at speed cameras sites (mean ATE = -30%). Our proposed method offers a promising approach for evaluation of transport safety interventions.|paper develop approxim bayesian doubli robust dr estim method quantifi causal effect speed camera road traffic accid previous empir work topic show divers rang estim effect base larg outcom regress model use empir bay approach simpl befor comparison issu causal confound receiv littl formal attent causal dr approach combin propens score ps model give averag treatment effect ate estim consist asymptot normal correct specif either two compon model develop approach within novel approxim bayesian framework deriv posterior predict distribut ate speed camera road traffic accid result england indic signific reduct number accid speed camera site mean ate propos method offer promis approach evalu transport safeti intervent|['Daniel J Graham', 'Cian Naik', 'Emma J McCoy']|['stat.AP']
2017-04-07T11:24:07Z|2017-03-16T19:11:58Z|http://arxiv.org/abs/1703.05799v1|http://arxiv.org/pdf/1703.05799v1|A new sample-based algorithms to compute the total sensitivity index|new sampl base algorithm comput total sensit index|Variance based sensitivity indices have established themselves as a reference among practitioners of sensitivity analysis of model output. It is not unusual to consider a variance based sensitivity analysis as informative if it produces at least the first order sensitivity indices Sj and the so-called total-effect sensitivity indices STj or Tj for all the uncertain factors of the mathematical model under analysis. Computational economy is critical in sensitivity analysis. It depends mostly upon the number of model evaluations needed to obtain stable values of the estimates. While for the first order indices efficient estimation procedures are available which are independent from the number of factors under analysis, this is less the case for the total sensitivity indices. When estimating the Tj one can either use a sample based approach, whose computational cost depends from the number of factors, or approaches based on meta-modelling / emulators, e.g. based on Gaussian processes. The present work focuses on sample-based estimation procedures for Tj, and tries different avenues to achieve an algorithmic improvement over the designs proposed in the existing best practices. One among the selected procedures appear to lead to a considerable improvement when the mean absolute error is considered.|varianc base sensit indic establish themselv refer among practition sensit analysi model output unusu consid varianc base sensit analysi inform produc least first order sensit indic sj call total effect sensit indic stj tj uncertain factor mathemat model analysi comput economi critic sensit analysi depend upon number model evalu need obtain stabl valu estim first order indic effici estim procedur avail independ number factor analysi less case total sensit indic estim tj one either use sampl base approach whose comput cost depend number factor approach base meta model emul base gaussian process present work focus sampl base estim procedur tj tri differ avenu achiev algorithm improv design propos exist best practic one among select procedur appear lead consider improv mean absolut error consid|['Andrea Saltelli', 'Daniel Albrecht', 'Stefano Tarantola', 'Federico Ferretti']|['stat.AP', '00A71', 'G.3; G.4']
2017-04-07T11:24:07Z|2017-03-16T16:00:00Z|http://arxiv.org/abs/1703.05687v1|http://arxiv.org/pdf/1703.05687v1|Gaussian process regression for forecasting battery state of health|gaussian process regress forecast batteri state health|Accurately predicting the future capacity and remaining useful life of batteries is necessary to ensure reliable system operation and to minimise maintenance costs. The complex nature of battery degradation has meant that mechanistic modelling of capacity fade has thus far remained intractable; however, with the advent of cloud-connected devices, data from cells in various applications is becoming increasingly available, and the feasibility of data-driven methods for battery prognostics is increasing. Here we propose Gaussian process (GP) regression for forecasting battery state of health, and highlight various advantages of GPs over other data-driven and mechanistic approaches. GPs are a type of Bayesian non-parametric method, and hence can model complex systems whilst handling uncertainty in a principled manner. Prior information can be exploited by GPs in a variety of ways: explicit mean functions can be used if the functional form of the underlying degradation model is available, and multiple-output GPs can effectively exploit correlations between data from different cells. We demonstrate the predictive capability of GPs for short-term and long-term (remaining useful life) forecasting on a selection of capacity vs. cycle datasets from lithium-ion cells.|accur predict futur capac remain use life batteri necessari ensur reliabl system oper minimis mainten cost complex natur batteri degrad meant mechanist model capac fade thus far remain intract howev advent cloud connect devic data cell various applic becom increas avail feasibl data driven method batteri prognost increas propos gaussian process gp regress forecast batteri state health highlight various advantag gps data driven mechanist approach gps type bayesian non parametr method henc model complex system whilst handl uncertainti principl manner prior inform exploit gps varieti way explicit mean function use function form degrad model avail multipl output gps effect exploit correl data differ cell demonstr predict capabl gps short term long term remain use life forecast select capac vs cycl dataset lithium ion cell|['Robert R. Richardson', 'Michael A. Osborne', 'David A. Howey']|['stat.AP', 'stat.ML', '62P30', 'J.2; G.3']
2017-04-07T11:24:07Z|2017-03-16T10:06:45Z|http://arxiv.org/abs/1703.05545v1|http://arxiv.org/pdf/1703.05545v1|The nature and origin of heavy tails in retweet activity|natur origin heavi tail retweet activ|Modern social media platforms facilitate the rapid spread of information online. Modelling phenomena such as social contagion and information diffusion are contingent upon a detailed understanding of the information-sharing processes. In Twitter, an important aspect of this occurs with retweets, where users rebroadcast the tweets of other users. To improve our understanding of how these distributions arise, we analyse the distribution of retweet times. We show that a power law with exponential cutoff provides a better fit than the power laws previously suggested. We explain this fit through the burstiness of human behaviour and the priorities individuals place on different tasks.|modern social media platform facilit rapid spread inform onlin model phenomena social contagion inform diffus conting upon detail understand inform share process twitter import aspect occur retweet user rebroadcast tweet user improv understand distribut aris analys distribut retweet time show power law exponenti cutoff provid better fit power law previous suggest explain fit bursti human behaviour prioriti individu place differ task|['Peter Mathews', 'Lewis Mitchell', 'Giang T. Nguyen', 'Nigel G. Bean']|['physics.soc-ph', 'cs.SI', 'stat.AP']
2017-04-07T11:24:07Z|2017-03-16T09:37:08Z|http://arxiv.org/abs/1703.05532v1|http://arxiv.org/pdf/1703.05532v1|Clustering of Gamma-Ray bursts through kernel principal component   analysis|cluster gamma ray burst kernel princip compon analysi|"We consider the problem related to clustering of gamma-ray bursts (from ""BATSE"" catalogue) through kernel principal component analysis in which our proposed kernel outperforms results of other competent kernels in terms of clustering accuracy and we obtain three physically interpretable groups of gamma-ray bursts. The effectivity of the suggested kernel in combination with kernel principal component analysis in revealing natural clusters in noisy and nonlinear data while reducing the dimension of the data is also explored in two simulated data sets."|consid problem relat cluster gamma ray burst bats catalogu kernel princip compon analysi propos kernel outperform result compet kernel term cluster accuraci obtain three physic interpret group gamma ray burst effect suggest kernel combin kernel princip compon analysi reveal natur cluster noisi nonlinear data reduc dimens data also explor two simul data set|['Soumita Modak', 'Asis Kumar Chattopadhyay', 'Tanuka Chattopadhyay']|['stat.AP', 'astro-ph.HE', 'astro-ph.IM', '62P35']
2017-04-07T11:24:07Z|2017-03-16T08:28:11Z|http://arxiv.org/abs/1703.05502v1|http://arxiv.org/pdf/1703.05502v1|Steganographic Generative Adversarial Networks|steganograph generat adversari network|"Steganography is collection of methods to hide secret information (""payload"") within non-secret information (""container""). Its counterpart, Steganalysis, is the practice of determining if a message contains a hidden payload, and recovering it if possible. Presence of hidden payloads is typically detected by a binary classifier. In the present study, we propose a new model for generating image-like containers based on Deep Convolutional Generative Adversarial Networks (DCGAN). This approach allows to generate more setganalysis-secure message embedding using standard steganography algorithms. Experiment results demonstrate that the new model successfully deceives the steganography analyzer, and for this reason, can be used in steganographic applications."|steganographi collect method hide secret inform payload within non secret inform contain counterpart steganalysi practic determin messag contain hidden payload recov possibl presenc hidden payload typic detect binari classifi present studi propos new model generat imag like contain base deep convolut generat adversari network dcgan approach allow generat setganalysi secur messag embed use standard steganographi algorithm experi result demonstr new model success deceiv steganographi analyz reason use steganograph applic|['Denis Volkhonskiy', 'Ivan Nazarov', 'Boris Borisenko', 'Evgeny Burnaev']|['cs.MM', 'cs.CR', 'cs.CV', 'stat.AP']
2017-04-07T11:24:07Z|2017-03-15T18:04:21Z|http://arxiv.org/abs/1703.05339v1|http://arxiv.org/pdf/1703.05339v1|Generalised additive mixed models for dynamic analysis in linguistics: a   practical introduction|generalis addit mix model dynam analysi linguist practic introduct|This is a hands-on introduction to Generalised Additive Mixed Models (GAMMs) in the context of linguistics with a particular focus on dynamic speech analysis (e.g. formant contours, pitch tracks, diachronic change, etc.). The main goal is to explain some of the main ideas underlying GAMMs, and to provide a practical guide to frequentist significance testing using these models. The introduction covers a range of topics including basis functions, the smoothing penalty, random smooths, difference smooths, smooth interactions, model comparison and autocorrelation. It is divided into two parts. The first part looks at what GAMMs are, how they work and why/when we should use them. Although the reader can replicate some of the example analyses in this section, this is not essential. The second part is a tutorial introduction that illustrates the process of fitting and evaluating GAMMs in the R statistical software environment, and the reader is strongly encouraged to work through the examples on their own machine.|hand introduct generalis addit mix model gamm context linguist particular focus dynam speech analysi formant contour pitch track diachron chang etc main goal explain main idea gamm provid practic guid frequentist signific test use model introduct cover rang topic includ basi function smooth penalti random smooth differ smooth smooth interact model comparison autocorrel divid two part first part look gamm work whi use although reader replic exampl analys section essenti second part tutori introduct illustr process fit evalu gamm statist softwar environ reader strong encourag work exampl machin|['Márton Sóskuthy']|['stat.AP']
2017-04-07T11:24:07Z|2017-03-15T17:03:56Z|http://arxiv.org/abs/1703.05264v1|http://arxiv.org/pdf/1703.05264v1|Smooth Image-on-Scalar Regression for Brain Mapping|smooth imag scalar regress brain map|Brain mapping is an increasingly important tool in neurology and psychiatry researches for the realization of data-driven personalized medicine in the big data era, which learns the statistical links between brain images and subject level features. Taking images as responses, the task raises a lot of challenges due to the high dimensionality of the image with relatively small number of samples, as well as the noisiness of measurements in medical images.   In this paper we propose a novel method {\it Smooth Image-on-scalar Regression} (SIR) for recovering the true association between an image outcome and scalar predictors. The estimator is achieved by minimizing a mean squared error with a total variation (TV) regularization term on the predicted mean image across all subjects. It denoises the images from all subjects and at the same time returns the coefficient maps estimation. We propose an algorithm to solve this optimization problem, which is efficient when combined with recent advances in graph fused lasso solvers. The statistical consistency of the estimator is shown via an oracle inequality.   Simulation results demonstrate that the proposed method outperforms existing methods with separate denoising and regression steps. Especially, SIR shows an evident advantage in recovering signals in small regions. We apply SIR on Alzheimer's Disease Neuroimaging Initiative data and produce interpretable brain maps of the PET image to patient-level features include age, gender, genotype and disease groups.|brain map increas import tool neurolog psychiatri research realize data driven person medicin big data era learn statist link brain imag subject level featur take imag respons task rais lot challeng due high dimension imag relat small number sampl well noisi measur medic imag paper propos novel method smooth imag scalar regress sir recov true associ imag outcom scalar predictor estim achiev minim mean squar error total variat tv regular term predict mean imag across subject denois imag subject time return coeffici map estim propos algorithm solv optim problem effici combin recent advanc graph fuse lasso solver statist consist estim shown via oracl inequ simul result demonstr propos method outperform exist method separ denois regress step especi sir show evid advantag recov signal small region appli sir alzheim diseas neuroimag initi data produc interpret brain map pet imag patient level featur includ age gender genotyp diseas group|['Ying Liu', 'Bowei Yan']|['stat.ME', 'stat.AP']
2017-04-07T11:24:07Z|2017-03-15T14:22:09Z|http://arxiv.org/abs/1703.05172v1|http://arxiv.org/pdf/1703.05172v1|Bayesian adaptive bandit-based designs using the Gittins index for   multi-armed trials with normally distributed endpoints|bayesian adapt bandit base design use gittin index multi arm trial normal distribut endpoint|Adaptive designs for multi-armed clinical trials have become increasingly popular recently in many areas of medical research because of their potential to shorten development times and to increase patient response. However, developing response-adaptive trial designs that offer patient benefit while ensuring the resulting trial avoids bias and provides a statistically rigorous comparison of the different treatments included is highly challenging. In this paper, the theory of Multi-Armed Bandit Problems is used to define a family of near optimal adaptive designs in the context of a clinical trial with a normally distributed endpoint with known variance. Through simulation studies based on an ongoing trial as a motivation we report the operating characteristics (type I error, power, bias) and patient benefit of these approaches and compare them to traditional and existing alternative designs. These results are then compared to those recently published in the context of Bernoulli endpoints. Many limitations and advantages are similar in both cases but there are also important differences, specially with respect to type I error control. This paper proposes a simulation-based testing procedure to correct for the observed type I error inflation that bandit-based and adaptive rules can induce. Results presented extend recent work by considering a normally distributed endpoint, a very common case in clinical practice yet mostly ignored in the response-adaptive theoretical literature, and illustrate the potential advantages of using these methods in a rare disease context. We also recommend a suitable modified implementation of the bandit-based adaptive designs for the case of common diseases.|adapt design multi arm clinic trial becom increas popular recent mani area medic research becaus potenti shorten develop time increas patient respons howev develop respons adapt trial design offer patient benefit ensur result trial avoid bias provid statist rigor comparison differ treatment includ high challeng paper theori multi arm bandit problem use defin famili near optim adapt design context clinic trial normal distribut endpoint known varianc simul studi base ongo trial motiv report oper characterist type error power bias patient benefit approach compar tradit exist altern design result compar recent publish context bernoulli endpoint mani limit advantag similar case also import differ special respect type error control paper propos simul base test procedur correct observ type error inflat bandit base adapt rule induc result present extend recent work consid normal distribut endpoint veri common case clinic practic yet ignor respons adapt theoret literatur illustr potenti advantag use method rare diseas context also recommend suitabl modifi implement bandit base adapt design case common diseas|['Adam Smith', 'Sofia S. Villar']|['stat.AP']
2017-04-07T11:24:14Z|2017-03-15T12:07:47Z|http://arxiv.org/abs/1703.05103v1|http://arxiv.org/pdf/1703.05103v1|Do pay-for-performance incentives lead to a better health outcome?|pay perform incent lead better health outcom|Pay-for-performance approaches have been widely adopted in order to drive improvements in the quality of healthcare provision. Previous studies evaluating the impact of these programs are either limited by the number of health outcomes or of medical conditions considered. In this paper, we evaluate the effectiveness of a pay-for-performance program on the basis of five health outcomes and across a wide range of medical conditions. The context of the study is the Lombardy region in Italy, where a rewarding program was introduced in 2012. The policy evaluation is based on a difference-in-differences approach. The model includes multiple dependent outcomes, that allow quantifying the joint effect of the program, and random effects, that account for the heterogeneity of the data at the ward and hospital level. Our results show that the policy had a positive effect on the hospitals' performance in terms of those outcomes that can be more influenced by a managerial activity, namely the number of readmissions, transfers and returns to the surgery room. No significant changes which can be related to the pay-for-performance introduction are observed for the number of voluntary discharges and for mortality. Finally, our study shows evidence that the medical wards have reacted more strongly to the pay-for-performance program than the surgical ones, whereas only limited evidence is found in support of a different policy reaction across different types of hospital ownership.|pay perform approach wide adopt order drive improv qualiti healthcar provis previous studi evalu impact program either limit number health outcom medic condit consid paper evalu effect pay perform program basi five health outcom across wide rang medic condit context studi lombardi region itali reward program introduc polici evalu base differ differ approach model includ multipl depend outcom allow quantifi joint effect program random effect account heterogen data ward hospit level result show polici posit effect hospit perform term outcom influenc manageri activ name number readmiss transfer return surgeri room signific chang relat pay perform introduct observ number voluntari discharg mortal final studi show evid medic ward react strong pay perform program surgic one wherea onli limit evid found support differ polici reaction across differ type hospit ownership|['Alina Peluso', 'Paolo Berta', 'Veronica Vinciotti']|['stat.AP']
2017-04-07T11:24:14Z|2017-03-15T06:42:41Z|http://arxiv.org/abs/1703.04961v1|http://arxiv.org/pdf/1703.04961v1|Predicting with limited data - Increasing the accuracy in VIS-NIR   diffuse reflectance spectroscopy by SMOTE|predict limit data increas accuraci vis nir diffus reflect spectroscopi smote|Diffuse reflectance spectroscopy is a powerful technique to predict soil properties. It can be used in situ to provide data inexpensively and rapidly compared to the standard laboratory measurements. Because most spectral data bases contain air-dried samples scanned in the laboratory, field spectra acquired in situ are either absent or rare in calibration data sets. However, when models are calibrated on air-dried spectra, prediction using field spectra are often inaccurate. We propose a framework to calibrate partial least squares models when field spectra are rare using synthetic minority oversampling technique (SMOTE). We calibrated a model to predict soil organic carbon content using air-dried spectra spiked with synthetic field spectra. The root mean-squared error of prediction decreased from 6.18 to 2.12 mg g$^{-1}$ and $R^2$ increased from $-$0.53 to 0.82 compared to the model calibrated on air-dried spectra only.|diffus reflect spectroscopi power techniqu predict soil properti use situ provid data inexpens rapid compar standard laboratori measur becaus spectral data base contain air dri sampl scan laboratori field spectra acquir situ either absent rare calibr data set howev model calibr air dri spectra predict use field spectra often inaccur propos framework calibr partial least squar model field spectra rare use synthet minor oversampl techniqu smote calibr model predict soil organ carbon content use air dri spectra spike synthet field spectra root mean squar error predict decreas mg increas compar model calibr air dri spectra onli|['Christina Bogner', 'Anna Kühnel', 'Bernd Huwe']|['stat.AP']
2017-04-07T11:24:14Z|2017-03-15T06:36:58Z|http://arxiv.org/abs/1703.04957v1|http://arxiv.org/pdf/1703.04957v1|An algorithm for removing sensitive information: application to   race-independent recidivism prediction|algorithm remov sensit inform applic race independ recidiv predict|"Predictive modeling is increasingly being employed to assist human decision-makers. One purported advantage of replacing or augmenting human judgment with computer models in high stakes settings-- such as sentencing, hiring, policing, college admissions, and parole decisions-- is the perceived ""neutrality"" of computers. It is argued that because computer models do not hold personal prejudice, the predictions they produce will be equally free from prejudice. There is growing recognition that employing algorithms does not remove the potential for bias, and can even amplify it if the training data were generated by a process that is itself biased. In this paper, we provide a probabilistic notion of algorithmic bias. We propose a method to eliminate bias from predictive models by removing all information regarding protected variables from the data to which the models will ultimately be trained. Unlike previous work in this area, our framework is general enough to accommodate data on any measurement scale. Motivated by models currently in use in the criminal justice system that inform decisions on pre-trial release and parole, we apply our proposed method to a dataset on the criminal histories of individuals at the time of sentencing to produce ""race-neutral"" predictions of re-arrest. In the process, we demonstrate that a common approach to creating ""race-neutral"" models-- omitting race as a covariate-- still results in racially disparate predictions. We then demonstrate that the application of our proposed method to these data removes racial disparities from predictions with minimal impact on predictive accuracy."|predict model increas employ assist human decis maker one purport advantag replac augment human judgment comput model high stake set sentenc hire polic colleg admiss parol decis perceiv neutral comput argu becaus comput model hold person prejudic predict produc equal free prejudic grow recognit employ algorithm doe remov potenti bias even amplifi train data generat process bias paper provid probabilist notion algorithm bias propos method elimin bias predict model remov inform regard protect variabl data model ultim train unlik previous work area framework general enough accommod data ani measur scale motiv model current use crimin justic system inform decis pre trial releas parol appli propos method dataset crimin histori individu time sentenc produc race neutral predict arrest process demonstr common approach creat race neutral model omit race covari still result racial dispar predict demonstr applic propos method data remov racial dispar predict minim impact predict accuraci|['James E. Johndrow', 'Kristian Lum']|['stat.AP']
2017-04-07T11:24:14Z|2017-03-14T23:04:14Z|http://arxiv.org/abs/1703.04812v1|http://arxiv.org/pdf/1703.04812v1|An alternative representation of the negative binomial-Lindley   distribution. New results and applications|altern represent negat binomi lindley distribut new result applic|In this paper we present an alternative representation of the Negative Binomial--Lindley distribution recently proposed by Zamani and Ismail (2010) which shows some advantages over the latter model. This new formulation provides a tractable model with attractive properties which makes it suitable for application not only in insurance settings but also in other fields where overdispersion is observed. Basic properties of the new distribution are studied. A recurrence for the probabilities of the new distribution and an integral equation for the probability density function of the compound version, when the claim severities are absolutely continuous, are derived. Estimation methods are discussed and a numerical application is given.|paper present altern represent negat binomi lindley distribut recent propos zamani ismail show advantag latter model new formul provid tractabl model attract properti make suitabl applic onli insur set also field overdispers observ basic properti new distribut studi recurr probabl new distribut integr equat probabl densiti function compound version claim sever absolut continu deriv estim method discuss numer applic given|['Emilio Gomez-Deniz', 'Enrique Calderin-Ojeda']|['stat.AP']
2017-04-07T11:24:14Z|2017-04-03T13:51:01Z|http://arxiv.org/abs/1703.06933v2|http://arxiv.org/pdf/1703.06933v2|Fast Radio Map Construction and Position Estimation via Direct Mapping   for WLAN Indoor Localization System|fast radio map construct posit estim via direct map wlan indoor local system|The main limitation that constrains the fast and comprehensive application of Wireless Local Area Network (WLAN) based indoor localization systems with Received Signal Strength (RSS) positioning algorithms is the building of the fingerprinting radio map, which is time-consuming especially when the indoor environment is large and/or with high frequent changes. Different approaches have been proposed to reduce workload, including fingerprinting deployment and update efforts, but the performance degrades greatly when the workload is reduced below a certain level. In this paper, we propose an indoor localization scenario that applies metric learning and manifold alignment to realize direct mapping localization (DML) using a low resolution radio map with single sample of RSS that reduces the fingerprinting workload by up to 87\%. Compared to previous work. The proposed two localization approaches, DML and $k$ nearest neighbors based on reconstructed radio map (reKNN), were shown to achieve less than 4.3\ m and 3.7\ m mean localization error respectively in a typical office environment with an area of approximately 170\ m$^2$, while the unsupervised localization with perturbation algorithm was shown to achieve 4.7\ m mean localization error with 8 times more workload than the proposed methods. As for the room level localization application, both DML and reKNN can meet the requirement with at most 9\ m of localization error which is enough to tell apart different rooms with over 99\% accuracy.|main limit constrain fast comprehens applic wireless local area network wlan base indoor local system receiv signal strength rss posit algorithm build fingerprint radio map time consum especi indoor environ larg high frequent chang differ approach propos reduc workload includ fingerprint deploy updat effort perform degrad great workload reduc certain level paper propos indoor local scenario appli metric learn manifold align realiz direct map local dml use low resolut radio map singl sampl rss reduc fingerprint workload compar previous work propos two local approach dml nearest neighbor base reconstruct radio map reknn shown achiev less mean local error respect typic offic environ area approxim unsupervis local perturb algorithm shown achiev mean local error time workload propos method room level local applic dml reknn meet requir local error enough tell apart differ room accuraci|['Caifa Zhou', 'Andreas Wieser', 'Xuezhi Tan']|['cs.NI', 'stat.AP']
2017-04-07T11:24:14Z|2017-03-14T18:16:06Z|http://arxiv.org/abs/1703.04642v1|http://arxiv.org/pdf/1703.04642v1|Robust Morphometric Analysis based on Landmarks. Applications|robust morphometr analysi base landmark applic|Procrustes Analysis is a Morphometric method based on Configurations of Landmarks that estimates the superimposition parameters by least-squares; for this reason, the procedure is very sensitive to outliers. In the first part of the paper we robustify this technique to classify individuals from a descriptive point of view. In the literature there are also classical results, based on the normality of the observations, to test whether there are significant differences between individuals. In the second part of the paper we determine a Von Mises plus Saddlepoint approximation for the tail probability of the Procrustes Statistic when the observations come from a model close to the normal. We conclude the paper with some applications using the Geographical Information System QGIS.|procrust analysi morphometr method base configur landmark estim superimposit paramet least squar reason procedur veri sensit outlier first part paper robustifi techniqu classifi individu descript point view literatur also classic result base normal observ test whether signific differ individu second part paper determin von mise plus saddlepoint approxim tail probabl procrust statist observ come model close normal conclud paper applic use geograph inform system qgis|['A. Garcia-Perez', 'M. A. Cabrero-Ortega']|['stat.AP']
2017-04-07T11:24:14Z|2017-03-13T11:31:42Z|http://arxiv.org/abs/1703.04341v1|http://arxiv.org/pdf/1703.04341v1|Response adaptive designs for binary responses: how to offer patient   benefit while being robust to time trends?|respons adapt design binari respons offer patient benefit robust time trend|"Response-adaptive randomisation (RAR) can considerably improve the chances of a successful treatment outcome for patients in a clinical trial by skewing the allocation probability towards better performing treatments as data accumulates. There is considerable interest in using RAR designs in drug development for rare diseases, where traditional designs are not feasible or ethically objectionable. In this paper we discuss and address a major criticism of RAR: the undesirable type I error inflation due to unknown time trends in the trial. Time trends can appear because of changes in the characteristics of recruited patients - so-called ""patient drift"". Patient drift is a realistic concern for clinical trials in rare diseases because these typically recruit patients over a very long period of time. We compute by simulations how large the type I error inflation is as a function of the time trend magnitude in order to determine in which contexts a potentially costly correction is actually necessary. We then assess the ability of different correction methods to preserve type I error in this context and their performance in terms of other operating characteristics, including patient benefit and power. We make recommendations of which correction methods are most suitable in the rare disease context for several RAR rules, differentiating between the two-armed and the multi-armed case. We further propose a RAR design for multi-armed clinical trials, which is computationally cheap and robust to several time trends considered."|respons adapt randomis rar consider improv chanc success treatment outcom patient clinic trial skew alloc probabl toward better perform treatment data accumul consider interest use rar design drug develop rare diseas tradit design feasibl ethic objection paper discuss address major critic rar undesir type error inflat due unknown time trend trial time trend appear becaus chang characterist recruit patient call patient drift patient drift realist concern clinic trial rare diseas becaus typic recruit patient veri long period time comput simul larg type error inflat function time trend magnitud order determin context potenti cost correct actual necessari assess abil differ correct method preserv type error context perform term oper characterist includ patient benefit power make recommend correct method suitabl rare diseas context sever rar rule differenti two arm multi arm case propos rar design multi arm clinic trial comput cheap robust sever time trend consid|['Sofia S. Villar', 'Jack Bowden', 'James Wason']|['stat.AP']
2017-04-07T11:24:14Z|2017-03-13T10:14:20Z|http://arxiv.org/abs/1703.04312v1|http://arxiv.org/pdf/1703.04312v1|Assessing Potential Wind Energy Resources in Saudi Arabia with a Skew-t   Distribution|assess potenti wind energi resourc saudi arabia skew distribut|Facing increasing domestic energy consumption from population growth and industrialization, Saudi Arabia is aiming to reduce its reliance on fossil fuels and to broaden its energy mix by expanding investment in renewable energy sources, including wind energy. A preliminary task in the development of wind energy infrastructure is the assessment of wind energy potential, a key aspect of which is the characterization of its spatio-temporal behavior. In this study we examine the impact of internal climate variability on seasonal wind power density fluctuations using 30 simulations from the Large Ensemble Project (LENS) developed at the National Center for Atmospheric Research. Furthermore, a spatio-temporal model for daily wind speed is proposed with neighbor-based cross-temporal dependence, and a multivariate skew-t distribution to capture the spatial patterns of higher order moments. The model can be used to generate synthetic time series over the entire spatial domain that adequately reproduces the internal variability of the LENS dataset.|face increas domest energi consumpt popul growth industri saudi arabia aim reduc relianc fossil fuel broaden energi mix expand invest renew energi sourc includ wind energi preliminari task develop wind energi infrastructur assess wind energi potenti key aspect character spatio tempor behavior studi examin impact intern climat variabl season wind power densiti fluctuat use simul larg ensembl project len develop nation center atmospher research furthermor spatio tempor model daili wind speed propos neighbor base cross tempor depend multivari skew distribut captur spatial pattern higher order moment model use generat synthet time seri entir spatial domain adequ reproduc intern variabl len dataset|['Felipe Tagle', 'Stefano Castruccio', 'Paola Crippa', 'Marc G. Genton']|['stat.AP']
2017-04-07T11:24:14Z|2017-03-12T08:11:29Z|http://arxiv.org/abs/1703.04081v1|http://arxiv.org/pdf/1703.04081v1|Feature overwriting as a finite mixture process: Evidence from   comprehension data|featur overwrit finit mixtur process evid comprehens data|"The ungrammatical sentence ""The key to the cabinets are on the table"" is known to lead to an illusion of grammaticality. As discussed in the meta-analysis by Jaeger et al., 2017, faster reading times are observed at the verb are in the agreement-attraction sentence above compared to the equally ungrammatical sentence ""The key to the cabinet are on the table"". One explanation for this facilitation effect is the feature percolation account: the plural feature on cabinets percolates up to the head noun key, leading to the illusion. An alternative account is in terms of cue-based retrieval (Lewis & Vasishth, 2005), which assumes that the non-subject noun cabinets is misretrieved due to a partial feature-match when a dependency completion process at the auxiliary initiates a memory access for a subject with plural marking. We present evidence for yet another explanation for the observed facilitation. Because the second sentence has two nouns with identical number, it is possible that these are, in some proportion of trials, more difficult to keep distinct, leading to slower reading times at the verb in the first sentence above; this is the feature overwriting account of Nairne, 1990. We show that the feature overwriting proposal can be implemented as a finite mixture process. We reanalysed ten published data-sets, fitting hierarchical Bayesian mixture models to these data assuming a two-mixture distribution. We show that in nine out of the ten studies, a mixture distribution corresponding to feature overwriting furnishes a superior fit over both the feature percolation and the cue-based retrieval accounts."|ungrammat sentenc key cabinet tabl known lead illus grammat discuss meta analysi jaeger et al faster read time observ verb agreement attract sentenc abov compar equal ungrammat sentenc key cabinet tabl one explan facilit effect featur percol account plural featur cabinet percol head noun key lead illus altern account term cue base retriev lewi vasishth assum non subject noun cabinet misretriev due partial featur match depend complet process auxiliari initi memori access subject plural mark present evid yet anoth explan observ facilit becaus second sentenc two noun ident number possibl proport trial difficult keep distinct lead slower read time verb first sentenc abov featur overwrit account nairn show featur overwrit propos implement finit mixtur process reanalys ten publish data set fit hierarch bayesian mixtur model data assum two mixtur distribut show nine ten studi mixtur distribut correspond featur overwrit furnish superior fit featur percol cue base retriev account|['Shravan Vasishth', 'Lena A. Jaeger', 'Bruno Nicenboim']|['stat.ML', 'cs.CL', 'stat.AP']
2017-04-07T11:24:14Z|2017-03-12T02:07:37Z|http://arxiv.org/abs/1703.04056v1|http://arxiv.org/pdf/1703.04056v1|Quantifying the strength of structural connectivity underlying   functional brain networks|quantifi strength structur connect function brain network|In recent years, there has been strong interest in neuroscience studies to investigate brain organization through networks of brain regions that demonstrate strong functional connectivity (FC). These networks are extracted from observed fMRI using data-driven analytic methods such as independent component analysis (ICA). A notable limitation of these FC methods is that they do not provide any information on the underlying structural connectivity (SC), which is believed to serve as the basis for interregional interactions in brain activity. We propose a new statistical measure of the strength of SC (sSC) underlying FC networks obtained from data-driven methods. The sSC measure is developed using information from diffusion tensor imaging (DTI) data, and can be applied to compare the strength of SC across different FC networks. Furthermore, we propose a reliability index for data-driven FC networks to measure the reproducibility of the networks through re-sampling the observed data. To perform statistical inference such as hypothesis testing on the sSC, we develop a formal variance estimator of sSC based a spatial semivariogram model with a novel distance metric. We demonstrate the performance of the sSC measure and its estimation and inference methods with simulation studies. For real data analysis, we apply our methods to a multimodal imaging study with resting-state fMRI and DTI data from 20 healthy controls and 20 subjects with major depressive disorder. Results show that well-known resting state networks all demonstrate higher SC within the network as compared to the average structural connections across the brain. We also found that sSC is positively associated with the reliability index, indicating that the FC networks that have stronger underlying SC are more reproducible across samples.|recent year strong interest neurosci studi investig brain organ network brain region demonstr strong function connect fc network extract observ fmri use data driven analyt method independ compon analysi ica notabl limit fc method provid ani inform structur connect sc believ serv basi interregion interact brain activ propos new statist measur strength sc ssc fc network obtain data driven method ssc measur develop use inform diffus tensor imag dti data appli compar strength sc across differ fc network furthermor propos reliabl index data driven fc network measur reproduc network sampl observ data perform statist infer hypothesi test ssc develop formal varianc estim ssc base spatial semivariogram model novel distanc metric demonstr perform ssc measur estim infer method simul studi real data analysi appli method multimod imag studi rest state fmri dti data healthi control subject major depress disord result show well known rest state network demonstr higher sc within network compar averag structur connect across brain also found ssc posit associ reliabl index indic fc network stronger sc reproduc across sampl|['Phebe Brenne Kemmer', 'F. DuBois Bowman', 'Helen Mayberg', 'Ying Guo']|['stat.AP', 'q-bio.NC']
2017-04-07T11:24:18Z|2017-03-10T22:46:09Z|http://arxiv.org/abs/1703.03862v1|http://arxiv.org/pdf/1703.03862v1|Joint Embedding of Graphs|joint embed graph|Feature extraction and dimension reduction for networks is critical in a wide variety of domains. Efficiently and accurately learning features for multiple graphs has important applications in statistical inference on graphs. We propose a method to jointly embed multiple undirected graphs. Given a set of graphs, the joint embedding method identifies a linear subspace spanned by rank one symmetric matrices and projects adjacency matrices of graphs into this subspace. The projection coefficients can be treated as features of the graphs. We also propose a random graph model which generalizes classical random graph model and can be used to model multiple graphs. We show through theory and numerical experiments that under the model, the joint embedding method produces estimates of parameters with small errors. Via simulation experiments, we demonstrate that the joint embedding method produces features which lead to state of the art performance in classifying graphs. Applying the joint embedding method to human brain graphs, we find it extract interpretable features that can be used to predict individual composite creativity index.|featur extract dimens reduct network critic wide varieti domain effici accur learn featur multipl graph import applic statist infer graph propos method joint emb multipl undirect graph given set graph joint embed method identifi linear subspac span rank one symmetr matric project adjac matric graph subspac project coeffici treat featur graph also propos random graph model general classic random graph model use model multipl graph show theori numer experi model joint embed method produc estim paramet small error via simul experi demonstr joint embed method produc featur lead state art perform classifi graph appli joint embed method human brain graph find extract interpret featur use predict individu composit creativ index|['Shangsi Wang', 'Joshua T. Vogelstein', 'Carey E. Priebe']|['stat.AP', 'cs.LG', 'stat.ML']
2017-04-07T11:24:18Z|2017-03-10T22:03:17Z|http://arxiv.org/abs/1703.03853v1|http://arxiv.org/pdf/1703.03853v1|PairCloneTree: Reconstruction of Tumor Subclone Phylogeny Based on   Mutation Pairs using Next Generation Sequencing Data|pairclonetre reconstruct tumor subclon phylogeni base mutat pair use next generat sequenc data|We present a latent feature allocation model to reconstruct tumor subclones subject to phylogenetic evolution that mimics tumor evolution. Similar to most current methods, we consider data from next-generation sequencing. Unlike most methods that use information in short reads mapped to single nucleotide variants (SNVs), we consider subclone reconstruction using pairs of two proximal SNVs that can be mapped by the same short reads. As part of the Bayesian inference model, we construct a phylogenetic tree prior. The use of the tree structure in the prior greatly strengthens inference. Only subclones that can be approximated by a phylogenetic tree are assigned non-negligible probability. The proposed Bayesian framework implies posterior distributions on the number of subclones, their genotypes, cellular proportions, and the phylogenetic tree spanned by the inferred subclones. The proposed method is validated against different sets of simulated and real-world data using single and multiple tumor samples. An open source software package is available at http://www.compgenome.org/pairclonetree|present latent featur alloc model reconstruct tumor subclon subject phylogenet evolut mimic tumor evolut similar current method consid data next generat sequenc unlik method use inform short read map singl nucleotid variant snvs consid subclon reconstruct use pair two proxim snvs map short read part bayesian infer model construct phylogenet tree prior use tree structur prior great strengthen infer onli subclon approxim phylogenet tree assign non neglig probabl propos bayesian framework impli posterior distribut number subclon genotyp cellular proport phylogenet tree span infer subclon propos method valid differ set simul real world data use singl multipl tumor sampl open sourc softwar packag avail http www compgenom org pairclonetre|['Tianjian Zhou', 'Subhajit Sengupta', 'Peter Mueller', 'Yuan Ji']|['stat.AP']
2017-04-07T11:24:18Z|2017-03-10T18:41:00Z|http://arxiv.org/abs/1703.03790v1|http://arxiv.org/pdf/1703.03790v1|Summertime, and the livin is easy: Winter and summer pseudoseasonal life   expectancy in the United States|summertim livin easi winter summer pseudoseason life expect unit state|"In temperate climates, mortality is seasonal with a winter-dominant pattern, due in part to pneumonia and influenza. Cardiac causes, which are the leading cause of death in the United States, are also winter-seasonal although it is not clear why. Interactions between circulating respiratory viruses (f.e., influenza) and cardiac conditions have been suggested as a cause of winter-dominant mortality patterns. We propose and implement a way to estimate an upper bound on mortality attributable to winter-dominant viruses like influenza. We calculate 'pseudo-seasonal' life expectancy, dividing the year into two six-month spans, one encompassing winter the other summer. During the summer when the circulation of respiratory viruses is drastically reduced, life expectancy is about one year longer. We also quantify the seasonal mortality difference in terms of seasonal ""equivalent ages"" (defined herein) and proportional hazards. We suggest that even if viruses cause excess winter cardiac mortality, the population-level mortality reduction of a perfect influenza vaccine would be much more modest than is often recognized."|temper climat mortal season winter domin pattern due part pneumonia influenza cardiac caus lead caus death unit state also winter season although clear whi interact circul respiratori virus influenza cardiac condit suggest caus winter domin mortal pattern propos implement way estim upper bound mortal attribut winter domin virus like influenza calcul pseudo season life expect divid year two six month span one encompass winter summer dure summer circul respiratori virus drastic reduc life expect one year longer also quantifi season mortal differ term season equival age defin herein proport hazard suggest even virus caus excess winter cardiac mortal popul level mortal reduct perfect influenza vaccin would much modest often recogn|['Tina Ho', 'Andrew Noymer']|['stat.AP']
2017-04-07T11:24:18Z|2017-03-10T16:35:40Z|http://arxiv.org/abs/1703.03753v1|http://arxiv.org/pdf/1703.03753v1|Latent Gaussian Mixture Models for Nationwide Kidney Transplant Center   Evaluation|latent gaussian mixtur model nationwid kidney transplant center evalu|Five year post-transplant survival rate is an important indicator on quality of care delivered by kidney transplant centers in the United States. To provide a fair assessment of each transplant center, an effect that represents the center-specific care quality, along with patient level risk factors, is often included in the risk adjustment model. In the past, the center effects have been modeled as either fixed effects or Gaussian random effects, with various pros and cons. Our numerical analyses reveal that the distributional assumptions do impact the prediction of center effects especially when the effect is extreme. To bridge the gap between these two approaches, we propose to model the transplant center effect as a latent random variable with a finite Gaussian mixture distribution. Such latent Gaussian mixture models provide a convenient framework to study the heterogeneity among the transplant centers. To overcome the weak identifiability issues, we propose to estimate the latent Gaussian mixture model using a penalized likelihood approach, and develop sequential locally restricted likelihood ratio tests to determine the number of components in the Gaussian mixture distribution. The fitted mixture model provides a convenient means of controlling the false discovery rate when screening for underperforming or outperforming transplant centers. The performance of the methods is verified by simulations and by the analysis of the motivating data example.|five year post transplant surviv rate import indic qualiti care deliv kidney transplant center unit state provid fair assess transplant center effect repres center specif care qualiti along patient level risk factor often includ risk adjust model past center effect model either fix effect gaussian random effect various pros con numer analys reveal distribut assumpt impact predict center effect especi effect extrem bridg gap two approach propos model transplant center effect latent random variabl finit gaussian mixtur distribut latent gaussian mixtur model provid conveni framework studi heterogen among transplant center overcom weak identifi issu propos estim latent gaussian mixtur model use penal likelihood approach develop sequenti local restrict likelihood ratio test determin number compon gaussian mixtur distribut fit mixtur model provid conveni mean control fals discoveri rate screen underperform outperform transplant center perform method verifi simul analysi motiv data exampl|['Lanfeng Pan', 'Yehua Li', 'Kevin He', 'Yanming Li', 'Yi Li']|['stat.AP']
2017-04-07T11:24:18Z|2017-03-09T16:56:27Z|http://arxiv.org/abs/1703.03340v1|http://arxiv.org/pdf/1703.03340v1|Adaptive Non-uniform Compressive Sampling for Time-varying Signals|adapt non uniform compress sampl time vari signal|In this paper, adaptive non-uniform compressive sampling (ANCS) of time-varying signals, which are sparse in a proper basis, is introduced. ANCS employs the measurements of previous time steps to distribute the sensing energy among coefficients more intelligently. To this aim, a Bayesian inference method is proposed that does not require any prior knowledge of importance levels of coefficients or sparsity of the signal. Our numerical simulations show that ANCS is able to achieve the desired non-uniform recovery of the signal. Moreover, if the signal is sparse in canonical basis, ANCS can reduce the number of required measurements significantly.|paper adapt non uniform compress sampl anc time vari signal spars proper basi introduc anc employ measur previous time step distribut sens energi among coeffici intellig aim bayesian infer method propos doe requir ani prior knowledg import level coeffici sparsiti signal numer simul show anc abl achiev desir non uniform recoveri signal moreov signal spars canon basi anc reduc number requir measur signific|['Alireza Zaeemzadeh', 'Mohsen Joneidi', 'Nazanin Rahnavard']|['stat.AP', 'cs.IT', 'math.IT']
2017-04-07T11:24:18Z|2017-03-09T10:19:53Z|http://arxiv.org/abs/1703.03213v1|http://arxiv.org/pdf/1703.03213v1|Kernel intensity estimation, bootstrapping and bandwidth selection for   inhomogeneous point processes depending on spatial covariates|kernel intens estim bootstrap bandwidth select inhomogen point process depend spatial covari|In the point process context, kernel intensity estimation has been mainly restricted to exploratory analysis due to its lack of consistency. However the use of covariates has allow to design consistent alternatives under some restrictive assumptions. In this paper we focus our attention on de\-fi\-ning an appropriate framework to derive a consistent kernel intensity estimator using covariates, as well as a consistent smooth bootstrap procedure. For spatial point processes with covariates there is no specific bandwidth selector, hence, we define two new data-driven procedures specifically designed for this scenario: a rule-of-thumb and a plug-in bandwidth based on the bootstrap method previously introduced. A simulation study is accomplished to understand the behaviour of these procedures in finite samples. Finally, we apply the techniques to a real set of data made up of wildfires in Canada during June 2015, using meteorological information as covariates.|point process context kernel intens estim main restrict exploratori analysi due lack consist howev use covari allow design consist altern restrict assumpt paper focus attent de fi ning appropri framework deriv consist kernel intens estim use covari well consist smooth bootstrap procedur spatial point process covari specif bandwidth selector henc defin two new data driven procedur specif design scenario rule thumb plug bandwidth base bootstrap method previous introduc simul studi accomplish understand behaviour procedur finit sampl final appli techniqu real set data made wildfir canada dure june use meteorolog inform covari|['M. I. Borrajo', 'W. González-Manteiga', 'M. D. Martínez-Miranda']|['stat.ME', 'stat.AP', '62G05, 62G09, 62H11, 60G55, 60-08']
2017-04-07T11:24:18Z|2017-03-08T15:23:00Z|http://arxiv.org/abs/1703.02870v1|http://arxiv.org/abs/1703.02870v1|Statistical Inference in Political Networks Research|statist infer polit network research|Researchers interested in statistically modeling network data have a well-established and quickly growing set of approaches from which to choose. Several of these methods have been regularly applied in research on political networks, while others have yet to permeate the field. Here, we review the most prominent methods of inferential network analysis---both for cross-sectionally and longitudinally observed networks including (temporal) exponential random graph models, latent space models, the quadratic assignment procedure, and stochastic actor oriented models. For each method, we summarize its analytic form, identify prominent published applications in political science and discuss computational considerations. We conclude with a set of guidelines for selecting a method for a given application.|research interest statist model network data well establish quick grow set approach choos sever method regular appli research polit network yet permeat field review promin method inferenti network analysi cross section longitudin observ network includ tempor exponenti random graph model latent space model quadrat assign procedur stochast actor orient model method summar analyt form identifi promin publish applic polit scienc discuss comput consider conclud set guidelin select method given applic|['Bruce A. Desmarais', 'Skyler J. Cranmer']|['stat.AP', 'cs.SI', 'physics.soc-ph']
2017-04-07T11:24:18Z|2017-03-08T00:47:45Z|http://arxiv.org/abs/1703.02650v1|http://arxiv.org/pdf/1703.02650v1|Joint Multichannel Deconvolution and Blind Source Separation|joint multichannel deconvolut blind sourc separ|Blind Source Separation (BSS) is a challenging matrix factorization problem that plays a central role in multichannel imaging science. In a large number of applications, such as astrophysics, current unmixing methods are limited since real-world mixtures are generally affected by extra instrumental effects like blurring. Therefore, BSS has to be solved jointly with a deconvolution problem, which requires tackling a new inverse problem: deconvolution BSS (DBSS). In this article, we introduce an innovative DBSS approach, called DecGMCA, based on sparse signal modeling and an efficient alternative projected least square algorithm. Numerical results demonstrate that the DecGMCA algorithm performs very well on simulations. It further highlights the importance of jointly solving BSS and deconvolution instead of considering these two problems independently. Furthermore, the performance of the proposed DecGMCA algorithm is demonstrated on simulated radio-interferometric data.|blind sourc separ bss challeng matrix factor problem play central role multichannel imag scienc larg number applic astrophys current unmix method limit sinc real world mixtur general affect extra instrument effect like blur therefor bss solv joint deconvolut problem requir tackl new invers problem deconvolut bss dbss articl introduc innov dbss approach call decgmca base spars signal model effici altern project least squar algorithm numer result demonstr decgmca algorithm perform veri well simul highlight import joint solv bss deconvolut instead consid two problem independ furthermor perform propos decgmca algorithm demonstr simul radio interferometr data|['Ming Jiang', 'Jérôme Bobin', 'Jean-Luc Starck']|['stat.AP', 'cs.IT', 'math.IT']
2017-04-07T11:24:18Z|2017-03-07T18:14:54Z|http://arxiv.org/abs/1703.02502v1|http://arxiv.org/pdf/1703.02502v1|Clustering Methods for Electricity Consumers: An Empirical Study in   Hvaler-Norway|cluster method electr consum empir studi hvaler norway|The development of Smart Grid in Norway in specific and Europe/US in general will shortly lead to the availability of massive amount of fine-grained spatio-temporal consumption data from domestic households. This enables the application of data mining techniques for traditional problems in power system. Clustering customers into appropriate groups is extremely useful for operators or retailers to address each group differently through dedicated tariffs or customer-tailored services. Currently, the task is done based on demographic data collected through questionnaire, which is error-prone. In this paper, we used three different clustering techniques (together with their variants) to automatically segment electricity consumers based on their consumption patterns. We also proposed a good way to extract consumption patterns for each consumer. The grouping results were assessed using four common internal validity indexes. We found that the combination of Self Organizing Map (SOM) and k-means algorithms produce the most insightful and useful grouping. We also discovered that grouping quality cannot be measured effectively by automatic indicators, which goes against common suggestions in literature.|develop smart grid norway specif europ us general short lead avail massiv amount fine grain spatio tempor consumpt data domest household enabl applic data mine techniqu tradit problem power system cluster custom appropri group extrem use oper retail address group differ dedic tariff custom tailor servic current task done base demograph data collect questionnair error prone paper use three differ cluster techniqu togeth variant automat segment electr consum base consumpt pattern also propos good way extract consumpt pattern consum group result assess use four common intern valid index found combin self organ map som mean algorithm produc insight use group also discov group qualiti cannot measur effect automat indic goe common suggest literatur|['The-Hien Dang-Ha', 'Roland Olsson', 'Hao Wang']|['stat.AP']
2017-04-07T11:24:18Z|2017-03-07T15:43:22Z|http://arxiv.org/abs/1703.02441v1|http://arxiv.org/pdf/1703.02441v1|Statistical Analysis of the Ricker Model|statist analysi ricker model|The Ricker model was introduced in the context of managing fishing stocks. It is a discrete non-linear iterative model given by $N(t+1)=rN(t)\exp(-N(t))$ where $N(t)$ is the population at time $t$. The model treated in this paper includes a random component $N(t+1)=rN(t)\exp(-N(t)+\varepsilon(t+1))$ and what is observed at time $t$ is a Poisson random variable with parameter $\varphi N(t)$. Such a model has been analysed using `synthetic likelihood' and ABC (Approximate Bayesian Computation). In contrast this paper takes a non-likelihood approach and treats the model in a consistent manner as an approximation. The goal is to specify those parameter values if any which are consistent with the data.|ricker model introduc context manag fish stock discret non linear iter model given rn exp popul time model treat paper includ random compon rn exp varepsilon observ time poisson random variabl paramet varphi model analys use synthet likelihood abc approxim bayesian comput contrast paper take non likelihood approach treat model consist manner approxim goal specifi paramet valu ani consist data|['Laurie Davies']|['stat.AP', '62M99']
2017-04-07T11:24:22Z|2017-03-07T11:15:14Z|http://arxiv.org/abs/1703.02329v1|http://arxiv.org/pdf/1703.02329v1|Time and media-use of Italian Generation Y: dimensions of leisure   preferences|time media use italian generat dimens leisur prefer|"Time spent in leisure is not a minor research question as it is acknowledged as a key aspect of one's quality of life. The primary aim of this article is to qualify time and Internet use of Italian Generation Y beyond media hype and assumptions. To this aim, we apply a multidimensional extension of Item Response Theory models to the Italian ""Multipurpose survey on households: aspects of daily life"" to ascertain the relevant dimensions of Generation Y time-use. We show that the use of technology is neither the first nor the foremost time-use activity of Italian Generation Y, who still prefers to use its time to socialise and have fun with friends in a non media-medalled manner."|time spent leisur minor research question acknowledg key aspect one qualiti life primari aim articl qualifi time internet use italian generat beyond media hype assumpt aim appli multidimension extens item respons theori model italian multipurpos survey household aspect daili life ascertain relev dimens generat time use show use technolog neither first foremost time use activ italian generat still prefer use time socialis fun friend non media medal manner|['Michela Gnaldi', 'Simone Del Sarto']|['stat.AP', 'cs.CY']
2017-04-07T11:24:22Z|2017-03-14T18:52:48Z|http://arxiv.org/abs/1703.02236v2|http://arxiv.org/pdf/1703.02236v2|Propensity score prediction for electronic healthcare databases using   Super Learner and High-dimensional Propensity Score Methods|propens score predict electron healthcar databas use super learner high dimension propens score method|"The optimal learner for prediction modeling varies depending on the underlying data-generating distribution. Super Learner (SL) is a generic ensemble learning algorithm that uses cross-validation to select among a ""library"" of candidate prediction models. The SL is not restricted to a single prediction model, but uses the strengths of a variety of learning algorithms to adapt to different databases. While the SL has been shown to perform well in a number of settings, it has not been thoroughly evaluated in large electronic healthcare databases that are common in pharmacoepidemiology and comparative effectiveness research. In this study, we applied and evaluated the performance of the SL in its ability to predict treatment assignment using three electronic healthcare databases. We considered a library of algorithms that consisted of both nonparametric and parametric models. We also considered a novel strategy for prediction modeling that combines the SL with the high-dimensional propensity score (hdPS) variable selection algorithm. Predictive performance was assessed using three metrics: the negative log-likelihood, area under the curve (AUC), and time complexity. Results showed that the best individual algorithm, in terms of predictive performance, varied across datasets. The SL was able to adapt to the given dataset and optimize predictive performance relative to any individual learner. Combining the SL with the hdPS was the most consistent prediction method and may be promising for PS estimation and prediction modeling in electronic healthcare databases."|optim learner predict model vari depend data generat distribut super learner sl generic ensembl learn algorithm use cross valid select among librari candid predict model sl restrict singl predict model use strength varieti learn algorithm adapt differ databas sl shown perform well number set thorough evalu larg electron healthcar databas common pharmacoepidemiolog compar effect research studi appli evalu perform sl abil predict treatment assign use three electron healthcar databas consid librari algorithm consist nonparametr parametr model also consid novel strategi predict model combin sl high dimension propens score hdps variabl select algorithm predict perform assess use three metric negat log likelihood area curv auc time complex result show best individu algorithm term predict perform vari across dataset sl abl adapt given dataset optim predict perform relat ani individu learner combin sl hdps consist predict method may promis ps estim predict model electron healthcar databas|['Cheng Ju', 'Mary Combs', 'Samuel D Lendle', 'Jessica M Franklin', 'Richard Wyss', 'Sebastian Schneeweiss', 'Mark J. van der Laan']|['stat.AP', 'stat.ML']
2017-04-07T11:24:22Z|2017-03-06T21:17:42Z|http://arxiv.org/abs/1703.02112v1|http://arxiv.org/pdf/1703.02112v1|Process convolution approaches for modeling interacting trajectories|process convolut approach model interact trajectori|"Gaussian processes are a fundamental statistical tool used in a wide range of applications. In the spatio-temporal setting, several families of covariance functions exist to accommodate a wide variety of dependence structures arising in different applications. These parametric families can be restrictive and are insufficient in some situations. In contrast, process convolutions represent a flexible, interpretable approach to defining the covariance of a Gaussian process and have modest requirements to ensure validity. We introduce a generalization of the process convolution approach that employs multiple convolutions sequentially to form a ""process convolution chain."" In our proposed multi-stage framework, complex dependencies that arise from a combination of different interacting mechanisms are decomposed into a series of interpretable kernel smoothers. We demonstrate an application of process convolution chains to model killer whale movement, in which the paths taken by multiple individuals are not independent, but reflect dynamic social interactions within the population. Our proposed model for dependent movement provides inference for the latent dynamic social structure in the study population. Additionally, by leveraging the positive dependence among individual paths, we achieve a reduction in uncertainty for the estimated locations of the whales, compared to a model that treats paths as independent."|gaussian process fundament statist tool use wide rang applic spatio tempor set sever famili covari function exist accommod wide varieti depend structur aris differ applic parametr famili restrict insuffici situat contrast process convolut repres flexibl interpret approach defin covari gaussian process modest requir ensur valid introduc general process convolut approach employ multipl convolut sequenti form process convolut chain propos multi stage framework complex depend aris combin differ interact mechan decompos seri interpret kernel smoother demonstr applic process convolut chain model killer whale movement path taken multipl individu independ reflect dynam social interact within popul propos model depend movement provid infer latent dynam social structur studi popul addit leverag posit depend among individu path achiev reduct uncertainti estim locat whale compar model treat path independ|['Henry R. Scharf', 'Mevin B. Hooten', 'Devin S. Johnson', 'John W. Durban']|['stat.ME', 'stat.AP']
2017-04-07T11:24:22Z|2017-03-06T19:33:24Z|http://arxiv.org/abs/1703.02078v1|http://arxiv.org/pdf/1703.02078v1|Cross-screening in observational studies that test many hypotheses|cross screen observ studi test mani hypothes|"We discuss observational studies that test many causal hypotheses, either hypotheses about many outcomes or many treatments. To be credible an observational study that tests many causal hypotheses must demonstrate that its conclusions are neither artifacts of multiple testing nor of small biases from nonrandom treatment assignment. In a sense that needs to be defined carefully, hidden within a sensitivity analysis for nonrandom assignment is an enormous correction for multiple testing: in the absence of bias, it is extremely improbable that multiple testing alone would create an association insensitive to moderate biases. We propose a new strategy called ""cross-screening"", different from but motivated by recent work of Bogomolov and Heller on replicability. Cross-screening splits the data in half at random, uses the first half to plan a study carried out on the second half, then uses the second half to plan a study carried out on the first half, and reports the more favorable conclusions of the two studies correcting using the Bonferroni inequality for having done two studies. If the two studies happen to concur, then they achieve Bogomolov-Heller replicability; however, importantly, replicability is not required for strong control of the family-wise error rate, and either study alone suffices for firm conclusions. In randomized studies with a few hypotheses, cross-split screening is not an attractive method when compared with conventional methods of multiplicity control, but it can become attractive when hundreds or thousands of hypotheses are subjected to sensitivity analyses in an observational study. We illustrate the technique by comparing 46 biomarkers in individuals who consume large quantities of fish versus little or no fish."|discuss observ studi test mani causal hypothes either hypothes mani outcom mani treatment credibl observ studi test mani causal hypothes must demonstr conclus neither artifact multipl test small bias nonrandom treatment assign sens need defin care hidden within sensit analysi nonrandom assign enorm correct multipl test absenc bias extrem improb multipl test alon would creat associ insensit moder bias propos new strategi call cross screen differ motiv recent work bogomolov heller replic cross screen split data half random use first half plan studi carri second half use second half plan studi carri first half report favor conclus two studi correct use bonferroni inequ done two studi two studi happen concur achiev bogomolov heller replic howev import replic requir strong control famili wise error rate either studi alon suffic firm conclus random studi hypothes cross split screen attract method compar convent method multipl control becom attract hundr thousand hypothes subject sensit analys observ studi illustr techniqu compar biomark individu consum larg quantiti fish versus littl fish|['Qingyuan Zhao', 'Dylan S. Small', 'Paul R. Rosenbaum']|['stat.ME', 'stat.AP']
2017-04-07T11:24:22Z|2017-03-06T15:58:26Z|http://arxiv.org/abs/1703.01937v1|http://arxiv.org/pdf/1703.01937v1|Reputation Dynamics in a Market for Illicit Drugs|reput dynam market illicit drug|We analyze reputation dynamics in an online market for illicit drugs using a novel dataset of prices and ratings. The market is a black market, and so contracts cannot be enforced. We study the role that reputation plays in alleviating adverse selection in this market. We document the following stylized facts: (i) There is a positive relationship between the price and the rating of a seller. This effect is increasing in the number of reviews left for a seller. A mature highly-rated seller charges a 20% higher price than a mature low-rated seller. (ii) Sellers with more reviews charge higher prices regardless of rating. (iii) Low-rated sellers are more likely to exit the market and make fewer sales. We show that these stylized facts are explained by a dynamic model of adverse selection, ratings, and exit, in which buyers form rational inferences about the quality of a seller jointly from his rating and number of sales. Sellers who receive low ratings initially charge the same price as highly-rated sellers since early reviews are less informative about quality. Bad sellers exit rather than face lower prices in the future. We provide conditions under which our model admits a unique equilibrium. We estimate the model, and use the result to compute the returns to reputation in the market. We find that the market would have collapsed due to adverse selection in the absence of a rating system.|analyz reput dynam onlin market illicit drug use novel dataset price rate market black market contract cannot enforc studi role reput play allevi advers select market document follow styliz fact posit relationship price rate seller effect increas number review left seller matur high rate seller charg higher price matur low rate seller ii seller review charg higher price regardless rate iii low rate seller like exit market make fewer sale show styliz fact explain dynam model advers select rate exit buyer form ration infer qualiti seller joint rate number sale seller receiv low rate initi charg price high rate seller sinc earli review less inform qualiti bad seller exit rather face lower price futur provid condit model admit uniqu equilibrium estim model use result comput return reput market find market would collaps due advers select absenc rate system|['Nick Janetos', 'Jan Tilly']|['stat.AP']
2017-04-07T11:24:22Z|2017-03-06T09:24:07Z|http://arxiv.org/abs/1703.01776v1|http://arxiv.org/pdf/1703.01776v1|Online Sequential Monte Carlo smoother for partially observed stochastic   differential equations|onlin sequenti mont carlo smoother partial observ stochast differenti equat|This paper introduces a new algorithm to approximate smoothed additive functionals for partially observed stochastic differential equations. This method relies on a recent procedure which allows to compute such approximations online, i.e. as the observations are received, and with a computational complexity growing linearly with the number of Monte Carlo samples. This online smoother cannot be used directly in the case of partially observed stochastic differential equations since the transition density of the latent data is usually unknown. We prove that a similar algorithm may still be defined for partially observed continuous processes by replacing this unknown quantity by an unbiased estimator obtained for instance using general Poisson estimators. We prove that this estimator is consistent and its performance are illustrated using data from two models.|paper introduc new algorithm approxim smooth addit function partial observ stochast differenti equat method reli recent procedur allow comput approxim onlin observ receiv comput complex grow linear number mont carlo sampl onlin smoother cannot use direct case partial observ stochast differenti equat sinc transit densiti latent data usual unknown prove similar algorithm may still defin partial observ continu process replac unknown quantiti unbias estim obtain instanc use general poisson estim prove estim consist perform illustr use data two model|['Pierre Gloaguen', 'Marie-Pierre Etienne', 'Sylvain Le Corff']|['stat.ME', 'stat.AP']
2017-04-07T11:24:22Z|2017-03-04T21:50:25Z|http://arxiv.org/abs/1703.01526v1|http://arxiv.org/abs/1703.01526v1|High Accuracy Classification of Parkinson's Disease through Shape   Analysis and Surface Fitting in $^{123}$I-Ioflupane SPECT Imaging|high accuraci classif parkinson diseas shape analysi surfac fit ioflupan spect imag|Early and accurate identification of parkinsonian syndromes (PS) involving presynaptic degeneration from non-degenerative variants such as Scans Without Evidence of Dopaminergic Deficit (SWEDD) and tremor disorders, is important for effective patient management as the course, therapy and prognosis differ substantially between the two groups. In this study, we use Single Photon Emission Computed Tomography (SPECT) images from healthy normal, early PD and SWEDD subjects, as obtained from the Parkinson's Progression Markers Initiative (PPMI) database, and process them to compute shape- and surface fitting-based features for the three groups. We use these features to develop and compare various classification models that can discriminate between scans showing dopaminergic deficit, as in PD, from scans without the deficit, as in healthy normal or SWEDD. Along with it, we also compare these features with Striatal Binding Ratio (SBR)-based features, which are well-established and clinically used, by computing a feature importance score using Random forests technique. We observe that the Support Vector Machine (SVM) classifier gave the best performance with an accuracy of 97.29%. These features also showed higher importance than the SBR-based features. We infer from the study that shape analysis and surface fitting are useful and promising methods for extracting discriminatory features that can be used to develop diagnostic models that might have the potential to help clinicians in the diagnostic process.|earli accur identif parkinsonian syndrom ps involv presynapt degener non degen variant scan without evid dopaminerg deficit swedd tremor disord import effect patient manag cours therapi prognosi differ substanti two group studi use singl photon emiss comput tomographi spect imag healthi normal earli pd swedd subject obtain parkinson progress marker initi ppmi databas process comput shape surfac fit base featur three group use featur develop compar various classif model discrimin scan show dopaminerg deficit pd scan without deficit healthi normal swedd along also compar featur striatal bind ratio sbr base featur well establish clinic use comput featur import score use random forest techniqu observ support vector machin svm classifi gave best perform accuraci featur also show higher import sbr base featur infer studi shape analysi surfac fit use promis method extract discriminatori featur use develop diagnost model might potenti help clinician diagnost process|['R. Prashanth', 'Sumantra Dutta Roy', 'Pravat K. Mandal', 'Shantanu Ghosh']|['stat.AP', 'cs.CV', 'physics.data-an', 'stat.CO', 'stat.ML']
2017-04-07T11:24:22Z|2017-03-04T19:07:42Z|http://arxiv.org/abs/1703.01506v1|http://arxiv.org/pdf/1703.01506v1|Accelerating Permutation Testing in Voxel-wise Analysis through Subspace   Tracking: A new plugin for SnPM|acceler permut test voxel wise analysi subspac track new plugin snpm|Permutation testing is a non-parametric method for obtaining the max null distribution used to compute corrected $p$-values to provide strong control of false positives. In neuroimaging, however, the computational burden of running such algorithm can be significant. We find that by viewing the permutation testing procedure as the construction of a very large permutation testing matrix $T$, one can exploit structural properties derived from the data and the test statistics to reduce the runtime under certain conditions. In particular, we see that $T$ has a low-rank plus a low-variance residual. This makes $T$ a good candidate for low-rank matrix completion methods, where only a very small number of entries of $T$ ($~0.35\%$ of all entries in our experiments) have to be computed to obtain good estimate of it. Based on this observation, we developed an algorithm, RapidPT, that is able to efficiently recover the max null distribution commonly obtained through regular permutation testing in neuroimage analysis. We present an extensive experimental validation on four varying sized datasets against two baselines: Statistical NonParametric Mapping (SnPM13) and a standard permutation testing implementation (referred to as NaivePT). We find that RapidPT achieves its best runtime performance on medium sized datasets ($50 \leq n \leq 200$), with speedup gains of 1.5x - 38x (vs. SnPM13) and 20x-1000x (vs. NaivePT). For larger datasets ($n \geq 200$) RapidPT outperforms NaivePT (6x - 200x), and provides substantial speedups over SnPM13 when performing more than 10000 permutations (2x - 15x). The Matlab implementation is available as a standalone toolbox called RapidPT. Our code is also integrated within SnPM13, and is able to leverage multi-core architectures when available.|permut test non parametr method obtain max null distribut use comput correct valu provid strong control fals posit neuroimag howev comput burden run algorithm signific find view permut test procedur construct veri larg permut test matrix one exploit structur properti deriv data test statist reduc runtim certain condit particular see low rank plus low varianc residu make good candid low rank matrix complet method onli veri small number entri entri experi comput obtain good estim base observ develop algorithm rapidpt abl effici recov max null distribut common obtain regular permut test neuroimag analysi present extens experiment valid four vari size dataset two baselin statist nonparametr map snpm standard permut test implement refer naivept find rapidpt achiev best runtim perform medium size dataset leq leq speedup gain vs snpm vs naivept larger dataset geq rapidpt outperform naivept provid substanti speedup snpm perform permut matlab implement avail standalon toolbox call rapidpt code also integr within snpm abl leverag multi core architectur avail|['Felipe Gutierrez-Barragan', 'Vamsi K. Ithapu', 'Chris Hinrichs', 'Camille Maumet', 'Sterling C. Johnson', 'Thomas E. Nichols', 'Vikas Singh', 'the ADNI']|['stat.AP', 'cs.CV', 'stat.ML']
2017-04-07T11:24:22Z|2017-03-03T16:29:21Z|http://arxiv.org/abs/1703.01234v1|http://arxiv.org/pdf/1703.01234v1|A Bayesian computer model analysis of Robust Bayesian analyses|bayesian comput model analysi robust bayesian analys|We harness the power of Bayesian emulation techniques, designed to aid the analysis of complex computer models, to examine the structure of complex Bayesian analyses themselves. These techniques facilitate robust Bayesian analyses and/or sensitivity analyses of complex problems, and hence allow global exploration of the impacts of choices made in both the likelihood and prior specification. We show how previously intractable problems in robustness studies can be overcome using emulation techniques, and how these methods allow other scientists to quickly extract approximations to posterior results corresponding to their own particular subjective specification. The utility and flexibility of our method is demonstrated on a reanalysis of a real application where Bayesian methods were employed to capture beliefs about river flow. We discuss the obvious extensions and directions of future research that such an approach opens up.|har power bayesian emul techniqu design aid analysi complex comput model examin structur complex bayesian analys themselv techniqu facilit robust bayesian analys sensit analys complex problem henc allow global explor impact choic made likelihood prior specif show previous intract problem robust studi overcom use emul techniqu method allow scientist quick extract approxim posterior result correspond particular subject specif util flexibl method demonstr reanalysi real applic bayesian method employ captur belief river flow discuss obvious extens direct futur research approach open|['Ian Vernon', 'John Paul Gosling']|['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']
2017-04-07T11:24:22Z|2017-03-03T06:28:36Z|http://arxiv.org/abs/1703.01051v1|http://arxiv.org/pdf/1703.01051v1|Interval Estimation of the Unknown Exponential Parameter Based on Time   Truncated Data|interv estim unknown exponenti paramet base time truncat data|In this paper we consider the statistical inference of the unknown parameter of an exponential distribution based on the time truncated data. The time truncated data occurs quite often in the reliability analysis for type-I or hybrid censoring cases. All the results available today are based on the conditional argument that at least one failure occurs during the experiment. In this paper we provide some inferential results based on the unconditional argument. We extend the results for some two-parameter distributions also.|paper consid statist infer unknown paramet exponenti distribut base time truncat data time truncat data occur quit often reliabl analysi type hybrid censor case result avail today base condit argument least one failur occur dure experi paper provid inferenti result base uncondit argument extend result two paramet distribut also|['Arnab Koley', 'Debasis Kundu']|['stat.AP', '62F10, 62F03, 62H12']
