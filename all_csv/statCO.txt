2017-03-28T14:03:09Z|2017-03-27T16:14:18Z|http://arxiv.org/abs/1703.09163v1|http://arxiv.org/pdf/1703.09163v1|Scalable Bayesian shrinkage and uncertainty quantification in   high-dimensional regression|scalabl bayesian shrinkag uncertainti quantif high dimension regress|"Bayesian shrinkage methods have generated a lot of recent interest as tools for high-dimensional regression and model selection. These methods naturally facilitate tractable uncertainty quantification and incorporation of prior information. A common feature of these models, including the Bayesian lasso, global-local shrinkage priors, and spike-and-slab priors is that the corresponding priors on the regression coefficients can be expressed as scale mixture of normals. While the three-step Gibbs sampler used to sample from the often intractable associated posterior density has been shown to be geometrically ergodic for several of these models (Khare and Hobert, 2013; Pal and Khare, 2014), it has been demonstrated recently that convergence of this sampler can still be quite slow in modern high-dimensional settings despite this apparent theoretical safeguard. We propose a new method to draw from the same posterior via a tractable two-step blocked Gibbs sampler. We demonstrate that our proposed two-step blocked sampler exhibits vastly superior convergence behavior compared to the original three- step sampler in high-dimensional regimes on both real and simulated data. We also provide a detailed theoretical underpinning to the new method in the context of the Bayesian lasso. First, we derive explicit upper bounds for the (geometric) rate of convergence. Furthermore, we demonstrate theoretically that while the original Bayesian lasso chain is not Hilbert-Schmidt, the proposed chain is trace class (and hence Hilbert-Schmidt). The trace class property has useful theoretical and practical implications. It implies that the corresponding Markov operator is compact, and its eigenvalues are summable. It also facilitates a rigorous comparison of the two-step blocked chain with ""sandwich"" algorithms which aim to improve performance of the two-step chain by inserting an inexpensive extra step."|bayesian shrinkag method generat lot recent interest tool high dimension regress model select method natur facilit tractabl uncertainti quantif incorpor prior inform common featur model includ bayesian lasso global local shrinkag prior spike slab prior correspond prior regress coeffici express scale mixtur normal three step gibb sampler use sampl often intract associ posterior densiti shown geometr ergod sever model khare hobert pal khare demonstr recent converg sampler still quit slow modern high dimension set despit appar theoret safeguard propos new method draw posterior via tractabl two step block gibb sampler demonstr propos two step block sampler exhibit vast superior converg behavior compar origin three step sampler high dimension regim real simul data also provid detail theoret underpin new method context bayesian lasso first deriv explicit upper bound geometr rate converg furthermor demonstr theoret origin bayesian lasso chain hilbert schmidt propos chain trace class henc hilbert schmidt trace class properti use theoret practic implic impli correspond markov oper compact eigenvalu summabl also facilit rigor comparison two step block chain sandwich algorithm aim improv perform two step chain insert inexpens extra step|['Bala Rajaratnam', 'Doug Sparks', 'Kshitij Khare', 'Liyuan Zhang']|['stat.CO']
2017-03-28T14:03:09Z|2017-03-27T13:41:00Z|http://arxiv.org/abs/1703.09074v1|http://arxiv.org/pdf/1703.09074v1|Randomized CP Tensor Decomposition|random cp tensor decomposit|The CANDECOMP/PARAFAC (CP) tensor decomposition is a popular dimensionality-reduction method for multiway data. Dimensionality reduction is often sought since many high-dimensional tensors have low intrinsic rank relative to the dimension of the ambient measurement space. However, the emergence of `big data' poses significant computational challenges for computing this fundamental tensor decomposition. Leveraging modern randomized algorithms, we demonstrate that the coherent structure can be learned from a smaller representation of the tensor in a fraction of the time. Moreover, the high-dimensional signal can be faithfully approximated from the compressed measurements. Thus, this simple but powerful algorithm enables one to compute the approximate CP decomposition even for massive tensors. The approximation error can thereby be controlled via oversampling and the computation of power iterations. In addition to theoretical results, several empirical results demonstrate the performance of the proposed algorithm.|candecomp parafac cp tensor decomposit popular dimension reduct method multiway data dimension reduct often sought sinc mani high dimension tensor low intrins rank relat dimens ambient measur space howev emerg big data pose signific comput challeng comput fundament tensor decomposit leverag modern random algorithm demonstr coher structur learn smaller represent tensor fraction time moreov high dimension signal faith approxim compress measur thus simpl power algorithm enabl one comput approxim cp decomposit even massiv tensor approxim error therebi control via oversampl comput power iter addit theoret result sever empir result demonstr perform propos algorithm|['N. Benjamin Erichson', 'Krithika Manohar', 'Steven L. Brunton', 'J. Nathan Kutz']|['cs.NA', 'stat.CO']
2017-03-28T14:03:09Z|2017-03-27T13:33:39Z|http://arxiv.org/abs/1703.09062v1|http://arxiv.org/pdf/1703.09062v1|A numerical method for the estimation of time-varying parameter models   in large dimensions|numer method estim time vari paramet model larg dimens|A novel numerical method for the estimation of large time-varying parameter (TVP) models is proposed. The Kalman filter and Kalman smoother estimates of the TVP model are derived within the context of generalised linear least squares and through the use of numerical linear algebra. The method developed is based on numerically stable and computationally efficient strategies. The computational cost is reduced by exploiting the special sparse structure of the TVP model and by utilising previous computations. The proposed method is also extended to solve the downdating problem of removing the effect of some observations from current estimates and also to the rolling window estimation of the TVP model. Experimental results show the effectiveness of the new strategies in high dimensions when a large number of covariates are included in the TVP model.|novel numer method estim larg time vari paramet tvp model propos kalman filter kalman smoother estim tvp model deriv within context generalis linear least squar use numer linear algebra method develop base numer stabl comput effici strategi comput cost reduc exploit special spars structur tvp model utilis previous comput propos method also extend solv downdat problem remov effect observ current estim also roll window estim tvp model experiment result show effect new strategi high dimens larg number covari includ tvp model|['Stella Hadjiantoni', 'Erricos J. Kontoghiorghes']|['stat.ME', 'stat.CO']
2017-03-28T14:03:09Z|2017-03-26T22:49:31Z|http://arxiv.org/abs/1703.08882v1|http://arxiv.org/pdf/1703.08882v1|A Mixture of Matrix Variate Skew-t Distributions|mixtur matrix variat skew distribut|Clustering is the process of finding underlying group structures in data. Although model-based clustering is firmly established in the multivariate case, there is relative paucity for matrix variate distributions, and there are even fewer examples using matrix variate skew distributions. In this paper, we look at parameter estimation for a finite mixture of matrix variate skew-t distributions in the context of model-based clustering. Simulated data is used for illustrative purposes.|cluster process find group structur data although model base cluster firm establish multivari case relat pauciti matrix variat distribut even fewer exampl use matrix variat skew distribut paper look paramet estim finit mixtur matrix variat skew distribut context model base cluster simul data use illustr purpos|['Michael P. B. Gallaugher', 'Paul D. McNicholas']|['stat.ME', 'stat.CO']
2017-03-28T14:03:09Z|2017-03-25T17:57:31Z|http://arxiv.org/abs/1703.08723v1|http://arxiv.org/pdf/1703.08723v1|Extending Growth Mixture Models Using Continuous Non-Elliptical   Distributions|extend growth mixtur model use continu non ellipt distribut|Growth mixture models (GMMs) incorporate both conventional random effects growth modeling and latent trajectory classes as in finite mixture modeling; therefore, they offer a way to handle the unobserved heterogeneity between subjects in their development. GMMs with Gaussian random effects dominate the literature. When the data are asymmetric and/or have heavier tails, more than one latent class is required to capture the observed variable distribution. Therefore, a GMM with continuous non-elliptical distributions is proposed to capture skewness and heavier tails in the data set. Specifically, multivariate skew-t distributions and generalized hyperbolic distributions are introduced to extend GMMs. When extending GMMs, four statistical models are considered with differing distributions of measurement errors and random effects. The mathematical development of a GMM with non-elliptical distributions relies on its relationship with the generalized inverse Gaussian distribution. Parameter estimation is outlined within the expectation-maximization framework before the performance of our GMM with non-elliptical distributions is illustrated on simulated and real data.|growth mixtur model gmms incorpor convent random effect growth model latent trajectori class finit mixtur model therefor offer way handl unobserv heterogen subject develop gmms gaussian random effect domin literatur data asymmetr heavier tail one latent class requir captur observ variabl distribut therefor gmm continu non ellipt distribut propos captur skew heavier tail data set specif multivari skew distribut general hyperbol distribut introduc extend gmms extend gmms four statist model consid differ distribut measur error random effect mathemat develop gmm non ellipt distribut reli relationship general invers gaussian distribut paramet estim outlin within expect maxim framework befor perform gmm non ellipt distribut illustr simul real data|['Yuhong Wei', 'Emilie Shireman', 'Paul D. McNicholas', 'Douglas L. Steinley']|['stat.ME', 'stat.AP', 'stat.CO']
2017-03-28T14:03:09Z|2017-03-25T11:20:21Z|http://arxiv.org/abs/1703.08676v1|http://arxiv.org/pdf/1703.08676v1|Statistical and Computational Tradeoff in Genetic Algorithm-Based   Estimation|statist comput tradeoff genet algorithm base estim|When a Genetic Algorithm (GA), or a stochastic algorithm in general, is employed in a statistical problem, the obtained result is affected by both variability due to sampling, that refers to the fact that only a sample is observed, and variability due to the stochastic elements of the algorithm. This topic can be easily set in a framework of statistical and computational tradeoff question, crucial in recent problems, for which statisticians must carefully set statistical and computational part of the analysis, taking account of some resource or time constraints. In the present work we analyze estimation problems tackled by GAs, for which variability of estimates can be decomposed in the two sources of variability, considering some constraints in the form of cost functions, related to both data acquisition and runtime of the algorithm. Simulation studies will be presented to discuss the statistical and computational tradeoff question.|genet algorithm ga stochast algorithm general employ statist problem obtain result affect variabl due sampl refer fact onli sampl observ variabl due stochast element algorithm topic easili set framework statist comput tradeoff question crucial recent problem statistician must care set statist comput part analysi take account resourc time constraint present work analyz estim problem tackl gas variabl estim decompos two sourc variabl consid constraint form cost function relat data acquisit runtim algorithm simul studi present discuss statist comput tradeoff question|['Manuel Rizzo', 'Francesco Battaglia']|['stat.CO']
2017-03-28T14:03:09Z|2017-03-24T23:49:33Z|http://arxiv.org/abs/1703.08627v1|http://arxiv.org/pdf/1703.08627v1|Random sampling of Latin squares via binary contingency tables and   probabilistic divide-and-conquer|random sampl latin squar via binari conting tabl probabilist divid conquer|We demonstrate a novel approach for the random sampling of Latin squares of order~$n$ via probabilistic divide-and-conquer. The algorithm divides the entries of the table modulo powers of $2$, and samples a corresponding binary contingency table at each level. The sampling distribution is based on the Boltzmann sampling heuristic, along with probabilistic divide-and-conquer.|demonstr novel approach random sampl latin squar order via probabilist divid conquer algorithm divid entri tabl modulo power sampl correspond binari conting tabl level sampl distribut base boltzmann sampl heurist along probabilist divid conquer|['Stephen DeSalvo']|['stat.CO']
2017-03-28T14:03:09Z|2017-03-24T17:17:45Z|http://arxiv.org/abs/1703.08520v1|http://arxiv.org/pdf/1703.08520v1|Rejection-free Ensemble MCMC with applications to Factorial Hidden   Markov Models|reject free ensembl mcmc applic factori hidden markov model|"Bayesian inference for complex models is challenging due to the need to explore high-dimensional spaces and multimodality and standard Monte Carlo samplers can have difficulties effectively exploring the posterior. We introduce a general purpose rejection-free ensemble Markov Chain Monte Carlo (MCMC) technique to improve on existing poorly mixing samplers. This is achieved by combining parallel tempering and an auxiliary variable move to exchange information between the chains. We demonstrate this ensemble MCMC scheme on Bayesian inference in Factorial Hidden Markov Models. This high-dimensional inference problem is difficult due to the exponentially sized latent variable space. Existing sampling approaches mix slowly and can get trapped in local modes. We show that the performance of these samplers is improved by our rejection-free ensemble technique and that the method is attractive and ""easy-to-use"" since no parameter tuning is required."|bayesian infer complex model challeng due need explor high dimension space multimod standard mont carlo sampler difficulti effect explor posterior introduc general purpos reject free ensembl markov chain mont carlo mcmc techniqu improv exist poor mix sampler achiev combin parallel temper auxiliari variabl move exchang inform chain demonstr ensembl mcmc scheme bayesian infer factori hidden markov model high dimension infer problem difficult due exponenti size latent variabl space exist sampl approach mix slowli get trap local mode show perform sampler improv reject free ensembl techniqu method attract easi use sinc paramet tune requir|['Kaspar Märtens', 'Michalis K Titsias', 'Christopher Yau']|['stat.CO', 'stat.ME', 'stat.ML']
2017-03-28T14:03:09Z|2017-03-21T15:42:38Z|http://arxiv.org/abs/1703.07285v1|http://arxiv.org/pdf/1703.07285v1|From safe screening rules to working sets for faster Lasso-type solvers|safe screen rule work set faster lasso type solver|Convex sparsity-promoting regularizations are ubiquitous in modern statistical learning. By construction, they yield solutions with few non-zero coefficients, which correspond to saturated constraints in the dual optimization formulation. Working set (WS) strategies are generic optimization techniques that consist in solving simpler problems that only consider a subset of constraints, whose indices form the WS. Working set methods therefore involve two nested iterations: the outer loop corresponds to the definition of the WS and the inner loop calls a solver for the subproblems. For the Lasso estimator a WS is a set of features, while for a Group Lasso it refers to a set of groups. In practice, WS are generally small in this context so the associated feature Gram matrix can fit in memory. Here we show that the Gauss-Southwell rule (a greedy strategy for block coordinate descent techniques) leads to fast solvers in this case. Combined with a working set strategy based on an aggressive use of so-called Gap Safe screening rules, we propose a solver achieving state-of-the-art performance on sparse learning problems. Results are presented on Lasso and multi-task Lasso estimators.|convex sparsiti promot regular ubiquit modern statist learn construct yield solut non zero coeffici correspond satur constraint dual optim formul work set ws strategi generic optim techniqu consist solv simpler problem onli consid subset constraint whose indic form ws work set method therefor involv two nest iter outer loop correspond definit ws inner loop call solver subproblem lasso estim ws set featur group lasso refer set group practic ws general small context associ featur gram matrix fit memori show gauss southwel rule greedi strategi block coordin descent techniqu lead fast solver case combin work set strategi base aggress use call gap safe screen rule propos solver achiev state art perform spars learn problem result present lasso multi task lasso estim|['Mathurin Massias', 'Alexandre Gramfort', 'Joseph Salmon']|['stat.ML', 'cs.LG', 'math.OC', 'stat.CO']
2017-03-28T14:03:09Z|2017-03-21T03:27:22Z|http://arxiv.org/abs/1703.07039v1|http://arxiv.org/pdf/1703.07039v1|A Simple Online Parameter Estimation Technique with Asymptotic   Guarantees|simpl onlin paramet estim techniqu asymptot guarante|In many modern settings, data are acquired iteratively over time, rather than all at once. Such settings are known as online, as opposed to offline or batch. We introduce a simple technique for online parameter estimation, which can operate in low memory settings, settings where data are correlated, and only requires a single inspection of the available data at each time period. We show that the estimators---constructed via the technique---are asymptotically normal under generous assumptions, and present a technique for the online computation of the covariance matrices for such estimators. A set of numerical studies demonstrates that our estimators can be as efficient as their offline counterparts, and that our technique generates estimates and confidence intervals that match their offline counterparts in various parameter estimation settings.|mani modern set data acquir iter time rather onc set known onlin oppos offlin batch introduc simpl techniqu onlin paramet estim oper low memori set set data correl onli requir singl inspect avail data time period show estim construct via techniqu asymptot normal generous assumpt present techniqu onlin comput covari matric estim set numer studi demonstr estim effici offlin counterpart techniqu generat estim confid interv match offlin counterpart various paramet estim set|['Hien D Nguyen']|['stat.CO']
2017-03-28T14:03:13Z|2017-03-19T10:51:41Z|http://arxiv.org/abs/1703.06419v1|http://arxiv.org/pdf/1703.06419v1|Multivariate Functional Data Visualization and Outlier Detection|multivari function data visual outlier detect|This article proposes a new graphical tool, the magnitude-shape (MS) plot, for visualizing both the magnitude and shape outlyingness of multivariate functional data. The proposed tool builds on the recent notion of functional directional outlyingness, which measures the centrality of functional data by simultaneously considering the level and the direction of their deviation from the central region. The MS-plot intuitively presents not only levels but also directions of magnitude outlyingness on the horizontal axis or plane, and demonstrates shape outlyingness on the vertical axis. A dividing curve or surface is provided to separate non-outlying data from the outliers. Both the simulated data and the practical examples confirm that the MS-plot is superior to existing tools for visualizing centrality and detecting outliers for functional data.|articl propos new graphic tool magnitud shape ms plot visual magnitud shape outlying multivari function data propos tool build recent notion function direct outlying measur central function data simultan consid level direct deviat central region ms plot intuit present onli level also direct magnitud outlying horizont axi plane demonstr shape outlying vertic axi divid curv surfac provid separ non data outlier simul data practic exampl confirm ms plot superior exist tool visual central detect outlier function data|['Wenlin Dai', 'Marc G. Genton']|['stat.ME', 'stat.CO']
2017-03-28T14:03:13Z|2017-03-18T22:12:08Z|http://arxiv.org/abs/1703.06359v1|http://arxiv.org/pdf/1703.06359v1|Fully symmetric kernel quadrature|fulli symmetr kernel quadratur|Kernel quadratures and other kernel-based approximation methods typically suffer from prohibitive cubic time and quadratic space complexity in the number of function evaluations. The problem arises because a system of linear equations needs to be solved. In this article we show that the weights of a kernel quadrature rule can be computed efficiently and exactly for up to tens of millions of nodes if the kernel, integration domain, and measure are fully symmetric and the node set is a union of fully symmetric sets. This is based on the observations that in such a setting there are only as many distinct weights as there are fully symmetric sets and that these weights can be solved from a linear system of equations constructed out of row sums of certain submatrices of the full kernel matrix. We present several numerical examples that show feasibility, both for a large number of nodes and in high dimensions, of the developed fully symmetric kernel quadrature rules. Most prominent of the fully symmetric kernel quadrature rules we propose are those that use sparse grids.|kernel quadratur kernel base approxim method typic suffer prohibit cubic time quadrat space complex number function evalu problem aris becaus system linear equat need solv articl show weight kernel quadratur rule comput effici exact ten million node kernel integr domain measur fulli symmetr node set union fulli symmetr set base observ set onli mani distinct weight fulli symmetr set weight solv linear system equat construct row sum certain submatric full kernel matrix present sever numer exampl show feasibl larg number node high dimens develop fulli symmetr kernel quadratur rule promin fulli symmetr kernel quadratur rule propos use spars grid|['Toni Karvonen', 'Simo Särkkä']|['math.NA', 'cs.NA', 'stat.CO']
2017-03-28T14:03:13Z|2017-03-17T21:49:28Z|http://arxiv.org/abs/1703.06206v1|http://arxiv.org/pdf/1703.06206v1|Sequential Monte Carlo Methods in the nimble R Package|sequenti mont carlo method nimbl packag|nimble is an R package for constructing algorithms and conducting inference on hierarchical models. The nimble package provides a unique combination of flexible model specification and the ability to program model-generic algorithms -- specifically, the package allows users to code models in the BUGS language, and it allows users to write algorithms that can be applied to any appropriately-specified BUGS model. In this paper, we introduce nimble's capabilities for state-space model analysis using Sequential Monte Carlo (SMC) techniques. We first provide an overview of state-space models and commonly used SMC algorithms. We then describe how to build a state-space model and conduct inference using existing SMC algorithms within nimble. SMC algorithms within nimble currently include the bootstrap filter, auxiliary particle filter, Liu and West filter, ensemble Kalman filter, and a particle MCMC sampler. These algorithms can be run in R or compiled into C++ for more efficient execution. Examples of applying SMC algorithms to a random walk model and a stochastic volatility model are provided. Finally, we give an overview of how model-generic algorithms are coded within nimble by providing code for a simple SMC algorithm.|nimbl packag construct algorithm conduct infer hierarch model nimbl packag provid uniqu combin flexibl model specif abil program model generic algorithm specif packag allow user code model bug languag allow user write algorithm appli ani appropri specifi bug model paper introduc nimbl capabl state space model analysi use sequenti mont carlo smc techniqu first provid overview state space model common use smc algorithm describ build state space model conduct infer use exist smc algorithm within nimbl smc algorithm within nimbl current includ bootstrap filter auxiliari particl filter liu west filter ensembl kalman filter particl mcmc sampler algorithm run compil effici execut exampl appli smc algorithm random walk model stochast volatil model provid final give overview model generic algorithm code within nimbl provid code simpl smc algorithm|['Nicholas Michaud', 'Perry de Valpine', 'Daniel Turek', 'Christopher J. Paciorek']|['stat.CO']
2017-03-28T14:03:13Z|2017-03-17T17:50:44Z|http://arxiv.org/abs/1703.06131v1|http://arxiv.org/pdf/1703.06131v1|Inference via low-dimensional couplings|infer via low dimension coupl|"Integration against an intractable probability measure is among the fundamental challenges of statistical inference, particularly in the Bayesian setting. A principled approach to this problem seeks a deterministic coupling of the measure of interest with a tractable ""reference"" measure (e.g., a standard Gaussian). This coupling is induced by a transport map, and enables direct simulation from the desired measure simply by evaluating the transport map at samples from the reference. Yet characterizing such a map---e.g., representing and evaluating it---grows challenging in high dimensions. The central contribution of this paper is to establish a link between the Markov properties of the target measure and the existence of certain low-dimensional couplings, induced by transport maps that are sparse or decomposable. Our analysis not only facilitates the construction of couplings in high-dimensional settings, but also suggests new inference methodologies. For instance, in the context of nonlinear and non-Gaussian state space models, we describe new online and single-pass variational algorithms that characterize the full posterior distribution of the sequential inference problem using operations only slightly more complex than regular filtering."|integr intract probabl measur among fundament challeng statist infer particular bayesian set principl approach problem seek determinist coupl measur interest tractabl refer measur standard gaussian coupl induc transport map enabl direct simul desir measur simpli evalu transport map sampl refer yet character map repres evalu grow challeng high dimens central contribut paper establish link markov properti target measur exist certain low dimension coupl induc transport map spars decompos analysi onli facilit construct coupl high dimension set also suggest new infer methodolog instanc context nonlinear non gaussian state space model describ new onlin singl pass variat algorithm character full posterior distribut sequenti infer problem use oper onli slight complex regular filter|['Alessio Spantini', 'Daniele Bigoni', 'Youssef Marzouk']|['stat.ME', 'stat.CO', 'stat.ML']
2017-03-28T14:03:13Z|2017-03-17T17:00:53Z|http://arxiv.org/abs/1703.06098v1|http://arxiv.org/pdf/1703.06098v1|Analysis of the Gibbs Sampler for Gaussian hierarchical models via   multigrid decomposition|analysi gibb sampler gaussian hierarch model via multigrid decomposit|We study the convergence properties of the Gibbs Sampler in the context of posterior distributions arising from Bayesian analysis of Gaussian hierarchical models. We consider centred and non-centred parameterizations as well as their hybrids including the full family of partially non-centred parameterizations. We develop a novel methodology based on multi-grid decompositions to derive analytic expressions for the convergence rates of the algorithm for an arbitrary number of layers in the hierarchy, while previous work was typically limited to the two-level case. Our work gives a complete understanding for the three-level symmetric case and this gives rise to approximations for the non-symmetric case. We also give analogous, if less explicit, results for models of arbitrary level. This theory gives rise to simple and easy-to-implement guidelines for the practical implementation of Gibbs samplers on conditionally Gaussian hierarchical models.|studi converg properti gibb sampler context posterior distribut aris bayesian analysi gaussian hierarch model consid centr non centr parameter well hybrid includ full famili partial non centr parameter develop novel methodolog base multi grid decomposit deriv analyt express converg rate algorithm arbitrari number layer hierarchi previous work typic limit two level case work give complet understand three level symmetr case give rise approxim non symmetr case also give analog less explicit result model arbitrari level theori give rise simpl easi implement guidelin practic implement gibb sampler condit gaussian hierarch model|['Giacomo Zanella', 'Gareth Roberts']|['stat.CO', 'math.PR', 'stat.ME', '60J22, 62F15, 65C40, 65C05']
2017-03-28T14:03:13Z|2017-03-17T12:11:34Z|http://arxiv.org/abs/1703.05984v1|http://arxiv.org/pdf/1703.05984v1|A Tutorial on Bridge Sampling|tutori bridg sampl|The marginal likelihood plays an important role in many areas of Bayesian statistics such as parameter estimation, model comparison, and model averaging. In most applications, however, the marginal likelihood is not analytically tractable and must be approximated using numerical methods. Here we provide a tutorial on bridge sampling (Bennett, 1976; Meng & Wong, 1996), a reliable and relatively straightforward sampling method that allows researchers to obtain the marginal likelihood for models of varying complexity. First, we introduce bridge sampling and three related sampling methods using the beta-binomial model as a running example. We then apply bridge sampling to estimate the marginal likelihood for the Expectancy Valence (EV) model---a popular model for reinforcement learning. Our results indicate that bridge sampling provides accurate estimates for both a single participant and a hierarchical version of the EV model. We conclude that bridge sampling is an attractive method for mathematical psychologists who typically aim to approximate the marginal likelihood for a limited set of possibly high-dimensional models.|margin likelihood play import role mani area bayesian statist paramet estim model comparison model averag applic howev margin likelihood analyt tractabl must approxim use numer method provid tutori bridg sampl bennett meng wong reliabl relat straightforward sampl method allow research obtain margin likelihood model vari complex first introduc bridg sampl three relat sampl method use beta binomi model run exampl appli bridg sampl estim margin likelihood expect valenc ev model popular model reinforc learn result indic bridg sampl provid accur estim singl particip hierarch version ev model conclud bridg sampl attract method mathemat psychologist typic aim approxim margin likelihood limit set possibl high dimension model|['Quentin F. Gronau', 'Alexandra Sarafoglou', 'Dora Matzke', 'Alexander Ly', 'Udo Boehm', 'Maarten Marsman', 'David S. Leslie', 'Jonathan J. Forster', 'Eric-Jan Wagenmakers', 'Helen Steingroever']|['stat.CO']
2017-03-28T14:03:13Z|2017-03-16T14:09:50Z|http://arxiv.org/abs/1703.06826v1|http://arxiv.org/pdf/1703.06826v1|RatingScaleReduction package: stepwise rating scale item reduction   without predictability loss|ratingscalereduct packag stepwis rate scale item reduct without predict loss|"This study presents an innovative method for reducing the number of rating scale items without predictability loss. The ""area under the re- ceiver operator curve method"" (AUC ROC) is used to implement in the RatingScaleReduction package posted on CRAN. Several cases have been used to illustrate how the stepwise method has reduced the number of rating scale items (variables)."|studi present innov method reduc number rate scale item without predict loss area ceiver oper curv method auc roc use implement ratingscalereduct packag post cran sever case use illustr stepwis method reduc number rate scale item variabl|['Waldemar W. Koczkodaj', 'Alicja Wolny-Dominiak']|['stat.CO', '94A50, 62C25, 62C99, 62P10']
2017-03-28T14:03:13Z|2017-03-16T09:01:22Z|http://arxiv.org/abs/1703.05511v1|http://arxiv.org/pdf/1703.05511v1|An Induced Natural Selection Heuristic for Evaluating Optimal Bayesian   Experimental Designs|induc natur select heurist evalu optim bayesian experiment design|"Bayesian optimal experimental design has immense potential to inform the collection of data, so as to subsequently enhance our understanding of a variety of processes. However, a major impediment is the difficulty in evaluating optimal designs for problems with large, or high-dimensional, design spaces. We propose an efficient search heuristic suitable for general optimisation problems, with a particular focus on optimal Bayesian experimental design problems. The heuristic evaluates the objective (utility) function at an initial, randomly generated set of input values. At each generation of the algorithm, input values are ""accepted"" if their corresponding objective (utility) function satisfies some acceptance criteria, and new inputs are sampled about these accepted points. We demonstrate the new algorithm by evaluating the optimal Bayesian experimental designs for two popular stochastic models: a Markovian death model, and a pharmacokinetic model. The designs from this new algorithm are compared to those evaluated by existing algorithms, and computation times are given as a demonstration of the computational efficiency. A comparison to the current ""gold-standard"" method are given, to demonstrate that INSH finds designs that contain a similar amount of information, but more computationally efficiently. We also consider a simple approach to the construction of sampling windows for the pharmacokinetic model using the output of the proposed algorithm."|bayesian optim experiment design immens potenti inform collect data subsequ enhanc understand varieti process howev major impedi difficulti evalu optim design problem larg high dimension design space propos effici search heurist suitabl general optimis problem particular focus optim bayesian experiment design problem heurist evalu object util function initi random generat set input valu generat algorithm input valu accept correspond object util function satisfi accept criteria new input sampl accept point demonstr new algorithm evalu optim bayesian experiment design two popular stochast model markovian death model pharmacokinet model design new algorithm compar evalu exist algorithm comput time given demonstr comput effici comparison current gold standard method given demonstr insh find design contain similar amount inform comput effici also consid simpl approach construct sampl window pharmacokinet model use output propos algorithm|['David J. Price', 'Nigel G. Bean', 'Joshua V. Ross', 'Jonathan Tuke']|['stat.CO']
2017-03-28T14:03:13Z|2017-03-16T05:00:37Z|http://arxiv.org/abs/1703.05471v1|http://arxiv.org/pdf/1703.05471v1|Model selection and parameter inference in phylogenetics using Nested   Sampling|model select paramet infer phylogenet use nest sampl|Bayesian inference methods rely on numerical algorithms for both model selection and parameter inference. In general, these algorithms require a high computational effort to yield reliable inferences. One of the major challenges in phylogenetics regards the estimation of the marginal likelihood. This quantity is commonly used for comparing different evolutionary models, but its calculation, even for simple models, incurs high computational cost. Another interesting challenge regards the estimation of the posterior distribution. Often, long Markov chains are required to get sufficient samples to carry out parameter inference, especially for tree distributions. In general, these problems are addressed separately by using different procedures. Nested sampling (NS) is a Bayesian algorithm which provides the means to estimate marginal likelihoods and to sample from the posterior distribution at no extra cost. In this paper, we introduce NS to phylogenetics. Its performance is analysed under different scenarios and compared to established methods. We conclude that NS is a very competitive and attractive algorithm for phylogenetic inference.|bayesian infer method reli numer algorithm model select paramet infer general algorithm requir high comput effort yield reliabl infer one major challeng phylogenet regard estim margin likelihood quantiti common use compar differ evolutionari model calcul even simpl model incur high comput cost anoth interest challeng regard estim posterior distribut often long markov chain requir get suffici sampl carri paramet infer especi tree distribut general problem address separ use differ procedur nest sampl ns bayesian algorithm provid mean estim margin likelihood sampl posterior distribut extra cost paper introduc ns phylogenet perform analys differ scenario compar establish method conclud ns veri competit attract algorithm phylogenet infer|['Patricio Maturana R.', 'Brendon J. Brewer', 'Steffen Klaere']|['q-bio.QM', 'stat.CO']
2017-03-28T14:03:14Z|2017-03-26T15:00:32Z|http://arxiv.org/abs/1703.05144v2|http://arxiv.org/pdf/1703.05144v2|Bergm: Bayesian exponential random graph models in R|bergm bayesian exponenti random graph model|The Bergm package provides a comprehensive framework for Bayesian inference using Markov chain Monte Carlo (MCMC) algorithms. It can also supply graphical Bayesian goodness-of-fit procedures that address the issue of model adequacy. The package is simple to use and represents an attractive way of analysing network data as it offers the advantage of a complete probabilistic treatment of uncertainty. Bergm is based on the ergm package and therefore it makes use of the same model set-up and network simulation algorithms. The Bergm package has been continually improved in terms of speed performance over the last years and now offers the end-user a feasible option for carrying out Bayesian inference for networks with several thousands of nodes.|bergm packag provid comprehens framework bayesian infer use markov chain mont carlo mcmc algorithm also suppli graphic bayesian good fit procedur address issu model adequaci packag simpl use repres attract way analys network data offer advantag complet probabilist treatment uncertainti bergm base ergm packag therefor make use model set network simul algorithm bergm packag continu improv term speed perform last year offer end user feasibl option carri bayesian infer network sever thousand node|['Alberto Caimo', 'Nial Friel']|['stat.CO']
2017-03-28T14:03:17Z|2017-03-15T10:20:32Z|http://arxiv.org/abs/1703.05060v1|http://arxiv.org/pdf/1703.05060v1|Online Learning for Distribution-Free Prediction|onlin learn distribut free predict|We develop an online learning method for prediction, which is important in problems with large and/or streaming data sets. We formulate the learning approach using a covariance-fitting methodology, and show that the resulting predictor has desirable computational and distribution-free properties: It is implemented online with a runtime that scales linearly in the number of samples; has a constant memory requirement; avoids local minima problems; and prunes away redundant feature dimensions without relying on restrictive assumptions on the data distribution. In conjunction with the split conformal approach, it also produces distribution-free prediction confidence intervals in a computationally efficient manner. The method is demonstrated on both real and synthetic datasets.|develop onlin learn method predict import problem larg stream data set formul learn approach use covari fit methodolog show result predictor desir comput distribut free properti implement onlin runtim scale linear number sampl constant memori requir avoid local minima problem prune away redund featur dimens without reli restrict assumpt data distribut conjunct split conform approach also produc distribut free predict confid interv comput effici manner method demonstr real synthet dataset|['Dave Zachariah', 'Petre Stoica', 'Thomas B. Schön']|['cs.LG', 'stat.CO', 'stat.ML']
2017-03-28T14:03:17Z|2017-03-15T01:18:57Z|http://arxiv.org/abs/1703.04866v1|http://arxiv.org/pdf/1703.04866v1|Multilevel Sequential Monte Carlo with Dimension-Independent   Likelihood-Informed Proposals|multilevel sequenti mont carlo dimens independ likelihood inform propos|In this article we develop a new sequential Monte Carlo (SMC) method for multilevel (ML) Monte Carlo estimation. In particular, the method can be used to estimate expectations with respect to a target probability distribution over an infinite-dimensional and non-compact space as given, for example, by a Bayesian inverse problem with Gaussian random field prior. Under suitable assumptions the MLSMC method has the optimal $O(\epsilon^{-2})$ bound on the cost to obtain a mean-square error of $O(\epsilon^2)$. The algorithm is accelerated by dimension-independent likelihood-informed (DILI) proposals designed for Gaussian priors, leveraging a novel variation which uses empirical sample covariance information in lieu of Hessian information, hence eliminating the requirement for gradient evaluations. The efficiency of the algorithm is illustrated on two examples: inversion of noisy pressure measurements in a PDE model of Darcy flow to recover the posterior distribution of the permeability field, and inversion of noisy measurements of the solution of an SDE to recover the posterior path measure.|articl develop new sequenti mont carlo smc method multilevel ml mont carlo estim particular method use estim expect respect target probabl distribut infinit dimension non compact space given exampl bayesian invers problem gaussian random field prior suitabl assumpt mlsmc method optim epsilon bound cost obtain mean squar error epsilon algorithm acceler dimens independ likelihood inform dili propos design gaussian prior leverag novel variat use empir sampl covari inform lieu hessian inform henc elimin requir gradient evalu effici algorithm illustr two exampl invers noisi pressur measur pde model darci flow recov posterior distribut permeabl field invers noisi measur solut sde recov posterior path measur|['Alexandros Beskos', 'Ajay Jasra', 'Kody Law', 'Youssef Marzouk', 'Yan Zhou']|['stat.CO']
2017-03-28T14:03:17Z|2017-03-18T09:09:43Z|http://arxiv.org/abs/1703.04467v2|http://arxiv.org/pdf/1703.04467v2|spmoran: An R package for Moran's eigenvector-based spatial regression   analysis|spmoran packag moran eigenvector base spatial regress analysi|"The objective of this study is illustrating how to use ""spmoran,"" which is an R package for Moran's eigenvector-based spatial regression analysis. spmoran estimates regression models in the presence of spatial dependence, including eigenvector spatial filtering (ESF) and random effects ESF (RE-ESF) models. These models are allowed to have spatially varying coefficients to capture spatial heterogeneity. These ESF and RE-ESF models are suitable to estimate and infer regression coefficients with/without spatial variation. spmoran implements these models in a computationally efficient manner. For the illustration, this study applies ESF and RE-ESF models for a land price analysis."|object studi illustr use spmoran packag moran eigenvector base spatial regress analysi spmoran estim regress model presenc spatial depend includ eigenvector spatial filter esf random effect esf esf model model allow spatial vari coeffici captur spatial heterogen esf esf model suitabl estim infer regress coeffici without spatial variat spmoran implement model comput effici manner illustr studi appli esf esf model land price analysi|['Daisuke Murakami']|['stat.OT', 'stat.CO']
2017-03-28T14:03:17Z|2017-03-13T11:19:28Z|http://arxiv.org/abs/1703.04334v1|http://arxiv.org/pdf/1703.04334v1|Probabilistic Matching: Causal Inference under Measurement Errors|probabilist match causal infer measur error|The abundance of data produced daily from large variety of sources has boosted the need of novel approaches on causal inference analysis from observational data. Observational data often contain noisy or missing entries. Moreover, causal inference studies may require unobserved high-level information which needs to be inferred from other observed attributes. In such cases, inaccuracies of the applied inference methods will result in noisy outputs. In this study, we propose a novel approach for causal inference when one or more key variables are noisy. Our method utilizes the knowledge about the uncertainty of the real values of key variables in order to reduce the bias induced by noisy measurements. We evaluate our approach in comparison with existing methods both on simulated and real scenarios and we demonstrate that our method reduces the bias and avoids false causal inference conclusions in most cases.|abund data produc daili larg varieti sourc boost need novel approach causal infer analysi observ data observ data often contain noisi miss entri moreov causal infer studi may requir unobserv high level inform need infer observ attribut case inaccuraci appli infer method result noisi output studi propos novel approach causal infer one key variabl noisi method util knowledg uncertainti real valu key variabl order reduc bias induc noisi measur evalu approach comparison exist method simul real scenario demonstr method reduc bias avoid fals causal infer conclus case|['Fani Tsapeli', 'Peter Tino', 'Mirco Musolesi']|['stat.ME', 'stat.CO', 'stat.ML']
2017-03-28T14:03:17Z|2017-03-11T20:07:06Z|http://arxiv.org/abs/1703.04025v1|http://arxiv.org/pdf/1703.04025v1|Learning Large-Scale Bayesian Networks with the sparsebn Package|learn larg scale bayesian network sparsebn packag|Learning graphical models from data is an important problem with wide applications, ranging from genomics to the social sciences. Nowadays datasets typically have upwards of thousands---sometimes tens or hundreds of thousands---of variables and far fewer samples. To meet this challenge, we develop a new R package called sparsebn for learning the structure of large, sparse graphical models with a focus on Bayesian networks. While there are many existing packages for this task within the R ecosystem, this package focuses on the unique setting of learning large networks from high-dimensional data, possibly with interventions. As such, the methods provided place a premium on scalability and consistency in a high-dimensional setting. Furthermore, in the presence of interventions, the methods implemented here achieve the goal of learning a causal network from data. The sparsebn package is open-source and available on CRAN.|learn graphic model data import problem wide applic rang genom social scienc nowaday dataset typic upward thousand sometim ten hundr thousand variabl far fewer sampl meet challeng develop new packag call sparsebn learn structur larg spars graphic model focus bayesian network mani exist packag task within ecosystem packag focus uniqu set learn larg network high dimension data possibl intervent method provid place premium scalabl consist high dimension set furthermor presenc intervent method implement achiev goal learn causal network data sparsebn packag open sourc avail cran|['Bryon Aragam', 'Jiaying Gu', 'Qing Zhou']|['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']
2017-03-28T14:03:17Z|2017-03-10T13:43:06Z|http://arxiv.org/abs/1703.03680v1|http://arxiv.org/pdf/1703.03680v1|Strong convergence rates of probabilistic integrators for ordinary   differential equations|strong converg rate probabilist integr ordinari differenti equat|Probabilistic integration of a continuous dynamical system is a way of systematically introducing model error, at scales no larger than errors inroduced by standard numerical discretisation, in order to enable thorough exploration of possible responses of the system to inputs. It is thus a potentially useful approach in a number of applications such as forward uncertainty quantification, inverse problems, and data assimilation. We extend the convergence analysis of probabilistic integrators for deterministic ordinary differential equations, as proposed by Conrad et al. (Stat. Comput., 2016), to establish mean-square convergence in the uniform norm on discrete- or continuous-time solutions under relaxed regularity assumptions on the driving vector fields and their induced flows. Specifically, we show that randomised high-order integrators for globally Lipschitz flows and randomised Euler integrators for dissipative vector fields with polynomially-bounded local Lipschitz constants all have the same mean-square convergence rate as their deterministic counterparts, provided that the variance of the integration noise is not of higher order than the corresponding deterministic integrator.|probabilist integr continu dynam system way systemat introduc model error scale larger error inroduc standard numer discretis order enabl thorough explor possibl respons system input thus potenti use approach number applic forward uncertainti quantif invers problem data assimil extend converg analysi probabilist integr determinist ordinari differenti equat propos conrad et al stat comput establish mean squar converg uniform norm discret continu time solut relax regular assumpt drive vector field induc flow specif show randomis high order integr global lipschitz flow randomis euler integr dissip vector field polynomi bound local lipschitz constant mean squar converg rate determinist counterpart provid varianc integr nois higher order correspond determinist integr|['H. C. Lie', 'A. M. Stuart', 'T. J. Sullivan']|['math.NA', 'math.PR', 'math.ST', 'stat.CO', 'stat.TH', '65L20, 65C99, 37H10, 68W20']
2017-03-28T14:03:17Z|2017-03-09T21:57:00Z|http://arxiv.org/abs/1703.03475v1|http://arxiv.org/pdf/1703.03475v1|Auxiliary Variables for Bayesian Inference in Multi-Class Queueing   Networks|auxiliari variabl bayesian infer multi class queue network|Queue networks describe complex stochastic systems of both theoretical and practical interest. They provide the means to assess alterations, diagnose poor performance and evaluate robustness across sets of interconnected resources. In the present paper, we focus on the underlying continuous-time Markov chains induced by these networks, and we present a flexible method for drawing parameter inference in multi-class Markovian cases with switching and different service disciplines. The approach is directed towards the inferential problem with missing data and introduces a slice sampling technique with mappings to the measurable space of task transitions between service stations. The method deals with time and tractability issues, can handle prior system knowledge and overcomes common restrictions on service rates across existing inferential frameworks. Finally, the proposed algorithm is validated on synthetic data and applied to a real data set, obtained from a service delivery tasking tool implemented in two university hospitals.|queue network describ complex stochast system theoret practic interest provid mean assess alter diagnos poor perform evalu robust across set interconnect resourc present paper focus continu time markov chain induc network present flexibl method draw paramet infer multi class markovian case switch differ servic disciplin approach direct toward inferenti problem miss data introduc slice sampl techniqu map measur space task transit servic station method deal time tractabl issu handl prior system knowledg overcom common restrict servic rate across exist inferenti framework final propos algorithm valid synthet data appli real data set obtain servic deliveri task tool implement two univers hospit|['Iker Perez', 'David Hodge', 'Theodore Kypraios']|['stat.CO']
2017-03-28T14:03:17Z|2017-03-09T17:17:39Z|http://arxiv.org/abs/1703.03352v1|http://arxiv.org/pdf/1703.03352v1|A log-linear time algorithm for constrained changepoint detection|log linear time algorithm constrain changepoint detect|Changepoint detection is a central problem in time series and genomic data. For some applications, it is natural to impose constraints on the directions of changes. One example is ChIP-seq data, for which adding an up-down constraint improves peak detection accuracy, but makes the optimization problem more complicated. We show how a recently proposed functional pruning technique can be adapted to solve such constrained changepoint detection problems. This leads to a new algorithm which can solve problems with arbitrary affine constraints on adjacent segment means, and which has empirical time complexity that is log-linear in the amount of data. This algorithm achieves state-of-the-art accuracy in a benchmark of several genomic data sets, and is orders of magnitude faster than existing algorithms that have similar accuracy. Our implementation is available as the PeakSegPDPA function in the coseg R package, https://github.com/tdhock/coseg|changepoint detect central problem time seri genom data applic natur impos constraint direct chang one exampl chip seq data ad constraint improv peak detect accuraci make optim problem complic show recent propos function prune techniqu adapt solv constrain changepoint detect problem lead new algorithm solv problem arbitrari affin constraint adjac segment mean empir time complex log linear amount data algorithm achiev state art accuraci benchmark sever genom data set order magnitud faster exist algorithm similar accuraci implement avail peaksegpdpa function coseg packag https github com tdhock coseg|['Toby Dylan Hocking', 'Guillem Rigaill', 'Paul Fearnhead', 'Guillaume Bourque']|['stat.CO', 'q-bio.GN', 'stat.ML']
2017-03-28T14:03:17Z|2017-03-10T21:00:26Z|http://arxiv.org/abs/1703.03004v2|http://arxiv.org/pdf/1703.03004v2|New approximation for GARCH parameters estimate|new approxim garch paramet estim|This paper presents a new approach for the optimization of GARCH parameters estimation. Firstly, we propose a method for the localization of the maximum. Thereafter, using the methods of least squares, we make a local approximation for the projection of the likelihood function curve on two dimensional planes by a polynomial of order two which will be used to calculate an estimation of the maximum.|paper present new approach optim garch paramet estim first propos method local maximum thereaft use method least squar make local approxim project likelihood function curv two dimension plane polynomi order two use calcul estim maximum|['Yakoub Boularouk', 'Nasr-eddine Hamri']|['stat.CO']
2017-03-28T14:03:17Z|2017-03-08T19:22:11Z|http://arxiv.org/abs/1703.02998v1|http://arxiv.org/pdf/1703.02998v1|A note on quickly sampling a sparse matrix with low rank expectation|note quick sampl spars matrix low rank expect|"Given matrices $X,Y \in R^{n \times K}$ and $S \in R^{K \times K}$ with positive elements, this paper proposes an algorithm fastRG to sample a sparse matrix $A$ with low rank expectation $E(A) = XSY^T$ and independent Poisson elements. This allows for quickly sampling from a broad class of stochastic blockmodel graphs (degree-corrected, mixed membership, overlapping) all of which are specific parameterizations of the generalized random product graph model defined in Section 2.2. The basic idea of fastRG is to first sample the number of edges $m$ and then sample each edge. The key insight is that because of the the low rank expectation, it is easy to sample individual edges. The naive ""element-wise"" algorithm requires $O(n^2)$ operations to generate the $n\times n$ adjacency matrix $A$. In sparse graphs, where $m = O(n)$, ignoring log terms, fastRG runs in time $O(n)$. An implementation in fastRG is available on github. A computational experiment in Section 2.4 simulates graphs up to $n=10,000,000$ nodes with $m = 100,000,000$ edges. For example, on a graph with $n=500,000$ and $m = 5,000,000$, fastRG runs in less than one second on a 3.5 GHz Intel i5."|given matric time time posit element paper propos algorithm fastrg sampl spars matrix low rank expect xsi independ poisson element allow quick sampl broad class stochast blockmodel graph degre correct mix membership overlap specif parameter general random product graph model defin section basic idea fastrg first sampl number edg sampl edg key insight becaus low rank expect easi sampl individu edg naiv element wise algorithm requir oper generat time adjac matrix spars graph ignor log term fastrg run time implement fastrg avail github comput experi section simul graph node edg exampl graph fastrg run less one second ghz intel|['Karl Rohe', 'Jun Tao', 'Xintian Han', 'Norbert Binkiewicz']|['stat.CO']
2017-03-28T14:03:21Z|2017-03-07T18:36:55Z|http://arxiv.org/abs/1703.02518v1|http://arxiv.org/pdf/1703.02518v1|Faster Coordinate Descent via Adaptive Importance Sampling|faster coordin descent via adapt import sampl|Coordinate descent methods employ random partial updates of decision variables in order to solve huge-scale convex optimization problems. In this work, we introduce new adaptive rules for the random selection of their updates. By adaptive, we mean that our selection rules are based on the dual residual or the primal-dual gap estimates and can change at each iteration. We theoretically characterize the performance of our selection rules and demonstrate improvements over the state-of-the-art, and extend our theory and algorithms to general convex objectives. Numerical evidence with hinge-loss support vector machines and Lasso confirm that the practice follows the theory.|coordin descent method employ random partial updat decis variabl order solv huge scale convex optim problem work introduc new adapt rule random select updat adapt mean select rule base dual residu primal dual gap estim chang iter theoret character perform select rule demonstr improv state art extend theori algorithm general convex object numer evid hing loss support vector machin lasso confirm practic follow theori|['Dmytro Perekrestenko', 'Volkan Cevher', 'Martin Jaggi']|['cs.LG', 'cs.CV', 'math.OC', 'stat.CO', 'stat.ML', 'G.1.6']
2017-03-28T14:03:21Z|2017-03-07T15:13:08Z|http://arxiv.org/abs/1703.02428v1|http://arxiv.org/pdf/1703.02428v1|Robust Bayesian Filtering and Smoothing Using Student's t Distribution|robust bayesian filter smooth use student distribut|State estimation in heavy-tailed process and measurement noise is an important challenge that must be addressed in, e.g., tracking scenarios with agile targets and outlier-corrupted measurements. The performance of the Kalman filter (KF) can deteriorate in such applications because of the close relation to the Gaussian distribution. Therefore, this paper describes the use of Student's t distribution to develop robust, scalable, and simple filtering and smoothing algorithms.   After a discussion of Student's t distribution, exact filtering in linear state-space models with t noise is analyzed. Intermediate approximation steps are used to arrive at filtering and smoothing algorithms that closely resemble the KF and the Rauch-Tung-Striebel (RTS) smoother except for a nonlinear measurement-dependent matrix update. The required approximations are discussed and an undesirable behavior of moment matching for t densities is revealed. A favorable approximation based on minimization of the Kullback-Leibler divergence is presented. Because of its relation to the KF, some properties and algorithmic extensions are inherited by the t filter. Instructive simulation examples demonstrate the performance and robustness of the novel algorithms.|state estim heavi tail process measur nois import challeng must address track scenario agil target outlier corrupt measur perform kalman filter kf deterior applic becaus close relat gaussian distribut therefor paper describ use student distribut develop robust scalabl simpl filter smooth algorithm discuss student distribut exact filter linear state space model nois analyz intermedi approxim step use arriv filter smooth algorithm close resembl kf rauch tung striebel rts smoother except nonlinear measur depend matrix updat requir approxim discuss undesir behavior moment match densiti reveal favor approxim base minim kullback leibler diverg present becaus relat kf properti algorithm extens inherit filter instruct simul exampl demonstr perform robust novel algorithm|['Michael Roth', 'Tohid Ardeshiri', 'Emre Özkan', 'Fredrik Gustafsson']|['stat.ME', 'cs.SY', 'stat.CO']
2017-03-28T14:03:21Z|2017-03-07T15:01:51Z|http://arxiv.org/abs/1703.02419v1|http://arxiv.org/pdf/1703.02419v1|Probabilistic learning of nonlinear dynamical systems using sequential   Monte Carlo|probabilist learn nonlinear dynam system use sequenti mont carlo|"Probabilistic modeling provides the capability to represent and manipulate uncertainty in data, models, decisions and predictions. We are concerned with the problem of learning probabilistic models of dynamical systems from measured data. Specifically, we consider learning of probabilistic nonlinear state space models. There is no closed-form solution available for this problem, implying that we are forced to use approximations. In this tutorial we will provide a self-contained introduction to one of the state-of-the-art methods---the particle Metropolis-Hastings algorithm---which has proven to offer very practical approximations. This is a Monte Carlo based method, where the so-called particle filter is used to guide a Markov chain Monte Carlo method through the parameter space. One of the key merits of the particle Metropolis-Hastings method is that it is guaranteed to converge to the ""true solution"" under mild assumptions, despite being based on a practical implementation of a particle filter (i.e., using a finite number of particles). We will also provide a motivating numerical example illustrating the method which we have implemented in an in-house developed modeling language, serving the purpose of abstracting away the underlying mathematics of the Monte Carlo approximations from the user. This modeling language will open up the power of sophisticated Monte Carlo methods, including particle Metropolis-Hastings, to a large group of users without requiring them to know all the underlying mathematical details."|probabilist model provid capabl repres manipul uncertainti data model decis predict concern problem learn probabilist model dynam system measur data specif consid learn probabilist nonlinear state space model close form solut avail problem impli forc use approxim tutori provid self contain introduct one state art method particl metropoli hast algorithm proven offer veri practic approxim mont carlo base method call particl filter use guid markov chain mont carlo method paramet space one key merit particl metropoli hast method guarante converg true solut mild assumpt despit base practic implement particl filter use finit number particl also provid motiv numer exampl illustr method implement hous develop model languag serv purpos abstract away mathemat mont carlo approxim user model languag open power sophist mont carlo method includ particl metropoli hast larg group user without requir know mathemat detail|['Thomas B. Schön', 'Andreas Svensson', 'Lawrence Murray', 'Fredrik Lindsten']|['stat.CO', 'cs.LG', 'cs.SY']
2017-03-28T14:03:21Z|2017-03-07T11:48:45Z|http://arxiv.org/abs/1703.02341v1|http://arxiv.org/pdf/1703.02341v1|An automatic adaptive method to combine summary statistics in   approximate Bayesian computation|automat adapt method combin summari statist approxim bayesian comput|To infer the parameters of mechanistic models with intractable likelihoods, techniques such as approximate Bayesian computation (ABC) are increasingly being adopted. One of the main disadvantages of ABC in practical situations, however, is that parameter inference must generally rely on summary statistics of the data. This is particularly the case for problems involving high-dimensional data, such as biological imaging experiments. However, some summary statistics contain more information about parameters of interest than others, and it is not always clear how to weight their contributions within the ABC framework. We address this problem by developing an automatic, adaptive algorithm that chooses weights for each summary statistic. Our algorithm aims to maximize the distance between the prior and the approximate posterior by automatically adapting the weights within the ABC distance function. To demonstrate the effectiveness of our algorithm, we apply it to several stochastic models of biochemical reaction networks, and a spatial model of diffusion, and compare our results with existing algorithms.|infer paramet mechanist model intract likelihood techniqu approxim bayesian comput abc increas adopt one main disadvantag abc practic situat howev paramet infer must general reli summari statist data particular case problem involv high dimension data biolog imag experi howev summari statist contain inform paramet interest alway clear weight contribut within abc framework address problem develop automat adapt algorithm choos weight summari statist algorithm aim maxim distanc prior approxim posterior automat adapt weight within abc distanc function demonstr effect algorithm appli sever stochast model biochem reaction network spatial model diffus compar result exist algorithm|['Jonathan U Harrison', 'Ruth E Baker']|['stat.CO']
2017-03-28T14:03:21Z|2017-03-13T11:11:48Z|http://arxiv.org/abs/1703.02337v2|http://arxiv.org/pdf/1703.02337v2|A Note on the Convergence of the Gaussian Mean Shift Algorithm|note converg gaussian mean shift algorithm|Mean shift (MS) algorithms are popular methods for mode finding in pattern analysis. Each MS algorithm can be phrased as a fixed-point iteration scheme, which operates on a kernel density estimate (KDE) based on some data. The ability of an MS algorithm to obtain the modes of its KDE depends on whether or not the fixed-point scheme converges. The convergence of MS algorithms have recently been proved under some general conditions via first principle arguments. We complement the recent proofs by demonstrating that the MS algorithm operating on a Gaussian KDE can be viewed as an MM (minorization-maximization) algorithm, and thus permits the application of convergence techniques for such constructions. For the Gaussian case, we extend upon the previously results by showing that the fixed-points of the MS algorithm are all stationary points of the KDE in cases where the stationary points may not necessarily be isolated.|mean shift ms algorithm popular method mode find pattern analysi ms algorithm phrase fix point iter scheme oper kernel densiti estim kde base data abil ms algorithm obtain mode kde depend whether fix point scheme converg converg ms algorithm recent prove general condit via first principl argument complement recent proof demonstr ms algorithm oper gaussian kde view mm minor maxim algorithm thus permit applic converg techniqu construct gaussian case extend upon previous result show fix point ms algorithm stationari point kde case stationari point may necessarili isol|['Hien D Nguyen']|['stat.CO']
2017-03-28T14:03:21Z|2017-03-07T09:33:21Z|http://arxiv.org/abs/1703.02293v1|http://arxiv.org/pdf/1703.02293v1|Variable selection for mixed data clustering: a model-based approach|variabl select mix data cluster model base approach|We propose two approaches for selecting variables in latent class analysis (i.e.,mixture model assuming within component independence), which is the common model-based clustering method for mixed data. The first approach consists in optimizing the BIC with a modified version of the EM algorithm. This approach simultaneously performs both model selection and parameter inference. The second approach consists in maximizing the MICL, which considers the clustering task, with an algorithm of alternate optimization. This approach performs model selection without requiring the maximum likelihood estimates for model comparison, then parameter inference is done for the unique selected model. Thus, the benefits of both approaches is to avoid the computation of the maximum likelihood estimates for each model comparison. Moreover, they also avoid the use of the standard algorithms for variable selection which are often suboptimal (e.g. stepwise method) and computationally expensive. The case of data with missing values is also discussed. The interest of both proposed criteria is shown on simulated and real data.|propos two approach select variabl latent class analysi mixtur model assum within compon independ common model base cluster method mix data first approach consist optim bic modifi version em algorithm approach simultan perform model select paramet infer second approach consist maxim micl consid cluster task algorithm altern optim approach perform model select without requir maximum likelihood estim model comparison paramet infer done uniqu select model thus benefit approach avoid comput maximum likelihood estim model comparison moreov also avoid use standard algorithm variabl select often suboptim stepwis method comput expens case data miss valu also discuss interest propos criteria shown simul real data|['Matthieu Marbac', 'Mohammed Sedki']|['stat.CO', '62F15']
2017-03-28T14:03:21Z|2017-03-07T07:44:52Z|http://arxiv.org/abs/1703.02251v1|http://arxiv.org/pdf/1703.02251v1|The Maximum Likelihood Degree of Toric Varieties|maximum likelihood degre toric varieti|We study the maximum likelihood degree (ML degree) of toric varieties, known as discrete exponential models in statistics. By introducing scaling coefficients to the monomial parameterization of the toric variety, one can change the ML degree. We show that the ML degree is equal to the degree of the toric variety for generic scalings, while it drops if and only if the scaling vector is in the locus of the principal $A$-determinant. We also illustrate how to compute the ML estimate of a toric variety numerically via homotopy continuation from a scaled toric variety with low ML degree. Throughout, we include examples motivated by algebraic geometry and statistics. We compute the ML degree of rational normal scrolls and a large class of Veronese-type varieties. In addition, we investigate the ML degree of scaled Segre varieties, hierarchical loglinear models, and graphical models.|studi maximum likelihood degre ml degre toric varieti known discret exponenti model statist introduc scale coeffici monomi parameter toric varieti one chang ml degre show ml degre equal degre toric varieti generic scale drop onli scale vector locus princip determin also illustr comput ml estim toric varieti numer via homotopi continu scale toric varieti low ml degre throughout includ exampl motiv algebra geometri statist comput ml degre ration normal scroll larg class verones type varieti addit investig ml degre scale segr varieti hierarch loglinear model graphic model|['Carlos Améndola', 'Nathan Bliss', 'Isaac Burke', 'Courtney R. Gibbons', 'Martin Helmer', 'Serkan Hoşten', 'Evan D. Nash', 'Jose Israel Rodriguez', 'Daniel Smolkin']|['math.AG', 'math.ST', 'stat.CO', 'stat.TH', '14Q15, 14M25, 13P15, 62F10']
2017-03-28T14:03:21Z|2017-03-07T06:40:44Z|http://arxiv.org/abs/1703.02237v1|http://arxiv.org/pdf/1703.02237v1|Scalable Collaborative Targeted Learning for High-Dimensional Data|scalabl collabor target learn high dimension data|Robust inference of a low-dimensional parameter in a large semi-parametric model relies on external estimators of infinite-dimensional features of the distribution of the data. Typically, only one of the latter is optimized for the sake of constructing a well behaved estimator of the low-dimensional parameter of interest. Optimizing more than one of them for the sake of achieving a better bias-variance trade-off in the estimation of the parameter of interest is the core idea driving the general template of the collaborative targeted minimum loss-based estimation (C-TMLE) procedure. The original implementation/instantiation of the C-TMLE template can be presented as a greedy forward stepwise C-TMLE algorithm. It does not scale well when the number $p$ of covariates increases drastically. This motivates the introduction of a novel instantiation of the C-TMLE template where the covariates are pre-ordered. Its time complexity is $\mathcal{O}(p)$ as opposed to the original $\mathcal{O}(p^2)$, a remarkable gain. We propose two pre-ordering strategies and suggest a rule of thumb to develop other meaningful strategies. Because it is usually unclear a priori which pre-ordering strategy to choose, we also introduce another implementation/instantiation called SL-C-TMLE algorithm that enables the data-driven choice of the better pre-ordering strategy given the problem at hand. Its time complexity is $\mathcal{O}(p)$ as well. The computational burden and relative performance of these algorithms were compared in simulation studies involving fully synthetic data or partially synthetic data based on a real world large electronic health database; and in analyses of three real, large electronic health databases. In all analyses involving electronic health databases, the greedy C-TMLE algorithm is unacceptably slow. Simulation studies indicate our scalable C-TMLE and SL-C-TMLE algorithms work well.|robust infer low dimension paramet larg semi parametr model reli extern estim infinit dimension featur distribut data typic onli one latter optim sake construct well behav estim low dimension paramet interest optim one sake achiev better bias varianc trade estim paramet interest core idea drive general templat collabor target minimum loss base estim tmle procedur origin implement instanti tmle templat present greedi forward stepwis tmle algorithm doe scale well number covari increas drastic motiv introduct novel instanti tmle templat covari pre order time complex mathcal oppos origin mathcal remark gain propos two pre order strategi suggest rule thumb develop meaning strategi becaus usual unclear priori pre order strategi choos also introduc anoth implement instanti call sl tmle algorithm enabl data driven choic better pre order strategi given problem hand time complex mathcal well comput burden relat perform algorithm compar simul studi involv fulli synthet data partial synthet data base real world larg electron health databas analys three real larg electron health databas analys involv electron health databas greedi tmle algorithm unaccept slow simul studi indic scalabl tmle sl tmle algorithm work well|['Cheng Ju', 'Susan Gruber', 'Samuel D. Lendle', 'Antoine Chambaz', 'Jessica M. Franklin', 'Richard Wyss', 'Sebastian Schneeweiss', 'Mark J. van der Laan']|['stat.CO', 'stat.ME']
2017-03-28T14:03:21Z|2017-03-25T17:47:33Z|http://arxiv.org/abs/1703.02177v2|http://arxiv.org/pdf/1703.02177v2|Mixtures of Generalized Hyperbolic Distributions and Mixtures of Skew-t   Distributions for Model-Based Clustering with Incomplete Data|mixtur general hyperbol distribut mixtur skew distribut model base cluster incomplet data|Robust clustering from incomplete data is an important topic because, in many practical situations, real data sets are heavy-tailed, asymmetric, and/or have arbitrary patterns of missing observations. Flexible methods and algorithms for model-based clustering are presented via mixture of the generalized hyperbolic distributions and its limiting case, the mixture of multivariate skew-t distributions. An analytically feasible EM algorithm is formulated for parameter estimation and imputation of missing values for mixture models employing missing at random mechanisms. The proposed methodologies are investigated through a simulation study with varying proportions of synthetic missing values and illustrated using a real dataset. Comparisons are made with those obtained from the traditional mixture of generalized hyperbolic distribution counterparts by filling in the missing data using the mean imputation method.|robust cluster incomplet data import topic becaus mani practic situat real data set heavi tail asymmetr arbitrari pattern miss observ flexibl method algorithm model base cluster present via mixtur general hyperbol distribut limit case mixtur multivari skew distribut analyt feasibl em algorithm formul paramet estim imput miss valu mixtur model employ miss random mechan propos methodolog investig simul studi vari proport synthet miss valu illustr use real dataset comparison made obtain tradit mixtur general hyperbol distribut counterpart fill miss data use mean imput method|['Yuhong Wei', 'Paul D. McNicholas']|['stat.ME', 'stat.CO']
2017-03-28T14:03:21Z|2017-03-06T23:48:50Z|http://arxiv.org/abs/1703.02151v1|http://arxiv.org/pdf/1703.02151v1|Computationally Efficient Simulation of Queues: The R Package   queuecomputer|comput effici simul queue packag queuecomput|Large networks of queueing systems model important real-world systems such as MapReduce clusters, web-servers, hospitals, call-centers and airport passenger terminals.To model such systems accurately we must infer queueing parameters from data. Unfortunately, for many queueing networks there is no clear way to proceed with parameter inference from data. Approximate Bayesian computation could offer a straight-forward way to infer parameters for such networks if we could simulate data quickly enough.   We present a computationally efficient method for simulating from a very general set of queueing networks with the R package queuecomputer. Remarkable speedups of more than 2 orders of magnitude are observed relative to the popular DES packages simmer and simpy. We replicate output from these packages to validate the package.   The package is modular and integrates well with the popular R package dplyr. Complex queueing networks with tandem, parallel and fork/join topologies can easily be built with these two packages together. We show how to use this package with two examples: a call-centre and an airport terminal.|larg network queue system model import real world system mapreduc cluster web server hospit call center airport passeng termin model system accur must infer queue paramet data unfortun mani queue network clear way proceed paramet infer data approxim bayesian comput could offer straight forward way infer paramet network could simul data quick enough present comput effici method simul veri general set queue network packag queuecomput remark speedup order magnitud observ relat popular des packag simmer simpi replic output packag valid packag packag modular integr well popular packag dplyr complex queue network tandem parallel fork join topolog easili built two packag togeth show use packag two exampl call centr airport termin|['Anthony Ebert', 'Paul Wu', 'Kerrie Mengersen', 'Fabrizio Ruggeri']|['stat.CO', 'math.OC']
2017-03-28T14:03:25Z|2017-03-06T19:44:45Z|http://arxiv.org/abs/1703.02081v1|http://arxiv.org/pdf/1703.02081v1|Estimation and prediction in sparse and unbalanced tables|estim predict spars unbalanc tabl|We consider the problem where we have a multi-way table of means, indexed by several factors, where each factor can have a large number of levels. The entry in each cell is the mean of some response, averaged over the observations falling into that cell. Some cells may be very sparsely populated, and in extreme cases, not populated at all. We might still like to estimate an expected response in such cells. We propose here a novel hierarchical ANOVA (HANOVA) representation for such data. Sparse cells will lean more on the lower-order interaction model for the data. These in turn could have components that are poorly represented in the data, in which case they rely on yet lower-order models. Our approach leads to a simple hierarchical algorithm, requiring repeated calculations of sub-table means of modified counts. The algorithm has shown superiority over the unshrinked methods in both simulations and real data sets.|consid problem multi way tabl mean index sever factor factor larg number level entri cell mean respons averag observ fall cell cell may veri spars popul extrem case popul might still like estim expect respons cell propos novel hierarch anova hanova represent data spars cell lean lower order interact model data turn could compon poor repres data case reli yet lower order model approach lead simpl hierarch algorithm requir repeat calcul sub tabl mean modifi count algorithm shown superior unshrink method simul real data set|['Qingyuan Zhao', 'Trevor Hastie', 'Daryl Pregibon']|['stat.CO']
2017-03-28T14:03:25Z|2017-03-04T21:50:25Z|http://arxiv.org/abs/1703.01526v1|http://arxiv.org/abs/1703.01526v1|High Accuracy Classification of Parkinson's Disease through Shape   Analysis and Surface Fitting in $^{123}$I-Ioflupane SPECT Imaging|high accuraci classif parkinson diseas shape analysi surfac fit ioflupan spect imag|Early and accurate identification of parkinsonian syndromes (PS) involving presynaptic degeneration from non-degenerative variants such as Scans Without Evidence of Dopaminergic Deficit (SWEDD) and tremor disorders, is important for effective patient management as the course, therapy and prognosis differ substantially between the two groups. In this study, we use Single Photon Emission Computed Tomography (SPECT) images from healthy normal, early PD and SWEDD subjects, as obtained from the Parkinson's Progression Markers Initiative (PPMI) database, and process them to compute shape- and surface fitting-based features for the three groups. We use these features to develop and compare various classification models that can discriminate between scans showing dopaminergic deficit, as in PD, from scans without the deficit, as in healthy normal or SWEDD. Along with it, we also compare these features with Striatal Binding Ratio (SBR)-based features, which are well-established and clinically used, by computing a feature importance score using Random forests technique. We observe that the Support Vector Machine (SVM) classifier gave the best performance with an accuracy of 97.29%. These features also showed higher importance than the SBR-based features. We infer from the study that shape analysis and surface fitting are useful and promising methods for extracting discriminatory features that can be used to develop diagnostic models that might have the potential to help clinicians in the diagnostic process.|earli accur identif parkinsonian syndrom ps involv presynapt degener non degen variant scan without evid dopaminerg deficit swedd tremor disord import effect patient manag cours therapi prognosi differ substanti two group studi use singl photon emiss comput tomographi spect imag healthi normal earli pd swedd subject obtain parkinson progress marker initi ppmi databas process comput shape surfac fit base featur three group use featur develop compar various classif model discrimin scan show dopaminerg deficit pd scan without deficit healthi normal swedd along also compar featur striatal bind ratio sbr base featur well establish clinic use comput featur import score use random forest techniqu observ support vector machin svm classifi gave best perform accuraci featur also show higher import sbr base featur infer studi shape analysi surfac fit use promis method extract discriminatori featur use develop diagnost model might potenti help clinician diagnost process|['R. Prashanth', 'Sumantra Dutta Roy', 'Pravat K. Mandal', 'Shantanu Ghosh']|['stat.AP', 'cs.CV', 'physics.data-an', 'stat.CO', 'stat.ML']
2017-03-28T14:03:25Z|2017-03-04T09:12:42Z|http://arxiv.org/abs/1703.01421v1|http://arxiv.org/pdf/1703.01421v1|$l_0$-estimation of piecewise-constant signals on graphs|estim piecewis constant signal graph|We study recovery of piecewise-constant signals over arbitrary graphs by the estimator minimizing an $l_0$-edge-penalized objective. Although exact minimization of this objective may be computationally intractable, we show that the same statistical risk guarantees are achieved by the alpha-expansion algorithm which approximately minimizes this objective in polynomial time. We establish that for graphs with small average vertex degree, these guarantees are rate-optimal in a minimax sense over classes of edge-sparse signals. For application to spatially inhomogeneous graphs, we propose minimization of an edge-weighted variant of this objective where each edge is weighted by its effective resistance or another measure of its contribution to the graph's connectivity. We establish minimax optimality of the resulting estimators over corresponding edge-weighted sparsity classes. We show theoretically that these risk guarantees are not always achieved by the estimator minimizing the $l_1$/total-variation relaxation, and empirically that the $l_0$-based estimates are more accurate in high signal-to-noise settings.|studi recoveri piecewis constant signal arbitrari graph estim minim edg penal object although exact minim object may comput intract show statist risk guarante achiev alpha expans algorithm approxim minim object polynomi time establish graph small averag vertex degre guarante rate optim minimax sens class edg spars signal applic spatial inhomogen graph propos minim edg weight variant object edg weight effect resist anoth measur contribut graph connect establish minimax optim result estim correspond edg weight sparsiti class show theoret risk guarante alway achiev estim minim total variat relax empir base estim accur high signal nois set|['Zhou Fan', 'Leying Guan']|['stat.ME', 'math.ST', 'stat.CO', 'stat.TH']
2017-03-28T14:03:25Z|2017-03-03T18:07:15Z|http://arxiv.org/abs/1703.01273v1|http://arxiv.org/pdf/1703.01273v1|Estimating Spatial Econometrics Models with Integrated Nested Laplace   Approximation|estim spatial econometr model integr nest laplac approxim|Integrated Nested Laplace Approximation provides a fast and effective method for marginal inference on Bayesian hierarchical models. This methodology has been implemented in the R-INLA package which permits INLA to be used from within R statistical software. Although INLA is implemented as a general methodology, its use in practice is limited to the models implemented in the R-INLA package.   Spatial autoregressive models are widely used in spatial econometrics but have until now been missing from the R-INLA package. In this paper, we describe the implementation and application of a new class of latent models in INLA made available through R-INLA. This new latent class implements a standard spatial lag model, which is widely used and that can be used to build more complex models in spatial econometrics.   The implementation of this latent model in R-INLA also means that all the other features of INLA can be used for model fitting, model selection and inference in spatial econometrics, as will be shown in this paper. Finally, we will illustrate the use of this new latent model and its applications with two datasets based on Gaussian and binary outcomes.|integr nest laplac approxim provid fast effect method margin infer bayesian hierarch model methodolog implement inla packag permit inla use within statist softwar although inla implement general methodolog use practic limit model implement inla packag spatial autoregress model wide use spatial econometr miss inla packag paper describ implement applic new class latent model inla made avail inla new latent class implement standard spatial lag model wide use use build complex model spatial econometr implement latent model inla also mean featur inla use model fit model select infer spatial econometr shown paper final illustr use new latent model applic two dataset base gaussian binari outcom|['Virgilio Gomez-Rubio', 'Roger S. Bivand', 'Håvard Rue']|['stat.CO']
2017-03-28T14:03:25Z|2017-03-03T16:29:21Z|http://arxiv.org/abs/1703.01234v1|http://arxiv.org/pdf/1703.01234v1|A Bayesian computer model analysis of Robust Bayesian analyses|bayesian comput model analysi robust bayesian analys|We harness the power of Bayesian emulation techniques, designed to aid the analysis of complex computer models, to examine the structure of complex Bayesian analyses themselves. These techniques facilitate robust Bayesian analyses and/or sensitivity analyses of complex problems, and hence allow global exploration of the impacts of choices made in both the likelihood and prior specification. We show how previously intractable problems in robustness studies can be overcome using emulation techniques, and how these methods allow other scientists to quickly extract approximations to posterior results corresponding to their own particular subjective specification. The utility and flexibility of our method is demonstrated on a reanalysis of a real application where Bayesian methods were employed to capture beliefs about river flow. We discuss the obvious extensions and directions of future research that such an approach opens up.|har power bayesian emul techniqu design aid analysi complex comput model examin structur complex bayesian analys themselv techniqu facilit robust bayesian analys sensit analys complex problem henc allow global explor impact choic made likelihood prior specif show previous intract problem robust studi overcom use emul techniqu method allow scientist quick extract approxim posterior result correspond particular subject specif util flexibl method demonstr reanalysi real applic bayesian method employ captur belief river flow discuss obvious extens direct futur research approach open|['Ian Vernon', 'John Paul Gosling']|['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']
2017-03-28T14:03:25Z|2017-03-03T10:44:47Z|http://arxiv.org/abs/1703.01106v1|http://arxiv.org/pdf/1703.01106v1|Differentially Private Bayesian Learning on Distributed Data|differenti privat bayesian learn distribut data|Many applications of machine learning, for example in health care, would benefit from methods that can guarantee privacy of data subjects. Differential privacy (DP) has become established as a standard for protecting learning results, but the proposed algorithms require a single trusted party to have access to the entire data, which is a clear weakness. We consider DP Bayesian learning in a distributed setting, where each party only holds a single sample or a few samples of the data. We propose a novel method for DP learning in this distributed setting, based on a secure multi-party sum function for aggregating summaries from the data holders. Each data holder adds their share of Gaussian noise to make the total computation differentially private using the Gaussian mechanism. We prove that the system can be made secure against a desired number of colluding data owners and robust against faulting data owners. The method builds on an asymptotically optimal and practically efficient DP Bayesian inference with rapidly diminishing extra cost.|mani applic machin learn exampl health care would benefit method guarante privaci data subject differenti privaci dp becom establish standard protect learn result propos algorithm requir singl trust parti access entir data clear weak consid dp bayesian learn distribut set parti onli hold singl sampl sampl data propos novel method dp learn distribut set base secur multi parti sum function aggreg summari data holder data holder add share gaussian nois make total comput differenti privat use gaussian mechan prove system made secur desir number collud data owner robust fault data owner method build asymptot optim practic effici dp bayesian infer rapid diminish extra cost|['Mikko Heikkilä', 'Yusuke Okimoto', 'Samuel Kaski', 'Kana Shimizu', 'Antti Honkela']|['stat.ML', 'cs.CR', 'cs.LG', 'stat.CO']
2017-03-28T14:03:25Z|2017-03-02T17:33:58Z|http://arxiv.org/abs/1703.00864v1|http://arxiv.org/pdf/1703.00864v1|The Unreasonable Effectiveness of Random Orthogonal Embeddings|unreason effect random orthogon embed|We present a general class of embeddings based on structured random matrices with orthogonal rows which can be applied in many machine learning applications including dimensionality reduction, kernel approximation and locality-sensitive hashing. We show that this class yields improvements over previous state-of-the-art methods either in computational efficiency (while providing similar accuracy) or in accuracy, or both. In particular, we propose the \textit{Orthogonal Johnson-Lindenstrauss Transform} (OJLT) which is as fast as earlier methods yet provably outperforms them in terms of accuracy, leading to a `free lunch' improvement over previous dimensionality reduction mechanisms. We introduce matrices with complex entries that further improve accuracy. Other applications include estimators for certain pointwise nonlinear Gaussian kernels, and speed improvements for approximate nearest-neighbor search in massive datasets with high-dimensional feature vectors.|present general class embed base structur random matric orthogon row appli mani machin learn applic includ dimension reduct kernel approxim local sensit hash show class yield improv previous state art method either comput effici provid similar accuraci accuraci particular propos textit orthogon johnson lindenstrauss transform ojlt fast earlier method yet provabl outperform term accuraci lead free lunch improv previous dimension reduct mechan introduc matric complex entri improv accuraci applic includ estim certain pointwis nonlinear gaussian kernel speed improv approxim nearest neighbor search massiv dataset high dimension featur vector|['Krzysztof Choromanski', 'Mark Rowland', 'Adrian Weller']|['stat.ML', 'stat.CO']
2017-03-28T14:03:25Z|2017-03-19T16:03:04Z|http://arxiv.org/abs/1703.00368v2|http://arxiv.org/pdf/1703.00368v2|Approximate Computational Approaches for Bayesian Sensor Placement in   High Dimensions|approxim comput approach bayesian sensor placement high dimens|Since the cost of installing and maintaining sensors is usually high, sensor locations are always strategically selected. For those aiming at inferring certain quantities of interest (QoI), it is desirable to explore the dependency between sensor measurements and QoI. One of the most popular metric for the dependency is mutual information which naturally measures how much information about one variable can be obtained given the other. However, computing mutual information is always challenging, and the result is unreliable in high dimension. In this paper, we propose an approach to find an approximate lower bound of mutual information and compute it in a lower dimension. Then, sensors are placed where highest mutual information (lower bound) is achieved and QoI is inferred via Bayes rule given sensor measurements. In addition, Bayesian optimization is introduced to provide a continuous mutual information surface over the domain and thus reduce the number of evaluations. A chemical release accident is simulated where multiple sensors are placed to locate the source of the release. The result shows that the proposed approach is both effective and efficient in inferring QoI.|sinc cost instal maintain sensor usual high sensor locat alway strateg select aim infer certain quantiti interest qoi desir explor depend sensor measur qoi one popular metric depend mutual inform natur measur much inform one variabl obtain given howev comput mutual inform alway challeng result unreli high dimens paper propos approach find approxim lower bound mutual inform comput lower dimens sensor place highest mutual inform lower bound achiev qoi infer via bay rule given sensor measur addit bayesian optim introduc provid continu mutual inform surfac domain thus reduc number evalu chemic releas accid simul multipl sensor place locat sourc releas result show propos approach effect effici infer qoi|['Xiao Lin', 'Asif Chowdhury', 'Xiaofan Wang', 'Gabriel Terejanu']|['stat.CO']
2017-03-28T14:03:25Z|2017-03-13T17:28:04Z|http://arxiv.org/abs/1702.08896v2|http://arxiv.org/pdf/1702.08896v2|Deep and Hierarchical Implicit Models|deep hierarch implicit model|Implicit probabilistic models are a flexible class for modeling data. They define a process to simulate observations, and unlike traditional models, they do not require a tractable likelihood function. In this paper, we develop two families of models: hierarchical implicit models and deep implicit models. They combine the idea of implicit densities with hierarchical Bayesian modeling and deep neural networks. The use of implicit models with Bayesian analysis has been limited by our ability to perform accurate and scalable inference. We develop likelihood-free variational inference (LFVI). Key to LFVI is specifying a variational family that is also implicit. This matches the model's flexibility and allows for accurate approximation of the posterior. Our work scales up implicit models to sizes previously not possible and advances their modeling design. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for text generation.|implicit probabilist model flexibl class model data defin process simul observ unlik tradit model requir tractabl likelihood function paper develop two famili model hierarch implicit model deep implicit model combin idea implicit densiti hierarch bayesian model deep neural network use implicit model bayesian analysi limit abil perform accur scalabl infer develop likelihood free variat infer lfvi key lfvi specifi variat famili also implicit match model flexibl allow accur approxim posterior work scale implicit model size previous possibl advanc model design demonstr divers applic larg scale physic simul predat prey popul ecolog bayesian generat adversari network discret data deep implicit model text generat|['Dustin Tran', 'Rajesh Ranganath', 'David M. Blei']|['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']
2017-03-28T14:03:25Z|2017-02-28T16:31:21Z|http://arxiv.org/abs/1702.08849v1|http://arxiv.org/pdf/1702.08849v1|Multi-Sensor Multi-object Tracking with the Generalized Labeled   Multi-Bernoulli Filter|multi sensor multi object track general label multi bernoulli filter|This paper proposes an efficient implementation of the multi-sensor generalized labeled multi-Bernoulli (GLMB) filter. The solution exploits the GLMB joint prediction and update together with a new technique for truncating the GLMB filtering density based on Gibbs sampling. The resulting algorithm has quadratic complexity in the number of hypothesized object and linear in the number of measurements of each individual sensors.|paper propos effici implement multi sensor general label multi bernoulli glmb filter solut exploit glmb joint predict updat togeth new techniqu truncat glmb filter densiti base gibb sampl result algorithm quadrat complex number hypothes object linear number measur individu sensor|['Ba Ngu Vo', 'Ba Tuong Vo']|['stat.CO']
2017-03-28T14:03:30Z|2017-02-28T13:34:02Z|http://arxiv.org/abs/1702.08781v1|http://arxiv.org/pdf/1702.08781v1|General Bayesian inference schemes in infinite mixture models|general bayesian infer scheme infinit mixtur model|Bayesian statistical models allow us to formalise our knowledge about the world and reason about our uncertainty, but there is a need for better procedures to accurately encode its complexity. One way to do so is through compositional models, which are formed by combining blocks consisting of simpler models. One can increase the complexity of the compositional model by either stacking more blocks or by using a not-so-simple model as a building block. This thesis is an example of the latter. One first aim is to expand the choice of Bayesian nonparametric (BNP) blocks for constructing tractable compositional models. So far, most of the models that have a Bayesian nonparametric component use a Dirichlet Process or a Pitman-Yor process because of the availability of tractable and compact representations. This thesis shows how to overcome certain intractabilities in order to obtain analogous compact representations for the class of Poisson-Kingman priors which includes the Dirichlet and Pitman-Yor processes.   A major impediment to the widespread use of Bayesian nonparametric building blocks is that inference is often costly, intractable or difficult to carry out. This is an active research area since dealing with the model's infinite dimensional component forbids the direct use of standard simulation-based methods. The main contribution of this thesis is a variety of inference schemes that tackle this problem: Markov chain Monte Carlo and Sequential Monte Carlo methods, which are exact inference schemes since they target the true posterior. The contributions of this thesis, in a larger context, provide general purpose exact inference schemes in the flavour or probabilistic programming: the user is able to choose from a variety of models, focusing only on the modelling part. Indeed, if the wide enough class of Poisson-Kingman priors is used as one of our blocks, this objective is achieved.|bayesian statist model allow us formalis knowledg world reason uncertainti need better procedur accur encod complex one way composit model form combin block consist simpler model one increas complex composit model either stack block use simpl model build block thesi exampl latter one first aim expand choic bayesian nonparametr bnp block construct tractabl composit model far model bayesian nonparametr compon use dirichlet process pitman yor process becaus avail tractabl compact represent thesi show overcom certain intract order obtain analog compact represent class poisson kingman prior includ dirichlet pitman yor process major impedi widespread use bayesian nonparametr build block infer often cost intract difficult carri activ research area sinc deal model infinit dimension compon forbid direct use standard simul base method main contribut thesi varieti infer scheme tackl problem markov chain mont carlo sequenti mont carlo method exact infer scheme sinc target true posterior contribut thesi larger context provid general purpos exact infer scheme flavour probabilist program user abl choos varieti model focus onli model part inde wide enough class poisson kingman prior use one block object achiev|['Maria Lomeli']|['stat.CO']
2017-03-28T14:03:30Z|2017-02-28T11:02:04Z|http://arxiv.org/abs/1702.08738v1|http://arxiv.org/pdf/1702.08738v1|Efficient simulation of high dimensional Gaussian vectors|effici simul high dimension gaussian vector|We describe a Markov chain Monte Carlo method to approximately simulate a centered d-dimensional Gaussian vector X with given covariance matrix. The standard Monte Carlo method is based on the Cholesky decomposition, which takes cubic time and has quadratic storage cost in d. In contrast, the storage cost of our algorithm is linear in d. We give a bound on the quadractic Wasserstein distance between the distribution of our sample and the target distribution. Our method can be used to estimate the expectation of h(X), where h is a real-valued function of d variables. Under certain conditions, we show that the mean square error of our method is inversely proportional to its running time. We also prove that, under suitable conditions, our method is faster than the standard Monte Carlo method by a factor nearly proportional to d. A numerical example is given.|describ markov chain mont carlo method approxim simul center dimension gaussian vector given covari matrix standard mont carlo method base choleski decomposit take cubic time quadrat storag cost contrast storag cost algorithm linear give bound quadract wasserstein distanc distribut sampl target distribut method use estim expect real valu function variabl certain condit show mean squar error method invers proport run time also prove suitabl condit method faster standard mont carlo method factor near proport numer exampl given|['Nabil Kahale']|['stat.CO', '60J22, 65C40']
2017-03-28T14:03:30Z|2017-02-27T22:52:29Z|http://arxiv.org/abs/1702.08572v1|http://arxiv.org/pdf/1702.08572v1|Comparison of Confidence Interval Estimators: an Index Approach|comparison confid interv estim index approach|We develop a confidence interval index for comparing confidence interval estimators based on the confidence interval length and coverage probability. We show that the confidence interval index has range of values within the neighborhood of the range of the coverage probability, [0,1]. In addition, a good confidence interval estimator is shown to have an index value approaching 1; and a bad confidence interval has an index value approaching 0. A simulation study is conducted to assess the finite sample performance of the index. Finally, the proposed index is illustrated with a practical example from the literature.|develop confid interv index compar confid interv estim base confid interv length coverag probabl show confid interv index rang valu within neighborhood rang coverag probabl addit good confid interv estim shown index valu approach bad confid interv index valu approach simul studi conduct assess finit sampl perform index final propos index illustr practic exampl literatur|['Richard Minkah', 'Tertius de Wet']|['stat.ME', 'stat.CO', '62F99, 62G99']
2017-03-28T14:03:30Z|2017-02-27T17:43:59Z|http://arxiv.org/abs/1702.08397v1|http://arxiv.org/pdf/1702.08397v1|Forward Event-Chain Monte Carlo: a general rejection-free and   irreversible Markov chain simulation method|forward event chain mont carlo general reject free irrevers markov chain simul method|This paper considers Event-Chain Monte Carlo simulation schemes in order to design an original irreversible Markov Chain Monte Carlo (MCMC) algorithm for the sampling of complex statistical models. The functioning principles of MCMC sampling methods are firstly recalled, as well as standard Event-Chain Monte Carlo simulation schemes are described. Then, a Forward Event-Chain Monte Carlo sampling methodology is proposed and introduced. This nonreversible MCMC rejection-free simulation algorithm is tested and run for the sampling of high-dimensional ill-conditioned Gaussian statistical distributions. Numerical experiments demonstrate the efficiency of the proposed approach, compared to standard Event-Chain and standard Monte Carlo sampling methods. Accelerations up to several magnitudes are exhibited.|paper consid event chain mont carlo simul scheme order design origin irrevers markov chain mont carlo mcmc algorithm sampl complex statist model function principl mcmc sampl method first recal well standard event chain mont carlo simul scheme describ forward event chain mont carlo sampl methodolog propos introduc nonrevers mcmc reject free simul algorithm test run sampl high dimension ill condit gaussian statist distribut numer experi demonstr effici propos approach compar standard event chain standard mont carlo sampl method acceler sever magnitud exhibit|['Manon Michel', 'Stéphane Sénécal']|['stat.CO']
2017-03-28T14:03:30Z|2017-02-27T12:12:46Z|http://arxiv.org/abs/1702.08251v1|http://arxiv.org/pdf/1702.08251v1|Hessian corrections to Hybrid Monte Carlo|hessian correct hybrid mont carlo|A method for the introduction of second-order derivatives of the log likelihood into HMC algorithms is introduced, which does not require the Hessian to be evaluated at each leapfrog step but only at the start and end of trajectories.|method introduct second order deriv log likelihood hmc algorithm introduc doe requir hessian evalu leapfrog step onli start end trajectori|['Thomas House']|['stat.CO']
2017-03-28T14:03:30Z|2017-02-27T12:03:01Z|http://arxiv.org/abs/1702.08248v1|http://arxiv.org/pdf/1702.08248v1|Scalable and Distributed Clustering via Lightweight Coresets|scalabl distribut cluster via lightweight coreset|Coresets are compact representations of data sets such that models trained on a coreset are provably competitive with models trained on the full data set. As such, they have been successfully used to scale up clustering models to massive data sets. While existing approaches generally only allow for multiplicative approximation errors, we propose a novel notion of coresets called lightweight coresets that allows for both multiplicative and additive errors. We provide a single algorithm to construct light-weight coresets for k-Means clustering, Bregman clustering and maximum likelihood estimation of Gaussian mixture models. The algorithm is substantially faster than existing constructions, embarrassingly parallel and resulting coresets are smaller. In an extensive experimental evaluation, we demonstrate that the proposed method outperforms existing coreset constructions.|coreset compact represent data set model train coreset provabl competit model train full data set success use scale cluster model massiv data set exist approach general onli allow multipl approxim error propos novel notion coreset call lightweight coreset allow multipl addit error provid singl algorithm construct light weight coreset mean cluster bregman cluster maximum likelihood estim gaussian mixtur model algorithm substanti faster exist construct embarrass parallel result coreset smaller extens experiment evalu demonstr propos method outperform exist coreset construct|['Olivier Bachem', 'Mario Lucic', 'Andreas Krause']|['stat.ML', 'cs.DC', 'cs.DS', 'cs.LG', 'stat.CO']
2017-03-28T14:03:30Z|2017-02-27T08:42:49Z|http://arxiv.org/abs/1702.08188v1|http://arxiv.org/pdf/1702.08188v1|dotCall64: An Efficient Interface to Compiled C/C++ and Fortran Code   Supporting Long Vectors|dotcal effici interfac compil fortran code support long vector|The R functions .C() and .Fortran() can be used to call compiled C/C++ and Fortran code from R. This so-called foreign function interface is convenient, since it does not require any interactions with the C API of R. However, it does not support long vectors (i.e., vectors of more than 2^31 elements). To overcome this limitation, the R package dotCall64 provides .C64(), which can be used to call compiled C/C++ and Fortran functions. It transparently supports long vectors and does the necessary castings to pass numeric R vectors to 64-bit integer arguments of the compiled code. Moreover, .C64() features a mechanism to avoid unnecessary copies of function arguments, making it efficient in terms of speed and memory usage.|function fortran use call compil fortran code call foreign function interfac conveni sinc doe requir ani interact api howev doe support long vector vector element overcom limit packag dotcal provid use call compil fortran function transpar support long vector doe necessari cast pass numer vector bit integ argument compil code moreov featur mechan avoid unnecessari copi function argument make effici term speed memori usag|['Florian Gerber', 'Kaspar Mösinger', 'Reinhard Furrer']|['stat.CO']
2017-03-28T14:03:30Z|2017-02-27T08:33:26Z|http://arxiv.org/abs/1702.08185v1|http://arxiv.org/pdf/1702.08185v1|An update on statistical boosting in biomedicine|updat statist boost biomedicin|Statistical boosting algorithms have triggered a lot of research during the last decade. They combine a powerful machine-learning approach with classical statistical modelling, offering various practical advantages like automated variable selection and implicit regularization of effect estimates. They are extremely flexible, as the underlying base-learners (regression functions defining the type of effect for the explanatory variables) can be combined with any kind of loss function (target function to be optimized, defining the type of regression setting). In this review article, we highlight the most recent methodological developments on statistical boosting regarding variable selection, functional regression and advanced time-to-event modelling. Additionally, we provide a short overview on relevant applications of statistical boosting in biomedicine.|statist boost algorithm trigger lot research dure last decad combin power machin learn approach classic statist model offer various practic advantag like autom variabl select implicit regular effect estim extrem flexibl base learner regress function defin type effect explanatori variabl combin ani kind loss function target function optim defin type regress set review articl highlight recent methodolog develop statist boost regard variabl select function regress advanc time event model addit provid short overview relev applic statist boost biomedicin|['Andreas Mayr', 'Benjamin Hofner', 'Elisabeth Waldmann', 'Tobias Hepp', 'Olaf Gefeller', 'Matthias Schmid']|['stat.AP', 'stat.CO', 'stat.ML']
2017-03-28T14:03:30Z|2017-02-27T04:17:36Z|http://arxiv.org/abs/1702.08140v1|http://arxiv.org/pdf/1702.08140v1|A mixture model approach to infer land-use influence on point referenced   water quality|mixtur model approach infer land use influenc point referenc water qualiti|The assessment of water quality across space and time is of considerable interest for both agricultural and public health reasons. The standard method to assess the water quality of a catchment, or a group of catchments, usually involves collecting point measurements of water quality and other additional information such as the date and time of measurements, rainfall amounts, the land-use and soil-type of the catchment and the elevation. Some of this auxiliary information will be point data, measured at the exact location, whereas other such as land-use will be areal data often in a compositional format. Two problems arise if analysts try to incorporate this information into a statistical model in order to predict (for example) the influence of land-use on water quality. First is the spatial change of support problem that arises when using areal data to predict outcomes at point locations. Secondly, the physical process driving water quality is not compositional, rather it is the observation process that provides compositional data. In this paper we present an approach that accounts for these two issues by using a latent variable to identify the land-use that most likely influences water quality. This latent variable is used in a spatial mixture model to help estimate the influence of land-use on water quality. We demonstrate the potential of this approach with data from a water quality research study in the Mount Lofty range, in South Australia.|assess water qualiti across space time consider interest agricultur public health reason standard method assess water qualiti catchment group catchment usual involv collect point measur water qualiti addit inform date time measur rainfal amount land use soil type catchment elev auxiliari inform point data measur exact locat wherea land use areal data often composit format two problem aris analyst tri incorpor inform statist model order predict exampl influenc land use water qualiti first spatial chang support problem aris use areal data predict outcom point locat second physic process drive water qualiti composit rather observ process provid composit data paper present approach account two issu use latent variabl identifi land use like influenc water qualiti latent variabl use spatial mixtur model help estim influenc land use water qualiti demonstr potenti approach data water qualiti research studi mount lofti rang south australia|['Adrien Ickowicz', 'Jessica H. Ford', 'Keith R. Hayes']|['stat.AP', 'stat.CO']
2017-03-28T14:03:30Z|2017-02-28T09:59:57Z|http://arxiv.org/abs/1702.08061v2|http://arxiv.org/pdf/1702.08061v2|The Ensemble Kalman Filter: A Signal Processing Perspective|ensembl kalman filter signal process perspect|The ensemble Kalman filter (EnKF) is a Monte Carlo based implementation of the Kalman filter (KF) for extremely high-dimensional, possibly nonlinear and non-Gaussian state estimation problems. Its ability to handle state dimensions in the order of millions has made the EnKF a popular algorithm in different geoscientific disciplines. Despite a similarly vital need for scalable algorithms in signal processing, e.g., to make sense of the ever increasing amount of sensor data, the EnKF is hardly discussed in our field.   This self-contained review paper is aimed at signal processing researchers and provides all the knowledge to get started with the EnKF. The algorithm is derived in a KF framework, without the often encountered geoscientific terminology. Algorithmic challenges and required extensions of the EnKF are provided, as well as relations to sigma-point KF and particle filters. The relevant EnKF literature is summarized in an extensive survey and unique simulation examples, including popular benchmark problems, complement the theory with practical insights. The signal processing perspective highlights new directions of research and facilitates the exchange of potentially beneficial ideas, both for the EnKF and high-dimensional nonlinear and non-Gaussian filtering in general.|ensembl kalman filter enkf mont carlo base implement kalman filter kf extrem high dimension possibl nonlinear non gaussian state estim problem abil handl state dimens order million made enkf popular algorithm differ geoscientif disciplin despit similar vital need scalabl algorithm signal process make sens ever increas amount sensor data enkf hard discuss field self contain review paper aim signal process research provid knowledg get start enkf algorithm deriv kf framework without often encount geoscientif terminolog algorithm challeng requir extens enkf provid well relat sigma point kf particl filter relev enkf literatur summar extens survey uniqu simul exampl includ popular benchmark problem complement theori practic insight signal process perspect highlight new direct research facilit exchang potenti benefici idea enkf high dimension nonlinear non gaussian filter general|['Michael Roth', 'Gustaf Hendeby', 'Carsten Fritsche', 'Fredrik Gustafsson']|['stat.ME', 'cs.SY', 'stat.CO']
2017-03-28T14:03:34Z|2017-02-26T17:50:02Z|http://arxiv.org/abs/1702.08446v1|http://arxiv.org/pdf/1702.08446v1|Monte Carlo on manifolds: sampling densities and integrating functions|mont carlo manifold sampl densiti integr function|We describe and analyze some Monte Carlo methods for manifolds in Euclidean space defined by equality and inequality constraints. First, we give an MCMC sampler for probability distributions defined by un-normalized densities on such manifolds. The sampler uses a specific orthogonal projection to the surface that requires only information about the tangent space to the manifold, obtainable from first derivatives of the constraint functions, hence avoiding the need for curvature information or second derivatives. Second, we use the sampler to develop a multi-stage algorithm to compute integrals over such manifolds. We provide single-run error estimates that avoid the need for multiple independent runs. Computational experiments on various test problems show that the algorithms and error estimates work in practice. The method is applied to compute the entropies of different sticky hard sphere systems. These predict the temperature or interaction energy at which loops of hard sticky spheres become preferable to chains.|describ analyz mont carlo method manifold euclidean space defin equal inequ constraint first give mcmc sampler probabl distribut defin un normal densiti manifold sampler use specif orthogon project surfac requir onli inform tangent space manifold obtain first deriv constraint function henc avoid need curvatur inform second deriv second use sampler develop multi stage algorithm comput integr manifold provid singl run error estim avoid need multipl independ run comput experi various test problem show algorithm error estim work practic method appli comput entropi differ sticki hard sphere system predict temperatur interact energi loop hard sticki sphere becom prefer chain|['Emilio Zappa', 'Miranda Holmes-Cerfon', 'Jonathan Goodman']|['math.NA', 'cond-mat.stat-mech', 'stat.CO']
2017-03-28T14:03:34Z|2017-02-25T17:47:33Z|http://arxiv.org/abs/1702.07930v1|http://arxiv.org/pdf/1702.07930v1|Upper-Bounding the Regularization Constant for Convex Sparse Signal   Reconstruction|upper bound regular constant convex spars signal reconstruct|Consider reconstructing a signal $x$ by minimizing a weighted sum of a convex differentiable negative log-likelihood (NLL) (data-fidelity) term and a convex regularization term that imposes a convex-set constraint on $x$ and enforces its sparsity using $\ell_1$-norm analysis regularization. We compute upper bounds on the regularization tuning constant beyond which the regularization term overwhelmingly dominates the NLL term so that the set of minimum points of the objective function does not change. Necessary and sufficient conditions for irrelevance of sparse signal regularization and a condition for the existence of finite upper bounds are established. We formulate an optimization problem for finding these bounds when the regularization term can be globally minimized by a feasible $x$ and also develop an alternating direction method of multipliers (ADMM) type method for their computation. Simulation examples show that the derived and empirical bounds match.|consid reconstruct signal minim weight sum convex differenti negat log likelihood nll data fidel term convex regular term impos convex set constraint enforc sparsiti use ell norm analysi regular comput upper bound regular tune constant beyond regular term overwhelm domin nll term set minimum point object function doe chang necessari suffici condit irrelev spars signal regular condit exist finit upper bound establish formul optim problem find bound regular term global minim feasibl also develop altern direct method multipli admm type method comput simul exampl show deriv empir bound match|['Renliang Gu', 'Aleksandar Dogandžić']|['stat.CO', 'math.OC']
2017-03-28T14:03:34Z|2017-02-25T03:46:20Z|http://arxiv.org/abs/1702.07830v1|http://arxiv.org/pdf/1702.07830v1|A Near-Optimal Sampling Strategy for Sparse Recovery of Polynomial Chaos   Expansions|near optim sampl strategi spars recoveri polynomi chao expans|Compressive sampling has become a widely used approach to construct polynomial chaos surrogates when the number of available simulation samples is limited. Originally, these expensive simulation samples would be obtained at random locations in the parameter space. It was later shown that the choice of sample locations could significantly impact the accuracy of resulting surrogates. This motivated new sampling strategies or design-of-experiment approaches, such as coherence-optimal sampling, which aim at improving the coherence property. In this paper, we propose a sampling strategy that can identify near-optimal sample locations that lead to improvement in local-coherence property and also enhancement of cross-correlation properties of measurement matrices. We provide theoretical motivations for the proposed sampling strategy along with several numerical examples that show that our near-optimal sampling strategy produces substantially more accurate results, compared to other sampling strategies.|compress sampl becom wide use approach construct polynomi chao surrog number avail simul sampl limit origin expens simul sampl would obtain random locat paramet space later shown choic sampl locat could signific impact accuraci result surrog motiv new sampl strategi design experi approach coher optim sampl aim improv coher properti paper propos sampl strategi identifi near optim sampl locat lead improv local coher properti also enhanc cross correl properti measur matric provid theoret motiv propos sampl strategi along sever numer exampl show near optim sampl strategi produc substanti accur result compar sampl strategi|['Negin Alemazkoor', 'Hadi Meidani']|['stat.CO']
2017-03-28T14:03:34Z|2017-02-24T17:54:23Z|http://arxiv.org/abs/1702.07685v1|http://arxiv.org/pdf/1702.07685v1|ROPE: high-dimensional network modeling with robust control of edge FDR|rope high dimension network model robust control edg fdr|Network modeling has become increasingly popular for analyzing genomic data, to aid in the interpretation and discovery of possible mechanistic components and therapeutic targets. However, genomic-scale networks are high-dimensional models and are usually estimated from a relatively small number of samples. Therefore, their usefulness is hampered by estimation instability. In addition, the complexity of the models is controlled by one or more penalization (tuning) parameters where small changes to these can lead to vastly different networks, thus making interpretation of models difficult. This necessitates the development of techniques to produce robust network models accompanied by estimation quality assessments.   We introduce Resampling of Penalized Estimates (ROPE): a novel statistical method for robust network modeling. The method utilizes resampling-based network estimation and integrates results from several levels of penalization through a constrained, over-dispersed beta-binomial mixture model. ROPE provides robust False Discovery Rate (FDR) control of network estimates and each edge is assigned a measure of validity, the q-value, corresponding to the FDR-level for which the edge would be included in the network model. We apply ROPE to several simulated data sets as well as genomic data from The Cancer Genome Atlas. We show that ROPE outperforms state-of-the-art methods in terms of FDR control and robust performance across data sets. We illustrate how to use ROPE to make a principled model selection for which genomic associations to study further. ROPE is available as an R package on CRAN.|network model becom increas popular analyz genom data aid interpret discoveri possibl mechanist compon therapeut target howev genom scale network high dimension model usual estim relat small number sampl therefor use hamper estim instabl addit complex model control one penal tune paramet small chang lead vast differ network thus make interpret model difficult necessit develop techniqu produc robust network model accompani estim qualiti assess introduc resampl penal estim rope novel statist method robust network model method util resampl base network estim integr result sever level penal constrain dispers beta binomi mixtur model rope provid robust fals discoveri rate fdr control network estim edg assign measur valid valu correspond fdr level edg would includ network model appli rope sever simul data set well genom data cancer genom atlas show rope outperform state art method term fdr control robust perform across data set illustr use rope make principl model select genom associ studi rope avail packag cran|['Jonatan Kallus', 'Jose Sanchez', 'Alexandra Jauhiainen', 'Sven Nelander', 'Rebecka Jörnsten']|['stat.CO']
2017-03-28T14:03:34Z|2017-02-24T17:01:59Z|http://arxiv.org/abs/1702.07662v1|http://arxiv.org/pdf/1702.07662v1|A Network Epidemic Model for Online Community Commissioning Data|network epidem model onlin communiti commiss data|Statistical models for network epidemics usually assume a Bernoulli random graph, in which any two nodes have the same probability of being connected. This assumption provides computational simplicity but does not describe real-life networks well. We propose an epidemic model based on the preferential attachment model, which adds nodes sequentially by simple rules to generate a network. A simulation study based on the subsequent Markov Chain Monte Carlo algorithm reveals an identifiability issue with the model parameters, so an alternative parameterisation is suggested. Finally, the model is applied to a set of online commissioning data.|statist model network epidem usual assum bernoulli random graph ani two node probabl connect assumpt provid comput simplic doe describ real life network well propos epidem model base preferenti attach model add node sequenti simpl rule generat network simul studi base subsequ markov chain mont carlo algorithm reveal identifi issu model paramet altern parameteris suggest final model appli set onlin commiss data|['Clement Lee', 'Andrew Garbett', 'Darren J. Wilkinson']|['stat.CO', 'cs.SI', 'stat.ME']
2017-03-28T14:03:34Z|2017-02-23T21:37:06Z|http://arxiv.org/abs/1702.07400v1|http://arxiv.org/pdf/1702.07400v1|Horseshoe Regularization for Feature Subset Selection|horsesho regular featur subset select|Feature subset selection arises in many high-dimensional applications in machine learning and statistics, such as compressed sensing and genomics. The $\ell_0$ penalty is ideal for this task, the caveat being it requires the NP-hard combinatorial evaluation of all models. A recent area of considerable interest is to develop efficient algorithms to fit models with a non-convex $\ell_\gamma$ penalty for $\gamma\in (0,1)$, which results in sparser models than the convex $\ell_1$ or lasso penalty, but is harder to fit. We propose an alternative, termed the horseshoe regularization penalty for feature subset selection, and demonstrate its theoretical and computational advantages. The distinguishing feature from existing non-convex optimization approaches is a full probabilistic representation of the penalty as the negative of the logarithm of a suitable prior, which in turn enables an efficient expectation-maximization algorithm for optimization and MCMC for uncertainty quantification. In synthetic and real data, the resulting algorithm provides better statistical performance, and the computation requires a fraction of time of state of the art non-convex solvers.|featur subset select aris mani high dimension applic machin learn statist compress sens genom ell penalti ideal task caveat requir np hard combinatori evalu model recent area consider interest develop effici algorithm fit model non convex ell gamma penalti gamma result sparser model convex ell lasso penalti harder fit propos altern term horsesho regular penalti featur subset select demonstr theoret comput advantag distinguish featur exist non convex optim approach full probabilist represent penalti negat logarithm suitabl prior turn enabl effici expect maxim algorithm optim mcmc uncertainti quantif synthet real data result algorithm provid better statist perform comput requir fraction time state art non convex solver|['Anindya Bhadra', 'Jyotishka Datta', 'Nicholas G. Polson', 'Brandon Willard']|['stat.ML', 'stat.CO']
2017-03-28T14:03:34Z|2017-02-23T04:40:41Z|http://arxiv.org/abs/1702.07094v1|http://arxiv.org/pdf/1702.07094v1|BigVAR: Tools for Modeling Sparse High-Dimensional Multivariate Time   Series|bigvar tool model spars high dimension multivari time seri|The R package BigVAR allows for the simultaneous estimation of high-dimensional time series by applying structured penalties to the conventional vector autoregression (VAR) and vector autoregression with exogenous variables (VARX) frameworks. Our methods can be utilized in many forecasting applications that make use of time-dependent data such as macroeconomics, finance, and internet traffic. Our package extends solution algorithms from the machine learning and signal processing literatures to a time dependent setting: selecting the regularization parameter by sequential cross validation and provides substantial improvements in forecasting performance over conventional methods. We offer a user-friendly interface that utilizes R's s4 object class structure which makes our methodology easily accessible to practicioners.   In this paper, we present an overview of our notation, the models that comprise BigVAR, and the functionality of our package with a detailed example using publicly available macroeconomic data. In addition, we present a simulation study comparing the performance of several procedures that refit the support selected by a BigVAR procedure according to several variants of least squares and conclude that refitting generally degrades forecast performance.|packag bigvar allow simultan estim high dimension time seri appli structur penalti convent vector autoregress var vector autoregress exogen variabl varx framework method util mani forecast applic make use time depend data macroeconom financ internet traffic packag extend solut algorithm machin learn signal process literatur time depend set select regular paramet sequenti cross valid provid substanti improv forecast perform convent method offer user friend interfac util object class structur make methodolog easili access practicion paper present overview notat model compris bigvar function packag detail exampl use public avail macroeconom data addit present simul studi compar perform sever procedur refit support select bigvar procedur accord sever variant least squar conclud refit general degrad forecast perform|['William Nicholson', 'David Matteson', 'Jacob Bien']|['stat.CO']
2017-03-28T14:03:34Z|2017-02-22T00:43:01Z|http://arxiv.org/abs/1702.06632v1|http://arxiv.org/pdf/1702.06632v1|A Balanced Algorithm for Sampling Abstract Simplicial Complexes|balanc algorithm sampl abstract simplici complex|We provide an algorithm for sampling the space of abstract simplicial complexes on a fixed number of vertices that aims to provide a balanced sampling over non-isomorphic complexes. Although sampling uniformly from geometrically distinct complexes is a difficult task with no known analytic algorithm, our generative and descriptive algorithm is designed with heuristics to help balance the combinatorial multiplicities of the states and more widely sample across the space of inequivalent configurations. We provide a formula for the exact probabilities with which this algorithm will produce a requested labeled state, and compare the algorithm to Kahle's multi-parameter model of exponential random simplicial complexes, demonstrating analytically that our algorithm performs better with respect to worst-case probability bounds on a given complex and providing numerical results illustrating the increased sampling efficiency over distinct classes.|provid algorithm sampl space abstract simplici complex fix number vertic aim provid balanc sampl non isomorph complex although sampl uniform geometr distinct complex difficult task known analyt algorithm generat descript algorithm design heurist help balanc combinatori multipl state wide sampl across space inequival configur provid formula exact probabl algorithm produc request label state compar algorithm kahl multi paramet model exponenti random simplici complex demonstr analyt algorithm perform better respect worst case probabl bound given complex provid numer result illustr increas sampl effici distinct class|['John Lombard']|['stat.CO', 'math.CO', 'math.PR']
2017-03-28T14:03:34Z|2017-02-23T19:01:53Z|http://arxiv.org/abs/1702.06488v2|http://arxiv.org/pdf/1702.06488v2|Distributed Estimation of Principal Eigenspaces|distribut estim princip eigenspac|"Principal component analysis (PCA) is fundamental to statistical machine learning. It extracts latent principal factors that contribute to the most variation of the data. When data are stored across multiple machines, however, communication cost can prohibit the computation of PCA in a central location and distributed algorithms for PCA are thus needed. This paper proposes and studies a distributed PCA algorithm: each node machine computes the top $K$ eigenvectors and transmits them to the central server; the central server then aggregates the information from all the node machines and conducts a PCA based on the aggregated information. We investigate the bias and variance for the resulting distributed estimator of the top $K$ eigenvectors. In particular, we show that for distributions with symmetric innovation, the distributed PCA is ""unbiased"". We derive the rate of convergence for distributed PCA estimators, which depends explicitly on the effective rank of covariance, eigen-gap, and the number of machines. We show that when the number of machines is not unreasonably large, the distributed PCA performs as well as the whole sample PCA, even without full access of whole data. The theoretical results are verified by an extensive simulation study. We also extend our analysis to the heterogeneous case where the population covariance matrices are different across local machines but share similar top eigen-structures."|princip compon analysi pca fundament statist machin learn extract latent princip factor contribut variat data data store across multipl machin howev communic cost prohibit comput pca central locat distribut algorithm pca thus need paper propos studi distribut pca algorithm node machin comput top eigenvector transmit central server central server aggreg inform node machin conduct pca base aggreg inform investig bias varianc result distribut estim top eigenvector particular show distribut symmetr innov distribut pca unbias deriv rate converg distribut pca estim depend explicit effect rank covari eigen gap number machin show number machin unreason larg distribut pca perform well whole sampl pca even without full access whole data theoret result verifi extens simul studi also extend analysi heterogen case popul covari matric differ across local machin share similar top eigen structur|['Jianqing Fan', 'Dong Wang', 'Kaizheng Wang', 'Ziwei Zhu']|['stat.CO', 'math.ST', 'stat.TH']
2017-03-28T14:03:34Z|2017-02-23T14:07:34Z|http://arxiv.org/abs/1702.06407v2|http://arxiv.org/pdf/1702.06407v2|General Semiparametric Shared Frailty Model Estimation and Simulation   with frailtySurv|general semiparametr share frailti model estim simul frailtysurv|The R package frailtySurv for simulating and fitting semi-parametric shared frailty models is introduced. frailtySurv implements semi-parametric consistent estimators for a variety of frailty distributions, including gamma, log-normal, inverse Gaussian and power variance function, and provides consistent estimators of the standard errors of the parameters' estimators. The parameters' estimators are asymptotically normally distributed, and therefore statistical inference based on the results of this package, such as hypothesis testing and confidence intervals, can be performed using the normal distribution. Extensive simulations demonstrate the flexibility and correct implementation of the estimator. Two case studies performed with publicly-available datasets demonstrate applicability of the package. In the Diabetic Retinopathy Study, the onset of blindness is clustered by patient, and in a large hard drive failure dataset, failure times are thought to be clustered by the hard drive manufacturer and model.|packag frailtysurv simul fit semi parametr share frailti model introduc frailtysurv implement semi parametr consist estim varieti frailti distribut includ gamma log normal invers gaussian power varianc function provid consist estim standard error paramet estim paramet estim asymptot normal distribut therefor statist infer base result packag hypothesi test confid interv perform use normal distribut extens simul demonstr flexibl correct implement estim two case studi perform public avail dataset demonstr applic packag diabet retinopathi studi onset blind cluster patient larg hard drive failur dataset failur time thought cluster hard drive manufactur model|['John V. Monaco', 'Malka Gorfine', 'Li Hsu']|['stat.CO', 'cs.MS']
2017-03-28T14:03:38Z|2017-03-20T19:49:02Z|http://arxiv.org/abs/1702.05698v2|http://arxiv.org/pdf/1702.05698v2|Online Robust Principal Component Analysis with Change Point Detection|onlin robust princip compon analysi chang point detect|Robust PCA methods are typically batch algorithms which requires loading all observations into memory before processing. This makes them inefficient to process big data. In this paper, we develop an efficient online robust principal component methods, namely online moving window robust principal component analysis (OMWRPCA). Unlike existing algorithms, OMWRPCA can successfully track not only slowly changing subspace but also abruptly changed subspace. By embedding hypothesis testing into the algorithm, OMWRPCA can detect change points of the underlying subspaces. Extensive simulation studies demonstrate the superior performance of OMWRPCA compared with other state-of-art approaches. We also apply the algorithm for real-time background subtraction of surveillance video.|robust pca method typic batch algorithm requir load observ memori befor process make ineffici process big data paper develop effici onlin robust princip compon method name onlin move window robust princip compon analysi omwrpca unlik exist algorithm omwrpca success track onli slowli chang subspac also abrupt chang subspac embed hypothesi test algorithm omwrpca detect chang point subspac extens simul studi demonstr superior perform omwrpca compar state art approach also appli algorithm real time background subtract surveil video|['Wei Xiao', 'Xiaolin Huang', 'Jorge Silva', 'Saba Emrani', 'Arin Chaudhuri']|['cs.LG', 'cs.CV', 'stat.AP', 'stat.CO', 'stat.ML']
2017-03-28T14:03:38Z|2017-02-18T00:04:25Z|http://arxiv.org/abs/1702.05546v1|http://arxiv.org/pdf/1702.05546v1|A Sequential Scheme for Large Scale Bayesian Multiple Testing|sequenti scheme larg scale bayesian multipl test|The problem of large scale multiple testing arises in many contexts, including testing for pairwise interaction among large numbers of neurons. With advances in technologies, it has become common to record from hundreds of neurons simultaneously, and this number is growing quickly, so that the number of pairwise tests can be very large. It is important to control the rate at which false positives occur. In addition, there is sometimes information that affects the probability of a positive result for any given pair. In the case of neurons, they are more likely to have correlated activity when they are close together, and when they respond similarly to various stimuli. Recently a method was developed to control false positives when covariate information, such as distances between pairs of neurons, is available. This method, however, relies on computationally-intensive Markov Chain Monte Carlo (MCMC). Here we develop an alternative, based on Sequential Monte Carlo, which scales well with the size of the dataset. This scheme considers data items sequentially, with relevant probabilities being updated at each step. Simulation experiments demonstrate that the proposed algorithm delivers results as accurately as the previous MCMC method with only a single pass through the data. We illustrate the method by using it to analyze neural recordings from extrastriate cortex in a macaque monkey. The scripts that implement the proposed algorithm with a synthetic dataset are available online at: https://github.com/robinlau1981/SMC_Multi_Testing.|problem larg scale multipl test aris mani context includ test pairwis interact among larg number neuron advanc technolog becom common record hundr neuron simultan number grow quick number pairwis test veri larg import control rate fals posit occur addit sometim inform affect probabl posit result ani given pair case neuron like correl activ close togeth respond similar various stimuli recent method develop control fals posit covari inform distanc pair neuron avail method howev reli comput intens markov chain mont carlo mcmc develop altern base sequenti mont carlo scale well size dataset scheme consid data item sequenti relev probabl updat step simul experi demonstr propos algorithm deliv result accur previous mcmc method onli singl pass data illustr method use analyz neural record extrastri cortex macaqu monkey script implement propos algorithm synthet dataset avail onlin https github com robinlau smc multi test|['Bin Liu', 'Giuseppe Vinci', 'Adam C. Snyder', 'Matthew A. Smith', 'Robert E. Kass']|['stat.CO']
2017-03-28T14:03:38Z|2017-02-17T21:16:40Z|http://arxiv.org/abs/1702.05518v1|http://arxiv.org/pdf/1702.05518v1|Sampling Strategies for Fast Updating of Gaussian Markov Random Fields|sampl strategi fast updat gaussian markov random field|Gaussian Markov random fields (GMRFs) are popular for modeling temporal or spatial dependence in large areal datasets due to their ease of interpretation and computational convenience afforded by conditional independence and their sparse precision matrices needed for random variable generation. Using such models inside a Markov chain Monte Carlo algorithm requires repeatedly simulating random fields. This is a nontrivial issue, especially when the full conditional precision matrix depends on parameters that change at each iteration. Typically in Bayesian computation, GMRFs are updated jointly in a block Gibbs sampler or one location at a time in a single-site sampler. The former approach leads to quicker convergence by updating correlated variables all at once, while the latter avoids solving large matrices. Efficient algorithms for sampling Markov random fields have become the focus of much recent research in the machine learning literature, much of which can be useful to statisticians. We briefly review recently proposed approaches with an eye toward implementation for statisticians without expertise in numerical analysis or advanced computing. In particular, we consider a version of block sampling in which the underlying graph can be cut so that conditionally independent sites are all updated together. This algorithm allows a practitioner to parallelize the updating of a subset locations or to take advantage of `vectorized' calculations in a high-level language such as R. Through both simulation and real data application, we demonstrate computational savings that can be achieved versus both traditional single-site updating and block updating, regardless of whether the data are on a regular or irregular lattice. We argue that this easily-implemented sampling routine provides a good compromise between statistical and computational efficiency when working with large datasets.|gaussian markov random field gmrfs popular model tempor spatial depend larg areal dataset due eas interpret comput conveni afford condit independ spars precis matric need random variabl generat use model insid markov chain mont carlo algorithm requir repeat simul random field nontrivi issu especi full condit precis matrix depend paramet chang iter typic bayesian comput gmrfs updat joint block gibb sampler one locat time singl site sampler former approach lead quicker converg updat correl variabl onc latter avoid solv larg matric effici algorithm sampl markov random field becom focus much recent research machin learn literatur much use statistician briefli review recent propos approach eye toward implement statistician without expertis numer analysi advanc comput particular consid version block sampl graph cut condit independ site updat togeth algorithm allow practition parallel updat subset locat take advantag vector calcul high level languag simul real data applic demonstr comput save achiev versus tradit singl site updat block updat regardless whether data regular irregular lattic argu easili implement sampl routin provid good compromis statist comput effici work larg dataset|['D. Andrew Brown', 'Christopher S. McMahan']|['stat.CO']
2017-03-28T14:03:38Z|2017-02-17T18:06:27Z|http://arxiv.org/abs/1702.05462v1|http://arxiv.org/pdf/1702.05462v1|Objective Bayesian Analysis for Change Point Problems|object bayesian analysi chang point problem|In this paper we present an objective approach to change point analysis. In particular, we look at the problem from two perspectives. The first focuses on the definition of an objective prior when the number of change points is known a priori. The second contribution aims to estimate the number of change points by using an objective approach, recently introduced in the literature, based on losses. The latter considers change point estimation as a model selection exercise. We show the performance of the proposed approach on simulated data and on real data sets.|paper present object approach chang point analysi particular look problem two perspect first focus definit object prior number chang point known priori second contribut aim estim number chang point use object approach recent introduc literatur base loss latter consid chang point estim model select exercis show perform propos approach simul data real data set|['Laurentiu Hinoveanu', 'Fabrizio Leisen', 'Cristiano Villa']|['stat.ME', 'math.ST', 'stat.AP', 'stat.CO', 'stat.ML', 'stat.TH']
2017-03-28T14:03:38Z|2017-02-15T11:52:14Z|http://arxiv.org/abs/1702.04561v1|http://arxiv.org/pdf/1702.04561v1|Probing for sparse and fast variable selection with model-based boosting|probe spars fast variabl select model base boost|We present a new variable selection method based on model-based gradient boosting and randomly permuted variables. Model-based boosting is a tool to fit a statistical model while performing variable selection at the same time. A drawback of the fitting lies in the need of multiple model fits on slightly altered data (e.g. cross-validation or bootstrap) to find the optimal number of boosting iterations and prevent overfitting. In our proposed approach, we augment the data set with randomly permuted versions of the true variables, so called shadow variables, and stop the step-wise fitting as soon as such a variable would be added to the model. This allows variable selection in a single fit of the model without requiring further parameter tuning. We show that our probing approach can compete with state-of-the-art selection methods like stability selection in a high-dimensional classification benchmark and apply it on gene expression data for the estimation of riboflavin production of Bacillus subtilis.|present new variabl select method base model base gradient boost random permut variabl model base boost tool fit statist model perform variabl select time drawback fit lie need multipl model fit slight alter data cross valid bootstrap find optim number boost iter prevent overfit propos approach augment data set random permut version true variabl call shadow variabl stop step wise fit soon variabl would ad model allow variabl select singl fit model without requir paramet tune show probe approach compet state art select method like stabil select high dimension classif benchmark appli gene express data estim riboflavin product bacillus subtili|['Janek Thomas', 'Tobias Hepp', 'Andreas Mayr', 'Bernd Bischl']|['stat.ML', 'stat.CO']
2017-03-28T14:03:38Z|2017-02-14T21:20:23Z|http://arxiv.org/abs/1702.04391v1|http://arxiv.org/pdf/1702.04391v1|Bootstrap-based inferential improvements in beta autoregressive moving   average model|bootstrap base inferenti improv beta autoregress move averag model|We consider the issue of performing accurate small sample inference in beta autoregressive moving average model, which is useful for modeling and forecasting continuous variables that assumes values in the interval $(0,1)$. The inferences based on conditional maximum likelihood estimation have good asymptotic properties, but their performances in small samples may be poor. This way, we propose bootstrap bias corrections of the point estimators and different bootstrap strategies for confidence interval improvements. Our Monte Carlo simulations show that finite sample inference based on bootstrap corrections is much more reliable than the usual inferences. We also presented an empirical application.|consid issu perform accur small sampl infer beta autoregress move averag model use model forecast continu variabl assum valu interv infer base condit maximum likelihood estim good asymptot properti perform small sampl may poor way propos bootstrap bias correct point estim differ bootstrap strategi confid interv improv mont carlo simul show finit sampl infer base bootstrap correct much reliabl usual infer also present empir applic|['Bruna Gregory Palm', 'Fábio M. Bayer']|['stat.CO']
2017-03-28T14:03:38Z|2017-02-13T17:23:02Z|http://arxiv.org/abs/1702.03891v1|http://arxiv.org/pdf/1702.03891v1|Spatial Models with the Integrated Nested Laplace Approximation within   Markov Chain Monte Carlo|spatial model integr nest laplac approxim within markov chain mont carlo|The Integrated Nested Laplace Approximation (INLA) is a convenient way to obtain approximations to the posterior marginals for parameters in Bayesian hierarchical models when the latent effects can be expressed as a Gaussian Markov Random Field (GMRF). In addition, its implementation in the R-INLA package for the R statistical software provides an easy way to fit models using INLA in practice. R-INLA implements a number of widely used latent models, including several spatial models. In addition, R-INLA can fit models in a fraction of the time than other computer intensive methods (e.g. Markov Chain Monte Carlo) take to fit the same model.   Although INLA provides a fast approximation to the marginals of the model parameters, it is difficult to use it with models not implemented in R-INLA. It is also difficult to make multivariate posterior inference on the parameters of the model as INLA focuses on the posterior marginals and not the joint posterior distribution.   In this paper we describe how to use INLA within the Metropolis-Hastings algorithm to fit spatial models and estimate the joint posterior distribution of a reduced number of parameters. We will illustrate the benefits of this new method with two examples on spatial econometrics and disease mapping where complex spatial models with several spatial structures need to be fitted.|integr nest laplac approxim inla conveni way obtain approxim posterior margin paramet bayesian hierarch model latent effect express gaussian markov random field gmrf addit implement inla packag statist softwar provid easi way fit model use inla practic inla implement number wide use latent model includ sever spatial model addit inla fit model fraction time comput intens method markov chain mont carlo take fit model although inla provid fast approxim margin model paramet difficult use model implement inla also difficult make multivari posterior infer paramet model inla focus posterior margin joint posterior distribut paper describ use inla within metropoli hast algorithm fit spatial model estim joint posterior distribut reduc number paramet illustr benefit new method two exampl spatial econometr diseas map complex spatial model sever spatial structur need fit|['Virgilio Gómez-Rubio', 'Francisco Palmí-Perales']|['stat.CO']
2017-03-28T14:03:38Z|2017-02-13T08:52:58Z|http://arxiv.org/abs/1702.03673v1|http://arxiv.org/pdf/1702.03673v1|Bayesian Probabilistic Numerical Methods|bayesian probabilist numer method|The emergent field of probabilistic numerics has thus far lacked rigorous statistical principals. This paper establishes Bayesian probabilistic numerical methods as those which can be cast as solutions to certain Bayesian inverse problems, albeit problems that are non-standard. This allows us to establish general conditions under which Bayesian probabilistic numerical methods are well-defined, encompassing both non-linear and non-Gaussian models. For general computation, a numerical approximation scheme is developed and its asymptotic convergence is established. The theoretical development is then extended to pipelines of computation, wherein probabilistic numerical methods are composed to solve more challenging numerical tasks. The contribution highlights an important research frontier at the interface of numerical analysis and uncertainty quantification, with some illustrative applications presented.|emerg field probabilist numer thus far lack rigor statist princip paper establish bayesian probabilist numer method cast solut certain bayesian invers problem albeit problem non standard allow us establish general condit bayesian probabilist numer method well defin encompass non linear non gaussian model general comput numer approxim scheme develop asymptot converg establish theoret develop extend pipelin comput wherein probabilist numer method compos solv challeng numer task contribut highlight import research frontier interfac numer analysi uncertainti quantif illustr applic present|['Jon Cockayne', 'Chris Oates', 'Tim Sullivan', 'Mark Girolami']|['stat.ME', 'cs.NA', 'math.NA', 'math.ST', 'stat.CO', 'stat.TH']
2017-03-28T14:03:38Z|2017-02-10T12:26:52Z|http://arxiv.org/abs/1702.03146v1|http://arxiv.org/pdf/1702.03146v1|Analysis of a nonlinear importance sampling scheme for Bayesian   parameter estimation in state-space models|analysi nonlinear import sampl scheme bayesian paramet estim state space model|The Bayesian estimation of the unknown parameters of state-space (dynamical) systems has received considerable attention over the past decade, with a handful of powerful algorithms being introduced. In this paper we tackle the theoretical analysis of the recently proposed {\it nonlinear} population Monte Carlo (NPMC). This is an iterative importance sampling scheme whose key features, compared to conventional importance samplers, are (i) the approximate computation of the importance weights (IWs) assigned to the Monte Carlo samples and (ii) the nonlinear transformation of these IWs in order to prevent the degeneracy problem that flaws the performance of conventional importance samplers. The contribution of the present paper is a rigorous proof of convergence of the nonlinear IS (NIS) scheme as the number of Monte Carlo samples, $M$, increases. Our analysis reveals that the NIS approximation errors converge to 0 almost surely and with the optimal Monte Carlo rate of $M^{-\frac{1}{2}}$. Moreover, we prove that this is achieved even when the mean estimation error of the IWs remains constant, a property that has been termed {\it exact approximation} in the Markov chain Monte Carlo literature. We illustrate these theoretical results by means of a computer simulation example involving the estimation of the parameters of a state-space model typically used for target tracking.|bayesian estim unknown paramet state space dynam system receiv consider attent past decad hand power algorithm introduc paper tackl theoret analysi recent propos nonlinear popul mont carlo npmc iter import sampl scheme whose key featur compar convent import sampler approxim comput import weight iw assign mont carlo sampl ii nonlinear transform iw order prevent degeneraci problem flaw perform convent import sampler contribut present paper rigor proof converg nonlinear nis scheme number mont carlo sampl increas analysi reveal nis approxim error converg almost sure optim mont carlo rate frac moreov prove achiev even mean estim error iw remain constant properti term exact approxim markov chain mont carlo literatur illustr theoret result mean comput simul exampl involv estim paramet state space model typic use target track|['Joaquin Miguez', 'Ines P. Mariño', 'Manuel A. Vazquez']|['stat.CO']
2017-03-28T14:03:38Z|2017-02-10T10:44:23Z|http://arxiv.org/abs/1702.03126v1|http://arxiv.org/pdf/1702.03126v1|Computational inference without proposal kernels|comput infer without propos kernel|Likelihood-free methods, such as approximate Bayesian computation, are powerful tools for practical inference problems with intractable likelihood functions. Markov chain Monte Carlo and sequential Monte Carlo variants of approximate Bayesian computation can be effective techniques for sampling posterior distributions without likelihoods. However, the efficiency of these methods depends crucially on the proposal kernel used to generate proposal posterior samples, and a poor choice can lead to extremely low efficiency. We propose a new method for likelihood-free Bayesian inference based upon ideas from multilevel Monte Carlo. Our method is accurate and does not require proposal kernels, thereby overcoming a key obstacle in the use of likelihood-free approaches in real-world situations.|likelihood free method approxim bayesian comput power tool practic infer problem intract likelihood function markov chain mont carlo sequenti mont carlo variant approxim bayesian comput effect techniqu sampl posterior distribut without likelihood howev effici method depend crucial propos kernel use generat propos posterior sampl poor choic lead extrem low effici propos new method likelihood free bayesian infer base upon idea multilevel mont carlo method accur doe requir propos kernel therebi overcom key obstacl use likelihood free approach real world situat|['David J. Warne', 'Ruth E. Baker', 'Matthew J. Simpson']|['stat.CO', '62F15, 65C05']
2017-03-28T14:03:42Z|2017-02-15T10:14:43Z|http://arxiv.org/abs/1702.03057v2|http://arxiv.org/pdf/1702.03057v2|Unbiased Multi-index Monte Carlo|unbias multi index mont carlo|We introduce a new class of Monte Carlo based approximations of expectations of random variables defined whose laws are not available directly, but only through certain discretisatizations. Sampling from the discretized versions of these laws can typically introduce a bias. In this paper, we show how to remove that bias, by introducing a new version of multi-index Monte Carlo (MIMC) that has the added advantage of reducing the computational effort, relative to i.i.d. sampling from the most precise discretization, for a given level of error. We cover extensions of results regarding variance and optimality criteria for the new approach. We apply the methodology to the problem of computing an unbiased mollified version of the solution of a partial differential equation with random coefficients. A second application concerns the Bayesian inference (the smoothing problem) of an infinite dimensional signal modelled by the solution of a stochastic partial differential equation that is observed on a discrete space grid and at discrete times. Both applications are complemented by numerical simulations.|introduc new class mont carlo base approxim expect random variabl defin whose law avail direct onli certain discretisat sampl discret version law typic introduc bias paper show remov bias introduc new version multi index mont carlo mimc ad advantag reduc comput effort relat sampl precis discret given level error cover extens result regard varianc optim criteria new approach appli methodolog problem comput unbias mollifi version solut partial differenti equat random coeffici second applic concern bayesian infer smooth problem infinit dimension signal model solut stochast partial differenti equat observ discret space grid discret time applic complement numer simul|['Dan Crisan', 'Jeremie Houssineau', 'Ajay Jasra']|['stat.CO']
2017-03-28T14:03:42Z|2017-02-14T04:22:00Z|http://arxiv.org/abs/1702.02707v2|http://arxiv.org/pdf/1702.02707v2|A Fast Algorithm for the Coordinate-wise Minimum Distance Estimation|fast algorithm coordin wise minimum distanc estim|Application of the minimum distance method to the linear regression model for estimating regression parameters is a difficult and time-consuming process due to the complexity of its distance function, and hence, it is computationally expensive. To deal with the computational cost, this paper proposes a fast algorithm which mainly uses technique of coordinate-wise minimization in order to estimate the regression parameters. R package based on the proposed algorithm and written in Rcpp is available online.|applic minimum distanc method linear regress model estim regress paramet difficult time consum process due complex distanc function henc comput expens deal comput cost paper propos fast algorithm main use techniqu coordin wise minim order estim regress paramet packag base propos algorithm written rcpp avail onlin|['Jiwoong Kim']|['stat.CO']
2017-03-28T14:03:42Z|2017-02-09T00:11:27Z|http://arxiv.org/abs/1702.02658v1|http://arxiv.org/pdf/1702.02658v1|Estimating the number of clusters using cross-validation|estim number cluster use cross valid|Many clustering methods, including k-means, require the user to specify the number of clusters as an input parameter. A variety of methods have been devised to choose the number of clusters automatically, but they often rely on strong modeling assumptions. This paper proposes a data-driven approach to estimate the number of clusters based on a novel form of cross-validation. The proposed method differs from ordinary cross-validation, because clustering is fundamentally an unsupervised learning problem. Simulation and real data analysis results show that the proposed method outperforms existing methods, especially in high-dimensional settings with heterogeneous or heavy-tailed noise. In a yeast cell cycle dataset, the proposed method finds a parsimonious clustering with interpretable gene groupings.|mani cluster method includ mean requir user specifi number cluster input paramet varieti method devis choos number cluster automat often reli strong model assumpt paper propos data driven approach estim number cluster base novel form cross valid propos method differ ordinari cross valid becaus cluster fundament unsupervis learn problem simul real data analysi result show propos method outperform exist method especi high dimension set heterogen heavi tail nois yeast cell cycl dataset propos method find parsimoni cluster interpret gene group|['Wei Fu', 'Patrick O. Perry']|['stat.ME', 'stat.CO']
2017-03-28T14:03:42Z|2017-02-06T14:01:20Z|http://arxiv.org/abs/1702.01618v1|http://arxiv.org/pdf/1702.01618v1|Learning of state-space models with highly informative observations: a   tempered Sequential Monte Carlo solution|learn state space model high inform observ temper sequenti mont carlo solut|Probabilistic (or Bayesian) modeling and learning offers interesting possibilities for systematic representation of uncertainty based on probability theory. Recent advances in Monte Carlo based methods have made previously intractable problem possible to solve using only the computational power available in a standard personal computer. For probabilistic learning of unknown parameters in nonlinear state-space models, methods based on the particle filter have proven useful. However, a notoriously challenging problem occurs when the observations are highly informative, i.e. when there is very little or no measurement noise present. The particle filter will then struggle in estimating one of the basic component in most parameter learning algorithms, the likelihood p(data parameters). To this end we suggest an algorithm which initially assumes that there is artificial measurement noise present. The variance of this noise is sequentially decreased in an adaptive fashion such that we in the end recover the original problem or possibly a very close approximation of it. Computationally the parameters are learned using a sequential Monte Carlo (SMC) sampler, which gives our proposed method a clear resemblance to the SMC^2 method. Another natural link is also made to the ideas underlying the so-called approximate Bayesian computation (ABC). We provide a theoretical justification (implying convergence results) for the suggested approach. We also illustrate it with numerical examples, and in particular show promising results for a challenging Wiener-Hammerstein benchmark.|probabilist bayesian model learn offer interest possibl systemat represent uncertainti base probabl theori recent advanc mont carlo base method made previous intract problem possibl solv use onli comput power avail standard person comput probabilist learn unknown paramet nonlinear state space model method base particl filter proven use howev notori challeng problem occur observ high inform veri littl measur nois present particl filter struggl estim one basic compon paramet learn algorithm likelihood data paramet end suggest algorithm initi assum artifici measur nois present varianc nois sequenti decreas adapt fashion end recov origin problem possibl veri close approxim comput paramet learn use sequenti mont carlo smc sampler give propos method clear resembl smc method anoth natur link also made idea call approxim bayesian comput abc provid theoret justif impli converg result suggest approach also illustr numer exampl particular show promis result challeng wiener hammerstein benchmark|['Andreas Svensson', 'Thomas B. Schön', 'Fredrik Lindsten']|['stat.CO', 'stat.ML']
2017-03-28T14:03:42Z|2017-03-22T14:35:39Z|http://arxiv.org/abs/1702.01418v2|http://arxiv.org/pdf/1702.01418v2|Choosing the number of groups in a latent stochastic block model for   dynamic networks|choos number group latent stochast block model dynam network|Latent stochastic block models are flexible statistical models that are widely used in social network analysis. In recent years, efforts have been made to extend these models to temporal dynamic networks, whereby the connections between nodes are observed at a number of different times. In this paper we extend the original stochastic block model by using a Markovian property to describe the evolution of nodes' cluster memberships over time. We recast the problem of clustering the nodes of the network into a model-based context, and show that the integrated completed likelihood can be evaluated analytically for a number of likelihood models. Then, we propose a scalable greedy algorithm to maximise this quantity, thereby estimating both the optimal partition and the ideal number of groups in a single inferential framework. Finally we propose applications of our methodology to both real and artificial datasets.|latent stochast block model flexibl statist model wide use social network analysi recent year effort made extend model tempor dynam network wherebi connect node observ number differ time paper extend origin stochast block model use markovian properti describ evolut node cluster membership time recast problem cluster node network model base context show integr complet likelihood evalu analyt number likelihood model propos scalabl greedi algorithm maximis quantiti therebi estim optim partit ideal number group singl inferenti framework final propos applic methodolog real artifici dataset|['Riccardo Rastelli', 'Pierre Latouche', 'Nial Friel']|['stat.ME', 'stat.CO']
2017-03-28T14:03:42Z|2017-02-05T04:55:14Z|http://arxiv.org/abs/1702.01373v1|http://arxiv.org/pdf/1702.01373v1|Exact heat kernel on a hypersphere and its applications in kernel SVM|exact heat kernel hyperspher applic kernel svm|"Many contemporary statistical learning methods assume a Euclidean feature space, however, the ""curse of dimensionality"" associated with high feature dimensions is particularly severe for the Euclidean distance. This paper presents a method for defining similarity based on hyperspherical geometry and shows that it often improves the performance of support vector machine compared to other competing similarity measures. Specifically, the idea of using heat diffusion on a hypersphere to measure similarity has been proposed and tested by \citet{Lafferty:2015uy}, demonstrating promising results based on an approximate heat kernel, however, the exact hyperspherical heat kernel hitherto remains unknown. In this paper, we derive an exact form of the heat kernel on a unit hypersphere in terms of a uniformly and absolutely convergent series in high-dimensional angular momentum eigenmodes. Being a natural measure of similarity between sample points dwelling on a hypersphere, the exact kernel often shows superior performance in kernel SVM classifications applied to text mining, tumor somatic mutation imputation, and stock market analysis. The improvement in classification accuracy compared with kernels based on Euclidean geometry may arise from ameliorating the curse of dimensionality on compact manifolds."|mani contemporari statist learn method assum euclidean featur space howev curs dimension associ high featur dimens particular sever euclidean distanc paper present method defin similar base hyperspher geometri show often improv perform support vector machin compar compet similar measur specif idea use heat diffus hyperspher measur similar propos test citet lafferti uy demonstr promis result base approxim heat kernel howev exact hyperspher heat kernel hitherto remain unknown paper deriv exact form heat kernel unit hyperspher term uniform absolut converg seri high dimension angular momentum eigenmod natur measur similar sampl point dwell hyperspher exact kernel often show superior perform kernel svm classif appli text mine tumor somat mutat imput stock market analysi improv classif accuraci compar kernel base euclidean geometri may aris amelior curs dimension compact manifold|['Chenchao Zhao', 'Jun S. Song']|['stat.ML', 'q-bio.QM', 'stat.CO']
2017-03-28T14:03:42Z|2017-02-04T18:58:02Z|http://arxiv.org/abs/1702.01326v1|http://arxiv.org/pdf/1702.01326v1|An Algorithm for Computing the Distribution Function of the Generalized   Poisson-Binomial Distribution|algorithm comput distribut function general poisson binomi distribut|The Poisson-binomial distribution is useful in many applied problems in engineering, actuarial science, and data mining. The Poisson-binomial distribution models the distribution of the sum of independent but not identically distributed Bernoulli random variables whose success probabilities vary. In this paper, we extend the Poisson-binomial distribution to the generalized Poisson-binomial (GPB) distribution. The GPB distribution is defined in cases where the Bernoulli variables can take any two arbitrary values instead of 0 and~1. The GPB distribution is useful in many areas such as voting theory, actuarial science, warranty prediction, and probability theory. With few previous works studying the GPB distribution, we derive the probability distribution via the discrete Fourier transform of the characteristic function of the distribution. We develop an efficient algorithm for computing the distribution function, which uses the fast Fourier transform. We test the accuracy of the developed algorithm upon comparing it with enumeration-based exact method and the results from the binomial distribution. We also study the computational time of the algorithm in various parameter settings. Finally, we discus the factors affecting the computational efficiency of this algorithm, and illustrate the use of the software package.|poisson binomi distribut use mani appli problem engin actuari scienc data mine poisson binomi distribut model distribut sum independ ident distribut bernoulli random variabl whose success probabl vari paper extend poisson binomi distribut general poisson binomi gpb distribut gpb distribut defin case bernoulli variabl take ani two arbitrari valu instead gpb distribut use mani area vote theori actuari scienc warranti predict probabl theori previous work studi gpb distribut deriv probabl distribut via discret fourier transform characterist function distribut develop effici algorithm comput distribut function use fast fourier transform test accuraci develop algorithm upon compar enumer base exact method result binomi distribut also studi comput time algorithm various paramet set final discus factor affect comput effici algorithm illustr use softwar packag|['Man Zhang', 'Yili Hong', 'Narayanaswamy Balakrishnan']|['stat.CO']
2017-03-28T14:03:42Z|2017-02-03T22:07:37Z|http://arxiv.org/abs/1702.01185v1|http://arxiv.org/pdf/1702.01185v1|Basis Adaptive Sample Efficient Polynomial Chaos (BASE-PC)|basi adapt sampl effici polynomi chao base pc|For a large class of orthogonal basis functions, there has been a recent identification of expansion methods for computing accurate, stable approximations of a quantity of interest. This paper presents, within the context of uncertainty quantification, a practical implementation using basis adaptation, and coherence motivated sampling, which under assumptions has satisfying guarantees. This implementation is referred to as Basis Adaptive Sample Efficient Polynomial Chaos (BASE-PC). A key component of this is the use of anisotropic polynomial order which admits evolving global bases for approximation in an efficient manner, leading to consistently stable approximation for a practical class of smooth functionals. This fully adaptive, non-intrusive method, requires no a priori information of the solution, and has satisfying theoretical guarantees of recovery. A key contribution to stability is the use of a presented correction sampling for coherence-optimal sampling in order to improve stability and accuracy within the adaptive basis scheme. Theoretically, the method may dramatically reduce the impact of dimensionality in function approximation, and numerically the method is demonstrated to perform well on problems with dimension up to 1000.|larg class orthogon basi function recent identif expans method comput accur stabl approxim quantiti interest paper present within context uncertainti quantif practic implement use basi adapt coher motiv sampl assumpt satisfi guarante implement refer basi adapt sampl effici polynomi chao base pc key compon use anisotrop polynomi order admit evolv global base approxim effici manner lead consist stabl approxim practic class smooth function fulli adapt non intrus method requir priori inform solut satisfi theoret guarante recoveri key contribut stabil use present correct sampl coher optim sampl order improv stabil accuraci within adapt basi scheme theoret method may dramat reduc impact dimension function approxim numer method demonstr perform well problem dimens|['Jerrad Hampton', 'Alireza Doostan']|['stat.CO', 'math.PR', 'math.ST', 'stat.TH']
2017-03-28T14:03:42Z|2017-02-03T21:23:46Z|http://arxiv.org/abs/1702.01166v1|http://arxiv.org/pdf/1702.01166v1|Optimal Subsampling for Large Sample Logistic Regression|optim subsampl larg sampl logist regress|For massive data, the family of subsampling algorithms is popular to downsize the data volume and reduce computational burden. Existing studies focus on approximating the ordinary least squares estimate in linear regression, where statistical leverage scores are often used to define subsampling probabilities. In this paper, we propose fast subsampling algorithms to efficiently approximate the maximum likelihood estimate in logistic regression. We first establish consistency and asymptotic normality of the estimator from a general subsampling algorithm, and then derive optimal subsampling probabilities that minimize the asymptotic mean squared error of the resultant estimator. An alternative minimization criterion is also proposed to further reduce the computational cost. The optimal subsampling probabilities depend on the full data estimate, so we develop a two-step algorithm to approximate the optimal subsampling procedure. This algorithm is computationally efficient and has a significant reduction in computing time compared to the full data approach. Consistency and asymptotic normality of the estimator from a two-step algorithm are also established. Synthetic and real data sets are used to evaluate the practical performance of the proposed method.|massiv data famili subsampl algorithm popular downsiz data volum reduc comput burden exist studi focus approxim ordinari least squar estim linear regress statist leverag score often use defin subsampl probabl paper propos fast subsampl algorithm effici approxim maximum likelihood estim logist regress first establish consist asymptot normal estim general subsampl algorithm deriv optim subsampl probabl minim asymptot mean squar error result estim altern minim criterion also propos reduc comput cost optim subsampl probabl depend full data estim develop two step algorithm approxim optim subsampl procedur algorithm comput effici signific reduct comput time compar full data approach consist asymptot normal estim two step algorithm also establish synthet real data set use evalu practic perform propos method|['HaiYing Wang', 'Rong Zhu', 'Ping Ma']|['stat.CO', 'stat.ME', 'stat.ML']
2017-03-28T14:03:42Z|2017-02-02T20:08:42Z|http://arxiv.org/abs/1702.00817v1|http://arxiv.org/abs/1702.00817v1|DCT-like Transform for Image Compression Requires 14 Additions Only|dct like transform imag compress requir addit onli|A low-complexity 8-point orthogonal approximate DCT is introduced. The proposed transform requires no multiplications or bit-shift operations. The derived fast algorithm requires only 14 additions, less than any existing DCT approximation. Moreover, in several image compression scenarios, the proposed transform could outperform the well-known signed DCT, as well as state-of-the-art algorithms.|low complex point orthogon approxim dct introduc propos transform requir multipl bit shift oper deriv fast algorithm requir onli addit less ani exist dct approxim moreov sever imag compress scenario propos transform could outperform well known sign dct well state art algorithm|['F. M. Bayer', 'R. J. Cintra']|['cs.MM', 'cs.DS', 'stat.AP', 'stat.CO']
2017-03-28T14:03:46Z|2017-02-01T19:44:14Z|http://arxiv.org/abs/1702.00434v1|http://arxiv.org/pdf/1702.00434v1|Applying Nearest Neighbor Gaussian Processes to Massive Spatial Data   Sets: Forest Canopy Height Prediction Across Tanana Valley Alaska|appli nearest neighbor gaussian process massiv spatial data set forest canopi height predict across tanana valley alaska|Light detection and ranging (LiDAR) data provide critical information on the three-dimensional structure of forests. However, collecting wall-to-wall LiDAR data at regional and global scales is cost prohibitive. As a result, studies employing LiDAR data from airborne platforms typically collect data via strip sampling; leaving large swaths of the forest domain unmeasured by the instrument. Frameworks to accommodate incomplete coverage information from LiDAR instruments are essential to advance our understanding of forest structure and begin effectively monitoring forest resource dynamics over time. Here, we define and assess several spatial regression models capable of delivering complete coverage forest canopy height prediction maps with associated uncertainty estimates using sparsely sampled LiDAR data. Despite the sparsity of the LiDAR data considered, the number of observations is large, e.g., n=5x10^6. Computational hurdles associated with developing the desired data products is overcome by using highly scalable hierarchical Nearest Neighbor Gaussian Process (NNGP) models. We outline new Markov chain Monte Carlo (MCMC) algorithms that provide improved convergence and run time over existing algorithms. We also propose a MCMC free hybrid implementation of NNGP. We assess the computational and inferential benefits of these alternate NNGP specifications using simulated data sets and LiDAR data collected over the US Forest Service Tanana Inventory Unit (TIU) in a remote portion of Interior Alaska. The resulting data product is the first statistically robust map of forest canopy for the TIU.|light detect rang lidar data provid critic inform three dimension structur forest howev collect wall wall lidar data region global scale cost prohibit result studi employ lidar data airborn platform typic collect data via strip sampl leav larg swath forest domain unmeasur instrument framework accommod incomplet coverag inform lidar instrument essenti advanc understand forest structur begin effect monitor forest resourc dynam time defin assess sever spatial regress model capabl deliv complet coverag forest canopi height predict map associ uncertainti estim use spars sampl lidar data despit sparsiti lidar data consid number observ larg comput hurdl associ develop desir data product overcom use high scalabl hierarch nearest neighbor gaussian process nngp model outlin new markov chain mont carlo mcmc algorithm provid improv converg run time exist algorithm also propos mcmc free hybrid implement nngp assess comput inferenti benefit altern nngp specif use simul data set lidar data collect us forest servic tanana inventori unit tiu remot portion interior alaska result data product first statist robust map forest canopi tiu|['Andrew O. Finley', 'Abhirup Datta', 'Bruce C. Cook', 'Douglas C. Morton', 'Hans E. Andersen', 'Sudipto Banerjee']|['stat.CO', 'stat.AP']
2017-03-28T14:03:46Z|2017-02-27T17:17:15Z|http://arxiv.org/abs/1702.00428v2|http://arxiv.org/pdf/1702.00428v2|Malliavin-based Multilevel Monte Carlo Estimators for Densities of   Max-stable Processes|malliavin base multilevel mont carlo estim densiti max stabl process|We introduce a class of unbiased Monte Carlo estimators for the multivariate density of max-stable fields generated by Gaussian processes. Our estimators take advantage of recent results on exact simulation of max-stable fields combined with identities studied in the Malliavin calculus literature and ideas developed in the multilevel Monte Carlo literature. Our approach allows estimating multivariate densities of max-stable fields with precision $\varepsilon $ at a computational cost of order $O\left( \varepsilon ^{-2}\log \log \log \left( 1/\varepsilon \right) \right) $.|introduc class unbias mont carlo estim multivari densiti max stabl field generat gaussian process estim take advantag recent result exact simul max stabl field combin ident studi malliavin calculus literatur idea develop multilevel mont carlo literatur approach allow estim multivari densiti max stabl field precis varepsilon comput cost order left varepsilon log log log left varepsilon right right|['Jose Blanchet', 'Zhipeng Liu']|['stat.CO', 'math.PR']
2017-03-28T14:03:46Z|2017-02-07T22:13:25Z|http://arxiv.org/abs/1702.00317v2|http://arxiv.org/pdf/1702.00317v2|On SGD's Failure in Practice: Characterizing and Overcoming Stalling|sgd failur practic character overcom stall|Stochastic Gradient Descent (SGD) is widely used in machine learning problems to efficiently perform empirical risk minimization, yet, in practice, SGD is known to stall before reaching the actual minimizer of the empirical risk. SGD stalling has often been attributed to its sensitivity to the conditioning of the problem; however, as we demonstrate, SGD will stall even when applied to a simple linear regression problem with unity condition number for standard learning rates. Thus, in this work, we numerically demonstrate and mathematically argue that stalling is a crippling and generic limitation of SGD and its variants in practice. Once we have established the problem of stalling, we generalize an existing framework for hedging against its effects, which (1) deters SGD and its variants from stalling, (2) still provides convergence guarantees, and (3) makes SGD and its variants more practical methods for minimization.|stochast gradient descent sgd wide use machin learn problem effici perform empir risk minim yet practic sgd known stall befor reach actual minim empir risk sgd stall often attribut sensit condit problem howev demonstr sgd stall even appli simpl linear regress problem uniti condit number standard learn rate thus work numer demonstr mathemat argu stall crippl generic limit sgd variant practic onc establish problem stall general exist framework hedg effect deter sgd variant stall still provid converg guarante make sgd variant practic method minim|['Vivak Patel']|['stat.ML', 'cs.LG', 'math.OC', 'stat.CO', '62L20, 62L12, 90C99', 'G.1.6; G.3; I.2.6']
2017-03-28T14:03:46Z|2017-02-01T10:55:12Z|http://arxiv.org/abs/1702.00204v1|http://arxiv.org/pdf/1702.00204v1|Bayesian model selection for the latent position cluster model for   Social Networks|bayesian model select latent posit cluster model social network|The latent position cluster model is a popular model for the statistical analysis of network data. This model assumes that there is an underlying latent space in which the actors follow a finite mixture distribution. Moreover, actors which are close in this latent space are more likely to be tied by an edge. This is an appealing approach since it allows the model to cluster actors which consequently provides the practitioner with useful qualitative information. However, exploring the uncertainty in the number of underlying latent components in the mixture distribution is a complex task. The current state-of-the-art is to use an approximate form of BIC for this purpose, where an approximation of the log-likelihood is used instead of the true log-likelihood which is unavailable. The main contribution of this paper is to show that through the use of conjugate prior distributions it is possible to analytically integrate out almost all of the model parameters, leaving a posterior distribution which depends on the allocation vector of the mixture model. This enables posterior inference over the number of components in the latent mixture distribution without using trans- dimensional MCMC algorithms such as reversible jump MCMC. Our approach is compared with the state-of-the-art latentnet (Krivitsky & Handcock 2015) and VBLPCM (Salter-Townshend & Murphy 2013) packages.|latent posit cluster model popular model statist analysi network data model assum latent space actor follow finit mixtur distribut moreov actor close latent space like tie edg appeal approach sinc allow model cluster actor consequ provid practition use qualit inform howev explor uncertainti number latent compon mixtur distribut complex task current state art use approxim form bic purpos approxim log likelihood use instead true log likelihood unavail main contribut paper show use conjug prior distribut possibl analyt integr almost model paramet leav posterior distribut depend alloc vector mixtur model enabl posterior infer number compon latent mixtur distribut without use tran dimension mcmc algorithm revers jump mcmc approach compar state art latentnet krivitski handcock vblpcm salter townshend murphi packag|['Caitriona Ryan', 'Jason Wyse', 'Nial Friel']|['stat.CO']
2017-03-28T14:03:46Z|2017-01-28T16:31:25Z|http://arxiv.org/abs/1701.08299v1|http://arxiv.org/pdf/1701.08299v1|Computing the aggregate loss distribution based on numerical inversion   of the compound empirical characteristic function of frequency and severity|comput aggreg loss distribut base numer invers compound empir characterist function frequenc sever|A non-parametric method for evaluation of the aggregate loss distribution (ALD) by combining and numerically inverting the empirical characteristic functions (CFs) is presented and illustrated. This approach to evaluate ALD is based on purely non-parametric considerations, i.e., based on the empirical CFs of frequency and severity of the claims in the actuarial risk applications. This approach can be, however, naturally generalized to a more complex semi-parametric modeling approach, e.g., by incorporating the generalized Pareto distribution fit of the severity distribution heavy tails, and/or by considering the weighted mixture of the parametric CFs (used to model the expert knowledge) and the empirical CFs (used to incorporate the knowledge based on the historical data - internal and/or external). Here we present a simple and yet efficient method and algorithms for numerical inversion of the CF, suitable for evaluation of the ALDs and the associated measures of interest important for applications, as, e.g., the value at risk (VaR). The presented approach is based on combination of the Gil-Pelaez inversion formulae for deriving the probability distribution (PDF and CDF) from the compound (empirical) CF and the trapezoidal rule used for numerical integration. The applicability of the suggested approach is illustrated by analysis of a well know insurance dataset, the Danish fire loss data.|non parametr method evalu aggreg loss distribut ald combin numer invert empir characterist function cfs present illustr approach evalu ald base pure non parametr consider base empir cfs frequenc sever claim actuari risk applic approach howev natur general complex semi parametr model approach incorpor general pareto distribut fit sever distribut heavi tail consid weight mixtur parametr cfs use model expert knowledg empir cfs use incorpor knowledg base histor data intern extern present simpl yet effici method algorithm numer invers cf suitabl evalu ald associ measur interest import applic valu risk var present approach base combin gil pelaez invers formula deriv probabl distribut pdf cdf compound empir cf trapezoid rule use numer integr applic suggest approach illustr analysi well know insur dataset danish fire loss data|['Viktor Witkovsky', 'Gejza Wimmer', 'Tomas Duby']|['stat.CO', 'q-fin.RM', 'stat.AP', '91B30, 62G32']
2017-03-28T14:03:46Z|2017-03-03T15:54:59Z|http://arxiv.org/abs/1701.08142v3|http://arxiv.org/pdf/1701.08142v3|Modelling Preference Data with the Wallenius Distribution|model prefer data wallenius distribut|The Wallenius distribution is a generalisation of the Hypergeometric distribution where weights are assigned to balls of different colours. This naturally defines a model for ranking categories which can be used for classification purposes. Since, in general, the resulting likelihood is not analytically available, we adopt an approximate Bayesian computational (ABC) approach for estimating the importance of the categories. We illustrate the performance of the estimation procedure on simulated datasets. Finally, we use the new model for analysing two datasets about movies ratings and Italian academic statisticians' journal preferences. The latter is a novel dataset collected by the authors.|wallenius distribut generalis hypergeometr distribut weight assign ball differ colour natur defin model rank categori use classif purpos sinc general result likelihood analyt avail adopt approxim bayesian comput abc approach estim import categori illustr perform estim procedur simul dataset final use new model analys two dataset movi rate italian academ statistician journal prefer latter novel dataset collect author|['Clara Grazian', 'Fabrizio Leisen', 'Brunero Liseo']|['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']
2017-03-28T14:03:46Z|2017-01-26T19:07:53Z|http://arxiv.org/abs/1701.07844v1|http://arxiv.org/pdf/1701.07844v1|Markov Chain Monte Carlo with the Integrated Nested Laplace   Approximation|markov chain mont carlo integr nest laplac approxim|The Integrated Nested Laplace Approximation (INLA) has established itself as a widely used method for approximate inference on Bayesian hierarchical models which can be represented as a latent Gaussian model (LGM). INLA is based on producing an accurate approximation to the posterior marginal distributions of the parameters in the model and some other quantities of interest by using repeated approximations to intermediate distributions and integrals that appear in the computation of the posterior marginals.   INLA focuses on models whose latent effects are a Gaussian Markov random field (GMRF). For this reason, we have explored alternative ways of expanding the number of possible models that can be fitted using the INLA methodology. In this paper, we present a novel approach that combines INLA and Markov chain Monte Carlo (MCMC). The aim is to consider a wider range of models that cannot be fitted with INLA unless some of the parameters of the model have been fixed. Hence, conditioning on these parameters the model could be fitted with the R-INLA package. We show how new values of these parameters can be drawn from their posterior by using conditional models fitted with INLA and standard MCMC algorithms, such as Metropolis-Hastings. Hence, this will extend the use of INLA to fit models that can be expressed as a conditional LGM. Also, this new approach can be used to build simpler MCMC samplers for complex models as it allows sampling only on a limited number parameters in the model.   We will demonstrate how our approach can extend the class of models that could benefit from INLA, and how the R-INLA package will ease its implementation. We will go through simple examples of this new approach before we discuss more advanced problems with datasets taken from relevant literature.|integr nest laplac approxim inla establish wide use method approxim infer bayesian hierarch model repres latent gaussian model lgm inla base produc accur approxim posterior margin distribut paramet model quantiti interest use repeat approxim intermedi distribut integr appear comput posterior margin inla focus model whose latent effect gaussian markov random field gmrf reason explor altern way expand number possibl model fit use inla methodolog paper present novel approach combin inla markov chain mont carlo mcmc aim consid wider rang model cannot fit inla unless paramet model fix henc condit paramet model could fit inla packag show new valu paramet drawn posterior use condit model fit inla standard mcmc algorithm metropoli hast henc extend use inla fit model express condit lgm also new approach use build simpler mcmc sampler complex model allow sampl onli limit number paramet model demonstr approach extend class model could benefit inla inla packag eas implement go simpl exampl new approach befor discuss advanc problem dataset taken relev literatur|['Virgilio Gómez-Rubio', 'Håvard Rue']|['stat.CO']
2017-03-28T14:03:46Z|2017-02-09T12:18:42Z|http://arxiv.org/abs/1701.07787v3|http://arxiv.org/pdf/1701.07787v3|Multi-locus data distinguishes between population growth and multiple   merger coalescents|multi locus data distinguish popul growth multipl merger coalesc|We introduce a low dimensional function of the site frequency spectrum that is tailor-made for distinguishing coalescent models with multiple mergers from Kingman coalescent models with population growth, and use this function to construct a hypothesis test between these two model classes. The null and alternative sampling distributions of our statistic are intractable, but its low dimensionality renders these distributions amenable to Monte Carlo estimation. We construct kernel density estimates of the sampling distributions based on simulated data, and show that the resulting hypothesis test dramatically improves on the statistical power of a current state-of-the-art method. A key reason for this improvement is the use of multi-locus data, in particular averaging observed site frequency spectra across unlinked loci to reduce sampling variance. We also demonstrate the robustness of our method to nuisance and tuning parameters. Finally we demonstrate that the same kernel density estimates can be used to conduct parameter estimation, and argue that our method is readily generalisable for applications in model selection, parameter inference and experimental design.|introduc low dimension function site frequenc spectrum tailor made distinguish coalesc model multipl merger kingman coalesc model popul growth use function construct hypothesi test two model class null altern sampl distribut statist intract low dimension render distribut amen mont carlo estim construct kernel densiti estim sampl distribut base simul data show result hypothesi test dramat improv statist power current state art method key reason improv use multi locus data particular averag observ site frequenc spectra across unlink loci reduc sampl varianc also demonstr robust method nuisanc tune paramet final demonstr kernel densiti estim use conduct paramet estim argu method readili generalis applic model select paramet infer experiment design|['Jere Koskela']|['q-bio.PE', 'q-bio.QM', 'stat.CO', 'stat.ME', '92D15 (Primary), 62M02, 62M05 (Secondary)']
2017-03-28T14:03:46Z|2017-01-25T21:43:03Z|http://arxiv.org/abs/1701.07496v1|http://arxiv.org/pdf/1701.07496v1|Phylogenetic Factor Analysis|phylogenet factor analysi|Phylogenetic comparative methods explore the relationships between quantitative traits adjusting for shared evolutionary history. This adjustment often occurs through a Brownian diffusion process along the branches of the phylogeny that generates model residuals or the traits themselves. For high-dimensional traits, inferring all pair-wise correlations within the multivariate diffusion is limiting. To circumvent this problem, we propose phylogenetic factor analysis (PFA) that assumes a small unknown number of independent evolutionary factors arise along the phylogeny and these factors generate clusters of dependent traits. Set in a Bayesian framework, PFA provides measures of uncertainty on the factor number and groupings, combines both continuous and discrete traits, integrates over missing measurements and incorporates phylogenetic uncertainty with the help of molecular sequences. We develop Gibbs samplers based on dynamic programming to estimate the PFA posterior distribution, over three-fold faster than for multivariate diffusion and a further order-of-magnitude more efficiently in the presence of latent traits. We further propose a novel marginal likelihood estimator for previously impractical models with discrete data and find that PFA also provides a better fit than multivariate diffusion in evolutionary questions in columbine flower development, placental reproduction transitions and triggerfish fin morphometry.|phylogenet compar method explor relationship quantit trait adjust share evolutionari histori adjust often occur brownian diffus process along branch phylogeni generat model residu trait themselv high dimension trait infer pair wise correl within multivari diffus limit circumv problem propos phylogenet factor analysi pfa assum small unknown number independ evolutionari factor aris along phylogeni factor generat cluster depend trait set bayesian framework pfa provid measur uncertainti factor number group combin continu discret trait integr miss measur incorpor phylogenet uncertainti help molecular sequenc develop gibb sampler base dynam program estim pfa posterior distribut three fold faster multivari diffus order magnitud effici presenc latent trait propos novel margin likelihood estim previous impract model discret data find pfa also provid better fit multivari diffus evolutionari question columbin flower develop placent reproduct transit triggerfish fin morphometri|['Max R. Tolkoff', 'Michael L. Alfaro', 'Guy Baele', 'Philippe Lemey', 'Marc A. Suchard']|['stat.ME', 'stat.AP', 'stat.CO']
2017-03-28T14:03:46Z|2017-01-23T20:26:42Z|http://arxiv.org/abs/1701.06619v1|http://arxiv.org/pdf/1701.06619v1|Bayesian Inference in the Presence of Intractable Normalizing Functions|bayesian infer presenc intract normal function|Models with intractable normalizing functions arise frequently in statistics. Common examples of such models include exponential random graph models for social networks and Markov point processes for ecology and disease modeling. Inference for these models is complicated because the normalizing functions of their probability distributions include the parameters of interest. In Bayesian analysis they result in so-called doubly intractable posterior distributions which pose significant computational challenges. Several Monte Carlo methods have emerged in recent years to address Bayesian inference for such models. We provide a framework for understanding the algorithms and elucidate connections among them. Through multiple simulated and real data examples, we compare and contrast the computational and statistical efficiency of these algorithms and discuss their theoretical bases. Our study provides practical recommendations for practitioners along with directions for future research for MCMC methodologists.|model intract normal function aris frequent statist common exampl model includ exponenti random graph model social network markov point process ecolog diseas model infer model complic becaus normal function probabl distribut includ paramet interest bayesian analysi result call doubli intract posterior distribut pose signific comput challeng sever mont carlo method emerg recent year address bayesian infer model provid framework understand algorithm elucid connect among multipl simul real data exampl compar contrast comput statist effici algorithm discuss theoret base studi provid practic recommend practition along direct futur research mcmc methodologist|['Jaewoo Park', 'Murali Haran']|['stat.CO', 'stat.AP']
2017-04-07T11:24:27Z|2017-04-05T14:23:53Z|http://arxiv.org/abs/1704.01445v1|http://arxiv.org/pdf/1704.01445v1|Bayesian Inference of Log Determinants|bayesian infer log determin|The log-determinant of a kernel matrix appears in a variety of machine learning problems, ranging from determinantal point processes and generalized Markov random fields, through to the training of Gaussian processes. Exact calculation of this term is often intractable when the size of the kernel matrix exceeds a few thousand. In the spirit of probabilistic numerics, we reinterpret the problem of computing the log-determinant as a Bayesian inference problem. In particular, we combine prior knowledge in the form of bounds from matrix theory and evidence derived from stochastic trace estimation to obtain probabilistic estimates for the log-determinant and its associated uncertainty within a given computational budget. Beyond its novelty and theoretic appeal, the performance of our proposal is competitive with state-of-the-art approaches to approximating the log-determinant, while also quantifying the uncertainty due to budget-constrained evidence.|log determin kernel matrix appear varieti machin learn problem rang determinant point process general markov random field train gaussian process exact calcul term often intract size kernel matrix exceed thousand spirit probabilist numer reinterpret problem comput log determin bayesian infer problem particular combin prior knowledg form bound matrix theori evid deriv stochast trace estim obtain probabilist estim log determin associ uncertainti within given comput budget beyond novelti theoret appeal perform propos competit state art approach approxim log determin also quantifi uncertainti due budget constrain evid|['Jack Fitzsimons', 'Kurt Cutajar', 'Michael Osborne', 'Stephen Roberts', 'Maurizio Filippone']|['stat.ML', 'cs.NA', 'stat.CO']
2017-04-07T11:24:27Z|2017-04-04T20:07:26Z|http://arxiv.org/abs/1704.01168v1|http://arxiv.org/pdf/1704.01168v1|Learning Approximately Objective Priors|learn approxim object prior|In modern probabilistic learning we often wish to perform automatic inference for Bayesian models. However, informative prior distributions can be costly and difficult to elicit, and, as a consequence, flat priors are often chosen with the hope that they are reasonably uninformative. Objective priors such as the Jeffreys and reference priors are generally preferable over flat priors but are not tractable to derive for many models of interest. We address this issue by proposing techniques for learning reference prior approximations: we select a parametric family and optimize a lower bound on the reference prior objective to find the member of the family that serves as a good approximation. Moreover, optimization can be made derivation-free via differentiable Monte Carlo expectations. We experimentally demonstrate the method's effectiveness by recovering Jeffreys priors and learning the Variational Autoencoder's reference prior.|modern probabilist learn often wish perform automat infer bayesian model howev inform prior distribut cost difficult elicit consequ flat prior often chosen hope reason uninform object prior jeffrey refer prior general prefer flat prior tractabl deriv mani model interest address issu propos techniqu learn refer prior approxim select parametr famili optim lower bound refer prior object find member famili serv good approxim moreov optim made deriv free via differenti mont carlo expect experiment demonstr method effect recov jeffrey prior learn variat autoencod refer prior|['Eric Nalisnick', 'Padhraic Smyth']|['stat.ML', 'stat.CO']
2017-04-07T11:24:27Z|2017-04-04T17:48:39Z|http://arxiv.org/abs/1704.01113v1|http://arxiv.org/pdf/1704.01113v1|Damped Posterior Linearization Filter|damp posterior linear filter|The iterated posterior linearization filter (IPLF) is an algorithm for Bayesian state estimation that performs the measurement update using iterative statistical regression. The main result behind IPLF is that the posterior approximation is more accurate when the statistical regression of measurement function is done in the posterior instead of the prior as is done in non-iterative Kalman filter extensions. In IPLF, each iteration in principle gives a better posterior estimate to obtain a better statistical regression and more accurate posterior estimate in the next iteration. However, IPLF may diverge. IPLF's fixed- points are not described as solutions to an optimization problem, which makes it challenging to improve its convergence properties. In this letter, we introduce a double-loop version of IPLF, where the inner loop computes the posterior mean using an optimization algorithm. Simulation results are presented to show that the proposed algorithm has better convergence than IPLF and its accuracy is similar to or better than other state-of-the-art algorithms.|iter posterior linear filter iplf algorithm bayesian state estim perform measur updat use iter statist regress main result behind iplf posterior approxim accur statist regress measur function done posterior instead prior done non iter kalman filter extens iplf iter principl give better posterior estim obtain better statist regress accur posterior estim next iter howev iplf may diverg iplf fix point describ solut optim problem make challeng improv converg properti letter introduc doubl loop version iplf inner loop comput posterior mean use optim algorithm simul result present show propos algorithm better converg iplf accuraci similar better state art algorithm|['Matti Raitoharju', 'Lennart Svensson', 'Ángel F. García-Fernández', 'Robert Piché']|['math.OC', 'stat.CO']
2017-04-07T11:24:27Z|2017-04-04T11:40:20Z|http://arxiv.org/abs/1704.00963v1|http://arxiv.org/pdf/1704.00963v1|Bayesian optimization with virtual derivative sign observations|bayesian optim virtual deriv sign observ|Bayesian optimization (BO) is a global optimization strategy designed to find the minimum of expensive black-box functions $g$ typically defined on a continuous sets of $\mathcal{R}^d$. Using a Gaussian process (GP) as a surrogate model for the objective and an acquisition function to systematically search its domain, BO strategies aim to minimize the amount of samples required to find the minimum of $g$. Although currently available acquisition functions address this goal with different degree of success, an over-exploration effect of the contour of $g$ is typically observed. This is due to the myopic nature of most acquisitions that greedily try to over-reduce uncertainty in the border of the search domain. In most real problems, however, like the configuration of machine learning algorithms, the function domain is conservatively large and with a high probability the global minimum is not at the boundary. We propose a method to incorporate this knowledge into the searching process by adding virtual derivative observations at the borders of the search space. We use the properties of GP models that allow us to easily impose conditions on the partial derivatives of the objective. The method is applicable with any acquisition function, it is easy to use and consistently reduces the number of evaluations required to find the minimum of $g$ irrespective of the acquisition used. We illustrate the benefits our approach in a simulation study with a battery of objective functions.|bayesian optim bo global optim strategi design find minimum expens black box function typic defin continu set mathcal use gaussian process gp surrog model object acquisit function systemat search domain bo strategi aim minim amount sampl requir find minimum although current avail acquisit function address goal differ degre success explor effect contour typic observ due myopic natur acquisit greedili tri reduc uncertainti border search domain real problem howev like configur machin learn algorithm function domain conserv larg high probabl global minimum boundari propos method incorpor knowledg search process ad virtual deriv observ border search space use properti gp model allow us easili impos condit partial deriv object method applic ani acquisit function easi use consist reduc number evalu requir find minimum irrespect acquisit use illustr benefit approach simul studi batteri object function|['Eero Siivola', 'Aki Vehtari', 'Jarno Vanhatalo', 'Javier González']|['stat.ML', 'stat.CO', 'stat.ME']
2017-04-07T11:24:27Z|2017-04-03T16:50:14Z|http://arxiv.org/abs/1704.00680v1|http://arxiv.org/pdf/1704.00680v1|A Consistent Bayesian Formulation for Stochastic Inverse Problems Based   on Push-forward Measures|consist bayesian formul stochast invers problem base push forward measur|We formulate, and present a numerical method for solving, an inverse problem for inferring parameters of a deterministic model from stochastic observational data (quantities of interest). The solution, given as a probability measure, is derived using a Bayesian updating approach for measurable maps that finds a posterior probability measure, that when propagated through the deterministic model produces a push-forward measure that exactly matches the observed probability measure on the data. Our approach for finding such posterior measures, which we call consistent Bayesian inference, is simple and only requires the computation of the push-forward probability measure induced by the combination of a prior probability measure and the deterministic model. We establish existence and uniqueness of observation-consistent posteriors and present stability and error analysis. We also discuss the relationships between consistent Bayesian inference, classical/statistical Bayesian inference, and a recently developed measure-theoretic approach for inference. Finally, analytical and numerical results are presented to highlight certain properties of the consistent Bayesian approach and the differences between this approach and the two aforementioned alternatives for inference.|formul present numer method solv invers problem infer paramet determinist model stochast observ data quantiti interest solut given probabl measur deriv use bayesian updat approach measur map find posterior probabl measur propag determinist model produc push forward measur exact match observ probabl measur data approach find posterior measur call consist bayesian infer simpl onli requir comput push forward probabl measur induc combin prior probabl measur determinist model establish exist uniqu observ consist posterior present stabil error analysi also discuss relationship consist bayesian infer classic statist bayesian infer recent develop measur theoret approach infer final analyt numer result present highlight certain properti consist bayesian approach differ approach two aforement altern infer|['T. Butler', 'J. D. Jakeman', 'T. Wildey']|['math.NA', 'stat.CO', '60H30, 60H35, 60B10']
2017-04-07T11:24:27Z|2017-04-03T13:49:25Z|http://arxiv.org/abs/1704.00587v1|http://arxiv.org/pdf/1704.00587v1|Analysis, detection and correction of misspecified discrete time state   space models|analysi detect correct misspecifi discret time state space model|Misspecifications (i.e. errors on the parameters) of state space models lead to incorrect inference of the hidden states. This paper studies weakly nonlin-ear state space models with additive Gaussian noises and proposes a method for detecting and correcting misspecifications. The latter induce a biased estimator of the hidden state but also happen to induce correlation on innovations and other residues. This property is used to find a well-defined objective function for which an optimisation routine is applied to recover the true parameters of the model. It is argued that this method can consistently estimate the bias on the parameter. We demonstrate the algorithm on various models of increasing complexity.|misspecif error paramet state space model lead incorrect infer hidden state paper studi weak nonlin ear state space model addit gaussian nois propos method detect correct misspecif latter induc bias estim hidden state also happen induc correl innov residu properti use find well defin object function optimis routin appli recov true paramet model argu method consist estim bias paramet demonstr algorithm various model increas complex|['Salima El Kolei', 'Frédéric Patras']|['stat.AP', 'stat.CO']
2017-04-07T11:24:27Z|2017-04-03T12:09:19Z|http://arxiv.org/abs/1704.00543v1|http://arxiv.org/pdf/1704.00543v1|Mixture Hidden Markov Models for Sequence Data: The seqHMM Package in R|mixtur hidden markov model sequenc data seqhmm packag|Sequence analysis is being more and more widely used for the analysis of social sequences and other multivariate categorical time series data. However, it is often complex to describe, visualize, and compare large sequence data, especially when there are multiple parallel sequences per subject. Hidden (latent) Markov models (HMMs) are able to detect underlying latent structures and they can be used in various longitudinal settings: to account for measurement error, to detect unobservable states, or to compress information across several types of observations. Extending to mixture hidden Markov models (MHMMs) allows clustering data into homogeneous subsets, with or without external covariates. The seqHMM package in R is designed for the efficient modeling of sequences and other categorical time series data containing one or multiple subjects with one or multiple interdependent sequences using HMMs and MHMMs. Also other restricted variants of the MHMM can be fitted, e.g., latent class models, Markov models, mixture Markov models, or even ordinary multinomial regression models with suitable parameterization of the HMM. Good graphical presentations of data and models are useful during the whole analysis process from the first glimpse at the data to model fitting and presentation of results. The package provides easy options for plotting parallel sequence data, and proposes visualizing HMMs as directed graphs.|sequenc analysi wide use analysi social sequenc multivari categor time seri data howev often complex describ visual compar larg sequenc data especi multipl parallel sequenc per subject hidden latent markov model hmms abl detect latent structur use various longitudin set account measur error detect unobserv state compress inform across sever type observ extend mixtur hidden markov model mhmms allow cluster data homogen subset without extern covari seqhmm packag design effici model sequenc categor time seri data contain one multipl subject one multipl interdepend sequenc use hmms mhmms also restrict variant mhmm fit latent class model markov model mixtur markov model even ordinari multinomi regress model suitabl parameter hmm good graphic present data model use dure whole analysi process first glimps data model fit present result packag provid easi option plot parallel sequenc data propos visual hmms direct graph|['Satu Helske', 'Jouni Helske']|['stat.CO', 'stat.AP']
2017-04-07T11:24:27Z|2017-04-03T10:40:15Z|http://arxiv.org/abs/1704.00520v1|http://arxiv.org/pdf/1704.00520v1|Efficient acquisition rules for model-based approximate Bayesian   computation|effici acquisit rule model base approxim bayesian comput|Approximate Bayesian computation (ABC) is a method for Bayesian inference when the likelihood is unavailable but simulating from the model is possible. However, many ABC algorithms require a large number of simulations, which can be costly. To reduce the computational cost, surrogate models and Bayesian optimisation (BO) have been proposed. Bayesian optimisation enables one to intelligently decide where to evaluate the model next, but standard BO strategies are designed for optimisation and not specifically for ABC inference. Our paper addresses this gap in the literature. We propose a new acquisition rule that selects the next evaluation where the uncertainty in the posterior distribution is largest. Experiments show that the proposed method often produces the most accurate approximations, especially in high-dimensional cases or in the presence of strong prior information, compared to common alternatives.|approxim bayesian comput abc method bayesian infer likelihood unavail simul model possibl howev mani abc algorithm requir larg number simul cost reduc comput cost surrog model bayesian optimis bo propos bayesian optimis enabl one intellig decid evalu model next standard bo strategi design optimis specif abc infer paper address gap literatur propos new acquisit rule select next evalu uncertainti posterior distribut largest experi show propos method often produc accur approxim especi high dimension case presenc strong prior inform compar common altern|['Marko Järvenpää', 'Michael U. Gutmann', 'Aki Vehtari', 'Pekka Marttinen']|['stat.ML', 'stat.CO', 'stat.ME']
2017-04-07T11:24:27Z|2017-04-01T03:57:44Z|http://arxiv.org/abs/1704.00117v1|http://arxiv.org/pdf/1704.00117v1|A Multi-Index Markov Chain Monte Carlo Method|multi index markov chain mont carlo method|In this article we consider computing expectations w.r.t.~probability laws associated to a certain class of stochastic systems. In order to achieve such a task, one must not only resort to numerical approximation of the expectation, but also to a biased discretization of the associated probability. We are concerned with the situation for which the discretization is required in multiple dimensions, for instance in space and time. In such contexts, it is known that the multi-index Monte Carlo (MIMC) method can improve upon i.i.d.~sampling from the most accurate approximation of the probability law. Indeed by a non-trivial modification of the multilevel Monte Carlo (MLMC) method and it can reduce the work to obtain a given level of error, relative to the afore mentioned i.i.d.~sampling and relative even to MLMC. In this article we consider the case when such probability laws are too complex to sampled independently. We develop a modification of the MIMC method which allows one to use standard Markov chain Monte Carlo (MCMC) algorithms to replace independent and coupled sampling, in certain contexts. We prove a variance theorem which shows that using our MIMCMC method is preferable, in the sense above, to i.i.d.~sampling from the most accurate approximation, under assumptions. The method is numerically illustrated on a problem associated to a stochastic partial differential equation (SPDE).|articl consid comput expect probabl law associ certain class stochast system order achiev task one must onli resort numer approxim expect also bias discret associ probabl concern situat discret requir multipl dimens instanc space time context known multi index mont carlo mimc method improv upon sampl accur approxim probabl law inde non trivial modif multilevel mont carlo mlmc method reduc work obtain given level error relat afor mention sampl relat even mlmc articl consid case probabl law complex sampl independ develop modif mimc method allow one use standard markov chain mont carlo mcmc algorithm replac independ coupl sampl certain context prove varianc theorem show use mimcmc method prefer sens abov sampl accur approxim assumpt method numer illustr problem associ stochast partial differenti equat spde|['Ajay Jasra', 'Kengo Kamatani', 'Kody Law', 'Yan Zhou']|['stat.CO']
2017-04-07T11:24:27Z|2017-03-30T11:49:27Z|http://arxiv.org/abs/1703.10423v1|http://arxiv.org/abs/1703.10423v1|Minimum energy path calculations with Gaussian process regression|minimum energi path calcul gaussian process regress|The calculation of minimum energy paths for transitions such as atomic and/or spin re-arrangements is an important task in many contexts and can often be used to determine the mechanism and rate of transitions. An important challenge is to reduce the computational effort in such calculations, especially when ab initio or electron density functional calculations are used to evaluate the energy since they can require large computational effort. Gaussian process regression is used here to reduce significantly the number of energy evaluations needed to find minimum energy paths of atomic rearrangements. By using results of previous calculations to construct an approximate energy surface and then converge to the minimum energy path on that surface in each Gaussian process iteration, the number of energy evaluations is reduced significantly as compared with regular nudged elastic band calculations. For a test problem involving rearrangements of a heptamer island on a crystal surface, the number of energy evaluations is reduced to less than a fifth. The scaling of the computational effort with the number of degrees of freedom as well as various possible further improvements to this approach are discussed.|calcul minimum energi path transit atom spin arrang import task mani context often use determin mechan rate transit import challeng reduc comput effort calcul especi ab initio electron densiti function calcul use evalu energi sinc requir larg comput effort gaussian process regress use reduc signific number energi evalu need find minimum energi path atom rearrang use result previous calcul construct approxim energi surfac converg minimum energi path surfac gaussian process iter number energi evalu reduc signific compar regular nudg elast band calcul test problem involv rearrang heptam island crystal surfac number energi evalu reduc less fifth scale comput effort number degre freedom well various possibl improv approach discuss|['Olli-Pekka Koistinen', 'Emile Maras', 'Aki Vehtari', 'Hannes Jónsson']|['physics.chem-ph', 'physics.atm-clus', 'physics.comp-ph', 'stat.CO', 'stat.ML']
2017-04-07T11:24:30Z|2017-03-30T08:54:34Z|http://arxiv.org/abs/1703.10364v1|http://arxiv.org/pdf/1703.10364v1|Quantifying Uncertainty in Transdimensional Markov Chain Monte Carlo   Using Discrete Markov Models|quantifi uncertainti transdimension markov chain mont carlo use discret markov model|Bayesian analysis often concerns an evaluation of models with different dimensionality as is necessary in, for example, model selection or mixture models. To facilitate this evaluation, transdimensional Markov chain Monte Carlo (MCMC) has proven to be a valuable tool. For instance, in case of model selection, this method relies on sampling a discrete model-indicator variable to estimate the posterior model probabilities. However, little attention has been paid to the precision of these estimates. If only few switches occur between the models in the transdimensional MCMC output, precision may be low and assessment based on the assumption of independent samples misleading. Here, we propose a new method to estimate the precision based on the observed transition matrix of the indicator variable. Essentially, the method samples from the posterior of the stationary distribution, thereby assessing the uncertainty in the estimated posterior model probabilities. Moreover, the method provides an estimate for the effective sample size of the MCMC output. In two model-selection examples, we show that the proposed approach provides a good assessment of the uncertainty associated with the estimated posterior model probabilities.|bayesian analysi often concern evalu model differ dimension necessari exampl model select mixtur model facilit evalu transdimension markov chain mont carlo mcmc proven valuabl tool instanc case model select method reli sampl discret model indic variabl estim posterior model probabl howev littl attent paid precis estim onli switch occur model transdimension mcmc output precis may low assess base assumpt independ sampl mislead propos new method estim precis base observ transit matrix indic variabl essenti method sampl posterior stationari distribut therebi assess uncertainti estim posterior model probabl moreov method provid estim effect sampl size mcmc output two model select exampl show propos approach provid good assess uncertainti associ estim posterior model probabl|['Daniel W. Heck', 'Antony M. Overstall', 'Quentin F. Gronau', 'Eric-Jan Wagenmakers']|['stat.ME', 'stat.CO']
2017-04-07T11:24:30Z|2017-03-29T20:56:31Z|http://arxiv.org/abs/1703.10245v1|http://arxiv.org/pdf/1703.10245v1|Bayesian Effect Fusion for Categorical Predictors|bayesian effect fusion categor predictor|In this paper, we propose a Bayesian approach to obtain a sparse representation of the effect of a categorical predictor in regression type models. As the effect of a categorical predictor is captured by a group of level effects, sparsity cannot only be achieved by excluding single irrelevant level effects but also by excluding the whole group of effects associated to a predictor or by fusing levels which have essentially the same effect on the response. To achieve this goal, we propose a prior which allows for almost perfect as well as almost zero dependence between level effects a priori. We show how this prior can be obtained by specifying spike and slab prior distributions on all effect differences associated to one categorical predictor and how restricted fusion can be implemented. An efficient MCMC method for posterior computation is developed. The performance of the proposed method is investigated on simulated data. Finally, we illustrate its application on real data from EU-SILC.|paper propos bayesian approach obtain spars represent effect categor predictor regress type model effect categor predictor captur group level effect sparsiti cannot onli achiev exclud singl irrelev level effect also exclud whole group effect associ predictor fuse level essenti effect respons achiev goal propos prior allow almost perfect well almost zero depend level effect priori show prior obtain specifi spike slab prior distribut effect differ associ one categor predictor restrict fusion implement effici mcmc method posterior comput develop perform propos method investig simul data final illustr applic real data eu silc|['Daniela Pauger', 'Helga Wagner']|['stat.CO']
2017-04-07T11:24:30Z|2017-03-29T08:32:21Z|http://arxiv.org/abs/1703.09930v1|http://arxiv.org/pdf/1703.09930v1|Adaptive Gaussian process approximation for Bayesian inference with   expensive likelihood functions|adapt gaussian process approxim bayesian infer expens likelihood function|We consider Bayesian inference problems with computationally intensive likelihood functions. We propose a Gaussian process (GP) based method to approximate the joint distribution of the unknown parameters and the data. In particular, we write the joint density approximately as a product of an approximate posterior density and an exponentiated GP surrogate. We then provide an adaptive algorithm to construct such an approximation, where an active learning method is used to choose the design points. With numerical examples, we illustrate that the proposed method has competitive performance against existing approaches for Bayesian computation.|consid bayesian infer problem comput intens likelihood function propos gaussian process gp base method approxim joint distribut unknown paramet data particular write joint densiti approxim product approxim posterior densiti exponenti gp surrog provid adapt algorithm construct approxim activ learn method use choos design point numer exampl illustr propos method competit perform exist approach bayesian comput|['Hongqiao Wang', 'Jinglai Li']|['stat.CO', 'stat.ML']
2017-04-07T11:24:30Z|2017-03-28T16:24:42Z|http://arxiv.org/abs/1703.09658v1|http://arxiv.org/pdf/1703.09658v1|An orthogonal basis expansion method for solving path-independent   stochastic differential equations|orthogon basi expans method solv path independ stochast differenti equat|In this article, we present an orthogonal basis expansion method for solving stochastic differential equations with a path-independent solution of the form $X_{t}=\phi(t,W_{t})$. For this purpose, we define a Hilbert space and construct an orthogonal basis for this inner product space with the aid of 2D-Hermite polynomials. With considering $X_{t}$ as orthogonal basis expansion, this method is implemented and the expansion coefficients are obtained by solving a system of nonlinear integro-differential equations. The strength of such a method is that expectation and variance of the solution is computed by these coefficients directly. Eventually, numerical results demonstrate its validity and efficiency in comparison with other numerical methods.|articl present orthogon basi expans method solv stochast differenti equat path independ solut form phi purpos defin hilbert space construct orthogon basi inner product space aid hermit polynomi consid orthogon basi expans method implement expans coeffici obtain solv system nonlinear integro differenti equat strength method expect varianc solut comput coeffici direct eventu numer result demonstr valid effici comparison numer method|['Hamidreza Rezazadeh', 'Amirhossein Sobhani', 'Rahman Farnoosh']|['stat.CO']
2017-04-07T11:24:30Z|2017-03-28T03:08:35Z|http://arxiv.org/abs/1703.09382v1|http://arxiv.org/pdf/1703.09382v1|Exact computation of GMM estimators for instrumental variable quantile   regression models|exact comput gmm estim instrument variabl quantil regress model|We show that the generalized method of moments (GMM) estimation problem in instrumental variable quantile regression (IVQR) models can be equivalently formulated as a mixed integer quadratic programming problem. This enables exact computation of the GMM estimators for the IVQR models. We illustrate the usefulness of our algorithm via Monte Carlo experiments and an application to demand for fish.|show general method moment gmm estim problem instrument variabl quantil regress ivqr model equival formul mix integ quadrat program problem enabl exact comput gmm estim ivqr model illustr use algorithm via mont carlo experi applic demand fish|['Le-Yu Chen', 'Sokbae Lee']|['stat.CO']
2017-04-07T11:24:30Z|2017-03-27T20:37:04Z|http://arxiv.org/abs/1703.09301v1|http://arxiv.org/pdf/1703.09301v1|Massive-scale estimation of exponential-family random graph models with   local dependence|massiv scale estim exponenti famili random graph model local depend|A flexible approach to modeling network data is based on exponential-family random graph models. We consider here exponential-family random graph models with additional structure in the form of local dependence, which have important conceptual and statistical advantages over models without additional structure. An open problem is how to estimate such models from large random graphs. We pave the ground for massive-scale estimation of such models by exploiting model structure for the purpose of parallel computing. The main idea is that we can first decompose random graphs into subgraphs with local dependence and then perform parallel computing on subgraphs. We hence propose a two-step likelihood-based approach. The first step estimates the local structure underlying random graphs. The second step estimates parameters given the estimated local structure of random graphs. Both steps can be implemented in parallel, which enables massive-scale estimation. We demonstrate the advantages of the two-step likelihood-based approach by simulations and an application to a large Amazon product network.|flexibl approach model network data base exponenti famili random graph model consid exponenti famili random graph model addit structur form local depend import conceptu statist advantag model without addit structur open problem estim model larg random graph pave ground massiv scale estim model exploit model structur purpos parallel comput main idea first decompos random graph subgraph local depend perform parallel comput subgraph henc propos two step likelihood base approach first step estim local structur random graph second step estim paramet given estim local structur random graph step implement parallel enabl massiv scale estim demonstr advantag two step likelihood base approach simul applic larg amazon product network|['Sergii Babkin', 'Michael Schweinberger']|['stat.CO', 'stat.ME']
2017-04-07T11:24:30Z|2017-03-27T16:14:18Z|http://arxiv.org/abs/1703.09163v1|http://arxiv.org/pdf/1703.09163v1|Scalable Bayesian shrinkage and uncertainty quantification in   high-dimensional regression|scalabl bayesian shrinkag uncertainti quantif high dimension regress|"Bayesian shrinkage methods have generated a lot of recent interest as tools for high-dimensional regression and model selection. These methods naturally facilitate tractable uncertainty quantification and incorporation of prior information. A common feature of these models, including the Bayesian lasso, global-local shrinkage priors, and spike-and-slab priors is that the corresponding priors on the regression coefficients can be expressed as scale mixture of normals. While the three-step Gibbs sampler used to sample from the often intractable associated posterior density has been shown to be geometrically ergodic for several of these models (Khare and Hobert, 2013; Pal and Khare, 2014), it has been demonstrated recently that convergence of this sampler can still be quite slow in modern high-dimensional settings despite this apparent theoretical safeguard. We propose a new method to draw from the same posterior via a tractable two-step blocked Gibbs sampler. We demonstrate that our proposed two-step blocked sampler exhibits vastly superior convergence behavior compared to the original three- step sampler in high-dimensional regimes on both real and simulated data. We also provide a detailed theoretical underpinning to the new method in the context of the Bayesian lasso. First, we derive explicit upper bounds for the (geometric) rate of convergence. Furthermore, we demonstrate theoretically that while the original Bayesian lasso chain is not Hilbert-Schmidt, the proposed chain is trace class (and hence Hilbert-Schmidt). The trace class property has useful theoretical and practical implications. It implies that the corresponding Markov operator is compact, and its eigenvalues are summable. It also facilitates a rigorous comparison of the two-step blocked chain with ""sandwich"" algorithms which aim to improve performance of the two-step chain by inserting an inexpensive extra step."|bayesian shrinkag method generat lot recent interest tool high dimension regress model select method natur facilit tractabl uncertainti quantif incorpor prior inform common featur model includ bayesian lasso global local shrinkag prior spike slab prior correspond prior regress coeffici express scale mixtur normal three step gibb sampler use sampl often intract associ posterior densiti shown geometr ergod sever model khare hobert pal khare demonstr recent converg sampler still quit slow modern high dimension set despit appar theoret safeguard propos new method draw posterior via tractabl two step block gibb sampler demonstr propos two step block sampler exhibit vast superior converg behavior compar origin three step sampler high dimension regim real simul data also provid detail theoret underpin new method context bayesian lasso first deriv explicit upper bound geometr rate converg furthermor demonstr theoret origin bayesian lasso chain hilbert schmidt propos chain trace class henc hilbert schmidt trace class properti use theoret practic implic impli correspond markov oper compact eigenvalu summabl also facilit rigor comparison two step block chain sandwich algorithm aim improv perform two step chain insert inexpens extra step|['Bala Rajaratnam', 'Doug Sparks', 'Kshitij Khare', 'Liyuan Zhang']|['stat.CO']
2017-04-07T11:24:30Z|2017-03-27T13:41:00Z|http://arxiv.org/abs/1703.09074v1|http://arxiv.org/pdf/1703.09074v1|Randomized CP Tensor Decomposition|random cp tensor decomposit|The CANDECOMP/PARAFAC (CP) tensor decomposition is a popular dimensionality-reduction method for multiway data. Dimensionality reduction is often sought since many high-dimensional tensors have low intrinsic rank relative to the dimension of the ambient measurement space. However, the emergence of `big data' poses significant computational challenges for computing this fundamental tensor decomposition. Leveraging modern randomized algorithms, we demonstrate that the coherent structure can be learned from a smaller representation of the tensor in a fraction of the time. Moreover, the high-dimensional signal can be faithfully approximated from the compressed measurements. Thus, this simple but powerful algorithm enables one to compute the approximate CP decomposition even for massive tensors. The approximation error can thereby be controlled via oversampling and the computation of power iterations. In addition to theoretical results, several empirical results demonstrate the performance of the proposed algorithm.|candecomp parafac cp tensor decomposit popular dimension reduct method multiway data dimension reduct often sought sinc mani high dimension tensor low intrins rank relat dimens ambient measur space howev emerg big data pose signific comput challeng comput fundament tensor decomposit leverag modern random algorithm demonstr coher structur learn smaller represent tensor fraction time moreov high dimension signal faith approxim compress measur thus simpl power algorithm enabl one comput approxim cp decomposit even massiv tensor approxim error therebi control via oversampl comput power iter addit theoret result sever empir result demonstr perform propos algorithm|['N. Benjamin Erichson', 'Krithika Manohar', 'Steven L. Brunton', 'J. Nathan Kutz']|['cs.NA', 'stat.CO']
2017-04-07T11:24:30Z|2017-03-27T13:33:39Z|http://arxiv.org/abs/1703.09062v1|http://arxiv.org/pdf/1703.09062v1|A numerical method for the estimation of time-varying parameter models   in large dimensions|numer method estim time vari paramet model larg dimens|A novel numerical method for the estimation of large time-varying parameter (TVP) models is proposed. The Kalman filter and Kalman smoother estimates of the TVP model are derived within the context of generalised linear least squares and through the use of numerical linear algebra. The method developed is based on numerically stable and computationally efficient strategies. The computational cost is reduced by exploiting the special sparse structure of the TVP model and by utilising previous computations. The proposed method is also extended to solve the downdating problem of removing the effect of some observations from current estimates and also to the rolling window estimation of the TVP model. Experimental results show the effectiveness of the new strategies in high dimensions when a large number of covariates are included in the TVP model.|novel numer method estim larg time vari paramet tvp model propos kalman filter kalman smoother estim tvp model deriv within context generalis linear least squar use numer linear algebra method develop base numer stabl comput effici strategi comput cost reduc exploit special spars structur tvp model utilis previous comput propos method also extend solv downdat problem remov effect observ current estim also roll window estim tvp model experiment result show effect new strategi high dimens larg number covari includ tvp model|['Stella Hadjiantoni', 'Erricos J. Kontoghiorghes']|['stat.ME', 'stat.CO']
2017-04-07T11:24:30Z|2017-03-27T02:18:36Z|http://arxiv.org/abs/1703.09570v1|http://arxiv.org/pdf/1703.09570v1|A Tidy Data Model for Natural Language Processing using cleanNLP|tidi data model natur languag process use cleannlp|The package cleanNLP provides a set of fast tools for converting a textual corpus into a set of normalized tables. The underlying natural language processing pipeline utilizes Stanford's CoreNLP library, exposing a number of annotation tasks for text written in English, French, German, and Spanish. Annotators include tokenization, part of speech tagging, named entity recognition, entity linking, sentiment analysis, dependency parsing, coreference resolution, and information extraction.|packag cleannlp provid set fast tool convert textual corpus set normal tabl natur languag process pipelin util stanford corenlp librari expos number annot task text written english french german spanish annot includ token part speech tag name entiti recognit entiti link sentiment analysi depend pars corefer resolut inform extract|['Taylor Arnold']|['cs.CL', 'stat.CO']
2017-04-07T11:24:34Z|2017-03-26T22:49:31Z|http://arxiv.org/abs/1703.08882v1|http://arxiv.org/pdf/1703.08882v1|A Mixture of Matrix Variate Skew-t Distributions|mixtur matrix variat skew distribut|Clustering is the process of finding underlying group structures in data. Although model-based clustering is firmly established in the multivariate case, there is relative paucity for matrix variate distributions, and there are even fewer examples using matrix variate skew distributions. In this paper, we look at parameter estimation for a finite mixture of matrix variate skew-t distributions in the context of model-based clustering. Simulated data is used for illustrative purposes.|cluster process find group structur data although model base cluster firm establish multivari case relat pauciti matrix variat distribut even fewer exampl use matrix variat skew distribut paper look paramet estim finit mixtur matrix variat skew distribut context model base cluster simul data use illustr purpos|['Michael P. B. Gallaugher', 'Paul D. McNicholas']|['stat.ME', 'stat.CO']
2017-04-07T11:24:34Z|2017-03-25T17:57:31Z|http://arxiv.org/abs/1703.08723v1|http://arxiv.org/pdf/1703.08723v1|Extending Growth Mixture Models Using Continuous Non-Elliptical   Distributions|extend growth mixtur model use continu non ellipt distribut|Growth mixture models (GMMs) incorporate both conventional random effects growth modeling and latent trajectory classes as in finite mixture modeling; therefore, they offer a way to handle the unobserved heterogeneity between subjects in their development. GMMs with Gaussian random effects dominate the literature. When the data are asymmetric and/or have heavier tails, more than one latent class is required to capture the observed variable distribution. Therefore, a GMM with continuous non-elliptical distributions is proposed to capture skewness and heavier tails in the data set. Specifically, multivariate skew-t distributions and generalized hyperbolic distributions are introduced to extend GMMs. When extending GMMs, four statistical models are considered with differing distributions of measurement errors and random effects. The mathematical development of a GMM with non-elliptical distributions relies on its relationship with the generalized inverse Gaussian distribution. Parameter estimation is outlined within the expectation-maximization framework before the performance of our GMM with non-elliptical distributions is illustrated on simulated and real data.|growth mixtur model gmms incorpor convent random effect growth model latent trajectori class finit mixtur model therefor offer way handl unobserv heterogen subject develop gmms gaussian random effect domin literatur data asymmetr heavier tail one latent class requir captur observ variabl distribut therefor gmm continu non ellipt distribut propos captur skew heavier tail data set specif multivari skew distribut general hyperbol distribut introduc extend gmms extend gmms four statist model consid differ distribut measur error random effect mathemat develop gmm non ellipt distribut reli relationship general invers gaussian distribut paramet estim outlin within expect maxim framework befor perform gmm non ellipt distribut illustr simul real data|['Yuhong Wei', 'Emilie Shireman', 'Paul D. McNicholas', 'Douglas L. Steinley']|['stat.ME', 'stat.AP', 'stat.CO']
2017-04-07T11:24:34Z|2017-03-25T11:20:21Z|http://arxiv.org/abs/1703.08676v1|http://arxiv.org/pdf/1703.08676v1|Statistical and Computational Tradeoff in Genetic Algorithm-Based   Estimation|statist comput tradeoff genet algorithm base estim|When a Genetic Algorithm (GA), or a stochastic algorithm in general, is employed in a statistical problem, the obtained result is affected by both variability due to sampling, that refers to the fact that only a sample is observed, and variability due to the stochastic elements of the algorithm. This topic can be easily set in a framework of statistical and computational tradeoff question, crucial in recent problems, for which statisticians must carefully set statistical and computational part of the analysis, taking account of some resource or time constraints. In the present work we analyze estimation problems tackled by GAs, for which variability of estimates can be decomposed in the two sources of variability, considering some constraints in the form of cost functions, related to both data acquisition and runtime of the algorithm. Simulation studies will be presented to discuss the statistical and computational tradeoff question.|genet algorithm ga stochast algorithm general employ statist problem obtain result affect variabl due sampl refer fact onli sampl observ variabl due stochast element algorithm topic easili set framework statist comput tradeoff question crucial recent problem statistician must care set statist comput part analysi take account resourc time constraint present work analyz estim problem tackl gas variabl estim decompos two sourc variabl consid constraint form cost function relat data acquisit runtim algorithm simul studi present discuss statist comput tradeoff question|['Manuel Rizzo', 'Francesco Battaglia']|['stat.CO']
2017-04-07T11:24:34Z|2017-03-24T23:49:33Z|http://arxiv.org/abs/1703.08627v1|http://arxiv.org/pdf/1703.08627v1|Random sampling of Latin squares via binary contingency tables and   probabilistic divide-and-conquer|random sampl latin squar via binari conting tabl probabilist divid conquer|We demonstrate a novel approach for the random sampling of Latin squares of order~$n$ via probabilistic divide-and-conquer. The algorithm divides the entries of the table modulo powers of $2$, and samples a corresponding binary contingency table at each level. The sampling distribution is based on the Boltzmann sampling heuristic, along with probabilistic divide-and-conquer.|demonstr novel approach random sampl latin squar order via probabilist divid conquer algorithm divid entri tabl modulo power sampl correspond binari conting tabl level sampl distribut base boltzmann sampl heurist along probabilist divid conquer|['Stephen DeSalvo']|['stat.CO']
2017-04-07T11:24:34Z|2017-03-24T17:17:45Z|http://arxiv.org/abs/1703.08520v1|http://arxiv.org/pdf/1703.08520v1|Rejection-free Ensemble MCMC with applications to Factorial Hidden   Markov Models|reject free ensembl mcmc applic factori hidden markov model|"Bayesian inference for complex models is challenging due to the need to explore high-dimensional spaces and multimodality and standard Monte Carlo samplers can have difficulties effectively exploring the posterior. We introduce a general purpose rejection-free ensemble Markov Chain Monte Carlo (MCMC) technique to improve on existing poorly mixing samplers. This is achieved by combining parallel tempering and an auxiliary variable move to exchange information between the chains. We demonstrate this ensemble MCMC scheme on Bayesian inference in Factorial Hidden Markov Models. This high-dimensional inference problem is difficult due to the exponentially sized latent variable space. Existing sampling approaches mix slowly and can get trapped in local modes. We show that the performance of these samplers is improved by our rejection-free ensemble technique and that the method is attractive and ""easy-to-use"" since no parameter tuning is required."|bayesian infer complex model challeng due need explor high dimension space multimod standard mont carlo sampler difficulti effect explor posterior introduc general purpos reject free ensembl markov chain mont carlo mcmc techniqu improv exist poor mix sampler achiev combin parallel temper auxiliari variabl move exchang inform chain demonstr ensembl mcmc scheme bayesian infer factori hidden markov model high dimension infer problem difficult due exponenti size latent variabl space exist sampl approach mix slowli get trap local mode show perform sampler improv reject free ensembl techniqu method attract easi use sinc paramet tune requir|['Kaspar Märtens', 'Michalis K Titsias', 'Christopher Yau']|['stat.CO', 'stat.ME', 'stat.ML']
2017-04-07T11:24:34Z|2017-03-21T15:42:38Z|http://arxiv.org/abs/1703.07285v1|http://arxiv.org/pdf/1703.07285v1|From safe screening rules to working sets for faster Lasso-type solvers|safe screen rule work set faster lasso type solver|Convex sparsity-promoting regularizations are ubiquitous in modern statistical learning. By construction, they yield solutions with few non-zero coefficients, which correspond to saturated constraints in the dual optimization formulation. Working set (WS) strategies are generic optimization techniques that consist in solving simpler problems that only consider a subset of constraints, whose indices form the WS. Working set methods therefore involve two nested iterations: the outer loop corresponds to the definition of the WS and the inner loop calls a solver for the subproblems. For the Lasso estimator a WS is a set of features, while for a Group Lasso it refers to a set of groups. In practice, WS are generally small in this context so the associated feature Gram matrix can fit in memory. Here we show that the Gauss-Southwell rule (a greedy strategy for block coordinate descent techniques) leads to fast solvers in this case. Combined with a working set strategy based on an aggressive use of so-called Gap Safe screening rules, we propose a solver achieving state-of-the-art performance on sparse learning problems. Results are presented on Lasso and multi-task Lasso estimators.|convex sparsiti promot regular ubiquit modern statist learn construct yield solut non zero coeffici correspond satur constraint dual optim formul work set ws strategi generic optim techniqu consist solv simpler problem onli consid subset constraint whose indic form ws work set method therefor involv two nest iter outer loop correspond definit ws inner loop call solver subproblem lasso estim ws set featur group lasso refer set group practic ws general small context associ featur gram matrix fit memori show gauss southwel rule greedi strategi block coordin descent techniqu lead fast solver case combin work set strategi base aggress use call gap safe screen rule propos solver achiev state art perform spars learn problem result present lasso multi task lasso estim|['Mathurin Massias', 'Alexandre Gramfort', 'Joseph Salmon']|['stat.ML', 'cs.LG', 'math.OC', 'stat.CO']
2017-04-07T11:24:34Z|2017-03-21T03:27:22Z|http://arxiv.org/abs/1703.07039v1|http://arxiv.org/pdf/1703.07039v1|A Simple Online Parameter Estimation Technique with Asymptotic   Guarantees|simpl onlin paramet estim techniqu asymptot guarante|In many modern settings, data are acquired iteratively over time, rather than all at once. Such settings are known as online, as opposed to offline or batch. We introduce a simple technique for online parameter estimation, which can operate in low memory settings, settings where data are correlated, and only requires a single inspection of the available data at each time period. We show that the estimators---constructed via the technique---are asymptotically normal under generous assumptions, and present a technique for the online computation of the covariance matrices for such estimators. A set of numerical studies demonstrates that our estimators can be as efficient as their offline counterparts, and that our technique generates estimates and confidence intervals that match their offline counterparts in various parameter estimation settings.|mani modern set data acquir iter time rather onc set known onlin oppos offlin batch introduc simpl techniqu onlin paramet estim oper low memori set set data correl onli requir singl inspect avail data time period show estim construct via techniqu asymptot normal generous assumpt present techniqu onlin comput covari matric estim set numer studi demonstr estim effici offlin counterpart techniqu generat estim confid interv match offlin counterpart various paramet estim set|['Hien D Nguyen']|['stat.CO']
2017-04-07T11:24:34Z|2017-03-19T10:51:41Z|http://arxiv.org/abs/1703.06419v1|http://arxiv.org/pdf/1703.06419v1|Multivariate Functional Data Visualization and Outlier Detection|multivari function data visual outlier detect|This article proposes a new graphical tool, the magnitude-shape (MS) plot, for visualizing both the magnitude and shape outlyingness of multivariate functional data. The proposed tool builds on the recent notion of functional directional outlyingness, which measures the centrality of functional data by simultaneously considering the level and the direction of their deviation from the central region. The MS-plot intuitively presents not only levels but also directions of magnitude outlyingness on the horizontal axis or plane, and demonstrates shape outlyingness on the vertical axis. A dividing curve or surface is provided to separate non-outlying data from the outliers. Both the simulated data and the practical examples confirm that the MS-plot is superior to existing tools for visualizing centrality and detecting outliers for functional data.|articl propos new graphic tool magnitud shape ms plot visual magnitud shape outlying multivari function data propos tool build recent notion function direct outlying measur central function data simultan consid level direct deviat central region ms plot intuit present onli level also direct magnitud outlying horizont axi plane demonstr shape outlying vertic axi divid curv surfac provid separ non data outlier simul data practic exampl confirm ms plot superior exist tool visual central detect outlier function data|['Wenlin Dai', 'Marc G. Genton']|['stat.ME', 'stat.CO']
2017-04-07T11:24:34Z|2017-03-18T22:12:08Z|http://arxiv.org/abs/1703.06359v1|http://arxiv.org/pdf/1703.06359v1|Fully symmetric kernel quadrature|fulli symmetr kernel quadratur|Kernel quadratures and other kernel-based approximation methods typically suffer from prohibitive cubic time and quadratic space complexity in the number of function evaluations. The problem arises because a system of linear equations needs to be solved. In this article we show that the weights of a kernel quadrature rule can be computed efficiently and exactly for up to tens of millions of nodes if the kernel, integration domain, and measure are fully symmetric and the node set is a union of fully symmetric sets. This is based on the observations that in such a setting there are only as many distinct weights as there are fully symmetric sets and that these weights can be solved from a linear system of equations constructed out of row sums of certain submatrices of the full kernel matrix. We present several numerical examples that show feasibility, both for a large number of nodes and in high dimensions, of the developed fully symmetric kernel quadrature rules. Most prominent of the fully symmetric kernel quadrature rules we propose are those that use sparse grids.|kernel quadratur kernel base approxim method typic suffer prohibit cubic time quadrat space complex number function evalu problem aris becaus system linear equat need solv articl show weight kernel quadratur rule comput effici exact ten million node kernel integr domain measur fulli symmetr node set union fulli symmetr set base observ set onli mani distinct weight fulli symmetr set weight solv linear system equat construct row sum certain submatric full kernel matrix present sever numer exampl show feasibl larg number node high dimens develop fulli symmetr kernel quadratur rule promin fulli symmetr kernel quadratur rule propos use spars grid|['Toni Karvonen', 'Simo Särkkä']|['math.NA', 'cs.NA', 'stat.CO']
2017-04-07T11:24:34Z|2017-03-17T21:49:28Z|http://arxiv.org/abs/1703.06206v1|http://arxiv.org/pdf/1703.06206v1|Sequential Monte Carlo Methods in the nimble R Package|sequenti mont carlo method nimbl packag|nimble is an R package for constructing algorithms and conducting inference on hierarchical models. The nimble package provides a unique combination of flexible model specification and the ability to program model-generic algorithms -- specifically, the package allows users to code models in the BUGS language, and it allows users to write algorithms that can be applied to any appropriately-specified BUGS model. In this paper, we introduce nimble's capabilities for state-space model analysis using Sequential Monte Carlo (SMC) techniques. We first provide an overview of state-space models and commonly used SMC algorithms. We then describe how to build a state-space model and conduct inference using existing SMC algorithms within nimble. SMC algorithms within nimble currently include the bootstrap filter, auxiliary particle filter, Liu and West filter, ensemble Kalman filter, and a particle MCMC sampler. These algorithms can be run in R or compiled into C++ for more efficient execution. Examples of applying SMC algorithms to a random walk model and a stochastic volatility model are provided. Finally, we give an overview of how model-generic algorithms are coded within nimble by providing code for a simple SMC algorithm.|nimbl packag construct algorithm conduct infer hierarch model nimbl packag provid uniqu combin flexibl model specif abil program model generic algorithm specif packag allow user code model bug languag allow user write algorithm appli ani appropri specifi bug model paper introduc nimbl capabl state space model analysi use sequenti mont carlo smc techniqu first provid overview state space model common use smc algorithm describ build state space model conduct infer use exist smc algorithm within nimbl smc algorithm within nimbl current includ bootstrap filter auxiliari particl filter liu west filter ensembl kalman filter particl mcmc sampler algorithm run compil effici execut exampl appli smc algorithm random walk model stochast volatil model provid final give overview model generic algorithm code within nimbl provid code simpl smc algorithm|['Nicholas Michaud', 'Perry de Valpine', 'Daniel Turek', 'Christopher J. Paciorek']|['stat.CO']
2017-04-07T11:24:38Z|2017-03-29T13:07:43Z|http://arxiv.org/abs/1703.06131v2|http://arxiv.org/pdf/1703.06131v2|Inference via low-dimensional couplings|infer via low dimension coupl|"Integration against an intractable probability measure is among the fundamental challenges of statistical inference, particularly in the Bayesian setting. A principled approach to this problem seeks a deterministic coupling of the measure of interest with a tractable ""reference"" measure (e.g., a standard Gaussian). This coupling is induced by a transport map, and enables direct simulation from the desired measure simply by evaluating the transport map at samples from the reference. Yet characterizing such a map---e.g., representing and evaluating it---grows challenging in high dimensions. The central contribution of this paper is to establish a link between the Markov properties of the target measure and the existence of certain low-dimensional couplings, induced by transport maps that are sparse or decomposable. Our analysis not only facilitates the construction of couplings in high-dimensional settings, but also suggests new inference methodologies. For instance, in the context of nonlinear and non-Gaussian state space models, we describe new variational algorithms for online filtering, smoothing, and parameter estimation. These algorithms implicitly characterize---via a transport map---the full posterior distribution of the sequential inference problem using local operations only incrementally more complex than regular filtering, while avoiding importance sampling or resampling."|integr intract probabl measur among fundament challeng statist infer particular bayesian set principl approach problem seek determinist coupl measur interest tractabl refer measur standard gaussian coupl induc transport map enabl direct simul desir measur simpli evalu transport map sampl refer yet character map repres evalu grow challeng high dimens central contribut paper establish link markov properti target measur exist certain low dimension coupl induc transport map spars decompos analysi onli facilit construct coupl high dimension set also suggest new infer methodolog instanc context nonlinear non gaussian state space model describ new variat algorithm onlin filter smooth paramet estim algorithm implicit character via transport map full posterior distribut sequenti infer problem use local oper onli increment complex regular filter avoid import sampl resampl|['Alessio Spantini', 'Daniele Bigoni', 'Youssef Marzouk']|['stat.ME', 'stat.CO', 'stat.ML']
2017-04-07T11:24:38Z|2017-03-17T17:00:53Z|http://arxiv.org/abs/1703.06098v1|http://arxiv.org/pdf/1703.06098v1|Analysis of the Gibbs Sampler for Gaussian hierarchical models via   multigrid decomposition|analysi gibb sampler gaussian hierarch model via multigrid decomposit|We study the convergence properties of the Gibbs Sampler in the context of posterior distributions arising from Bayesian analysis of Gaussian hierarchical models. We consider centred and non-centred parameterizations as well as their hybrids including the full family of partially non-centred parameterizations. We develop a novel methodology based on multi-grid decompositions to derive analytic expressions for the convergence rates of the algorithm for an arbitrary number of layers in the hierarchy, while previous work was typically limited to the two-level case. Our work gives a complete understanding for the three-level symmetric case and this gives rise to approximations for the non-symmetric case. We also give analogous, if less explicit, results for models of arbitrary level. This theory gives rise to simple and easy-to-implement guidelines for the practical implementation of Gibbs samplers on conditionally Gaussian hierarchical models.|studi converg properti gibb sampler context posterior distribut aris bayesian analysi gaussian hierarch model consid centr non centr parameter well hybrid includ full famili partial non centr parameter develop novel methodolog base multi grid decomposit deriv analyt express converg rate algorithm arbitrari number layer hierarchi previous work typic limit two level case work give complet understand three level symmetr case give rise approxim non symmetr case also give analog less explicit result model arbitrari level theori give rise simpl easi implement guidelin practic implement gibb sampler condit gaussian hierarch model|['Giacomo Zanella', 'Gareth Roberts']|['stat.CO', 'math.PR', 'stat.ME', '60J22, 62F15, 65C40, 65C05']
2017-04-07T11:24:38Z|2017-03-17T12:11:34Z|http://arxiv.org/abs/1703.05984v1|http://arxiv.org/pdf/1703.05984v1|A Tutorial on Bridge Sampling|tutori bridg sampl|The marginal likelihood plays an important role in many areas of Bayesian statistics such as parameter estimation, model comparison, and model averaging. In most applications, however, the marginal likelihood is not analytically tractable and must be approximated using numerical methods. Here we provide a tutorial on bridge sampling (Bennett, 1976; Meng & Wong, 1996), a reliable and relatively straightforward sampling method that allows researchers to obtain the marginal likelihood for models of varying complexity. First, we introduce bridge sampling and three related sampling methods using the beta-binomial model as a running example. We then apply bridge sampling to estimate the marginal likelihood for the Expectancy Valence (EV) model---a popular model for reinforcement learning. Our results indicate that bridge sampling provides accurate estimates for both a single participant and a hierarchical version of the EV model. We conclude that bridge sampling is an attractive method for mathematical psychologists who typically aim to approximate the marginal likelihood for a limited set of possibly high-dimensional models.|margin likelihood play import role mani area bayesian statist paramet estim model comparison model averag applic howev margin likelihood analyt tractabl must approxim use numer method provid tutori bridg sampl bennett meng wong reliabl relat straightforward sampl method allow research obtain margin likelihood model vari complex first introduc bridg sampl three relat sampl method use beta binomi model run exampl appli bridg sampl estim margin likelihood expect valenc ev model popular model reinforc learn result indic bridg sampl provid accur estim singl particip hierarch version ev model conclud bridg sampl attract method mathemat psychologist typic aim approxim margin likelihood limit set possibl high dimension model|['Quentin F. Gronau', 'Alexandra Sarafoglou', 'Dora Matzke', 'Alexander Ly', 'Udo Boehm', 'Maarten Marsman', 'David S. Leslie', 'Jonathan J. Forster', 'Eric-Jan Wagenmakers', 'Helen Steingroever']|['stat.CO']
2017-04-07T11:24:38Z|2017-03-16T14:09:50Z|http://arxiv.org/abs/1703.06826v1|http://arxiv.org/pdf/1703.06826v1|RatingScaleReduction package: stepwise rating scale item reduction   without predictability loss|ratingscalereduct packag stepwis rate scale item reduct without predict loss|"This study presents an innovative method for reducing the number of rating scale items without predictability loss. The ""area under the re- ceiver operator curve method"" (AUC ROC) is used to implement in the RatingScaleReduction package posted on CRAN. Several cases have been used to illustrate how the stepwise method has reduced the number of rating scale items (variables)."|studi present innov method reduc number rate scale item without predict loss area ceiver oper curv method auc roc use implement ratingscalereduct packag post cran sever case use illustr stepwis method reduc number rate scale item variabl|['Waldemar W. Koczkodaj', 'Alicja Wolny-Dominiak']|['stat.CO', '94A50, 62C25, 62C99, 62P10']
2017-04-07T11:24:38Z|2017-03-16T09:01:22Z|http://arxiv.org/abs/1703.05511v1|http://arxiv.org/pdf/1703.05511v1|An Induced Natural Selection Heuristic for Evaluating Optimal Bayesian   Experimental Designs|induc natur select heurist evalu optim bayesian experiment design|"Bayesian optimal experimental design has immense potential to inform the collection of data, so as to subsequently enhance our understanding of a variety of processes. However, a major impediment is the difficulty in evaluating optimal designs for problems with large, or high-dimensional, design spaces. We propose an efficient search heuristic suitable for general optimisation problems, with a particular focus on optimal Bayesian experimental design problems. The heuristic evaluates the objective (utility) function at an initial, randomly generated set of input values. At each generation of the algorithm, input values are ""accepted"" if their corresponding objective (utility) function satisfies some acceptance criteria, and new inputs are sampled about these accepted points. We demonstrate the new algorithm by evaluating the optimal Bayesian experimental designs for two popular stochastic models: a Markovian death model, and a pharmacokinetic model. The designs from this new algorithm are compared to those evaluated by existing algorithms, and computation times are given as a demonstration of the computational efficiency. A comparison to the current ""gold-standard"" method are given, to demonstrate that INSH finds designs that contain a similar amount of information, but more computationally efficiently. We also consider a simple approach to the construction of sampling windows for the pharmacokinetic model using the output of the proposed algorithm."|bayesian optim experiment design immens potenti inform collect data subsequ enhanc understand varieti process howev major impedi difficulti evalu optim design problem larg high dimension design space propos effici search heurist suitabl general optimis problem particular focus optim bayesian experiment design problem heurist evalu object util function initi random generat set input valu generat algorithm input valu accept correspond object util function satisfi accept criteria new input sampl accept point demonstr new algorithm evalu optim bayesian experiment design two popular stochast model markovian death model pharmacokinet model design new algorithm compar evalu exist algorithm comput time given demonstr comput effici comparison current gold standard method given demonstr insh find design contain similar amount inform comput effici also consid simpl approach construct sampl window pharmacokinet model use output propos algorithm|['David J. Price', 'Nigel G. Bean', 'Joshua V. Ross', 'Jonathan Tuke']|['stat.CO']
2017-04-07T11:24:38Z|2017-03-16T05:00:37Z|http://arxiv.org/abs/1703.05471v1|http://arxiv.org/pdf/1703.05471v1|Model selection and parameter inference in phylogenetics using Nested   Sampling|model select paramet infer phylogenet use nest sampl|Bayesian inference methods rely on numerical algorithms for both model selection and parameter inference. In general, these algorithms require a high computational effort to yield reliable inferences. One of the major challenges in phylogenetics regards the estimation of the marginal likelihood. This quantity is commonly used for comparing different evolutionary models, but its calculation, even for simple models, incurs high computational cost. Another interesting challenge regards the estimation of the posterior distribution. Often, long Markov chains are required to get sufficient samples to carry out parameter inference, especially for tree distributions. In general, these problems are addressed separately by using different procedures. Nested sampling (NS) is a Bayesian algorithm which provides the means to estimate marginal likelihoods and to sample from the posterior distribution at no extra cost. In this paper, we introduce NS to phylogenetics. Its performance is analysed under different scenarios and compared to established methods. We conclude that NS is a very competitive and attractive algorithm for phylogenetic inference.|bayesian infer method reli numer algorithm model select paramet infer general algorithm requir high comput effort yield reliabl infer one major challeng phylogenet regard estim margin likelihood quantiti common use compar differ evolutionari model calcul even simpl model incur high comput cost anoth interest challeng regard estim posterior distribut often long markov chain requir get suffici sampl carri paramet infer especi tree distribut general problem address separ use differ procedur nest sampl ns bayesian algorithm provid mean estim margin likelihood sampl posterior distribut extra cost paper introduc ns phylogenet perform analys differ scenario compar establish method conclud ns veri competit attract algorithm phylogenet infer|['Patricio Maturana R.', 'Brendon J. Brewer', 'Steffen Klaere']|['q-bio.QM', 'stat.CO']
2017-04-07T11:24:38Z|2017-03-26T15:00:32Z|http://arxiv.org/abs/1703.05144v2|http://arxiv.org/pdf/1703.05144v2|Bergm: Bayesian exponential random graph models in R|bergm bayesian exponenti random graph model|The Bergm package provides a comprehensive framework for Bayesian inference using Markov chain Monte Carlo (MCMC) algorithms. It can also supply graphical Bayesian goodness-of-fit procedures that address the issue of model adequacy. The package is simple to use and represents an attractive way of analysing network data as it offers the advantage of a complete probabilistic treatment of uncertainty. Bergm is based on the ergm package and therefore it makes use of the same model set-up and network simulation algorithms. The Bergm package has been continually improved in terms of speed performance over the last years and now offers the end-user a feasible option for carrying out Bayesian inference for networks with several thousands of nodes.|bergm packag provid comprehens framework bayesian infer use markov chain mont carlo mcmc algorithm also suppli graphic bayesian good fit procedur address issu model adequaci packag simpl use repres attract way analys network data offer advantag complet probabilist treatment uncertainti bergm base ergm packag therefor make use model set network simul algorithm bergm packag continu improv term speed perform last year offer end user feasibl option carri bayesian infer network sever thousand node|['Alberto Caimo', 'Nial Friel']|['stat.CO']
2017-04-07T11:24:38Z|2017-03-15T10:20:32Z|http://arxiv.org/abs/1703.05060v1|http://arxiv.org/pdf/1703.05060v1|Online Learning for Distribution-Free Prediction|onlin learn distribut free predict|We develop an online learning method for prediction, which is important in problems with large and/or streaming data sets. We formulate the learning approach using a covariance-fitting methodology, and show that the resulting predictor has desirable computational and distribution-free properties: It is implemented online with a runtime that scales linearly in the number of samples; has a constant memory requirement; avoids local minima problems; and prunes away redundant feature dimensions without relying on restrictive assumptions on the data distribution. In conjunction with the split conformal approach, it also produces distribution-free prediction confidence intervals in a computationally efficient manner. The method is demonstrated on both real and synthetic datasets.|develop onlin learn method predict import problem larg stream data set formul learn approach use covari fit methodolog show result predictor desir comput distribut free properti implement onlin runtim scale linear number sampl constant memori requir avoid local minima problem prune away redund featur dimens without reli restrict assumpt data distribut conjunct split conform approach also produc distribut free predict confid interv comput effici manner method demonstr real synthet dataset|['Dave Zachariah', 'Petre Stoica', 'Thomas B. Schön']|['cs.LG', 'stat.CO', 'stat.ML']
2017-04-07T11:24:38Z|2017-03-15T01:18:57Z|http://arxiv.org/abs/1703.04866v1|http://arxiv.org/pdf/1703.04866v1|Multilevel Sequential Monte Carlo with Dimension-Independent   Likelihood-Informed Proposals|multilevel sequenti mont carlo dimens independ likelihood inform propos|In this article we develop a new sequential Monte Carlo (SMC) method for multilevel (ML) Monte Carlo estimation. In particular, the method can be used to estimate expectations with respect to a target probability distribution over an infinite-dimensional and non-compact space as given, for example, by a Bayesian inverse problem with Gaussian random field prior. Under suitable assumptions the MLSMC method has the optimal $O(\epsilon^{-2})$ bound on the cost to obtain a mean-square error of $O(\epsilon^2)$. The algorithm is accelerated by dimension-independent likelihood-informed (DILI) proposals designed for Gaussian priors, leveraging a novel variation which uses empirical sample covariance information in lieu of Hessian information, hence eliminating the requirement for gradient evaluations. The efficiency of the algorithm is illustrated on two examples: inversion of noisy pressure measurements in a PDE model of Darcy flow to recover the posterior distribution of the permeability field, and inversion of noisy measurements of the solution of an SDE to recover the posterior path measure.|articl develop new sequenti mont carlo smc method multilevel ml mont carlo estim particular method use estim expect respect target probabl distribut infinit dimension non compact space given exampl bayesian invers problem gaussian random field prior suitabl assumpt mlsmc method optim epsilon bound cost obtain mean squar error epsilon algorithm acceler dimens independ likelihood inform dili propos design gaussian prior leverag novel variat use empir sampl covari inform lieu hessian inform henc elimin requir gradient evalu effici algorithm illustr two exampl invers noisi pressur measur pde model darci flow recov posterior distribut permeabl field invers noisi measur solut sde recov posterior path measur|['Alexandros Beskos', 'Ajay Jasra', 'Kody Law', 'Youssef Marzouk', 'Yan Zhou']|['stat.CO']
2017-04-07T11:24:38Z|2017-03-18T09:09:43Z|http://arxiv.org/abs/1703.04467v2|http://arxiv.org/pdf/1703.04467v2|spmoran: An R package for Moran's eigenvector-based spatial regression   analysis|spmoran packag moran eigenvector base spatial regress analysi|"The objective of this study is illustrating how to use ""spmoran,"" which is an R package for Moran's eigenvector-based spatial regression analysis. spmoran estimates regression models in the presence of spatial dependence, including eigenvector spatial filtering (ESF) and random effects ESF (RE-ESF) models. These models are allowed to have spatially varying coefficients to capture spatial heterogeneity. These ESF and RE-ESF models are suitable to estimate and infer regression coefficients with/without spatial variation. spmoran implements these models in a computationally efficient manner. For the illustration, this study applies ESF and RE-ESF models for a land price analysis."|object studi illustr use spmoran packag moran eigenvector base spatial regress analysi spmoran estim regress model presenc spatial depend includ eigenvector spatial filter esf random effect esf esf model model allow spatial vari coeffici captur spatial heterogen esf esf model suitabl estim infer regress coeffici without spatial variat spmoran implement model comput effici manner illustr studi appli esf esf model land price analysi|['Daisuke Murakami']|['stat.OT', 'stat.CO']
2017-04-07T11:24:42Z|2017-03-13T11:19:28Z|http://arxiv.org/abs/1703.04334v1|http://arxiv.org/pdf/1703.04334v1|Probabilistic Matching: Causal Inference under Measurement Errors|probabilist match causal infer measur error|The abundance of data produced daily from large variety of sources has boosted the need of novel approaches on causal inference analysis from observational data. Observational data often contain noisy or missing entries. Moreover, causal inference studies may require unobserved high-level information which needs to be inferred from other observed attributes. In such cases, inaccuracies of the applied inference methods will result in noisy outputs. In this study, we propose a novel approach for causal inference when one or more key variables are noisy. Our method utilizes the knowledge about the uncertainty of the real values of key variables in order to reduce the bias induced by noisy measurements. We evaluate our approach in comparison with existing methods both on simulated and real scenarios and we demonstrate that our method reduces the bias and avoids false causal inference conclusions in most cases.|abund data produc daili larg varieti sourc boost need novel approach causal infer analysi observ data observ data often contain noisi miss entri moreov causal infer studi may requir unobserv high level inform need infer observ attribut case inaccuraci appli infer method result noisi output studi propos novel approach causal infer one key variabl noisi method util knowledg uncertainti real valu key variabl order reduc bias induc noisi measur evalu approach comparison exist method simul real scenario demonstr method reduc bias avoid fals causal infer conclus case|['Fani Tsapeli', 'Peter Tino', 'Mirco Musolesi']|['stat.ME', 'stat.CO', 'stat.ML']
2017-04-07T11:24:42Z|2017-03-11T20:07:06Z|http://arxiv.org/abs/1703.04025v1|http://arxiv.org/pdf/1703.04025v1|Learning Large-Scale Bayesian Networks with the sparsebn Package|learn larg scale bayesian network sparsebn packag|Learning graphical models from data is an important problem with wide applications, ranging from genomics to the social sciences. Nowadays datasets typically have upwards of thousands---sometimes tens or hundreds of thousands---of variables and far fewer samples. To meet this challenge, we develop a new R package called sparsebn for learning the structure of large, sparse graphical models with a focus on Bayesian networks. While there are many existing packages for this task within the R ecosystem, this package focuses on the unique setting of learning large networks from high-dimensional data, possibly with interventions. As such, the methods provided place a premium on scalability and consistency in a high-dimensional setting. Furthermore, in the presence of interventions, the methods implemented here achieve the goal of learning a causal network from data. The sparsebn package is open-source and available on CRAN.|learn graphic model data import problem wide applic rang genom social scienc nowaday dataset typic upward thousand sometim ten hundr thousand variabl far fewer sampl meet challeng develop new packag call sparsebn learn structur larg spars graphic model focus bayesian network mani exist packag task within ecosystem packag focus uniqu set learn larg network high dimension data possibl intervent method provid place premium scalabl consist high dimension set furthermor presenc intervent method implement achiev goal learn causal network data sparsebn packag open sourc avail cran|['Bryon Aragam', 'Jiaying Gu', 'Qing Zhou']|['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']
2017-04-07T11:24:42Z|2017-03-29T09:21:47Z|http://arxiv.org/abs/1703.03680v2|http://arxiv.org/pdf/1703.03680v2|Strong convergence rates of probabilistic integrators for ordinary   differential equations|strong converg rate probabilist integr ordinari differenti equat|Probabilistic integration of a continuous dynamical system is a way of systematically introducing model error, at scales no larger than errors inroduced by standard numerical discretisation, in order to enable thorough exploration of possible responses of the system to inputs. It is thus a potentially useful approach in a number of applications such as forward uncertainty quantification, inverse problems, and data assimilation. We extend the convergence analysis of probabilistic integrators for deterministic ordinary differential equations, as proposed by Conrad et al. (Stat. Comput., 2016), to establish mean-square convergence in the uniform norm on discrete- or continuous-time solutions under relaxed regularity assumptions on the driving vector fields and their induced flows. Specifically, we show that randomised high-order integrators for globally Lipschitz flows and randomised Euler integrators for dissipative vector fields with polynomially-bounded local Lipschitz constants all have the same mean-square convergence rate as their deterministic counterparts, provided that the variance of the integration noise is not of higher order than the corresponding deterministic integrator.|probabilist integr continu dynam system way systemat introduc model error scale larger error inroduc standard numer discretis order enabl thorough explor possibl respons system input thus potenti use approach number applic forward uncertainti quantif invers problem data assimil extend converg analysi probabilist integr determinist ordinari differenti equat propos conrad et al stat comput establish mean squar converg uniform norm discret continu time solut relax regular assumpt drive vector field induc flow specif show randomis high order integr global lipschitz flow randomis euler integr dissip vector field polynomi bound local lipschitz constant mean squar converg rate determinist counterpart provid varianc integr nois higher order correspond determinist integr|['H. C. Lie', 'A. M. Stuart', 'T. J. Sullivan']|['math.NA', 'math.PR', 'math.ST', 'stat.CO', 'stat.TH', '65L20, 65C99, 37H10, 68W20']
2017-04-07T11:24:42Z|2017-03-09T21:57:00Z|http://arxiv.org/abs/1703.03475v1|http://arxiv.org/pdf/1703.03475v1|Auxiliary Variables for Bayesian Inference in Multi-Class Queueing   Networks|auxiliari variabl bayesian infer multi class queue network|Queue networks describe complex stochastic systems of both theoretical and practical interest. They provide the means to assess alterations, diagnose poor performance and evaluate robustness across sets of interconnected resources. In the present paper, we focus on the underlying continuous-time Markov chains induced by these networks, and we present a flexible method for drawing parameter inference in multi-class Markovian cases with switching and different service disciplines. The approach is directed towards the inferential problem with missing data and introduces a slice sampling technique with mappings to the measurable space of task transitions between service stations. The method deals with time and tractability issues, can handle prior system knowledge and overcomes common restrictions on service rates across existing inferential frameworks. Finally, the proposed algorithm is validated on synthetic data and applied to a real data set, obtained from a service delivery tasking tool implemented in two university hospitals.|queue network describ complex stochast system theoret practic interest provid mean assess alter diagnos poor perform evalu robust across set interconnect resourc present paper focus continu time markov chain induc network present flexibl method draw paramet infer multi class markovian case switch differ servic disciplin approach direct toward inferenti problem miss data introduc slice sampl techniqu map measur space task transit servic station method deal time tractabl issu handl prior system knowledg overcom common restrict servic rate across exist inferenti framework final propos algorithm valid synthet data appli real data set obtain servic deliveri task tool implement two univers hospit|['Iker Perez', 'David Hodge', 'Theodore Kypraios']|['stat.CO']
2017-04-07T11:24:42Z|2017-03-09T17:17:39Z|http://arxiv.org/abs/1703.03352v1|http://arxiv.org/pdf/1703.03352v1|A log-linear time algorithm for constrained changepoint detection|log linear time algorithm constrain changepoint detect|Changepoint detection is a central problem in time series and genomic data. For some applications, it is natural to impose constraints on the directions of changes. One example is ChIP-seq data, for which adding an up-down constraint improves peak detection accuracy, but makes the optimization problem more complicated. We show how a recently proposed functional pruning technique can be adapted to solve such constrained changepoint detection problems. This leads to a new algorithm which can solve problems with arbitrary affine constraints on adjacent segment means, and which has empirical time complexity that is log-linear in the amount of data. This algorithm achieves state-of-the-art accuracy in a benchmark of several genomic data sets, and is orders of magnitude faster than existing algorithms that have similar accuracy. Our implementation is available as the PeakSegPDPA function in the coseg R package, https://github.com/tdhock/coseg|changepoint detect central problem time seri genom data applic natur impos constraint direct chang one exampl chip seq data ad constraint improv peak detect accuraci make optim problem complic show recent propos function prune techniqu adapt solv constrain changepoint detect problem lead new algorithm solv problem arbitrari affin constraint adjac segment mean empir time complex log linear amount data algorithm achiev state art accuraci benchmark sever genom data set order magnitud faster exist algorithm similar accuraci implement avail peaksegpdpa function coseg packag https github com tdhock coseg|['Toby Dylan Hocking', 'Guillem Rigaill', 'Paul Fearnhead', 'Guillaume Bourque']|['stat.CO', 'q-bio.GN', 'stat.ML']
2017-04-07T11:24:42Z|2017-03-10T21:00:26Z|http://arxiv.org/abs/1703.03004v2|http://arxiv.org/pdf/1703.03004v2|New approximation for GARCH parameters estimate|new approxim garch paramet estim|This paper presents a new approach for the optimization of GARCH parameters estimation. Firstly, we propose a method for the localization of the maximum. Thereafter, using the methods of least squares, we make a local approximation for the projection of the likelihood function curve on two dimensional planes by a polynomial of order two which will be used to calculate an estimation of the maximum.|paper present new approach optim garch paramet estim first propos method local maximum thereaft use method least squar make local approxim project likelihood function curv two dimension plane polynomi order two use calcul estim maximum|['Yakoub Boularouk', 'Nasr-eddine Hamri']|['stat.CO']
2017-04-07T11:24:42Z|2017-03-08T19:22:11Z|http://arxiv.org/abs/1703.02998v1|http://arxiv.org/pdf/1703.02998v1|A note on quickly sampling a sparse matrix with low rank expectation|note quick sampl spars matrix low rank expect|"Given matrices $X,Y \in R^{n \times K}$ and $S \in R^{K \times K}$ with positive elements, this paper proposes an algorithm fastRG to sample a sparse matrix $A$ with low rank expectation $E(A) = XSY^T$ and independent Poisson elements. This allows for quickly sampling from a broad class of stochastic blockmodel graphs (degree-corrected, mixed membership, overlapping) all of which are specific parameterizations of the generalized random product graph model defined in Section 2.2. The basic idea of fastRG is to first sample the number of edges $m$ and then sample each edge. The key insight is that because of the the low rank expectation, it is easy to sample individual edges. The naive ""element-wise"" algorithm requires $O(n^2)$ operations to generate the $n\times n$ adjacency matrix $A$. In sparse graphs, where $m = O(n)$, ignoring log terms, fastRG runs in time $O(n)$. An implementation in fastRG is available on github. A computational experiment in Section 2.4 simulates graphs up to $n=10,000,000$ nodes with $m = 100,000,000$ edges. For example, on a graph with $n=500,000$ and $m = 5,000,000$, fastRG runs in less than one second on a 3.5 GHz Intel i5."|given matric time time posit element paper propos algorithm fastrg sampl spars matrix low rank expect xsi independ poisson element allow quick sampl broad class stochast blockmodel graph degre correct mix membership overlap specif parameter general random product graph model defin section basic idea fastrg first sampl number edg sampl edg key insight becaus low rank expect easi sampl individu edg naiv element wise algorithm requir oper generat time adjac matrix spars graph ignor log term fastrg run time implement fastrg avail github comput experi section simul graph node edg exampl graph fastrg run less one second ghz intel|['Karl Rohe', 'Jun Tao', 'Xintian Han', 'Norbert Binkiewicz']|['stat.CO']
2017-04-07T11:24:42Z|2017-03-07T18:36:55Z|http://arxiv.org/abs/1703.02518v1|http://arxiv.org/pdf/1703.02518v1|Faster Coordinate Descent via Adaptive Importance Sampling|faster coordin descent via adapt import sampl|Coordinate descent methods employ random partial updates of decision variables in order to solve huge-scale convex optimization problems. In this work, we introduce new adaptive rules for the random selection of their updates. By adaptive, we mean that our selection rules are based on the dual residual or the primal-dual gap estimates and can change at each iteration. We theoretically characterize the performance of our selection rules and demonstrate improvements over the state-of-the-art, and extend our theory and algorithms to general convex objectives. Numerical evidence with hinge-loss support vector machines and Lasso confirm that the practice follows the theory.|coordin descent method employ random partial updat decis variabl order solv huge scale convex optim problem work introduc new adapt rule random select updat adapt mean select rule base dual residu primal dual gap estim chang iter theoret character perform select rule demonstr improv state art extend theori algorithm general convex object numer evid hing loss support vector machin lasso confirm practic follow theori|['Dmytro Perekrestenko', 'Volkan Cevher', 'Martin Jaggi']|['cs.LG', 'cs.CV', 'math.OC', 'stat.CO', 'stat.ML', 'G.1.6']
2017-04-07T11:24:42Z|2017-03-07T15:13:08Z|http://arxiv.org/abs/1703.02428v1|http://arxiv.org/pdf/1703.02428v1|Robust Bayesian Filtering and Smoothing Using Student's t Distribution|robust bayesian filter smooth use student distribut|State estimation in heavy-tailed process and measurement noise is an important challenge that must be addressed in, e.g., tracking scenarios with agile targets and outlier-corrupted measurements. The performance of the Kalman filter (KF) can deteriorate in such applications because of the close relation to the Gaussian distribution. Therefore, this paper describes the use of Student's t distribution to develop robust, scalable, and simple filtering and smoothing algorithms.   After a discussion of Student's t distribution, exact filtering in linear state-space models with t noise is analyzed. Intermediate approximation steps are used to arrive at filtering and smoothing algorithms that closely resemble the KF and the Rauch-Tung-Striebel (RTS) smoother except for a nonlinear measurement-dependent matrix update. The required approximations are discussed and an undesirable behavior of moment matching for t densities is revealed. A favorable approximation based on minimization of the Kullback-Leibler divergence is presented. Because of its relation to the KF, some properties and algorithmic extensions are inherited by the t filter. Instructive simulation examples demonstrate the performance and robustness of the novel algorithms.|state estim heavi tail process measur nois import challeng must address track scenario agil target outlier corrupt measur perform kalman filter kf deterior applic becaus close relat gaussian distribut therefor paper describ use student distribut develop robust scalabl simpl filter smooth algorithm discuss student distribut exact filter linear state space model nois analyz intermedi approxim step use arriv filter smooth algorithm close resembl kf rauch tung striebel rts smoother except nonlinear measur depend matrix updat requir approxim discuss undesir behavior moment match densiti reveal favor approxim base minim kullback leibler diverg present becaus relat kf properti algorithm extens inherit filter instruct simul exampl demonstr perform robust novel algorithm|['Michael Roth', 'Tohid Ardeshiri', 'Emre Özkan', 'Fredrik Gustafsson']|['stat.ME', 'cs.SY', 'stat.CO']
2017-04-07T11:24:42Z|2017-03-07T15:01:51Z|http://arxiv.org/abs/1703.02419v1|http://arxiv.org/pdf/1703.02419v1|Probabilistic learning of nonlinear dynamical systems using sequential   Monte Carlo|probabilist learn nonlinear dynam system use sequenti mont carlo|"Probabilistic modeling provides the capability to represent and manipulate uncertainty in data, models, decisions and predictions. We are concerned with the problem of learning probabilistic models of dynamical systems from measured data. Specifically, we consider learning of probabilistic nonlinear state space models. There is no closed-form solution available for this problem, implying that we are forced to use approximations. In this tutorial we will provide a self-contained introduction to one of the state-of-the-art methods---the particle Metropolis-Hastings algorithm---which has proven to offer very practical approximations. This is a Monte Carlo based method, where the so-called particle filter is used to guide a Markov chain Monte Carlo method through the parameter space. One of the key merits of the particle Metropolis-Hastings method is that it is guaranteed to converge to the ""true solution"" under mild assumptions, despite being based on a practical implementation of a particle filter (i.e., using a finite number of particles). We will also provide a motivating numerical example illustrating the method which we have implemented in an in-house developed modeling language, serving the purpose of abstracting away the underlying mathematics of the Monte Carlo approximations from the user. This modeling language will open up the power of sophisticated Monte Carlo methods, including particle Metropolis-Hastings, to a large group of users without requiring them to know all the underlying mathematical details."|probabilist model provid capabl repres manipul uncertainti data model decis predict concern problem learn probabilist model dynam system measur data specif consid learn probabilist nonlinear state space model close form solut avail problem impli forc use approxim tutori provid self contain introduct one state art method particl metropoli hast algorithm proven offer veri practic approxim mont carlo base method call particl filter use guid markov chain mont carlo method paramet space one key merit particl metropoli hast method guarante converg true solut mild assumpt despit base practic implement particl filter use finit number particl also provid motiv numer exampl illustr method implement hous develop model languag serv purpos abstract away mathemat mont carlo approxim user model languag open power sophist mont carlo method includ particl metropoli hast larg group user without requir know mathemat detail|['Thomas B. Schön', 'Andreas Svensson', 'Lawrence Murray', 'Fredrik Lindsten']|['stat.CO', 'cs.LG', 'cs.SY']
2017-04-07T11:24:46Z|2017-03-07T11:48:45Z|http://arxiv.org/abs/1703.02341v1|http://arxiv.org/pdf/1703.02341v1|An automatic adaptive method to combine summary statistics in   approximate Bayesian computation|automat adapt method combin summari statist approxim bayesian comput|To infer the parameters of mechanistic models with intractable likelihoods, techniques such as approximate Bayesian computation (ABC) are increasingly being adopted. One of the main disadvantages of ABC in practical situations, however, is that parameter inference must generally rely on summary statistics of the data. This is particularly the case for problems involving high-dimensional data, such as biological imaging experiments. However, some summary statistics contain more information about parameters of interest than others, and it is not always clear how to weight their contributions within the ABC framework. We address this problem by developing an automatic, adaptive algorithm that chooses weights for each summary statistic. Our algorithm aims to maximize the distance between the prior and the approximate posterior by automatically adapting the weights within the ABC distance function. To demonstrate the effectiveness of our algorithm, we apply it to several stochastic models of biochemical reaction networks, and a spatial model of diffusion, and compare our results with existing algorithms.|infer paramet mechanist model intract likelihood techniqu approxim bayesian comput abc increas adopt one main disadvantag abc practic situat howev paramet infer must general reli summari statist data particular case problem involv high dimension data biolog imag experi howev summari statist contain inform paramet interest alway clear weight contribut within abc framework address problem develop automat adapt algorithm choos weight summari statist algorithm aim maxim distanc prior approxim posterior automat adapt weight within abc distanc function demonstr effect algorithm appli sever stochast model biochem reaction network spatial model diffus compar result exist algorithm|['Jonathan U Harrison', 'Ruth E Baker']|['stat.CO']
2017-04-07T11:24:46Z|2017-03-13T11:11:48Z|http://arxiv.org/abs/1703.02337v2|http://arxiv.org/pdf/1703.02337v2|A Note on the Convergence of the Gaussian Mean Shift Algorithm|note converg gaussian mean shift algorithm|Mean shift (MS) algorithms are popular methods for mode finding in pattern analysis. Each MS algorithm can be phrased as a fixed-point iteration scheme, which operates on a kernel density estimate (KDE) based on some data. The ability of an MS algorithm to obtain the modes of its KDE depends on whether or not the fixed-point scheme converges. The convergence of MS algorithms have recently been proved under some general conditions via first principle arguments. We complement the recent proofs by demonstrating that the MS algorithm operating on a Gaussian KDE can be viewed as an MM (minorization-maximization) algorithm, and thus permits the application of convergence techniques for such constructions. For the Gaussian case, we extend upon the previously results by showing that the fixed-points of the MS algorithm are all stationary points of the KDE in cases where the stationary points may not necessarily be isolated.|mean shift ms algorithm popular method mode find pattern analysi ms algorithm phrase fix point iter scheme oper kernel densiti estim kde base data abil ms algorithm obtain mode kde depend whether fix point scheme converg converg ms algorithm recent prove general condit via first principl argument complement recent proof demonstr ms algorithm oper gaussian kde view mm minor maxim algorithm thus permit applic converg techniqu construct gaussian case extend upon previous result show fix point ms algorithm stationari point kde case stationari point may necessarili isol|['Hien D Nguyen']|['stat.CO']
2017-04-07T11:24:46Z|2017-03-07T09:33:21Z|http://arxiv.org/abs/1703.02293v1|http://arxiv.org/pdf/1703.02293v1|Variable selection for mixed data clustering: a model-based approach|variabl select mix data cluster model base approach|We propose two approaches for selecting variables in latent class analysis (i.e.,mixture model assuming within component independence), which is the common model-based clustering method for mixed data. The first approach consists in optimizing the BIC with a modified version of the EM algorithm. This approach simultaneously performs both model selection and parameter inference. The second approach consists in maximizing the MICL, which considers the clustering task, with an algorithm of alternate optimization. This approach performs model selection without requiring the maximum likelihood estimates for model comparison, then parameter inference is done for the unique selected model. Thus, the benefits of both approaches is to avoid the computation of the maximum likelihood estimates for each model comparison. Moreover, they also avoid the use of the standard algorithms for variable selection which are often suboptimal (e.g. stepwise method) and computationally expensive. The case of data with missing values is also discussed. The interest of both proposed criteria is shown on simulated and real data.|propos two approach select variabl latent class analysi mixtur model assum within compon independ common model base cluster method mix data first approach consist optim bic modifi version em algorithm approach simultan perform model select paramet infer second approach consist maxim micl consid cluster task algorithm altern optim approach perform model select without requir maximum likelihood estim model comparison paramet infer done uniqu select model thus benefit approach avoid comput maximum likelihood estim model comparison moreov also avoid use standard algorithm variabl select often suboptim stepwis method comput expens case data miss valu also discuss interest propos criteria shown simul real data|['Matthieu Marbac', 'Mohammed Sedki']|['stat.CO', '62F15']
2017-04-07T11:24:46Z|2017-03-07T07:44:52Z|http://arxiv.org/abs/1703.02251v1|http://arxiv.org/pdf/1703.02251v1|The Maximum Likelihood Degree of Toric Varieties|maximum likelihood degre toric varieti|We study the maximum likelihood degree (ML degree) of toric varieties, known as discrete exponential models in statistics. By introducing scaling coefficients to the monomial parameterization of the toric variety, one can change the ML degree. We show that the ML degree is equal to the degree of the toric variety for generic scalings, while it drops if and only if the scaling vector is in the locus of the principal $A$-determinant. We also illustrate how to compute the ML estimate of a toric variety numerically via homotopy continuation from a scaled toric variety with low ML degree. Throughout, we include examples motivated by algebraic geometry and statistics. We compute the ML degree of rational normal scrolls and a large class of Veronese-type varieties. In addition, we investigate the ML degree of scaled Segre varieties, hierarchical loglinear models, and graphical models.|studi maximum likelihood degre ml degre toric varieti known discret exponenti model statist introduc scale coeffici monomi parameter toric varieti one chang ml degre show ml degre equal degre toric varieti generic scale drop onli scale vector locus princip determin also illustr comput ml estim toric varieti numer via homotopi continu scale toric varieti low ml degre throughout includ exampl motiv algebra geometri statist comput ml degre ration normal scroll larg class verones type varieti addit investig ml degre scale segr varieti hierarch loglinear model graphic model|['Carlos Améndola', 'Nathan Bliss', 'Isaac Burke', 'Courtney R. Gibbons', 'Martin Helmer', 'Serkan Hoşten', 'Evan D. Nash', 'Jose Israel Rodriguez', 'Daniel Smolkin']|['math.AG', 'math.ST', 'stat.CO', 'stat.TH', '14Q15, 14M25, 13P15, 62F10']
2017-04-07T11:24:46Z|2017-03-07T06:40:44Z|http://arxiv.org/abs/1703.02237v1|http://arxiv.org/pdf/1703.02237v1|Scalable Collaborative Targeted Learning for High-Dimensional Data|scalabl collabor target learn high dimension data|Robust inference of a low-dimensional parameter in a large semi-parametric model relies on external estimators of infinite-dimensional features of the distribution of the data. Typically, only one of the latter is optimized for the sake of constructing a well behaved estimator of the low-dimensional parameter of interest. Optimizing more than one of them for the sake of achieving a better bias-variance trade-off in the estimation of the parameter of interest is the core idea driving the general template of the collaborative targeted minimum loss-based estimation (C-TMLE) procedure. The original implementation/instantiation of the C-TMLE template can be presented as a greedy forward stepwise C-TMLE algorithm. It does not scale well when the number $p$ of covariates increases drastically. This motivates the introduction of a novel instantiation of the C-TMLE template where the covariates are pre-ordered. Its time complexity is $\mathcal{O}(p)$ as opposed to the original $\mathcal{O}(p^2)$, a remarkable gain. We propose two pre-ordering strategies and suggest a rule of thumb to develop other meaningful strategies. Because it is usually unclear a priori which pre-ordering strategy to choose, we also introduce another implementation/instantiation called SL-C-TMLE algorithm that enables the data-driven choice of the better pre-ordering strategy given the problem at hand. Its time complexity is $\mathcal{O}(p)$ as well. The computational burden and relative performance of these algorithms were compared in simulation studies involving fully synthetic data or partially synthetic data based on a real world large electronic health database; and in analyses of three real, large electronic health databases. In all analyses involving electronic health databases, the greedy C-TMLE algorithm is unacceptably slow. Simulation studies indicate our scalable C-TMLE and SL-C-TMLE algorithms work well.|robust infer low dimension paramet larg semi parametr model reli extern estim infinit dimension featur distribut data typic onli one latter optim sake construct well behav estim low dimension paramet interest optim one sake achiev better bias varianc trade estim paramet interest core idea drive general templat collabor target minimum loss base estim tmle procedur origin implement instanti tmle templat present greedi forward stepwis tmle algorithm doe scale well number covari increas drastic motiv introduct novel instanti tmle templat covari pre order time complex mathcal oppos origin mathcal remark gain propos two pre order strategi suggest rule thumb develop meaning strategi becaus usual unclear priori pre order strategi choos also introduc anoth implement instanti call sl tmle algorithm enabl data driven choic better pre order strategi given problem hand time complex mathcal well comput burden relat perform algorithm compar simul studi involv fulli synthet data partial synthet data base real world larg electron health databas analys three real larg electron health databas analys involv electron health databas greedi tmle algorithm unaccept slow simul studi indic scalabl tmle sl tmle algorithm work well|['Cheng Ju', 'Susan Gruber', 'Samuel D. Lendle', 'Antoine Chambaz', 'Jessica M. Franklin', 'Richard Wyss', 'Sebastian Schneeweiss', 'Mark J. van der Laan']|['stat.CO', 'stat.ME']
2017-04-07T11:24:46Z|2017-03-25T17:47:33Z|http://arxiv.org/abs/1703.02177v2|http://arxiv.org/pdf/1703.02177v2|Mixtures of Generalized Hyperbolic Distributions and Mixtures of Skew-t   Distributions for Model-Based Clustering with Incomplete Data|mixtur general hyperbol distribut mixtur skew distribut model base cluster incomplet data|Robust clustering from incomplete data is an important topic because, in many practical situations, real data sets are heavy-tailed, asymmetric, and/or have arbitrary patterns of missing observations. Flexible methods and algorithms for model-based clustering are presented via mixture of the generalized hyperbolic distributions and its limiting case, the mixture of multivariate skew-t distributions. An analytically feasible EM algorithm is formulated for parameter estimation and imputation of missing values for mixture models employing missing at random mechanisms. The proposed methodologies are investigated through a simulation study with varying proportions of synthetic missing values and illustrated using a real dataset. Comparisons are made with those obtained from the traditional mixture of generalized hyperbolic distribution counterparts by filling in the missing data using the mean imputation method.|robust cluster incomplet data import topic becaus mani practic situat real data set heavi tail asymmetr arbitrari pattern miss observ flexibl method algorithm model base cluster present via mixtur general hyperbol distribut limit case mixtur multivari skew distribut analyt feasibl em algorithm formul paramet estim imput miss valu mixtur model employ miss random mechan propos methodolog investig simul studi vari proport synthet miss valu illustr use real dataset comparison made obtain tradit mixtur general hyperbol distribut counterpart fill miss data use mean imput method|['Yuhong Wei', 'Paul D. McNicholas']|['stat.ME', 'stat.CO']
2017-04-07T11:24:46Z|2017-03-06T23:48:50Z|http://arxiv.org/abs/1703.02151v1|http://arxiv.org/pdf/1703.02151v1|Computationally Efficient Simulation of Queues: The R Package   queuecomputer|comput effici simul queue packag queuecomput|Large networks of queueing systems model important real-world systems such as MapReduce clusters, web-servers, hospitals, call-centers and airport passenger terminals.To model such systems accurately we must infer queueing parameters from data. Unfortunately, for many queueing networks there is no clear way to proceed with parameter inference from data. Approximate Bayesian computation could offer a straight-forward way to infer parameters for such networks if we could simulate data quickly enough.   We present a computationally efficient method for simulating from a very general set of queueing networks with the R package queuecomputer. Remarkable speedups of more than 2 orders of magnitude are observed relative to the popular DES packages simmer and simpy. We replicate output from these packages to validate the package.   The package is modular and integrates well with the popular R package dplyr. Complex queueing networks with tandem, parallel and fork/join topologies can easily be built with these two packages together. We show how to use this package with two examples: a call-centre and an airport terminal.|larg network queue system model import real world system mapreduc cluster web server hospit call center airport passeng termin model system accur must infer queue paramet data unfortun mani queue network clear way proceed paramet infer data approxim bayesian comput could offer straight forward way infer paramet network could simul data quick enough present comput effici method simul veri general set queue network packag queuecomput remark speedup order magnitud observ relat popular des packag simmer simpi replic output packag valid packag packag modular integr well popular packag dplyr complex queue network tandem parallel fork join topolog easili built two packag togeth show use packag two exampl call centr airport termin|['Anthony Ebert', 'Paul Wu', 'Kerrie Mengersen', 'Fabrizio Ruggeri']|['stat.CO', 'math.OC']
2017-04-07T11:24:46Z|2017-03-06T19:44:45Z|http://arxiv.org/abs/1703.02081v1|http://arxiv.org/pdf/1703.02081v1|Estimation and prediction in sparse and unbalanced tables|estim predict spars unbalanc tabl|We consider the problem where we have a multi-way table of means, indexed by several factors, where each factor can have a large number of levels. The entry in each cell is the mean of some response, averaged over the observations falling into that cell. Some cells may be very sparsely populated, and in extreme cases, not populated at all. We might still like to estimate an expected response in such cells. We propose here a novel hierarchical ANOVA (HANOVA) representation for such data. Sparse cells will lean more on the lower-order interaction model for the data. These in turn could have components that are poorly represented in the data, in which case they rely on yet lower-order models. Our approach leads to a simple hierarchical algorithm, requiring repeated calculations of sub-table means of modified counts. The algorithm has shown superiority over the unshrinked methods in both simulations and real data sets.|consid problem multi way tabl mean index sever factor factor larg number level entri cell mean respons averag observ fall cell cell may veri spars popul extrem case popul might still like estim expect respons cell propos novel hierarch anova hanova represent data spars cell lean lower order interact model data turn could compon poor repres data case reli yet lower order model approach lead simpl hierarch algorithm requir repeat calcul sub tabl mean modifi count algorithm shown superior unshrink method simul real data set|['Qingyuan Zhao', 'Trevor Hastie', 'Daryl Pregibon']|['stat.CO']
2017-04-07T11:24:46Z|2017-03-04T21:50:25Z|http://arxiv.org/abs/1703.01526v1|http://arxiv.org/abs/1703.01526v1|High Accuracy Classification of Parkinson's Disease through Shape   Analysis and Surface Fitting in $^{123}$I-Ioflupane SPECT Imaging|high accuraci classif parkinson diseas shape analysi surfac fit ioflupan spect imag|Early and accurate identification of parkinsonian syndromes (PS) involving presynaptic degeneration from non-degenerative variants such as Scans Without Evidence of Dopaminergic Deficit (SWEDD) and tremor disorders, is important for effective patient management as the course, therapy and prognosis differ substantially between the two groups. In this study, we use Single Photon Emission Computed Tomography (SPECT) images from healthy normal, early PD and SWEDD subjects, as obtained from the Parkinson's Progression Markers Initiative (PPMI) database, and process them to compute shape- and surface fitting-based features for the three groups. We use these features to develop and compare various classification models that can discriminate between scans showing dopaminergic deficit, as in PD, from scans without the deficit, as in healthy normal or SWEDD. Along with it, we also compare these features with Striatal Binding Ratio (SBR)-based features, which are well-established and clinically used, by computing a feature importance score using Random forests technique. We observe that the Support Vector Machine (SVM) classifier gave the best performance with an accuracy of 97.29%. These features also showed higher importance than the SBR-based features. We infer from the study that shape analysis and surface fitting are useful and promising methods for extracting discriminatory features that can be used to develop diagnostic models that might have the potential to help clinicians in the diagnostic process.|earli accur identif parkinsonian syndrom ps involv presynapt degener non degen variant scan without evid dopaminerg deficit swedd tremor disord import effect patient manag cours therapi prognosi differ substanti two group studi use singl photon emiss comput tomographi spect imag healthi normal earli pd swedd subject obtain parkinson progress marker initi ppmi databas process comput shape surfac fit base featur three group use featur develop compar various classif model discrimin scan show dopaminerg deficit pd scan without deficit healthi normal swedd along also compar featur striatal bind ratio sbr base featur well establish clinic use comput featur import score use random forest techniqu observ support vector machin svm classifi gave best perform accuraci featur also show higher import sbr base featur infer studi shape analysi surfac fit use promis method extract discriminatori featur use develop diagnost model might potenti help clinician diagnost process|['R. Prashanth', 'Sumantra Dutta Roy', 'Pravat K. Mandal', 'Shantanu Ghosh']|['stat.AP', 'cs.CV', 'physics.data-an', 'stat.CO', 'stat.ML']
2017-04-07T11:24:46Z|2017-03-04T09:12:42Z|http://arxiv.org/abs/1703.01421v1|http://arxiv.org/pdf/1703.01421v1|$l_0$-estimation of piecewise-constant signals on graphs|estim piecewis constant signal graph|We study recovery of piecewise-constant signals over arbitrary graphs by the estimator minimizing an $l_0$-edge-penalized objective. Although exact minimization of this objective may be computationally intractable, we show that the same statistical risk guarantees are achieved by the alpha-expansion algorithm which approximately minimizes this objective in polynomial time. We establish that for graphs with small average vertex degree, these guarantees are rate-optimal in a minimax sense over classes of edge-sparse signals. For application to spatially inhomogeneous graphs, we propose minimization of an edge-weighted variant of this objective where each edge is weighted by its effective resistance or another measure of its contribution to the graph's connectivity. We establish minimax optimality of the resulting estimators over corresponding edge-weighted sparsity classes. We show theoretically that these risk guarantees are not always achieved by the estimator minimizing the $l_1$/total-variation relaxation, and empirically that the $l_0$-based estimates are more accurate in high signal-to-noise settings.|studi recoveri piecewis constant signal arbitrari graph estim minim edg penal object although exact minim object may comput intract show statist risk guarante achiev alpha expans algorithm approxim minim object polynomi time establish graph small averag vertex degre guarante rate optim minimax sens class edg spars signal applic spatial inhomogen graph propos minim edg weight variant object edg weight effect resist anoth measur contribut graph connect establish minimax optim result estim correspond edg weight sparsiti class show theoret risk guarante alway achiev estim minim total variat relax empir base estim accur high signal nois set|['Zhou Fan', 'Leying Guan']|['stat.ME', 'math.ST', 'stat.CO', 'stat.TH']
2017-04-07T11:24:50Z|2017-03-03T18:07:15Z|http://arxiv.org/abs/1703.01273v1|http://arxiv.org/pdf/1703.01273v1|Estimating Spatial Econometrics Models with Integrated Nested Laplace   Approximation|estim spatial econometr model integr nest laplac approxim|Integrated Nested Laplace Approximation provides a fast and effective method for marginal inference on Bayesian hierarchical models. This methodology has been implemented in the R-INLA package which permits INLA to be used from within R statistical software. Although INLA is implemented as a general methodology, its use in practice is limited to the models implemented in the R-INLA package.   Spatial autoregressive models are widely used in spatial econometrics but have until now been missing from the R-INLA package. In this paper, we describe the implementation and application of a new class of latent models in INLA made available through R-INLA. This new latent class implements a standard spatial lag model, which is widely used and that can be used to build more complex models in spatial econometrics.   The implementation of this latent model in R-INLA also means that all the other features of INLA can be used for model fitting, model selection and inference in spatial econometrics, as will be shown in this paper. Finally, we will illustrate the use of this new latent model and its applications with two datasets based on Gaussian and binary outcomes.|integr nest laplac approxim provid fast effect method margin infer bayesian hierarch model methodolog implement inla packag permit inla use within statist softwar although inla implement general methodolog use practic limit model implement inla packag spatial autoregress model wide use spatial econometr miss inla packag paper describ implement applic new class latent model inla made avail inla new latent class implement standard spatial lag model wide use use build complex model spatial econometr implement latent model inla also mean featur inla use model fit model select infer spatial econometr shown paper final illustr use new latent model applic two dataset base gaussian binari outcom|['Virgilio Gomez-Rubio', 'Roger S. Bivand', 'Håvard Rue']|['stat.CO']
2017-04-07T11:24:50Z|2017-03-03T16:29:21Z|http://arxiv.org/abs/1703.01234v1|http://arxiv.org/pdf/1703.01234v1|A Bayesian computer model analysis of Robust Bayesian analyses|bayesian comput model analysi robust bayesian analys|We harness the power of Bayesian emulation techniques, designed to aid the analysis of complex computer models, to examine the structure of complex Bayesian analyses themselves. These techniques facilitate robust Bayesian analyses and/or sensitivity analyses of complex problems, and hence allow global exploration of the impacts of choices made in both the likelihood and prior specification. We show how previously intractable problems in robustness studies can be overcome using emulation techniques, and how these methods allow other scientists to quickly extract approximations to posterior results corresponding to their own particular subjective specification. The utility and flexibility of our method is demonstrated on a reanalysis of a real application where Bayesian methods were employed to capture beliefs about river flow. We discuss the obvious extensions and directions of future research that such an approach opens up.|har power bayesian emul techniqu design aid analysi complex comput model examin structur complex bayesian analys themselv techniqu facilit robust bayesian analys sensit analys complex problem henc allow global explor impact choic made likelihood prior specif show previous intract problem robust studi overcom use emul techniqu method allow scientist quick extract approxim posterior result correspond particular subject specif util flexibl method demonstr reanalysi real applic bayesian method employ captur belief river flow discuss obvious extens direct futur research approach open|['Ian Vernon', 'John Paul Gosling']|['stat.ME', 'stat.AP', 'stat.CO', 'stat.ML']
2017-04-07T11:24:50Z|2017-03-03T10:44:47Z|http://arxiv.org/abs/1703.01106v1|http://arxiv.org/pdf/1703.01106v1|Differentially Private Bayesian Learning on Distributed Data|differenti privat bayesian learn distribut data|Many applications of machine learning, for example in health care, would benefit from methods that can guarantee privacy of data subjects. Differential privacy (DP) has become established as a standard for protecting learning results, but the proposed algorithms require a single trusted party to have access to the entire data, which is a clear weakness. We consider DP Bayesian learning in a distributed setting, where each party only holds a single sample or a few samples of the data. We propose a novel method for DP learning in this distributed setting, based on a secure multi-party sum function for aggregating summaries from the data holders. Each data holder adds their share of Gaussian noise to make the total computation differentially private using the Gaussian mechanism. We prove that the system can be made secure against a desired number of colluding data owners and robust against faulting data owners. The method builds on an asymptotically optimal and practically efficient DP Bayesian inference with rapidly diminishing extra cost.|mani applic machin learn exampl health care would benefit method guarante privaci data subject differenti privaci dp becom establish standard protect learn result propos algorithm requir singl trust parti access entir data clear weak consid dp bayesian learn distribut set parti onli hold singl sampl sampl data propos novel method dp learn distribut set base secur multi parti sum function aggreg summari data holder data holder add share gaussian nois make total comput differenti privat use gaussian mechan prove system made secur desir number collud data owner robust fault data owner method build asymptot optim practic effici dp bayesian infer rapid diminish extra cost|['Mikko Heikkilä', 'Yusuke Okimoto', 'Samuel Kaski', 'Kana Shimizu', 'Antti Honkela']|['stat.ML', 'cs.CR', 'cs.LG', 'stat.CO']
2017-04-07T11:24:50Z|2017-03-02T17:33:58Z|http://arxiv.org/abs/1703.00864v1|http://arxiv.org/pdf/1703.00864v1|The Unreasonable Effectiveness of Random Orthogonal Embeddings|unreason effect random orthogon embed|We present a general class of embeddings based on structured random matrices with orthogonal rows which can be applied in many machine learning applications including dimensionality reduction, kernel approximation and locality-sensitive hashing. We show that this class yields improvements over previous state-of-the-art methods either in computational efficiency (while providing similar accuracy) or in accuracy, or both. In particular, we propose the \textit{Orthogonal Johnson-Lindenstrauss Transform} (OJLT) which is as fast as earlier methods yet provably outperforms them in terms of accuracy, leading to a `free lunch' improvement over previous dimensionality reduction mechanisms. We introduce matrices with complex entries that further improve accuracy. Other applications include estimators for certain pointwise nonlinear Gaussian kernels, and speed improvements for approximate nearest-neighbor search in massive datasets with high-dimensional feature vectors.|present general class embed base structur random matric orthogon row appli mani machin learn applic includ dimension reduct kernel approxim local sensit hash show class yield improv previous state art method either comput effici provid similar accuraci accuraci particular propos textit orthogon johnson lindenstrauss transform ojlt fast earlier method yet provabl outperform term accuraci lead free lunch improv previous dimension reduct mechan introduc matric complex entri improv accuraci applic includ estim certain pointwis nonlinear gaussian kernel speed improv approxim nearest neighbor search massiv dataset high dimension featur vector|['Krzysztof Choromanski', 'Mark Rowland', 'Adrian Weller']|['stat.ML', 'stat.CO']
2017-04-07T11:24:50Z|2017-03-19T16:03:04Z|http://arxiv.org/abs/1703.00368v2|http://arxiv.org/pdf/1703.00368v2|Approximate Computational Approaches for Bayesian Sensor Placement in   High Dimensions|approxim comput approach bayesian sensor placement high dimens|Since the cost of installing and maintaining sensors is usually high, sensor locations are always strategically selected. For those aiming at inferring certain quantities of interest (QoI), it is desirable to explore the dependency between sensor measurements and QoI. One of the most popular metric for the dependency is mutual information which naturally measures how much information about one variable can be obtained given the other. However, computing mutual information is always challenging, and the result is unreliable in high dimension. In this paper, we propose an approach to find an approximate lower bound of mutual information and compute it in a lower dimension. Then, sensors are placed where highest mutual information (lower bound) is achieved and QoI is inferred via Bayes rule given sensor measurements. In addition, Bayesian optimization is introduced to provide a continuous mutual information surface over the domain and thus reduce the number of evaluations. A chemical release accident is simulated where multiple sensors are placed to locate the source of the release. The result shows that the proposed approach is both effective and efficient in inferring QoI.|sinc cost instal maintain sensor usual high sensor locat alway strateg select aim infer certain quantiti interest qoi desir explor depend sensor measur qoi one popular metric depend mutual inform natur measur much inform one variabl obtain given howev comput mutual inform alway challeng result unreli high dimens paper propos approach find approxim lower bound mutual inform comput lower dimens sensor place highest mutual inform lower bound achiev qoi infer via bay rule given sensor measur addit bayesian optim introduc provid continu mutual inform surfac domain thus reduc number evalu chemic releas accid simul multipl sensor place locat sourc releas result show propos approach effect effici infer qoi|['Xiao Lin', 'Asif Chowdhury', 'Xiaofan Wang', 'Gabriel Terejanu']|['stat.CO']
2017-04-07T11:24:50Z|2017-03-13T17:28:04Z|http://arxiv.org/abs/1702.08896v2|http://arxiv.org/pdf/1702.08896v2|Deep and Hierarchical Implicit Models|deep hierarch implicit model|Implicit probabilistic models are a flexible class for modeling data. They define a process to simulate observations, and unlike traditional models, they do not require a tractable likelihood function. In this paper, we develop two families of models: hierarchical implicit models and deep implicit models. They combine the idea of implicit densities with hierarchical Bayesian modeling and deep neural networks. The use of implicit models with Bayesian analysis has been limited by our ability to perform accurate and scalable inference. We develop likelihood-free variational inference (LFVI). Key to LFVI is specifying a variational family that is also implicit. This matches the model's flexibility and allows for accurate approximation of the posterior. Our work scales up implicit models to sizes previously not possible and advances their modeling design. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for text generation.|implicit probabilist model flexibl class model data defin process simul observ unlik tradit model requir tractabl likelihood function paper develop two famili model hierarch implicit model deep implicit model combin idea implicit densiti hierarch bayesian model deep neural network use implicit model bayesian analysi limit abil perform accur scalabl infer develop likelihood free variat infer lfvi key lfvi specifi variat famili also implicit match model flexibl allow accur approxim posterior work scale implicit model size previous possibl advanc model design demonstr divers applic larg scale physic simul predat prey popul ecolog bayesian generat adversari network discret data deep implicit model text generat|['Dustin Tran', 'Rajesh Ranganath', 'David M. Blei']|['stat.ML', 'cs.LG', 'stat.CO', 'stat.ME']
2017-04-07T11:24:50Z|2017-02-28T16:31:21Z|http://arxiv.org/abs/1702.08849v1|http://arxiv.org/pdf/1702.08849v1|Multi-Sensor Multi-object Tracking with the Generalized Labeled   Multi-Bernoulli Filter|multi sensor multi object track general label multi bernoulli filter|This paper proposes an efficient implementation of the multi-sensor generalized labeled multi-Bernoulli (GLMB) filter. The solution exploits the GLMB joint prediction and update together with a new technique for truncating the GLMB filtering density based on Gibbs sampling. The resulting algorithm has quadratic complexity in the number of hypothesized object and linear in the number of measurements of each individual sensors.|paper propos effici implement multi sensor general label multi bernoulli glmb filter solut exploit glmb joint predict updat togeth new techniqu truncat glmb filter densiti base gibb sampl result algorithm quadrat complex number hypothes object linear number measur individu sensor|['Ba Ngu Vo', 'Ba Tuong Vo']|['stat.CO']
2017-04-07T11:24:50Z|2017-02-28T13:34:02Z|http://arxiv.org/abs/1702.08781v1|http://arxiv.org/pdf/1702.08781v1|General Bayesian inference schemes in infinite mixture models|general bayesian infer scheme infinit mixtur model|Bayesian statistical models allow us to formalise our knowledge about the world and reason about our uncertainty, but there is a need for better procedures to accurately encode its complexity. One way to do so is through compositional models, which are formed by combining blocks consisting of simpler models. One can increase the complexity of the compositional model by either stacking more blocks or by using a not-so-simple model as a building block. This thesis is an example of the latter. One first aim is to expand the choice of Bayesian nonparametric (BNP) blocks for constructing tractable compositional models. So far, most of the models that have a Bayesian nonparametric component use a Dirichlet Process or a Pitman-Yor process because of the availability of tractable and compact representations. This thesis shows how to overcome certain intractabilities in order to obtain analogous compact representations for the class of Poisson-Kingman priors which includes the Dirichlet and Pitman-Yor processes.   A major impediment to the widespread use of Bayesian nonparametric building blocks is that inference is often costly, intractable or difficult to carry out. This is an active research area since dealing with the model's infinite dimensional component forbids the direct use of standard simulation-based methods. The main contribution of this thesis is a variety of inference schemes that tackle this problem: Markov chain Monte Carlo and Sequential Monte Carlo methods, which are exact inference schemes since they target the true posterior. The contributions of this thesis, in a larger context, provide general purpose exact inference schemes in the flavour or probabilistic programming: the user is able to choose from a variety of models, focusing only on the modelling part. Indeed, if the wide enough class of Poisson-Kingman priors is used as one of our blocks, this objective is achieved.|bayesian statist model allow us formalis knowledg world reason uncertainti need better procedur accur encod complex one way composit model form combin block consist simpler model one increas complex composit model either stack block use simpl model build block thesi exampl latter one first aim expand choic bayesian nonparametr bnp block construct tractabl composit model far model bayesian nonparametr compon use dirichlet process pitman yor process becaus avail tractabl compact represent thesi show overcom certain intract order obtain analog compact represent class poisson kingman prior includ dirichlet pitman yor process major impedi widespread use bayesian nonparametr build block infer often cost intract difficult carri activ research area sinc deal model infinit dimension compon forbid direct use standard simul base method main contribut thesi varieti infer scheme tackl problem markov chain mont carlo sequenti mont carlo method exact infer scheme sinc target true posterior contribut thesi larger context provid general purpos exact infer scheme flavour probabilist program user abl choos varieti model focus onli model part inde wide enough class poisson kingman prior use one block object achiev|['Maria Lomeli']|['stat.CO']
2017-04-07T11:24:50Z|2017-02-28T11:02:04Z|http://arxiv.org/abs/1702.08738v1|http://arxiv.org/pdf/1702.08738v1|Efficient simulation of high dimensional Gaussian vectors|effici simul high dimension gaussian vector|We describe a Markov chain Monte Carlo method to approximately simulate a centered d-dimensional Gaussian vector X with given covariance matrix. The standard Monte Carlo method is based on the Cholesky decomposition, which takes cubic time and has quadratic storage cost in d. In contrast, the storage cost of our algorithm is linear in d. We give a bound on the quadractic Wasserstein distance between the distribution of our sample and the target distribution. Our method can be used to estimate the expectation of h(X), where h is a real-valued function of d variables. Under certain conditions, we show that the mean square error of our method is inversely proportional to its running time. We also prove that, under suitable conditions, our method is faster than the standard Monte Carlo method by a factor nearly proportional to d. A numerical example is given.|describ markov chain mont carlo method approxim simul center dimension gaussian vector given covari matrix standard mont carlo method base choleski decomposit take cubic time quadrat storag cost contrast storag cost algorithm linear give bound quadract wasserstein distanc distribut sampl target distribut method use estim expect real valu function variabl certain condit show mean squar error method invers proport run time also prove suitabl condit method faster standard mont carlo method factor near proport numer exampl given|['Nabil Kahale']|['stat.CO', '60J22, 65C40']
2017-04-07T11:24:50Z|2017-02-27T22:52:29Z|http://arxiv.org/abs/1702.08572v1|http://arxiv.org/pdf/1702.08572v1|Comparison of Confidence Interval Estimators: an Index Approach|comparison confid interv estim index approach|We develop a confidence interval index for comparing confidence interval estimators based on the confidence interval length and coverage probability. We show that the confidence interval index has range of values within the neighborhood of the range of the coverage probability, [0,1]. In addition, a good confidence interval estimator is shown to have an index value approaching 1; and a bad confidence interval has an index value approaching 0. A simulation study is conducted to assess the finite sample performance of the index. Finally, the proposed index is illustrated with a practical example from the literature.|develop confid interv index compar confid interv estim base confid interv length coverag probabl show confid interv index rang valu within neighborhood rang coverag probabl addit good confid interv estim shown index valu approach bad confid interv index valu approach simul studi conduct assess finit sampl perform index final propos index illustr practic exampl literatur|['Richard Minkah', 'Tertius de Wet']|['stat.ME', 'stat.CO', '62F99, 62G99']
2017-04-07T11:24:53Z|2017-02-27T17:43:59Z|http://arxiv.org/abs/1702.08397v1|http://arxiv.org/pdf/1702.08397v1|Forward Event-Chain Monte Carlo: a general rejection-free and   irreversible Markov chain simulation method|forward event chain mont carlo general reject free irrevers markov chain simul method|This paper considers Event-Chain Monte Carlo simulation schemes in order to design an original irreversible Markov Chain Monte Carlo (MCMC) algorithm for the sampling of complex statistical models. The functioning principles of MCMC sampling methods are firstly recalled, as well as standard Event-Chain Monte Carlo simulation schemes are described. Then, a Forward Event-Chain Monte Carlo sampling methodology is proposed and introduced. This nonreversible MCMC rejection-free simulation algorithm is tested and run for the sampling of high-dimensional ill-conditioned Gaussian statistical distributions. Numerical experiments demonstrate the efficiency of the proposed approach, compared to standard Event-Chain and standard Monte Carlo sampling methods. Accelerations up to several magnitudes are exhibited.|paper consid event chain mont carlo simul scheme order design origin irrevers markov chain mont carlo mcmc algorithm sampl complex statist model function principl mcmc sampl method first recal well standard event chain mont carlo simul scheme describ forward event chain mont carlo sampl methodolog propos introduc nonrevers mcmc reject free simul algorithm test run sampl high dimension ill condit gaussian statist distribut numer experi demonstr effici propos approach compar standard event chain standard mont carlo sampl method acceler sever magnitud exhibit|['Manon Michel', 'Stéphane Sénécal']|['stat.CO']
2017-04-07T11:24:53Z|2017-02-27T12:12:46Z|http://arxiv.org/abs/1702.08251v1|http://arxiv.org/pdf/1702.08251v1|Hessian corrections to Hybrid Monte Carlo|hessian correct hybrid mont carlo|A method for the introduction of second-order derivatives of the log likelihood into HMC algorithms is introduced, which does not require the Hessian to be evaluated at each leapfrog step but only at the start and end of trajectories.|method introduct second order deriv log likelihood hmc algorithm introduc doe requir hessian evalu leapfrog step onli start end trajectori|['Thomas House']|['stat.CO']
2017-04-07T11:24:53Z|2017-02-27T12:03:01Z|http://arxiv.org/abs/1702.08248v1|http://arxiv.org/pdf/1702.08248v1|Scalable and Distributed Clustering via Lightweight Coresets|scalabl distribut cluster via lightweight coreset|Coresets are compact representations of data sets such that models trained on a coreset are provably competitive with models trained on the full data set. As such, they have been successfully used to scale up clustering models to massive data sets. While existing approaches generally only allow for multiplicative approximation errors, we propose a novel notion of coresets called lightweight coresets that allows for both multiplicative and additive errors. We provide a single algorithm to construct light-weight coresets for k-Means clustering, Bregman clustering and maximum likelihood estimation of Gaussian mixture models. The algorithm is substantially faster than existing constructions, embarrassingly parallel and resulting coresets are smaller. In an extensive experimental evaluation, we demonstrate that the proposed method outperforms existing coreset constructions.|coreset compact represent data set model train coreset provabl competit model train full data set success use scale cluster model massiv data set exist approach general onli allow multipl approxim error propos novel notion coreset call lightweight coreset allow multipl addit error provid singl algorithm construct light weight coreset mean cluster bregman cluster maximum likelihood estim gaussian mixtur model algorithm substanti faster exist construct embarrass parallel result coreset smaller extens experiment evalu demonstr propos method outperform exist coreset construct|['Olivier Bachem', 'Mario Lucic', 'Andreas Krause']|['stat.ML', 'cs.DC', 'cs.DS', 'cs.LG', 'stat.CO']
2017-04-07T11:24:53Z|2017-02-27T08:42:49Z|http://arxiv.org/abs/1702.08188v1|http://arxiv.org/pdf/1702.08188v1|dotCall64: An Efficient Interface to Compiled C/C++ and Fortran Code   Supporting Long Vectors|dotcal effici interfac compil fortran code support long vector|The R functions .C() and .Fortran() can be used to call compiled C/C++ and Fortran code from R. This so-called foreign function interface is convenient, since it does not require any interactions with the C API of R. However, it does not support long vectors (i.e., vectors of more than 2^31 elements). To overcome this limitation, the R package dotCall64 provides .C64(), which can be used to call compiled C/C++ and Fortran functions. It transparently supports long vectors and does the necessary castings to pass numeric R vectors to 64-bit integer arguments of the compiled code. Moreover, .C64() features a mechanism to avoid unnecessary copies of function arguments, making it efficient in terms of speed and memory usage.|function fortran use call compil fortran code call foreign function interfac conveni sinc doe requir ani interact api howev doe support long vector vector element overcom limit packag dotcal provid use call compil fortran function transpar support long vector doe necessari cast pass numer vector bit integ argument compil code moreov featur mechan avoid unnecessari copi function argument make effici term speed memori usag|['Florian Gerber', 'Kaspar Mösinger', 'Reinhard Furrer']|['stat.CO']
2017-04-07T11:24:53Z|2017-02-27T08:33:26Z|http://arxiv.org/abs/1702.08185v1|http://arxiv.org/pdf/1702.08185v1|An update on statistical boosting in biomedicine|updat statist boost biomedicin|Statistical boosting algorithms have triggered a lot of research during the last decade. They combine a powerful machine-learning approach with classical statistical modelling, offering various practical advantages like automated variable selection and implicit regularization of effect estimates. They are extremely flexible, as the underlying base-learners (regression functions defining the type of effect for the explanatory variables) can be combined with any kind of loss function (target function to be optimized, defining the type of regression setting). In this review article, we highlight the most recent methodological developments on statistical boosting regarding variable selection, functional regression and advanced time-to-event modelling. Additionally, we provide a short overview on relevant applications of statistical boosting in biomedicine.|statist boost algorithm trigger lot research dure last decad combin power machin learn approach classic statist model offer various practic advantag like autom variabl select implicit regular effect estim extrem flexibl base learner regress function defin type effect explanatori variabl combin ani kind loss function target function optim defin type regress set review articl highlight recent methodolog develop statist boost regard variabl select function regress advanc time event model addit provid short overview relev applic statist boost biomedicin|['Andreas Mayr', 'Benjamin Hofner', 'Elisabeth Waldmann', 'Tobias Hepp', 'Olaf Gefeller', 'Matthias Schmid']|['stat.AP', 'stat.CO', 'stat.ML']
2017-04-07T11:24:53Z|2017-02-27T04:17:36Z|http://arxiv.org/abs/1702.08140v1|http://arxiv.org/pdf/1702.08140v1|A mixture model approach to infer land-use influence on point referenced   water quality|mixtur model approach infer land use influenc point referenc water qualiti|The assessment of water quality across space and time is of considerable interest for both agricultural and public health reasons. The standard method to assess the water quality of a catchment, or a group of catchments, usually involves collecting point measurements of water quality and other additional information such as the date and time of measurements, rainfall amounts, the land-use and soil-type of the catchment and the elevation. Some of this auxiliary information will be point data, measured at the exact location, whereas other such as land-use will be areal data often in a compositional format. Two problems arise if analysts try to incorporate this information into a statistical model in order to predict (for example) the influence of land-use on water quality. First is the spatial change of support problem that arises when using areal data to predict outcomes at point locations. Secondly, the physical process driving water quality is not compositional, rather it is the observation process that provides compositional data. In this paper we present an approach that accounts for these two issues by using a latent variable to identify the land-use that most likely influences water quality. This latent variable is used in a spatial mixture model to help estimate the influence of land-use on water quality. We demonstrate the potential of this approach with data from a water quality research study in the Mount Lofty range, in South Australia.|assess water qualiti across space time consider interest agricultur public health reason standard method assess water qualiti catchment group catchment usual involv collect point measur water qualiti addit inform date time measur rainfal amount land use soil type catchment elev auxiliari inform point data measur exact locat wherea land use areal data often composit format two problem aris analyst tri incorpor inform statist model order predict exampl influenc land use water qualiti first spatial chang support problem aris use areal data predict outcom point locat second physic process drive water qualiti composit rather observ process provid composit data paper present approach account two issu use latent variabl identifi land use like influenc water qualiti latent variabl use spatial mixtur model help estim influenc land use water qualiti demonstr potenti approach data water qualiti research studi mount lofti rang south australia|['Adrien Ickowicz', 'Jessica H. Ford', 'Keith R. Hayes']|['stat.AP', 'stat.CO']
2017-04-07T11:24:53Z|2017-02-28T09:59:57Z|http://arxiv.org/abs/1702.08061v2|http://arxiv.org/pdf/1702.08061v2|The Ensemble Kalman Filter: A Signal Processing Perspective|ensembl kalman filter signal process perspect|The ensemble Kalman filter (EnKF) is a Monte Carlo based implementation of the Kalman filter (KF) for extremely high-dimensional, possibly nonlinear and non-Gaussian state estimation problems. Its ability to handle state dimensions in the order of millions has made the EnKF a popular algorithm in different geoscientific disciplines. Despite a similarly vital need for scalable algorithms in signal processing, e.g., to make sense of the ever increasing amount of sensor data, the EnKF is hardly discussed in our field.   This self-contained review paper is aimed at signal processing researchers and provides all the knowledge to get started with the EnKF. The algorithm is derived in a KF framework, without the often encountered geoscientific terminology. Algorithmic challenges and required extensions of the EnKF are provided, as well as relations to sigma-point KF and particle filters. The relevant EnKF literature is summarized in an extensive survey and unique simulation examples, including popular benchmark problems, complement the theory with practical insights. The signal processing perspective highlights new directions of research and facilitates the exchange of potentially beneficial ideas, both for the EnKF and high-dimensional nonlinear and non-Gaussian filtering in general.|ensembl kalman filter enkf mont carlo base implement kalman filter kf extrem high dimension possibl nonlinear non gaussian state estim problem abil handl state dimens order million made enkf popular algorithm differ geoscientif disciplin despit similar vital need scalabl algorithm signal process make sens ever increas amount sensor data enkf hard discuss field self contain review paper aim signal process research provid knowledg get start enkf algorithm deriv kf framework without often encount geoscientif terminolog algorithm challeng requir extens enkf provid well relat sigma point kf particl filter relev enkf literatur summar extens survey uniqu simul exampl includ popular benchmark problem complement theori practic insight signal process perspect highlight new direct research facilit exchang potenti benefici idea enkf high dimension nonlinear non gaussian filter general|['Michael Roth', 'Gustaf Hendeby', 'Carsten Fritsche', 'Fredrik Gustafsson']|['stat.ME', 'cs.SY', 'stat.CO']
2017-04-07T11:24:53Z|2017-02-26T17:50:02Z|http://arxiv.org/abs/1702.08446v1|http://arxiv.org/pdf/1702.08446v1|Monte Carlo on manifolds: sampling densities and integrating functions|mont carlo manifold sampl densiti integr function|We describe and analyze some Monte Carlo methods for manifolds in Euclidean space defined by equality and inequality constraints. First, we give an MCMC sampler for probability distributions defined by un-normalized densities on such manifolds. The sampler uses a specific orthogonal projection to the surface that requires only information about the tangent space to the manifold, obtainable from first derivatives of the constraint functions, hence avoiding the need for curvature information or second derivatives. Second, we use the sampler to develop a multi-stage algorithm to compute integrals over such manifolds. We provide single-run error estimates that avoid the need for multiple independent runs. Computational experiments on various test problems show that the algorithms and error estimates work in practice. The method is applied to compute the entropies of different sticky hard sphere systems. These predict the temperature or interaction energy at which loops of hard sticky spheres become preferable to chains.|describ analyz mont carlo method manifold euclidean space defin equal inequ constraint first give mcmc sampler probabl distribut defin un normal densiti manifold sampler use specif orthogon project surfac requir onli inform tangent space manifold obtain first deriv constraint function henc avoid need curvatur inform second deriv second use sampler develop multi stage algorithm comput integr manifold provid singl run error estim avoid need multipl independ run comput experi various test problem show algorithm error estim work practic method appli comput entropi differ sticki hard sphere system predict temperatur interact energi loop hard sticki sphere becom prefer chain|['Emilio Zappa', 'Miranda Holmes-Cerfon', 'Jonathan Goodman']|['math.NA', 'cond-mat.stat-mech', 'stat.CO']
2017-04-07T11:24:53Z|2017-02-25T17:47:33Z|http://arxiv.org/abs/1702.07930v1|http://arxiv.org/pdf/1702.07930v1|Upper-Bounding the Regularization Constant for Convex Sparse Signal   Reconstruction|upper bound regular constant convex spars signal reconstruct|Consider reconstructing a signal $x$ by minimizing a weighted sum of a convex differentiable negative log-likelihood (NLL) (data-fidelity) term and a convex regularization term that imposes a convex-set constraint on $x$ and enforces its sparsity using $\ell_1$-norm analysis regularization. We compute upper bounds on the regularization tuning constant beyond which the regularization term overwhelmingly dominates the NLL term so that the set of minimum points of the objective function does not change. Necessary and sufficient conditions for irrelevance of sparse signal regularization and a condition for the existence of finite upper bounds are established. We formulate an optimization problem for finding these bounds when the regularization term can be globally minimized by a feasible $x$ and also develop an alternating direction method of multipliers (ADMM) type method for their computation. Simulation examples show that the derived and empirical bounds match.|consid reconstruct signal minim weight sum convex differenti negat log likelihood nll data fidel term convex regular term impos convex set constraint enforc sparsiti use ell norm analysi regular comput upper bound regular tune constant beyond regular term overwhelm domin nll term set minimum point object function doe chang necessari suffici condit irrelev spars signal regular condit exist finit upper bound establish formul optim problem find bound regular term global minim feasibl also develop altern direct method multipli admm type method comput simul exampl show deriv empir bound match|['Renliang Gu', 'Aleksandar Dogandžić']|['stat.CO', 'math.OC']
2017-04-07T11:24:53Z|2017-02-25T03:46:20Z|http://arxiv.org/abs/1702.07830v1|http://arxiv.org/pdf/1702.07830v1|A Near-Optimal Sampling Strategy for Sparse Recovery of Polynomial Chaos   Expansions|near optim sampl strategi spars recoveri polynomi chao expans|Compressive sampling has become a widely used approach to construct polynomial chaos surrogates when the number of available simulation samples is limited. Originally, these expensive simulation samples would be obtained at random locations in the parameter space. It was later shown that the choice of sample locations could significantly impact the accuracy of resulting surrogates. This motivated new sampling strategies or design-of-experiment approaches, such as coherence-optimal sampling, which aim at improving the coherence property. In this paper, we propose a sampling strategy that can identify near-optimal sample locations that lead to improvement in local-coherence property and also enhancement of cross-correlation properties of measurement matrices. We provide theoretical motivations for the proposed sampling strategy along with several numerical examples that show that our near-optimal sampling strategy produces substantially more accurate results, compared to other sampling strategies.|compress sampl becom wide use approach construct polynomi chao surrog number avail simul sampl limit origin expens simul sampl would obtain random locat paramet space later shown choic sampl locat could signific impact accuraci result surrog motiv new sampl strategi design experi approach coher optim sampl aim improv coher properti paper propos sampl strategi identifi near optim sampl locat lead improv local coher properti also enhanc cross correl properti measur matric provid theoret motiv propos sampl strategi along sever numer exampl show near optim sampl strategi produc substanti accur result compar sampl strategi|['Negin Alemazkoor', 'Hadi Meidani']|['stat.CO']
2017-04-07T11:24:57Z|2017-02-24T17:54:23Z|http://arxiv.org/abs/1702.07685v1|http://arxiv.org/pdf/1702.07685v1|ROPE: high-dimensional network modeling with robust control of edge FDR|rope high dimension network model robust control edg fdr|Network modeling has become increasingly popular for analyzing genomic data, to aid in the interpretation and discovery of possible mechanistic components and therapeutic targets. However, genomic-scale networks are high-dimensional models and are usually estimated from a relatively small number of samples. Therefore, their usefulness is hampered by estimation instability. In addition, the complexity of the models is controlled by one or more penalization (tuning) parameters where small changes to these can lead to vastly different networks, thus making interpretation of models difficult. This necessitates the development of techniques to produce robust network models accompanied by estimation quality assessments.   We introduce Resampling of Penalized Estimates (ROPE): a novel statistical method for robust network modeling. The method utilizes resampling-based network estimation and integrates results from several levels of penalization through a constrained, over-dispersed beta-binomial mixture model. ROPE provides robust False Discovery Rate (FDR) control of network estimates and each edge is assigned a measure of validity, the q-value, corresponding to the FDR-level for which the edge would be included in the network model. We apply ROPE to several simulated data sets as well as genomic data from The Cancer Genome Atlas. We show that ROPE outperforms state-of-the-art methods in terms of FDR control and robust performance across data sets. We illustrate how to use ROPE to make a principled model selection for which genomic associations to study further. ROPE is available as an R package on CRAN.|network model becom increas popular analyz genom data aid interpret discoveri possibl mechanist compon therapeut target howev genom scale network high dimension model usual estim relat small number sampl therefor use hamper estim instabl addit complex model control one penal tune paramet small chang lead vast differ network thus make interpret model difficult necessit develop techniqu produc robust network model accompani estim qualiti assess introduc resampl penal estim rope novel statist method robust network model method util resampl base network estim integr result sever level penal constrain dispers beta binomi mixtur model rope provid robust fals discoveri rate fdr control network estim edg assign measur valid valu correspond fdr level edg would includ network model appli rope sever simul data set well genom data cancer genom atlas show rope outperform state art method term fdr control robust perform across data set illustr use rope make principl model select genom associ studi rope avail packag cran|['Jonatan Kallus', 'Jose Sanchez', 'Alexandra Jauhiainen', 'Sven Nelander', 'Rebecka Jörnsten']|['stat.CO']
2017-04-07T11:24:57Z|2017-02-24T17:01:59Z|http://arxiv.org/abs/1702.07662v1|http://arxiv.org/pdf/1702.07662v1|A Network Epidemic Model for Online Community Commissioning Data|network epidem model onlin communiti commiss data|Statistical models for network epidemics usually assume a Bernoulli random graph, in which any two nodes have the same probability of being connected. This assumption provides computational simplicity but does not describe real-life networks well. We propose an epidemic model based on the preferential attachment model, which adds nodes sequentially by simple rules to generate a network. A simulation study based on the subsequent Markov Chain Monte Carlo algorithm reveals an identifiability issue with the model parameters, so an alternative parameterisation is suggested. Finally, the model is applied to a set of online commissioning data.|statist model network epidem usual assum bernoulli random graph ani two node probabl connect assumpt provid comput simplic doe describ real life network well propos epidem model base preferenti attach model add node sequenti simpl rule generat network simul studi base subsequ markov chain mont carlo algorithm reveal identifi issu model paramet altern parameteris suggest final model appli set onlin commiss data|['Clement Lee', 'Andrew Garbett', 'Darren J. Wilkinson']|['stat.CO', 'cs.SI', 'stat.ME']
2017-04-07T11:24:57Z|2017-02-23T21:37:06Z|http://arxiv.org/abs/1702.07400v1|http://arxiv.org/pdf/1702.07400v1|Horseshoe Regularization for Feature Subset Selection|horsesho regular featur subset select|Feature subset selection arises in many high-dimensional applications in machine learning and statistics, such as compressed sensing and genomics. The $\ell_0$ penalty is ideal for this task, the caveat being it requires the NP-hard combinatorial evaluation of all models. A recent area of considerable interest is to develop efficient algorithms to fit models with a non-convex $\ell_\gamma$ penalty for $\gamma\in (0,1)$, which results in sparser models than the convex $\ell_1$ or lasso penalty, but is harder to fit. We propose an alternative, termed the horseshoe regularization penalty for feature subset selection, and demonstrate its theoretical and computational advantages. The distinguishing feature from existing non-convex optimization approaches is a full probabilistic representation of the penalty as the negative of the logarithm of a suitable prior, which in turn enables an efficient expectation-maximization algorithm for optimization and MCMC for uncertainty quantification. In synthetic and real data, the resulting algorithm provides better statistical performance, and the computation requires a fraction of time of state of the art non-convex solvers.|featur subset select aris mani high dimension applic machin learn statist compress sens genom ell penalti ideal task caveat requir np hard combinatori evalu model recent area consider interest develop effici algorithm fit model non convex ell gamma penalti gamma result sparser model convex ell lasso penalti harder fit propos altern term horsesho regular penalti featur subset select demonstr theoret comput advantag distinguish featur exist non convex optim approach full probabilist represent penalti negat logarithm suitabl prior turn enabl effici expect maxim algorithm optim mcmc uncertainti quantif synthet real data result algorithm provid better statist perform comput requir fraction time state art non convex solver|['Anindya Bhadra', 'Jyotishka Datta', 'Nicholas G. Polson', 'Brandon Willard']|['stat.ML', 'stat.CO']
2017-04-07T11:24:57Z|2017-02-23T04:40:41Z|http://arxiv.org/abs/1702.07094v1|http://arxiv.org/pdf/1702.07094v1|BigVAR: Tools for Modeling Sparse High-Dimensional Multivariate Time   Series|bigvar tool model spars high dimension multivari time seri|The R package BigVAR allows for the simultaneous estimation of high-dimensional time series by applying structured penalties to the conventional vector autoregression (VAR) and vector autoregression with exogenous variables (VARX) frameworks. Our methods can be utilized in many forecasting applications that make use of time-dependent data such as macroeconomics, finance, and internet traffic. Our package extends solution algorithms from the machine learning and signal processing literatures to a time dependent setting: selecting the regularization parameter by sequential cross validation and provides substantial improvements in forecasting performance over conventional methods. We offer a user-friendly interface that utilizes R's s4 object class structure which makes our methodology easily accessible to practicioners.   In this paper, we present an overview of our notation, the models that comprise BigVAR, and the functionality of our package with a detailed example using publicly available macroeconomic data. In addition, we present a simulation study comparing the performance of several procedures that refit the support selected by a BigVAR procedure according to several variants of least squares and conclude that refitting generally degrades forecast performance.|packag bigvar allow simultan estim high dimension time seri appli structur penalti convent vector autoregress var vector autoregress exogen variabl varx framework method util mani forecast applic make use time depend data macroeconom financ internet traffic packag extend solut algorithm machin learn signal process literatur time depend set select regular paramet sequenti cross valid provid substanti improv forecast perform convent method offer user friend interfac util object class structur make methodolog easili access practicion paper present overview notat model compris bigvar function packag detail exampl use public avail macroeconom data addit present simul studi compar perform sever procedur refit support select bigvar procedur accord sever variant least squar conclud refit general degrad forecast perform|['William Nicholson', 'David Matteson', 'Jacob Bien']|['stat.CO']
2017-04-07T11:24:57Z|2017-02-22T00:43:01Z|http://arxiv.org/abs/1702.06632v1|http://arxiv.org/pdf/1702.06632v1|A Balanced Algorithm for Sampling Abstract Simplicial Complexes|balanc algorithm sampl abstract simplici complex|We provide an algorithm for sampling the space of abstract simplicial complexes on a fixed number of vertices that aims to provide a balanced sampling over non-isomorphic complexes. Although sampling uniformly from geometrically distinct complexes is a difficult task with no known analytic algorithm, our generative and descriptive algorithm is designed with heuristics to help balance the combinatorial multiplicities of the states and more widely sample across the space of inequivalent configurations. We provide a formula for the exact probabilities with which this algorithm will produce a requested labeled state, and compare the algorithm to Kahle's multi-parameter model of exponential random simplicial complexes, demonstrating analytically that our algorithm performs better with respect to worst-case probability bounds on a given complex and providing numerical results illustrating the increased sampling efficiency over distinct classes.|provid algorithm sampl space abstract simplici complex fix number vertic aim provid balanc sampl non isomorph complex although sampl uniform geometr distinct complex difficult task known analyt algorithm generat descript algorithm design heurist help balanc combinatori multipl state wide sampl across space inequival configur provid formula exact probabl algorithm produc request label state compar algorithm kahl multi paramet model exponenti random simplici complex demonstr analyt algorithm perform better respect worst case probabl bound given complex provid numer result illustr increas sampl effici distinct class|['John Lombard']|['stat.CO', 'math.CO', 'math.PR']
2017-04-07T11:24:57Z|2017-02-23T19:01:53Z|http://arxiv.org/abs/1702.06488v2|http://arxiv.org/pdf/1702.06488v2|Distributed Estimation of Principal Eigenspaces|distribut estim princip eigenspac|"Principal component analysis (PCA) is fundamental to statistical machine learning. It extracts latent principal factors that contribute to the most variation of the data. When data are stored across multiple machines, however, communication cost can prohibit the computation of PCA in a central location and distributed algorithms for PCA are thus needed. This paper proposes and studies a distributed PCA algorithm: each node machine computes the top $K$ eigenvectors and transmits them to the central server; the central server then aggregates the information from all the node machines and conducts a PCA based on the aggregated information. We investigate the bias and variance for the resulting distributed estimator of the top $K$ eigenvectors. In particular, we show that for distributions with symmetric innovation, the distributed PCA is ""unbiased"". We derive the rate of convergence for distributed PCA estimators, which depends explicitly on the effective rank of covariance, eigen-gap, and the number of machines. We show that when the number of machines is not unreasonably large, the distributed PCA performs as well as the whole sample PCA, even without full access of whole data. The theoretical results are verified by an extensive simulation study. We also extend our analysis to the heterogeneous case where the population covariance matrices are different across local machines but share similar top eigen-structures."|princip compon analysi pca fundament statist machin learn extract latent princip factor contribut variat data data store across multipl machin howev communic cost prohibit comput pca central locat distribut algorithm pca thus need paper propos studi distribut pca algorithm node machin comput top eigenvector transmit central server central server aggreg inform node machin conduct pca base aggreg inform investig bias varianc result distribut estim top eigenvector particular show distribut symmetr innov distribut pca unbias deriv rate converg distribut pca estim depend explicit effect rank covari eigen gap number machin show number machin unreason larg distribut pca perform well whole sampl pca even without full access whole data theoret result verifi extens simul studi also extend analysi heterogen case popul covari matric differ across local machin share similar top eigen structur|['Jianqing Fan', 'Dong Wang', 'Kaizheng Wang', 'Ziwei Zhu']|['stat.CO', 'math.ST', 'stat.TH']
2017-04-07T11:24:57Z|2017-02-23T14:07:34Z|http://arxiv.org/abs/1702.06407v2|http://arxiv.org/pdf/1702.06407v2|General Semiparametric Shared Frailty Model Estimation and Simulation   with frailtySurv|general semiparametr share frailti model estim simul frailtysurv|The R package frailtySurv for simulating and fitting semi-parametric shared frailty models is introduced. frailtySurv implements semi-parametric consistent estimators for a variety of frailty distributions, including gamma, log-normal, inverse Gaussian and power variance function, and provides consistent estimators of the standard errors of the parameters' estimators. The parameters' estimators are asymptotically normally distributed, and therefore statistical inference based on the results of this package, such as hypothesis testing and confidence intervals, can be performed using the normal distribution. Extensive simulations demonstrate the flexibility and correct implementation of the estimator. Two case studies performed with publicly-available datasets demonstrate applicability of the package. In the Diabetic Retinopathy Study, the onset of blindness is clustered by patient, and in a large hard drive failure dataset, failure times are thought to be clustered by the hard drive manufacturer and model.|packag frailtysurv simul fit semi parametr share frailti model introduc frailtysurv implement semi parametr consist estim varieti frailti distribut includ gamma log normal invers gaussian power varianc function provid consist estim standard error paramet estim paramet estim asymptot normal distribut therefor statist infer base result packag hypothesi test confid interv perform use normal distribut extens simul demonstr flexibl correct implement estim two case studi perform public avail dataset demonstr applic packag diabet retinopathi studi onset blind cluster patient larg hard drive failur dataset failur time thought cluster hard drive manufactur model|['John V. Monaco', 'Malka Gorfine', 'Li Hsu']|['stat.CO', 'cs.MS']
2017-04-07T11:24:57Z|2017-03-20T19:49:02Z|http://arxiv.org/abs/1702.05698v2|http://arxiv.org/pdf/1702.05698v2|Online Robust Principal Component Analysis with Change Point Detection|onlin robust princip compon analysi chang point detect|Robust PCA methods are typically batch algorithms which requires loading all observations into memory before processing. This makes them inefficient to process big data. In this paper, we develop an efficient online robust principal component methods, namely online moving window robust principal component analysis (OMWRPCA). Unlike existing algorithms, OMWRPCA can successfully track not only slowly changing subspace but also abruptly changed subspace. By embedding hypothesis testing into the algorithm, OMWRPCA can detect change points of the underlying subspaces. Extensive simulation studies demonstrate the superior performance of OMWRPCA compared with other state-of-art approaches. We also apply the algorithm for real-time background subtraction of surveillance video.|robust pca method typic batch algorithm requir load observ memori befor process make ineffici process big data paper develop effici onlin robust princip compon method name onlin move window robust princip compon analysi omwrpca unlik exist algorithm omwrpca success track onli slowli chang subspac also abrupt chang subspac embed hypothesi test algorithm omwrpca detect chang point subspac extens simul studi demonstr superior perform omwrpca compar state art approach also appli algorithm real time background subtract surveil video|['Wei Xiao', 'Xiaolin Huang', 'Jorge Silva', 'Saba Emrani', 'Arin Chaudhuri']|['cs.LG', 'cs.CV', 'stat.AP', 'stat.CO', 'stat.ML']
2017-04-07T11:24:57Z|2017-02-18T00:04:25Z|http://arxiv.org/abs/1702.05546v1|http://arxiv.org/pdf/1702.05546v1|A Sequential Scheme for Large Scale Bayesian Multiple Testing|sequenti scheme larg scale bayesian multipl test|The problem of large scale multiple testing arises in many contexts, including testing for pairwise interaction among large numbers of neurons. With advances in technologies, it has become common to record from hundreds of neurons simultaneously, and this number is growing quickly, so that the number of pairwise tests can be very large. It is important to control the rate at which false positives occur. In addition, there is sometimes information that affects the probability of a positive result for any given pair. In the case of neurons, they are more likely to have correlated activity when they are close together, and when they respond similarly to various stimuli. Recently a method was developed to control false positives when covariate information, such as distances between pairs of neurons, is available. This method, however, relies on computationally-intensive Markov Chain Monte Carlo (MCMC). Here we develop an alternative, based on Sequential Monte Carlo, which scales well with the size of the dataset. This scheme considers data items sequentially, with relevant probabilities being updated at each step. Simulation experiments demonstrate that the proposed algorithm delivers results as accurately as the previous MCMC method with only a single pass through the data. We illustrate the method by using it to analyze neural recordings from extrastriate cortex in a macaque monkey. The scripts that implement the proposed algorithm with a synthetic dataset are available online at: https://github.com/robinlau1981/SMC_Multi_Testing.|problem larg scale multipl test aris mani context includ test pairwis interact among larg number neuron advanc technolog becom common record hundr neuron simultan number grow quick number pairwis test veri larg import control rate fals posit occur addit sometim inform affect probabl posit result ani given pair case neuron like correl activ close togeth respond similar various stimuli recent method develop control fals posit covari inform distanc pair neuron avail method howev reli comput intens markov chain mont carlo mcmc develop altern base sequenti mont carlo scale well size dataset scheme consid data item sequenti relev probabl updat step simul experi demonstr propos algorithm deliv result accur previous mcmc method onli singl pass data illustr method use analyz neural record extrastri cortex macaqu monkey script implement propos algorithm synthet dataset avail onlin https github com robinlau smc multi test|['Bin Liu', 'Giuseppe Vinci', 'Adam C. Snyder', 'Matthew A. Smith', 'Robert E. Kass']|['stat.CO']
2017-04-07T11:24:57Z|2017-02-17T21:16:40Z|http://arxiv.org/abs/1702.05518v1|http://arxiv.org/pdf/1702.05518v1|Sampling Strategies for Fast Updating of Gaussian Markov Random Fields|sampl strategi fast updat gaussian markov random field|Gaussian Markov random fields (GMRFs) are popular for modeling temporal or spatial dependence in large areal datasets due to their ease of interpretation and computational convenience afforded by conditional independence and their sparse precision matrices needed for random variable generation. Using such models inside a Markov chain Monte Carlo algorithm requires repeatedly simulating random fields. This is a nontrivial issue, especially when the full conditional precision matrix depends on parameters that change at each iteration. Typically in Bayesian computation, GMRFs are updated jointly in a block Gibbs sampler or one location at a time in a single-site sampler. The former approach leads to quicker convergence by updating correlated variables all at once, while the latter avoids solving large matrices. Efficient algorithms for sampling Markov random fields have become the focus of much recent research in the machine learning literature, much of which can be useful to statisticians. We briefly review recently proposed approaches with an eye toward implementation for statisticians without expertise in numerical analysis or advanced computing. In particular, we consider a version of block sampling in which the underlying graph can be cut so that conditionally independent sites are all updated together. This algorithm allows a practitioner to parallelize the updating of a subset locations or to take advantage of `vectorized' calculations in a high-level language such as R. Through both simulation and real data application, we demonstrate computational savings that can be achieved versus both traditional single-site updating and block updating, regardless of whether the data are on a regular or irregular lattice. We argue that this easily-implemented sampling routine provides a good compromise between statistical and computational efficiency when working with large datasets.|gaussian markov random field gmrfs popular model tempor spatial depend larg areal dataset due eas interpret comput conveni afford condit independ spars precis matric need random variabl generat use model insid markov chain mont carlo algorithm requir repeat simul random field nontrivi issu especi full condit precis matrix depend paramet chang iter typic bayesian comput gmrfs updat joint block gibb sampler one locat time singl site sampler former approach lead quicker converg updat correl variabl onc latter avoid solv larg matric effici algorithm sampl markov random field becom focus much recent research machin learn literatur much use statistician briefli review recent propos approach eye toward implement statistician without expertis numer analysi advanc comput particular consid version block sampl graph cut condit independ site updat togeth algorithm allow practition parallel updat subset locat take advantag vector calcul high level languag simul real data applic demonstr comput save achiev versus tradit singl site updat block updat regardless whether data regular irregular lattic argu easili implement sampl routin provid good compromis statist comput effici work larg dataset|['D. Andrew Brown', 'Christopher S. McMahan']|['stat.CO']
2017-04-07T11:25:01Z|2017-02-17T18:06:27Z|http://arxiv.org/abs/1702.05462v1|http://arxiv.org/pdf/1702.05462v1|Objective Bayesian Analysis for Change Point Problems|object bayesian analysi chang point problem|In this paper we present an objective approach to change point analysis. In particular, we look at the problem from two perspectives. The first focuses on the definition of an objective prior when the number of change points is known a priori. The second contribution aims to estimate the number of change points by using an objective approach, recently introduced in the literature, based on losses. The latter considers change point estimation as a model selection exercise. We show the performance of the proposed approach on simulated data and on real data sets.|paper present object approach chang point analysi particular look problem two perspect first focus definit object prior number chang point known priori second contribut aim estim number chang point use object approach recent introduc literatur base loss latter consid chang point estim model select exercis show perform propos approach simul data real data set|['Laurentiu Hinoveanu', 'Fabrizio Leisen', 'Cristiano Villa']|['stat.ME', 'math.ST', 'stat.AP', 'stat.CO', 'stat.ML', 'stat.TH']
2017-04-07T11:25:01Z|2017-02-15T11:52:14Z|http://arxiv.org/abs/1702.04561v1|http://arxiv.org/pdf/1702.04561v1|Probing for sparse and fast variable selection with model-based boosting|probe spars fast variabl select model base boost|We present a new variable selection method based on model-based gradient boosting and randomly permuted variables. Model-based boosting is a tool to fit a statistical model while performing variable selection at the same time. A drawback of the fitting lies in the need of multiple model fits on slightly altered data (e.g. cross-validation or bootstrap) to find the optimal number of boosting iterations and prevent overfitting. In our proposed approach, we augment the data set with randomly permuted versions of the true variables, so called shadow variables, and stop the step-wise fitting as soon as such a variable would be added to the model. This allows variable selection in a single fit of the model without requiring further parameter tuning. We show that our probing approach can compete with state-of-the-art selection methods like stability selection in a high-dimensional classification benchmark and apply it on gene expression data for the estimation of riboflavin production of Bacillus subtilis.|present new variabl select method base model base gradient boost random permut variabl model base boost tool fit statist model perform variabl select time drawback fit lie need multipl model fit slight alter data cross valid bootstrap find optim number boost iter prevent overfit propos approach augment data set random permut version true variabl call shadow variabl stop step wise fit soon variabl would ad model allow variabl select singl fit model without requir paramet tune show probe approach compet state art select method like stabil select high dimension classif benchmark appli gene express data estim riboflavin product bacillus subtili|['Janek Thomas', 'Tobias Hepp', 'Andreas Mayr', 'Bernd Bischl']|['stat.ML', 'stat.CO']
2017-04-07T11:25:01Z|2017-02-14T21:20:23Z|http://arxiv.org/abs/1702.04391v1|http://arxiv.org/pdf/1702.04391v1|Bootstrap-based inferential improvements in beta autoregressive moving   average model|bootstrap base inferenti improv beta autoregress move averag model|We consider the issue of performing accurate small sample inference in beta autoregressive moving average model, which is useful for modeling and forecasting continuous variables that assumes values in the interval $(0,1)$. The inferences based on conditional maximum likelihood estimation have good asymptotic properties, but their performances in small samples may be poor. This way, we propose bootstrap bias corrections of the point estimators and different bootstrap strategies for confidence interval improvements. Our Monte Carlo simulations show that finite sample inference based on bootstrap corrections is much more reliable than the usual inferences. We also presented an empirical application.|consid issu perform accur small sampl infer beta autoregress move averag model use model forecast continu variabl assum valu interv infer base condit maximum likelihood estim good asymptot properti perform small sampl may poor way propos bootstrap bias correct point estim differ bootstrap strategi confid interv improv mont carlo simul show finit sampl infer base bootstrap correct much reliabl usual infer also present empir applic|['Bruna Gregory Palm', 'Fábio M. Bayer']|['stat.CO']
2017-04-07T11:25:01Z|2017-02-13T17:23:02Z|http://arxiv.org/abs/1702.03891v1|http://arxiv.org/pdf/1702.03891v1|Spatial Models with the Integrated Nested Laplace Approximation within   Markov Chain Monte Carlo|spatial model integr nest laplac approxim within markov chain mont carlo|The Integrated Nested Laplace Approximation (INLA) is a convenient way to obtain approximations to the posterior marginals for parameters in Bayesian hierarchical models when the latent effects can be expressed as a Gaussian Markov Random Field (GMRF). In addition, its implementation in the R-INLA package for the R statistical software provides an easy way to fit models using INLA in practice. R-INLA implements a number of widely used latent models, including several spatial models. In addition, R-INLA can fit models in a fraction of the time than other computer intensive methods (e.g. Markov Chain Monte Carlo) take to fit the same model.   Although INLA provides a fast approximation to the marginals of the model parameters, it is difficult to use it with models not implemented in R-INLA. It is also difficult to make multivariate posterior inference on the parameters of the model as INLA focuses on the posterior marginals and not the joint posterior distribution.   In this paper we describe how to use INLA within the Metropolis-Hastings algorithm to fit spatial models and estimate the joint posterior distribution of a reduced number of parameters. We will illustrate the benefits of this new method with two examples on spatial econometrics and disease mapping where complex spatial models with several spatial structures need to be fitted.|integr nest laplac approxim inla conveni way obtain approxim posterior margin paramet bayesian hierarch model latent effect express gaussian markov random field gmrf addit implement inla packag statist softwar provid easi way fit model use inla practic inla implement number wide use latent model includ sever spatial model addit inla fit model fraction time comput intens method markov chain mont carlo take fit model although inla provid fast approxim margin model paramet difficult use model implement inla also difficult make multivari posterior infer paramet model inla focus posterior margin joint posterior distribut paper describ use inla within metropoli hast algorithm fit spatial model estim joint posterior distribut reduc number paramet illustr benefit new method two exampl spatial econometr diseas map complex spatial model sever spatial structur need fit|['Virgilio Gómez-Rubio', 'Francisco Palmí-Perales']|['stat.CO']
2017-04-07T11:25:01Z|2017-02-13T08:52:58Z|http://arxiv.org/abs/1702.03673v1|http://arxiv.org/pdf/1702.03673v1|Bayesian Probabilistic Numerical Methods|bayesian probabilist numer method|The emergent field of probabilistic numerics has thus far lacked rigorous statistical principals. This paper establishes Bayesian probabilistic numerical methods as those which can be cast as solutions to certain Bayesian inverse problems, albeit problems that are non-standard. This allows us to establish general conditions under which Bayesian probabilistic numerical methods are well-defined, encompassing both non-linear and non-Gaussian models. For general computation, a numerical approximation scheme is developed and its asymptotic convergence is established. The theoretical development is then extended to pipelines of computation, wherein probabilistic numerical methods are composed to solve more challenging numerical tasks. The contribution highlights an important research frontier at the interface of numerical analysis and uncertainty quantification, with some illustrative applications presented.|emerg field probabilist numer thus far lack rigor statist princip paper establish bayesian probabilist numer method cast solut certain bayesian invers problem albeit problem non standard allow us establish general condit bayesian probabilist numer method well defin encompass non linear non gaussian model general comput numer approxim scheme develop asymptot converg establish theoret develop extend pipelin comput wherein probabilist numer method compos solv challeng numer task contribut highlight import research frontier interfac numer analysi uncertainti quantif illustr applic present|['Jon Cockayne', 'Chris Oates', 'Tim Sullivan', 'Mark Girolami']|['stat.ME', 'cs.NA', 'math.NA', 'math.ST', 'stat.CO', 'stat.TH']
2017-04-07T11:25:01Z|2017-02-10T12:26:52Z|http://arxiv.org/abs/1702.03146v1|http://arxiv.org/pdf/1702.03146v1|Analysis of a nonlinear importance sampling scheme for Bayesian   parameter estimation in state-space models|analysi nonlinear import sampl scheme bayesian paramet estim state space model|The Bayesian estimation of the unknown parameters of state-space (dynamical) systems has received considerable attention over the past decade, with a handful of powerful algorithms being introduced. In this paper we tackle the theoretical analysis of the recently proposed {\it nonlinear} population Monte Carlo (NPMC). This is an iterative importance sampling scheme whose key features, compared to conventional importance samplers, are (i) the approximate computation of the importance weights (IWs) assigned to the Monte Carlo samples and (ii) the nonlinear transformation of these IWs in order to prevent the degeneracy problem that flaws the performance of conventional importance samplers. The contribution of the present paper is a rigorous proof of convergence of the nonlinear IS (NIS) scheme as the number of Monte Carlo samples, $M$, increases. Our analysis reveals that the NIS approximation errors converge to 0 almost surely and with the optimal Monte Carlo rate of $M^{-\frac{1}{2}}$. Moreover, we prove that this is achieved even when the mean estimation error of the IWs remains constant, a property that has been termed {\it exact approximation} in the Markov chain Monte Carlo literature. We illustrate these theoretical results by means of a computer simulation example involving the estimation of the parameters of a state-space model typically used for target tracking.|bayesian estim unknown paramet state space dynam system receiv consider attent past decad hand power algorithm introduc paper tackl theoret analysi recent propos nonlinear popul mont carlo npmc iter import sampl scheme whose key featur compar convent import sampler approxim comput import weight iw assign mont carlo sampl ii nonlinear transform iw order prevent degeneraci problem flaw perform convent import sampler contribut present paper rigor proof converg nonlinear nis scheme number mont carlo sampl increas analysi reveal nis approxim error converg almost sure optim mont carlo rate frac moreov prove achiev even mean estim error iw remain constant properti term exact approxim markov chain mont carlo literatur illustr theoret result mean comput simul exampl involv estim paramet state space model typic use target track|['Joaquin Miguez', 'Ines P. Mariño', 'Manuel A. Vazquez']|['stat.CO']
2017-04-07T11:25:01Z|2017-02-10T10:44:23Z|http://arxiv.org/abs/1702.03126v1|http://arxiv.org/pdf/1702.03126v1|Computational inference without proposal kernels|comput infer without propos kernel|Likelihood-free methods, such as approximate Bayesian computation, are powerful tools for practical inference problems with intractable likelihood functions. Markov chain Monte Carlo and sequential Monte Carlo variants of approximate Bayesian computation can be effective techniques for sampling posterior distributions without likelihoods. However, the efficiency of these methods depends crucially on the proposal kernel used to generate proposal posterior samples, and a poor choice can lead to extremely low efficiency. We propose a new method for likelihood-free Bayesian inference based upon ideas from multilevel Monte Carlo. Our method is accurate and does not require proposal kernels, thereby overcoming a key obstacle in the use of likelihood-free approaches in real-world situations.|likelihood free method approxim bayesian comput power tool practic infer problem intract likelihood function markov chain mont carlo sequenti mont carlo variant approxim bayesian comput effect techniqu sampl posterior distribut without likelihood howev effici method depend crucial propos kernel use generat propos posterior sampl poor choic lead extrem low effici propos new method likelihood free bayesian infer base upon idea multilevel mont carlo method accur doe requir propos kernel therebi overcom key obstacl use likelihood free approach real world situat|['David J. Warne', 'Ruth E. Baker', 'Matthew J. Simpson']|['stat.CO', '62F15, 65C05']
2017-04-07T11:25:01Z|2017-02-15T10:14:43Z|http://arxiv.org/abs/1702.03057v2|http://arxiv.org/pdf/1702.03057v2|Unbiased Multi-index Monte Carlo|unbias multi index mont carlo|We introduce a new class of Monte Carlo based approximations of expectations of random variables defined whose laws are not available directly, but only through certain discretisatizations. Sampling from the discretized versions of these laws can typically introduce a bias. In this paper, we show how to remove that bias, by introducing a new version of multi-index Monte Carlo (MIMC) that has the added advantage of reducing the computational effort, relative to i.i.d. sampling from the most precise discretization, for a given level of error. We cover extensions of results regarding variance and optimality criteria for the new approach. We apply the methodology to the problem of computing an unbiased mollified version of the solution of a partial differential equation with random coefficients. A second application concerns the Bayesian inference (the smoothing problem) of an infinite dimensional signal modelled by the solution of a stochastic partial differential equation that is observed on a discrete space grid and at discrete times. Both applications are complemented by numerical simulations.|introduc new class mont carlo base approxim expect random variabl defin whose law avail direct onli certain discretisat sampl discret version law typic introduc bias paper show remov bias introduc new version multi index mont carlo mimc ad advantag reduc comput effort relat sampl precis discret given level error cover extens result regard varianc optim criteria new approach appli methodolog problem comput unbias mollifi version solut partial differenti equat random coeffici second applic concern bayesian infer smooth problem infinit dimension signal model solut stochast partial differenti equat observ discret space grid discret time applic complement numer simul|['Dan Crisan', 'Jeremie Houssineau', 'Ajay Jasra']|['stat.CO']
2017-04-07T11:25:01Z|2017-02-14T04:22:00Z|http://arxiv.org/abs/1702.02707v2|http://arxiv.org/pdf/1702.02707v2|A Fast Algorithm for the Coordinate-wise Minimum Distance Estimation|fast algorithm coordin wise minimum distanc estim|Application of the minimum distance method to the linear regression model for estimating regression parameters is a difficult and time-consuming process due to the complexity of its distance function, and hence, it is computationally expensive. To deal with the computational cost, this paper proposes a fast algorithm which mainly uses technique of coordinate-wise minimization in order to estimate the regression parameters. R package based on the proposed algorithm and written in Rcpp is available online.|applic minimum distanc method linear regress model estim regress paramet difficult time consum process due complex distanc function henc comput expens deal comput cost paper propos fast algorithm main use techniqu coordin wise minim order estim regress paramet packag base propos algorithm written rcpp avail onlin|['Jiwoong Kim']|['stat.CO']
2017-04-07T11:25:01Z|2017-02-09T00:11:27Z|http://arxiv.org/abs/1702.02658v1|http://arxiv.org/pdf/1702.02658v1|Estimating the number of clusters using cross-validation|estim number cluster use cross valid|Many clustering methods, including k-means, require the user to specify the number of clusters as an input parameter. A variety of methods have been devised to choose the number of clusters automatically, but they often rely on strong modeling assumptions. This paper proposes a data-driven approach to estimate the number of clusters based on a novel form of cross-validation. The proposed method differs from ordinary cross-validation, because clustering is fundamentally an unsupervised learning problem. Simulation and real data analysis results show that the proposed method outperforms existing methods, especially in high-dimensional settings with heterogeneous or heavy-tailed noise. In a yeast cell cycle dataset, the proposed method finds a parsimonious clustering with interpretable gene groupings.|mani cluster method includ mean requir user specifi number cluster input paramet varieti method devis choos number cluster automat often reli strong model assumpt paper propos data driven approach estim number cluster base novel form cross valid propos method differ ordinari cross valid becaus cluster fundament unsupervis learn problem simul real data analysi result show propos method outperform exist method especi high dimension set heterogen heavi tail nois yeast cell cycl dataset propos method find parsimoni cluster interpret gene group|['Wei Fu', 'Patrick O. Perry']|['stat.ME', 'stat.CO']
