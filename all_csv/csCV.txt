2017-03-28T14:08:43Z|2017-03-27T17:52:55Z|http://arxiv.org/abs/1703.09211v1|http://arxiv.org/pdf/1703.09211v1|Coherent Online Video Style Transfer|coher onlin video style transfer|Training a feed-forward network for fast neural style transfer of images is proven to be successful. However, the naive extension to process video frame by frame is prone to producing flickering results. We propose the first end-to-end network for online video style transfer, which generates temporally coherent stylized video sequences in near real-time. Two key ideas include an efficient network by incorporating short-term coherence, and propagating short-term coherence to long-term, which ensures the consistency over larger period of time. Our network can incorporate different image stylization networks. We show that the proposed method clearly outperforms the per-frame baseline both qualitatively and quantitatively. Moreover, it can achieve visually comparable coherence to optimization-based video style transfer, but is three orders of magnitudes faster in runtime.|train feed forward network fast neural style transfer imag proven success howev naiv extens process video frame frame prone produc flicker result propos first end end network onlin video style transfer generat tempor coher styliz video sequenc near real time two key idea includ effici network incorpor short term coher propag short term coher long term ensur consist larger period time network incorpor differ imag stylize network show propos method clear outperform per frame baselin qualit quantit moreov achiev visual compar coher optim base video style transfer three order magnitud faster runtim|['Dongdong Chen', 'Jing Liao', 'Yuan Lu', 'Nenghai Yu', 'Gang Hua']|['cs.CV']
2017-03-28T14:08:43Z|2017-03-27T17:52:18Z|http://arxiv.org/abs/1703.09210v1|http://arxiv.org/pdf/1703.09210v1|StyleBank: An Explicit Representation for Neural Image Style Transfer|stylebank explicit represent neural imag style transfer|We propose StyleBank, which is composed of multiple convolution filter banks and each filter bank explicitly represents one style, for neural image style transfer. To transfer an image to a specific style, the corresponding filter bank is operated on top of the intermediate feature embedding produced by a single auto-encoder. The StyleBank and the auto-encoder are jointly learnt, where the learning is conducted in such a way that the auto-encoder does not encode any style information thanks to the flexibility introduced by the explicit filter bank representation. It also enables us to conduct incremental learning to add a new image style by learning a new filter bank while holding the auto-encoder fixed. The explicit style representation along with the flexible network design enables us to fuse styles at not only the image level, but also the region level. Our method is the first style transfer network that links back to traditional texton mapping methods, and hence provides new understanding on neural style transfer. Our method is easy to train, runs in real-time, and produces results that qualitatively better or at least comparable to existing methods.|propos stylebank compos multipl convolut filter bank filter bank explicit repres one style neural imag style transfer transfer imag specif style correspond filter bank oper top intermedi featur embed produc singl auto encod stylebank auto encod joint learnt learn conduct way auto encod doe encod ani style inform thank flexibl introduc explicit filter bank represent also enabl us conduct increment learn add new imag style learn new filter bank hold auto encod fix explicit style represent along flexibl network design enabl us fuse style onli imag level also region level method first style transfer network link back tradit texton map method henc provid new understand neural style transfer method easi train run real time produc result qualit better least compar exist method|['Dongdong Chen', 'Yuan Lu', 'Jing Liao', 'Nenghai Yu', 'Gang Hua']|['cs.CV']
2017-03-28T14:08:43Z|2017-03-27T17:37:33Z|http://arxiv.org/abs/1703.09200v1|http://arxiv.org/pdf/1703.09200v1|Deep Poincare Map For Robust Medical Image Segmentation|deep poincar map robust medic imag segment|Precise segmentation is a prerequisite for an accurate quantification of the imaged objects. It is a very challenging task in many medical imaging applications due to relatively poor image quality and data scarcity. In this work, we present an innovative segmentation paradigm, named Deep Poincare Map (DPM), by coupling the dynamical system theory with a novel deep learning based approach. Firstly, we model the image segmentation process as a dynamical system, in which limit cycle models the boundary of the region of interest (ROI). Secondly, instead of segmenting the ROI directly, convolutional neural network is employed to predict the vector field of the dynamical system. Finally, the boundary of the ROI is identified using the Poincare map and the flow integration. We demonstrate that our segmentation model can be built using a very limited number of train- ing data. By cross-validation, we can achieve a mean Dice score of 94% compared to the manual delineation (ground truth) of the left ventricle ROI defined by clinical experts on a cardiac MRI dataset. Compared with other state-of-the-art methods, we can conclude that the proposed DPM method is adaptive, accurate and robust. It is straightforward to apply this method for other medical imaging applications.|precis segment prerequisit accur quantif imag object veri challeng task mani medic imag applic due relat poor imag qualiti data scarciti work present innov segment paradigm name deep poincar map dpm coupl dynam system theori novel deep learn base approach first model imag segment process dynam system limit cycl model boundari region interest roi second instead segment roi direct convolut neural network employ predict vector field dynam system final boundari roi identifi use poincar map flow integr demonstr segment model built use veri limit number train ing data cross valid achiev mean dice score compar manual delin ground truth left ventricl roi defin clinic expert cardiac mri dataset compar state art method conclud propos dpm method adapt accur robust straightforward appli method medic imag applic|['Yuanhan Mo', 'Fangde Liu', 'Jingqing Zhang', 'Guang Yang', 'Taigang He', 'Yike Guo']|['cs.CV']
2017-03-28T14:08:43Z|2017-03-27T17:36:33Z|http://arxiv.org/abs/1703.09199v1|http://arxiv.org/pdf/1703.09199v1|Introduction To The Monogenic Signal|introduct monogen signal|The monogenic signal is an image analysis methodology that was introduced by Felsberg and Sommer in 2001 and has been employed for a variety of purposes in image processing and computer vision research. In particular, it has been found to be useful in the analysis of ultrasound imagery in several research scenarios mostly in work done within the BioMedIA lab at Oxford. However, the literature on the monogenic signal can be difficult to penetrate due to the lack of a single resource to explain the various principles from basics. The purpose of this document is therefore to introduce the principles, purpose, applications, and limitations of the methodology. It assumes some background knowledge from the fields of image and signal processing, in particular a good knowledge of Fourier transforms as applied to signals and images. We will not attempt to provide a thorough math- ematical description or derivation of the monogenic signal, but rather focus on developing an intuition for understanding and using the methodology and refer the reader elsewhere for a more mathematical treatment.|monogen signal imag analysi methodolog introduc felsberg sommer employ varieti purpos imag process comput vision research particular found use analysi ultrasound imageri sever research scenario work done within biomedia lab oxford howev literatur monogen signal difficult penetr due lack singl resourc explain various principl basic purpos document therefor introduc principl purpos applic limit methodolog assum background knowledg field imag signal process particular good knowledg fourier transform appli signal imag attempt provid thorough math emat descript deriv monogen signal rather focus develop intuit understand use methodolog refer reader elsewher mathemat treatment|['Christopher P. Bridge']|['cs.CV']
2017-03-28T14:08:43Z|2017-03-27T16:48:03Z|http://arxiv.org/abs/1703.09179v1|http://arxiv.org/pdf/1703.09179v1|Transfer learning for music classification and regression tasks|transfer learn music classif regress task|In this paper, we present a transfer learning approach for music classification and regression tasks. We propose to use a pretrained convnet feature, a concatenated feature vector using activations of feature maps of multiple layers in a trained convolutional network. We show that how this convnet feature can serve as a general-purpose music representation. In the experiment, a convnet is trained for music tagging and then transferred for many music-related classification and regression tasks as well as an audio-related classification task. In experiments, the convnet feature outperforms the baseline MFCC feature in all tasks and many reported approaches of aggregating MFCCs and low- and high-level music features.|paper present transfer learn approach music classif regress task propos use pretrain convnet featur concaten featur vector use activ featur map multipl layer train convolut network show convnet featur serv general purpos music represent experi convnet train music tag transfer mani music relat classif regress task well audio relat classif task experi convnet featur outperform baselin mfcc featur task mani report approach aggreg mfccs low high level music featur|['Keunwoo Choi', 'Gy√∂rgy Fazekas', 'Mark Sandler', 'Kyunghyun Cho']|['cs.CV', 'cs.AI', 'cs.MM', 'cs.SD']
2017-03-28T14:08:43Z|2017-03-27T16:21:26Z|http://arxiv.org/abs/1703.09167v1|http://arxiv.org/pdf/1703.09167v1|A Study on the Extraction and Analysis of a Large Set of Eye Movement   Features during Reading|studi extract analysi larg set eye movement featur dure read|This work presents a study on the extraction and analysis of a set of 101 categories of eye movement features from three types of eye movement events: fixations, saccades, and post-saccadic oscillations. The eye movements were recorded during a reading task. For the categories of features with multiple instances in a recording we extract corresponding feature subtypes by calculating descriptive statistics on the distributions of these instances. A unified framework of detailed descriptions and mathematical formulas are provided for the extraction of the feature set. The analysis of feature values is performed using a large database of eye movement recordings from a normative population of 298 subjects. We demonstrate the central tendency and overall variability of feature values over the experimental population, and more importantly, we quantify the test-retest reliability (repeatability) of each separate feature. The described methods and analysis can provide valuable tools in fields exploring the eye movements, such as in behavioral studies, attention and cognition research, medical research, biometric recognition, and human-computer interaction.|work present studi extract analysi set categori eye movement featur three type eye movement event fixat saccad post saccad oscil eye movement record dure read task categori featur multipl instanc record extract correspond featur subtyp calcul descript statist distribut instanc unifi framework detail descript mathemat formula provid extract featur set analysi featur valu perform use larg databas eye movement record normat popul subject demonstr central tendenc overal variabl featur valu experiment popul import quantifi test retest reliabl repeat separ featur describ method analysi provid valuabl tool field explor eye movement behavior studi attent cognit research medic research biometr recognit human comput interact|['Ioannis Rigas', 'Lee Friedman', 'Oleg Komogortsev']|['cs.CV', 'cs.HC', 'q-bio.QM']
2017-03-28T14:08:43Z|2017-03-27T16:09:20Z|http://arxiv.org/abs/1703.09161v1|http://arxiv.org/pdf/1703.09161v1|A Dynamic Programming Solution to Bounded Dejittering Problems|dynam program solut bound dejitt problem|We propose a dynamic programming solution to image dejittering problems with bounded displacements and obtain efficient algorithms for the removal of line jitter, line pixel jitter, and pixel jitter.|propos dynam program solut imag dejitt problem bound displac obtain effici algorithm remov line jitter line pixel jitter pixel jitter|['Lukas F. Lang']|['math.OC', 'cs.CV']
2017-03-28T14:08:43Z|2017-03-27T15:57:27Z|http://arxiv.org/abs/1703.09157v1|http://arxiv.org/pdf/1703.09157v1|Reweighted Infrared Patch-Tensor Model With Both Non-Local and Local   Priors for Single-Frame Small Target Detection|reweight infrar patch tensor model non local local prior singl frame small target detect|Many state-of-the-art methods have been proposed for infrared small target detection. They work well on the images with homogeneous backgrounds and high-contrast targets. However, when facing highly heterogeneous backgrounds, they would not perform very well, mainly due to: 1) the existence of strong edges and other interfering components, 2) not utilizing the priors fully. Inspired by this, we propose a novel method to exploit both local and non-local priors simultaneously. Firstly, we employ a new infrared patch-tensor (IPT) model to represent the image and preserve its spatial correlations. Exploiting the target sparse prior and background non-local self-correlation prior, the target-background separation is modeled as a robust low-rank tensor recovery problem. Moreover, with the help of the structure tensor and reweighted idea, we design an entry-wise local-structure-adaptive and sparsity enhancing weight to replace the globally constant weighting parameter. The decomposition could be achieved via the element-wise reweighted higher-order robust principal component analysis with an additional convergence condition according to the practical situation of target detection. Extensive experiments demonstrate that our model outperforms the other state-of-the-arts, in particular for the images with very dim targets and heavy clutters.|mani state art method propos infrar small target detect work well imag homogen background high contrast target howev face high heterogen background would perform veri well main due exist strong edg interf compon util prior fulli inspir propos novel method exploit local non local prior simultan first employ new infrar patch tensor ipt model repres imag preserv spatial correl exploit target spars prior background non local self correl prior target background separ model robust low rank tensor recoveri problem moreov help structur tensor reweight idea design entri wise local structur adapt sparsiti enhanc weight replac global constant weight paramet decomposit could achiev via element wise reweight higher order robust princip compon analysi addit converg condit accord practic situat target detect extens experi demonstr model outperform state art particular imag veri dim target heavi clutter|['Yimian Dai', 'Yiquan Wu']|['cs.CV']
2017-03-28T14:08:43Z|2017-03-27T15:31:00Z|http://arxiv.org/abs/1703.09145v1|http://arxiv.org/pdf/1703.09145v1|"Multi-Path Region-Based Convolutional Neural Network for Accurate   Detection of Unconstrained ""Hard Faces"""|multi path region base convolut neural network accur detect unconstrain hard face|"Large-scale variations still pose a challenge in unconstrained face detection. To the best of our knowledge, no current face detection algorithm can detect a face as large as 800 x 800 pixels while simultaneously detecting another one as small as 8 x 8 pixels within a single image with equally high accuracy. We propose a two-stage cascaded face detection framework, Multi-Path Region-based Convolutional Neural Network (MP-RCNN), that seamlessly combines a deep neural network with a classic learning strategy, to tackle this challenge. The first stage is a Multi-Path Region Proposal Network (MP-RPN) that proposes faces at three different scales. It simultaneously utilizes three parallel outputs of the convolutional feature maps to predict multi-scale candidate face regions. The ""atrous"" convolution trick (convolution with up-sampled filters) and a newly proposed sampling layer for ""hard"" examples are embedded in MP-RPN to further boost its performance. The second stage is a Boosted Forests classifier, which utilizes deep facial features pooled from inside the candidate face regions as well as deep contextual features pooled from a larger region surrounding the candidate face regions. This step is included to further remove hard negative samples. Experiments show that this approach achieves state-of-the-art face detection performance on the WIDER FACE dataset ""hard"" partition, outperforming the former best result by 9.6% for the Average Precision."|larg scale variat still pose challeng unconstrain face detect best knowledg current face detect algorithm detect face larg pixel simultan detect anoth one small pixel within singl imag equal high accuraci propos two stage cascad face detect framework multi path region base convolut neural network mp rcnn seamless combin deep neural network classic learn strategi tackl challeng first stage multi path region propos network mp rpn propos face three differ scale simultan util three parallel output convolut featur map predict multi scale candid face region atrous convolut trick convolut sampl filter newli propos sampl layer hard exampl embed mp rpn boost perform second stage boost forest classifi util deep facial featur pool insid candid face region well deep contextu featur pool larger region surround candid face region step includ remov hard negat sampl experi show approach achiev state art face detect perform wider face dataset hard partit outperform former best result averag precis|['Yuguang Liu', 'Martin D. Levine']|['cs.CV']
2017-03-28T14:08:43Z|2017-03-27T15:13:49Z|http://arxiv.org/abs/1703.09137v1|http://arxiv.org/pdf/1703.09137v1|Where to put the Image in an Image Caption Generator|put imag imag caption generat|When a neural language model is used for caption generation, the image information can be fed to the neural network either by directly incorporating it in a recurrent neural network -- conditioning the language model by injecting image features -- or in a layer following the recurrent neural network -- conditioning the language model by merging the image features. While merging implies that visual features are bound at the end of the caption generation process, injecting can bind the visual features at a variety stages. In this paper we empirically show that late binding is superior to early binding in terms of different evaluation metrics. This suggests that the different modalities (visual and linguistic) for caption generation should not be jointly encoded by the RNN; rather, the multimodal integration should be delayed to a subsequent stage. Furthermore, this suggests that recurrent neural networks should not be viewed as actually generating text, but only as encoding it for prediction in a subsequent layer.|neural languag model use caption generat imag inform fed neural network either direct incorpor recurr neural network condit languag model inject imag featur layer follow recurr neural network condit languag model merg imag featur merg impli visual featur bound end caption generat process inject bind visual featur varieti stage paper empir show late bind superior earli bind term differ evalu metric suggest differ modal visual linguist caption generat joint encod rnn rather multimod integr delay subsequ stage furthermor suggest recurr neural network view actual generat text onli encod predict subsequ layer|['Marc Tanti', 'Albert Gatt', 'Kenneth P. Camilleri']|['cs.NE', 'cs.CL', 'cs.CV']
2017-03-28T14:08:48Z|2017-03-27T13:44:26Z|http://arxiv.org/abs/1703.09076v1|http://arxiv.org/pdf/1703.09076v1|Active Convolution: Learning the Shape of Convolution for Image   Classification|activ convolut learn shape convolut imag classif|In recent years, deep learning has achieved great success in many computer vision applications. Convolutional neural networks (CNNs) have lately emerged as a major approach to image classification. Most research on CNNs thus far has focused on developing architectures such as the Inception and residual networks. The convolution layer is the core of the CNN, but few studies have addressed the convolution unit itself. In this paper, we introduce a convolution unit called the active convolution unit (ACU). A new convolution has no fixed shape, because of which we can define any form of convolution. Its shape can be learned through backpropagation during training. Our proposed unit has a few advantages. First, the ACU is a generalization of convolution; it can define not only all conventional convolutions, but also convolutions with fractional pixel coordinates. We can freely change the shape of the convolution, which provides greater freedom to form CNN structures. Second, the shape of the convolution is learned while training and there is no need to tune it by hand. Third, the ACU can learn better than a conventional unit, where we obtained the improvement simply by changing the conventional convolution to an ACU. We tested our proposed method on plain and residual networks, and the results showed significant improvement using our method on various datasets and architectures in comparison with the baseline.|recent year deep learn achiev great success mani comput vision applic convolut neural network cnns late emerg major approach imag classif research cnns thus far focus develop architectur incept residu network convolut layer core cnn studi address convolut unit paper introduc convolut unit call activ convolut unit acu new convolut fix shape becaus defin ani form convolut shape learn backpropag dure train propos unit advantag first acu general convolut defin onli convent convolut also convolut fraction pixel coordin freeli chang shape convolut provid greater freedom form cnn structur second shape convolut learn train need tune hand third acu learn better convent unit obtain improv simpli chang convent convolut acu test propos method plain residu network result show signific improv use method various dataset architectur comparison baselin|['Yunho Jeon', 'Junmo Kim']|['cs.CV']
2017-03-28T14:08:48Z|2017-03-27T12:42:13Z|http://arxiv.org/abs/1703.09039v1|http://arxiv.org/pdf/1703.09039v1|Efficient Processing of Deep Neural Networks: A Tutorial and Survey|effici process deep neural network tutori survey|Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of deep neural network to improve energy-efficiency and throughput without sacrificing performance accuracy or increasing hardware cost are critical to enabling the wide deployment of DNNs in AI systems.   This article aims to provide a comprehensive tutorial and survey about the recent advances towards the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various platforms and architectures that support DNNs, and highlight key trends in recent efficient processing techniques that reduce the computation cost of DNNs either solely via hardware design changes or via joint hardware design and network algorithm changes. It will also summarize various development resources that can enable researchers and practitioners to quickly get started on DNN design, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic co-design, being proposed in academia and industry.   The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand trade-offs between various architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand of recent implementation trends and opportunities.|deep neural network dnns current wide use mani artifici intellig ai applic includ comput vision speech recognit robot dnns deliv state art accuraci mani ai task come cost high comput complex accord techniqu enabl effici process deep neural network improv energi effici throughput without sacrif perform accuraci increas hardwar cost critic enabl wide deploy dnns ai system articl aim provid comprehens tutori survey recent advanc toward goal enabl effici process dnns specif provid overview dnns discuss various platform architectur support dnns highlight key trend recent effici process techniqu reduc comput cost dnns either sole via hardwar design chang via joint hardwar design network algorithm chang also summar various develop resourc enabl research practition quick get start dnn design highlight import benchmark metric design consider use evalu rapid grow number dnn hardwar design option includ algorithm co design propos academia industri reader take away follow concept articl understand key design consider dnns abl evalu differ dnn hardwar implement benchmark comparison metric understand trade various architectur platform abl evalu util various dnn design techniqu effici process understand recent implement trend opportun|['Vivienne Sze', 'Yu-Hsin Chen', 'Tien-Ju Yang', 'Joel Emer']|['cs.CV']
2017-03-28T14:08:48Z|2017-03-27T12:14:07Z|http://arxiv.org/abs/1703.09026v1|http://arxiv.org/pdf/1703.09026v1|Trespassing the Boundaries: Labeling Temporal Bounds for Object   Interactions in Egocentric Video|trespass boundari label tempor bound object interact egocentr video|Manual annotations of temporal bounds for object interactions (i.e. start and end times) are typical training input to recognition, localization and detection algorithms. For three publicly available egocentric datasets, we uncover inconsistencies in ground truth temporal bounds within and across annotators and datasets. We systematically assess the robustness of state-of-the-art approaches to changes in labeled temporal bounds, for object interaction recognition. As boundaries are trespassed, a drop of up to 10% is observed for both Improved Dense Trajectories and Two-Stream Convolutional Neural Network. We demonstrate that such disagreement stems from a limited understanding of the distinct phases of an action, and propose annotating based on the Rubicon Boundaries, inspired by a similarly named cognitive model, for consistent temporal bounds of object interactions. Evaluated on a public dataset, we report a 4% increase in overall accuracy, and an increase in accuracy for 55% of classes when Rubicon Boundaries are used for temporal annotations.|manual annot tempor bound object interact start end time typic train input recognit local detect algorithm three public avail egocentr dataset uncov inconsist ground truth tempor bound within across annot dataset systemat assess robust state art approach chang label tempor bound object interact recognit boundari trespass drop observ improv dens trajectori two stream convolut neural network demonstr disagr stem limit understand distinct phase action propos annot base rubicon boundari inspir similar name cognit model consist tempor bound object interact evalu public dataset report increas overal accuraci increas accuraci class rubicon boundari use tempor annot|['Davide Moltisanti', 'Michael Wray', 'Walterio Mayol-Cuevas', 'Dima Damen']|['cs.CV']
2017-03-28T14:08:48Z|2017-03-27T09:51:55Z|http://arxiv.org/abs/1703.08987v1|http://arxiv.org/pdf/1703.08987v1|Simultaneous Perception and Path Generation Using Fully Convolutional   Neural Networks|simultan percept path generat use fulli convolut neural network|In this work, a novel learning-based approach has been developed to generate driving paths by integrating LIDAR point clouds, GPS-IMU information, and Google driving directions. The system is based on a fully convolutional neural network that jointly learns to carry out perception and path generation from real-world driving sequences and that is trained using automatically generated training examples. Several combinations of input data were tested in order to assess the performance gain provided by specific information modalities. The fully convolutional neural network trained using all the available sensors together with driving directions achieved the best MaxF score of 88.13% when considering a region of interest of 60x60 meters. By considering a smaller region of interest, the agreement between predicted paths and ground-truth increased to 92.60%. The positive results obtained in this work indicate that the proposed system may help fill the gap between low-level scene parsing and behavior-reflex approaches by generating outputs that are close to vehicle control and at the same time human-interpretable.|work novel learn base approach develop generat drive path integr lidar point cloud gps imu inform googl drive direct system base fulli convolut neural network joint learn carri percept path generat real world drive sequenc train use automat generat train exampl sever combin input data test order assess perform gain provid specif inform modal fulli convolut neural network train use avail sensor togeth drive direct achiev best maxf score consid region interest meter consid smaller region interest agreement predict path ground truth increas posit result obtain work indic propos system may help fill gap low level scene pars behavior reflex approach generat output close vehicl control time human interpret|['Luca Caltagirone', 'Mauro Bellone', 'Lennart Svensson', 'Mattias Wahde']|['cs.CV']
2017-03-28T14:08:48Z|2017-03-27T08:23:47Z|http://arxiv.org/abs/1703.08966v1|http://arxiv.org/pdf/1703.08966v1|Mastering Sketching: Adversarial Augmentation for Structured Prediction|master sketch adversari augment structur predict|We present an integral framework for training sketch simplification networks that convert challenging rough sketches into clean line drawings. Our approach augments a simplification network with a discriminator network, training both networks jointly so that the discriminator network discerns whether a line drawing is a real training data or the output of the simplification network, which in turn tries to fool it. This approach has two major advantages. First, because the discriminator network learns the structure in line drawings, it encourages the output sketches of the simplification network to be more similar in appearance to the training sketches. Second, we can also train the simplification network with additional unsupervised data, using the discriminator network as a substitute teacher. Thus, by adding only rough sketches without simplified line drawings, or only line drawings without the original rough sketches, we can improve the quality of the sketch simplification. We show how our framework can be used to train models that significantly outperform the state of the art in the sketch simplification task, despite using the same architecture for inference. We additionally present an approach to optimize for a single image, which improves accuracy at the cost of additional computation time. Finally, we show that, using the same framework, it is possible to train the network to perform the inverse problem, i.e., convert simple line sketches into pencil drawings, which is not possible using the standard mean squared error loss. We validate our framework with two user tests, where our approach is preferred to the state of the art in sketch simplification 92.3% of the time and obtains 1.2 more points on a scale of 1 to 5.|present integr framework train sketch simplif network convert challeng rough sketch clean line draw approach augment simplif network discrimin network train network joint discrimin network discern whether line draw real train data output simplif network turn tri fool approach two major advantag first becaus discrimin network learn structur line draw encourag output sketch simplif network similar appear train sketch second also train simplif network addit unsupervis data use discrimin network substitut teacher thus ad onli rough sketch without simplifi line draw onli line draw without origin rough sketch improv qualiti sketch simplif show framework use train model signific outperform state art sketch simplif task despit use architectur infer addit present approach optim singl imag improv accuraci cost addit comput time final show use framework possibl train network perform invers problem convert simpl line sketch pencil draw possibl use standard mean squar error loss valid framework two user test approach prefer state art sketch simplif time obtain point scale|['Edgar Simo-Serra', 'Satoshi Iizuka', 'Hiroshi Ishikawa']|['cs.CV']
2017-03-28T14:08:48Z|2017-03-27T07:49:43Z|http://arxiv.org/abs/1703.08961v1|http://arxiv.org/pdf/1703.08961v1|Scaling the Scattering Transform: Deep Hybrid Networks|scale scatter transform deep hybrid network|We use the scattering network as a generic and fixed initialization of the first layers of a supervised hybrid deep network. We show that early layers do not necessarily need to be learned, providing the best results to-date with pre-defined representations while being competitive with Deep CNNs. Using a shallow cascade of 1x1 convolutions, which encodes scattering coefficients that correspond to spatial windows of very small sizes, permits to obtain AlexNet accuracy on the imagenet ILSVRC2012. We demonstrate that this local encoding explicitly learns in-variance w.r.t. rotations. Combining scattering networks with a modern ResNet, we achieve a single-crop top 5 error of 11.4% on imagenet ILSVRC2012, comparable to the Resnet-18 architecture, while utilizing only 10 layers. We also find that hybrid architectures can yield excellent performance in the small sample regime, exceeding their end-to-end counterparts, through their ability to incorporate geometrical priors. We demonstrate this on subsets of the CIFAR-10 dataset and by setting a new state-of-the-art on the STL-10 dataset.|use scatter network generic fix initi first layer supervis hybrid deep network show earli layer necessarili need learn provid best result date pre defin represent competit deep cnns use shallow cascad convolut encod scatter coeffici correspond spatial window veri small size permit obtain alexnet accuraci imagenet ilsvrc demonstr local encod explicit learn varianc rotat combin scatter network modern resnet achiev singl crop top error imagenet ilsvrc compar resnet architectur util onli layer also find hybrid architectur yield excel perform small sampl regim exceed end end counterpart abil incorpor geometr prior demonstr subset cifar dataset set new state art stl dataset|['Edouard Oyallon', 'Eugene Belilovsky', 'Sergey Zagoruyko']|['cs.CV', 'cs.LG']
2017-03-28T14:08:48Z|2017-03-27T03:50:51Z|http://arxiv.org/abs/1703.08919v1|http://arxiv.org/pdf/1703.08919v1|MIHash: Online Hashing with Mutual Information|mihash onlin hash mutual inform|Learning-based adaptive hashing methods are widely used for nearest neighbor retrieval. Recently, online hashing methods have demonstrated a good performance-complexity tradeoff by learning hash functions from streaming data. In this paper, we aim to advance the state-of-the-art for online hashing. We first address a key challenge that has often been ignored: the binary codes for indexed data must be recomputed to keep pace with updates to the hash functions. We propose an efficient quality measure for hash functions, based on an information-theoretic quantity, mutual information, and use it successfully as a criterion to eliminate unnecessary hash table updates. Next, we show that mutual information can also be used as an objective in learning hash functions, using gradient-based optimization. Experiments on image retrieval benchmarks (including a 2.5M image dataset) confirm the effectiveness of our formulation, both in reducing hash table recomputations and in learning high-quality hash functions.|learn base adapt hash method wide use nearest neighbor retriev recent onlin hash method demonstr good perform complex tradeoff learn hash function stream data paper aim advanc state art onlin hash first address key challeng often ignor binari code index data must recomput keep pace updat hash function propos effici qualiti measur hash function base inform theoret quantiti mutual inform use success criterion elimin unnecessari hash tabl updat next show mutual inform also use object learn hash function use gradient base optim experi imag retriev benchmark includ imag dataset confirm effect formul reduc hash tabl recomput learn high qualiti hash function|['Fatih Cakir', 'Kun He', 'Sarah Adel Bargal', 'Stan Sclaroff']|['cs.CV']
2017-03-28T14:08:48Z|2017-03-27T03:46:58Z|http://arxiv.org/abs/1703.08917v1|http://arxiv.org/pdf/1703.08917v1|A Visual Measure of Changes to Weighted Self-Organizing Map Patterns|visual measur chang weight self organ map pattern|Estimating output changes by input changes is the main task in causal analysis. In previous work, input and output Self-Organizing Maps (SOMs) were associated for causal analysis of multivariate and nonlinear data. Based on the association, a weight distribution of the output conditional on a given input was obtained over the output map space. Such a weighted SOM pattern of the output changes when the input changes. In order to analyze the change, it is important to measure the difference of the patterns. Many methods have been proposed for the dissimilarity measure of patterns. However, it remains a major challenge when attempting to measure how the patterns change. In this paper, we propose a visualization approach that simplifies the comparison of the difference in terms of the pattern property. Using this approach, the change can be analyzed by integrating colors and star glyph shapes representing the property dissimilarity. Ecological data is used to demonstrate the usefulness of our approach and the experimental results show that our approach provides the change information effectively.|estim output chang input chang main task causal analysi previous work input output self organ map som associ causal analysi multivari nonlinear data base associ weight distribut output condit given input obtain output map space weight som pattern output chang input chang order analyz chang import measur differ pattern mani method propos dissimilar measur pattern howev remain major challeng attempt measur pattern chang paper propos visual approach simplifi comparison differ term pattern properti use approach chang analyz integr color star glyph shape repres properti dissimilar ecolog data use demonstr use approach experiment result show approach provid chang inform effect|['Younjin Chung', 'Joachim Gudmundsson', 'Masahiro Takatsuka']|['cs.CV']
2017-03-28T14:08:48Z|2017-03-27T03:08:58Z|http://arxiv.org/abs/1703.08912v1|http://arxiv.org/pdf/1703.08912v1|Exploiting Color Name Space for Salient Object Detection|exploit color name space salient object detect|In this paper, we will investigate the contribution of color names for salient object detection. Each input image is first converted to the color name space, which is consisted of 11 probabilistic channels. By exploring the topological structure relationship between the figure and the ground, we obtain a saliency map through a linear combination of a set of sequential attention maps. To overcome the limitation of only exploiting the surroundedness cue, two global cues with respect to color names are invoked for guiding the computation of another weighted saliency map. Finally, we integrate the two saliency maps into a unified framework to infer the saliency result. In addition, an improved post-processing procedure is introduced to effectively suppress the background while uniformly highlight the salient objects. Experimental results show that the proposed model produces more accurate saliency maps and performs well against 23 saliency models in terms of three evaluation metrics on three public datasets.|paper investig contribut color name salient object detect input imag first convert color name space consist probabilist channel explor topolog structur relationship figur ground obtain salienc map linear combin set sequenti attent map overcom limit onli exploit surrounded cue two global cue respect color name invok guid comput anoth weight salienc map final integr two salienc map unifi framework infer salienc result addit improv post process procedur introduc effect suppress background uniform highlight salient object experiment result show propos model produc accur salienc map perform well salienc model term three evalu metric three public dataset|['Jing Lou', 'Huan Wang', 'Longtao Chen', 'Qingyuan Xia', 'Wei Zhu', 'Mingwu Ren']|['cs.CV', 'I.4']
2017-03-28T14:08:48Z|2017-03-27T01:44:41Z|http://arxiv.org/abs/1703.08897v1|http://arxiv.org/pdf/1703.08897v1|Transductive Zero-Shot Learning with Adaptive Structural Embedding|transduct zero shot learn adapt structur embed|Zero-shot learning (ZSL) endows the computer vision system with the inferential capability to recognize instances of a new category that has never seen before. Two fundamental challenges in it are visual-semantic embedding and domain adaptation in cross-modality learning and unseen class prediction steps, respectively. To address both challenges, this paper presents two corresponding methods named Adaptive STructural Embedding (ASTE) and Self-PAsed Selective Strategy (SPASS), respectively. Specifically, ASTE formulates the visualsemantic interactions in a latent structural SVM framework to adaptively adjust the slack variables to embody the different reliableness among training instances. In this way, the reliable instances are imposed with small punishments, wheras the less reliable instances are imposed with more severe punishments. Thus, it ensures a more discriminative embedding. On the other hand, SPASS offers a framework to alleviate the domain shift problem in ZSL, which exploits the unseen data in an easy to hard fashion. Particularly, SPASS borrows the idea from selfpaced learning by iteratively selecting the unseen instances from reliable to less reliable to gradually adapt the knowledge from the seen domain to the unseen domain. Subsequently, by combining SPASS and ASTE, we present a self-paced Transductive ASTE (TASTE) method to progressively reinforce the classification capacity. Extensive experiments on three benchmark datasets (i.e., AwA, CUB, and aPY) demonstrate the superiorities of ASTE and TASTE. Furthermore, we also propose a fast training (FT) strategy to improve the efficiency of most of existing ZSL methods. The FT strategy is surprisingly simple and general enough, which can speed up the training time of most existing methods by 4~300 times while holding the previous performance.|zero shot learn zsl endow comput vision system inferenti capabl recogn instanc new categori never seen befor two fundament challeng visual semant embed domain adapt cross modal learn unseen class predict step respect address challeng paper present two correspond method name adapt structur embed ast self pase select strategi spass respect specif ast formul visualsemant interact latent structur svm framework adapt adjust slack variabl embodi differ reliabl among train instanc way reliabl instanc impos small punish whera less reliabl instanc impos sever punish thus ensur discrimin embed hand spass offer framework allevi domain shift problem zsl exploit unseen data easi hard fashion particular spass borrow idea selfpac learn iter select unseen instanc reliabl less reliabl gradual adapt knowledg seen domain unseen domain subsequ combin spass ast present self pace transduct ast tast method progress reinforc classif capac extens experi three benchmark dataset awa cub api demonstr superior ast tast furthermor also propos fast train ft strategi improv effici exist zsl method ft strategi surpris simpl general enough speed train time exist method time hold previous perform|['Yunlong Yu', 'Zhong Ji', 'Jichang Guo', 'Yanwei Pang']|['cs.CV']
2017-03-28T14:08:52Z|2017-03-27T01:36:38Z|http://arxiv.org/abs/1703.08893v1|http://arxiv.org/pdf/1703.08893v1|Transductive Zero-Shot Learning with a Self-training dictionary approach|transduct zero shot learn self train dictionari approach|As an important and challenging problem in computer vision, zero-shot learning (ZSL) aims at automatically recognizing the instances from unseen object classes without training data. To address this problem, ZSL is usually carried out in the following two aspects: 1) capturing the domain distribution connections between seen classes data and unseen classes data; and 2) modeling the semantic interactions between the image feature space and the label embedding space. Motivated by these observations, we propose a bidirectional mapping based semantic relationship modeling scheme that seeks for crossmodal knowledge transfer by simultaneously projecting the image features and label embeddings into a common latent space. Namely, we have a bidirectional connection relationship that takes place from the image feature space to the latent space as well as from the label embedding space to the latent space. To deal with the domain shift problem, we further present a transductive learning approach that formulates the class prediction problem in an iterative refining process, where the object classification capacity is progressively reinforced through bootstrapping-based model updating over highly reliable instances. Experimental results on three benchmark datasets (AwA, CUB and SUN) demonstrate the effectiveness of the proposed approach against the state-of-the-art approaches.|import challeng problem comput vision zero shot learn zsl aim automat recogn instanc unseen object class without train data address problem zsl usual carri follow two aspect captur domain distribut connect seen class data unseen class data model semant interact imag featur space label embed space motiv observ propos bidirect map base semant relationship model scheme seek crossmod knowledg transfer simultan project imag featur label embed common latent space name bidirect connect relationship take place imag featur space latent space well label embed space latent space deal domain shift problem present transduct learn approach formul class predict problem iter refin process object classif capac progress reinforc bootstrap base model updat high reliabl instanc experiment result three benchmark dataset awa cub sun demonstr effect propos approach state art approach|['Yunlong Yu', 'Zhong Ji', 'Xi Li', 'Jichang Guo', 'Zhongfei Zhang', 'Haibin Ling', 'Fei Wu']|['cs.CV']
2017-03-28T14:08:52Z|2017-03-26T20:28:02Z|http://arxiv.org/abs/1703.08866v1|http://arxiv.org/pdf/1703.08866v1|Multi-View Deep Learning for Consistent Semantic Mapping with RGB-D   Cameras|multi view deep learn consist semant map rgb camera|Visual scene understanding is an important capability that enables robots to purposefully act in their environment. In this paper, we propose a novel approach to object-class segmentation from multiple RGB-D views using deep learning. We train a deep neural network to predict object-class semantics that is consistent from several view points in a semi-supervised way. At test time, the semantics predictions of our network can be fused more consistently in semantic keyframe maps than predictions of a network trained on individual views. We base our network architecture on a recent single-view deep learning approach to RGB and depth fusion for semantic object-class segmentation and enhance it with multi-scale loss minimization. We obtain the camera trajectory using RGB-D SLAM and warp the predictions of RGB-D images into ground-truth annotated frames in order to enforce multi-view consistency during training. At test time, predictions from multiple views are fused into keyframes. We propose and analyze several methods for enforcing multi-view consistency during training and testing. We evaluate the benefit of multi-view consistency training and demonstrate that pooling of deep features and fusion over multiple views outperforms single-view baselines on the NYUDv2 benchmark for semantic segmentation. Our end-to-end trained network achieves state-of-the-art performance on the NYUDv2 dataset in single-view segmentation as well as multi-view semantic fusion.|visual scene understand import capabl enabl robot purpos act environ paper propos novel approach object class segment multipl rgb view use deep learn train deep neural network predict object class semant consist sever view point semi supervis way test time semant predict network fuse consist semant keyfram map predict network train individu view base network architectur recent singl view deep learn approach rgb depth fusion semant object class segment enhanc multi scale loss minim obtain camera trajectori use rgb slam warp predict rgb imag ground truth annot frame order enforc multi view consist dure train test time predict multipl view fuse keyfram propos analyz sever method enforc multi view consist dure train test evalu benefit multi view consist train demonstr pool deep featur fusion multipl view outperform singl view baselin nyudv benchmark semant segment end end train network achiev state art perform nyudv dataset singl view segment well multi view semant fusion|['Lingni Ma', 'J√∂rg St√ºckler', 'Christian Kerl', 'Daniel Cremers']|['cs.CV']
2017-03-28T14:08:52Z|2017-03-26T16:20:36Z|http://arxiv.org/abs/1703.08840v1|http://arxiv.org/pdf/1703.08840v1|Inferring The Latent Structure of Human Decision-Making from Raw Visual   Inputs|infer latent structur human decis make raw visual input|The goal of imitation learning is to match example expert behavior, without access to a reinforcement signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are not explicitly modeled. We introduce an extension to the Generative Adversarial Imitation Learning method that can infer the latent structure of human decision-making in an unsupervised way. Our method can not only imitate complex behaviors, but also learn interpretable and meaningful representations. We demonstrate that the approach is applicable to high-dimensional environments including raw visual inputs. In the highway driving domain, we show that a model learned from demonstrations is able to both produce different styles of human-like driving behaviors and accurately anticipate human actions. Our method surpasses various baselines in terms of performance and functionality.|goal imit learn match exampl expert behavior without access reinforc signal expert demonstr provid human howev often show signific variabl due latent factor explicit model introduc extens generat adversari imit learn method infer latent structur human decis make unsupervis way method onli imit complex behavior also learn interpret meaning represent demonstr approach applic high dimension environ includ raw visual input highway drive domain show model learn demonstr abl produc differ style human like drive behavior accur anticip human action method surpass various baselin term perform function|['Yunzhu Li', 'Jiaming Song', 'Stefano Ermon']|['cs.LG', 'cs.AI', 'cs.CV']
2017-03-28T14:08:52Z|2017-03-26T16:18:48Z|http://arxiv.org/abs/1703.08837v1|http://arxiv.org/abs/1703.08837v1|Person Re-Identification by Camera Correlation Aware Feature   Augmentation|person identif camera correl awar featur augment|The challenge of person re-identification (re-id) is to match individual images of the same person captured by different non-overlapping camera views against significant and unknown cross-view feature distortion. While a large number of distance metric/subspace learning models have been developed for re-id, the cross-view transformations they learned are view-generic and thus potentially less effective in quantifying the feature distortion inherent to each camera view. Learning view-specific feature transformations for re-id (i.e., view-specific re-id), an under-studied approach, becomes an alternative resort for this problem. In this work, we formulate a novel view-specific person re-identification framework from the feature augmentation point of view, called Camera coRrelation Aware Feature augmenTation (CRAFT). Specifically, CRAFT performs cross-view adaptation by automatically measuring camera correlation from cross-view visual data distribution and adaptively conducting feature augmentation to transform the original features into a new adaptive space. Through our augmentation framework, view-generic learning algorithms can be readily generalized to learn and optimize view-specific sub-models whilst simultaneously modelling view-generic discrimination information. Therefore, our framework not only inherits the strength of view-generic model learning but also provides an effective way to take into account view specific characteristics. Our CRAFT framework can be extended to jointly learn view-specific feature transformations for person re-id across a large network with more than two cameras, a largely under-investigated but realistic re-id setting. Additionally, we present a domain-generic deep person appearance representation which is designed particularly to be towards view invariant for facilitating cross-view adaptation by CRAFT.|challeng person identif id match individu imag person captur differ non overlap camera view signific unknown cross view featur distort larg number distanc metric subspac learn model develop id cross view transform learn view generic thus potenti less effect quantifi featur distort inher camera view learn view specif featur transform id view specif id studi approach becom altern resort problem work formul novel view specif person identif framework featur augment point view call camera correl awar featur augment craft specif craft perform cross view adapt automat measur camera correl cross view visual data distribut adapt conduct featur augment transform origin featur new adapt space augment framework view generic learn algorithm readili general learn optim view specif sub model whilst simultan model view generic discrimin inform therefor framework onli inherit strength view generic model learn also provid effect way take account view specif characterist craft framework extend joint learn view specif featur transform person id across larg network two camera larg investig realist id set addit present domain generic deep person appear represent design particular toward view invari facilit cross view adapt craft|['Ying-Cong Chen', 'Xiatian Zhu', 'Wei-Shi Zheng', 'Jian-Huang Lai']|['cs.CV']
2017-03-28T14:08:52Z|2017-03-26T16:17:55Z|http://arxiv.org/abs/1703.08836v1|http://arxiv.org/pdf/1703.08836v1|Learned multi-patch similarity|learn multi patch similar|Estimating a depth map from multiple views of a scene is a fundamental task in computer vision. As soon as more than two viewpoints are available, one faces the very basic question how to measure similarity across >2 image patches. Surprisingly, no direct solution exists, instead it is common to fall back to more or less robust averaging of two-view similarities. Encouraged by the success of machine learning, and in particular convolutional neural networks, we propose to learn a matching function which directly maps multiple image patches to a scalar similarity score. Experiments on several multi-view datasets demonstrate that this approach has advantages over methods based on pairwise patch similarity.|estim depth map multipl view scene fundament task comput vision soon two viewpoint avail one face veri basic question measur similar across imag patch surpris direct solut exist instead common fall back less robust averag two view similar encourag success machin learn particular convolut neural network propos learn match function direct map multipl imag patch scalar similar score experi sever multi view dataset demonstr approach advantag method base pairwis patch similar|['Wilfried Hartmann', 'Silvano Galliani', 'Michal Havlena', 'Konrad Schindler', 'Luc Van Gool']|['cs.CV', 'cs.LG']
2017-03-28T14:08:52Z|2017-03-26T06:34:45Z|http://arxiv.org/abs/1703.08774v1|http://arxiv.org/pdf/1703.08774v1|Who Said What: Modeling Individual Labelers Improves Classification|said model individu label improv classif|Data are often labeled by many different experts with each expert only labeling a small fraction of the data and each data point being labeled by several experts. This reduces the workload on individual experts and also gives a better estimate of the unobserved ground truth. When experts disagree, the standard approaches are to treat the majority opinion as the correct label or to model the correct label as a distribution. These approaches, however, do not make any use of potentially valuable information about which expert produced which label. To make use of this extra information, we propose modeling the experts individually and then learning averaging weights for combining them, possibly in sample-specific ways. This allows us to give more weight to more reliable experts and take advantage of the unique strengths of individual experts at classifying certain types of data. Here we show that our approach leads to improvements in computer-aided diagnosis of diabetic retinopathy. We also show that our method performs better than competing algorithms by Welinder and Perona, and by Mnih and Hinton. Our work offers an innovative approach for dealing with the myriad real-world settings that use expert opinions to define labels for training.|data often label mani differ expert expert onli label small fraction data data point label sever expert reduc workload individu expert also give better estim unobserv ground truth expert disagre standard approach treat major opinion correct label model correct label distribut approach howev make ani use potenti valuabl inform expert produc label make use extra inform propos model expert individu learn averag weight combin possibl sampl specif way allow us give weight reliabl expert take advantag uniqu strength individu expert classifi certain type data show approach lead improv comput aid diagnosi diabet retinopathi also show method perform better compet algorithm welind perona mnih hinton work offer innov approach deal myriad real world set use expert opinion defin label train|['Melody Y. Guan', 'Varun Gulshan', 'Andrew M. Dai', 'Geoffrey E. Hinton']|['cs.LG', 'cs.CV']
2017-03-28T14:08:52Z|2017-03-26T05:53:39Z|http://arxiv.org/abs/1703.08772v1|http://arxiv.org/pdf/1703.08772v1|Multivariate Regression with Gross Errors on Manifold-valued Data|multivari regress gross error manifold valu data|We consider the topic of multivariate regression on manifold-valued output, that is, for a multivariate observation, its output response lies on a manifold. Moreover, we propose a new regression model to deal with the presence of grossly corrupted manifold-valued responses, a bottleneck issue commonly encountered in practical scenarios. Our model first takes a correction step on the grossly corrupted responses via geodesic curves on the manifold, and then performs multivariate linear regression on the corrected data. This results in a nonconvex and nonsmooth optimization problem on manifolds. To this end, we propose a dedicated approach named PALMR, by utilizing and extending the proximal alternating linearized minimization techniques. Theoretically, we investigate its convergence property, where it is shown to converge to a critical point under mild conditions. Empirically, we test our model on both synthetic and real diffusion tensor imaging data, and show that our model outperforms other multivariate regression models when manifold-valued responses contain gross errors, and is effective in identifying gross errors.|consid topic multivari regress manifold valu output multivari observ output respons lie manifold moreov propos new regress model deal presenc grossli corrupt manifold valu respons bottleneck issu common encount practic scenario model first take correct step grossli corrupt respons via geodes curv manifold perform multivari linear regress correct data result nonconvex nonsmooth optim problem manifold end propos dedic approach name palmr util extend proxim altern linear minim techniqu theoret investig converg properti shown converg critic point mild condit empir test model synthet real diffus tensor imag data show model outperform multivari regress model manifold valu respons contain gross error effect identifi gross error|['Xiaowei Zhang', 'Xudong Shi', 'Yu Sun', 'Li Cheng']|['stat.ML', 'cs.CV', 'math.OC']
2017-03-28T14:08:52Z|2017-03-26T05:48:38Z|http://arxiv.org/abs/1703.08770v1|http://arxiv.org/pdf/1703.08770v1|SCAN: Structure Correcting Adversarial Network for Chest X-rays Organ   Segmentation|scan structur correct adversari network chest ray organ segment|Chest X-ray (CXR) is one of the most commonly prescribed medical imaging procedures, often with over 2-10x more scans than other imaging modalities such as MRI, CT scan, and PET scans. These voluminous CXR scans place significant workloads on radiologists and medical practitioners. Organ segmentation is a crucial step to obtain effective computer-aided detection on CXR. In this work, we propose Structure Correcting Adversarial Network (SCAN) to segment lung fields and the heart in CXR images. SCAN incorporates a critic network to impose on the convolutional segmentation network the structural regularities emerging from human physiology. During training, the critic network learns to discriminate between the ground truth organ annotations from the masks synthesized by the segmentation network. Through this adversarial process the critic network learns the higher order structures and guides the segmentation model to achieve realistic segmentation outcomes. Extensive experiments show that our method produces highly accurate and natural segmentation. Using only very limited training data available, our model reaches human-level performance without relying on any existing trained model or dataset. Our method also generalizes well to CXR images from a different patient population and disease profiles, surpassing the current state-of-the-art.|chest ray cxr one common prescrib medic imag procedur often scan imag modal mri ct scan pet scan volumin cxr scan place signific workload radiologist medic practition organ segment crucial step obtain effect comput aid detect cxr work propos structur correct adversari network scan segment lung field heart cxr imag scan incorpor critic network impos convolut segment network structur regular emerg human physiolog dure train critic network learn discrimin ground truth organ annot mask synthes segment network adversari process critic network learn higher order structur guid segment model achiev realist segment outcom extens experi show method produc high accur natur segment use onli veri limit train data avail model reach human level perform without reli ani exist train model dataset method also general well cxr imag differ patient popul diseas profil surpass current state art|['Wei Dai', 'Joseph Doyle', 'Xiaodan Liang', 'Hao Zhang', 'Nanqing Dong', 'Yuan Li', 'Eric P. Xing']|['cs.CV']
2017-03-28T14:08:52Z|2017-03-26T05:44:56Z|http://arxiv.org/abs/1703.08769v1|http://arxiv.org/pdf/1703.08769v1|Open Vocabulary Scene Parsing|open vocabulari scene pars|Recognizing arbitrary objects in the wild has been a challenging problem due to the limitations of existing classification models and datasets. In this paper, we propose a new task that aims at parsing scene with a large and open vocabulary, and several evaluation metrics are explored for this problem. Our proposed approach to this problem is a joint image pixel and word concept embeddings framework, where word concepts are connected by semantic relations. We validate the open vocabulary prediction ability of our framework on ADE20K dataset which covers a wide variety of scenes and objects. We further explore the trained joint embedding space to show its interpretability.|recogn arbitrari object wild challeng problem due limit exist classif model dataset paper propos new task aim pars scene larg open vocabulari sever evalu metric explor problem propos approach problem joint imag pixel word concept embed framework word concept connect semant relat valid open vocabulari predict abil framework adek dataset cover wide varieti scene object explor train joint embed space show interpret|['Hang Zhao', 'Xavier Puig', 'Bolei Zhou', 'Sanja Fidler', 'Antonio Torralba']|['cs.CV', 'cs.AI']
2017-03-28T14:08:52Z|2017-03-26T04:15:10Z|http://arxiv.org/abs/1703.08764v1|http://arxiv.org/pdf/1703.08764v1|Structured Learning of Tree Potentials in CRF for Image Segmentation|structur learn tree potenti crf imag segment|We propose a new approach to image segmentation, which exploits the advantages of both conditional random fields (CRFs) and decision trees. In the literature, the potential functions of CRFs are mostly defined as a linear combination of some pre-defined parametric models, and then methods like structured support vector machines (SSVMs) are applied to learn those linear coefficients. We instead formulate the unary and pairwise potentials as nonparametric forests---ensembles of decision trees, and learn the ensemble parameters and the trees in a unified optimization problem within the large-margin framework. In this fashion, we easily achieve nonlinear learning of potential functions on both unary and pairwise terms in CRFs. Moreover, we learn class-wise decision trees for each object that appears in the image. Due to the rich structure and flexibility of decision trees, our approach is powerful in modelling complex data likelihoods and label relationships. The resulting optimization problem is very challenging because it can have exponentially many variables and constraints. We show that this challenging optimization can be efficiently solved by combining a modified column generation and cutting-planes techniques. Experimental results on both binary (Graz-02, Weizmann horse, Oxford flower) and multi-class (MSRC-21, PASCAL VOC 2012) segmentation datasets demonstrate the power of the learned nonlinear nonparametric potentials.|propos new approach imag segment exploit advantag condit random field crfs decis tree literatur potenti function crfs defin linear combin pre defin parametr model method like structur support vector machin ssvms appli learn linear coeffici instead formul unari pairwis potenti nonparametr forest ensembl decis tree learn ensembl paramet tree unifi optim problem within larg margin framework fashion easili achiev nonlinear learn potenti function unari pairwis term crfs moreov learn class wise decis tree object appear imag due rich structur flexibl decis tree approach power model complex data likelihood label relationship result optim problem veri challeng becaus exponenti mani variabl constraint show challeng optim effici solv combin modifi column generat cut plane techniqu experiment result binari graz weizmann hors oxford flower multi class msrc pascal voc segment dataset demonstr power learn nonlinear nonparametr potenti|['Fayao Liu', 'Guosheng Lin', 'Ruizhi Qiao', 'Chunhua Shen']|['cs.CV']
2017-03-28T14:08:55Z|2017-03-25T20:33:45Z|http://arxiv.org/abs/1703.08738v1|http://arxiv.org/pdf/1703.08738v1|Sketch-based Face Editing in Video Using Identity Deformation Transfer|sketch base face edit video use ident deform transfer|We address the problem of using hand-drawn sketch to edit facial identity, such as enlarging the shape or modifying the position of eyes or mouth, in the whole video. This task is formulated as a 3D face model reconstruction and deformation problem. We first introduce a two-stage real-time 3D face model fitting schema to recover facial identity and expressions from the video. We recognize the user's editing intention from the input sketch as a set of facial modifications. A novel identity deformation algorithm is then proposed to transfer these deformations from 2D space to 3D facial identity directly, while preserving the facial expressions. Finally, these changes are propagated to the whole video with the modified identity. Experimental results demonstrate that our method can effectively edit facial identity in video based on the input sketch with high consistency and fidelity.|address problem use hand drawn sketch edit facial ident enlarg shape modifi posit eye mouth whole video task formul face model reconstruct deform problem first introduc two stage real time face model fit schema recov facial ident express video recogn user edit intent input sketch set facial modif novel ident deform algorithm propos transfer deform space facial ident direct preserv facial express final chang propag whole video modifi ident experiment result demonstr method effect edit facial ident video base input sketch high consist fidel|['Long Zhao', 'Fangda Han', 'Mubbasir Kapadia', 'Vladimir Pavlovic', 'Dimitris Metaxas']|['cs.CV']
2017-03-28T14:08:56Z|2017-03-25T16:49:03Z|http://arxiv.org/abs/1703.08710v1|http://arxiv.org/pdf/1703.08710v1|Count-ception: Counting by Fully Convolutional Redundant Counting|count ception count fulli convolut redund count|Counting objects in digital images is a process that should be replaced by machines. This tedious task is time consuming and prone to errors due to fatigue of human annotators. The goal is to have a system that takes as input an image and returns a count of the objects inside and justification for the prediction in the form of object localization. We repose a problem, originally posed by Lempitsky and Zisserman, to instead predict a count map which contains redundant counts based on the receptive field of a smaller regression network. The regression network predicts a count of the objects that exist inside this frame. By processing the image in a fully convolutional way each pixel is going to be accounted for some number of times, the number of windows which include it, which is the size of each window, (i.e., 32x32 = 1024). To recover the true count take the average over the redundant predictions. Our contribution is redundant counting instead of predicting a density map in order to average over errors. We also propose a novel deep neural network architecture adapted from the Inception family of networks called the Count-ception network. Together our approach results in a 20% gain over the state of the art method by Xie, Noble, and Zisserman in 2016.|count object digit imag process replac machin tedious task time consum prone error due fatigu human annot goal system take input imag return count object insid justif predict form object local repos problem origin pose lempitski zisserman instead predict count map contain redund count base recept field smaller regress network regress network predict count object exist insid frame process imag fulli convolut way pixel go account number time number window includ size window recov true count take averag redund predict contribut redund count instead predict densiti map order averag error also propos novel deep neural network architectur adapt incept famili network call count ception network togeth approach result gain state art method xie nobl zisserman|['Joseph Paul Cohen', 'Henry Z. Lo', 'Yoshua Bengio']|['cs.CV', 'cs.LG', 'stat.ML']
2017-03-28T14:08:56Z|2017-03-25T14:36:12Z|http://arxiv.org/abs/1703.08697v1|http://arxiv.org/abs/1703.08697v1|Improving the Accuracy of the CogniLearn System for Cognitive Behavior   Assessment|improv accuraci cognilearn system cognit behavior assess|"HTKS is a game-like cognitive assessment method, designed for children between four and eight years of age. During the HTKS assessment, a child responds to a sequence of requests, such as ""touch your head"" or ""touch your toes"". The cognitive challenge stems from the fact that the children are instructed to interpret these requests not literally, but by touching a different body part than the one stated. In prior work, we have developed the CogniLearn system, that captures data from subjects performing the HTKS game, and analyzes the motion of the subjects. In this paper we propose some specific improvements that make the motion analysis module more accurate. As a result of these improvements, the accuracy in recognizing cases where subjects touch their toes has gone from 76.46% in our previous work to 97.19% in this paper."|htks game like cognit assess method design children four eight year age dure htks assess child respond sequenc request touch head touch toe cognit challeng stem fact children instruct interpret request liter touch differ bodi part one state prior work develop cognilearn system captur data subject perform htks game analyz motion subject paper propos specif improv make motion analysi modul accur result improv accuraci recogn case subject touch toe gone previous work paper|['Amir Ghaderi', 'Srujana Gattupalli', 'Dylan Ebert', 'Ali Sharifara', 'Vassilis Athitsos', 'Fillia Makedon']|['cs.CV']
2017-03-28T14:08:56Z|2017-03-25T06:18:38Z|http://arxiv.org/abs/1703.08653v1|http://arxiv.org/pdf/1703.08653v1|Bayesian Optimization for Refining Object Proposals|bayesian optim refin object propos|We develop a general-purpose algorithm using a Bayesian optimization framework for the efficient refinement of object proposals. While recent research has achieved substantial progress for object localization and related objectives in computer vision, current state-of-the-art object localization procedures are nevertheless encumbered by inefficiency and inaccuracy. We present a novel, computationally efficient method for refining inaccurate bounding-box proposals for a target object using Bayesian optimization. Offline, image features from a convolutional neural network are used to train a model to predict the offset distance of an object proposal from a target object. Online, this model is used in a Bayesian active search to improve inaccurate object proposals. In experiments, we compare our approach to a state-of-the-art bounding-box regression method for localization refinement of pedestrian object proposals. Our method exhibits a substantial improvement for the task of localization refinement over this baseline regression method.|develop general purpos algorithm use bayesian optim framework effici refin object propos recent research achiev substanti progress object local relat object comput vision current state art object local procedur nevertheless encumb ineffici inaccuraci present novel comput effici method refin inaccur bound box propos target object use bayesian optim offlin imag featur convolut neural network use train model predict offset distanc object propos target object onlin model use bayesian activ search improv inaccur object propos experi compar approach state art bound box regress method local refin pedestrian object propos method exhibit substanti improv task local refin baselin regress method|['Anthony D. Rhodes', 'Jordan Witte', 'Melanie Mitchell', 'Bruno Jedynak']|['cs.CV']
2017-03-28T14:08:56Z|2017-03-25T05:51:42Z|http://arxiv.org/abs/1703.08651v1|http://arxiv.org/pdf/1703.08651v1|More is Less: A More Complicated Network with Less Inference Complexity|less complic network less infer complex|In this paper, we present a novel and general network structure towards accelerating the inference process of convolutional neural networks, which is more complicated in network structure yet with less inference complexity. The core idea is to equip each original convolutional layer with another low-cost collaborative layer (LCCL), and the element-wise multiplication of the ReLU outputs of these two parallel layers produces the layer-wise output. The combined layer is potentially more discriminative than the original convolutional layer, and its inference is faster for two reasons: 1) the zero cells of the LCCL feature maps will remain zero after element-wise multiplication, and thus it is safe to skip the calculation of the corresponding high-cost convolution in the original convolutional layer, 2) LCCL is very fast if it is implemented as a 1*1 convolution or only a single filter shared by all channels. Extensive experiments on the CIFAR-10, CIFAR-100 and ILSCRC-2012 benchmarks show that our proposed network structure can accelerate the inference process by 32\% on average with negligible performance drop.|paper present novel general network structur toward acceler infer process convolut neural network complic network structur yet less infer complex core idea equip origin convolut layer anoth low cost collabor layer lccl element wise multipl relu output two parallel layer produc layer wise output combin layer potenti discrimin origin convolut layer infer faster two reason zero cell lccl featur map remain zero element wise multipl thus safe skip calcul correspond high cost convolut origin convolut layer lccl veri fast implement convolut onli singl filter share channel extens experi cifar cifar ilscrc benchmark show propos network structur acceler infer process averag neglig perform drop|['Xuanyi Dong', 'Junshi Huang', 'Yi Yang', 'Shuicheng Yan']|['cs.CV']
2017-03-28T14:08:56Z|2017-03-24T23:50:52Z|http://arxiv.org/abs/1703.08628v1|http://arxiv.org/pdf/1703.08628v1|AMAT: Medial Axis Transform for Natural Images|amat medial axi transform natur imag|The medial axis transform (MAT) is a powerful shape abstraction that has been successfully used in shape editing, matching and retrieval. Despite its long history, the MAT has not found widespread use in tasks involving natural images, due to the lack of a generalization that accommodates color and texture. In this paper we introduce Appearance-MAT (AMAT), by framing the MAT of natural images as a weighted geometric set cover problem. We make the following contributions: i) we extend previous medial point detection methods for color images, by associating each medial point with a local scale; ii) inspired by the invertibility property of the binary MAT, we also associate each medial point with a local encoding that allows us to invert the AMAT, reconstructing the input image; iii) we describe a clustering scheme that takes advantage of the additional scale and appearance information to group individual points into medial branches, providing a shape decomposition of the underlying image regions. In our experiments, we show state-of-the-art performance in medial point detection on Berkeley Medial AXes (BMAX500), a new dataset of medial axes based on the established BSDS500 database. We also measure the quality of reconstructed images from the same dataset, obtained by inverting their computed AMAT. Our approach delivers significantly better reconstruction quality with respect to three baselines, using just 10% of the image pixels. Our code is available at https://github.com/tsogkas/amat.|medial axi transform mat power shape abstract success use shape edit match retriev despit long histori mat found widespread use task involv natur imag due lack general accommod color textur paper introduc appear mat amat frame mat natur imag weight geometr set cover problem make follow contribut extend previous medial point detect method color imag associ medial point local scale ii inspir invert properti binari mat also associ medial point local encod allow us invert amat reconstruct input imag iii describ cluster scheme take advantag addit scale appear inform group individu point medial branch provid shape decomposit imag region experi show state art perform medial point detect berkeley medial axe bmax new dataset medial axe base establish bsds databas also measur qualiti reconstruct imag dataset obtain invert comput amat approach deliv signific better reconstruct qualiti respect three baselin use imag pixel code avail https github com tsogka amat|['Stavros Tsogkas', 'Sven Dickinson']|['cs.CV']
2017-03-28T14:08:56Z|2017-03-24T22:43:05Z|http://arxiv.org/abs/1703.08617v1|http://arxiv.org/pdf/1703.08617v1|Temporal Non-Volume Preserving Approach to Facial Age-Progression and   Age-Invariant Face Recognition|tempor non volum preserv approach facial age progress age invari face recognit|Modeling the long-term facial aging process is extremely challenging due to the presence of large and non-linear variations during the face development stages. In order to efficiently address the problem, this work first decomposes the aging process into multiple short-term stages. Then, a novel generative probabilistic model, named Temporal Non-Volume Preserving (TNVP) transformation, is presented to model the facial aging process at each stage. Unlike Generative Adversarial Networks (GANs), which requires an empirical balance threshold, and Restricted Boltzmann Machines (RBM), an intractable model, our proposed TNVP approach guarantees a tractable density function, exact inference and evaluation for embedding the feature transformations between faces in consecutive stages. Our model shows its advantages not only in capturing the non-linear age related variance in each stage but also producing a smooth synthesis in age progression across faces. Our approach can model any face in the wild provided with only four basic landmark points. Moreover, the structure can be transformed into a deep convolutional network while keeping the advantages of probabilistic models with tractable log-likelihood density estimation. Our method is evaluated in both terms of synthesizing age-progressed faces and cross-age face verification and consistently shows the state-of-the-art results in various face aging databases, i.e. FG-NET, MORPH, AginG Faces in the Wild (AGFW), and Cross-Age Celebrity Dataset (CACD). A large-scale face verification on Megaface challenge 1 is also performed to further show the advantages of our proposed approach.|model long term facial age process extrem challeng due presenc larg non linear variat dure face develop stage order effici address problem work first decompos age process multipl short term stage novel generat probabilist model name tempor non volum preserv tnvp transform present model facial age process stage unlik generat adversari network gan requir empir balanc threshold restrict boltzmann machin rbm intract model propos tnvp approach guarante tractabl densiti function exact infer evalu embed featur transform face consecut stage model show advantag onli captur non linear age relat varianc stage also produc smooth synthesi age progress across face approach model ani face wild provid onli four basic landmark point moreov structur transform deep convolut network keep advantag probabilist model tractabl log likelihood densiti estim method evalu term synthes age progress face cross age face verif consist show state art result various face age databas fg net morph age face wild agfw cross age celebr dataset cacd larg scale face verif megafac challeng also perform show advantag propos approach|['Chi Nhan Duong', 'Kha Gia Quach', 'Khoa Luu', 'T. Hoang Ngan le', 'Marios Savvides']|['cs.CV']
2017-03-28T14:08:56Z|2017-03-24T21:26:16Z|http://arxiv.org/abs/1703.08603v1|http://arxiv.org/pdf/1703.08603v1|Adversarial Examples for Semantic Segmentation and Object Detection|adversari exampl semant segment object detect|It has been well demonstrated that adversarial examples, i.e., natural images with visually imperceptible perturbations added, generally exist for deep networks to fail on image classification. In this paper, we extend adversarial examples to semantic segmentation and object detection which are much more difficult. Our observation is that both segmentation and detection are based on classifying multiple targets on an image (e.g., the basic target is a pixel or a receptive field in segmentation, and an object proposal in detection), which inspires us to optimize a loss function over a set of pixels/proposals for generating adversarial perturbations. Based on this idea, we propose a novel algorithm named Dense Adversary Generation (DAG), which generates a large family of adversarial examples, and applies to a wide range of state-of-the-art deep networks for segmentation and detection. We also find that the adversarial perturbations can be transferred across networks with different training data, based on different architectures, and even for different recognition tasks. In particular, the transferability across networks with the same architecture is more significant than in other cases. Besides, summing up heterogeneous perturbations often leads to better transfer performance, which provides an effective method of black-box adversarial attack.|well demonstr adversari exampl natur imag visual impercept perturb ad general exist deep network fail imag classif paper extend adversari exampl semant segment object detect much difficult observ segment detect base classifi multipl target imag basic target pixel recept field segment object propos detect inspir us optim loss function set pixel propos generat adversari perturb base idea propos novel algorithm name dens adversari generat dag generat larg famili adversari exampl appli wide rang state art deep network segment detect also find adversari perturb transfer across network differ train data base differ architectur even differ recognit task particular transfer across network architectur signific case besid sum heterogen perturb often lead better transfer perform provid effect method black box adversari attack|['Cihang Xie', 'Jianyu Wang', 'Zhishuai Zhang', 'Yuyin Zhou', 'Lingxi Xie', 'Alan Yuille']|['cs.CV']
2017-03-28T14:08:56Z|2017-03-24T19:43:20Z|http://arxiv.org/abs/1703.08580v1|http://arxiv.org/pdf/1703.08580v1|Deep Residual Learning for Instrument Segmentation in Robotic Surgery|deep residu learn instrument segment robot surgeri|Detection, tracking, and pose estimation of surgical instruments are crucial tasks for computer assistance during minimally invasive robotic surgery. In the majority of cases, the first step is the automatic segmentation of surgical tools. Prior work has focused on binary segmentation, where the objective is to label every pixel in an image as tool or background. We improve upon previous work in two major ways. First, we leverage recent techniques such as deep residual learning and dilated convolutions to advance binary-segmentation performance. Second, we extend the approach to multi-class segmentation, which lets us segment different parts of the tool, in addition to background. We demonstrate the performance of this method on the MICCAI Endoscopic Vision Challenge Robotic Instruments dataset.|detect track pose estim surgic instrument crucial task comput assist dure minim invas robot surgeri major case first step automat segment surgic tool prior work focus binari segment object label everi pixel imag tool background improv upon previous work two major way first leverag recent techniqu deep residu learn dilat convolut advanc binari segment perform second extend approach multi class segment let us segment differ part tool addit background demonstr perform method miccai endoscop vision challeng robot instrument dataset|['Daniil Pakhomov', 'Vittal Premachandran', 'Max Allan', 'Mahdi Azizian', 'Nassir Navab']|['cs.CV']
2017-03-28T14:08:56Z|2017-03-24T17:14:58Z|http://arxiv.org/abs/1703.08516v1|http://arxiv.org/pdf/1703.08516v1|Radiomics strategies for risk assessment of tumour failure in   head-and-neck cancer|radiom strategi risk assess tumour failur head neck cancer|Quantitative extraction of high-dimensional mineable data from medical images is a process known as radiomics. Radiomics is foreseen as an essential prognostic tool for cancer risk assessment and the quantification of intratumoural heterogeneity. In this work, 1615 radiomic features (quantifying tumour image intensity, shape, texture) extracted from pre-treatment FDG-PET and CT images of 300 patients from four different cohorts were analyzed for the risk assessment of locoregional recurrences (LR) and distant metastases (DM) in head-and-neck cancer. Prediction models combining radiomic and clinical variables were constructed via random forests and imbalance-adjustment strategies using two of the four cohorts. Independent validation of the prediction and prognostic performance of the models was carried out on the other two cohorts (LR: AUC = 0.69 and CI = 0.67; DM: AUC = 0.86 and CI = 0.88). Furthermore, the results obtained via Kaplan-Meier analysis demonstrated the potential of radiomics for assessing the risk of specific tumour outcomes using multiple stratification groups. This could have important clinical impact, notably by allowing for a better personalization of chemo-radiation treatments for head-and-neck cancer patients from different risk groups.|quantit extract high dimension mineabl data medic imag process known radiom radiom foreseen essenti prognost tool cancer risk assess quantif intratumour heterogen work radiom featur quantifi tumour imag intens shape textur extract pre treatment fdg pet ct imag patient four differ cohort analyz risk assess locoregion recurr lr distant metastas dm head neck cancer predict model combin radiom clinic variabl construct via random forest imbal adjust strategi use two four cohort independ valid predict prognost perform model carri two cohort lr auc ci dm auc ci furthermor result obtain via kaplan meier analysi demonstr potenti radiom assess risk specif tumour outcom use multipl stratif group could import clinic impact notabl allow better person chemo radiat treatment head neck cancer patient differ risk group|['Martin Valli√®res', 'Emily Kay-Rivest', 'L√©o Jean Perrin', 'Xavier Liem', 'Christophe Furstoss', 'Hugo J. W. L. Aerts', 'Nader Khaouam', 'Phuc Felix Nguyen-Tan', 'Chang-Shu Wang', 'Khalil Sultanem', 'Jan Seuntjens', 'Issam El Naqa']|['cs.CV', 'I.2.1; I.2.10; I.4.7; I.4.9; J.3']
2017-03-28T14:09:00Z|2017-03-24T16:41:19Z|http://arxiv.org/abs/1703.08497v1|http://arxiv.org/pdf/1703.08497v1|Local Deep Neural Networks for Age and Gender Classification|local deep neural network age gender classif|Local deep neural networks have been recently introduced for gender recognition. Although, they achieve very good performance they are very computationally expensive to train. In this work, we introduce a simplified version of local deep neural networks which significantly reduces the training time. Instead of using hundreds of patches per image, as suggested by the original method, we propose to use 9 overlapping patches per image which cover the entire face region. This results in a much reduced training time, since just 9 patches are extracted per image instead of hundreds, at the expense of a slightly reduced performance. We tested the proposed modified local deep neural networks approach on the LFW and Adience databases for the task of gender and age classification. For both tasks and both databases the performance is up to 1% lower compared to the original version of the algorithm. We have also investigated which patches are more discriminative for age and gender classification. It turns out that the mouth and eyes regions are useful for age classification, whereas just the eye region is useful for gender classification.|local deep neural network recent introduc gender recognit although achiev veri good perform veri comput expens train work introduc simplifi version local deep neural network signific reduc train time instead use hundr patch per imag suggest origin method propos use overlap patch per imag cover entir face region result much reduc train time sinc patch extract per imag instead hundr expens slight reduc perform test propos modifi local deep neural network approach lfw adienc databas task gender age classif task databas perform lower compar origin version algorithm also investig patch discrimin age gender classif turn mouth eye region use age classif wherea eye region use gender classif|['Zukang Liao', 'Stavros Petridis', 'Maja Pantic']|['cs.CV']
2017-03-28T14:09:00Z|2017-03-24T16:28:57Z|http://arxiv.org/abs/1703.08493v1|http://arxiv.org/pdf/1703.08493v1|Multi-stage Multi-recursive-input Fully Convolutional Networks for   Neuronal Boundary Detection|multi stage multi recurs input fulli convolut network neuron boundari detect|In the field of connectomics, neuroscientists seek to identify cortical connectivity comprehensively. Neuronal boundary detection from the Electron Microscopy (EM) images is often done to assist the automatic reconstruction of neuronal circuit. But the segmentation of EM images is a challenging problem, as it requires the detector to be able to detect both filament-like thin and blob-like thick membrane, while suppressing the ambiguous intracellular structure. In this paper, we propose multi-stage multi-recursive-input fully convolutional networks to address this problem. The multiple recursive inputs for one stage, i.e., the multiple side outputs with different receptive field sizes learned from the lower stage, provide multi-scale contextual boundary information for the consecutive learning. This design is biologically-plausible, as it likes a human visual system to compare different possible segmentation solutions to address the ambiguous boundary issue. Our multi-stage networks are trained end-to-end. It achieves promising results on a public available mouse piriform cortex dataset, which significantly outperforms other competitors.|field connectom neuroscientist seek identifi cortic connect comprehens neuron boundari detect electron microscopi em imag often done assist automat reconstruct neuron circuit segment em imag challeng problem requir detector abl detect filament like thin blob like thick membran suppress ambigu intracellular structur paper propos multi stage multi recurs input fulli convolut network address problem multipl recurs input one stage multipl side output differ recept field size learn lower stage provid multi scale contextu boundari inform consecut learn design biolog plausibl like human visual system compar differ possibl segment solut address ambigu boundari issu multi stage network train end end achiev promis result public avail mous piriform cortex dataset signific outperform competitor|['Wei Shen', 'Bin Wang', 'Yuan Jiang', 'Yan Wang', 'Alan Yuille']|['cs.CV']
2017-03-28T14:09:00Z|2017-03-24T16:27:57Z|http://arxiv.org/abs/1703.08492v1|http://arxiv.org/pdf/1703.08492v1|Content-Based Image Retrieval Based on Late Fusion of Binary and Local   Descriptors|content base imag retriev base late fusion binari local descriptor|One of the challenges in Content-Based Image Retrieval (CBIR) is to reduce the semantic gaps between low-level features and high-level semantic concepts. In CBIR, the images are represented in the feature space and the performance of CBIR depends on the type of selected feature representation. Late fusion also known as visual words integration is applied to enhance the performance of image retrieval. The recent advances in image retrieval diverted the focus of research towards the use of binary descriptors as they are reported computationally efficient. In this paper, we aim to investigate the late fusion of Fast Retina Keypoint (FREAK) and Scale Invariant Feature Transform (SIFT). The late fusion of binary and local descriptor is selected because among binary descriptors, FREAK has shown good results in classification-based problems while SIFT is robust to translation, scaling, rotation and small distortions. The late fusion of FREAK and SIFT integrates the performance of both feature descriptors for an effective image retrieval. Experimental results and comparisons show that the proposed late fusion enhances the performances of image retrieval.|one challeng content base imag retriev cbir reduc semant gap low level featur high level semant concept cbir imag repres featur space perform cbir depend type select featur represent late fusion also known visual word integr appli enhanc perform imag retriev recent advanc imag retriev divert focus research toward use binari descriptor report comput effici paper aim investig late fusion fast retina keypoint freak scale invari featur transform sift late fusion binari local descriptor select becaus among binari descriptor freak shown good result classif base problem sift robust translat scale rotat small distort late fusion freak sift integr perform featur descriptor effect imag retriev experiment result comparison show propos late fusion enhanc perform imag retriev|['Nouman Ali', 'Danish Ali Mazhar', 'Zeshan Iqbal', 'Rehan Ashraf', 'Jawad Ahmed', 'Farrukh Zeeshan Khan']|['cs.CV']
2017-03-28T14:09:00Z|2017-03-24T15:41:01Z|http://arxiv.org/abs/1703.08472v1|http://arxiv.org/pdf/1703.08472v1|Medical Image Retrieval using Deep Convolutional Neural Network|medic imag retriev use deep convolut neural network|With a widespread use of digital imaging data in hospitals, the size of medical image repositories is increasing rapidly. This causes difficulty in managing and querying these large databases leading to the need of content based medical image retrieval (CBMIR) systems. A major challenge in CBMIR systems is the semantic gap that exists between the low level visual information captured by imaging devices and high level semantic information perceived by human. The efficacy of such systems is more crucial in terms of feature representations that can characterize the high-level information completely. In this paper, we propose a framework of deep learning for CBMIR system by using deep Convolutional Neural Network (CNN) that is trained for classification of medical images. An intermodal dataset that contains twenty four classes and five modalities is used to train the network. The learned features and the classification results are used to retrieve medical images. For retrieval, best results are achieved when class based predictions are used. An average classification accuracy of 99.77% and a mean average precision of 0.69 is achieved for retrieval task. The proposed method is best suited to retrieve multimodal medical images for different body organs.|widespread use digit imag data hospit size medic imag repositori increas rapid caus difficulti manag queri larg databas lead need content base medic imag retriev cbmir system major challeng cbmir system semant gap exist low level visual inform captur imag devic high level semant inform perceiv human efficaci system crucial term featur represent character high level inform complet paper propos framework deep learn cbmir system use deep convolut neural network cnn train classif medic imag intermod dataset contain twenti four class five modal use train network learn featur classif result use retriev medic imag retriev best result achiev class base predict use averag classif accuraci mean averag precis achiev retriev task propos method best suit retriev multimod medic imag differ bodi organ|['Adnan Qayyum', 'Syed Muhammad Anwar', 'Muhammad Awais', 'Muhammad Majid']|['cs.CV']
2017-03-28T14:09:00Z|2017-03-27T07:23:05Z|http://arxiv.org/abs/1703.08448v2|http://arxiv.org/pdf/1703.08448v2|Object Region Mining with Adversarial Erasing: A Simple Classification   to Semantic Segmentation Approach|object region mine adversari eras simpl classif semant segment approach|We investigate a principle way to progressively mine discriminative object regions using classification networks to address the weakly-supervised semantic segmentation problems. Classification networks are only responsive to small and sparse discriminative regions from the object of interest, which deviates from the requirement of the segmentation task that needs to localize dense, interior and integral regions for pixel-wise inference. To mitigate this gap, we propose a new adversarial erasing approach for localizing and expanding object regions progressively. Starting with a single small object region, our proposed approach drives the classification network to sequentially discover new and complement object regions by erasing the current mined regions in an adversarial manner. These localized regions eventually constitute a dense and complete object region for learning semantic segmentation. To further enhance the quality of the discovered regions by adversarial erasing, an online prohibitive segmentation learning approach is developed to collaborate with adversarial erasing by providing auxiliary segmentation supervision modulated by the more reliable classification scores. Despite its apparent simplicity, the proposed approach achieves 55.0% and 55.7% mean Intersection-over-Union (mIoU) scores on PASCAL VOC 2012 val and test sets, which are the new state-of-the-arts.|investig principl way progress mine discrimin object region use classif network address weak supervis semant segment problem classif network onli respons small spars discrimin region object interest deviat requir segment task need local dens interior integr region pixel wise infer mitig gap propos new adversari eras approach local expand object region progress start singl small object region propos approach drive classif network sequenti discov new complement object region eras current mine region adversari manner local region eventu constitut dens complet object region learn semant segment enhanc qualiti discov region adversari eras onlin prohibit segment learn approach develop collabor adversari eras provid auxiliari segment supervis modul reliabl classif score despit appar simplic propos approach achiev mean intersect union miou score pascal voc val test set new state art|['Yunchao Wei', 'Jiashi Feng', 'Xiaodan Liang', 'Ming-Ming Cheng', 'Yao Zhao', 'Shuicheng Yan']|['cs.CV']
2017-03-28T14:09:00Z|2017-03-24T12:41:38Z|http://arxiv.org/abs/1703.08388v1|http://arxiv.org/pdf/1703.08388v1|DeepVisage: Making face recognition simple yet with powerful   generalization skills|deepvisag make face recognit simpl yet power general skill|Face recognition (FR) methods report significant performance by adopting the convolutional neural network (CNN) based learning methods. Although CNNs are mostly trained by optimizing the softmax loss, the recent trend shows an improvement of accuracy with different strategies, such as task-specific CNN learning with different loss functions, fine-tuning on target dataset, metric learning and concatenating features from multiple CNNs. Incorporating these tasks obviously requires additional efforts. Moreover, it demotivates the discovery of efficient CNN models for FR which are trained only with identity labels. We focus on this fact and propose an easily trainable and single CNN based FR method. Our CNN model exploits the residual learning framework. Additionally, it uses normalized features to compute the loss. Our extensive experiments show excellent generalization on different datasets. We obtain very competitive and state-of-the-art results on the LFW, IJB-A, YouTube faces and CACD datasets.|face recognit fr method report signific perform adopt convolut neural network cnn base learn method although cnns train optim softmax loss recent trend show improv accuraci differ strategi task specif cnn learn differ loss function fine tune target dataset metric learn concaten featur multipl cnns incorpor task obvious requir addit effort moreov demotiv discoveri effici cnn model fr train onli ident label focus fact propos easili trainabl singl cnn base fr method cnn model exploit residu learn framework addit use normal featur comput loss extens experi show excel general differ dataset obtain veri competit state art result lfw ijb youtub face cacd dataset|['Abul Hasnat', 'Julien Bohn√©', 'St√©phane Gentric', 'Liming Chen']|['cs.CV']
2017-03-28T14:09:00Z|2017-03-24T11:58:14Z|http://arxiv.org/abs/1703.08378v1|http://arxiv.org/pdf/1703.08378v1|Feature Fusion using Extended Jaccard Graph and Stochastic Gradient   Descent for Robot|featur fusion use extend jaccard graph stochast gradient descent robot|Robot vision is a fundamental device for human-robot interaction and robot complex tasks. In this paper, we use Kinect and propose a feature graph fusion (FGF) for robot recognition. Our feature fusion utilizes RGB and depth information to construct fused feature from Kinect. FGF involves multi-Jaccard similarity to compute a robust graph and utilize word embedding method to enhance the recognition results. We also collect DUT RGB-D face dataset and a benchmark datset to evaluate the effectiveness and efficiency of our method. The experimental results illustrate FGF is robust and effective to face and object datasets in robot applications.|robot vision fundament devic human robot interact robot complex task paper use kinect propos featur graph fusion fgf robot recognit featur fusion util rgb depth inform construct fuse featur kinect fgf involv multi jaccard similar comput robust graph util word embed method enhanc recognit result also collect dut rgb face dataset benchmark datset evalu effect effici method experiment result illustr fgf robust effect face object dataset robot applic|['Shenglan Liu', 'Muxin Sun', 'Wei Wang', 'Feilong Wang']|['cs.CV', 'cs.LG', 'cs.RO']
2017-03-28T14:09:00Z|2017-03-24T11:39:26Z|http://arxiv.org/abs/1703.08366v1|http://arxiv.org/pdf/1703.08366v1|A Hybrid Deep Learning Approach for Texture Analysis|hybrid deep learn approach textur analysi|Texture classification is a problem that has various applications such as remote sensing and forest species recognition. Solutions tend to be custom fit to the dataset used but fails to generalize. The Convolutional Neural Network (CNN) in combination with Support Vector Machine (SVM) form a robust selection between powerful invariant feature extractor and accurate classifier. The fusion of experts provides stability in classification rates among different datasets.|textur classif problem various applic remot sens forest speci recognit solut tend custom fit dataset use fail general convolut neural network cnn combin support vector machin svm form robust select power invari featur extractor accur classifi fusion expert provid stabil classif rate among differ dataset|['Hussein Adly', 'Mohamed Moustafa']|['cs.CV']
2017-03-28T14:09:00Z|2017-03-24T11:17:00Z|http://arxiv.org/abs/1703.08359v1|http://arxiv.org/pdf/1703.08359v1|Scalable Person Re-identification on Supervised Smoothed Manifold|scalabl person identif supervis smooth manifold|Most existing person re-identification algorithms either extract robust visual features or learn discriminative metrics for person images. However, the underlying manifold which those images reside on is rarely investigated. That raises a problem that the learned metric is not smooth with respect to the local geometry structure of the data manifold.   In this paper, we study person re-identification with manifold-based affinity learning, which did not receive enough attention from this area. An unconventional manifold-preserving algorithm is proposed, which can 1) make the best use of supervision from training data, whose label information is given as pairwise constraints; 2) scale up to large repositories with low on-line time complexity; and 3) be plunged into most existing algorithms, serving as a generic postprocessing procedure to further boost the identification accuracies. Extensive experimental results on five popular person re-identification benchmarks consistently demonstrate the effectiveness of our method. Especially, on the largest CUHK03 and Market-1501, our method outperforms the state-of-the-art alternatives by a large margin with high efficiency, which is more appropriate for practical applications.|exist person identif algorithm either extract robust visual featur learn discrimin metric person imag howev manifold imag resid rare investig rais problem learn metric smooth respect local geometri structur data manifold paper studi person identif manifold base affin learn receiv enough attent area unconvent manifold preserv algorithm propos make best use supervis train data whose label inform given pairwis constraint scale larg repositori low line time complex plung exist algorithm serv generic postprocess procedur boost identif accuraci extens experiment result five popular person identif benchmark consist demonstr effect method especi largest cuhk market method outperform state art altern larg margin high effici appropri practic applic|['Song Bai', 'Xiang Bai', 'Qi Tian']|['cs.CV']
2017-03-28T14:09:00Z|2017-03-24T10:11:03Z|http://arxiv.org/abs/1703.08338v1|http://arxiv.org/pdf/1703.08338v1|Improving Classification by Improving Labelling: Introducing   Probabilistic Multi-Label Object Interaction Recognition|improv classif improv label introduc probabilist multi label object interact recognit|This work deviates from easy-to-define class boundaries for object interactions. For the task of object interaction recognition, often captured using an egocentric view, we show that semantic ambiguities in verbs and recognising sub-interactions along with concurrent interactions result in legitimate class overlaps (Figure 1). We thus aim to model the mapping between observations and interaction classes, as well as class overlaps, towards a probabilistic multi-label classifier that emulates human annotators. Given a video segment containing an object interaction, we model the probability for a verb, out of a list of possible verbs, to be used to annotate that interaction. The proba- bility is learnt from crowdsourced annotations, and is tested on two public datasets, comprising 1405 video sequences for which we provide annotations on 90 verbs. We outper- form conventional single-label classification by 11% and 6% on the two datasets respectively, and show that learning from annotation probabilities outperforms majority voting and enables discovery of co-occurring labels.|work deviat easi defin class boundari object interact task object interact recognit often captur use egocentr view show semant ambigu verb recognis sub interact along concurr interact result legitim class overlap figur thus aim model map observ interact class well class overlap toward probabilist multi label classifi emul human annot given video segment contain object interact model probabl verb list possibl verb use annot interact proba biliti learnt crowdsourc annot test two public dataset compris video sequenc provid annot verb outper form convent singl label classif two dataset respect show learn annot probabl outperform major vote enabl discoveri co occur label|['Michael Wray', 'Davide Moltisanti', 'Walterio Mayol-Cuevas', 'Dima Damen']|['cs.CV']
2017-03-28T14:09:05Z|2017-03-24T05:54:11Z|http://arxiv.org/abs/1703.08289v1|http://arxiv.org/pdf/1703.08289v1|Deep Direct Regression for Multi-Oriented Scene Text Detection|deep direct regress multi orient scene text detect|In this paper, we first provide a new perspective to divide existing high performance object detection methods into direct and indirect regressions. Direct regression performs boundary regression by predicting the offsets from a given point, while indirect regression predicts the offsets from some bounding box proposals. Then we analyze the drawbacks of the indirect regression, which the recent state-of-the-art detection structures like Faster-RCNN and SSD follows, for multi-oriented scene text detection, and point out the potential superiority of direct regression. To verify this point of view, we propose a deep direct regression based method for multi-oriented scene text detection. Our detection framework is simple and effective with a fully convolutional network and one-step post processing. The fully convolutional network is optimized in an end-to-end way and has bi-task outputs where one is pixel-wise classification between text and non-text, and the other is direct regression to determine the vertex coordinates of quadrilateral text boundaries. The proposed method is particularly beneficial for localizing incidental scene texts. On the ICDAR2015 Incidental Scene Text benchmark, our method achieves the F1-measure of 81%, which is a new state-of-the-art and significantly outperforms previous approaches. On other standard datasets with focused scene texts, our method also reaches the state-of-the-art performance.|paper first provid new perspect divid exist high perform object detect method direct indirect regress direct regress perform boundari regress predict offset given point indirect regress predict offset bound box propos analyz drawback indirect regress recent state art detect structur like faster rcnn ssd follow multi orient scene text detect point potenti superior direct regress verifi point view propos deep direct regress base method multi orient scene text detect detect framework simpl effect fulli convolut network one step post process fulli convolut network optim end end way bi task output one pixel wise classif text non text direct regress determin vertex coordin quadrilater text boundari propos method particular benefici local incident scene text icdar incident scene text benchmark method achiev measur new state art signific outperform previous approach standard dataset focus scene text method also reach state art perform|['Wenhao He', 'Xu-Yao Zhang', 'Fei Yin', 'Cheng-Lin Liu']|['cs.CV']
2017-03-28T14:09:05Z|2017-03-24T03:01:29Z|http://arxiv.org/abs/1703.08274v1|http://arxiv.org/pdf/1703.08274v1|View Adaptive Recurrent Neural Networks for High Performance Human   Action Recognition from Skeleton Data|view adapt recurr neural network high perform human action recognit skeleton data|Skeleton-based human action recognition has recently attracted increasing attention due to the popularity of 3D skeleton data. One main challenge lies in the large view variations in captured human actions. We propose a novel view adaptation scheme to automatically regulate observation viewpoints during the occurrence of an action. Rather than re-positioning the skeletons based on a human defined prior criterion, we design a view adaptive recurrent neural network (RNN) with LSTM architecture, which enables the network itself to adapt to the most suitable observation viewpoints from end to end. Extensive experiment analyses show that the proposed view adaptive RNN model strives to (1) transform the skeletons of various views to much more consistent viewpoints and (2) maintain the continuity of the action rather than transforming every frame to the same position with the same body orientation. Our model achieves state-of-the-art performance on three benchmark datasets. On the current largest NTU RGB+D dataset, our scheme outperforms the state of the art by an impressive 6% gain in accuracy.|skeleton base human action recognit recent attract increas attent due popular skeleton data one main challeng lie larg view variat captur human action propos novel view adapt scheme automat regul observ viewpoint dure occurr action rather posit skeleton base human defin prior criterion design view adapt recurr neural network rnn lstm architectur enabl network adapt suitabl observ viewpoint end end extens experi analys show propos view adapt rnn model strive transform skeleton various view much consist viewpoint maintain continu action rather transform everi frame posit bodi orient model achiev state art perform three benchmark dataset current largest ntu rgb dataset scheme outperform state art impress gain accuraci|['Pengfei Zhang', 'Cuiling Lan', 'Junliang Xing', 'Wenjun Zeng', 'Jianru Xue', 'Nanning Zheng']|['cs.CV']
2017-03-28T14:09:05Z|2017-03-23T22:25:05Z|http://arxiv.org/abs/1703.08245v1|http://arxiv.org/pdf/1703.08245v1|On the Robustness of Convolutional Neural Networks to Internal   Architecture and Weight Perturbations|robust convolut neural network intern architectur weight perturb|Deep convolutional neural networks are generally regarded as robust function approximators. So far, this intuition is based on perturbations to external stimuli such as the images to be classified. Here we explore the robustness of convolutional neural networks to perturbations to the internal weights and architecture of the network itself. We show that convolutional networks are surprisingly robust to a number of internal perturbations in the higher convolutional layers but the bottom convolutional layers are much more fragile. For instance, Alexnet shows less than a 30% decrease in classification performance when randomly removing over 70% of weight connections in the top convolutional or dense layers but performance is almost at chance with the same perturbation in the first convolutional layer. Finally, we suggest further investigations which could continue to inform the robustness of convolutional networks to internal perturbations.|deep convolut neural network general regard robust function approxim far intuit base perturb extern stimuli imag classifi explor robust convolut neural network perturb intern weight architectur network show convolut network surpris robust number intern perturb higher convolut layer bottom convolut layer much fragil instanc alexnet show less decreas classif perform random remov weight connect top convolut dens layer perform almost chanc perturb first convolut layer final suggest investig could continu inform robust convolut network intern perturb|['Nicholas Cheney', 'Martin Schrimpf', 'Gabriel Kreiman']|['cs.LG', 'cs.CV']
2017-03-28T14:09:05Z|2017-03-23T21:25:48Z|http://arxiv.org/abs/1703.08238v1|http://arxiv.org/pdf/1703.08238v1|Semi-Automatic Segmentation and Ultrasonic Characterization of Solid   Breast Lesions|semi automat segment ultrason character solid breast lesion|Characterization of breast lesions is an essential prerequisite to detect breast cancer in an early stage. Automatic segmentation makes this categorization method robust by freeing it from subjectivity and human error. Both spectral and morphometric features are successfully used for differentiating between benign and malignant breast lesions. In this thesis, we used empirical mode decomposition method for semi-automatic segmentation. Sonographic features like ehcogenicity, heterogeneity, FNPA, margin definition, Hurst coefficient, compactness, roundness, aspect ratio, convexity, solidity, form factor were calculated to be used as our characterization parameters. All of these parameters did not give desired comparative results. But some of them namely echogenicity, heterogeneity, margin definition, aspect ratio and convexity gave good results and were used for characterization.|character breast lesion essenti prerequisit detect breast cancer earli stage automat segment make categor method robust free subject human error spectral morphometr featur success use differenti benign malign breast lesion thesi use empir mode decomposit method semi automat segment sonograph featur like ehcogen heterogen fnpa margin definit hurst coeffici compact round aspect ratio convex solid form factor calcul use character paramet paramet give desir compar result name echogen heterogen margin definit aspect ratio convex gave good result use character|['Mohammad Saad Billah', 'Tahmida Binte Mahmud']|['cs.CV']
2017-03-28T14:09:05Z|2017-03-23T16:46:00Z|http://arxiv.org/abs/1703.08136v1|http://arxiv.org/pdf/1703.08136v1|Visually grounded learning of keyword prediction from untranscribed   speech|visual ground learn keyword predict untranscrib speech|"During language acquisition, infants have the benefit of visual cues to ground spoken language. Robots similarly have access to audio and visual sensors. Recent work has shown that images and spoken captions can be mapped into a meaningful common space, allowing images to be retrieved using speech and vice versa. In this setting of images paired with untranscribed spoken captions, we consider whether computer vision systems can be used to obtain textual labels for the speech. Concretely, we use an image-to-words multi-label visual classifier to tag images with soft textual labels, and then train a neural network to map from the speech to these soft targets. We show that the resulting speech system is able to predict which words occur in an utterance---acting as a spoken bag-of-words classifier---without seeing any parallel speech and text. We find that the model often confuses semantically related words, e.g. ""man"" and ""person"", making it even more effective as a semantic keyword spotter."|dure languag acquisit infant benefit visual cue ground spoken languag robot similar access audio visual sensor recent work shown imag spoken caption map meaning common space allow imag retriev use speech vice versa set imag pair untranscrib spoken caption consid whether comput vision system use obtain textual label speech concret use imag word multi label visual classifi tag imag soft textual label train neural network map speech soft target show result speech system abl predict word occur utter act spoken bag word classifi without see ani parallel speech text find model often confus semant relat word man person make even effect semant keyword spotter|['Herman Kamper', 'Shane Settle', 'Gregory Shakhnarovich', 'Karen Livescu']|['cs.CL', 'cs.CV']
2017-03-28T14:09:05Z|2017-03-24T12:12:37Z|http://arxiv.org/abs/1703.08132v2|http://arxiv.org/pdf/1703.08132v2|Weakly Supervised Action Learning with RNN based Fine-to-coarse Modeling|weak supervis action learn rnn base fine coars model|We present an approach for weakly supervised learning of human actions. Given a set of videos and an ordered list of the occurring actions, the goal is to infer start and end frames of the related action classes within the video and to train the respective action classifiers without any need for hand labeled frame boundaries. To address this task, we propose a combination of a discriminative representation of subactions, modeled by a recurrent neural network, and a coarse probabilistic model to allow for a temporal alignment and inference over long sequences. While this system alone already generates good results, we show that the performance can be further improved by approximating the number of subactions to the characteristics of the different action classes. To this end, we adapt the number of subaction classes by iterating realignment and reestimation during training. The proposed system is evaluated on two benchmark datasets, the Breakfast and the Hollywood extended dataset, showing a competitive performance on various weak learning tasks such as temporal action segmentation and action alignment.|present approach weak supervis learn human action given set video order list occur action goal infer start end frame relat action class within video train respect action classifi without ani need hand label frame boundari address task propos combin discrimin represent subact model recurr neural network coars probabilist model allow tempor align infer long sequenc system alon alreadi generat good result show perform improv approxim number subact characterist differ action class end adapt number subact class iter realign reestim dure train propos system evalu two benchmark dataset breakfast hollywood extend dataset show competit perform various weak learn task tempor action segment action align|['Alexander Richard', 'Hilde Kuehne', 'Juergen Gall']|['cs.CV']
2017-03-28T14:09:05Z|2017-03-23T15:57:23Z|http://arxiv.org/abs/1703.08120v1|http://arxiv.org/pdf/1703.08120v1|Recurrent and Contextual Models for Visual Question Answering|recurr contextu model visual question answer|We propose a series of recurrent and contextual neural network models for multiple choice visual question answering on the Visual7W dataset. Motivated by divergent trends in model complexities in the literature, we explore the balance between model expressiveness and simplicity by studying incrementally more complex architectures. We start with LSTM-encoding of input questions and answers; build on this with context generation by LSTM-encodings of neural image and question representations and attention over images; and evaluate the diversity and predictive power of our models and the ensemble thereof. All models are evaluated against a simple baseline inspired by the current state-of-the-art, consisting of involving simple concatenation of bag-of-words and CNN representations for the text and images, respectively. Generally, we observe marked variation in image-reasoning performance between our models not obvious from their overall performance, as well as evidence of dataset bias. Our standalone models achieve accuracies up to $64.6\%$, while the ensemble of all models achieves the best accuracy of $66.67\%$, within $0.5\%$ of the current state-of-the-art for Visual7W.|propos seri recurr contextu neural network model multipl choic visual question answer visualw dataset motiv diverg trend model complex literatur explor balanc model express simplic studi increment complex architectur start lstm encod input question answer build context generat lstm encod neural imag question represent attent imag evalu divers predict power model ensembl thereof model evalu simpl baselin inspir current state art consist involv simpl concaten bag word cnn represent text imag respect general observ mark variat imag reason perform model obvious overal perform well evid dataset bias standalon model achiev accuraci ensembl model achiev best accuraci within current state art visualw|['Abhijit Sharang', 'Eric Lau']|['cs.CL', 'cs.CV']
2017-03-28T14:09:05Z|2017-03-23T15:56:33Z|http://arxiv.org/abs/1703.08119v1|http://arxiv.org/pdf/1703.08119v1|Quality Resilient Deep Neural Networks|qualiti resili deep neural network|"We study deep neural networks for classification of images with quality distortions. We first show that networks fine-tuned on distorted data greatly outperform the original networks when tested on distorted data. However, fine-tuned networks perform poorly on quality distortions that they have not been trained for. We propose a mixture of experts ensemble method that is robust to different types of distortions. The ""experts"" in our model are trained on a particular type of distortion. The output of the model is a weighted sum of the expert models, where the weights are determined by a separate gating network. The gating network is trained to predict optimal weights for a particular distortion type and level. During testing, the network is blind to the distortion level and type, yet can still assign appropriate weights to the expert models. We additionally investigate weight sharing methods for the mixture model and show that improved performance can be achieved with a large reduction in the number of unique network parameters."|studi deep neural network classif imag qualiti distort first show network fine tune distort data great outperform origin network test distort data howev fine tune network perform poor qualiti distort train propos mixtur expert ensembl method robust differ type distort expert model train particular type distort output model weight sum expert model weight determin separ gate network gate network train predict optim weight particular distort type level dure test network blind distort level type yet still assign appropri weight expert model addit investig weight share method mixtur model show improv perform achiev larg reduct number uniqu network paramet|['Samuel Dodge', 'Lina Karam']|['cs.CV']
2017-03-28T14:09:05Z|2017-03-23T14:46:46Z|http://arxiv.org/abs/1703.08089v1|http://arxiv.org/pdf/1703.08089v1|A Bag-of-Words Equivalent Recurrent Neural Network for Action   Recognition|bag word equival recurr neural network action recognit|The traditional bag-of-words approach has found a wide range of applications in computer vision. The standard pipeline consists of a generation of a visual vocabulary, a quantization of the features into histograms of visual words, and a classification step for which usually a support vector machine in combination with a non-linear kernel is used. Given large amounts of data, however, the model suffers from a lack of discriminative power. This applies particularly for action recognition, where the vast amount of video features needs to be subsampled for unsupervised visual vocabulary generation. Moreover, the kernel computation can be very expensive on large datasets. In this work, we propose a recurrent neural network that is equivalent to the traditional bag-of-words approach but enables for the application of discriminative training. The model further allows to incorporate the kernel computation into the neural network directly, solving the complexity issue and allowing to represent the complete classification system within a single network. We evaluate our method on four recent action recognition benchmarks and show that the conventional model as well as sparse coding methods are outperformed.|tradit bag word approach found wide rang applic comput vision standard pipelin consist generat visual vocabulari quantize featur histogram visual word classif step usual support vector machin combin non linear kernel use given larg amount data howev model suffer lack discrimin power appli particular action recognit vast amount video featur need subsampl unsupervis visual vocabulari generat moreov kernel comput veri expens larg dataset work propos recurr neural network equival tradit bag word approach enabl applic discrimin train model allow incorpor kernel comput neural network direct solv complex issu allow repres complet classif system within singl network evalu method four recent action recognit benchmark show convent model well spars code method outperform|['Alexander Richard', 'Juergen Gall']|['cs.CV']
2017-03-28T14:09:05Z|2017-03-23T12:55:34Z|http://arxiv.org/abs/1703.08050v1|http://arxiv.org/pdf/1703.08050v1|Is Second-order Information Helpful for Large-scale Visual Recognition?|second order inform help larg scale visual recognit|By stacking deeper layers of convolutions and nonlinearity, convolutional networks (ConvNets) effectively learn from low-level to high-level features and discriminative representations. Since the end goal of large-scale recognition is to delineate the complex boundaries of thousands of classes in a large-dimensional space, adequate exploration of feature distributions is important for realizing full potentials of ConvNets. However, state-of-the-art works concentrate only on deeper or wider architecture design, while rarely exploring feature statistics higher than first-order. We take a step towards addressing this problem. Our method consists in covariance pooling, instead of the most commonly used first-order pooling, of high-level convolutional features. The main challenges involved are robust covariance estimation given a small sample of large-dimensional features and usage of the manifold structure of covariance matrices. To address these challenges, we present a Matrix Power Normalized Covariance (MPN-COV) method. We develop the forward and backward propagation formulas regarding the nonlinear matrix functions such that MPN-COV can be trained end-to-end. In addition, we analyze both qualitatively and quantitatively its advantage over the widely used Log-Euclidean metric. On the ImageNet 2012 validation set, by combining MPN-COV we achieve over 4%, 3% and 2.5% gains for AlexNet, VGG-M and VGG-16, respectively; integration of MPN-COV into 50-layer ResNet outperforms ResNet-101 and is comparable to ResNet-152, both of which use first-order, global average pooling.|stack deeper layer convolut nonlinear convolut network convnet effect learn low level high level featur discrimin represent sinc end goal larg scale recognit delin complex boundari thousand class larg dimension space adequ explor featur distribut import realiz full potenti convnet howev state art work concentr onli deeper wider architectur design rare explor featur statist higher first order take step toward address problem method consist covari pool instead common use first order pool high level convolut featur main challeng involv robust covari estim given small sampl larg dimension featur usag manifold structur covari matric address challeng present matrix power normal covari mpn cov method develop forward backward propag formula regard nonlinear matrix function mpn cov train end end addit analyz qualit quantit advantag wide use log euclidean metric imagenet valid set combin mpn cov achiev gain alexnet vgg vgg respect integr mpn cov layer resnet outperform resnet compar resnet use first order global averag pool|['Peihua Li', 'Jiangtao Xie', 'Qilong Wang', 'Wangmeng Zuo']|['cs.CV']
2017-03-28T14:09:09Z|2017-03-23T12:19:09Z|http://arxiv.org/abs/1703.08033v1|http://arxiv.org/pdf/1703.08033v1|Generative Adversarial Residual Pairwise Networks for One Shot Learning|generat adversari residu pairwis network one shot learn|Deep neural networks achieve unprecedented performance levels over many tasks and scale well with large quantities of data, but performance in the low-data regime and tasks like one shot learning still lags behind. While recent work suggests many hypotheses from better optimization to more complicated network structures, in this work we hypothesize that having a learnable and more expressive similarity objective is an essential missing component. Towards overcoming that, we propose a network design inspired by deep residual networks that allows the efficient computation of this more expressive pairwise similarity objective. Further, we argue that regularization is key in learning with small amounts of data, and propose an additional generator network based on the Generative Adversarial Networks where the discriminator is our residual pairwise network. This provides a strong regularizer by leveraging the generated data samples. The proposed model can generate plausible variations of exemplars over unseen classes and outperforms strong discriminative baselines for few shot classification tasks. Notably, our residual pairwise network design outperforms previous state-of-theart on the challenging mini-Imagenet dataset for one shot learning by getting over 55% accuracy for the 5-way classification task over unseen classes.|deep neural network achiev unpreced perform level mani task scale well larg quantiti data perform low data regim task like one shot learn still lag behind recent work suggest mani hypothes better optim complic network structur work hypothes learnabl express similar object essenti miss compon toward overcom propos network design inspir deep residu network allow effici comput express pairwis similar object argu regular key learn small amount data propos addit generat network base generat adversari network discrimin residu pairwis network provid strong regular leverag generat data sampl propos model generat plausibl variat exemplar unseen class outperform strong discrimin baselin shot classif task notabl residu pairwis network design outperform previous state theart challeng mini imagenet dataset one shot learn get accuraci way classif task unseen class|['Akshay Mehrotra', 'Ambedkar Dukkipati']|['cs.CV', 'cs.NE']
2017-03-28T14:09:09Z|2017-03-26T02:43:13Z|http://arxiv.org/abs/1703.08025v2|http://arxiv.org/pdf/1703.08025v2|Saliency-guided video classification via adaptively weighted learning|salienc guid video classif via adapt weight learn|Video classification is productive in many practical applications, and the recent deep learning has greatly improved its accuracy. However, existing works often model video frames indiscriminately, but from the view of motion, video frames can be decomposed into salient and non-salient areas naturally. Salient and non-salient areas should be modeled with different networks, for the former present both appearance and motion information, and the latter present static background information. To address this problem, in this paper, video saliency is predicted by optical flow without supervision firstly. Then two streams of 3D CNN are trained individually for raw frames and optical flow on salient areas, and another 2D CNN is trained for raw frames on non-salient areas. For the reason that these three streams play different roles for each class, the weights of each stream are adaptively learned for each class. Experimental results show that saliency-guided modeling and adaptively weighted learning can reinforce each other, and we achieve the state-of-the-art results.|video classif product mani practic applic recent deep learn great improv accuraci howev exist work often model video frame indiscrimin view motion video frame decompos salient non salient area natur salient non salient area model differ network former present appear motion inform latter present static background inform address problem paper video salienc predict optic flow without supervis first two stream cnn train individu raw frame optic flow salient area anoth cnn train raw frame non salient area reason three stream play differ role class weight stream adapt learn class experiment result show salienc guid model adapt weight learn reinforc achiev state art result|['Yunzhen Zhao', 'Yuxin Peng']|['cs.CV']
2017-03-28T14:09:09Z|2017-03-24T08:24:07Z|http://arxiv.org/abs/1703.08014v2|http://arxiv.org/pdf/1703.08014v2|Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse   IMUs|spars inerti poser automat human pose estim spars imus|We address the problem of making human motion capture in the wild more practical by using a small set of inertial sensors attached to the body. Since the problem is heavily under-constrained, previous methods either use a large number of sensors, which is intrusive, or they require additional video input. We take a different approach and constrain the problem by: (i) making use of a realistic statistical body model that includes anthropometric constraints and (ii) using a joint optimization framework to fit the model to orientation and acceleration measurements over multiple frames. The resulting tracker Sparse Inertial Poser (SIP) enables 3D human pose estimation using only 6 sensors (attached to the wrists, lower legs, back and head) and works for arbitrary human motions. Experiments on the recently released TNT15 dataset show that, using the same number of sensors, SIP achieves higher accuracy than the dataset baseline without using any video data. We further demonstrate the effectiveness of SIP on newly recorded challenging motions in outdoor scenarios such as climbing or jumping over a wall.|address problem make human motion captur wild practic use small set inerti sensor attach bodi sinc problem heavili constrain previous method either use larg number sensor intrus requir addit video input take differ approach constrain problem make use realist statist bodi model includ anthropometr constraint ii use joint optim framework fit model orient acceler measur multipl frame result tracker spars inerti poser sip enabl human pose estim use onli sensor attach wrist lower leg back head work arbitrari human motion experi recent releas tnt dataset show use number sensor sip achiev higher accuraci dataset baselin without use ani video data demonstr effect sip newli record challeng motion outdoor scenario climb jump wall|['Timo von Marcard', 'Bodo Rosenhahn', 'Michael J. Black', 'Gerard Pons-Moll']|['cs.CV', 'cs.GR']
2017-03-28T14:09:09Z|2017-03-24T09:30:41Z|http://arxiv.org/abs/1703.08013v2|http://arxiv.org/pdf/1703.08013v2|A learning-based approach to text image retrieval: using CNN features   and improved similarity metrics|learn base approach text imag retriev use cnn featur improv similar metric|Text content can have different visual presentation ways with roughly similar characters. While conventional text image retrieval depends on complex model of OCR-based text recognition and text similarity detection, this paper proposes a new learning-based approach to text image retrieval with the purpose of finding out the original or similar text through a query text image. Firstly, features of text images are extracted by the CNN network to obtain the deep visual representations. Then, the dimension of CNN features is reduced by PCA method to improve the efficiency of similarity detection. Based on that, an improved similarity metrics with article theme relevance filtering is proposed to improve the retrieval accuracy. In experimental procedure, we collect a group of academic papers both including English and Chinese as the text database, and cut them into pieces of text image. A text image with changed text content is used as the query image, experimental results show that the proposed approach has good ability to retrieve the original text content.|text content differ visual present way rough similar charact convent text imag retriev depend complex model ocr base text recognit text similar detect paper propos new learn base approach text imag retriev purpos find origin similar text queri text imag first featur text imag extract cnn network obtain deep visual represent dimens cnn featur reduc pca method improv effici similar detect base improv similar metric articl theme relev filter propos improv retriev accuraci experiment procedur collect group academ paper includ english chines text databas cut piec text imag text imag chang text content use queri imag experiment result show propos approach good abil retriev origin text content|['Mao Tan', 'Si-Ping Yuan', 'Yong-Xin Su']|['cs.CV', 'cs.IR', 'cs.LG']
2017-03-28T14:09:09Z|2017-03-23T11:02:42Z|http://arxiv.org/abs/1703.08001v1|http://arxiv.org/pdf/1703.08001v1|Nonlinear Spectral Image Fusion|nonlinear spectral imag fusion|In this paper we demonstrate that the framework of nonlinear spectral decompositions based on total variation (TV) regularization is very well suited for image fusion as well as more general image manipulation tasks. The well-localized and edge-preserving spectral TV decomposition allows to select frequencies of a certain image to transfer particular features, such as wrinkles in a face, from one image to another. We illustrate the effectiveness of the proposed approach in several numerical experiments, including a comparison to the competing techniques of Poisson image editing, linear osmosis, wavelet fusion and Laplacian pyramid fusion. We conclude that the proposed spectral TV image decomposition framework is a valuable tool for semi- and fully-automatic image editing and fusion.|paper demonstr framework nonlinear spectral decomposit base total variat tv regular veri well suit imag fusion well general imag manipul task well local edg preserv spectral tv decomposit allow select frequenc certain imag transfer particular featur wrinkl face one imag anoth illustr effect propos approach sever numer experi includ comparison compet techniqu poisson imag edit linear osmosi wavelet fusion laplacian pyramid fusion conclud propos spectral tv imag decomposit framework valuabl tool semi fulli automat imag edit fusion|['Martin Benning', 'Michael M√∂ller', 'Raz Z. Nossek', 'Martin Burger', 'Daniel Cremers', 'Guy Gilboa', 'Carola-Bibiane Sch√∂nlieb']|['cs.CV', 'math.NA', '35P30, 62H35, 65M70, 94A08', 'G.1.3; G.1.6; G.1.8; I.4.0; I.4.5']
2017-03-28T14:09:09Z|2017-03-23T11:01:27Z|http://arxiv.org/abs/1703.08000v1|http://arxiv.org/pdf/1703.08000v1|Weakly Supervised Object Localization Using Things and Stuff Transfer|weak supervis object local use thing stuff transfer|We propose to help weakly supervised object localization for classes where location annotations are not available, by transferring things and stuff knowledge from a source set with available annotations. The source and target classes might share similar appearance (e.g. bear fur is similar to cat fur) or appear against similar background (e.g. horse and sheep appear against grass). To exploit this, we acquire three types of knowledge from the source set: a segmentation model trained on both thing and stuff classes; similarity relations between target and source classes; and co-occurrence relations between thing and stuff classes in the source. The segmentation model is used to generate thing and stuff segmentation maps on a target image, while the class similarity and co-occurrence knowledge help refining them. We then incorporate these maps as new cues into a multiple instance learning framework (MIL), propagating the transferred knowledge from the pixel level to the object proposal level. In extensive experiments, we conduct our transfer from the PASCAL Context dataset (source) to the ILSVRC, COCO and PASCAL VOC 2007 datasets (targets). We evaluate our transfer across widely different thing classes, including some that are not similar in appearance, but appear against similar background. The results demonstrate significant improvement over standard MIL, and we outperform the state-of-the-art in the transfer setting.|propos help weak supervis object local class locat annot avail transfer thing stuff knowledg sourc set avail annot sourc target class might share similar appear bear fur similar cat fur appear similar background hors sheep appear grass exploit acquir three type knowledg sourc set segment model train thing stuff class similar relat target sourc class co occurr relat thing stuff class sourc segment model use generat thing stuff segment map target imag class similar co occurr knowledg help refin incorpor map new cue multipl instanc learn framework mil propag transfer knowledg pixel level object propos level extens experi conduct transfer pascal context dataset sourc ilsvrc coco pascal voc dataset target evalu transfer across wide differ thing class includ similar appear appear similar background result demonstr signific improv standard mil outperform state art transfer set|['Miaojing Shi', 'Holger Caesar', 'Vittorio Ferrari']|['cs.CV']
2017-03-28T14:09:09Z|2017-03-23T09:49:37Z|http://arxiv.org/abs/1703.07980v1|http://arxiv.org/pdf/1703.07980v1|Discriminatively Boosted Image Clustering with Fully Convolutional   Auto-Encoders|discrimin boost imag cluster fulli convolut auto encod|Traditional image clustering methods take a two-step approach, feature learning and clustering, sequentially. However, recent research results demonstrated that combining the separated phases in a unified framework and training them jointly can achieve a better performance. In this paper, we first introduce fully convolutional auto-encoders for image feature learning and then propose a unified clustering framework to learn image representations and cluster centers jointly based on a fully convolutional auto-encoder and soft $k$-means scores. At initial stages of the learning procedure, the representations extracted from the auto-encoder may not be very discriminative for latter clustering. We address this issue by adopting a boosted discriminative distribution, where high score assignments are highlighted and low score ones are de-emphasized. With the gradually boosted discrimination, clustering assignment scores are discriminated and cluster purities are enlarged. Experiments on several vision benchmark datasets show that our methods can achieve a state-of-the-art performance.|tradit imag cluster method take two step approach featur learn cluster sequenti howev recent research result demonstr combin separ phase unifi framework train joint achiev better perform paper first introduc fulli convolut auto encod imag featur learn propos unifi cluster framework learn imag represent cluster center joint base fulli convolut auto encod soft mean score initi stage learn procedur represent extract auto encod may veri discrimin latter cluster address issu adopt boost discrimin distribut high score assign highlight low score one de emphas gradual boost discrimin cluster assign score discrimin cluster puriti enlarg experi sever vision benchmark dataset show method achiev state art perform|['Fengfu Li', 'Hong Qiao', 'Bo Zhang', 'Xuanyang Xi']|['cs.CV', 'cs.LG']
2017-03-28T14:09:09Z|2017-03-23T09:06:13Z|http://arxiv.org/abs/1703.07971v1|http://arxiv.org/pdf/1703.07971v1|Image-based Localization using Hourglass Networks|imag base local use hourglass network|In this paper, we propose an encoder-decoder convolutional neural network (CNN) architecture for estimating camera pose (orientation and location) from a single RGB-image. The architecture has a hourglass shape consisting of a chain of convolution and up-convolution layers followed by a regression part. The up-convolution layers are introduced to preserve the fine-grained information of the input image. Following the common practice, we train our model in end-to-end manner utilizing transfer learning from large scale classification data. The experiments demonstrate the performance of the approach on data exhibiting different lighting conditions, reflections, and motion blur. The results indicate a clear improvement over the previous state-of-the-art even when compared to methods that utilize sequence of test frames instead of a single frame.|paper propos encod decod convolut neural network cnn architectur estim camera pose orient locat singl rgb imag architectur hourglass shape consist chain convolut convolut layer follow regress part convolut layer introduc preserv fine grain inform input imag follow common practic train model end end manner util transfer learn larg scale classif data experi demonstr perform approach data exhibit differ light condit reflect motion blur result indic clear improv previous state art even compar method util sequenc test frame instead singl frame|['Iaroslav Melekhov', 'Juha Ylioinas', 'Juho Kannala', 'Esa Rahtu']|['cs.CV']
2017-03-28T14:09:09Z|2017-03-23T07:52:31Z|http://arxiv.org/abs/1703.07957v1|http://arxiv.org/pdf/1703.07957v1|Robust SfM with Little Image Overlap|robust sfm littl imag overlap|Usual Structure-from-Motion (SfM) techniques require at least trifocal overlaps to calibrate cameras and reconstruct a scene. We consider here scenarios of reduced image sets with little overlap, possibly as low as two images at most seeing the same part of the scene. We propose a new method, based on line coplanarity hypotheses, for estimating the relative scale of two independent bifocal calibrations sharing a camera, without the need of any trifocal information or Manhattan-world assumption. We use it to compute SfM in a chain of up-to-scale relative motions. For accuracy, we however also make use of trifocal information for line and/or point features, when present, relaxing usual trifocal constraints. For robustness to wrong assumptions and mismatches, we embed all constraints in a parameterless RANSAC-like approach. Experiments show that we can calibrate datasets that previously could not, and that this wider applicability does not come at the cost of inaccuracy.|usual structur motion sfm techniqu requir least trifoc overlap calibr camera reconstruct scene consid scenario reduc imag set littl overlap possibl low two imag see part scene propos new method base line coplanar hypothes estim relat scale two independ bifoc calibr share camera without need ani trifoc inform manhattan world assumpt use comput sfm chain scale relat motion accuraci howev also make use trifoc inform line point featur present relax usual trifoc constraint robust wrong assumpt mismatch emb constraint parameterless ransac like approach experi show calibr dataset previous could wider applic doe come cost inaccuraci|['Yohann Salaun', 'Renaud Marlet', 'Pascal Monasse']|['cs.CV']
2017-03-28T14:09:09Z|2017-03-23T05:22:22Z|http://arxiv.org/abs/1703.07939v1|http://arxiv.org/pdf/1703.07939v1|Recurrent Multimodal Interaction for Referring Image Segmentation|recurr multimod interact refer imag segment|In this paper we are interested in the problem of image segmentation given natural language descriptions, i.e. referring expressions. Existing works tackle this problem by first modeling images and sentences independently and then segment images by combining these two types of representations. We argue that learning word-to-image interaction is more native in the sense of jointly modeling two modalities for the image segmentation task, and we propose convolutional multimodal LSTM to encode the sequential interactions between individual words, visual information, and spatial information. We show that our proposed model outperforms the baseline model on benchmark datasets. In addition, we analyze the intermediate output of the proposed multimodal LSTM approach and empirically explains how this approach enforces a more effective word-to-image interaction.|paper interest problem imag segment given natur languag descript refer express exist work tackl problem first model imag sentenc independ segment imag combin two type represent argu learn word imag interact nativ sens joint model two modal imag segment task propos convolut multimod lstm encod sequenti interact individu word visual inform spatial inform show propos model outperform baselin model benchmark dataset addit analyz intermedi output propos multimod lstm approach empir explain approach enforc effect word imag interact|['Chenxi Liu', 'Zhe Lin', 'Xiaohui Shen', 'Jimei Yang', 'Xin Lu', 'Alan Yuille']|['cs.CV']
2017-03-28T14:09:13Z|2017-03-23T05:21:24Z|http://arxiv.org/abs/1703.07938v1|http://arxiv.org/pdf/1703.07938v1|Planar Object Tracking in the Wild: A Benchmark|planar object track wild benchmark|Planar object tracking plays an important role in computer vision and related fields. While several benchmarks have been constructed for evaluating state-of-the-art algorithms, there is a lack of video sequences captured in the wild rather than in constrained laboratory environment. In this paper, we present a carefully designed planar object tracking benchmark containing 210 videos of 30 planar objects sampled in the natural environment. In particular, for each object, we shoot seven videos involving various challenging factors, namely scale change, rotation, perspective distortion, motion blur, occlusion, out-of-view, and unconstrained. The ground truth is carefully annotated semi-manually to ensure the quality. Moreover, eleven state-of-the-art algorithms are evaluated on the benchmark using two evaluation metrics, with detailed analysis provided for the evaluation results. We expect the proposed benchmark to benefit future studies on planar object tracking.|planar object track play import role comput vision relat field sever benchmark construct evalu state art algorithm lack video sequenc captur wild rather constrain laboratori environ paper present care design planar object track benchmark contain video planar object sampl natur environ particular object shoot seven video involv various challeng factor name scale chang rotat perspect distort motion blur occlus view unconstrain ground truth care annot semi manual ensur qualiti moreov eleven state art algorithm evalu benchmark use two evalu metric detail analysi provid evalu result expect propos benchmark benefit futur studi planar object track|['Pengpeng Liang', 'Yifan Wu', 'Haibin Ling']|['cs.CV']
2017-03-28T14:09:13Z|2017-03-23T04:25:48Z|http://arxiv.org/abs/1703.07928v1|http://arxiv.org/pdf/1703.07928v1|Guided Perturbations: Self Corrective Behavior in Convolutional Neural   Networks|guid perturb self correct behavior convolut neural network|Convolutional Neural Networks have been a subject of great importance over the past decade and great strides have been made in their utility for producing state of the art performance in many computer vision problems. However, the behavior of deep networks is yet to be fully understood and is still an active area of research. In this work, we present an intriguing behavior: pre-trained CNNs can be made to improve their predictions by structurally perturbing the input. We observe that these perturbations - referred as Guided Perturbations - enable a trained network to improve its prediction performance without any learning or change in network weights. We perform various ablative experiments to understand how these perturbations affect the local context and feature representations. Furthermore, we demonstrate that this idea can improve performance of several existing approaches on semantic segmentation and scene labeling tasks on the PASCAL VOC dataset and supervised classification tasks on MNIST and CIFAR10 datasets.|convolut neural network subject great import past decad great stride made util produc state art perform mani comput vision problem howev behavior deep network yet fulli understood still activ area research work present intrigu behavior pre train cnns made improv predict structur perturb input observ perturb refer guid perturb enabl train network improv predict perform without ani learn chang network weight perform various ablat experi understand perturb affect local context featur represent furthermor demonstr idea improv perform sever exist approach semant segment scene label task pascal voc dataset supervis classif task mnist cifar dataset|['Swami Sankaranarayanan', 'Arpit Jain', 'Ser Nam Lim']|['cs.CV', 'cs.AI', 'stat.ML']
2017-03-28T14:09:13Z|2017-03-23T03:48:08Z|http://arxiv.org/abs/1703.07920v1|http://arxiv.org/pdf/1703.07920v1|Changing Fashion Cultures|chang fashion cultur|The paper presents a novel concept that analyzes and visualizes worldwide fashion trends. Our goal is to reveal cutting-edge fashion trends without displaying an ordinary fashion style. To achieve the fashion-based analysis, we created a new fashion culture database (FCDB), which consists of 76 million geo-tagged images in 16 cosmopolitan cities. By grasping a fashion trend of mixed fashion styles,the paper also proposes an unsupervised fashion trend descriptor (FTD) using a fashion descriptor, a codeword vetor, and temporal analysis. To unveil fashion trends in the FCDB, the temporal analysis in FTD effectively emphasizes consecutive features between two different times. In experiments, we clearly show the analysis of fashion trends and fashion-based city similarity. As the result of large-scale data collection and an unsupervised analyzer, the proposed approach achieves world-level fashion visualization in a time series. The code, model, and FCDB will be publicly available after the construction of the project page.|paper present novel concept analyz visual worldwid fashion trend goal reveal cut edg fashion trend without display ordinari fashion style achiev fashion base analysi creat new fashion cultur databas fcdb consist million geo tag imag cosmopolitan citi grasp fashion trend mix fashion style paper also propos unsupervis fashion trend descriptor ftd use fashion descriptor codeword vetor tempor analysi unveil fashion trend fcdb tempor analysi ftd effect emphas consecut featur two differ time experi clear show analysi fashion trend fashion base citi similar result larg scale data collect unsupervis analyz propos approach achiev world level fashion visual time seri code model fcdb public avail construct project page|['Kaori Abe', 'Teppei Suzuki', 'Shunya Ueta', 'Akio Nakamura', 'Yutaka Satoh', 'Hirokatsu Kataoka']|['cs.CV', 'cs.DB', 'cs.MM']
2017-03-28T14:09:13Z|2017-03-23T02:50:32Z|http://arxiv.org/abs/1703.07910v1|http://arxiv.org/pdf/1703.07910v1|Bidirectional-Convolutional LSTM Based Spectral-Spatial Feature Learning   for Hyperspectral Image Classification|bidirect convolut lstm base spectral spatial featur learn hyperspectr imag classif|This paper proposes a novel deep learning framework named bidirectional-convolutional long short term memory (Bi-CLSTM) network to automatically learn the spectral-spatial feature from hyperspectral images (HSIs). In the network, the issue of spectral feature extraction is considered as a sequence learning problem, and a recurrent connection operator across the spectral domain is used to address it. Meanwhile, inspired from the widely used convolutional neural network (CNN), a convolution operator across the spatial domain is incorporated into the network to extract the spatial feature. Besides, to sufficiently capture the spectral information, a bidirectional recurrent connection is proposed. In the classification phase, the learned features are concatenated into a vector and fed to a softmax classifier via a fully-connected operator. To validate the effectiveness of the proposed Bi-CLSTM framework, we compare it with several state-of-the-art methods, including the CNN framework, on three widely used HSIs. The obtained results show that Bi-CLSTM can improve the classification performance as compared to other methods.|paper propos novel deep learn framework name bidirect convolut long short term memori bi clstm network automat learn spectral spatial featur hyperspectr imag hsis network issu spectral featur extract consid sequenc learn problem recurr connect oper across spectral domain use address meanwhil inspir wide use convolut neural network cnn convolut oper across spatial domain incorpor network extract spatial featur besid suffici captur spectral inform bidirect recurr connect propos classif phase learn featur concaten vector fed softmax classifi via fulli connect oper valid effect propos bi clstm framework compar sever state art method includ cnn framework three wide use hsis obtain result show bi clstm improv classif perform compar method|['Qingshan Liu', 'Feng Zhou', 'Renlong Hang', 'Xiaotong Yuan']|['cs.CV']
2017-03-28T14:09:13Z|2017-03-23T01:51:14Z|http://arxiv.org/abs/1703.08173v1|http://arxiv.org/pdf/1703.08173v1|Single Image Super-resolution with a Parameter Economic Residual-like   Convolutional Neural Network|singl imag super resolut paramet econom residu like convolut neural network|Recent years have witnessed great success of convolutional neural network (CNN) for various problems both in low and high level visions. Especially noteworthy is the residual network which was originally proposed to handle high-level vision problems and enjoys several merits. This paper aims to extend the merits of residual network, such as skip connection induced fast training, for a typical low-level vision problem, i.e., single image super-resolution. In general, the two main challenges of existing deep CNN for supper-resolution lie in the gradient exploding/vanishing problem and large amount of parameters or computational cost as CNN goes deeper. Correspondingly, the skip connections or identity mapping shortcuts are utilized to avoid gradient exploding/vanishing problem. To tackle with the second problem, a parameter economic CNN architecture which has carefully designed width, depth and skip connections was proposed. Different residual-like architectures for image superresolution has also been compared. Experimental results have demonstrated that the proposed CNN model can not only achieve state-of-the-art PSNR and SSIM results for single image super-resolution but also produce visually pleasant results. This paper has extended the mmm 2017 paper with more experiments and explanations.|recent year wit great success convolut neural network cnn various problem low high level vision especi noteworthi residu network origin propos handl high level vision problem enjoy sever merit paper aim extend merit residu network skip connect induc fast train typic low level vision problem singl imag super resolut general two main challeng exist deep cnn supper resolut lie gradient explod vanish problem larg amount paramet comput cost cnn goe deeper correspond skip connect ident map shortcut util avoid gradient explod vanish problem tackl second problem paramet econom cnn architectur care design width depth skip connect propos differ residu like architectur imag superresolut also compar experiment result demonstr propos cnn model onli achiev state art psnr ssim result singl imag super resolut also produc visual pleasant result paper extend mmm paper experi explan|['Yudong Liang', 'Ze Yang', 'Kai Zhang', 'Yihui He', 'Jinjun Wang', 'Nanning Zheng']|['cs.CV']
2017-03-28T14:09:13Z|2017-03-22T23:35:51Z|http://arxiv.org/abs/1703.07886v1|http://arxiv.org/pdf/1703.07886v1|Robust Kronecker-Decomposable Component Analysis for Low Rank Modeling|robust kroneck decompos compon analysi low rank model|Dictionary learning and component analysis are part of one of the most well-studied and active research fields, at the intersection of signal and image processing, computer vision, and statistical machine learning. In dictionary learning, the current methods of choice are arguably K-SVD and its variants, which learn a dictionary (i.e., a decomposition) for sparse coding via Singular Value Decomposition. In robust component analysis, leading methods derive from Principal Component Pursuit (PCP), which recovers a low-rank matrix from sparse corruptions of unknown magnitude and support. While K-SVD is sensitive to the presence of noise and outliers in the training set, PCP does not provide a dictionary that respects the structure of the data (e.g., images), and requires expensive SVD computations when solved by convex relaxation. In this paper, we introduce a new robust decomposition of images by combining ideas from sparse dictionary learning and PCP. We propose a novel Kronecker-decomposable component analysis which is robust to gross corruption, can be used for low-rank modeling, and leverages separability to solve significantly smaller problems. We design an efficient learning algorithm by drawing links with a restricted form of tensor factorization. The effectiveness of the proposed approach is demonstrated on real-world applications, namely background subtraction and image denoising, by performing a thorough comparison with the current state of the art.|dictionari learn compon analysi part one well studi activ research field intersect signal imag process comput vision statist machin learn dictionari learn current method choic arguabl svd variant learn dictionari decomposit spars code via singular valu decomposit robust compon analysi lead method deriv princip compon pursuit pcp recov low rank matrix spars corrupt unknown magnitud support svd sensit presenc nois outlier train set pcp doe provid dictionari respect structur data imag requir expens svd comput solv convex relax paper introduc new robust decomposit imag combin idea spars dictionari learn pcp propos novel kroneck decompos compon analysi robust gross corrupt use low rank model leverag separ solv signific smaller problem design effici learn algorithm draw link restrict form tensor factor effect propos approach demonstr real world applic name background subtract imag denois perform thorough comparison current state art|['Mehdi Bahri', 'Yannis Panagakis', 'Stefanos Zafeiriou']|['stat.ML', 'cs.CV']
2017-03-28T14:09:13Z|2017-03-22T20:00:15Z|http://arxiv.org/abs/1703.07834v1|http://arxiv.org/pdf/1703.07834v1|Large Pose 3D Face Reconstruction from a Single Image via Direct   Volumetric CNN Regression|larg pose face reconstruct singl imag via direct volumetr cnn regress|3D face reconstruction is a fundamental Computer Vision problem of extraordinary difficulty. Current systems often assume the availability of multiple facial images (sometimes from the same subject) as input, and must address a number of methodological challenges such as establishing dense correspondences across large facial poses, expressions, and non-uniform illumination. In general these methods require complex and inefficient pipelines for model building and fitting. In this work, we propose to address many of these limitations by training a Convolutional Neural Network (CNN) on an appropriate dataset consisting of 2D images and 3D facial models or scans. Our CNN works with just a single 2D facial image, does not require accurate alignment nor establishes dense correspondence between images, works for arbitrary facial poses and expressions, and can be used to reconstruct the whole 3D facial geometry (including the non-visible parts of the face) bypassing the construction (during training) and fitting (during testing) of a 3D Morphable Model. We achieve this via a simple CNN architecture that performs direct regression of a volumetric representation of the 3D facial geometry from a single 2D image. We also demonstrate how the related task of facial landmark localization can be incorporated into the proposed framework and help improve reconstruction quality, especially for the cases of large poses and facial expressions. Testing code will be made available online, along with pre-trained models http://aaronsplace.co.uk/papers/jackson2017recon.|face reconstruct fundament comput vision problem extraordinari difficulti current system often assum avail multipl facial imag sometim subject input must address number methodolog challeng establish dens correspond across larg facial pose express non uniform illumin general method requir complex ineffici pipelin model build fit work propos address mani limit train convolut neural network cnn appropri dataset consist imag facial model scan cnn work singl facial imag doe requir accur align establish dens correspond imag work arbitrari facial pose express use reconstruct whole facial geometri includ non visibl part face bypass construct dure train fit dure test morphabl model achiev via simpl cnn architectur perform direct regress volumetr represent facial geometri singl imag also demonstr relat task facial landmark local incorpor propos framework help improv reconstruct qualiti especi case larg pose facial express test code made avail onlin along pre train model http aaronsplac co uk paper jacksonrecon|['Aaron S. Jackson', 'Adrian Bulat', 'Vasileios Argyriou', 'Georgios Tzimiropoulos']|['cs.CV']
2017-03-28T14:09:13Z|2017-03-22T18:51:51Z|http://arxiv.org/abs/1703.07815v1|http://arxiv.org/pdf/1703.07815v1|Cross-View Image Matching for Geo-localization in Urban Environments|cross view imag match geo local urban environ|In this paper, we address the problem of cross-view image geo-localization. Specifically, we aim to estimate the GPS location of a query street view image by finding the matching images in a reference database of geo-tagged bird's eye view images, or vice versa. To this end, we present a new framework for cross-view image geo-localization by taking advantage of the tremendous success of deep convolutional neural networks (CNNs) in image classification and object detection. First, we employ the Faster R-CNN to detect buildings in the query and reference images. Next, for each building in the query image, we retrieve the $k$ nearest neighbors from the reference buildings using a Siamese network trained on both positive matching image pairs and negative pairs. To find the correct NN for each query building, we develop an efficient multiple nearest neighbors matching method based on dominant sets. We evaluate the proposed framework on a new dataset that consists of pairs of street view and bird's eye view images. Experimental results show that the proposed method achieves better geo-localization accuracy than other approaches and is able to generalize to images at unseen locations.|paper address problem cross view imag geo local specif aim estim gps locat queri street view imag find match imag refer databas geo tag bird eye view imag vice versa end present new framework cross view imag geo local take advantag tremend success deep convolut neural network cnns imag classif object detect first employ faster cnn detect build queri refer imag next build queri imag retriev nearest neighbor refer build use siames network train posit match imag pair negat pair find correct nn queri build develop effici multipl nearest neighbor match method base domin set evalu propos framework new dataset consist pair street view bird eye view imag experiment result show propos method achiev better geo local accuraci approach abl general imag unseen locat|['Yicong Tian', 'Chen Chen', 'Mubarak Shah']|['cs.CV']
2017-03-28T14:09:13Z|2017-03-22T18:49:05Z|http://arxiv.org/abs/1703.07814v1|http://arxiv.org/pdf/1703.07814v1|R-C3D: Region Convolutional 3D Network for Temporal Activity Detection|cd region convolut network tempor activ detect|We address the problem of activity detection in continuous, untrimmed video streams. This is a difficult task that requires extracting meaningful spatio-temporal features to capture activities, accurately localizing the start and end times of each activity, and also dealing with very large data volumes. We introduce a new model, Region Convolutional 3D Network (R-C3D), which encodes the video streams using a three-dimensional fully convolutional network, then generates candidate temporal regions containing activities, and finally classifies selected regions into specific activities. Computation is saved due to the sharing of convolutional features between the proposal and the classification pipelines. The entire model is trained end-to-end with jointly optimized localization and classification losses. R-C3D is faster than existing methods (569 frames per second on a single Titan X Maxwell GPU) and achieves state-of-the-art results on THUMOS'14 (10\% absolute improvement). We further demonstrate that our model is a general activity detection framework that does not rely on assumptions about particular dataset properties by evaluating our approach on ActivityNet and Charades.|address problem activ detect continu untrim video stream difficult task requir extract meaning spatio tempor featur captur activ accur local start end time activ also deal veri larg data volum introduc new model region convolut network cd encod video stream use three dimension fulli convolut network generat candid tempor region contain activ final classifi select region specif activ comput save due share convolut featur propos classif pipelin entir model train end end joint optim local classif loss cd faster exist method frame per second singl titan maxwel gpu achiev state art result thumo absolut improv demonstr model general activ detect framework doe reli assumpt particular dataset properti evalu approach activitynet charad|['Huijuan Xu', 'Abir Das', 'Kate Saenko']|['cs.CV']
2017-03-28T14:09:13Z|2017-03-27T13:39:01Z|http://arxiv.org/abs/1703.07737v2|http://arxiv.org/pdf/1703.07737v2|In Defense of the Triplet Loss for Person Re-Identification|defens triplet loss person identif|In the past few years, the field of computer vision has gone through a revolution fueled mainly by the advent of large datasets and the adoption of deep convolutional neural networks for end-to-end learning. The person re-identification subfield is no exception to this, thanks to the notable publication of the Market-1501 and MARS datasets and several strong deep learning approaches. Unfortunately, a prevailing belief in the community seems to be that the triplet loss is inferior to using surrogate losses (classification, verification) followed by a separate metric learning step. We show that, for models trained from scratch as well as pretrained ones, using a variant of the triplet loss to perform end-to-end deep metric learning outperforms any other published method by a large margin.|past year field comput vision gone revolut fuel main advent larg dataset adopt deep convolut neural network end end learn person identif subfield except thank notabl public market mar dataset sever strong deep learn approach unfortun prevail belief communiti seem triplet loss inferior use surrog loss classif verif follow separ metric learn step show model train scratch well pretrain one use variant triplet loss perform end end deep metric learn outperform ani publish method larg margin|['Alexander Hermans', 'Lucas Beyer', 'Bastian Leibe']|['cs.CV', 'cs.NE']
2017-03-28T14:09:17Z|2017-03-22T15:46:49Z|http://arxiv.org/abs/1703.07715v1|http://arxiv.org/pdf/1703.07715v1|Classifying Symmetrical Differences and Temporal Change in Mammography   Using Deep Neural Networks|classifi symmetr differ tempor chang mammographi use deep neural network|We investigate the addition of symmetry and temporal context information to a deep Convolutional Neural Network (CNN) with the purpose of detecting malignant soft tissue lesions in mammography. We employ a simple linear mapping that takes the location of a mass candidate and maps it to either the contra-lateral or prior mammogram and Regions Of Interest (ROI) are extracted around each location. We subsequently explore two different architectures (1) a fusion model employing two datastreams were both ROIs are fed to the network during training and testing and (2) a stage-wise approach where a single ROI CNN is trained on the primary image and subsequently used as feature extractor for both primary and symmetrical or prior ROIs. A 'shallow' Gradient Boosted Tree (GBT) classifier is then trained on the concatenation of these features and used to classify the joint representation. Results shown a significant increase in performance using the first architecture and symmetry information, but only marginal gains in performance using temporal data and the other setting. We feel results are promising and can greatly be improved when more temporal data becomes available.|investig addit symmetri tempor context inform deep convolut neural network cnn purpos detect malign soft tissu lesion mammographi employ simpl linear map take locat mass candid map either contra later prior mammogram region interest roi extract around locat subsequ explor two differ architectur fusion model employ two datastream roi fed network dure train test stage wise approach singl roi cnn train primari imag subsequ use featur extractor primari symmetr prior roi shallow gradient boost tree gbt classifi train concaten featur use classifi joint represent result shown signific increas perform use first architectur symmetri inform onli margin gain perform use tempor data set feel result promis great improv tempor data becom avail|['Thijs Kooi', 'Nico Karssemeijer']|['cs.CV']
2017-03-28T14:09:17Z|2017-03-22T14:45:15Z|http://arxiv.org/abs/1703.07684v1|http://arxiv.org/pdf/1703.07684v1|Predicting Deeper into the Future of Semantic Segmentation|predict deeper futur semant segment|The ability to predict and therefore to anticipate the future is an important attribute of intelligence. It is also of utmost importance in real-time systems, e.g. in robotics or autonomous driving, which depend on visual scene understanding for decision making. While prediction of the raw RGB pixel values in future video frames has been studied in previous work, here we focus on predicting semantic segmentations of future frames. More precisely, given a sequence of semantically segmented video frames, our goal is to predict segmentation maps of not yet observed video frames that lie up to a second or further in the future. We develop an autoregressive convolutional neural network that learns to iteratively generate multiple frames. Our results on the Cityscapes dataset show that directly predicting future segmentations is substantially better than predicting and then segmenting future RGB frames. Our models predict trajectories of cars and pedestrians much more accurately (25%) than baselines that copy the most recent semantic segmentation or warp it using optical flow. Prediction results up to half a second in the future are visually convincing, the mean IoU of predicted segmentations reaching two thirds of the real future segmentations.|abil predict therefor anticip futur import attribut intellig also utmost import real time system robot autonom drive depend visual scene understand decis make predict raw rgb pixel valu futur video frame studi previous work focus predict semant segment futur frame precis given sequenc semant segment video frame goal predict segment map yet observ video frame lie second futur develop autoregress convolut neural network learn iter generat multipl frame result cityscap dataset show direct predict futur segment substanti better predict segment futur rgb frame model predict trajectori car pedestrian much accur baselin copi recent semant segment warp use optic flow predict result half second futur visual convinc mean iou predict segment reach two third real futur segment|['Natalia Neverova', 'Pauline Luc', 'Camille Couprie', 'Jakob Verbeek', 'Yann LeCun']|['cs.CV', 'cs.LG']
2017-03-28T14:09:17Z|2017-03-22T13:48:47Z|http://arxiv.org/abs/1703.07655v1|http://arxiv.org/pdf/1703.07655v1|ASP: Learning to Forget with Adaptive Synaptic Plasticity in Spiking   Neural Networks|asp learn forget adapt synapt plastic spike neural network|"A fundamental feature of learning in animals is the ""ability to forget"" that allows an organism to perceive, model and make decisions from disparate streams of information and adapt to changing environments. Against this backdrop, we present a novel unsupervised learning mechanism ASP (Adaptive Synaptic Plasticity) for improved recognition with Spiking Neural Networks (SNNs) for real time on-line learning in a dynamic environment. We incorporate an adaptive weight decay mechanism with the traditional Spike Timing Dependent Plasticity (STDP) learning to model adaptivity in SNNs. The leak rate of the synaptic weights is modulated based on the temporal correlation between the spiking patterns of the pre- and post-synaptic neurons. This mechanism helps in gradual forgetting of insignificant data while retaining significant, yet old, information. ASP, thus, maintains a balance between forgetting and immediate learning to construct a stable-plastic self-adaptive SNN for continuously changing inputs. We demonstrate that the proposed learning methodology addresses catastrophic forgetting while yielding significantly improved accuracy over the conventional STDP learning method for digit recognition applications. Additionally, we observe that the proposed learning model automatically encodes selective attention towards relevant features in the input data while eliminating the influence of background noise (or denoising) further improving the robustness of the ASP learning."|fundament featur learn anim abil forget allow organ perceiv model make decis dispar stream inform adapt chang environ backdrop present novel unsupervis learn mechan asp adapt synapt plastic improv recognit spike neural network snns real time line learn dynam environ incorpor adapt weight decay mechan tradit spike time depend plastic stdp learn model adapt snns leak rate synapt weight modul base tempor correl spike pattern pre post synapt neuron mechan help gradual forget insignific data retain signific yet old inform asp thus maintain balanc forget immedi learn construct stabl plastic self adapt snn continu chang input demonstr propos learn methodolog address catastroph forget yield signific improv accuraci convent stdp learn method digit recognit applic addit observ propos learn model automat encod select attent toward relev featur input data elimin influenc background nois denois improv robust asp learn|['Priyadarshini Panda', 'Jason M. Allred', 'Shriram Ramanathan', 'Kaushik Roy']|['cs.NE', 'cs.CV']
2017-03-28T14:09:17Z|2017-03-22T13:35:49Z|http://arxiv.org/abs/1703.07645v1|http://arxiv.org/pdf/1703.07645v1|Neural Ctrl-F: Segmentation-free Query-by-String Word Spotting in   Handwritten Manuscript Collections|neural ctrl segment free queri string word spot handwritten manuscript collect|In this paper, we approach the problem of segmentation-free query-by-string word spotting for handwritten documents. In other words, we use methods inspired from computer vision and machine learning to search for words in large collections of digitized manuscripts. In particular, we are interested in historical handwritten texts, which are often far more challenging than modern printed documents. This task is important, as it provides people with a way to quickly find what they are looking for in large collections that are tedious and difficult to read manually. To this end, we introduce an end-to-end trainable model based on deep neural networks that we call Ctrl-F-Net. Given a full manuscript page, the model simultaneously generates region proposals, and embeds these into a distributed word embedding space, where searches are performed. We evaluate the model on common benchmarks for handwritten word spotting, outperforming the previous state-of-the-art segmentation-free approaches by a large margin, and in some cases even segmentation-based approaches. One interesting real-life application of our approach is to help historians to find and count specific words in court records that are related to women's sustenance activities and division of labor. We provide promising preliminary experiments that validate our method on this task.|paper approach problem segment free queri string word spot handwritten document word use method inspir comput vision machin learn search word larg collect digit manuscript particular interest histor handwritten text often far challeng modern print document task import provid peopl way quick find look larg collect tedious difficult read manual end introduc end end trainabl model base deep neural network call ctrl net given full manuscript page model simultan generat region propos emb distribut word embed space search perform evalu model common benchmark handwritten word spot outperform previous state art segment free approach larg margin case even segment base approach one interest real life applic approach help historian find count specif word court record relat women susten activ divis labor provid promis preliminari experi valid method task|['Tomas Wilkinson', 'Jonas Lindstr√∂m', 'Anders Brun']|['cs.CV']
2017-03-28T14:09:17Z|2017-03-22T10:35:58Z|http://arxiv.org/abs/1703.07595v1|http://arxiv.org/pdf/1703.07595v1|Can you tell where in India I am from? Comparing humans and computers on   fine-grained race face classification|tell india compar human comput fine grain race face classif|Faces form the basis for a rich variety of judgments in humans, yet the underlying features remain poorly understood. Although fine-grained distinctions within a race might more strongly constrain possible facial features used by humans than in case of coarse categories such as race or gender, such fine grained distinctions are relatively less studied. Fine-grained race classification is also interesting because even humans may not be perfectly accurate on these tasks. This allows us to compare errors made by humans and machines, in contrast to standard object detection tasks where human performance is nearly perfect. We have developed a novel face database of close to 1650 diverse Indian faces labeled for fine-grained race (South vs North India) as well as for age, weight, height and gender. We then asked close to 130 human subjects who were instructed to categorize each face as belonging toa Northern or Southern state in India. We then compared human performance on this task with that of computational models trained on the ground-truth labels. Our main results are as follows: (1) Humans are highly consistent (average accuracy: 63.6%), with some faces being consistently classified with > 90% accuracy and others consistently misclassified with < 30% accuracy; (2) Models trained on ground-truth labels showed slightly worse performance (average accuracy: 62%) but showed higher accuracy (72.2%) on faces classified with > 80% accuracy by humans. This was true for models trained on simple spatial and intensity measurements extracted from faces as well as deep neural networks trained on race or gender classification; (3) Using overcomplete banks of features derived from each face part, we found that mouth shape was the single largest contributor towards fine-grained race classification, whereas distances between face parts was the strongest predictor of gender.|face form basi rich varieti judgment human yet featur remain poor understood although fine grain distinct within race might strong constrain possibl facial featur use human case coars categori race gender fine grain distinct relat less studi fine grain race classif also interest becaus even human may perfect accur task allow us compar error made human machin contrast standard object detect task human perform near perfect develop novel face databas close divers indian face label fine grain race south vs north india well age weight height gender ask close human subject instruct categor face belong toa northern southern state india compar human perform task comput model train ground truth label main result follow human high consist averag accuraci face consist classifi accuraci consist misclassifi accuraci model train ground truth label show slight wors perform averag accuraci show higher accuraci face classifi accuraci human true model train simpl spatial intens measur extract face well deep neural network train race gender classif use overcomplet bank featur deriv face part found mouth shape singl largest contributor toward fine grain race classif wherea distanc face part strongest predictor gender|['Harish Katti', 'S. P. Arun']|['cs.CV']
2017-03-28T14:09:17Z|2017-03-22T09:25:49Z|http://arxiv.org/abs/1703.07579v1|http://arxiv.org/pdf/1703.07579v1|An End-to-End Approach to Natural Language Object Retrieval via   Context-Aware Deep Reinforcement Learning|end end approach natur languag object retriev via context awar deep reinforc learn|We propose an end-to-end approach to the natural language object retrieval task, which localizes an object within an image according to a natural language description, i.e., referring expression. Previous works divide this problem into two independent stages: first, compute region proposals from the image without the exploration of the language description; second, score the object proposals with regard to the referring expression and choose the top-ranked proposals. The object proposals are generated independently from the referring expression, which makes the proposal generation redundant and even irrelevant to the referred object. In this work, we train an agent with deep reinforcement learning, which learns to move and reshape a bounding box to localize the object according to the referring expression. We incorporate both the spatial and temporal context information into the training procedure. By simultaneously exploiting local visual information, the spatial and temporal context and the referring language a priori, the agent selects an appropriate action to take at each time. A special action is defined to indicate when the agent finds the referred object, and terminate the procedure. We evaluate our model on various datasets, and our algorithm significantly outperforms the compared algorithms. Notably, the accuracy improvement of our method over the recent method GroundeR and SCRC on the ReferItGame dataset are 7.67% and 18.25%, respectively.|propos end end approach natur languag object retriev task local object within imag accord natur languag descript refer express previous work divid problem two independ stage first comput region propos imag without explor languag descript second score object propos regard refer express choos top rank propos object propos generat independ refer express make propos generat redund even irrelev refer object work train agent deep reinforc learn learn move reshap bound box local object accord refer express incorpor spatial tempor context inform train procedur simultan exploit local visual inform spatial tempor context refer languag priori agent select appropri action take time special action defin indic agent find refer object termin procedur evalu model various dataset algorithm signific outperform compar algorithm notabl accuraci improv method recent method grounder scrc referitgam dataset respect|['Fan Wu', 'Zhongwen Xu', 'Yi Yang']|['cs.CV']
2017-03-28T14:09:17Z|2017-03-22T09:03:25Z|http://arxiv.org/abs/1703.07570v1|http://arxiv.org/pdf/1703.07570v1|Deep MANTA: A Coarse-to-fine Many-Task Network for joint 2D and 3D   vehicle analysis from monocular image|deep manta coars fine mani task network joint vehicl analysi monocular imag|In this paper, we present a novel approach, called Deep MANTA (Deep Many-Tasks), for many-task vehicle analysis from a given image. A robust convolutional network is introduced for simultaneous vehicle detection, part localization, visibility characterization and 3D dimension estimation. Its architecture is based on a new coarse-to-fine object proposal that boosts the vehicle detection. Moreover, the Deep MANTA network is able to localize vehicle parts even if these parts are not visible. In the inference, the network's outputs are used by a real time robust pose estimation algorithm for fine orientation estimation and 3D vehicle localization. We show in experiments that our method outperforms monocular state-of-the-art approaches on vehicle detection, orientation and 3D location tasks on the very challenging KITTI benchmark.|paper present novel approach call deep manta deep mani task mani task vehicl analysi given imag robust convolut network introduc simultan vehicl detect part local visibl character dimens estim architectur base new coars fine object propos boost vehicl detect moreov deep manta network abl local vehicl part even part visibl infer network output use real time robust pose estim algorithm fine orient estim vehicl local show experi method outperform monocular state art approach vehicl detect orient locat task veri challeng kitti benchmark|['Florian Chabot', 'Mohamed Chaouch', 'Jaonary Rabarisoa', 'C√©line Teuli√®re', 'Thierry Chateau']|['cs.CV']
2017-03-28T14:09:17Z|2017-03-23T13:56:01Z|http://arxiv.org/abs/1703.07523v2|http://arxiv.org/pdf/1703.07523v2|Deeply-Supervised CNN for Prostate Segmentation|deepli supervis cnn prostat segment|Prostate segmentation from Magnetic Resonance (MR) images plays an important role in image guided interven- tion. However, the lack of clear boundary specifically at the apex and base, and huge variation of shape and texture between the images from different patients make the task very challenging. To overcome these problems, in this paper, we propose a deeply supervised convolutional neural network (CNN) utilizing the convolutional information to accurately segment the prostate from MR images. The proposed model can effectively detect the prostate region with additional deeply supervised layers compared with other approaches. Since some information will be abandoned after convolution, it is necessary to pass the features extracted from early stages to later stages. The experimental results show that significant segmentation accuracy improvement has been achieved by our proposed method compared to other reported approaches.|prostat segment magnet reson mr imag play import role imag guid interven tion howev lack clear boundari specif apex base huge variat shape textur imag differ patient make task veri challeng overcom problem paper propos deepli supervis convolut neural network cnn util convolut inform accur segment prostat mr imag propos model effect detect prostat region addit deepli supervis layer compar approach sinc inform abandon convolut necessari pass featur extract earli stage later stage experiment result show signific segment accuraci improv achiev propos method compar report approach|['Qikui Zhu', 'Bo Du', 'Baris Turkbey', 'Peter L . Choyke', 'Pingkun Yan']|['cs.CV']
2017-03-28T14:09:17Z|2017-03-22T04:40:51Z|http://arxiv.org/abs/1703.07519v1|http://arxiv.org/pdf/1703.07519v1|Joint Intermodal and Intramodal Label Transfers for Extremely Rare or   Unseen Classes|joint intermod intramod label transfer extrem rare unseen class|In this paper, we present a label transfer model from texts to images for image classification tasks. The problem of image classification is often much more challenging than text classification. On one hand, labeled text data is more widely available than the labeled images for classification tasks. On the other hand, text data tends to have natural semantic interpretability, and they are often more directly related to class labels. On the contrary, the image features are not directly related to concepts inherent in class labels. One of our goals in this paper is to develop a model for revealing the functional relationships between text and image features as to directly transfer intermodal and intramodal labels to annotate the images. This is implemented by learning a transfer function as a bridge to propagate the labels between two multimodal spaces. However, the intermodal label transfers could be undermined by blindly transferring the labels of noisy texts to annotate images. To mitigate this problem, we present an intramodal label transfer process, which complements the intermodal label transfer by transferring the image labels instead when relevant text is absent from the source corpus. In addition, we generalize the inter-modal label transfer to zero-shot learning scenario where there are only text examples available to label unseen classes of images without any positive image examples. We evaluate our algorithm on an image classification task and show the effectiveness with respect to the other compared algorithms.|paper present label transfer model text imag imag classif task problem imag classif often much challeng text classif one hand label text data wide avail label imag classif task hand text data tend natur semant interpret often direct relat class label contrari imag featur direct relat concept inher class label one goal paper develop model reveal function relationship text imag featur direct transfer intermod intramod label annot imag implement learn transfer function bridg propag label two multimod space howev intermod label transfer could undermin blind transfer label noisi text annot imag mitig problem present intramod label transfer process complement intermod label transfer transfer imag label instead relev text absent sourc corpus addit general inter modal label transfer zero shot learn scenario onli text exampl avail label unseen class imag without ani posit imag exampl evalu algorithm imag classif task show effect respect compar algorithm|['Guo-Jun Qi', 'Wei Liu', 'Charu Aggarwal', 'Thomas Huang']|['cs.CV']
2017-03-28T14:09:17Z|2017-03-22T04:31:38Z|http://arxiv.org/abs/1703.07514v1|http://arxiv.org/pdf/1703.07514v1|Video Frame Interpolation via Adaptive Convolution|video frame interpol via adapt convolut|Video frame interpolation typically involves two steps: motion estimation and pixel synthesis. Such a two-step approach heavily depends on the quality of motion estimation. This paper presents a robust video frame interpolation method that combines these two steps into a single process. Specifically, our method considers pixel synthesis for the interpolated frame as local convolution over two input frames. The convolution kernel captures both the local motion between the input frames and the coefficients for pixel synthesis. Our method employs a deep fully convolutional neural network to estimate a spatially-adaptive convolution kernel for each pixel. This deep neural network can be directly trained end to end using widely available video data without any difficult-to-obtain ground-truth data like optical flow. Our experiments show that the formulation of video interpolation as a single convolution process allows our method to gracefully handle challenges like occlusion, blur, and abrupt brightness change and enables high-quality video frame interpolation.|video frame interpol typic involv two step motion estim pixel synthesi two step approach heavili depend qualiti motion estim paper present robust video frame interpol method combin two step singl process specif method consid pixel synthesi interpol frame local convolut two input frame convolut kernel captur local motion input frame coeffici pixel synthesi method employ deep fulli convolut neural network estim spatial adapt convolut kernel pixel deep neural network direct train end end use wide avail video data without ani difficult obtain ground truth data like optic flow experi show formul video interpol singl convolut process allow method grace handl challeng like occlus blur abrupt bright chang enabl high qualiti video frame interpol|['Simon Niklaus', 'Long Mai', 'Feng Liu']|['cs.CV']
2017-03-28T14:09:21Z|2017-03-22T04:21:41Z|http://arxiv.org/abs/1703.07511v1|http://arxiv.org/pdf/1703.07511v1|Deep Photo Style Transfer|deep photo style transfer|This paper introduces a deep-learning approach to photographic style transfer that handles a large variety of image content while faithfully transferring the reference style. Our approach builds upon recent work on painterly transfer that separates style from the content of an image by considering different layers of a neural network. However, as is, this approach is not suitable for photorealistic style transfer. Even when both the input and reference images are photographs, the output still exhibits distortions reminiscent of a painting. Our contribution is to constrain the transformation from the input to the output to be locally affine in colorspace, and to express this constraint as a custom CNN layer through which we can backpropagate. We show that this approach successfully suppresses distortion and yields satisfying photorealistic style transfers in a broad variety of scenarios, including transfer of the time of day, weather, season, and artistic edits.|paper introduc deep learn approach photograph style transfer handl larg varieti imag content faith transfer refer style approach build upon recent work painter transfer separ style content imag consid differ layer neural network howev approach suitabl photorealist style transfer even input refer imag photograph output still exhibit distort reminisc paint contribut constrain transform input output local affin colorspac express constraint custom cnn layer backpropag show approach success suppress distort yield satisfi photorealist style transfer broad varieti scenario includ transfer time day weather season artist edit|['Fujun Luan', 'Sylvain Paris', 'Eli Shechtman', 'Kavita Bala']|['cs.CV']
2017-03-28T14:09:21Z|2017-03-22T00:51:14Z|http://arxiv.org/abs/1703.07479v1|http://arxiv.org/pdf/1703.07479v1|Knowledge Transfer for Melanoma Screening with Deep Learning|knowledg transfer melanoma screen deep learn|Knowledge transfer impacts the performance of deep learning -- the state of the art for image classification tasks, including automated melanoma screening. Deep learning's greed for large amounts of training data poses a challenge for medical tasks, which we can alleviate by recycling knowledge from models trained on different tasks, in a scheme called transfer learning. Although much of the best art on automated melanoma screening employs some form of transfer learning, a systematic evaluation was missing. Here we investigate the presence of transfer, from which task the transfer is sourced, and the application of fine tuning (i.e., retraining of the deep learning model after transfer). We also test the impact of picking deeper (and more expensive) models. Our results favor deeper models, pre-trained over ImageNet, with fine-tuning, reaching an AUC of 80.7% and 84.5% for the two skin-lesion datasets evaluated.|knowledg transfer impact perform deep learn state art imag classif task includ autom melanoma screen deep learn greed larg amount train data pose challeng medic task allevi recycl knowledg model train differ task scheme call transfer learn although much best art autom melanoma screen employ form transfer learn systemat evalu miss investig presenc transfer task transfer sourc applic fine tune retrain deep learn model transfer also test impact pick deeper expens model result favor deeper model pre train imagenet fine tune reach auc two skin lesion dataset evalu|['Afonso Menegola', 'Michel Fornaciali', 'Ramon Pires', 'Fl√°via Vasques Bittencourt', 'Sandra Avila', 'Eduardo Valle']|['cs.CV']
2017-03-28T14:09:21Z|2017-03-23T21:37:40Z|http://arxiv.org/abs/1703.07478v2|http://arxiv.org/pdf/1703.07478v2|Spatially-Varying Blur Detection Based on Multiscale Fused and Sorted   Transform Coefficients of Gradient Magnitudes|spatial vari blur detect base multiscal fuse sort transform coeffici gradient magnitud|The detection of spatially-varying blur without having any information about the blur type is a challenging task. In this paper, we propose a novel effective approach to address the blur detection problem from a single image without requiring any knowledge about the blur type, level, or camera settings. Our approach computes blur detection maps based on a novel High-frequency multiscale Fusion and Sort Transform (HiFST) of gradient magnitudes. The evaluations of the proposed approach on a diverse set of blurry images with different blur types, levels, and contents demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods qualitatively and quantitatively.|detect spatial vari blur without ani inform blur type challeng task paper propos novel effect approach address blur detect problem singl imag without requir ani knowledg blur type level camera set approach comput blur detect map base novel high frequenc multiscal fusion sort transform hifst gradient magnitud evalu propos approach divers set blurri imag differ blur type level content demonstr propos algorithm perform favor state art method qualit quantit|['S. Alireza Golestaneh', 'Lina J. Karam']|['cs.CV']
2017-03-28T14:09:21Z|2017-03-22T00:22:49Z|http://arxiv.org/abs/1703.07475v1|http://arxiv.org/pdf/1703.07475v1|PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action   Understanding|pku mmd larg scale benchmark continu multi modal human action understand|Despite the fact that many 3D human activity benchmarks being proposed, most existing action datasets focus on the action recognition tasks for the segmented videos. There is a lack of standard large-scale benchmarks, especially for current popular data-hungry deep learning based methods. In this paper, we introduce a new large scale benchmark (PKU-MMD) for continuous multi-modality 3D human action understanding and cover a wide range of complex human activities with well annotated information. PKU-MMD contains 1076 long video sequences in 51 action categories, performed by 66 subjects in three camera views. It contains almost 20,000 action instances and 5.4 million frames in total. Our dataset also provides multi-modality data sources, including RGB, depth, Infrared Radiation and Skeleton. With different modalities, we conduct extensive experiments on our dataset in terms of two scenarios and evaluate different methods by various metrics, including a new proposed evaluation protocol 2D-AP. We believe this large-scale dataset will benefit future researches on action detection for the community.|despit fact mani human activ benchmark propos exist action dataset focus action recognit task segment video lack standard larg scale benchmark especi current popular data hungri deep learn base method paper introduc new larg scale benchmark pku mmd continu multi modal human action understand cover wide rang complex human activ well annot inform pku mmd contain long video sequenc action categori perform subject three camera view contain almost action instanc million frame total dataset also provid multi modal data sourc includ rgb depth infrar radiat skeleton differ modal conduct extens experi dataset term two scenario evalu differ method various metric includ new propos evalu protocol ap believ larg scale dataset benefit futur research action detect communiti|['Chunhui Liu', 'Yueyu Hu', 'Yanghao Li', 'Sijie Song', 'Jiaying Liu']|['cs.CV']
2017-03-28T14:09:21Z|2017-03-21T23:56:51Z|http://arxiv.org/abs/1703.07473v1|http://arxiv.org/pdf/1703.07473v1|Episode-Based Active Learning with Bayesian Neural Networks|episod base activ learn bayesian neural network|We investigate different strategies for active learning with Bayesian deep neural networks. We focus our analysis on scenarios where new, unlabeled data is obtained episodically, such as commonly encountered in mobile robotics applications. An evaluation of different strategies for acquisition, updating, and final training on the CIFAR-10 dataset shows that incremental network updates with final training on the accumulated acquisition set are essential for best performance, while limiting the amount of required human labeling labor.|investig differ strategi activ learn bayesian deep neural network focus analysi scenario new unlabel data obtain episod common encount mobil robot applic evalu differ strategi acquisit updat final train cifar dataset show increment network updat final train accumul acquisit set essenti best perform limit amount requir human label labor|['Feras Dayoub', 'Niko S√ºnderhauf', 'Peter Corke']|['cs.CV', 'cs.LG', 'stat.ML']
2017-03-28T14:09:21Z|2017-03-24T21:17:05Z|http://arxiv.org/abs/1703.07464v2|http://arxiv.org/pdf/1703.07464v2|No Fuss Distance Metric Learning using Proxies|fuss distanc metric learn use proxi|We address the problem of distance metric learning (DML), defined as learning a distance consistent with a notion of semantic similarity. Traditionally, for this problem supervision is expressed in the form of sets of points that follow an ordinal relationship -- an anchor point $x$ is similar to a set of positive points $Y$, and dissimilar to a set of negative points $Z$, and a loss defined over these distances is minimized.   While the specifics of the optimization differ, in this work we collectively call this type of supervision Triplets and all methods that follow this pattern Triplet-Based methods. These methods are challenging to optimize. A main issue is the need for finding informative triplets, which is usually achieved by a variety of tricks such as increasing the batch size, hard or semi-hard triplet mining, etc, but even with these tricks, the convergence rate of such methods is slow. In this paper we propose to optimize the triplet loss on a different space of triplets, consisting of an anchor data point and similar and dissimilar proxy points. These proxies approximate the original data points, so that a triplet loss over the proxies is a tight upper bound of the original loss. This proxy-based loss is empirically better behaved. As a result, the proxy-loss improves on state-of-art results for three standard zero-shot learning datasets, by up to 15% points, while converging three times as fast as other triplet-based losses.|address problem distanc metric learn dml defin learn distanc consist notion semant similar tradit problem supervis express form set point follow ordin relationship anchor point similar set posit point dissimilar set negat point loss defin distanc minim specif optim differ work collect call type supervis triplet method follow pattern triplet base method method challeng optim main issu need find inform triplet usual achiev varieti trick increas batch size hard semi hard triplet mine etc even trick converg rate method slow paper propos optim triplet loss differ space triplet consist anchor data point similar dissimilar proxi point proxi approxim origin data point triplet loss proxi tight upper bound origin loss proxi base loss empir better behav result proxi loss improv state art result three standard zero shot learn dataset point converg three time fast triplet base loss|['Yair Movshovitz-Attias', 'Alexander Toshev', 'Thomas K. Leung', 'Sergey Ioffe', 'Saurabh Singh']|['cs.CV']
2017-03-28T14:09:21Z|2017-03-21T21:05:21Z|http://arxiv.org/abs/1703.07431v1|http://arxiv.org/pdf/1703.07431v1|IOD-CNN: Integrating Object Detection Networks for Event Recognition|iod cnn integr object detect network event recognit|Many previous methods have showed the importance of considering semantically relevant objects for performing event recognition, yet none of the methods have exploited the power of deep convolutional neural networks to directly integrate relevant object information into a unified network. We present a novel unified deep CNN architecture which integrates architecturally different, yet semantically-related object detection networks to enhance the performance of the event recognition task. Our architecture allows the sharing of the convolutional layers and a fully connected layer which effectively integrates event recognition, rigid object detection and non-rigid object detection.|mani previous method show import consid semant relev object perform event recognit yet none method exploit power deep convolut neural network direct integr relev object inform unifi network present novel unifi deep cnn architectur integr architectur differ yet semant relat object detect network enhanc perform event recognit task architectur allow share convolut layer fulli connect layer effect integr event recognit rigid object detect non rigid object detect|['Sungmin Eum', 'Hyungtae Lee', 'Heesung Kwon', 'David Doermann']|['cs.CV']
2017-03-28T14:09:21Z|2017-03-21T19:40:25Z|http://arxiv.org/abs/1703.07402v1|http://arxiv.org/pdf/1703.07402v1|Simple Online and Realtime Tracking with a Deep Association Metric|simpl onlin realtim track deep associ metric|Simple Online and Realtime Tracking (SORT) is a pragmatic approach to multiple object tracking with a focus on simple, effective algorithms. In this paper, we integrate appearance information to improve the performance of SORT. Due to this extension we are able to track objects through longer periods of occlusions, effectively reducing the number of identity switches. In spirit of the original framework we place much of the computational complexity into an offline pre-training stage where we learn a deep association metric on a large-scale person re-identification dataset. During online application, we establish measurement-to-track associations using nearest neighbor queries in visual appearance space. Experimental evaluation shows that our extensions reduce the number of identity switches by 45%, achieving overall competitive performance at high frame rates.|simpl onlin realtim track sort pragmat approach multipl object track focus simpl effect algorithm paper integr appear inform improv perform sort due extens abl track object longer period occlus effect reduc number ident switch spirit origin framework place much comput complex offlin pre train stage learn deep associ metric larg scale person identif dataset dure onlin applic establish measur track associ use nearest neighbor queri visual appear space experiment evalu show extens reduc number ident switch achiev overal competit perform high frame rate|['Nicolai Wojke', 'Alex Bewley', 'Dietrich Paulus']|['cs.CV']
2017-03-28T14:09:21Z|2017-03-21T17:41:46Z|http://arxiv.org/abs/1703.07334v1|http://arxiv.org/pdf/1703.07334v1|Pop-up SLAM: Semantic Monocular Plane SLAM for Low-texture Environments|pop slam semant monocular plane slam low textur environ|Existing simultaneous localization and mapping (SLAM) algorithms are not robust in challenging low-texture environments because there are only few salient features. The resulting sparse or semi-dense map also conveys little information for motion planning. Though some work utilize plane or scene layout for dense map regularization, they require decent state estimation from other sources. In this paper, we propose real-time monocular plane SLAM to demonstrate that scene understanding could improve both state estimation and dense mapping especially in low-texture environments. The plane measurements come from a pop-up 3D plane model applied to each single image. We also combine planes with point based SLAM to improve robustness. On a public TUM dataset, our algorithm generates a dense semantic 3D model with pixel depth error of 6.2 cm while existing SLAM algorithms fail. On a 60 m long dataset with loops, our method creates a much better 3D model with state estimation error of 0.67%.|exist simultan local map slam algorithm robust challeng low textur environ becaus onli salient featur result spars semi dens map also convey littl inform motion plan though work util plane scene layout dens map regular requir decent state estim sourc paper propos real time monocular plane slam demonstr scene understand could improv state estim dens map especi low textur environ plane measur come pop plane model appli singl imag also combin plane point base slam improv robust public tum dataset algorithm generat dens semant model pixel depth error cm exist slam algorithm fail long dataset loop method creat much better model state estim error|['Shichao Yang', 'Yu Song', 'Michael Kaess', 'Sebastian Scherer']|['cs.CV', 'cs.RO']
2017-03-28T14:09:21Z|2017-03-21T17:37:36Z|http://arxiv.org/abs/1703.07332v1|http://arxiv.org/pdf/1703.07332v1|How far are we from solving the 2D & 3D Face Alignment problem? (and a   dataset of 230,000 3D facial landmarks)|far solv face align problem dataset facial landmark|"This paper investigates how far a very deep neural network is from attaining close to saturating performance on existing 2D and 3D face alignment datasets. To this end, we make the following three contributions: (a) we construct, for the first time, a very strong baseline by combining a state-of-the-art architecture for landmark localization with a state-of-the-art residual block, train it on a very large yet synthetically expanded 2D facial landmark dataset and finally evaluate it on all other 2D facial landmark datasets. (b) We create a guided by 2D landmarks network which converts 2D landmark annotations to 3D and unifies all existing datasets, leading to the creation of LS3D-W, the largest and most challenging 3D facial landmark dataset to date (~230,000 images). (c) Following that, we train a neural network for 3D face alignment and evaluate it on the newly introduced LS3D-W. (d) We further look into the effect of all ""traditional"" factors affecting face alignment performance like large pose, initialization and resolution, and introduce a ""new"" one, namely the size of the network. (e) We show that both 2D and 3D face alignment networks achieve performance of remarkable accuracy which is probably close to saturating the datasets used. Demo code and pre-trained models can be downloaded from http://www.cs.nott.ac.uk/~psxab5/face-alignment/"|paper investig far veri deep neural network attain close satur perform exist face align dataset end make follow three contribut construct first time veri strong baselin combin state art architectur landmark local state art residu block train veri larg yet synthet expand facial landmark dataset final evalu facial landmark dataset creat guid landmark network convert landmark annot unifi exist dataset lead creation lsd largest challeng facial landmark dataset date imag follow train neural network face align evalu newli introduc lsd look effect tradit factor affect face align perform like larg pose initi resolut introduc new one name size network show face align network achiev perform remark accuraci probabl close satur dataset use demo code pre train model download http www cs nott ac uk psxab face align|['Adrian Bulat', 'Georgios Tzimiropoulos']|['cs.CV']
