2017-03-28T14:05:16Z|2017-03-27T16:48:03Z|http://arxiv.org/abs/1703.09179v1|http://arxiv.org/pdf/1703.09179v1|Transfer learning for music classification and regression tasks|transfer learn music classif regress task|In this paper, we present a transfer learning approach for music classification and regression tasks. We propose to use a pretrained convnet feature, a concatenated feature vector using activations of feature maps of multiple layers in a trained convolutional network. We show that how this convnet feature can serve as a general-purpose music representation. In the experiment, a convnet is trained for music tagging and then transferred for many music-related classification and regression tasks as well as an audio-related classification task. In experiments, the convnet feature outperforms the baseline MFCC feature in all tasks and many reported approaches of aggregating MFCCs and low- and high-level music features.|paper present transfer learn approach music classif regress task propos use pretrain convnet featur concaten featur vector use activ featur map multipl layer train convolut network show convnet featur serv general purpos music represent experi convnet train music tag transfer mani music relat classif regress task well audio relat classif task experi convnet featur outperform baselin mfcc featur task mani report approach aggreg mfccs low high level music featur|['Keunwoo Choi', 'Gy√∂rgy Fazekas', 'Mark Sandler', 'Kyunghyun Cho']|['cs.CV', 'cs.AI', 'cs.MM', 'cs.SD']
2017-03-28T14:05:16Z|2017-03-27T06:19:38Z|http://arxiv.org/abs/1703.08944v1|http://arxiv.org/abs/1703.08944v1|Intelligent bidirectional rapidly-exploring random trees for optimal   motion planning in complex cluttered environments|intellig bidirect rapid explor random tree optim motion plan complex clutter environ|The sampling based motion planning algorithm known as Rapidly-exploring Random Trees (RRT) has gained the attention of many researchers due to their computational efficiency and effectiveness. Recently, a variant of RRT called RRT* has been proposed that ensures asymptotic optimality. Subsequently its bidirectional version has also been introduced in the literature known as Bidirectional-RRT* (B-RRT*). We introduce a new variant called Intelligent Bidirectional-RRT* (IB-RRT*) which is an improved variant of the optimal RRT* and bidirectional version of RRT* (B-RRT*) algorithms and is specially designed for complex cluttered environments. IB-RRT* utilizes the bidirectional trees approach and introduces intelligent sample insertion heuristic for fast convergence to the optimal path solution using uniform sampling heuristics. The proposed algorithm is evaluated theoretically and experimental results are presented that compares IB-RRT* with RRT* and B-RRT*. Moreover, experimental results demonstrate the superior efficiency of IB-RRT* in comparison with RRT* and B-RRT in complex cluttered environments.|sampl base motion plan algorithm known rapid explor random tree rrt gain attent mani research due comput effici effect recent variant rrt call rrt propos ensur asymptot optim subsequ bidirect version also introduc literatur known bidirect rrt rrt introduc new variant call intellig bidirect rrt ib rrt improv variant optim rrt bidirect version rrt rrt algorithm special design complex clutter environ ib rrt util bidirect tree approach introduc intellig sampl insert heurist fast converg optim path solut use uniform sampl heurist propos algorithm evalu theoret experiment result present compar ib rrt rrt rrt moreov experiment result demonstr superior effici ib rrt comparison rrt rrt complex clutter environ|['Ahmed Hussain Qureshi', 'Yasar Ayaz']|['cs.RO', 'cs.AI']
2017-03-28T14:05:16Z|2017-03-27T04:03:56Z|http://arxiv.org/abs/1703.08922v1|http://arxiv.org/pdf/1703.08922v1|On Automating the Doctrine of Double Effect|autom doctrin doubl effect|The doctrine of double effect ($\mathcal{DDE}$) is a long-studied ethical principle that governs when actions that have both positive and negative effects are to be allowed. The goal in this paper is to automate $\mathcal{DDE}$. We briefly present $\mathcal{DDE}$, and use a first-order modal logic, the deontic cognitive event calculus, as our framework to formalize the doctrine. We present formalizations of increasingly stronger versions of the principle, including what is known as the doctrine of triple effect. We then use our framework to simulate successfully scenarios that have been used to test for the presence of the principle in human subjects. Our framework can be used in two different modes: One can use it to build $\mathcal{DDE}$-compliant autonomous systems from scratch, or one can use it to verify that a given AI system is $\mathcal{DDE}$-compliant, by applying a $\mathcal{DDE}$ layer on an existing system or model. For the latter mode, the underlying AI system can be built using any architecture (planners, deep neural networks, bayesian networks, knowledge-representation systems, or a hybrid); as long as the system exposes a few parameters in its model, such verification is possible. The role of the $\mathcal{DDE}$ layer here is akin to a (dynamic or static) software verifier that examines existing software modules. Finally, we end by presenting initial work on how one can apply our $\mathcal{DDE}$ layer to the STRIPS-style planning model, and to a modified POMDP model.|doctrin doubl effect mathcal dde long studi ethic principl govern action posit negat effect allow goal paper autom mathcal dde briefli present mathcal dde use first order modal logic deontic cognit event calculus framework formal doctrin present formal increas stronger version principl includ known doctrin tripl effect use framework simul success scenario use test presenc principl human subject framework use two differ mode one use build mathcal dde compliant autonom system scratch one use verifi given ai system mathcal dde compliant appli mathcal dde layer exist system model latter mode ai system built use ani architectur planner deep neural network bayesian network knowledg represent system hybrid long system expos paramet model verif possibl role mathcal dde layer akin dynam static softwar verifi examin exist softwar modul final end present initi work one appli mathcal dde layer strip style plan model modifi pomdp model|['Naveen Sundar Govindarajulu', 'Selmer Bringsjord']|['cs.AI', 'cs.LO', 'cs.RO']
2017-03-28T14:05:16Z|2017-03-26T19:39:50Z|http://arxiv.org/abs/1703.08862v1|http://arxiv.org/pdf/1703.08862v1|Socially Aware Motion Planning with Deep Reinforcement Learning|social awar motion plan deep reinforc learn|For robotic vehicles to navigate safely and efficiently in pedestrian-rich environments, it is important to model subtle human behaviors and navigation rules. However, while instinctive to humans, socially compliant navigation is still difficult to quantify due to the stochasticity in people's behaviors. Existing works are mostly focused on using feature-matching techniques to describe and imitate human paths, but often do not generalize well since the feature values can vary from person to person, and even run to run. This work notes that while it is challenging to directly specify the details of what to do (precise mechanisms of human navigation), it is straightforward to specify what not to do (violations of social norms). Specifically, using deep reinforcement learning, this work develops a time-efficient navigation policy that respects common social norms. The proposed method is shown to enable fully autonomous navigation of a robotic vehicle moving at human walking speed in an environment with many pedestrians.|robot vehicl navig safe effici pedestrian rich environ import model subtl human behavior navig rule howev instinct human social compliant navig still difficult quantifi due stochast peopl behavior exist work focus use featur match techniqu describ imit human path often general well sinc featur valu vari person person even run run work note challeng direct specifi detail precis mechan human navig straightforward specifi violat social norm specif use deep reinforc learn work develop time effici navig polici respect common social norm propos method shown enabl fulli autonom navig robot vehicl move human walk speed environ mani pedestrian|['Yu Fan Chen', 'Michael Everett', 'Miao Liu', 'Jonathan P. How']|['cs.RO', 'cs.AI', 'cs.HC']
2017-03-28T14:05:16Z|2017-03-26T16:20:36Z|http://arxiv.org/abs/1703.08840v1|http://arxiv.org/pdf/1703.08840v1|Inferring The Latent Structure of Human Decision-Making from Raw Visual   Inputs|infer latent structur human decis make raw visual input|The goal of imitation learning is to match example expert behavior, without access to a reinforcement signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are not explicitly modeled. We introduce an extension to the Generative Adversarial Imitation Learning method that can infer the latent structure of human decision-making in an unsupervised way. Our method can not only imitate complex behaviors, but also learn interpretable and meaningful representations. We demonstrate that the approach is applicable to high-dimensional environments including raw visual inputs. In the highway driving domain, we show that a model learned from demonstrations is able to both produce different styles of human-like driving behaviors and accurately anticipate human actions. Our method surpasses various baselines in terms of performance and functionality.|goal imit learn match exampl expert behavior without access reinforc signal expert demonstr provid human howev often show signific variabl due latent factor explicit model introduc extens generat adversari imit learn method infer latent structur human decis make unsupervis way method onli imit complex behavior also learn interpret meaning represent demonstr approach applic high dimension environ includ raw visual input highway drive domain show model learn demonstr abl produc differ style human like drive behavior accur anticip human action method surpass various baselin term perform function|['Yunzhu Li', 'Jiaming Song', 'Stefano Ermon']|['cs.LG', 'cs.AI', 'cs.CV']
2017-03-28T14:05:16Z|2017-03-26T15:26:34Z|http://arxiv.org/abs/1703.08825v1|http://arxiv.org/pdf/1703.08825v1|Surrogate Model of Multi-Period Flexibility from a Home Energy   Management System|surrog model multi period flexibl home energi manag system|Near-future electric distribution grids operation will have to rely on demand-side flexibility, both by implementation of demand response strategies and by taking advantage of the intelligent management of increasingly common small-scale energy storage. Home energy management systems (HEMS) will play a crucial role on the flexibility provision to both system operators and market players like aggregators. Modeling multi-period flexibility from residential consumers (HEMS flexibility), such as battery storage and electric water heater, while complying with internal constraints (comfort levels, data privacy) and uncertainty is a complex task. This paper describes a computational method that is capable of efficiently define and learn the feasible flexibility set from controllable resources connected to a HEMS. An Evolutionary Particle Swarm Optimization (EPSO) algorithm is adopted and reshaped to derive a set of feasible temporal trajectories for the residential net-load, considering storage, flexible appliances, and predefined costumer preferences, as well as load and photovoltaic (PV) forecast uncertainty. A support vector data description (SVDD) algorithm is used to build models capable of classifying feasible and unfeasible HEMS operating trajectories upon request from an optimization/control algorithm operated by a DSO or market player.|near futur electr distribut grid oper reli demand side flexibl implement demand respons strategi take advantag intellig manag increas common small scale energi storag home energi manag system hem play crucial role flexibl provis system oper market player like aggreg model multi period flexibl residenti consum hem flexibl batteri storag electr water heater compli intern constraint comfort level data privaci uncertainti complex task paper describ comput method capabl effici defin learn feasibl flexibl set control resourc connect hem evolutionari particl swarm optim epso algorithm adopt reshap deriv set feasibl tempor trajectori residenti net load consid storag flexibl applianc predefin costum prefer well load photovolta pv forecast uncertainti support vector data descript svdd algorithm use build model capabl classifi feasibl unfeas hem oper trajectori upon request optim control algorithm oper dso market player|['Rui Pinto', 'Ricardo Bessa', 'Manuel Matos']|['cs.NE', 'cs.AI']
2017-03-28T14:05:16Z|2017-03-26T05:44:56Z|http://arxiv.org/abs/1703.08769v1|http://arxiv.org/pdf/1703.08769v1|Open Vocabulary Scene Parsing|open vocabulari scene pars|Recognizing arbitrary objects in the wild has been a challenging problem due to the limitations of existing classification models and datasets. In this paper, we propose a new task that aims at parsing scene with a large and open vocabulary, and several evaluation metrics are explored for this problem. Our proposed approach to this problem is a joint image pixel and word concept embeddings framework, where word concepts are connected by semantic relations. We validate the open vocabulary prediction ability of our framework on ADE20K dataset which covers a wide variety of scenes and objects. We further explore the trained joint embedding space to show its interpretability.|recogn arbitrari object wild challeng problem due limit exist classif model dataset paper propos new task aim pars scene larg open vocabulari sever evalu metric explor problem propos approach problem joint imag pixel word concept embed framework word concept connect semant relat valid open vocabulari predict abil framework adek dataset cover wide varieti scene object explor train joint embed space show interpret|['Hang Zhao', 'Xavier Puig', 'Bolei Zhou', 'Sanja Fidler', 'Antonio Torralba']|['cs.CV', 'cs.AI']
2017-03-28T14:05:16Z|2017-03-26T03:47:54Z|http://arxiv.org/abs/1703.08762v1|http://arxiv.org/pdf/1703.08762v1|Team Formation for Scheduling Educational Material in Massive Online   Classes|team format schedul educ materi massiv onlin class|Whether teaching in a classroom or a Massive Online Open Course it is crucial to present the material in a way that benefits the audience as a whole. We identify two important tasks to solve towards this objective, 1 group students so that they can maximally benefit from peer interaction and 2 find an optimal schedule of the educational material for each group. Thus, in this paper, we solve the problem of team formation and content scheduling for education. Given a time frame d, a set of students S with their required need to learn different activities T and given k as the number of desired groups, we study the problem of finding k group of students. The goal is to teach students within time frame d such that their potential for learning is maximized and find the best schedule for each group. We show this problem to be NP-hard and develop a polynomial algorithm for it. We show our algorithm to be effective both on synthetic as well as a real data set. For our experiments, we use real data on students' grades in a Computer Science department. As part of our contribution, we release a semi-synthetic dataset that mimics the properties of the real data.|whether teach classroom massiv onlin open cours crucial present materi way benefit audienc whole identifi two import task solv toward object group student maxim benefit peer interact find optim schedul educ materi group thus paper solv problem team format content schedul educ given time frame set student requir need learn differ activ given number desir group studi problem find group student goal teach student within time frame potenti learn maxim find best schedul group show problem np hard develop polynomi algorithm show algorithm effect synthet well real data set experi use real data student grade comput scienc depart part contribut releas semi synthet dataset mimic properti real data|['Sanaz Bahargam', 'D√≥ra Erdos', 'Azer Bestavros', 'Evimaria Terzi']|['cs.AI']
2017-03-28T14:05:16Z|2017-03-25T15:37:09Z|http://arxiv.org/abs/1703.08705v1|http://arxiv.org/pdf/1703.08705v1|Comparing Rule-Based and Deep Learning Models for Patient Phenotyping|compar rule base deep learn model patient phenotyp|Objective: We investigate whether deep learning techniques for natural language processing (NLP) can be used efficiently for patient phenotyping. Patient phenotyping is a classification task for determining whether a patient has a medical condition, and is a crucial part of secondary analysis of healthcare data. We assess the performance of deep learning algorithms and compare them with classical NLP approaches.   Materials and Methods: We compare convolutional neural networks (CNNs), n-gram models, and approaches based on cTAKES that extract pre-defined medical concepts from clinical notes and use them to predict patient phenotypes. The performance is tested on 10 different phenotyping tasks using 1,610 discharge summaries extracted from the MIMIC-III database.   Results: CNNs outperform other phenotyping algorithms in all 10 tasks. The average F1-score of our model is 76 (PPV of 83, and sensitivity of 71) with our model having an F1-score up to 37 points higher than alternative approaches. We additionally assess the interpretability of our model by presenting a method that extracts the most salient phrases for a particular prediction.   Conclusion: We show that NLP methods based on deep learning improve the performance of patient phenotyping. Our CNN-based algorithm automatically learns the phrases associated with each patient phenotype. As such, it reduces the annotation complexity for clinical domain experts, who are normally required to develop task-specific annotation rules and identify relevant phrases. Our method performs well in terms of both performance and interpretability, which indicates that deep learning is an effective approach to patient phenotyping based on clinicians' notes.|object investig whether deep learn techniqu natur languag process nlp use effici patient phenotyp patient phenotyp classif task determin whether patient medic condit crucial part secondari analysi healthcar data assess perform deep learn algorithm compar classic nlp approach materi method compar convolut neural network cnns gram model approach base ctake extract pre defin medic concept clinic note use predict patient phenotyp perform test differ phenotyp task use discharg summari extract mimic iii databas result cnns outperform phenotyp algorithm task averag score model ppv sensit model score point higher altern approach addit assess interpret model present method extract salient phrase particular predict conclus show nlp method base deep learn improv perform patient phenotyp cnn base algorithm automat learn phrase associ patient phenotyp reduc annot complex clinic domain expert normal requir develop task specif annot rule identifi relev phrase method perform well term perform interpret indic deep learn effect approach patient phenotyp base clinician note|['Sebastian Gehrmann', 'Franck Dernoncourt', 'Yeran Li', 'Eric T. Carlson', 'Joy T. Wu', 'Jonathan Welt', 'John Foote Jr.', 'Edward T. Moseley', 'David W. Grant', 'Patrick D. Tyler', 'Leo Anthony Celi']|['cs.CL', 'cs.AI', 'cs.NE', 'stat.ML']
2017-03-28T14:05:16Z|2017-03-24T15:43:39Z|http://arxiv.org/abs/1703.08475v1|http://arxiv.org/pdf/1703.08475v1|Overcoming Catastrophic Forgetting by Incremental Moment Matching|overcom catastroph forget increment moment match|Catastrophic forgetting is a problem which refers to losing the information of the first task after training from the second task in continual learning of neural networks. To resolve this problem, we propose the incremental moment matching (IMM), which uses the Bayesian neural network framework. IMM assumes that the posterior distribution of parameters of neural networks is approximated with Gaussian distribution and incrementally matches the moment of the posteriors, which are trained for the first and second task, respectively. To make our Gaussian assumption reasonable, the IMM procedure utilizes various transfer learning techniques including weight transfer, L2-norm of old and new parameters, and a newly proposed variant of dropout using old parameters. We analyze our methods on the MNIST and CIFAR-10 datasets, and then evaluate them on a real-world life-log dataset collected using Google Glass. Experimental results show that IMM produces state-of-the-art performance in a variety of datasets.|catastroph forget problem refer lose inform first task train second task continu learn neural network resolv problem propos increment moment match imm use bayesian neural network framework imm assum posterior distribut paramet neural network approxim gaussian distribut increment match moment posterior train first second task respect make gaussian assumpt reason imm procedur util various transfer learn techniqu includ weight transfer norm old new paramet newli propos variant dropout use old paramet analyz method mnist cifar dataset evalu real world life log dataset collect use googl glass experiment result show imm produc state art perform varieti dataset|['Sang-Woo Lee', 'Jin-Hwa Kim', 'Jung-Woo Ha', 'Byoung-Tak Zhang']|['cs.LG', 'cs.AI']
2017-03-28T14:05:20Z|2017-03-24T14:40:31Z|http://arxiv.org/abs/1703.08428v1|http://arxiv.org/abs/1703.08428v1|Calendar.help: Designing a Workflow-Based Scheduling Agent with Humans   in the Loop|calendar help design workflow base schedul agent human loop|Although information workers may complain about meetings, they are an essential part of their work life. Consequently, busy people spend a significant amount of time scheduling meetings. We present Calendar.help, a system that provides fast, efficient scheduling through structured workflows. Users interact with the system via email, delegating their scheduling needs to the system as if it were a human personal assistant. Common scheduling scenarios are broken down using well-defined workflows and completed as a series of microtasks that are automated when possible and executed by a human otherwise. Unusual scenarios fall back to a trained human assistant who executes them as unstructured macrotasks. We describe the iterative approach we used to develop Calendar.help, and share the lessons learned from scheduling thousands of meetings during a year of real-world deployments. Our findings provide insight into how complex information tasks can be broken down into repeatable components that can be executed efficiently to improve productivity.|although inform worker may complain meet essenti part work life consequ busi peopl spend signific amount time schedul meet present calendar help system provid fast effici schedul structur workflow user interact system via email deleg schedul need system human person assist common schedul scenario broken use well defin workflow complet seri microtask autom possibl execut human otherwis unusu scenario fall back train human assist execut unstructur macrotask describ iter approach use develop calendar help share lesson learn schedul thousand meet dure year real world deploy find provid insight complex inform task broken repeat compon execut effici improv product|['Justin Cranshaw', 'Emad Elwany', 'Todd Newman', 'Rafal Kocielnik', 'Bowen Yu', 'Sandeep Soni', 'Jaime Teevan', 'Andr√©s Monroy-Hern√°ndez']|['cs.HC', 'cs.AI', 'cs.CL']
2017-03-28T14:05:20Z|2017-03-24T13:00:52Z|http://arxiv.org/abs/1703.08397v1|http://arxiv.org/pdf/1703.08397v1|Reasoning by Cases in Structured Argumentation|reason case structur argument|We extend the $ASPIC^+$ framework for structured argumentation so as to allow applications of the reasoning by cases inference scheme for defeasible arguments. Given an argument with conclusion `$A$ or $B$', an argument based on $A$ with conclusion $C$, and an argument based on $B$ with conclusion $C$, we allow the construction of an argument with conclusion $C$. We show how our framework leads to different results than other approaches in non-monotonic logic for dealing with disjunctive information, such as disjunctive default theory or approaches based on the OR-rule (which allows to derive a defeasible rule `If ($A$ or $B$) then $C$', given two defeasible rules `If $A$ then $C$' and `If $B$ then $C$'). We raise new questions regarding the subtleties of reasoning defeasibly with disjunctive information, and show that its formalization is more intricate than one would presume.|extend aspic framework structur argument allow applic reason case infer scheme defeas argument given argument conclus argument base conclus argument base conclus allow construct argument conclus show framework lead differ result approach non monoton logic deal disjunct inform disjunct default theori approach base rule allow deriv defeas rule given two defeas rule rais new question regard subtleti reason defeas disjunct inform show formal intric one would presum|['Mathieu Beirlaen', 'Jesse Heyninck', 'Christian Stra√üer']|['cs.AI', '68T27', 'I.2.3; I.2.4']
2017-03-28T14:05:20Z|2017-03-24T12:07:34Z|http://arxiv.org/abs/1703.08383v1|http://arxiv.org/pdf/1703.08383v1|Smart Augmentation - Learning an Optimal Data Augmentation Strategy|smart augment learn optim data augment strategi|A recurring problem faced when training neural networks is that there is typically not enough data to maximize the generalization capability of deep neural networks(DNN). There are many techniques to address this, including data augmentation, dropout, and transfer learning. In this paper, we introduce an additional method which we call Smart Augmentation and we show how to use it to increase the accuracy and reduce overfitting on a target network. Smart Augmentation works by creating a network that learns how to generate augmented data during the training process of a target network in a way that reduces that networks loss. This allows us to learn augmentations that minimize the error of that network.   Smart Augmentation has shown the potential to increase accuracy by demonstrably significant measures on all datasets tested. In addition, it has shown potential to achieve similar or improved performance levels with significantly smaller network sizes in a number of tested cases.|recur problem face train neural network typic enough data maxim general capabl deep neural network dnn mani techniqu address includ data augment dropout transfer learn paper introduc addit method call smart augment show use increas accuraci reduc overfit target network smart augment work creat network learn generat augment data dure train process target network way reduc network loss allow us learn augment minim error network smart augment shown potenti increas accuraci demonstr signific measur dataset test addit shown potenti achiev similar improv perform level signific smaller network size number test case|['Joseph Lemley', 'Shabab Bazrafkan', 'Peter Corcoran']|['cs.AI', 'cs.LG', 'stat.ML']
2017-03-28T14:05:20Z|2017-03-24T01:59:11Z|http://arxiv.org/abs/1703.08262v1|http://arxiv.org/pdf/1703.08262v1|Supervisor Synthesis of POMDP based on Automata Learning|supervisor synthesi pomdp base automata learn|As a general and thus popular model for autonomous systems, partially observable Markov decision process (POMDP) can capture uncertainties from different sources like sensing noises, actuation errors, and uncertain environments. However, its comprehensiveness makes the planning and control in POMDP difficult. Traditional POMDP planning problems target to find the optimal policy to maximize the expectation of accumulated rewards. But for safety critical applications, guarantees of system performance described by formal specifications are desired, which motivates us to consider formal methods to synthesize supervisor for POMDP. With system specifications given by Probabilistic Computation Tree Logic (PCTL), we propose a supervisory control framework with a type of deterministic finite automata (DFA), za-DFA, as the controller form. While the existing work mainly relies on optimization techniques to learn fixed-size finite state controllers (FSCs), we develop an $L^*$ learning based algorithm to determine both space and transitions of za-DFA. Membership queries and different oracles for conjectures are defined. The learning algorithm is sound and complete. An example is given in detailed steps to illustrate the supervisor synthesis algorithm.|general thus popular model autonom system partial observ markov decis process pomdp captur uncertainti differ sourc like sens nois actuat error uncertain environ howev comprehens make plan control pomdp difficult tradit pomdp plan problem target find optim polici maxim expect accumul reward safeti critic applic guarante system perform describ formal specif desir motiv us consid formal method synthes supervisor pomdp system specif given probabilist comput tree logic pctl propos supervisori control framework type determinist finit automata dfa za dfa control form exist work main reli optim techniqu learn fix size finit state control fscs develop learn base algorithm determin space transit za dfa membership queri differ oracl conjectur defin learn algorithm sound complet exampl given detail step illustr supervisor synthesi algorithm|['Xiaobin Zhang', 'Bo Wu', 'Hai Lin']|['cs.SY', 'cs.AI', 'cs.FL']
2017-03-28T14:05:20Z|2017-03-23T17:07:14Z|http://arxiv.org/abs/1703.08144v1|http://arxiv.org/pdf/1703.08144v1|Note Value Recognition for Rhythm Transcription Using a Markov Random   Field Model for Musical Scores and Performances of Piano Music|note valu recognit rhythm transcript use markov random field model music score perform piano music|This paper presents a statistical method for music transcription that can estimate score times of note onsets and offsets from polyphonic MIDI performance signals. Because performed note durations can deviate largely from score-indicated values, previous methods had the problem of not being able to accurately estimate offset score times (or note values) and thus could only output incomplete musical scores. Based on observations that the pitch context and onset score times are influential on the configuration of note values, we construct a context-tree model that provides prior distributions of note values using these features and combine it with a performance model in the framework of Markov random fields. Evaluation results showed that our method reduces the average error rate by around 40 percent compared to existing/simple methods. We also confirmed that, in our model, the score model plays a more important role than the performance model, and it automatically captures the voice structure by unsupervised learning.|paper present statist method music transcript estim score time note onset offset polyphon midi perform signal becaus perform note durat deviat larg score indic valu previous method problem abl accur estim offset score time note valu thus could onli output incomplet music score base observ pitch context onset score time influenti configur note valu construct context tree model provid prior distribut note valu use featur combin perform model framework markov random field evalu result show method reduc averag error rate around percent compar exist simpl method also confirm model score model play import role perform model automat captur voic structur unsupervis learn|['Eita Nakamura', 'Kazuyoshi Yoshii', 'Simon Dixon']|['cs.AI', 'cs.SD']
2017-03-28T14:05:20Z|2017-03-23T15:15:26Z|http://arxiv.org/abs/1703.08098v1|http://arxiv.org/pdf/1703.08098v1|An overview of embedding models of entities and relationships for   knowledge base completion|overview embed model entiti relationship knowledg base complet|Knowledge bases of real-world facts about entities and their relationships are useful resources for a variety of natural language processing tasks. However, because knowledge bases are typically incomplete, it is useful to be able to perform knowledge base completion, i.e., predict whether a relationship not in the knowledge base is likely to be true. This article presents an overview of embedding models of entities and relationships for knowledge base completion, with up-to-date experimental results on two standard evaluation tasks of link prediction (i.e. entity prediction) and triple classification.|knowledg base real world fact entiti relationship use resourc varieti natur languag process task howev becaus knowledg base typic incomplet use abl perform knowledg base complet predict whether relationship knowledg base like true articl present overview embed model entiti relationship knowledg base complet date experiment result two standard evalu task link predict entiti predict tripl classif|['Dat Quoc Nguyen']|['cs.CL', 'cs.AI', 'cs.IR']
2017-03-28T14:05:20Z|2017-03-23T12:32:10Z|http://arxiv.org/abs/1703.08041v1|http://arxiv.org/pdf/1703.08041v1|Resolving the Complexity of Some Fundamental Problems in Computational   Social Choice|resolv complex fundament problem comput social choic|This thesis is in the area called computational social choice which is an intersection area of algorithms and social choice theory.|thesi area call comput social choic intersect area algorithm social choic theori|['Palash Dey']|['cs.DS', 'cs.AI', 'cs.MA']
2017-03-28T14:05:20Z|2017-03-23T10:44:18Z|http://arxiv.org/abs/1703.07994v1|http://arxiv.org/pdf/1703.07994v1|Containment for Rule-Based Ontology-Mediated Queries|contain rule base ontolog mediat queri|Many efforts have been dedicated to identifying restrictions on ontologies expressed as tuple-generating dependencies (tgds), a.k.a. existential rules, that lead to the decidability for the problem of answering ontology-mediated queries (OMQs). This has given rise to three families of formalisms: guarded, non-recursive, and sticky sets of tgds. In this work, we study the containment problem for OMQs expressed in such formalisms, which is a key ingredient for solving static analysis tasks associated with them. Our main contribution is the development of specially tailored techniques for OMQ containment under the classes of tgds stated above. This enables us to obtain sharp complexity bounds for the problems at hand. We also apply our techniques to pinpoint the complexity of problems associated with two emerging applications of OMQ containment: distribution over components and UCQ rewritability of OMQs.|mani effort dedic identifi restrict ontolog express tupl generat depend tgds existenti rule lead decid problem answer ontolog mediat queri omq given rise three famili formal guard non recurs sticki set tgds work studi contain problem omq express formal key ingredi solv static analysi task associ main contribut develop special tailor techniqu omq contain class tgds state abov enabl us obtain sharp complex bound problem hand also appli techniqu pinpoint complex problem associ two emerg applic omq contain distribut compon ucq rewrit omq|['Pablo Barcelo', 'Gerald Berger', 'Andreas Pieris']|['cs.DB', 'cs.AI', 'cs.LO']
2017-03-28T14:05:20Z|2017-03-23T07:13:28Z|http://arxiv.org/abs/1703.07948v1|http://arxiv.org/pdf/1703.07948v1|Fast Stochastic Variance Reduced Gradient Method with Momentum   Acceleration for Machine Learning|fast stochast varianc reduc gradient method momentum acceler machin learn|Recently, research on accelerated stochastic gradient descent methods (e.g., SVRG) has made exciting progress (e.g., linear convergence for strongly convex problems). However, the best-known methods (e.g., Katyusha) requires at least two auxiliary variables and two momentum parameters. In this paper, we propose a fast stochastic variance reduction gradient (FSVRG) method, in which we design a novel update rule with the Nesterov's momentum and incorporate the technique of growing epoch size. FSVRG has only one auxiliary variable and one momentum weight, and thus it is much simpler and has much lower per-iteration complexity. We prove that FSVRG achieves linear convergence for strongly convex problems and the optimal $\mathcal{O}(1/T^2)$ convergence rate for non-strongly convex problems, where $T$ is the number of outer-iterations. We also extend FSVRG to directly solve the problems with non-smooth component functions, such as SVM. Finally, we empirically study the performance of FSVRG for solving various machine learning problems such as logistic regression, ridge regression, Lasso and SVM. Our results show that FSVRG outperforms the state-of-the-art stochastic methods, including Katyusha.|recent research acceler stochast gradient descent method svrg made excit progress linear converg strong convex problem howev best known method katyusha requir least two auxiliari variabl two momentum paramet paper propos fast stochast varianc reduct gradient fsvrg method design novel updat rule nesterov momentum incorpor techniqu grow epoch size fsvrg onli one auxiliari variabl one momentum weight thus much simpler much lower per iter complex prove fsvrg achiev linear converg strong convex problem optim mathcal converg rate non strong convex problem number outer iter also extend fsvrg direct solv problem non smooth compon function svm final empir studi perform fsvrg solv various machin learn problem logist regress ridg regress lasso svm result show fsvrg outperform state art stochast method includ katyusha|['Fanhua Shang', 'Yuanyuan Liu', 'James Cheng', 'Jiacheng Zhuo']|['cs.LG', 'cs.AI', 'math.OC', 'stat.ML']
2017-03-28T14:05:20Z|2017-03-23T05:23:34Z|http://arxiv.org/abs/1703.07940v1|http://arxiv.org/pdf/1703.07940v1|Unsupervised Basis Function Adaptation for Reinforcement Learning|unsupervis basi function adapt reinforc learn|When using reinforcement learning (RL) algorithms to evaluate a policy it is common, given a large state space, to introduce some form of approximation architecture for the value function (VF). The exact form of this architecture can have a significant effect on the accuracy of the VF estimate, however, and determining a suitable approximation architecture can often be a highly complex task. Consequently there is a large amount of interest in the potential for allowing RL algorithms to adaptively generate (i.e. to learn) approximation architectures.   We investigate a method of adapting approximation architectures which uses feedback regarding the frequency with which an agent has visited certain states to guide which areas of the state space to approximate with greater detail. We introduce an algorithm based upon this idea which adapts a state aggregation approximation architecture on-line.   Assuming $S$ states, we demonstrate theoretically that - provided the following relatively non-restrictive assumptions are satisfied: (a) the number of cells $X$ in the state aggregation architecture is of order $\sqrt{S}\ln{S}\log_2{S}$ or greater, (b) the policy and transition function are close to deterministic, and (c) the prior for the transition function is uniformly distributed - our algorithm can guarantee, assuming we use an appropriate scoring function to measure VF error, error which is arbitrarily close to zero as $S$ becomes large. It is able to do this despite having only $O(X\log_2{S})$ space complexity (and negligible time complexity). We conclude by generating a set of empirical results which support the theoretical results.|use reinforc learn rl algorithm evalu polici common given larg state space introduc form approxim architectur valu function vf exact form architectur signific effect accuraci vf estim howev determin suitabl approxim architectur often high complex task consequ larg amount interest potenti allow rl algorithm adapt generat learn approxim architectur investig method adapt approxim architectur use feedback regard frequenc agent visit certain state guid area state space approxim greater detail introduc algorithm base upon idea adapt state aggreg approxim architectur line assum state demonstr theoret provid follow relat non restrict assumpt satisfi number cell state aggreg architectur order sqrt ln log greater polici transit function close determinist prior transit function uniform distribut algorithm guarante assum use appropri score function measur vf error error arbitrarili close zero becom larg abl despit onli log space complex neglig time complex conclud generat set empir result support theoret result|['Edward Barker', 'Charl Ras']|['cs.LG', 'cs.AI', 'stat.ML']
2017-03-28T14:05:24Z|2017-03-23T04:26:46Z|http://arxiv.org/abs/1703.07929v1|http://arxiv.org/pdf/1703.07929v1|Diversification-Based Learning in Computing and Optimization|diversif base learn comput optim|Diversification-Based Learning (DBL) derives from a collection of principles and methods introduced in the field of metaheuristics that have broad applications in computing and optimization. We show that the DBL framework goes significantly beyond that of the more recent Opposition-based learning (OBL) framework introduced in Tizhoosh (2005), which has become the focus of numerous research initiatives in machine learning and metaheuristic optimization. We unify and extend earlier proposals in metaheuristic search (Glover, 1997, Glover and Laguna, 1997) to give a collection of approaches that are more flexible and comprehensive than OBL for creating intensification and diversification strategies in metaheuristic search. We also describe potential applications of DBL to various subfields of machine learning and optimization.|diversif base learn dbl deriv collect principl method introduc field metaheurist broad applic comput optim show dbl framework goe signific beyond recent opposit base learn obl framework introduc tizhoosh becom focus numer research initi machin learn metaheurist optim unifi extend earlier propos metaheurist search glover glover laguna give collect approach flexibl comprehens obl creat intensif diversif strategi metaheurist search also describ potenti applic dbl various subfield machin learn optim|['Fred Glover', 'Jin-Kao Hao']|['cs.AI']
2017-03-28T14:05:24Z|2017-03-23T04:25:48Z|http://arxiv.org/abs/1703.07928v1|http://arxiv.org/pdf/1703.07928v1|Guided Perturbations: Self Corrective Behavior in Convolutional Neural   Networks|guid perturb self correct behavior convolut neural network|Convolutional Neural Networks have been a subject of great importance over the past decade and great strides have been made in their utility for producing state of the art performance in many computer vision problems. However, the behavior of deep networks is yet to be fully understood and is still an active area of research. In this work, we present an intriguing behavior: pre-trained CNNs can be made to improve their predictions by structurally perturbing the input. We observe that these perturbations - referred as Guided Perturbations - enable a trained network to improve its prediction performance without any learning or change in network weights. We perform various ablative experiments to understand how these perturbations affect the local context and feature representations. Furthermore, we demonstrate that this idea can improve performance of several existing approaches on semantic segmentation and scene labeling tasks on the PASCAL VOC dataset and supervised classification tasks on MNIST and CIFAR10 datasets.|convolut neural network subject great import past decad great stride made util produc state art perform mani comput vision problem howev behavior deep network yet fulli understood still activ area research work present intrigu behavior pre train cnns made improv predict structur perturb input observ perturb refer guid perturb enabl train network improv predict perform without ani learn chang network weight perform various ablat experi understand perturb affect local context featur represent furthermor demonstr idea improv perform sever exist approach semant segment scene label task pascal voc dataset supervis classif task mnist cifar dataset|['Swami Sankaranarayanan', 'Arpit Jain', 'Ser Nam Lim']|['cs.CV', 'cs.AI', 'stat.ML']
2017-03-28T14:05:24Z|2017-03-22T19:08:48Z|http://arxiv.org/abs/1703.07822v1|http://arxiv.org/pdf/1703.07822v1|Information-theoretic Model Identification and Policy Search using   Physics Engines with Application to Robotic Manipulation|inform theoret model identif polici search use physic engin applic robot manipul|We consider the problem of a robot learning the mechanical properties of objects through physical interaction with the object, and introduce a practical, data-efficient approach for identifying the motion models of these objects. The proposed method utilizes a physics engine, where the robot seeks to identify the inertial and friction parameters of the object by simulating its motion under different values of the parameters and identifying those that result in a simulation which matches the observed real motions. The problem is solved in a Bayesian optimization framework. The same framework is used for both identifying the model of an object online and searching for a policy that would minimize a given cost function according to the identified model. Experimental results both in simulation and using a real robot indicate that the proposed method outperforms state-of-the-art model-free reinforcement learning approaches.|consid problem robot learn mechan properti object physic interact object introduc practic data effici approach identifi motion model object propos method util physic engin robot seek identifi inerti friction paramet object simul motion differ valu paramet identifi result simul match observ real motion problem solv bayesian optim framework framework use identifi model object onlin search polici would minim given cost function accord identifi model experiment result simul use real robot indic propos method outperform state art model free reinforc learn approach|['Shaojun Zhu', 'Andrew Kimmel', 'Abdeslam Boularias']|['cs.RO', 'cs.AI', 'cs.LG']
2017-03-28T14:05:24Z|2017-03-22T18:20:07Z|http://arxiv.org/abs/1703.07805v1|http://arxiv.org/abs/1703.07805v1|Supervised Typing of Big Graphs using Semantic Embeddings|supervis type big graph use semant embed|We propose a supervised algorithm for generating type embeddings in the same semantic vector space as a given set of entity embeddings. The algorithm is agnostic to the derivation of the underlying entity embeddings. It does not require any manual feature engineering, generalizes well to hundreds of types and achieves near-linear scaling on Big Graphs containing many millions of triples and instances by virtue of an incremental execution. We demonstrate the utility of the embeddings on a type recommendation task, outperforming a non-parametric feature-agnostic baseline while achieving 15x speedup and near-constant memory usage on a full partition of DBpedia. Using state-of-the-art visualization, we illustrate the agreement of our extensionally derived DBpedia type embeddings with the manually curated domain ontology. Finally, we use the embeddings to probabilistically cluster about 4 million DBpedia instances into 415 types in the DBpedia ontology.|propos supervis algorithm generat type embed semant vector space given set entiti embed algorithm agnost deriv entiti embed doe requir ani manual featur engin general well hundr type achiev near linear scale big graph contain mani million tripl instanc virtu increment execut demonstr util embed type recommend task outperform non parametr featur agnost baselin achiev speedup near constant memori usag full partit dbpedia use state art visual illustr agreement extension deriv dbpedia type embed manual curat domain ontolog final use embed probabilist cluster million dbpedia instanc type dbpedia ontolog|['Mayank Kejriwal', 'Pedro Szekely']|['cs.CL', 'cs.AI']
2017-03-28T14:05:24Z|2017-03-22T17:27:57Z|http://arxiv.org/abs/1703.07758v1|http://arxiv.org/pdf/1703.07758v1|S-Concave Distributions: Towards Broader Distributions for   Noise-Tolerant and Sample-Efficient Learning Algorithms|concav distribut toward broader distribut nois toler sampl effici learn algorithm|We provide new results concerning noise-tolerant and sample-efficient learning algorithms under $s$-concave distributions over $\mathbb{R}^n$ for $-\frac{1}{2n+3}\le s\le 0$. The new class of $s$-concave distributions is a broad and natural generalization of log-concavity, and includes many important additional distributions, e.g., the Pareto distribution and $t$-distribution. This class has been studied in the context of efficient sampling, integration, and optimization, but much remains unknown concerning the geometry of this class of distributions and their applications in the context of learning.   The challenge is that unlike the commonly used distributions in learning (uniform or more generally log-concave distributions), this broader class is not closed under the marginalization operator and many such distributions are fat-tailed. In this work, we introduce new convex geometry tools to study the properties of s-concave distributions and use these properties to provide bounds on quantities of interest to learning including the probability of disagreement between two halfspaces, disagreement outside a band, and disagreement coefficient. We use these results to significantly generalize prior results for margin-based active learning, disagreement-based active learning, and passively learning of intersections of halfspaces.   Our analysis of geometric properties of s-concave distributions might be of independent interest to optimization more broadly.|provid new result concern nois toler sampl effici learn algorithm concav distribut mathbb frac le le new class concav distribut broad natur general log concav includ mani import addit distribut pareto distribut distribut class studi context effici sampl integr optim much remain unknown concern geometri class distribut applic context learn challeng unlik common use distribut learn uniform general log concav distribut broader class close margin oper mani distribut fat tail work introduc new convex geometri tool studi properti concav distribut use properti provid bound quantiti interest learn includ probabl disagr two halfspac disagr outsid band disagr coeffici use result signific general prior result margin base activ learn disagr base activ learn passiv learn intersect halfspac analysi geometr properti concav distribut might independ interest optim broad|['Maria-Florina Balcan', 'Hongyang Zhang']|['stat.ML', 'cs.AI', 'cs.LG']
2017-03-28T14:05:24Z|2017-03-24T01:00:03Z|http://arxiv.org/abs/1703.07726v3|http://arxiv.org/pdf/1703.07726v3|\$1 Today or \$2 Tomorrow? The Answer is in Your Facebook Likes|today tomorrow answer facebook like|"In economics and psychology, delay discounting is often used to characterize how individuals choose between a smaller immediate reward and a larger delayed reward. People with higher delay discounting rate (DDR) often choose smaller but more immediate rewards (a ""today person""). In contrast, people with a lower discounting rate often choose a larger future rewards (a ""tomorrow person""). Since the ability to modulate the desire of immediate gratification for long term rewards plays an important role in our decision-making, the lower discounting rate often predicts better social, academic and health outcomes. In contrast, the higher discounting rate is often associated with problematic behaviors such as alcohol/drug abuse, pathological gambling and credit card default. Thus, research on understanding and moderating delay discounting has the potential to produce substantial societal benefits."|econom psycholog delay discount often use character individu choos smaller immedi reward larger delay reward peopl higher delay discount rate ddr often choos smaller immedi reward today person contrast peopl lower discount rate often choos larger futur reward tomorrow person sinc abil modul desir immedi gratif long term reward play import role decis make lower discount rate often predict better social academ health outcom contrast higher discount rate often associ problemat behavior alcohol drug abus patholog gambl credit card default thus research understand moder delay discount potenti produc substanti societ benefit|['Tao Ding', 'Warren K. Bickel', 'Shimei Pan']|['cs.AI', 'cs.CY', 'cs.SI']
2017-03-28T14:05:24Z|2017-03-22T15:34:23Z|http://arxiv.org/abs/1703.07710v1|http://arxiv.org/pdf/1703.07710v1|UBEV - A More Practical Algorithm for Episodic RL with Near-Optimal PAC   and Regret Guarantees|ubev practic algorithm episod rl near optim pac regret guarante|We present UBEV, a simple and efficient reinforcement learning algorithm for fixed-horizon episodic Markov decision processes. The main contribution is a proof that UBEV enjoys a sample-complexity bound that holds for all accuracy levels simultaneously with high probability, and matches the lower bound except for logarithmic terms and one factor of the horizon. A consequence of the fact that our sample-complexity bound holds for all accuracy levels is that the new algorithm achieves a sub-linear regret of O(sqrt(SAT)), which is the first time the dependence on the size of the state space has provably appeared inside the square root. A brief empirical evaluation shows that UBEV is practically superior to existing algorithms with known sample-complexity guarantees.|present ubev simpl effici reinforc learn algorithm fix horizon episod markov decis process main contribut proof ubev enjoy sampl complex bound hold accuraci level simultan high probabl match lower bound except logarithm term one factor horizon consequ fact sampl complex bound hold accuraci level new algorithm achiev sub linear regret sqrt sat first time depend size state space provabl appear insid squar root brief empir evalu show ubev practic superior exist algorithm known sampl complex guarante|['Christoph Dann', 'Tor Lattimore', 'Emma Brunskill']|['cs.LG', 'cs.AI', 'stat.ML']
2017-03-28T14:05:24Z|2017-03-22T11:53:53Z|http://arxiv.org/abs/1703.07608v1|http://arxiv.org/pdf/1703.07608v1|Deep Exploration via Randomized Value Functions|deep explor via random valu function|We study the use of randomized value functions to guide deep exploration in reinforcement learning. This offers an elegant means for synthesizing statistically and computationally efficient exploration with common practical approaches to value function learning. We present several reinforcement learning algorithms that leverage randomized value functions and demonstrate their efficacy through computational studies. We also prove a regret bound that establishes statistical efficiency with a tabular representation.|studi use random valu function guid deep explor reinforc learn offer eleg mean synthes statist comput effici explor common practic approach valu function learn present sever reinforc learn algorithm leverag random valu function demonstr efficaci comput studi also prove regret bound establish statist effici tabular represent|['Ian Osband', 'Daniel Russo', 'Zheng Wen', 'Benjamin Van Roy']|['stat.ML', 'cs.AI', 'cs.LG']
2017-03-28T14:05:24Z|2017-03-21T23:29:47Z|http://arxiv.org/abs/1703.07469v1|http://arxiv.org/pdf/1703.07469v1|RobustFill: Neural Program Learning under Noisy I/O|robustfil neural program learn noisi|The problem of automatically generating a computer program from some specification has been studied since the early days of AI. Recently, two competing approaches for automatic program learning have received significant attention: (1) neural program synthesis, where a neural network is conditioned on input/output (I/O) examples and learns to generate a program, and (2) neural program induction, where a neural network generates new outputs directly using a latent program representation.   Here, for the first time, we directly compare both approaches on a large-scale, real-world learning task. We additionally contrast to rule-based program synthesis, which uses hand-crafted semantics to guide the program generation. Our neural models use a modified attention RNN to allow encoding of variable-sized sets of I/O pairs. Our best synthesis model achieves 92% accuracy on a real-world test set, compared to the 34% accuracy of the previous best neural synthesis approach. The synthesis model also outperforms a comparable induction model on this task, but we more importantly demonstrate that the strength of each approach is highly dependent on the evaluation metric and end-user application. Finally, we show that we can train our neural models to remain very robust to the type of noise expected in real-world data (e.g., typos), while a highly-engineered rule-based system fails entirely.|problem automat generat comput program specif studi sinc earli day ai recent two compet approach automat program learn receiv signific attent neural program synthesi neural network condit input output exampl learn generat program neural program induct neural network generat new output direct use latent program represent first time direct compar approach larg scale real world learn task addit contrast rule base program synthesi use hand craft semant guid program generat neural model use modifi attent rnn allow encod variabl size set pair best synthesi model achiev accuraci real world test set compar accuraci previous best neural synthesi approach synthesi model also outperform compar induct model task import demonstr strength approach high depend evalu metric end user applic final show train neural model remain veri robust type nois expect real world data typo high engin rule base system fail entir|['Jacob Devlin', 'Jonathan Uesato', 'Surya Bhupatiraju', 'Rishabh Singh', 'Abdel-rahman Mohamed', 'Pushmeet Kohli']|['cs.AI']
2017-03-28T14:05:24Z|2017-03-21T19:12:35Z|http://arxiv.org/abs/1703.07394v1|http://arxiv.org/pdf/1703.07394v1|Deep Learning for Explicitly Modeling Optimization Landscapes|deep learn explicit model optim landscap|In all but the most trivial optimization problems, the structure of the solutions exhibit complex interdependencies between the input parameters. Decades of research with stochastic search techniques has shown the benefit of explicitly modeling the interactions between sets of parameters and the overall quality of the solutions discovered. We demonstrate a novel method, based on learning deep networks, to model the global landscapes of optimization problems. To represent the search space concisely and accurately, the deep networks must encode information about the underlying parameter interactions and their contributions to the quality of the solution. Once the networks are trained, the networks are probed to reveal parameter combinations with high expected performance with respect to the optimization task. These estimates are used to initialize fast, randomized, local search algorithms, which in turn expose more information about the search space that is subsequently used to refine the models. We demonstrate the technique on multiple optimization problems that have arisen in a variety of real-world domains, including: packing, graphics, job scheduling, layout and compression. The problems include combinatoric search spaces, discontinuous and highly non-linear spaces, and span binary, higher-cardinality discrete, as well as continuous parameters. Strengths, limitations, and extensions of the approach are extensively discussed and demonstrated.|trivial optim problem structur solut exhibit complex interdepend input paramet decad research stochast search techniqu shown benefit explicit model interact set paramet overal qualiti solut discov demonstr novel method base learn deep network model global landscap optim problem repres search space concis accur deep network must encod inform paramet interact contribut qualiti solut onc network train network probe reveal paramet combin high expect perform respect optim task estim use initi fast random local search algorithm turn expos inform search space subsequ use refin model demonstr techniqu multipl optim problem arisen varieti real world domain includ pack graphic job schedul layout compress problem includ combinator search space discontinu high non linear space span binari higher cardin discret well continu paramet strength limit extens approach extens discuss demonstr|['Shumeet Baluja']|['cs.NE', 'cs.AI', 'cs.LG']
2017-03-28T14:05:28Z|2017-03-21T18:34:34Z|http://arxiv.org/abs/1703.07384v1|http://arxiv.org/pdf/1703.07384v1|Ontology Based Pivoted normalization using Vector Based Approach for   information Retrieval|ontolog base pivot normal use vector base approach inform retriev|The proposed methodology is procedural i.e. it follows finite number of steps that extracts relevant documents according to users query. It is based on principles of Data Mining for analyzing web data. Data Mining first adapts integration of data to generate warehouse. Then, it extracts useful information with the help of algorithm. The task of representing extracted documents is done by using Vector Based Statistical Approach that represents each document in set of Terms.|propos methodolog procedur follow finit number step extract relev document accord user queri base principl data mine analyz web data data mine first adapt integr data generat warehous extract use inform help algorithm task repres extract document done use vector base statist approach repres document set term|['Vishal Jain', 'Dr. Mayank Singh']|['cs.IR', 'cs.AI']
2017-03-28T14:05:28Z|2017-03-21T18:29:05Z|http://arxiv.org/abs/1703.07381v1|http://arxiv.org/pdf/1703.07381v1|Improving Statistical Multimedia Information Retrieval Model by using   Ontology|improv statist multimedia inform retriev model use ontolog|A typical IR system that delivers and stores information is affected by problem of matching between user query and available content on web. Use of Ontology represents the extracted terms in form of network graph consisting of nodes, edges, index terms etc. The above mentioned IR approaches provide relevance thus satisfying users query. The paper also emphasis on analyzing multimedia documents and performs calculation for extracted terms using different statistical formulas. The proposed model developed reduces semantic gap and satisfies user needs efficiently.|typic ir system deliv store inform affect problem match user queri avail content web use ontolog repres extract term form network graph consist node edg index term etc abov mention ir approach provid relev thus satisfi user queri paper also emphasi analyz multimedia document perform calcul extract term use differ statist formula propos model develop reduc semant gap satisfi user need effici|['Gagandeep Singh Narula', 'Vishal Jain']|['cs.IR', 'cs.AI']
2017-03-28T14:05:28Z|2017-03-22T00:24:03Z|http://arxiv.org/abs/1703.07326v2|http://arxiv.org/pdf/1703.07326v2|One-Shot Imitation Learning|one shot imit learn|Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning.   Specifically, we consider the setting where there is a very large set of tasks, and each task has many instantiations. For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states. At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks. A neural net is trained that takes as input one demonstration and the current state (which initially is the initial state of the other demonstration of the pair), and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. At test time, a demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. The use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks.   Videos available at https://bit.ly/one-shot-imitation .|imit learn common appli solv differ task isol usual requir either care featur engin signific number sampl far desir ideal robot abl learn veri demonstr ani given task instant general new situat task without requir task specif engin paper propos meta learn framework achiev capabl call one shot imit learn specif consid set veri larg set task task mani instanti exampl task could stack block tabl singl tower anoth task could place block tabl two block tower etc case differ instanc task would consist differ set block differ initi state train time algorithm present pair demonstr subset task neural net train take input one demonstr current state initi initi state demonstr pair output action goal result sequenc state action match close possibl second demonstr test time demonstr singl instanc new task present neural net expect perform well new instanc new task use soft attent allow model general condit task unseen train data anticip train model much greater varieti task set obtain general system turn ani demonstr robust polici accomplish overwhelm varieti task video avail https bit ly one shot imit|['Yan Duan', 'Marcin Andrychowicz', 'Bradly C. Stadie', 'Jonathan Ho', 'Jonas Schneider', 'Ilya Sutskever', 'Pieter Abbeel', 'Wojciech Zaremba']|['cs.AI', 'cs.LG', 'cs.NE', 'cs.RO']
2017-03-28T14:05:28Z|2017-03-22T17:08:40Z|http://arxiv.org/abs/1703.07255v2|http://arxiv.org/pdf/1703.07255v2|ZM-Net: Real-time Zero-shot Image Manipulation Network|zm net real time zero shot imag manipul network|Many problems in image processing and computer vision (e.g. colorization, style transfer) can be posed as 'manipulating' an input image into a corresponding output image given a user-specified guiding signal. A holy-grail solution towards generic image manipulation should be able to efficiently alter an input image with any personalized signals (even signals unseen during training), such as diverse paintings and arbitrary descriptive attributes. However, existing methods are either inefficient to simultaneously process multiple signals (let alone generalize to unseen signals), or unable to handle signals from other modalities. In this paper, we make the first attempt to address the zero-shot image manipulation task. We cast this problem as manipulating an input image according to a parametric model whose key parameters can be conditionally generated from any guiding signal (even unseen ones). To this end, we propose the Zero-shot Manipulation Net (ZM-Net), a fully-differentiable architecture that jointly optimizes an image-transformation network (TNet) and a parameter network (PNet). The PNet learns to generate key transformation parameters for the TNet given any guiding signal while the TNet performs fast zero-shot image manipulation according to both signal-dependent parameters from the PNet and signal-invariant parameters from the TNet itself. Extensive experiments show that our ZM-Net can perform high-quality image manipulation conditioned on different forms of guiding signals (e.g. style images and attributes) in real-time (tens of milliseconds per image) even for unseen signals. Moreover, a large-scale style dataset with over 20,000 style images is also constructed to promote further research.|mani problem imag process comput vision color style transfer pose manipul input imag correspond output imag given user specifi guid signal holi grail solut toward generic imag manipul abl effici alter input imag ani person signal even signal unseen dure train divers paint arbitrari descript attribut howev exist method either ineffici simultan process multipl signal let alon general unseen signal unabl handl signal modal paper make first attempt address zero shot imag manipul task cast problem manipul input imag accord parametr model whose key paramet condit generat ani guid signal even unseen one end propos zero shot manipul net zm net fulli differenti architectur joint optim imag transform network tnet paramet network pnet pnet learn generat key transform paramet tnet given ani guid signal tnet perform fast zero shot imag manipul accord signal depend paramet pnet signal invari paramet tnet extens experi show zm net perform high qualiti imag manipul condit differ form guid signal style imag attribut real time ten millisecond per imag even unseen signal moreov larg scale style dataset style imag also construct promot research|['Hao Wang', 'Xiaodan Liang', 'Hao Zhang', 'Dit-Yan Yeung', 'Eric P. Xing']|['cs.CV', 'cs.AI', 'cs.GR', 'cs.LG', 'stat.ML']
2017-03-28T14:05:28Z|2017-03-21T09:57:59Z|http://arxiv.org/abs/1703.07116v1|http://arxiv.org/pdf/1703.07116v1|Interest-Driven Discovery of Local Process Models|interest driven discoveri local process model|Local Process Models (LPM) describe structured fragments of process behavior occurring in the context of less structured business processes. Traditional LPM discovery aims to generate a collection of process models that describe highly frequent behavior, but these models do not always provide useful answers for questions posed by process analysts aiming at business process improvement. We propose a framework for goal-driven LPM discovery, based on utility functions and constraints. We describe four scopes on which these utility functions and constrains can be defined, and show that utility functions and constraints on different scopes can be combined to form composite utility functions/constraints. Finally, we demonstrate the applicability of our approach by presenting several actionable business insights discovered with LPM discovery on two real life data sets.|local process model lpm describ structur fragment process behavior occur context less structur busi process tradit lpm discoveri aim generat collect process model describ high frequent behavior model alway provid use answer question pose process analyst aim busi process improv propos framework goal driven lpm discoveri base util function constraint describ four scope util function constrain defin show util function constraint differ scope combin form composit util function constraint final demonstr applic approach present sever action busi insight discov lpm discoveri two real life data set|['Niek Tax', 'Benjamin Dalmas', 'Natalia Sidorova', 'Wil M P van der Aalst', 'Sylvie Norre']|['cs.DB', 'cs.AI']
2017-03-28T14:05:28Z|2017-03-21T07:09:27Z|http://arxiv.org/abs/1703.07075v1|http://arxiv.org/pdf/1703.07075v1|Pseudorehearsal in value function approximation|pseudorehears valu function approxim|Catastrophic forgetting is of special importance in reinforcement learning, as the data distribution is generally non-stationary over time. We study and compare several pseudorehearsal approaches for Q-learning with function approximation in a pole balancing task. We have found that pseudorehearsal seems to assist learning even in such very simple problems, given proper initialization of the rehearsal parameters.|catastroph forget special import reinforc learn data distribut general non stationari time studi compar sever pseudorehears approach learn function approxim pole balanc task found pseudorehears seem assist learn even veri simpl problem given proper initi rehears paramet|['Vladimir Marochko', 'Leonard Johard', 'Manuel Mazzara']|['cs.AI']
2017-03-28T14:05:28Z|2017-03-21T04:56:14Z|http://arxiv.org/abs/1703.07055v1|http://arxiv.org/pdf/1703.07055v1|Investigation of Language Understanding Impact for Reinforcement   Learning Based Dialogue Systems|investig languag understand impact reinforc learn base dialogu system|Language understanding is a key component in a spoken dialogue system. In this paper, we investigate how the language understanding module influences the dialogue system performance by conducting a series of systematic experiments on a task-oriented neural dialogue system in a reinforcement learning based setting. The empirical study shows that among different types of language understanding errors, slot-level errors can have more impact on the overall performance of a dialogue system compared to intent-level errors. In addition, our experiments demonstrate that the reinforcement learning based dialogue system is able to learn when and what to confirm in order to achieve better performance and greater robustness.|languag understand key compon spoken dialogu system paper investig languag understand modul influenc dialogu system perform conduct seri systemat experi task orient neural dialogu system reinforc learn base set empir studi show among differ type languag understand error slot level error impact overal perform dialogu system compar intent level error addit experi demonstr reinforc learn base dialogu system abl learn confirm order achiev better perform greater robust|['Xiujun Li', 'Yun-Nung Chen', 'Lihong Li', 'Jianfeng Gao', 'Asli Celikyilmaz']|['cs.CL', 'cs.AI', 'cs.LG']
2017-03-28T14:05:28Z|2017-03-23T20:06:15Z|http://arxiv.org/abs/1703.07022v2|http://arxiv.org/pdf/1703.07022v2|Recurrent Topic-Transition GAN for Visual Paragraph Generation|recurr topic transit gan visual paragraph generat|A natural image usually conveys rich semantic content and can be viewed from different angles. Existing image description methods are largely restricted by small sets of biased visual paragraph annotations, and fail to cover rich underlying semantics. In this paper, we investigate a semi-supervised paragraph generative framework that is able to synthesize diverse and semantically coherent paragraph descriptions by reasoning over local semantic regions and exploiting linguistic knowledge. The proposed Recurrent Topic-Transition Generative Adversarial Network (RTT-GAN) builds an adversarial framework between a structured paragraph generator and multi-level paragraph discriminators. The paragraph generator generates sentences recurrently by incorporating region-based visual and language attention mechanisms at each step. The quality of generated paragraph sentences is assessed by multi-level adversarial discriminators from two aspects, namely, plausibility at sentence level and topic-transition coherence at paragraph level. The joint adversarial training of RTT-GAN drives the model to generate realistic paragraphs with smooth logical transition between sentence topics. Extensive quantitative experiments on image and video paragraph datasets demonstrate the effectiveness of our RTT-GAN in both supervised and semi-supervised settings. Qualitative results on telling diverse stories for an image also verify the interpretability of RTT-GAN.|natur imag usual convey rich semant content view differ angl exist imag descript method larg restrict small set bias visual paragraph annot fail cover rich semant paper investig semi supervis paragraph generat framework abl synthes divers semant coher paragraph descript reason local semant region exploit linguist knowledg propos recurr topic transit generat adversari network rtt gan build adversari framework structur paragraph generat multi level paragraph discrimin paragraph generat generat sentenc recurr incorpor region base visual languag attent mechan step qualiti generat paragraph sentenc assess multi level adversari discrimin two aspect name plausibl sentenc level topic transit coher paragraph level joint adversari train rtt gan drive model generat realist paragraph smooth logic transit sentenc topic extens quantit experi imag video paragraph dataset demonstr effect rtt gan supervis semi supervis set qualit result tell divers stori imag also verifi interpret rtt gan|['Xiaodan Liang', 'Zhiting Hu', 'Hao Zhang', 'Chuang Gan', 'Eric P. Xing']|['cs.CV', 'cs.AI', 'cs.LG']
2017-03-28T14:05:28Z|2017-03-20T19:32:40Z|http://arxiv.org/abs/1703.06939v1|http://arxiv.org/pdf/1703.06939v1|Distributed Constraint Problems for Utilitarian Agents with Privacy   Concerns, Recast as POMDPs|distribut constraint problem utilitarian agent privaci concern recast pomdp|Privacy has traditionally been a major motivation for distributed problem solving. Distributed Constraint Satisfaction Problem (DisCSP) as well as Distributed Constraint Optimization Problem (DCOP) are fundamental models used to solve various families of distributed problems. Even though several approaches have been proposed to quantify and preserve privacy in such problems, none of them is exempt from limitations. Here we approach the problem by assuming that computation is performed among utilitarian agents. We introduce a utilitarian approach where the utility of each state is estimated as the difference between the reward for reaching an agreement on assignments of shared variables and the cost of privacy loss. We investigate extensions to solvers where agents integrate the utility function to guide their search and decide which action to perform, defining thereby their policy. We show that these extended solvers succeed in significantly reducing privacy loss without significant degradation of the solution quality.|privaci tradit major motiv distribut problem solv distribut constraint satisfact problem discsp well distribut constraint optim problem dcop fundament model use solv various famili distribut problem even though sever approach propos quantifi preserv privaci problem none exempt limit approach problem assum comput perform among utilitarian agent introduc utilitarian approach util state estim differ reward reach agreement assign share variabl cost privaci loss investig extens solver agent integr util function guid search decid action perform defin therebi polici show extend solver succeed signific reduc privaci loss without signific degrad solut qualiti|['Julien Savaux', 'Julien Vion', 'Sylvain Piechowiak', 'Ren√© Mandiau', 'Toshihiro Matsui', 'Katsutoshi Hirayama', 'Makoto Yokoo', 'Shakre Elmane', 'Marius Silaghi']|['cs.AI']
2017-03-28T14:05:28Z|2017-03-20T19:17:14Z|http://arxiv.org/abs/1703.06931v1|http://arxiv.org/pdf/1703.06931v1|Learning Correspondence Structures for Person Re-identification|learn correspond structur person identif|This paper addresses the problem of handling spatial misalignments due to camera-view changes or human-pose variations in person re-identification. We first introduce a boosting-based approach to learn a correspondence structure which indicates the patch-wise matching probabilities between images from a target camera pair. The learned correspondence structure can not only capture the spatial correspondence pattern between cameras but also handle the viewpoint or human-pose variation in individual images. We further introduce a global constraint-based matching process. It integrates a global matching constraint over the learned correspondence structure to exclude cross-view misalignments during the image patch matching process, hence achieving a more reliable matching score between images. Finally, we also extend our approach by introducing a multi-structure scheme, which learns a set of local correspondence structures to capture the spatial correspondence sub-patterns between a camera pair, so as to handle the spatial misalignments between individual images in a more precise way. Experimental results on various datasets demonstrate the effectiveness of our approach.|paper address problem handl spatial misalign due camera view chang human pose variat person identif first introduc boost base approach learn correspond structur indic patch wise match probabl imag target camera pair learn correspond structur onli captur spatial correspond pattern camera also handl viewpoint human pose variat individu imag introduc global constraint base match process integr global match constraint learn correspond structur exclud cross view misalign dure imag patch match process henc achiev reliabl match score imag final also extend approach introduc multi structur scheme learn set local correspond structur captur spatial correspond sub pattern camera pair handl spatial misalign individu imag precis way experiment result various dataset demonstr effect approach|['Weiyao Lin', 'Yang Shen', 'Junchi Yan', 'Mingliang Xu', 'Jianxin Wu', 'Jingdong Wang', 'Ke Lu']|['cs.CV', 'cs.AI', 'cs.MM']
2017-03-28T14:05:32Z|2017-03-20T16:03:36Z|http://arxiv.org/abs/1703.06815v1|http://arxiv.org/pdf/1703.06815v1|Foundations for a Probabilistic Event Calculus|foundat probabilist event calculus|We present PEC, an Event Calculus (EC) style action language for reasoning about probabilistic causal and narrative information. It has an action language style syntax similar to that of the EC variant Modular-E. Its semantics is given in terms of possible worlds which constitute possible evolutions of the domain, and builds on that of EFEC, an epistemic extension of EC. We also describe an ASP implementation of PEC and show the sense in which this is sound and complete.|present pec event calculus ec style action languag reason probabilist causal narrat inform action languag style syntax similar ec variant modular semant given term possibl world constitut possibl evolut domain build efec epistem extens ec also describ asp implement pec show sens sound complet|"[""Fabio Aurelio D'Asaro"", 'Antonis Bikakis', 'Luke Dickens', 'Rob Miller']"|['cs.AI']
2017-03-28T14:05:32Z|2017-03-20T11:44:00Z|http://arxiv.org/abs/1703.06692v1|http://arxiv.org/pdf/1703.06692v1|QMDP-Net: Deep Learning for Planning under Partial Observability|qmdp net deep learn plan partial observ|This paper introduces QMDP-net, a neural network architecture for planning under partial observability. The QMDP-net combines the strengths of model-free learning and model-based planning. It is a recurrent policy network, but it represents a policy by connecting a model with a planning algorithm that solves the model, thus embedding the solution structure of planning in the network architecture. The QMDP-net is fully differentiable and allows end-to-end training. We train a QMDP-net over a set of different environments so that it can generalize over new ones. In preliminary experiments, QMDP-net showed strong performance on several robotic tasks in simulation. Interestingly, it also sometimes outperformed the QMDP algorithm, which generated the data for learning, because of QMDP-net's robustness resulting from end-to-end learning.|paper introduc qmdp net neural network architectur plan partial observ qmdp net combin strength model free learn model base plan recurr polici network repres polici connect model plan algorithm solv model thus embed solut structur plan network architectur qmdp net fulli differenti allow end end train train qmdp net set differ environ general new one preliminari experi qmdp net show strong perform sever robot task simul interest also sometim outperform qmdp algorithm generat data learn becaus qmdp net robust result end end learn|['Peter Karkus', 'David Hsu', 'Wee Sun Lee']|['cs.AI', 'cs.LG', 'cs.NE', 'stat.ML']
2017-03-28T14:05:32Z|2017-03-20T09:28:38Z|http://arxiv.org/abs/1703.06642v1|http://arxiv.org/pdf/1703.06642v1|Towards a Quantum World Wide Web|toward quantum world wide web|We elaborate a quantum model for corpora of written documents, like the pages forming the World Wide Web. To that end, we are guided by how physicists constructed quantum theory for microscopic entities, which unlike classical objects cannot be fully represented in our spatial theater. We suggest that a similar construction needs to be carried out by linguists and computational scientists, to capture the full meaning content of collections of documental entities. More precisely, we show how to associate a quantum-like 'entity of meaning' to a 'language entity formed by printed documents', considering the latter as the collection of traces that are left by the former, in specific results of search actions that we describe as measurements. In other words, we offer a perspective where a collection of documents, like the Web, is described as the space of manifestation of a more complex entity - the QWeb - which is the object of our modeling, drawing its inspiration from previous studies on operational-realistic approaches to quantum physics and quantum modeling of human cognition and decision-making. We emphasize that a consistent QWeb model needs to account for the observed correlations between words appearing in printed documents, e.g., co-occurrences, as the latter would depend on the 'meaning connections' existing between the concepts that are associated with these words. In that respect, we show that both 'context and interference (quantum) effects' are required to explain the probabilities calculated by counting the relative number of documents containing certain words and co-ocurrrences of words.|elabor quantum model corpora written document like page form world wide web end guid physicist construct quantum theori microscop entiti unlik classic object cannot fulli repres spatial theater suggest similar construct need carri linguist comput scientist captur full mean content collect document entiti precis show associ quantum like entiti mean languag entiti form print document consid latter collect trace left former specif result search action describ measur word offer perspect collect document like web describ space manifest complex entiti qweb object model draw inspir previous studi oper realist approach quantum physic quantum model human cognit decis make emphas consist qweb model need account observ correl word appear print document co occurr latter would depend mean connect exist concept associ word respect show context interfer quantum effect requir explain probabl calcul count relat number document contain certain word co ocurrr word|['Diederik Aerts', 'Jonito Aerts Arguelles', 'Lester Beltran', 'Lyneth Beltran', 'Isaac Distrito', 'Massimiliano Sassoli de Bianchi', 'Sandro Sozzo', 'Tomas Veloz']|['cs.AI', 'cs.CL', 'quant-ph']
2017-03-28T14:05:32Z|2017-03-20T04:47:14Z|http://arxiv.org/abs/1703.06597v1|http://arxiv.org/pdf/1703.06597v1|Artificial Intelligence and Economic Theories|artifici intellig econom theori|The advent of artificial intelligence has changed many disciplines such as engineering, social science and economics. Artificial intelligence is a computational technique which is inspired by natural intelligence such as the swarming of birds, the working of the brain and the pathfinding of the ants. These techniques have impact on economic theories. This book studies the impact of artificial intelligence on economic theories, a subject that has not been extensively studied. The theories that are considered are: demand and supply, asymmetrical information, pricing, rational choice, rational expectation, game theory, efficient market hypotheses, mechanism design, prospect, bounded rationality, portfolio theory, rational counterfactual and causality. The benefit of this book is that it evaluates existing theories of economics and update them based on the developments in artificial intelligence field.|advent artifici intellig chang mani disciplin engin social scienc econom artifici intellig comput techniqu inspir natur intellig swarm bird work brain pathfind ant techniqu impact econom theori book studi impact artifici intellig econom theori subject extens studi theori consid demand suppli asymmetr inform price ration choic ration expect game theori effici market hypothes mechan design prospect bound ration portfolio theori ration counterfactu causal benefit book evalu exist theori econom updat base develop artifici intellig field|['Tshilidzi Marwala', 'Evan Hurwitz']|['cs.AI']
2017-03-28T14:05:32Z|2017-03-21T17:41:23Z|http://arxiv.org/abs/1703.06585v2|http://arxiv.org/pdf/1703.06585v2|Learning Cooperative Visual Dialog Agents with Deep Reinforcement   Learning|learn cooper visual dialog agent deep reinforc learn|We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative 'image guessing' game between two agents -- Qbot and Abot -- who communicate in natural language dialog so that Qbot can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end -- from pixels to multi-agent multi-round dialog to game reward.   We demonstrate two experimental results.   First, as a 'sanity check' demonstration of pure RL (from scratch), we show results on a synthetic world, where the agents communicate in ungrounded vocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes (shape/color/style). Thus, we demonstrate the emergence of grounded language and communication among 'visual' dialog agents with no human supervision.   Second, we conduct large-scale real-image experiments on the VisDial dataset, where we pretrain with supervised dialog data and show that the RL 'fine-tuned' agents significantly outperform SL agents. Interestingly, the RL Qbot learns to ask questions that Abot is good at, ultimately resulting in more informative dialog and a better team.|introduc first goal driven train visual question answer dialog agent specif pose cooper imag guess game two agent qbot abot communic natur languag dialog qbot select unseen imag lineup imag use deep reinforc learn rl learn polici agent end end pixel multi agent multi round dialog game reward demonstr two experiment result first saniti check demonstr pure rl scratch show result synthet world agent communic unground vocabulari symbol pre specifi mean find two bot invent communic protocol start use certain symbol ask answer certain visual attribut shape color style thus demonstr emerg ground languag communic among visual dialog agent human supervis second conduct larg scale real imag experi visdial dataset pretrain supervis dialog data show rl fine tune agent signific outperform sl agent interest rl qbot learn ask question abot good ultim result inform dialog better team|['Abhishek Das', 'Satwik Kottur', 'Jos√© M. F. Moura', 'Stefan Lee', 'Dhruv Batra']|['cs.CV', 'cs.AI', 'cs.CL', 'cs.LG']
2017-03-28T14:05:32Z|2017-03-20T02:29:53Z|http://arxiv.org/abs/1703.06565v1|http://arxiv.org/pdf/1703.06565v1|Evidence Updating for Stream-Processing in Big-Data: Robust Conditioning   in Soft and Hard Fusion Environments|evid updat stream process big data robust condit soft hard fusion environ|Conditioning is the primary method for belief revision in data fusion systems employing probabilistic inferencing. However, big-data environments, where soft (i.e., human or human-based) sources are commonly utilized in addition to hard (i.e., physics-based sensors, pose several challenges to traditional conditioning tasks primarily due to the numerous data/source imperfections that are characteristic of such data. The objective of this paper is to investigate the most natural extension of Bayes conditioning based evidence updates in the presence of such large-scale data uncertainties and source/sensor imperfections. By viewing the evidence updating process as a thought experiment, we devise an elegant strategy for robust evidence updating in the presence of extreme uncertainties characteristic of big-data environments. In particular, we look at the formulation of a belief theoretic evidence updating mechanism that is derived as a natural extension of Bayes conditional approach when the incoming evidence takes the form of a general belief function. Proposed method generalizes the belief theoretic Fagin-Halpern conditional notion, and provides a novel evidence updating strategy that is derived as a natural extension of Bayes conditional applied in a highly uncertain and complex fusion scenario that is characteristic of big-data environments. The presented extension differs fundamentally from the previously published work on Conditional Update Equation (CUE) as well as authors own work. An overview of this development is provided via illustrative examples. Furthermore, insights into parameter selection under various fusion contexts are also provided.|condit primari method belief revis data fusion system employ probabilist inferenc howev big data environ soft human human base sourc common util addit hard physic base sensor pose sever challeng tradit condit task primarili due numer data sourc imperfect characterist data object paper investig natur extens bay condit base evid updat presenc larg scale data uncertainti sourc sensor imperfect view evid updat process thought experi devis eleg strategi robust evid updat presenc extrem uncertainti characterist big data environ particular look formul belief theoret evid updat mechan deriv natur extens bay condit approach incom evid take form general belief function propos method general belief theoret fagin halpern condit notion provid novel evid updat strategi deriv natur extens bay condit appli high uncertain complex fusion scenario characterist big data environ present extens differ fundament previous publish work condit updat equat cue well author work overview develop provid via illustr exampl furthermor insight paramet select various fusion context also provid|['Thanuka Wickramarathne']|['cs.AI']
2017-03-28T14:05:32Z|2017-03-19T17:31:13Z|http://arxiv.org/abs/1703.06471v1|http://arxiv.org/pdf/1703.06471v1|Multi-Timescale, Gradient Descent, Temporal Difference Learning with   Linear Options|multi timescal gradient descent tempor differ learn linear option|Deliberating on large or continuous state spaces have been long standing challenges in reinforcement learning. Temporal Abstraction have somewhat made this possible, but efficiently planing using temporal abstraction still remains an issue. Moreover using spatial abstractions to learn policies for various situations at once while using temporal abstraction models is an open problem. We propose here an efficient algorithm which is convergent under linear function approximation while planning using temporally abstract actions. We show how this algorithm can be used along with randomly generated option models over multiple time scales to plan agents which need to act real time. Using these randomly generated option models over multiple time scales are shown to reduce number of decision epochs required to solve the given task, hence effectively reducing the time needed for deliberation.|deliber larg continu state space long stand challeng reinforc learn tempor abstract somewhat made possibl effici plane use tempor abstract still remain issu moreov use spatial abstract learn polici various situat onc use tempor abstract model open problem propos effici algorithm converg linear function approxim plan use tempor abstract action show algorithm use along random generat option model multipl time scale plan agent need act real time use random generat option model multipl time scale shown reduc number decis epoch requir solv given task henc effect reduc time need deliber|['Peeyush Kumar', 'Doina Precup']|['cs.AI']
2017-03-28T14:05:32Z|2017-03-19T15:21:32Z|http://arxiv.org/abs/1703.06452v1|http://arxiv.org/pdf/1703.06452v1|Deep Neural Networks for Semantic Segmentation of Multispectral Remote   Sensing Imagery|deep neural network semant segment multispectr remot sens imageri|A semantic segmentation algorithm must assign a label to every pixel in an image. Recently, semantic segmentation of RGB imagery has advanced significantly due to deep learning. Because creating datasets for semantic segmentation is laborious, these datasets tend to be significantly smaller than object recognition datasets. This makes it difficult to directly train a deep neural network for semantic segmentation, because it will be prone to overfitting. To cope with this, deep learning models typically use convolutional neural networks pre-trained on large-scale image classification datasets, which are then fine-tuned for semantic segmentation. For non-RGB imagery, this is currently not possible because large-scale labeled non-RGB datasets do not exist. In this paper, we developed two deep neural networks for semantic segmentation of multispectral remote sensing imagery. Prior to training on the target dataset, we initialize the networks with large amounts of synthetic multispectral imagery. We show that this significantly improves results on real-world remote sensing imagery, and we establish a new state-of-the-art result on the challenging Hamlin Beach State Park Dataset.|semant segment algorithm must assign label everi pixel imag recent semant segment rgb imageri advanc signific due deep learn becaus creat dataset semant segment labori dataset tend signific smaller object recognit dataset make difficult direct train deep neural network semant segment becaus prone overfit cope deep learn model typic use convolut neural network pre train larg scale imag classif dataset fine tune semant segment non rgb imageri current possibl becaus larg scale label non rgb dataset exist paper develop two deep neural network semant segment multispectr remot sens imageri prior train target dataset initi network larg amount synthet multispectr imageri show signific improv result real world remot sens imageri establish new state art result challeng hamlin beach state park dataset|['Ronald Kemker', 'Christopher Kanan']|['cs.CV', 'cs.AI']
2017-03-28T14:05:32Z|2017-03-18T21:25:29Z|http://arxiv.org/abs/1703.06354v1|http://arxiv.org/pdf/1703.06354v1|Goal Conflict in Designing an Autonomous Artificial System|goal conflict design autonom artifici system|Research on human self-regulation has shown that people hold many goals simultaneously and have complex self-regulation mechanisms to deal with this goal conflict. Artificial autonomous systems may also need to find ways to cope with conflicting goals. Indeed, the intricate interplay among different goals may be critical to the design as well as long-term safety and stability of artificial autonomous systems. I discuss some of the critical features of the human self-regulation system and how it might be applied to an artificial system. Furthermore, the implications of goal conflict for the reliability and stability of artificial autonomous systems and ensuring their alignment with human goals and ethics is examined.|research human self regul shown peopl hold mani goal simultan complex self regul mechan deal goal conflict artifici autonom system may also need find way cope conflict goal inde intric interplay among differ goal may critic design well long term safeti stabil artifici autonom system discuss critic featur human self regul system might appli artifici system furthermor implic goal conflict reliabl stabil artifici autonom system ensur align human goal ethic examin|['Mark Muraven']|['cs.AI']
2017-03-28T14:05:32Z|2017-03-21T08:11:19Z|http://arxiv.org/abs/1703.06321v2|http://arxiv.org/pdf/1703.06321v2|Solving the Goddard problem by an influence diagram|solv goddard problem influenc diagram|Influence diagrams are a decision-theoretic extension of probabilistic graphical models. In this paper we show how they can be used to solve the Goddard problem. We present results of numerical experiments with this problem and compare the solutions provided by influence diagrams with the optimal solution.|influenc diagram decis theoret extens probabilist graphic model paper show use solv goddard problem present result numer experi problem compar solut provid influenc diagram optim solut|['Ji≈ô√≠ Vomlel', 'V√°clav Kratochv√≠l']|['cs.AI', '68T37', 'I.2']
2017-03-28T14:05:37Z|2017-03-18T10:52:53Z|http://arxiv.org/abs/1703.06283v1|http://arxiv.org/pdf/1703.06283v1|Recognition in-the-Tail: Training Detectors for Unusual Pedestrians with   Synthetic Imposters|recognit tail train detector unusu pedestrian synthet impost|"As autonomous vehicles become an every-day reality, high-accuracy pedestrian detection is of paramount practical importance. Pedestrian detection is a highly researched topic with mature methods, but most datasets focus on common scenes of people engaged in typical walking poses on sidewalks. But performance is most crucial for dangerous scenarios, such as children playing in the street or people using bicycles/skateboards in unexpected ways. Such ""in-the-tail"" data is notoriously hard to observe, making both training and testing difficult. To analyze this problem, we have collected a novel annotated dataset of dangerous scenarios called the Precarious Pedestrian dataset. Even given a dedicated collection effort, it is relatively small by contemporary standards (around 1000 images). To explore large-scale data-driven learning, we explore the use of synthetic data generated by a game engine. A significant challenge is selected the right ""priors"" or parameters for synthesis: we would like realistic data with realistic poses and object configurations. Inspired by Generative Adversarial Networks, we generate a massive amount of synthetic data and train a discriminative classifier to select a realistic subset, which we deem Synthetic Imposters. We demonstrate that this pipeline allows one to generate realistic training data by making use of rendering/animation engines. Interestingly, we also demonstrate that such data can be used to rank algorithms, suggesting that Synthetic Imposters can also be used for ""in-the-tail"" validation at test-time, a notoriously difficult challenge for real-world deployment."|autonom vehicl becom everi day realiti high accuraci pedestrian detect paramount practic import pedestrian detect high research topic matur method dataset focus common scene peopl engag typic walk pose sidewalk perform crucial danger scenario children play street peopl use bicycl skateboard unexpect way tail data notori hard observ make train test difficult analyz problem collect novel annot dataset danger scenario call precari pedestrian dataset even given dedic collect effort relat small contemporari standard around imag explor larg scale data driven learn explor use synthet data generat game engin signific challeng select right prior paramet synthesi would like realist data realist pose object configur inspir generat adversari network generat massiv amount synthet data train discrimin classifi select realist subset deem synthet impost demonstr pipelin allow one generat realist train data make use render anim engin interest also demonstr data use rank algorithm suggest synthet impost also use tail valid test time notori difficult challeng real world deploy|['Shiyu Huang', 'Deva Ramanan']|['cs.CV', 'cs.AI']
2017-03-28T14:05:37Z|2017-03-18T09:04:05Z|http://arxiv.org/abs/1703.06275v1|http://arxiv.org/pdf/1703.06275v1|Evolving Game Skill-Depth using General Video Game AI Agents|evolv game skill depth use general video game ai agent|Most games have, or can be generalised to have, a number of parameters that may be varied in order to provide instances of games that lead to very different player experiences. The space of possible parameter settings can be seen as a search space, and we can therefore use a Random Mutation Hill Climbing algorithm or other search methods to find the parameter settings that induce the best games. One of the hardest parts of this approach is defining a suitable fitness function. In this paper we explore the possibility of using one of a growing set of General Video Game AI agents to perform automatic play-testing. This enables a very general approach to game evaluation based on estimating the skill-depth of a game. Agent-based play-testing is computationally expensive, so we compare two simple but efficient optimisation algorithms: the Random Mutation Hill-Climber and the Multi-Armed Bandit Random Mutation Hill-Climber. For the test game we use a space-battle game in order to provide a suitable balance between simulation speed and potential skill-depth. Results show that both algorithms are able to rapidly evolve game versions with significant skill-depth, but that choosing a suitable resampling number is essential in order to combat the effects of noise.|game generalis number paramet may vari order provid instanc game lead veri differ player experi space possibl paramet set seen search space therefor use random mutat hill climb algorithm search method find paramet set induc best game one hardest part approach defin suitabl fit function paper explor possibl use one grow set general video game ai agent perform automat play test enabl veri general approach game evalu base estim skill depth game agent base play test comput expens compar two simpl effici optimis algorithm random mutat hill climber multi arm bandit random mutat hill climber test game use space battl game order provid suitabl balanc simul speed potenti skill depth result show algorithm abl rapid evolv game version signific skill depth choos suitabl resampl number essenti order combat effect nois|['Jialin Liu', 'Julian Togelius', 'Diego Perez-Liebana', 'Simon M. Lucas']|['cs.AI']
2017-03-28T14:05:37Z|2017-03-21T14:26:33Z|http://arxiv.org/abs/1703.06207v2|http://arxiv.org/pdf/1703.06207v2|Cooperating with Machines|cooper machin|Since Alan Turing envisioned Artificial Intelligence (AI) [1], a major driving force behind technical progress has been competition with human cognition. Historical milestones have been frequently associated with computers matching or outperforming humans in difficult cognitive tasks (e.g. face recognition [2], personality classification [3], driving cars [4], or playing video games [5]), or defeating humans in strategic zero-sum encounters (e.g. Chess [6], Checkers [7], Jeopardy! [8], Poker [9], or Go [10]). In contrast, less attention has been given to developing autonomous machines that establish mutually cooperative relationships with people who may not share the machine's preferences. A main challenge has been that human cooperation does not require sheer computational power, but rather relies on intuition [11], cultural norms [12], emotions and signals [13, 14, 15, 16], and pre-evolved dispositions toward cooperation [17], common-sense mechanisms that are difficult to encode in machines for arbitrary contexts. Here, we combine a state-of-the-art machine-learning algorithm with novel mechanisms for generating and acting on signals to produce a new learning algorithm that cooperates with people and other machines at levels that rival human cooperation in a variety of two-player repeated stochastic games. This is the first general-purpose algorithm that is capable, given a description of a previously unseen game environment, of learning to cooperate with people within short timescales in scenarios previously unanticipated by algorithm designers. This is achieved without complex opponent modeling or higher-order theories of mind, thus showing that flexible, fast, and general human-machine cooperation is computationally achievable using a non-trivial, but ultimately simple, set of algorithmic mechanisms.|sinc alan ture envis artifici intellig ai major drive forc behind technic progress competit human cognit histor mileston frequent associ comput match outperform human difficult cognit task face recognit person classif drive car play video game defeat human strateg zero sum encount chess checker jeopardi poker go contrast less attent given develop autonom machin establish mutual cooper relationship peopl may share machin prefer main challeng human cooper doe requir sheer comput power rather reli intuit cultur norm emot signal pre evolv disposit toward cooper common sens mechan difficult encod machin arbitrari context combin state art machin learn algorithm novel mechan generat act signal produc new learn algorithm cooper peopl machin level rival human cooper varieti two player repeat stochast game first general purpos algorithm capabl given descript previous unseen game environ learn cooper peopl within short timescal scenario previous unanticip algorithm design achiev without complex oppon model higher order theori mind thus show flexibl fast general human machin cooper comput achiev use non trivial ultim simpl set algorithm mechan|['Jacob W. Crandall', 'Mayada Oudah', 'Tennom', 'Fatimah Ishowo-Oloko', 'Sherief Abdallah', 'Jean-Fran√ßois Bonnefon', 'Manuel Cebrian', 'Azim Shariff', 'Michael A. Goodrich', 'Iyad Rahwan']|['cs.AI']
2017-03-28T14:05:37Z|2017-03-25T15:54:36Z|http://arxiv.org/abs/1703.06182v2|http://arxiv.org/pdf/1703.06182v2|Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under   Partial Observability|deep decentr multi task multi agent reinforc learn partial observ|Many real-world tasks involve multiple agents with partial observability and limited communication. Learning is challenging in these settings due to local viewpoints of agents, which perceive the world as non-stationary due to concurrently-exploring teammates. Approaches that learn specialized policies for individual tasks face major problems when applied to the real world: not only do agents have to learn and store a distinct policy for each task, but in practice the identity of the task is often non-observable, making these algorithms inapplicable. This paper formalizes and addresses the problem of multi-task multi-agent reinforcement learning under partial observability. We introduce a decentralized single-task learning approach that is robust to concurrent interactions of teammates, and present an approach for distilling single-task policies into a unified policy that performs well across multiple related tasks, without explicit provision of task identity.|mani real world task involv multipl agent partial observ limit communic learn challeng set due local viewpoint agent perceiv world non stationari due concurr explor teammat approach learn special polici individu task face major problem appli real world onli agent learn store distinct polici task practic ident task often non observ make algorithm inapplic paper formal address problem multi task multi agent reinforc learn partial observ introduc decentr singl task learn approach robust concurr interact teammat present approach distil singl task polici unifi polici perform well across multipl relat task without explicit provis task ident|['Shayegan Omidshafiei', 'Jason Pazis', 'Christopher Amato', 'Jonathan P. How', 'John Vian']|['cs.LG', 'cs.AI']
2017-03-28T14:05:37Z|2017-03-17T17:09:14Z|http://arxiv.org/abs/1703.06103v1|http://arxiv.org/pdf/1703.06103v1|Modeling Relational Data with Graph Convolutional Networks|model relat data graph convolut network|Knowledge bases play a crucial role in many applications, for example question answering and information retrieval. Despite the great effort invested in creating and maintaining them, even the largest representatives (e.g., Yago, DBPedia or Wikidata) are highly incomplete. We introduce relational graph convolutional networks (R-GCNs) and apply them to two standard knowledge base completion tasks: link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing attributes of entities). R-GCNs are a generalization of graph convolutional networks, a recent class of neural networks operating on graphs, and are developed specifically to deal with highly multi-relational data, characteristic of realistic knowledge bases. Our methods achieve competitive results on standard benchmarks for both tasks.|knowledg base play crucial role mani applic exampl question answer inform retriev despit great effort invest creat maintain even largest repres yago dbpedia wikidata high incomplet introduc relat graph convolut network gcns appli two standard knowledg base complet task link predict recoveri miss fact subject predic object tripl entiti classif recoveri miss attribut entiti gcns general graph convolut network recent class neural network oper graph develop specif deal high multi relat data characterist realist knowledg base method achiev competit result standard benchmark task|['Michael Schlichtkrull', 'Thomas N. Kipf', 'Peter Bloem', 'Rianne van den Berg', 'Ivan Titov', 'Max Welling']|['stat.ML', 'cs.AI', 'cs.DB', 'cs.LG']
2017-03-28T14:05:37Z|2017-03-17T15:00:03Z|http://arxiv.org/abs/1703.06045v1|http://arxiv.org/pdf/1703.06045v1|Approximation Complexity of Maximum A Posteriori Inference in   Sum-Product Networks|approxim complex maximum posteriori infer sum product network|We discuss the computational complexity of approximating maximum a posteriori inference in sum-product networks. We first show NP-hardness in three-level trees by a reduction from maximum independent set; this implies non-approximability within a sublinear factor. We show that this is a tight bound, as we can find an approximation within a linear factor in three-level networks. We then show that in four-level trees it is NP-hard to approximate the problem within a factor $2^{f(n)}$ for any sublinear function $f$ of the size of the input $n$. Again, this is bound is tight, as we prove that the usual max-product algorithm finds (in any network) approximations within factor $2^{c n}$ from some constant $c < 1$. Last, we present a simple algorithm, and show that it provably produces solutions at least as good as, and potentially much better than, the max-product algorithm.|discuss comput complex approxim maximum posteriori infer sum product network first show np hard three level tree reduct maximum independ set impli non approxim within sublinear factor show tight bound find approxim within linear factor three level network show four level tree np hard approxim problem within factor ani sublinear function size input bound tight prove usual max product algorithm find ani network approxim within factor constant last present simpl algorithm show provabl produc solut least good potenti much better max product algorithm|['Denis Deratani Mau√°', 'Cassio P. de Campos']|['cs.AI', '68T37']
2017-03-28T14:05:37Z|2017-03-16T21:08:31Z|http://arxiv.org/abs/1703.05820v1|http://arxiv.org/pdf/1703.05820v1|Particle Value Functions|particl valu function|The policy gradients of the expected return objective can react slowly to rare rewards. Yet, in some cases agents may wish to emphasize the low or high returns regardless of their probability. Borrowing from the economics and control literature, we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example. This risk-sensitive value function is not always applicable to reinforcement learning problems, so we introduce the particle value function defined by a particle filter over the distributions of an agent's experience, which bounds the risk-sensitive one. We illustrate the benefit of the policy gradients of this objective in Cliffworld.|polici gradient expect return object react slowli rare reward yet case agent may wish emphas low high return regardless probabl borrow econom control literatur review risk sensit valu function aris exponenti util illustr effect exampl risk sensit valu function alway applic reinforc learn problem introduc particl valu function defin particl filter distribut agent experi bound risk sensit one illustr benefit polici gradient object cliffworld|['Chris J. Maddison', 'Dieterich Lawson', 'George Tucker', 'Nicolas Heess', 'Arnaud Doucet', 'Andriy Mnih', 'Yee Whye Teh']|['cs.LG', 'cs.AI']
2017-03-28T14:05:37Z|2017-03-16T13:36:41Z|http://arxiv.org/abs/1703.05614v1|http://arxiv.org/pdf/1703.05614v1|ParaGraphE: A Library for Parallel Knowledge Graph Embedding|paragraph librari parallel knowledg graph embed|Knowledge graph embedding aims at translating the knowledge graph into numerical representations by transforming the entities and relations into con- tinuous low-dimensional vectors. Recently, many methods [1, 5, 3, 2, 6] have been proposed to deal with this problem, but existing single-thread implemen- tations of them are time-consuming for large-scale knowledge graphs. Here, we design a unified parallel framework to parallelize these methods, which achieves a significant time reduction without in uencing the accuracy. We name our framework as ParaGraphE, which provides a library for parallel knowledge graph embedding. The source code can be downloaded from https: //github.com/LIBBLE/LIBBLE-MultiThread/tree/master/ParaGraphE.|knowledg graph embed aim translat knowledg graph numer represent transform entiti relat con tinuous low dimension vector recent mani method propos deal problem exist singl thread implemen tation time consum larg scale knowledg graph design unifi parallel framework parallel method achiev signific time reduct without uenc accuraci name framework paragraph provid librari parallel knowledg graph embed sourc code download https github com libbl libbl multithread tree master paragraph|['Xiao-Fan Niu', 'Wu-Jun Li']|['cs.AI']
2017-03-28T14:05:37Z|2017-03-16T13:07:54Z|http://arxiv.org/abs/1703.06109v1|http://arxiv.org/pdf/1703.06109v1|Generalised Reichenbachian Common Cause Systems|generalis reichenbachian common caus system|The principle of the common cause claims that if an improbable coincidence has occurred, there must exist a common cause. This is generally taken to mean that positive correlations between non-causally related events should disappear when conditioning on the action of some underlying common cause. The extended interpretation of the principle, by contrast, urges that common causes should be called for in order to explain positive deviations between the estimated correlation of two events and the expected value of their correlation. The aim of this paper is to provide the extended reading of the principle with a general probabilistic model, capturing the simultaneous action of a system of multiple common causes. To this end, two distinct models are elaborated, and the necessary and sufficient conditions for their existence are determined.|principl common caus claim improb coincid occur must exist common caus general taken mean posit correl non causal relat event disappear condit action common caus extend interpret principl contrast urg common caus call order explain posit deviat estim correl two event expect valu correl aim paper provid extend read principl general probabilist model captur simultan action system multipl common caus end two distinct model elabor necessari suffici condit exist determin|['Claudio Mazzola']|['stat.OT', 'cs.AI']
2017-03-28T14:05:37Z|2017-03-16T12:53:52Z|http://arxiv.org/abs/1703.06042v1|http://arxiv.org/pdf/1703.06042v1|A Visual Web Tool to Perform What-If Analysis of Optimization Approaches|visual web tool perform analysi optim approach|In Operation Research, practical evaluation is essential to validate the efficacy of optimization approaches. This paper promotes the usage of performance profiles as a standard practice to visualize and analyze experimental results. It introduces a Web tool to construct and export performance profiles as SVG or HTML files. In addition, the application relies on a methodology to estimate the benefit of hypothetical solver improvements. Therefore, the tool allows one to employ what-if analysis to screen possible research directions, and identify those having the best potential. The approach is showcased on two Operation Research technologies: Constraint Programming and Mixed Integer Linear Programming.|oper research practic evalu essenti valid efficaci optim approach paper promot usag perform profil standard practic visual analyz experiment result introduc web tool construct export perform profil svg html file addit applic reli methodolog estim benefit hypothet solver improv therefor tool allow one employ analysi screen possibl research direct identifi best potenti approach showcas two oper research technolog constraint program mix integ linear program|['Sascha Van Cauwelaert', 'Michele Lombardi', 'Pierre Schaus']|['cs.AI', 'cs.PF']
2017-03-28T14:05:41Z|2017-03-16T03:36:28Z|http://arxiv.org/abs/1703.05468v1|http://arxiv.org/abs/1703.05468v1|Database Learning: Toward a Database that Becomes Smarter Every Time|databas learn toward databas becom smarter everi time|In today's databases, previous query answers rarely benefit answering future queries. For the first time, to the best of our knowledge, we change this paradigm in an approximate query processing (AQP) context. We make the following observation: the answer to each query reveals some degree of knowledge about the answer to another query because their answers stem from the same underlying distribution that has produced the entire dataset. Exploiting and refining this knowledge should allow us to answer queries more analytically, rather than by reading enormous amounts of raw data. Also, processing more queries should continuously enhance our knowledge of the underlying distribution, and hence lead to increasingly faster response times for future queries.   We call this novel idea---learning from past query answers---Database Learning. We exploit the principle of maximum entropy to produce answers, which are in expectation guaranteed to be more accurate than existing sample-based approximations. Empowered by this idea, we build a query engine on top of Spark SQL, called Verdict. We conduct extensive experiments on real-world query traces from a large customer of a major database vendor. Our results demonstrate that database learning supports 73.7% of these queries, speeding them up by up to 23.0x for the same accuracy level compared to existing AQP systems.|today databas previous queri answer rare benefit answer futur queri first time best knowledg chang paradigm approxim queri process aqp context make follow observ answer queri reveal degre knowledg answer anoth queri becaus answer stem distribut produc entir dataset exploit refin knowledg allow us answer queri analyt rather read enorm amount raw data also process queri continu enhanc knowledg distribut henc lead increas faster respons time futur queri call novel idea learn past queri answer databas learn exploit principl maximum entropi produc answer expect guarante accur exist sampl base approxim empow idea build queri engin top spark sql call verdict conduct extens experi real world queri trace larg custom major databas vendor result demonstr databas learn support queri speed accuraci level compar exist aqp system|['Yongjoo Park', 'Ahmad Shahab Tajik', 'Michael Cafarella', 'Barzan Mozafari']|['cs.DB', 'cs.AI']
2017-03-28T14:05:41Z|2017-03-16T01:37:25Z|http://arxiv.org/abs/1703.05452v1|http://arxiv.org/pdf/1703.05452v1|Efficient Online Learning for Optimizing Value of Information: Theory   and Application to Interactive Troubleshooting|effici onlin learn optim valu inform theori applic interact troubleshoot|We consider the optimal value of information (VoI) problem, where the goal is to sequentially select a set of tests with a minimal cost, so that one can efficiently make the best decision based on the observed outcomes. Existing algorithms are either heuristics with no guarantees, or scale poorly (with exponential run time in terms of the number of available tests). Moreover, these methods assume a known distribution over the test outcomes, which is often not the case in practice. We propose an efficient sampling-based online learning framework to address the above issues. First, assuming the distribution over hypotheses is known, we propose a dynamic hypothesis enumeration strategy, which allows efficient information gathering with strong theoretical guarantees. We show that with sufficient amount of samples, one can identify a near-optimal decision with high probability. Second, when the parameters of the hypotheses distribution are unknown, we propose an algorithm which learns the parameters progressively via posterior sampling in an online fashion. We further establish a rigorous bound on the expected regret. We demonstrate the effectiveness of our approach on a real-world interactive troubleshooting application and show that one can efficiently make high-quality decisions with low cost.|consid optim valu inform voi problem goal sequenti select set test minim cost one effici make best decis base observ outcom exist algorithm either heurist guarante scale poor exponenti run time term number avail test moreov method assum known distribut test outcom often case practic propos effici sampl base onlin learn framework address abov issu first assum distribut hypothes known propos dynam hypothesi enumer strategi allow effici inform gather strong theoret guarante show suffici amount sampl one identifi near optim decis high probabl second paramet hypothes distribut unknown propos algorithm learn paramet progress via posterior sampl onlin fashion establish rigor bound expect regret demonstr effect approach real world interact troubleshoot applic show one effici make high qualiti decis low cost|['Yuxin Chen', 'Jean-Michel Renders', 'Morteza Haghir Chehreghani', 'Andreas Krause']|['cs.AI', 'cs.LG', 'stat.ML']
2017-03-28T14:05:41Z|2017-03-16T01:31:33Z|http://arxiv.org/abs/1703.05449v1|http://arxiv.org/pdf/1703.05449v1|Minimax Regret Bounds for Reinforcement Learning|minimax regret bound reinforc learn|"We consider the problem of efficient exploration in finite horizon MDPs.We show that an optimistic modification to model-based value iteration, can achieve a regret bound $\tilde{O}( \sqrt{HSAT} + H^2S^2A+H\sqrt{T})$ where $H$ is the time horizon, $S$ the number of states, $A$ the number of actions and $T$ the time elapsed. This result improves over the best previous known bound $\tilde{O}(HS \sqrt{AT})$ achieved by the UCRL2 algorithm.The key significance of our new results is that when $T\geq H^3S^3A$ and $SA\geq H$, it leads to a regret of $\tilde{O}(\sqrt{HSAT})$ that matches the established lower bounds of $\Omega(\sqrt{HSAT})$ up to a logarithmic factor. Our analysis contain two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in $S$), and we use ""exploration bonuses"" based on Bernstein's inequality, together with using a recursive -Bellman-type- Law of Total Variance (to improve scaling in $H$)."|consid problem effici explor finit horizon mdps show optimist modif model base valu iter achiev regret bound tild sqrt hsat sqrt time horizon number state number action time elaps result improv best previous known bound tild hs sqrt achiev ucrl algorithm key signific new result geq sa geq lead regret tild sqrt hsat match establish lower bound omega sqrt hsat logarithm factor analysi contain two key insight use care applic concentr inequ optim valu function whole rather transit probabl improv scale use explor bonus base bernstein inequ togeth use recurs bellman type law total varianc improv scale|['Mohammad Gheshlaghi Azar', 'Ian Osband', 'R√©mi Munos']|['stat.ML', 'cs.AI', 'cs.LG']
2017-03-28T14:05:41Z|2017-03-16T01:14:36Z|http://arxiv.org/abs/1703.05446v1|http://arxiv.org/pdf/1703.05446v1|Look into Person: Self-supervised Structure-sensitive Learning and A New   Benchmark for Human Parsing|look person self supervis structur sensit learn new benchmark human pars|"Human parsing has recently attracted a lot of research interests due to its huge application potentials. However existing datasets have limited number of images and annotations, and lack the variety of human appearances and the coverage of challenging cases in unconstrained environment. In this paper, we introduce a new benchmark ""Look into Person (LIP)"" that makes a significant advance in terms of scalability, diversity and difficulty, a contribution that we feel is crucial for future developments in human-centric analysis. This comprehensive dataset contains over 50,000 elaborately annotated images with 19 semantic part labels, which are captured from a wider range of viewpoints, occlusions and background complexity. Given these rich annotations we perform detailed analyses of the leading human parsing approaches, gaining insights into the success and failures of these methods. Furthermore, in contrast to the existing efforts on improving the feature discriminative capability, we solve human parsing by exploring a novel self-supervised structure-sensitive learning approach, which imposes human pose structures into parsing results without resorting to extra supervision (i.e., no need for specifically labeling human joints in model training). Our self-supervised learning framework can be injected into any advanced neural networks to help incorporate rich high-level knowledge regarding human joints from a global perspective and improve the parsing results. Extensive evaluations on our LIP and the public PASCAL-Person-Part dataset demonstrate the superiority of our method."|human pars recent attract lot research interest due huge applic potenti howev exist dataset limit number imag annot lack varieti human appear coverag challeng case unconstrain environ paper introduc new benchmark look person lip make signific advanc term scalabl divers difficulti contribut feel crucial futur develop human centric analysi comprehens dataset contain elabor annot imag semant part label captur wider rang viewpoint occlus background complex given rich annot perform detail analys lead human pars approach gain insight success failur method furthermor contrast exist effort improv featur discrimin capabl solv human pars explor novel self supervis structur sensit learn approach impos human pose structur pars result without resort extra supervis need specif label human joint model train self supervis learn framework inject ani advanc neural network help incorpor rich high level knowledg regard human joint global perspect improv pars result extens evalu lip public pascal person part dataset demonstr superior method|['Ke Gong', 'Xiaodan Liang', 'Xiaohui Shen', 'Liang Lin']|['cs.CV', 'cs.AI', 'cs.LG']
2017-03-28T14:05:41Z|2017-03-16T01:06:07Z|http://arxiv.org/abs/1703.05320v1|http://arxiv.org/pdf/1703.05320v1|Legal Question Answering using Ranking SVM and Deep Convolutional Neural   Network|legal question answer use rank svm deep convolut neural network|This paper presents a study of employing Ranking SVM and Convolutional Neural Network for two missions: legal information retrieval and question answering in the Competition on Legal Information Extraction/Entailment. For the first task, our proposed model used a triple of features (LSI, Manhattan, Jaccard), and is based on paragraph level instead of article level as in previous studies. In fact, each single-paragraph article corresponds to a particular paragraph in a huge multiple-paragraph article. For the legal question answering task, additional statistical features from information retrieval task integrated into Convolutional Neural Network contribute to higher accuracy.|paper present studi employ rank svm convolut neural network two mission legal inform retriev question answer competit legal inform extract entail first task propos model use tripl featur lsi manhattan jaccard base paragraph level instead articl level previous studi fact singl paragraph articl correspond particular paragraph huge multipl paragraph articl legal question answer task addit statist featur inform retriev task integr convolut neural network contribut higher accuraci|['Phong-Khac Do', 'Huy-Tien Nguyen', 'Chien-Xuan Tran', 'Minh-Tien Nguyen', 'Minh-Le Nguyen']|['cs.CL', 'cs.AI', '14J30 (Primary)', 'H.3; H.3.3; I.2.7']
2017-03-28T14:05:41Z|2017-03-15T21:20:44Z|http://arxiv.org/abs/1703.05390v1|http://arxiv.org/pdf/1703.05390v1|Convolutional Recurrent Neural Networks for Small-Footprint Keyword   Spotting|convolut recurr neural network small footprint keyword spot|Keyword spotting (KWS) constitutes a major component of human-technology interfaces. Maximizing the detection accuracy at a low false alarm (FA) rate, while minimizing the footprint size, latency and complexity are the goals for KWS. Towards achieving them, we study Convolutional Recurrent Neural Networks (CRNNs). Inspired by large-scale state-of-the-art speech recognition systems, we combine the strengths of convolutional layers and recurrent layers to exploit local structure and long-range context. We analyze the effect of architecture parameters, and propose training strategies to improve performance. With only ~230k parameters, our CRNN model yields acceptably low latency, and achieves 97.71% accuracy at 0.5 FA/hour for 5 dB signal-to-noise ratio.|keyword spot kws constitut major compon human technolog interfac maxim detect accuraci low fals alarm fa rate minim footprint size latenc complex goal kws toward achiev studi convolut recurr neural network crnns inspir larg scale state art speech recognit system combin strength convolut layer recurr layer exploit local structur long rang context analyz effect architectur paramet propos train strategi improv perform onli paramet crnn model yield accept low latenc achiev accuraci fa hour db signal nois ratio|['Sercan O. Arik', 'Markus Kliegl', 'Rewon Child', 'Joel Hestness', 'Andrew Gibiansky', 'Chris Fougner', 'Ryan Prenger', 'Adam Coates']|['cs.CL', 'cs.AI', 'cs.LG']
2017-03-28T14:05:41Z|2017-03-15T20:23:45Z|http://arxiv.org/abs/1703.05376v1|http://arxiv.org/pdf/1703.05376v1|Concentration Bounds for Two Timescale Stochastic Approximation with   Applications to Reinforcement Learning|concentr bound two timescal stochast approxim applic reinforc learn|Two-timescale Stochastic Approximation (SA) algorithms are widely used in Reinforcement Learning (RL). In such methods, the iterates consist of two parts that are updated using different stepsizes. We develop the first convergence rate result for these algorithms; in particular, we provide a general methodology for analyzing two-timescale linear SA. We apply our methodology to two-timescale RL algorithms such as GTD(0), GTD2, and TDC.|two timescal stochast approxim sa algorithm wide use reinforc learn rl method iter consist two part updat use differ stepsiz develop first converg rate result algorithm particular provid general methodolog analyz two timescal linear sa appli methodolog two timescal rl algorithm gtd gtd tdc|['Gal Dalal', 'Balazs Szorenyi', 'Gugan Thoppe', 'Shie Mannor']|['cs.AI']
2017-03-28T14:05:41Z|2017-03-15T17:01:20Z|http://arxiv.org/abs/1703.05260v1|http://arxiv.org/pdf/1703.05260v1|InScript: Narrative texts annotated with script information|inscript narrat text annot script inform|This paper presents the InScript corpus (Narrative Texts Instantiating Script structure). InScript is a corpus of 1,000 stories centered around 10 different scenarios. Verbs and noun phrases are annotated with event and participant types, respectively. Additionally, the text is annotated with coreference information. The corpus shows rich lexical variation and will serve as a unique resource for the study of the role of script knowledge in natural language processing.|paper present inscript corpus narrat text instanti script structur inscript corpus stori center around differ scenario verb noun phrase annot event particip type respect addit text annot corefer inform corpus show rich lexic variat serv uniqu resourc studi role script knowledg natur languag process|['Ashutosh Modi', 'Tatjana Anikina', 'Simon Ostermann', 'Manfred Pinkal']|['cs.CL', 'cs.AI']
2017-03-28T14:05:41Z|2017-03-15T15:19:28Z|http://arxiv.org/abs/1703.05204v1|http://arxiv.org/pdf/1703.05204v1|On Inconsistency Indices and Inconsistency Axioms in Pairwise   Comparisons|inconsist indic inconsist axiom pairwis comparison|Pairwise comparisons are an important tool of modern (multiple criteria) decision making. Since human judgments are often inconsistent, many studies focused on the ways how to express and measure this inconsistency, and several inconsistency indices were proposed as an alternative to Saaty inconsistency index and inconsistency ratio for reciprocal pairwise comparisons matrices. This paper aims to: firstly, introduce a new measure of inconsistency of pairwise comparisons and to prove its basic properties; secondly, to postulate an additional axiom, an upper boundary axiom, to an existing set of axioms; and the last, but not least, the paper provides proofs of satisfaction of this additional axiom by selected inconsistency indices as well as it provides their numerical comparison.|pairwis comparison import tool modern multipl criteria decis make sinc human judgment often inconsist mani studi focus way express measur inconsist sever inconsist indic propos altern saati inconsist index inconsist ratio reciproc pairwis comparison matric paper aim first introduc new measur inconsist pairwis comparison prove basic properti second postul addit axiom upper boundari axiom exist set axiom last least paper provid proof satisfact addit axiom select inconsist indic well provid numer comparison|['Jiri Mazurek']|['cs.AI']
2017-03-28T14:05:41Z|2017-03-15T15:13:42Z|http://arxiv.org/abs/1703.05201v1|http://arxiv.org/pdf/1703.05201v1|Fuzzy Rankings: Properties and Applications|fuzzi rank properti applic|In practice, a ranking of objects with respect to given set of criteria is of considerable importance. However, due to lack of knowledge, information of time pressure, decision makers might not be able to provide a (crisp) ranking of objects from the top to the bottom. Instead, some objects might be ranked equally, or better than other objects only to some degree. In such cases, a generalization of crisp rankings to fuzzy rankings can be more useful. The aim of the article is to introduce the notion of a fuzzy ranking and to discuss its several properties, namely orderings, similarity and indecisiveness. The proposed approach can be used both for group decision making or multiple criteria decision making when uncertainty is involved.|practic rank object respect given set criteria consider import howev due lack knowledg inform time pressur decis maker might abl provid crisp rank object top bottom instead object might rank equal better object onli degre case general crisp rank fuzzi rank use aim articl introduc notion fuzzi rank discuss sever properti name order similar indecis propos approach use group decis make multipl criteria decis make uncertainti involv|['Ji≈ô√≠ Mazurek']|['cs.AI']
2017-03-28T14:05:45Z|2017-03-15T07:57:51Z|http://arxiv.org/abs/1703.04990v1|http://arxiv.org/pdf/1703.04990v1|Neural Programming by Example|neural program exampl|Programming by Example (PBE) targets at automatically inferring a computer program for accomplishing a certain task from sample input and output. In this paper, we propose a deep neural networks (DNN) based PBE model called Neural Programming by Example (NPBE), which can learn from input-output strings and induce programs that solve the string manipulation problems. Our NPBE model has four neural network based components: a string encoder, an input-output analyzer, a program generator, and a symbol selector. We demonstrate the effectiveness of NPBE by training it end-to-end to solve some common string manipulation problems in spreadsheet systems. The results show that our model can induce string manipulation programs effectively. Our work is one step towards teaching DNN to generate computer programs.|program exampl pbe target automat infer comput program accomplish certain task sampl input output paper propos deep neural network dnn base pbe model call neural program exampl npbe learn input output string induc program solv string manipul problem npbe model four neural network base compon string encod input output analyz program generat symbol selector demonstr effect npbe train end end solv common string manipul problem spreadsheet system result show model induc string manipul program effect work one step toward teach dnn generat comput program|['Chengxun Shu', 'Hongyu Zhang']|['cs.AI', 'cs.NE', 'cs.SE']
2017-03-28T14:05:45Z|2017-03-15T05:43:48Z|http://arxiv.org/abs/1703.04940v1|http://arxiv.org/pdf/1703.04940v1|Resilience: A Criterion for Learning in the Presence of Arbitrary   Outliers|resili criterion learn presenc arbitrari outlier|We introduce a criterion, resilience, which allows properties of a dataset (such as its mean or best low rank approximation) to be robustly computed, even in the presence of a large fraction of arbitrary additional data. Resilience is a weaker condition than most other properties considered so far in the literature, and yet enables robust estimation in a broader variety of settings, including the previously unstudied problem of robust mean estimation in $\ell_p$-norms.|introduc criterion resili allow properti dataset mean best low rank approxim robust comput even presenc larg fraction arbitrari addit data resili weaker condit properti consid far literatur yet enabl robust estim broader varieti set includ previous unstudi problem robust mean estim ell norm|['Jacob Steinhardt', 'Moses Charikar', 'Gregory Valiant']|['cs.LG', 'cs.AI', 'cs.CC', 'cs.CR', 'stat.ML']
2017-03-28T14:05:45Z|2017-03-17T00:56:18Z|http://arxiv.org/abs/1703.04912v2|http://arxiv.org/pdf/1703.04912v2|Syntax-Preserving Belief Change Operators for Logic Programs|syntax preserv belief chang oper logic program|Recent methods have adapted the well-established AGM and belief base frameworks for belief change to cover belief revision in logic programs. In this study here, we present two new sets of belief change operators for logic programs. They focus on preserving the explicit relationships expressed in the rules of a program, a feature that is missing in purely semantic approaches that consider programs only in their entirety. In particular, operators of the latter class fail to satisfy preservation and support, two important properties for belief change in logic programs required to ensure intuitive results.   We address this shortcoming of existing approaches by introducing partial meet and ensconcement constructions for logic program belief change, which allow us to define syntax-preserving operators that satisfy preservation and support. Our work is novel in that our constructions not only preserve more information from a logic program during a change operation than existing ones, but they also facilitate natural definitions of contraction operators, the first in the field to the best of our knowledge.   In order to evaluate the rationality of our operators, we translate the revision and contraction postulates from the AGM and belief base frameworks to the logic programming setting. We show that our operators fully comply with the belief base framework and formally state the interdefinability between our operators. We further propose an algorithm that is based on modularising a logic program to reduce partial meet and ensconcement revisions or contractions to performing the operation only on the relevant modules of that program. Finally, we compare our approach to two state-of-the-art logic program revision methods and demonstrate that our operators address the shortcomings of one and generalise the other method.|recent method adapt well establish agm belief base framework belief chang cover belief revis logic program studi present two new set belief chang oper logic program focus preserv explicit relationship express rule program featur miss pure semant approach consid program onli entireti particular oper latter class fail satisfi preserv support two import properti belief chang logic program requir ensur intuit result address shortcom exist approach introduc partial meet ensconc construct logic program belief chang allow us defin syntax preserv oper satisfi preserv support work novel construct onli preserv inform logic program dure chang oper exist one also facilit natur definit contract oper first field best knowledg order evalu ration oper translat revis contract postul agm belief base framework logic program set show oper fulli compli belief base framework formal state interdefin oper propos algorithm base modularis logic program reduc partial meet ensconc revis contract perform oper onli relev modul program final compar approach two state art logic program revis method demonstr oper address shortcom one generalis method|['Sebastian Binnewies', 'Zhiqiang Zhuang', 'Kewen Wang', 'Bela Stantic']|['cs.AI', 'I.2.3; I.2.4; F.4.1']
2017-03-28T14:05:45Z|2017-03-15T03:30:13Z|http://arxiv.org/abs/1703.04908v1|http://arxiv.org/pdf/1703.04908v1|Emergence of Grounded Compositional Language in Multi-Agent Populations|emerg ground composit languag multi agent popul|By capturing statistical patterns in large corpora, machine learning has enabled significant advances in natural language processing, including in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply capturing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. Towards this end, we propose a multi-agent learning environment and learning methods that bring about emergence of a basic compositional language. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of non-verbal communication such as pointing and guiding when language communication is unavailable.|captur statist pattern larg corpora machin learn enabl signific advanc natur languag process includ machin translat question answer sentiment analysi howev agent intellig interact human simpli captur statist pattern insuffici paper investig ground composit languag emerg mean achiev goal multi agent popul toward end propos multi agent learn environ learn method bring emerg basic composit languag languag repres stream abstract discret symbol utter agent time nonetheless coher structur possess defin vocabulari syntax also observ emerg non verbal communic point guid languag communic unavail|['Igor Mordatch', 'Pieter Abbeel']|['cs.AI', 'cs.CL']
2017-03-28T14:05:45Z|2017-03-15T01:04:49Z|http://arxiv.org/abs/1703.04862v1|http://arxiv.org/pdf/1703.04862v1|Exploring the Combination Rules of D Numbers From a Perspective of   Conflict Redistribution|explor combin rule number perspect conflict redistribut|Dempster-Shafer theory of evidence is widely applied to uncertainty modelling and knowledge reasoning because of its advantages in dealing with uncertain information. But some conditions or requirements, such as exclusiveness hypothesis and completeness constraint, limit the development and application of that theory to a large extend. To overcome the shortcomings and enhance its capability of representing the uncertainty, a novel model, called D numbers, has been proposed recently. However, many key issues, for example how to implement the combination of D numbers, remain unsolved. In the paper, we have explored the combination of D Numbers from a perspective of conflict redistribution, and proposed two combination rules being suitable for different situations for the fusion of two D numbers. The proposed combination rules can reduce to the classical Dempster's rule in Dempster-Shafer theory under a certain conditions. Numerical examples and discussion about the proposed rules are also given in the paper.|dempster shafer theori evid wide appli uncertainti model knowledg reason becaus advantag deal uncertain inform condit requir exclus hypothesi complet constraint limit develop applic theori larg extend overcom shortcom enhanc capabl repres uncertainti novel model call number propos recent howev mani key issu exampl implement combin number remain unsolv paper explor combin number perspect conflict redistribut propos two combin rule suitabl differ situat fusion two number propos combin rule reduc classic dempster rule dempster shafer theori certain condit numer exampl discuss propos rule also given paper|['Xinyang Deng', 'Wen Jiang']|['cs.AI']
2017-03-28T14:05:45Z|2017-03-14T23:09:45Z|http://arxiv.org/abs/1703.04816v1|http://arxiv.org/pdf/1703.04816v1|FastQA: A Simple and Efficient Neural Architecture for Question   Answering|fastqa simpl effici neural architectur question answer|Recent development of large-scale question answering (QA) datasets triggered a substantial amount of research into end-to-end neural architectures for QA. Increasingly complex systems have been conceived without comparison to a simpler neural baseline system that would justify their complexity. In this work, we propose a simple heuristic that guided the development of FastQA, an efficient end-to-end neural model for question answering that is very competitive with existing models. We further demonstrate, that an extended version (FastQAExt) achieves state-of-the-art results on recent benchmark datasets, namely SQuAD, NewsQA and MsMARCO, outperforming most existing models. However, we show that increasing the complexity of FastQA to FastQAExt does not yield any systematic improvements. We argue that the same holds true for most existing systems that are similar to FastQAExt. A manual analysis reveals that our proposed heuristic explains most predictions of our model, which indicates that modeling a simple heuristic is enough to achieve strong performance on extractive QA datasets. The overall strong performance of FastQA puts results of existing, more complex models into perspective.|recent develop larg scale question answer qa dataset trigger substanti amount research end end neural architectur qa increas complex system conceiv without comparison simpler neural baselin system would justifi complex work propos simpl heurist guid develop fastqa effici end end neural model question answer veri competit exist model demonstr extend version fastqaext achiev state art result recent benchmark dataset name squad newsqa msmarco outperform exist model howev show increas complex fastqa fastqaext doe yield ani systemat improv argu hold true exist system similar fastqaext manual analysi reveal propos heurist explain predict model indic model simpl heurist enough achiev strong perform extract qa dataset overal strong perform fastqa put result exist complex model perspect|['Dirk Weissenborn', 'Georg Wiese', 'Laura Seiffe']|['cs.CL', 'cs.AI', 'cs.NE']
2017-03-28T14:05:45Z|2017-03-14T22:13:20Z|http://arxiv.org/abs/1703.04756v1|http://arxiv.org/pdf/1703.04756v1|Weighted Voting Via No-Regret Learning|weight vote via regret learn|Voting systems typically treat all voters equally. We argue that perhaps they should not: Voters who have supported good choices in the past should be given higher weight than voters who have supported bad ones. To develop a formal framework for desirable weighting schemes, we draw on no-regret learning. Specifically, given a voting rule, we wish to design a weighting scheme such that applying the voting rule, with voters weighted by the scheme, leads to choices that are almost as good as those endorsed by the best voter in hindsight. We derive possibility and impossibility results for the existence of such weighting schemes, depending on whether the voting rule and the weighting scheme are deterministic or randomized, as well as on the social choice axioms satisfied by the voting rule.|vote system typic treat voter equal argu perhap voter support good choic past given higher weight voter support bad one develop formal framework desir weight scheme draw regret learn specif given vote rule wish design weight scheme appli vote rule voter weight scheme lead choic almost good endors best voter hindsight deriv possibl imposs result exist weight scheme depend whether vote rule weight scheme determinist random well social choic axiom satisfi vote rule|['Nika Haghtalab', 'Ritesh Noothigattu', 'Ariel D. Procaccia']|['cs.GT', 'cs.AI', 'cs.LG', 'cs.MA']
2017-03-28T14:05:45Z|2017-03-17T08:12:10Z|http://arxiv.org/abs/1703.04741v2|http://arxiv.org/pdf/1703.04741v2|Towards Moral Autonomous Systems|toward moral autonom system|Both the ethics of autonomous systems and the problems of their technical implementation have by now been studied in some detail. Less attention has been given to the areas in which these two separate concerns meet. This paper, written by both philosophers and engineers of autonomous systems, addresses a number of issues in machine ethics that are located at precisely the intersection between ethics and engineering. We first discuss different approaches towards the conceptual design of autonomous systems and their implications on the ethics implementation in such systems. Then we examine problematic areas regarding the specification and verification of ethical behavior in autonomous systems, particularly with a view towards the requirements of future legislation. We discuss transparency and accountability issues that will be crucial for any future wide deployment of autonomous systems in society. Finally we consider the, often overlooked, possibility of intentional misuse of AI systems and the possible dangers arising out of deliberately unethical design, implementation, and use of autonomous robots.|ethic autonom system problem technic implement studi detail less attent given area two separ concern meet paper written philosoph engin autonom system address number issu machin ethic locat precis intersect ethic engin first discuss differ approach toward conceptu design autonom system implic ethic implement system examin problemat area regard specif verif ethic behavior autonom system particular view toward requir futur legisl discuss transpar account issu crucial ani futur wide deploy autonom system societi final consid often overlook possibl intent misus ai system possibl danger aris deliber uneth design implement use autonom robot|['Vicky Charisi', 'Louise Dennis', 'Michael Fisher', 'Robert Lieck', 'Andreas Matthias', 'Marija Slavkovik', 'Janina Sombetzki', 'Alan F. T. Winfield', 'Roman Yampolskiy']|['cs.AI']
2017-03-28T14:05:45Z|2017-03-14T21:07:01Z|http://arxiv.org/abs/1703.04730v1|http://arxiv.org/pdf/1703.04730v1|Understanding Black-box Predictions via Influence Functions|understand black box predict via influenc function|How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, identifying the points most responsible for a given prediction. Applying ideas from second-order optimization, we scale up influence functions to modern machine learning settings and show that they can be applied to high-dimensional black-box models, even in non-convex and non-differentiable settings. We give a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for many different purposes: to understand model behavior, debug models and detect dataset errors, and even identify and exploit vulnerabilities to adversarial training-set attacks.|explain predict black box model paper use influenc function classic techniqu robust statist trace model predict learn algorithm back train data identifi point respons given predict appli idea second order optim scale influenc function modern machin learn set show appli high dimension black box model even non convex non differenti set give simpl effici implement requir onli oracl access gradient hessian vector product linear model convolut neural network demonstr influenc function use mani differ purpos understand model behavior debug model detect dataset error even identifi exploit vulner adversari train set attack|['Pang Wei Koh', 'Percy Liang']|['stat.ML', 'cs.AI', 'cs.LG']
2017-03-28T14:05:45Z|2017-03-14T19:14:32Z|http://arxiv.org/abs/1703.04677v1|http://arxiv.org/pdf/1703.04677v1|A computational investigation of sources of variability in sentence   comprehension difficulty in aphasia|comput investig sourc variabl sentenc comprehens difficulti aphasia|We present a computational evaluation of three hypotheses about sources of deficit in sentence comprehension in aphasia: slowed processing, intermittent deficiency, and resource reduction. The ACT-R based Lewis & Vasishth 2005 model is used to implement these three proposals. Slowed processing is implemented as slowed default production-rule firing time; intermittent deficiency as increased random noise in activation of chunks in memory; and resource reduction as reduced goal activation. As data, we considered subject vs. object relatives presented in a self-paced listening modality to 56 individuals with aphasia (IWA) and 46 matched controls. The participants heard the sentences and carried out a picture verification task to decide on an interpretation of the sentence. These response accuracies are used to identify the best parameters (for each participant) that correspond to the three hypotheses mentioned above. We show that controls have more tightly clustered (less variable) parameter values than IWA; specifically, compared to controls, among IWA there are more individuals with low goal activations, high noise, and slow default action times. This suggests that (i) individual patients show differential amounts of deficit along the three dimensions of slowed processing, intermittent deficient, and resource reduction, (ii) overall, there is evidence for all three sources of deficit playing a role, and (iii) IWA have a more variable range of parameter values than controls. In sum, this study contributes a proof of concept of a quantitative implementation of, and evidence for, these three accounts of comprehension deficits in aphasia.|present comput evalu three hypothes sourc deficit sentenc comprehens aphasia slow process intermitt defici resourc reduct act base lewi vasishth model use implement three propos slow process implement slow default product rule fire time intermitt defici increas random nois activ chunk memori resourc reduct reduc goal activ data consid subject vs object relat present self pace listen modal individu aphasia iwa match control particip heard sentenc carri pictur verif task decid interpret sentenc respons accuraci use identifi best paramet particip correspond three hypothes mention abov show control tight cluster less variabl paramet valu iwa specif compar control among iwa individu low goal activ high nois slow default action time suggest individu patient show differenti amount deficit along three dimens slow process intermitt defici resourc reduct ii overal evid three sourc deficit play role iii iwa variabl rang paramet valu control sum studi contribut proof concept quantit implement evid three account comprehens deficit aphasia|['Paul M√§tzig', 'Shravan Vasishth', 'Felix Engelmann', 'David Caplan']|['cs.CL', 'cs.AI']
2017-03-28T14:05:50Z|2017-03-14T17:15:42Z|http://arxiv.org/abs/1703.04587v1|http://arxiv.org/pdf/1703.04587v1|Minimizing Maximum Regret in Commitment Constrained Sequential Decision   Making|minim maximum regret commit constrain sequenti decis make|In cooperative multiagent planning, it can often be beneficial for an agent to make commitments about aspects of its behavior to others, allowing them in turn to plan their own behaviors without taking the agent's detailed behavior into account. Extending previous work in the Bayesian setting, we consider instead a worst-case setting in which the agent has a set of possible environments (MDPs) it could be in, and develop a commitment semantics that allows for probabilistic guarantees on the agent's behavior in any of the environments it could end up facing. Crucially, an agent receives observations (of reward and state transitions) that allow it to potentially eliminate possible environments and thus obtain higher utility by adapting its policy to the history of observations. We develop algorithms and provide theory and some preliminary empirical results showing that they ensure an agent meets its commitments with history-dependent policies while minimizing maximum regret over the possible environments.|cooper multiag plan often benefici agent make commit aspect behavior allow turn plan behavior without take agent detail behavior account extend previous work bayesian set consid instead worst case set agent set possibl environ mdps could develop commit semant allow probabilist guarante agent behavior ani environ could end face crucial agent receiv observ reward state transit allow potenti elimin possibl environ thus obtain higher util adapt polici histori observ develop algorithm provid theori preliminari empir result show ensur agent meet commit histori depend polici minim maximum regret possibl environ|['Qi Zhang', 'Satinder Singh', 'Edmund Durfee']|['cs.AI']
2017-03-28T14:05:50Z|2017-03-13T17:58:36Z|http://arxiv.org/abs/1703.04529v1|http://arxiv.org/pdf/1703.04529v1|Task-based End-to-end Model Learning|task base end end model learn|As machine learning techniques have become more ubiquitous, it has become common to see machine learning prediction algorithms operating within some larger process. However, the criteria by which we train machine learning algorithms often differ from the ultimate criteria on which we evaluate them. This paper proposes an end-to-end approach for learning probabilistic machine learning models within the context of stochastic programming, in a manner that directly captures the ultimate task-based objective for which they will be used. We then present two experimental evaluations of the proposed approach, one as applied to a generic inventory stock problem and the second to a real-world electrical grid scheduling task. In both cases, we show that the proposed approach can outperform both a traditional modeling approach and a purely black-box policy optimization approach.|machin learn techniqu becom ubiquit becom common see machin learn predict algorithm oper within larger process howev criteria train machin learn algorithm often differ ultim criteria evalu paper propos end end approach learn probabilist machin learn model within context stochast program manner direct captur ultim task base object use present two experiment evalu propos approach one appli generic inventori stock problem second real world electr grid schedul task case show propos approach outperform tradit model approach pure black box polici optim approach|['Priya L. Donti', 'Brandon Amos', 'J. Zico Kolter']|['cs.LG', 'cs.AI']
2017-03-28T14:05:50Z|2017-03-13T17:34:18Z|http://arxiv.org/abs/1703.04498v1|http://arxiv.org/pdf/1703.04498v1|High-Throughput and Language-Agnostic Entity Disambiguation and Linking   on User Generated Data|high throughput languag agnost entiti disambigu link user generat data|The Entity Disambiguation and Linking (EDL) task matches entity mentions in text to a unique Knowledge Base (KB) identifier such as a Wikipedia or Freebase id. It plays a critical role in the construction of a high quality information network, and can be further leveraged for a variety of information retrieval and NLP tasks such as text categorization and document tagging. EDL is a complex and challenging problem due to ambiguity of the mentions and real world text being multi-lingual. Moreover, EDL systems need to have high throughput and should be lightweight in order to scale to large datasets and run on off-the-shelf machines. More importantly, these systems need to be able to extract and disambiguate dense annotations from the data in order to enable an Information Retrieval or Extraction task running on the data to be more efficient and accurate. In order to address all these challenges, we present the Lithium EDL system and algorithm - a high-throughput, lightweight, language-agnostic EDL system that extracts and correctly disambiguates 75% more entities than state-of-the-art EDL systems and is significantly faster than them.|entiti disambigu link edl task match entiti mention text uniqu knowledg base kb identifi wikipedia freebas id play critic role construct high qualiti inform network leverag varieti inform retriev nlp task text categor document tag edl complex challeng problem due ambigu mention real world text multi lingual moreov edl system need high throughput lightweight order scale larg dataset run shelf machin import system need abl extract disambigu dens annot data order enabl inform retriev extract task run data effici accur order address challeng present lithium edl system algorithm high throughput lightweight languag agnost edl system extract correct disambigu entiti state art edl system signific faster|['Preeti Bhargava', 'Nemanja Spasojevic', 'Guoning Hu']|['cs.IR', 'cs.AI', 'cs.CL']
2017-03-28T14:05:50Z|2017-03-13T17:13:51Z|http://arxiv.org/abs/1703.04489v1|http://arxiv.org/pdf/1703.04489v1|Reinforcement Learning for Transition-Based Mention Detection|reinforc learn transit base mention detect|This paper describes an application of reinforcement learning to the mention detection task. We define a novel action-based formulation for the mention detection task, in which a model can flexibly revise past labeling decisions by grouping together tokens and assigning partial mention labels. We devise a method to create mention-level episodes and we train a model by rewarding correctly labeled complete mentions, irrespective of the inner structure created. The model yields results which are on par with a competitive supervised counterpart while being more flexible in terms of achieving targeted behavior through reward modeling and generating internal mention structure, especially on longer mentions.|paper describ applic reinforc learn mention detect task defin novel action base formul mention detect task model flexibl revis past label decis group togeth token assign partial mention label devis method creat mention level episod train model reward correct label complet mention irrespect inner structur creat model yield result par competit supervis counterpart flexibl term achiev target behavior reward model generat intern mention structur especi longer mention|['Georgiana Dinu', 'Wael Hamza', 'Radu Florian']|['cs.CL', 'cs.AI']
2017-03-28T14:05:50Z|2017-03-13T13:45:13Z|http://arxiv.org/abs/1703.04389v1|http://arxiv.org/pdf/1703.04389v1|Bayesian Optimization with Gradients|bayesian optim gradient|In recent years, Bayesian optimization has proven successful for global optimization of expensive-to-evaluate multimodal objective functions. However, unlike most optimization methods, Bayesian optimization typically does not use derivative information. In this paper we show how Bayesian optimization can exploit derivative information to decrease the number of objective function evaluations required for good performance. In particular, we develop a novel Bayesian optimization algorithm, the derivative-enabled knowledge-gradient (dKG), for which we show one-step Bayes-optimality, asymptotic consistency, and greater one-step value of information than is possible in the derivative-free setting. Our procedure accommodates noisy and incomplete derivative information, and comes in both sequential and batch forms. We show dKG provides state-of-the-art performance compared to a wide range of optimization procedures with and without gradients, on benchmarks including logistic regression, kernel learning, and k-nearest neighbors.|recent year bayesian optim proven success global optim expens evalu multimod object function howev unlik optim method bayesian optim typic doe use deriv inform paper show bayesian optim exploit deriv inform decreas number object function evalu requir good perform particular develop novel bayesian optim algorithm deriv enabl knowledg gradient dkg show one step bay optim asymptot consist greater one step valu inform possibl deriv free set procedur accommod noisi incomplet deriv inform come sequenti batch form show dkg provid state art perform compar wide rang optim procedur without gradient benchmark includ logist regress kernel learn nearest neighbor|['Jian Wu', 'Matthias Poloczek', 'Andrew Gordon Wilson', 'Peter I. Frazier']|['stat.ML', 'cs.AI', 'cs.LG', 'math.OC']
2017-03-28T14:05:50Z|2017-03-13T13:32:46Z|http://arxiv.org/abs/1703.04382v1|http://arxiv.org/pdf/1703.04382v1|Cost-Based Intuitionist Probabilities on Spaces of Graphs, Hypergraphs   and Theorems|cost base intuitionist probabl space graph hypergraph theorem|A novel partial order is defined on the space of digraphs or hypergraphs, based on assessing the cost of producing a graph via a sequence of elementary transformations. Leveraging work by Knuth and Skilling on the foundations of inference, and the structure of Heyting algebras on graph space, this partial order is used to construct an intuitionistic probability measure that applies to either digraphs or hypergraphs. As logical inference steps can be represented as transformations on hypergraphs representing logical statements, this also yields an intuitionistic probability measure on spaces of theorems. The central result is also extended to yield intuitionistic probabilities based on more general weighted rule systems defined over bicartesian closed categories.|novel partial order defin space digraph hypergraph base assess cost produc graph via sequenc elementari transform leverag work knuth skill foundat infer structur heyt algebra graph space partial order use construct intuitionist probabl measur appli either digraph hypergraph logic infer step repres transform hypergraph repres logic statement also yield intuitionist probabl measur space theorem central result also extend yield intuitionist probabl base general weight rule system defin bicartesian close categori|['Ben Goertzel']|['cs.AI']
2017-03-28T14:05:50Z|2017-03-13T13:06:49Z|http://arxiv.org/abs/1703.04368v1|http://arxiv.org/pdf/1703.04368v1|Symbol Grounding via Chaining of Morphisms|symbol ground via chain morphism|"A new model of symbol grounding is presented, in which the structures of natural language, logical semantics, perception and action are represented categorically, and symbol grounding is modeled via the composition of morphisms between the relevant categories. This model gives conceptual insight into the fundamentally systematic nature of symbol grounding, and also connects naturally to practical real-world AI systems in current research and commercial use. Specifically, it is argued that the structure of linguistic syntax can be modeled as a certain asymmetric monoidal category, as e.g. implicit in the link grammar formalism; the structure of spatiotemporal relationships and action plans can be modeled similarly using ""image grammars"" and ""action grammars""; and common-sense logical semantic structure can be modeled using dependently-typed lambda calculus with uncertain truth values. Given these formalisms, the grounding of linguistic descriptions in spatiotemporal perceptions and coordinated actions consists of following morphisms from language to logic through to spacetime and body (for comprehension), and vice versa (for generation). The mapping is indicated between the spatial relationships in the Region Connection Calculus and Allen Interval Algebra and corresponding entries in the link grammar syntax parsing dictionary. Further, the abstractions introduced here are shown to naturally model the structures and systems currently being deployed in the context of using the OpenCog cognitive architecture to control Hanson Robotics humanoid robots."|new model symbol ground present structur natur languag logic semant percept action repres categor symbol ground model via composit morphism relev categori model give conceptu insight fundament systemat natur symbol ground also connect natur practic real world ai system current research commerci use specif argu structur linguist syntax model certain asymmetr monoid categori implicit link grammar formal structur spatiotempor relationship action plan model similar use imag grammar action grammar common sens logic semant structur model use depend type lambda calculus uncertain truth valu given formal ground linguist descript spatiotempor percept coordin action consist follow morphism languag logic spacetim bodi comprehens vice versa generat map indic spatial relationship region connect calculus allen interv algebra correspond entri link grammar syntax pars dictionari abstract introduc shown natur model structur system current deploy context use opencog cognit architectur control hanson robot humanoid robot|['Ruiting Lian', 'Ben Goertzel', 'Linas Vepstas', 'David Hanson', 'Changle Zhou']|['cs.AI']
2017-03-28T14:05:50Z|2017-03-13T12:49:20Z|http://arxiv.org/abs/1703.04363v1|http://arxiv.org/pdf/1703.04363v1|Deep Value Networks Learn to Evaluate and Iteratively Refine Structured   Outputs|deep valu network learn evalu iter refin structur output|We approach structured output prediction by learning a deep value network (DVN) that evaluates different output structures for a given input. For example, when applied to image segmentation, the value network takes an image and a segmentation mask as inputs and predicts a scalar score evaluating the mask quality and its correspondence with the image. Once the value network is optimized, at inference, it finds output structures that maximize the score of the value net via gradient descent on continuous relaxations of structured outputs. Thus DVN takes advantage of the joint modeling of the inputs and outputs. Our framework applies to a wide range of structured output prediction problems. We conduct experiments on multi-label classification based on text data and on image segmentation problems. DVN outperforms several strong baselines and the state-of-the-art results on these benchmarks. In addition, on image segmentation, the proposed deep value network learns complex shape priors and effectively combines image information with the prior to obtain competitive segmentation results.|approach structur output predict learn deep valu network dvn evalu differ output structur given input exampl appli imag segment valu network take imag segment mask input predict scalar score evalu mask qualiti correspond imag onc valu network optim infer find output structur maxim score valu net via gradient descent continu relax structur output thus dvn take advantag joint model input output framework appli wide rang structur output predict problem conduct experi multi label classif base text data imag segment problem dvn outperform sever strong baselin state art result benchmark addit imag segment propos deep valu network learn complex shape prior effect combin imag inform prior obtain competit segment result|['Michael Gygli', 'Mohammad Norouzi', 'Anelia Angelova']|['cs.LG', 'cs.AI', 'cs.CV']
2017-03-28T14:05:50Z|2017-03-13T12:48:15Z|http://arxiv.org/abs/1703.04361v1|http://arxiv.org/pdf/1703.04361v1|Toward a Formal Model of Cognitive Synergy|toward formal model cognit synergi|"""Cognitive synergy"" refers to a dynamic in which multiple cognitive processes, cooperating to control the same cognitive system, assist each other in overcoming bottlenecks encountered during their internal processing. Cognitive synergy has been posited as a key feature of real-world general intelligence, and has been used explicitly in the design of the OpenCog cognitive architecture. Here category theory and related concepts are used to give a formalization of the cognitive synergy concept.   A series of formal models of intelligent agents is proposed, with increasing specificity and complexity: simple reinforcement learning agents; ""cognit"" agents with an abstract memory and processing model; hypergraph-based agents (in which ""cognit"" operations are carried out via hypergraphs); hypergraph agents with a rich language of nodes and hyperlinks (such as the OpenCog framework provides); ""PGMC"" agents whose rich hypergraphs are endowed with cognitive processes guided via Probabilistic Growth and Mining of Combinations; and finally variations of the PrimeAGI design, which is currently being built on top of OpenCog.   A notion of cognitive synergy is developed for cognitive processes acting within PGMC agents, based on developing a formal notion of ""stuckness,"" and defining synergy as a relationship between cognitive processes in which they can help each other out when they get stuck. It is proposed that cognitive processes relating to each other synergetically, associate in a certain way with functors that map into each other via natural transformations. Cognitive synergy is proposed to correspond to a certain inequality regarding the relative costs of different paths through certain commutation diagrams.   Applications of this notion of cognitive synergy to particular cognitive phenomena, and specific cognitive processes in the PrimeAGI design, are discussed."|cognit synergi refer dynam multipl cognit process cooper control cognit system assist overcom bottleneck encount dure intern process cognit synergi posit key featur real world general intellig use explicit design opencog cognit architectur categori theori relat concept use give formal cognit synergi concept seri formal model intellig agent propos increas specif complex simpl reinforc learn agent cognit agent abstract memori process model hypergraph base agent cognit oper carri via hypergraph hypergraph agent rich languag node hyperlink opencog framework provid pgmc agent whose rich hypergraph endow cognit process guid via probabilist growth mine combin final variat primeagi design current built top opencog notion cognit synergi develop cognit process act within pgmc agent base develop formal notion stuck defin synergi relationship cognit process help get stuck propos cognit process relat synerget associ certain way functor map via natur transform cognit synergi propos correspond certain inequ regard relat cost differ path certain commut diagram applic notion cognit synergi particular cognit phenomena specif cognit process primeagi design discuss|['Ben Goertzel']|['cs.AI']
2017-03-28T14:05:50Z|2017-03-13T03:29:23Z|http://arxiv.org/abs/1703.04232v1|http://arxiv.org/pdf/1703.04232v1|Numerical Integration and Dynamic Discretization in Heuristic Search   Planning over Hybrid Domains|numer integr dynam discret heurist search plan hybrid domain|In this paper we look into the problem of planning over hybrid domains, where change can be both discrete and instantaneous, or continuous over time. In addition, it is required that each state on the trajectory induced by the execution of plans complies with a given set of global constraints. We approach the computation of plans for such domains as the problem of searching over a deterministic state model. In this model, some of the successor states are obtained by solving numerically the so-called initial value problem over a set of ordinary differential equations (ODE) given by the current plan prefix. These equations hold over time intervals whose duration is determined dynamically, according to whether zero crossing events take place for a set of invariant conditions. The resulting planner, FS+, incorporates these features together with effective heuristic guidance. FS+ does not impose any of the syntactic restrictions on process effects often found on the existing literature on Hybrid Planning. A key concept of our approach is that a clear separation is struck between planning and simulation time steps. The former is the time allowed to observe the evolution of a given dynamical system before committing to a future course of action, whilst the later is part of the model of the environment. FS+ is shown to be a robust planner over a diverse set of hybrid domains, taken from the existing literature on hybrid planning and systems.|paper look problem plan hybrid domain chang discret instantan continu time addit requir state trajectori induc execut plan compli given set global constraint approach comput plan domain problem search determinist state model model successor state obtain solv numer call initi valu problem set ordinari differenti equat ode given current plan prefix equat hold time interv whose durat determin dynam accord whether zero cross event take place set invari condit result planner fs incorpor featur togeth effect heurist guidanc fs doe impos ani syntact restrict process effect often found exist literatur hybrid plan key concept approach clear separ struck plan simul time step former time allow observ evolut given dynam system befor commit futur cours action whilst later part model environ fs shown robust planner divers set hybrid domain taken exist literatur hybrid plan system|['Miquel Ramirez', 'Enrico Scala', 'Patrik Haslum', 'Sylvie Thiebaux']|['cs.AI', 'I.2.8; F.2.2; I.6; J.2']
2017-03-28T14:05:54Z|2017-03-13T01:49:27Z|http://arxiv.org/abs/1703.04221v1|http://arxiv.org/pdf/1703.04221v1|A Hierarchical Framework of Cloud Resource Allocation and Power   Management Using Deep Reinforcement Learning|hierarch framework cloud resourc alloc power manag use deep reinforc learn|Automatic decision-making approaches, such as reinforcement learning (RL), have been applied to (partially) solve the resource allocation problem adaptively in the cloud computing system. However, a complete cloud resource allocation framework exhibits high dimensions in state and action spaces, which prohibit the usefulness of traditional RL techniques. In addition, high power consumption has become one of the critical concerns in design and control of cloud computing systems, which degrades system reliability and increases cooling cost. An effective dynamic power management (DPM) policy should minimize power consumption while maintaining performance degradation within an acceptable level. Thus, a joint virtual machine (VM) resource allocation and power management framework is critical to the overall cloud computing system. Moreover, novel solution framework is necessary to address the even higher dimensions in state and action spaces. In this paper, we propose a novel hierarchical framework for solving the overall resource allocation and power management problem in cloud computing systems. The proposed hierarchical framework comprises a global tier for VM resource allocation to the servers and a local tier for distributed power management of local servers. The emerging deep reinforcement learning (DRL) technique, which can deal with complicated control problems with large state space, is adopted to solve the global tier problem. Furthermore, an autoencoder and a novel weight sharing structure are adopted to handle the high-dimensional state space and accelerate the convergence speed. On the other hand, the local tier of distributed server power managements comprises an LSTM based workload predictor and a model-free RL based power manager, operating in a distributed manner.|automat decis make approach reinforc learn rl appli partial solv resourc alloc problem adapt cloud comput system howev complet cloud resourc alloc framework exhibit high dimens state action space prohibit use tradit rl techniqu addit high power consumpt becom one critic concern design control cloud comput system degrad system reliabl increas cool cost effect dynam power manag dpm polici minim power consumpt maintain perform degrad within accept level thus joint virtual machin vm resourc alloc power manag framework critic overal cloud comput system moreov novel solut framework necessari address even higher dimens state action space paper propos novel hierarch framework solv overal resourc alloc power manag problem cloud comput system propos hierarch framework compris global tier vm resourc alloc server local tier distribut power manag local server emerg deep reinforc learn drl techniqu deal complic control problem larg state space adopt solv global tier problem furthermor autoencod novel weight share structur adopt handl high dimension state space acceler converg speed hand local tier distribut server power manag compris lstm base workload predictor model free rl base power manag oper distribut manner|['Ning Liu', 'Zhe Li', 'Zhiyuan Xu', 'Jielong Xu', 'Sheng Lin', 'Qinru Qiu', 'Jian Tang', 'Yanzhi Wang']|['cs.DC', 'cs.AI']
2017-03-28T14:05:54Z|2017-03-15T08:16:37Z|http://arxiv.org/abs/1703.04159v2|http://arxiv.org/pdf/1703.04159v2|Any-Angle Pathfinding for Multiple Agents Based on SIPP Algorithm|ani angl pathfind multipl agent base sipp algorithm|The problem of finding conflict-free trajectories for multiple agents of identical circular shape, operating in shared 2D workspace, is addressed in the paper and decoupled, e.g., prioritized, approach is used to solve this problem. Agents' workspace is tessellated into the square grid on which any-angle moves are allowed, e.g. each agent can move into an arbitrary direction as long as this move follows the straight line segment whose endpoints are tied to the distinct grid elements. A novel any-angle planner based on Safe Interval Path Planning (SIPP) algorithm is proposed to find trajectories for an agent moving amidst dynamic obstacles (other agents) on a grid. This algorithm is then used as part of a prioritized multi-agent planner AA-SIPP(m). On the theoretical, side we show that AA-SIPP(m) is complete under well-defined conditions. On the experimental side, in simulation tests with up to 200 agents involved, we show that our planner finds much better solutions in terms of cost (up to 20%) compared to the planners relying on cardinal moves only.|problem find conflict free trajectori multipl agent ident circular shape oper share workspac address paper decoupl priorit approach use solv problem agent workspac tessel squar grid ani angl move allow agent move arbitrari direct long move follow straight line segment whose endpoint tie distinct grid element novel ani angl planner base safe interv path plan sipp algorithm propos find trajectori agent move amidst dynam obstacl agent grid algorithm use part priorit multi agent planner aa sipp theoret side show aa sipp complet well defin condit experiment side simul test agent involv show planner find much better solut term cost compar planner reli cardin move onli|['Konstantin Yakovlev', 'Anton Andreychuk']|['cs.AI']
2017-03-28T14:05:54Z|2017-03-12T13:17:08Z|http://arxiv.org/abs/1703.04115v1|http://arxiv.org/pdf/1703.04115v1|BetaRun 2017 Team Description Paper: Variety, Complexity, and Learning|betarun team descript paper varieti complex learn|RoboCup offers a set of benchmark problems for Artificial Intelligence in form of official world championships since 1997. The most tactical advanced and richest in terms of behavioural complexity of these is the 2D Soccer Simulation League, a simulated robotic soccer competition. BetaRun is a new attempt combining both machine learning and manual programming approaches, with the ultimate goal to arrive at a team that is trained entirely from observing and playing games, and a successor of the World Champion team Gliders 2016.|robocup offer set benchmark problem artifici intellig form offici world championship sinc tactic advanc richest term behaviour complex soccer simul leagu simul robot soccer competit betarun new attempt combin machin learn manual program approach ultim goal arriv team train entir observ play game successor world champion team glider|['Olivia Michael', 'Oliver Obst']|['cs.AI']
2017-03-28T14:05:54Z|2017-03-25T03:21:57Z|http://arxiv.org/abs/1703.04071v2|http://arxiv.org/pdf/1703.04071v2|A Compact DNN: Approaching GoogLeNet-Level Accuracy of Classification   and Domain Adaptation|compact dnn approach googlenet level accuraci classif domain adapt|Recently, DNN model compression based on network architecture design, e.g., SqueezeNet, attracted a lot attention. No accuracy drop on image classification is observed on these extremely compact networks, compared to well-known models. An emerging question, however, is whether these model compression techniques hurt DNN's learning ability other than classifying images on a single dataset. Our preliminary experiment shows that these compression methods could degrade domain adaptation (DA) ability, though the classification performance is preserved. Therefore, we propose a new compact network architecture and unsupervised DA method in this paper. The DNN is built on a new basic module Conv-M which provides more diverse feature extractors without significantly increasing parameters. The unified framework of our DA method will simultaneously learn invariance across domains, reduce divergence of feature representations, and adapt label prediction. Our DNN has 4.1M parameters, which is only 6.7% of AlexNet or 59% of GoogLeNet. Experiments show that our DNN obtains GoogLeNet-level accuracy both on classification and DA, and our DA method slightly outperforms previous competitive ones. Put all together, our DA strategy based on our DNN achieves state-of-the-art on sixteen of total eighteen DA tasks on popular Office-31 and Office-Caltech datasets.|recent dnn model compress base network architectur design squeezenet attract lot attent accuraci drop imag classif observ extrem compact network compar well known model emerg question howev whether model compress techniqu hurt dnn learn abil classifi imag singl dataset preliminari experi show compress method could degrad domain adapt da abil though classif perform preserv therefor propos new compact network architectur unsupervis da method paper dnn built new basic modul conv provid divers featur extractor without signific increas paramet unifi framework da method simultan learn invari across domain reduc diverg featur represent adapt label predict dnn paramet onli alexnet googlenet experi show dnn obtain googlenet level accuraci classif da da method slight outperform previous competit one put togeth da strategi base dnn achiev state art sixteen total eighteen da task popular offic offic caltech dataset|['Chunpeng Wu', 'Wei Wen', 'Tariq Afzal', 'Yongmei Zhang', 'Yiran Chen', 'Hai Li']|['cs.CV', 'cs.AI', 'cs.NE']
2017-03-28T14:05:54Z|2017-03-11T20:24:06Z|http://arxiv.org/abs/1703.04565v1|http://arxiv.org/pdf/1703.04565v1|Fuzzy Model Tree For Early Effort Estimation|fuzzi model tree earli effort estim|Use Case Points (UCP) is a well-known method to estimate the project size, based on Use Case diagram, at early phases of software development. Although the Use Case diagram is widely accepted as a de-facto model for analyzing object oriented software requirements over the world, UCP method did not take sufficient amount of attention because, as yet, there is no consensus on how to produce software effort from UCP. This paper aims to study the potential of using Fuzzy Model Tree to derive effort estimates based on UCP size measure using a dataset collected for that purpose. The proposed approach has been validated against Treeboost model, Multiple Linear Regression and classical effort estimation based on the UCP model. The obtained results are promising and show better performance than those obtained by classical UCP, Multiple Linear Regression and slightly better than those obtained by Tree boost model.|use case point ucp well known method estim project size base use case diagram earli phase softwar develop although use case diagram wide accept de facto model analyz object orient softwar requir world ucp method take suffici amount attent becaus yet consensus produc softwar effort ucp paper aim studi potenti use fuzzi model tree deriv effort estim base ucp size measur use dataset collect purpos propos approach valid treeboost model multipl linear regress classic effort estim base ucp model obtain result promis show better perform obtain classic ucp multipl linear regress slight better obtain tree boost model|['Mohammad Azzeh', 'Ali Bou Nassif']|['cs.SE', 'cs.AI']
2017-03-28T14:05:54Z|2017-03-11T20:19:05Z|http://arxiv.org/abs/1703.04567v1|http://arxiv.org/pdf/1703.04567v1|Learning best K analogies from data distribution for case-based software   effort estimation|learn best analog data distribut case base softwar effort estim|Case-Based Reasoning (CBR) has been widely used to generate good software effort estimates. The predictive performance of CBR is a dataset dependent and subject to extremely large space of configuration possibilities. Regardless of the type of adaptation technique, deciding on the optimal number of similar cases to be used before applying CBR is a key challenge. In this paper we propose a new technique based on Bisecting k-medoids clustering algorithm to better understanding the structure of a dataset and discovering the the optimal cases for each individual project by excluding irrelevant cases. Results obtained showed that understanding of the data characteristic prior prediction stage can help in automatically finding the best number of cases for each test project. Performance figures of the proposed estimation method are better than those of other regular K-based CBR methods.|case base reason cbr wide use generat good softwar effort estim predict perform cbr dataset depend subject extrem larg space configur possibl regardless type adapt techniqu decid optim number similar case use befor appli cbr key challeng paper propos new techniqu base bisect medoid cluster algorithm better understand structur dataset discov optim case individu project exclud irrelev case result obtain show understand data characterist prior predict stage help automat find best number case test project perform figur propos estim method better regular base cbr method|['Mohammad Azzeh', 'Yousef Elsheikh']|['cs.SE', 'cs.AI']
2017-03-28T14:05:54Z|2017-03-11T09:08:48Z|http://arxiv.org/abs/1703.03933v1|http://arxiv.org/pdf/1703.03933v1|Micro-Objective Learning : Accelerating Deep Reinforcement Learning   through the Discovery of Continuous Subgoals|micro object learn acceler deep reinforc learn discoveri continu subgoal|Recently, reinforcement learning has been successfully applied to the logical game of Go, various Atari games, and even a 3D game, Labyrinth, though it continues to have problems in sparse reward settings. It is difficult to explore, but also difficult to exploit, a small number of successes when learning policy. To solve this issue, the subgoal and option framework have been proposed. However, discovering subgoals online is too expensive to be used to learn options in large state spaces. We propose Micro-objective learning (MOL) to solve this problem. The main idea is to estimate how important a state is while training and to give an additional reward proportional to its importance. We evaluated our algorithm in two Atari games: Montezuma's Revenge and Seaquest. With three experiments to each game, MOL significantly improved the baseline scores. Especially in Montezuma's Revenge, MOL achieved two times better results than the previous state-of-the-art model.|recent reinforc learn success appli logic game go various atari game even game labyrinth though continu problem spars reward set difficult explor also difficult exploit small number success learn polici solv issu subgoal option framework propos howev discov subgoal onlin expens use learn option larg state space propos micro object learn mol solv problem main idea estim import state train give addit reward proport import evalu algorithm two atari game montezuma reveng seaquest three experi game mol signific improv baselin score especi montezuma reveng mol achiev two time better result previous state art model|['Sungtae Lee', 'Sang-Woo Lee', 'Jinyoung Choi', 'Dong-Hyun Kwak', 'Byoung-Tak Zhang']|['cs.AI']
2017-03-28T14:05:54Z|2017-03-11T07:46:51Z|http://arxiv.org/abs/1703.03924v1|http://arxiv.org/pdf/1703.03924v1|Real-Time Machine Learning: The Missing Pieces|real time machin learn miss piec|Machine learning applications are increasingly deployed not only to serve predictions using static models, but also as tightly-integrated components of feedback loops involving dynamic, real-time decision making. These applications pose a new set of requirements, none of which are difficult to achieve in isolation, but the combination of which creates a challenge for existing distributed execution frameworks: computation with millisecond latency at high throughput, adaptive construction of arbitrary task graphs, and execution of heterogeneous kernels over diverse sets of resources. We assert that a new distributed execution framework is needed for such ML applications and propose a candidate approach with a proof-of-concept architecture that achieves a 63x performance improvement over a state-of-the-art execution framework for a representative application.|machin learn applic increas deploy onli serv predict use static model also tight integr compon feedback loop involv dynam real time decis make applic pose new set requir none difficult achiev isol combin creat challeng exist distribut execut framework comput millisecond latenc high throughput adapt construct arbitrari task graph execut heterogen kernel divers set resourc assert new distribut execut framework need ml applic propos candid approach proof concept architectur achiev perform improv state art execut framework repres applic|['Robert Nishihara', 'Philipp Moritz', 'Stephanie Wang', 'Alexey Tumanov', 'William Paul', 'Johann Schleier-Smith', 'Richard Liaw', 'Michael I. Jordan', 'Ion Stoica']|['cs.DC', 'cs.AI', 'cs.LG']
2017-03-28T14:05:54Z|2017-03-11T06:37:09Z|http://arxiv.org/abs/1703.03916v1|http://arxiv.org/pdf/1703.03916v1|Axioms in Model-based Planners|axiom model base planner|Axioms can be used to model derived predicates in domain- independent planning models. Formulating models which use axioms can sometimes result in problems with much smaller search spaces and shorter plans than the original model. Previous work on axiom-aware planners focused solely on state- space search planners. We propose axiom-aware planners based on answer set programming and integer programming. We evaluate them on PDDL domains with axioms and show that they can exploit additional expressivity of axioms.|axiom use model deriv predic domain independ plan model formul model use axiom sometim result problem much smaller search space shorter plan origin model previous work axiom awar planner focus sole state space search planner propos axiom awar planner base answer set program integ program evalu pddl domain axiom show exploit addit express axiom|['Shuwa Miura', 'Alex Fukunaga']|['cs.AI']
2017-03-28T14:05:54Z|2017-03-11T05:35:09Z|http://arxiv.org/abs/1703.03912v1|http://arxiv.org/pdf/1703.03912v1|The Curse of Correlation in Security Games and Principle of Max-Entropy|curs correl secur game principl max entropi|In this paper, we identify and study a fundamental, yet underexplored, phenomenon in security games, which we term the Curse of Correlation (CoC). Specifically, we observe that there is inevitable correlation among the protection status of different targets. Such correlation is a crucial concern, especially in spatio-temporal domains like conservation area patrolling, where attackers can monitor patrollers at certain areas and then infer their patrolling routes using such correlation. To mitigate this issue, we introduce the principle of max-entropy to security games, and focus on designing entropy-maximizing defending strategies for the spatio-temporal security game -- a major victim of CoC. We prove that the problem is #P-hard in general, but propose efficient algorithms in well-motivated special settings. Our experiments show significant advantages of the max-entropy algorithms against previous algorithms.|paper identifi studi fundament yet underexplor phenomenon secur game term curs correl coc specif observ inevit correl among protect status differ target correl crucial concern especi spatio tempor domain like conserv area patrol attack monitor patrol certain area infer patrol rout use correl mitig issu introduc principl max entropi secur game focus design entropi maxim defend strategi spatio tempor secur game major victim coc prove problem hard general propos effici algorithm well motiv special set experi show signific advantag max entropi algorithm previous algorithm|['Haifeng Xu', 'Milind Tambe', 'Shaddin Dughmi', 'Venil Loyd Noronha']|['cs.GT', 'cs.AI', 'cs.CR']
