{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### マージしたコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import feedparser\n",
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Arxiv(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.date_format = '%Y-%m-%dT%H:%M:%SZ'  #Formatting directives\n",
    "        self.root_url = 'http://export.arxiv.org/api/'\n",
    "        self.qsearch = \"query?search_query=\"\n",
    "        \n",
    "    \n",
    "    def count_total_papers(self, category):\n",
    "        # category = stat.ML\n",
    "        url = self.root_url + self.qsearch + \"cat:\" + category + \"&start=0&sortBy=submittedDate&sortOrder=descending&max_results=1\"\n",
    "        d = feedparser.parse(url)\n",
    "        print('status: ' + str(d['status']))\n",
    "        if d['status'] != 200:\n",
    "            print(\"cannot GET url...\")\n",
    "            return None\n",
    "        else:\n",
    "            return int(d['feed']['opensearch_totalresults'])\n",
    "    \n",
    "    def retrieve_results(self, url):\n",
    "        return feedparser.parse(url)['entries']\n",
    "    \n",
    "    def open_data(self, output_type, cat, dir_):\n",
    "        if output_type == 1:\n",
    "            return open('./'+dir_+'_csv/'+cat+'.csv', 'a')\n",
    "        elif output_type == 2:\n",
    "            return open('./'+dir_+'_txt/'+cat+'.txt', 'a')\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def refactor_data(self, result, output_type):\n",
    "        fd = FormatData()\n",
    "        link = result['link']\n",
    "        try:\n",
    "            pdf = result['links'][1]['href']\n",
    "        except:\n",
    "            pdf = ''\n",
    "        title = result['title']\n",
    "        summary = result['summary']\n",
    "        authors = result['authors']\n",
    "        tags = result['tags']\n",
    "        if output_type == 1:\n",
    "            title = title.replace(',', '')\n",
    "            summary = summary.replace(',', '')\n",
    "            # make authors into one string\n",
    "            author_names = fd.arr2csv(authors, 'name')\n",
    "            # get category keyword\n",
    "            tag_names = fd.arr2csv(tags, 'term')\n",
    "        else:\n",
    "            author_names = fd.arr2psv(authors, 'name')\n",
    "            tag_names = fd.arr2psv(tags, 'term')\n",
    "        return [link, pdf, title, summary, author_names, tag_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FormatData(object):\n",
    "    '''\n",
    "    object for preprocessing raw data \n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def init_nltk(self):\n",
    "        corpus = ['snowball_data', 'stopwords'] # corpus/model for nltk\n",
    "        for corpa in corpus:\n",
    "            if self.download_model(corpa):\n",
    "                print('checked that snowball_data exists...')\n",
    "            else:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def download_model(self, model):\n",
    "        return nltk.download(model) # returns boolean\n",
    "    \n",
    "    def format_word(self, word, digits=True, char_length=2):\n",
    "        sno = nltk.stem.SnowballStemmer('english')\n",
    "        if digits == True:\n",
    "            word = ''.join([i for i in word if not i.isdigit()])\n",
    "        if len(word) < char_length:\n",
    "            return None\n",
    "        else:\n",
    "            return sno.stem(word)\n",
    "    \n",
    "    def format_string(self, a):\n",
    "        sno = nltk.stem.SnowballStemmer('english')\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        a = a.lower()\n",
    "        a = re.sub('[^a-zA-Z0-9]', ' ', a)\n",
    "        words = a.split(' ')\n",
    "        _new_words = []\n",
    "        for word in words:\n",
    "            word = self.format_word(word)\n",
    "            if not (word is None):\n",
    "                if word not in stopwords:\n",
    "                    _new_words.append(word)\n",
    "        a = ' '.join(_new_words) # return string concatinating every words\n",
    "        return a\n",
    "    \n",
    "    def arr2csv(self, items, query):\n",
    "        item_list = items[0][query]\n",
    "        items.pop(0)\n",
    "        if len(items) > 0:\n",
    "            for item in items:\n",
    "                item_list = item_list.replace(',', '') + '|' + item[query]\n",
    "        return item_list\n",
    "    \n",
    "    def arr2psv(self, items, query):\n",
    "        arr = []\n",
    "        for item in items:\n",
    "            arr.append(item[query].replace('|', ' '))\n",
    "        return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataRetriever(Arxiv): # Object for getting data\n",
    "    def __init__(self, total_paper=100, max_number=10):\n",
    "        Arxiv.__init__(self)\n",
    "        self.total_paper = total_paper # number of total papers to get\n",
    "        self.max_number = max_number # number of papers per each api requests (maximum request size is 10)\n",
    "        pass\n",
    "    \n",
    "    def data2df(self, categories, delay):\n",
    "        fd = FormatData()\n",
    "        df = pd.DataFrame()\n",
    "        data = []\n",
    "        \n",
    "        for cat in categories:\n",
    "            total_num = self.count_total_papers(cat)\n",
    "        \n",
    "            if total_num < self.total_paper:\n",
    "                total_paper_ = total_num\n",
    "            else:\n",
    "                total_paper_=self.total_paper\n",
    "            remainder = total_paper_%self.max_number\n",
    "            num_run = total_paper_//self.max_number\n",
    "            if remainder > 0:\n",
    "                num_run += 1\n",
    "            \n",
    "            for n in range(0, num_run):\n",
    "                url = self.root_url + self.qsearch + \"cat:\" + cat+ \"&start=\" + str(n*self.max_number) + \"&sortBy=submittedDate&sortOrder=descending&max_results=\" + str(self.max_number)\n",
    "                d = feedparser.parse(url)\n",
    "                results = d['entries']\n",
    "                for result in results:\n",
    "                    fd = FormatData()\n",
    "                    # get today's time\n",
    "                    unix_epoch = time.time()           #the popular UNIX epoch time in seconds\n",
    "                    ts = datetime.datetime.fromtimestamp(unix_epoch)\n",
    "                    today = ts.strftime(self.date_format)\n",
    "\n",
    "                    updated = result['updated']\n",
    "                    us = datetime.datetime.strptime(updated, self.date_format)\n",
    "\n",
    "                    if (ts - us).days != delay:\n",
    "                        continue\n",
    "                    else:\n",
    "                        link = result['link']\n",
    "                        pdf = result['links'][1]['href']\n",
    "                        title = result['title'].replace('|', ' ')\n",
    "                        title = title.replace('\\n', ' ')\n",
    "                        title_refact = fd.format_string(result['title'])\n",
    "\n",
    "                        summary = result['summary'].replace('|', ' ')\n",
    "                        summary = summary.replace('\\n', ' ')\n",
    "                        summary_refact = fd.format_string(result['summary'])\n",
    "                        authors = result['authors']\n",
    "                        author_names = fd.arr2psv(authors, 'name')\n",
    "                        tags = result['tags']\n",
    "                        tag_names = fd.arr2psv(tags, 'term')\n",
    "                        data.append([today, updated, link, pdf, title, title_refact, summary, summary_refact, author_names, tag_names])\n",
    "                df = pd.DataFrame(data)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CategoryClassifier(object): # object for classifying categories\n",
    "    def __init__(self):\n",
    "        self.header_names =  [\"プログラム実行日時\", \"論文更新日時\", \"論文リンク\", \"PDFリンク\", \"元論文タイトル\", \"論文タイトル\", \"元サマリ\", \"サマリ\", \"著者\", \"事前付与ジャンル\"]\n",
    "        self.cat_names = [\"ニューラルネットワーク\", \"自然言語処理\", \"マーケティング\", \"画像解析\", \"音声解析\", \"強化学習\"]\n",
    "        self.today = datetime.date.today()\n",
    "        \n",
    "    def classify(self, df_recv, pcat_list, word_dict_list, word_num_all_list, threshold = 1.2, title_weight = 15):\n",
    "        df_recv.columns = self.header_names\n",
    "\n",
    "        pred_list = []\n",
    "        pred_result = []\n",
    "        for j, row in df_recv.iterrows(): #元のデータフレームからテキストを抽出\n",
    "            document = (row[\"論文タイトル\"] + \" \") * title_weight + row[\"サマリ\"]\n",
    "            words = document.split()\n",
    "\n",
    "            cat_prob = []\n",
    "            for i, word_dict in enumerate(word_dict_list):\n",
    "                prob = np.log(pcat_list[i]) #初期化\n",
    "                for word in words:\n",
    "                    if word in word_dict:\n",
    "                        prob += np.log(word_dict[word])\n",
    "                    else:\n",
    "                        prob += np.log(1.0/word_num_all_list[i])\n",
    "                cat_prob.append(prob)\n",
    "            pred_result.append(list((cat_prob - np.mean(cat_prob))/np.std(cat_prob) > threshold)) #事前に算出した閾値\n",
    "\n",
    "            df_pred = pd.DataFrame(pred_result)\n",
    "            df_classified = df_recv.join(df_pred)        \n",
    "            df_classified.columns = self.header_names + self.cat_names #ヘッダ追加\n",
    "        return df_classified\n",
    "    \n",
    "    def output_csv(self, df):\n",
    "        drop_list = ['プログラム実行日時', '論文更新日時', 'PDFリンク', '論文タイトル', 'サマリ', '事前付与ジャンル', 'ニューラルネットワーク', '自然言語処理', 'マーケティング', '画像解析', '音声解析', '強化学習']\n",
    "        df_all = pd.DataFrame(columns = ['論文リンク',  '元論文タイトル', '元サマリ', '著者'])\n",
    "        for cat in self.cat_names:\n",
    "            df_classified = df[df[cat]== True]\n",
    "            df_classified = df_classified.drop(drop_list, axis = 1)\n",
    "            df_classified['カテゴリ'] = [cat] * len(df_classified)\n",
    "            df_all = pd.concat([df_all, df_classified])\n",
    "        df_all.to_csv(\"./sample_csv/\" + str(self.today.month) +\"_\" + str(self.today.day) + \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status: 200\n",
      "status: 200\n",
      "status: 200\n",
      "status: 200\n",
      "status: 200\n",
      "status: 200\n",
      "status: 200\n",
      "status: 200\n",
      "status: 200\n",
      "status: 200\n",
      "status: 200\n",
      "status: 200\n",
      "status: 200\n",
      "status: 200\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    categories = [\"stat.ML\", \"stat.AP\", \"stat.CO\", \"stat.ME\", \"stat.TH\", \"cs.AI\", \"cs.CL\", \"cs.CC\", \"cs.CG\", \"cs.GT\", \"cs.CV\", \"cs.DS\", \"cs.MA\", \"cs.SD\"]\n",
    "    \n",
    "    # パラメータ\n",
    "    delay = 2\n",
    "    threshold = 1.2\n",
    "    title_weight = 15\n",
    "\n",
    "    dr = DataRetriever()\n",
    "    cc = CategoryClassifier()\n",
    "    df_recv = dr.data2df(categories, delay)\n",
    "    \n",
    "    # pickle load\n",
    "    with open('pcat_list.dump', 'rb') as f:\n",
    "        pcat_list = pickle.load(f)\n",
    "    with open('word_dict_list.dump', 'rb') as f:\n",
    "        word_dict_list = pickle.load(f)\n",
    "    with open('word_num_all_list.dump', 'rb') as f:\n",
    "        word_num_all_list = pickle.load(f)\n",
    "\n",
    "    df_classified = cc.classify(df_recv, pcat_list, word_dict_list, word_num_all_list, threshold, title_weight)\n",
    "    cc.output_csv(df_classified) #CSV書き出し\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
